10824	def query_by_group ( cls , group_or_id , with_invitations = False , * * kwargs ) : if isinstance ( group_or_id , Group ) : id_group = group_or_id . id else : id_group = group_or_id if not with_invitations : return cls . _filter ( cls . query . filter_by ( id_group = id_group ) , * * kwargs ) else : return cls . query . filter ( Membership . id_group == id_group , db . or_ ( Membership . state == MembershipState . PENDING_USER , Membership . state == MembershipState . ACTIVE ) )
10501	def waitForWindowToDisappear ( self , winName , timeout = 10 ) : callback = AXCallbacks . elemDisappearedCallback retelem = None args = ( retelem , self ) # For some reason for the AXUIElementDestroyed notification to fire, # we need to have a reference to it first win = self . findFirst ( AXRole = 'AXWindow' , AXTitle = winName ) return self . waitFor ( timeout , 'AXUIElementDestroyed' , callback = callback , args = args , AXRole = 'AXWindow' , AXTitle = winName )
6660	def random_forest_error ( forest , X_train , X_test , inbag = None , calibrate = True , memory_constrained = False , memory_limit = None ) : if inbag is None : inbag = calc_inbag ( X_train . shape [ 0 ] , forest ) pred = np . array ( [ tree . predict ( X_test ) for tree in forest ] ) . T pred_mean = np . mean ( pred , 0 ) pred_centered = pred - pred_mean n_trees = forest . n_estimators V_IJ = _core_computation ( X_train , X_test , inbag , pred_centered , n_trees , memory_constrained , memory_limit ) V_IJ_unbiased = _bias_correction ( V_IJ , inbag , pred_centered , n_trees ) # Correct for cases where resampling is done without replacement: if np . max ( inbag ) == 1 : variance_inflation = 1 / ( 1 - np . mean ( inbag ) ) ** 2 V_IJ_unbiased *= variance_inflation if not calibrate : return V_IJ_unbiased if V_IJ_unbiased . shape [ 0 ] <= 20 : print ( "No calibration with n_samples <= 20" ) return V_IJ_unbiased if calibrate : calibration_ratio = 2 n_sample = np . ceil ( n_trees / calibration_ratio ) new_forest = copy . deepcopy ( forest ) new_forest . estimators_ = np . random . permutation ( new_forest . estimators_ ) [ : int ( n_sample ) ] new_forest . n_estimators = int ( n_sample ) results_ss = random_forest_error ( new_forest , X_train , X_test , calibrate = False , memory_constrained = memory_constrained , memory_limit = memory_limit ) # Use this second set of variance estimates # to estimate scale of Monte Carlo noise sigma2_ss = np . mean ( ( results_ss - V_IJ_unbiased ) ** 2 ) delta = n_sample / n_trees sigma2 = ( delta ** 2 + ( 1 - delta ) ** 2 ) / ( 2 * ( 1 - delta ) ** 2 ) * sigma2_ss # Use Monte Carlo noise scale estimate for empirical Bayes calibration V_IJ_calibrated = calibrateEB ( V_IJ_unbiased , sigma2 ) return V_IJ_calibrated
10472	def _addKeyToQueue ( self , keychr , modFlags = 0 , globally = False ) : # Awkward, but makes modifier-key-only combinations possible # (since sendKeyWithModifiers() calls this) if not keychr : return if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) if keychr in self . keyboard [ 'upperSymbols' ] and not modFlags : self . _sendKeyWithModifiers ( keychr , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr . isupper ( ) and not modFlags : self . _sendKeyWithModifiers ( keychr . lower ( ) , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr not in self . keyboard : self . _clearEventQueue ( ) raise ValueError ( 'Key %s not found in keyboard layout' % keychr ) # Press the key keyDown = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , True ) # Release the key keyUp = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , False ) # Set modflags on keyDown (default None): Quartz . CGEventSetFlags ( keyDown , modFlags ) # Set modflags on keyUp: Quartz . CGEventSetFlags ( keyUp , modFlags ) # Post the event to the given app if not globally : # To direct output to the correct application need the PSN (macOS <=10.10) or PID(macOS > 10.10): macVer , _ , _ = platform . mac_ver ( ) macVer = int ( macVer . split ( '.' ) [ 1 ] ) if macVer > 10 : appPid = self . _getPid ( ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyUp ) ) else : appPsn = self . _getPsnForPid ( self . _getPid ( ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyUp ) ) else : self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyDown ) ) self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyUp ) )
12793	def get ( self , url = None , parse_data = True , key = None , parameters = None ) : return self . _fetch ( "GET" , url , post_data = None , parse_data = parse_data , key = key , parameters = parameters )
10078	def _process_files ( self , record_id , data ) : if self . files : assert not self . files . bucket . locked self . files . bucket . locked = True snapshot = self . files . bucket . snapshot ( lock = True ) data [ '_files' ] = self . files . dumps ( bucket = snapshot . id ) yield data db . session . add ( RecordsBuckets ( record_id = record_id , bucket_id = snapshot . id ) ) else : yield data
12626	def recursive_find_search ( folder_path , regex = '' ) : outlist = [ ] for root , dirs , files in os . walk ( folder_path ) : outlist . extend ( [ op . join ( root , f ) for f in files if re . search ( regex , f ) ] ) return outlist
13048	def f_annotation_filter ( annotations , type_uri , number ) : filtered = [ annotation for annotation in annotations if annotation . type_uri == type_uri ] number = min ( [ len ( filtered ) , number ] ) if number == 0 : return None else : return filtered [ number - 1 ]
11215	def compare_token ( expected : Union [ str , bytes ] , actual : Union [ str , bytes ] ) -> bool : expected = util . to_bytes ( expected ) actual = util . to_bytes ( actual ) _ , expected_sig_seg = expected . rsplit ( b'.' , 1 ) _ , actual_sig_seg = actual . rsplit ( b'.' , 1 ) expected_sig = util . b64_decode ( expected_sig_seg ) actual_sig = util . b64_decode ( actual_sig_seg ) return compare_signature ( expected_sig , actual_sig )
13269	def deparagraph ( element , doc ) : if isinstance ( element , Para ) : # Check if siblings exist; don't process the paragraph in that case. if element . next is not None : return element elif element . prev is not None : return element # Remove the Para wrapper from the lone paragraph. # `Plain` is a container that isn't rendered as a paragraph. return Plain ( * element . content )
1722	def is_lval ( t ) : if not t : return False i = iter ( t ) if i . next ( ) not in IDENTIFIER_START : return False return all ( e in IDENTIFIER_PART for e in i )
7845	def get_items ( self ) : ret = [ ] l = self . xpath_ctxt . xpathEval ( "d:item" ) if l is not None : for i in l : ret . append ( DiscoItem ( self , i ) ) return ret
12835	def on_exit_stage ( self ) : # 1. Let the forum react to the end of the game. Local forums don't # react to this, but remote forums take the opportunity to stop # trying to extract tokens from messages. self . forum . on_finish_game ( ) # 2. Let the actors react to the end of the game. for actor in self . actors : actor . on_finish_game ( ) # 3. Let the world react to the end of the game. with self . world . _unlock_temporarily ( ) : self . world . on_finish_game ( )
3415	def model_from_dict ( obj ) : if 'reactions' not in obj : raise ValueError ( 'Object has no reactions attribute. Cannot load.' ) model = Model ( ) model . add_metabolites ( [ metabolite_from_dict ( metabolite ) for metabolite in obj [ 'metabolites' ] ] ) model . genes . extend ( [ gene_from_dict ( gene ) for gene in obj [ 'genes' ] ] ) model . add_reactions ( [ reaction_from_dict ( reaction , model ) for reaction in obj [ 'reactions' ] ] ) objective_reactions = [ rxn for rxn in obj [ 'reactions' ] if rxn . get ( 'objective_coefficient' , 0 ) != 0 ] coefficients = { model . reactions . get_by_id ( rxn [ 'id' ] ) : rxn [ 'objective_coefficient' ] for rxn in objective_reactions } set_objective ( model , coefficients ) for k , v in iteritems ( obj ) : if k in { 'id' , 'name' , 'notes' , 'compartments' , 'annotation' } : setattr ( model , k , v ) return model
709	def runWithPermutationsScript ( permutationsFilePath , options , outputLabel , permWorkDir ) : global g_currentVerbosityLevel if "verbosityCount" in options : g_currentVerbosityLevel = options [ "verbosityCount" ] del options [ "verbosityCount" ] else : g_currentVerbosityLevel = 1 _setupInterruptHandling ( ) options [ "permutationsScriptPath" ] = permutationsFilePath options [ "outputLabel" ] = outputLabel options [ "outDir" ] = permWorkDir options [ "permWorkDir" ] = permWorkDir # Assume it's a permutations python script runOptions = _injectDefaultOptions ( options ) _validateOptions ( runOptions ) return _runAction ( runOptions )
12356	def wait ( self ) : interval_seconds = 5 while True : actions = self . actions ( ) slept = False for a in actions : if a [ 'status' ] == 'in-progress' : # n.b. gevent will monkey patch time . sleep ( interval_seconds ) slept = True break if not slept : break
5352	def __studies ( self , retention_time ) : cfg = self . config . get_conf ( ) if 'studies' not in cfg [ self . backend_section ] or not cfg [ self . backend_section ] [ 'studies' ] : logger . debug ( 'No studies for %s' % self . backend_section ) return studies = [ study for study in cfg [ self . backend_section ] [ 'studies' ] if study . strip ( ) != "" ] if not studies : logger . debug ( 'No studies for %s' % self . backend_section ) return logger . debug ( "Executing studies for %s: %s" % ( self . backend_section , studies ) ) time . sleep ( 2 ) # Wait so enrichment has finished in ES enrich_backend = self . _get_enrich_backend ( ) ocean_backend = self . _get_ocean_backend ( enrich_backend ) active_studies = [ ] all_studies = enrich_backend . studies all_studies_names = [ study . __name__ for study in enrich_backend . studies ] # Time to check that configured studies are valid logger . debug ( "All studies in %s: %s" , self . backend_section , all_studies_names ) logger . debug ( "Configured studies %s" , studies ) cfg_studies_types = [ study . split ( ":" ) [ 0 ] for study in studies ] if not set ( cfg_studies_types ) . issubset ( set ( all_studies_names ) ) : logger . error ( 'Wrong studies names for %s: %s' , self . backend_section , studies ) raise RuntimeError ( 'Wrong studies names ' , self . backend_section , studies ) for study in enrich_backend . studies : if study . __name__ in cfg_studies_types : active_studies . append ( study ) enrich_backend . studies = active_studies print ( "Executing for %s the studies %s" % ( self . backend_section , [ study for study in studies ] ) ) studies_args = self . __load_studies ( ) do_studies ( ocean_backend , enrich_backend , studies_args , retention_time = retention_time ) # Return studies to its original value enrich_backend . studies = all_studies
231	def compute_sector_exposures ( positions , sectors , sector_dict = SECTORS ) : sector_ids = sector_dict . keys ( ) long_exposures = [ ] short_exposures = [ ] gross_exposures = [ ] net_exposures = [ ] positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) long_exposure = positions_wo_cash [ positions_wo_cash > 0 ] . sum ( axis = 'columns' ) short_exposure = positions_wo_cash [ positions_wo_cash < 0 ] . abs ( ) . sum ( axis = 'columns' ) gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) for sector_id in sector_ids : in_sector = positions_wo_cash [ sectors == sector_id ] long_sector = in_sector [ in_sector > 0 ] . sum ( axis = 'columns' ) . divide ( long_exposure ) short_sector = in_sector [ in_sector < 0 ] . sum ( axis = 'columns' ) . divide ( short_exposure ) gross_sector = in_sector . abs ( ) . sum ( axis = 'columns' ) . divide ( gross_exposure ) net_sector = long_sector . subtract ( short_sector ) long_exposures . append ( long_sector ) short_exposures . append ( short_sector ) gross_exposures . append ( gross_sector ) net_exposures . append ( net_sector ) return long_exposures , short_exposures , gross_exposures , net_exposures
2872	def all_info_files ( self ) : try : for info_file in list_files_in_dir ( self . info_dir ) : if not os . path . basename ( info_file ) . endswith ( '.trashinfo' ) : self . on_non_trashinfo_found ( ) else : yield info_file except OSError : # when directory does not exist pass
6496	def _get_mappings ( self , doc_type ) : # Try loading the mapping from the cache. mapping = ElasticSearchEngine . get_mappings ( self . index_name , doc_type ) # Fall back to Elasticsearch if not mapping : mapping = self . _es . indices . get_mapping ( index = self . index_name , doc_type = doc_type , ) . get ( self . index_name , { } ) . get ( 'mappings' , { } ) . get ( doc_type , { } ) # Cache the mapping, if one was retrieved if mapping : ElasticSearchEngine . set_mappings ( self . index_name , doc_type , mapping ) return mapping
83	def Pepper ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : replacement01 = iap . ForceSign ( iap . Beta ( 0.5 , 0.5 ) - 0.5 , positive = False , mode = "invert" ) + 0.5 replacement = replacement01 * 255 if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = p , replacement = replacement , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
11611	def run ( self , model , tol = 0.001 , max_iters = 999 , verbose = True ) : orig_err_states = np . seterr ( all = 'raise' ) np . seterr ( under = 'ignore' ) if verbose : print print "Iter No Time (hh:mm:ss) Total change (TPM) " print "------- --------------- ----------------------" num_iters = 0 err_sum = 1000000.0 time0 = time . time ( ) target_err = 1000000.0 * tol while err_sum > target_err and num_iters < max_iters : prev_isoform_expression = self . get_allelic_expression ( ) . sum ( axis = 0 ) prev_isoform_expression *= ( 1000000.0 / prev_isoform_expression . sum ( ) ) self . update_allelic_expression ( model = model ) curr_isoform_expression = self . get_allelic_expression ( ) . sum ( axis = 0 ) curr_isoform_expression *= ( 1000000.0 / curr_isoform_expression . sum ( ) ) err = np . abs ( curr_isoform_expression - prev_isoform_expression ) err_sum = err . sum ( ) num_iters += 1 if verbose : time1 = time . time ( ) delmin , s = divmod ( int ( time1 - time0 ) , 60 ) h , m = divmod ( delmin , 60 ) print " %5d %4d:%02d:%02d %9.1f / 1000000" % ( num_iters , h , m , s , err_sum )
6466	def color ( self , index ) : if self . colors == 16 : if index >= 8 : return self . csi ( 'bold' ) + self . csi ( 'setaf' , index - 8 ) else : return self . csi ( 'sgr0' ) + self . csi ( 'setaf' , index ) else : return self . csi ( 'setaf' , index )
714	def loadSavedHyperSearchJob ( cls , permWorkDir , outputLabel ) : jobID = cls . __loadHyperSearchJobID ( permWorkDir = permWorkDir , outputLabel = outputLabel ) searchJob = _HyperSearchJob ( nupicJobID = jobID ) return searchJob
48	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 3 , copy = True , raise_if_out_of_image = False ) : if copy : image = np . copy ( image ) if image . ndim == 2 : assert ia . is_single_number ( color ) , ( "Got a 2D image. Expected then 'color' to be a single number, " "but got %s." % ( str ( color ) , ) ) elif image . ndim == 3 and ia . is_single_number ( color ) : color = [ color ] * image . shape [ - 1 ] input_dtype = image . dtype alpha_color = color if alpha < 0.01 : # keypoint invisible, nothing to do return image elif alpha > 0.99 : alpha = 1 else : image = image . astype ( np . float32 , copy = False ) alpha_color = alpha * np . array ( color ) height , width = image . shape [ 0 : 2 ] y , x = self . y_int , self . x_int x1 = max ( x - size // 2 , 0 ) x2 = min ( x + 1 + size // 2 , width ) y1 = max ( y - size // 2 , 0 ) y2 = min ( y + 1 + size // 2 , height ) x1_clipped , x2_clipped = np . clip ( [ x1 , x2 ] , 0 , width ) y1_clipped , y2_clipped = np . clip ( [ y1 , y2 ] , 0 , height ) x1_clipped_ooi = ( x1_clipped < 0 or x1_clipped >= width ) x2_clipped_ooi = ( x2_clipped < 0 or x2_clipped >= width + 1 ) y1_clipped_ooi = ( y1_clipped < 0 or y1_clipped >= height ) y2_clipped_ooi = ( y2_clipped < 0 or y2_clipped >= height + 1 ) x_ooi = ( x1_clipped_ooi and x2_clipped_ooi ) y_ooi = ( y1_clipped_ooi and y2_clipped_ooi ) x_zero_size = ( x2_clipped - x1_clipped ) < 1 # min size is 1px y_zero_size = ( y2_clipped - y1_clipped ) < 1 if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size : if alpha == 1 : image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] = color else : image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] = ( ( 1 - alpha ) * image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] + alpha_color ) else : if raise_if_out_of_image : raise Exception ( "Cannot draw keypoint x=%.8f, y=%.8f on image with " "shape %s." % ( y , x , image . shape ) ) if image . dtype . name != input_dtype . name : if input_dtype . name == "uint8" : image = np . clip ( image , 0 , 255 , out = image ) image = image . astype ( input_dtype , copy = False ) return image
6500	def perform_search ( search_term , user = None , size = 10 , from_ = 0 , course_id = None ) : # field_, filter_ and exclude_dictionary(s) can be overridden by calling application # field_dictionary includes course if course_id provided ( field_dictionary , filter_dictionary , exclude_dictionary ) = SearchFilterGenerator . generate_field_filters ( user = user , course_id = course_id ) searcher = SearchEngine . get_search_engine ( getattr ( settings , "COURSEWARE_INDEX_NAME" , "courseware_index" ) ) if not searcher : raise NoSearchEngineError ( "No search engine specified in settings.SEARCH_ENGINE" ) results = searcher . search_string ( search_term , field_dictionary = field_dictionary , filter_dictionary = filter_dictionary , exclude_dictionary = exclude_dictionary , size = size , from_ = from_ , doc_type = "courseware_content" , ) # post-process the result for result in results [ "results" ] : result [ "data" ] = SearchResultProcessor . process_result ( result [ "data" ] , search_term , user ) results [ "access_denied_count" ] = len ( [ r for r in results [ "results" ] if r [ "data" ] is None ] ) results [ "results" ] = [ r for r in results [ "results" ] if r [ "data" ] is not None ] return results
11179	def authorize_url ( self ) : auth_url = OAUTH_ROOT + '/authorize' params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , } return "{}?{}" . format ( auth_url , urlencode ( params ) )
1213	def _run_single ( self , thread_id , agent , environment , deterministic = False , max_episode_timesteps = - 1 , episode_finished = None , testing = False , sleep = None ) : # figure out whether we are using the deprecated way of "episode_finished" reporting old_episode_finished = False if episode_finished is not None and len ( getargspec ( episode_finished ) . args ) == 1 : old_episode_finished = True episode = 0 # Run this single worker (episode loop) as long as global count thresholds have not been reached. while not self . should_stop : state = environment . reset ( ) agent . reset ( ) self . global_timestep , self . global_episode = agent . timestep , agent . episode episode_reward = 0 # Time step (within episode) loop time_step = 0 time_start = time . time ( ) while True : action , internals , states = agent . act ( states = state , deterministic = deterministic , buffered = False ) reward = 0 for repeat in xrange ( self . repeat_actions ) : state , terminal , step_reward = environment . execute ( action = action ) reward += step_reward if terminal : break if not testing : # agent.observe(reward=reward, terminal=terminal) # Insert everything at once. agent . atomic_observe ( states = state , actions = action , internals = internals , reward = reward , terminal = terminal ) if sleep is not None : time . sleep ( sleep ) time_step += 1 episode_reward += reward if terminal or time_step == max_episode_timesteps : break # Abort the episode (discard its results) when global says so. if self . should_stop : return self . global_timestep += time_step # Avoid race condition where order in episode_rewards won't match order in episode_timesteps. self . episode_list_lock . acquire ( ) self . episode_rewards . append ( episode_reward ) self . episode_timesteps . append ( time_step ) self . episode_times . append ( time . time ( ) - time_start ) self . episode_list_lock . release ( ) if episode_finished is not None : # old way of calling episode_finished if old_episode_finished : summary_data = { "thread_id" : thread_id , "episode" : episode , "timestep" : time_step , "episode_reward" : episode_reward } if not episode_finished ( summary_data ) : return # New way with BasicRunner (self) and thread-id. elif not episode_finished ( self , thread_id ) : return episode += 1
5852	def get_dataset_file ( self , dataset_id , file_path , version = None ) : return self . get_dataset_files ( dataset_id , "^{}$" . format ( file_path ) , version_number = version ) [ 0 ]
2332	def uniform_noise ( points ) : return np . random . rand ( 1 ) * np . random . uniform ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
978	def _countOverlapIndices ( self , i , j ) : if self . bucketMap . has_key ( i ) and self . bucketMap . has_key ( j ) : iRep = self . bucketMap [ i ] jRep = self . bucketMap [ j ] return self . _countOverlap ( iRep , jRep ) else : raise ValueError ( "Either i or j don't exist" )
713	def __startSearch ( self ) : # This search uses a pre-existing permutations script params = _ClientJobUtils . makeSearchJobParamsDict ( options = self . _options , forRunning = True ) if self . _options [ "action" ] == "dryRun" : args = [ sys . argv [ 0 ] , "--params=%s" % ( json . dumps ( params ) ) ] print print "==================================================================" print "RUNNING PERMUTATIONS INLINE as \"DRY RUN\"..." print "==================================================================" jobID = hypersearch_worker . main ( args ) else : cmdLine = _setUpExports ( self . _options [ "exports" ] ) # Begin the new search. The {JOBID} string is replaced by the actual # jobID returned from jobInsert. cmdLine += "$HYPERSEARCH" maxWorkers = self . _options [ "maxWorkers" ] jobID = self . __cjDAO . jobInsert ( client = "GRP" , cmdLine = cmdLine , params = json . dumps ( params ) , minimumWorkers = 1 , maximumWorkers = maxWorkers , jobType = self . __cjDAO . JOB_TYPE_HS ) cmdLine = "python -m nupic.swarming.hypersearch_worker" " --jobID=%d" % ( jobID ) self . _launchWorkers ( cmdLine , maxWorkers ) searchJob = _HyperSearchJob ( jobID ) # Save search ID to file (this is used for report generation) self . __saveHyperSearchJobID ( permWorkDir = self . _options [ "permWorkDir" ] , outputLabel = self . _options [ "outputLabel" ] , hyperSearchJob = searchJob ) if self . _options [ "action" ] == "dryRun" : print "Successfully executed \"dry-run\" hypersearch, jobID=%d" % ( jobID ) else : print "Successfully submitted new HyperSearch job, jobID=%d" % ( jobID ) _emit ( Verbosity . DEBUG , "Each worker executing the command line: %s" % ( cmdLine , ) ) return searchJob
11021	def _generate_circle ( self ) : total_weight = 0 for node in self . nodes : total_weight += self . weights . get ( node , 1 ) for node in self . nodes : weight = 1 if node in self . weights : weight = self . weights . get ( node ) factor = math . floor ( ( 40 * len ( self . nodes ) * weight ) / total_weight ) for j in range ( 0 , int ( factor ) ) : b_key = bytearray ( self . _hash_digest ( '%s-%s' % ( node , j ) ) ) for i in range ( 0 , 3 ) : key = self . _hash_val ( b_key , lambda x : x + i * 4 ) self . ring [ key ] = node self . _sorted_keys . append ( key ) self . _sorted_keys . sort ( )
13396	def settings_and_attributes ( self ) : attrs = self . setting_values ( ) attrs . update ( self . __dict__ ) skip = [ "_instance_settings" , "aliases" ] for a in skip : del attrs [ a ] return attrs
3512	def optimizely ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return OptimizelyNode ( )
3776	def T_dependent_property_derivative ( self , T , order = 1 ) : if self . method : # retest within range if self . test_method_validity ( T , self . method ) : try : return self . calculate_derivative ( T , self . method , order ) except : # pragma: no cover pass sorted_valid_methods = self . select_valid_methods ( T ) for method in sorted_valid_methods : try : return self . calculate_derivative ( T , method , order ) except : pass return None
7199	def get_chip ( self , coordinates , catid , chip_type = 'PAN' , chip_format = 'TIF' , filename = 'chip.tif' ) : def t2s1 ( t ) : # Tuple to string 1 return str ( t ) . strip ( '(,)' ) . replace ( ',' , '' ) def t2s2 ( t ) : # Tuple to string 2 return str ( t ) . strip ( '(,)' ) . replace ( ' ' , '' ) if len ( coordinates ) != 4 : print ( 'Wrong coordinate entry' ) return False W , S , E , N = coordinates box = ( ( W , S ) , ( W , N ) , ( E , N ) , ( E , S ) , ( W , S ) ) box_wkt = 'POLYGON ((' + ',' . join ( [ t2s1 ( corner ) for corner in box ] ) + '))' # get IDAHO images which intersect box results = self . get_images_by_catid_and_aoi ( catid = catid , aoi_wkt = box_wkt ) description = self . describe_images ( results ) pan_id , ms_id , num_bands = None , None , 0 for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : if 'PAN' in part . keys ( ) : pan_id = part [ 'PAN' ] [ 'id' ] bucket = part [ 'PAN' ] [ 'bucket' ] if 'WORLDVIEW_8_BAND' in part . keys ( ) : ms_id = part [ 'WORLDVIEW_8_BAND' ] [ 'id' ] num_bands = 8 bucket = part [ 'WORLDVIEW_8_BAND' ] [ 'bucket' ] elif 'RGBN' in part . keys ( ) : ms_id = part [ 'RGBN' ] [ 'id' ] num_bands = 4 bucket = part [ 'RGBN' ] [ 'bucket' ] # specify band information band_str = '' if chip_type == 'PAN' : band_str = pan_id + '?bands=0' elif chip_type == 'MS' : band_str = ms_id + '?' elif chip_type == 'PS' : if num_bands == 8 : band_str = ms_id + '?bands=4,2,1&panId=' + pan_id elif num_bands == 4 : band_str = ms_id + '?bands=0,1,2&panId=' + pan_id # specify location information location_str = '&upperLeft={}&lowerRight={}' . format ( t2s2 ( ( W , N ) ) , t2s2 ( ( E , S ) ) ) service_url = 'https://idaho.geobigdata.io/v1/chip/bbox/' + bucket + '/' url = service_url + band_str + location_str url += '&format=' + chip_format + '&token=' + self . gbdx_connection . access_token r = requests . get ( url ) if r . status_code == 200 : with open ( filename , 'wb' ) as f : f . write ( r . content ) return True else : print ( 'Cannot download chip' ) return False
2658	def _create_deployment ( self , deployment ) : api_response = self . kube_client . create_namespaced_deployment ( body = deployment , namespace = self . namespace ) logger . debug ( "Deployment created. status='{0}'" . format ( str ( api_response . status ) ) )
4637	def claim ( self , account = None , * * kwargs ) : if not account : if "default_account" in self . blockchain . config : account = self . blockchain . config [ "default_account" ] if not account : raise ValueError ( "You need to provide an account" ) account = self . account_class ( account , blockchain_instance = self . blockchain ) pubkeys = self . blockchain . wallet . getPublicKeys ( ) addresses = dict ( ) for p in pubkeys : if p [ : len ( self . blockchain . prefix ) ] != self . blockchain . prefix : continue pubkey = self . publickey_class ( p , prefix = self . blockchain . prefix ) addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = False , version = 0 , prefix = self . blockchain . prefix , ) ) ] = pubkey addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = True , version = 0 , prefix = self . blockchain . prefix , ) ) ] = pubkey addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = False , version = 56 , prefix = self . blockchain . prefix , ) ) ] = pubkey addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = True , version = 56 , prefix = self . blockchain . prefix , ) ) ] = pubkey if self [ "owner" ] not in addresses . keys ( ) : raise MissingKeyError ( "Need key for address {}" . format ( self [ "owner" ] ) ) op = self . operations . Balance_claim ( * * { "fee" : { "amount" : 0 , "asset_id" : "1.3.0" } , "deposit_to_account" : account [ "id" ] , "balance_to_claim" : self [ "id" ] , "balance_owner_key" : addresses [ self [ "owner" ] ] , "total_claimed" : self [ "balance" ] , "prefix" : self . blockchain . prefix , } ) signers = [ account [ "name" ] , # The fee payer and receiver account addresses . get ( self [ "owner" ] ) , # The genesis balance! ] return self . blockchain . finalizeOp ( op , signers , "active" , * * kwargs )
10	def save_policy ( self , path ) : with open ( path , 'wb' ) as f : pickle . dump ( self . policy , f )
13045	def f_i18n_iso ( isocode , lang = "eng" ) : if lang not in flask_nemo . _data . AVAILABLE_TRANSLATIONS : lang = "eng" try : return flask_nemo . _data . ISOCODES [ isocode ] [ lang ] except KeyError : return "Unknown"
7227	def paint ( self ) : snippet = { 'fill-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'fill-color' : VectorStyle . get_style_value ( self . color ) , 'fill-outline-color' : VectorStyle . get_style_value ( self . outline_color ) } if self . translate : snippet [ 'fill-translate' ] = self . translate return snippet
5257	def block_to_fork ( block_number ) : forks_by_block = { 0 : "frontier" , 1150000 : "homestead" , # 1920000 Dao 2463000 : "tangerine_whistle" , 2675000 : "spurious_dragon" , 4370000 : "byzantium" , #7280000: "constantinople", # Same Block as petersburg, commented to avoid conflicts 7280000 : "petersburg" , 9999999 : "serenity" # to be replaced after Serenity launch } fork_names = list ( forks_by_block . values ( ) ) fork_blocks = list ( forks_by_block . keys ( ) ) return fork_names [ bisect ( fork_blocks , block_number ) - 1 ]
11635	def oauth2_access_parser ( self , raw_access ) : parsed_access = json . loads ( raw_access . content . decode ( 'utf-8' ) ) self . access_token = parsed_access [ 'access_token' ] self . token_type = parsed_access [ 'token_type' ] self . refresh_token = parsed_access [ 'refresh_token' ] self . guid = parsed_access [ 'xoauth_yahoo_guid' ] credentials = { 'access_token' : self . access_token , 'token_type' : self . token_type , 'refresh_token' : self . refresh_token , 'guid' : self . guid } return credentials
1745	def _set_perms ( self , perms ) : assert isinstance ( perms , str ) and len ( perms ) <= 3 and perms . strip ( ) in [ '' , 'r' , 'w' , 'x' , 'rw' , 'r x' , 'rx' , 'rwx' , 'wx' , ] self . _perms = perms
3413	def _update_optional ( cobra_object , new_dict , optional_attribute_dict , ordered_keys ) : for key in ordered_keys : default = optional_attribute_dict [ key ] value = getattr ( cobra_object , key ) if value is None or value == default : continue new_dict [ key ] = _fix_type ( value )
13522	def _make_url ( self , slug ) : if slug . startswith ( "http" ) : return slug return "{0}{1}" . format ( self . server_url , slug )
6799	def database_renderer ( self , name = None , site = None , role = None ) : name = name or self . env . default_db_name site = site or self . genv . SITE role = role or self . genv . ROLE key = ( name , site , role ) self . vprint ( 'checking key:' , key ) if key not in self . _database_renderers : self . vprint ( 'No cached db renderer, generating...' ) if self . verbose : print ( 'db.name:' , name ) print ( 'db.databases:' , self . env . databases ) print ( 'db.databases[%s]:' % name , self . env . databases . get ( name ) ) d = type ( self . genv ) ( self . lenv ) d . update ( self . get_database_defaults ( ) ) d . update ( self . env . databases . get ( name , { } ) ) d [ 'db_name' ] = name if self . verbose : print ( 'db.d:' ) pprint ( d , indent = 4 ) print ( 'db.connection_handler:' , d . connection_handler ) if d . connection_handler == CONNECTION_HANDLER_DJANGO : self . vprint ( 'Using django handler...' ) dj = self . get_satchel ( 'dj' ) if self . verbose : print ( 'Loading Django DB settings for site {} and role {}.' . format ( site , role ) , file = sys . stderr ) dj . set_db ( name = name , site = site , role = role ) _d = dj . local_renderer . collect_genv ( include_local = True , include_global = False ) # Copy "dj_db_*" into "db_*". for k , v in _d . items ( ) : if k . startswith ( 'dj_db_' ) : _d [ k [ 3 : ] ] = v del _d [ k ] if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) elif d . connection_handler and d . connection_handler . startswith ( CONNECTION_HANDLER_CUSTOM + ':' ) : _callable_str = d . connection_handler [ len ( CONNECTION_HANDLER_CUSTOM + ':' ) : ] self . vprint ( 'Using custom handler %s...' % _callable_str ) _d = str_to_callable ( _callable_str ) ( role = self . genv . ROLE ) if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) r = LocalRenderer ( self , lenv = d ) # Optionally set any root logins needed for administrative commands. self . set_root_login ( r ) self . _database_renderers [ key ] = r else : self . vprint ( 'Cached db renderer found.' ) return self . _database_renderers [ key ]
11297	def _check_for_exceptions ( self , resp , multiple_rates ) : if resp [ 'rCode' ] != 100 : raise exceptions . get_exception_for_code ( resp [ 'rCode' ] ) ( resp ) results = resp [ 'results' ] if len ( results ) == 0 : raise exceptions . ZipTaxNoResults ( 'No results found' ) if len ( results ) > 1 and not multiple_rates : # It's fine if all the taxes are the same rates = [ result [ 'taxSales' ] for result in results ] if len ( set ( rates ) ) != 1 : raise exceptions . ZipTaxMultipleResults ( 'Multiple results found but requested only one' )
11157	def print_big_dir_and_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table1 = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p1 , size1 in size_table1 [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size1 ) , p1 . abspath ) ) size_table2 = sorted ( [ ( p , p . size ) for p in p1 . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p2 , size2 in size_table2 [ : top_n ] : print ( " {:<9} {:<9}" . format ( repr_data_size ( size2 ) , p2 . abspath ) )
12053	def getIDsFromFiles ( files ) : if type ( files ) is str : files = glob . glob ( files + "/*.*" ) IDs = [ ] for fname in files : if fname [ - 4 : ] . lower ( ) == '.abf' : ext = fname . split ( '.' ) [ - 1 ] IDs . append ( os . path . basename ( fname ) . replace ( '.' + ext , '' ) ) return sorted ( IDs )
6214	def load_gltf ( self ) : with open ( self . path ) as fd : self . meta = GLTFMeta ( self . path , json . load ( fd ) )
8517	def _warn_if_not_finite ( X ) : X = np . asanyarray ( X ) # First try an O(n) time, O(1) space solution for the common case that # everything is finite; fall back to O(n) space np.isfinite to prevent # false positives from overflow in sum method if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : warnings . warn ( "Result contains NaN, infinity" " or a value too large for %r." % X . dtype , category = UserWarning )
2207	def truepath ( path , real = False ) : path = expanduser ( path ) path = expandvars ( path ) if real : path = realpath ( path ) else : path = abspath ( path ) path = normpath ( path ) return path
867	def clear ( cls ) : # Clear the in-memory settings cache, forcing reload upon subsequent "get" # request. super ( Configuration , cls ) . clear ( ) # Reset in-memory custom configuration info. _CustomConfigurationFileWrapper . clear ( persistent = False )
1307	def IsDesktopLocked ( ) -> bool : isLocked = False desk = ctypes . windll . user32 . OpenDesktopW ( ctypes . c_wchar_p ( 'Default' ) , 0 , 0 , 0x0100 ) # DESKTOP_SWITCHDESKTOP = 0x0100 if desk : isLocked = not ctypes . windll . user32 . SwitchDesktop ( desk ) ctypes . windll . user32 . CloseDesktop ( desk ) return isLocked
8840	def missing_some ( data , min_required , args ) : if min_required < 1 : return [ ] found = 0 not_found = object ( ) ret = [ ] for arg in args : if get_var ( data , arg , not_found ) is not_found : ret . append ( arg ) else : found += 1 if found >= min_required : return [ ] return ret
9316	def _to_json ( resp ) : try : return resp . json ( ) except ValueError as e : # Maybe better to report the original request URL? six . raise_from ( InvalidJSONError ( "Invalid JSON was received from " + resp . request . url ) , e )
9175	def db_connect ( connection_string = None , * * kwargs ) : if connection_string is None : connection_string = get_current_registry ( ) . settings [ CONNECTION_STRING ] db_conn = psycopg2 . connect ( connection_string , * * kwargs ) try : with db_conn : yield db_conn finally : db_conn . close ( )
1004	def _inferPhase2 ( self ) : # Init to zeros to start self . infPredictedState [ 't' ] . fill ( 0 ) self . cellConfidence [ 't' ] . fill ( 0 ) self . colConfidence [ 't' ] . fill ( 0 ) # Phase 2 - Compute new predicted state and update cell and column # confidences for c in xrange ( self . numberOfCols ) : # For each cell in the column for i in xrange ( self . cellsPerColumn ) : # For each segment in the cell for s in self . cells [ c ] [ i ] : # See if it has the min number of active synapses numActiveSyns = self . _getSegmentActivityLevel ( s , self . infActiveState [ 't' ] , connectedSynapsesOnly = False ) if numActiveSyns < self . activationThreshold : continue # Incorporate the confidence into the owner cell and column if self . verbosity >= 6 : print "incorporating DC from cell[%d,%d]: " % ( c , i ) , s . debugPrint ( ) dc = s . dutyCycle ( ) self . cellConfidence [ 't' ] [ c , i ] += dc self . colConfidence [ 't' ] [ c ] += dc # If we reach threshold on the connected synapses, predict it # If not active, skip over it if self . _isSegmentActive ( s , self . infActiveState [ 't' ] ) : self . infPredictedState [ 't' ] [ c , i ] = 1 # Normalize column and cell confidences sumConfidences = self . colConfidence [ 't' ] . sum ( ) if sumConfidences > 0 : self . colConfidence [ 't' ] /= sumConfidences self . cellConfidence [ 't' ] /= sumConfidences # Are we predicting the required minimum number of columns? numPredictedCols = self . infPredictedState [ 't' ] . max ( axis = 1 ) . sum ( ) if numPredictedCols >= 0.5 * self . avgInputDensity : return True else : return False
5056	def get_catalog_admin_url_template ( mode = 'change' ) : api_base_url = getattr ( settings , "COURSE_CATALOG_API_URL" , "" ) # Extract FQDN (Fully Qualified Domain Name) from API URL. match = re . match ( r"^(?P<fqdn>(?:https?://)?[^/]+)" , api_base_url ) if not match : return "" # Return matched FQDN from catalog api url appended with catalog admin path if mode == 'change' : return match . group ( "fqdn" ) . rstrip ( "/" ) + "/admin/catalogs/catalog/{catalog_id}/change/" elif mode == 'add' : return match . group ( "fqdn" ) . rstrip ( "/" ) + "/admin/catalogs/catalog/add/"
1882	def new_symbolic_buffer ( self , nbytes , * * options ) : label = options . get ( 'label' ) avoid_collisions = False if label is None : label = 'buffer' avoid_collisions = True taint = options . get ( 'taint' , frozenset ( ) ) expr = self . _constraints . new_array ( name = label , index_max = nbytes , value_bits = 8 , taint = taint , avoid_collisions = avoid_collisions ) self . _input_symbols . append ( expr ) if options . get ( 'cstring' , False ) : for i in range ( nbytes - 1 ) : self . _constraints . add ( expr [ i ] != 0 ) return expr
8700	def __exchange ( self , output , timeout = None ) : self . __writeln ( output ) self . _port . flush ( ) return self . __expect ( timeout = timeout or self . _timeout )
7931	def send_message ( source_jid , password , target_jid , body , subject = None , message_type = "chat" , message_thread = None , settings = None ) : # pylint: disable=R0913,R0912 if sys . version_info . major < 3 : # pylint: disable-msg=W0404 from locale import getpreferredencoding encoding = getpreferredencoding ( ) if isinstance ( source_jid , str ) : source_jid = source_jid . decode ( encoding ) if isinstance ( password , str ) : password = password . decode ( encoding ) if isinstance ( target_jid , str ) : target_jid = target_jid . decode ( encoding ) if isinstance ( body , str ) : body = body . decode ( encoding ) if isinstance ( message_type , str ) : message_type = message_type . decode ( encoding ) if isinstance ( message_thread , str ) : message_thread = message_thread . decode ( encoding ) if not isinstance ( source_jid , JID ) : source_jid = JID ( source_jid ) if not isinstance ( target_jid , JID ) : target_jid = JID ( target_jid ) msg = Message ( to_jid = target_jid , body = body , subject = subject , stanza_type = message_type ) def action ( client ) : """Send a mesage `msg` via a client.""" client . stream . send ( msg ) if settings is None : settings = XMPPSettings ( { "starttls" : True , "tls_verify_peer" : False } ) if password is not None : settings [ "password" ] = password handler = FireAndForget ( source_jid , action , settings ) try : handler . run ( ) except KeyboardInterrupt : handler . disconnect ( ) raise
9879	def _random_coincidences ( value_domain , n , n_v ) : n_v_column = n_v . reshape ( - 1 , 1 ) return ( n_v_column . dot ( n_v_column . T ) - np . eye ( len ( value_domain ) ) * n_v_column ) / ( n - 1 )
1886	def _hook_xfer_mem ( self , uc , access , address , size , value , data ) : assert access in ( UC_MEM_WRITE , UC_MEM_READ , UC_MEM_FETCH ) if access == UC_MEM_WRITE : self . _cpu . write_int ( address , value , size * 8 ) # If client code is attempting to read a value, we need to bring it # in from Manticore state. If we try to mem_write it here, Unicorn # will segfault. We add the value to a list of things that need to # be written, and ask to restart the emulation. elif access == UC_MEM_READ : value = self . _cpu . read_bytes ( address , size ) if address in self . _should_be_written : return True self . _should_be_written [ address ] = value self . _should_try_again = True return False return True
10326	def canonical_averages ( ps , microcanonical_averages_arrays ) : num_sites = microcanonical_averages_arrays [ 'N' ] num_edges = microcanonical_averages_arrays [ 'M' ] spanning_cluster = ( 'spanning_cluster' in microcanonical_averages_arrays ) ret = dict ( ) ret [ 'ps' ] = ps ret [ 'N' ] = num_sites ret [ 'M' ] = num_edges ret [ 'max_cluster_size' ] = np . empty ( ps . size ) ret [ 'max_cluster_size_ci' ] = np . empty ( ( ps . size , 2 ) ) if spanning_cluster : ret [ 'spanning_cluster' ] = np . empty ( ps . size ) ret [ 'spanning_cluster_ci' ] = np . empty ( ( ps . size , 2 ) ) ret [ 'moments' ] = np . empty ( ( 5 , ps . size ) ) ret [ 'moments_ci' ] = np . empty ( ( 5 , ps . size , 2 ) ) for p_index , p in enumerate ( ps ) : binomials = _binomial_pmf ( n = num_edges , p = p ) for key , value in microcanonical_averages_arrays . items ( ) : if len ( key ) <= 1 : continue if key in [ 'max_cluster_size' , 'spanning_cluster' ] : ret [ key ] [ p_index ] = np . sum ( binomials * value ) elif key in [ 'max_cluster_size_ci' , 'spanning_cluster_ci' ] : ret [ key ] [ p_index ] = np . sum ( np . tile ( binomials , ( 2 , 1 ) ) . T * value , axis = 0 ) elif key == 'moments' : ret [ key ] [ : , p_index ] = np . sum ( np . tile ( binomials , ( 5 , 1 ) ) * value , axis = 1 ) elif key == 'moments_ci' : ret [ key ] [ : , p_index ] = np . sum ( np . rollaxis ( np . tile ( binomials , ( 5 , 2 , 1 ) ) , 2 , 1 ) * value , axis = 1 ) else : raise NotImplementedError ( '{}-dimensional array' . format ( value . ndim ) ) return ret
9675	def _calculate_float ( self , byte_array ) : if len ( byte_array ) != 4 : return None return struct . unpack ( 'f' , struct . pack ( '4B' , * byte_array ) ) [ 0 ]
5042	def enroll_users_in_course ( cls , enterprise_customer , course_id , course_mode , emails ) : existing_users , unregistered_emails = cls . get_users_by_email ( emails ) successes = [ ] pending = [ ] failures = [ ] for user in existing_users : succeeded = cls . enroll_user ( enterprise_customer , user , course_mode , course_id ) if succeeded : successes . append ( user ) else : failures . append ( user ) for email in unregistered_emails : pending_user = enterprise_customer . enroll_user_pending_registration ( email , course_mode , course_id ) pending . append ( pending_user ) return successes , pending , failures
11106	def get_pickling_errors ( obj , seen = None ) : if seen == None : seen = [ ] if hasattr ( obj , "__getstate__" ) : state = obj . __getstate__ ( ) #elif hasattr(obj, "__dict__"): # state = obj.__dict__ else : return None #try: # state = obj.__getstate__() #except AttributeError as e: # #state = obj.__dict__ # return str(e) if state == None : return 'object state is None' if isinstance ( state , tuple ) : if not isinstance ( state [ 0 ] , dict ) : state = state [ 1 ] else : state = state [ 0 ] . update ( state [ 1 ] ) result = { } for i in state : try : pickle . dumps ( state [ i ] , protocol = 2 ) except pickle . PicklingError as e : if not state [ i ] in seen : seen . append ( state [ i ] ) result [ i ] = get_pickling_errors ( state [ i ] , seen ) return result
2283	def check_R_package ( self , package ) : test_package = not bool ( launch_R_script ( "{}/R_templates/test_import.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , { "{package}" : package } , verbose = True ) ) return test_package
6530	def purge_config_cache ( location = None ) : cache_path = get_cache_path ( location ) if location : os . remove ( cache_path ) else : shutil . rmtree ( cache_path )
3225	def iter_project ( projects , key_file = None ) : def decorator ( func ) : @ wraps ( func ) def decorated_function ( * args , * * kwargs ) : item_list = [ ] exception_map = { } for project in projects : if isinstance ( project , string_types ) : kwargs [ 'project' ] = project if key_file : kwargs [ 'key_file' ] = key_file elif isinstance ( project , dict ) : kwargs [ 'project' ] = project [ 'project' ] kwargs [ 'key_file' ] = project [ 'key_file' ] itm , exc = func ( * args , * * kwargs ) item_list . extend ( itm ) exception_map . update ( exc ) return ( item_list , exception_map ) return decorated_function return decorator
13000	def calculate_diagram_ranges ( data ) : data = round_arr_teff_luminosity ( data ) temps = data [ 'temp' ] x_range = [ 1.05 * np . amax ( temps ) , .95 * np . amin ( temps ) ] lums = data [ 'lum' ] y_range = [ .50 * np . amin ( lums ) , 2 * np . amax ( lums ) ] return ( x_range , y_range )
6070	def sersic_constant ( self ) : return ( 2 * self . sersic_index ) - ( 1. / 3. ) + ( 4. / ( 405. * self . sersic_index ) ) + ( 46. / ( 25515. * self . sersic_index ** 2 ) ) + ( 131. / ( 1148175. * self . sersic_index ** 3 ) ) - ( 2194697. / ( 30690717750. * self . sersic_index ** 4 ) )
866	def setCustomProperties ( cls , properties ) : _getLogger ( ) . info ( "Setting custom configuration properties=%r; caller=%r" , properties , traceback . format_stack ( ) ) _CustomConfigurationFileWrapper . edit ( properties ) for propertyName , value in properties . iteritems ( ) : cls . set ( propertyName , value )
8079	def star ( self , startx , starty , points = 20 , outer = 100 , inner = 50 , draw = True , * * kwargs ) : # Taken from Nodebox. self . beginpath ( * * kwargs ) self . moveto ( startx , starty + outer ) for i in range ( 1 , int ( 2 * points ) ) : angle = i * pi / points x = sin ( angle ) y = cos ( angle ) if i % 2 : radius = inner else : radius = outer x = startx + radius * x y = starty + radius * y self . lineto ( x , y ) return self . endpath ( draw )
4784	def contains_ignoring_case ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) if isinstance ( self . val , str_types ) : if len ( items ) == 1 : if not isinstance ( items [ 0 ] , str_types ) : raise TypeError ( 'given arg must be a string' ) if items [ 0 ] . lower ( ) not in self . val . lower ( ) : self . _err ( 'Expected <%s> to case-insensitive contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if not isinstance ( i , str_types ) : raise TypeError ( 'given args must all be strings' ) if i . lower ( ) not in self . val . lower ( ) : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) elif isinstance ( self . val , Iterable ) : missing = [ ] for i in items : if not isinstance ( i , str_types ) : raise TypeError ( 'given args must all be strings' ) found = False for v in self . val : if not isinstance ( v , str_types ) : raise TypeError ( 'val items must all be strings' ) if i . lower ( ) == v . lower ( ) : found = True break if not found : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
3325	def _generate_lock ( self , principal , lock_type , lock_scope , lock_depth , lock_owner , path , timeout ) : if timeout is None : timeout = LockManager . LOCK_TIME_OUT_DEFAULT elif timeout < 0 : timeout = - 1 lock_dict = { "root" : path , "type" : lock_type , "scope" : lock_scope , "depth" : lock_depth , "owner" : lock_owner , "timeout" : timeout , "principal" : principal , } # self . storage . create ( path , lock_dict ) return lock_dict
6159	def IIR_sos_header ( fname_out , SOS_mat ) : Ns , Mcol = SOS_mat . shape f = open ( fname_out , 'wt' ) f . write ( '//define a IIR SOS CMSIS-DSP coefficient array\n\n' ) f . write ( '#include <stdint.h>\n\n' ) f . write ( '#ifndef STAGES\n' ) f . write ( '#define STAGES %d\n' % Ns ) f . write ( '#endif\n' ) f . write ( '/*********************************************************/\n' ) f . write ( '/* IIR SOS Filter Coefficients */\n' ) f . write ( 'float32_t ba_coeff[%d] = { //b0,b1,b2,a1,a2,... by stage\n' % ( 5 * Ns ) ) for k in range ( Ns ) : if ( k < Ns - 1 ) : f . write ( ' %+-13e, %+-13e, %+-13e,\n' % ( SOS_mat [ k , 0 ] , SOS_mat [ k , 1 ] , SOS_mat [ k , 2 ] ) ) f . write ( ' %+-13e, %+-13e,\n' % ( - SOS_mat [ k , 4 ] , - SOS_mat [ k , 5 ] ) ) else : f . write ( ' %+-13e, %+-13e, %+-13e,\n' % ( SOS_mat [ k , 0 ] , SOS_mat [ k , 1 ] , SOS_mat [ k , 2 ] ) ) f . write ( ' %+-13e, %+-13e\n' % ( - SOS_mat [ k , 4 ] , - SOS_mat [ k , 5 ] ) ) # for k in range(Ns): # if (k < Ns-1): # f.write(' %15.12f, %15.12f, %15.12f,\n' % \ # (SOS_mat[k,0],SOS_mat[k,1],SOS_mat[k,2])) # f.write(' %15.12f, %15.12f,\n' % \ # (-SOS_mat[k,4],-SOS_mat[k,5])) # else: # f.write(' %15.12f, %15.12f, %15.12f,\n' % \ # (SOS_mat[k,0],SOS_mat[k,1],SOS_mat[k,2])) # f.write(' %15.12f, %15.12f\n' % \ # (-SOS_mat[k,4],-SOS_mat[k,5])) f . write ( '};\n' ) f . write ( '/*********************************************************/\n' ) f . close ( )
2167	def list_resource_commands ( self ) : resource_path = os . path . abspath ( os . path . join ( os . path . dirname ( __file__ ) , os . pardir , 'resources' ) ) answer = set ( [ ] ) for _ , name , _ in pkgutil . iter_modules ( [ resource_path ] ) : res = tower_cli . get_resource ( name ) if not getattr ( res , 'internal' , False ) : answer . add ( name ) return sorted ( answer )
5881	def is_highlink_density ( self , element ) : links = self . parser . getElementsByTag ( element , tag = 'a' ) if not links : return False text = self . parser . getText ( element ) words = text . split ( ' ' ) words_number = float ( len ( words ) ) link_text_parts = [ ] for link in links : link_text_parts . append ( self . parser . getText ( link ) ) link_text = '' . join ( link_text_parts ) link_words = link_text . split ( ' ' ) number_of_link_words = float ( len ( link_words ) ) number_of_links = float ( len ( links ) ) link_divisor = float ( number_of_link_words / words_number ) score = float ( link_divisor * number_of_links ) if score >= 1.0 : return True return False
9820	def project ( ctx , project ) : # pylint:disable=redefined-outer-name if ctx . invoked_subcommand not in [ 'create' , 'list' ] : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project
2560	def heartbeat ( self ) : heartbeat = ( HEARTBEAT_CODE ) . to_bytes ( 4 , "little" ) r = self . task_incoming . send ( heartbeat ) logger . debug ( "Return from heartbeat : {}" . format ( r ) )
2091	def copy ( self , pk = None , new_name = None , * * kwargs ) : orig = self . read ( pk , fail_on_no_results = True , fail_on_multiple_results = True ) orig = orig [ 'results' ] [ 0 ] # Remove default values (anything where the value is None). self . _pop_none ( kwargs ) newresource = copy ( orig ) newresource . pop ( 'id' ) basename = newresource [ 'name' ] . split ( '@' , 1 ) [ 0 ] . strip ( ) # Modify data to fit the call pattern of the tower-cli method for field in self . fields : if field . multiple and field . name in newresource : newresource [ field . name ] = ( newresource . get ( field . name ) , ) if new_name is None : # copy client-side, the old mechanism newresource [ 'name' ] = "%s @ %s" % ( basename , time . strftime ( '%X' ) ) newresource . update ( kwargs ) return self . write ( create_on_missing = True , fail_on_found = True , * * newresource ) else : # copy server-side, the new mechanism if kwargs : raise exc . TowerCLIError ( 'Cannot override {} and also use --new-name.' . format ( kwargs . keys ( ) ) ) copy_endpoint = '{}/{}/copy/' . format ( self . endpoint . strip ( '/' ) , pk ) return client . post ( copy_endpoint , data = { 'name' : new_name } ) . json ( )
13512	def is_colour ( value ) : global PREDEFINED , HEX_MATCH , RGB_MATCH , RGBA_MATCH , HSL_MATCH , HSLA_MATCH value = value . strip ( ) # hex match if HEX_MATCH . match ( value ) or RGB_MATCH . match ( value ) or RGBA_MATCH . match ( value ) or HSL_MATCH . match ( value ) or HSLA_MATCH . match ( value ) or value in PREDEFINED : return True return False
13549	def pip_install ( * args ) : download_cache = ( '--download-cache=%s ' % options . paved . pip . download_cache ) if options . paved . pip . download_cache else '' shv ( 'pip install %s%s' % ( download_cache , ' ' . join ( args ) ) )
6365	def to_dict ( self ) : return { 'tp' : self . _tp , 'tn' : self . _tn , 'fp' : self . _fp , 'fn' : self . _fn }
5526	def grab ( bbox = None , childprocess = None , backend = None ) : if childprocess is None : childprocess = childprocess_default_value ( ) return _grab ( to_file = False , childprocess = childprocess , backend = backend , bbox = bbox )
10514	def windowuptime ( self , window_name ) : tmp_time = self . _remote_windowuptime ( window_name ) if tmp_time : tmp_time = tmp_time . split ( '-' ) start_time = tmp_time [ 0 ] . split ( ' ' ) end_time = tmp_time [ 1 ] . split ( ' ' ) _start_time = datetime . datetime ( int ( start_time [ 0 ] ) , int ( start_time [ 1 ] ) , int ( start_time [ 2 ] ) , int ( start_time [ 3 ] ) , int ( start_time [ 4 ] ) , int ( start_time [ 5 ] ) ) _end_time = datetime . datetime ( int ( end_time [ 0 ] ) , int ( end_time [ 1 ] ) , int ( end_time [ 2 ] ) , int ( end_time [ 3 ] ) , int ( end_time [ 4 ] ) , int ( end_time [ 5 ] ) ) return _start_time , _end_time return None
12967	def random ( self , cascadeFetch = False ) : matchedKeys = list ( self . getPrimaryKeys ( ) ) obj = None # Loop so we don't return None when there are items, if item is deleted between getting key and getting obj while matchedKeys and not obj : key = matchedKeys . pop ( random . randint ( 0 , len ( matchedKeys ) - 1 ) ) obj = self . get ( key , cascadeFetch = cascadeFetch ) return obj
11634	def generate_oauth2_headers ( self ) : encoded_credentials = base64 . b64encode ( ( '{0}:{1}' . format ( self . consumer_key , self . consumer_secret ) ) . encode ( 'utf-8' ) ) headers = { 'Authorization' : 'Basic {0}' . format ( encoded_credentials . decode ( 'utf-8' ) ) , 'Content-Type' : 'application/x-www-form-urlencoded' } return headers
11480	def _upload_as_item ( local_file , parent_folder_id , file_path , reuse_existing = False ) : current_item_id = _create_or_reuse_item ( local_file , parent_folder_id , reuse_existing ) _create_bitstream ( file_path , local_file , current_item_id ) for callback in session . item_upload_callbacks : callback ( session . communicator , session . token , current_item_id )
7264	def validate ( method ) : # Name error template name_error = 'configuration option "{}" is not supported' @ functools . wraps ( method ) def validator ( self , name , * args ) : if name not in self . allowed_opts : raise ValueError ( name_error . format ( name ) ) return method ( self , name , * args ) return validator
6052	def sparse_to_unmasked_sparse_from_mask_and_pixel_centres ( total_sparse_pixels , mask , unmasked_sparse_grid_pixel_centres ) : pix_to_full_pix = np . zeros ( total_sparse_pixels ) pixel_index = 0 for full_pixel_index in range ( unmasked_sparse_grid_pixel_centres . shape [ 0 ] ) : y = unmasked_sparse_grid_pixel_centres [ full_pixel_index , 0 ] x = unmasked_sparse_grid_pixel_centres [ full_pixel_index , 1 ] if not mask [ y , x ] : pix_to_full_pix [ pixel_index ] = full_pixel_index pixel_index += 1 return pix_to_full_pix
1422	def loads ( string ) : f = StringIO . StringIO ( string ) marshaller = JavaObjectUnmarshaller ( f ) marshaller . add_transformer ( DefaultObjectTransformer ( ) ) return marshaller . readObject ( )
1765	def pop_int ( self , force = False ) : value = self . read_int ( self . STACK , force = force ) self . STACK += self . address_bit_size // 8 return value
1282	def block_html ( self , html ) : if self . options . get ( 'skip_style' ) and html . lower ( ) . startswith ( '<style' ) : return '' if self . options . get ( 'escape' ) : return escape ( html ) return html
7556	def random_combination ( nsets , n , k ) : sets = set ( ) while len ( sets ) < nsets : newset = tuple ( sorted ( np . random . choice ( n , k , replace = False ) ) ) sets . add ( newset ) return tuple ( sets )
6452	def dist_abs ( self , src , tar ) : if tar == src : return 0 elif not src : return len ( tar ) elif not tar : return len ( src ) src_bag = Counter ( src ) tar_bag = Counter ( tar ) return max ( sum ( ( src_bag - tar_bag ) . values ( ) ) , sum ( ( tar_bag - src_bag ) . values ( ) ) , )
7920	def __prepare_local ( data ) : if not data : return None data = unicode ( data ) try : local = NODEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( local . encode ( "utf-8" ) ) > 1023 : raise JIDError ( u"Local part too long" ) return local
8743	def delete_floatingip ( context , id ) : LOG . info ( 'delete_floatingip %s for tenant %s' % ( id , context . tenant_id ) ) _delete_flip ( context , id , ip_types . FLOATING )
12005	def _remove_header ( self , data , options ) : version_info = self . _get_version_info ( options [ 'version' ] ) header_size = version_info [ 'header_size' ] if options [ 'flags' ] [ 'timestamp' ] : header_size += version_info [ 'timestamp_size' ] data = data [ header_size : ] return data
7080	def tic_objectsearch ( objectid , idcol_to_use = "ID" , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 90.0 , refresh = 5.0 , maxtimeout = 180.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : params = { 'columns' : '*' , 'filters' : [ { "paramName" : idcol_to_use , "values" : [ str ( objectid ) ] } ] } service = 'Mast.Catalogs.Filtered.Tic' return mast_query ( service , params , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
9677	def calculate_bin_boundary ( self , bb ) : return min ( enumerate ( OPC_LOOKUP ) , key = lambda x : abs ( x [ 1 ] - bb ) ) [ 0 ]
10982	def locate_spheres ( image , feature_rad , dofilter = False , order = ( 3 , 3 , 3 ) , trim_edge = True , * * kwargs ) : # We just want a smoothed field model of the image so that the residuals # are simply the particles without other complications m = models . SmoothFieldModel ( ) I = ilms . LegendrePoly2P1D ( order = order , constval = image . get_image ( ) . mean ( ) ) s = states . ImageState ( image , [ I ] , pad = 0 , mdl = m ) if dofilter : opt . do_levmarq ( s , s . params ) pos = addsub . feature_guess ( s , feature_rad , trim_edge = trim_edge , * * kwargs ) [ 0 ] return pos
8910	def owsproxy_delegate ( request ) : twitcher_url = request . registry . settings . get ( 'twitcher.url' ) protected_path = request . registry . settings . get ( 'twitcher.ows_proxy_protected_path' , '/ows' ) url = twitcher_url + protected_path + '/proxy' if request . matchdict . get ( 'service_name' ) : url += '/' + request . matchdict . get ( 'service_name' ) if request . matchdict . get ( 'access_token' ) : url += '/' + request . matchdict . get ( 'service_name' ) url += '?' + urlparse . urlencode ( request . params ) LOGGER . debug ( "delegate to owsproxy: %s" , url ) # forward request to target (without Host Header) # h = dict(request.headers) # h.pop("Host", h) resp = requests . request ( method = request . method . upper ( ) , url = url , data = request . body , headers = request . headers , verify = False ) return Response ( resp . content , status = resp . status_code , headers = resp . headers )
5015	def filter_queryset ( self , request , queryset , view ) : if request . user . is_staff : email = request . query_params . get ( 'email' , None ) username = request . query_params . get ( 'username' , None ) query_parameters = { } if email : query_parameters . update ( email = email ) if username : query_parameters . update ( username = username ) if query_parameters : users = User . objects . filter ( * * query_parameters ) . values_list ( 'id' , flat = True ) queryset = queryset . filter ( user_id__in = users ) else : queryset = queryset . filter ( user_id = request . user . id ) return queryset
2657	def notify ( self , event_id ) : self . _event_buffer . extend ( [ event_id ] ) self . _event_count += 1 if self . _event_count >= self . threshold : logger . debug ( "Eventcount >= threshold" ) self . make_callback ( kind = "event" )
5981	def setup_figure ( figsize , as_subplot ) : if not as_subplot : fig = plt . figure ( figsize = figsize ) return fig
10264	def collapse_orthologies_by_namespace ( graph : BELGraph , victim_namespace : Strings , survivor_namespace : str ) -> None : _collapse_edge_by_namespace ( graph , victim_namespace , survivor_namespace , ORTHOLOGOUS )
8902	def _multiple_self_ref_fk_check ( class_model ) : self_fk = [ ] for f in class_model . _meta . concrete_fields : if f . related_model in self_fk : return True if f . related_model == class_model : self_fk . append ( class_model ) return False
13104	def start_scan ( self , scan_id ) : requests . post ( self . url + 'scans/{}/launch' . format ( scan_id ) , verify = False , headers = self . headers )
6066	def einstein_radius_rescaled ( self ) : return ( ( 3 - self . slope ) / ( 1 + self . axis_ratio ) ) * self . einstein_radius ** ( self . slope - 1 )
4997	def assign_enterprise_learner_role ( sender , instance , * * kwargs ) : # pylint: disable=unused-argument if kwargs [ 'created' ] and instance . user : enterprise_learner_role , __ = SystemWideEnterpriseRole . objects . get_or_create ( name = ENTERPRISE_LEARNER_ROLE ) SystemWideEnterpriseUserRoleAssignment . objects . get_or_create ( user = instance . user , role = enterprise_learner_role )
2227	def _digest_hasher ( hasher , hashlen , base ) : # Get a 128 character hex string hex_text = hasher . hexdigest ( ) # Shorten length of string (by increasing base) base_text = _convert_hexstr_base ( hex_text , base ) # Truncate text = base_text [ : hashlen ] return text
10800	def _newcall ( self , rvecs ) : # 1. Initial guess for output: sigma = 1 * self . filter_size out = self . _eval_firstorder ( rvecs , self . d , sigma ) # 2. There are differences between 0th order at the points and # the passed data, so we iterate to remove: ondata = self . _eval_firstorder ( self . x , self . d , sigma ) for i in range ( self . iterations ) : out += self . _eval_firstorder ( rvecs , self . d - ondata , sigma ) ondata += self . _eval_firstorder ( self . x , self . d - ondata , sigma ) sigma *= self . damp return out
13426	def update_message ( self , message ) : url = "/2/messages/%s" % message . message_id data = self . _put_resource ( url , message . json_data ( ) ) return self . message_from_json ( data )
11010	def preview ( context ) : config = context . obj pelican ( config , '--verbose' , '--ignore-cache' ) server_proc = None os . chdir ( config [ 'OUTPUT_DIR' ] ) try : try : command = 'python -m http.server ' + str ( PORT ) server_proc = run ( command , bg = True ) time . sleep ( 3 ) click . launch ( 'http://localhost:8000' ) time . sleep ( 5 ) pelican ( config , '--autoreload' ) except Exception : if server_proc is not None : server_proc . kill ( ) raise except KeyboardInterrupt : abort ( context )
8536	def pop_data ( self , nbytes ) : last_timestamp = 0 data = [ ] for packet in self . pop ( nbytes ) : last_timestamp = packet . timestamp data . append ( packet . data . data ) return '' . join ( data ) , last_timestamp
4008	def _increase_file_handle_limit ( ) : logging . info ( 'Increasing file handle limit to {}' . format ( constants . FILE_HANDLE_LIMIT ) ) resource . setrlimit ( resource . RLIMIT_NOFILE , ( constants . FILE_HANDLE_LIMIT , resource . RLIM_INFINITY ) )
1746	def access_ok ( self , access ) : for c in access : if c not in self . perms : return False return True
11399	def update_keywords ( self ) : for field in record_get_field_instances ( self . record , '653' , ind1 = '1' ) : subs = field_get_subfields ( field ) new_subs = [ ] if 'a' in subs : for val in subs [ 'a' ] : new_subs . extend ( [ ( '9' , 'author' ) , ( 'a' , val ) ] ) new_field = create_field ( subfields = new_subs , ind1 = '1' ) record_replace_field ( self . record , '653' , new_field , field_position_global = field [ 4 ] )
2470	def set_file_notice ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_notice_set : self . file_notice_set = True if validations . validate_file_notice ( text ) : self . file ( doc ) . notice = str_from_text ( text ) else : raise SPDXValueError ( 'File::Notice' ) else : raise CardinalityError ( 'File::Notice' ) else : raise OrderError ( 'File::Notice' )
8815	def update_interfaces ( self , added_sg , updated_sg , removed_sg ) : if not ( added_sg or updated_sg or removed_sg ) : return with self . sessioned ( ) as session : self . _set_security_groups ( session , added_sg ) self . _unset_security_groups ( session , removed_sg ) combined = added_sg + updated_sg + removed_sg self . _refresh_interfaces ( session , combined )
947	def _printAvailableCheckpoints ( experimentDir ) : checkpointParentDir = getCheckpointParentDir ( experimentDir ) if not os . path . exists ( checkpointParentDir ) : print "No available checkpoints." return checkpointDirs = [ x for x in os . listdir ( checkpointParentDir ) if _isCheckpointDir ( os . path . join ( checkpointParentDir , x ) ) ] if not checkpointDirs : print "No available checkpoints." return print "Available checkpoints:" checkpointList = [ _checkpointLabelFromCheckpointDir ( x ) for x in checkpointDirs ] for checkpoint in sorted ( checkpointList ) : print "\t" , checkpoint print print "To start from a checkpoint:" print " python run_opf_experiment.py experiment --load <CHECKPOINT>" print "For example, to start from the checkpoint \"MyCheckpoint\":" print " python run_opf_experiment.py experiment --load MyCheckpoint"
4545	def draw_circle ( setter , x0 , y0 , r , color = None ) : f = 1 - r ddF_x = 1 ddF_y = - 2 * r x = 0 y = r setter ( x0 , y0 + r , color ) setter ( x0 , y0 - r , color ) setter ( x0 + r , y0 , color ) setter ( x0 - r , y0 , color ) while x < y : if f >= 0 : y -= 1 ddF_y += 2 f += ddF_y x += 1 ddF_x += 2 f += ddF_x setter ( x0 + x , y0 + y , color ) setter ( x0 - x , y0 + y , color ) setter ( x0 + x , y0 - y , color ) setter ( x0 - x , y0 - y , color ) setter ( x0 + y , y0 + x , color ) setter ( x0 - y , y0 + x , color ) setter ( x0 + y , y0 - x , color ) setter ( x0 - y , y0 - x , color )
2582	def load_checkpoints ( self , checkpointDirs ) : self . memo_lookup_table = None if not checkpointDirs : return { } if type ( checkpointDirs ) is not list : raise BadCheckpoint ( "checkpointDirs expects a list of checkpoints" ) return self . _load_checkpoints ( checkpointDirs )
12849	def watch_method ( self , method_name , callback ) : # Make sure a token method with the given name exists, and complain if # nothing is found. try : method = getattr ( self , method_name ) except AttributeError : raise ApiUsageError ( """\ {self.__class__.__name__} has no such method {method_name}() to watch. This error usually means that you used the @watch_token decorator on a method of a token extension class that didn't match the name of any method in the corresponding token class. Check for typos.""" ) # Wrap the method in a WatchedMethod object, if that hasn't already # been done. This object manages a list of callback method and takes # responsibility for calling them after the method itself has been # called. if not isinstance ( method , Token . WatchedMethod ) : setattr ( self , method_name , Token . WatchedMethod ( method ) ) method = getattr ( self , method_name ) # Add the given callback to the watched method. method . add_watcher ( callback )
8482	def env_key ( key , default ) : env = key . upper ( ) . replace ( '.' , '_' ) return os . environ . get ( env , default )
7717	def _roster_set ( self , item , callback , error_callback ) : stanza = Iq ( to_jid = self . server , stanza_type = "set" ) payload = RosterPayload ( [ item ] ) stanza . set_payload ( payload ) def success_cb ( result_stanza ) : """Success callback for roster set.""" if callback : callback ( item ) def error_cb ( error_stanza ) : """Error callback for roster set.""" if error_callback : error_callback ( error_stanza ) else : logger . error ( "Roster change of '{0}' failed" . format ( item . jid ) ) processor = self . stanza_processor processor . set_response_handlers ( stanza , success_cb , error_cb ) processor . send ( stanza )
7290	def make_key ( * args , * * kwargs ) : sep = kwargs . get ( 'sep' , u"_" ) exclude_last_string = kwargs . get ( 'exclude_last_string' , False ) string_array = [ ] for arg in args : if isinstance ( arg , list ) : string_array . append ( six . text_type ( sep . join ( arg ) ) ) else : if exclude_last_string : new_key_array = arg . split ( sep ) [ : - 1 ] if len ( new_key_array ) > 0 : string_array . append ( make_key ( new_key_array ) ) else : string_array . append ( six . text_type ( arg ) ) return sep . join ( string_array )
12463	def print_error ( message , wrap = True ) : if wrap : message = 'ERROR: {0}. Exit...' . format ( message . rstrip ( '.' ) ) colorizer = ( _color_wrap ( colorama . Fore . RED ) if colorama else lambda message : message ) return print ( colorizer ( message ) , file = sys . stderr )
7735	def set_stringprep_cache_size ( size ) : # pylint: disable-msg=W0603 global _stringprep_cache_size _stringprep_cache_size = size if len ( Profile . cache_items ) > size : remove = Profile . cache_items [ : - size ] for profile , key in remove : try : del profile . cache [ key ] except KeyError : pass Profile . cache_items = Profile . cache_items [ - size : ]
5157	def _add_install ( self , context ) : contents = self . _render_template ( 'install.sh' , context ) self . config . setdefault ( 'files' , [ ] ) # file list might be empty # add install.sh to list of included files self . _add_unique_file ( { "path" : "/install.sh" , "contents" : contents , "mode" : "755" } )
2487	def create_conjunction_node ( self , conjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . ConjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( conjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
6705	def create ( self , username , groups = None , uid = None , create_home = None , system = False , password = None , home_dir = None ) : r = self . local_renderer r . env . username = username args = [ ] if uid : args . append ( '-u %s' % uid ) if create_home is None : create_home = not system if create_home is True : if home_dir : args . append ( '--home %s' % home_dir ) elif create_home is False : args . append ( '--no-create-home' ) if password is None : pass elif password : crypted_password = _crypt_password ( password ) args . append ( '-p %s' % quote ( crypted_password ) ) else : args . append ( '--disabled-password' ) args . append ( '--gecos ""' ) if system : args . append ( '--system' ) r . env . args = ' ' . join ( args ) r . env . groups = ( groups or '' ) . strip ( ) r . sudo ( 'adduser {args} {username} || true' ) if groups : for group in groups . split ( ' ' ) : group = group . strip ( ) if not group : continue r . sudo ( 'adduser %s %s || true' % ( username , group ) )
4164	def embed_code_links ( app , exception ) : if exception is not None : return # No need to waste time embedding hyperlinks when not running the examples # XXX: also at the time of writing this fixes make html-noplot # for some reason I don't fully understand if not app . builder . config . plot_gallery : return # XXX: Whitelist of builders for which it makes sense to embed # hyperlinks inside the example html. Note that the link embedding # require searchindex.js to exist for the links to the local doc # and there does not seem to be a good way of knowing which # builders creates a searchindex.js. if app . builder . name not in [ 'html' , 'readthedocs' ] : return print ( 'Embedding documentation hyperlinks in examples..' ) gallery_conf = app . config . sphinx_gallery_conf gallery_dirs = gallery_conf [ 'gallery_dirs' ] if not isinstance ( gallery_dirs , list ) : gallery_dirs = [ gallery_dirs ] for gallery_dir in gallery_dirs : _embed_code_links ( app , gallery_conf , gallery_dir )
3109	def locked_put ( self , credentials ) : entity , _ = self . model_class . objects . get_or_create ( * * { self . key_name : self . key_value } ) setattr ( entity , self . property_name , credentials ) entity . save ( )
3470	def get_coefficient ( self , metabolite_id ) : if isinstance ( metabolite_id , Metabolite ) : return self . _metabolites [ metabolite_id ] _id_to_metabolites = { m . id : m for m in self . _metabolites } return self . _metabolites [ _id_to_metabolites [ metabolite_id ] ]
9840	def __array ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'type' ) : tok = self . __consume ( ) if not tok . iscode ( 'STRING' ) : raise DXParseError ( 'array: type was "%s", not a string.' % tok . text ) self . currentobject [ 'type' ] = tok . value ( ) elif tok . equals ( 'rank' ) : tok = self . __consume ( ) try : self . currentobject [ 'rank' ] = tok . value ( 'INTEGER' ) except ValueError : raise DXParseError ( 'array: rank was "%s", not an integer.' % tok . text ) elif tok . equals ( 'items' ) : tok = self . __consume ( ) try : self . currentobject [ 'size' ] = tok . value ( 'INTEGER' ) except ValueError : raise DXParseError ( 'array: items was "%s", not an integer.' % tok . text ) elif tok . equals ( 'data' ) : tok = self . __consume ( ) if not tok . iscode ( 'STRING' ) : raise DXParseError ( 'array: data was "%s", not a string.' % tok . text ) if tok . text != 'follows' : raise NotImplementedError ( 'array: Only the "data follows header" format is supported.' ) if not self . currentobject [ 'size' ] : raise DXParseError ( "array: missing number of items" ) # This is the slow part. Once we get here, we are just # reading in a long list of numbers. Conversion to floats # will be done later when the numpy array is created. # Don't assume anything about whitespace or the number of elements per row self . currentobject [ 'array' ] = [ ] while len ( self . currentobject [ 'array' ] ) < self . currentobject [ 'size' ] : self . currentobject [ 'array' ] . extend ( self . dxfile . readline ( ) . strip ( ) . split ( ) ) # If you assume that there are three elements per row # (except the last) the following version works and is a little faster. # for i in range(int(numpy.ceil(self.currentobject['size']/3))): # self.currentobject['array'].append(self.dxfile.readline()) # self.currentobject['array'] = ' '.join(self.currentobject['array']).split() elif tok . equals ( 'attribute' ) : # not used at the moment attribute = self . __consume ( ) . value ( ) if not self . __consume ( ) . equals ( 'string' ) : raise DXParseError ( 'array: "string" expected.' ) value = self . __consume ( ) . value ( ) else : raise DXParseError ( 'array: ' + str ( tok ) + ' not recognized.' )
12802	def get_room_by_name ( self , name ) : rooms = self . get_rooms ( ) for room in rooms or [ ] : if room [ "name" ] == name : return self . get_room ( room [ "id" ] ) raise RoomNotFoundException ( "Room %s not found" % name )
3215	def get_vpc_flow_logs ( vpc , * * conn ) : fl_result = describe_flow_logs ( Filters = [ { "Name" : "resource-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) fl_ids = [ ] for fl in fl_result : fl_ids . append ( fl [ "FlowLogId" ] ) return fl_ids
2242	def split_modpath ( modpath , check = True ) : if six . PY2 : if modpath . endswith ( '.pyc' ) : modpath = modpath [ : - 1 ] modpath_ = abspath ( expanduser ( modpath ) ) if check : if not exists ( modpath_ ) : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) raise ValueError ( 'modpath={} is not a module' . format ( modpath ) ) if isdir ( modpath_ ) and not exists ( join ( modpath , '__init__.py' ) ) : # dirs without inits are not modules raise ValueError ( 'modpath={} is not a module' . format ( modpath ) ) full_dpath , fname_ext = split ( modpath_ ) _relmod_parts = [ fname_ext ] # Recurse down directories until we are out of the package dpath = full_dpath while exists ( join ( dpath , '__init__.py' ) ) : dpath , dname = split ( dpath ) _relmod_parts . append ( dname ) relmod_parts = _relmod_parts [ : : - 1 ] rel_modpath = os . path . sep . join ( relmod_parts ) return dpath , rel_modpath
4176	def window_gaussian ( N , alpha = 2.5 ) : t = linspace ( - ( N - 1 ) / 2. , ( N - 1 ) / 2. , N ) #t = linspace(-(N)/2., (N)/2., N) w = exp ( - 0.5 * ( alpha * t / ( N / 2. ) ) ** 2. ) return w
3001	def cryptoDF ( token = '' , version = '' ) : df = pd . DataFrame ( crypto ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
7148	def with_payment_id ( self , payment_id = 0 ) : payment_id = numbers . PaymentID ( payment_id ) if not payment_id . is_short ( ) : raise TypeError ( "Payment ID {0} has more than 64 bits and cannot be integrated" . format ( payment_id ) ) prefix = 54 if self . is_testnet ( ) else 25 if self . is_stagenet ( ) else 19 data = bytearray ( [ prefix ] ) + self . _decoded [ 1 : 65 ] + struct . pack ( '>Q' , int ( payment_id ) ) checksum = bytearray ( keccak_256 ( data ) . digest ( ) [ : 4 ] ) return IntegratedAddress ( base58 . encode ( hexlify ( data + checksum ) ) )
5149	def _add_file ( self , tar , name , contents , mode = DEFAULT_FILE_MODE ) : byte_contents = BytesIO ( contents . encode ( 'utf8' ) ) info = tarfile . TarInfo ( name = name ) info . size = len ( contents ) # mtime must be 0 or any checksum operation # will return a different digest even when content is the same info . mtime = 0 info . type = tarfile . REGTYPE info . mode = int ( mode , 8 ) # permissions converted to decimal notation tar . addfile ( tarinfo = info , fileobj = byte_contents )
13116	def create_connection ( conf ) : host_config = { } host_config [ 'hosts' ] = [ conf . get ( 'jackal' , 'host' ) ] if int ( conf . get ( 'jackal' , 'use_ssl' ) ) : host_config [ 'use_ssl' ] = True if conf . get ( 'jackal' , 'ca_certs' ) : host_config [ 'ca_certs' ] = conf . get ( 'jackal' , 'ca_certs' ) if int ( conf . get ( 'jackal' , 'client_certs' ) ) : host_config [ 'client_cert' ] = conf . get ( 'jackal' , 'client_cert' ) host_config [ 'client_key' ] = conf . get ( 'jackal' , 'client_key' ) # Disable hostname checking for now. host_config [ 'ssl_assert_hostname' ] = False connections . create_connection ( * * host_config )
7091	def _cpinfo_key_worker ( task ) : cpfile , keyspeclist = task keystoget = [ x [ 0 ] for x in keyspeclist ] nonesubs = [ x [ - 2 ] for x in keyspeclist ] nansubs = [ x [ - 1 ] for x in keyspeclist ] # reform the keystoget into a list of lists for i , k in enumerate ( keystoget ) : thisk = k . split ( '.' ) if sys . version_info [ : 2 ] < ( 3 , 4 ) : thisk = [ ( int ( x ) if x . isdigit ( ) else x ) for x in thisk ] else : thisk = [ ( int ( x ) if x . isdecimal ( ) else x ) for x in thisk ] keystoget [ i ] = thisk # add in the objectid as well to match to the object catalog later keystoget . insert ( 0 , [ 'objectid' ] ) nonesubs . insert ( 0 , '' ) nansubs . insert ( 0 , '' ) # get all the keys we need vals = checkplot_infokey_worker ( ( cpfile , keystoget ) ) # if they have some Nones, nans, etc., reform them as expected for val , nonesub , nansub , valind in zip ( vals , nonesubs , nansubs , range ( len ( vals ) ) ) : if val is None : outval = nonesub elif isinstance ( val , float ) and not np . isfinite ( val ) : outval = nansub elif isinstance ( val , ( list , tuple ) ) : outval = ', ' . join ( val ) else : outval = val vals [ valind ] = outval return vals
4387	def adsAddRoute ( net_id , ip_address ) : # type: (SAmsNetId, str) -> None add_route = _adsDLL . AdsAddRoute add_route . restype = ctypes . c_long # Convert ip address to bytes (PY3) and get pointer. ip_address_p = ctypes . c_char_p ( ip_address . encode ( "utf-8" ) ) error_code = add_route ( net_id , ip_address_p ) if error_code : raise ADSError ( error_code )
1650	def _DropCommonSuffixes ( filename ) : for suffix in itertools . chain ( ( '%s.%s' % ( test_suffix . lstrip ( '_' ) , ext ) for test_suffix , ext in itertools . product ( _test_suffixes , GetNonHeaderExtensions ( ) ) ) , ( '%s.%s' % ( suffix , ext ) for suffix , ext in itertools . product ( [ 'inl' , 'imp' , 'internal' ] , GetHeaderExtensions ( ) ) ) ) : if ( filename . endswith ( suffix ) and len ( filename ) > len ( suffix ) and filename [ - len ( suffix ) - 1 ] in ( '-' , '_' ) ) : return filename [ : - len ( suffix ) - 1 ] return os . path . splitext ( filename ) [ 0 ]
11248	def median ( data ) : ordered = sorted ( data ) length = len ( ordered ) if length % 2 == 0 : return ( ordered [ math . floor ( length / 2 ) - 1 ] + ordered [ math . floor ( length / 2 ) ] ) / 2.0 elif length % 2 != 0 : return ordered [ math . floor ( length / 2 ) ]
8736	def construct_datetime ( cls , * args , * * kwargs ) : if len ( args ) == 1 : arg = args [ 0 ] method = cls . __get_dt_constructor ( type ( arg ) . __module__ , type ( arg ) . __name__ , ) result = method ( arg ) try : result = result . replace ( tzinfo = kwargs . pop ( 'tzinfo' ) ) except KeyError : pass if kwargs : first_key = kwargs . keys ( ) [ 0 ] tmpl = ( "{first_key} is an invalid keyword " "argument for this function." ) raise TypeError ( tmpl . format ( * * locals ( ) ) ) else : result = datetime . datetime ( * args , * * kwargs ) return result
103	def compute_paddings_for_aspect_ratio ( arr , aspect_ratio ) : do_assert ( arr . ndim in [ 2 , 3 ] ) do_assert ( aspect_ratio > 0 ) height , width = arr . shape [ 0 : 2 ] do_assert ( height > 0 ) aspect_ratio_current = width / height pad_top = 0 pad_right = 0 pad_bottom = 0 pad_left = 0 if aspect_ratio_current < aspect_ratio : # vertical image, height > width diff = ( aspect_ratio * height ) - width pad_right = int ( np . ceil ( diff / 2 ) ) pad_left = int ( np . floor ( diff / 2 ) ) elif aspect_ratio_current > aspect_ratio : # horizontal image, width > height diff = ( ( 1 / aspect_ratio ) * width ) - height pad_top = int ( np . floor ( diff / 2 ) ) pad_bottom = int ( np . ceil ( diff / 2 ) ) return pad_top , pad_right , pad_bottom , pad_left
1463	def build ( self , bldr ) : stage_names = sets . Set ( ) for source in self . _sources : source . _build ( bldr , stage_names ) for source in self . _sources : if not source . _all_built ( ) : raise RuntimeError ( "Topology cannot be fully built! Are all sources added?" )
2452	def set_pkg_home ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_home_set : self . package_home_set = True if validations . validate_pkg_homepage ( location ) : doc . package . homepage = location return True else : raise SPDXValueError ( 'Package::HomePage' ) else : raise CardinalityError ( 'Package::HomePage' )
8783	def _get_base_network_info ( self , context , network_id , base_net_driver ) : driver_name = base_net_driver . get_name ( ) net_info = { "network_type" : driver_name } LOG . debug ( '_get_base_network_info: %s %s' % ( driver_name , network_id ) ) # If the driver is NVP, we need to look up the lswitch id we should # be attaching to. if driver_name == 'NVP' : LOG . debug ( 'looking up lswitch ids for network %s' % ( network_id ) ) lswitch_ids = base_net_driver . get_lswitch_ids_for_network ( context , network_id ) if not lswitch_ids or len ( lswitch_ids ) > 1 : msg = ( 'lswitch id lookup failed, %s ids found.' % ( len ( lswitch_ids ) ) ) LOG . error ( msg ) raise IronicException ( msg ) lswitch_id = lswitch_ids . pop ( ) LOG . info ( 'found lswitch for network %s: %s' % ( network_id , lswitch_id ) ) net_info [ 'lswitch_id' ] = lswitch_id LOG . debug ( '_get_base_network_info finished: %s %s %s' % ( driver_name , network_id , net_info ) ) return net_info
1067	def getheaders ( self , name ) : result = [ ] current = '' have_header = 0 for s in self . getallmatchingheaders ( name ) : if s [ 0 ] . isspace ( ) : if current : current = "%s\n %s" % ( current , s . strip ( ) ) else : current = s . strip ( ) else : if have_header : result . append ( current ) current = s [ s . find ( ":" ) + 1 : ] . strip ( ) have_header = 1 if have_header : result . append ( current ) return result
11854	def scanner ( self , j , word ) : for ( i , j , A , alpha , Bb ) in self . chart [ j ] : if Bb and self . grammar . isa ( word , Bb [ 0 ] ) : self . add_edge ( [ i , j + 1 , A , alpha + [ ( Bb [ 0 ] , word ) ] , Bb [ 1 : ] ] )
2259	def dzip ( items1 , items2 , cls = dict ) : try : len ( items1 ) except TypeError : items1 = list ( items1 ) try : len ( items2 ) except TypeError : items2 = list ( items2 ) if len ( items1 ) == 0 and len ( items2 ) == 1 : # Corner case: # allow the first list to be empty and the second list to broadcast a # value. This means that the equality check wont work for the case # where items1 and items2 are supposed to correspond, but the length of # items2 is 1. items2 = [ ] if len ( items2 ) == 1 and len ( items1 ) > 1 : items2 = items2 * len ( items1 ) if len ( items1 ) != len ( items2 ) : raise ValueError ( 'out of alignment len(items1)=%r, len(items2)=%r' % ( len ( items1 ) , len ( items2 ) ) ) return cls ( zip ( items1 , items2 ) )
6127	def contained_in ( filename , directory ) : filename = os . path . normcase ( os . path . abspath ( filename ) ) directory = os . path . normcase ( os . path . abspath ( directory ) ) return os . path . commonprefix ( [ filename , directory ] ) == directory
2053	def STRD ( cpu , src1 , src2 , dest , offset = None ) : assert src1 . type == 'register' assert src2 . type == 'register' assert dest . type == 'memory' val1 = src1 . read ( ) val2 = src2 . read ( ) writeback = cpu . _compute_writeback ( dest , offset ) cpu . write_int ( dest . address ( ) , val1 , 32 ) cpu . write_int ( dest . address ( ) + 4 , val2 , 32 ) cpu . _cs_hack_ldr_str_writeback ( dest , offset , writeback )
2122	def associate_failure_node ( self , parent , child = None , * * kwargs ) : return self . _assoc_or_create ( 'failure' , parent , child , * * kwargs )
7032	def check_existing_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) if os . path . exists ( APIKEYFILE ) : # check if this file is readable/writeable by user only fileperm = oct ( os . stat ( APIKEYFILE ) [ stat . ST_MODE ] ) if fileperm == '0100600' or fileperm == '0o100600' : with open ( APIKEYFILE ) as infd : apikey , expires = infd . read ( ) . strip ( '\n' ) . split ( ) # get today's datetime now = datetime . now ( utc ) if sys . version_info [ : 2 ] < ( 3 , 7 ) : # this hideous incantation is required for lesser Pythons expdt = datetime . strptime ( expires . replace ( 'Z' , '' ) , '%Y-%m-%dT%H:%M:%S.%f' ) . replace ( tzinfo = utc ) else : expdt = datetime . fromisoformat ( expires . replace ( 'Z' , '+00:00' ) ) if now > expdt : LOGERROR ( 'API key has expired. expiry was on: %s' % expires ) return False , apikey , expires else : return True , apikey , expires else : LOGWARNING ( 'The API key file %s has bad permissions ' 'and is insecure, not reading it.\n' '(you need to chmod 600 this file)' % APIKEYFILE ) return False , None , None else : LOGWARNING ( 'No LCC-Server API key ' 'found in: {apikeyfile}' . format ( apikeyfile = APIKEYFILE ) ) return False , None , None
6626	def availableBranches ( self ) : return [ GithubComponentVersion ( '' , b [ 0 ] , b [ 1 ] , self . name , cache_key = None ) for b in _getBranchHeads ( self . repo ) . items ( ) ]
2446	def create_package ( self , doc , name ) : if not self . package_set : self . package_set = True doc . package = package . Package ( name = name ) return True else : raise CardinalityError ( 'Package::Name' )
6662	def generate_csr ( self , domain = '' , r = None ) : r = r or self . local_renderer r . env . domain = domain or r . env . domain role = self . genv . ROLE or ALL site = self . genv . SITE or self . genv . default_site print ( 'self.genv.default_site:' , self . genv . default_site , file = sys . stderr ) print ( 'site.csr0:' , site , file = sys . stderr ) ssl_dst = 'roles/%s/ssl' % ( role , ) print ( 'ssl_dst:' , ssl_dst ) if not os . path . isdir ( ssl_dst ) : os . makedirs ( ssl_dst ) for site , site_data in self . iter_sites ( ) : print ( 'site.csr1:' , site , file = sys . stderr ) assert r . env . domain , 'No SSL domain defined.' r . env . ssl_base_dst = '%s/%s' % ( ssl_dst , r . env . domain . replace ( '*.' , '' ) ) r . env . ssl_csr_year = date . today ( ) . year r . local ( 'openssl req -nodes -newkey rsa:{ssl_length} ' '-subj "/C={ssl_country}/ST={ssl_state}/L={ssl_city}/O={ssl_organization}/CN={ssl_domain}" ' '-keyout {ssl_base_dst}.{ssl_csr_year}.key -out {ssl_base_dst}.{ssl_csr_year}.csr' )
64	def is_out_of_image ( self , image , fully = True , partly = False ) : if self . is_fully_within_image ( image ) : return False elif self . is_partly_within_image ( image ) : return partly else : return fully
7837	def remove ( self ) : if self . disco is None : return self . xmlnode . unlinkNode ( ) oldns = self . xmlnode . ns ( ) ns = self . xmlnode . newNs ( oldns . getContent ( ) , None ) self . xmlnode . replaceNs ( oldns , ns ) common_root . addChild ( self . xmlnode ( ) ) self . disco = None
11439	def _get_children_by_tag_name ( node , name ) : try : return [ child for child in node . childNodes if child . nodeName == name ] except TypeError : return [ ]
5447	def _parse_gcs_uri ( self , raw_uri ) : # Assume URI is a directory path. raw_uri = directory_fmt ( raw_uri ) _ , docker_path = _gcs_uri_rewriter ( raw_uri ) docker_uri = os . path . join ( self . _relative_path , docker_path ) return docker_uri
10956	def get ( self , name ) : for c in self . comps : if c . category == name : return c return None
7040	def list_recent_datasets ( lcc_server , nrecent = 25 ) : urlparams = { 'nsets' : nrecent } urlqs = urlencode ( urlparams ) url = '%s/api/datasets?%s' % ( lcc_server , urlqs ) try : LOGINFO ( 'getting list of recent publicly ' 'visible and owned datasets from %s' % ( lcc_server , ) ) # check if we have an API key already have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) # if not, get a new one if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) # if apikey is not None, add it in as an Authorization: Bearer [apikey] # header if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } # hit the server req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) recent_datasets = json . loads ( resp . read ( ) ) [ 'result' ] return recent_datasets except HTTPError as e : LOGERROR ( 'could not retrieve recent datasets list, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
3025	def _save_private_file ( filename , json_contents ) : temp_filename = tempfile . mktemp ( ) file_desc = os . open ( temp_filename , os . O_WRONLY | os . O_CREAT , 0o600 ) with os . fdopen ( file_desc , 'w' ) as file_handle : json . dump ( json_contents , file_handle , sort_keys = True , indent = 2 , separators = ( ',' , ': ' ) ) shutil . move ( temp_filename , filename )
9868	def rt_subscription_running ( self ) : return ( self . _tibber_control . sub_manager is not None and self . _tibber_control . sub_manager . is_running and self . _subscription_id is not None )
65	def clip_out_of_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] ia . do_assert ( height > 0 ) ia . do_assert ( width > 0 ) eps = np . finfo ( np . float32 ) . eps x1 = np . clip ( self . x1 , 0 , width - eps ) x2 = np . clip ( self . x2 , 0 , width - eps ) y1 = np . clip ( self . y1 , 0 , height - eps ) y2 = np . clip ( self . y2 , 0 , height - eps ) return self . copy ( x1 = x1 , y1 = y1 , x2 = x2 , y2 = y2 , label = self . label )
2565	def async_process ( fn ) : def run ( * args , * * kwargs ) : proc = mp . Process ( target = fn , args = args , kwargs = kwargs ) proc . start ( ) return proc return run
11732	def registerGoodClass ( self , class_ ) : # Class itself added to "good" list self . _valid_classes . append ( class_ ) # Recurse into any inner classes for name , cls in class_members ( class_ ) : if self . isValidClass ( cls ) : self . registerGoodClass ( cls )
6228	def init ( window = None , project = None , timeline = None ) : from demosys . effects . registry import Effect from demosys . scene import camera window . timeline = timeline # Inject attributes into the base Effect class setattr ( Effect , '_window' , window ) setattr ( Effect , '_ctx' , window . ctx ) setattr ( Effect , '_project' , project ) # Set up the default system camera window . sys_camera = camera . SystemCamera ( aspect = window . aspect_ratio , fov = 60.0 , near = 1 , far = 1000 ) setattr ( Effect , '_sys_camera' , window . sys_camera ) print ( "Loading started at" , time . time ( ) ) project . load ( ) # Initialize timer timer_cls = import_string ( settings . TIMER ) window . timer = timer_cls ( ) window . timer . start ( )
2653	def push_file ( self , local_source , remote_dir ) : remote_dest = remote_dir + '/' + os . path . basename ( local_source ) try : self . makedirs ( remote_dir , exist_ok = True ) except IOError as e : logger . exception ( "Pushing {0} to {1} failed" . format ( local_source , remote_dir ) ) if e . errno == 2 : raise BadScriptPath ( e , self . hostname ) elif e . errno == 13 : raise BadPermsScriptPath ( e , self . hostname ) else : logger . exception ( "File push failed due to SFTP client failure" ) raise FileCopyException ( e , self . hostname ) try : self . sftp_client . put ( local_source , remote_dest , confirm = True ) # Set perm because some systems require the script to be executable self . sftp_client . chmod ( remote_dest , 0o777 ) except Exception as e : logger . exception ( "File push from local source {} to remote destination {} failed" . format ( local_source , remote_dest ) ) raise FileCopyException ( e , self . hostname ) return remote_dest
12211	def invalidate_cache ( user , size = None ) : sizes = set ( AUTO_GENERATE_AVATAR_SIZES ) if size is not None : sizes . add ( size ) for prefix in cached_funcs : for size in sizes : cache . delete ( get_cache_key ( user , size , prefix ) )
6709	def check ( self ) : self . _validate_settings ( ) r = self . local_renderer r . env . alias = r . env . aliases [ 0 ] r . sudo ( r . env . check_command_template )
11074	def get_by_username ( self , username ) : res = filter ( lambda x : x . username == username , self . users . values ( ) ) if len ( res ) > 0 : return res [ 0 ] return None
8028	def groupify ( function ) : @ wraps ( function ) def wrapper ( paths , * args , * * kwargs ) : # pylint: disable=missing-docstring groups = { } for path in paths : key = function ( path , * args , * * kwargs ) if key is not None : groups . setdefault ( key , set ( ) ) . add ( path ) return groups return wrapper
1170	def _format_text ( self , text ) : text_width = max ( self . width - self . current_indent , 11 ) indent = " " * self . current_indent return textwrap . fill ( text , text_width , initial_indent = indent , subsequent_indent = indent )
10704	def get_device ( _id ) : url = DEVICE_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
8870	def create_metafile ( bgen_filepath , metafile_filepath , verbose = True ) : if verbose : verbose = 1 else : verbose = 0 bgen_filepath = make_sure_bytes ( bgen_filepath ) metafile_filepath = make_sure_bytes ( metafile_filepath ) assert_file_exist ( bgen_filepath ) assert_file_readable ( bgen_filepath ) if exists ( metafile_filepath ) : raise ValueError ( f"The file {metafile_filepath} already exists." ) with bgen_file ( bgen_filepath ) as bgen : nparts = _estimate_best_npartitions ( lib . bgen_nvariants ( bgen ) ) metafile = lib . bgen_create_metafile ( bgen , metafile_filepath , nparts , verbose ) if metafile == ffi . NULL : raise RuntimeError ( f"Error while creating metafile: {metafile_filepath}." ) if lib . bgen_close_metafile ( metafile ) != 0 : raise RuntimeError ( f"Error while closing metafile: {metafile_filepath}." )
4190	def window_poisson_hanning ( N , alpha = 2 ) : w1 = window_hann ( N ) w2 = window_poisson ( N , alpha = alpha ) return w1 * w2
11571	def set_bit_map ( self , shape , color ) : for row in range ( 0 , 8 ) : data = shape [ row ] # shift data into buffer bit_mask = 0x80 for column in range ( 0 , 8 ) : if data & bit_mask : self . set_pixel ( row , column , color , True ) bit_mask >>= 1 self . output_entire_buffer ( )
11605	def convert_ranges ( cls , ranges , length ) : result = [ ] for start , end in ranges : if end is None : result . append ( ( start , length - 1 ) ) elif start is None : s = length - end result . append ( ( 0 if s < 0 else s , length - 1 ) ) else : result . append ( ( start , end if end < length else length - 1 ) ) return result
4793	def is_subset_of ( self , * supersets ) : if not isinstance ( self . val , Iterable ) : raise TypeError ( 'val is not iterable' ) if len ( supersets ) == 0 : raise ValueError ( 'one or more superset args must be given' ) missing = [ ] if hasattr ( self . val , 'keys' ) and callable ( getattr ( self . val , 'keys' ) ) and hasattr ( self . val , '__getitem__' ) : # flatten superset dicts superdict = { } for l , j in enumerate ( supersets ) : self . _check_dict_like ( j , check_values = False , name = 'arg #%d' % ( l + 1 ) ) for k in j . keys ( ) : superdict . update ( { k : j [ k ] } ) for i in self . val . keys ( ) : if i not in superdict : missing . append ( { i : self . val [ i ] } ) # bad key elif self . val [ i ] != superdict [ i ] : missing . append ( { i : self . val [ i ] } ) # bad val if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superdict ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) else : # flatten supersets superset = set ( ) for j in supersets : try : for k in j : superset . add ( k ) except Exception : superset . add ( j ) for i in self . val : if i not in superset : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superset ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) return self
10252	def highlight_edges ( graph : BELGraph , edges = None , color : Optional [ str ] = None ) -> None : color = color or EDGE_HIGHLIGHT_DEFAULT_COLOR for u , v , k , d in edges if edges is not None else graph . edges ( keys = True , data = True ) : graph [ u ] [ v ] [ k ] [ EDGE_HIGHLIGHT ] = color
4612	def block_timestamp ( self , block_num ) : return int ( self . block_class ( block_num , blockchain_instance = self . blockchain ) . time ( ) . timestamp ( ) )
6736	def reboot_or_dryrun ( * args , * * kwargs ) : from fabric . state import connections verbose = get_verbose ( ) dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) # Use 'wait' as max total wait time kwargs . setdefault ( 'wait' , 120 ) wait = int ( kwargs [ 'wait' ] ) command = kwargs . get ( 'command' , 'reboot' ) now = int ( kwargs . get ( 'now' , 0 ) ) print ( 'now:' , now ) if now : command += ' now' # Shorter timeout for a more granular cycle than the default. timeout = int ( kwargs . get ( 'timeout' , 30 ) ) reconnect_hostname = kwargs . pop ( 'new_hostname' , env . host_string ) if 'dryrun' in kwargs : del kwargs [ 'dryrun' ] if dryrun : print ( '%s sudo: %s' % ( render_command_prefix ( ) , command ) ) else : if is_local ( ) : if raw_input ( 'reboot localhost now? ' ) . strip ( ) [ 0 ] . lower ( ) != 'y' : return attempts = int ( round ( float ( wait ) / float ( timeout ) ) ) # Don't bleed settings, since this is supposed to be self-contained. # User adaptations will probably want to drop the "with settings()" and # just have globally set timeout/attempts values. with settings ( warn_only = True ) : _sudo ( command ) env . host_string = reconnect_hostname success = False for attempt in xrange ( attempts ) : # Try to make sure we don't slip in before pre-reboot lockdown if verbose : print ( 'Waiting for %s seconds, wait %i of %i' % ( timeout , attempt + 1 , attempts ) ) time . sleep ( timeout ) # This is actually an internal-ish API call, but users can simply drop # it in real fabfile use -- the next run/sudo/put/get/etc call will # automatically trigger a reconnect. # We use it here to force the reconnect while this function is still in # control and has the above timeout settings enabled. try : if verbose : print ( 'Reconnecting to:' , env . host_string ) # This will fail until the network interface comes back up. connections . connect ( env . host_string ) # This will also fail until SSH is running again. with settings ( timeout = timeout ) : _run ( 'echo hello' ) success = True break except Exception as e : print ( 'Exception:' , e ) if not success : raise Exception ( 'Reboot failed or took longer than %s seconds.' % wait )
10345	def get_merged_namespace_names ( locations , check_keywords = True ) : resources = { location : get_bel_resource ( location ) for location in locations } if check_keywords : resource_keywords = set ( config [ 'Namespace' ] [ 'Keyword' ] for config in resources . values ( ) ) if 1 != len ( resource_keywords ) : raise ValueError ( 'Tried merging namespaces with different keywords: {}' . format ( resource_keywords ) ) result = { } for resource in resources : result . update ( resource [ 'Values' ] ) return result
6611	def put ( self , task , * args , * * kwargs ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return package = TaskPackage ( task = task , args = args , kwargs = kwargs ) return self . dropbox . put ( package )
3725	def dipole_moment ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in _dipole_CCDB . index and not np . isnan ( _dipole_CCDB . at [ CASRN , 'Dipole' ] ) : methods . append ( CCCBDB ) if CASRN in _dipole_Muller . index and not np . isnan ( _dipole_Muller . at [ CASRN , 'Dipole' ] ) : methods . append ( MULLER ) if CASRN in _dipole_Poling . index and not np . isnan ( _dipole_Poling . at [ CASRN , 'Dipole' ] ) : methods . append ( POLING ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CCCBDB : _dipole = float ( _dipole_CCDB . at [ CASRN , 'Dipole' ] ) elif Method == MULLER : _dipole = float ( _dipole_Muller . at [ CASRN , 'Dipole' ] ) elif Method == POLING : _dipole = float ( _dipole_Poling . at [ CASRN , 'Dipole' ] ) elif Method == NONE : _dipole = None else : raise Exception ( 'Failure in in function' ) return _dipole
1127	def Seq ( first_rule , * rest_of_rules , * * kwargs ) : @ llrule ( kwargs . get ( "loc" , None ) , first_rule . expected ) def rule ( parser ) : result = first_rule ( parser ) if result is unmatched : return result results = [ result ] for rule in rest_of_rules : result = rule ( parser ) if result is unmatched : return result results . append ( result ) return tuple ( results ) return rule
9414	def _make_user_class ( session , name ) : attrs = session . eval ( 'fieldnames(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) methods = session . eval ( 'methods(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) ref = weakref . ref ( session ) doc = _DocDescriptor ( ref , name ) values = dict ( __doc__ = doc , _name = name , _ref = ref , _attrs = attrs , __module__ = 'oct2py.dynamic' ) for method in methods : doc = _MethodDocDescriptor ( ref , name , method ) cls_name = '%s_%s' % ( name , method ) method_values = dict ( __doc__ = doc ) method_cls = type ( str ( cls_name ) , ( OctaveUserClassMethod , ) , method_values ) values [ method ] = method_cls ( ref , method , name ) for attr in attrs : values [ attr ] = OctaveUserClassAttr ( ref , attr , attr ) return type ( str ( name ) , ( OctaveUserClass , ) , values )
2206	def compressuser ( path , home = '~' ) : path = normpath ( path ) userhome_dpath = userhome ( ) if path . startswith ( userhome_dpath ) : if len ( path ) == len ( userhome_dpath ) : path = home elif path [ len ( userhome_dpath ) ] == os . path . sep : path = home + path [ len ( userhome_dpath ) : ] return path
9415	def from_value ( cls , value ) : instance = OctaveUserClass . __new__ ( cls ) instance . _address = '%s_%s' % ( instance . _name , id ( instance ) ) instance . _ref ( ) . push ( instance . _address , value ) return instance
4663	def new_tx ( self , * args , * * kwargs ) : builder = self . transactionbuilder_class ( * args , blockchain_instance = self , * * kwargs ) self . _txbuffers . append ( builder ) return builder
7877	def _bind_success ( self , stanza ) : # pylint: disable-msg=R0201 payload = stanza . get_payload ( ResourceBindingPayload ) jid = payload . jid if not jid : raise BadRequestProtocolError ( u"<jid/> element mising in" " the bind response" ) self . stream . me = jid self . stream . event ( AuthorizedEvent ( self . stream . me ) )
4544	def _add_redundant_arguments ( parser ) : parser . add_argument ( '-a' , '--animation' , default = None , help = 'Default animation type if no animation is specified' ) if deprecated . allowed ( ) : # pragma: no cover parser . add_argument ( '--dimensions' , '--dim' , default = None , help = 'DEPRECATED: x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '--shape' , default = None , help = 'x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '-l' , '--layout' , default = None , help = 'Default layout class if no layout is specified' ) parser . add_argument ( '--numbers' , '-n' , default = 'python' , choices = NUMBER_TYPES , help = NUMBERS_HELP ) parser . add_argument ( '-p' , '--path' , default = None , help = PATH_HELP )
6437	def dist_abs ( self , src , tar , weights = 'exponential' , max_length = 8 , normalized = False ) : # Calculate the eudex hashes and XOR them xored = eudex ( src , max_length = max_length ) ^ eudex ( tar , max_length = max_length ) # Simple hamming distance (all bits are equal) if not weights : binary = bin ( xored ) distance = binary . count ( '1' ) if normalized : return distance / ( len ( binary ) - 2 ) return distance # If weights is a function, it should create a generator, # which we now use to populate a list if callable ( weights ) : weights = weights ( ) elif weights == 'exponential' : weights = Eudex . gen_exponential ( ) elif weights == 'fibonacci' : weights = Eudex . gen_fibonacci ( ) if isinstance ( weights , GeneratorType ) : weights = [ next ( weights ) for _ in range ( max_length ) ] [ : : - 1 ] # Sum the weighted hamming distance distance = 0 max_distance = 0 while ( xored or normalized ) and weights : max_distance += 8 * weights [ - 1 ] distance += bin ( xored & 0xFF ) . count ( '1' ) * weights . pop ( ) xored >>= 8 if normalized : distance /= max_distance return distance
5806	def parse_alert ( server_handshake_bytes ) : for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x15' : continue if len ( record_data ) != 2 : return None return ( int_from_bytes ( record_data [ 0 : 1 ] ) , int_from_bytes ( record_data [ 1 : 2 ] ) ) return None
10978	def leave ( group_id ) : group = Group . query . get_or_404 ( group_id ) if group . can_leave ( current_user ) : try : group . remove_member ( current_user ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( url_for ( '.index' ) ) flash ( _ ( 'You have successfully left %(group_name)s group.' , group_name = group . name ) , 'success' ) return redirect ( url_for ( '.index' ) ) flash ( _ ( 'You cannot leave the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
10072	def record_schema ( self ) : schema_path = current_jsonschemas . url_to_path ( self [ '$schema' ] ) schema_prefix = current_app . config [ 'DEPOSIT_JSONSCHEMAS_PREFIX' ] if schema_path and schema_path . startswith ( schema_prefix ) : return current_jsonschemas . path_to_url ( schema_path [ len ( schema_prefix ) : ] )
12278	def run_executable ( repo , args , includes ) : # Get platform information mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) platform_metadata = repomgr . get_metadata ( ) print ( "Obtaining Commit Information" ) ( executable , commiturl ) = find_executable_commitpath ( repo , args ) # Create a local directory tmpdir = tempfile . mkdtemp ( ) # Construct the strace command print ( "Running the command" ) strace_filename = os . path . join ( tmpdir , 'strace.out.txt' ) cmd = [ "strace.py" , "-f" , "-o" , strace_filename , "-s" , "1024" , "-q" , "--" ] + args # Run the command p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) out , err = p . communicate ( ) # Capture the stdout/stderr stdout = os . path . join ( tmpdir , 'stdout.log.txt' ) with open ( stdout , 'w' ) as fd : fd . write ( out . decode ( 'utf-8' ) ) stderr = os . path . join ( tmpdir , 'stderr.log.txt' ) with open ( stderr , 'w' ) as fd : fd . write ( err . decode ( 'utf-8' ) ) # Check the strace output files = extract_files ( strace_filename , includes ) # Now insert the execution metadata execution_metadata = { 'likelyexecutable' : executable , 'commitpath' : commiturl , 'args' : args , } execution_metadata . update ( platform_metadata ) for i in range ( len ( files ) ) : files [ i ] [ 'execution_metadata' ] = execution_metadata return files
6680	def copy ( self , source , destination , recursive = False , use_sudo = False ) : func = use_sudo and run_as_root or self . run options = '-r ' if recursive else '' func ( '/bin/cp {0}{1} {2}' . format ( options , quote ( source ) , quote ( destination ) ) )
3209	def get_load_balancer ( load_balancer , flags = FLAGS . ALL ^ FLAGS . POLICY_TYPES , * * conn ) : # Python 2 and 3 support: try : basestring except NameError as _ : basestring = str if isinstance ( load_balancer , basestring ) : load_balancer = dict ( LoadBalancerName = load_balancer ) return registry . build_out ( flags , start_with = load_balancer , pass_datastructure = True , * * conn )
10111	def iterrows ( lines_or_file , namedtuples = False , dicts = False , encoding = 'utf-8' , * * kw ) : if namedtuples and dicts : raise ValueError ( 'either namedtuples or dicts can be chosen as output format' ) elif namedtuples : _reader = NamedTupleReader elif dicts : _reader = UnicodeDictReader else : _reader = UnicodeReader with _reader ( lines_or_file , encoding = encoding , * * fix_kw ( kw ) ) as r : for item in r : yield item
5794	def _extract_error ( ) : error_num = errno ( ) try : error_string = os . strerror ( error_num ) except ( ValueError ) : return str_cls ( error_num ) if isinstance ( error_string , str_cls ) : return error_string return _try_decode ( error_string )
5464	def get_action_by_id ( op , action_id ) : actions = get_actions ( op ) if actions and 1 <= action_id < len ( actions ) : return actions [ action_id - 1 ]
13162	def raw_sql ( cls , cur , query : str , values : tuple ) : yield from cur . execute ( query , values ) return ( yield from cur . fetchall ( ) )
1690	def UpdatePreprocessor ( self , line ) : if Match ( r'^\s*#\s*(if|ifdef|ifndef)\b' , line ) : # Beginning of #if block, save the nesting stack here. The saved # stack will allow us to restore the parsing state in the #else case. self . pp_stack . append ( _PreprocessorInfo ( copy . deepcopy ( self . stack ) ) ) elif Match ( r'^\s*#\s*(else|elif)\b' , line ) : # Beginning of #else block if self . pp_stack : if not self . pp_stack [ - 1 ] . seen_else : # This is the first #else or #elif block. Remember the # whole nesting stack up to this point. This is what we # keep after the #endif. self . pp_stack [ - 1 ] . seen_else = True self . pp_stack [ - 1 ] . stack_before_else = copy . deepcopy ( self . stack ) # Restore the stack to how it was before the #if self . stack = copy . deepcopy ( self . pp_stack [ - 1 ] . stack_before_if ) else : # TODO(unknown): unexpected #else, issue warning? pass elif Match ( r'^\s*#\s*endif\b' , line ) : # End of #if or #else blocks. if self . pp_stack : # If we saw an #else, we will need to restore the nesting # stack to its former state before the #else, otherwise we # will just continue from where we left off. if self . pp_stack [ - 1 ] . seen_else : # Here we can just use a shallow copy since we are the last # reference to it. self . stack = self . pp_stack [ - 1 ] . stack_before_else # Drop the corresponding #if self . pp_stack . pop ( ) else : # TODO(unknown): unexpected #endif, issue warning? pass
12699	def _parse_data_fields ( self , fields , tag_id = "tag" , sub_id = "code" ) : for field in fields : params = field . params if tag_id not in params : continue # take care of iX/indX (indicator) parameters field_repr = OrderedDict ( [ [ self . i1_name , params . get ( self . i1_name , " " ) ] , [ self . i2_name , params . get ( self . i2_name , " " ) ] , ] ) # process all subfields for subfield in field . find ( "subfield" ) : if sub_id not in subfield . params : continue content = MARCSubrecord ( val = subfield . getContent ( ) . strip ( ) , i1 = field_repr [ self . i1_name ] , i2 = field_repr [ self . i2_name ] , other_subfields = field_repr ) # add or append content to list of other contents code = subfield . params [ sub_id ] if code in field_repr : field_repr [ code ] . append ( content ) else : field_repr [ code ] = [ content ] tag = params [ tag_id ] if tag in self . datafields : self . datafields [ tag ] . append ( field_repr ) else : self . datafields [ tag ] = [ field_repr ]
1914	def enqueue ( self , state ) : # save the state to secondary storage state_id = self . _workspace . save_state ( state ) self . put ( state_id ) self . _publish ( 'did_enqueue_state' , state_id , state ) return state_id
9684	def sn ( self ) : string = [ ] # Send the command byte and sleep for 9 ms self . cnxn . xfer ( [ 0x10 ] ) sleep ( 9e-3 ) # Read the info string by sending 60 empty bytes for i in range ( 60 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] string . append ( chr ( resp ) ) sleep ( 0.1 ) return '' . join ( string )
6631	def set ( self , path , value = None , filename = None ) : if filename is None : config = self . _firstConfig ( ) [ 1 ] else : config = self . configs [ filename ] path = _splitPath ( path ) for el in path [ : - 1 ] : if el in config : config = config [ el ] else : config [ el ] = OrderedDict ( ) config = config [ el ] config [ path [ - 1 ] ] = value
4564	def fill ( strip , item , start = 0 , stop = None , step = 1 ) : if stop is None : stop = len ( strip ) for i in range ( start , stop , step ) : strip [ i ] = item
2624	def get_instance_state ( self , instances = None ) : if instances : desc = self . client . describe_instances ( InstanceIds = instances ) else : desc = self . client . describe_instances ( InstanceIds = self . instances ) # pprint.pprint(desc['Reservations'],indent=4) for i in range ( len ( desc [ 'Reservations' ] ) ) : instance = desc [ 'Reservations' ] [ i ] [ 'Instances' ] [ 0 ] self . instance_states [ instance [ 'InstanceId' ] ] = instance [ 'State' ] [ 'Name' ] return self . instance_states
13792	def get_function ( function_name ) : module , basename = str ( function_name ) . rsplit ( '.' , 1 ) try : return getattr ( __import__ ( module , fromlist = [ basename ] ) , basename ) except ( ImportError , AttributeError ) : raise FunctionNotFound ( function_name )
4720	def tcase_enter ( trun , tsuite , tcase ) : #pylint: disable=locally-disabled, unused-argument if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tcase:enter" ) cij . emph ( "rnr:tcase:enter { fname: %r }" % tcase [ "fname" ] ) cij . emph ( "rnr:tcase:enter { log_fpath: %r }" % tcase [ "log_fpath" ] ) rcode = 0 for hook in tcase [ "hooks" ] [ "enter" ] : # tcase ENTER-hooks rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tcase:exit: { rcode: %r }" % rcode , rcode ) return rcode
12012	def do_photometry ( self ) : std_f = np . zeros ( 4 ) data_save = np . zeros_like ( self . postcard ) self . obs_flux = np . zeros_like ( self . reference_flux ) for i in range ( 4 ) : g = np . where ( self . qs == i ) [ 0 ] wh = np . where ( self . times [ g ] > 54947 ) data_save [ g ] = np . roll ( self . postcard [ g ] , int ( self . roll_best [ i , 0 ] ) , axis = 1 ) data_save [ g ] = np . roll ( data_save [ g ] , int ( self . roll_best [ i , 1 ] ) , axis = 2 ) self . target_flux_pixels = data_save [ : , self . targets == 1 ] self . target_flux = np . sum ( self . target_flux_pixels , axis = 1 ) self . obs_flux [ g ] = self . target_flux [ g ] / self . reference_flux [ g ] self . obs_flux [ g ] /= np . median ( self . obs_flux [ g [ wh ] ] ) fitline = np . polyfit ( self . times [ g ] [ wh ] , self . obs_flux [ g ] [ wh ] , 1 ) std_f [ i ] = np . max ( [ np . std ( self . obs_flux [ g ] [ wh ] / ( fitline [ 0 ] * self . times [ g ] [ wh ] + fitline [ 1 ] ) ) , 0.001 ] ) self . flux_uncert = std_f
1427	def create_parser ( subparsers ) : parser = subparsers . add_parser ( 'update' , help = 'Update a topology' , usage = "%(prog)s [options] cluster/[role]/[env] <topology-name> " + "[--component-parallelism <name:value>] " + "[--container-number value] " + "[--runtime-config [component:]<name:value>]" , add_help = True ) args . add_titles ( parser ) args . add_cluster_role_env ( parser ) args . add_topology ( parser ) args . add_config ( parser ) args . add_dry_run ( parser ) args . add_service_url ( parser ) args . add_verbose ( parser ) # Special parameters for update command def parallelism_type ( value ) : pattern = re . compile ( r"^[\w\.-]+:[\d]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for component parallelism (<component_name:value>): %s" % value ) return value parser . add_argument ( '--component-parallelism' , action = 'append' , type = parallelism_type , required = False , help = 'Component name and the new parallelism value ' + 'colon-delimited: <component_name>:<parallelism>' ) def runtime_config_type ( value ) : pattern = re . compile ( r"^([\w\.-]+:){1,2}[\w\.-]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for runtime config ([component:]<name:value>): %s" % value ) return value parser . add_argument ( '--runtime-config' , action = 'append' , type = runtime_config_type , required = False , help = 'Runtime configurations for topology and components ' + 'colon-delimited: [component:]<name>:<value>' ) def container_number_type ( value ) : pattern = re . compile ( r"^\d+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for container number (value): %s" % value ) return value parser . add_argument ( '--container-number' , action = 'append' , type = container_number_type , required = False , help = 'Number of containers <value>' ) parser . set_defaults ( subcommand = 'update' ) return parser
2501	def value_error ( self , key , bad_value ) : msg = ERROR_MESSAGES [ key ] . format ( bad_value ) self . logger . log ( msg ) self . error = True
6987	def _varfeatures_worker ( task ) : try : ( lcfile , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) = task return get_varfeatures ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , mindet = mindet , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
9612	def _execute ( self , command , data = None , unpack = True ) : if not data : data = { } data . setdefault ( 'element_id' , self . element_id ) return self . _driver . _execute ( command , data , unpack )
8366	def rendering_finished ( self , size , frame , cairo_ctx ) : surface = cairo_ctx . get_target ( ) if self . format == 'png' : surface . write_to_png ( self . _output_file ( frame ) ) surface . finish ( ) surface . flush ( )
7213	def get_proj ( prj_code ) : if prj_code in CUSTOM_PRJ : proj = pyproj . Proj ( CUSTOM_PRJ [ prj_code ] ) else : proj = pyproj . Proj ( init = prj_code ) return proj
7465	def _parse_01 ( ofiles , individual = False ) : ## parse results from outfiles cols = [ ] dats = [ ] for ofile in ofiles : ## parse file with open ( ofile ) as infile : dat = infile . read ( ) lastbits = dat . split ( ".mcmc.txt\n\n" ) [ 1 : ] results = lastbits [ 0 ] . split ( "\n\n" ) [ 0 ] . split ( ) ## get shape from ... shape = ( ( ( len ( results ) - 3 ) / 4 ) , 4 ) dat = np . array ( results [ 3 : ] ) . reshape ( shape ) cols . append ( dat [ : , 3 ] . astype ( float ) ) if not individual : ## get mean results across reps cols = np . array ( cols ) cols = cols . sum ( axis = 0 ) / len ( ofiles ) #10. dat [ : , 3 ] = cols . astype ( str ) ## format as a DF df = pd . DataFrame ( dat [ : , 1 : ] ) df . columns = [ "delim" , "prior" , "posterior" ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) df [ "nspecies" ] = nspecies return df else : ## get mean results across reps #return cols res = [ ] for i in xrange ( len ( cols ) ) : x = dat x [ : , 3 ] = cols [ i ] . astype ( str ) x = pd . DataFrame ( x [ : , 1 : ] ) x . columns = [ 'delim' , 'prior' , 'posterior' ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) x [ "nspecies" ] = nspecies res . append ( x ) return res
6964	def get ( self ) : # generate the project's list of checkplots project_checkplots = self . currentproject [ 'checkplots' ] project_checkplotbasenames = [ os . path . basename ( x ) for x in project_checkplots ] project_checkplotindices = range ( len ( project_checkplots ) ) # get the sortkey and order project_cpsortkey = self . currentproject [ 'sortkey' ] if self . currentproject [ 'sortorder' ] == 'asc' : project_cpsortorder = 'ascending' elif self . currentproject [ 'sortorder' ] == 'desc' : project_cpsortorder = 'descending' # get the filterkey and condition project_cpfilterstatements = self . currentproject [ 'filterstatements' ] self . render ( 'cpindex.html' , project_checkplots = project_checkplots , project_cpsortorder = project_cpsortorder , project_cpsortkey = project_cpsortkey , project_cpfilterstatements = project_cpfilterstatements , project_checkplotbasenames = project_checkplotbasenames , project_checkplotindices = project_checkplotindices , project_checkplotfile = self . cplistfile , readonly = self . readonly , baseurl = self . baseurl )
10951	def update ( self , params , values ) : return super ( State , self ) . update ( params , values )
989	def add ( reader , writer , column , start , stop , value ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( value ) ( row [ column ] ) + value writer . appendRecord ( row )
9424	def _open ( self , archive ) : try : handle = unrarlib . RAROpenArchiveEx ( ctypes . byref ( archive ) ) except unrarlib . UnrarException : raise BadRarFile ( "Invalid RAR file." ) return handle
11070	def proxy_factory ( BaseSchema , label , ProxiedClass , get_key ) : def local ( ) : key = get_key ( ) try : return proxies [ BaseSchema ] [ label ] [ key ] except KeyError : proxies [ BaseSchema ] [ label ] [ key ] = ProxiedClass ( ) return proxies [ BaseSchema ] [ label ] [ key ] return LocalProxy ( local )
3600	def http_connection ( timeout ) : def wrapper ( f ) : def wrapped ( * args , * * kwargs ) : if not ( 'connection' in kwargs ) or not kwargs [ 'connection' ] : connection = requests . Session ( ) kwargs [ 'connection' ] = connection else : connection = kwargs [ 'connection' ] if not getattr ( connection , 'timeout' , False ) : connection . timeout = timeout connection . headers . update ( { 'Content-type' : 'application/json' } ) return f ( * args , * * kwargs ) return wraps ( f ) ( wrapped ) return wrapper
13864	def tsms ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) ) * 1000 + int ( round ( when . microsecond / 1000.0 ) )
4771	def contains ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] not in self . val : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain key <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : self . _err ( 'Expected <%s> to contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain keys %s, but did not contain key%s %s.' % ( self . val , self . _fmt_items ( items ) , '' if len ( missing ) == 0 else 's' , self . _fmt_items ( missing ) ) ) else : self . _err ( 'Expected <%s> to contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
2445	def reset_package ( self ) : # FIXME: this state does not make sense self . package_set = False self . package_vers_set = False self . package_file_name_set = False self . package_supplier_set = False self . package_originator_set = False self . package_down_location_set = False self . package_home_set = False self . package_verif_set = False self . package_chk_sum_set = False self . package_source_info_set = False self . package_conc_lics_set = False self . package_license_declared_set = False self . package_license_comment_set = False self . package_cr_text_set = False self . package_summary_set = False self . package_desc_set = False
2995	def _getJsonIEXCloud ( url , token = '' , version = 'beta' ) : url = _URL_PREFIX2 . format ( version = version ) + url resp = requests . get ( urlparse ( url ) . geturl ( ) , proxies = _PYEX_PROXIES , params = { 'token' : token } ) if resp . status_code == 200 : return resp . json ( ) raise PyEXception ( 'Response %d - ' % resp . status_code , resp . text )
13455	def _parse_args ( args ) : # parser uses custom usage string, with 'usage: ' removed, as it is # added automatically via argparser. parser = argparse . ArgumentParser ( description = "Remove and/or rearrange " + "sections from each line of a file(s)." , usage = _usage ( ) [ len ( 'usage: ' ) : ] ) parser . add_argument ( '-b' , "--bytes" , action = 'store' , type = lst , default = [ ] , help = "Bytes to select" ) parser . add_argument ( '-c' , "--chars" , action = 'store' , type = lst , default = [ ] , help = "Character to select" ) parser . add_argument ( '-f' , "--fields" , action = 'store' , type = lst , default = [ ] , help = "Fields to select" ) parser . add_argument ( '-d' , "--delimiter" , action = 'store' , default = "\t" , help = "Sets field delimiter(default is TAB)" ) parser . add_argument ( '-e' , "--regex" , action = 'store_true' , help = 'Enable regular expressions to be used as input ' + 'delimiter' ) parser . add_argument ( '-s' , '--skip' , action = 'store_true' , help = "Skip lines that do not contain input delimiter." ) parser . add_argument ( '-S' , "--separator" , action = 'store' , default = "\t" , help = "Sets field separator for output." ) parser . add_argument ( 'file' , nargs = '*' , default = "-" , help = "File(s) to cut" ) return parser . parse_args ( args )
12595	def get_aad_token ( endpoint , no_verify ) : #pylint: disable-msg=too-many-locals from azure . servicefabric . service_fabric_client_ap_is import ( ServiceFabricClientAPIs ) from sfctl . auth import ClientCertAuthentication from sfctl . config import set_aad_metadata auth = ClientCertAuthentication ( None , None , no_verify ) client = ServiceFabricClientAPIs ( auth , base_url = endpoint ) aad_metadata = client . get_aad_metadata ( ) if aad_metadata . type != "aad" : raise CLIError ( "Not AAD cluster" ) aad_resource = aad_metadata . metadata tenant_id = aad_resource . tenant authority_uri = aad_resource . login + '/' + tenant_id context = adal . AuthenticationContext ( authority_uri , api_version = None ) cluster_id = aad_resource . cluster client_id = aad_resource . client set_aad_metadata ( authority_uri , cluster_id , client_id ) code = context . acquire_user_code ( cluster_id , client_id ) print ( code [ 'message' ] ) token = context . acquire_token_with_device_code ( cluster_id , code , client_id ) print ( "Succeed!" ) return token , context . cache
5986	def bulge_disk_tag_from_align_bulge_disks ( align_bulge_disk_centre , align_bulge_disk_axis_ratio , align_bulge_disk_phi ) : align_bulge_disk_centre_tag = align_bulge_disk_centre_tag_from_align_bulge_disk_centre ( align_bulge_disk_centre = align_bulge_disk_centre ) align_bulge_disk_axis_ratio_tag = align_bulge_disk_axis_ratio_tag_from_align_bulge_disk_axis_ratio ( align_bulge_disk_axis_ratio = align_bulge_disk_axis_ratio ) align_bulge_disk_phi_tag = align_bulge_disk_phi_tag_from_align_bulge_disk_phi ( align_bulge_disk_phi = align_bulge_disk_phi ) return align_bulge_disk_centre_tag + align_bulge_disk_axis_ratio_tag + align_bulge_disk_phi_tag
11733	def isValidClass ( self , class_ ) : module = inspect . getmodule ( class_ ) valid = ( module in self . _valid_modules or ( hasattr ( module , '__file__' ) and module . __file__ in self . _valid_named_modules ) ) return valid and not private ( class_ )
10753	def open_archive ( fs_url , archive ) : it = pkg_resources . iter_entry_points ( 'fs.archive.open_archive' ) entry_point = next ( ( ep for ep in it if archive . endswith ( ep . name ) ) , None ) if entry_point is None : raise UnsupportedProtocol ( 'unknown archive extension: {}' . format ( archive ) ) try : archive_opener = entry_point . load ( ) except pkg_resources . DistributionNotFound as df : # pragma: no cover six . raise_from ( UnsupportedProtocol ( 'extension {} requires {}' . format ( entry_point . name , df . req ) ) , None ) try : binfile = None archive_fs = None fs = open_fs ( fs_url ) if issubclass ( archive_opener , base . ArchiveFS ) : try : binfile = fs . openbin ( archive , 'r+' ) except errors . ResourceNotFound : binfile = fs . openbin ( archive , 'w' ) except errors . ResourceReadOnly : binfile = fs . openbin ( archive , 'r' ) archive_opener = archive_opener . _read_fs_cls elif issubclass ( archive_opener , base . ArchiveReadFS ) : binfile = fs . openbin ( archive , 'r' ) if not hasattr ( binfile , 'name' ) : binfile . name = basename ( archive ) archive_fs = archive_opener ( binfile ) except Exception : getattr ( archive_fs , 'close' , lambda : None ) ( ) getattr ( binfile , 'close' , lambda : None ) ( ) raise else : return archive_fs
1787	def DAA ( cpu ) : cpu . AF = Operators . OR ( ( cpu . AL & 0x0f ) > 9 , cpu . AF ) oldAL = cpu . AL cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL + 6 , cpu . AL ) cpu . CF = Operators . ITE ( cpu . AF , Operators . OR ( cpu . CF , cpu . AL < oldAL ) , cpu . CF ) cpu . CF = Operators . OR ( ( cpu . AL & 0xf0 ) > 0x90 , cpu . CF ) cpu . AL = Operators . ITEBV ( 8 , cpu . CF , cpu . AL + 0x60 , cpu . AL ) """ #old not-symbolic aware version... if ((cpu.AL & 0x0f) > 9) or cpu.AF: oldAL = cpu.AL cpu.AL = cpu.AL + 6 cpu.CF = Operators.OR(cpu.CF, cpu.AL < oldAL) cpu.AF = True else: cpu.AF = False if ((cpu.AL & 0xf0) > 0x90) or cpu.CF: cpu.AL = cpu.AL + 0x60 cpu.CF = True else: cpu.CF = False """ cpu . ZF = cpu . AL == 0 cpu . SF = ( cpu . AL & 0x80 ) != 0 cpu . PF = cpu . _calculate_parity_flag ( cpu . AL )
6962	def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , bytes ) : return obj . decode ( ) elif isinstance ( obj , complex ) : return ( obj . real , obj . imag ) elif ( isinstance ( obj , ( float , np . float64 , np . float_ ) ) and not np . isfinite ( obj ) ) : return None elif isinstance ( obj , ( np . int8 , np . int16 , np . int32 , np . int64 ) ) : return int ( obj ) else : return json . JSONEncoder . default ( self , obj )
34	def gpu_count ( ) : if shutil . which ( 'nvidia-smi' ) is None : return 0 output = subprocess . check_output ( [ 'nvidia-smi' , '--query-gpu=gpu_name' , '--format=csv' ] ) return max ( 0 , len ( output . split ( b'\n' ) ) - 2 )
6967	def smooth_magseries_gaussfilt ( mags , windowsize , windowfwhm = 7 ) : convkernel = Gaussian1DKernel ( windowfwhm , x_size = windowsize ) smoothed = convolve ( mags , convkernel , boundary = 'extend' ) return smoothed
10378	def calculate_concordance_probability ( graph : BELGraph , key : str , cutoff : Optional [ float ] = None , permutations : Optional [ int ] = None , percentage : Optional [ float ] = None , use_ambiguous : bool = False , permute_type : str = 'shuffle_node_data' , ) -> Tuple [ float , List [ float ] , float ] : if permute_type == 'random_by_edges' : permute_func = partial ( random_by_edges , percentage = percentage ) elif permute_type == 'shuffle_node_data' : permute_func = partial ( shuffle_node_data , key = key , percentage = percentage ) elif permute_type == 'shuffle_relations' : permute_func = partial ( shuffle_relations , percentage = percentage ) else : raise ValueError ( 'Invalid permute_type: {}' . format ( permute_type ) ) graph : BELGraph = graph . copy ( ) collapse_to_genes ( graph ) collapse_all_variants ( graph ) score = calculate_concordance ( graph , key , cutoff = cutoff ) distribution = [ ] for _ in range ( permutations or 500 ) : permuted_graph = permute_func ( graph ) permuted_graph_scores = calculate_concordance ( permuted_graph , key , cutoff = cutoff , use_ambiguous = use_ambiguous ) distribution . append ( permuted_graph_scores ) return score , distribution , one_sided ( score , distribution )
4046	def num_tagitems ( self , tag ) : query = "/{t}/{u}/tags/{ta}/items" . format ( u = self . library_id , t = self . library_type , ta = tag ) return self . _totals ( query )
9467	def conference_play ( self , call_params ) : path = '/' + self . api_version + '/ConferencePlay/' method = 'POST' return self . request ( path , method , call_params )
5471	def lookup_job_tasks ( provider , statuses , user_ids = None , job_ids = None , job_names = None , task_ids = None , task_attempts = None , labels = None , create_time_min = None , create_time_max = None , max_tasks = 0 , page_size = 0 , summary_output = False ) : tasks_generator = provider . lookup_job_tasks ( statuses , user_ids = user_ids , job_ids = job_ids , job_names = job_names , task_ids = task_ids , task_attempts = task_attempts , labels = labels , create_time_min = create_time_min , create_time_max = create_time_max , max_tasks = max_tasks , page_size = page_size ) # Yield formatted tasks. for task in tasks_generator : yield _prepare_row ( task , True , summary_output )
170	def draw_mask ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : heatmap = self . draw_heatmap_array ( image_shape , alpha_lines = 1.0 , alpha_points = 1.0 , size_lines = size_lines , size_points = size_points , antialiased = False , raise_if_out_of_image = raise_if_out_of_image ) return heatmap > 0.5
7304	def set_permissions_in_context ( self , context = { } ) : context [ 'has_view_permission' ] = self . mongoadmin . has_view_permission ( self . request ) context [ 'has_edit_permission' ] = self . mongoadmin . has_edit_permission ( self . request ) context [ 'has_add_permission' ] = self . mongoadmin . has_add_permission ( self . request ) context [ 'has_delete_permission' ] = self . mongoadmin . has_delete_permission ( self . request ) return context
6781	def get_current_thumbprint ( self , components = None ) : components = str_to_component_list ( components ) if self . verbose : print ( 'deploy.get_current_thumbprint.components:' , components ) manifest_data = { } # {component:data} for component_name , func in sorted ( manifest_recorder . items ( ) ) : self . vprint ( 'Checking thumbprint for component %s...' % component_name ) manifest_key = assert_valid_satchel ( component_name ) service_name = clean_service_name ( component_name ) if service_name not in self . genv . services : self . vprint ( 'Skipping unused component:' , component_name ) continue elif components and service_name not in components : self . vprint ( 'Skipping non-matching component:' , component_name ) continue try : self . vprint ( 'Retrieving manifest for %s...' % component_name ) manifest_data [ manifest_key ] = func ( ) if self . verbose : pprint ( manifest_data [ manifest_key ] , indent = 4 ) except exceptions . AbortDeployment as e : raise return manifest_data
1248	def is_action_available ( self , action ) : temp_state = np . rot90 ( self . _state , action ) return self . _is_action_available_left ( temp_state )
7363	def with_prefix ( self , prefix , strict = False ) : def decorated ( func ) : return EventHandler ( func = func , event = self . event , prefix = prefix , strict = strict ) return decorated
3767	def zs_to_ws ( zs , MWs ) : Mavg = sum ( zi * MWi for zi , MWi in zip ( zs , MWs ) ) ws = [ zi * MWi / Mavg for zi , MWi in zip ( zs , MWs ) ] return ws
10727	def _handle_array ( toks ) : if len ( toks ) == 5 and toks [ 1 ] == '{' and toks [ 4 ] == '}' : subtree = toks [ 2 : 4 ] signature = '' . join ( s for ( _ , s ) in subtree ) [ key_func , value_func ] = [ f for ( f , _ ) in subtree ] def the_dict_func ( a_dict , variant = 0 ) : """ Function for generating a Dictionary from a dict. :param a_dict: the dictionary to transform :type a_dict: dict of (`a * `b) :param int variant: variant level :returns: a dbus dictionary of transformed values and level :rtype: Dictionary * int """ elements = [ ( key_func ( x ) , value_func ( y ) ) for ( x , y ) in a_dict . items ( ) ] level = 0 if elements == [ ] else max ( max ( x , y ) for ( ( _ , x ) , ( _ , y ) ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Dictionary ( ( ( x , y ) for ( ( x , _ ) , ( y , _ ) ) in elements ) , signature = signature , variant_level = obj_level ) , func_level ) return ( the_dict_func , 'a{' + signature + '}' ) if len ( toks ) == 2 : ( func , sig ) = toks [ 1 ] def the_array_func ( a_list , variant = 0 ) : """ Function for generating an Array from a list. :param a_list: the list to transform :type a_list: list of `a :param int variant: variant level of the value :returns: a dbus Array of transformed values and variant level :rtype: Array * int """ if isinstance ( a_list , dict ) : raise IntoDPValueError ( a_list , "a_list" , "is a dict, must be an array" ) elements = [ func ( x ) for x in a_list ] level = 0 if elements == [ ] else max ( x for ( _ , x ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Array ( ( x for ( x , _ ) in elements ) , signature = sig , variant_level = obj_level ) , func_level ) return ( the_array_func , 'a' + sig ) raise IntoDPValueError ( toks , "toks" , "unexpected tokens" )
13196	def ensure_format ( doc , format ) : assert format in ( 'xml' , 'json' ) if getattr ( doc , 'tag' , None ) == 'open511' : if format == 'json' : return xml_to_json ( doc ) elif isinstance ( doc , dict ) and 'meta' in doc : if format == 'xml' : return json_doc_to_xml ( doc ) else : raise ValueError ( "Unrecognized input document" ) return doc
6646	def _mirrorStructure ( dictionary , value ) : result = type ( dictionary ) ( ) for k in dictionary . keys ( ) : if isinstance ( dictionary [ k ] , dict ) : result [ k ] = _mirrorStructure ( dictionary [ k ] , value ) else : result [ k ] = value return result
12544	def nifti_out ( f ) : @ wraps ( f ) def wrapped ( * args , * * kwargs ) : r = f ( * args , * * kwargs ) img = read_img ( args [ 0 ] ) return nib . Nifti1Image ( r , affine = img . get_affine ( ) , header = img . header ) return wrapped
1252	def add_random_tile ( self ) : x_pos , y_pos = np . where ( self . _state == 0 ) assert len ( x_pos ) != 0 empty_index = np . random . choice ( len ( x_pos ) ) value = np . random . choice ( [ 1 , 2 ] , p = [ 0.9 , 0.1 ] ) self . _state [ x_pos [ empty_index ] , y_pos [ empty_index ] ] = value
8915	def fetch_by_name ( self , name ) : service = self . name_index . get ( name ) if not service : raise ServiceNotFound return Service ( service )
12900	def get_equalisers ( self ) : if not self . __equalisers : self . __equalisers = yield from self . handle_list ( self . API . get ( 'equalisers' ) ) return self . __equalisers
5778	def _bcrypt_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = BcryptConst . BCRYPT_PAD_PKCS1 if rsa_oaep_padding is True : flags = BcryptConst . BCRYPT_PAD_OAEP padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_OAEP_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) # This has to be assigned to a variable to prevent cffi from gc'ing it hash_buffer = buffer_from_unicode ( BcryptConst . BCRYPT_SHA1_ALGORITHM ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . pbLabel = null ( ) padding_info_struct . cbLabel = 0 padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) else : padding_info = null ( ) out_len = new ( bcrypt , 'ULONG *' ) res = bcrypt . BCryptEncrypt ( certificate_or_public_key . key_handle , data , len ( data ) , padding_info , null ( ) , 0 , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) res = bcrypt . BCryptEncrypt ( certificate_or_public_key . key_handle , data , len ( data ) , padding_info , null ( ) , 0 , buffer , buffer_len , out_len , flags ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) )
6636	def publish ( self , registry = None ) : if ( registry is None ) or ( registry == registry_access . Registry_Base_URL ) : if 'private' in self . description and self . description [ 'private' ] : return "this %s is private and cannot be published" % ( self . description_filename . split ( '.' ) [ 0 ] ) upload_archive = os . path . join ( self . path , 'upload.tar.gz' ) fsutils . rmF ( upload_archive ) fd = os . open ( upload_archive , os . O_CREAT | os . O_EXCL | os . O_RDWR | getattr ( os , "O_BINARY" , 0 ) ) with os . fdopen ( fd , 'rb+' ) as tar_file : tar_file . truncate ( ) self . generateTarball ( tar_file ) logger . debug ( 'generated tar file of length %s' , tar_file . tell ( ) ) tar_file . seek ( 0 ) # calculate the hash of the file before we upload it: shasum = hashlib . sha256 ( ) while True : chunk = tar_file . read ( 1000 ) if not chunk : break shasum . update ( chunk ) logger . debug ( 'generated tar file has hash %s' , shasum . hexdigest ( ) ) tar_file . seek ( 0 ) with self . findAndOpenReadme ( ) as readme_file_wrapper : if not readme_file_wrapper : logger . warning ( "no readme.md file detected" ) with open ( self . getDescriptionFile ( ) , 'r' ) as description_file : return registry_access . publish ( self . getRegistryNamespace ( ) , self . getName ( ) , self . getVersion ( ) , description_file , tar_file , readme_file_wrapper . file , readme_file_wrapper . extension ( ) . lower ( ) , registry = registry )
4294	def supported_versions ( django , cms ) : cms_version = None django_version = None try : cms_version = Decimal ( cms ) except ( ValueError , InvalidOperation ) : try : cms_version = CMS_VERSION_MATRIX [ str ( cms ) ] except KeyError : pass try : django_version = Decimal ( django ) except ( ValueError , InvalidOperation ) : try : django_version = DJANGO_VERSION_MATRIX [ str ( django ) ] except KeyError : # pragma: no cover pass try : if ( cms_version and django_version and not ( LooseVersion ( VERSION_MATRIX [ compat . unicode ( cms_version ) ] [ 0 ] ) <= LooseVersion ( compat . unicode ( django_version ) ) <= LooseVersion ( VERSION_MATRIX [ compat . unicode ( cms_version ) ] [ 1 ] ) ) ) : raise RuntimeError ( 'Django and django CMS versions doesn\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django_version , cms_version ) ) except KeyError : raise RuntimeError ( 'Django and django CMS versions doesn\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django_version , cms_version ) ) return ( compat . unicode ( django_version ) if django_version else django_version , compat . unicode ( cms_version ) if cms_version else cms_version )
9753	def experiment ( ctx , project , experiment ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'experiment' ] = experiment
9963	def get_impls ( interfaces ) : if interfaces is None : return None elif isinstance ( interfaces , Mapping ) : return { name : interfaces [ name ] . _impl for name in interfaces } elif isinstance ( interfaces , Sequence ) : return [ interfaces . _impl for interfaces in interfaces ] else : return interfaces . _impl
7189	def fix_line_numbers ( body ) : maxline = 0 for node in body . pre_order ( ) : maxline += node . prefix . count ( '\n' ) if isinstance ( node , Leaf ) : node . lineno = maxline maxline += str ( node . value ) . count ( '\n' )
12160	def parent ( groups , ID ) : if ID in groups . keys ( ) : return ID # already a parent if not ID in groups . keys ( ) : for actualParent in groups . keys ( ) : if ID in groups [ actualParent ] : return actualParent # found the actual parent return None
1704	def outer_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
9952	def get_object ( name : str ) : # TODO: Duplicate of system.get_object elms = name . split ( "." ) parent = get_models ( ) [ elms . pop ( 0 ) ] while len ( elms ) > 0 : obj = elms . pop ( 0 ) parent = getattr ( parent , obj ) return parent
10262	def _collapse_edge_passing_predicates ( graph : BELGraph , edge_predicates : EdgePredicates = None ) -> None : for u , v , _ in filter_edges ( graph , edge_predicates = edge_predicates ) : collapse_pair ( graph , survivor = u , victim = v )
9991	def _get_dynamic_base ( self , bases_ ) : bases = tuple ( base . bases [ 0 ] if base . is_dynamic ( ) else base for base in bases_ ) if len ( bases ) == 1 : return bases [ 0 ] elif len ( bases ) > 1 : return self . model . get_dynamic_base ( bases ) else : RuntimeError ( "must not happen" )
9894	def _uptime_solaris ( ) : global __boottime try : kstat = ctypes . CDLL ( 'libkstat.so' ) except ( AttributeError , OSError ) : return None # kstat doesn't have uptime, but it does have boot time. # Unfortunately, getting at it isn't perfectly straightforward. # First, let's pretend to be kstat.h # Constant KSTAT_STRLEN = 31 # According to every kstat.h I could find. # Data structures class anon_union ( ctypes . Union ) : # The ``value'' union in kstat_named_t actually has a bunch more # members, but we're only using it for boot_time, so we only need # the padding and the one we're actually using. _fields_ = [ ( 'c' , ctypes . c_char * 16 ) , ( 'time' , ctypes . c_int ) ] class kstat_named_t ( ctypes . Structure ) : _fields_ = [ ( 'name' , ctypes . c_char * KSTAT_STRLEN ) , ( 'data_type' , ctypes . c_char ) , ( 'value' , anon_union ) ] # Function signatures kstat . kstat_open . restype = ctypes . c_void_p kstat . kstat_lookup . restype = ctypes . c_void_p kstat . kstat_lookup . argtypes = [ ctypes . c_void_p , ctypes . c_char_p , ctypes . c_int , ctypes . c_char_p ] kstat . kstat_read . restype = ctypes . c_int kstat . kstat_read . argtypes = [ ctypes . c_void_p , ctypes . c_void_p , ctypes . c_void_p ] kstat . kstat_data_lookup . restype = ctypes . POINTER ( kstat_named_t ) kstat . kstat_data_lookup . argtypes = [ ctypes . c_void_p , ctypes . c_char_p ] # Now, let's do something useful. # Initialise kstat control structure. kc = kstat . kstat_open ( ) if not kc : return None # We're looking for unix:0:system_misc:boot_time. ksp = kstat . kstat_lookup ( kc , 'unix' , 0 , 'system_misc' ) if ksp and kstat . kstat_read ( kc , ksp , None ) != - 1 : data = kstat . kstat_data_lookup ( ksp , 'boot_time' ) if data : __boottime = data . contents . value . time # Clean-up. kstat . kstat_close ( kc ) if __boottime is not None : return time . time ( ) - __boottime return None
3709	def calculate ( self , T , method ) : if method == RACKETT : Vm = Rackett ( T , self . Tc , self . Pc , self . Zc ) elif method == YAMADA_GUNN : Vm = Yamada_Gunn ( T , self . Tc , self . Pc , self . omega ) elif method == BHIRUD_NORMAL : Vm = Bhirud_normal ( T , self . Tc , self . Pc , self . omega ) elif method == TOWNSEND_HALES : Vm = Townsend_Hales ( T , self . Tc , self . Vc , self . omega ) elif method == HTCOSTALD : Vm = COSTALD ( T , self . Tc , self . Vc , self . omega ) elif method == YEN_WOODS_SAT : Vm = Yen_Woods_saturation ( T , self . Tc , self . Vc , self . Zc ) elif method == MMSNM0 : Vm = SNM0 ( T , self . Tc , self . Vc , self . omega ) elif method == MMSNM0FIT : Vm = SNM0 ( T , self . Tc , self . Vc , self . omega , self . SNM0_delta_SRK ) elif method == CAMPBELL_THODOS : Vm = Campbell_Thodos ( T , self . Tb , self . Tc , self . Pc , self . MW , self . dipole ) elif method == HTCOSTALDFIT : Vm = COSTALD ( T , self . Tc , self . COSTALD_Vchar , self . COSTALD_omega_SRK ) elif method == RACKETTFIT : Vm = Rackett ( T , self . Tc , self . Pc , self . RACKETT_Z_RA ) elif method == PERRYDIPPR : A , B , C , D = self . DIPPR_coeffs Vm = 1. / EQ105 ( T , A , B , C , D ) elif method == CRC_INORG_L : rho = CRC_inorganic ( T , self . CRC_INORG_L_rho , self . CRC_INORG_L_k , self . CRC_INORG_L_Tm ) Vm = rho_to_Vm ( rho , self . CRC_INORG_L_MW ) elif method == VDI_PPDS : A , B , C , D = self . VDI_PPDS_coeffs tau = 1. - T / self . VDI_PPDS_Tc rho = self . VDI_PPDS_rhoc + A * tau ** 0.35 + B * tau ** ( 2 / 3. ) + C * tau + D * tau ** ( 4 / 3. ) Vm = rho_to_Vm ( rho , self . VDI_PPDS_MW ) elif method == CRC_INORG_L_CONST : Vm = self . CRC_INORG_L_CONST_Vm elif method == COOLPROP : Vm = 1. / CoolProp_T_dependent_property ( T , self . CASRN , 'DMOLAR' , 'l' ) elif method in self . tabular_data : Vm = self . interpolate ( T , method ) return Vm
3038	def put ( self , credentials ) : self . acquire_lock ( ) try : self . locked_put ( credentials ) finally : self . release_lock ( )
74	def EdgeDetect ( alpha = 0 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ 0 , 1 , 0 ] , [ 1 , - 4 , 1 ] , [ 0 , 1 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
12323	def api_call_action ( func ) : def _inner ( * args , * * kwargs ) : return func ( * args , * * kwargs ) _inner . __name__ = func . __name__ _inner . __doc__ = func . __doc__ return _inner
12083	def filesByExtension ( fnames ) : byExt = { "abf" : [ ] , "jpg" : [ ] , "tif" : [ ] } # prime it with empties for fname in fnames : ext = os . path . splitext ( fname ) [ 1 ] . replace ( "." , '' ) . lower ( ) if not ext in byExt . keys ( ) : byExt [ ext ] = [ ] byExt [ ext ] = byExt [ ext ] + [ fname ] return byExt
2627	def show_summary ( self ) : self . get_instance_state ( ) status_string = "EC2 Summary:\n\tVPC IDs: {}\n\tSubnet IDs: \ {}\n\tSecurity Group ID: {}\n\tRunning Instance IDs: {}\n" . format ( self . vpc_id , self . sn_ids , self . sg_id , self . instances ) status_string += "\tInstance States:\n\t\t" self . get_instance_state ( ) for state in self . instance_states . keys ( ) : status_string += "Instance ID: {} State: {}\n\t\t" . format ( state , self . instance_states [ state ] ) status_string += "\n" logger . info ( status_string ) return status_string
13408	def setupUI ( self ) : labelSizePolicy = QSizePolicy ( QSizePolicy . Fixed , QSizePolicy . Fixed ) labelSizePolicy . setHorizontalStretch ( 0 ) labelSizePolicy . setVerticalStretch ( 0 ) menuSizePolicy = QSizePolicy ( QSizePolicy . Expanding , QSizePolicy . Fixed ) menuSizePolicy . setHorizontalStretch ( 0 ) menuSizePolicy . setVerticalStretch ( 0 ) logTypeLayout = QHBoxLayout ( ) logTypeLayout . setSpacing ( 0 ) typeLabel = QLabel ( "Log Type:" ) typeLabel . setMinimumSize ( QSize ( 65 , 0 ) ) typeLabel . setMaximumSize ( QSize ( 65 , 16777215 ) ) typeLabel . setSizePolicy ( labelSizePolicy ) logTypeLayout . addWidget ( typeLabel ) self . logType = QComboBox ( self ) self . logType . setMinimumSize ( QSize ( 100 , 0 ) ) self . logType . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . logType . sizePolicy ( ) . hasHeightForWidth ( ) ) self . logType . setSizePolicy ( menuSizePolicy ) logTypeLayout . addWidget ( self . logType ) logTypeLayout . setStretch ( 1 , 6 ) programLayout = QHBoxLayout ( ) programLayout . setSpacing ( 0 ) programLabel = QLabel ( "Program:" ) programLabel . setMinimumSize ( QSize ( 60 , 0 ) ) programLabel . setMaximumSize ( QSize ( 60 , 16777215 ) ) programLabel . setSizePolicy ( labelSizePolicy ) programLayout . addWidget ( programLabel ) self . programName = QComboBox ( self ) self . programName . setMinimumSize ( QSize ( 100 , 0 ) ) self . programName . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . programName . sizePolicy ( ) . hasHeightForWidth ( ) ) self . programName . setSizePolicy ( menuSizePolicy ) programLayout . addWidget ( self . programName ) programLayout . setStretch ( 1 , 6 ) # Initial instance allows adding additional menus, all following menus can only remove themselves. if self . initialInstance : self . logButton = QPushButton ( "+" , self ) self . logButton . setToolTip ( "Add logbook" ) else : self . logButton = QPushButton ( "-" ) self . logButton . setToolTip ( "Remove logbook" ) self . logButton . setMinimumSize ( QSize ( 16 , 16 ) ) # 24x24 self . logButton . setMaximumSize ( QSize ( 16 , 16 ) ) # 24x24 self . logButton . setObjectName ( "roundButton" ) # self.logButton.setAutoFillBackground(True) # region = QRegion(QRect(self.logButton.x()+15, self.logButton.y()+14, 20, 20), QRegion.Ellipse) # self.logButton.setMask(region) self . logButton . setStyleSheet ( "QPushButton {border-radius: 8px;}" ) self . _logSelectLayout = QHBoxLayout ( ) self . _logSelectLayout . setSpacing ( 6 ) self . _logSelectLayout . addLayout ( logTypeLayout ) self . _logSelectLayout . addLayout ( programLayout ) self . _logSelectLayout . addWidget ( self . logButton ) self . _logSelectLayout . setStretch ( 0 , 6 ) self . _logSelectLayout . setStretch ( 1 , 6 )
4411	def store ( self , key : object , value : object ) : self . _user_data . update ( { key : value } )
1784	def CMP ( cpu , src1 , src2 ) : arg0 = src1 . read ( ) arg1 = Operators . SEXTEND ( src2 . read ( ) , src2 . size , src1 . size ) # Affected Flags o..szapc cpu . _calculate_CMP_flags ( src1 . size , arg0 - arg1 , arg0 , arg1 )
4578	def set_one ( desc , name , value ) : old_value = desc . get ( name ) if old_value is None : raise KeyError ( 'No section "%s"' % name ) if value is None : value = type ( old_value ) ( ) elif name in CLASS_SECTIONS : if isinstance ( value , str ) : value = { 'typename' : aliases . resolve ( value ) } elif isinstance ( value , type ) : value = { 'typename' : class_name . class_name ( value ) } elif not isinstance ( value , dict ) : raise TypeError ( 'Expected dict, str or type, got "%s"' % value ) typename = value . get ( 'typename' ) if typename : s = 's' if name == 'driver' else '' path = 'bibliopixel.' + name + s importer . import_symbol ( typename , path ) elif name == 'shape' : if not isinstance ( value , ( list , int , tuple , str ) ) : raise TypeError ( 'Expected shape, got "%s"' % value ) elif type ( old_value ) is not type ( value ) : raise TypeError ( 'Expected %s but got "%s" of type %s' % ( type ( old_value ) , value , type ( value ) ) ) desc [ name ] = value
7390	def add_edges ( self ) : for group , edgelist in self . edges . items ( ) : for ( u , v , d ) in edgelist : self . draw_edge ( u , v , d , group )
13251	async def process_sphinx_technote ( session , github_api_token , ltd_product_data , mongo_collection = None ) : logger = logging . getLogger ( __name__ ) github_url = ltd_product_data [ 'doc_repo' ] github_url = normalize_repo_root_url ( github_url ) repo_slug = parse_repo_slug_from_url ( github_url ) try : metadata_yaml = await download_metadata_yaml ( session , github_url ) except aiohttp . ClientResponseError as err : # metadata.yaml not found; probably not a Sphinx technote logger . debug ( 'Tried to download %s\'s metadata.yaml, got status %d' , ltd_product_data [ 'slug' ] , err . code ) raise NotSphinxTechnoteError ( ) # Extract data from the GitHub API github_query = GitHubQuery . load ( 'technote_repo' ) github_variables = { "orgName" : repo_slug . owner , "repoName" : repo_slug . repo } github_data = await github_request ( session , github_api_token , query = github_query , variables = github_variables ) try : jsonld = reduce_technote_metadata ( github_url , metadata_yaml , github_data , ltd_product_data ) except Exception as exception : message = "Issue building JSON-LD for technote %s" logger . exception ( message , github_url , exception ) raise if mongo_collection is not None : await _upload_to_mongodb ( mongo_collection , jsonld ) logger . info ( 'Ingested technote %s into MongoDB' , github_url ) return jsonld
10744	def print_runtime ( function ) : def wrapper ( * args , * * kwargs ) : pr = cProfile . Profile ( ) pr . enable ( ) output = function ( * args , * * kwargs ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort_stats ( 'tot' ) . print_stats ( 20 ) return output return wrapper
6145	def DSP_callback_toc ( self ) : if self . Tcapture > 0 : self . DSP_toc . append ( time . time ( ) - self . start_time )
12381	def delete ( self , request , response ) : if self . slug is None : # Mass-DELETE is not implemented. raise http . exceptions . NotImplemented ( ) # Ensure we're allowed to destroy a resource. self . assert_operations ( 'destroy' ) # Delegate to `destroy` to destroy the item. self . destroy ( ) # Build the response object. self . response . status = http . client . NO_CONTENT self . make_response ( )
589	def setAutoDetectWaitRecords ( self , waitRecords ) : if not isinstance ( waitRecords , int ) : raise HTMPredictionModelInvalidArgument ( "Invalid argument type \'%s\'. WaitRecord " "must be a number." % ( type ( waitRecords ) ) ) if len ( self . saved_states ) > 0 and waitRecords < self . saved_states [ 0 ] . ROWID : raise HTMPredictionModelInvalidArgument ( "Invalid value. autoDetectWaitRecord value " "must be valid record within output stream. Current minimum ROWID in " "output stream is %d." % ( self . saved_states [ 0 ] . ROWID ) ) self . _autoDetectWaitRecords = waitRecords # Update all the states in the classifier's cache for state in self . saved_states : self . _updateState ( state )
932	def run ( self , inputRecord ) : # 0-based prediction index for ModelResult predictionNumber = self . _numPredictions self . _numPredictions += 1 result = opf_utils . ModelResult ( predictionNumber = predictionNumber , rawInput = inputRecord ) return result
8484	def _update ( self , conf_dict , base_name = None ) : for name in conf_dict : # Skip private names if name . startswith ( '_' ) : continue value = conf_dict [ name ] # Skip Namespace if it's imported if value is Namespace : continue # Use a base namespace if base_name : name = base_name + '.' + name if isinstance ( value , Namespace ) : for name , value in value . iteritems ( name ) : self . set ( name , value ) # Automatically call any functions in the settings module, and if # they return a value other than None, that value becomes a setting elif callable ( value ) : value = value ( ) if value is not None : self . set ( name , value ) else : self . set ( name , value )
1978	def sched ( self ) : if len ( self . procs ) > 1 : logger . info ( "SCHED:" ) logger . info ( "\tProcess: %r" , self . procs ) logger . info ( "\tRunning: %r" , self . running ) logger . info ( "\tRWait: %r" , self . rwait ) logger . info ( "\tTWait: %r" , self . twait ) logger . info ( "\tTimers: %r" , self . timers ) logger . info ( "\tCurrent clock: %d" , self . clocks ) logger . info ( "\tCurrent cpu: %d" , self . _current ) if len ( self . running ) == 0 : logger . info ( "None running checking if there is some process waiting for a timeout" ) if all ( [ x is None for x in self . timers ] ) : raise Deadlock ( ) self . clocks = min ( [ x for x in self . timers if x is not None ] ) + 1 self . check_timers ( ) assert len ( self . running ) != 0 , "DEADLOCK!" self . _current = self . running [ 0 ] return next_index = ( self . running . index ( self . _current ) + 1 ) % len ( self . running ) next = self . running [ next_index ] if len ( self . procs ) > 1 : logger . info ( "\tTransfer control from process %d to %d" , self . _current , next ) self . _current = next
2277	def parse_generator_doubling ( config ) : start = 1 if 'start' in config : start = int ( config [ 'start' ] ) # We cannot simply use start as the variable, because of scoping # limitations def generator ( ) : val = start while ( True ) : yield val val = val * 2 return generator ( )
6907	def equatorial_to_galactic ( ra , decl , equinox = 'J2000' ) : # convert the ra/decl to gl, gb radecl = SkyCoord ( ra = ra * u . degree , dec = decl * u . degree , equinox = equinox ) gl = radecl . galactic . l . degree gb = radecl . galactic . b . degree return gl , gb
2145	def request ( self , method , url , * args , * * kwargs ) : # If the URL has the api/vX at the front strip it off # This is common to have if you are extracting a URL from an existing object. # For example, any of the 'related' fields of an object will have this import re url = re . sub ( "^/?api/v[0-9]+/" , "" , url ) # Piece together the full URL. use_version = not url . startswith ( '/o/' ) url = '%s%s' % ( self . get_prefix ( use_version ) , url . lstrip ( '/' ) ) # Ansible Tower expects authenticated requests; add the authentication # from settings if it's provided. kwargs . setdefault ( 'auth' , BasicTowerAuth ( settings . username , settings . password , self ) ) # POST and PUT requests will send JSON by default; make this # the content_type by default. This makes it such that we don't have # to constantly write that in our code, which gets repetitive. headers = kwargs . get ( 'headers' , { } ) if method . upper ( ) in ( 'PATCH' , 'POST' , 'PUT' ) : headers . setdefault ( 'Content-Type' , 'application/json' ) kwargs [ 'headers' ] = headers # If debugging is on, print the URL and data being sent. debug . log ( '%s %s' % ( method , url ) , fg = 'blue' , bold = True ) if method in ( 'POST' , 'PUT' , 'PATCH' ) : debug . log ( 'Data: %s' % kwargs . get ( 'data' , { } ) , fg = 'blue' , bold = True ) if method == 'GET' or kwargs . get ( 'params' , None ) : debug . log ( 'Params: %s' % kwargs . get ( 'params' , { } ) , fg = 'blue' , bold = True ) debug . log ( '' ) # If this is a JSON request, encode the data value. if headers . get ( 'Content-Type' , '' ) == 'application/json' : kwargs [ 'data' ] = json . dumps ( kwargs . get ( 'data' , { } ) ) r = self . _make_request ( method , url , args , kwargs ) # Sanity check: Did the server send back some kind of internal error? # If so, bubble this up. if r . status_code >= 500 : raise exc . ServerError ( 'The Tower server sent back a server error. ' 'Please try again later.' ) # Sanity check: Did we fail to authenticate properly? # If so, fail out now; this is always a failure. if r . status_code == 401 : raise exc . AuthError ( 'Invalid Tower authentication credentials (HTTP 401).' ) # Sanity check: Did we get a forbidden response, which means that # the user isn't allowed to do this? Report that. if r . status_code == 403 : raise exc . Forbidden ( "You don't have permission to do that (HTTP 403)." ) # Sanity check: Did we get a 404 response? # Requests with primary keys will return a 404 if there is no response, # and we want to consistently trap these. if r . status_code == 404 : raise exc . NotFound ( 'The requested object could not be found.' ) # Sanity check: Did we get a 405 response? # A 405 means we used a method that isn't allowed. Usually this # is a bad request, but it requires special treatment because the # API sends it as a logic error in a few situations (e.g. trying to # cancel a job that isn't running). if r . status_code == 405 : raise exc . MethodNotAllowed ( "The Tower server says you can't make a request with the " "%s method to that URL (%s)." % ( method , url ) , ) # Sanity check: Did we get some other kind of error? # If so, write an appropriate error message. if r . status_code >= 400 : raise exc . BadRequest ( 'The Tower server claims it was sent a bad request.\n\n' '%s %s\nParams: %s\nData: %s\n\nResponse: %s' % ( method , url , kwargs . get ( 'params' , None ) , kwargs . get ( 'data' , None ) , r . content . decode ( 'utf8' ) ) ) # Django REST Framework intelligently prints API keys in the # order that they are defined in the models and serializer. # # We want to preserve this behavior when it is possible to do so # with minimal effort, because while the order has no explicit meaning, # we make some effort to order keys in a convenient manner. # # To this end, make this response into an APIResponse subclass # (defined below), which has a `json` method that doesn't lose key # order. r . __class__ = APIResponse # Return the response object. return r
8597	def delete_group ( self , group_id ) : response = self . _perform_request ( url = '/um/groups/%s' % group_id , method = 'DELETE' ) return response
3310	def _run_ext_wsgiutils ( app , config , mode ) : from wsgidav . server import ext_wsgiutils_server _logger . info ( "Running WsgiDAV {} on wsgidav.ext_wsgiutils_server..." . format ( __version__ ) ) _logger . warning ( "WARNING: This single threaded server (ext-wsgiutils) is not meant for production." ) try : ext_wsgiutils_server . serve ( config , app ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
9794	def _ignore_path ( cls , path , ignore_list = None , white_list = None ) : ignore_list = ignore_list or [ ] white_list = white_list or [ ] return ( cls . _matches_patterns ( path , ignore_list ) and not cls . _matches_patterns ( path , white_list ) )
8186	def draw ( self , dx = 0 , dy = 0 , weighted = False , directed = False , highlight = [ ] , traffic = None ) : self . update ( ) # Draw the graph background. s = self . styles . default s . graph_background ( s ) # Center the graph on the canvas. _ctx . push ( ) _ctx . translate ( self . x + dx , self . y + dy ) # Indicate betweenness centrality. if traffic : if isinstance ( traffic , bool ) : traffic = 5 for n in self . nodes_by_betweenness ( ) [ : traffic ] : try : s = self . styles [ n . style ] except : s = self . styles . default if s . graph_traffic : s . graph_traffic ( s , n , self . alpha ) # Draw the edges and their labels. s = self . styles . default if s . edges : s . edges ( s , self . edges , self . alpha , weighted , directed ) # Draw each node in the graph. # Apply individual style to each node (or default). for n in self . nodes : try : s = self . styles [ n . style ] except : s = self . styles . default if s . node : s . node ( s , n , self . alpha ) # Highlight the given shortest path. try : s = self . styles . highlight except : s = self . styles . default if s . path : s . path ( s , self , highlight ) # Draw node id's as labels on each node. for n in self . nodes : try : s = self . styles [ n . style ] except : s = self . styles . default if s . node_label : s . node_label ( s , n , self . alpha ) # Events for clicked and dragged nodes. # Nodes will resist being dragged by attraction and repulsion, # put the event listener on top to get more direct feedback. #self.events.update() _ctx . pop ( )
5140	def new_noncomment ( self , start_lineno , end_lineno ) : block = NonComment ( start_lineno , end_lineno ) self . blocks . append ( block ) self . current_block = block
1968	def wait ( self , readfds , writefds , timeout ) : logger . debug ( "WAIT:" ) logger . debug ( f"\tProcess {self._current} is going to wait for [ {readfds!r} {writefds!r} {timeout!r} ]" ) logger . debug ( f"\tProcess: {self.procs!r}" ) logger . debug ( f"\tRunning: {self.running!r}" ) logger . debug ( f"\tRWait: {self.rwait!r}" ) logger . debug ( f"\tTWait: {self.twait!r}" ) logger . debug ( f"\tTimers: {self.timers!r}" ) for fd in readfds : self . rwait [ fd ] . add ( self . _current ) for fd in writefds : self . twait [ fd ] . add ( self . _current ) if timeout is not None : self . timers [ self . _current ] = self . clocks + timeout procid = self . _current # self.sched() next_index = ( self . running . index ( procid ) + 1 ) % len ( self . running ) self . _current = self . running [ next_index ] logger . debug ( f"\tTransfer control from process {procid} to {self._current}" ) logger . debug ( f"\tREMOVING {procid!r} from {self.running!r}. Current: {self._current!r}" ) self . running . remove ( procid ) if self . _current not in self . running : logger . debug ( "\tCurrent not running. Checking for timers..." ) self . _current = None self . check_timers ( )
11992	def set_signature_passphrases ( self , signature_passphrases ) : self . signature_passphrases = self . _update_dict ( signature_passphrases , { } , replace_data = True )
12075	def analyze ( fname = False , save = True , show = None ) : if fname and os . path . exists ( fname . replace ( ".abf" , ".rst" ) ) : print ( "SKIPPING DUE TO RST FILE" ) return swhlab . plotting . core . IMAGE_SAVE = save if show is None : if cm . isIpython ( ) : swhlab . plotting . core . IMAGE_SHOW = True else : swhlab . plotting . core . IMAGE_SHOW = False #swhlab.plotting.core.IMAGE_SHOW=show abf = ABF ( fname ) # ensure it's a class print ( ">>>>> PROTOCOL >>>>>" , abf . protocomment ) runFunction = "proto_unknown" if "proto_" + abf . protocomment in globals ( ) : runFunction = "proto_" + abf . protocomment abf . log . debug ( "running %s()" % ( runFunction ) ) plt . close ( 'all' ) # get ready globals ( ) [ runFunction ] ( abf ) # run that function try : globals ( ) [ runFunction ] ( abf ) # run that function except : abf . log . error ( "EXCEPTION DURING PROTOCOL FUNCTION" ) abf . log . error ( sys . exc_info ( ) [ 0 ] ) return "ERROR" plt . close ( 'all' ) # clean up return "SUCCESS"
2936	def deserialize_assign ( self , workflow , start_node ) : name = start_node . getAttribute ( 'name' ) attrib = start_node . getAttribute ( 'field' ) value = start_node . getAttribute ( 'value' ) kwargs = { } if name == '' : _exc ( 'name attribute required' ) if attrib != '' and value != '' : _exc ( 'Both, field and right-value attributes found' ) elif attrib == '' and value == '' : _exc ( 'field or value attribute required' ) elif value != '' : kwargs [ 'right' ] = value else : kwargs [ 'right_attribute' ] = attrib return operators . Assign ( name , * * kwargs )
3442	def to_json ( model , sort = False , * * kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ u"version" ] = JSON_SPEC return json . dumps ( obj , allow_nan = False , * * kwargs )
2996	def marketNewsDF ( count = 10 , token = '' , version = '' ) : df = pd . DataFrame ( marketNews ( count , token , version ) ) _toDatetime ( df ) _reindex ( df , 'datetime' ) return df
266	def format_asset ( asset ) : try : import zipline . assets except ImportError : return asset if isinstance ( asset , zipline . assets . Asset ) : return asset . symbol else : return asset
6138	def set_default_sim_param ( self , * args , * * kwargs ) : if len ( args ) is 1 and isinstance ( args [ 0 ] , SimulationParameter ) : self . __default_param = args [ 0 ] else : self . __default_param = SimulationParameter ( * args , * * kwargs ) return
5456	def _validate_label ( cls , name , value ) : # Rules for labels are described in: # https://cloud.google.com/compute/docs/labeling-resources#restrictions # * Keys and values cannot be longer than 63 characters each. # * Keys and values can only contain lowercase letters, numeric characters, # underscores, and dashes. # * International characters are allowed. # * Label keys must start with a lowercase letter and international # characters are allowed. # * Label keys cannot be empty. cls . _check_label_name ( name ) cls . _check_label_value ( value ) # Ensure that reserved labels are not being used. if not cls . _allow_reserved_keys and name in RESERVED_LABELS : raise ValueError ( 'Label flag (%s=...) must not use reserved keys: %r' % ( name , list ( RESERVED_LABELS ) ) )
7408	def worker ( self ) : ## subsample loci fullseqs = self . sample_loci ( ) ## find all iterations of samples for this quartet liters = itertools . product ( * self . imap . values ( ) ) ## run tree inference for each iteration of sampledict hashval = uuid . uuid4 ( ) . hex weights = [ ] for ridx , lidx in enumerate ( liters ) : ## get subalignment for this iteration and make to nex a , b , c , d = lidx sub = { } for i in lidx : if self . rmap [ i ] == "p1" : sub [ "A" ] = fullseqs [ i ] elif self . rmap [ i ] == "p2" : sub [ "B" ] = fullseqs [ i ] elif self . rmap [ i ] == "p3" : sub [ "C" ] = fullseqs [ i ] else : sub [ "D" ] = fullseqs [ i ] ## write as nexus file nex = [ ] for tax in list ( "ABCD" ) : nex . append ( ">{} {}" . format ( tax , sub [ tax ] ) ) ## check for too much missing or lack of variants nsites , nvar = count_var ( nex ) ## only run test if there's variation present if nvar > self . minsnps : ## format as nexus file nexus = "{} {}\n" . format ( 4 , len ( fullseqs [ a ] ) ) + "\n" . join ( nex ) ## infer ML tree treeorder = self . run_tree_inference ( nexus , "{}.{}" . format ( hashval , ridx ) ) ## add to list weights . append ( treeorder ) ## cleanup - remove all files with the hash val rfiles = glob . glob ( os . path . join ( tempfile . tempdir , "*{}*" . format ( hashval ) ) ) for rfile in rfiles : if os . path . exists ( rfile ) : os . remove ( rfile ) ## return result as weights for the set topologies. trees = [ "ABCD" , "ACBD" , "ADBC" ] wdict = { i : float ( weights . count ( i ) ) / len ( weights ) for i in trees } return wdict
8265	def _interpolate ( self , colors , n = 100 ) : gradient = [ ] for i in _range ( n ) : l = len ( colors ) - 1 x = int ( 1.0 * i / n * l ) x = min ( x + 0 , l ) y = min ( x + 1 , l ) base = 1.0 * n / l * x d = ( i - base ) / ( 1.0 * n / l ) r = colors [ x ] . r * ( 1 - d ) + colors [ y ] . r * d g = colors [ x ] . g * ( 1 - d ) + colors [ y ] . g * d b = colors [ x ] . b * ( 1 - d ) + colors [ y ] . b * d a = colors [ x ] . a * ( 1 - d ) + colors [ y ] . a * d gradient . append ( color ( r , g , b , a , mode = "rgb" ) ) gradient . append ( colors [ - 1 ] ) return gradient
7696	def delayed_call ( self , delay , function ) : main_loop = self handler = [ ] class DelayedCallHandler ( TimeoutHandler ) : """Wrapper timeout handler class for the delayed call.""" # pylint: disable=R0903 @ timeout_handler ( delay , False ) def callback ( self ) : """Wrapper timeout handler method for the delayed call.""" try : function ( ) finally : main_loop . remove_handler ( handler [ 0 ] ) handler . append ( DelayedCallHandler ( ) ) self . add_handler ( handler [ 0 ] )
13781	def FindExtensionByName ( self , full_name ) : full_name = _NormalizeFullyQualifiedName ( full_name ) message_name , _ , extension_name = full_name . rpartition ( '.' ) try : # Most extensions are nested inside a message. scope = self . FindMessageTypeByName ( message_name ) except KeyError : # Some extensions are defined at file scope. scope = self . FindFileContainingSymbol ( full_name ) return scope . extensions_by_name [ extension_name ]
9409	def _extract ( data , session = None ) : # Extract each item of a list. if isinstance ( data , list ) : return [ _extract ( d , session ) for d in data ] # Ignore leaf objects. if not isinstance ( data , np . ndarray ) : return data # Extract user defined classes. if isinstance ( data , MatlabObject ) : cls = session . _get_user_class ( data . classname ) return cls . from_value ( data ) # Extract struct data. if data . dtype . names : # Singular struct if data . size == 1 : return _create_struct ( data , session ) # Struct array return StructArray ( data , session ) # Extract cells. if data . dtype . kind == 'O' : return Cell ( data , session ) # Compress singleton values. if data . size == 1 : return data . item ( ) # Compress empty values. if data . size == 0 : if data . dtype . kind in 'US' : return '' return [ ] # Return standard array. return data
6919	def _autocorr_func1 ( mags , lag , maglen , magmed , magstd ) : lagindex = nparange ( 1 , maglen - lag ) products = ( mags [ lagindex ] - magmed ) * ( mags [ lagindex + lag ] - magmed ) acorr = ( 1.0 / ( ( maglen - lag ) * magstd ) ) * npsum ( products ) return acorr
9902	def with_data ( path , data ) : # De-jsonize data if necessary if isinstance ( data , str ) : data = json . loads ( data ) # Make sure this is really a new file if os . path . exists ( path ) : raise ValueError ( "File exists, not overwriting data. Set the " "'data' attribute on a normally-initialized " "'livejson.File' instance if you really " "want to do this." ) else : f = File ( path ) f . data = data return f
8714	def file_format ( self ) : log . info ( 'Formating, can take minutes depending on flash size...' ) res = self . __exchange ( 'file.format()' , timeout = 300 ) if 'format done' not in res : log . error ( res ) else : log . info ( res ) return res
10070	def preserve ( method = None , result = True , fields = None ) : if method is None : return partial ( preserve , result = result , fields = fields ) fields = fields or ( '_deposit' , ) @ wraps ( method ) def wrapper ( self , * args , * * kwargs ) : """Check current deposit status.""" data = { field : self [ field ] for field in fields if field in self } result_ = method ( self , * args , * * kwargs ) replace = result_ if result else self for field in data : replace [ field ] = data [ field ] return result_ return wrapper
7670	def add ( self , jam , on_conflict = 'fail' ) : if on_conflict not in [ 'overwrite' , 'fail' , 'ignore' ] : raise ParameterError ( "on_conflict='{}' is not in ['fail', " "'overwrite', 'ignore']." . format ( on_conflict ) ) if not self . file_metadata == jam . file_metadata : if on_conflict == 'overwrite' : self . file_metadata = jam . file_metadata elif on_conflict == 'fail' : raise JamsError ( "Metadata conflict! " "Resolve manually or force-overwrite it." ) self . annotations . extend ( jam . annotations ) self . sandbox . update ( * * jam . sandbox )
4474	def __serial_transform ( self , jam , steps ) : # This uses the round-robin itertools recipe if six . PY2 : attr = 'next' else : attr = '__next__' pending = len ( steps ) nexts = itertools . cycle ( getattr ( iter ( D . transform ( jam ) ) , attr ) for ( name , D ) in steps ) while pending : try : for next_jam in nexts : yield next_jam ( ) except StopIteration : pending -= 1 nexts = itertools . cycle ( itertools . islice ( nexts , pending ) )
3675	def charge ( self ) : try : if not self . rdkitmol : return charge_from_formula ( self . formula ) else : return Chem . GetFormalCharge ( self . rdkitmol ) except : return charge_from_formula ( self . formula )
1737	def parse_exponent ( source , start ) : if not source [ start ] in { 'e' , 'E' } : if source [ start ] in IDENTIFIER_PART : raise SyntaxError ( 'Invalid number literal!' ) return start start += 1 if source [ start ] in { '-' , '+' } : start += 1 FOUND = False # we need at least one dig after exponent while source [ start ] in NUMS : FOUND = True start += 1 if not FOUND or source [ start ] in IDENTIFIER_PART : raise SyntaxError ( 'Invalid number literal!' ) return start
3070	def request ( http , uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : # NOTE: Allowing http or http.request is temporary (See Issue 601). http_callable = getattr ( http , 'request' , http ) return http_callable ( uri , method = method , body = body , headers = headers , redirections = redirections , connection_type = connection_type )
10708	def delete_vacation ( _id ) : arequest = requests . delete ( VACATIONS_URL + "/" + _id , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "Failed to delete vacation. " + status_code ) return False return True
8033	def find_dupes ( paths , exact = False , ignores = None , min_size = 0 ) : groups = { '' : getPaths ( paths , ignores ) } groups = groupBy ( groups , sizeClassifier , 'sizes' , min_size = min_size ) # This serves one of two purposes depending on run-mode: # - Minimize number of files checked by full-content comparison (hash) # - Minimize chances of file handle exhaustion and limit seeking (exact) groups = groupBy ( groups , hashClassifier , 'header hashes' , limit = HEAD_SIZE ) if exact : groups = groupBy ( groups , groupByContent , fun_desc = 'contents' ) else : groups = groupBy ( groups , hashClassifier , fun_desc = 'hashes' ) return groups
7382	def has_edge_within_group ( self , group ) : assert group in self . nodes . keys ( ) , "{0} not one of the group of nodes" . format ( group ) nodelist = self . nodes [ group ] for n1 , n2 in self . simplified_edges ( ) : if n1 in nodelist and n2 in nodelist : return True
7738	def check_unassigned ( self , data ) : for char in data : for lookup in self . unassigned : if lookup ( char ) : raise StringprepError ( "Unassigned character: {0!r}" . format ( char ) ) return data
4423	async def handle_event ( self , event ) : if isinstance ( event , ( TrackStuckEvent , TrackExceptionEvent ) ) or isinstance ( event , TrackEndEvent ) and event . reason == 'FINISHED' : await self . play ( )
8147	def hue ( self , img1 , img2 ) : import colorsys p1 = list ( img1 . getdata ( ) ) p2 = list ( img2 . getdata ( ) ) for i in range ( len ( p1 ) ) : r1 , g1 , b1 , a1 = p1 [ i ] r1 = r1 / 255.0 g1 = g1 / 255.0 b1 = b1 / 255.0 h1 , s1 , v1 = colorsys . rgb_to_hsv ( r1 , g1 , b1 ) r2 , g2 , b2 , a2 = p2 [ i ] r2 = r2 / 255.0 g2 = g2 / 255.0 b2 = b2 / 255.0 h2 , s2 , v2 = colorsys . rgb_to_hsv ( r2 , g2 , b2 ) r3 , g3 , b3 = colorsys . hsv_to_rgb ( h2 , s1 , v1 ) r3 = int ( r3 * 255 ) g3 = int ( g3 * 255 ) b3 = int ( b3 * 255 ) p1 [ i ] = ( r3 , g3 , b3 , a1 ) img = Image . new ( "RGBA" , img1 . size , 255 ) img . putdata ( p1 ) return img
35	def setup_mpi_gpus ( ) : if 'CUDA_VISIBLE_DEVICES' not in os . environ : if sys . platform == 'darwin' : # This Assumes if you're on OSX you're just ids = [ ] # doing a smoke test and don't want GPUs else : lrank , _lsize = get_local_rank_size ( MPI . COMM_WORLD ) ids = [ lrank ] os . environ [ "CUDA_VISIBLE_DEVICES" ] = "," . join ( map ( str , ids ) )
9568	def get_chat_id ( self , message ) : if message . chat . type == 'private' : return message . user . id return message . chat . id
7671	def save ( self , path_or_file , strict = True , fmt = 'auto' ) : self . validate ( strict = strict ) with _open ( path_or_file , mode = 'w' , fmt = fmt ) as fdesc : json . dump ( self . __json__ , fdesc , indent = 2 )
11890	def set_brightness ( self , brightness ) : command = "C {},,,,{},\r\n" . format ( self . _zid , brightness ) response = self . _hub . send_command ( command ) _LOGGER . debug ( "Set brightness %s: %s" , repr ( command ) , response ) return response
12232	def register_prefs ( * args , * * kwargs ) : swap_settings_module = bool ( kwargs . get ( 'swap_settings_module' , True ) ) if __PATCHED_LOCALS_SENTINEL not in get_frame_locals ( 2 ) : raise SitePrefsException ( 'Please call `patch_locals()` right before the `register_prefs()`.' ) bind_proxy ( args , * * kwargs ) unpatch_locals ( ) swap_settings_module and proxy_settings_module ( )
6512	def _set ( self , name , gender , country_values ) : if '+' in name : for replacement in [ '' , ' ' , '-' ] : self . _set ( name . replace ( '+' , replacement ) , gender , country_values ) else : if name not in self . names : self . names [ name ] = { } self . names [ name ] [ gender ] = country_values
242	def create_perf_attrib_tear_sheet ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True , return_fig = False , factor_partitions = FACTOR_PARTITIONS ) : portfolio_exposures , perf_attrib_data = perf_attrib . perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars ) display ( Markdown ( "## Performance Relative to Common Risk Factors" ) ) # aggregate perf attrib stats and show summary table perf_attrib . show_perf_attrib_stats ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars ) # one section for the returns plot, and for each factor grouping # one section for factor returns, and one for risk exposures vertical_sections = 1 + 2 * max ( len ( factor_partitions ) , 1 ) current_section = 0 fig = plt . figure ( figsize = [ 14 , vertical_sections * 6 ] ) gs = gridspec . GridSpec ( vertical_sections , 1 , wspace = 0.5 , hspace = 0.5 ) perf_attrib . plot_returns ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 if factor_partitions is not None : for factor_type , partitions in factor_partitions . iteritems ( ) : columns_to_select = perf_attrib_data . columns . intersection ( partitions ) perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data [ columns_to_select ] , ax = plt . subplot ( gs [ current_section ] ) , title = ( 'Cumulative common {} returns attribution' ) . format ( factor_type ) ) current_section += 1 for factor_type , partitions in factor_partitions . iteritems ( ) : perf_attrib . plot_risk_exposures ( portfolio_exposures [ portfolio_exposures . columns . intersection ( partitions ) ] , ax = plt . subplot ( gs [ current_section ] ) , title = 'Daily {} factor exposures' . format ( factor_type ) ) current_section += 1 else : perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 perf_attrib . plot_risk_exposures ( portfolio_exposures , ax = plt . subplot ( gs [ current_section ] ) ) gs . tight_layout ( fig ) if return_fig : return fig
594	def _cacheSequenceInfoType ( self ) : hasReset = self . resetFieldName is not None hasSequenceId = self . sequenceIdFieldName is not None if hasReset and not hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_RESET_ONLY self . _prevSequenceId = 0 elif not hasReset and hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_SEQUENCEID_ONLY self . _prevSequenceId = None elif hasReset and hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_BOTH else : self . _sequenceInfoType = self . SEQUENCEINFO_NONE
8662	def migrate ( src_path , src_passphrase , src_backend , dst_path , dst_passphrase , dst_backend ) : src_storage = STORAGE_MAPPING [ src_backend ] ( * * _parse_path_string ( src_path ) ) dst_storage = STORAGE_MAPPING [ dst_backend ] ( * * _parse_path_string ( dst_path ) ) src_stash = Stash ( src_storage , src_passphrase ) dst_stash = Stash ( dst_storage , dst_passphrase ) # TODO: Test that re-encryption does not occur on similar # passphrases keys = src_stash . export ( ) dst_stash . load ( src_passphrase , keys = keys )
445	def prefetch_input_data ( reader , file_pattern , is_training , batch_size , values_per_shard , input_queue_capacity_factor = 16 , num_reader_threads = 1 , shard_queue_name = "filename_queue" , value_queue_name = "input_queue" ) : data_files = [ ] for pattern in file_pattern . split ( "," ) : data_files . extend ( tf . gfile . Glob ( pattern ) ) if not data_files : tl . logging . fatal ( "Found no input files matching %s" , file_pattern ) else : tl . logging . info ( "Prefetching values from %d files matching %s" , len ( data_files ) , file_pattern ) if is_training : print ( " is_training == True : RandomShuffleQueue" ) filename_queue = tf . train . string_input_producer ( data_files , shuffle = True , capacity = 16 , name = shard_queue_name ) min_queue_examples = values_per_shard * input_queue_capacity_factor capacity = min_queue_examples + 100 * batch_size values_queue = tf . RandomShuffleQueue ( capacity = capacity , min_after_dequeue = min_queue_examples , dtypes = [ tf . string ] , name = "random_" + value_queue_name ) else : print ( " is_training == False : FIFOQueue" ) filename_queue = tf . train . string_input_producer ( data_files , shuffle = False , capacity = 1 , name = shard_queue_name ) capacity = values_per_shard + 3 * batch_size values_queue = tf . FIFOQueue ( capacity = capacity , dtypes = [ tf . string ] , name = "fifo_" + value_queue_name ) enqueue_ops = [ ] for _ in range ( num_reader_threads ) : _ , value = reader . read ( filename_queue ) enqueue_ops . append ( values_queue . enqueue ( [ value ] ) ) tf . train . queue_runner . add_queue_runner ( tf . train . queue_runner . QueueRunner ( values_queue , enqueue_ops ) ) tf . summary . scalar ( "queue/%s/fraction_of_%d_full" % ( values_queue . name , capacity ) , tf . cast ( values_queue . size ( ) , tf . float32 ) * ( 1. / capacity ) ) return values_queue
12269	def evaluate ( self , repo , spec , args ) : status = [ ] with cd ( repo . rootdir ) : files = spec . get ( 'files' , [ '*' ] ) resource_files = repo . find_matching_files ( files ) files = glob2 . glob ( "**/*" ) disk_files = [ f for f in files if os . path . isfile ( f ) and f != "datapackage.json" ] allfiles = list ( set ( resource_files + disk_files ) ) allfiles . sort ( ) for f in allfiles : if f in resource_files and f in disk_files : r = repo . get_resource ( f ) coded_sha256 = r [ 'sha256' ] computed_sha256 = compute_sha256 ( f ) if computed_sha256 != coded_sha256 : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "Mismatch in checksum on disk and in datapackage.json" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'OK' , 'message' : "" } ) elif f in resource_files : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In datapackage.json but not in repo" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In repo but not in datapackage.json" } ) return status
4094	def AIC ( N , rho , k ) : from numpy import log , array #k+1 #todo check convention. agrees with octave res = N * log ( array ( rho ) ) + 2. * ( array ( k ) + 1 ) return res
11567	def stepper_request_library_version ( self ) : data = [ self . STEPPER_LIBRARY_VERSION ] self . _command_handler . send_sysex ( self . _command_handler . STEPPER_DATA , data )
2139	def associate ( self , group , parent , * * kwargs ) : parent_id = self . lookup_with_inventory ( parent , kwargs . get ( 'inventory' , None ) ) [ 'id' ] group_id = self . lookup_with_inventory ( group , kwargs . get ( 'inventory' , None ) ) [ 'id' ] return self . _assoc ( 'children' , parent_id , group_id )
4655	def verify_authority ( self ) : try : if not self . blockchain . rpc . verify_authority ( self . json ( ) ) : raise InsufficientAuthorityError except Exception as e : raise e
428	def load_and_preprocess_imdb_data ( n_gram = None ) : X_train , y_train , X_test , y_test = tl . files . load_imdb_dataset ( nb_words = VOCAB_SIZE ) if n_gram is not None : X_train = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_train ] ) X_test = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_test ] ) return X_train , y_train , X_test , y_test
8013	async def _create_upstream_applications ( self ) : loop = asyncio . get_event_loop ( ) for steam_name , ApplicationsCls in self . applications . items ( ) : application = ApplicationsCls ( self . scope ) upstream_queue = asyncio . Queue ( ) self . application_streams [ steam_name ] = upstream_queue self . application_futures [ steam_name ] = loop . create_task ( application ( upstream_queue . get , partial ( self . dispatch_downstream , steam_name = steam_name ) ) )
10785	def add_missing_particles ( st , rad = 'calc' , tries = 50 , * * kwargs ) : if rad == 'calc' : rad = guess_add_radii ( st ) guess , npart = feature_guess ( st , rad , * * kwargs ) tries = np . min ( [ tries , npart ] ) accepts , new_poses = check_add_particles ( st , guess [ : tries ] , rad = rad , * * kwargs ) return accepts , new_poses
13037	def overview ( ) : doc = Host ( ) search = doc . search ( ) search . aggs . bucket ( 'tag_count' , 'terms' , field = 'tags' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) print_line ( "{0:<25} {1}" . format ( 'Tag' , 'Count' ) ) print_line ( "-" * 30 ) for entry in response . aggregations . tag_count . buckets : print_line ( "{0:<25} {1}" . format ( entry . key , entry . doc_count ) )
6144	def DSP_callback_tic ( self ) : if self . Tcapture > 0 : self . DSP_tic . append ( time . time ( ) - self . start_time )
6256	def get_finder ( import_path ) : Finder = import_string ( import_path ) if not issubclass ( Finder , BaseFileSystemFinder ) : raise ImproperlyConfigured ( 'Finder {} is not a subclass of core.finders.FileSystemFinder' . format ( import_path ) ) return Finder ( )
4885	def allow_request ( self , request , view ) : service_users = get_service_usernames ( ) # User service user throttling rates for service user. if request . user . username in service_users : self . update_throttle_scope ( ) return super ( ServiceUserThrottle , self ) . allow_request ( request , view )
8003	def get_form ( self , form_type = "form" ) : if self . form : if self . form . type != form_type : raise ValueError ( "Bad form type in the jabber:iq:register element" ) return self . form form = Form ( form_type , instructions = self . instructions ) form . add_field ( "FORM_TYPE" , [ u"jabber:iq:register" ] , "hidden" ) for field in legacy_fields : field_type , field_label = legacy_fields [ field ] value = getattr ( self , field ) if value is None : continue if form_type == "form" : if not value : value = None form . add_field ( name = field , field_type = field_type , label = field_label , value = value , required = True ) else : form . add_field ( name = field , value = value ) return form
1993	def load_state ( self , state_id , delete = True ) : return self . _store . load_state ( f'{self._prefix}{state_id:08x}{self._suffix}' , delete = delete )
7949	def send_stream_tail ( self ) : with self . lock : if not self . _socket or self . _hup : logger . debug ( u"Cannot send stream closing tag: already closed" ) return data = self . _serializer . emit_tail ( ) try : self . _write ( data . encode ( "utf-8" ) ) except ( IOError , SystemError , socket . error ) , err : logger . debug ( u"Sending stream closing tag failed: {0}" . format ( err ) ) self . _serializer = None self . _hup = True if self . _tls_state is None : try : self . _socket . shutdown ( socket . SHUT_WR ) except socket . error : pass self . _set_state ( "closing" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
6652	def findProgram ( self , builddir , program ) : # if this is an exact match, do no further checking: if os . path . isfile ( os . path . join ( builddir , program ) ) : logging . info ( 'found %s' % program ) return program exact_matches = [ ] insensitive_matches = [ ] approx_matches = [ ] for path , dirs , files in os . walk ( builddir ) : if program in files : exact_matches . append ( os . path . relpath ( os . path . join ( path , program ) , builddir ) ) continue files_lower = [ f . lower ( ) for f in files ] if program . lower ( ) in files_lower : insensitive_matches . append ( os . path . relpath ( os . path . join ( path , files [ files_lower . index ( program . lower ( ) ) ] ) , builddir ) ) continue # !!! TODO: in the future add approximate string matching (typos, # etc.), for now we just test stripping any paths off program, and # looking for substring matches: pg_basen_lower_noext = os . path . splitext ( os . path . basename ( program ) . lower ( ) ) [ 0 ] for f in files_lower : if pg_basen_lower_noext in f : approx_matches . append ( os . path . relpath ( os . path . join ( path , files [ files_lower . index ( f ) ] ) , builddir ) ) if len ( exact_matches ) == 1 : logging . info ( 'found %s at %s' , program , exact_matches [ 0 ] ) return exact_matches [ 0 ] elif len ( exact_matches ) > 1 : logging . error ( '%s matches multiple executables, please use a full path (one of %s)' % ( program , ', or ' . join ( [ '"' + os . path . join ( m , program ) + '"' for m in exact_matches ] ) ) ) return None # if we have matches with and without a file extension, prefer the # no-file extension version, and discard the others (so we avoid # picking up post-processed files): reduced_approx_matches = [ ] for m in approx_matches : root = os . path . splitext ( m ) [ 0 ] if ( m == root ) or ( root not in approx_matches ) : reduced_approx_matches . append ( m ) approx_matches = reduced_approx_matches for matches in ( insensitive_matches , approx_matches ) : if len ( matches ) == 1 : logging . info ( 'found %s at %s' % ( program , matches [ 0 ] ) ) return matches [ 0 ] elif len ( matches ) > 1 : logging . error ( '%s is similar to several executables found. Please use an exact name:\n%s' % ( program , '\n' . join ( matches ) ) ) return None logging . error ( 'could not find program "%s" to debug' % program ) return None
1325	def _saliency_map ( self , a , image , target , labels , mask , fast = False ) : # pixel influence on target class alphas = a . gradient ( image , target ) * mask # pixel influence on sum of residual classes # (don't evaluate if fast == True) if fast : betas = - np . ones_like ( alphas ) else : betas = np . sum ( [ a . gradient ( image , label ) * mask - alphas for label in labels ] , 0 ) # compute saliency map # (take into account both pos. & neg. perturbations) salmap = np . abs ( alphas ) * np . abs ( betas ) * np . sign ( alphas * betas ) # find optimal pixel & direction of perturbation idx = np . argmin ( salmap ) idx = np . unravel_index ( idx , mask . shape ) pix_sign = np . sign ( alphas ) [ idx ] return idx , pix_sign
6898	def serial_periodicfeatures ( pfpkl_list , lcbasedir , outdir , starfeaturesdir = None , fourierorder = 5 , # these are depth, duration, ingress duration transitparams = ( - 0.01 , 0.1 , 0.1 ) , # these are depth, duration, depth ratio, secphase ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , starfeatures = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None # make sure to make the output directory if it doesn't exist if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : pfpkl_list = pfpkl_list [ : maxobjects ] LOGINFO ( '%s periodfinding pickles to process' % len ( pfpkl_list ) ) # if the starfeaturedir is provided, try to find a starfeatures pickle for # each periodfinding pickle in pfpkl_list if starfeaturesdir and os . path . exists ( starfeaturesdir ) : starfeatures_list = [ ] LOGINFO ( 'collecting starfeatures pickles...' ) for pfpkl in pfpkl_list : sfpkl1 = os . path . basename ( pfpkl ) . replace ( 'periodfinding' , 'starfeatures' ) sfpkl2 = sfpkl1 . replace ( '.gz' , '' ) sfpath1 = os . path . join ( starfeaturesdir , sfpkl1 ) sfpath2 = os . path . join ( starfeaturesdir , sfpkl2 ) if os . path . exists ( sfpath1 ) : starfeatures_list . append ( sfpkl1 ) elif os . path . exists ( sfpath2 ) : starfeatures_list . append ( sfpkl2 ) else : starfeatures_list . append ( None ) else : starfeatures_list = [ None for x in pfpkl_list ] # generate the task list kwargs = { 'fourierorder' : fourierorder , 'transitparams' : transitparams , 'ebparams' : ebparams , 'pdiff_threshold' : pdiff_threshold , 'sidereal_threshold' : sidereal_threshold , 'sampling_peak_multiplier' : sampling_peak_multiplier , 'sampling_startp' : sampling_startp , 'sampling_endp' : sampling_endp , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'sigclip' : sigclip , 'verbose' : verbose } tasks = [ ( x , lcbasedir , outdir , y , kwargs ) for ( x , y ) in zip ( pfpkl_list , starfeatures_list ) ] LOGINFO ( 'processing periodfinding pickles...' ) for task in tqdm ( tasks ) : _periodicfeatures_worker ( task )
782	def jobReactivateRunningJobs ( self ) : # Get a database connection and cursor with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_cjm_conn_id=%%s, ' ' _eng_allocate_new_workers=TRUE ' ' WHERE status=%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . _connectionID , self . STATUS_RUNNING ] ) return
11273	def get_dict ( self ) : return dict ( current_page = self . current_page , total_page_count = self . total_page_count , items = self . items , total_item_count = self . total_item_count , page_size = self . page_size )
12439	def serialize ( self , data , response = None , request = None , format = None ) : if isinstance ( self , Resource ) : if not request : # Ensure we have a response object. request = self . _request Serializer = None if format : # An explicit format was given; do not attempt to auto-detect # a serializer. Serializer = self . meta . serializers [ format ] if not Serializer : # Determine an appropriate serializer to use by # introspecting the request object and looking at the `Accept` # header. media_ranges = ( request . get ( 'Accept' ) or '*/*' ) . strip ( ) if not media_ranges : # Default the media ranges to */* media_ranges = '*/*' if media_ranges != '*/*' : # Parse the media ranges and determine the serializer # that is the closest match. media_types = six . iterkeys ( self . _serializer_map ) media_type = mimeparse . best_match ( media_types , media_ranges ) if media_type : format = self . _serializer_map [ media_type ] Serializer = self . meta . serializers [ format ] else : # Client indicated no preference; use the default. default = self . meta . default_serializer Serializer = self . meta . serializers [ default ] if Serializer : try : # Attempt to serialize the data using the determined # serializer. serializer = Serializer ( request , response ) return serializer . serialize ( data ) , serializer except ValueError : # Failed to serialize the data. pass # Either failed to determine a serializer or failed to serialize # the data; construct a list of available and valid encoders. available = { } for name in self . meta . allowed_serializers : Serializer = self . meta . serializers [ name ] instance = Serializer ( request , None ) if instance . can_serialize ( data ) : available [ name ] = Serializer . media_types [ 0 ] # Raise a Not Acceptable exception. raise http . exceptions . NotAcceptable ( available )
12724	def erps ( self , erps ) : _set_params ( self . ode_obj , 'ERP' , erps , self . ADOF + self . LDOF )
11018	def balance ( ctx ) : backend = plugins_registry . get_backends_by_class ( ZebraBackend ) [ 0 ] timesheet_collection = get_timesheet_collection_for_context ( ctx , None ) hours_to_be_pushed = timesheet_collection . get_hours ( pushed = False , ignored = False , unmapped = False ) today = datetime . date . today ( ) user_info = backend . get_user_info ( ) timesheets = backend . get_timesheets ( get_first_dow ( today ) , get_last_dow ( today ) ) total_duration = sum ( [ float ( timesheet [ 'time' ] ) for timesheet in timesheets ] ) vacation = hours_to_days ( user_info [ 'vacation' ] [ 'difference' ] ) vacation_balance = '{} days, {:.2f} hours' . format ( * vacation ) hours_balance = user_info [ 'hours' ] [ 'hours' ] [ 'balance' ] click . echo ( "Hours balance: {}" . format ( signed_number ( hours_balance ) ) ) click . echo ( "Hours balance after push: {}" . format ( signed_number ( hours_balance + hours_to_be_pushed ) ) ) click . echo ( "Hours done this week: {:.2f}" . format ( total_duration ) ) click . echo ( "Vacation left: {}" . format ( vacation_balance ) )
12779	def get_stream ( self , error_callback = None , live = True ) : self . join ( ) return Stream ( self , error_callback = error_callback , live = live )
8318	def connect_table ( self , table , chunk , markup ) : k = markup . find ( chunk ) i = markup . rfind ( "\n=" , 0 , k ) j = markup . find ( "\n" , i + 1 ) paragraph_title = markup [ i : j ] . strip ( ) . strip ( "= " ) for paragraph in self . paragraphs : if paragraph . title == paragraph_title : paragraph . tables . append ( table ) table . paragraph = paragraph
2892	def connect_outgoing ( self , taskspec , sequence_flow_id , sequence_flow_name , documentation ) : self . connect ( taskspec ) s = SequenceFlow ( sequence_flow_id , sequence_flow_name , documentation , taskspec ) self . outgoing_sequence_flows [ taskspec . name ] = s self . outgoing_sequence_flows_by_id [ sequence_flow_id ] = s
8983	def get_instruction_id ( self , instruction_or_id ) : if isinstance ( instruction_or_id , tuple ) : return _InstructionId ( instruction_or_id ) return _InstructionId ( instruction_or_id . type , instruction_or_id . hex_color )
2408	def extract_features_and_generate_model ( essays , algorithm = util_functions . AlgorithmTypes . regression ) : f = feature_extractor . FeatureExtractor ( ) f . initialize_dictionaries ( essays ) train_feats = f . gen_feats ( essays ) set_score = numpy . asarray ( essays . _score , dtype = numpy . int ) if len ( util_functions . f7 ( list ( set_score ) ) ) > 5 : algorithm = util_functions . AlgorithmTypes . regression else : algorithm = util_functions . AlgorithmTypes . classification clf , clf2 = get_algorithms ( algorithm ) cv_error_results = get_cv_error ( clf2 , train_feats , essays . _score ) try : clf . fit ( train_feats , set_score ) except ValueError : log . exception ( "Not enough classes (0,1,etc) in sample." ) set_score [ 0 ] = 1 set_score [ 1 ] = 0 clf . fit ( train_feats , set_score ) return f , clf , cv_error_results
9709	def heappop_max ( heap ) : lastelt = heap . pop ( ) # raises appropriate IndexError if heap is empty if heap : returnitem = heap [ 0 ] heap [ 0 ] = lastelt _siftup_max ( heap , 0 ) return returnitem return lastelt
2116	def convert ( self , value , param , ctx ) : # Protect against corner cases of invalid inputs if not isinstance ( value , str ) : return value if isinstance ( value , six . binary_type ) : value = value . decode ( 'UTF-8' ) # Read from a file under these cases if value . startswith ( '@' ) : filename = os . path . expanduser ( value [ 1 : ] ) file_obj = super ( Variables , self ) . convert ( filename , param , ctx ) if hasattr ( file_obj , 'read' ) : # Sometimes click.File may return a buffer and not a string return file_obj . read ( ) return file_obj # No file, use given string return value
5124	def show_type ( self , edge_type , * * kwargs ) : for v in self . g . nodes ( ) : e = ( v , v ) if self . g . is_edge ( e ) and self . g . ep ( e , 'edge_type' ) == edge_type : ei = self . g . edge_index [ e ] self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_highlight' ] ) self . g . set_vp ( v , 'vertex_color' , self . edge2queue [ ei ] . colors [ 'vertex_color' ] ) else : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_inactive' ] ) self . g . set_vp ( v , 'vertex_color' , [ 0 , 0 , 0 , 0.9 ] ) for e in self . g . edges ( ) : if self . g . ep ( e , 'edge_type' ) == edge_type : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_active' ] ) else : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_inactive' ] ) self . draw ( update_colors = False , * * kwargs ) self . _update_all_colors ( )
3567	def start_scan ( self , timeout_sec = TIMEOUT_SEC ) : self . _scan_started . clear ( ) self . _adapter . StartDiscovery ( ) if not self . _scan_started . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to start scanning!' )
6604	def result_relpath ( self , package_index ) : dirname = 'task_{:05d}' . format ( package_index ) # e.g., 'task_00009' ret = os . path . join ( 'results' , dirname , 'result.p.gz' ) # e.g., 'results/task_00009/result.p.gz' return ret
4338	def phaser ( self , gain_in = 0.8 , gain_out = 0.74 , delay = 3 , decay = 0.4 , speed = 0.5 , modulation_shape = 'sinusoidal' ) : if not is_number ( gain_in ) or gain_in <= 0 or gain_in > 1 : raise ValueError ( "gain_in must be a number between 0 and 1." ) if not is_number ( gain_out ) or gain_out <= 0 or gain_out > 1 : raise ValueError ( "gain_out must be a number between 0 and 1." ) if not is_number ( delay ) or delay <= 0 or delay > 5 : raise ValueError ( "delay must be a positive number." ) if not is_number ( decay ) or decay < 0.1 or decay > 0.5 : raise ValueError ( "decay must be a number between 0.1 and 0.5." ) if not is_number ( speed ) or speed < 0.1 or speed > 2 : raise ValueError ( "speed must be a positive number." ) if modulation_shape not in [ 'sinusoidal' , 'triangular' ] : raise ValueError ( "modulation_shape must be one of 'sinusoidal', 'triangular'." ) effect_args = [ 'phaser' , '{:f}' . format ( gain_in ) , '{:f}' . format ( gain_out ) , '{:f}' . format ( delay ) , '{:f}' . format ( decay ) , '{:f}' . format ( speed ) ] if modulation_shape == 'sinusoidal' : effect_args . append ( '-s' ) elif modulation_shape == 'triangular' : effect_args . append ( '-t' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'phaser' ) return self
8699	def __write ( self , output , binary = False ) : if not binary : log . debug ( 'write: %s' , output ) else : log . debug ( 'write binary: %s' , hexify ( output ) ) self . _port . write ( output ) self . _port . flush ( )
12501	def _smooth_data_array ( arr , affine , fwhm , copy = True ) : if arr . dtype . kind == 'i' : if arr . dtype == np . int64 : arr = arr . astype ( np . float64 ) else : arr = arr . astype ( np . float32 ) if copy : arr = arr . copy ( ) # Zeroe possible NaNs and Inf in the image. arr [ np . logical_not ( np . isfinite ( arr ) ) ] = 0 try : # Keep the 3D part of the affine. affine = affine [ : 3 , : 3 ] # Convert from FWHM in mm to a sigma. fwhm_sigma_ratio = np . sqrt ( 8 * np . log ( 2 ) ) vox_size = np . sqrt ( np . sum ( affine ** 2 , axis = 0 ) ) sigma = fwhm / ( fwhm_sigma_ratio * vox_size ) for n , s in enumerate ( sigma ) : ndimage . gaussian_filter1d ( arr , s , output = arr , axis = n ) except : raise ValueError ( 'Error smoothing the array.' ) else : return arr
1437	def update_reduced_metric ( self , name , value , key = None ) : if name not in self . metrics : Log . error ( "In update_reduced_metric(): %s is not registered in the metric" , name ) if key is None and isinstance ( self . metrics [ name ] , ReducedMetric ) : self . metrics [ name ] . update ( value ) elif key is not None and isinstance ( self . metrics [ name ] , MultiReducedMetric ) : self . metrics [ name ] . update ( key , value ) else : Log . error ( "In update_count(): %s is registered but not supported with this method" , name )
2059	def disassemble_instruction ( self , code , pc ) : return next ( self . disasm . disasm ( code , pc ) )
6463	def usage_function ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available functions:' ) for function in sorted ( FUNCTION ) : doc = FUNCTION [ function ] . __doc__ . strip ( ) . splitlines ( ) [ 0 ] print ( ' %-12s %s' % ( function + ':' , doc ) ) return 0
8388	def merge_configs ( main , tweaks ) : for section in tweaks . sections ( ) : for option in tweaks . options ( section ) : value = tweaks . get ( section , option ) if option . endswith ( "+" ) : option = option [ : - 1 ] value = main . get ( section , option ) + value main . set ( section , option , value )
4949	def _transform_item ( self , content_metadata_item ) : content_metadata_type = content_metadata_item [ 'content_type' ] transformed_item = { } for integrated_channel_schema_key , edx_data_schema_key in self . DATA_TRANSFORM_MAPPING . items ( ) : # Look for transformer functions defined on subclasses. # Favor content type-specific functions. transformer = ( getattr ( self , 'transform_{content_type}_{edx_data_schema_key}' . format ( content_type = content_metadata_type , edx_data_schema_key = edx_data_schema_key ) , None ) or getattr ( self , 'transform_{edx_data_schema_key}' . format ( edx_data_schema_key = edx_data_schema_key ) , None ) ) if transformer : transformed_item [ integrated_channel_schema_key ] = transformer ( content_metadata_item ) else : # The concrete subclass does not define an override for the given field, # so just use the data key to index the content metadata item dictionary. try : transformed_item [ integrated_channel_schema_key ] = content_metadata_item [ edx_data_schema_key ] except KeyError : # There may be a problem with the DATA_TRANSFORM_MAPPING on # the concrete subclass or the concrete subclass does not implement # the appropriate field tranformer function. LOGGER . exception ( 'Failed to transform content metadata item field [%s] for [%s]: [%s]' , edx_data_schema_key , self . enterprise_customer . name , content_metadata_item , ) return transformed_item
307	def show_profit_attribution ( round_trips ) : total_pnl = round_trips [ 'pnl' ] . sum ( ) pnl_attribution = round_trips . groupby ( 'symbol' ) [ 'pnl' ] . sum ( ) / total_pnl pnl_attribution . name = '' pnl_attribution . index = pnl_attribution . index . map ( utils . format_asset ) utils . print_table ( pnl_attribution . sort_values ( inplace = False , ascending = False , ) , name = 'Profitability (PnL / PnL total) per name' , float_format = '{:.2%}' . format , )
6774	def deploy ( self , site = None ) : r = self . local_renderer self . deploy_logrotate ( ) cron_crontabs = [ ] # if self.verbose: # print('hostname: "%s"' % (hostname,), file=sys.stderr) for _site , site_data in self . iter_sites ( site = site ) : r . env . cron_stdout_log = r . format ( r . env . stdout_log_template ) r . env . cron_stderr_log = r . format ( r . env . stderr_log_template ) r . sudo ( 'touch {cron_stdout_log}' ) r . sudo ( 'touch {cron_stderr_log}' ) r . sudo ( 'sudo chown {user}:{user} {cron_stdout_log}' ) r . sudo ( 'sudo chown {user}:{user} {cron_stderr_log}' ) if self . verbose : print ( 'site:' , site , file = sys . stderr ) print ( 'env.crontabs_selected:' , self . env . crontabs_selected , file = sys . stderr ) for selected_crontab in self . env . crontabs_selected : lines = self . env . crontabs_available . get ( selected_crontab , [ ] ) if self . verbose : print ( 'lines:' , lines , file = sys . stderr ) for line in lines : cron_crontabs . append ( r . format ( line ) ) if not cron_crontabs : return cron_crontabs = self . env . crontab_headers + cron_crontabs cron_crontabs . append ( '\n' ) r . env . crontabs_rendered = '\n' . join ( cron_crontabs ) fn = self . write_to_file ( content = r . env . crontabs_rendered ) print ( 'fn:' , fn ) r . env . put_remote_path = r . put ( local_path = fn ) if isinstance ( r . env . put_remote_path , ( tuple , list ) ) : r . env . put_remote_path = r . env . put_remote_path [ 0 ] r . sudo ( 'crontab -u {cron_user} {put_remote_path}' )
4975	def get_global_context ( request , enterprise_customer ) : platform_name = get_configuration_value ( "PLATFORM_NAME" , settings . PLATFORM_NAME ) # pylint: disable=no-member return { 'enterprise_customer' : enterprise_customer , 'LMS_SEGMENT_KEY' : settings . LMS_SEGMENT_KEY , 'LANGUAGE_CODE' : get_language_from_request ( request ) , 'tagline' : get_configuration_value ( "ENTERPRISE_TAGLINE" , settings . ENTERPRISE_TAGLINE ) , 'platform_description' : get_configuration_value ( "PLATFORM_DESCRIPTION" , settings . PLATFORM_DESCRIPTION , ) , 'LMS_ROOT_URL' : settings . LMS_ROOT_URL , 'platform_name' : platform_name , 'header_logo_alt_text' : _ ( '{platform_name} home page' ) . format ( platform_name = platform_name ) , 'welcome_text' : constants . WELCOME_TEXT . format ( platform_name = platform_name ) , 'enterprise_welcome_text' : constants . ENTERPRISE_WELCOME_TEXT . format ( enterprise_customer_name = enterprise_customer . name , platform_name = platform_name , strong_start = '<strong>' , strong_end = '</strong>' , line_break = '<br/>' , privacy_policy_link_start = "<a href='{pp_url}' target='_blank'>" . format ( pp_url = get_configuration_value ( 'PRIVACY' , 'https://www.edx.org/edx-privacy-policy' , type = 'url' ) , ) , privacy_policy_link_end = "</a>" , ) , }
9973	def _get_namedrange ( book , rangename , sheetname = None ) : def cond ( namedef ) : if namedef . type . upper ( ) == "RANGE" : if namedef . name . upper ( ) == rangename . upper ( ) : if sheetname is None : if not namedef . localSheetId : return True else : # sheet local name sheet_id = [ sht . upper ( ) for sht in book . sheetnames ] . index ( sheetname . upper ( ) ) if namedef . localSheetId == sheet_id : return True return False def get_destinations ( name_def ) : """Workaround for the bug in DefinedName.destinations""" from openpyxl . formula import Tokenizer from openpyxl . utils . cell import SHEETRANGE_RE if name_def . type == "RANGE" : tok = Tokenizer ( "=" + name_def . value ) for part in tok . items : if part . subtype == "RANGE" : m = SHEETRANGE_RE . match ( part . value ) if m . group ( "quoted" ) : sheet_name = m . group ( "quoted" ) else : sheet_name = m . group ( "notquoted" ) yield sheet_name , m . group ( "cells" ) namedef = next ( ( item for item in book . defined_names . definedName if cond ( item ) ) , None ) if namedef is None : return None dests = get_destinations ( namedef ) xlranges = [ ] sheetnames_upper = [ name . upper ( ) for name in book . sheetnames ] for sht , addr in dests : if sheetname : sht = sheetname index = sheetnames_upper . index ( sht . upper ( ) ) xlranges . append ( book . worksheets [ index ] [ addr ] ) if len ( xlranges ) == 1 : return xlranges [ 0 ] else : return xlranges
13839	def ConsumeFloat ( self ) : try : result = ParseFloat ( self . token ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
9784	def stop ( ctx , yes ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) if not yes and not click . confirm ( "Are sure you want to stop " "job `{}`" . format ( _build ) ) : click . echo ( 'Existing without stopping build job.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . build_job . stop ( user , project_name , _build ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Build job is being stopped." )
9735	def get_3d_markers ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPosition , component_info , data , component_position )
8533	def is_isomorphic_to ( self , other ) : return ( isinstance ( other , self . __class__ ) and len ( self . fields ) == len ( other . fields ) and all ( a . is_isomorphic_to ( b ) for a , b in zip ( self . fields , other . fields ) ) )
12264	def _send_file_internal ( self , * args , * * kwargs ) : super ( Key , self ) . _send_file_internal ( * args , * * kwargs ) mimicdb . backend . sadd ( tpl . bucket % self . bucket . name , self . name ) mimicdb . backend . hmset ( tpl . key % ( self . bucket . name , self . name ) , dict ( size = self . size , md5 = self . md5 ) )
9231	def fetch_events_async ( self , issues , tag_name ) : if not issues : return issues max_simultaneous_requests = self . options . max_simultaneous_requests verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project self . events_cnt = 0 if verbose : print ( "fetching events for {} {}... " . format ( len ( issues ) , tag_name ) ) def worker ( issue ) : page = 1 issue [ 'events' ] = [ ] while page > 0 : rc , data = gh . repos [ user ] [ repo ] . issues [ issue [ 'number' ] ] . events . get ( page = page , per_page = PER_PAGE_NUMBER ) if rc == 200 : issue [ 'events' ] . extend ( data ) self . events_cnt += len ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) threads = [ ] cnt = len ( issues ) for i in range ( 0 , ( cnt // max_simultaneous_requests ) + 1 ) : for j in range ( max_simultaneous_requests ) : idx = i * max_simultaneous_requests + j if idx == cnt : break t = threading . Thread ( target = worker , args = ( issues [ idx ] , ) ) threads . append ( t ) t . start ( ) if verbose > 2 : print ( "." , end = "" ) if not idx % PER_PAGE_NUMBER : print ( "" ) for t in threads : t . join ( ) if verbose > 2 : print ( "." )
12655	def remove_dcm2nii_underprocessed ( filepaths ) : cln_flist = [ ] # sort them by size len_sorted = sorted ( filepaths , key = len ) for idx , fpath in enumerate ( len_sorted ) : remove = False # get the basename and the rest of the files fname = op . basename ( fpath ) rest = len_sorted [ idx + 1 : ] # check if the basename is in the basename of the rest of the files for rest_fpath in rest : rest_file = op . basename ( rest_fpath ) if rest_file . endswith ( fname ) : remove = True break if not remove : cln_flist . append ( fpath ) return cln_flist
4099	def MDL ( N , rho , k ) : from numpy import log #p = arange(1, len(rho)+1) mdl = N * log ( rho ) + k * log ( N ) return mdl
10549	def get_results ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'result' , params = params ) if type ( res ) . __name__ == 'list' : return [ Result ( result ) for result in res ] else : return res except : # pragma: no cover raise
5250	def start ( self ) : # flush event queue in defensive way logger = _get_logger ( self . debug ) started = self . _session . start ( ) if started : ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SESSION_STATUS : raise RuntimeError ( 'Expected a "SESSION_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SESSION_STATUS : raise RuntimeError ( 'Expected a "SESSION_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) else : ev = self . _session . nextEvent ( self . timeout ) if ev . eventType ( ) == blpapi . Event . SESSION_STATUS : for msg in ev : logger . warning ( 'Message Received:\n{}' . format ( msg ) ) raise ConnectionError ( 'Could not start blpapi.Session' ) self . _init_services ( ) return self
4565	def pop_legacy_palette ( kwds , * color_defaults ) : palette = kwds . pop ( 'palette' , None ) if palette : legacy = [ k for k , _ in color_defaults if k in kwds ] if legacy : raise ValueError ( 'Cannot set palette and ' + ', ' . join ( legacy ) ) return palette values = [ kwds . pop ( k , v ) for k , v in color_defaults ] if values and color_defaults [ 0 ] [ 0 ] in ( 'colors' , 'palette' ) : values = values [ 0 ] return make . colors ( values or None )
5364	def stderr ( self ) : if self . _streaming : stderr = [ ] while not self . __stderr . empty ( ) : try : line = self . __stderr . get_nowait ( ) stderr . append ( line ) except : pass else : stderr = self . __stderr return stderr
10157	def get_transition_viewset_method ( transition_name , * * kwargs ) : @ detail_route ( methods = [ 'post' ] , * * kwargs ) def inner_func ( self , request , pk = None , * * kwargs ) : object = self . get_object ( ) transition_method = getattr ( object , transition_name ) transition_method ( by = self . request . user ) if self . save_after_transition : object . save ( ) serializer = self . get_serializer ( object ) return Response ( serializer . data ) return inner_func
1525	def to_table ( result ) : max_count = 20 table , count = [ ] , 0 for role , envs_topos in result . items ( ) : for env , topos in envs_topos . items ( ) : for topo in topos : count += 1 if count > max_count : continue else : table . append ( [ role , env , topo ] ) header = [ 'role' , 'env' , 'topology' ] rest_count = 0 if count <= max_count else count - max_count return table , header , rest_count
5819	def _map_oids ( oids ) : new_oids = set ( ) for oid in oids : if oid in _oid_map : new_oids |= _oid_map [ oid ] return oids | new_oids
817	def Distribution ( pos , size , counts , dtype ) : x = numpy . zeros ( size , dtype = dtype ) if hasattr ( pos , '__iter__' ) : # calculate normalization constant total = 0 for i in pos : total += counts [ i ] total = float ( total ) # set included positions to normalized probability for i in pos : x [ i ] = counts [ i ] / total # If we don't have a set of positions, assume there's only one position else : x [ pos ] = 1 return x
12252	def delete_keys ( self , * args , * * kwargs ) : ikeys = iter ( kwargs . get ( 'keys' , args [ 0 ] if args else [ ] ) ) while True : try : key = ikeys . next ( ) except StopIteration : break if isinstance ( key , basestring ) : mimicdb . backend . srem ( tpl . bucket % self . name , key ) mimicdb . backend . delete ( tpl . key % ( self . name , key ) ) elif isinstance ( key , BotoKey ) or isinstance ( key , Key ) : mimicdb . backend . srem ( tpl . bucket % self . name , key . name ) mimicdb . backend . delete ( tpl . key % ( self . name , key . name ) ) return super ( Bucket , self ) . delete_keys ( * args , * * kwargs )
1925	def get_group ( name : str ) -> _Group : global _groups if name in _groups : return _groups [ name ] group = _Group ( name ) _groups [ name ] = group return group
5624	def absolute_path ( path = None , base_dir = None ) : if path_is_remote ( path ) : return path else : if os . path . isabs ( path ) : return path else : if base_dir is None or not os . path . isabs ( base_dir ) : raise TypeError ( "base_dir must be an absolute path." ) return os . path . abspath ( os . path . join ( base_dir , path ) )
11990	def const_equal ( str_a , str_b ) : if len ( str_a ) != len ( str_b ) : return False result = True for i in range ( len ( str_a ) ) : result &= ( str_a [ i ] == str_b [ i ] ) return result
5554	def _raw_at_zoom ( config , zooms ) : params_per_zoom = { } for zoom in zooms : params = { } for name , element in config . items ( ) : if name not in _RESERVED_PARAMETERS : out_element = _element_at_zoom ( name , element , zoom ) if out_element is not None : params [ name ] = out_element params_per_zoom [ zoom ] = params return params_per_zoom
5965	def make_main_index ( struct , selection = '"Protein"' , ndx = 'main.ndx' , oldndx = None ) : logger . info ( "Building the main index file {ndx!r}..." . format ( * * vars ( ) ) ) # pass 1: select # get a list of groups # need the first "" to get make_ndx to spit out the group list. _ , out , _ = gromacs . make_ndx ( f = struct , n = oldndx , o = ndx , stdout = False , input = ( "" , "q" ) ) groups = cbook . parse_ndxlist ( out ) # find the matching groups, # there is a nasty bug in GROMACS where make_ndx may have multiple # groups, which caused the previous approach to fail big time. # this is a work around the make_ndx bug. # striping the "" allows compatibility with existing make_ndx selection commands. selection = selection . strip ( "\"" ) selected_groups = [ g for g in groups if g [ 'name' ] . lower ( ) == selection . lower ( ) ] if len ( selected_groups ) > 1 : logging . warn ( "make_ndx created duplicated groups, performing work around" ) if len ( selected_groups ) <= 0 : msg = "no groups found for selection {0}, available groups are {1}" . format ( selection , groups ) logging . error ( msg ) raise ValueError ( msg ) # Found at least one matching group, we're OK # index of last group last = len ( groups ) - 1 assert last == groups [ - 1 ] [ 'nr' ] group = selected_groups [ 0 ] # pass 2: # 1) last group is __main__ # 2) __environment__ is everything else (eg SOL, ions, ...) _ , out , _ = gromacs . make_ndx ( f = struct , n = ndx , o = ndx , stdout = False , # make copy selected group, this now has index last + 1 input = ( "{0}" . format ( group [ 'nr' ] ) , # rename this to __main__ "name {0} __main__" . format ( last + 1 ) , # make a complement to this group, it get index last + 2 "! \"__main__\"" , # rename this to __environment__ "name {0} __environment__" . format ( last + 2 ) , # list the groups "" , # quit "q" ) ) return cbook . parse_ndxlist ( out )
2265	def dict_isect ( * args ) : if not args : return { } else : dictclass = OrderedDict if isinstance ( args [ 0 ] , OrderedDict ) else dict common_keys = set . intersection ( * map ( set , args ) ) first_dict = args [ 0 ] return dictclass ( ( k , first_dict [ k ] ) for k in common_keys )
7182	def parse_signature_type_comment ( type_comment ) : try : result = ast3 . parse ( type_comment , '<func_type>' , 'func_type' ) except SyntaxError : raise ValueError ( f"invalid function signature type comment: {type_comment!r}" ) assert isinstance ( result , ast3 . FunctionType ) if len ( result . argtypes ) == 1 : argtypes = result . argtypes [ 0 ] else : argtypes = result . argtypes return argtypes , result . returns
8145	def sharpen ( self , value = 1.0 ) : s = ImageEnhance . Sharpness ( self . img ) self . img = s . enhance ( value )
6046	def array_2d_from_array_1d ( self , padded_array_1d ) : padded_array_2d = self . map_to_2d_keep_padded ( padded_array_1d ) pad_size_0 = self . mask . shape [ 0 ] - self . image_shape [ 0 ] pad_size_1 = self . mask . shape [ 1 ] - self . image_shape [ 1 ] return ( padded_array_2d [ pad_size_0 // 2 : self . mask . shape [ 0 ] - pad_size_0 // 2 , pad_size_1 // 2 : self . mask . shape [ 1 ] - pad_size_1 // 2 ] )
3283	def read ( self , size = None ) : while size is None or len ( self . buffer ) < size : try : self . buffer += next ( self . data_stream ) except StopIteration : break sized_chunk = self . buffer [ : size ] if size is None : self . buffer = "" else : self . buffer = self . buffer [ size : ] return sized_chunk
5435	def tasks_file_to_task_descriptors ( tasks , retries , input_file_param_util , output_file_param_util ) : task_descriptors = [ ] path = tasks [ 'path' ] task_min = tasks . get ( 'min' ) task_max = tasks . get ( 'max' ) # Load the file and set up a Reader that tokenizes the fields param_file = dsub_util . load_file ( path ) reader = csv . reader ( param_file , delimiter = '\t' ) # Read the first line and extract the parameters header = six . advance_iterator ( reader ) job_params = parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) # Build a list of records from the parsed input file for row in reader : # Tasks are numbered starting at 1 and since the first line of the TSV # file is a header, the first task appears on line 2. task_id = reader . line_num - 1 if task_min and task_id < task_min : continue if task_max and task_id > task_max : continue if len ( row ) != len ( job_params ) : dsub_util . print_error ( 'Unexpected number of fields %s vs %s: line %s' % ( len ( row ) , len ( job_params ) , reader . line_num ) ) # Each row can contain "envs", "inputs", "outputs" envs = set ( ) inputs = set ( ) outputs = set ( ) labels = set ( ) for i in range ( 0 , len ( job_params ) ) : param = job_params [ i ] name = param . name if isinstance ( param , job_model . EnvParam ) : envs . add ( job_model . EnvParam ( name , row [ i ] ) ) elif isinstance ( param , job_model . LabelParam ) : labels . add ( job_model . LabelParam ( name , row [ i ] ) ) elif isinstance ( param , job_model . InputFileParam ) : inputs . add ( input_file_param_util . make_param ( name , row [ i ] , param . recursive ) ) elif isinstance ( param , job_model . OutputFileParam ) : outputs . add ( output_file_param_util . make_param ( name , row [ i ] , param . recursive ) ) task_descriptors . append ( job_model . TaskDescriptor ( { 'task-id' : task_id , 'task-attempt' : 1 if retries else None } , { 'labels' : labels , 'envs' : envs , 'inputs' : inputs , 'outputs' : outputs } , job_model . Resources ( ) ) ) # Ensure that there are jobs to execute (and not just a header) if not task_descriptors : raise ValueError ( 'No tasks added from %s' % path ) return task_descriptors
4775	def contains_duplicates ( self ) : try : if len ( self . val ) != len ( set ( self . val ) ) : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain duplicates, but did not.' % self . val )
6773	def uninstall_blacklisted ( self ) : from burlap . system import distrib_family blacklisted_packages = self . env . blacklisted_packages if not blacklisted_packages : print ( 'No blacklisted packages.' ) return else : family = distrib_family ( ) if family == DEBIAN : self . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq purge %s' % ' ' . join ( blacklisted_packages ) ) else : raise NotImplementedError ( 'Unknown family: %s' % family )
10875	def get_polydisp_pts_wts ( kfki , sigkf , dist_type = 'gaussian' , nkpts = 3 ) : if dist_type . lower ( ) == 'gaussian' : pts , wts = np . polynomial . hermite . hermgauss ( nkpts ) kfkipts = np . abs ( kfki + sigkf * np . sqrt ( 2 ) * pts ) elif dist_type . lower ( ) == 'laguerre' or dist_type . lower ( ) == 'gamma' : k_scale = sigkf ** 2 / kfki associated_order = kfki ** 2 / sigkf ** 2 - 1 #Associated Laguerre with alpha >~170 becomes numerically unstable, so: max_order = 150 if associated_order > max_order or associated_order < ( - 1 + 1e-3 ) : warnings . warn ( 'Numerically unstable sigk, clipping' , RuntimeWarning ) associated_order = np . clip ( associated_order , - 1 + 1e-3 , max_order ) kfkipts , wts = la_roots ( nkpts , associated_order ) kfkipts *= k_scale else : raise ValueError ( 'dist_type must be either gaussian or laguerre' ) return kfkipts , wts / wts . sum ( )
1112	def make_file ( self , fromlines , tolines , fromdesc = '' , todesc = '' , context = False , numlines = 5 ) : return self . _file_template % dict ( styles = self . _styles , legend = self . _legend , table = self . make_table ( fromlines , tolines , fromdesc , todesc , context = context , numlines = numlines ) )
7088	def jd_to_datetime ( jd , returniso = False ) : tt = astime . Time ( jd , format = 'jd' , scale = 'utc' ) if returniso : return tt . iso else : return tt . datetime
2195	def write ( self , msg ) : if self . redirect is not None : self . redirect . write ( msg ) if six . PY2 : from xdoctest . utils . util_str import ensure_unicode msg = ensure_unicode ( msg ) super ( TeeStringIO , self ) . write ( msg )
5920	def fit ( self , xy = False , * * kwargs ) : kwargs . setdefault ( 's' , self . tpr ) kwargs . setdefault ( 'n' , self . ndx ) kwargs [ 'f' ] = self . xtc force = kwargs . pop ( 'force' , self . force ) if xy : fitmode = 'rotxy+transxy' kwargs . pop ( 'fit' , None ) infix_default = '_fitxy' else : fitmode = kwargs . pop ( 'fit' , 'rot+trans' ) # user can use 'progressive', too infix_default = '_fit' dt = kwargs . get ( 'dt' ) if dt : infix_default += '_dt{0:d}ps' . format ( int ( dt ) ) # dt in ps kwargs . setdefault ( 'o' , self . outfile ( self . infix_filename ( None , self . xtc , infix_default , 'xtc' ) ) ) fitgroup = kwargs . pop ( 'fitgroup' , 'backbone' ) kwargs . setdefault ( 'input' , [ fitgroup , "system" ] ) if kwargs . get ( 'center' , False ) : logger . warn ( "Transformer.fit(): center=%(center)r used: centering should not be combined with fitting." , kwargs ) if len ( kwargs [ 'inputs' ] ) != 3 : logger . error ( "If you insist on centering you must provide three groups in the 'input' kwarg: (center, fit, output)" ) raise ValuError ( "Insufficient index groups for centering,fitting,output" ) logger . info ( "Fitting trajectory %r to with xy=%r..." , kwargs [ 'f' ] , xy ) logger . info ( "Fitting on index group %(fitgroup)r" , vars ( ) ) with utilities . in_dir ( self . dirname ) : if self . check_file_exists ( kwargs [ 'o' ] , resolve = "indicate" , force = force ) : logger . warn ( "File %r exists; force regenerating it with force=True." , kwargs [ 'o' ] ) else : gromacs . trjconv ( fit = fitmode , * * kwargs ) logger . info ( "Fitted trajectory (fitmode=%s): %r." , fitmode , kwargs [ 'o' ] ) return { 'tpr' : self . rp ( kwargs [ 's' ] ) , 'xtc' : self . rp ( kwargs [ 'o' ] ) }
2746	def edit ( self ) : input_params = { "name" : self . name , "public_key" : self . public_key , } data = self . get_data ( "account/keys/%s" % self . id , type = PUT , params = input_params ) if data : self . id = data [ 'ssh_key' ] [ 'id' ]
10758	def writable_stream ( handle ) : if isinstance ( handle , io . IOBase ) and sys . version_info >= ( 3 , 5 ) : return handle . writable ( ) try : handle . write ( b'' ) except ( io . UnsupportedOperation , IOError ) : return False else : return True
1764	def push_int ( self , value , force = False ) : self . STACK -= self . address_bit_size // 8 self . write_int ( self . STACK , value , force = force ) return self . STACK
112	def is_activated ( self , images , augmenter , parents , default ) : if self . activator is None : return default else : return self . activator ( images , augmenter , parents , default )
2635	def parent_callback ( self , executor_fu ) : with self . _update_lock : if not executor_fu . done ( ) : raise ValueError ( "done callback called, despite future not reporting itself as done" ) # this is for consistency checking if executor_fu != self . parent : if executor_fu . exception ( ) is None and not isinstance ( executor_fu . result ( ) , RemoteExceptionWrapper ) : # ... then we completed with a value, not an exception or wrapped exception, # but we've got an updated executor future. # This is bad - for example, we've started a retry even though we have a result raise ValueError ( "internal consistency error: AppFuture done callback called without an exception, but parent has been changed since then" ) try : res = executor_fu . result ( ) if isinstance ( res , RemoteExceptionWrapper ) : res . reraise ( ) super ( ) . set_result ( executor_fu . result ( ) ) except Exception as e : if executor_fu . retries_left > 0 : # ignore this exception, because assume some later # parent executor, started external to this class, # will provide the answer pass else : super ( ) . set_exception ( e )
4753	def tcase_parse_descr ( tcase ) : descr_short = "SHORT" descr_long = "LONG" try : comment = tcase_comment ( tcase ) except ( IOError , OSError , ValueError ) as exc : comment = [ ] cij . err ( "tcase_parse_descr: failed: %r, tcase: %r" % ( exc , tcase ) ) comment = [ l for l in comment if l . strip ( ) ] # Remove empty lines for line_number , line in enumerate ( comment ) : if line . startswith ( "#" ) : comment [ line_number ] = line [ 1 : ] if comment : descr_short = comment [ 0 ] if len ( comment ) > 1 : descr_long = "\n" . join ( comment [ 1 : ] ) return descr_short , descr_long
4698	def fmt ( lbaf = 3 ) : if env ( ) : cij . err ( "cij.nvme.exists: Invalid NVMe ENV." ) return 1 nvme = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ "nvme" , "format" , nvme [ "DEV_PATH" ] , "-l" , str ( lbaf ) ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True ) return rcode
4768	def is_type_of ( self , some_type ) : if type ( some_type ) is not type and not issubclass ( type ( some_type ) , type ) : raise TypeError ( 'given arg must be a type' ) if type ( self . val ) is not some_type : if hasattr ( self . val , '__name__' ) : t = self . val . __name__ elif hasattr ( self . val , '__class__' ) : t = self . val . __class__ . __name__ else : t = 'unknown' self . _err ( 'Expected <%s:%s> to be of type <%s>, but was not.' % ( self . val , t , some_type . __name__ ) ) return self
10128	def update ( self , dt ) : self . translate ( dt * self . velocity ) self . rotate ( dt * self . angular_velocity )
8578	def list_servers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
8326	def setup ( self , parent = None , previous = None ) : self . parent = parent self . previous = previous self . next = None self . previousSibling = None self . nextSibling = None if self . parent and self . parent . contents : self . previousSibling = self . parent . contents [ - 1 ] self . previousSibling . nextSibling = self
1045	def float_pack ( x , size ) : if size == 8 : MIN_EXP = - 1021 # = sys.float_info.min_exp MAX_EXP = 1024 # = sys.float_info.max_exp MANT_DIG = 53 # = sys.float_info.mant_dig BITS = 64 elif size == 4 : MIN_EXP = - 125 # C's FLT_MIN_EXP MAX_EXP = 128 # FLT_MAX_EXP MANT_DIG = 24 # FLT_MANT_DIG BITS = 32 else : raise ValueError ( "invalid size value" ) sign = math . copysign ( 1.0 , x ) < 0.0 if math . isinf ( x ) : mant = 0 exp = MAX_EXP - MIN_EXP + 2 elif math . isnan ( x ) : mant = 1 << ( MANT_DIG - 2 ) # other values possible exp = MAX_EXP - MIN_EXP + 2 elif x == 0.0 : mant = 0 exp = 0 else : m , e = math . frexp ( abs ( x ) ) # abs(x) == m * 2**e exp = e - ( MIN_EXP - 1 ) if exp > 0 : # Normal case. mant = round_to_nearest ( m * ( 1 << MANT_DIG ) ) mant -= 1 << MANT_DIG - 1 else : # Subnormal case. if exp + MANT_DIG - 1 >= 0 : mant = round_to_nearest ( m * ( 1 << exp + MANT_DIG - 1 ) ) else : mant = 0 exp = 0 # Special case: rounding produced a MANT_DIG-bit mantissa. assert 0 <= mant <= 1 << MANT_DIG - 1 if mant == 1 << MANT_DIG - 1 : mant = 0 exp += 1 # Raise on overflow (in some circumstances, may want to return # infinity instead). if exp >= MAX_EXP - MIN_EXP + 2 : raise OverflowError ( "float too large to pack in this format" ) # check constraints assert 0 <= mant < 1 << MANT_DIG - 1 assert 0 <= exp <= MAX_EXP - MIN_EXP + 2 assert 0 <= sign <= 1 return ( ( sign << BITS - 1 ) | ( exp << MANT_DIG - 1 ) ) | mant
2146	def _separate ( self , kwargs ) : self . _pop_none ( kwargs ) result = { } for field in Resource . config_fields : if field in kwargs : result [ field ] = kwargs . pop ( field ) if field in Resource . json_fields : # If result[field] is not a string we can continue on if not isinstance ( result [ field ] , six . string_types ) : continue try : data = json . loads ( result [ field ] ) result [ field ] = data except ValueError : raise exc . TowerCLIError ( 'Provided json file format ' 'invalid. Please recheck.' ) return result
7959	def handle_hup ( self ) : with self . lock : if self . _state == 'connecting' and self . _dst_addrs : self . _hup = False self . _set_state ( "connect" ) return self . _hup = True
9389	def check_sla ( self , sla , diff_metric ) : try : if sla . display is '%' : diff_val = float ( diff_metric [ 'percent_diff' ] ) else : diff_val = float ( diff_metric [ 'absolute_diff' ] ) except ValueError : return False if not ( sla . check_sla_passed ( diff_val ) ) : self . sla_failures += 1 self . sla_failure_list . append ( DiffSLAFailure ( sla , diff_metric ) ) return True
11713	def instance ( self , counter = None ) : if not counter : history = self . history ( ) if not history : return history else : return Response . _from_json ( history [ 'pipelines' ] [ 0 ] ) return self . _get ( '/instance/{counter:d}' . format ( counter = counter ) )
4407	async def listen ( self ) : while not self . _shutdown : try : data = json . loads ( await self . _ws . recv ( ) ) except websockets . ConnectionClosed as error : log . warning ( 'Disconnected from Lavalink: {}' . format ( str ( error ) ) ) for g in self . _lavalink . players . _players . copy ( ) . keys ( ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( g ) ) await ws . voice_state ( int ( g ) , None ) self . _lavalink . players . clear ( ) if self . _shutdown : break if await self . _attempt_reconnect ( ) : return log . warning ( 'Unable to reconnect to Lavalink!' ) break op = data . get ( 'op' , None ) log . debug ( 'Received WebSocket data {}' . format ( str ( data ) ) ) if not op : return log . debug ( 'Received WebSocket message without op {}' . format ( str ( data ) ) ) if op == 'event' : log . debug ( 'Received event of type {}' . format ( data [ 'type' ] ) ) player = self . _lavalink . players [ int ( data [ 'guildId' ] ) ] event = None if data [ 'type' ] == 'TrackEndEvent' : event = TrackEndEvent ( player , data [ 'track' ] , data [ 'reason' ] ) elif data [ 'type' ] == 'TrackExceptionEvent' : event = TrackExceptionEvent ( player , data [ 'track' ] , data [ 'error' ] ) elif data [ 'type' ] == 'TrackStuckEvent' : event = TrackStuckEvent ( player , data [ 'track' ] , data [ 'thresholdMs' ] ) if event : await self . _lavalink . dispatch_event ( event ) elif op == 'playerUpdate' : await self . _lavalink . update_state ( data ) elif op == 'stats' : self . _lavalink . stats . _update ( data ) await self . _lavalink . dispatch_event ( StatsUpdateEvent ( self . _lavalink . stats ) ) log . debug ( 'Closing WebSocket...' ) await self . _ws . close ( )
10304	def min_tanimoto_set_similarity ( x : Iterable [ X ] , y : Iterable [ X ] ) -> float : a , b = set ( x ) , set ( y ) if not a or not b : return 0.0 return len ( a & b ) / min ( len ( a ) , len ( b ) )
2691	def iter_cython ( path ) : for dir_path , dir_names , file_names in os . walk ( path ) : for file_name in file_names : if file_name . startswith ( '.' ) : continue if os . path . splitext ( file_name ) [ 1 ] not in ( '.pyx' , '.pxd' ) : continue yield os . path . join ( dir_path , file_name )
6342	def docs_of_words ( self ) : return [ [ words for sents in doc for words in sents ] for doc in self . corpus ]
4113	def rc2is ( k ) : assert numpy . isrealobj ( k ) , 'Inverse sine parameters not defined for complex reflection coefficients.' if max ( numpy . abs ( k ) ) >= 1 : raise ValueError ( 'All reflection coefficients should have magnitude less than unity.' ) return ( 2 / numpy . pi ) * numpy . arcsin ( k )
4460	def geo ( lat , lon , radius , unit = 'km' ) : return GeoValue ( lat , lon , radius , unit )
4508	def error ( self , fail = True , action = '' ) : e = 'There was an unknown error communicating with the device.' if action : e = 'While %s: %s' % ( action , e ) log . error ( e ) if fail : raise IOError ( e )
11775	def EnsembleLearner ( learners ) : def train ( dataset ) : predictors = [ learner ( dataset ) for learner in learners ] def predict ( example ) : return mode ( predictor ( example ) for predictor in predictors ) return predict return train
3719	def conductivity ( CASRN = None , AvailableMethods = False , Method = None , full_info = True ) : def list_methods ( ) : methods = [ ] if CASRN in Lange_cond_pure . index : methods . append ( LANGE_COND ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == LANGE_COND : kappa = float ( Lange_cond_pure . at [ CASRN , 'Conductivity' ] ) if full_info : T = float ( Lange_cond_pure . at [ CASRN , 'T' ] ) elif Method == NONE : kappa , T = None , None else : raise Exception ( 'Failure in in function' ) if full_info : return kappa , T else : return kappa
8534	def read ( cls , data , protocol = None , fallback_protocol = TBinaryProtocol , finagle_thrift = False , max_fields = MAX_FIELDS , max_list_size = MAX_LIST_SIZE , max_map_size = MAX_MAP_SIZE , max_set_size = MAX_SET_SIZE , read_values = False ) : # do we have enough data? if len ( data ) < cls . MIN_MESSAGE_SIZE : raise ValueError ( 'not enough data' ) if protocol is None : protocol = cls . detect_protocol ( data , fallback_protocol ) trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) # finagle-thrift prepends a RequestHeader # # See: http://git.io/vsziG header = None if finagle_thrift : try : header = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) except : # reset stream, maybe it's not finagle-thrift trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) # unpack the message method , mtype , seqid = proto . readMessageBegin ( ) mtype = cls . message_type_to_str ( mtype ) if len ( method ) == 0 or method . isspace ( ) or method . startswith ( ' ' ) : raise ValueError ( 'no method name' ) if len ( method ) > cls . MAX_METHOD_LENGTH : raise ValueError ( 'method name too long' ) # we might have made it until this point by mere chance, so filter out # suspicious method names valid = range ( 33 , 127 ) if any ( ord ( char ) not in valid for char in method ) : raise ValueError ( 'invalid method name' % method ) args = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) proto . readMessageEnd ( ) # Note: this is a bit fragile, the right thing would be to count bytes # as we read them (i.e.: when calling readI32, etc). msglen = trans . _buffer . tell ( ) return cls ( method , mtype , seqid , args , header , msglen ) , msglen
3315	def _find ( self , url ) : # Query the permanent view to find a url vr = self . db . view ( "properties/by_url" , key = url , include_docs = True ) _logger . debug ( "find(%r) returned %s" % ( url , len ( vr ) ) ) assert len ( vr ) <= 1 , "Found multiple matches for %r" % url for row in vr : assert row . doc return row . doc return None
7240	def window_cover ( self , window_shape , pad = True ) : size_y , size_x = window_shape [ 0 ] , window_shape [ 1 ] _ndepth , _nheight , _nwidth = self . shape nheight , _m = divmod ( _nheight , size_y ) nwidth , _n = divmod ( _nwidth , size_x ) img = self if pad is True : new_height , new_width = _nheight , _nwidth if _m != 0 : new_height = ( nheight + 1 ) * size_y if _n != 0 : new_width = ( nwidth + 1 ) * size_x if ( new_height , new_width ) != ( _nheight , _nwidth ) : bounds = box ( 0 , 0 , new_width , new_height ) geom = ops . transform ( self . __geo_transform__ . fwd , bounds ) img = self [ geom ] row_lims = range ( 0 , img . shape [ 1 ] , size_y ) col_lims = range ( 0 , img . shape [ 2 ] , size_x ) for maxy , maxx in product ( row_lims , col_lims ) : reg = img [ : , maxy : ( maxy + size_y ) , maxx : ( maxx + size_x ) ] if pad is False : if reg . shape [ 1 : ] == window_shape : yield reg else : yield reg
5409	def _validate_ram ( ram_in_mb ) : return int ( GoogleV2CustomMachine . _MEMORY_MULTIPLE * math . ceil ( ram_in_mb / GoogleV2CustomMachine . _MEMORY_MULTIPLE ) )
7829	def add_option ( self , value , label ) : if type ( value ) is list : warnings . warn ( ".add_option() accepts single value now." , DeprecationWarning , stacklevel = 1 ) value = value [ 0 ] if self . type not in ( "list-multi" , "list-single" ) : raise ValueError ( "Options are allowed only for list types." ) option = Option ( value , label ) self . options . append ( option ) return option
12136	def fields ( self ) : parse = list ( string . Formatter ( ) . parse ( self . pattern ) ) return [ f for f in zip ( * parse ) [ 1 ] if f is not None ]
13893	def _HandleContentsEol ( contents , eol_style ) : if eol_style == EOL_STYLE_NONE : return contents if eol_style == EOL_STYLE_UNIX : return contents . replace ( '\r\n' , eol_style ) . replace ( '\r' , eol_style ) if eol_style == EOL_STYLE_MAC : return contents . replace ( '\r\n' , eol_style ) . replace ( '\n' , eol_style ) if eol_style == EOL_STYLE_WINDOWS : return contents . replace ( '\r\n' , '\n' ) . replace ( '\r' , '\n' ) . replace ( '\n' , EOL_STYLE_WINDOWS ) raise ValueError ( 'Unexpected eol style: %r' % ( eol_style , ) )
12744	def get_internal_urls ( self ) : internal_urls = self . get_subfields ( "856" , "u" , i1 = "4" , i2 = "0" ) internal_urls . extend ( self . get_subfields ( "998" , "a" ) ) internal_urls . extend ( self . get_subfields ( "URL" , "u" ) ) return map ( lambda x : x . replace ( "&amp;" , "&" ) , internal_urls )
1469	def process_tick ( self , tup ) : curtime = int ( time . time ( ) ) window_info = WindowContext ( curtime - self . window_duration , curtime ) self . processWindow ( window_info , list ( self . current_tuples ) ) for tup in self . current_tuples : self . ack ( tup ) self . current_tuples . clear ( )
10228	def get_separate_unstable_correlation_triples ( graph : BELGraph ) -> Iterable [ NodeTriple ] : cg = get_correlation_graph ( graph ) for a , b , c in get_correlation_triangles ( cg ) : if POSITIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and NEGATIVE_CORRELATION in cg [ a ] [ c ] : yield b , a , c if POSITIVE_CORRELATION in cg [ a ] [ b ] and NEGATIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield a , b , c if NEGATIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield c , a , b
6468	def csi_wrap ( self , value , capname , * args ) : if isinstance ( value , str ) : value = value . encode ( 'utf-8' ) return b'' . join ( [ self . csi ( capname , * args ) , value , self . csi ( 'sgr0' ) , ] )
12256	def lbfgs ( x , rho , f_df , maxiter = 20 ) : def f_df_augmented ( theta ) : f , df = f_df ( theta ) obj = f + ( rho / 2. ) * np . linalg . norm ( theta - x ) ** 2 grad = df + rho * ( theta - x ) return obj , grad res = scipy_minimize ( f_df_augmented , x , jac = True , method = 'L-BFGS-B' , options = { 'maxiter' : maxiter , 'disp' : False } ) return res . x
12479	def rcfile ( appname , section = None , args = { } , strip_dashes = True ) : if strip_dashes : for k in args . keys ( ) : args [ k . lstrip ( '-' ) ] = args . pop ( k ) environ = get_environment ( appname ) if section is None : section = appname config = get_config ( appname , section , args . get ( 'config' , '' ) , args . get ( 'path' , '' ) ) config = merge ( merge ( args , config ) , environ ) if not config : raise IOError ( 'Could not find any rcfile for application ' '{}.' . format ( appname ) ) return config
13024	def get ( self , pk ) : if type ( pk ) == str : # Probably an int, give it a shot try : pk = int ( pk ) except ValueError : pass return self . select ( "SELECT {0} FROM " + self . table + " WHERE " + self . pk + " = {1};" , self . columns , pk )
11195	def cp ( resume , quiet , dataset_uri , dest_base_uri ) : _copy ( resume , quiet , dataset_uri , dest_base_uri )
8098	def create ( self , stylename , * * kwargs ) : if stylename == "default" : self [ stylename ] = style ( stylename , self . _ctx , * * kwargs ) return self [ stylename ] k = kwargs . get ( "template" , "default" ) s = self [ stylename ] = self [ k ] . copy ( stylename ) for attr in kwargs : if s . __dict__ . has_key ( attr ) : s . __dict__ [ attr ] = kwargs [ attr ] return s
8554	def reserve_ipblock ( self , ipblock ) : properties = { "name" : ipblock . name } if ipblock . location : properties [ 'location' ] = ipblock . location if ipblock . size : properties [ 'size' ] = str ( ipblock . size ) raw = { "properties" : properties , } response = self . _perform_request ( url = '/ipblocks' , method = 'POST' , data = json . dumps ( raw ) ) return response
13816	def Parse ( text , message ) : if not isinstance ( text , six . text_type ) : text = text . decode ( 'utf-8' ) try : if sys . version_info < ( 2 , 7 ) : # object_pair_hook is not supported before python2.7 js = json . loads ( text ) else : js = json . loads ( text , object_pairs_hook = _DuplicateChecker ) except ValueError as e : raise ParseError ( 'Failed to load JSON: {0}.' . format ( str ( e ) ) ) _ConvertMessage ( js , message ) return message
1189	def _randbelow ( self , n ) : # TODO # change once int.bit_length is implemented. # k = n.bit_length() k = _int_bit_length ( n ) r = self . getrandbits ( k ) while r >= n : r = self . getrandbits ( k ) return r
33	def reset ( self , * * kwargs ) : if self . was_real_done : obs = self . env . reset ( * * kwargs ) else : # no-op step to advance from terminal/lost life state obs , _ , _ , _ = self . env . step ( 0 ) self . lives = self . env . unwrapped . ale . lives ( ) return obs
10236	def get_graphs_by_ids ( self , network_ids : Iterable [ int ] ) -> List [ BELGraph ] : return [ self . networks [ network_id ] for network_id in network_ids ]
270	def check_intraday ( estimate , returns , positions , transactions ) : if estimate == 'infer' : if positions is not None and transactions is not None : if detect_intraday ( positions , transactions ) : warnings . warn ( 'Detected intraday strategy; inferring positi' + 'ons from transactions. Set estimate_intraday' + '=False to disable.' ) return estimate_intraday ( returns , positions , transactions ) else : return positions else : return positions elif estimate : if positions is not None and transactions is not None : return estimate_intraday ( returns , positions , transactions ) else : raise ValueError ( 'Positions and txns needed to estimate intraday' ) else : return positions
10919	def do_levmarq ( s , param_names , damping = 0.1 , decrease_damp_factor = 10. , run_length = 6 , eig_update = True , collect_stats = False , rz_order = 0 , run_type = 2 , * * kwargs ) : if rz_order > 0 : aug = AugmentedState ( s , param_names , rz_order = rz_order ) lm = LMAugmentedState ( aug , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , * * kwargs ) else : lm = LMGlobals ( s , param_names , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , * * kwargs ) if run_type == 2 : lm . do_run_2 ( ) elif run_type == 1 : lm . do_run_1 ( ) else : raise ValueError ( 'run_type=1,2 only' ) if collect_stats : return lm . get_termination_stats ( )
363	def natural_keys ( text ) : # - alist.sort(key=natural_keys) sorts in human order # http://nedbatchelder.com/blog/200712/human_sorting.html # (See Toothy's implementation in the comments) def atoi ( text ) : return int ( text ) if text . isdigit ( ) else text return [ atoi ( c ) for c in re . split ( '(\d+)' , text ) ]
6446	def _cond_bb ( self , word , suffix_len ) : return ( len ( word ) - suffix_len >= 3 and word [ - suffix_len - 3 : - suffix_len ] != 'met' and word [ - suffix_len - 4 : - suffix_len ] != 'ryst' )
324	def rolling_volatility ( returns , rolling_vol_window ) : return returns . rolling ( rolling_vol_window ) . std ( ) * np . sqrt ( APPROX_BDAYS_PER_YEAR )
9924	def create ( self , * args , * * kwargs ) : is_primary = kwargs . pop ( "is_primary" , False ) with transaction . atomic ( ) : email = super ( EmailAddressManager , self ) . create ( * args , * * kwargs ) if is_primary : email . set_primary ( ) return email
3711	def calculate ( self , T , P , zs , ws , method ) : if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeLiquids ] return Amgat ( zs , Vms ) elif method == COSTALD_MIXTURE : return COSTALD_mixture ( zs , T , self . Tcs , self . Vcs , self . omegas ) elif method == COSTALD_MIXTURE_FIT : return COSTALD_mixture ( zs , T , self . Tcs , self . COSTALD_Vchars , self . COSTALD_omegas ) elif method == RACKETT : return Rackett_mixture ( T , zs , self . MWs , self . Tcs , self . Pcs , self . Zcs ) elif method == RACKETT_PARAMETERS : return Rackett_mixture ( T , zs , self . MWs , self . Tcs , self . Pcs , self . Z_RAs ) elif method == LALIBERTE : ws = list ( ws ) ws . pop ( self . index_w ) rho = Laliberte_density ( T , ws , self . wCASs ) MW = mixing_simple ( zs , self . MWs ) return rho_to_Vm ( rho , MW ) else : raise Exception ( 'Method not valid' )
1295	def import_demo_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_demo_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )
4954	def _disconnect_user_post_save_for_migrations ( self , sender , * * kwargs ) : # pylint: disable=unused-argument from django . db . models . signals import post_save post_save . disconnect ( sender = self . auth_user_model , dispatch_uid = USER_POST_SAVE_DISPATCH_UID )
12772	def follow_markers ( self , start = 0 , end = 1e100 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , frame in enumerate ( self . markers ) : if frame_no < start : continue if frame_no >= end : break for states in self . _step_to_marker_frame ( frame_no ) : yield states
2503	def handle_lics ( self , lics ) : # Handle extracted licensing info type. if ( lics , RDF . type , self . spdx_namespace [ 'ExtractedLicensingInfo' ] ) in self . graph : return self . parse_only_extr_license ( lics ) # Assume resource, hence the path separator ident_start = lics . rfind ( '/' ) + 1 if ident_start == 0 : # special values such as spdx:noassertion special = self . to_special_value ( lics ) if special == lics : if self . LICS_REF_REGEX . match ( lics ) : # Is a license ref i.e LicenseRef-1 return document . License . from_identifier ( lics ) else : # Not a known license form raise SPDXValueError ( 'License' ) else : # is a special value return special else : # license url return document . License . from_identifier ( lics [ ident_start : ] )
13694	def main ( ) : global DEBUG argd = docopt ( USAGESTR , version = VERSIONSTR , script = SCRIPT ) DEBUG = argd [ '--debug' ] width = parse_int ( argd [ '--width' ] or DEFAULT_WIDTH ) or 1 indent = parse_int ( argd [ '--indent' ] or ( argd [ '--INDENT' ] or 0 ) ) prepend = ' ' * ( indent * 4 ) if prepend and argd [ '--indent' ] : # Smart indent, change max width based on indention. width -= len ( prepend ) userprepend = argd [ '--prepend' ] or ( argd [ '--PREPEND' ] or '' ) prepend = '' . join ( ( prepend , userprepend ) ) if argd [ '--prepend' ] : # Smart indent, change max width based on prepended text. width -= len ( userprepend ) userappend = argd [ '--append' ] or ( argd [ '--APPEND' ] or '' ) if argd [ '--append' ] : width -= len ( userappend ) if argd [ 'WORDS' ] : # Try each argument as a file name. argd [ 'WORDS' ] = ( ( try_read_file ( w ) if len ( w ) < 256 else w ) for w in argd [ 'WORDS' ] ) words = ' ' . join ( ( w for w in argd [ 'WORDS' ] if w ) ) else : # No text/filenames provided, use stdin for input. words = read_stdin ( ) block = FormatBlock ( words ) . iter_format_block ( chars = argd [ '--chars' ] , fill = argd [ '--fill' ] , prepend = prepend , strip_first = argd [ '--stripfirst' ] , append = userappend , strip_last = argd [ '--striplast' ] , width = width , newlines = argd [ '--newlines' ] , lstrip = argd [ '--lstrip' ] , ) for i , line in enumerate ( block ) : if argd [ '--enumerate' ] : # Current line number format supports up to 999 lines before # messing up. Who would format 1000 lines like this anyway? print ( '{: >3}: {}' . format ( i + 1 , line ) ) else : print ( line ) return 0
11397	def add_cms_link ( self ) : intnote = record_get_field_values ( self . record , '690' , filter_subfield_code = "a" , filter_subfield_value = 'INTNOTE' ) if intnote : val_088 = record_get_field_values ( self . record , tag = '088' , filter_subfield_code = "a" ) for val in val_088 : if 'CMS' in val : url = ( 'http://weblib.cern.ch/abstract?CERN-CMS' + val . split ( 'CMS' , 1 ) [ - 1 ] ) record_add_field ( self . record , tag = '856' , ind1 = '4' , subfields = [ ( 'u' , url ) ] )
3774	def select_valid_methods ( self , T ) : # Consider either only the user's methods or all methods # Tabular data will be in both when inserted if self . forced : considered_methods = list ( self . user_methods ) else : considered_methods = list ( self . all_methods ) # User methods (incl. tabular data); add back later, after ranking the rest if self . user_methods : [ considered_methods . remove ( i ) for i in self . user_methods ] # Index the rest of the methods by ranked_methods, and add them to a list, sorted_methods preferences = sorted ( [ self . ranked_methods . index ( i ) for i in considered_methods ] ) sorted_methods = [ self . ranked_methods [ i ] for i in preferences ] # Add back the user's methods to the top, in order. if self . user_methods : [ sorted_methods . insert ( 0 , i ) for i in reversed ( self . user_methods ) ] sorted_valid_methods = [ ] for method in sorted_methods : if self . test_method_validity ( T , method ) : sorted_valid_methods . append ( method ) return sorted_valid_methods
4966	def clean ( self ) : cleaned_data = super ( ManageLearnersForm , self ) . clean ( ) # Here we take values from `data` (and not `cleaned_data`) as we need raw values - field clean methods # might "invalidate" the value and set it to None, while all we care here is if it was provided at all or not email_or_username = self . data . get ( self . Fields . EMAIL_OR_USERNAME , None ) bulk_upload_csv = self . files . get ( self . Fields . BULK_UPLOAD , None ) if not email_or_username and not bulk_upload_csv : raise ValidationError ( ValidationMessages . NO_FIELDS_SPECIFIED ) if email_or_username and bulk_upload_csv : raise ValidationError ( ValidationMessages . BOTH_FIELDS_SPECIFIED ) if email_or_username : mode = self . Modes . MODE_SINGULAR else : mode = self . Modes . MODE_BULK cleaned_data [ self . Fields . MODE ] = mode cleaned_data [ self . Fields . NOTIFY ] = self . clean_notify ( ) self . _validate_course ( ) self . _validate_program ( ) if self . data . get ( self . Fields . PROGRAM , None ) and self . data . get ( self . Fields . COURSE , None ) : raise ValidationError ( ValidationMessages . COURSE_AND_PROGRAM_ERROR ) return cleaned_data
11048	def dataReceived ( self , data ) : self . resetTimeout ( ) lines = ( self . _buffer + data ) . splitlines ( ) # str.splitlines() doesn't split the string after a trailing newline # character so we must check if there is a trailing newline and, if so, # clear the buffer as the line is "complete". Else, the line is # incomplete and we keep the last line in the buffer. if data . endswith ( b'\n' ) or data . endswith ( b'\r' ) : self . _buffer = b'' else : self . _buffer = lines . pop ( - 1 ) for line in lines : if self . transport . disconnecting : # this is necessary because the transport may be told to lose # the connection by a line within a larger packet, and it is # important to disregard all the lines in that packet following # the one that told it to close. return if len ( line ) > self . _max_length : self . lineLengthExceeded ( line ) return else : self . lineReceived ( line ) if len ( self . _buffer ) > self . _max_length : self . lineLengthExceeded ( self . _buffer ) return
5611	def memory_file ( data = None , profile = None ) : memfile = MemoryFile ( ) profile . update ( width = data . shape [ - 2 ] , height = data . shape [ - 1 ] ) with memfile . open ( * * profile ) as dataset : dataset . write ( data ) return memfile
8724	def from_timestamp ( ts ) : return datetime . datetime . utcfromtimestamp ( ts ) . replace ( tzinfo = pytz . utc )
1815	def SETNLE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . ZF == False , cpu . SF == cpu . OF ) , 1 , 0 ) )
10018	def application_exists ( self ) : response = self . ebs . describe_applications ( application_names = [ self . app_name ] ) return len ( response [ 'DescribeApplicationsResponse' ] [ 'DescribeApplicationsResult' ] [ 'Applications' ] ) > 0
2056	def CBZ ( cpu , op , dest ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , op . read ( ) , cpu . PC , dest . read ( ) )
13137	def _unicode ( string ) : for encoding in [ 'utf-8' , 'latin1' ] : try : result = unicode ( string , encoding ) return result except UnicodeDecodeError : pass result = unicode ( string , 'utf-8' , 'replace' ) return result
2669	def cleanup_old_versions ( src , keep_last_versions , config_file = 'config.yaml' , profile_name = None , ) : if keep_last_versions <= 0 : print ( "Won't delete all versions. Please do this manually" ) else : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) response = client . list_versions_by_function ( FunctionName = cfg . get ( 'function_name' ) , ) versions = response . get ( 'Versions' ) if len ( response . get ( 'Versions' ) ) < keep_last_versions : print ( 'Nothing to delete. (Too few versions published)' ) else : version_numbers = [ elem . get ( 'Version' ) for elem in versions [ 1 : - keep_last_versions ] ] for version_number in version_numbers : try : client . delete_function ( FunctionName = cfg . get ( 'function_name' ) , Qualifier = version_number , ) except botocore . exceptions . ClientError as e : print ( 'Skipping Version {}: {}' . format ( version_number , e . message ) )
44	def parse_cmdline_kwargs ( args ) : def parse ( v ) : assert isinstance ( v , str ) try : return eval ( v ) except ( NameError , SyntaxError ) : return v return { k : parse ( v ) for k , v in parse_unknown_args ( args ) . items ( ) }
8919	def _get_service ( self ) : if "service" in self . document . attrib : value = self . document . attrib [ "service" ] . lower ( ) if value in allowed_service_types : self . params [ "service" ] = value else : raise OWSInvalidParameterValue ( "Service %s is not supported" % value , value = "service" ) else : raise OWSMissingParameterValue ( 'Parameter "service" is missing' , value = "service" ) return self . params [ "service" ]
4778	def is_not_empty ( self ) : if len ( self . val ) == 0 : if isinstance ( self . val , str_types ) : self . _err ( 'Expected not empty string, but was empty.' ) else : self . _err ( 'Expected not empty, but was empty.' ) return self
7289	def has_digit ( string_or_list , sep = "_" ) : if isinstance ( string_or_list , ( tuple , list ) ) : list_length = len ( string_or_list ) if list_length : return six . text_type ( string_or_list [ - 1 ] ) . isdigit ( ) else : return False else : return has_digit ( string_or_list . split ( sep ) )
6737	def get_component_settings ( prefixes = None ) : prefixes = prefixes or [ ] assert isinstance ( prefixes , ( tuple , list ) ) , 'Prefixes must be a sequence type, not %s.' % type ( prefixes ) data = { } for name in prefixes : name = name . lower ( ) . strip ( ) for k in sorted ( env ) : if k . startswith ( '%s_' % name ) : new_k = k [ len ( name ) + 1 : ] data [ new_k ] = env [ k ] return data
7700	def verify_roster_set ( self , fix = False , settings = None ) : # pylint: disable=R0912 try : self . _verify ( ( None , u"remove" ) , fix ) except ValueError , err : raise BadRequestProtocolError ( unicode ( err ) ) if self . ask : if fix : self . ask = None else : raise BadRequestProtocolError ( "'ask' in roster set" ) if self . approved : if fix : self . approved = False else : raise BadRequestProtocolError ( "'approved' in roster set" ) if settings is None : settings = XMPPSettings ( ) name_length_limit = settings [ "roster_name_length_limit" ] if self . name and len ( self . name ) > name_length_limit : raise NotAcceptableProtocolError ( u"Roster item name too long" ) group_length_limit = settings [ "roster_group_name_length_limit" ] for group in self . groups : if not group : raise NotAcceptableProtocolError ( u"Roster group name empty" ) if len ( group ) > group_length_limit : raise NotAcceptableProtocolError ( u"Roster group name too long" ) if self . _duplicate_group : raise BadRequestProtocolError ( u"Item group duplicated" )
2432	def add_creator ( self , doc , creator ) : if validations . validate_creator ( creator ) : doc . creation_info . add_creator ( creator ) return True else : raise SPDXValueError ( 'CreationInfo::Creator' )
8396	def trans_new ( name , transform , inverse , breaks = None , minor_breaks = None , _format = None , domain = ( - np . inf , np . inf ) , doc = '' , * * kwargs ) : def _get ( func ) : if isinstance ( func , ( classmethod , staticmethod , MethodType ) ) : return func else : return staticmethod ( func ) klass_name = '{}_trans' . format ( name ) d = { 'transform' : _get ( transform ) , 'inverse' : _get ( inverse ) , 'domain' : domain , '__doc__' : doc , * * kwargs } if breaks : d [ 'breaks_' ] = _get ( breaks ) if minor_breaks : d [ 'minor_breaks' ] = _get ( minor_breaks ) if _format : d [ 'format' ] = _get ( _format ) return type ( klass_name , ( trans , ) , d )
762	def getRandomWithMods ( inputSpace , maxChanges ) : size = len ( inputSpace ) ind = np . random . random_integers ( 0 , size - 1 , 1 ) [ 0 ] value = copy . deepcopy ( inputSpace [ ind ] ) if maxChanges == 0 : return value return modifyBits ( value , maxChanges )
1989	def save_stream ( self , key , binary = False ) : mode = 'wb' if binary else 'w' with open ( os . path . join ( self . uri , key ) , mode ) as f : yield f
11038	def maybe_key_vault ( client , mount_path ) : d = client . read_kv2 ( 'client_key' , mount_path = mount_path ) def get_or_create_key ( client_key ) : if client_key is not None : key_data = client_key [ 'data' ] [ 'data' ] key = _load_pem_private_key_bytes ( key_data [ 'key' ] . encode ( 'utf-8' ) ) return JWKRSA ( key = key ) else : key = generate_private_key ( u'rsa' ) key_data = { 'key' : _dump_pem_private_key_bytes ( key ) . decode ( 'utf-8' ) } d = client . create_or_update_kv2 ( 'client_key' , key_data , mount_path = mount_path ) return d . addCallback ( lambda _result : JWKRSA ( key = key ) ) return d . addCallback ( get_or_create_key )
12238	def rosenbrock ( theta ) : x , y = theta obj = ( 1 - x ) ** 2 + 100 * ( y - x ** 2 ) ** 2 grad = np . zeros ( 2 ) grad [ 0 ] = 2 * x - 400 * ( x * y - x ** 3 ) - 2 grad [ 1 ] = 200 * ( y - x ** 2 ) return obj , grad
3026	def save_to_well_known_file ( credentials , well_known_file = None ) : # TODO(orestica): move this method to tools.py # once the argparse import gets fixed (it is not present in Python 2.6) if well_known_file is None : well_known_file = _get_well_known_file ( ) config_dir = os . path . dirname ( well_known_file ) if not os . path . isdir ( config_dir ) : raise OSError ( 'Config directory does not exist: {0}' . format ( config_dir ) ) credentials_data = credentials . serialization_data _save_private_file ( well_known_file , credentials_data )
7089	def jd_corr ( jd , ra , dec , obslon = None , obslat = None , obsalt = None , jd_type = 'bjd' ) : if not HAVEKERNEL : LOGERROR ( 'no JPL kernel available, can\'t continue!' ) return # Source unit-vector ## Assume coordinates in ICRS ## Set distance to unit (kilometers) # convert the angles to degrees rarad = np . radians ( ra ) decrad = np . radians ( dec ) cosra = np . cos ( rarad ) sinra = np . sin ( rarad ) cosdec = np . cos ( decrad ) sindec = np . sin ( decrad ) # this assumes that the target is very far away src_unitvector = np . array ( [ cosdec * cosra , cosdec * sinra , sindec ] ) # Convert epochs to astropy.time.Time ## Assume JD(UTC) if ( obslon is None ) or ( obslat is None ) or ( obsalt is None ) : t = astime . Time ( jd , scale = 'utc' , format = 'jd' ) else : t = astime . Time ( jd , scale = 'utc' , format = 'jd' , location = ( '%.5fd' % obslon , '%.5fd' % obslat , obsalt ) ) # Get Earth-Moon barycenter position ## NB: jplephem uses Barycentric Dynamical Time, e.g. JD(TDB) ## and gives positions relative to solar system barycenter barycenter_earthmoon = jplkernel [ 0 , 3 ] . compute ( t . tdb . jd ) # Get Moon position vectors from the center of Earth to the Moon # this means we get the following vectors from the ephemerides # Earth Barycenter (3) -> Moon (301) # Earth Barycenter (3) -> Earth (399) # so the final vector is [3,301] - [3,399] # units are in km moonvector = ( jplkernel [ 3 , 301 ] . compute ( t . tdb . jd ) - jplkernel [ 3 , 399 ] . compute ( t . tdb . jd ) ) # Compute Earth position vectors (this is for the center of the earth with # respect to the solar system barycenter) # all these units are in km pos_earth = ( barycenter_earthmoon - moonvector * 1.0 / ( 1.0 + EMRAT ) ) if jd_type == 'bjd' : # Compute BJD correction ## Assume source vectors parallel at Earth and Solar System ## Barycenter ## i.e. source is at infinity # the romer_delay correction is (r.dot.n)/c where: # r is the vector from SSB to earth center # n is the unit vector from correction_seconds = np . dot ( pos_earth . T , src_unitvector ) / CLIGHT_KPS correction_days = correction_seconds / SEC_P_DAY elif jd_type == 'hjd' : # Compute HJD correction via Sun ephemeris # this is the position vector of the center of the sun in km # Solar System Barycenter (0) -> Sun (10) pos_sun = jplkernel [ 0 , 10 ] . compute ( t . tdb . jd ) # this is the vector from the center of the sun to the center of the # earth sun_earth_vec = pos_earth - pos_sun # calculate the heliocentric correction correction_seconds = np . dot ( sun_earth_vec . T , src_unitvector ) / CLIGHT_KPS correction_days = correction_seconds / SEC_P_DAY # TDB is the appropriate time scale for these ephemerides new_jd = t . tdb . jd + correction_days return new_jd
4602	def merge ( * projects ) : result = { } for project in projects : for name , section in ( project or { } ) . items ( ) : if name not in PROJECT_SECTIONS : raise ValueError ( UNKNOWN_SECTION_ERROR % name ) if section is None : result [ name ] = type ( result [ name ] ) ( ) continue if name in NOT_MERGEABLE + SPECIAL_CASE : result [ name ] = section continue if section and not isinstance ( section , ( dict , str ) ) : cname = section . __class__ . __name__ raise ValueError ( SECTION_ISNT_DICT_ERROR % ( name , cname ) ) if name == 'animation' : # Useful hack to allow you to load projects as animations. adesc = load . load_if_filename ( section ) if adesc : section = adesc . get ( 'animation' , { } ) section [ 'run' ] = adesc . get ( 'run' , { } ) result_section = result . setdefault ( name , { } ) section = construct . to_type ( section ) for k , v in section . items ( ) : if v is None : result_section . pop ( k , None ) else : result_section [ k ] = v return result
5096	def refresh_persistent_maps ( self ) : for robot in self . _robots : resp2 = ( requests . get ( urljoin ( self . ENDPOINT , 'users/me/robots/{}/persistent_maps' . format ( robot . serial ) ) , headers = self . _headers ) ) resp2 . raise_for_status ( ) self . _persistent_maps . update ( { robot . serial : resp2 . json ( ) } )
10440	def getcpustat ( self , process_name ) : # Create an instance of process stat _stat_inst = ProcessStats ( process_name ) _stat_list = [ ] for p in _stat_inst . get_cpu_memory_stat ( ) : try : _stat_list . append ( p . get_cpu_percent ( ) ) except psutil . AccessDenied : pass return _stat_list
4957	def parse_csv ( file_stream , expected_columns = None ) : reader = unicodecsv . DictReader ( file_stream , encoding = "utf-8" ) if expected_columns and set ( expected_columns ) - set ( reader . fieldnames ) : raise ValidationError ( ValidationMessages . MISSING_EXPECTED_COLUMNS . format ( expected_columns = ", " . join ( expected_columns ) , actual_columns = ", " . join ( reader . fieldnames ) ) ) # "yield from reader" would be nicer, but we're on python2.7 yet. for row in reader : yield row
2773	def create ( self , * args , * * kwargs ) : rules_dict = [ rule . __dict__ for rule in self . forwarding_rules ] params = { 'name' : self . name , 'region' : self . region , 'forwarding_rules' : rules_dict , 'redirect_http_to_https' : self . redirect_http_to_https } if self . droplet_ids and self . tag : raise ValueError ( 'droplet_ids and tag are mutually exclusive args' ) elif self . tag : params [ 'tag' ] = self . tag else : params [ 'droplet_ids' ] = self . droplet_ids if self . algorithm : params [ 'algorithm' ] = self . algorithm if self . health_check : params [ 'health_check' ] = self . health_check . __dict__ if self . sticky_sessions : params [ 'sticky_sessions' ] = self . sticky_sessions . __dict__ data = self . get_data ( 'load_balancers/' , type = POST , params = params ) if data : self . id = data [ 'load_balancer' ] [ 'id' ] self . ip = data [ 'load_balancer' ] [ 'ip' ] self . algorithm = data [ 'load_balancer' ] [ 'algorithm' ] self . health_check = HealthCheck ( * * data [ 'load_balancer' ] [ 'health_check' ] ) self . sticky_sessions = StickySesions ( * * data [ 'load_balancer' ] [ 'sticky_sessions' ] ) self . droplet_ids = data [ 'load_balancer' ] [ 'droplet_ids' ] self . status = data [ 'load_balancer' ] [ 'status' ] self . created_at = data [ 'load_balancer' ] [ 'created_at' ] return self
7921	def __prepare_domain ( data ) : # pylint: disable=R0912 if not data : raise JIDError ( "Domain must be given" ) data = unicode ( data ) if not data : raise JIDError ( "Domain must be given" ) if u'[' in data : if data [ 0 ] == u'[' and data [ - 1 ] == u']' : try : addr = _validate_ip_address ( socket . AF_INET6 , data [ 1 : - 1 ] ) return "[{0}]" . format ( addr ) except ValueError , err : logger . debug ( "ValueError: {0}" . format ( err ) ) raise JIDError ( u"Invalid IPv6 literal in JID domainpart" ) else : raise JIDError ( u"Invalid use of '[' or ']' in JID domainpart" ) elif data [ 0 ] . isdigit ( ) and data [ - 1 ] . isdigit ( ) : try : addr = _validate_ip_address ( socket . AF_INET , data ) except ValueError , err : logger . debug ( "ValueError: {0}" . format ( err ) ) data = UNICODE_DOT_RE . sub ( u"." , data ) data = data . rstrip ( u"." ) labels = data . split ( u"." ) try : labels = [ idna . nameprep ( label ) for label in labels ] except UnicodeError : raise JIDError ( u"Domain name invalid" ) for label in labels : if not STD3_LABEL_RE . match ( label ) : raise JIDError ( u"Domain name invalid" ) try : idna . ToASCII ( label ) except UnicodeError : raise JIDError ( u"Domain name invalid" ) domain = u"." . join ( labels ) if len ( domain . encode ( "utf-8" ) ) > 1023 : raise JIDError ( u"Domain name too long" ) return domain
7842	def set_category ( self , category ) : if not category : raise ValueError ( "Category is required in DiscoIdentity" ) category = unicode ( category ) self . xmlnode . setProp ( "category" , category . encode ( "utf-8" ) )
12566	def get_dataset ( self , ds_name , mode = 'r' ) : if ds_name in self . _datasets : return self . _datasets [ ds_name ] else : return self . create_empty_dataset ( ds_name )
6232	def calc_scene_bbox ( self ) : bbox_min , bbox_max = None , None for node in self . root_nodes : bbox_min , bbox_max = node . calc_global_bbox ( matrix44 . create_identity ( ) , bbox_min , bbox_max ) self . bbox_min = bbox_min self . bbox_max = bbox_max self . diagonal_size = vector3 . length ( self . bbox_max - self . bbox_min )
11918	def get_dataframe ( self ) : assert self . dataframe is not None , ( "'%s' should either include a `dataframe` attribute, " "or override the `get_dataframe()` method." % self . __class__ . __name__ ) dataframe = self . dataframe return dataframe
8044	def leapfrog ( self , kind , value = None ) : while self . current is not None : if self . current . kind == kind and ( value is None or self . current . value == value ) : self . consume ( kind ) return self . stream . move ( )
3257	def publish_featuretype ( self , name , store , native_crs , srs = None , jdbc_virtual_table = None , native_name = None ) : # @todo native_srs doesn't seem to get detected, even when in the DB # metadata (at least for postgis in geometry_columns) and then there # will be a misconfigured layer if native_crs is None : raise ValueError ( "must specify native_crs" ) srs = srs or native_crs feature_type = FeatureType ( self , store . workspace , store , name ) # because name is the in FeatureType base class, work around that # and hack in these others that don't have xml properties feature_type . dirty [ 'name' ] = name feature_type . dirty [ 'srs' ] = srs feature_type . dirty [ 'nativeCRS' ] = native_crs feature_type . enabled = True feature_type . advertised = True feature_type . title = name if native_name is not None : feature_type . native_name = native_name headers = { "Content-type" : "application/xml" , "Accept" : "application/xml" } resource_url = store . resource_url if jdbc_virtual_table is not None : feature_type . metadata = ( { 'JDBC_VIRTUAL_TABLE' : jdbc_virtual_table } ) params = dict ( ) resource_url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "datastores" , store . name , "featuretypes.xml" ] , params ) resp = self . http_request ( resource_url , method = 'post' , data = feature_type . message ( ) , headers = headers ) if resp . status_code not in ( 200 , 201 , 202 ) : FailedRequestError ( 'Failed to publish feature type {} : {}, {}' . format ( name , resp . status_code , resp . text ) ) self . _cache . clear ( ) feature_type . fetch ( ) return feature_type
4584	def image_to_colorlist ( image , container = list ) : deprecated . deprecated ( 'util.gif.image_to_colorlist' ) return container ( convert_mode ( image ) . getdata ( ) )
5397	def _delocalize_outputs_commands ( self , task_dir , outputs , user_project ) : commands = [ ] for o in outputs : if o . recursive or not o . value : continue # The destination path is o.uri.path, which is the target directory # (rather than o.uri, which includes the filename or wildcard). dest_path = o . uri . path local_path = task_dir + '/' + _DATA_SUBDIR + '/' + o . docker_path if o . file_provider == job_model . P_LOCAL : commands . append ( 'mkdir -p "%s"' % dest_path ) # Use gsutil even for local files (explained in _localize_inputs_command). if o . file_provider in [ job_model . P_LOCAL , job_model . P_GCS ] : if user_project : command = 'gsutil -u %s -mq cp "%s" "%s"' % ( user_project , local_path , dest_path ) else : command = 'gsutil -mq cp "%s" "%s"' % ( local_path , dest_path ) commands . append ( command ) return '\n' . join ( commands )
3829	async def remove_user ( self , remove_user_request ) : response = hangouts_pb2 . RemoveUserResponse ( ) await self . _pb_request ( 'conversations/removeuser' , remove_user_request , response ) return response
5601	def serve ( mapchete_file , port = None , internal_cache = None , zoom = None , bounds = None , overwrite = False , readonly = False , memory = False , input_file = None , debug = False , logfile = None ) : app = create_app ( mapchete_files = [ mapchete_file ] , zoom = zoom , bounds = bounds , single_input_file = input_file , mode = _get_mode ( memory , readonly , overwrite ) , debug = debug ) if os . environ . get ( "MAPCHETE_TEST" ) == "TRUE" : logger . debug ( "don't run flask app, MAPCHETE_TEST environment detected" ) else : app . run ( threaded = True , debug = True , port = port , host = '0.0.0.0' , extra_files = [ mapchete_file ] )
7705	def remove_item ( self , jid ) : if jid not in self . _jids : raise KeyError ( jid ) index = self . _jids [ jid ] for i in range ( index , len ( self . _jids ) ) : self . _jids [ self . _items [ i ] . jid ] -= 1 del self . _jids [ jid ] del self . _items [ index ]
8655	def search_messages ( session , thread_id , query , limit = 20 , offset = 0 , message_context_details = None , window_above = None , window_below = None ) : query = { 'thread_id' : thread_id , 'query' : query , 'limit' : limit , 'offset' : offset } if message_context_details : query [ 'message_context_details' ] = message_context_details if window_above : query [ 'window_above' ] = window_above if window_below : query [ 'window_below' ] = window_below # GET /api/messages/0.1/messages/search response = make_get_request ( session , 'messages/search' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MessagesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9603	def from_object ( cls , obj ) : return cls ( obj . get ( 'sessionId' , None ) , obj . get ( 'status' , 0 ) , obj . get ( 'value' , None ) )
8483	def set ( self , name , value ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) log . info ( " %s = %s" , name , repr ( value ) ) # Acquire our lock to change the config with self . mut_lock : self . settings [ name ] = value
8066	def get_source ( self , doc ) : start_iter = doc . get_start_iter ( ) end_iter = doc . get_end_iter ( ) source = doc . get_text ( start_iter , end_iter , False ) return source
9801	def config ( list ) : # pylint:disable=redefined-builtin if list : _config = GlobalConfigManager . get_config_or_default ( ) Printer . print_header ( 'Current config:' ) dict_tabulate ( _config . to_dict ( ) )
12110	def input_options ( self , options , prompt = 'Select option' , default = None ) : check_options = [ x . lower ( ) for x in options ] while True : response = input ( '%s [%s]: ' % ( prompt , ', ' . join ( options ) ) ) . lower ( ) if response in check_options : return response . strip ( ) elif response == '' and default is not None : return default . lower ( ) . strip ( )
8237	def right_complement ( clr ) : right = split_complementary ( clr ) [ 2 ] colors = complementary ( clr ) colors [ 3 ] . h = right . h colors [ 4 ] . h = right . h colors [ 5 ] . h = right . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 5 ] , colors [ 4 ] , colors [ 3 ] ) return colors
1708	def connect ( command , data = None , env = None , cwd = None ) : # TODO: support piped commands command_str = expand_args ( command ) . pop ( ) environ = dict ( os . environ ) environ . update ( env or { } ) process = subprocess . Popen ( command_str , universal_newlines = True , shell = False , env = environ , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = 0 , cwd = cwd , ) return ConnectedCommand ( process = process )
10147	def _ref ( self , param , base_name = None ) : name = base_name or param . get ( 'title' , '' ) or param . get ( 'name' , '' ) pointer = self . json_pointer + name self . parameter_registry [ name ] = param return { '$ref' : pointer }
9472	def validateRequest ( self , uri , postVars , expectedSignature ) : # append the POST variables sorted by key to the uri s = uri for k , v in sorted ( postVars . items ( ) ) : s += k + v # compute signature and compare signatures return ( base64 . encodestring ( hmac . new ( self . auth_token , s , sha1 ) . digest ( ) ) . strip ( ) == expectedSignature )
13017	def addHook ( self , name , callable ) : if name not in self . _hooks : self . _hooks [ name ] = [ ] self . _hooks [ name ] . append ( callable )
10125	def flip_y ( self , center = None ) : if center is None : self . poly . flop ( ) else : self . poly . flop ( center [ 1 ] ) return self
5291	def get ( self , request , * args , * * kwargs ) : form_class = self . get_form_class ( ) form = self . get_form ( form_class ) inlines = self . construct_inlines ( ) return self . render_to_response ( self . get_context_data ( form = form , inlines = inlines , * * kwargs ) )
3713	def calculate ( self , T , P , zs , ws , method ) : if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeGases ] return mixing_simple ( zs , Vms ) elif method == IDEAL : return ideal_gas ( T , P ) elif method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP_zs ( T = T , P = P , zs = zs ) return self . eos [ 0 ] . V_g else : raise Exception ( 'Method not valid' )
9224	def convergent_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] < 3 : if value < 0.0 : return - convergent_round ( - value ) epsilon = 0.0000001 integral_part , _ = divmod ( value , 1 ) if abs ( value - ( integral_part + 0.5 ) ) < epsilon : if integral_part % 2.0 < epsilon : return integral_part else : nearest_even = integral_part + 0.5 return math . ceil ( nearest_even ) return round ( value , ndigits )
5837	def tsne ( self , data_view_id ) : analysis = self . _data_analysis ( data_view_id ) projections = analysis [ 'projections' ] tsne = Tsne ( ) for k , v in projections . items ( ) : projection = Projection ( xs = v [ 'x' ] , ys = v [ 'y' ] , responses = v [ 'label' ] , tags = v [ 'inputs' ] , uids = v [ 'uid' ] ) tsne . add_projection ( k , projection ) return tsne
6418	def sim_editex ( src , tar , cost = ( 0 , 1 , 2 ) , local = False ) : return Editex ( ) . sim ( src , tar , cost , local )
2758	def get_floating_ip ( self , ip ) : return FloatingIP . get_object ( api_token = self . token , ip = ip )
3451	def find_blocked_reactions ( model , reaction_list = None , zero_cutoff = None , open_exchanges = False , processes = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) with model : if open_exchanges : for reaction in model . exchanges : reaction . bounds = ( min ( reaction . lower_bound , - 1000 ) , max ( reaction . upper_bound , 1000 ) ) if reaction_list is None : reaction_list = model . reactions # Limit the search space to reactions which have zero flux. If the # reactions already carry flux in this solution, # then they cannot be blocked. model . slim_optimize ( ) solution = get_solution ( model , reactions = reaction_list ) reaction_list = solution . fluxes [ solution . fluxes . abs ( ) < zero_cutoff ] . index . tolist ( ) # Run FVA to find reactions where both the minimal and maximal flux # are zero (below the cut off). flux_span = flux_variability_analysis ( model , fraction_of_optimum = 0. , reaction_list = reaction_list , processes = processes ) return flux_span [ flux_span . abs ( ) . max ( axis = 1 ) < zero_cutoff ] . index . tolist ( )
3755	def Carcinogen ( CASRN , AvailableMethods = False , Method = None ) : methods = [ COMBINED , IARC , NTP ] if AvailableMethods : return methods if not Method : Method = methods [ 0 ] if Method == IARC : if CASRN in IARC_data . index : status = IARC_codes [ IARC_data . at [ CASRN , 'group' ] ] else : status = UNLISTED elif Method == NTP : if CASRN in NTP_data . index : status = NTP_codes [ NTP_data . at [ CASRN , 'Listing' ] ] else : status = UNLISTED elif Method == COMBINED : status = { } for method in methods [ 1 : ] : status [ method ] = Carcinogen ( CASRN , Method = method ) else : raise Exception ( 'Failure in in function' ) return status
1142	def _long2bytesBigEndian ( n , blocksize = 0 ) : # After much testing, this algorithm was deemed to be the fastest. s = b'' pack = struct . pack while n > 0 : s = pack ( '>I' , n & 0xffffffff ) + s n = n >> 32 # Strip off leading zeros. for i in range ( len ( s ) ) : if s [ i ] != '\000' : break else : # Only happens when n == 0. s = '\000' i = 0 s = s [ i : ] # Add back some pad bytes. This could be done more efficiently # w.r.t. the de-padding being done above, but sigh... if blocksize > 0 and len ( s ) % blocksize : s = ( blocksize - len ( s ) % blocksize ) * '\000' + s return s
7624	def pattern_to_mireval ( ann ) : # It's easier to work with dictionaries, since we can't assume # sequential pattern or occurrence identifiers patterns = defaultdict ( lambda : defaultdict ( list ) ) # Iterate over the data in interval-value format for time , observation in zip ( * ann . to_event_values ( ) ) : pattern_id = observation [ 'pattern_id' ] occurrence_id = observation [ 'occurrence_id' ] obs = ( time , observation [ 'midi_pitch' ] ) # Push this note observation into the correct pattern/occurrence patterns [ pattern_id ] [ occurrence_id ] . append ( obs ) # Convert to list-list-tuple format for mir_eval return [ list ( _ . values ( ) ) for _ in six . itervalues ( patterns ) ]
2718	def remove_droplets ( self , droplet ) : droplets = droplet if not isinstance ( droplets , list ) : droplets = [ droplet ] # Extracting data from the Droplet object resources = self . __extract_resources_from_droplets ( droplets ) if len ( resources ) > 0 : return self . __remove_resources ( resources ) return False
11600	def get_queryset ( self , request ) : qs = super ( GalleryAdmin , self ) . get_queryset ( request ) return qs . annotate ( photo_count = Count ( 'photos' ) )
376	def imresize ( x , size = None , interp = 'bicubic' , mode = None ) : if size is None : size = [ 100 , 100 ] if x . shape [ - 1 ] == 1 : # greyscale x = scipy . misc . imresize ( x [ : , : , 0 ] , size , interp = interp , mode = mode ) return x [ : , : , np . newaxis ] else : # rgb, bgr, rgba return scipy . misc . imresize ( x , size , interp = interp , mode = mode )
7026	def objectlist_conesearch ( racenter , declcenter , searchradiusarcsec , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g_mean_mag' , 'l' , 'b' , 'parallax' , 'parallax_error' , 'pmra' , 'pmra_error' , 'pmdec' , 'pmdec_error' ) , extra_filter = None , returnformat = 'csv' , forcefetch = False , cachedir = '~/.astrobase/gaia-cache' , verbose = True , timeout = 15.0 , refresh = 2.0 , maxtimeout = 300.0 , maxtries = 3 , complete_query_later = True ) : # this was generated using the awesome query generator at: # https://gea.esac.esa.int/archive/ # NOTE: here we don't resolve the table name right away. this is because # some of the GAIA mirrors use different table names, so we leave the table # name to be resolved by the lower level tap_query function. this is done by # the {{table}} construct. query = ( "select {columns}, " "(DISTANCE(POINT('ICRS', " "{{table}}.ra, {{table}}.dec), " "POINT('ICRS', {ra_center:.5f}, {decl_center:.5f})))*3600.0 " "AS dist_arcsec " "from {{table}} where " "CONTAINS(POINT('ICRS',{{table}}.ra, {{table}}.dec)," "CIRCLE('ICRS',{ra_center:.5f},{decl_center:.5f}," "{search_radius:.6f}))=1 " "{extra_filter_str}" "ORDER by dist_arcsec asc " ) if extra_filter is not None : extra_filter_str = ' and %s ' % extra_filter else : extra_filter_str = '' formatted_query = query . format ( ra_center = racenter , decl_center = declcenter , search_radius = searchradiusarcsec / 3600.0 , extra_filter_str = extra_filter_str , columns = ', ' . join ( columns ) ) return tap_query ( formatted_query , gaia_mirror = gaia_mirror , returnformat = returnformat , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , complete_query_later = complete_query_later )
8914	def list_services ( self ) : my_services = [ ] for service in self . name_index . values ( ) : my_services . append ( Service ( service ) ) return my_services
8231	def speed ( self , framerate = None ) : if framerate is not None : self . _speed = framerate self . _dynamic = True else : return self . _speed
7012	def lcdict_to_pickle ( lcdict , outfile = None ) : if not outfile and lcdict [ 'objectid' ] : outfile = '%s-hplc.pkl' % lcdict [ 'objectid' ] elif not outfile and not lcdict [ 'objectid' ] : outfile = 'hplc.pkl' with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) if os . path . exists ( outfile ) : LOGINFO ( 'lcdict for object: %s -> %s OK' % ( lcdict [ 'objectid' ] , outfile ) ) return outfile else : LOGERROR ( 'could not make a pickle for this lcdict!' ) return None
2993	def otcSymbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( otcSymbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
4635	def child ( self , offset256 ) : a = bytes ( self . pubkey ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . derive_from_seed ( s )
118	def imap_batches_unordered ( self , batches , chunksize = 1 ) : assert ia . is_generator ( batches ) , ( "Expected to get a generator as 'batches', got type %s. " + "Call map_batches() if you use lists." ) % ( type ( batches ) , ) # TODO change this to 'yield from' once switched to 3.3+ gen = self . pool . imap_unordered ( _Pool_starworker , self . _handle_batch_ids_gen ( batches ) , chunksize = chunksize ) for batch in gen : yield batch
2672	def upload ( src , requirements = None , local_package = None , config_file = 'config.yaml' , profile_name = None , ) : # Load and parse the config file. path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) # Copy all the pip dependencies required to run your code into a temporary # folder then add the handler file in the root of this directory. # Zip the contents of this folder into a single file and output to the dist # directory. path_to_zip_file = build ( src , config_file = config_file , requirements = requirements , local_package = local_package , ) upload_s3 ( cfg , path_to_zip_file )
8112	def hash ( self , id ) : h = md5 ( id ) . hexdigest ( ) return os . path . join ( self . path , h + self . type )
12615	def count ( self , table_name , sample ) : return len ( list ( search_sample ( table = self . table ( table_name ) , sample = sample ) ) )
546	def _writePrediction ( self , result ) : self . __predictionCache . append ( result ) if self . _isBestModel : self . __flushPredictionCache ( )
4727	def s20_to_gen ( self , pugrp , punit , chunk , sectr ) : cmd = [ "nvm_addr s20_to_gen" , self . envs [ "DEV_PATH" ] , "%d %d %d %d" % ( pugrp , punit , chunk , sectr ) ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.liblight.s20_to_gen: cmd fail" ) return int ( re . findall ( r"val: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
4674	def getPrivateKeyForPublicKey ( self , pub ) : if str ( pub ) not in self . store : raise KeyNotFound return self . store . getPrivateKeyForPublicKey ( str ( pub ) )
13174	def next ( self , name = None ) : if self . parent is None or self . index is None : return None for idx in xrange ( self . index + 1 , len ( self . parent ) ) : if name is None or self . parent [ idx ] . tagname == name : return self . parent [ idx ]
9355	def money ( min = 0 , max = 10 ) : value = random . choice ( range ( min * 100 , max * 100 ) ) return "%1.2f" % ( float ( value ) / 100 )
1754	def write_register ( self , register , value ) : self . _publish ( 'will_write_register' , register , value ) value = self . _regfile . write ( register , value ) self . _publish ( 'did_write_register' , register , value ) return value
2103	def _produce_raw_method ( self ) : def method ( res_self , * * kwargs ) : obj_pk = kwargs . get ( method . _res_name ) other_obj_pk = kwargs . get ( method . _other_name ) internal_method = getattr ( res_self , method . _internal_name ) return internal_method ( method . _relationship , obj_pk , other_obj_pk ) return method
3428	def remove_metabolites ( self , metabolite_list , destructive = False ) : if not hasattr ( metabolite_list , '__iter__' ) : metabolite_list = [ metabolite_list ] # Make sure metabolites exist in model metabolite_list = [ x for x in metabolite_list if x . id in self . metabolites ] for x in metabolite_list : x . _model = None # remove reference to the metabolite in all groups associated_groups = self . get_associated_groups ( x ) for group in associated_groups : group . remove_members ( x ) if not destructive : for the_reaction in list ( x . _reaction ) : the_coefficient = the_reaction . _metabolites [ x ] the_reaction . subtract_metabolites ( { x : the_coefficient } ) else : for x in list ( x . _reaction ) : x . remove_from_model ( ) self . metabolites -= metabolite_list to_remove = [ self . solver . constraints [ m . id ] for m in metabolite_list ] self . remove_cons_vars ( to_remove ) context = get_context ( self ) if context : context ( partial ( self . metabolites . __iadd__ , metabolite_list ) ) for x in metabolite_list : context ( partial ( setattr , x , '_model' , self ) )
8236	def left_complement ( clr ) : left = split_complementary ( clr ) [ 1 ] colors = complementary ( clr ) colors [ 3 ] . h = left . h colors [ 4 ] . h = left . h colors [ 5 ] . h = left . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 3 ] , colors [ 4 ] , colors [ 5 ] ) return colors
5345	def compose_git ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'source_repo' ] ) > 0 ] : repos = [ ] for url in data [ p ] [ 'source_repo' ] : if len ( url [ 'url' ] . split ( ) ) > 1 : # Error at upstream the project 'tools.corrosion' repo = url [ 'url' ] . split ( ) [ 1 ] . replace ( '/c/' , '/gitroot/' ) else : repo = url [ 'url' ] . replace ( '/c/' , '/gitroot/' ) if repo not in repos : repos . append ( repo ) projects [ p ] [ 'git' ] = repos return projects
5796	def handle_sec_error ( error , exception_class = None ) : if error == 0 : return if error in set ( [ SecurityConst . errSSLClosedNoNotify , SecurityConst . errSSLClosedAbort ] ) : raise TLSDisconnectError ( 'The remote end closed the connection' ) if error == SecurityConst . errSSLClosedGraceful : raise TLSGracefulDisconnectError ( 'The remote end closed the connection' ) cf_error_string = Security . SecCopyErrorMessageString ( error , null ( ) ) output = CFHelpers . cf_string_to_unicode ( cf_error_string ) CoreFoundation . CFRelease ( cf_error_string ) if output is None or output == '' : output = 'OSStatus %s' % error if exception_class is None : exception_class = OSError raise exception_class ( output )
11804	def assign ( self , var , val , assignment ) : oldval = assignment . get ( var , None ) if val != oldval : if oldval is not None : # Remove old val if there was one self . record_conflict ( assignment , var , oldval , - 1 ) self . record_conflict ( assignment , var , val , + 1 ) CSP . assign ( self , var , val , assignment )
8461	def get_cookiecutter_config ( template , default_config = None , version = None ) : default_config = default_config or { } config_dict = cc_config . get_user_config ( ) repo_dir , _ = cc_repository . determine_repo_dir ( template = template , abbreviations = config_dict [ 'abbreviations' ] , clone_to_dir = config_dict [ 'cookiecutters_dir' ] , checkout = version , no_input = True ) context_file = os . path . join ( repo_dir , 'cookiecutter.json' ) context = cc_generate . generate_context ( context_file = context_file , default_context = { * * config_dict [ 'default_context' ] , * * default_config } ) return repo_dir , cc_prompt . prompt_for_config ( context )
4795	def does_not_contain_value ( self , * values ) : self . _check_dict_like ( self . val , check_getitem = False ) if len ( values ) == 0 : raise ValueError ( 'one or more value args must be given' ) else : found = [ ] for v in values : if v in self . val . values ( ) : found . append ( v ) if found : self . _err ( 'Expected <%s> to not contain values %s, but did contain %s.' % ( self . val , self . _fmt_items ( values ) , self . _fmt_items ( found ) ) ) return self
5973	def generate_submit_array ( templates , directories , * * kwargs ) : dirname = kwargs . setdefault ( 'dirname' , os . path . curdir ) reldirs = [ relpath ( p , start = dirname ) for p in asiterable ( directories ) ] missing = [ p for p in ( os . path . join ( dirname , subdir ) for subdir in reldirs ) if not os . path . exists ( p ) ] if len ( missing ) > 0 : logger . debug ( "template=%(template)r: dirname=%(dirname)r reldirs=%(reldirs)r" , vars ( ) ) logger . error ( "Some directories are not accessible from the array script: " "%(missing)r" , vars ( ) ) def write_script ( template ) : qsystem = detect_queuing_system ( template ) if qsystem is None or not qsystem . has_arrays ( ) : logger . warning ( "Not known how to make a job array for %(template)r; skipping..." , vars ( ) ) return None kwargs [ 'jobarray_string' ] = qsystem . array ( reldirs ) return generate_submit_scripts ( template , * * kwargs ) [ 0 ] # returns list of length 1 # must use config.get_templates() because we need to access the file for detecting return [ write_script ( template ) for template in config . get_templates ( templates ) ]
8386	def amend_filename ( filename , amend ) : base , ext = os . path . splitext ( filename ) amended_name = base + amend + ext return amended_name
6819	def sync_media ( self , sync_set = None , clean = 0 , iter_local_paths = 0 ) : # Ensure a site is selected. self . genv . SITE = self . genv . SITE or self . genv . default_site r = self . local_renderer clean = int ( clean ) self . vprint ( 'Getting site data for %s...' % self . genv . SITE ) self . set_site_specifics ( self . genv . SITE ) sync_sets = r . env . sync_sets if sync_set : sync_sets = [ sync_set ] ret_paths = [ ] for _sync_set in sync_sets : for paths in r . env . sync_sets [ _sync_set ] : r . env . sync_local_path = os . path . abspath ( paths [ 'local_path' ] % self . genv ) if paths [ 'local_path' ] . endswith ( '/' ) and not r . env . sync_local_path . endswith ( '/' ) : r . env . sync_local_path += '/' if iter_local_paths : ret_paths . append ( r . env . sync_local_path ) continue r . env . sync_remote_path = paths [ 'remote_path' ] % self . genv if clean : r . sudo ( 'rm -Rf {apache_sync_remote_path}' ) print ( 'Syncing %s to %s...' % ( r . env . sync_local_path , r . env . sync_remote_path ) ) r . env . tmp_chmod = paths . get ( 'chmod' , r . env . chmod ) r . sudo ( 'mkdir -p {apache_sync_remote_path}' ) r . sudo ( 'chmod -R {apache_tmp_chmod} {apache_sync_remote_path}' ) r . local ( 'rsync -rvz --progress --recursive --no-p --no-g ' '--rsh "ssh -o StrictHostKeyChecking=no -i {key_filename}" {apache_sync_local_path} {user}@{host_string}:{apache_sync_remote_path}' ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {apache_sync_remote_path}' ) if iter_local_paths : return ret_paths
489	def close ( self ) : self . _logger . info ( "Closing" ) if self . _conn is not None : self . _conn . close ( ) self . _conn = None else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
3011	def locked_get ( self ) : filters = { self . key_name : self . key_value } query = self . session . query ( self . model_class ) . filter_by ( * * filters ) entity = query . first ( ) if entity : credential = getattr ( entity , self . property_name ) if credential and hasattr ( credential , 'set_store' ) : credential . set_store ( self ) return credential else : return None
550	def __checkCancelation ( self ) : # Update a hadoop job counter at least once every 600 seconds so it doesn't # think our map task is dead print >> sys . stderr , "reporter:counter:HypersearchWorker,numRecords,50" # See if the job got cancelled jobCancel = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'cancel' ] ) [ 0 ] if jobCancel : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isCanceled = True self . _logger . info ( "Model %s canceled because Job %s was stopped." , self . _modelID , self . _jobID ) else : stopReason = self . _jobsDAO . modelsGetFields ( self . _modelID , [ 'engStop' ] ) [ 0 ] if stopReason is None : pass elif stopReason == ClientJobsDAO . STOP_REASON_KILLED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isKilled = True self . _logger . info ( "Model %s canceled because it was killed by hypersearch" , self . _modelID ) elif stopReason == ClientJobsDAO . STOP_REASON_STOPPED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isCanceled = True self . _logger . info ( "Model %s stopped because hypersearch ended" , self . _modelID ) else : raise RuntimeError ( "Unexpected stop reason encountered: %s" % ( stopReason ) )
4166	def tf2zp ( b , a ) : from numpy import roots assert len ( b ) == len ( a ) , "length of the vectors a and b must be identical. fill with zeros if needed." g = b [ 0 ] / a [ 0 ] z = roots ( b ) p = roots ( a ) return z , p , g
9021	def _create_pattern_set ( self , pattern , values ) : type_ = self . _get_type ( values ) version = self . _get_version ( values ) comment = values . get ( COMMENT ) self . _pattern_set = self . _spec . new_pattern_set ( type_ , version , pattern , self , comment )
3337	def join_uri ( uri , * segments ) : sub = "/" . join ( segments ) if not sub : return uri return uri . rstrip ( "/" ) + "/" + sub
216	def setdefault ( self , key : str , value : str ) -> str : set_key = key . lower ( ) . encode ( "latin-1" ) set_value = value . encode ( "latin-1" ) for idx , ( item_key , item_value ) in enumerate ( self . _list ) : if item_key == set_key : return item_value . decode ( "latin-1" ) self . _list . append ( ( set_key , set_value ) ) return value
13725	def register_credentials ( self , credentials = None , user = None , user_file = None , password = None , password_file = None ) : # lets store all kind of credential data into this dict if credentials is not None : self . credentials = credentials else : self . credentials = { } # set the user from CLI or file if user : self . credentials [ "user" ] = user elif user_file : with open ( user_file , "r" ) as of : # what would the file entry look like? pattern = re . compile ( "^user: " ) for l in of : if re . match ( pattern , l ) : # strip away the newline l = l [ 0 : - 1 ] self . credentials [ "user" ] = re . sub ( pattern , "" , l ) # remove any surrounding quotes if self . credentials [ "user" ] [ 0 : 1 ] == '"' and self . credentials [ "user" ] [ - 1 : ] == '"' : self . credentials [ "user" ] = self . credentials [ "user" ] [ 1 : - 1 ] # set the password from CLI or file if password : self . credentials [ "password" ] = password elif password_file : with open ( password_file , "r" ) as of : # what would the file entry look like? pattern = re . compile ( "^password: " ) for l in of : if re . match ( pattern , l ) : # strip away the newline l = l [ 0 : - 1 ] self . credentials [ "password" ] = re . sub ( pattern , "" , l ) # remove any surrounding quotes if self . credentials [ "password" ] [ 0 : 1 ] == '"' and self . credentials [ "password" ] [ - 1 : ] == '"' : self . credentials [ "password" ] = self . credentials [ "password" ] [ 1 : - 1 ] # if both user and password is set, # 1. encode to base 64 for basic auth if "user" in self . credentials and "password" in self . credentials : c = self . credentials [ "user" ] + ":" + self . credentials [ "password" ] self . credentials [ "base64" ] = b64encode ( c . encode ( ) ) . decode ( "ascii" )
6844	def create_supervisor_services ( self , site ) : self . vprint ( 'create_supervisor_services:' , site ) self . set_site_specifics ( site = site ) r = self . local_renderer if self . verbose : print ( 'r.env:' ) pprint ( r . env , indent = 4 ) self . vprint ( 'r.env.has_worker:' , r . env . has_worker ) if not r . env . has_worker : self . vprint ( 'skipping: no celery worker' ) return if self . name . lower ( ) not in self . genv . services : self . vprint ( 'skipping: celery not enabled' ) return hostname = self . current_hostname target_sites = self . genv . available_sites_by_host . get ( hostname , None ) if target_sites and site not in target_sites : self . vprint ( 'skipping: site not supported on this server' ) return self . render_paths ( ) conf_name = 'celery_%s.conf' % site ret = r . render_to_string ( 'celery/celery_supervisor.template.conf' ) return conf_name , ret
10706	def get_vacations ( ) : arequest = requests . get ( VACATIONS_URL , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
12820	def _filename ( draw , result_type = None ) : # Various ASCII chars have a special meaning for the operating system, # so make them more common ascii_char = characters ( min_codepoint = 0x01 , max_codepoint = 0x7f ) if os . name == 'nt' : # pragma: no cover # Windows paths can contain all surrogates and even surrogate pairs # if two paths are concatenated. This makes it more likely for them to # be generated. surrogate = characters ( min_codepoint = 0xD800 , max_codepoint = 0xDFFF ) uni_char = characters ( min_codepoint = 0x1 ) text_strategy = text ( alphabet = one_of ( uni_char , surrogate , ascii_char ) ) def text_to_bytes ( path ) : fs_enc = sys . getfilesystemencoding ( ) try : return path . encode ( fs_enc , 'surrogatepass' ) except UnicodeEncodeError : return path . encode ( fs_enc , 'replace' ) bytes_strategy = text_strategy . map ( text_to_bytes ) else : latin_char = characters ( min_codepoint = 0x01 , max_codepoint = 0xff ) bytes_strategy = text ( alphabet = one_of ( latin_char , ascii_char ) ) . map ( lambda t : t . encode ( 'latin-1' ) ) unix_path_text = bytes_strategy . map ( lambda b : b . decode ( sys . getfilesystemencoding ( ) , 'surrogateescape' if PY3 else 'ignore' ) ) # Two surrogates generated through surrogateescape can generate # a valid utf-8 sequence when encoded and result in a different # code point when decoded again. Can happen when two paths get # concatenated. Shuffling makes it possible to generate such a case. text_strategy = permutations ( draw ( unix_path_text ) ) . map ( u"" . join ) if result_type is None : return draw ( one_of ( bytes_strategy , text_strategy ) ) elif result_type is bytes : return draw ( bytes_strategy ) else : return draw ( text_strategy )
7715	def update_item ( self , jid , name = NO_CHANGE , groups = NO_CHANGE , callback = None , error_callback = None ) : # pylint: disable=R0913 item = self . roster [ jid ] if name is NO_CHANGE and groups is NO_CHANGE : return if name is NO_CHANGE : name = item . name if groups is NO_CHANGE : groups = item . groups item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
10947	def reset ( self ) : inds = list ( range ( self . state . obj_get_positions ( ) . shape [ 0 ] ) ) self . _rad_nms = self . state . param_particle_rad ( inds ) self . _pos_nms = self . state . param_particle_pos ( inds ) self . _initial_rad = np . copy ( self . state . state [ self . _rad_nms ] ) self . _initial_pos = np . copy ( self . state . state [ self . _pos_nms ] ) . reshape ( ( - 1 , 3 ) ) self . param_vals [ self . rscale_mask ] = 0
8935	def provider ( workdir , commit = True , * * kwargs ) : return SCM_PROVIDER [ auto_detect ( workdir ) ] ( workdir , commit = commit , * * kwargs )
12694	def is_disjoint ( set1 , set2 , warn ) : for elem in set2 : if elem in set1 : raise ValueError ( warn ) return True
2143	def ordered_dump ( data , Dumper = yaml . Dumper , * * kws ) : class OrderedDumper ( Dumper ) : pass def _dict_representer ( dumper , data ) : return dumper . represent_mapping ( yaml . resolver . BaseResolver . DEFAULT_MAPPING_TAG , data . items ( ) ) OrderedDumper . add_representer ( OrderedDict , _dict_representer ) return yaml . dump ( data , None , OrderedDumper , * * kws )
8763	def update_security_group_rule ( context , id , security_group_rule ) : LOG . info ( "update_security_group_rule for tenant %s" % ( context . tenant_id ) ) new_rule = security_group_rule [ "security_group_rule" ] # Only allow updatable fields new_rule = _filter_update_security_group_rule ( new_rule ) with context . session . begin ( ) : rule = db_api . security_group_rule_find ( context , id = id , scope = db_api . ONE ) if not rule : raise sg_ext . SecurityGroupRuleNotFound ( id = id ) db_rule = db_api . security_group_rule_update ( context , rule , * * new_rule ) group_id = db_rule . group_id group = db_api . security_group_find ( context , id = group_id , scope = db_api . ONE ) if not group : raise sg_ext . SecurityGroupNotFound ( id = group_id ) if group : _perform_async_update_rule ( context , group_id , group , rule . id , RULE_UPDATE ) return v . _make_security_group_rule_dict ( db_rule )
4729	def __run ( self , shell = True , echo = True ) : if env ( ) : return 1 cij . emph ( "cij.dmesg.start: shell: %r, cmd: %r" % ( shell , self . __prefix + self . __suffix ) ) return cij . ssh . command ( self . __prefix , shell , echo , self . __suffix )
8631	def get_project_by_id ( session , project_id , project_details = None , user_details = None ) : # GET /api/projects/0.1/projects/<int:project_id> query = { } if project_details : query . update ( project_details ) if user_details : query . update ( user_details ) response = make_get_request ( session , 'projects/{}' . format ( project_id ) , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
1537	def add_spec ( self , * specs ) : for spec in specs : if not isinstance ( spec , HeronComponentSpec ) : raise TypeError ( "Argument to add_spec needs to be HeronComponentSpec, given: %s" % str ( spec ) ) if spec . name is None : raise ValueError ( "TopologyBuilder cannot take a spec without name" ) if spec . name == "config" : raise ValueError ( "config is a reserved name" ) if spec . name in self . _specs : raise ValueError ( "Attempting to add duplicate spec name: %r %r" % ( spec . name , spec ) ) self . _specs [ spec . name ] = spec
6703	def enter_password_change ( self , username = None , old_password = None ) : from fabric . state import connections from fabric . network import disconnect_all r = self . local_renderer # print('self.genv.user:', self.genv.user) # print('self.env.passwords:', self.env.passwords) r . genv . user = r . genv . user or username r . pc ( 'Changing password for user {user} via interactive prompts.' ) r . env . old_password = r . env . default_passwords [ self . genv . user ] # print('self.genv.user:', self.genv.user) # print('self.env.passwords:', self.env.passwords) r . env . new_password = self . env . passwords [ self . genv . user ] if old_password : r . env . old_password = old_password prompts = { '(current) UNIX password: ' : r . env . old_password , 'Enter new UNIX password: ' : r . env . new_password , 'Retype new UNIX password: ' : r . env . new_password , #"Login password for '%s': " % r.genv.user: r.env.new_password, # "Login password for '%s': " % r.genv.user: r.env.old_password, } print ( 'prompts:' , prompts ) r . env . password = r . env . old_password with self . settings ( warn_only = True ) : ret = r . _local ( "sshpass -p '{password}' ssh -o StrictHostKeyChecking=no {user}@{host_string} echo hello" , capture = True ) #code 1 = good password, but prompts needed #code 5 = bad password #code 6 = good password, but host public key is unknown if ret . return_code in ( 1 , 6 ) or 'hello' in ret : # Login succeeded, so we haven't yet changed the password, so use the default password. self . genv . password = r . env . old_password elif self . genv . user in self . genv . user_passwords : # Otherwise, use the password or key set in the config. self . genv . password = r . env . new_password else : # Default password fails and there's no current password, so clear. self . genv . password = None print ( 'using password:' , self . genv . password ) # Note, the correct current password should be set in host.initrole(), not here. #r.genv.password = r.env.new_password #r.genv.password = r.env.new_password with self . settings ( prompts = prompts ) : ret = r . _run ( 'echo checking for expired password' ) print ( 'ret:[%s]' % ret ) do_disconnect = 'passwd: password updated successfully' in ret print ( 'do_disconnect:' , do_disconnect ) if do_disconnect : # We need to disconnect to reset the session or else Linux will again prompt # us to change our password. disconnect_all ( ) # Further logins should require the new password. self . genv . password = r . env . new_password
13565	def register ( app ) : # Pick a handler based on the requested format. Currently we assume the # caller wants JSON. error_handler = json . http_exception_error_handler @ app . errorhandler ( 400 ) def handle_bad_request ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 404 ) def handle_not_found ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 405 ) def handle_method_not_allowed ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 422 ) def handle_unprocessable_entity ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 500 ) def handle_internal_server_error ( exception ) : return error_handler ( exception )
10318	def _microcanonical_average_spanning_cluster ( has_spanning_cluster , alpha ) : ret = dict ( ) runs = has_spanning_cluster . size # Bayesian posterior mean for Binomial proportion (uniform prior) k = has_spanning_cluster . sum ( dtype = np . float ) ret [ 'spanning_cluster' ] = ( ( k + 1 ) / ( runs + 2 ) ) # Bayesian credible interval for Binomial proportion (uniform # prior) ret [ 'spanning_cluster_ci' ] = scipy . stats . beta . ppf ( [ alpha / 2 , 1 - alpha / 2 ] , k + 1 , runs - k + 1 ) return ret
1839	def JNE ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . ZF , target . read ( ) , cpu . PC )
6171	def freq_resp ( self , mode = 'dB' , fs = 8000 , ylim = [ - 100 , 2 ] ) : iir_d . freqz_resp_cas_list ( [ self . sos ] , mode , fs = fs ) pylab . grid ( ) pylab . ylim ( ylim )
5975	def anumb_to_atom ( self , anumb ) : assert isinstance ( anumb , int ) , "anumb must be integer" if not self . _anumb_to_atom : # empty dictionary if self . atoms : for atom in self . atoms : self . _anumb_to_atom [ atom . number ] = atom return self . _anumb_to_atom [ anumb ] else : self . logger ( "no atoms in the molecule" ) return False else : if anumb in self . _anumb_to_atom : return self . _anumb_to_atom [ anumb ] else : self . logger ( "no such atom number ({0:d}) in the molecule" . format ( anumb ) ) return False
4013	def _ensure_managed_repos_dir_exists ( ) : if not os . path . exists ( constants . REPOS_DIR ) : os . makedirs ( constants . REPOS_DIR )
3687	def a_alpha_and_derivatives ( self , T , full = True , quick = True ) : if not full : return self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 else : if quick : Tc , kappa = self . Tc , self . kappa x0 = T ** 0.5 x1 = Tc ** - 0.5 x2 = kappa * ( x0 * x1 - 1. ) - 1. x3 = self . a * kappa a_alpha = self . a * x2 * x2 da_alpha_dT = x1 * x2 * x3 / x0 d2a_alpha_dT2 = x3 * ( - 0.5 * T ** - 1.5 * x1 * x2 + 0.5 / ( T * Tc ) * kappa ) else : a_alpha = self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 da_alpha_dT = - self . a * self . kappa * sqrt ( T / self . Tc ) * ( self . kappa * ( - sqrt ( T / self . Tc ) + 1. ) + 1. ) / T d2a_alpha_dT2 = self . a * self . kappa * ( self . kappa / self . Tc - sqrt ( T / self . Tc ) * ( self . kappa * ( sqrt ( T / self . Tc ) - 1. ) - 1. ) / T ) / ( 2. * T ) return a_alpha , da_alpha_dT , d2a_alpha_dT2
12884	def field_type ( self ) : if not self . model : return 'JSON' database = self . model . _meta . database if isinstance ( database , Proxy ) : database = database . obj if Json and isinstance ( database , PostgresqlDatabase ) : return 'JSON' return 'TEXT'
7166	def load_entity ( self , name , file_name , reload_cache = False ) : Entity . verify_name ( name ) self . entities . load ( Entity . wrap_name ( name ) , file_name , reload_cache ) with open ( file_name ) as f : self . padaos . add_entity ( name , f . read ( ) . split ( '\n' ) ) self . must_train = True
7644	def can_convert ( annotation , target_namespace ) : # If we're already in the target namespace, do nothing if annotation . namespace == target_namespace : return True if target_namespace in __CONVERSION__ : # Look for a way to map this namespace to the target for source in __CONVERSION__ [ target_namespace ] : if annotation . search ( namespace = source ) : return True return False
12418	def capture_stdout ( ) : stdout = sys . stdout try : capture_out = StringIO ( ) sys . stdout = capture_out yield capture_out finally : sys . stdout = stdout
9625	def register ( self , cls ) : preview = cls ( site = self ) logger . debug ( 'Registering %r with %r' , preview , self ) index = self . __previews . setdefault ( preview . module , { } ) index [ cls . __name__ ] = preview
12547	def abs_img ( img ) : bool_img = np . abs ( read_img ( img ) . get_data ( ) ) return bool_img . astype ( int )
5223	def ccy_pair ( local , base = 'USD' ) -> CurrencyPair : ccy_param = param . load_info ( cat = 'ccy' ) if f'{local}{base}' in ccy_param : info = ccy_param [ f'{local}{base}' ] elif f'{base}{local}' in ccy_param : info = ccy_param [ f'{base}{local}' ] info [ 'factor' ] = 1. / info . get ( 'factor' , 1. ) info [ 'power' ] = - info . get ( 'power' , 1 ) elif base . lower ( ) == local . lower ( ) : info = dict ( ticker = '' ) info [ 'factor' ] = 1. if base [ - 1 ] . lower ( ) == base [ - 1 ] : info [ 'factor' ] /= 100. if local [ - 1 ] . lower ( ) == local [ - 1 ] : info [ 'factor' ] *= 100. else : logger = logs . get_logger ( ccy_pair ) logger . error ( f'incorrect currency - local {local} / base {base}' ) return CurrencyPair ( ticker = '' , factor = 1. , power = 1 ) if 'factor' not in info : info [ 'factor' ] = 1. if 'power' not in info : info [ 'power' ] = 1 return CurrencyPair ( * * info )
10642	def Ra ( L : float , Ts : float , Tf : float , alpha : float , beta : float , nu : float ) -> float : return g * beta * ( Ts - Tinf ) * L ** 3.0 / ( nu * alpha )
8181	def remove_node ( self , id ) : if self . has_key ( id ) : n = self [ id ] self . nodes . remove ( n ) del self [ id ] # Remove all edges involving id and all links to it. for e in list ( self . edges ) : if n in ( e . node1 , e . node2 ) : if n in e . node1 . links : e . node1 . links . remove ( n ) if n in e . node2 . links : e . node2 . links . remove ( n ) self . edges . remove ( e )
1517	def start_master_nodes ( masters , cl_args ) : pids = [ ] for master in masters : Log . info ( "Starting master on %s" % master ) cmd = "%s agent -config %s >> /tmp/nomad_server_log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_nomad_master_config_file ( cl_args ) ) if not is_self ( master ) : cmd = ssh_remote_execute ( cmd , master , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) pids . append ( { "pid" : pid , "dest" : master } ) errors = [ ] for entry in pids : pid = entry [ "pid" ] return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : errors . append ( "Failed to start master on %s with error:\n%s" % ( entry [ "dest" ] , output [ 1 ] ) ) if errors : for error in errors : Log . error ( error ) sys . exit ( - 1 ) Log . info ( "Done starting masters" )
8114	def angle ( x0 , y0 , x1 , y1 ) : return degrees ( atan2 ( y1 - y0 , x1 - x0 ) )
3468	def _update_awareness ( self ) : for x in self . _metabolites : x . _reaction . add ( self ) for x in self . _genes : x . _reaction . add ( self )
3352	def query ( self , search_function , attribute = None ) : def select_attribute ( x ) : if attribute is None : return x else : return getattr ( x , attribute ) try : # if the search_function is a regular expression regex_searcher = re . compile ( search_function ) if attribute is not None : matches = ( i for i in self if regex_searcher . findall ( select_attribute ( i ) ) != [ ] ) else : # Don't regex on objects matches = ( i for i in self if regex_searcher . findall ( getattr ( i , 'id' ) ) != [ ] ) except TypeError : matches = ( i for i in self if search_function ( select_attribute ( i ) ) ) results = self . __class__ ( ) results . _extend_nocheck ( matches ) return results
2181	def parse_authorization_response ( self , url ) : log . debug ( "Parsing token from query part of url %s" , url ) token = dict ( urldecode ( urlparse ( url ) . query ) ) log . debug ( "Updating internal client token attribute." ) self . _populate_attributes ( token ) self . token = token return token
5886	def close ( self ) : if self . fetcher is not None : self . shutdown_network ( ) self . finalizer . atexit = False
10082	def _prepare_edit ( self , record ) : data = record . dumps ( ) # Keep current record revision for merging. data [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] = record . revision_id data [ '_deposit' ] [ 'status' ] = 'draft' data [ '$schema' ] = self . build_deposit_schema ( record ) return data
13513	def reynolds_number ( length , speed , temperature = 25 ) : kinematic_viscosity = interpolate . interp1d ( [ 0 , 10 , 20 , 25 , 30 , 40 ] , np . array ( [ 18.54 , 13.60 , 10.50 , 9.37 , 8.42 , 6.95 ] ) / 10 ** 7 ) # Data from http://web.mit.edu/seawater/2017_MIT_Seawater_Property_Tables_r2.pdf Re = length * speed / kinematic_viscosity ( temperature ) return Re
2222	def _rectify_base ( base ) : if base is NoParam or base == 'default' : return DEFAULT_ALPHABET elif base in [ 26 , 'abc' , 'alpha' ] : return _ALPHABET_26 elif base in [ 16 , 'hex' ] : return _ALPHABET_16 elif base in [ 10 , 'dec' ] : return _ALPHABET_10 else : if not isinstance ( base , ( list , tuple ) ) : raise TypeError ( 'Argument `base` must be a key, list, or tuple; not {}' . format ( type ( base ) ) ) return base
2317	def _run_pc ( self , data , fixedEdges = None , fixedGaps = None , verbose = True ) : # Checking coherence of arguments # print(self.arguments) if ( self . arguments [ '{CITEST}' ] == self . dir_CI_test [ 'hsic' ] and self . arguments [ '{METHOD_INDEP}' ] == self . dir_method_indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the hsic test,' ' setting the hsic.gamma method.' ) self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ 'hsic_gamma' ] elif ( self . arguments [ '{CITEST}' ] == self . dir_CI_test [ 'gaussian' ] and self . arguments [ '{METHOD_INDEP}' ] != self . dir_method_indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the selected test,' ' setting the classic correlation-based method.' ) self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ 'corr' ] # Run PC id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt_pc' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt_pc' + id + '/' def retrieve_result ( ) : return read_csv ( '/tmp/cdt_pc' + id + '/result.csv' , delimiter = ',' ) . values try : data . to_csv ( '/tmp/cdt_pc' + id + '/data.csv' , header = False , index = False ) if fixedGaps is not None and fixedEdges is not None : fixedGaps . to_csv ( '/tmp/cdt_pc' + id + '/fixedgaps.csv' , index = False , header = False ) fixedEdges . to_csv ( '/tmp/cdt_pc' + id + '/fixededges.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' pc_result = launch_R_script ( "{}/R_templates/pc.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , self . arguments , output_function = retrieve_result , verbose = verbose ) # Cleanup except Exception as e : rmtree ( '/tmp/cdt_pc' + id + '' ) raise e except KeyboardInterrupt : rmtree ( '/tmp/cdt_pc' + id + '/' ) raise KeyboardInterrupt rmtree ( '/tmp/cdt_pc' + id + '' ) return pc_result
12575	def apply_mask ( self , mask_img ) : self . set_mask ( mask_img ) return self . get_data ( masked = True , smoothed = True , safe_copy = True )
9277	def parse ( packet ) : if not isinstance ( packet , string_type_parse ) : raise TypeError ( "Expected packet to be str/unicode/bytes, got %s" , type ( packet ) ) if len ( packet ) == 0 : raise ParseError ( "packet is empty" , packet ) # attempt to detect encoding if isinstance ( packet , bytes ) : packet = _unicode_packet ( packet ) packet = packet . rstrip ( "\r\n" ) logger . debug ( "Parsing: %s" , packet ) # split into head and body try : ( head , body ) = packet . split ( ':' , 1 ) except : raise ParseError ( "packet has no body" , packet ) if len ( body ) == 0 : raise ParseError ( "packet body is empty" , packet ) parsed = { 'raw' : packet , } # parse head try : parsed . update ( parse_header ( head ) ) except ParseError as msg : raise ParseError ( str ( msg ) , packet ) # parse body packet_type = body [ 0 ] body = body [ 1 : ] if len ( body ) == 0 and packet_type != '>' : raise ParseError ( "packet body is empty after packet type character" , packet ) # attempt to parse the body try : _try_toparse_body ( packet_type , body , parsed ) # capture ParseErrors and attach the packet except ( UnknownFormat , ParseError ) as exp : exp . packet = packet raise # if we fail all attempts to parse, try beacon packet if 'format' not in parsed : if not re . match ( r"^(AIR.*|ALL.*|AP.*|BEACON|CQ.*|GPS.*|DF.*|DGPS.*|" "DRILL.*|DX.*|ID.*|JAVA.*|MAIL.*|MICE.*|QST.*|QTH.*|" "RTCM.*|SKY.*|SPACE.*|SPC.*|SYM.*|TEL.*|TEST.*|TLM.*|" "WX.*|ZIP.*|UIDIGI)$" , parsed [ 'to' ] ) : raise UnknownFormat ( "format is not supported" , packet ) parsed . update ( { 'format' : 'beacon' , 'text' : packet_type + body , } ) logger . debug ( "Parsed ok." ) return parsed
2162	def update ( self , inventory_source , monitor = False , wait = False , timeout = None , * * kwargs ) : # Establish that we are able to update this inventory source # at all. debug . log ( 'Asking whether the inventory source can be updated.' , header = 'details' ) r = client . get ( '%s%d/update/' % ( self . endpoint , inventory_source ) ) if not r . json ( ) [ 'can_update' ] : raise exc . BadRequest ( 'Tower says it cannot run an update against this inventory source.' ) # Run the update. debug . log ( 'Updating the inventory source.' , header = 'details' ) r = client . post ( '%s%d/update/' % ( self . endpoint , inventory_source ) , data = { } ) inventory_update_id = r . json ( ) [ 'inventory_update' ] # If we were told to monitor the project update's status, do so. if monitor or wait : if monitor : result = self . monitor ( inventory_update_id , parent_pk = inventory_source , timeout = timeout ) elif wait : result = self . wait ( inventory_update_id , parent_pk = inventory_source , timeout = timeout ) inventory = client . get ( '/inventory_sources/%d/' % result [ 'inventory_source' ] ) . json ( ) [ 'inventory' ] result [ 'inventory' ] = int ( inventory ) return result # Done. return { 'id' : inventory_update_id , 'status' : 'ok' }
9763	def upload ( sync = True ) : # pylint:disable=assign-to-new-keyword project = ProjectManager . get_config_or_raise ( ) files = IgnoreManager . get_unignored_file_paths ( ) try : with create_tarfile ( files , project . name ) as file_path : with get_files_in_current_directory ( 'repo' , [ file_path ] ) as ( files , files_size ) : try : PolyaxonClient ( ) . project . upload_repo ( project . user , project . name , files , files_size , sync = sync ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not upload code for project `{}`.' . format ( project . name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) Printer . print_error ( 'Check the project exists, ' 'and that you have access rights, ' 'this could happen as well when uploading large files.' 'If you are running a notebook and mounting the code to the notebook, ' 'you should stop it before uploading.' ) sys . exit ( 1 ) Printer . print_success ( 'Files uploaded.' ) except Exception as e : Printer . print_error ( "Could not upload the file." ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
1643	def CheckSectionSpacing ( filename , clean_lines , class_info , linenum , error ) : # Skip checks if the class is small, where small means 25 lines or less. # 25 lines seems like a good cutoff since that's the usual height of # terminals, and any class that can't fit in one screen can't really # be considered "small". # # Also skip checks if we are on the first line. This accounts for # classes that look like # class Foo { public: ... }; # # If we didn't find the end of the class, last_line would be zero, # and the check will be skipped by the first condition. if ( class_info . last_line - class_info . starting_linenum <= 24 or linenum <= class_info . starting_linenum ) : return matched = Match ( r'\s*(public|protected|private):' , clean_lines . lines [ linenum ] ) if matched : # Issue warning if the line before public/protected/private was # not a blank line, but don't do this if the previous line contains # "class" or "struct". This can happen two ways: # - We are at the beginning of the class. # - We are forward-declaring an inner class that is semantically # private, but needed to be public for implementation reasons. # Also ignores cases where the previous line ends with a backslash as can be # common when defining classes in C macros. prev_line = clean_lines . lines [ linenum - 1 ] if ( not IsBlankLine ( prev_line ) and not Search ( r'\b(class|struct)\b' , prev_line ) and not Search ( r'\\$' , prev_line ) ) : # Try a bit harder to find the beginning of the class. This is to # account for multi-line base-specifier lists, e.g.: # class Derived # : public Base { end_class_head = class_info . starting_linenum for i in range ( class_info . starting_linenum , linenum ) : if Search ( r'\{\s*$' , clean_lines . lines [ i ] ) : end_class_head = i break if end_class_head < linenum - 1 : error ( filename , linenum , 'whitespace/blank_line' , 3 , '"%s:" should be preceded by a blank line' % matched . group ( 1 ) )
12556	def save_varlist ( filename , varnames , varlist ) : variables = { } for i , vn in enumerate ( varnames ) : variables [ vn ] = varlist [ i ] ExportData . save_variables ( filename , variables )
11108	def walk_directories_relative_path ( self , relativePath = "" ) : def walk_directories ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) dirNames = dict . keys ( directories ) for d in sorted ( dirNames ) : yield os . path . join ( relativePath , d ) for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = dict . __getitem__ ( directories , k ) for e in walk_directories ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_directories ( dir , relativePath = '' )
7058	def s3_put_file ( local_file , bucket , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : client . upload_file ( local_file , bucket , os . path . basename ( local_file ) ) return 's3://%s/%s' % ( bucket , os . path . basename ( local_file ) ) except Exception as e : LOGEXCEPTION ( 'could not upload %s to bucket: %s' % ( local_file , bucket ) ) if raiseonfail : raise return None
9986	def get_description ( ) : with open ( path . join ( here , 'README.rst' ) , 'r' ) as f : data = f . read ( ) return data
5067	def get_cache_key ( * * kwargs ) : key = '__' . join ( [ '{}:{}' . format ( item , value ) for item , value in iteritems ( kwargs ) ] ) return hashlib . md5 ( key . encode ( 'utf-8' ) ) . hexdigest ( )
9446	def transfer_call ( self , call_params ) : path = '/' + self . api_version + '/TransferCall/' method = 'POST' return self . request ( path , method , call_params )
10886	def corners ( self ) : corners = [ ] for ind in itertools . product ( * ( ( 0 , 1 ) , ) * self . dim ) : ind = np . array ( ind ) corners . append ( self . l + ind * self . r ) return np . array ( corners )
12096	def save ( self , * args , * * kwargs ) : current_activable_value = getattr ( self , self . ACTIVATABLE_FIELD_NAME ) is_active_changed = self . id is None or self . __original_activatable_value != current_activable_value self . __original_activatable_value = current_activable_value ret_val = super ( BaseActivatableModel , self ) . save ( * args , * * kwargs ) # Emit the signals for when the is_active flag is changed if is_active_changed : model_activations_changed . send ( self . __class__ , instance_ids = [ self . id ] , is_active = current_activable_value ) if self . activatable_field_updated : model_activations_updated . send ( self . __class__ , instance_ids = [ self . id ] , is_active = current_activable_value ) return ret_val
11660	def fit ( self , X , y = None ) : self . kmeans_fit_ = copy ( self . kmeans ) X = as_features ( X , stack = True ) self . kmeans_fit_ . fit ( X . stacked_features ) return self
3230	def service_list ( service = None , key_name = None , * * kwargs ) : resp_list = [ ] req = service . list ( * * kwargs ) while req is not None : resp = req . execute ( ) if key_name and key_name in resp : resp_list . extend ( resp [ key_name ] ) else : resp_list . append ( resp ) # Not all list calls have a list_next if hasattr ( service , 'list_next' ) : req = service . list_next ( previous_request = req , previous_response = resp ) else : req = None return resp_list
13092	def load_targets ( self ) : ldap_services = [ ] if self . ldap : ldap_services = self . search . get_services ( ports = [ 389 ] , up = True ) self . ldap_strings = [ "ldap://{}" . format ( service . address ) for service in ldap_services ] self . services = self . search . get_services ( tags = [ 'smb_signing_disabled' ] ) self . ips = [ str ( service . address ) for service in self . services ]
10872	def get_Kprefactor ( z , cos_theta , zint = 100.0 , n2n1 = 0.95 , get_hdet = False , * * kwargs ) : phase = f_theta ( cos_theta , zint , z , n2n1 = n2n1 , * * kwargs ) to_return = np . exp ( - 1j * phase ) if not get_hdet : to_return *= np . outer ( np . ones_like ( z ) , np . sqrt ( cos_theta ) ) return to_return
1629	def GetHeaderGuardCPPVariable ( filename ) : # Restores original filename in case that cpplint is invoked from Emacs's # flymake. filename = re . sub ( r'_flymake\.h$' , '.h' , filename ) filename = re . sub ( r'/\.flymake/([^/]*)$' , r'/\1' , filename ) # Replace 'c++' with 'cpp'. filename = filename . replace ( 'C++' , 'cpp' ) . replace ( 'c++' , 'cpp' ) fileinfo = FileInfo ( filename ) file_path_from_root = fileinfo . RepositoryName ( ) if _root : suffix = os . sep # On Windows using directory separator will leave us with # "bogus escape error" unless we properly escape regex. if suffix == '\\' : suffix += '\\' file_path_from_root = re . sub ( '^' + _root + suffix , '' , file_path_from_root ) return re . sub ( r'[^a-zA-Z0-9]' , '_' , file_path_from_root ) . upper ( ) + '_'
3343	def calc_base64 ( s ) : s = compat . to_bytes ( s ) s = compat . base64_encodebytes ( s ) . strip ( ) # return bytestring return compat . to_native ( s )
235	def compute_cap_exposures ( positions , caps ) : long_exposures = [ ] short_exposures = [ ] gross_exposures = [ ] net_exposures = [ ] positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) tot_gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) tot_long_exposure = positions_wo_cash [ positions_wo_cash > 0 ] . sum ( axis = 'columns' ) tot_short_exposure = positions_wo_cash [ positions_wo_cash < 0 ] . abs ( ) . sum ( axis = 'columns' ) for bucket_name , boundaries in CAP_BUCKETS . items ( ) : in_bucket = positions_wo_cash [ ( caps >= boundaries [ 0 ] ) & ( caps <= boundaries [ 1 ] ) ] gross_bucket = in_bucket . abs ( ) . sum ( axis = 'columns' ) . divide ( tot_gross_exposure ) long_bucket = in_bucket [ in_bucket > 0 ] . sum ( axis = 'columns' ) . divide ( tot_long_exposure ) short_bucket = in_bucket [ in_bucket < 0 ] . sum ( axis = 'columns' ) . divide ( tot_short_exposure ) net_bucket = long_bucket . subtract ( short_bucket ) gross_exposures . append ( gross_bucket ) long_exposures . append ( long_bucket ) short_exposures . append ( short_bucket ) net_exposures . append ( net_bucket ) return long_exposures , short_exposures , gross_exposures , net_exposures
5177	def resources ( self , type_ = None , title = None , * * kwargs ) : if type_ is None : resources = self . __api . resources ( query = EqualsOperator ( "certname" , self . name ) , * * kwargs ) elif type_ is not None and title is None : resources = self . __api . resources ( type_ = type_ , query = EqualsOperator ( "certname" , self . name ) , * * kwargs ) else : resources = self . __api . resources ( type_ = type_ , title = title , query = EqualsOperator ( "certname" , self . name ) , * * kwargs ) return resources
10189	def consume ( self , event_type , no_ack = True , payload = True ) : assert event_type in self . events return current_queues . queues [ 'stats-{}' . format ( event_type ) ] . consume ( payload = payload )
10221	def preprocessing_br_projection_excel ( path : str ) -> pd . DataFrame : if not os . path . exists ( path ) : raise ValueError ( "Error: %s file not found" % path ) return pd . read_excel ( path , sheetname = 0 , header = 0 )
10140	def check_max_filesize ( chosen_file , max_size ) : if os . path . getsize ( chosen_file ) > max_size : return False else : return True
12754	def joint_distances ( self ) : return [ ( ( np . array ( j . anchor ) - j . anchor2 ) ** 2 ) . sum ( ) for j in self . joints ]
8831	def send ( self , s ) : self . _socket . send ( s . encode ( ) ) return self . read ( )
3783	def select_valid_methods_P ( self , T , P ) : # Same as select_valid_methods but with _P added to variables if self . forced_P : considered_methods = list ( self . user_methods_P ) else : considered_methods = list ( self . all_methods_P ) if self . user_methods_P : [ considered_methods . remove ( i ) for i in self . user_methods_P ] preferences = sorted ( [ self . ranked_methods_P . index ( i ) for i in considered_methods ] ) sorted_methods = [ self . ranked_methods_P [ i ] for i in preferences ] if self . user_methods_P : [ sorted_methods . insert ( 0 , i ) for i in reversed ( self . user_methods_P ) ] sorted_valid_methods_P = [ ] for method in sorted_methods : if self . test_method_validity_P ( T , P , method ) : sorted_valid_methods_P . append ( method ) return sorted_valid_methods_P
12074	def _update_state ( self , vals ) : self . _steps_complete += 1 if self . _steps_complete == self . max_steps : self . _termination_info = ( False , self . _best_val , self . _arg ) return StopIteration arg_inc , arg_dec = vals best_val = min ( arg_inc , arg_dec , self . _best_val ) if best_val == self . _best_val : self . _termination_info = ( True , best_val , self . _arg ) return StopIteration self . _arg += self . stepsize if ( arg_dec > arg_inc ) else - self . stepsize self . _best_val = best_val return [ { self . key : self . _arg + self . stepsize } , { self . key : self . _arg - self . stepsize } ]
6545	def is_connected ( self ) : # need to wrap in try/except b/c of wc3270's socket connection dynamics try : # this is basically a no-op, but it results in the the current status # getting updated self . exec_command ( b"Query(ConnectionState)" ) # connected status is like 'C(192.168.1.1)', disconnected is 'N' return self . status . connection_state . startswith ( b"C(" ) except NotConnectedException : return False
715	def __saveHyperSearchJobID ( cls , permWorkDir , outputLabel , hyperSearchJob ) : jobID = hyperSearchJob . getJobID ( ) filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , outputLabel = outputLabel ) if os . path . exists ( filePath ) : _backupFile ( filePath ) d = dict ( hyperSearchJobID = jobID ) with open ( filePath , "wb" ) as jobIdPickleFile : pickle . dump ( d , jobIdPickleFile )
4805	def _err ( self , msg ) : out = '%s%s' % ( '[%s] ' % self . description if len ( self . description ) > 0 else '' , msg ) if self . kind == 'warn' : print ( out ) return self elif self . kind == 'soft' : global _soft_err _soft_err . append ( out ) return self else : raise AssertionError ( out )
10628	def T ( self , T ) : self . _T = T self . _Hfr = self . _calculate_Hfr ( T )
8514	def dict_merge ( base , top ) : out = dict ( top ) for key in base : if key in top : if isinstance ( base [ key ] , dict ) and isinstance ( top [ key ] , dict ) : out [ key ] = dict_merge ( base [ key ] , top [ key ] ) else : out [ key ] = base [ key ] return out
7019	def merge_hatpi_textlc_apertures ( lclist ) : lcaps = { } framekeys = [ ] for lc in lclist : lcd = read_hatpi_textlc ( lc ) # figure what aperture this is and put it into the lcdict. if two LCs # with the same aperture (i.e. TF1 and TF1) are provided, the later one # in the lclist will overwrite the previous one, for col in lcd [ 'columns' ] : if col . startswith ( 'itf' ) : lcaps [ col ] = lcd thisframekeys = lcd [ 'frk' ] . tolist ( ) framekeys . extend ( thisframekeys ) # uniqify the framekeys framekeys = sorted ( list ( set ( framekeys ) ) )
7045	def all_nonperiodic_features ( times , mags , errs , magsarefluxes = False , stetson_weightbytimediff = True ) : # remove nans first finiteind = npisfinite ( times ) & npisfinite ( mags ) & npisfinite ( errs ) ftimes , fmags , ferrs = times [ finiteind ] , mags [ finiteind ] , errs [ finiteind ] # remove zero errors nzind = npnonzero ( ferrs ) ftimes , fmags , ferrs = ftimes [ nzind ] , fmags [ nzind ] , ferrs [ nzind ] xfeatures = nonperiodic_lightcurve_features ( times , mags , errs , magsarefluxes = magsarefluxes ) stetj = stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = stetson_weightbytimediff ) stetk = stetson_kindex ( fmags , ferrs ) xfeatures . update ( { 'stetsonj' : stetj , 'stetsonk' : stetk } ) return xfeatures
1840	def JNG ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . OR ( cpu . ZF , cpu . SF != cpu . OF ) , target . read ( ) , cpu . PC )
4758	def main ( args ) : trun = cij . runner . trun_from_file ( args . trun_fpath ) rehome ( trun [ "conf" ] [ "OUTPUT" ] , args . output , trun ) postprocess ( trun ) cij . emph ( "main: reports are uses tmpl_fpath: %r" % args . tmpl_fpath ) cij . emph ( "main: reports are here args.output: %r" % args . output ) html_fpath = os . sep . join ( [ args . output , "%s.html" % args . tmpl_name ] ) cij . emph ( "html_fpath: %r" % html_fpath ) try : # Create and store HTML report with open ( html_fpath , 'w' ) as html_file : html_file . write ( dset_to_html ( trun , args . tmpl_fpath ) ) except ( IOError , OSError , ValueError ) as exc : import traceback traceback . print_exc ( ) cij . err ( "rprtr:main: exc: %s" % exc ) return 1 return 0
3388	def _bounds_dist ( self , p ) : prob = self . problem lb_dist = ( p - prob . variable_bounds [ 0 , ] ) . min ( ) ub_dist = ( prob . variable_bounds [ 1 , ] - p ) . min ( ) if prob . bounds . shape [ 0 ] > 0 : const = prob . inequalities . dot ( p ) const_lb_dist = ( const - prob . bounds [ 0 , ] ) . min ( ) const_ub_dist = ( prob . bounds [ 1 , ] - const ) . min ( ) lb_dist = min ( lb_dist , const_lb_dist ) ub_dist = min ( ub_dist , const_ub_dist ) return np . array ( [ lb_dist , ub_dist ] )
11470	def get_filesize ( self , filename ) : result = [ ] def dir_callback ( val ) : result . append ( val . split ( ) [ 4 ] ) self . _ftp . dir ( filename , dir_callback ) return result [ 0 ]
3037	def new_from_json ( cls , json_data ) : json_data_as_unicode = _helpers . _from_bytes ( json_data ) data = json . loads ( json_data_as_unicode ) # Find and call the right classmethod from_json() to restore # the object. module_name = data [ '_module' ] try : module_obj = __import__ ( module_name ) except ImportError : # In case there's an object from the old package structure, # update it module_name = module_name . replace ( '.googleapiclient' , '' ) module_obj = __import__ ( module_name ) module_obj = __import__ ( module_name , fromlist = module_name . split ( '.' ) [ : - 1 ] ) kls = getattr ( module_obj , data [ '_class' ] ) return kls . from_json ( json_data_as_unicode )
9626	def detail_view ( self , request , module , preview ) : try : preview = self . __previews [ module ] [ preview ] except KeyError : raise Http404 # The provided module/preview does not exist in the index. return preview . detail_view ( request )
6589	def open ( self ) : self . workingArea . open ( ) self . runid_pkgidx_map = { } self . runid_to_return = deque ( )
11125	def rename_file ( self , relativePath , name , newName , replace = False , verbose = True ) : # normalize path relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage # check directory in repository assert name in dict . __getitem__ ( dirInfoDict , "files" ) , "file '%s' is not found in repository relative path '%s'" % ( name , relativePath ) # get real path realPath = os . path . join ( self . __path , relativePath , name ) assert os . path . isfile ( realPath ) , "file '%s' is not found in system" % realPath # assert directory new name doesn't exist in repository assert newName not in dict . __getitem__ ( dirInfoDict , "files" ) , "file '%s' already exists in repository relative path '%s'" % ( newName , relativePath ) # check new directory in system newRealPath = os . path . join ( self . __path , relativePath , newName ) if os . path . isfile ( newRealPath ) : if replace : os . remove ( newRealPath ) if verbose : warnings . warn ( "file '%s' already exists found in system, it is now replaced by '%s' because 'replace' flag is True." % ( newRealPath , realPath ) ) else : raise Exception ( "file '%s' already exists in system but not registered in repository." % newRealPath ) # rename file os . rename ( realPath , newRealPath ) dict . __setitem__ ( dict . __getitem__ ( dirInfoDict , "files" ) , newName , dict . __getitem__ ( dirInfoDict , "files" ) . pop ( name ) ) # save repository self . save ( )
8082	def relcurveto ( self , h1x , h1y , h2x , h2y , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . relcurveto ( h1x , h1y , h2x , h2y , x , y )
3738	def Tstar ( T , epsilon_k = None , epsilon = None ) : if epsilon_k : _Tstar = T / ( epsilon_k ) elif epsilon : _Tstar = k * T / epsilon else : raise Exception ( 'Either epsilon/k or epsilon must be provided' ) return _Tstar
7297	def get_attrs ( model_field , disabled = False ) : attrs = { } attrs [ 'class' ] = 'span6 xlarge' if disabled or isinstance ( model_field , ObjectIdField ) : attrs [ 'class' ] += ' disabled' attrs [ 'readonly' ] = 'readonly' return attrs
5514	def async_enterable ( f ) : @ functools . wraps ( f ) def wrapper ( * args , * * kwargs ) : class AsyncEnterableInstance : async def __aenter__ ( self ) : self . context = await f ( * args , * * kwargs ) return await self . context . __aenter__ ( ) async def __aexit__ ( self , * args , * * kwargs ) : await self . context . __aexit__ ( * args , * * kwargs ) def __await__ ( self ) : return f ( * args , * * kwargs ) . __await__ ( ) return AsyncEnterableInstance ( ) return wrapper
5894	def render ( self , name , value , attrs = { } ) : if value is None : value = '' final_attrs = self . build_attrs ( attrs , name = name ) quill_app = apps . get_app_config ( 'quill' ) quill_config = getattr ( quill_app , self . config ) return mark_safe ( render_to_string ( quill_config [ 'template' ] , { 'final_attrs' : flatatt ( final_attrs ) , 'value' : value , 'id' : final_attrs [ 'id' ] , 'config' : self . config , } ) )
1895	def _recv ( self ) -> str : buf , left , right = self . __readline_and_count ( ) bufl = [ buf ] while left != right : buf , l , r = self . __readline_and_count ( ) bufl . append ( buf ) left += l right += r buf = '' . join ( bufl ) . strip ( ) logger . debug ( '<%s' , buf ) if '(error' in bufl [ 0 ] : raise Exception ( f"Error in smtlib: {bufl[0]}" ) return buf
890	def _growSynapses ( cls , connections , random , segment , nDesiredNewSynapes , prevWinnerCells , initialPermanence , maxSynapsesPerSegment ) : candidates = list ( prevWinnerCells ) for synapse in connections . synapsesForSegment ( segment ) : i = binSearch ( candidates , synapse . presynapticCell ) if i != - 1 : del candidates [ i ] nActual = min ( nDesiredNewSynapes , len ( candidates ) ) # Check if we're going to surpass the maximum number of synapses. overrun = connections . numSynapses ( segment ) + nActual - maxSynapsesPerSegment if overrun > 0 : cls . _destroyMinPermanenceSynapses ( connections , random , segment , overrun , prevWinnerCells ) # Recalculate in case we weren't able to destroy as many synapses as needed. nActual = min ( nActual , maxSynapsesPerSegment - connections . numSynapses ( segment ) ) for _ in range ( nActual ) : i = random . getUInt32 ( len ( candidates ) ) connections . createSynapse ( segment , candidates [ i ] , initialPermanence ) del candidates [ i ]
13394	def setting ( self , name_hyphen ) : if name_hyphen in self . _instance_settings : value = self . _instance_settings [ name_hyphen ] [ 1 ] else : msg = "No setting named '%s'" % name_hyphen raise UserFeedback ( msg ) if hasattr ( value , 'startswith' ) and value . startswith ( "$" ) : env_var = value . lstrip ( "$" ) if env_var in os . environ : return os . getenv ( env_var ) else : msg = "'%s' is not defined in your environment" % env_var raise UserFeedback ( msg ) elif hasattr ( value , 'startswith' ) and value . startswith ( "\$" ) : return value . replace ( "\$" , "$" ) else : return value
2891	def connect_outgoing ( self , outgoing_task , outgoing_task_node , sequence_flow_node , is_default ) : self . task . connect_outgoing ( outgoing_task , sequence_flow_node . get ( 'id' ) , sequence_flow_node . get ( 'name' , None ) , self . parser . _parse_documentation ( sequence_flow_node , task_parser = self ) )
12961	def count ( self ) : conn = self . _get_connection ( ) numFilters = len ( self . filters ) numNotFilters = len ( self . notFilters ) if numFilters + numNotFilters == 0 : return conn . scard ( self . _get_ids_key ( ) ) if numNotFilters == 0 : if numFilters == 1 : ( filterFieldName , filterValue ) = self . filters [ 0 ] return conn . scard ( self . _get_key_for_index ( filterFieldName , filterValue ) ) indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] return len ( conn . sinter ( indexKeys ) ) notIndexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . notFilters ] if numFilters == 0 : return len ( conn . sdiff ( self . _get_ids_key ( ) , * notIndexKeys ) ) indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] tempKey = self . _getTempKey ( ) pipeline = conn . pipeline ( ) pipeline . sinterstore ( tempKey , * indexKeys ) pipeline . sdiff ( tempKey , * notIndexKeys ) pipeline . delete ( tempKey ) pks = pipeline . execute ( ) [ 1 ] # sdiff return len ( pks )
627	def _hashCoordinate ( coordinate ) : coordinateStr = "," . join ( str ( v ) for v in coordinate ) # Compute the hash and convert to 64 bit int. hash = int ( int ( hashlib . md5 ( coordinateStr ) . hexdigest ( ) , 16 ) % ( 2 ** 64 ) ) return hash
3505	def loopless_solution ( model , fluxes = None ) : # Need to reoptimize otherwise spurious solution artifacts can cause # all kinds of havoc # TODO: check solution status if fluxes is None : sol = model . optimize ( objective_sense = None ) fluxes = sol . fluxes with model : prob = model . problem # Needs one fixed bound for cplex... loopless_obj_constraint = prob . Constraint ( model . objective . expression , lb = - 1e32 , name = "loopless_obj_constraint" ) model . add_cons_vars ( [ loopless_obj_constraint ] ) _add_cycle_free ( model , fluxes ) solution = model . optimize ( objective_sense = None ) solution . objective_value = loopless_obj_constraint . primal return solution
9641	def _display_details ( var_data ) : meta_keys = ( key for key in list ( var_data . keys ( ) ) if key . startswith ( 'META_' ) ) for key in meta_keys : display_key = key [ 5 : ] . capitalize ( ) pprint ( '{0}: {1}' . format ( display_key , var_data . pop ( key ) ) ) pprint ( var_data )
6051	def map_2d_array_to_masked_1d_array_from_array_2d_and_mask ( mask , array_2d ) : total_image_pixels = mask_util . total_regular_pixels_from_mask ( mask ) array_1d = np . zeros ( shape = total_image_pixels ) index = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : array_1d [ index ] = array_2d [ y , x ] index += 1 return array_1d
12540	def is_dicom_file ( filepath ) : if not os . path . exists ( filepath ) : raise IOError ( 'File {} not found.' . format ( filepath ) ) filename = os . path . basename ( filepath ) if filename == 'DICOMDIR' : return False try : _ = dicom . read_file ( filepath ) except Exception as exc : log . debug ( 'Checking if {0} was a DICOM, but returned ' 'False.' . format ( filepath ) ) return False return True
11364	def run_shell_command ( commands , * * kwargs ) : p = subprocess . Popen ( commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , * * kwargs ) output , error = p . communicate ( ) return p . returncode , output , error
9405	def _get_user_class ( self , name ) : self . _user_classes . setdefault ( name , _make_user_class ( self , name ) ) return self . _user_classes [ name ]
4196	def TOEPLITZ ( T0 , TC , TR , Z ) : assert len ( TC ) > 0 assert len ( TC ) == len ( TR ) M = len ( TC ) X = numpy . zeros ( M + 1 , dtype = complex ) A = numpy . zeros ( M , dtype = complex ) B = numpy . zeros ( M , dtype = complex ) P = T0 if P == 0 : raise ValueError ( "P must be different from zero" ) if P == 0 : raise ValueError ( "P must be different from zero" ) X [ 0 ] = Z [ 0 ] / T0 for k in range ( 0 , M ) : save1 = TC [ k ] save2 = TR [ k ] beta = X [ 0 ] * TC [ k ] if k == 0 : temp1 = - save1 / P temp2 = - save2 / P else : for j in range ( 0 , k ) : save1 = save1 + A [ j ] * TC [ k - j - 1 ] save2 = save2 + B [ j ] * TR [ k - j - 1 ] beta = beta + X [ j + 1 ] * TC [ k - j - 1 ] temp1 = - save1 / P temp2 = - save2 / P P = P * ( 1. - ( temp1 * temp2 ) ) if P <= 0 : raise ValueError ( "singular matrix" ) A [ k ] = temp1 B [ k ] = temp2 alpha = ( Z [ k + 1 ] - beta ) / P if k == 0 : X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * B [ k - j ] continue for j in range ( 0 , k ) : kj = k - j - 1 save1 = A [ j ] A [ j ] = save1 + temp1 * B [ kj ] B [ kj ] = B [ kj ] + temp2 * save1 X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * B [ k - j ] return X
910	def handleInputRecord ( self , inputRecord ) : results = self . __model . run ( inputRecord ) shouldContinue = self . __currentPhase . advance ( ) if not shouldContinue : self . __advancePhase ( ) return results
2963	def get_source_chains ( self , blockade_id ) : result = { } if not blockade_id : raise ValueError ( "invalid blockade_id" ) lines = self . get_chain_rules ( "FORWARD" ) for line in lines : parts = line . split ( ) if len ( parts ) < 4 : continue try : partition_index = parse_partition_index ( blockade_id , parts [ 0 ] ) except ValueError : continue # not a rule targetting a blockade chain source = parts [ 3 ] if source : result [ source ] = partition_index return result
4138	def scale_image ( in_fname , out_fname , max_width , max_height ) : # local import to avoid testing dependency on PIL: try : from PIL import Image except ImportError : import Image img = Image . open ( in_fname ) width_in , height_in = img . size scale_w = max_width / float ( width_in ) scale_h = max_height / float ( height_in ) if height_in * scale_w <= max_height : scale = scale_w else : scale = scale_h if scale >= 1.0 and in_fname == out_fname : return width_sc = int ( round ( scale * width_in ) ) height_sc = int ( round ( scale * height_in ) ) # resize the image img . thumbnail ( ( width_sc , height_sc ) , Image . ANTIALIAS ) # insert centered thumb = Image . new ( 'RGB' , ( max_width , max_height ) , ( 255 , 255 , 255 ) ) pos_insert = ( ( max_width - width_sc ) // 2 , ( max_height - height_sc ) // 2 ) thumb . paste ( img , pos_insert ) thumb . save ( out_fname ) # Use optipng to perform lossless compression on the resized image if # software is installed if os . environ . get ( 'SKLEARN_DOC_OPTIPNG' , False ) : try : subprocess . call ( [ "optipng" , "-quiet" , "-o" , "9" , out_fname ] ) except Exception : warnings . warn ( 'Install optipng to reduce the size of the \ generated images' )
7003	def collect_nonperiodic_features ( featuresdir , magcol , outfile , pklglob = 'varfeatures-*.pkl' , featurestouse = NONPERIODIC_FEATURES_TO_COLLECT , maxobjects = None , labeldict = None , labeltype = 'binary' , ) : # list of input pickles generated by varfeatures in lcproc.py pklist = glob . glob ( os . path . join ( featuresdir , pklglob ) ) if maxobjects : pklist = pklist [ : maxobjects ] # fancy progress bar with tqdm if present if TQDM : listiterator = tqdm ( pklist ) else : listiterator = pklist # go through all the varfeatures arrays feature_dict = { 'objectids' : [ ] , 'magcol' : magcol , 'availablefeatures' : [ ] } LOGINFO ( 'collecting features for magcol: %s' % magcol ) for pkl in listiterator : with open ( pkl , 'rb' ) as infd : varf = pickle . load ( infd ) # update the objectid list objectid = varf [ 'objectid' ] if objectid not in feature_dict [ 'objectids' ] : feature_dict [ 'objectids' ] . append ( objectid ) thisfeatures = varf [ magcol ] if featurestouse and len ( featurestouse ) > 0 : featurestoget = featurestouse else : featurestoget = NONPERIODIC_FEATURES_TO_COLLECT # collect all the features for this magcol/objectid combination for feature in featurestoget : # update the global feature list if necessary if ( ( feature not in feature_dict [ 'availablefeatures' ] ) and ( feature in thisfeatures ) ) : feature_dict [ 'availablefeatures' ] . append ( feature ) feature_dict [ feature ] = [ ] if feature in thisfeatures : feature_dict [ feature ] . append ( thisfeatures [ feature ] ) # now that we've collected all the objects and their features, turn the list # into arrays, and then concatenate them for feat in feature_dict [ 'availablefeatures' ] : feature_dict [ feat ] = np . array ( feature_dict [ feat ] ) feature_dict [ 'objectids' ] = np . array ( feature_dict [ 'objectids' ] ) feature_array = np . column_stack ( [ feature_dict [ feat ] for feat in feature_dict [ 'availablefeatures' ] ] ) feature_dict [ 'features_array' ] = feature_array # if there's a labeldict available, use it to generate a label array. this # feature collection is now a training set. if isinstance ( labeldict , dict ) : labelarray = np . zeros ( feature_dict [ 'objectids' ] . size , dtype = np . int64 ) # populate the labels for each object in the training set for ind , objectid in enumerate ( feature_dict [ 'objectids' ] ) : if objectid in labeldict : # if this is a binary classifier training set, convert bools to # ones and zeros if labeltype == 'binary' : if labeldict [ objectid ] : labelarray [ ind ] = 1 # otherwise, use the actual class label integer elif labeltype == 'classes' : labelarray [ ind ] = labeldict [ objectid ] feature_dict [ 'labels_array' ] = labelarray feature_dict [ 'kwargs' ] = { 'pklglob' : pklglob , 'featurestouse' : featurestouse , 'maxobjects' : maxobjects , 'labeltype' : labeltype } # write the info to the output pickle with open ( outfile , 'wb' ) as outfd : pickle . dump ( feature_dict , outfd , pickle . HIGHEST_PROTOCOL ) # return the feature_dict return feature_dict
24	def update ( self , new_val ) : if self . _value is None : self . _value = new_val else : self . _value = self . _gamma * self . _value + ( 1.0 - self . _gamma ) * new_val
11164	def atime ( self ) : try : return self . _stat . st_atime except : # pragma: no cover self . _stat = self . stat ( ) return self . atime
5141	def new_comment ( self , string , start , end , line ) : prefix = line [ : start [ 1 ] ] if prefix . strip ( ) : # Oops! Trailing comment, not a comment block. self . current_block . add ( string , start , end , line ) else : # A comment block. block = Comment ( start [ 0 ] , end [ 0 ] , string ) self . blocks . append ( block ) self . current_block = block
7976	def _reset ( self ) : ClientStream . _reset ( self ) self . available_auth_methods = None self . auth_stanza = None self . registration_callback = None
10915	def _check_groups ( s , groups ) : ans = [ ] for g in groups : ans . extend ( g ) if np . unique ( ans ) . size != np . size ( ans ) : return False elif np . unique ( ans ) . size != s . obj_get_positions ( ) . shape [ 0 ] : return False else : return ( np . arange ( s . obj_get_radii ( ) . size ) == np . sort ( ans ) ) . all ( )
13405	def prepareImages ( self , fileName , logType ) : import subprocess if self . imageType == "png" : self . imagePixmap . save ( fileName + ".png" , "PNG" , - 1 ) if logType == "Physics" : makePostScript = "convert " + fileName + ".png " + fileName + ".ps" process = subprocess . Popen ( makePostScript , shell = True ) process . wait ( ) thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 ) else : renameImage = "cp " + self . image + " " + fileName + ".gif" process = subprocess . Popen ( renameImage , shell = True ) process . wait ( ) if logType == "Physics" : thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 )
12353	def rename ( self , name , wait = True ) : return self . _action ( 'rename' , name = name , wait = wait )
12086	def html_index ( self , launch = False , showChildren = False ) : self . makePics ( ) # ensure all pics are converted # generate menu html = '<a href="index_splash.html" target="content">./%s/</a><br>' % os . path . basename ( self . abfFolder ) for ID in smartSort ( self . fnamesByCell . keys ( ) ) : link = '' if ID + ".html" in self . fnames2 : link = 'href="%s.html" target="content"' % ID html += ( '<a %s>%s</a><br>' % ( link , ID ) ) # show the parent ABF (ID) if showChildren : for fname in self . fnamesByCell [ ID ] : thisID = os . path . splitext ( fname ) [ 0 ] files2 = [ x for x in self . fnames2 if x . startswith ( thisID ) and not x . endswith ( ".html" ) ] html += '<i>%s</i>' % thisID # show the child ABF if len ( files2 ) : html += ' (%s)' % len ( files2 ) # show number of supporting files html += '<br>' html += "<br>" style . save ( html , self . abfFolder2 + "/index_menu.html" ) self . html_index_splash ( ) # make splash page style . frames ( self . abfFolder2 + "/index.html" , launch = launch )
5741	def result ( self , timeout = None ) : start = time . time ( ) while True : task = self . get_task ( ) if not task or task . status not in ( FINISHED , FAILED ) : if not timeout : continue elif time . time ( ) - start < timeout : continue else : raise TimeoutError ( ) if task . status == FAILED : raise task . result return task . result
5494	def expand_mentions ( text , embed_names = True ) : if embed_names : mention_format = "@<{name} {url}>" else : mention_format = "@<{url}>" def handle_mention ( match ) : source = get_source_by_name ( match . group ( 1 ) ) if source is None : return "@{0}" . format ( match . group ( 1 ) ) return mention_format . format ( name = source . nick , url = source . url ) return short_mention_re . sub ( handle_mention , text )
6033	def from_shape_pixel_scale_and_sub_grid_size ( cls , shape , pixel_scale , sub_grid_size = 2 ) : regular_grid = RegularGrid . from_shape_and_pixel_scale ( shape = shape , pixel_scale = pixel_scale ) sub_grid = SubGrid . from_shape_pixel_scale_and_sub_grid_size ( shape = shape , pixel_scale = pixel_scale , sub_grid_size = sub_grid_size ) blurring_grid = np . array ( [ [ 0.0 , 0.0 ] ] ) return GridStack ( regular_grid , sub_grid , blurring_grid )
9994	def set_attr ( self , name , value ) : if not is_valid_name ( name ) : raise ValueError ( "Invalid name '%s'" % name ) if name in self . namespace : if name in self . refs : if name in self . self_refs : self . new_ref ( name , value ) else : raise KeyError ( "Ref '%s' cannot be changed" % name ) elif name in self . cells : if self . cells [ name ] . is_scalar ( ) : self . cells [ name ] . set_value ( ( ) , value ) else : raise AttributeError ( "Cells '%s' is not a scalar." % name ) else : raise ValueError else : self . new_ref ( name , value )
8009	def execute ( self ) : # Save the execution time first. # If execute() fails, executed_at will be set, with no executed_agreement set. self . executed_at = now ( ) self . save ( ) with transaction . atomic ( ) : ret = BillingAgreement . execute ( self . id ) ret . user = self . user ret . save ( ) self . executed_agreement = ret self . save ( ) return ret
11014	def set_real_value_class ( self ) : if self . value_class is not None and isinstance ( self . value_class , str ) : module_name , dot , class_name = self . value_class . rpartition ( "." ) module = __import__ ( module_name , fromlist = [ class_name ] ) self . value_class = getattr ( module , class_name ) self . _initialized = True
11406	def records_identical ( rec1 , rec2 , skip_005 = True , ignore_field_order = False , ignore_subfield_order = False , ignore_duplicate_subfields = False , ignore_duplicate_controlfields = False ) : rec1_keys = set ( rec1 . keys ( ) ) rec2_keys = set ( rec2 . keys ( ) ) if skip_005 : rec1_keys . discard ( "005" ) rec2_keys . discard ( "005" ) if rec1_keys != rec2_keys : return False for key in rec1_keys : if ignore_duplicate_controlfields and key . startswith ( '00' ) : if set ( field [ 3 ] for field in rec1 [ key ] ) != set ( field [ 3 ] for field in rec2 [ key ] ) : return False continue rec1_fields = rec1 [ key ] rec2_fields = rec2 [ key ] if len ( rec1_fields ) != len ( rec2_fields ) : # They already differs in length... return False if ignore_field_order : # We sort the fields, first by indicators and then by anything else rec1_fields = sorted ( rec1_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 3 ] , elem [ 0 ] ) ) rec2_fields = sorted ( rec2_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 3 ] , elem [ 0 ] ) ) else : # We sort the fields, first by indicators, then by global position # and then by anything else rec1_fields = sorted ( rec1_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 4 ] , elem [ 3 ] , elem [ 0 ] ) ) rec2_fields = sorted ( rec2_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 4 ] , elem [ 3 ] , elem [ 0 ] ) ) for field1 , field2 in zip ( rec1_fields , rec2_fields ) : if ignore_duplicate_subfields : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] or set ( field1 [ 0 ] ) != set ( field2 [ 0 ] ) : return False elif ignore_subfield_order : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] or sorted ( field1 [ 0 ] ) != sorted ( field2 [ 0 ] ) : return False elif field1 [ : 4 ] != field2 [ : 4 ] : return False return True
4678	def getActiveKeyForAccount ( self , name ) : account = self . rpc . get_account ( name ) for authority in account [ "active" ] [ "key_auths" ] : try : return self . getPrivateKeyForPublicKey ( authority [ 0 ] ) except Exception : pass return False
3865	async def leave ( self ) : is_group_conversation = ( self . _conversation . type == hangouts_pb2 . CONVERSATION_TYPE_GROUP ) try : if is_group_conversation : await self . _client . remove_user ( hangouts_pb2 . RemoveUserRequest ( request_header = self . _client . get_request_header ( ) , event_request_header = self . _get_event_request_header ( ) , ) ) else : await self . _client . delete_conversation ( hangouts_pb2 . DeleteConversationRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , delete_upper_bound_timestamp = parsers . to_timestamp ( datetime . datetime . now ( tz = datetime . timezone . utc ) ) ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to leave conversation: {}' . format ( e ) ) raise
5818	def get_path ( temp_dir = None , cache_length = 24 , cert_callback = None ) : ca_path , temp = _ca_path ( temp_dir ) # Windows and OS X if temp and _cached_path_needs_update ( ca_path , cache_length ) : empty_set = set ( ) any_purpose = '2.5.29.37.0' apple_ssl = '1.2.840.113635.100.1.3' win_server_auth = '1.3.6.1.5.5.7.3.1' with path_lock : if _cached_path_needs_update ( ca_path , cache_length ) : with open ( ca_path , 'wb' ) as f : for cert , trust_oids , reject_oids in extract_from_system ( cert_callback , True ) : if sys . platform == 'darwin' : if trust_oids != empty_set and any_purpose not in trust_oids and apple_ssl not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( apple_ssl in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue elif sys . platform == 'win32' : if trust_oids != empty_set and any_purpose not in trust_oids and win_server_auth not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( win_server_auth in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue if cert_callback : cert_callback ( Certificate . load ( cert ) , None ) f . write ( armor ( 'CERTIFICATE' , cert ) ) if not ca_path : raise CACertsError ( 'No CA certs found' ) return ca_path
4435	async def get_tracks ( self , query ) : log . debug ( 'Requesting tracks for query {}' . format ( query ) ) async with self . http . get ( self . rest_uri + quote ( query ) , headers = { 'Authorization' : self . password } ) as res : return await res . json ( content_type = None )
2628	def teardown ( self ) : self . shut_down_instance ( self . instances ) self . instances = [ ] try : self . client . delete_internet_gateway ( InternetGatewayId = self . internet_gateway ) self . internet_gateway = None self . client . delete_route_table ( RouteTableId = self . route_table ) self . route_table = None for subnet in list ( self . sn_ids ) : # Cast to list ensures that this is a copy # Which is important because it means that # the length of the list won't change during iteration self . client . delete_subnet ( SubnetId = subnet ) self . sn_ids . remove ( subnet ) self . client . delete_security_group ( GroupId = self . sg_id ) self . sg_id = None self . client . delete_vpc ( VpcId = self . vpc_id ) self . vpc_id = None except Exception as e : logger . error ( "{}" . format ( e ) ) raise e self . show_summary ( ) os . remove ( self . config [ 'state_file_path' ] )
7031	def specwindow_lsp ( times , mags , errs , magsarefluxes = False , startp = None , endp = None , stepsize = 1.0e-4 , autofreq = True , nbestpeaks = 5 , periodepsilon = 0.1 , sigclip = 10.0 , nworkers = None , glspfunc = _glsp_worker_specwindow , verbose = True ) : # run the LSP using glsp_worker_specwindow as the worker lspres = pgen_lsp ( times , mags , errs , magsarefluxes = magsarefluxes , startp = startp , endp = endp , autofreq = autofreq , nbestpeaks = nbestpeaks , periodepsilon = periodepsilon , stepsize = stepsize , nworkers = nworkers , sigclip = sigclip , glspfunc = glspfunc , verbose = verbose ) # update the resultdict to indicate we're a spectral window function lspres [ 'method' ] = 'win' if lspres [ 'lspvals' ] is not None : # renormalize the periodogram to between 0 and 1 like the usual GLS. lspmax = npnanmax ( lspres [ 'lspvals' ] ) if npisfinite ( lspmax ) : lspres [ 'lspvals' ] = lspres [ 'lspvals' ] / lspmax lspres [ 'nbestlspvals' ] = [ x / lspmax for x in lspres [ 'nbestlspvals' ] ] lspres [ 'bestlspval' ] = lspres [ 'bestlspval' ] / lspmax return lspres
9244	def set_date_from_event ( self , event , issue ) : if not event . get ( 'commit_id' , None ) : issue [ 'actual_date' ] = timestring_to_datetime ( issue [ 'closed_at' ] ) return try : commit = self . fetcher . fetch_commit ( event ) issue [ 'actual_date' ] = timestring_to_datetime ( commit [ 'author' ] [ 'date' ] ) except ValueError : print ( "WARNING: Can't fetch commit {0}. " "It is probably referenced from another repo." . format ( event [ 'commit_id' ] ) ) issue [ 'actual_date' ] = timestring_to_datetime ( issue [ 'closed_at' ] )
1644	def GetPreviousNonBlankLine ( clean_lines , linenum ) : prevlinenum = linenum - 1 while prevlinenum >= 0 : prevline = clean_lines . elided [ prevlinenum ] if not IsBlankLine ( prevline ) : # if not a blank line... return ( prevline , prevlinenum ) prevlinenum -= 1 return ( '' , - 1 )
2489	def create_extracted_license ( self , lic ) : licenses = list ( self . graph . triples ( ( None , self . spdx_namespace . licenseId , lic . identifier ) ) ) if len ( licenses ) != 0 : return licenses [ 0 ] [ 0 ] # return subject in first triple else : license_node = BNode ( ) type_triple = ( license_node , RDF . type , self . spdx_namespace . ExtractedLicensingInfo ) self . graph . add ( type_triple ) ident_triple = ( license_node , self . spdx_namespace . licenseId , Literal ( lic . identifier ) ) self . graph . add ( ident_triple ) text_triple = ( license_node , self . spdx_namespace . extractedText , Literal ( lic . text ) ) self . graph . add ( text_triple ) if lic . full_name is not None : name_triple = ( license_node , self . spdx_namespace . licenseName , self . to_special_value ( lic . full_name ) ) self . graph . add ( name_triple ) for ref in lic . cross_ref : triple = ( license_node , RDFS . seeAlso , URIRef ( ref ) ) self . graph . add ( triple ) if lic . comment is not None : comment_triple = ( license_node , RDFS . comment , Literal ( lic . comment ) ) self . graph . add ( comment_triple ) return license_node
1919	def linux ( cls , path , argv = None , envp = None , entry_symbol = None , symbolic_files = None , concrete_start = '' , pure_symbolic = False , stdin_size = None , * * kwargs ) : if stdin_size is None : stdin_size = consts . stdin_size try : return cls ( _make_linux ( path , argv , envp , entry_symbol , symbolic_files , concrete_start , pure_symbolic , stdin_size ) , * * kwargs ) except elftools . common . exceptions . ELFError : raise Exception ( f'Invalid binary: {path}' )
6197	def numeric_params ( self ) : nparams = dict ( D = ( self . diffusion_coeff . mean ( ) , 'Diffusion coefficient (m^2/s)' ) , np = ( self . num_particles , 'Number of simulated particles' ) , t_step = ( self . t_step , 'Simulation time-step (s)' ) , t_max = ( self . t_max , 'Simulation total time (s)' ) , ID = ( self . ID , 'Simulation ID (int)' ) , EID = ( self . EID , 'IPython Engine ID (int)' ) , pico_mol = ( self . concentration ( ) * 1e12 , 'Particles concentration (pM)' ) ) return nparams
79	def Dropout ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : if ia . is_single_number ( p ) : p2 = iap . Binomial ( 1 - p ) elif ia . is_iterable ( p ) : ia . do_assert ( len ( p ) == 2 ) ia . do_assert ( p [ 0 ] < p [ 1 ] ) ia . do_assert ( 0 <= p [ 0 ] <= 1.0 ) ia . do_assert ( 0 <= p [ 1 ] <= 1.0 ) p2 = iap . Binomial ( iap . Uniform ( 1 - p [ 1 ] , 1 - p [ 0 ] ) ) elif isinstance ( p , iap . StochasticParameter ) : p2 = p else : raise Exception ( "Expected p to be float or int or StochasticParameter, got %s." % ( type ( p ) , ) ) if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return MultiplyElementwise ( p2 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
488	def _trackInstanceAndCheckForConcurrencyViolation ( self ) : global g_max_concurrency , g_max_concurrency_raise_exception assert g_max_concurrency is not None assert self not in self . _clsOutstandingInstances , repr ( self ) # Populate diagnostic info self . _creationTracebackString = traceback . format_stack ( ) # Check for concurrency violation if self . _clsNumOutstanding >= g_max_concurrency : # NOTE: It's possible for _clsNumOutstanding to be greater than # len(_clsOutstandingInstances) if concurrency check was enabled after # unrelease allocations. errorMsg = ( "With numOutstanding=%r, exceeded concurrency limit=%r " "when requesting %r. OTHER TRACKED UNRELEASED " "INSTANCES (%s): %r" ) % ( self . _clsNumOutstanding , g_max_concurrency , self , len ( self . _clsOutstandingInstances ) , self . _clsOutstandingInstances , ) self . _logger . error ( errorMsg ) if g_max_concurrency_raise_exception : raise ConcurrencyExceededError ( errorMsg ) # Add self to tracked instance set self . _clsOutstandingInstances . add ( self ) self . _addedToInstanceSet = True return
12338	def stitch_macro ( path , output_folder = None ) : output_folder = output_folder or path debug ( 'stitching ' + path + ' to ' + output_folder ) fields = glob ( _pattern ( path , _field ) ) # assume we have rectangle of fields xs = [ attribute ( field , 'X' ) for field in fields ] ys = [ attribute ( field , 'Y' ) for field in fields ] x_min , x_max = min ( xs ) , max ( xs ) y_min , y_max = min ( ys ) , max ( ys ) fields_column = len ( set ( xs ) ) fields_row = len ( set ( ys ) ) # assume all fields are the same # and get properties from images in first field images = glob ( _pattern ( fields [ 0 ] , _image ) ) # assume attributes are the same on all images attr = attributes ( images [ 0 ] ) # find all channels and z-stacks channels = [ ] z_stacks = [ ] for image in images : channel = attribute_as_str ( image , 'C' ) if channel not in channels : channels . append ( channel ) z = attribute_as_str ( image , 'Z' ) if z not in z_stacks : z_stacks . append ( z ) debug ( 'channels ' + str ( channels ) ) debug ( 'z-stacks ' + str ( z_stacks ) ) # create macro _ , extension = os . path . splitext ( images [ - 1 ] ) if extension == '.tif' : # assume .ome.tif extension = '.ome.tif' macros = [ ] output_files = [ ] for Z in z_stacks : for C in channels : filenames = os . path . join ( _field + '--X{xx}--Y{yy}' , _image + '--L' + attr . L + '--S' + attr . S + '--U' + attr . U + '--V' + attr . V + '--J' + attr . J + '--E' + attr . E + '--O' + attr . O + '--X{xx}--Y{yy}' + '--T' + attr . T + '--Z' + Z + '--C' + C + extension ) debug ( 'filenames ' + filenames ) cur_attr = attributes ( filenames ) . _asdict ( ) f = 'stitched--U{U}--V{V}--C{C}--Z{Z}.png' . format ( * * cur_attr ) output = os . path . join ( output_folder , f ) debug ( 'output ' + output ) output_files . append ( output ) if os . path . isfile ( output ) : # file already exists print ( 'leicaexperiment stitched file already' ' exists {}' . format ( output ) ) continue macros . append ( fijibin . macro . stitch ( path , filenames , fields_column , fields_row , output_filename = output , x_start = x_min , y_start = y_min ) ) return ( output_files , macros )
9812	def revoke ( username ) : try : PolyaxonClient ( ) . user . revoke_superuser ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not revoke superuser role from user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Superuser role was revoked successfully from user `{}`." . format ( username ) )
7411	def sample_loci ( self ) : ## store idx of passing loci idxs = np . random . choice ( self . idxs , self . ntests ) ## open handle, make a proper generator to reduce mem with open ( self . data ) as indata : liter = ( indata . read ( ) . strip ( ) . split ( "|\n" ) ) ## store data as dict seqdata = { i : "" for i in self . samples } ## put chunks into a list for idx , loc in enumerate ( liter ) : if idx in idxs : ## parse chunk lines = loc . split ( "\n" ) [ : - 1 ] names = [ i . split ( ) [ 0 ] for i in lines ] seqs = [ i . split ( ) [ 1 ] for i in lines ] dd = { i : j for i , j in zip ( names , seqs ) } ## add data to concatenated seqdict for name in seqdata : if name in names : seqdata [ name ] += dd [ name ] else : seqdata [ name ] += "N" * len ( seqs [ 0 ] ) ## concatenate into a phylip file return seqdata
11380	def do_oembed ( parser , token ) : args = token . split_contents ( ) template_dir = None var_name = None if len ( args ) > 2 : if len ( args ) == 3 and args [ 1 ] == 'in' : template_dir = args [ 2 ] elif len ( args ) == 3 and args [ 1 ] == 'as' : var_name = args [ 2 ] elif len ( args ) == 4 and args [ 2 ] == 'in' : template_dir = args [ 3 ] elif len ( args ) == 4 and args [ 2 ] == 'as' : var_name = args [ 3 ] elif len ( args ) == 6 and args [ 4 ] == 'as' : template_dir = args [ 3 ] var_name = args [ 5 ] else : raise template . TemplateSyntaxError ( "OEmbed either takes a single " "(optional) argument: WIDTHxHEIGHT, where WIDTH and HEIGHT " "are positive integers, and or an optional 'in " " \"template_dir\"' argument set." ) if template_dir : if not ( template_dir [ 0 ] == template_dir [ - 1 ] and template_dir [ 0 ] in ( '"' , "'" ) ) : raise template . TemplateSyntaxError ( "template_dir must be quoted" ) template_dir = template_dir [ 1 : - 1 ] if len ( args ) >= 2 and 'x' in args [ 1 ] : width , height = args [ 1 ] . lower ( ) . split ( 'x' ) if not width and height : raise template . TemplateSyntaxError ( "OEmbed's optional WIDTHxHEIGH" "T argument requires WIDTH and HEIGHT to be positive integers." ) else : width , height = None , None nodelist = parser . parse ( ( 'endoembed' , ) ) parser . delete_first_token ( ) return OEmbedNode ( nodelist , width , height , template_dir , var_name )
2817	def convert_adaptive_max_pool2d ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting adaptive_avg_pool2d...' ) if names == 'short' : tf_name = 'APOL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) global_pool = keras . layers . GlobalMaxPooling2D ( data_format = 'channels_first' , name = tf_name ) layers [ scope_name ] = global_pool ( layers [ inputs [ 0 ] ] ) def target_layer ( x ) : import keras return keras . backend . expand_dims ( x ) lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name + 'E' ) layers [ scope_name ] = lambda_layer ( layers [ scope_name ] ) # double expand dims layers [ scope_name ] = lambda_layer ( layers [ scope_name ] )
8018	async def disconnect ( self , code ) : try : await asyncio . wait ( self . application_futures . values ( ) , return_when = asyncio . ALL_COMPLETED , timeout = self . application_close_timeout ) except asyncio . TimeoutError : pass
117	def imap_batches ( self , batches , chunksize = 1 ) : assert ia . is_generator ( batches ) , ( "Expected to get a generator as 'batches', got type %s. " + "Call map_batches() if you use lists." ) % ( type ( batches ) , ) # TODO change this to 'yield from' once switched to 3.3+ gen = self . pool . imap ( _Pool_starworker , self . _handle_batch_ids_gen ( batches ) , chunksize = chunksize ) for batch in gen : yield batch
7906	def __groupchat_message ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : self . __logger . debug ( "groupchat message from unknown source" ) return False rs . process_groupchat_message ( stanza ) return True
4643	def items ( self ) : query = "SELECT {}, {} from {}" . format ( self . __key__ , self . __value__ , self . __tablename__ ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( query ) r = [ ] for key , value in cursor . fetchall ( ) : r . append ( ( key , value ) ) return r
1114	def _collect_lines ( self , diffs ) : fromlist , tolist , flaglist = [ ] , [ ] , [ ] # pull from/to data and flags from mdiff style iterator for fromdata , todata , flag in diffs : try : # store HTML markup of the lines into the lists fromlist . append ( self . _format_line ( 0 , flag , * fromdata ) ) tolist . append ( self . _format_line ( 1 , flag , * todata ) ) except TypeError : # exceptions occur for lines where context separators go fromlist . append ( None ) tolist . append ( None ) flaglist . append ( flag ) return fromlist , tolist , flaglist
9863	def get_homes ( self , only_active = True ) : return [ self . get_home ( home_id ) for home_id in self . get_home_ids ( only_active ) ]
10736	def split_list ( l , N ) : npmode = isinstance ( l , np . ndarray ) if npmode : l = list ( l ) g = np . concatenate ( ( np . array ( [ 0 ] ) , np . cumsum ( split_integer ( len ( l ) , length = N ) ) ) ) s = [ l [ g [ i ] : g [ i + 1 ] ] for i in range ( N ) ] if npmode : s = [ np . array ( sl ) for sl in s ] return s
11485	def upload ( file_pattern , destination = 'Private' , leaf_folders_as_items = False , reuse_existing = False ) : session . token = verify_credentials ( ) # Logic for finding the proper folder to place the files in. parent_folder_id = None user_folders = session . communicator . list_user_folders ( session . token ) if destination . startswith ( '/' ) : parent_folder_id = _find_resource_id_from_path ( destination ) else : for cur_folder in user_folders : if cur_folder [ 'name' ] == destination : parent_folder_id = cur_folder [ 'folder_id' ] if parent_folder_id is None : print ( 'Unable to locate specified destination. Defaulting to {0}.' . format ( user_folders [ 0 ] [ 'name' ] ) ) parent_folder_id = user_folders [ 0 ] [ 'folder_id' ] for current_file in glob . iglob ( file_pattern ) : current_file = os . path . normpath ( current_file ) if os . path . isfile ( current_file ) : print ( 'Uploading item from {0}' . format ( current_file ) ) _upload_as_item ( os . path . basename ( current_file ) , parent_folder_id , current_file , reuse_existing ) else : _upload_folder_recursive ( current_file , parent_folder_id , leaf_folders_as_items , reuse_existing )
180	def to_bounding_box ( self ) : from . bbs import BoundingBox # we don't have to mind the case of len(.) == 1 here, because # zero-sized BBs are considered valid if len ( self . coords ) == 0 : return None return BoundingBox ( x1 = np . min ( self . xx ) , y1 = np . min ( self . yy ) , x2 = np . max ( self . xx ) , y2 = np . max ( self . yy ) , label = self . label )
3877	async def _on_event ( self , event_ ) : conv_id = event_ . conversation_id . id try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for event notification: %s' , conv_id ) else : self . _sync_timestamp = parsers . from_timestamp ( event_ . timestamp ) conv_event = conv . add_event ( event_ ) # conv_event may be None if the event was a duplicate. if conv_event is not None : await self . on_event . fire ( conv_event ) await conv . on_event . fire ( conv_event )
3685	def set_from_PT ( self , Vs ) : # All roots will have some imaginary component; ignore them if > 1E-9 good_roots = [ ] bad_roots = [ ] for i in Vs : j = i . real if abs ( i . imag ) > 1E-9 or j < 0 : bad_roots . append ( i ) else : good_roots . append ( j ) if len ( bad_roots ) == 2 : V = good_roots [ 0 ] self . phase = self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) if self . phase == 'l' : self . V_l = V else : self . V_g = V else : # Even in the case of three real roots, it is still the min/max that make sense self . V_l , self . V_g = min ( good_roots ) , max ( good_roots ) [ self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) for V in [ self . V_l , self . V_g ] ] self . phase = 'l/g'
10543	def create_task ( project_id , info , n_answers = 30 , priority_0 = 0 , quorum = 0 ) : try : task = dict ( project_id = project_id , info = info , calibration = 0 , priority_0 = priority_0 , n_answers = n_answers , quorum = quorum ) res = _pybossa_req ( 'post' , 'task' , payload = task ) if res . get ( 'id' ) : return Task ( res ) else : return res except : # pragma: no cover raise
4442	def add_suggestions ( self , * suggestions , * * kwargs ) : pipe = self . redis . pipeline ( ) for sug in suggestions : args = [ AutoCompleter . SUGADD_COMMAND , self . key , sug . string , sug . score ] if kwargs . get ( 'increment' ) : args . append ( AutoCompleter . INCR ) if sug . payload : args . append ( 'PAYLOAD' ) args . append ( sug . payload ) pipe . execute_command ( * args ) return pipe . execute ( ) [ - 1 ]
9914	def create ( self , validated_data ) : email_query = models . EmailAddress . objects . filter ( email = self . validated_data [ "email" ] ) if email_query . exists ( ) : email = email_query . get ( ) email . send_duplicate_notification ( ) else : email = super ( EmailSerializer , self ) . create ( validated_data ) email . send_confirmation ( ) user = validated_data . get ( "user" ) query = models . EmailAddress . objects . filter ( is_primary = True , user = user ) if not query . exists ( ) : email . set_primary ( ) return email
11340	def set_target_celsius ( self , celsius , mode = config . SCHEDULE_HOLD ) : temperature = celsius_to_nuheat ( celsius ) self . set_target_temperature ( temperature , mode )
5264	def pathcase ( string ) : string = snakecase ( string ) if not string : return string return re . sub ( r"_" , "/" , string )
12773	def _step_to_marker_frame ( self , frame_no , dt = None ) : # update the positions and velocities of the markers. self . markers . detach ( ) self . markers . reposition ( frame_no ) self . markers . attach ( frame_no ) # detect collisions. self . ode_space . collide ( None , self . on_collision ) # record the state of each skeleton body. states = self . skeleton . get_body_states ( ) self . skeleton . set_body_states ( states ) # yield the current simulation state to our caller. yield states # update the ode world. self . ode_world . step ( dt or self . dt ) # clear out contact joints to prepare for the next frame. self . ode_contactgroup . empty ( )
12040	def checkOut ( thing , html = True ) : msg = "" for name in sorted ( dir ( thing ) ) : if not "__" in name : msg += "<b>%s</b>\n" % name try : msg += " ^-VALUE: %s\n" % getattr ( thing , name ) ( ) except : pass if html : html = '<html><body><code>' + msg + '</code></body></html>' html = html . replace ( " " , "&nbsp;" ) . replace ( "\n" , "<br>" ) fname = tempfile . gettempdir ( ) + "/swhlab/checkout.html" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname ) print ( msg . replace ( '<b>' , '' ) . replace ( '</b>' , '' ) )
9811	def grant ( username ) : try : PolyaxonClient ( ) . user . grant_superuser ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not grant superuser role to user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Superuser role was granted successfully to user `{}`." . format ( username ) )
3240	def get_group_policy_document ( group_name , policy_name , client = None , * * kwargs ) : return client . get_group_policy ( GroupName = group_name , PolicyName = policy_name , * * kwargs ) [ 'PolicyDocument' ]
12600	def concat_sheets ( xl_path : str , sheetnames = None , add_tab_names = False ) : xl_path , choice = _check_xl_path ( xl_path ) if sheetnames is None : sheetnames = get_sheet_list ( xl_path ) sheets = pd . read_excel ( xl_path , sheetname = sheetnames ) if add_tab_names : for tab in sheets : sheets [ tab ] [ 'Tab' ] = [ tab ] * len ( sheets [ tab ] ) return pd . concat ( [ sheets [ tab ] for tab in sheets ] )
11005	def _req ( self , url , method = 'GET' , * * kw ) : send = requests . post if method == 'POST' else requests . get try : r = send ( url , headers = self . _token_header ( ) , timeout = self . settings [ 'timeout' ] , * * kw ) except requests . exceptions . Timeout : raise ApiError ( 'Request timed out (%s seconds)' % self . settings [ 'timeout' ] ) try : json = r . json ( ) except ValueError : raise ApiError ( 'Received not JSON response from API' ) if json . get ( 'status' ) != 'ok' : raise ApiError ( 'API error: received unexpected json from API: %s' % json ) return json
13206	def _parse_doc_ref ( self ) : command = LatexCommand ( 'setDocRef' , { 'name' : 'handle' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no setDocRef' ) self . _handle = None self . _series = None self . _serial = None return self . _handle = parsed [ 'handle' ] try : self . _series , self . _serial = self . _handle . split ( '-' , 1 ) except ValueError : self . _logger . warning ( 'lsstdoc handle cannot be parsed into ' 'series and serial: %r' , self . _handle ) self . _series = None self . _serial = None
2497	def handle_package_has_file_helper ( self , pkg_file ) : nodes = list ( self . graph . triples ( ( None , self . spdx_namespace . fileName , Literal ( pkg_file . name ) ) ) ) if len ( nodes ) == 1 : return nodes [ 0 ] [ 0 ] else : raise InvalidDocumentError ( 'handle_package_has_file_helper could not' + ' find file node for file: {0}' . format ( pkg_file . name ) )
13262	def task ( func , * * config ) : if func . __name__ == func . __qualname__ : assert not func . __qualname__ in _task_list , "Can not define the same task \"{}\" twice" . format ( func . __qualname__ ) logger . debug ( "Found task %s" , func ) _task_list [ func . __qualname__ ] = Task ( plugin_class = None , func = func , config = config ) else : func . yaz_task_config = config return func
11832	def mate ( self , other ) : c = random . randrange ( len ( self . genes ) ) return self . __class__ ( self . genes [ : c ] + other . genes [ c : ] )
9627	def url ( self ) : return reverse ( '%s:detail' % URL_NAMESPACE , kwargs = { 'module' : self . module , 'preview' : type ( self ) . __name__ , } )
1396	def removeTopology ( self , topology_name , state_manager_name ) : topologies = [ ] for top in self . topologies : if ( top . name == topology_name and top . state_manager_name == state_manager_name ) : # Remove topologyInfo if ( topology_name , state_manager_name ) in self . topologyInfos : self . topologyInfos . pop ( ( topology_name , state_manager_name ) ) else : topologies . append ( top ) self . topologies = topologies
9926	def handle ( self , * args , * * kwargs ) : cutoff = timezone . now ( ) cutoff -= app_settings . CONFIRMATION_EXPIRATION cutoff -= app_settings . CONFIRMATION_SAVE_PERIOD queryset = models . EmailConfirmation . objects . filter ( created_at__lte = cutoff ) count = queryset . count ( ) queryset . delete ( ) if count : self . stdout . write ( self . style . SUCCESS ( "Removed {count} old email confirmation(s)" . format ( count = count ) ) ) else : self . stdout . write ( "No email confirmations to remove." )
12706	def state ( self , state ) : assert self . name == state . name , 'state name "{}" != body name "{}"' . format ( state . name , self . name ) self . position = state . position self . quaternion = state . quaternion self . linear_velocity = state . linear_velocity self . angular_velocity = state . angular_velocity
13183	def dict_to_row ( cls , observation_data ) : row = [ ] row . append ( observation_data [ 'name' ] ) row . append ( observation_data [ 'date' ] ) row . append ( observation_data [ 'magnitude' ] ) comment_code = observation_data . get ( 'comment_code' , 'na' ) if not comment_code : comment_code = 'na' row . append ( comment_code ) comp1 = observation_data . get ( 'comp1' , 'na' ) if not comp1 : comp1 = 'na' row . append ( comp1 ) comp2 = observation_data . get ( 'comp2' , 'na' ) if not comp2 : comp2 = 'na' row . append ( comp2 ) chart = observation_data . get ( 'chart' , 'na' ) if not chart : chart = 'na' row . append ( chart ) notes = observation_data . get ( 'notes' , 'na' ) if not notes : notes = 'na' row . append ( notes ) return row
9923	def save ( self ) : try : email = models . EmailAddress . objects . get ( email = self . validated_data [ "email" ] , is_verified = False ) logger . debug ( "Resending verification email to %s" , self . validated_data [ "email" ] , ) email . send_confirmation ( ) except models . EmailAddress . DoesNotExist : logger . debug ( "Not resending verification email to %s because the address " "doesn't exist in the database." , self . validated_data [ "email" ] , )
9005	def add_new_pattern ( self , id_ , name = None ) : if name is None : name = id_ pattern = self . _parser . new_pattern ( id_ , name ) self . _patterns . append ( pattern ) return pattern
8884	def fit ( self , X , y = None ) : # Check that X have correct shape X = check_array ( X ) self . _x_min = X . min ( axis = 0 ) # axis=0 will find the minimum values ​​by columns (for each feature) self . _x_max = X . max ( axis = 0 ) # axis=0 will find the minimum values ​​by columns (for each feature) return self
11575	def encoder_data ( self , data ) : prev_val = self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) # set value so that it shows positive and negative values if val > 8192 : val -= 16384 pin = data [ 0 ] with self . pymata . data_lock : self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if prev_val != val : callback = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_CALLBACK ] if callback is not None : callback ( [ self . pymata . ENCODER , pin , self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] ] )
7703	def get_items_by_group ( self , group , case_sensitive = True ) : result = [ ] if not group : for item in self . _items : if not item . groups : result . append ( item ) return result if not case_sensitive : group = group . lower ( ) for item in self . _items : if group in item . groups : result . append ( item ) elif not case_sensitive and group in [ g . lower ( ) for g in item . groups ] : result . append ( item ) return result
8077	def ellipsemode ( self , mode = None ) : if mode in ( self . CORNER , self . CENTER , self . CORNERS ) : self . ellipsemode = mode return self . ellipsemode elif mode is None : return self . ellipsemode else : raise ShoebotError ( _ ( "ellipsemode: invalid input" ) )
5714	def _validate_zip ( the_zip ) : datapackage_jsons = [ f for f in the_zip . namelist ( ) if f . endswith ( 'datapackage.json' ) ] if len ( datapackage_jsons ) != 1 : msg = 'DataPackage must have only one "datapackage.json" (had {n})' raise exceptions . DataPackageException ( msg . format ( n = len ( datapackage_jsons ) ) )
12419	def capture_stderr ( ) : stderr = sys . stderr try : capture_out = StringIO ( ) sys . stderr = capture_out yield capture_out finally : sys . stderr = stderr
4894	def _collect_grades_data ( self , enterprise_enrollment , course_details ) : if self . grades_api is None : self . grades_api = GradesApiClient ( self . user ) course_id = enterprise_enrollment . course_id username = enterprise_enrollment . enterprise_customer_user . user . username try : grades_data = self . grades_api . get_course_grade ( course_id , username ) except HttpNotFoundError as error : # Grade not found, so we have nothing to report. if hasattr ( error , 'content' ) : response_content = json . loads ( error . content ) if response_content . get ( 'error_code' , '' ) == 'user_not_enrolled' : # This means the user has an enterprise enrollment record but is not enrolled in the course yet LOGGER . info ( "User [%s] not enrolled in course [%s], enterprise enrollment [%d]" , username , course_id , enterprise_enrollment . pk ) return None , None , None LOGGER . error ( "No grades data found for [%d]: [%s], [%s]" , enterprise_enrollment . pk , course_id , username ) return None , None , None # Prepare to process the course end date and pass/fail grade course_end_date = course_details . get ( 'end' ) if course_end_date is not None : course_end_date = parse_datetime ( course_end_date ) now = timezone . now ( ) is_passing = grades_data . get ( 'passed' ) # We can consider a course complete if: # * the course's end date has passed if course_end_date is not None and course_end_date < now : completed_date = course_end_date grade = self . grade_passing if is_passing else self . grade_failing # * Or, the learner has a passing grade (as of now) elif is_passing : completed_date = now grade = self . grade_passing # Otherwise, the course is still in progress else : completed_date = None grade = self . grade_incomplete return completed_date , grade , is_passing
7399	def swap ( self , qs ) : try : replacement = qs [ 0 ] except IndexError : # already first/last return if not self . _valid_ordering_reference ( replacement ) : raise ValueError ( "%r can only be swapped with instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) self . order , replacement . order = replacement . order , self . order self . save ( ) replacement . save ( )
12872	def one_of ( these ) : ch = peek ( ) try : if ( ch is EndOfFile ) or ( ch not in these ) : fail ( list ( these ) ) except TypeError : if ch != these : fail ( [ these ] ) next ( ) return ch
1631	def CheckHeaderFileIncluded ( filename , include_state , error ) : # Do not check test files fileinfo = FileInfo ( filename ) if Search ( _TEST_FILE_SUFFIX , fileinfo . BaseName ( ) ) : return for ext in GetHeaderExtensions ( ) : basefilename = filename [ 0 : len ( filename ) - len ( fileinfo . Extension ( ) ) ] headerfile = basefilename + '.' + ext if not os . path . exists ( headerfile ) : continue headername = FileInfo ( headerfile ) . RepositoryName ( ) first_include = None for section_list in include_state . include_list : for f in section_list : if headername in f [ 0 ] or f [ 0 ] in headername : return if not first_include : first_include = f [ 1 ] error ( filename , first_include , 'build/include' , 5 , '%s should include its header file %s' % ( fileinfo . RepositoryName ( ) , headername ) )
12054	def inspectABF ( abf = exampleABF , saveToo = False , justPlot = False ) : pylab . close ( 'all' ) print ( " ~~ inspectABF()" ) if type ( abf ) is str : abf = swhlab . ABF ( abf ) swhlab . plot . new ( abf , forceNewFigure = True ) if abf . sweepInterval * abf . sweeps < 60 * 5 : #shorter than 5 minutes pylab . subplot ( 211 ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . sweep ( abf , 'all' ) pylab . subplot ( 212 ) swhlab . plot . sweep ( abf , 'all' , continuous = True ) swhlab . plot . comments ( abf ) else : print ( " -- plotting as long recording" ) swhlab . plot . sweep ( abf , 'all' , continuous = True , minutes = True ) swhlab . plot . comments ( abf , minutes = True ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . annotate ( abf ) if justPlot : return if saveToo : path = os . path . split ( abf . fname ) [ 0 ] basename = os . path . basename ( abf . fname ) pylab . savefig ( os . path . join ( path , "_" + basename . replace ( ".abf" , ".png" ) ) ) pylab . show ( ) return
1486	def _modules_to_main ( modList ) : if not modList : return main = sys . modules [ '__main__' ] for modname in modList : if isinstance ( modname , str ) : try : mod = __import__ ( modname ) except Exception : sys . stderr . write ( 'warning: could not import %s\n. ' 'Your function may unexpectedly error due to this import failing;' 'A version mismatch is likely. Specific error was:\n' % modname ) print_exec ( sys . stderr ) else : setattr ( main , mod . __name__ , mod )
1372	def get_heron_libs ( local_jars ) : heron_lib_dir = get_heron_lib_dir ( ) heron_libs = [ os . path . join ( heron_lib_dir , f ) for f in local_jars ] return heron_libs
6404	def get_feature ( vector , feature ) : # :param bool binary: if False, -1, 0, & 1 represent -, 0, & + # if True, only binary oppositions are allowed: # 0 & 1 represent - & + and 0s are mapped to - if feature not in _FEATURE_MASK : raise AttributeError ( "feature must be one of: '" + "', '" . join ( ( 'consonantal' , 'sonorant' , 'syllabic' , 'labial' , 'round' , 'coronal' , 'anterior' , 'distributed' , 'dorsal' , 'high' , 'low' , 'back' , 'tense' , 'pharyngeal' , 'ATR' , 'voice' , 'spread_glottis' , 'constricted_glottis' , 'continuant' , 'strident' , 'lateral' , 'delayed_release' , 'nasal' , ) ) + "'" ) # each feature mask contains two bits, one each for - and + mask = _FEATURE_MASK [ feature ] # the lower bit represents + pos_mask = mask >> 1 retvec = [ ] for char in vector : if char < 0 : retvec . append ( float ( 'NaN' ) ) else : masked = char & mask if masked == 0 : retvec . append ( 0 ) # 0 elif masked == mask : retvec . append ( 2 ) # +/- elif masked & pos_mask : retvec . append ( 1 ) # + else : retvec . append ( - 1 ) # - return retvec
1768	def _publish_instruction_as_executed ( self , insn ) : self . _icount += 1 self . _publish ( 'did_execute_instruction' , self . _last_pc , self . PC , insn )
2472	def reset_file_stat ( self ) : # FIXME: this state does not make sense self . file_spdx_id_set = False self . file_comment_set = False self . file_type_set = False self . file_chksum_set = False self . file_conc_lics_set = False self . file_license_comment_set = False self . file_notice_set = False self . file_copytext_set = False
5019	def validate_image_extension ( value ) : config = get_app_config ( ) ext = os . path . splitext ( value . name ) [ 1 ] if config and not ext . lower ( ) in config . valid_image_extensions : raise ValidationError ( _ ( "Unsupported file extension." ) )
10671	def _finalise_result_ ( compound , value , mass ) : result = value / 3.6E6 # J/x -> kWh/x result = result / compound . molar_mass # x/mol -> x/kg result = result * mass # x/kg -> x return result
10784	def should_particle_exist ( absent_err , present_err , absent_d , present_d , im_change_frac = 0.2 , min_derr = 0.1 ) : delta_im = np . ravel ( present_d - absent_d ) im_change = np . dot ( delta_im , delta_im ) err_cutoff = max ( [ im_change_frac * im_change , min_derr ] ) return ( absent_err - present_err ) >= err_cutoff
13851	def is_hidden ( path ) : full_path = os . path . abspath ( path ) name = os . path . basename ( full_path ) def no ( path ) : return False platform_hidden = globals ( ) . get ( 'is_hidden_' + platform . system ( ) , no ) return name . startswith ( '.' ) or platform_hidden ( full_path )
6084	def blurred_image_of_planes_from_1d_images_and_convolver ( total_planes , image_plane_image_1d_of_planes , image_plane_blurring_image_1d_of_planes , convolver , map_to_scaled_array ) : blurred_image_of_planes = [ ] for plane_index in range ( total_planes ) : # If all entries are zero, there was no light profile / pixeization if np . count_nonzero ( image_plane_image_1d_of_planes [ plane_index ] ) > 0 : blurred_image_1d_of_plane = blurred_image_1d_from_1d_unblurred_and_blurring_images ( unblurred_image_1d = image_plane_image_1d_of_planes [ plane_index ] , blurring_image_1d = image_plane_blurring_image_1d_of_planes [ plane_index ] , convolver = convolver ) blurred_image_of_plane = map_to_scaled_array ( array_1d = blurred_image_1d_of_plane ) blurred_image_of_planes . append ( blurred_image_of_plane ) else : blurred_image_of_planes . append ( None ) return blurred_image_of_planes
3475	def compartments ( self ) : if self . _compartments is None : self . _compartments = { met . compartment for met in self . _metabolites if met . compartment is not None } return self . _compartments
12415	def send ( self , * args , * * kwargs ) : self . write ( * args , * * kwargs ) self . flush ( )
4665	def id ( self ) : # Store signatures temporarily since they are not part of # transaction id sigs = self . data [ "signatures" ] self . data . pop ( "signatures" , None ) # Generage Hash of the seriliazed version h = hashlib . sha256 ( bytes ( self ) ) . digest ( ) # recover signatures self . data [ "signatures" ] = sigs # Return properly truncated tx hash return hexlify ( h [ : 20 ] ) . decode ( "ascii" )
5556	def _filter_by_zoom ( element = None , conf_string = None , zoom = None ) : for op_str , op_func in [ # order of operators is important: # prematurely return in cases of "<=" or ">=", otherwise # _strip_zoom() cannot parse config strings starting with "<" # or ">" ( "=" , operator . eq ) , ( "<=" , operator . le ) , ( ">=" , operator . ge ) , ( "<" , operator . lt ) , ( ">" , operator . gt ) , ] : if conf_string . startswith ( op_str ) : return element if op_func ( zoom , _strip_zoom ( conf_string , op_str ) ) else None
10831	def get ( cls , group , admin ) : try : ga = cls . query . filter_by ( group = group , admin_id = admin . get_id ( ) , admin_type = resolve_admin_type ( admin ) ) . one ( ) return ga except Exception : return None
9133	def get_data_dir ( module_name : str ) -> str : module_name = module_name . lower ( ) data_dir = os . path . join ( BIO2BEL_DIR , module_name ) os . makedirs ( data_dir , exist_ok = True ) return data_dir
13011	def get_own_ip ( ) : own_ip = None interfaces = psutil . net_if_addrs ( ) for _ , details in interfaces . items ( ) : for detail in details : if detail . family == socket . AF_INET : ip_address = ipaddress . ip_address ( detail . address ) if not ( ip_address . is_link_local or ip_address . is_loopback ) : own_ip = str ( ip_address ) break return own_ip
5953	def stop_logging ( ) : from . import log logger = logging . getLogger ( "gromacs" ) logger . info ( "GromacsWrapper %s STOPPED logging" , get_version ( ) ) log . clear_handlers ( logger )
11768	def weighted_sampler ( seq , weights ) : totals = [ ] for w in weights : totals . append ( w + totals [ - 1 ] if totals else w ) return lambda : seq [ bisect . bisect ( totals , random . uniform ( 0 , totals [ - 1 ] ) ) ]
6242	def load_shader ( self , shader_type : str , path : str ) : if path : resolved_path = self . find_program ( path ) if not resolved_path : raise ValueError ( "Cannot find {} shader '{}'" . format ( shader_type , path ) ) print ( "Loading:" , path ) with open ( resolved_path , 'r' ) as fd : return fd . read ( )
6417	def stem ( self , word ) : word = normalize ( 'NFKD' , text_type ( word . lower ( ) ) ) word = '' . join ( c for c in word if c in { 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'q' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'x' , 'y' , 'z' , } ) # Rule 2 word = word . replace ( 'j' , 'i' ) . replace ( 'v' , 'u' ) # Rule 3 if word [ - 3 : ] == 'que' : # This diverges from the paper by also returning 'que' itself # unstemmed if word [ : - 3 ] in self . _keep_que or word == 'que' : return { 'n' : word , 'v' : word } else : word = word [ : - 3 ] # Base case will mean returning the words as is noun = word verb = word # Rule 4 for endlen in range ( 4 , 0 , - 1 ) : if word [ - endlen : ] in self . _n_endings [ endlen ] : if len ( word ) - 2 >= endlen : noun = word [ : - endlen ] else : noun = word break for endlen in range ( 6 , 0 , - 1 ) : if word [ - endlen : ] in self . _v_endings_strip [ endlen ] : if len ( word ) - 2 >= endlen : verb = word [ : - endlen ] else : verb = word break if word [ - endlen : ] in self . _v_endings_alter [ endlen ] : if word [ - endlen : ] in { 'iuntur' , 'erunt' , 'untur' , 'iunt' , 'unt' , } : new_word = word [ : - endlen ] + 'i' addlen = 1 elif word [ - endlen : ] in { 'beris' , 'bor' , 'bo' } : new_word = word [ : - endlen ] + 'bi' addlen = 2 else : new_word = word [ : - endlen ] + 'eri' addlen = 3 # Technically this diverges from the paper by considering the # length of the stem without the new suffix if len ( new_word ) >= 2 + addlen : verb = new_word else : verb = word break return { 'n' : noun , 'v' : verb }
9621	def gamepad ( self ) : state = _xinput_state ( ) _xinput . XInputGetState ( self . ControllerID - 1 , pointer ( state ) ) self . dwPacketNumber = state . dwPacketNumber return state . XINPUT_GAMEPAD
10045	def create ( cls , object_type = None , object_uuid = None , * * kwargs ) : assert 'pid_value' in kwargs kwargs . setdefault ( 'status' , cls . default_status ) return super ( DepositProvider , cls ) . create ( object_type = object_type , object_uuid = object_uuid , * * kwargs )
4348	def trim ( self , start_time , end_time = None ) : if not is_number ( start_time ) or start_time < 0 : raise ValueError ( "start_time must be a positive number." ) effect_args = [ 'trim' , '{:f}' . format ( start_time ) ] if end_time is not None : if not is_number ( end_time ) or end_time < 0 : raise ValueError ( "end_time must be a positive number." ) if start_time >= end_time : raise ValueError ( "start_time must be smaller than end_time." ) effect_args . append ( '{:f}' . format ( end_time - start_time ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'trim' ) return self
5064	def update_query_parameters ( url , query_parameters ) : scheme , netloc , path , query_string , fragment = urlsplit ( url ) url_params = parse_qs ( query_string ) # Update url query parameters url_params . update ( query_parameters ) return urlunsplit ( ( scheme , netloc , path , urlencode ( sorted ( url_params . items ( ) ) , doseq = True ) , fragment ) , )
1518	def start_slave_nodes ( slaves , cl_args ) : pids = [ ] for slave in slaves : Log . info ( "Starting slave on %s" % slave ) cmd = "%s agent -config %s >> /tmp/nomad_client.log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_nomad_slave_config_file ( cl_args ) ) if not is_self ( slave ) : cmd = ssh_remote_execute ( cmd , slave , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) pids . append ( { "pid" : pid , "dest" : slave } ) errors = [ ] for entry in pids : pid = entry [ "pid" ] return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : errors . append ( "Failed to start slave on %s with error:\n%s" % ( entry [ "dest" ] , output [ 1 ] ) ) if errors : for error in errors : Log . error ( error ) sys . exit ( - 1 ) Log . info ( "Done starting slaves" )
1762	def push_bytes ( self , data , force = False ) : self . STACK -= len ( data ) self . write_bytes ( self . STACK , data , force ) return self . STACK
13352	def monitor ( self , sleep = 5 ) : manager = FileModificationObjectManager ( ) timestamps = { } filebodies = { } # register original timestamp and filebody to dict for file in self . f_repository : timestamps [ file ] = self . _get_mtime ( file ) filebodies [ file ] = open ( file ) . read ( ) while True : for file in self . f_repository : mtime = timestamps [ file ] fbody = filebodies [ file ] modified = self . _check_modify ( file , mtime , fbody ) # file not modify -> continue if not modified : continue # file modifies -> create the modification object new_mtime = self . _get_mtime ( file ) new_fbody = open ( file ) . read ( ) obj = FileModificationObject ( file , ( mtime , new_mtime ) , ( fbody , new_fbody ) ) # overwrite new timestamp and filebody timestamps [ file ] = new_mtime filebodies [ file ] = new_fbody # append file modification object to manager manager . add_object ( obj ) # return new modification object yield obj time . sleep ( sleep )
13664	def set_item ( filename , item ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : # load the JSON data into memory products_data = json . load ( products_file ) # check if UUID already exists uuid_list = [ i for i in filter ( lambda z : z [ "uuid" ] == str ( item [ "uuid" ] ) , products_data ) ] if len ( uuid_list ) == 0 : # add the new item to the JSON file products_data . append ( item ) # save the new JSON to the temp file json . dump ( products_data , temp_file ) return True return None
13699	def make_seekable ( fileobj ) : if sys . version_info < ( 3 , 0 ) and isinstance ( fileobj , file ) : filename = fileobj . name fileobj = io . FileIO ( fileobj . fileno ( ) , closefd = False ) fileobj . name = filename assert isinstance ( fileobj , io . IOBase ) , "fileobj must be an instance of io.IOBase or a file, got %s" % type ( fileobj ) return fileobj if fileobj . seekable ( ) else ArchiveTemp ( fileobj )
6234	def start ( self ) : if self . initialized : mixer . music . unpause ( ) else : mixer . music . play ( ) # FIXME: Calling play twice to ensure the music is actually playing mixer . music . play ( ) self . initialized = True self . paused = False
13350	def add_file ( self , file , * * kwargs ) : if os . access ( file , os . F_OK ) : if file in self . f_repository : raise DuplicationError ( "file already added." ) self . f_repository . append ( file ) else : raise IOError ( "file not found." )
12124	def to_table ( args , vdims = [ ] ) : if not Table : return "HoloViews Table not available" kdims = [ dim for dim in args . constant_keys + args . varying_keys if dim not in vdims ] items = [ tuple ( [ spec [ k ] for k in kdims + vdims ] ) for spec in args . specs ] return Table ( items , kdims = kdims , vdims = vdims )
9749	def create_body_index ( xml_string ) : xml = ET . fromstring ( xml_string ) body_to_index = { } for index , body in enumerate ( xml . findall ( "*/Body/Name" ) ) : body_to_index [ body . text . strip ( ) ] = index return body_to_index
10763	def get_unique_token ( self ) : if self . _unique_token is None : self . _unique_token = self . _random_token ( ) return self . _unique_token
12604	def duplicated ( values : Sequence ) : vals = pd . Series ( values ) return vals [ vals . duplicated ( ) ]
4813	def _document_frequency ( X ) : if sp . isspmatrix_csr ( X ) : return np . bincount ( X . indices , minlength = X . shape [ 1 ] ) return np . diff ( sp . csc_matrix ( X , copy = False ) . indptr )
1767	def execute ( self ) : if issymbolic ( self . PC ) : raise ConcretizeRegister ( self , 'PC' , policy = 'ALL' ) if not self . memory . access_ok ( self . PC , 'x' ) : raise InvalidMemoryAccess ( self . PC , 'x' ) self . _publish ( 'will_decode_instruction' , self . PC ) insn = self . decode_instruction ( self . PC ) self . _last_pc = self . PC self . _publish ( 'will_execute_instruction' , self . PC , insn ) # FIXME (theo) why just return here? if insn . address != self . PC : return name = self . canonicalize_instruction_name ( insn ) if logger . level == logging . DEBUG : logger . debug ( self . render_instruction ( insn ) ) for l in self . render_registers ( ) : register_logger . debug ( l ) try : if self . _concrete and 'SYSCALL' in name : self . emu . sync_unicorn_to_manticore ( ) if self . _concrete and 'SYSCALL' not in name : self . emulate ( insn ) if self . PC == self . _break_unicorn_at : logger . debug ( "Switching from Unicorn to Manticore" ) self . _break_unicorn_at = None self . _concrete = False else : implementation = getattr ( self , name , None ) if implementation is not None : implementation ( * insn . operands ) else : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . warning ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) self . backup_emulate ( insn ) except ( Interruption , Syscall ) as e : e . on_handled = lambda : self . _publish_instruction_as_executed ( insn ) raise e else : self . _publish_instruction_as_executed ( insn )
8247	def str_to_rgb ( self , str ) : str = str . lower ( ) for ch in "_- " : str = str . replace ( ch , "" ) # if named_hues.has_key(str): # clr = color(named_hues[str], 1, 1, mode="hsb") # return clr.r, clr.g, clr.b if named_colors . has_key ( str ) : return named_colors [ str ] for suffix in [ "ish" , "ed" , "y" , "like" ] : str = re . sub ( "(.*?)" + suffix + "$" , "\\1" , str ) str = re . sub ( "(.*?)dd$" , "\\1d" , str ) matches = [ ] for name in named_colors : if name in str or str in name : matches . append ( named_colors [ name ] ) if len ( matches ) > 0 : return choice ( matches ) return named_colors [ "transparent" ]
10933	def check_completion ( self ) : terminate = False term_dict = self . get_termination_stats ( get_cos = self . costol is not None ) terminate |= np . all ( np . abs ( term_dict [ 'delta_vals' ] ) < self . paramtol ) terminate |= ( term_dict [ 'delta_err' ] < self . errtol ) terminate |= ( term_dict [ 'exp_err' ] < self . exptol ) terminate |= ( term_dict [ 'frac_err' ] < self . fractol ) if self . costol is not None : terminate |= ( curcos < term_dict [ 'model_cosine' ] ) return terminate
13043	def create_pipe_workers ( configfile , directory ) : type_map = { 'service' : ServiceSearch , 'host' : HostSearch , 'range' : RangeSearch , 'user' : UserSearch } config = configparser . ConfigParser ( ) config . read ( configfile ) if not len ( config . sections ( ) ) : print_error ( "No named pipes configured" ) return print_notification ( "Starting {} pipes in directory {}" . format ( len ( config . sections ( ) ) , directory ) ) workers = [ ] for name in config . sections ( ) : section = config [ name ] query = create_query ( section ) object_type = type_map [ section [ 'type' ] ] args = ( name , os . path . join ( directory , name ) , object_type , query , section [ 'format' ] , bool ( section . get ( 'unique' , 0 ) ) ) workers . append ( multiprocessing . Process ( target = pipe_worker , args = args ) ) return workers
3301	def element_content_as_string ( element ) : if len ( element ) == 0 : return element . text or "" # Make sure, None is returned as '' stream = compat . StringIO ( ) for childnode in element : stream . write ( xml_to_bytes ( childnode , pretty_print = False ) + "\n" ) # print(xml_to_bytes(childnode, pretty_print=False), file=stream) s = stream . getvalue ( ) stream . close ( ) return s
10852	def otsu_threshold ( data , bins = 255 ) : h0 , x0 = np . histogram ( data . ravel ( ) , bins = bins ) h = h0 . astype ( 'float' ) / h0 . sum ( ) #normalize x = 0.5 * ( x0 [ 1 : ] + x0 [ : - 1 ] ) #bin center wk = np . array ( [ h [ : i + 1 ] . sum ( ) for i in range ( h . size ) ] ) #omega_k mk = np . array ( [ sum ( x [ : i + 1 ] * h [ : i + 1 ] ) for i in range ( h . size ) ] ) #mu_k mt = mk [ - 1 ] #mu_T sb = ( mt * wk - mk ) ** 2 / ( wk * ( 1 - wk ) + 1e-15 ) #sigma_b ind = sb . argmax ( ) return 0.5 * ( x0 [ ind ] + x0 [ ind + 1 ] )
11728	def _assert_contains ( haystack , needle , invert , escape = False ) : myneedle = re . escape ( needle ) if escape else needle matched = re . search ( myneedle , haystack , re . M ) if ( invert and matched ) or ( not invert and not matched ) : raise AssertionError ( "'%s' %sfound in '%s'" % ( needle , "" if invert else "not " , haystack ) )
11311	def get_record ( self ) : self . recid = self . get_recid ( ) self . remove_controlfields ( ) self . update_system_numbers ( ) self . add_systemnumber ( "Inspire" , recid = self . recid ) self . add_control_number ( "003" , "SzGeCERN" ) self . update_collections ( ) self . update_languages ( ) self . update_reportnumbers ( ) self . update_authors ( ) self . update_journals ( ) self . update_subject_categories ( "INSPIRE" , "SzGeCERN" , "categories_cds" ) self . update_pagenumber ( ) self . update_notes ( ) self . update_experiments ( ) self . update_isbn ( ) self . update_dois ( ) self . update_links_and_ffts ( ) self . update_date ( ) self . update_date_year ( ) self . update_hidden_notes ( ) self . update_oai_info ( ) self . update_cnum ( ) self . update_conference_info ( ) self . fields_list = [ "909" , "541" , "961" , "970" , "690" , "695" , "981" , ] self . strip_fields ( ) if "ANNOUNCEMENT" in self . collections : self . update_conference_111 ( ) self . update_conference_links ( ) record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) if "THESIS" in self . collections : self . update_thesis_information ( ) self . update_thesis_supervisors ( ) if "PROCEEDINGS" in self . collections : # Special proceeding syntax self . update_title_to_proceeding ( ) self . update_author_to_proceeding ( ) record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) # 690 tags if self . tag_as_cern : record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CERN" ) ] ) return self . record
309	def plot_cones ( name , bounds , oos_returns , num_samples = 1000 , ax = None , cone_std = ( 1. , 1.5 , 2. ) , random_seed = None , num_strikes = 3 ) : if ax is None : fig = figure . Figure ( figsize = ( 10 , 8 ) ) FigureCanvasAgg ( fig ) axes = fig . add_subplot ( 111 ) else : axes = ax returns = ep . cum_returns ( oos_returns , starting_value = 1. ) bounds_tmp = bounds . copy ( ) returns_tmp = returns . copy ( ) cone_start = returns . index [ 0 ] colors = [ "green" , "orange" , "orangered" , "darkred" ] for c in range ( num_strikes + 1 ) : if c > 0 : tmp = returns . loc [ cone_start : ] bounds_tmp = bounds_tmp . iloc [ 0 : len ( tmp ) ] bounds_tmp = bounds_tmp . set_index ( tmp . index ) crossing = ( tmp < bounds_tmp [ float ( - 2. ) ] . iloc [ : len ( tmp ) ] ) if crossing . sum ( ) <= 0 : break cone_start = crossing . loc [ crossing ] . index [ 0 ] returns_tmp = returns . loc [ cone_start : ] bounds_tmp = ( bounds - ( 1 - returns . loc [ cone_start ] ) ) for std in cone_std : x = returns_tmp . index y1 = bounds_tmp [ float ( std ) ] . iloc [ : len ( returns_tmp ) ] y2 = bounds_tmp [ float ( - std ) ] . iloc [ : len ( returns_tmp ) ] axes . fill_between ( x , y1 , y2 , color = colors [ c ] , alpha = 0.5 ) # Plot returns line graph label = 'Cumulative returns = {:.2f}%' . format ( ( returns . iloc [ - 1 ] - 1 ) * 100 ) axes . plot ( returns . index , returns . values , color = 'black' , lw = 3. , label = label ) if name is not None : axes . set_title ( name ) axes . axhline ( 1 , color = 'black' , alpha = 0.2 ) axes . legend ( frameon = True , framealpha = 0.5 ) if ax is None : return fig else : return axes
12258	def sdcone ( x , rho ) : U , V = np . linalg . eigh ( x ) return V . dot ( np . diag ( np . maximum ( U , 0 ) ) . dot ( V . T ) )
3976	def _expand_libs_in_apps ( specs ) : for app_name , app_spec in specs [ 'apps' ] . iteritems ( ) : if 'depends' in app_spec and 'libs' in app_spec [ 'depends' ] : app_spec [ 'depends' ] [ 'libs' ] = _get_dependent ( 'libs' , app_name , specs , 'apps' )
12081	def figure_protocols ( self ) : self . log . debug ( "creating overlayed protocols plot" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) plt . plot ( self . abf . protoX , self . abf . protoY , color = 'r' ) self . marginX = 0 self . decorate ( protocol = True )
9269	def version_of_first_item ( self ) : try : sections = read_changelog ( self . options ) return sections [ 0 ] [ "version" ] except ( IOError , TypeError ) : return self . get_temp_tag_for_repo_creation ( )
4111	def rc2poly ( kr , r0 = None ) : # Initialize the recursion from . levinson import levup p = len ( kr ) #% p is the order of the prediction polynomial. a = numpy . array ( [ 1 , kr [ 0 ] ] ) #% a is a true polynomial. e = numpy . zeros ( len ( kr ) ) if r0 is None : e0 = 0 else : e0 = r0 e [ 0 ] = e0 * ( 1. - numpy . conj ( numpy . conjugate ( kr [ 0 ] ) * kr [ 0 ] ) ) # Continue the recursion for k=2,3,...,p, where p is the order of the # prediction polynomial. for k in range ( 1 , p ) : [ a , e [ k ] ] = levup ( a , kr [ k ] , e [ k - 1 ] ) efinal = e [ - 1 ] return a , efinal
5593	def tiles_from_geom ( self , geometry , zoom ) : for tile in self . tile_pyramid . tiles_from_geom ( geometry , zoom ) : yield self . tile ( * tile . id )
12310	def pull_stream ( self , uri , * * kwargs ) : return self . protocol . execute ( 'pullStream' , uri = uri , * * kwargs )
814	def pickByDistribution ( distribution , r = None ) : if r is None : r = random x = r . uniform ( 0 , sum ( distribution ) ) for i , d in enumerate ( distribution ) : if x <= d : return i x -= d
5484	def execute ( api ) : try : return api . execute ( ) except Exception as exception : now = datetime . now ( ) . strftime ( '%Y-%m-%d %H:%M:%S.%f' ) _print_error ( '%s: Exception %s: %s' % ( now , type ( exception ) . __name__ , str ( exception ) ) ) # Re-raise exception to be handled by retry logic raise exception
10584	def remove_account ( self , name ) : acc_to_remove = None for a in self . accounts : if a . name == name : acc_to_remove = a if acc_to_remove is not None : self . accounts . remove ( acc_to_remove )
2071	def col_transform ( self , col , digits ) : if col is None or float ( col ) < 0.0 : return None else : col = self . number_to_base ( int ( col ) , self . base , digits ) if len ( col ) == digits : return col else : return [ 0 for _ in range ( digits - len ( col ) ) ] + col
3857	def is_quiet ( self ) : level = self . _conversation . self_conversation_state . notification_level return level == hangouts_pb2 . NOTIFICATION_LEVEL_QUIET
2707	def top_sentences ( kernel , path ) : key_sent = { } i = 0 if isinstance ( path , str ) : path = json_iter ( path ) for meta in path : graf = meta [ "graf" ] tagged_sent = [ WordNode . _make ( x ) for x in graf ] text = " " . join ( [ w . raw for w in tagged_sent ] ) m_sent = mh_digest ( [ str ( w . word_id ) for w in tagged_sent ] ) dist = sum ( [ m_sent . jaccard ( m ) * rl . rank for rl , m in kernel ] ) key_sent [ text ] = ( dist , i ) i += 1 for text , ( dist , i ) in sorted ( key_sent . items ( ) , key = lambda x : x [ 1 ] [ 0 ] , reverse = True ) : yield SummarySent ( dist = dist , idx = i , text = text )
429	def read_image ( image , path = '' ) : return imageio . imread ( os . path . join ( path , image ) )
11220	def compare ( self , jwt : 'Jwt' , compare_dates : bool = False ) -> bool : if self . secret != jwt . secret : return False if self . payload != jwt . payload : return False if self . alg != jwt . alg : return False if self . header != jwt . header : return False expected_claims = self . registered_claims actual_claims = jwt . registered_claims if not compare_dates : strip = [ 'exp' , 'nbf' , 'iat' ] expected_claims = { k : { v if k not in strip else None } for k , v in expected_claims . items ( ) } actual_claims = { k : { v if k not in strip else None } for k , v in actual_claims . items ( ) } if expected_claims != actual_claims : return False return True
8673	def purge_stash ( force , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Purging stash...' ) stash . purge ( force ) # Maybe we should verify that the list is empty # afterwards? click . echo ( 'Purge complete!' ) except GhostError as ex : sys . exit ( ex )
11949	def configure_custom ( self , config ) : c = config . pop ( '()' ) if not hasattr ( c , '__call__' ) and hasattr ( types , 'ClassType' ) and isinstance ( c , types . ClassType ) : c = self . resolve ( c ) props = config . pop ( '.' , None ) # Check for valid identifiers kwargs = dict ( ( k , config [ k ] ) for k in config if valid_ident ( k ) ) result = c ( * * kwargs ) if props : for name , value in props . items ( ) : setattr ( result , name , value ) return result
4642	def _haveKey ( self , key ) : query = ( "SELECT {} FROM {} WHERE {}=?" . format ( self . __value__ , self . __tablename__ , self . __key__ ) , ( key , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False
10250	def is_node_highlighted ( graph : BELGraph , node : BaseEntity ) -> bool : return NODE_HIGHLIGHT in graph . node [ node ]
3791	def refractive_index ( CASRN , T = None , AvailableMethods = False , Method = None , full_info = True ) : def list_methods ( ) : methods = [ ] if CASRN in CRC_RI_organic . index : methods . append ( CRC ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CRC : _RI = float ( CRC_RI_organic . at [ CASRN , 'RI' ] ) if full_info : _T = float ( CRC_RI_organic . at [ CASRN , 'RIT' ] ) elif Method == NONE : _RI , _T = None , None else : raise Exception ( 'Failure in in function' ) if full_info : return _RI , _T else : return _RI
160	def height ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . yy ) - np . min ( self . yy )
3584	def find_device ( self , service_uuids = [ ] , name = None , timeout_sec = TIMEOUT_SEC ) : start = time . time ( ) while True : # Call find_devices and grab the first result if any are found. found = self . find_devices ( service_uuids , name ) if len ( found ) > 0 : return found [ 0 ] # No device was found. Check if the timeout is exceeded and wait to # try again. if time . time ( ) - start >= timeout_sec : # Failed to find a device within the timeout. return None time . sleep ( 1 )
13242	def period ( self ) : start_time = self . root . findtext ( 'daily_start_time' ) if start_time : return Period ( text_to_time ( start_time ) , text_to_time ( self . root . findtext ( 'daily_end_time' ) ) ) return Period ( datetime . time ( 0 , 0 ) , datetime . time ( 23 , 59 ) )
7729	def get_muc_child ( self ) : if self . muc_child : return self . muc_child if not self . xmlnode . children : return None n = self . xmlnode . children while n : if n . name not in ( "x" , "query" ) : n = n . next continue ns = n . ns ( ) if not ns : n = n . next continue ns_uri = ns . getContent ( ) if ( n . name , ns_uri ) == ( "x" , MUC_NS ) : self . muc_child = MucX ( n ) return self . muc_child if ( n . name , ns_uri ) == ( "x" , MUC_USER_NS ) : self . muc_child = MucUserX ( n ) return self . muc_child if ( n . name , ns_uri ) == ( "query" , MUC_ADMIN_NS ) : self . muc_child = MucAdminQuery ( n ) return self . muc_child if ( n . name , ns_uri ) == ( "query" , MUC_OWNER_NS ) : self . muc_child = MucOwnerX ( n ) return self . muc_child n = n . next
1106	def get_matching_blocks ( self ) : if self . matching_blocks is not None : return self . matching_blocks la , lb = len ( self . a ) , len ( self . b ) # This is most naturally expressed as a recursive algorithm, but # at least one user bumped into extreme use cases that exceeded # the recursion limit on their box. So, now we maintain a list # ('queue`) of blocks we still need to look at, and append partial # results to `matching_blocks` in a loop; the matches are sorted # at the end. queue = [ ( 0 , la , 0 , lb ) ] matching_blocks = [ ] while queue : alo , ahi , blo , bhi = queue . pop ( ) i , j , k = x = self . find_longest_match ( alo , ahi , blo , bhi ) # a[alo:i] vs b[blo:j] unknown # a[i:i+k] same as b[j:j+k] # a[i+k:ahi] vs b[j+k:bhi] unknown if k : # if k is 0, there was no matching block matching_blocks . append ( x ) if alo < i and blo < j : queue . append ( ( alo , i , blo , j ) ) if i + k < ahi and j + k < bhi : queue . append ( ( i + k , ahi , j + k , bhi ) ) matching_blocks . sort ( ) # It's possible that we have adjacent equal blocks in the # matching_blocks list now. Starting with 2.5, this code was added # to collapse them. i1 = j1 = k1 = 0 non_adjacent = [ ] for i2 , j2 , k2 in matching_blocks : # Is this block adjacent to i1, j1, k1? if i1 + k1 == i2 and j1 + k1 == j2 : # Yes, so collapse them -- this just increases the length of # the first block by the length of the second, and the first # block so lengthened remains the block to compare against. k1 += k2 else : # Not adjacent. Remember the first block (k1==0 means it's # the dummy we started with), and make the second block the # new block to compare against. if k1 : non_adjacent . append ( ( i1 , j1 , k1 ) ) i1 , j1 , k1 = i2 , j2 , k2 if k1 : non_adjacent . append ( ( i1 , j1 , k1 ) ) non_adjacent . append ( ( la , lb , 0 ) ) self . matching_blocks = map ( Match . _make , non_adjacent ) return self . matching_blocks
8674	def export_keys ( output_path , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Exporting stash to {0}...' . format ( output_path ) ) stash . export ( output_path = output_path ) click . echo ( 'Export complete!' ) except GhostError as ex : sys . exit ( ex )
6220	def create ( self ) : dtype = NP_COMPONENT_DTYPE [ self . component_type . value ] data = numpy . frombuffer ( self . buffer . read ( byte_length = self . byte_length , byte_offset = self . byte_offset ) , count = self . count * self . components , dtype = dtype , ) return dtype , data
905	def write ( self , proto ) : proto . iteration = self . _iteration pHistScores = proto . init ( 'historicalScores' , len ( self . _historicalScores ) ) for i , score in enumerate ( list ( self . _historicalScores ) ) : _ , value , anomalyScore = score record = pHistScores [ i ] record . value = float ( value ) record . anomalyScore = float ( anomalyScore ) if self . _distribution : proto . distribution . name = self . _distribution [ "distribution" ] [ "name" ] proto . distribution . mean = float ( self . _distribution [ "distribution" ] [ "mean" ] ) proto . distribution . variance = float ( self . _distribution [ "distribution" ] [ "variance" ] ) proto . distribution . stdev = float ( self . _distribution [ "distribution" ] [ "stdev" ] ) proto . distribution . movingAverage . windowSize = float ( self . _distribution [ "movingAverage" ] [ "windowSize" ] ) historicalValues = self . _distribution [ "movingAverage" ] [ "historicalValues" ] pHistValues = proto . distribution . movingAverage . init ( "historicalValues" , len ( historicalValues ) ) for i , value in enumerate ( historicalValues ) : pHistValues [ i ] = float ( value ) #proto.distribution.movingAverage.historicalValues = self._distribution["movingAverage"]["historicalValues"] proto . distribution . movingAverage . total = float ( self . _distribution [ "movingAverage" ] [ "total" ] ) historicalLikelihoods = self . _distribution [ "historicalLikelihoods" ] pHistLikelihoods = proto . distribution . init ( "historicalLikelihoods" , len ( historicalLikelihoods ) ) for i , likelihood in enumerate ( historicalLikelihoods ) : pHistLikelihoods [ i ] = float ( likelihood ) proto . probationaryPeriod = self . _probationaryPeriod proto . learningPeriod = self . _learningPeriod proto . reestimationPeriod = self . _reestimationPeriod proto . historicWindowSize = self . _historicalScores . maxlen
3107	def _retrieve_info ( self , http ) : if self . invalid : info = _metadata . get_service_account_info ( http , service_account = self . service_account_email or 'default' ) self . invalid = False self . service_account_email = info [ 'email' ] self . scopes = info [ 'scopes' ]
9075	def sendMultiPart ( smtp , gpg_context , sender , recipients , subject , text , attachments ) : sent = 0 for to in recipients : if not to . startswith ( '<' ) : uid = '<%s>' % to else : uid = to if not checkRecipient ( gpg_context , uid ) : continue msg = MIMEMultipart ( ) msg [ 'From' ] = sender msg [ 'To' ] = to msg [ 'Subject' ] = subject msg [ "Date" ] = formatdate ( localtime = True ) msg . preamble = u'This is an email in encrypted multipart format.' attach = MIMEText ( str ( gpg_context . encrypt ( text . encode ( 'utf-8' ) , uid , always_trust = True ) ) ) attach . set_charset ( 'UTF-8' ) msg . attach ( attach ) for attachment in attachments : with open ( attachment , 'rb' ) as fp : attach = MIMEBase ( 'application' , 'octet-stream' ) attach . set_payload ( str ( gpg_context . encrypt_file ( fp , uid , always_trust = True ) ) ) attach . add_header ( 'Content-Disposition' , 'attachment' , filename = basename ( '%s.pgp' % attachment ) ) msg . attach ( attach ) # TODO: need to catch exception? # yes :-) we need to adjust the status accordingly (>500 so it will be destroyed) smtp . begin ( ) smtp . sendmail ( sender , to , msg . as_string ( ) ) smtp . quit ( ) sent += 1 return sent
5815	def _read_callback ( connection_id , data_buffer , data_length_pointer ) : self = None try : self = _connection_refs . get ( connection_id ) if not self : socket = _socket_refs . get ( connection_id ) else : socket = self . _socket if not self and not socket : return 0 bytes_requested = deref ( data_length_pointer ) timeout = socket . gettimeout ( ) error = None data = b'' try : while len ( data ) < bytes_requested : # Python 2 on Travis CI seems to have issues with blocking on # recv() for longer than the socket timeout value, so we select if timeout is not None and timeout > 0.0 : read_ready , _ , _ = select . select ( [ socket ] , [ ] , [ ] , timeout ) if len ( read_ready ) == 0 : raise socket_ . error ( errno . EAGAIN , 'timed out' ) chunk = socket . recv ( bytes_requested - len ( data ) ) data += chunk if chunk == b'' : if len ( data ) == 0 : if timeout is None : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort break except ( socket_ . error ) as e : error = e . errno if error is not None and error != errno . EAGAIN : if error == errno . ECONNRESET or error == errno . EPIPE : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort if self and not self . _done_handshake : # SecureTransport doesn't bother to check if the TLS record header # is valid before asking to read more data, which can result in # connection hangs. Here we do basic checks to get around the issue. if len ( data ) >= 3 and len ( self . _server_hello ) == 0 : # Check to ensure it is an alert or handshake first valid_record_type = data [ 0 : 1 ] in set ( [ b'\x15' , b'\x16' ] ) # Check if the protocol version is SSL 3.0 or TLS 1.0-1.3 valid_protocol_version = data [ 1 : 3 ] in set ( [ b'\x03\x00' , b'\x03\x01' , b'\x03\x02' , b'\x03\x03' , b'\x03\x04' ] ) if not valid_record_type or not valid_protocol_version : self . _server_hello += data + _read_remaining ( socket ) return SecurityConst . errSSLProtocol self . _server_hello += data write_to_buffer ( data_buffer , data ) pointer_set ( data_length_pointer , len ( data ) ) if len ( data ) != bytes_requested : return SecurityConst . errSSLWouldBlock return 0 except ( KeyboardInterrupt ) as e : if self : self . _exception = e return SecurityConst . errSSLClosedAbort
177	def concatenate ( self , other ) : if not isinstance ( other , LineString ) : other = LineString ( other ) return self . deepcopy ( coords = np . concatenate ( [ self . coords , other . coords ] , axis = 0 ) )
3866	async def rename ( self , name ) : await self . _client . rename_conversation ( hangouts_pb2 . RenameConversationRequest ( request_header = self . _client . get_request_header ( ) , new_name = name , event_request_header = self . _get_event_request_header ( ) , ) )
7075	def run_periodfinding ( simbasedir , pfmethods = ( 'gls' , 'pdm' , 'bls' ) , pfkwargs = ( { } , { } , { 'startp' : 1.0 , 'maxtransitduration' : 0.3 } ) , getblssnr = False , sigclip = 5.0 , nperiodworkers = 10 , ncontrolworkers = 4 , liststartindex = None , listmaxobjects = None ) : # get the info from the simbasedir with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) lcfpaths = siminfo [ 'lcfpath' ] pfdir = os . path . join ( simbasedir , 'periodfinding' ) # get the column defs for the fakelcs timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] # register the fakelc pklc as a custom lcproc format # now we should be able to use all lcproc functions correctly fakelc_formatkey = 'fake-%s' % siminfo [ 'lcformat' ] lcproc . register_lcformat ( fakelc_formatkey , '*-fakelc.pkl' , timecols , magcols , errcols , 'astrobase.lcproc' , '_read_pklc' , magsarefluxes = siminfo [ 'magsarefluxes' ] ) if liststartindex : lcfpaths = lcfpaths [ liststartindex : ] if listmaxobjects : lcfpaths = lcfpaths [ : listmaxobjects ] pfinfo = periodsearch . parallel_pf ( lcfpaths , pfdir , lcformat = fakelc_formatkey , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nperiodworkers = nperiodworkers , ncontrolworkers = ncontrolworkers ) with open ( os . path . join ( simbasedir , 'fakelc-periodsearch.pkl' ) , 'wb' ) as outfd : pickle . dump ( pfinfo , outfd , pickle . HIGHEST_PROTOCOL ) return os . path . join ( simbasedir , 'fakelc-periodsearch.pkl' )
1364	def get_argument_length ( self ) : try : length = self . get_argument ( constants . PARAM_LENGTH ) return length except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
11490	def _download_item ( item_id , path = '.' , item = None ) : session . token = verify_credentials ( ) filename , content_iter = session . communicator . download_item ( item_id , session . token ) item_path = os . path . join ( path , filename ) print ( 'Creating file at {0}' . format ( item_path ) ) out_file = open ( item_path , 'wb' ) for block in content_iter : out_file . write ( block ) out_file . close ( ) for callback in session . item_download_callbacks : if not item : item = session . communicator . item_get ( session . token , item_id ) callback ( session . communicator , session . token , item , item_path )
8062	def do_help ( self , arg ) : print ( self . response_prompt , file = self . stdout ) return cmd . Cmd . do_help ( self , arg )
7545	def calculate_depths ( data , samples , lbview ) : ## send jobs to be processed on engines start = time . time ( ) printstr = " calculating depths | {} | s5 |" recaljobs = { } maxlens = [ ] for sample in samples : recaljobs [ sample . name ] = lbview . apply ( recal_hidepth , * ( data , sample ) ) ## block until finished while 1 : ready = [ i . ready ( ) for i in recaljobs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break ## check for failures and collect results modsamples = [ ] for sample in samples : if not recaljobs [ sample . name ] . successful ( ) : LOGGER . error ( " sample %s failed: %s" , sample . name , recaljobs [ sample . name ] . exception ( ) ) else : modsample , _ , maxlen , _ , _ = recaljobs [ sample . name ] . result ( ) modsamples . append ( modsample ) maxlens . append ( maxlen ) ## reset global maxlen if something changed data . _hackersonly [ "max_fragment_length" ] = int ( max ( maxlens ) ) + 4 return samples
8729	def strptime ( s , fmt , tzinfo = None ) : res = time . strptime ( s , fmt ) return datetime . datetime ( tzinfo = tzinfo , * res [ : 6 ] )
12532	def _store_dicom_paths ( self , folders ) : if isinstance ( folders , str ) : folders = [ folders ] for folder in folders : if not os . path . exists ( folder ) : raise FolderNotFound ( folder ) self . items . extend ( list ( find_all_dicom_files ( folder ) ) )
3587	def remove ( self , cbobject ) : with self . _lock : if cbobject in self . _metadata : del self . _metadata [ cbobject ]
3472	def subtract_metabolites ( self , metabolites , combine = True , reversibly = True ) : self . add_metabolites ( { k : - v for k , v in iteritems ( metabolites ) } , combine = combine , reversibly = reversibly )
6531	def get_user_config ( project_path , use_cache = True ) : if sys . platform == 'win32' : user_config = os . path . expanduser ( r'~\\tidypy' ) else : user_config = os . path . join ( os . getenv ( 'XDG_CONFIG_HOME' ) or os . path . expanduser ( '~/.config' ) , 'tidypy' ) if os . path . exists ( user_config ) : with open ( user_config , 'r' ) as config_file : config = pytoml . load ( config_file ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
3487	def _check ( value , message ) : if value is None : LOGGER . error ( 'Error: LibSBML returned a null value trying ' 'to <' + message + '>.' ) elif type ( value ) is int : if value == libsbml . LIBSBML_OPERATION_SUCCESS : return else : LOGGER . error ( 'Error encountered trying to <' + message + '>.' ) LOGGER . error ( 'LibSBML error code {}: {}' . format ( str ( value ) , libsbml . OperationReturnValue_toString ( value ) . strip ( ) ) ) else : return
6007	def load_image ( image_path , image_hdu , pixel_scale ) : return ScaledSquarePixelArray . from_fits_with_pixel_scale ( file_path = image_path , hdu = image_hdu , pixel_scale = pixel_scale )
5121	def reset_colors ( self ) : for k , e in enumerate ( self . g . edges ( ) ) : self . g . set_ep ( e , 'edge_color' , self . edge2queue [ k ] . colors [ 'edge_color' ] ) for v in self . g . nodes ( ) : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_fill_color' ] )
1808	def SETC ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
11156	def print_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table = sorted ( [ ( p , p . size ) for p in self . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size_table [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size ) , p . abspath ) )
12346	def compress ( self , delete_tif = False , folder = None ) : return compress ( self . images , delete_tif , folder )
13444	def _rindex ( mylist : Sequence [ T ] , x : T ) -> int : return len ( mylist ) - mylist [ : : - 1 ] . index ( x ) - 1
7503	def _byteify ( data , ignore_dicts = False ) : if isinstance ( data , unicode ) : return data . encode ( "utf-8" ) if isinstance ( data , list ) : return [ _byteify ( item , ignore_dicts = True ) for item in data ] if isinstance ( data , dict ) and not ignore_dicts : return { _byteify ( key , ignore_dicts = True ) : _byteify ( value , ignore_dicts = True ) for key , value in data . iteritems ( ) } return data
2055	def ADDW ( cpu , dest , src , add ) : aligned_pc = ( cpu . instruction . address + 4 ) & 0xfffffffc if src . type == 'register' and src . reg in ( 'PC' , 'R15' ) : src = aligned_pc else : src = src . read ( ) dest . write ( src + add . read ( ) )
4098	def FPE ( N , rho , k = None ) : #k #todo check convention. agrees with octave fpe = rho * ( N + k + 1. ) / ( N - k - 1 ) return fpe
11954	def upload_gif ( gif ) : client_id = os . environ . get ( 'IMGUR_API_ID' ) client_secret = os . environ . get ( 'IMGUR_API_SECRET' ) if client_id is None or client_secret is None : click . echo ( 'Cannot upload - could not find IMGUR_API_ID or IMGUR_API_SECRET environment variables' ) return client = ImgurClient ( client_id , client_secret ) click . echo ( 'Uploading file {}' . format ( click . format_filename ( gif ) ) ) response = client . upload_from_path ( gif ) click . echo ( 'File uploaded - see your gif at {}' . format ( response [ 'link' ] ) )
3679	def economic_status ( self ) : if self . __economic_status : return self . __economic_status else : self . __economic_status = economic_status ( self . CAS , Method = 'Combined' ) return self . __economic_status
59	def intersection ( self , other , default = None ) : x1_i = max ( self . x1 , other . x1 ) y1_i = max ( self . y1 , other . y1 ) x2_i = min ( self . x2 , other . x2 ) y2_i = min ( self . y2 , other . y2 ) if x1_i > x2_i or y1_i > y2_i : return default else : return BoundingBox ( x1 = x1_i , y1 = y1_i , x2 = x2_i , y2 = y2_i )
13129	def initialize_indices ( ) : Host . init ( ) Range . init ( ) Service . init ( ) User . init ( ) Credential . init ( ) Log . init ( )
9372	def read_stream ( schema , stream , * , buffer_size = io . DEFAULT_BUFFER_SIZE ) : reader = _lancaster . Reader ( schema ) buf = stream . read ( buffer_size ) remainder = b'' while len ( buf ) > 0 : values , n = reader . read_seq ( buf ) yield from values remainder = buf [ n : ] buf = stream . read ( buffer_size ) if len ( buf ) > 0 and len ( remainder ) > 0 : ba = bytearray ( ) ba . extend ( remainder ) ba . extend ( buf ) buf = memoryview ( ba ) . tobytes ( ) if len ( remainder ) > 0 : raise EOFError ( '{} bytes remaining but could not continue reading ' 'from stream' . format ( len ( remainder ) ) )
2929	def write_manifest ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'Manifest' ) for f in sorted ( self . manifest . keys ( ) ) : config . set ( 'Manifest' , f . replace ( '\\' , '/' ) . lower ( ) , self . manifest [ f ] ) ini = StringIO ( ) config . write ( ini ) self . manifest_data = ini . getvalue ( ) self . package_zip . writestr ( self . MANIFEST_FILE , self . manifest_data )
13763	def _wrap_color ( self , code , text , format = None , style = None ) : color = None if code [ : 3 ] == self . bg . PREFIX : color = self . bg . COLORS . get ( code , None ) if not color : color = self . fg . COLORS . get ( code , None ) if not color : raise Exception ( 'Color code not found' ) if format and format not in self . formats : raise Exception ( 'Color format not found' ) fmt = "0;" if format == 'bold' : fmt = "1;" elif format == 'underline' : fmt = "4;" # Manage the format parts = color . split ( '[' ) color = '{0}[{1}{2}' . format ( parts [ 0 ] , fmt , parts [ 1 ] ) if self . has_colors and self . colors_enabled : # Set brightness st = '' if style : st = self . st . COLORS . get ( style , '' ) return "{0}{1}{2}{3}" . format ( st , color , text , self . st . COLORS [ 'reset_all' ] ) else : return text
13006	def utime ( self , * args , * * kwargs ) : os . utime ( self . extended_path , * args , * * kwargs )
1305	def GetConsoleOriginalTitle ( ) -> str : if IsNT6orHigher : arrayType = ctypes . c_wchar * MAX_PATH values = arrayType ( ) ctypes . windll . kernel32 . GetConsoleOriginalTitleW ( values , MAX_PATH ) return values . value else : raise RuntimeError ( 'GetConsoleOriginalTitle is not supported on Windows XP or lower.' )
3807	def nested_formula_parser ( formula , check = True ) : formula = formula . replace ( '[' , '' ) . replace ( ']' , '' ) charge_splits = bracketed_charge_re . split ( formula ) if len ( charge_splits ) > 1 : formula = charge_splits [ 0 ] else : formula = formula . split ( '+' ) [ 0 ] . split ( '-' ) [ 0 ] stack = [ [ ] ] last = stack [ 0 ] tokens = formula_token_matcher_rational . findall ( formula ) # The set of letters in the tokens should match the set of letters if check : token_letters = set ( [ j for i in tokens for j in i if j in letter_set ] ) formula_letters = set ( i for i in formula if i in letter_set ) if formula_letters != token_letters : raise Exception ( 'Input may not be a formula; extra letters were detected' ) for token in tokens : if token == "(" : stack . append ( [ ] ) last = stack [ - 1 ] elif token == ")" : temp_dict = { } for d in last : for ele , count in d . items ( ) : if ele in temp_dict : temp_dict [ ele ] = temp_dict [ ele ] + count else : temp_dict [ ele ] = count stack . pop ( ) last = stack [ - 1 ] last . append ( temp_dict ) elif token . isalpha ( ) : last . append ( { token : 1 } ) else : v = float ( token ) v_int = int ( v ) if v_int == v : v = v_int last [ - 1 ] = { ele : count * v for ele , count in last [ - 1 ] . items ( ) } ans = { } for d in last : for ele , count in d . items ( ) : if ele in ans : ans [ ele ] = ans [ ele ] + count else : ans [ ele ] = count return ans
3486	def _check_required ( sbase , value , attribute ) : if ( value is None ) or ( value == "" ) : msg = "Required attribute '%s' cannot be found or parsed in '%s'" % ( attribute , sbase ) if hasattr ( sbase , "getId" ) and sbase . getId ( ) : msg += " with id '%s'" % sbase . getId ( ) elif hasattr ( sbase , "getName" ) and sbase . getName ( ) : msg += " with name '%s'" % sbase . getName ( ) elif hasattr ( sbase , "getMetaId" ) and sbase . getMetaId ( ) : msg += " with metaId '%s'" % sbase . getName ( ) raise CobraSBMLError ( msg ) return value
12634	def transform ( self ) : if self . dcmf1 is None or self . dcmf2 is None : return np . inf for field_name in self . field_weights : if ( str ( getattr ( self . dcmf1 , field_name , '' ) ) != str ( getattr ( self . dcmf2 , field_name , '' ) ) ) : return False return True
11899	def _get_src_from_image ( img , fallback_image_file ) : # If the image is None, then we can't process, so we should return the # path to the file itself if img is None : return fallback_image_file # Target format should be the same as the original image format, unless it's # a TIF/TIFF, which can't be displayed by most browsers; we convert these # to jpeg target_format = img . format if target_format . lower ( ) in [ 'tif' , 'tiff' ] : target_format = 'JPEG' # If we have an actual Image, great - put together the base64 image string try : bytesio = io . BytesIO ( ) img . save ( bytesio , target_format ) byte_value = bytesio . getvalue ( ) b64 = base64 . b64encode ( byte_value ) return 'data:image/%s;base64,%s' % ( target_format . lower ( ) , b64 ) except IOError as exptn : print ( 'IOError while saving image bytes: %s' % exptn ) return fallback_image_file
12817	def _length ( self ) : self . _build_chunk_headers ( ) length = 0 if self . _data : for field in self . _data : length += len ( self . _chunk_headers [ field ] ) length += len ( self . _data [ field ] ) length += 2 if self . _files : for field in self . _files : length += len ( self . _chunk_headers [ field ] ) length += self . _file_size ( field ) length += 2 length += len ( self . boundary ) length += 6 return length
10521	def oneright ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarhorizontal ( window_name , object_name ) : raise LdtpServerException ( 'Object not horizontal scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 maxValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue >= 1 : raise LdtpServerException ( 'Maximum limit reached' ) object_handle . AXValue += maxValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to increase scrollbar' )
13169	def _match ( self , pred ) : if not pred : return True # Strip off the [ and ] pred = pred [ 1 : - 1 ] if pred . startswith ( '@' ) : # An attribute predicate checks the existence (and optionally value) of an attribute on this tag. pred = pred [ 1 : ] if '=' in pred : attr , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '"' , "'" ) : value = value [ 1 : ] if value [ - 1 ] in ( '"' , "'" ) : value = value [ : - 1 ] return self . attrs . get ( attr ) == value else : return pred in self . attrs elif num_re . match ( pred ) : # An index predicate checks whether we are the n-th child of our parent (0-based). index = int ( pred ) if index < 0 : if self . parent : # For negative indexes, count from the end of the list. return self . index == ( len ( self . parent . _children ) + index ) else : # If we're the root node, the only index we could be is 0. return index == 0 else : return index == self . index else : if '=' in pred : tag , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '"' , "'" ) : value = value [ 1 : ] if value [ - 1 ] in ( '"' , "'" ) : value = value [ : - 1 ] for c in self . _children : if c . tagname == tag and c . data == value : return True else : # A plain [tag] predicate means we match if we have a child with tagname "tag". for c in self . _children : if c . tagname == pred : return True return False
3306	def _run__cherrypy ( app , config , mode ) : assert mode == "cherrypy-wsgiserver" try : from cherrypy import wsgiserver from cherrypy . wsgiserver . ssl_builtin import BuiltinSSLAdapter _logger . warning ( "WARNING: cherrypy.wsgiserver is deprecated." ) _logger . warning ( " Starting with CherryPy 9.0 the functionality from cherrypy.wsgiserver" ) _logger . warning ( " was moved to the cheroot project." ) _logger . warning ( " Consider using --server=cheroot." ) except ImportError : _logger . error ( "*" * 78 ) _logger . error ( "ERROR: Could not import cherrypy.wsgiserver." ) _logger . error ( "Try `pip install cherrypy` or specify another server using the --server option." ) _logger . error ( "Note that starting with CherryPy 9.0, the server was moved to" ) _logger . error ( "the cheroot project, so it is recommended to use `-server=cheroot`" ) _logger . error ( "and run `pip install cheroot` instead." ) _logger . error ( "*" * 78 ) raise server_name = "WsgiDAV/{} {} Python/{}" . format ( __version__ , wsgiserver . CherryPyWSGIServer . version , util . PYTHON_VERSION ) wsgiserver . CherryPyWSGIServer . version = server_name # Support SSL ssl_certificate = _get_checked_path ( config . get ( "ssl_certificate" ) , config ) ssl_private_key = _get_checked_path ( config . get ( "ssl_private_key" ) , config ) ssl_certificate_chain = _get_checked_path ( config . get ( "ssl_certificate_chain" ) , config ) protocol = "http" if ssl_certificate : assert ssl_private_key wsgiserver . CherryPyWSGIServer . ssl_adapter = BuiltinSSLAdapter ( ssl_certificate , ssl_private_key , ssl_certificate_chain ) protocol = "https" _logger . info ( "SSL / HTTPS enabled." ) _logger . info ( "Running {}" . format ( server_name ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , "server_name" : server_name , } # Override or add custom args server_args . update ( config . get ( "server_args" , { } ) ) server = wsgiserver . CherryPyWSGIServer ( * * server_args ) # If the caller passed a startup event, monkey patch the server to set it # when the request handler loop is entered startup_event = config . get ( "startup_event" ) if startup_event : def _patched_tick ( ) : server . tick = org_tick # undo the monkey patch org_tick ( ) _logger . info ( "CherryPyWSGIServer is ready" ) startup_event . set ( ) org_tick = server . tick server . tick = _patched_tick try : server . start ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) finally : server . stop ( ) return
6199	def simulate_diffusion ( self , save_pos = False , total_emission = True , radial = False , rs = None , seed = 1 , path = './' , wrap_func = wrap_periodic , chunksize = 2 ** 19 , chunkslice = 'times' , verbose = True ) : if rs is None : rs = np . random . RandomState ( seed = seed ) self . open_store_traj ( chunksize = chunksize , chunkslice = chunkslice , radial = radial , path = path ) # Save current random state for reproducibility self . traj_group . _v_attrs [ 'init_random_state' ] = rs . get_state ( ) em_store = self . emission_tot if total_emission else self . emission print ( '- Start trajectories simulation - %s' % ctime ( ) , flush = True ) if verbose : print ( '[PID %d] Diffusion time:' % os . getpid ( ) , end = '' ) i_chunk = 0 t_chunk_size = self . emission . chunkshape [ 1 ] chunk_duration = t_chunk_size * self . t_step par_start_pos = self . particles . positions prev_time = 0 for time_size in iter_chunksize ( self . n_samples , t_chunk_size ) : if verbose : curr_time = int ( chunk_duration * ( i_chunk + 1 ) ) if curr_time > prev_time : print ( ' %ds' % curr_time , end = '' , flush = True ) prev_time = curr_time POS , em = self . _sim_trajectories ( time_size , par_start_pos , rs , total_emission = total_emission , save_pos = save_pos , radial = radial , wrap_func = wrap_func ) ## Append em to the permanent storage # if total_emission, data is just a linear array # otherwise is a 2-D array (self.num_particles, c_size) em_store . append ( em ) if save_pos : self . position . append ( np . vstack ( POS ) . astype ( 'float32' ) ) i_chunk += 1 self . store . h5file . flush ( ) # Save current random state self . traj_group . _v_attrs [ 'last_random_state' ] = rs . get_state ( ) self . store . h5file . flush ( ) print ( '\n- End trajectories simulation - %s' % ctime ( ) , flush = True )
5903	def glob_parts ( prefix , ext ) : if ext . startswith ( '.' ) : ext = ext [ 1 : ] files = glob . glob ( prefix + '.' + ext ) + glob . glob ( prefix + '.part[0-9][0-9][0-9][0-9].' + ext ) files . sort ( ) # at least some rough sorting... return files
9529	def get_encrypted_field ( base_class ) : assert not isinstance ( base_class , models . Field ) field_name = 'Encrypted' + base_class . __name__ if base_class not in FIELD_CACHE : FIELD_CACHE [ base_class ] = type ( field_name , ( EncryptedMixin , base_class ) , { 'base_class' : base_class , } ) return FIELD_CACHE [ base_class ]
12598	def read_xl ( xl_path : str ) : xl_path , choice = _check_xl_path ( xl_path ) reader = XL_READERS [ choice ] return reader ( xl_path )
12781	def set_name ( self , name ) : if not self . _campfire . get_user ( ) . admin : return False result = self . _connection . put ( "room/%s" % self . id , { "room" : { "name" : name } } ) if result [ "success" ] : self . _load ( ) return result [ "success" ]
11750	def _register_blueprint ( self , app , bp , bundle_path , child_path , description ) : base_path = sanitize_path ( self . _journey_path + bundle_path + child_path ) app . register_blueprint ( bp , url_prefix = base_path ) return { 'name' : bp . name , 'path' : child_path , 'import_name' : bp . import_name , 'description' : description , 'routes' : self . get_blueprint_routes ( app , base_path ) }
1342	def samples ( dataset = 'imagenet' , index = 0 , batchsize = 1 , shape = ( 224 , 224 ) , data_format = 'channels_last' ) : from PIL import Image images , labels = [ ] , [ ] basepath = os . path . dirname ( __file__ ) samplepath = os . path . join ( basepath , 'data' ) files = os . listdir ( samplepath ) for idx in range ( index , index + batchsize ) : i = idx % 20 # get filename and label file = [ n for n in files if '{}_{:02d}_' . format ( dataset , i ) in n ] [ 0 ] label = int ( file . split ( '.' ) [ 0 ] . split ( '_' ) [ - 1 ] ) # open file path = os . path . join ( samplepath , file ) image = Image . open ( path ) if dataset == 'imagenet' : image = image . resize ( shape ) image = np . asarray ( image , dtype = np . float32 ) if dataset != 'mnist' and data_format == 'channels_first' : image = np . transpose ( image , ( 2 , 0 , 1 ) ) images . append ( image ) labels . append ( label ) labels = np . array ( labels ) images = np . stack ( images ) return images , labels
67	def extract_from_image ( self , image , pad = True , pad_max = None , prevent_zero_size = True ) : pad_top = 0 pad_right = 0 pad_bottom = 0 pad_left = 0 height , width = image . shape [ 0 ] , image . shape [ 1 ] x1 , x2 , y1 , y2 = self . x1_int , self . x2_int , self . y1_int , self . y2_int # When y values get into the range (H-0.5, H), the *_int functions round them to H. # That is technically sensible, but in the case of extraction leads to a black border, # which is both ugly and unexpected after calling cut_out_of_image(). Here we correct for # that because of beauty reasons. # Same is the case for x coordinates. fully_within = self . is_fully_within_image ( image ) if fully_within : y1 , y2 = np . clip ( [ y1 , y2 ] , 0 , height - 1 ) x1 , x2 = np . clip ( [ x1 , x2 ] , 0 , width - 1 ) # TODO add test if prevent_zero_size : if abs ( x2 - x1 ) < 1 : x2 = x1 + 1 if abs ( y2 - y1 ) < 1 : y2 = y1 + 1 if pad : # if the bb is outside of the image area, the following pads the image # first with black pixels until the bb is inside the image # and only then extracts the image area # TODO probably more efficient to initialize an array of zeros # and copy only the portions of the bb into that array that are # natively inside the image area if x1 < 0 : pad_left = abs ( x1 ) x2 = x2 + pad_left width = width + pad_left x1 = 0 if y1 < 0 : pad_top = abs ( y1 ) y2 = y2 + pad_top height = height + pad_top y1 = 0 if x2 >= width : pad_right = x2 - width if y2 >= height : pad_bottom = y2 - height paddings = [ pad_top , pad_right , pad_bottom , pad_left ] any_padded = any ( [ val > 0 for val in paddings ] ) if any_padded : if pad_max is None : pad_max = max ( paddings ) image = ia . pad ( image , top = min ( pad_top , pad_max ) , right = min ( pad_right , pad_max ) , bottom = min ( pad_bottom , pad_max ) , left = min ( pad_left , pad_max ) ) return image [ y1 : y2 , x1 : x2 ] else : within_image = ( ( 0 , 0 , 0 , 0 ) <= ( x1 , y1 , x2 , y2 ) < ( width , height , width , height ) ) out_height , out_width = ( y2 - y1 ) , ( x2 - x1 ) nonzero_height = ( out_height > 0 ) nonzero_width = ( out_width > 0 ) if within_image and nonzero_height and nonzero_width : return image [ y1 : y2 , x1 : x2 ] if prevent_zero_size : out_height = 1 out_width = 1 else : out_height = 0 out_width = 0 if image . ndim == 2 : return np . zeros ( ( out_height , out_width ) , dtype = image . dtype ) return np . zeros ( ( out_height , out_width , image . shape [ - 1 ] ) , dtype = image . dtype )
11322	def generate_dirlist_html ( FS , filepath ) : yield '<table class="dirlist">' if filepath == '/' : filepath = '' for name in FS . listdir ( filepath ) : full_path = pathjoin ( filepath , name ) if FS . isdir ( full_path ) : full_path = full_path + '/' yield u'<tr><td><a href="{0}">{0}</a></td></tr>' . format ( cgi . escape ( full_path ) ) # TODO XXX yield '</table>'
8820	def delete_network ( context , id ) : LOG . info ( "delete_network %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : net = db_api . network_find ( context = context , limit = None , sorts = [ 'id' ] , marker = None , page_reverse = False , id = id , scope = db_api . ONE ) if not net : raise n_exc . NetworkNotFound ( net_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( net . id ) : raise n_exc . NotAuthorized ( net_id = id ) if net . ports : raise n_exc . NetworkInUse ( net_id = id ) net_driver = registry . DRIVER_REGISTRY . get_driver ( net [ "network_plugin" ] ) net_driver . delete_network ( context , id ) for subnet in net [ "subnets" ] : subnets . _delete_subnet ( context , subnet ) db_api . network_delete ( context , net )
9068	def _lml_arbitrary_scale ( self ) : s = self . scale D = self . _D n = len ( self . _y ) lml = - self . _df * log2pi - n * log ( s ) lml -= sum ( npsum ( log ( d ) ) for d in D ) d = ( mTQ - yTQ for ( mTQ , yTQ ) in zip ( self . _mTQ , self . _yTQ ) ) lml -= sum ( ( i / j ) @ i for ( i , j ) in zip ( d , D ) ) / s return lml / 2
844	def _rebuildPartitionIdMap ( self , partitionIdList ) : self . _partitionIdMap = { } for row , partitionId in enumerate ( partitionIdList ) : indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( row ) self . _partitionIdMap [ partitionId ] = indices
11529	def upload_json_results ( self , token , filepath , community_id , producer_display_name , metric_name , producer_revision , submit_time , * * kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'communityId' ] = community_id parameters [ 'producerDisplayName' ] = producer_display_name parameters [ 'metricName' ] = metric_name parameters [ 'producerRevision' ] = producer_revision parameters [ 'submitTime' ] = submit_time optional_keys = [ 'config_item_id' , 'test_dataset_id' , 'truth_dataset_id' , 'silent' , 'unofficial' , 'build_results_url' , 'branch' , 'extra_urls' , 'params' ] for key in optional_keys : if key in kwargs : if key == 'config_item_id' : parameters [ 'configItemId' ] = kwargs [ key ] elif key == 'test_dataset_id' : parameters [ 'testDatasetId' ] = kwargs [ key ] elif key == 'truth_dataset_id' : parameters [ 'truthDatasetId' ] = kwargs [ key ] elif key == 'parent_keys' : parameters [ 'parentKeys' ] = kwargs [ key ] elif key == 'build_results_url' : parameters [ 'buildResultsUrl' ] = kwargs [ key ] elif key == 'extra_urls' : parameters [ 'extraUrls' ] = json . dumps ( kwargs [ key ] ) elif key == 'params' : parameters [ key ] = json . dumps ( kwargs [ key ] ) elif key == 'silent' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'unofficial' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] else : parameters [ key ] = kwargs [ key ] file_payload = open ( filepath , 'rb' ) response = self . request ( 'midas.tracker.results.upload.json' , parameters , file_payload ) return response
11302	def provider_for_url ( self , url ) : for provider , regex in self . get_registry ( ) . items ( ) : if re . match ( regex , url ) is not None : return provider raise OEmbedMissingEndpoint ( 'No endpoint matches URL: %s' % url )
9501	def _disassemble ( self , lineno_width = 3 , mark_as_current = False ) : fields = [ ] # Column: Source code line number if lineno_width : if self . starts_line is not None : lineno_fmt = "%%%dd" % lineno_width fields . append ( lineno_fmt % self . starts_line ) else : fields . append ( ' ' * lineno_width ) # Column: Current instruction indicator if mark_as_current : fields . append ( '-->' ) else : fields . append ( ' ' ) # Column: Jump target marker if self . is_jump_target : fields . append ( '>>' ) else : fields . append ( ' ' ) # Column: Instruction offset from start of code sequence fields . append ( repr ( self . offset ) . rjust ( 4 ) ) # Column: Opcode name fields . append ( self . opname . ljust ( 20 ) ) # Column: Opcode argument if self . arg is not None : fields . append ( repr ( self . arg ) . rjust ( 5 ) ) # Column: Opcode argument details if self . argrepr : fields . append ( '(' + self . argrepr + ')' ) return ' ' . join ( fields ) . rstrip ( )
8220	def do_window_close ( self , widget , data = None ) : publish_event ( QUIT_EVENT ) if self . has_server : self . sock . close ( ) self . hide_variables_window ( ) self . destroy ( ) self . window_open = False
6787	def preview ( self , components = None , ask = 0 ) : ask = int ( ask ) self . init ( ) component_order , plan_funcs = self . get_component_funcs ( components = components ) print ( '\n%i changes found for host %s.\n' % ( len ( component_order ) , self . genv . host_string ) ) if component_order and plan_funcs : if self . verbose : print ( 'These components have changed:\n' ) for component in sorted ( component_order ) : print ( ( ' ' * 4 ) + component ) print ( 'Deployment plan for host %s:\n' % self . genv . host_string ) for func_name , _ in plan_funcs : print ( success_str ( ( ' ' * 4 ) + func_name ) ) if component_order : print ( ) if ask and self . genv . host_string == self . genv . hosts [ - 1 ] : if component_order : if not raw_input ( 'Begin deployment? [yn] ' ) . strip ( ) . lower ( ) . startswith ( 'y' ) : sys . exit ( 0 ) else : sys . exit ( 0 )
6953	def bootstrap_falsealarmprob ( lspinfo , times , mags , errs , nbootstrap = 250 , magsarefluxes = False , sigclip = 10.0 , npeaks = None ) : # figure out how many periods to work on if ( npeaks and ( 0 < npeaks < len ( lspinfo [ 'nbestperiods' ] ) ) ) : nperiods = npeaks else : LOGWARNING ( 'npeaks not specified or invalid, ' 'getting FAP for all %s periodogram peaks' % len ( lspinfo [ 'nbestperiods' ] ) ) nperiods = len ( lspinfo [ 'nbestperiods' ] ) nbestperiods = lspinfo [ 'nbestperiods' ] [ : nperiods ] nbestpeaks = lspinfo [ 'nbestlspvals' ] [ : nperiods ] # get rid of nans first and sigclip stimes , smags , serrs = sigclip_magseries ( times , mags , errs , magsarefluxes = magsarefluxes , sigclip = sigclip ) allpeaks = [ ] allperiods = [ ] allfaps = [ ] alltrialbestpeaks = [ ] # make sure there are enough points to calculate a spectrum if len ( stimes ) > 9 and len ( smags ) > 9 and len ( serrs ) > 9 : for ind , period , peak in zip ( range ( len ( nbestperiods ) ) , nbestperiods , nbestpeaks ) : LOGINFO ( 'peak %s: running %s trials...' % ( ind + 1 , nbootstrap ) ) trialbestpeaks = [ ] for _trial in range ( nbootstrap ) : # get a scrambled index tindex = np . random . randint ( 0 , high = mags . size , size = mags . size ) # get the kwargs dict out of the lspinfo if 'kwargs' in lspinfo : kwargs = lspinfo [ 'kwargs' ] # update the kwargs with some local stuff kwargs . update ( { 'magsarefluxes' : magsarefluxes , 'sigclip' : sigclip , 'verbose' : False } ) else : kwargs = { 'magsarefluxes' : magsarefluxes , 'sigclip' : sigclip , 'verbose' : False } # run the periodogram with scrambled mags and errs # and the appropriate keyword arguments lspres = LSPMETHODS [ lspinfo [ 'method' ] ] ( times , mags [ tindex ] , errs [ tindex ] , * * kwargs ) trialbestpeaks . append ( lspres [ 'bestlspval' ] ) trialbestpeaks = np . array ( trialbestpeaks ) alltrialbestpeaks . append ( trialbestpeaks ) # calculate the FAP for a trial peak j = FAP[j] = # (1.0 + sum(trialbestpeaks[i] > peak[j]))/(ntrialbestpeaks + 1) if lspinfo [ 'method' ] != 'pdm' : falsealarmprob = ( ( 1.0 + trialbestpeaks [ trialbestpeaks > peak ] . size ) / ( trialbestpeaks . size + 1.0 ) ) # for PDM, we're looking for a peak smaller than the best peak # because values closer to 0.0 are more significant else : falsealarmprob = ( ( 1.0 + trialbestpeaks [ trialbestpeaks < peak ] . size ) / ( trialbestpeaks . size + 1.0 ) ) LOGINFO ( 'FAP for peak %s, period: %.6f = %.3g' % ( ind + 1 , period , falsealarmprob ) ) allpeaks . append ( peak ) allperiods . append ( period ) allfaps . append ( falsealarmprob ) return { 'peaks' : allpeaks , 'periods' : allperiods , 'probabilities' : allfaps , 'alltrialbestpeaks' : alltrialbestpeaks } else : LOGERROR ( 'not enough mag series points to calculate periodogram' ) return None
9508	def union_fill_gap ( self , i ) : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) )
6897	def _periodicfeatures_worker ( task ) : pfpickle , lcbasedir , outdir , starfeatures , kwargs = task try : return get_periodicfeatures ( pfpickle , lcbasedir , outdir , starfeatures = starfeatures , * * kwargs ) except Exception as e : LOGEXCEPTION ( 'failed to get periodicfeatures for %s' % pfpickle )
6674	def get_owner ( self , path , use_sudo = False ) : func = use_sudo and run_as_root or self . run # I'd prefer to use quiet=True, but that's not supported with older # versions of Fabric. with self . settings ( hide ( 'running' , 'stdout' ) , warn_only = True ) : result = func ( 'stat -c %%U "%(path)s"' % locals ( ) ) if result . failed and 'stat: illegal option' in result : # Try the BSD version of stat return func ( 'stat -f %%Su "%(path)s"' % locals ( ) ) return result
1954	def symbolic_run_get_cons ( trace ) : m2 = Manticore . linux ( prog , workspace_url = 'mem:' ) f = Follower ( trace ) m2 . verbosity ( VERBOSITY ) m2 . register_plugin ( f ) def on_term_testcase ( mcore , state , stateid , err ) : with m2 . locked_context ( ) as ctx : readdata = [ ] for name , fd , data in state . platform . syscall_trace : if name in ( '_receive' , '_read' ) and fd == 0 : readdata . append ( data ) ctx [ 'readdata' ] = readdata ctx [ 'constraints' ] = list ( state . constraints . constraints ) m2 . subscribe ( 'will_terminate_state' , on_term_testcase ) m2 . run ( ) constraints = m2 . context [ 'constraints' ] datas = m2 . context [ 'readdata' ] return constraints , datas
7831	def add_field ( self , name = None , values = None , field_type = None , label = None , options = None , required = False , desc = None , value = None ) : field = Field ( name , values , field_type , label , options , required , desc , value ) self . fields . append ( field ) return field
4575	def hsv2rgb_360 ( hsv ) : h , s , v = hsv r , g , b = colorsys . hsv_to_rgb ( h / 360.0 , s , v ) return ( int ( r * 255.0 ) , int ( g * 255.0 ) , int ( b * 255.0 ) )
12085	def folderScan ( self , abfFolder = None ) : if abfFolder is None and 'abfFolder' in dir ( self ) : abfFolder = self . abfFolder else : self . abfFolder = abfFolder self . abfFolder = os . path . abspath ( self . abfFolder ) self . log . info ( "scanning [%s]" , self . abfFolder ) if not os . path . exists ( self . abfFolder ) : self . log . error ( "path doesn't exist: [%s]" , abfFolder ) return self . abfFolder2 = os . path . abspath ( self . abfFolder + "/swhlab/" ) if not os . path . exists ( self . abfFolder2 ) : self . log . error ( "./swhlab/ doesn't exist. creating it..." ) os . mkdir ( self . abfFolder2 ) self . fnames = os . listdir ( self . abfFolder ) self . fnames2 = os . listdir ( self . abfFolder2 ) self . log . debug ( "./ has %d files" , len ( self . fnames ) ) self . log . debug ( "./swhlab/ has %d files" , len ( self . fnames2 ) ) self . fnamesByExt = filesByExtension ( self . fnames ) if not "abf" in self . fnamesByExt . keys ( ) : self . log . error ( "no ABF files found" ) self . log . debug ( "found %d ABFs" , len ( self . fnamesByExt [ "abf" ] ) ) self . cells = findCells ( self . fnames ) # list of cells by their ID self . log . debug ( "found %d cells" % len ( self . cells ) ) self . fnamesByCell = filesByCell ( self . fnames , self . cells ) # only ABFs self . log . debug ( "grouped cells by number of source files: %s" % str ( [ len ( self . fnamesByCell [ elem ] ) for elem in self . fnamesByCell ] ) )
7509	def _save ( self ) : ## save each attribute as dict fulldict = copy . deepcopy ( self . __dict__ ) for i , j in fulldict . items ( ) : if isinstance ( j , Params ) : fulldict [ i ] = j . __dict__ fulldumps = json . dumps ( fulldict , sort_keys = False , indent = 4 , separators = ( "," , ":" ) , ) ## save to file, make dir if it wasn't made earlier assemblypath = os . path . join ( self . dirs , self . name + ".tet.json" ) if not os . path . exists ( self . dirs ) : os . mkdir ( self . dirs ) ## protect save from interruption done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( KeyboardInterrupt , SystemExit ) : print ( '.' ) continue
1503	def template_uploader_yaml ( cl_args , masters ) : single_master = masters [ 0 ] uploader_config_template = "%s/standalone/templates/uploader.template.yaml" % cl_args [ "config_path" ] uploader_config_actual = "%s/standalone/uploader.yaml" % cl_args [ "config_path" ] template_file ( uploader_config_template , uploader_config_actual , { "<http_uploader_uri>" : "http://%s:9000/api/v1/file/upload" % single_master } )
13234	def make_aware ( value , timezone ) : if hasattr ( timezone , 'localize' ) and value not in ( datetime . datetime . min , datetime . datetime . max ) : # available for pytz time zones return timezone . localize ( value , is_dst = None ) else : # may be wrong around DST changes return value . replace ( tzinfo = timezone )
9824	def update ( ctx , name , description , tags , private ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description if private is not None : update_dict [ 'is_public' ] = not private tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the project.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . project . update_project ( user , project_name , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Project updated." ) get_project_details ( response )
9432	def _c_func ( func , restype , argtypes , errcheck = None ) : func . restype = restype func . argtypes = argtypes if errcheck is not None : func . errcheck = errcheck return func
1315	def DeleteLog ( ) -> None : if os . path . exists ( Logger . FileName ) : os . remove ( Logger . FileName )
6148	def unique_cpx_roots ( rlist , tol = 0.001 ) : uniq = [ rlist [ 0 ] ] mult = [ 1 ] for k in range ( 1 , len ( rlist ) ) : N_uniq = len ( uniq ) for m in range ( N_uniq ) : if abs ( rlist [ k ] - uniq [ m ] ) <= tol : mult [ m ] += 1 uniq [ m ] = ( uniq [ m ] * ( mult [ m ] - 1 ) + rlist [ k ] ) / float ( mult [ m ] ) break uniq = np . hstack ( ( uniq , rlist [ k ] ) ) mult = np . hstack ( ( mult , [ 1 ] ) ) return np . array ( uniq ) , np . array ( mult )
12500	def sigma2fwhm ( sigma ) : sigma = np . asarray ( sigma ) return np . sqrt ( 8 * np . log ( 2 ) ) * sigma
1977	def sys_deallocate ( self , cpu , addr , size ) : logger . info ( "DEALLOCATE(0x%08x, %d)" % ( addr , size ) ) if addr & 0xfff != 0 : logger . info ( "DEALLOCATE: addr is not page aligned" ) return Decree . CGC_EINVAL if size == 0 : logger . info ( "DEALLOCATE:length is zero" ) return Decree . CGC_EINVAL # unlikely AND WRONG!!! # if addr > Decree.CGC_SSIZE_MAX or addr+size > Decree.CGC_SSIZE_MAX: # logger.info("DEALLOCATE: part of the region being deallocated is outside the valid address range of the process") # return Decree.CGC_EINVAL cpu . memory . munmap ( addr , size ) self . syscall_trace . append ( ( "_deallocate" , - 1 , size ) ) return 0
1709	def send ( self , str , end = '\n' ) : return self . _process . stdin . write ( str + end )
9247	def generate_sub_section ( self , issues , prefix ) : log = "" if issues : if not self . options . simple_list : log += u"{0}\n\n" . format ( prefix ) for issue in issues : merge_string = self . get_string_for_issue ( issue ) log += u"- {0}\n" . format ( merge_string ) log += "\n" return log
8784	def create_port ( self , context , network_id , port_id , * * kwargs ) : LOG . info ( "create_port %s %s %s" % ( context . tenant_id , network_id , port_id ) ) # sanity check if not kwargs . get ( 'base_net_driver' ) : raise IronicException ( msg = 'base_net_driver required.' ) base_net_driver = kwargs [ 'base_net_driver' ] if not kwargs . get ( 'device_id' ) : raise IronicException ( msg = 'device_id required.' ) device_id = kwargs [ 'device_id' ] if not kwargs . get ( 'instance_node_id' ) : raise IronicException ( msg = 'instance_node_id required.' ) instance_node_id = kwargs [ 'instance_node_id' ] if not kwargs . get ( 'mac_address' ) : raise IronicException ( msg = 'mac_address is required.' ) mac_address = str ( netaddr . EUI ( kwargs [ "mac_address" ] [ "address" ] ) ) mac_address = mac_address . replace ( '-' , ':' ) # TODO(morgabra): Change this when we enable security groups. if kwargs . get ( 'security_groups' ) : msg = 'ironic driver does not support security group operations.' raise IronicException ( msg = msg ) # unroll the given address models into a fixed_ips list we can # pass downstream fixed_ips = [ ] addresses = kwargs . get ( 'addresses' ) if not isinstance ( addresses , list ) : addresses = [ addresses ] for address in addresses : fixed_ips . append ( self . _make_fixed_ip_dict ( context , address ) ) body = { "id" : port_id , "network_id" : network_id , "device_id" : device_id , "device_owner" : kwargs . get ( 'device_owner' , '' ) , "tenant_id" : context . tenant_id or "quark" , "roles" : context . roles , "mac_address" : mac_address , "fixed_ips" : fixed_ips , "switch:hardware_id" : instance_node_id , "dynamic_network" : not STRATEGY . is_provider_network ( network_id ) } net_info = self . _get_base_network_info ( context , network_id , base_net_driver ) body . update ( net_info ) try : LOG . info ( "creating downstream port: %s" % ( body ) ) port = self . _create_port ( context , body ) LOG . info ( "created downstream port: %s" % ( port ) ) return { "uuid" : port [ 'port' ] [ 'id' ] , "vlan_id" : port [ 'port' ] [ 'vlan_id' ] } except Exception as e : msg = "failed to create downstream port. Exception: %s" % ( e ) raise IronicException ( msg = msg )
4801	def is_named ( self , filename ) : self . is_file ( ) if not isinstance ( filename , str_types ) : raise TypeError ( 'given filename arg must be a path' ) val_filename = os . path . basename ( os . path . abspath ( self . val ) ) if val_filename != filename : self . _err ( 'Expected filename <%s> to be equal to <%s>, but was not.' % ( val_filename , filename ) ) return self
11442	def _warning ( code ) : if isinstance ( code , str ) : return code message = '' if isinstance ( code , tuple ) : if isinstance ( code [ 0 ] , str ) : message = code [ 1 ] code = code [ 0 ] return CFG_BIBRECORD_WARNING_MSGS . get ( code , '' ) + message
4559	def next ( self , length ) : return Segment ( self . strip , length , self . offset + self . length )
9271	def filter_since_tag ( self , all_tags ) : tag = self . detect_since_tag ( ) if not tag or tag == REPO_CREATED_TAG_NAME : return copy . deepcopy ( all_tags ) filtered_tags = [ ] tag_names = [ t [ "name" ] for t in all_tags ] try : idx = tag_names . index ( tag ) except ValueError : self . warn_if_tag_not_found ( tag , "since-tag" ) return copy . deepcopy ( all_tags ) since_tag = all_tags [ idx ] since_date = self . get_time_of_tag ( since_tag ) for t in all_tags : tag_date = self . get_time_of_tag ( t ) if since_date <= tag_date : filtered_tags . append ( t ) return filtered_tags
6796	def manage_async ( self , command = '' , name = 'process' , site = ALL , exclude_sites = '' , end_message = '' , recipients = '' ) : exclude_sites = exclude_sites . split ( ':' ) r = self . local_renderer for _site , site_data in self . iter_sites ( site = site , no_secure = True ) : if _site in exclude_sites : continue r . env . SITE = _site r . env . command = command r . env . end_email_command = '' r . env . recipients = recipients or '' r . env . end_email_command = '' if end_message : end_message = end_message + ' for ' + _site end_message = end_message . replace ( ' ' , '_' ) r . env . end_message = end_message r . env . end_email_command = r . format ( '{manage_cmd} send_mail --subject={end_message} --recipients={recipients}' ) r . env . name = name . format ( * * r . genv ) r . run ( 'screen -dmS {name} bash -c "export SITE={SITE}; ' 'export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} {command} --traceback; {end_email_command}"; sleep 3;' )
9885	def load_all_variables ( self ) : self . data = { } # need to add r variable names file_var_names = self . z_variable_info . keys ( ) # collect variable information for each # organize it neatly for fortran call dim_sizes = [ ] rec_nums = [ ] data_types = [ ] names = [ ] for i , name in enumerate ( file_var_names ) : dim_sizes . extend ( self . z_variable_info [ name ] [ 'dim_sizes' ] ) rec_nums . append ( self . z_variable_info [ name ] [ 'rec_num' ] ) data_types . append ( self . z_variable_info [ name ] [ 'data_type' ] ) names . append ( name . ljust ( 256 ) ) dim_sizes = np . array ( dim_sizes ) rec_nums = np . array ( rec_nums ) data_types = np . array ( data_types ) # individually load all variables by each data type self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'real4' ] , fortran_cdf . get_multi_z_real4 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'float' ] , fortran_cdf . get_multi_z_real4 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'real8' ] , fortran_cdf . get_multi_z_real8 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'double' ] , fortran_cdf . get_multi_z_real8 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'int4' ] , fortran_cdf . get_multi_z_int4 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'uint4' ] , fortran_cdf . get_multi_z_int4 , data_offset = 2 ** 32 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'int2' ] , fortran_cdf . get_multi_z_int2 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'uint2' ] , fortran_cdf . get_multi_z_int2 , data_offset = 2 ** 16 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'int1' ] , fortran_cdf . get_multi_z_int1 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'uint1' ] , fortran_cdf . get_multi_z_int1 , data_offset = 2 ** 8 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'byte' ] , fortran_cdf . get_multi_z_int1 ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'epoch' ] , fortran_cdf . get_multi_z_real8 , epoch = True ) self . _call_multi_fortran_z ( names , data_types , rec_nums , 2 * dim_sizes , self . cdf_data_types [ 'epoch16' ] , fortran_cdf . get_multi_z_epoch16 , epoch16 = True ) self . _call_multi_fortran_z ( names , data_types , rec_nums , dim_sizes , self . cdf_data_types [ 'TT2000' ] , fortran_cdf . get_multi_z_tt2000 , epoch = True ) # mark data has been loaded self . data_loaded = True
10315	def pair_has_contradiction ( graph : BELGraph , u : BaseEntity , v : BaseEntity ) -> bool : relations = { data [ RELATION ] for data in graph [ u ] [ v ] . values ( ) } return relation_set_has_contradictions ( relations )
809	def _storeSample ( self , inputVector , trueCatIndex , partition = 0 ) : # If this is the first sample, then allocate a numpy array # of the appropriate size in which to store all samples. if self . _samples is None : self . _samples = numpy . zeros ( ( 0 , len ( inputVector ) ) , dtype = RealNumpyDType ) assert self . _labels is None self . _labels = [ ] # Add the sample vector and category lable self . _samples = numpy . concatenate ( ( self . _samples , numpy . atleast_2d ( inputVector ) ) , axis = 0 ) self . _labels += [ trueCatIndex ] # Add the partition ID if self . _partitions is None : self . _partitions = [ ] if partition is None : partition = 0 self . _partitions += [ partition ]
8963	def isodate ( datestamp = None , microseconds = False ) : datestamp = datestamp or datetime . datetime . now ( ) if not microseconds : usecs = datetime . timedelta ( microseconds = datestamp . microsecond ) datestamp = datestamp - usecs return datestamp . isoformat ( b' ' if PY2 else u' ' )
11514	def search_item_by_name ( self , name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name if token : parameters [ 'token' ] = token response = self . request ( 'midas.item.searchbyname' , parameters ) return response [ 'items' ]
1944	def _hook_syscall ( self , uc , data ) : logger . debug ( f"Stopping emulation at {hex(uc.reg_read(self._to_unicorn_id('RIP')))} to perform syscall" ) self . sync_unicorn_to_manticore ( ) from . . native . cpu . abstractcpu import Syscall self . _to_raise = Syscall ( ) uc . emu_stop ( )
11783	def add_example ( self , example ) : self . check_example ( example ) self . examples . append ( example )
3859	def update_conversation ( self , conversation ) : # StateUpdate.conversation is actually a delta; fields that aren't # specified are assumed to be unchanged. Until this class is # refactored, hide this by saving and restoring previous values where # necessary. new_state = conversation . self_conversation_state old_state = self . _conversation . self_conversation_state self . _conversation = conversation # delivery_medium_option if not new_state . delivery_medium_option : new_state . delivery_medium_option . extend ( old_state . delivery_medium_option ) # latest_read_timestamp old_timestamp = old_state . self_read_state . latest_read_timestamp new_timestamp = new_state . self_read_state . latest_read_timestamp if new_timestamp == 0 : new_state . self_read_state . latest_read_timestamp = old_timestamp # user_read_state(s) for new_entry in conversation . read_state : tstamp = parsers . from_timestamp ( new_entry . latest_read_timestamp ) if tstamp == 0 : continue uid = parsers . from_participantid ( new_entry . participant_id ) if uid not in self . _watermarks or self . _watermarks [ uid ] < tstamp : self . _watermarks [ uid ] = tstamp
8238	def analogous ( clr , angle = 10 , contrast = 0.25 ) : contrast = max ( 0 , min ( contrast , 1.0 ) ) clr = color ( clr ) colors = colorlist ( clr ) for i , j in [ ( 1 , 2.2 ) , ( 2 , 1 ) , ( - 1 , - 0.5 ) , ( - 2 , 1 ) ] : c = clr . rotate_ryb ( angle * i ) t = 0.44 - j * 0.1 if clr . brightness - contrast * j < t : c . brightness = t else : c . brightness = clr . brightness - contrast * j c . saturation -= 0.05 colors . append ( c ) return colors
11045	def init_storage_dir ( storage_dir ) : storage_path = FilePath ( storage_dir ) # Create the default wildcard certificate if it doesn't already exist default_cert_path = storage_path . child ( 'default.pem' ) if not default_cert_path . exists ( ) : default_cert_path . setContent ( generate_wildcard_pem_bytes ( ) ) # Create a directory for unmanaged certs. We don't touch this again, but it # needs to be there and it makes sense to create it at the same time as # everything else. unmanaged_certs_path = storage_path . child ( 'unmanaged-certs' ) if not unmanaged_certs_path . exists ( ) : unmanaged_certs_path . createDirectory ( ) # Store certificates in a directory inside the storage directory, so # HAProxy will read just the certificates there. certs_path = storage_path . child ( 'certs' ) if not certs_path . exists ( ) : certs_path . createDirectory ( ) return storage_path , certs_path
6828	def fetch ( self , path , use_sudo = False , user = None , remote = None ) : if path is None : raise ValueError ( "Path to the working copy is needed to fetch from a remote repository." ) if remote is not None : cmd = 'git fetch %s' % remote else : cmd = 'git fetch' with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
13286	def get_dataframe_from_variable ( nc , data_var ) : time_var = nc . get_variables_by_attributes ( standard_name = 'time' ) [ 0 ] depth_vars = nc . get_variables_by_attributes ( axis = lambda v : v is not None and v . lower ( ) == 'z' ) depth_vars += nc . get_variables_by_attributes ( standard_name = lambda v : v in [ 'height' , 'depth' 'surface_altitude' ] , positive = lambda x : x is not None ) # Find the correct depth variable depth_var = None for d in depth_vars : try : if d . _name in data_var . coordinates . split ( " " ) or d . _name in data_var . dimensions : depth_var = d break except AttributeError : continue times = netCDF4 . num2date ( time_var [ : ] , units = time_var . units , calendar = getattr ( time_var , 'calendar' , 'standard' ) ) original_times_size = times . size if depth_var is None and hasattr ( data_var , 'sensor_depth' ) : depth_type = get_type ( data_var . sensor_depth ) depths = np . asarray ( [ data_var . sensor_depth ] * len ( times ) ) . flatten ( ) values = data_var [ : ] . flatten ( ) elif depth_var is None : depths = np . asarray ( [ np . nan ] * len ( times ) ) . flatten ( ) depth_type = get_type ( depths ) values = data_var [ : ] . flatten ( ) else : depths = depth_var [ : ] depth_type = get_type ( depths ) if len ( data_var . shape ) > 1 : times = np . repeat ( times , depths . size ) depths = np . tile ( depths , original_times_size ) values = data_var [ : , : ] . flatten ( ) else : values = data_var [ : ] . flatten ( ) if getattr ( depth_var , 'positive' , 'down' ) . lower ( ) == 'up' : logger . warning ( "Converting depths to positive down before returning the DataFrame" ) depths = depths * - 1 # https://github.com/numpy/numpy/issues/4595 # We can't call astype on a MaskedConstant if ( isinstance ( depths , np . ma . core . MaskedConstant ) or ( hasattr ( depths , 'mask' ) and depths . mask . all ( ) ) ) : depths = np . asarray ( [ np . nan ] * len ( times ) ) . flatten ( ) df = pd . DataFrame ( { 'time' : times , 'value' : values . astype ( data_var . dtype ) , 'unit' : data_var . units if hasattr ( data_var , 'units' ) else np . nan , 'depth' : depths . astype ( depth_type ) } ) df . set_index ( [ pd . DatetimeIndex ( df [ 'time' ] ) , pd . Float64Index ( df [ 'depth' ] ) ] , inplace = True ) return df
12366	def update ( self , id , name ) : return super ( Keys , self ) . update ( id , name = name )
7747	def _process_handler_result ( self , response ) : if response is None or response is False : return False if isinstance ( response , Stanza ) : self . send ( response ) return True try : response = iter ( response ) except TypeError : return bool ( response ) for stanza in response : if isinstance ( stanza , Stanza ) : self . send ( stanza ) else : logger . warning ( u"Unexpected object in stanza handler result:" u" {0!r}" . format ( stanza ) ) return True
13466	def set_moments ( self , sx , sxp , sxxp ) : self . _sx = sx self . _sxp = sxp self . _sxxp = sxxp emit = _np . sqrt ( sx ** 2 * sxp ** 2 - sxxp ** 2 ) self . _store_emit ( emit = emit )
4909	def _delete ( self , url , data , scope ) : self . _create_session ( scope ) response = self . session . delete ( url , data = data ) return response . status_code , response . text
13422	def get_all ( self , key = None ) : key = self . definition . main_key if key is None else key key = self . definition . key_synonyms . get ( key , key ) entries = self . _get_all ( key ) if key in self . definition . scalar_nonunique_keys : return set ( entries ) return entries
7646	def note_hz_to_midi ( annotation ) : annotation . namespace = 'note_midi' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = 12 * ( np . log2 ( obs . value ) - np . log2 ( 440.0 ) ) + 69 ) return annotation
2754	def get_all_sshkeys ( self ) : data = self . get_data ( "account/keys/" ) ssh_keys = list ( ) for jsoned in data [ 'ssh_keys' ] : ssh_key = SSHKey ( * * jsoned ) ssh_key . token = self . token ssh_keys . append ( ssh_key ) return ssh_keys
12818	def _build_chunk_headers ( self ) : if hasattr ( self , "_chunk_headers" ) and self . _chunk_headers : return self . _chunk_headers = { } for field in self . _files : self . _chunk_headers [ field ] = self . _headers ( field , True ) for field in self . _data : self . _chunk_headers [ field ] = self . _headers ( field )
9273	def filter_between_tags ( self , all_tags ) : tag_names = [ t [ "name" ] for t in all_tags ] between_tags = [ ] for tag in self . options . between_tags : try : idx = tag_names . index ( tag ) except ValueError : raise ChangelogGeneratorError ( "ERROR: can't find tag {0}, specified with " "--between-tags option." . format ( tag ) ) between_tags . append ( all_tags [ idx ] ) between_tags = self . sort_tags_by_date ( between_tags ) if len ( between_tags ) == 1 : # if option --between-tags was only 1 tag given, duplicate it # to generate the changelog only for that one tag. between_tags . append ( between_tags [ 0 ] ) older = self . get_time_of_tag ( between_tags [ 1 ] ) newer = self . get_time_of_tag ( between_tags [ 0 ] ) for tag in all_tags : if older < self . get_time_of_tag ( tag ) < newer : between_tags . append ( tag ) if older == newer : between_tags . pop ( 0 ) return between_tags
12197	def get_task_options ( ) : options = ( ) task_classes = get_tasks ( ) for cls in task_classes : options += cls . option_list return options
2088	def delete ( self , pk = None , fail_on_missing = False , * * kwargs ) : # If we weren't given a primary key, determine which record we're deleting. if not pk : existing_data = self . _lookup ( fail_on_missing = fail_on_missing , * * kwargs ) if not existing_data : return { 'changed' : False } pk = existing_data [ 'id' ] # Attempt to delete the record. If it turns out the record doesn't exist, handle the 404 appropriately # (this is an okay response if `fail_on_missing` is False). url = '%s%s/' % ( self . endpoint , pk ) debug . log ( 'DELETE %s' % url , fg = 'blue' , bold = True ) try : client . delete ( url ) return { 'changed' : True } except exc . NotFound : if fail_on_missing : raise return { 'changed' : False }
5467	def get_action_image ( op , name ) : action = _get_action_by_name ( op , name ) if action : return action . get ( 'imageUri' )
8323	def isList ( l ) : return hasattr ( l , '__iter__' ) or ( type ( l ) in ( types . ListType , types . TupleType ) )
10205	def extract_date ( self , date ) : if isinstance ( date , six . string_types ) : try : date = dateutil . parser . parse ( date ) except ValueError : raise ValueError ( 'Invalid date format for statistic {}.' ) . format ( self . query_name ) if not isinstance ( date , datetime ) : raise TypeError ( 'Invalid date type for statistic {}.' ) . format ( self . query_name ) return date
8495	def _get_module_filename ( module ) : # Split up the module and its containing package, if it has one module = module . split ( '.' ) package = '.' . join ( module [ : - 1 ] ) module = module [ - 1 ] try : if not package : # We aren't accessing a module within a package, but rather a top # level package, so it's a straight up import module = __import__ ( module ) else : # Import the package containing our desired module package = __import__ ( package , fromlist = [ module ] ) # Get the module from that package module = getattr ( package , module , None ) filename = getattr ( module , '__file__' , None ) if not filename : # No filename? Nothing to do here return Unparseable ( ) # If we get a .pyc, strip the c to get .py so we can parse the source if filename . endswith ( '.pyc' ) : filename = filename [ : - 1 ] if not os . path . exists ( filename ) and os . path . isfile ( filename ) : # If there's only a .pyc and no .py it's a compile package or # egg and we can't get at the source for parsing return Unparseable ( ) # If we have a package, we want the directory not the init file if filename . endswith ( '__init__.py' ) : filename = filename [ : - 11 ] # Yey, we found it return filename except ImportError : # Definitely not a valid module or package return
1862	def SCAS ( cpu , dest , src ) : dest_reg = dest . reg mem_reg = src . mem . base # , src.type, src.read() size = dest . size arg0 = dest . read ( ) arg1 = src . read ( ) res = arg0 - arg1 cpu . _calculate_CMP_flags ( size , res , arg0 , arg1 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( mem_reg , cpu . read_register ( mem_reg ) + increment )
3819	async def add_user ( self , add_user_request ) : response = hangouts_pb2 . AddUserResponse ( ) await self . _pb_request ( 'conversations/adduser' , add_user_request , response ) return response
5723	def _buffer_incomplete_responses ( raw_output , buf ) : if raw_output : if buf : # concatenate buffer and new output raw_output = b"" . join ( [ buf , raw_output ] ) buf = None if b"\n" not in raw_output : # newline was not found, so assume output is incomplete and store in buffer buf = raw_output raw_output = None elif not raw_output . endswith ( b"\n" ) : # raw output doesn't end in a newline, so store everything after the last newline (if anything) # in the buffer, and parse everything before it remainder_offset = raw_output . rindex ( b"\n" ) + 1 buf = raw_output [ remainder_offset : ] raw_output = raw_output [ : remainder_offset ] return ( raw_output , buf )
7277	def set_video_pos ( self , x1 , y1 , x2 , y2 ) : position = "%s %s %s %s" % ( str ( x1 ) , str ( y1 ) , str ( x2 ) , str ( y2 ) ) self . _player_interface . VideoPos ( ObjectPath ( '/not/used' ) , String ( position ) )
13258	def _file_path ( self , uid ) : file_name = '%s.doentry' % ( uid ) return os . path . join ( self . dayone_journal_path , file_name )
5679	def get_all_route_shapes ( self , use_shapes = True ) : cur = self . conn . cursor ( ) # all shape_id:s corresponding to a route_I: # query = "SELECT DISTINCT name, shape_id, trips.route_I, route_type # FROM trips LEFT JOIN routes USING(route_I)" # data1 = pd.read_sql_query(query, self.conn) # one (arbitrary) shape_id per route_I ("one direction") -> less than half of the routes query = "SELECT routes.name as name, shape_id, route_I, trip_I, routes.type, " " agency_id, agencies.name as agency_name, max(end_time_ds-start_time_ds) as trip_duration " "FROM trips " "LEFT JOIN routes " "USING(route_I) " "LEFT JOIN agencies " "USING(agency_I) " "GROUP BY routes.route_I" data = pd . read_sql_query ( query , self . conn ) routeShapes = [ ] for i , row in enumerate ( data . itertuples ( ) ) : datum = { "name" : str ( row . name ) , "type" : int ( row . type ) , "route_I" : row . route_I , "agency" : str ( row . agency_id ) , "agency_name" : str ( row . agency_name ) } # this function should be made also non-shape friendly (at this point) if use_shapes and row . shape_id : shape = shapes . get_shape_points2 ( cur , row . shape_id ) lats = shape [ 'lats' ] lons = shape [ 'lons' ] else : stop_shape = self . get_trip_stop_coordinates ( row . trip_I ) lats = list ( stop_shape [ 'lat' ] ) lons = list ( stop_shape [ 'lon' ] ) datum [ 'lats' ] = [ float ( lat ) for lat in lats ] datum [ 'lons' ] = [ float ( lon ) for lon in lons ] routeShapes . append ( datum ) return routeShapes
11791	def mrv ( assignment , csp ) : return argmin_random_tie ( [ v for v in csp . vars if v not in assignment ] , lambda var : num_legal_values ( csp , var , assignment ) )
6638	def getScript ( self , scriptname ) : script = self . description . get ( 'scripts' , { } ) . get ( scriptname , None ) if script is not None : if isinstance ( script , str ) or isinstance ( script , type ( u'unicode string' ) ) : import shlex script = shlex . split ( script ) # if the command is a python script, run it with the python # interpreter being used to run yotta, also fetch the absolute path # to the script relative to this module (so that the script can be # distributed with the module, no matter what current working # directory it will be executed in): if len ( script ) and script [ 0 ] . lower ( ) . endswith ( '.py' ) : if not os . path . isabs ( script [ 0 ] ) : absscript = os . path . abspath ( os . path . join ( self . path , script [ 0 ] ) ) logger . debug ( 'rewriting script %s to be absolute path %s' , script [ 0 ] , absscript ) script [ 0 ] = absscript import sys script = [ sys . executable ] + script return script
5709	def redirect ( self , request ) : url = request . path querystring = request . GET . copy ( ) if self . logout_key and self . logout_key in request . GET : del querystring [ self . logout_key ] if querystring : url = '%s?%s' % ( url , querystring . urlencode ( ) ) return HttpResponseRedirect ( url )
2887	def _try_disconnect ( self , ref ) : with self . lock : weak = [ s [ 0 ] for s in self . weak_subscribers ] try : index = weak . index ( ref ) except ValueError : # subscriber was already removed by a call to disconnect() pass else : self . weak_subscribers . pop ( index )
2806	def convert_elementwise_sub ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting elementwise_sub ...' ) model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'S' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) sub = keras . layers . Subtract ( name = tf_name ) layers [ scope_name ] = sub ( [ model0 , model1 ] )
9903	def is_configured ( self , project , * * kwargs ) : params = self . get_option return bool ( params ( 'server_host' , project ) and params ( 'server_port' , project ) )
12503	def _smooth_array ( arr , affine , fwhm = None , ensure_finite = True , copy = True , * * kwargs ) : if arr . dtype . kind == 'i' : if arr . dtype == np . int64 : arr = arr . astype ( np . float64 ) else : # We don't need crazy precision arr = arr . astype ( np . float32 ) if copy : arr = arr . copy ( ) if ensure_finite : # SPM tends to put NaNs in the data outside the brain arr [ np . logical_not ( np . isfinite ( arr ) ) ] = 0 if fwhm == 'fast' : arr = _fast_smooth_array ( arr ) elif fwhm is not None : # Keep only the scale part. affine = affine [ : 3 , : 3 ] # Convert from a FWHM to a sigma: fwhm_over_sigma_ratio = np . sqrt ( 8 * np . log ( 2 ) ) vox_size = np . sqrt ( np . sum ( affine ** 2 , axis = 0 ) ) sigma = fwhm / ( fwhm_over_sigma_ratio * vox_size ) for n , s in enumerate ( sigma ) : ndimage . gaussian_filter1d ( arr , s , output = arr , axis = n , * * kwargs ) return arr
10546	def get_taskruns ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'taskrun' , params = params ) if type ( res ) . __name__ == 'list' : return [ TaskRun ( taskrun ) for taskrun in res ] else : raise TypeError except : raise
12893	def get_power ( self ) : power = ( yield from self . handle_int ( self . API . get ( 'power' ) ) ) return bool ( power )
4841	def get_program_by_uuid ( self , program_uuid ) : return self . _load_data ( self . PROGRAMS_ENDPOINT , resource_id = program_uuid , default = None )
2448	def set_pkg_file_name ( self , doc , name ) : self . assert_package_exists ( ) if not self . package_file_name_set : self . package_file_name_set = True doc . package . file_name = name return True else : raise CardinalityError ( 'Package::FileName' )
10279	def neurommsig_topology ( graph : BELGraph , nodes : List [ BaseEntity ] ) -> float : nodes = list ( nodes ) number_nodes = len ( nodes ) if number_nodes <= 1 : # log.debug('') return 0.0 unnormalized_sum = sum ( u in graph [ v ] for u , v in itt . product ( nodes , repeat = 2 ) if v in graph and u != v ) return unnormalized_sum / ( number_nodes * ( number_nodes - 1.0 ) )
12046	def pickle_save ( thing , fname ) : pickle . dump ( thing , open ( fname , "wb" ) , pickle . HIGHEST_PROTOCOL ) return thing
7109	def get_from_cache ( url : str , cache_dir : Path = None ) -> Path : cache_dir . mkdir ( parents = True , exist_ok = True ) filename = re . sub ( r'.+/' , '' , url ) # get cache path to put the file cache_path = cache_dir / filename if cache_path . exists ( ) : return cache_path # make HEAD request to check ETag response = requests . head ( url ) if response . status_code != 200 : if "www.dropbox.com" in url : # dropbox return code 301, so we ignore this error pass else : raise IOError ( "HEAD request failed for url {}" . format ( url ) ) # add ETag to filename if it exists # etag = response.headers.get("ETag") if not cache_path . exists ( ) : # Download to temporary file, then copy to cache dir once finished. # Otherwise you get corrupt cache entries if the download gets interrupted. fd , temp_filename = tempfile . mkstemp ( ) logger . info ( "%s not found in cache, downloading to %s" , url , temp_filename ) # GET file object req = requests . get ( url , stream = True ) content_length = req . headers . get ( 'Content-Length' ) total = int ( content_length ) if content_length is not None else None progress = Tqdm . tqdm ( unit = "B" , total = total ) with open ( temp_filename , 'wb' ) as temp_file : for chunk in req . iter_content ( chunk_size = 1024 ) : if chunk : # filter out keep-alive new chunks progress . update ( len ( chunk ) ) temp_file . write ( chunk ) progress . close ( ) logger . info ( "copying %s to cache at %s" , temp_filename , cache_path ) shutil . copyfile ( temp_filename , str ( cache_path ) ) logger . info ( "removing temp file %s" , temp_filename ) os . close ( fd ) os . remove ( temp_filename ) return cache_path
4089	def with_logger ( cls ) : attr_name = '_logger' cls_name = cls . __qualname__ module = cls . __module__ if module is not None : cls_name = module + '.' + cls_name else : raise AssertionError setattr ( cls , attr_name , logging . getLogger ( cls_name ) ) return cls
11069	def mongo ( daemon = False , port = 20771 ) : cmd = "mongod --port {0}" . format ( port ) if daemon : cmd += " --fork" run ( cmd )
5349	def compose_title ( projects , data ) : for project in data : projects [ project ] = { 'meta' : { 'title' : data [ project ] [ 'title' ] } } return projects
11313	def update_cnum ( self ) : if "ConferencePaper" not in self . collections : cnums = record_get_field_values ( self . record , '773' , code = "w" ) for cnum in cnums : cnum_subs = [ ( "9" , "INSPIRE-CNUM" ) , ( "a" , cnum ) ] record_add_field ( self . record , "035" , subfields = cnum_subs )
12218	def _bind_args ( sig , param_matchers , args , kwargs ) : #Bind to signature. May throw its own TypeError bound = sig . bind ( * args , * * kwargs ) if not all ( param_matcher ( bound . arguments [ param_name ] ) for param_name , param_matcher in param_matchers ) : raise TypeError return bound
11191	def write ( proto_dataset_uri , input ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri ) _validate_and_put_readme ( proto_dataset , input . read ( ) )
12923	def start_tag ( self ) : direct_attributes = ( attribute . render ( self ) for attribute in self . render_attributes ) attributes = ( ) if hasattr ( self , '_attributes' ) : attributes = ( '{0}="{1}"' . format ( key , value ) for key , value in self . attributes . items ( ) if value ) rendered_attributes = " " . join ( filter ( bool , chain ( direct_attributes , attributes ) ) ) return '<{0}{1}{2}{3}>' . format ( self . tag , ' ' if rendered_attributes else '' , rendered_attributes , ' /' if self . tag_self_closes else "" )
13315	def command ( self ) : cmd = self . config . get ( 'command' , None ) if cmd is None : return cmd = cmd [ platform ] return cmd [ 'path' ] , cmd [ 'args' ]
6002	def regular_to_pix ( self ) : return mapper_util . voronoi_regular_to_pix_from_grids_and_geometry ( regular_grid = self . grid_stack . regular , regular_to_nearest_pix = self . grid_stack . pix . regular_to_nearest_pix , pixel_centres = self . geometry . pixel_centres , pixel_neighbors = self . geometry . pixel_neighbors , pixel_neighbors_size = self . geometry . pixel_neighbors_size ) . astype ( 'int' )
6089	def for_data_and_tracer ( cls , lens_data , tracer , padded_tracer = None ) : if tracer . has_light_profile and not tracer . has_pixelization : return LensProfileFit ( lens_data = lens_data , tracer = tracer , padded_tracer = padded_tracer ) elif not tracer . has_light_profile and tracer . has_pixelization : return LensInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) elif tracer . has_light_profile and tracer . has_pixelization : return LensProfileInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) else : raise exc . FittingException ( 'The fit routine did not call a Fit class - check the ' 'properties of the tracer' )
12091	def proto_02_01_MT70 ( abf = exampleABF ) : standard_overlayWithAverage ( abf ) swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = 'check' , resize = False )
1019	def getSimplePatterns ( numOnes , numPatterns , patternOverlap = 0 ) : assert ( patternOverlap < numOnes ) # How many new bits are introduced in each successive pattern? numNewBitsInEachPattern = numOnes - patternOverlap numCols = numNewBitsInEachPattern * numPatterns + patternOverlap p = [ ] for i in xrange ( numPatterns ) : x = numpy . zeros ( numCols , dtype = 'float32' ) startBit = i * numNewBitsInEachPattern nextStartBit = startBit + numOnes x [ startBit : nextStartBit ] = 1 p . append ( x ) return p
3827	async def get_suggested_entities ( self , get_suggested_entities_request ) : response = hangouts_pb2 . GetSuggestedEntitiesResponse ( ) await self . _pb_request ( 'contacts/getsuggestedentities' , get_suggested_entities_request , response ) return response
11757	def pl_true ( exp , model = { } ) : op , args = exp . op , exp . args if exp == TRUE : return True elif exp == FALSE : return False elif is_prop_symbol ( op ) : return model . get ( exp ) elif op == '~' : p = pl_true ( args [ 0 ] , model ) if p is None : return None else : return not p elif op == '|' : result = False for arg in args : p = pl_true ( arg , model ) if p is True : return True if p is None : result = None return result elif op == '&' : result = True for arg in args : p = pl_true ( arg , model ) if p is False : return False if p is None : result = None return result p , q = args if op == '>>' : return pl_true ( ~ p | q , model ) elif op == '<<' : return pl_true ( p | ~ q , model ) pt = pl_true ( p , model ) if pt is None : return None qt = pl_true ( q , model ) if qt is None : return None if op == '<=>' : return pt == qt elif op == '^' : return pt != qt else : raise ValueError , "illegal operator in logic expression" + str ( exp )
2861	def _transaction_end ( self ) : # Ask to return response bytes immediately. self . _command . append ( '\x87' ) # Send the entire command to the MPSSE. self . _ft232h . _write ( '' . join ( self . _command ) ) # Read response bytes and return them. return bytearray ( self . _ft232h . _poll_read ( self . _expected ) )
11084	def save ( self , msg , args ) : self . send_message ( msg . channel , "Saving current state..." ) self . _bot . plugins . save_state ( ) self . send_message ( msg . channel , "Done." )
11228	def before ( self , dt , inc = False ) : if self . _cache_complete : gen = self . _cache else : gen = self last = None if inc : for i in gen : if i > dt : break last = i else : for i in gen : if i >= dt : break last = i return last
10023	def environment_name_for_cname ( self , env_cname ) : envs = self . get_environments ( ) for env in envs : if env [ 'Status' ] != 'Terminated' and 'CNAME' in env and env [ 'CNAME' ] and env [ 'CNAME' ] . lower ( ) . startswith ( env_cname . lower ( ) + '.' ) : return env [ 'EnvironmentName' ] return None
5074	def track_enrollment ( pathway , user_id , course_run_id , url_path = None ) : track_event ( user_id , 'edx.bi.user.enterprise.onboarding' , { 'pathway' : pathway , 'url_path' : url_path , 'course_run_id' : course_run_id , } )
10945	def _do_run ( self , mode = '1' ) : for a in range ( len ( self . particle_groups ) ) : group = self . particle_groups [ a ] lp = LMParticles ( self . state , group , * * self . _kwargs ) if mode == 'internal' : lp . J , lp . JTJ , lp . _dif_tile = self . _load_j_diftile ( a ) if mode == '1' : lp . do_run_1 ( ) if mode == '2' : lp . do_run_2 ( ) if mode == 'internal' : lp . do_internal_run ( ) self . stats . append ( lp . get_termination_stats ( get_cos = self . get_cos ) ) if self . save_J and ( mode != 'internal' ) : self . _dump_j_diftile ( a , lp . J , lp . _dif_tile ) self . _has_saved_J [ a ] = True
3521	def performable ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return PerformableNode ( )
12226	def on_pref_update ( * args , * * kwargs ) : Preference . update_prefs ( * args , * * kwargs ) Preference . read_prefs ( get_prefs ( ) )
297	def plot_return_quantiles ( returns , live_start_date = None , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) is_returns = returns if live_start_date is None else returns . loc [ returns . index < live_start_date ] is_weekly = ep . aggregate_returns ( is_returns , 'weekly' ) is_monthly = ep . aggregate_returns ( is_returns , 'monthly' ) sns . boxplot ( data = [ is_returns , is_weekly , is_monthly ] , palette = [ "#4c72B0" , "#55A868" , "#CCB974" ] , ax = ax , * * kwargs ) if live_start_date is not None : oos_returns = returns . loc [ returns . index >= live_start_date ] oos_weekly = ep . aggregate_returns ( oos_returns , 'weekly' ) oos_monthly = ep . aggregate_returns ( oos_returns , 'monthly' ) sns . swarmplot ( data = [ oos_returns , oos_weekly , oos_monthly ] , ax = ax , color = "red" , marker = "d" , * * kwargs ) red_dots = matplotlib . lines . Line2D ( [ ] , [ ] , color = "red" , marker = "d" , label = "Out-of-sample data" , linestyle = '' ) ax . legend ( handles = [ red_dots ] , frameon = True , framealpha = 0.5 ) ax . set_xticklabels ( [ 'Daily' , 'Weekly' , 'Monthly' ] ) ax . set_title ( 'Return quantiles' ) return ax
9236	def _signal_handler_map ( self ) : result = { } for signum , handler in self . signal_map . items ( ) : result [ signum ] = self . _get_signal_handler ( handler ) return result
12496	def warn_if_not_float ( X , estimator = 'This algorithm' ) : if not isinstance ( estimator , str ) : estimator = estimator . __class__ . __name__ if X . dtype . kind != 'f' : warnings . warn ( "%s assumes floating point values as input, " "got %s" % ( estimator , X . dtype ) ) return True return False
6989	def parallel_varfeatures ( lclist , outdir , maxobjects = None , timecols = None , magcols = None , errcols = None , mindet = 1000 , lcformat = 'hat-sql' , lcformatdir = None , nworkers = NCPUS ) : # make sure to make the output directory if it doesn't exist if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : lclist = lclist [ : maxobjects ] tasks = [ ( x , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) for x in lclist ] with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( varfeatures_worker , tasks ) results = [ x for x in resultfutures ] resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( lclist , results ) } return resdict
9850	def export ( self , filename , file_format = None , type = None , typequote = '"' ) : exporter = self . _get_exporter ( filename , file_format = file_format ) exporter ( filename , type = type , typequote = typequote )
7844	def set_type ( self , item_type ) : if not item_type : raise ValueError ( "Type is required in DiscoIdentity" ) item_type = unicode ( item_type ) self . xmlnode . setProp ( "type" , item_type . encode ( "utf-8" ) )
3417	def _cell ( x ) : x_no_none = [ i if i is not None else "" for i in x ] return array ( x_no_none , dtype = np_object )
2751	def get_images ( self , private = False , type = None ) : params = { } if private : params [ 'private' ] = 'true' if type : params [ 'type' ] = type data = self . get_data ( "images/" , params = params ) images = list ( ) for jsoned in data [ 'images' ] : image = Image ( * * jsoned ) image . token = self . token images . append ( image ) return images
7586	def _get_boots ( arr , nboots ) : ## hold results (nboots, [dstat, ]) boots = np . zeros ( ( nboots , ) ) ## iterate to fill boots for bidx in xrange ( nboots ) : ## sample with replacement lidx = np . random . randint ( 0 , arr . shape [ 0 ] , arr . shape [ 0 ] ) tarr = arr [ lidx ] _ , _ , dst = _prop_dstat ( tarr ) boots [ bidx ] = dst ## return bootarr return boots
11521	def add_condor_dag ( self , token , batchmaketaskid , dagfilename , dagmanoutfilename ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'batchmaketaskid' ] = batchmaketaskid parameters [ 'dagfilename' ] = dagfilename parameters [ 'outfilename' ] = dagmanoutfilename response = self . request ( 'midas.batchmake.add.condor.dag' , parameters ) return response
11810	def index_document ( self , text , url ) : ## For now, use first line for title title = text [ : text . index ( '\n' ) ] . strip ( ) docwords = words ( text ) docid = len ( self . documents ) self . documents . append ( Document ( title , url , len ( docwords ) ) ) for word in docwords : if word not in self . stopwords : self . index [ word ] [ docid ] += 1
6929	def trapezoid_transit_func ( transitparams , times , mags , errs , get_ntransitpoints = False ) : ( transitperiod , transitepoch , transitdepth , transitduration , ingressduration ) = transitparams # generate the phases iphase = ( times - transitepoch ) / transitperiod iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] zerolevel = np . median ( pmags ) modelmags = np . full_like ( phase , zerolevel ) halftransitduration = transitduration / 2.0 bottomlevel = zerolevel - transitdepth slope = transitdepth / ingressduration # the four contact points of the eclipse firstcontact = 1.0 - halftransitduration secondcontact = firstcontact + ingressduration thirdcontact = halftransitduration - ingressduration fourthcontact = halftransitduration ## the phase indices ## # during ingress ingressind = ( phase > firstcontact ) & ( phase < secondcontact ) # at transit bottom bottomind = ( phase > secondcontact ) | ( phase < thirdcontact ) # during egress egressind = ( phase > thirdcontact ) & ( phase < fourthcontact ) # count the number of points in transit in_transit_points = ingressind | bottomind | egressind n_transit_points = np . sum ( in_transit_points ) # set the mags modelmags [ ingressind ] = zerolevel - slope * ( phase [ ingressind ] - firstcontact ) modelmags [ bottomind ] = bottomlevel modelmags [ egressind ] = bottomlevel + slope * ( phase [ egressind ] - thirdcontact ) if get_ntransitpoints : return modelmags , phase , ptimes , pmags , perrs , n_transit_points else : return modelmags , phase , ptimes , pmags , perrs
9513	def orfs ( self , frame = 0 , revcomp = False ) : assert frame in [ 0 , 1 , 2 ] if revcomp : self . revcomp ( ) aa_seq = self . translate ( frame = frame ) . seq . rstrip ( 'X' ) if revcomp : self . revcomp ( ) orfs = _orfs_from_aa_seq ( aa_seq ) for i in range ( len ( orfs ) ) : if revcomp : start = len ( self ) - ( orfs [ i ] . end * 3 + 3 ) - frame end = len ( self ) - ( orfs [ i ] . start * 3 ) - 1 - frame else : start = orfs [ i ] . start * 3 + frame end = orfs [ i ] . end * 3 + 2 + frame orfs [ i ] = intervals . Interval ( start , end ) return orfs
11462	def update_subject_categories ( self , primary , secondary , kb ) : category_fields = record_get_field_instances ( self . record , tag = '650' , ind1 = '1' , ind2 = '7' ) record_delete_fields ( self . record , "650" ) for field in category_fields : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : new_value = self . get_config_item ( value , kb ) if new_value != value : new_subs = [ ( '2' , secondary ) , ( 'a' , new_value ) ] else : new_subs = [ ( '2' , primary ) , ( 'a' , value ) ] record_add_field ( self . record , "650" , ind1 = "1" , ind2 = "7" , subfields = new_subs ) break
8575	def update_nic ( self , datacenter_id , server_id , nic_id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
300	def plot_slippage_sensitivity ( returns , positions , transactions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) avg_returns_given_slippage = pd . Series ( ) for bps in range ( 1 , 100 ) : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) avg_returns = ep . annual_return ( adj_returns ) avg_returns_given_slippage . loc [ bps ] = avg_returns avg_returns_given_slippage . plot ( alpha = 1.0 , lw = 2 , ax = ax ) ax . set_title ( 'Average annual returns given additional per-dollar slippage' ) ax . set_xticks ( np . arange ( 0 , 100 , 10 ) ) ax . set_ylabel ( 'Average annual return' ) ax . set_xlabel ( 'Per-dollar slippage (bps)' ) return ax
8954	def glob2re ( part ) : return "[^/]*" . join ( re . escape ( bit ) . replace ( r'\[\^' , '[^' ) . replace ( r'\[' , '[' ) . replace ( r'\]' , ']' ) for bit in part . split ( "*" ) )
12023	def check_phase ( self ) : plus_minus = set ( [ '+' , '-' ] ) for k , g in groupby ( sorted ( [ line for line in self . lines if line [ 'line_type' ] == 'feature' and line [ 'type' ] == 'CDS' and 'Parent' in line [ 'attributes' ] ] , key = lambda x : x [ 'attributes' ] [ 'Parent' ] ) , key = lambda x : x [ 'attributes' ] [ 'Parent' ] ) : cds_list = list ( g ) strand_set = list ( set ( [ line [ 'strand' ] for line in cds_list ] ) ) if len ( strand_set ) != 1 : for line in cds_list : self . add_line_error ( line , { 'message' : 'Inconsistent CDS strand with parent: {0:s}' . format ( k ) , 'error_type' : 'STRAND' } ) continue if len ( cds_list ) == 1 : if cds_list [ 0 ] [ 'phase' ] != 0 : self . add_line_error ( cds_list [ 0 ] , { 'message' : 'Wrong phase {0:d}, should be {1:d}' . format ( cds_list [ 0 ] [ 'phase' ] , 0 ) , 'error_type' : 'PHASE' } ) continue strand = strand_set [ 0 ] if strand not in plus_minus : # don't process unknown strands continue if strand == '-' : # sort end descending sorted_cds_list = sorted ( cds_list , key = lambda x : x [ 'end' ] , reverse = True ) else : sorted_cds_list = sorted ( cds_list , key = lambda x : x [ 'start' ] ) phase = 0 for line in sorted_cds_list : if line [ 'phase' ] != phase : self . add_line_error ( line , { 'message' : 'Wrong phase {0:d}, should be {1:d}' . format ( line [ 'phase' ] , phase ) , 'error_type' : 'PHASE' } ) phase = ( 3 - ( ( line [ 'end' ] - line [ 'start' ] + 1 - phase ) % 3 ) ) % 3
2521	def p_file_type ( self , f_term , predicate ) : try : for _ , _ , ftype in self . graph . triples ( ( f_term , predicate , None ) ) : try : if ftype . endswith ( 'binary' ) : ftype = 'BINARY' elif ftype . endswith ( 'source' ) : ftype = 'SOURCE' elif ftype . endswith ( 'other' ) : ftype = 'OTHER' elif ftype . endswith ( 'archive' ) : ftype = 'ARCHIVE' self . builder . set_file_type ( self . doc , ftype ) except SPDXValueError : self . value_error ( 'FILE_TYPE' , ftype ) except CardinalityError : self . more_than_one_error ( 'file type' )
8398	def breaks ( self , limits ) : # clip the breaks to the domain, # e.g. probabilities will be in [0, 1] domain vmin = np . max ( [ self . domain [ 0 ] , limits [ 0 ] ] ) vmax = np . min ( [ self . domain [ 1 ] , limits [ 1 ] ] ) breaks = np . asarray ( self . breaks_ ( [ vmin , vmax ] ) ) # Some methods(mpl_breaks, extended_breaks) that # calculate breaks take the limits as guide posts and # not hard limits. breaks = breaks . compress ( ( breaks >= self . domain [ 0 ] ) & ( breaks <= self . domain [ 1 ] ) ) return breaks
3160	def update ( self , campaign_id , data ) : self . campaign_id = campaign_id return self . _mc_client . _put ( url = self . _build_path ( campaign_id , 'content' ) , data = data )
4220	def backends ( cls ) : allowed = ( keyring for keyring in filter ( backend . _limit , backend . get_all_keyring ( ) ) if not isinstance ( keyring , ChainerBackend ) and keyring . priority > 0 ) return sorted ( allowed , key = backend . by_priority , reverse = True )
178	def subdivide ( self , points_per_edge ) : if len ( self . coords ) <= 1 or points_per_edge < 1 : return self . deepcopy ( ) coords = interpolate_points ( self . coords , nb_steps = points_per_edge , closed = False ) return self . deepcopy ( coords = coords )
2764	def get_all_snapshots ( self ) : data = self . get_data ( "snapshots/" ) return [ Snapshot ( token = self . token , * * snapshot ) for snapshot in data [ 'snapshots' ] ]
945	def _checkpointLabelFromCheckpointDir ( checkpointDir ) : assert checkpointDir . endswith ( g_defaultCheckpointExtension ) lastSegment = os . path . split ( checkpointDir ) [ 1 ] checkpointLabel = lastSegment [ 0 : - len ( g_defaultCheckpointExtension ) ] return checkpointLabel
3374	def remove_cons_vars_from_problem ( model , what ) : context = get_context ( model ) model . solver . remove ( what ) if context : context ( partial ( model . solver . add , what ) )
12763	def load_attachments ( self , source , skeleton ) : self . targets = { } self . offsets = { } filename = source if isinstance ( source , str ) : source = open ( source ) else : filename = '(file-{})' . format ( id ( source ) ) for i , line in enumerate ( source ) : tokens = line . split ( '#' ) [ 0 ] . strip ( ) . split ( ) if not tokens : continue label = tokens . pop ( 0 ) if label not in self . channels : logging . info ( '%s:%d: unknown marker %s' , filename , i , label ) continue if not tokens : continue name = tokens . pop ( 0 ) bodies = [ b for b in skeleton . bodies if b . name == name ] if len ( bodies ) != 1 : logging . info ( '%s:%d: %d skeleton bodies match %s' , filename , i , len ( bodies ) , name ) continue b = self . targets [ label ] = bodies [ 0 ] o = self . offsets [ label ] = np . array ( list ( map ( float , tokens ) ) ) * b . dimensions / 2 logging . info ( '%s <--> %s, offset %s' , label , b . name , o )
8566	def get_loadbalancer_members ( self , datacenter_id , loadbalancer_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s/balancednics?depth=%s' % ( datacenter_id , loadbalancer_id , str ( depth ) ) ) return response
7478	def sort_seeds ( uhandle , usort ) : cmd = [ "sort" , "-k" , "2" , uhandle , "-o" , usort ] proc = sps . Popen ( cmd , close_fds = True ) proc . communicate ( )
5341	def __get_menu_entries ( self , kibiter_major ) : menu_entries = [ ] for entry in self . panels_menu : if entry [ 'source' ] not in self . data_sources : continue parent_menu_item = { 'name' : entry [ 'name' ] , 'title' : entry [ 'name' ] , 'description' : "" , 'type' : "menu" , 'dashboards' : [ ] } for subentry in entry [ 'menu' ] : try : dash_name = get_dashboard_name ( subentry [ 'panel' ] ) except FileNotFoundError : logging . error ( "Can't open dashboard file %s" , subentry [ 'panel' ] ) continue # The name for the entry is in self.panels_menu child_item = { "name" : subentry [ 'name' ] , "title" : subentry [ 'name' ] , "description" : "" , "type" : "entry" , "panel_id" : dash_name } parent_menu_item [ 'dashboards' ] . append ( child_item ) menu_entries . append ( parent_menu_item ) return menu_entries
3384	def generate_fva_warmup ( self ) : self . n_warmup = 0 reactions = self . model . reactions self . warmup = np . zeros ( ( 2 * len ( reactions ) , len ( self . model . variables ) ) ) self . model . objective = Zero for sense in ( "min" , "max" ) : self . model . objective_direction = sense for i , r in enumerate ( reactions ) : variables = ( self . model . variables [ self . fwd_idx [ i ] ] , self . model . variables [ self . rev_idx [ i ] ] ) # Omit fixed reactions if they are non-homogeneous if r . upper_bound - r . lower_bound < self . bounds_tol : LOGGER . info ( "skipping fixed reaction %s" % r . id ) continue self . model . objective . set_linear_coefficients ( { variables [ 0 ] : 1 , variables [ 1 ] : - 1 } ) self . model . slim_optimize ( ) if not self . model . solver . status == OPTIMAL : LOGGER . info ( "can not maximize reaction %s, skipping it" % r . id ) continue primals = self . model . solver . primal_values sol = [ primals [ v . name ] for v in self . model . variables ] self . warmup [ self . n_warmup , ] = sol self . n_warmup += 1 # Reset objective self . model . objective . set_linear_coefficients ( { variables [ 0 ] : 0 , variables [ 1 ] : 0 } ) # Shrink to measure self . warmup = self . warmup [ 0 : self . n_warmup , : ] # Remove redundant search directions keep = np . logical_not ( self . _is_redundant ( self . warmup ) ) self . warmup = self . warmup [ keep , : ] self . n_warmup = self . warmup . shape [ 0 ] # Catch some special cases if len ( self . warmup . shape ) == 1 or self . warmup . shape [ 0 ] == 1 : raise ValueError ( "Your flux cone consists only of a single point!" ) elif self . n_warmup == 2 : if not self . problem . homogeneous : raise ValueError ( "Can not sample from an inhomogenous problem" " with only 2 search directions :(" ) LOGGER . info ( "All search directions on a line, adding another one." ) newdir = self . warmup . T . dot ( [ 0.25 , 0.25 ] ) self . warmup = np . vstack ( [ self . warmup , newdir ] ) self . n_warmup += 1 # Shrink warmup points to measure self . warmup = shared_np_array ( ( self . n_warmup , len ( self . model . variables ) ) , self . warmup )
183	def to_segmentation_map ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : from . segmaps import SegmentationMapOnImage return SegmentationMapOnImage ( self . draw_mask ( image_shape , size_lines = size_lines , size_points = size_points , raise_if_out_of_image = raise_if_out_of_image ) , shape = image_shape )
12087	def html_singleAll ( self , template = "basic" ) : for fname in smartSort ( self . cells ) : if template == "fixed" : self . html_single_fixed ( fname ) else : self . html_single_basic ( fname )
12860	def add_period ( self , p , holiday_obj = None ) : if isinstance ( p , ( list , tuple ) ) : return [ BusinessDate . add_period ( self , pd ) for pd in p ] elif isinstance ( p , str ) : period = BusinessPeriod ( p ) else : period = p res = self res = BusinessDate . add_months ( res , period . months ) res = BusinessDate . add_years ( res , period . years ) res = BusinessDate . add_days ( res , period . days ) if period . businessdays : if holiday_obj : res = BusinessDate . add_business_days ( res , period . businessdays , holiday_obj ) else : res = BusinessDate . add_business_days ( res , period . businessdays , period . holiday ) return res
6934	def add_cmds_cplist ( cplist , cmdpkl , require_cmd_magcolor = True , save_cmd_pngs = False ) : # load the CMD first to save on IO with open ( cmdpkl , 'rb' ) as infd : cmd = pickle . load ( infd ) for cpf in cplist : add_cmd_to_checkplot ( cpf , cmd , require_cmd_magcolor = require_cmd_magcolor , save_cmd_pngs = save_cmd_pngs )
4819	def connect ( self ) : if JwtBuilder is None : raise NotConnectedToOpenEdX ( "This package must be installed in an OpenEdX environment." ) now = int ( time ( ) ) jwt = JwtBuilder . create_jwt_for_user ( self . user ) self . client = EdxRestApiClient ( self . API_BASE_URL , append_slash = self . APPEND_SLASH , jwt = jwt , ) self . expires_at = now + self . expires_in
4689	def init_aes ( shared_secret , nonce ) : " Shared Secret " ss = hashlib . sha512 ( unhexlify ( shared_secret ) ) . digest ( ) " Seed " seed = bytes ( str ( nonce ) , "ascii" ) + hexlify ( ss ) seed_digest = hexlify ( hashlib . sha512 ( seed ) . digest ( ) ) . decode ( "ascii" ) " AES " key = unhexlify ( seed_digest [ 0 : 64 ] ) iv = unhexlify ( seed_digest [ 64 : 96 ] ) return AES . new ( key , AES . MODE_CBC , iv )
7070	def precision ( ntp , nfp ) : if ( ntp + nfp ) > 0 : return ntp / ( ntp + nfp ) else : return np . nan
12041	def matrixToHTML ( data , names = None , units = None , bookName = None , sheetName = None , xCol = None ) : if not names : names = [ "" ] * len ( data [ 0 ] ) if data . dtype . names : names = list ( data . dtype . names ) if not units : units = [ "" ] * len ( data [ 0 ] ) for i in range ( len ( units ) ) : if names [ i ] in UNITS . keys ( ) : units [ i ] = UNITS [ names [ i ] ] if 'recarray' in str ( type ( data ) ) : #make it a regular array data = data . view ( float ) . reshape ( data . shape + ( - 1 , ) ) if xCol and xCol in names : xCol = names . index ( xCol ) names . insert ( 0 , names [ xCol ] ) units . insert ( 0 , units [ xCol ] ) data = np . insert ( data , 0 , data [ : , xCol ] , 1 ) htmlFname = tempfile . gettempdir ( ) + "/swhlab/WKS-%s.%s.html" % ( bookName , sheetName ) html = """<body> <style> body { background-color: #ababab; padding:20px; } table { font-size:12px; border-spacing: 0; border-collapse: collapse; //border:2px solid #000000; } .name {background-color:#fafac8;text-align:center;} .units {background-color:#fafac8;text-align:center;} .data0 {background-color:#FFFFFF;font-family: monospace;text-align:center;} .data1 {background-color:#FAFAFA;font-family: monospace;text-align:center;} .labelRow {background-color:#e0dfe4; text-align:right;border:1px solid #000000;} .labelCol {background-color:#e0dfe4; text-align:center;border:1px solid #000000;} td { border:1px solid #c0c0c0; padding:5px; //font-family: Verdana, Geneva, sans-serif; font-family: Arial, Helvetica, sans-serif } </style> <html>""" html += "<h1>FauxRigin</h1>" if bookName or sheetName : html += '<code><b>%s / %s</b></code><br><br>' % ( bookName , sheetName ) html += "<table>" #cols=list(range(len(names))) colNames = [ '' ] for i in range ( len ( units ) ) : label = "%s (%d)" % ( chr ( i + ord ( 'A' ) ) , i ) colNames . append ( label ) html += htmlListToTR ( colNames , 'labelCol' , 'labelCol' ) html += htmlListToTR ( [ 'Long Name' ] + list ( names ) , 'name' , td1Class = 'labelRow' ) html += htmlListToTR ( [ 'Units' ] + list ( units ) , 'units' , td1Class = 'labelRow' ) cutOff = False for y in range ( len ( data ) ) : html += htmlListToTR ( [ y + 1 ] + list ( data [ y ] ) , trClass = 'data%d' % ( y % 2 ) , td1Class = 'labelRow' ) if y >= 200 : cutOff = True break html += "</table>" html = html . replace ( ">nan<" , ">--<" ) html = html . replace ( ">None<" , "><" ) if cutOff : html += "<h3>... showing only %d of %d rows ...</h3>" % ( y , len ( data ) ) html += "</body></html>" with open ( htmlFname , 'w' ) as f : f . write ( html ) webbrowser . open ( htmlFname ) return
6761	def configure ( self ) : lm = self . last_manifest for tracker in self . get_trackers ( ) : self . vprint ( 'Checking tracker:' , tracker ) last_thumbprint = lm [ '_tracker_%s' % tracker . get_natural_key_hash ( ) ] self . vprint ( 'last thumbprint:' , last_thumbprint ) has_changed = tracker . is_changed ( last_thumbprint ) self . vprint ( 'Tracker changed:' , has_changed ) if has_changed : self . vprint ( 'Change detected!' ) tracker . act ( )
676	def __shouldSysExit ( self , iteration ) : if self . _exitAfter is None or iteration < self . _exitAfter : return False results = self . _jobsDAO . modelsGetFieldsForJob ( self . _jobID , [ 'params' ] ) modelIDs = [ e [ 0 ] for e in results ] modelNums = [ json . loads ( e [ 1 ] [ 0 ] ) [ 'structuredParams' ] [ '__model_num' ] for e in results ] sameModelNumbers = filter ( lambda x : x [ 1 ] == self . modelIndex , zip ( modelIDs , modelNums ) ) firstModelID = min ( zip ( * sameModelNumbers ) [ 0 ] ) return firstModelID == self . _modelID
7220	def to_geotiff ( arr , path = './output.tif' , proj = None , spec = None , bands = None , * * kwargs ) : assert has_rasterio , "To create geotiff images please install rasterio" try : img_md = arr . rda . metadata [ "image" ] x_size = img_md [ "tileXSize" ] y_size = img_md [ "tileYSize" ] except ( AttributeError , KeyError ) : x_size = kwargs . get ( "chunk_size" , 256 ) y_size = kwargs . get ( "chunk_size" , 256 ) try : tfm = kwargs [ 'transform' ] if 'transform' in kwargs else arr . affine except : tfm = None dtype = arr . dtype . name if arr . dtype . name != 'int8' else 'uint8' if spec is not None and spec . lower ( ) == 'rgb' : if bands is None : bands = arr . _rgb_bands # skip if already DRA'ed if not arr . options . get ( 'dra' ) : # add the RDA HistogramDRA op to get a RGB 8-bit image from gbdxtools . rda . interface import RDA rda = RDA ( ) dra = rda . HistogramDRA ( arr ) # Reset the bounds and select the bands on the new Dask arr = dra . aoi ( bbox = arr . bounds ) arr = arr [ bands , ... ] . astype ( np . uint8 ) dtype = 'uint8' else : if bands is not None : arr = arr [ bands , ... ] meta = { 'width' : arr . shape [ 2 ] , 'height' : arr . shape [ 1 ] , 'count' : arr . shape [ 0 ] , 'dtype' : dtype , 'driver' : 'GTiff' , 'transform' : tfm } if proj is not None : meta [ "crs" ] = { 'init' : proj } if "tiled" in kwargs and kwargs [ "tiled" ] : meta . update ( blockxsize = x_size , blockysize = y_size , tiled = "yes" ) with rasterio . open ( path , "w" , * * meta ) as dst : writer = rio_writer ( dst ) result = store ( arr , writer , compute = False ) result . compute ( scheduler = threaded_get ) return path
4106	def MINEIGVAL ( T0 , T , TOL ) : M = len ( T ) eigval = 10 eigvalold = 1 eigvec = numpy . zeros ( M + 1 , dtype = complex ) for k in range ( 0 , M + 1 ) : eigvec [ k ] = 1 + 0j it = 0 #print 'initialisation',T0, T, eigval, eigvec maxit = 15 while abs ( eigvalold - eigval ) > TOL * eigvalold and it < maxit : it = it + 1 eigvalold = eigval #print 'iteration ',it, 'eigvalold=',eigvalold, 'eigval=', eigval eig = toeplitz . HERMTOEP ( T0 , T , eigvec ) SUM = 0 save = 0. + 0j for k in range ( 0 , M + 1 ) : SUM = SUM + eig [ k ] . real ** 2 + eig [ k ] . imag ** 2 save = save + eig [ k ] * eigvec [ k ] . conjugate ( ) SUM = 1. / SUM eigval = save . real * SUM for k in range ( 0 , M + 1 ) : eigvec [ k ] = SUM * eig [ k ] if it == maxit : print ( 'warning reached max number of iteration (%s)' % maxit ) return eigval , eigvec
13440	def unlock_file ( filename ) : lockfile = "%s.lock" % filename if isfile ( lockfile ) : os . remove ( lockfile ) return True else : return False
9535	def get_complete_version ( version = None ) : if version is None : from django_cryptography import VERSION as version else : assert len ( version ) == 5 assert version [ 3 ] in ( 'alpha' , 'beta' , 'rc' , 'final' ) return version
12288	def datapackage_exists ( repo ) : datapath = os . path . join ( repo . rootdir , "datapackage.json" ) return os . path . exists ( datapath )
4233	def autodetect_url ( ) : for url in [ "http://routerlogin.net:5000" , "https://routerlogin.net" , "http://routerlogin.net" ] : try : r = requests . get ( url + "/soap/server_sa/" , headers = _get_soap_headers ( "Test:1" , "test" ) , verify = False ) if r . status_code == 200 : return url except requests . exceptions . RequestException : pass return None
10814	def add_member ( self , user , state = MembershipState . ACTIVE ) : return Membership . create ( self , user , state )
12119	def headerHTML ( self , fname = None ) : if fname is None : fname = self . fname . replace ( ".abf" , "_header.html" ) html = "<html><body><code>" html += "<h2>abfinfo() for %s.abf</h2>" % self . ID html += self . abfinfo ( ) . replace ( "<" , "&lt;" ) . replace ( ">" , "&gt;" ) . replace ( "\n" , "<br>" ) html += "<h2>Header for %s.abf</h2>" % self . ID html += pprint . pformat ( self . header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "WRITING HEADER TO:" ) print ( fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( )
7608	def get_all_cards ( self , timeout : int = None ) : url = self . api . CARDS return self . _get_model ( url , timeout = timeout )
10269	def node_is_upstream_leaf ( graph : BELGraph , node : BaseEntity ) -> bool : return 0 == len ( graph . predecessors ( node ) ) and 1 == len ( graph . successors ( node ) )
3796	def setup_a_alpha_and_derivatives ( self , i , T = None ) : if not hasattr ( self , 'kappas' ) : self . kappas = [ ] for Tc , kappa0 , kappa1 , kappa2 , kappa3 in zip ( self . Tcs , self . kappa0s , self . kappa1s , self . kappa2s , self . kappa3s ) : Tr = T / Tc kappa = kappa0 + ( ( kappa1 + kappa2 * ( kappa3 - Tr ) * ( 1. - Tr ** 0.5 ) ) * ( 1. + Tr ** 0.5 ) * ( 0.7 - Tr ) ) self . kappas . append ( kappa ) ( self . a , self . kappa , self . kappa0 , self . kappa1 , self . kappa2 , self . kappa3 , self . Tc ) = ( self . ais [ i ] , self . kappas [ i ] , self . kappa0s [ i ] , self . kappa1s [ i ] , self . kappa2s [ i ] , self . kappa3s [ i ] , self . Tcs [ i ] )
6278	def draw ( self , current_time , frame_time ) : self . set_default_viewport ( ) self . timeline . draw ( current_time , frame_time , self . fbo )
1063	def readheaders ( self ) : self . dict = { } self . unixfrom = '' self . headers = lst = [ ] self . status = '' headerseen = "" firstline = 1 startofline = unread = tell = None if hasattr ( self . fp , 'unread' ) : unread = self . fp . unread elif self . seekable : tell = self . fp . tell while 1 : if tell : try : startofline = tell ( ) except IOError : startofline = tell = None self . seekable = 0 line = self . fp . readline ( ) if not line : self . status = 'EOF in headers' break # Skip unix From name time lines if firstline and line . startswith ( 'From ' ) : self . unixfrom = self . unixfrom + line continue firstline = 0 if headerseen and line [ 0 ] in ' \t' : # It's a continuation line. lst . append ( line ) x = ( self . dict [ headerseen ] + "\n " + line . strip ( ) ) self . dict [ headerseen ] = x . strip ( ) continue elif self . iscomment ( line ) : # It's a comment. Ignore it. continue elif self . islast ( line ) : # Note! No pushback here! The delimiter line gets eaten. break headerseen = self . isheader ( line ) if headerseen : # It's a legal header line, save it. lst . append ( line ) self . dict [ headerseen ] = line [ len ( headerseen ) + 1 : ] . strip ( ) continue elif headerseen is not None : # An empty header name. These aren't allowed in HTTP, but it's # probably a benign mistake. Don't add the header, just keep # going. continue else : # It's not a header line; throw it back and stop here. if not self . dict : self . status = 'No headers' else : self . status = 'Non-header line where header expected' # Try to undo the read. if unread : unread ( line ) elif tell : self . fp . seek ( startofline ) else : self . status = self . status + '; bad seek' break
5967	def solvate ( struct = 'top/protein.pdb' , top = 'top/system.top' , distance = 0.9 , boxtype = 'dodecahedron' , concentration = 0 , cation = 'NA' , anion = 'CL' , water = 'tip4p' , solvent_name = 'SOL' , with_membrane = False , ndx = 'main.ndx' , mainselection = '"Protein"' , dirname = 'solvate' , * * kwargs ) : sol = solvate_sol ( struct = struct , top = top , distance = distance , boxtype = boxtype , water = water , solvent_name = solvent_name , with_membrane = with_membrane , dirname = dirname , * * kwargs ) ion = solvate_ion ( struct = sol [ 'struct' ] , top = top , concentration = concentration , cation = cation , anion = anion , solvent_name = solvent_name , ndx = ndx , mainselection = mainselection , dirname = dirname , * * kwargs ) return ion
742	def readFromFile ( cls , f , packed = True ) : # Get capnproto schema from instance schema = cls . getSchema ( ) # Read from file if packed : proto = schema . read_packed ( f ) else : proto = schema . read ( f ) # Return first-class instance initialized from proto obj return cls . read ( proto )
5503	def config ( ctx , key , value , remove , edit ) : conf = ctx . obj [ "conf" ] if not edit and not key : raise click . BadArgumentUsage ( "You have to specify either a key or use --edit." ) if edit : return click . edit ( filename = conf . config_file ) if remove : try : conf . cfg . remove_option ( key [ 0 ] , key [ 1 ] ) except Exception as e : logger . debug ( e ) else : conf . write_config ( ) return if not value : try : click . echo ( conf . cfg . get ( key [ 0 ] , key [ 1 ] ) ) except Exception as e : logger . debug ( e ) return if not conf . cfg . has_section ( key [ 0 ] ) : conf . cfg . add_section ( key [ 0 ] ) conf . cfg . set ( key [ 0 ] , key [ 1 ] , value ) conf . write_config ( )
8179	def clear ( self ) : dict . clear ( self ) self . nodes = [ ] self . edges = [ ] self . root = None self . layout . i = 0 self . alpha = 0
6038	def yticks ( self ) : return np . linspace ( np . min ( self [ : , 0 ] ) , np . max ( self [ : , 0 ] ) , 4 )
12367	def create ( self , name , ip_address ) : return ( self . post ( name = name , ip_address = ip_address ) . get ( self . singular , None ) )
11688	def get_changeset ( changeset ) : url = 'https://www.openstreetmap.org/api/0.6/changeset/{}/download' . format ( changeset ) return ET . fromstring ( requests . get ( url ) . content )
10120	def rectangle ( cls , vertices , * * kwargs ) : bottom_left , top_right = vertices top_left = [ bottom_left [ 0 ] , top_right [ 1 ] ] bottom_right = [ top_right [ 0 ] , bottom_left [ 1 ] ] return cls ( [ bottom_left , bottom_right , top_right , top_left ] , * * kwargs )
1668	def ShouldCheckNamespaceIndentation ( nesting_state , is_namespace_indent_item , raw_lines_no_comments , linenum ) : is_forward_declaration = IsForwardClassDeclaration ( raw_lines_no_comments , linenum ) if not ( is_namespace_indent_item or is_forward_declaration ) : return False # If we are in a macro, we do not want to check the namespace indentation. if IsMacroDefinition ( raw_lines_no_comments , linenum ) : return False return IsBlockInNameSpace ( nesting_state , is_forward_declaration )
12405	def reverse ( self ) : if self . _original_target_content : with open ( self . target , 'w' ) as fp : fp . write ( self . _original_target_content )
2064	def migrate ( self , expression , name_migration_map = None ) : if name_migration_map is None : name_migration_map = { } # name_migration_map -> object_migration_map # Based on the name mapping in name_migration_map build an object to # object mapping to be used in the replacing of variables # inv: object_migration_map's keys should ALWAYS be external/foreign # expressions, and its values should ALWAYS be internal/local expressions object_migration_map = { } #List of foreign vars used in expression foreign_vars = itertools . filterfalse ( self . is_declared , get_variables ( expression ) ) for foreign_var in foreign_vars : # If a variable with the same name was previously migrated if foreign_var . name in name_migration_map : migrated_name = name_migration_map [ foreign_var . name ] native_var = self . get_variable ( migrated_name ) assert native_var is not None , "name_migration_map contains a variable that does not exist in this ConstraintSet" object_migration_map [ foreign_var ] = native_var else : # foreign_var was not found in the local declared variables nor # any variable with the same name was previously migrated # let's make a new unique internal name for it migrated_name = foreign_var . name if migrated_name in self . _declarations : migrated_name = self . _make_unique_name ( f'{foreign_var.name}_migrated' ) # Create and declare a new variable of given type if isinstance ( foreign_var , Bool ) : new_var = self . new_bool ( name = migrated_name ) elif isinstance ( foreign_var , BitVec ) : new_var = self . new_bitvec ( foreign_var . size , name = migrated_name ) elif isinstance ( foreign_var , Array ) : # Note that we are discarding the ArrayProxy encapsulation new_var = self . new_array ( index_max = foreign_var . index_max , index_bits = foreign_var . index_bits , value_bits = foreign_var . value_bits , name = migrated_name ) . array else : raise NotImplemented ( f"Unknown expression type {type(var)} encountered during expression migration" ) # Update the var to var mapping object_migration_map [ foreign_var ] = new_var # Update the name to name mapping name_migration_map [ foreign_var . name ] = new_var . name # Actually replace each appearance of migrated variables by the new ones migrated_expression = replace ( expression , object_migration_map ) return migrated_expression
3273	def as_DAVError ( e ) : if isinstance ( e , DAVError ) : return e elif isinstance ( e , Exception ) : # traceback.print_exc() return DAVError ( HTTP_INTERNAL_ERROR , src_exception = e ) else : return DAVError ( HTTP_INTERNAL_ERROR , "{}" . format ( e ) )
6948	def jhk_to_sdssg ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSG_JHK , SDSSG_JH , SDSSG_JK , SDSSG_HK , SDSSG_J , SDSSG_H , SDSSG_K )
12292	def annotate_metadata_code ( repo , files ) : package = repo . package package [ 'code' ] = [ ] for p in files : matching_files = glob2 . glob ( "**/{}" . format ( p ) ) for f in matching_files : absf = os . path . abspath ( f ) print ( "Add commit data for {}" . format ( f ) ) package [ 'code' ] . append ( OrderedDict ( [ ( 'script' , f ) , ( 'permalink' , repo . manager . permalink ( repo , absf ) ) , ( 'mimetypes' , mimetypes . guess_type ( absf ) [ 0 ] ) , ( 'sha256' , compute_sha256 ( absf ) ) ] ) )
9674	def get_days_span ( self , month_index ) : is_first_month = month_index == 0 is_last_month = month_index == self . __len__ ( ) - 1 y = int ( self . start_date . year + ( self . start_date . month + month_index ) / 13 ) m = int ( ( self . start_date . month + month_index ) % 12 or 12 ) total = calendar . monthrange ( y , m ) [ 1 ] if is_first_month and is_last_month : return ( self . end_date - self . start_date ) . days + 1 else : if is_first_month : return total - self . start_date . day + 1 elif is_last_month : return self . end_date . day else : return total
11595	def _rc_renamenx ( self , src , dst ) : if self . exists ( dst ) : return False return self . _rc_rename ( src , dst )
1175	def unlock ( self ) : if self . queue : function , argument = self . queue . popleft ( ) function ( argument ) else : self . locked = False
4157	def arma_estimate ( X , P , Q , lag ) : R = CORRELATION ( X , maxlags = lag , norm = 'unbiased' ) R0 = R [ 0 ] #C Estimate the AR parameters (no error weighting is used). #C Number of equation errors is M-Q . MPQ = lag - Q + P N = len ( X ) Y = np . zeros ( N - P , dtype = complex ) for K in range ( 0 , MPQ ) : KPQ = K + Q - P + 1 if KPQ < 0 : Y [ K ] = R [ - KPQ ] . conjugate ( ) if KPQ == 0 : Y [ K ] = R0 if KPQ > 0 : Y [ K ] = R [ KPQ ] # The resize is very important for the normalissation. Y . resize ( lag ) if P <= 4 : res = arcovar_marple ( Y . copy ( ) , P ) #! Eq. (10.12) ar_params = res [ 0 ] else : res = arcovar ( Y . copy ( ) , P ) #! Eq. (10.12) ar_params = res [ 0 ] # the .copy is used to prevent a reference somewhere. this is a bug # to be tracked down. Y . resize ( N - P ) #C Filter the original time series for k in range ( P , N ) : SUM = X [ k ] #SUM += sum([ar_params[j]*X[k-j-1] for j in range(0,P)]) for j in range ( 0 , P ) : SUM = SUM + ar_params [ j ] * X [ k - j - 1 ] #! Eq. (10.17) Y [ k - P ] = SUM # Estimate the MA parameters (a "long" AR of order at least 2*IQ #C is suggested) #Y.resize(N-P) ma_params , rho = ma ( Y , Q , 2 * Q ) #! Eq. (10.3) return ar_params , ma_params , rho
4631	def point ( self ) : string = unhexlify ( self . unCompressed ( ) ) return ecdsa . VerifyingKey . from_string ( string [ 1 : ] , curve = ecdsa . SECP256k1 ) . pubkey . point
6243	def load ( self ) : self . _open_image ( ) width , height , depth = self . image . size [ 0 ] , self . image . size [ 1 ] // self . layers , self . layers components , data = image_data ( self . image ) texture = self . ctx . texture_array ( ( width , height , depth ) , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build_mipmaps ( ) self . _close_image ( ) return texture
5585	def output_is_valid ( self , process_data ) : if self . METADATA [ "data_type" ] == "raster" : return ( is_numpy_or_masked_array ( process_data ) or is_numpy_or_masked_array_with_tags ( process_data ) ) elif self . METADATA [ "data_type" ] == "vector" : return is_feature_list ( process_data )
9092	def _get_old_entry_identifiers ( namespace : Namespace ) -> Set [ NamespaceEntry ] : return { term . identifier for term in namespace . entries }
5109	def set_num_servers ( self , n ) : if not isinstance ( n , numbers . Integral ) and n is not infty : the_str = "n must be an integer or infinity.\n{0}" raise TypeError ( the_str . format ( str ( self ) ) ) elif n <= 0 : the_str = "n must be a positive integer or infinity.\n{0}" raise ValueError ( the_str . format ( str ( self ) ) ) else : self . num_servers = n
6044	def padded_grid_from_shape_psf_shape_and_pixel_scale ( cls , shape , psf_shape , pixel_scale ) : padded_shape = ( shape [ 0 ] + psf_shape [ 0 ] - 1 , shape [ 1 ] + psf_shape [ 1 ] - 1 ) padded_regular_grid = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( padded_shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) padded_mask = msk . Mask . unmasked_for_shape_and_pixel_scale ( shape = padded_shape , pixel_scale = pixel_scale ) return PaddedRegularGrid ( arr = padded_regular_grid , mask = padded_mask , image_shape = shape )
829	def encodedBitDescription ( self , bitOffset , formatted = False ) : # Find which field it's in ( prevFieldName , prevFieldOffset ) = ( None , None ) description = self . getDescription ( ) for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if formatted : offset = offset + i if bitOffset == offset - 1 : prevFieldName = "separator" prevFieldOffset = bitOffset break if bitOffset < offset : break ( prevFieldName , prevFieldOffset ) = ( name , offset ) # Return the field name and offset within the field # return (fieldName, bitOffset - fieldOffset) width = self . getDisplayWidth ( ) if formatted else self . getWidth ( ) if prevFieldOffset is None or bitOffset > self . getWidth ( ) : raise IndexError ( "Bit is outside of allowable range: [0 - %d]" % width ) return ( prevFieldName , bitOffset - prevFieldOffset )
13879	def AppendToFile ( filename , contents , eol_style = EOL_STYLE_NATIVE , encoding = None , binary = False ) : _AssertIsLocal ( filename ) assert isinstance ( contents , six . text_type ) ^ binary , 'Must always receive unicode contents, unless binary=True' if not binary : # Replaces eol on each line by the given eol_style. contents = _HandleContentsEol ( contents , eol_style ) # Handle encoding here, and always write in binary mode. We can't use io.open because it # tries to do its own line ending handling. contents = contents . encode ( encoding or sys . getfilesystemencoding ( ) ) oss = open ( filename , 'ab' ) try : oss . write ( contents ) finally : oss . close ( )
8849	def setup_actions ( self ) : self . actionOpen . triggered . connect ( self . on_open ) self . actionNew . triggered . connect ( self . on_new ) self . actionSave . triggered . connect ( self . on_save ) self . actionSave_as . triggered . connect ( self . on_save_as ) self . actionQuit . triggered . connect ( QtWidgets . QApplication . instance ( ) . quit ) self . tabWidget . current_changed . connect ( self . on_current_tab_changed ) self . tabWidget . last_tab_closed . connect ( self . on_last_tab_closed ) self . actionAbout . triggered . connect ( self . on_about ) self . actionRun . triggered . connect ( self . on_run ) self . interactiveConsole . process_finished . connect ( self . on_process_finished ) self . actionConfigure_run . triggered . connect ( self . on_configure_run )
1338	def crossentropy ( label , logits ) : assert logits . ndim == 1 # for numerical reasons we subtract the max logit # (mathematically it doesn't matter!) # otherwise exp(logits) might become too large or too small logits = logits - np . max ( logits ) e = np . exp ( logits ) s = np . sum ( e ) ce = np . log ( s ) - logits [ label ] return ce
4993	def transmit_learner_data ( username , channel_code , channel_pk ) : start = time . time ( ) api_user = User . objects . get ( username = username ) integrated_channel = INTEGRATED_CHANNEL_CHOICES [ channel_code ] . objects . get ( pk = channel_pk ) LOGGER . info ( 'Processing learners for integrated channel using configuration: [%s]' , integrated_channel ) # Note: learner data transmission code paths don't raise any uncaught exception, so we don't need a broad # try-except block here. integrated_channel . transmit_learner_data ( api_user ) duration = time . time ( ) - start LOGGER . info ( 'Learner data transmission task for integrated channel configuration [%s] took [%s] seconds' , integrated_channel , duration )
4133	def codestr2rst ( codestr , lang = 'python' ) : code_directive = "\n.. code-block:: {0}\n\n" . format ( lang ) indented_block = indent ( codestr , ' ' * 4 ) return code_directive + indented_block
1464	def __replace ( config , wildcards , config_file ) : for config_key in config : config_value = config [ config_key ] original_value = config_value if isinstance ( config_value , str ) : for token in wildcards : if wildcards [ token ] : config_value = config_value . replace ( token , wildcards [ token ] ) found = re . findall ( r'\${[A-Z_]+}' , config_value ) if found : raise ValueError ( "%s=%s in file %s contains unsupported or unset wildcard tokens: %s" % ( config_key , original_value , config_file , ", " . join ( found ) ) ) config [ config_key ] = config_value return config
6579	def _send_cmd ( self , cmd ) : self . _process . stdin . write ( "{}\n" . format ( cmd ) . encode ( "utf-8" ) ) self . _process . stdin . flush ( )
9705	def run ( self ) : while self . isRunning . is_set ( ) : try : try : # self.checkTUN() self . monitorTUN ( ) except timeout_decorator . TimeoutError as error : # No data received so just move on pass self . checkSerial ( ) except KeyboardInterrupt : break
9199	def reversals ( series , left = False , right = False ) : series = iter ( series ) x_last , x = next ( series ) , next ( series ) d_last = ( x - x_last ) if left : yield x_last for x_next in series : if x_next == x : continue d_next = x_next - x if d_last * d_next < 0 : yield x x_last , x = x , x_next d_last = d_next if right : yield x_next
9315	def _ensure_datetime_to_string ( maybe_dttm ) : if isinstance ( maybe_dttm , datetime . datetime ) : maybe_dttm = _format_datetime ( maybe_dttm ) return maybe_dttm
13284	def list_from_document ( cls , doc ) : objs = [ ] for feu in doc . xpath ( '//FEU' ) : detail_els = feu . xpath ( 'event-element-details/event-element-detail' ) for idx , detail in enumerate ( detail_els ) : objs . append ( cls ( feu , detail , id_suffix = idx , number_in_group = len ( detail_els ) ) ) return objs
11612	def report_read_counts ( self , filename , grp_wise = False , reorder = 'as-is' , notes = None ) : expected_read_counts = self . probability . sum ( axis = APM . Axis . READ ) if grp_wise : lname = self . probability . gname expected_read_counts = expected_read_counts * self . grp_conv_mat else : lname = self . probability . lname total_read_counts = expected_read_counts . sum ( axis = 0 ) if reorder == 'decreasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) report_order = report_order [ : : - 1 ] elif reorder == 'increasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) elif reorder == 'as-is' : report_order = np . arange ( len ( lname ) ) # report in the original locus order cntdata = np . vstack ( ( expected_read_counts , total_read_counts ) ) fhout = open ( filename , 'w' ) fhout . write ( "locus\t" + "\t" . join ( self . probability . hname ) + "\ttotal" ) if notes is not None : fhout . write ( "\tnotes" ) fhout . write ( "\n" ) for locus_id in report_order : lname_cur = lname [ locus_id ] fhout . write ( "\t" . join ( [ lname_cur ] + map ( str , cntdata [ : , locus_id ] . ravel ( ) ) ) ) if notes is not None : fhout . write ( "\t%s" % notes [ lname_cur ] ) fhout . write ( "\n" ) fhout . close ( )
7255	def get ( self , catID , includeRelationships = False ) : url = '%(base_url)s/record/%(catID)s' % { 'base_url' : self . base_url , 'catID' : catID } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( )
7317	def sendmail ( self , msg_from , msg_to , msg ) : SMTP_dummy . msg_from = msg_from SMTP_dummy . msg_to = msg_to SMTP_dummy . msg = msg
13596	def get ( f , key , default = None ) : if key in f . keys ( ) : val = f [ key ] . value if default is None : return val else : if _np . shape ( val ) == _np . shape ( default ) : return val return default
13907	def create_commands ( self , commands , parser ) : self . apply_defaults ( commands ) def create_single_command ( command ) : keys = command [ 'keys' ] del command [ 'keys' ] kwargs = { } for item in command : kwargs [ item ] = command [ item ] parser . add_argument ( * keys , * * kwargs ) if len ( commands ) > 1 : for command in commands : create_single_command ( command ) else : create_single_command ( commands [ 0 ] )
11536	def map_pin ( self , abstract_pin_id , physical_pin_id ) : if physical_pin_id : self . _pin_mapping [ abstract_pin_id ] = physical_pin_id else : self . _pin_mapping . pop ( abstract_pin_id , None )
1258	def get_savable_components ( self ) : components = self . get_components ( ) components = [ components [ name ] for name in sorted ( components ) ] return set ( filter ( lambda x : isinstance ( x , util . SavableComponent ) , components ) )
8395	def show_help ( ) : print ( """\ Manage local config files from masters in edx_lint. Commands: """ ) for cmd in [ write_main , check_main , list_main ] : print ( cmd . __doc__ . lstrip ( "\n" ) )
12416	def end ( self , * args , * * kwargs ) : self . send ( * args , * * kwargs ) self . close ( )
10615	def clone ( self ) : result = copy . copy ( self ) result . _compound_masses = copy . deepcopy ( self . _compound_masses ) return result
6642	def satisfyDependenciesRecursive ( self , available_components = None , search_dirs = None , update_installed = False , traverse_links = False , target = None , test = False ) : def provider ( dspec , available_components , search_dirs , working_directory , update_installed , dep_of = None ) : r = access . satisfyFromAvailable ( dspec . name , available_components ) if r : if r . isTestDependency ( ) and not dspec . is_test_dependency : logger . debug ( 'test dependency subsequently occurred as real dependency: %s' , r . getName ( ) ) r . setTestDependency ( False ) return r update_if_installed = False if update_installed is True : update_if_installed = True elif update_installed : update_if_installed = dspec . name in update_installed r = access . satisfyVersionFromSearchPaths ( dspec . name , dspec . versionReq ( ) , search_dirs , update_if_installed , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if r : r . setTestDependency ( dspec . is_test_dependency ) return r # before resorting to install this module, check if we have an # existing linked module (which wasn't picked up because it didn't # match the version specification) - if we do, then we shouldn't # try to install, but should return that anyway: default_path = os . path . join ( self . modulesPath ( ) , dspec . name ) if fsutils . isLink ( default_path ) : r = Component ( default_path , test_dependency = dspec . is_test_dependency , installed_linked = fsutils . isLink ( default_path ) , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if r : assert ( r . installedLinked ( ) ) return r else : logger . error ( 'linked module %s is invalid: %s' , dspec . name , r . getError ( ) ) return r r = access . satisfyVersionByInstalling ( dspec . name , dspec . versionReq ( ) , self . modulesPath ( ) , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if not r : logger . error ( 'could not install %s' % dspec . name ) if r is not None : r . setTestDependency ( dspec . is_test_dependency ) return r return self . __getDependenciesRecursiveWithProvider ( available_components = available_components , search_dirs = search_dirs , target = target , traverse_links = traverse_links , update_installed = update_installed , provider = provider , test = test )
13400	def addLogbook ( self , physDef = "LCLS" , mccDef = "MCC" , initialInstance = False ) : if self . logMenuCount < 5 : self . logMenus . append ( LogSelectMenu ( self . logui . multiLogLayout , initialInstance ) ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 1 ] , self . physics_programs , physDef ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 0 ] , self . mcc_programs , mccDef ) self . logMenus [ - 1 ] . show ( ) self . logMenuCount += 1 if initialInstance : # Initial logbook menu can add additional menus, all others can only remove themselves. QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , self . addLogbook ) else : from functools import partial QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , partial ( self . removeLogbook , self . logMenus [ - 1 ] ) )
5107	def fetch_data ( self , return_header = False ) : qdata = [ ] for d in self . data . values ( ) : qdata . extend ( d ) dat = np . zeros ( ( len ( qdata ) , 6 ) ) if len ( qdata ) > 0 : dat [ : , : 5 ] = np . array ( qdata ) dat [ : , 5 ] = self . edge [ 2 ] dType = [ ( 'a' , float ) , ( 's' , float ) , ( 'd' , float ) , ( 'q' , float ) , ( 'n' , float ) , ( 'id' , float ) ] dat = np . array ( [ tuple ( d ) for d in dat ] , dtype = dType ) dat = np . sort ( dat , order = 'a' ) dat = np . array ( [ tuple ( d ) for d in dat ] ) if return_header : return dat , 'arrival,service,departure,num_queued,num_total,q_id' return dat
8650	def post_review ( session , review ) : # POST /api/projects/0.1/reviews/ response = make_post_request ( session , 'reviews' , json_data = review ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise ReviewNotPostedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9544	def add_value_check ( self , field_name , value_check , code = VALUE_CHECK_FAILED , message = MESSAGES [ VALUE_CHECK_FAILED ] , modulus = 1 ) : # guard conditions assert field_name in self . _field_names , 'unexpected field name: %s' % field_name assert callable ( value_check ) , 'value check must be a callable function' t = field_name , value_check , code , message , modulus self . _value_checks . append ( t )
8923	def baseurl ( url ) : parsed_url = urlparse . urlparse ( url ) if not parsed_url . netloc or parsed_url . scheme not in ( "http" , "https" ) : raise ValueError ( 'bad url' ) service_url = "%s://%s%s" % ( parsed_url . scheme , parsed_url . netloc , parsed_url . path . strip ( ) ) return service_url
9437	def strip_ethernet ( packet ) : if not isinstance ( packet , Ethernet ) : packet = Ethernet ( packet ) payload = packet . payload return payload
7850	def add_feature ( self , var ) : if self . has_feature ( var ) : return n = self . xmlnode . newChild ( None , "feature" , None ) n . setProp ( "var" , to_utf8 ( var ) )
11279	def _encode_ids ( * args ) : ids = [ ] for v in args : if isinstance ( v , basestring ) : qv = v . encode ( 'utf-8' ) if isinstance ( v , unicode ) else v ids . append ( urllib . quote ( qv ) ) else : qv = str ( v ) ids . append ( urllib . quote ( qv ) ) return ';' . join ( ids )
1395	def addNewTopology ( self , state_manager , topologyName ) : topology = Topology ( topologyName , state_manager . name ) Log . info ( "Adding new topology: %s, state_manager: %s" , topologyName , state_manager . name ) self . topologies . append ( topology ) # Register a watch on topology and change # the topologyInfo on any new change. topology . register_watch ( self . setTopologyInfo ) def on_topology_pplan ( data ) : """watch physical plan""" Log . info ( "Watch triggered for topology pplan: " + topologyName ) topology . set_physical_plan ( data ) if not data : Log . debug ( "No data to be set" ) def on_topology_packing_plan ( data ) : """watch packing plan""" Log . info ( "Watch triggered for topology packing plan: " + topologyName ) topology . set_packing_plan ( data ) if not data : Log . debug ( "No data to be set" ) def on_topology_execution_state ( data ) : """watch execution state""" Log . info ( "Watch triggered for topology execution state: " + topologyName ) topology . set_execution_state ( data ) if not data : Log . debug ( "No data to be set" ) def on_topology_tmaster ( data ) : """set tmaster""" Log . info ( "Watch triggered for topology tmaster: " + topologyName ) topology . set_tmaster ( data ) if not data : Log . debug ( "No data to be set" ) def on_topology_scheduler_location ( data ) : """set scheduler location""" Log . info ( "Watch triggered for topology scheduler location: " + topologyName ) topology . set_scheduler_location ( data ) if not data : Log . debug ( "No data to be set" ) # Set watches on the pplan, execution_state, tmaster and scheduler_location. state_manager . get_pplan ( topologyName , on_topology_pplan ) state_manager . get_packing_plan ( topologyName , on_topology_packing_plan ) state_manager . get_execution_state ( topologyName , on_topology_execution_state ) state_manager . get_tmaster ( topologyName , on_topology_tmaster ) state_manager . get_scheduler_location ( topologyName , on_topology_scheduler_location )
13423	def line ( self , line ) : # Remove empty strings in case of multiple instances of delimiter return [ x for x in re . split ( self . delimiter , line . rstrip ( ) ) if x != '' ]
9848	def _load_dx ( self , filename ) : dx = OpenDX . field ( 0 ) dx . read ( filename ) grid , edges = dx . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
673	def runHotgym ( numRecords ) : # Create a data source for the network. dataSource = FileRecordStream ( streamID = _INPUT_FILE_PATH ) numRecords = min ( numRecords , dataSource . getDataRowCount ( ) ) network = createNetwork ( dataSource ) # Set predicted field network . regions [ "sensor" ] . setParameter ( "predictedField" , "consumption" ) # Enable learning for all regions. network . regions [ "SP" ] . setParameter ( "learningMode" , 1 ) network . regions [ "TM" ] . setParameter ( "learningMode" , 1 ) network . regions [ "classifier" ] . setParameter ( "learningMode" , 1 ) # Enable inference for all regions. network . regions [ "SP" ] . setParameter ( "inferenceMode" , 1 ) network . regions [ "TM" ] . setParameter ( "inferenceMode" , 1 ) network . regions [ "classifier" ] . setParameter ( "inferenceMode" , 1 ) results = [ ] N = 1 # Run the network, N iterations at a time. for iteration in range ( 0 , numRecords , N ) : network . run ( N ) predictionResults = getPredictionResults ( network , "classifier" ) oneStep = predictionResults [ 1 ] [ "predictedValue" ] oneStepConfidence = predictionResults [ 1 ] [ "predictionConfidence" ] fiveStep = predictionResults [ 5 ] [ "predictedValue" ] fiveStepConfidence = predictionResults [ 5 ] [ "predictionConfidence" ] result = ( oneStep , oneStepConfidence * 100 , fiveStep , fiveStepConfidence * 100 ) print "1-step: {:16} ({:4.4}%)\t 5-step: {:16} ({:4.4}%)" . format ( * result ) results . append ( result ) return results
3127	def update ( self , template_id , data ) : if 'name' not in data : raise KeyError ( 'The template must have a name' ) if 'html' not in data : raise KeyError ( 'The template must have html' ) self . template_id = template_id return self . _mc_client . _patch ( url = self . _build_path ( template_id ) , data = data )
6271	def sphere ( radius = 0.5 , sectors = 32 , rings = 16 ) -> VAO : R = 1.0 / ( rings - 1 ) S = 1.0 / ( sectors - 1 ) vertices = [ 0 ] * ( rings * sectors * 3 ) normals = [ 0 ] * ( rings * sectors * 3 ) uvs = [ 0 ] * ( rings * sectors * 2 ) v , n , t = 0 , 0 , 0 for r in range ( rings ) : for s in range ( sectors ) : y = math . sin ( - math . pi / 2 + math . pi * r * R ) x = math . cos ( 2 * math . pi * s * S ) * math . sin ( math . pi * r * R ) z = math . sin ( 2 * math . pi * s * S ) * math . sin ( math . pi * r * R ) uvs [ t ] = s * S uvs [ t + 1 ] = r * R vertices [ v ] = x * radius vertices [ v + 1 ] = y * radius vertices [ v + 2 ] = z * radius normals [ n ] = x normals [ n + 1 ] = y normals [ n + 2 ] = z t += 2 v += 3 n += 3 indices = [ 0 ] * rings * sectors * 6 i = 0 for r in range ( rings - 1 ) : for s in range ( sectors - 1 ) : indices [ i ] = r * sectors + s indices [ i + 1 ] = ( r + 1 ) * sectors + ( s + 1 ) indices [ i + 2 ] = r * sectors + ( s + 1 ) indices [ i + 3 ] = r * sectors + s indices [ i + 4 ] = ( r + 1 ) * sectors + s indices [ i + 5 ] = ( r + 1 ) * sectors + ( s + 1 ) i += 6 vbo_vertices = numpy . array ( vertices , dtype = numpy . float32 ) vbo_normals = numpy . array ( normals , dtype = numpy . float32 ) vbo_uvs = numpy . array ( uvs , dtype = numpy . float32 ) vbo_elements = numpy . array ( indices , dtype = numpy . uint32 ) vao = VAO ( "sphere" , mode = mlg . TRIANGLES ) # VBOs vao . buffer ( vbo_vertices , '3f' , [ 'in_position' ] ) vao . buffer ( vbo_normals , '3f' , [ 'in_normal' ] ) vao . buffer ( vbo_uvs , '2f' , [ 'in_uv' ] ) vao . index_buffer ( vbo_elements , index_element_size = 4 ) return vao
10748	def validate_bands ( self , bands ) : if not isinstance ( bands , list ) : logger . error ( 'Parameter bands must be a "list"' ) raise TypeError ( 'Parameter bands must be a "list"' ) valid_bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] for band in bands : if band not in valid_bands : logger . error ( '%s is not a valid band' % band ) raise InvalidBandError ( '%s is not a valid band' % band )
10188	def publish ( self , event_type , events ) : assert event_type in self . events current_queues . queues [ 'stats-{}' . format ( event_type ) ] . publish ( events )
4033	def parse ( s ) : if IS_PY3 : r = sre_parse . parse ( s , flags = U ) else : r = sre_parse . parse ( s . decode ( 'utf-8' ) , flags = U ) return list ( r )
3830	async def rename_conversation ( self , rename_conversation_request ) : response = hangouts_pb2 . RenameConversationResponse ( ) await self . _pb_request ( 'conversations/renameconversation' , rename_conversation_request , response ) return response
9642	def set_trace ( context ) : try : import ipdb as pdb except ImportError : import pdb print ( "For best results, pip install ipdb." ) print ( "Variables that are available in the current context:" ) render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) pprint ( availables ) print ( 'Type `availables` to show this list.' ) print ( 'Type <variable_name> to access one.' ) print ( 'Use render("template string") to test template rendering' ) # Cram context variables into the local scope for var in availables : locals ( ) [ var ] = context [ var ] pdb . set_trace ( ) return ''
7576	def _call_structure ( mname , ename , sname , name , workdir , seed , ntaxa , nsites , kpop , rep ) : ## create call string outname = os . path . join ( workdir , "{}-K-{}-rep-{}" . format ( name , kpop , rep ) ) cmd = [ "structure" , "-m" , mname , "-e" , ename , "-K" , str ( kpop ) , "-D" , str ( seed ) , "-N" , str ( ntaxa ) , "-L" , str ( nsites ) , "-i" , sname , "-o" , outname ] ## call the shell function proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) comm = proc . communicate ( ) ## cleanup oldfiles = [ mname , ename , sname ] for oldfile in oldfiles : if os . path . exists ( oldfile ) : os . remove ( oldfile ) return comm
13722	def log ( self , url = None , credentials = None , do_verify_certificate = True ) : if url is None : url = self . url if re . match ( "file://" , url ) : self . log_file ( url ) elif re . match ( "https://" , url ) or re . match ( "http://" , url ) : self . log_post ( url , credentials , do_verify_certificate ) else : self . log_stdout ( )
12018	def disassemble ( self ) : ser_pb = open ( self . input_file , 'rb' ) . read ( ) # Read serialized pb file fd = FileDescriptorProto ( ) fd . ParseFromString ( ser_pb ) self . name = fd . name self . _print ( '// Reversed by pbd (https://github.com/rsc-dev/pbd)' ) self . _print ( 'syntax = "proto2";' ) self . _print ( '' ) if len ( fd . package ) > 0 : self . _print ( 'package {};' . format ( fd . package ) ) self . package = fd . package else : self . _print ( '// Package not defined' ) self . _walk ( fd )
12612	def search_unique ( self , table_name , sample , unique_fields = None ) : return search_unique ( table = self . table ( table_name ) , sample = sample , unique_fields = unique_fields )
3284	def handle_error ( self , request , client_address ) : ei = sys . exc_info ( ) e = ei [ 1 ] # Suppress stack trace when client aborts connection disgracefully: # 10053: Software caused connection abort # 10054: Connection reset by peer if e . args [ 0 ] in ( 10053 , 10054 ) : _logger . error ( "*** Caught socket.error: {}" . format ( e ) ) return # This is what BaseHTTPServer.HTTPServer.handle_error does, but with # added thread ID and using stderr _logger . error ( "-" * 40 , file = sys . stderr ) _logger . error ( "<{}> Exception happened during processing of request from {}" . format ( threading . currentThread ( ) . ident , client_address ) ) _logger . error ( client_address , file = sys . stderr ) traceback . print_exc ( ) _logger . error ( "-" * 40 , file = sys . stderr ) _logger . error ( request , file = sys . stderr )
3777	def calculate_integral ( self , T1 , T2 , method ) : return float ( quad ( self . calculate , T1 , T2 , args = ( method ) ) [ 0 ] )
1877	def MOVSD ( cpu , dest , src ) : assert dest . type != 'memory' or src . type != 'memory' value = Operators . EXTRACT ( src . read ( ) , 0 , 64 ) if dest . size > src . size : value = Operators . ZEXTEND ( value , dest . size ) dest . write ( value )
760	def appendInputWithNSimilarValues ( inputs , numNear = 10 ) : numInputs = len ( inputs ) skipOne = False for i in xrange ( numInputs ) : input = inputs [ i ] numChanged = 0 newInput = copy . deepcopy ( input ) for j in xrange ( len ( input ) - 1 ) : if skipOne : skipOne = False continue if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) newInput = copy . deepcopy ( newInput ) #print input #print newInput numChanged += 1 skipOne = True if numChanged == numNear : break
8616	def _b ( s , encoding = 'utf-8' ) : if six . PY2 : # This is Python2 if isinstance ( s , str ) : return s elif isinstance ( s , unicode ) : # noqa, pylint: disable=undefined-variable return s . encode ( encoding ) else : # And this is Python3 if isinstance ( s , bytes ) : return s elif isinstance ( s , str ) : return s . encode ( encoding ) raise TypeError ( "Invalid argument %r for _b()" % ( s , ) )
10664	def molar_mass ( compound = '' ) : result = 0.0 if compound is None or len ( compound ) == 0 : return result compound = compound . strip ( ) parsed = parse_compound ( compound ) return parsed . molar_mass ( )
11459	def strip_fields ( self ) : for tag in self . record . keys ( ) : if tag in self . fields_list : record_delete_fields ( self . record , tag )
6322	def resolve_loader ( self , meta : ProgramDescription ) : if not meta . loader : meta . loader = 'single' if meta . path else 'separate' for loader_cls in self . _loaders : if loader_cls . name == meta . loader : meta . loader_cls = loader_cls break else : raise ImproperlyConfigured ( ( "Program {} has no loader class registered." "Check PROGRAM_LOADERS or PROGRAM_DIRS" ) . format ( meta . path ) )
5734	def _get_result_msg_and_payload ( result , stream ) : groups = _GDB_MI_RESULT_RE . match ( result ) . groups ( ) token = int ( groups [ 0 ] ) if groups [ 0 ] != "" else None message = groups [ 1 ] if groups [ 2 ] is None : payload = None else : stream . advance_past_chars ( [ "," ] ) payload = _parse_dict ( stream ) return token , message , payload
1930	def add ( self , name : str , default = None , description : str = None ) : if name in self . _vars : raise ConfigError ( f"{self.name}.{name} already defined." ) if name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default ) self . _vars [ name ] = v
3544	def compute_exit_code ( config , exception = None ) : code = 0 if exception is not None : code = code | 1 if config . surviving_mutants > 0 : code = code | 2 if config . surviving_mutants_timeout > 0 : code = code | 4 if config . suspicious_mutants > 0 : code = code | 8 return code
2268	def to_dict ( self ) : return self . _base ( ( key , ( value . to_dict ( ) if isinstance ( value , AutoDict ) else value ) ) for key , value in self . items ( ) )
10444	def getobjectproperty ( self , window_name , object_name , prop ) : try : obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) except atomac . _a11y . ErrorInvalidUIElement : # During the test, when the window closed and reopened # ErrorInvalidUIElement exception will be thrown self . _windows = { } # Call the method again, after updating apps obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) if obj_info and prop != "obj" and prop in obj_info : if prop == "class" : # ldtp_class_type are compatible with Linux and Windows class name # If defined class name exist return that, # else return as it is return ldtp_class_type . get ( obj_info [ prop ] , obj_info [ prop ] ) else : return obj_info [ prop ] raise LdtpServerException ( 'Unknown property "%s" in %s' % ( prop , object_name ) )
917	def info ( self , msg , * args , * * kwargs ) : self . _baseLogger . info ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )
4984	def extend_course ( course , enterprise_customer , request ) : course_run_id = course [ 'course_runs' ] [ 0 ] [ 'key' ] try : catalog_api_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) except ImproperlyConfigured : error_code = 'ENTPEV000' LOGGER . error ( 'CourseCatalogApiServiceClient is improperly configured. ' 'Returned error code {error_code} to user {userid} ' 'and enterprise_customer {enterprise_customer} ' 'for course_run_id {course_run_id}' . format ( error_code = error_code , userid = request . user . id , enterprise_customer = enterprise_customer . uuid , course_run_id = course_run_id , ) ) messages . add_generic_error_message_with_code ( request , error_code ) return ( { } , error_code ) course_details , course_run_details = catalog_api_client . get_course_and_course_run ( course_run_id ) if not course_details or not course_run_details : error_code = 'ENTPEV001' LOGGER . error ( 'User {userid} of enterprise customer {enterprise_customer} encountered an error.' 'No course_details or course_run_details found for ' 'course_run_id {course_run_id}. ' 'The following error code reported to the user: {error_code}' . format ( userid = request . user . id , enterprise_customer = enterprise_customer . uuid , course_run_id = course_run_id , error_code = error_code , ) ) messages . add_generic_error_message_with_code ( request , error_code ) return ( { } , error_code ) weeks_to_complete = course_run_details [ 'weeks_to_complete' ] course_run_image = course_run_details [ 'image' ] or { } course . update ( { 'course_image_uri' : course_run_image . get ( 'src' , '' ) , 'course_title' : course_run_details [ 'title' ] , 'course_level_type' : course_run_details . get ( 'level_type' , '' ) , 'course_short_description' : course_run_details [ 'short_description' ] or '' , 'course_full_description' : clean_html_for_template_rendering ( course_run_details [ 'full_description' ] or '' ) , 'expected_learning_items' : course_details . get ( 'expected_learning_items' , [ ] ) , 'staff' : course_run_details . get ( 'staff' , [ ] ) , 'course_effort' : ungettext_min_max ( '{} hour per week' , '{} hours per week' , '{}-{} hours per week' , course_run_details [ 'min_effort' ] or None , course_run_details [ 'max_effort' ] or None , ) or '' , 'weeks_to_complete' : ungettext ( '{} week' , '{} weeks' , weeks_to_complete ) . format ( weeks_to_complete ) if weeks_to_complete else '' , } ) return course , None
960	def validateOpfJsonValue ( value , opfJsonSchemaFilename ) : # Create a path by joining the filename with our local json schema root jsonSchemaPath = os . path . join ( os . path . dirname ( __file__ ) , "jsonschema" , opfJsonSchemaFilename ) # Validate jsonhelpers . validate ( value , schemaPath = jsonSchemaPath ) return
10011	def get ( vals , key , default_val = None ) : val = vals for part in key . split ( '.' ) : if isinstance ( val , dict ) : val = val . get ( part , None ) if val is None : return default_val else : return default_val return val
8532	def can_diff ( msg_a , msg_b ) : if msg_a . method != msg_b . method : return False , 'method name of messages do not match' if len ( msg_a . args ) != len ( msg_b . args ) or not msg_a . args . is_isomorphic_to ( msg_b . args ) : return False , 'argument signature of methods do not match' return True , None
5057	def build_notification_message ( template_context , template_configuration = None ) : if ( template_configuration is not None and template_configuration . html_template and template_configuration . plaintext_template ) : plain_msg , html_msg = template_configuration . render_all_templates ( template_context ) else : plain_msg = render_to_string ( 'enterprise/emails/user_notification.txt' , template_context ) html_msg = render_to_string ( 'enterprise/emails/user_notification.html' , template_context ) return plain_msg , html_msg
6576	def populate_fields ( api_client , instance , data ) : for key , value in instance . __class__ . _fields . items ( ) : default = getattr ( value , "default" , None ) newval = data . get ( value . field , default ) if isinstance ( value , SyntheticField ) : newval = value . formatter ( api_client , data , newval ) setattr ( instance , key , newval ) continue model_class = getattr ( value , "model" , None ) if newval and model_class : if isinstance ( newval , list ) : newval = model_class . from_json_list ( api_client , newval ) else : newval = model_class . from_json ( api_client , newval ) if newval and value . formatter : newval = value . formatter ( api_client , newval ) setattr ( instance , key , newval )
7471	def build_tmp_h5 ( data , samples ) : ## get samples and names, sorted snames = [ i . name for i in samples ] snames . sort ( ) ## Build an array for quickly indexing consens reads from catg files. ## save as a npy int binary file. uhandle = os . path . join ( data . dirs . across , data . name + ".utemp.sort" ) bseeds = os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) ## send as first async1 job get_seeds_and_hits ( uhandle , bseeds , snames )
9800	def bookmark ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) try : PolyaxonClient ( ) . experiment_group . bookmark ( user , project_name , _group ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not bookmark group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiments group is bookmarked." )
1710	def Js ( val , Clamped = False ) : if isinstance ( val , PyJs ) : return val elif val is None : return undefined elif isinstance ( val , basestring ) : return PyJsString ( val , StringPrototype ) elif isinstance ( val , bool ) : return true if val else false elif isinstance ( val , float ) or isinstance ( val , int ) or isinstance ( val , long ) or ( NUMPY_AVAILABLE and isinstance ( val , ( numpy . int8 , numpy . uint8 , numpy . int16 , numpy . uint16 , numpy . int32 , numpy . uint32 , numpy . float32 , numpy . float64 ) ) ) : # This is supposed to speed things up. may not be the case if val in NUM_BANK : return NUM_BANK [ val ] return PyJsNumber ( float ( val ) , NumberPrototype ) elif isinstance ( val , FunctionType ) : return PyJsFunction ( val , FunctionPrototype ) #elif isinstance(val, ModuleType): # mod = {} # for name in dir(val): # value = getattr(val, name) # if isinstance(value, ModuleType): # continue # prevent recursive module conversion # try: # jsval = HJs(value) # except RuntimeError: # print 'Could not convert %s to PyJs object!' % name # continue # mod[name] = jsval # return Js(mod) #elif isintance(val, ClassType): elif isinstance ( val , dict ) : # convert to object temp = PyJsObject ( { } , ObjectPrototype ) for k , v in six . iteritems ( val ) : temp . put ( Js ( k ) , Js ( v ) ) return temp elif isinstance ( val , ( list , tuple ) ) : #Convert to array return PyJsArray ( val , ArrayPrototype ) # convert to typedarray elif isinstance ( val , JsObjectWrapper ) : return val . __dict__ [ '_obj' ] elif NUMPY_AVAILABLE and isinstance ( val , numpy . ndarray ) : if val . dtype == numpy . int8 : return PyJsInt8Array ( val , Int8ArrayPrototype ) elif val . dtype == numpy . uint8 and not Clamped : return PyJsUint8Array ( val , Uint8ArrayPrototype ) elif val . dtype == numpy . uint8 and Clamped : return PyJsUint8ClampedArray ( val , Uint8ClampedArrayPrototype ) elif val . dtype == numpy . int16 : return PyJsInt16Array ( val , Int16ArrayPrototype ) elif val . dtype == numpy . uint16 : return PyJsUint16Array ( val , Uint16ArrayPrototype ) elif val . dtype == numpy . int32 : return PyJsInt32Array ( val , Int32ArrayPrototype ) elif val . dtype == numpy . uint32 : return PyJsUint16Array ( val , Uint32ArrayPrototype ) elif val . dtype == numpy . float32 : return PyJsFloat32Array ( val , Float32ArrayPrototype ) elif val . dtype == numpy . float64 : return PyJsFloat64Array ( val , Float64ArrayPrototype ) else : # try to convert to js object return py_wrap ( val )
12100	def _append_log ( self , specs ) : self . _spec_log += specs # This should be removed log_path = os . path . join ( self . root_directory , ( "%s.log" % self . batch_name ) ) core . Log . write_log ( log_path , [ spec for ( _ , spec ) in specs ] , allow_append = True )
8213	def export_svg ( self ) : d = "" if len ( self . _points ) > 0 : d += "M " + str ( self . _points [ 0 ] . x ) + " " + str ( self . _points [ 0 ] . y ) + " " for pt in self . _points : if pt . cmd == MOVETO : d += "M " + str ( pt . x ) + " " + str ( pt . y ) + " " elif pt . cmd == LINETO : d += "L " + str ( pt . x ) + " " + str ( pt . y ) + " " elif pt . cmd == CURVETO : d += "C " d += str ( pt . ctrl1 . x ) + " " + str ( pt . ctrl1 . y ) + " " d += str ( pt . ctrl2 . x ) + " " + str ( pt . ctrl2 . y ) + " " d += str ( pt . x ) + " " + str ( pt . y ) + " " c = "rgb(" c += str ( int ( self . path_color . r * 255 ) ) + "," c += str ( int ( self . path_color . g * 255 ) ) + "," c += str ( int ( self . path_color . b * 255 ) ) + ")" s = '<?xml version="1.0"?>\n' s += '<svg width="' + str ( _ctx . WIDTH ) + 'pt" height="' + str ( _ctx . HEIGHT ) + 'pt">\n' s += '<g>\n' s += '<path d="' + d + '" fill="none" stroke="' + c + '" stroke-width="' + str ( self . strokewidth ) + '" />\n' s += '</g>\n' s += '</svg>\n' f = open ( self . file + ".svg" , "w" ) f . write ( s ) f . close ( )
3324	def lock_string ( lock_dict ) : if not lock_dict : return "Lock: None" if lock_dict [ "expire" ] < 0 : expire = "Infinite ({})" . format ( lock_dict [ "expire" ] ) else : expire = "{} (in {} seconds)" . format ( util . get_log_time ( lock_dict [ "expire" ] ) , lock_dict [ "expire" ] - time . time ( ) ) return "Lock(<{}..>, '{}', {}, {}, depth-{}, until {}" . format ( # first 4 significant token characters lock_dict . get ( "token" , "?" * 30 ) [ 18 : 22 ] , lock_dict . get ( "root" ) , lock_dict . get ( "principal" ) , lock_dict . get ( "scope" ) , lock_dict . get ( "depth" ) , expire , )
11378	def get_publication_date ( self , xml_doc ) : start_date = get_value_in_tag ( xml_doc , "prism:coverDate" ) if not start_date : start_date = get_value_in_tag ( xml_doc , "prism:coverDisplayDate" ) if not start_date : start_date = get_value_in_tag ( xml_doc , 'oa:openAccessEffective' ) if start_date : start_date = datetime . datetime . strptime ( start_date , "%Y-%m-%dT%H:%M:%SZ" ) return start_date . strftime ( "%Y-%m-%d" ) import dateutil . parser #dateutil.parser.parse cant process dates like April-June 2016 start_date = re . sub ( '([A-Z][a-z]+)[\s\-][A-Z][a-z]+ (\d{4})' , r'\1 \2' , start_date ) try : date = dateutil . parser . parse ( start_date ) except ValueError : return '' # Special case where we ignore the deduced day form dateutil # in case it was not given in the first place. if len ( start_date . split ( " " ) ) == 3 : return date . strftime ( "%Y-%m-%d" ) else : return date . strftime ( "%Y-%m" ) else : if len ( start_date ) is 8 : start_date = time . strftime ( '%Y-%m-%d' , time . strptime ( start_date , '%Y%m%d' ) ) elif len ( start_date ) is 6 : start_date = time . strftime ( '%Y-%m' , time . strptime ( start_date , '%Y%m' ) ) return start_date
116	def map_batches_async ( self , batches , chunksize = None , callback = None , error_callback = None ) : assert isinstance ( batches , list ) , ( "Expected to get a list as 'batches', got type %s. " + "Call imap_batches() if you use generators." ) % ( type ( batches ) , ) return self . pool . map_async ( _Pool_starworker , self . _handle_batch_ids ( batches ) , chunksize = chunksize , callback = callback , error_callback = error_callback )
12069	def save ( abf , fname = None , tag = None , width = 700 , close = True , facecolor = 'w' , resize = True ) : if len ( pylab . gca ( ) . get_lines ( ) ) == 0 : print ( "can't save, no figure!" ) return if resize : pylab . tight_layout ( ) pylab . subplots_adjust ( bottom = .1 ) annotate ( abf ) if tag : fname = abf . outpath + abf . ID + "_" + tag + ".png" inchesX , inchesY = pylab . gcf ( ) . get_size_inches ( ) dpi = width / inchesX if fname : if not os . path . exists ( abf . outpath ) : os . mkdir ( abf . outpath ) print ( " <- saving [%s] at %d DPI (%dx%d)" % ( os . path . basename ( fname ) , dpi , inchesX * dpi , inchesY * dpi ) ) pylab . savefig ( fname , dpi = dpi , facecolor = facecolor ) else : pylab . show ( ) if close : pylab . close ( )
12052	def getNotesForABF ( abfFile ) : parent = getParent ( abfFile ) parent = os . path . basename ( parent ) . replace ( ".abf" , "" ) expFile = os . path . dirname ( abfFile ) + "/experiment.txt" if not os . path . exists ( expFile ) : return "no experiment file" with open ( expFile ) as f : raw = f . readlines ( ) for line in raw : if line [ 0 ] == '~' : line = line [ 1 : ] . strip ( ) if line . startswith ( parent ) : while "\t\t" in line : line = line . replace ( "\t\t" , "\t" ) line = line . replace ( "\t" , "\n" ) return line return "experiment.txt found, but didn't contain %s" % parent
11460	def add_systemnumber ( self , source , recid = None ) : if not recid : recid = self . get_recid ( ) if not self . hidden and recid : record_add_field ( self . record , tag = '035' , subfields = [ ( '9' , source ) , ( 'a' , recid ) ] )
5388	def _task_sort_function ( task ) : return ( task . get_field ( 'create-time' ) , int ( task . get_field ( 'task-id' , 0 ) ) , int ( task . get_field ( 'task-attempt' , 0 ) ) )
10943	def calc_J ( self ) : r0 = self . state . residuals . copy ( ) . ravel ( ) dl = np . zeros ( self . param_vals . size ) p0 = self . param_vals . copy ( ) J = [ ] for a in range ( self . param_vals . size ) : dl *= 0 dl [ a ] += self . dl self . update_function ( p0 + dl ) r1 = self . state . residuals . copy ( ) . ravel ( ) J . append ( ( r1 - r0 ) / self . dl ) self . update_function ( p0 ) return np . array ( J )
10722	def _wrapper ( func ) : @ functools . wraps ( func ) def the_func ( expr ) : """ The actual function. :param object expr: the expression to be xformed to dbus-python types """ try : return func ( expr ) except ( TypeError , ValueError ) as err : raise IntoDPValueError ( expr , "expr" , "could not be transformed" ) from err return the_func
4886	def update_throttle_scope ( self ) : self . scope = SERVICE_USER_SCOPE self . rate = self . get_rate ( ) self . num_requests , self . duration = self . parse_rate ( self . rate )
12667	def vector_to_volume ( arr , mask , order = 'C' ) : if mask . dtype != np . bool : raise ValueError ( "mask must be a boolean array" ) if arr . ndim != 1 : raise ValueError ( "vector must be a 1-dimensional array" ) if arr . ndim == 2 and any ( v == 1 for v in arr . shape ) : log . debug ( 'Got an array of shape {}, flattening for my purposes.' . format ( arr . shape ) ) arr = arr . flatten ( ) volume = np . zeros ( mask . shape [ : 3 ] , dtype = arr . dtype , order = order ) volume [ mask ] = arr return volume
3786	def TP_dependent_property_derivative_T ( self , T , P , order = 1 ) : sorted_valid_methods_P = self . select_valid_methods_P ( T , P ) for method in sorted_valid_methods_P : try : return self . calculate_derivative_T ( T , P , method , order ) except : pass return None
8675	def load_keys ( key_file , origin_passphrase , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) click . echo ( 'Importing all keys from {0}...' . format ( key_file ) ) stash . load ( origin_passphrase , key_file = key_file ) click . echo ( 'Import complete!' )
752	def setResultsPerChoice ( self , resultsPerChoice ) : # Keep track of the results obtained for each choice. self . _resultsPerChoice = [ [ ] ] * len ( self . choices ) for ( choiceValue , values ) in resultsPerChoice : choiceIndex = self . choices . index ( choiceValue ) self . _resultsPerChoice [ choiceIndex ] = list ( values )
529	def Array ( dtype , size = None , ref = False ) : def getArrayType ( self ) : """A little function to replace the getType() method of arrays It returns a string representation of the array element type instead of the integer value (NTA_BasicType enum) returned by the origianl array """ return self . _dtype # ArrayRef can't be allocated if ref : assert size is None index = basicTypes . index ( dtype ) if index == - 1 : raise Exception ( 'Invalid data type: ' + dtype ) if size and size <= 0 : raise Exception ( 'Array size must be positive' ) suffix = 'ArrayRef' if ref else 'Array' arrayFactory = getattr ( engine_internal , dtype + suffix ) arrayFactory . getType = getArrayType if size : a = arrayFactory ( size ) else : a = arrayFactory ( ) a . _dtype = basicTypes [ index ] return a
780	def jobInsert ( self , client , cmdLine , clientInfo = '' , clientKey = '' , params = '' , alreadyRunning = False , minimumWorkers = 0 , maximumWorkers = 0 , jobType = '' , priority = DEFAULT_JOB_PRIORITY ) : jobHash = self . _normalizeHash ( uuid . uuid1 ( ) . bytes ) @ g_retrySQL def insertWithRetries ( ) : with ConnectionFactory . get ( ) as conn : return self . _insertOrGetUniqueJobNoRetries ( conn , client = client , cmdLine = cmdLine , jobHash = jobHash , clientInfo = clientInfo , clientKey = clientKey , params = params , minimumWorkers = minimumWorkers , maximumWorkers = maximumWorkers , jobType = jobType , priority = priority , alreadyRunning = alreadyRunning ) try : jobID = insertWithRetries ( ) except : self . _logger . exception ( 'jobInsert FAILED: jobType=%r; client=%r; clientInfo=%r; clientKey=%r;' 'jobHash=%r; cmdLine=%r' , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) raise else : self . _logger . info ( 'jobInsert: returning jobID=%s. jobType=%r; client=%r; clientInfo=%r; ' 'clientKey=%r; jobHash=%r; cmdLine=%r' , jobID , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) return jobID
2513	def get_file_name ( self , f_term ) : for _ , _ , name in self . graph . triples ( ( f_term , self . spdx_namespace [ 'fileName' ] , None ) ) : return name return
560	def anyGoodSprintsActive ( self ) : if self . _state [ 'lastGoodSprint' ] is not None : goodSprints = self . _state [ 'sprints' ] [ 0 : self . _state [ 'lastGoodSprint' ] + 1 ] else : goodSprints = self . _state [ 'sprints' ] for sprint in goodSprints : if sprint [ 'status' ] == 'active' : anyActiveSprints = True break else : anyActiveSprints = False return anyActiveSprints
8040	def is_public ( self ) : # Check if we are a setter/deleter method, and mark as private if so. for decorator in self . decorators : # Given 'foo', match 'foo.bar' but not 'foobar' or 'sfoo' if re . compile ( r"^{}\." . format ( self . name ) ) . match ( decorator . name ) : return False name_is_public = ( not self . name . startswith ( "_" ) or self . name in VARIADIC_MAGIC_METHODS or self . is_magic ) return self . parent . is_public and name_is_public
8228	def show ( self , format = 'png' , as_data = False ) : from io import BytesIO b = BytesIO ( ) if format == 'png' : from IPython . display import Image surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , self . WIDTH , self . HEIGHT ) self . snapshot ( surface ) surface . write_to_png ( b ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return Image ( data ) elif format == 'svg' : from IPython . display import SVG surface = cairo . SVGSurface ( b , self . WIDTH , self . HEIGHT ) surface . finish ( ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return SVG ( data )
12029	def setsweeps ( self ) : for sweep in range ( self . sweeps ) : self . setsweep ( sweep ) yield self . sweep
5666	def _run ( self ) : if self . _has_run : raise RuntimeError ( "This spreader instance has already been run: " "create a new Spreader object for a new run." ) i = 1 while self . event_heap . size ( ) > 0 and len ( self . _uninfected_stops ) > 0 : event = self . event_heap . pop_next_event ( ) this_stop = self . _stop_I_to_spreading_stop [ event . from_stop_I ] if event . arr_time_ut > self . start_time_ut + self . max_duration_ut : break if this_stop . can_infect ( event ) : target_stop = self . _stop_I_to_spreading_stop [ event . to_stop_I ] already_visited = target_stop . has_been_visited ( ) target_stop . visit ( event ) if not already_visited : self . _uninfected_stops . remove ( event . to_stop_I ) print ( i , self . event_heap . size ( ) ) transfer_distances = self . gtfs . get_straight_line_transfer_distances ( event . to_stop_I ) self . event_heap . add_walk_events_to_heap ( transfer_distances , event , self . start_time_ut , self . walk_speed , self . _uninfected_stops , self . max_duration_ut ) i += 1 self . _has_run = True
9856	def get_data ( self , * * kwargs ) : limit = int ( kwargs . get ( 'limit' , 288 ) ) end_date = kwargs . get ( 'end_date' , False ) if end_date and isinstance ( end_date , datetime . datetime ) : end_date = self . convert_datetime ( end_date ) if self . mac_address is not None : service_address = 'devices/%s' % self . mac_address self . api_instance . log ( 'SERVICE ADDRESS: %s' % service_address ) data = dict ( limit = limit ) # If endDate is left blank (not passed in), the most recent results will be returned. if end_date : data . update ( { 'endDate' : end_date } ) self . api_instance . log ( 'DATA:' ) self . api_instance . log ( data ) return self . api_instance . api_call ( service_address , * * data )
10967	def sync_params ( self ) : def _normalize ( comps , param ) : vals = [ c . get_values ( param ) for c in comps ] diff = any ( [ vals [ i ] != vals [ i + 1 ] for i in range ( len ( vals ) - 1 ) ] ) if diff : for c in comps : c . set_values ( param , vals [ 0 ] ) for param , comps in iteritems ( self . lmap ) : if isinstance ( comps , list ) and len ( comps ) > 1 : _normalize ( comps , param )
9798	def update ( ctx , name , description , tags ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the experiment group.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment_group . update_experiment_group ( user , project_name , _group , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment group updated." ) get_group_details ( response )
10596	def h_x ( self , L , theta , Ts , * * statef ) : Nu_x = self . Nu_x ( L , theta , Ts , * * statef ) k = self . _fluid . k ( T = self . Tr ) return Nu_x * k / L
8014	async def send_upstream ( self , message , stream_name = None ) : if stream_name is None : for steam_queue in self . application_streams . values ( ) : await steam_queue . put ( message ) return steam_queue = self . application_streams . get ( stream_name ) if steam_queue is None : raise ValueError ( "Invalid multiplexed frame received (stream not mapped)" ) await steam_queue . put ( message )
9044	def listen ( self , func ) : self . _C0 . listen ( func ) self . _C1 . listen ( func )
671	def createNetwork ( dataSource ) : with open ( _PARAMS_PATH , "r" ) as f : modelParams = yaml . safe_load ( f ) [ "modelParams" ] # Create a network that will hold the regions. network = Network ( ) # Add a sensor region. network . addRegion ( "sensor" , "py.RecordSensor" , '{}' ) # Set the encoder and data source of the sensor region. sensorRegion = network . regions [ "sensor" ] . getSelf ( ) sensorRegion . encoder = createEncoder ( modelParams [ "sensorParams" ] [ "encoders" ] ) sensorRegion . dataSource = dataSource # Make sure the SP input width matches the sensor region output width. modelParams [ "spParams" ] [ "inputWidth" ] = sensorRegion . encoder . getWidth ( ) # Add SP and TM regions. network . addRegion ( "SP" , "py.SPRegion" , json . dumps ( modelParams [ "spParams" ] ) ) network . addRegion ( "TM" , "py.TMRegion" , json . dumps ( modelParams [ "tmParams" ] ) ) # Add a classifier region. clName = "py.%s" % modelParams [ "clParams" ] . pop ( "regionName" ) network . addRegion ( "classifier" , clName , json . dumps ( modelParams [ "clParams" ] ) ) # Add all links createSensorToClassifierLinks ( network , "sensor" , "classifier" ) createDataOutLink ( network , "sensor" , "SP" ) createFeedForwardLink ( network , "SP" , "TM" ) createFeedForwardLink ( network , "TM" , "classifier" ) # Reset links are optional, since the sensor region does not send resets. createResetLink ( network , "sensor" , "SP" ) createResetLink ( network , "sensor" , "TM" ) # Make sure all objects are initialized. network . initialize ( ) return network
8505	def _default_value_only ( self ) : line = self . source [ self . col_offset : ] regex = re . compile ( '''pyconfig\.[eginst]+\(['"][^)]+?['"], ?(.*?)\)''' ) match = regex . match ( line ) if not match : return '' return match . group ( 1 )
1166	def join ( self , timeout = None ) : if not self . __initialized : raise RuntimeError ( "Thread.__init__() not called" ) if not self . __started . is_set ( ) : raise RuntimeError ( "cannot join thread before it is started" ) if self is current_thread ( ) : raise RuntimeError ( "cannot join current thread" ) if __debug__ : if not self . __stopped : self . _note ( "%s.join(): waiting until thread stops" , self ) self . __block . acquire ( ) try : if timeout is None : while not self . __stopped : self . __block . wait ( ) if __debug__ : self . _note ( "%s.join(): thread stopped" , self ) else : deadline = _time ( ) + timeout while not self . __stopped : delay = deadline - _time ( ) if delay <= 0 : if __debug__ : self . _note ( "%s.join(): timed out" , self ) break self . __block . wait ( delay ) else : if __debug__ : self . _note ( "%s.join(): thread stopped" , self ) finally : self . __block . release ( )
378	def samplewise_norm ( x , rescale = None , samplewise_center = False , samplewise_std_normalization = False , channel_index = 2 , epsilon = 1e-7 ) : if rescale : x *= rescale if x . shape [ channel_index ] == 1 : # greyscale if samplewise_center : x = x - np . mean ( x ) if samplewise_std_normalization : x = x / np . std ( x ) return x elif x . shape [ channel_index ] == 3 : # rgb if samplewise_center : x = x - np . mean ( x , axis = channel_index , keepdims = True ) if samplewise_std_normalization : x = x / ( np . std ( x , axis = channel_index , keepdims = True ) + epsilon ) return x else : raise Exception ( "Unsupported channels %d" % x . shape [ channel_index ] )
7104	def transform ( self , transformer ) : self . transformers . append ( transformer ) from languageflow . transformer . tagged import TaggedTransformer if isinstance ( transformer , TaggedTransformer ) : self . X , self . y = transformer . transform ( self . sentences ) if isinstance ( transformer , TfidfVectorizer ) : self . X = transformer . fit_transform ( self . X ) if isinstance ( transformer , CountVectorizer ) : self . X = transformer . fit_transform ( self . X ) if isinstance ( transformer , NumberRemover ) : self . X = transformer . transform ( self . X ) if isinstance ( transformer , MultiLabelBinarizer ) : self . y = transformer . fit_transform ( self . y )
4300	def _install_aldryn ( config_data ) : # pragma: no cover import requests media_project = os . path . join ( config_data . project_directory , 'dist' , 'media' ) static_main = False static_project = os . path . join ( config_data . project_directory , 'dist' , 'static' ) template_target = os . path . join ( config_data . project_directory , 'templates' ) tmpdir = tempfile . mkdtemp ( ) aldrynzip = requests . get ( data . ALDRYN_BOILERPLATE ) zip_open = zipfile . ZipFile ( BytesIO ( aldrynzip . content ) ) zip_open . extractall ( path = tmpdir ) for component in os . listdir ( os . path . join ( tmpdir , 'aldryn-boilerplate-standard-master' ) ) : src = os . path . join ( tmpdir , 'aldryn-boilerplate-standard-master' , component ) dst = os . path . join ( config_data . project_directory , component ) if os . path . isfile ( src ) : shutil . copy ( src , dst ) else : shutil . copytree ( src , dst ) shutil . rmtree ( tmpdir ) return media_project , static_main , static_project , template_target
13560	def decorate ( msg = "" , waitmsg = "Please wait" ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( * args , * * kwargs ) : spin = Spinner ( msg = msg , waitmsg = waitmsg ) spin . start ( ) a = None try : a = func ( * args , * * kwargs ) except Exception as e : spin . msg = "Something went wrong: " spin . stop_spinning ( ) spin . join ( ) raise e spin . stop_spinning ( ) spin . join ( ) return a return wrapper return decorator
8880	def predict_proba ( self , X ) : # Check is fit had been called check_is_fitted ( self , [ 'tree' ] ) # Check data X = check_array ( X ) return self . tree . query ( X ) [ 0 ] . flatten ( )
2603	def client_file ( self ) : return os . path . join ( self . ipython_dir , 'profile_{0}' . format ( self . profile ) , 'security/ipcontroller-client.json' )
4794	def contains_value ( self , * values ) : self . _check_dict_like ( self . val , check_getitem = False ) if len ( values ) == 0 : raise ValueError ( 'one or more value args must be given' ) missing = [ ] for v in values : if v not in self . val . values ( ) : missing . append ( v ) if missing : self . _err ( 'Expected <%s> to contain values %s, but did not contain %s.' % ( self . val , self . _fmt_items ( values ) , self . _fmt_items ( missing ) ) ) return self
12677	def escape ( to_escape , safe = SAFE , escape_char = ESCAPE_CHAR , allow_collisions = False ) : if isinstance ( to_escape , bytes ) : # always work on text to_escape = to_escape . decode ( 'utf8' ) if not isinstance ( safe , set ) : safe = set ( safe ) if allow_collisions : safe . add ( escape_char ) elif escape_char in safe : # escape char can't be in safe list safe . remove ( escape_char ) chars = [ ] for c in to_escape : if c in safe : chars . append ( c ) else : chars . append ( _escape_char ( c , escape_char ) ) return u'' . join ( chars )
10854	def sphere_triangle_cdf ( dr , a , alpha ) : p0 = ( dr + alpha ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 > dr ) * ( dr > - alpha ) p1 = 1 * ( dr > 0 ) - ( alpha - dr ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 < dr ) * ( dr < alpha ) return ( 1 - np . clip ( p0 + p1 , 0 , 1 ) )
1674	def ParseArguments ( args ) : try : ( opts , filenames ) = getopt . getopt ( args , '' , [ 'help' , 'output=' , 'verbose=' , 'counting=' , 'filter=' , 'root=' , 'repository=' , 'linelength=' , 'extensions=' , 'exclude=' , 'headers=' , 'quiet' , 'recursive' ] ) except getopt . GetoptError : PrintUsage ( 'Invalid arguments.' ) verbosity = _VerboseLevel ( ) output_format = _OutputFormat ( ) filters = '' counting_style = '' recursive = False for ( opt , val ) in opts : if opt == '--help' : PrintUsage ( None ) elif opt == '--output' : if val not in ( 'emacs' , 'vs7' , 'eclipse' , 'junit' ) : PrintUsage ( 'The only allowed output formats are emacs, vs7, eclipse ' 'and junit.' ) output_format = val elif opt == '--verbose' : verbosity = int ( val ) elif opt == '--filter' : filters = val if not filters : PrintCategories ( ) elif opt == '--counting' : if val not in ( 'total' , 'toplevel' , 'detailed' ) : PrintUsage ( 'Valid counting options are total, toplevel, and detailed' ) counting_style = val elif opt == '--root' : global _root _root = val elif opt == '--repository' : global _repository _repository = val elif opt == '--linelength' : global _line_length try : _line_length = int ( val ) except ValueError : PrintUsage ( 'Line length must be digits.' ) elif opt == '--exclude' : global _excludes if not _excludes : _excludes = set ( ) _excludes . update ( glob . glob ( val ) ) elif opt == '--extensions' : global _valid_extensions try : _valid_extensions = set ( val . split ( ',' ) ) except ValueError : PrintUsage ( 'Extensions must be comma seperated list.' ) elif opt == '--headers' : global _header_extensions try : _header_extensions = set ( val . split ( ',' ) ) except ValueError : PrintUsage ( 'Extensions must be comma seperated list.' ) elif opt == '--recursive' : recursive = True elif opt == '--quiet' : global _quiet _quiet = True if not filenames : PrintUsage ( 'No files were specified.' ) if recursive : filenames = _ExpandDirectories ( filenames ) if _excludes : filenames = _FilterExcludedFiles ( filenames ) _SetOutputFormat ( output_format ) _SetVerboseLevel ( verbosity ) _SetFilters ( filters ) _SetCountingStyle ( counting_style ) return filenames
11037	def maybe_key ( pem_path ) : acme_key_file = pem_path . child ( u'client.key' ) if acme_key_file . exists ( ) : key = _load_pem_private_key_bytes ( acme_key_file . getContent ( ) ) else : key = generate_private_key ( u'rsa' ) acme_key_file . setContent ( _dump_pem_private_key_bytes ( key ) ) return succeed ( JWKRSA ( key = key ) )
6956	def _transit_model ( times , t0 , per , rp , a , inc , ecc , w , u , limb_dark , exp_time_minutes = 2 , supersample_factor = 7 ) : params = batman . TransitParams ( ) # object to store transit parameters params . t0 = t0 # time of periastron params . per = per # orbital period params . rp = rp # planet radius (in stellar radii) params . a = a # semi-major axis (in stellar radii) params . inc = inc # orbital inclination (in degrees) params . ecc = ecc # the eccentricity of the orbit params . w = w # longitude of periastron (in degrees) params . u = u # limb darkening coefficient list params . limb_dark = limb_dark # limb darkening model to use t = times m = batman . TransitModel ( params , t , exp_time = exp_time_minutes / 60. / 24. , supersample_factor = supersample_factor ) return params , m
5988	def scaled_deflection_stack_from_plane_and_scaling_factor ( plane , scaling_factor ) : def scale ( grid ) : return np . multiply ( scaling_factor , grid ) if plane . deflection_stack is not None : return plane . deflection_stack . apply_function ( scale ) else : return None
11716	def edit ( self , config , etag ) : data = self . _json_encode ( config ) headers = self . _default_headers ( ) if etag is not None : headers [ "If-Match" ] = etag return self . _request ( self . name , ok_status = None , data = data , headers = headers , method = "PUT" )
5245	def current_missing ( * * kwargs ) -> int : data_path = os . environ . get ( BBG_ROOT , '' ) . replace ( '\\' , '/' ) if not data_path : return 0 return len ( files . all_files ( f'{data_path}/Logs/{missing_info(**kwargs)}' ) )
250	def adjust_returns_for_slippage ( returns , positions , transactions , slippage_bps ) : slippage = 0.0001 * slippage_bps portfolio_value = positions . sum ( axis = 1 ) pnl = portfolio_value * returns traded_value = get_txn_vol ( transactions ) . txn_volume slippage_dollars = traded_value * slippage adjusted_pnl = pnl . add ( - slippage_dollars , fill_value = 0 ) adjusted_returns = returns * adjusted_pnl / pnl return adjusted_returns
421	def delete_training_log ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) self . db . TrainLog . delete_many ( kwargs ) logging . info ( "[Database] Delete TrainLog SUCCESS" )
13473	def loop_in_background ( interval , callback ) : loop = GeventLoop ( interval , callback ) loop . start ( ) try : yield loop finally : if loop . has_started ( ) : loop . stop ( )
1852	def SAR ( cpu , dest , src ) : OperandSize = dest . size countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] count = src . read ( ) & countMask value = dest . read ( ) res = Operators . SAR ( OperandSize , value , Operators . ZEXTEND ( count , OperandSize ) ) dest . write ( res ) SIGN_MASK = ( 1 << ( OperandSize - 1 ) ) # We can't use this one as the 'true' expression gets eagerly calculated even on count == 0 + cpu.CF = Operators.ITE(count!=0, ((value >> Operators.ZEXTEND(count-1, OperandSize)) & 1) !=0, cpu.CF) # cpu.CF = Operators.ITE(count!=0, ((value >> Operators.ZEXTEND(count-1, OperandSize)) & 1) !=0, cpu.CF) if issymbolic ( count ) : # We can't use this one as the EXTRACT op needs the offset arguments to be concrete # cpu.CF = Operators.ITE(count!=0, Operands.EXTRACT(value,count-1,1) !=0, cpu.CF) cpu . CF = Operators . ITE ( Operators . AND ( count != 0 , count <= OperandSize ) , ( ( value >> Operators . ZEXTEND ( count - 1 , OperandSize ) ) & 1 ) != 0 , cpu . CF ) else : if count != 0 : if count > OperandSize : count = OperandSize cpu . CF = Operators . EXTRACT ( value , count - 1 , 1 ) != 0 # on count == 0 AF is unaffected, for count > 0, AF is undefined. # in either case, do not touch AF cpu . ZF = Operators . ITE ( count != 0 , res == 0 , cpu . ZF ) cpu . SF = Operators . ITE ( count != 0 , ( res & SIGN_MASK ) != 0 , cpu . SF ) cpu . OF = Operators . ITE ( count == 1 , False , cpu . OF ) cpu . PF = Operators . ITE ( count != 0 , cpu . _calculate_parity_flag ( res ) , cpu . PF )
7055	def register_lcformat ( formatkey , fileglob , timecols , magcols , errcols , readerfunc_module , readerfunc , readerfunc_kwargs = None , normfunc_module = None , normfunc = None , normfunc_kwargs = None , magsarefluxes = False , overwrite_existing = False , lcformat_dir = '~/.astrobase/lcformat-jsons' ) : LOGINFO ( 'adding %s to LC format registry...' % formatkey ) # search for the lcformat_dir and create it if it doesn't exist lcformat_dpath = os . path . abspath ( os . path . expanduser ( lcformat_dir ) ) if not os . path . exists ( lcformat_dpath ) : os . makedirs ( lcformat_dpath ) lcformat_jsonpath = os . path . join ( lcformat_dpath , '%s.json' % formatkey ) if os . path . exists ( lcformat_jsonpath ) and not overwrite_existing : LOGERROR ( 'There is an existing lcformat JSON: %s ' 'for this formatkey: %s and ' 'overwrite_existing = False, skipping...' % ( lcformat_jsonpath , formatkey ) ) return None # see if we can import the reader module readermodule = _check_extmodule ( readerfunc_module , formatkey ) if not readermodule : LOGERROR ( "could not import the required " "module: %s to read %s light curves" % ( readerfunc_module , formatkey ) ) return None # then, get the function we need to read the light curve try : getattr ( readermodule , readerfunc ) readerfunc_in = readerfunc except AttributeError : LOGEXCEPTION ( 'Could not get the specified reader ' 'function: %s for lcformat: %s ' 'from module: %s' % ( formatkey , readerfunc_module , readerfunc ) ) raise # see if we can import the normalization module if normfunc_module : normmodule = _check_extmodule ( normfunc_module , formatkey ) if not normmodule : LOGERROR ( "could not import the required " "module: %s to normalize %s light curves" % ( normfunc_module , formatkey ) ) return None else : normmodule = None # finally, get the function we need to normalize the light curve if normfunc_module and normfunc : try : getattr ( normmodule , normfunc ) normfunc_in = normfunc except AttributeError : LOGEXCEPTION ( 'Could not get the specified norm ' 'function: %s for lcformat: %s ' 'from module: %s' % ( normfunc , formatkey , normfunc_module ) ) raise else : normfunc_in = None # if we made it to here, then everything's good. generate the JSON # structure formatdict = { 'fileglob' : fileglob , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'magsarefluxes' : magsarefluxes , 'lcreader_module' : readerfunc_module , 'lcreader_func' : readerfunc_in , 'lcreader_kwargs' : readerfunc_kwargs , 'lcnorm_module' : normfunc_module , 'lcnorm_func' : normfunc_in , 'lcnorm_kwargs' : normfunc_kwargs } # write this to the lcformat directory with open ( lcformat_jsonpath , 'w' ) as outfd : json . dump ( formatdict , outfd , indent = 4 ) return lcformat_jsonpath
10670	def _split_compound_string_ ( compound_string ) : formula = compound_string . replace ( ']' , '' ) . split ( '[' ) [ 0 ] phase = compound_string . replace ( ']' , '' ) . split ( '[' ) [ 1 ] return formula , phase
13310	def fullStats ( a , b ) : stats = [ [ 'bias' , 'Bias' , bias ( a , b ) ] , [ 'stderr' , 'Standard Deviation Error' , stderr ( a , b ) ] , [ 'mae' , 'Mean Absolute Error' , mae ( a , b ) ] , [ 'rmse' , 'Root Mean Square Error' , rmse ( a , b ) ] , [ 'nmse' , 'Normalized Mean Square Error' , nmse ( a , b ) ] , [ 'mfbe' , 'Mean Fractionalized bias Error' , mfbe ( a , b ) ] , [ 'fa2' , 'Factor of Two' , fa ( a , b , 2 ) ] , [ 'foex' , 'Factor of Exceedance' , foex ( a , b ) ] , [ 'correlation' , 'Correlation R' , correlation ( a , b ) ] , [ 'determination' , 'Coefficient of Determination r2' , determination ( a , b ) ] , [ 'gmb' , 'Geometric Mean Bias' , gmb ( a , b ) ] , [ 'gmv' , 'Geometric Mean Variance' , gmv ( a , b ) ] , [ 'fmt' , 'Figure of Merit in Time' , fmt ( a , b ) ] ] rec = np . rec . fromrecords ( stats , names = ( 'stat' , 'description' , 'result' ) ) df = pd . DataFrame . from_records ( rec , index = 'stat' ) return df
12024	def adopt ( self , old_parent , new_parent ) : try : # assume line_data(dict) old_id = old_parent [ 'attributes' ] [ 'ID' ] except TypeError : try : # assume line_index(int) old_id = self . lines [ old_parent ] [ 'attributes' ] [ 'ID' ] except TypeError : # assume feature_id(str) old_id = old_parent old_feature = self . features [ old_id ] old_indexes = [ ld [ 'line_index' ] for ld in old_feature ] try : # assume line_data(dict) new_id = new_parent [ 'attributes' ] [ 'ID' ] except TypeError : try : # assume line_index(int) new_id = self . lines [ new_parent ] [ 'attributes' ] [ 'ID' ] except TypeError : # assume feature_id(str) new_id = new_parent new_feature = self . features [ new_id ] new_indexes = [ ld [ 'line_index' ] for ld in new_feature ] # build a list of children to be moved # add the child to the new parent's children list if its not already there # update the child's parent list and parent attribute # finally remove the old parent's children list children = old_feature [ 0 ] [ 'children' ] new_parent_children_set = set ( [ ld [ 'line_index' ] for ld in new_feature [ 0 ] [ 'children' ] ] ) for child in children : if child [ 'line_index' ] not in new_parent_children_set : new_parent_children_set . add ( child [ 'line_index' ] ) for new_ld in new_feature : new_ld [ 'children' ] . append ( child ) child [ 'parents' ] . append ( new_feature ) child [ 'attributes' ] [ 'Parent' ] . append ( new_id ) # remove multiple, list.remove() only removes 1 child [ 'parents' ] = [ f for f in child [ 'parents' ] if f [ 0 ] [ 'attributes' ] [ 'ID' ] != old_id ] child [ 'attributes' ] [ 'Parent' ] = [ d for d in child [ 'attributes' ] [ 'Parent' ] if d != old_id ] for old_ld in old_feature : old_ld [ 'children' ] = [ ] return children
11246	def future_value ( present_value , annual_rate , periods_per_year , years ) : # The nominal interest rate per period (rate) is how much interest you earn during a # particular length of time, before accounting for compounding. This is typically # expressed as a percentage. rate_per_period = annual_rate / float ( periods_per_year ) # How many periods in the future the calculation is for. periods = periods_per_year * years return present_value * ( 1 + rate_per_period ) ** periods
7472	def get_nloci ( data ) : bseeds = os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) with h5py . File ( bseeds ) as io5 : return io5 [ "seedsarr" ] . shape [ 0 ]
2466	def set_concluded_license ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_conc_lics_set : self . file_conc_lics_set = True if validations . validate_lics_conc ( lic ) : self . file ( doc ) . conc_lics = lic return True else : raise SPDXValueError ( 'File::ConcludedLicense' ) else : raise CardinalityError ( 'File::ConcludedLicense' ) else : raise OrderError ( 'File::ConcludedLicense' )
3884	def from_conv_part_data ( conv_part_data , self_user_id ) : user_id = UserID ( chat_id = conv_part_data . id . chat_id , gaia_id = conv_part_data . id . gaia_id ) return User ( user_id , conv_part_data . fallback_name , None , None , [ ] , ( self_user_id == user_id ) or ( self_user_id is None ) )
9682	def set_fan_power ( self , power ) : # Check to make sure the value is a single byte if power > 255 : raise ValueError ( "The fan power should be a single byte (0-255)." ) # Send the command byte and wait 10 ms a = self . cnxn . xfer ( [ 0x42 ] ) [ 0 ] sleep ( 10e-3 ) # Send the next two bytes b = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] c = self . cnxn . xfer ( [ power ] ) [ 0 ] sleep ( 0.1 ) return True if a == 0xF3 and b == 0x42 and c == 0x00 else False
13857	def sendmsg ( self , message , recipient_mobiles = [ ] , url = 'http://services.ambientmobile.co.za/sms' , concatenate_message = True , message_id = str ( time ( ) ) . replace ( "." , "" ) , reply_path = None , allow_duplicates = True , allow_invalid_numbers = True , ) : if not recipient_mobiles or not ( isinstance ( recipient_mobiles , list ) or isinstance ( recipient_mobiles , tuple ) ) : raise AmbientSMSError ( "Missing recipients" ) if not message or not len ( message ) : raise AmbientSMSError ( "Missing message" ) postXMLList = [ ] postXMLList . append ( "<api-key>%s</api-key>" % self . api_key ) postXMLList . append ( "<password>%s</password>" % self . password ) postXMLList . append ( "<recipients>%s</recipients>" % "" . join ( [ "<mobile>%s</mobile>" % m for m in recipient_mobiles ] ) ) postXMLList . append ( "<msg>%s</msg>" % message ) postXMLList . append ( "<concat>%s</concat>" % ( 1 if concatenate_message else 0 ) ) postXMLList . append ( "<message_id>%s</message_id>" % message_id ) postXMLList . append ( "<allow_duplicates>%s</allow_duplicates>" % ( 1 if allow_duplicates else 0 ) ) postXMLList . append ( "<allow_invalid_numbers>%s</allow_invalid_numbers>" % ( 1 if allow_invalid_numbers else 0 ) ) if reply_path : postXMLList . append ( "<reply_path>%s</reply_path>" % reply_path ) postXML = '<sms>%s</sms>' % "" . join ( postXMLList ) result = self . curl ( url , postXML ) status = result . get ( "status" , None ) if status and int ( status ) in [ 0 , 1 , 2 ] : return result else : raise AmbientSMSError ( int ( status ) )
11041	def get_single_header ( headers , key ) : raw_headers = headers . getRawHeaders ( key ) if raw_headers is None : return None # Take the final header as the authorative header , _ = cgi . parse_header ( raw_headers [ - 1 ] ) return header
9465	def conference_record_start ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStart/' method = 'POST' return self . request ( path , method , call_params )
3380	def add_lexicographic_constraints ( model , objectives , objective_direction = 'max' ) : if type ( objective_direction ) is not list : objective_direction = [ objective_direction ] * len ( objectives ) constraints = [ ] for rxn_id , obj_dir in zip ( objectives , objective_direction ) : model . objective = model . reactions . get_by_id ( rxn_id ) model . objective_direction = obj_dir constraints . append ( fix_objective_as_constraint ( model ) ) return pd . Series ( constraints , index = objectives )
2697	def build_graph ( json_iter ) : global DEBUG , WordNode graph = nx . DiGraph ( ) for meta in json_iter : if DEBUG : print ( meta [ "graf" ] ) for pair in get_tiles ( map ( WordNode . _make , meta [ "graf" ] ) ) : if DEBUG : print ( pair ) for word_id in pair : if not graph . has_node ( word_id ) : graph . add_node ( word_id ) try : graph . edge [ pair [ 0 ] ] [ pair [ 1 ] ] [ "weight" ] += 1.0 except KeyError : graph . add_edge ( pair [ 0 ] , pair [ 1 ] , weight = 1.0 ) return graph
1489	def save_file ( self , obj ) : # pylint: disable=too-many-branches try : import StringIO as pystringIO #we can't use cStringIO as it lacks the name attribute except ImportError : import io as pystringIO # pylint: disable=reimported if not hasattr ( obj , 'name' ) or not hasattr ( obj , 'mode' ) : raise pickle . PicklingError ( "Cannot pickle files that do not map to an actual file" ) if obj is sys . stdout : return self . save_reduce ( getattr , ( sys , 'stdout' ) , obj = obj ) if obj is sys . stderr : return self . save_reduce ( getattr , ( sys , 'stderr' ) , obj = obj ) if obj is sys . stdin : raise pickle . PicklingError ( "Cannot pickle standard input" ) if hasattr ( obj , 'isatty' ) and obj . isatty ( ) : raise pickle . PicklingError ( "Cannot pickle files that map to tty objects" ) if 'r' not in obj . mode : raise pickle . PicklingError ( "Cannot pickle files that are not opened for reading" ) name = obj . name try : fsize = os . stat ( name ) . st_size except OSError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be stat" % name ) if obj . closed : #create an empty closed string io retval = pystringIO . StringIO ( "" ) retval . close ( ) elif not fsize : #empty file retval = pystringIO . StringIO ( "" ) try : tmpfile = file ( name ) tst = tmpfile . read ( 1 ) except IOError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be read" % name ) tmpfile . close ( ) if tst != '' : raise pickle . PicklingError ( "Cannot pickle file %s as it does not appear to map to a physical, real file" % name ) else : try : tmpfile = file ( name ) contents = tmpfile . read ( ) tmpfile . close ( ) except IOError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be read" % name ) retval = pystringIO . StringIO ( contents ) curloc = obj . tell ( ) retval . seek ( curloc ) retval . name = name self . save ( retval ) self . memoize ( obj )
3944	def serialize ( self ) : segment = hangouts_pb2 . Segment ( type = self . type_ , text = self . text , formatting = hangouts_pb2 . Formatting ( bold = self . is_bold , italic = self . is_italic , strikethrough = self . is_strikethrough , underline = self . is_underline , ) , ) if self . link_target is not None : segment . link_data . link_target = self . link_target return segment
13050	def main ( ) : search = ServiceSearch ( ) services = search . get_services ( up = True , tags = [ '!header_scan' ] ) print_notification ( "Scanning {} services" . format ( len ( services ) ) ) # Disable the insecure request warning urllib3 . disable_warnings ( urllib3 . exceptions . InsecureRequestWarning ) pool = Pool ( 100 ) count = 0 for service in services : count += 1 if count % 50 == 0 : print_notification ( "Checking {}/{} services" . format ( count , len ( services ) ) ) pool . spawn ( check_service , service ) pool . join ( ) print_notification ( "Completed, 'http' tag added to services that respond to http, 'https' tag added to services that respond to https." )
9855	def _read_header ( self , ccp4file ) : bsaflag = self . _detect_byteorder ( ccp4file ) # Parse the top of the header (4-byte words, 1 to 25). nheader = struct . calcsize ( self . _headerfmt ) names = [ r . key for r in self . _header_struct ] bintopheader = ccp4file . read ( 25 * 4 ) def decode_header ( header , bsaflag = '@' ) : h = dict ( zip ( names , struct . unpack ( bsaflag + self . _headerfmt , header ) ) ) h [ 'bsaflag' ] = bsaflag return h header = decode_header ( bintopheader , bsaflag ) for rec in self . _header_struct : if not rec . is_legal_dict ( header ) : warnings . warn ( "Key %s: Illegal value %r" % ( rec . key , header [ rec . key ] ) ) # Parse the latter half of the header (4-byte words, 26 to 256). if ( header [ 'lskflg' ] ) : skewmatrix = np . fromfile ( ccp4file , dtype = np . float32 , count = 9 ) header [ 'skwmat' ] = skewmatrix . reshape ( ( 3 , 3 ) ) header [ 'skwtrn' ] = np . fromfile ( ccp4file , dtype = np . float32 , count = 3 ) else : header [ 'skwmat' ] = header [ 'skwtrn' ] = None ccp4file . seek ( 12 * 4 , 1 ) ccp4file . seek ( 15 * 4 , 1 ) # Skip future use section. ccp4file . seek ( 4 , 1 ) # Skip map text, already used above to verify format. # TODO: Compare file specified endianness to one obtained above. endiancode = struct . unpack ( bsaflag + '4b' , ccp4file . read ( 4 ) ) header [ 'endianness' ] = 'little' if endiancode == ( 0x44 , 0x41 , 0 , 0 ) else 'big' header [ 'arms' ] = struct . unpack ( bsaflag + 'f' , ccp4file . read ( 4 ) ) [ 0 ] header [ 'nlabl' ] = struct . unpack ( bsaflag + 'I' , ccp4file . read ( 4 ) ) [ 0 ] if header [ 'nlabl' ] : binlabel = ccp4file . read ( 80 * header [ 'nlabl' ] ) flag = bsaflag + str ( 80 * header [ 'nlabl' ] ) + 's' label = struct . unpack ( flag , binlabel ) [ 0 ] header [ 'label' ] = label . decode ( 'utf-8' ) . rstrip ( '\x00' ) else : header [ 'label' ] = None ccp4file . seek ( 256 * 4 ) # TODO: Parse symmetry records, if any. return header
1661	def ExpectingFunctionArgs ( clean_lines , linenum ) : line = clean_lines . elided [ linenum ] return ( Match ( r'^\s*MOCK_(CONST_)?METHOD\d+(_T)?\(' , line ) or ( linenum >= 2 and ( Match ( r'^\s*MOCK_(?:CONST_)?METHOD\d+(?:_T)?\((?:\S+,)?\s*$' , clean_lines . elided [ linenum - 1 ] ) or Match ( r'^\s*MOCK_(?:CONST_)?METHOD\d+(?:_T)?\(\s*$' , clean_lines . elided [ linenum - 2 ] ) or Search ( r'\bstd::m?function\s*\<\s*$' , clean_lines . elided [ linenum - 1 ] ) ) ) )
3087	def _get_entity ( self ) : if self . _is_ndb ( ) : return self . _model . get_by_id ( self . _key_name ) else : return self . _model . get_by_key_name ( self . _key_name )
8945	def url_as_file ( url , ext = None ) : if ext : ext = '.' + ext . strip ( '.' ) # normalize extension url_hint = 'www-{}-' . format ( urlparse ( url ) . hostname or 'any' ) if url . startswith ( 'file://' ) : url = os . path . abspath ( url [ len ( 'file://' ) : ] ) if os . path . isabs ( url ) : with open ( url , 'rb' ) as handle : content = handle . read ( ) else : content = requests . get ( url ) . content with tempfile . NamedTemporaryFile ( suffix = ext or '' , prefix = url_hint , delete = False ) as handle : handle . write ( content ) try : yield handle . name finally : if os . path . exists ( handle . name ) : os . remove ( handle . name )
4787	def matches ( self , pattern ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if not isinstance ( pattern , str_types ) : raise TypeError ( 'given pattern arg must be a string' ) if len ( pattern ) == 0 : raise ValueError ( 'given pattern arg must not be empty' ) if re . search ( pattern , self . val ) is None : self . _err ( 'Expected <%s> to match pattern <%s>, but did not.' % ( self . val , pattern ) ) return self
5150	def parse ( self , native ) : if not hasattr ( self , 'parser' ) or not self . parser : raise NotImplementedError ( 'Parser class not specified' ) parser = self . parser ( native ) self . intermediate_data = parser . intermediate_data del parser self . to_netjson ( )
7101	def on_marker ( self , mid ) : self . marker = Circle ( __id__ = mid ) self . parent ( ) . markers [ mid ] = self #: Required so the packer can pass the id self . marker . setTag ( mid ) d = self . declaration if d . clickable : self . set_clickable ( d . clickable ) #: Can free the options now del self . options
11477	def _create_or_reuse_folder ( local_folder , parent_folder_id , reuse_existing = False ) : local_folder_name = os . path . basename ( local_folder ) folder_id = None if reuse_existing : # check by name to see if the folder already exists in the folder children = session . communicator . folder_children ( session . token , parent_folder_id ) folders = children [ 'folders' ] for folder in folders : if folder [ 'name' ] == local_folder_name : folder_id = folder [ 'folder_id' ] break if folder_id is None : # create the item for the subdir new_folder = session . communicator . create_folder ( session . token , local_folder_name , parent_folder_id ) folder_id = new_folder [ 'folder_id' ] return folder_id
8654	def get_messages ( session , query , limit = 10 , offset = 0 ) : query [ 'limit' ] = limit query [ 'offset' ] = offset # GET /api/messages/0.1/messages response = make_get_request ( session , 'messages' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MessagesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
7501	def get_shape ( spans , loci ) : width = 0 for idx in xrange ( loci . shape [ 0 ] ) : width += spans [ loci [ idx ] , 1 ] - spans [ loci [ idx ] , 0 ] return width
1778	def OR ( cpu , dest , src ) : res = dest . write ( dest . read ( ) | src . read ( ) ) # Defined Flags: szp cpu . _calculate_logic_flags ( dest . size , res )
8607	def add_group_user ( self , group_id , user_id ) : data = { "id" : user_id } response = self . _perform_request ( url = '/um/groups/%s/users' % group_id , method = 'POST' , data = json . dumps ( data ) ) return response
1467	def process ( self , tup ) : curtime = int ( time . time ( ) ) self . current_tuples . append ( ( tup , curtime ) ) self . _expire ( curtime )
11283	def iter ( self , prev = None ) : if self . next : generator = self . next . iter ( self . func ( prev , * self . args , * * self . kw ) ) else : generator = self . func ( prev , * self . args , * * self . kw ) return generator
4377	def broadcast_event ( self , event , * args ) : pkt = dict ( type = "event" , name = event , args = args , endpoint = self . ns_name ) for sessid , socket in six . iteritems ( self . socket . server . sockets ) : socket . send_packet ( pkt )
11607	def social_widget_render ( parser , token ) : bits = token . split_contents ( ) tag_name = bits [ 0 ] if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" % tag_name ) args = [ ] kwargs = { } bits = bits [ 1 : ] if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to %s tag" % tag_name ) name , value = match . groups ( ) if name : # Replacing hyphens with underscores because # variable names cannot contain hyphens. name = name . replace ( '-' , '_' ) kwargs [ name ] = parser . compile_filter ( value ) else : args . append ( parser . compile_filter ( value ) ) return SocialWidgetNode ( args , kwargs )
6829	def pull ( self , path , use_sudo = False , user = None , force = False ) : if path is None : raise ValueError ( "Path to the working copy is needed to pull from a remote repository." ) options = [ ] if force : options . append ( '--force' ) options = ' ' . join ( options ) cmd = 'git pull %s' % options with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
1153	def _hash ( self ) : MAX = sys . maxint MASK = 2 * MAX + 1 n = len ( self ) h = 1927868237 * ( n + 1 ) h &= MASK for x in self : hx = hash ( x ) h ^= ( hx ^ ( hx << 16 ) ^ 89869747 ) * 3644798167 h &= MASK h = h * 69069 + 907133923 h &= MASK if h > MAX : h -= MASK + 1 if h == - 1 : h = 590923713 return h
10786	def add_subtract ( st , max_iter = 7 , max_npart = 'calc' , max_mem = 2e8 , always_check_remove = False , * * kwargs ) : if max_npart == 'calc' : max_npart = 0.05 * st . obj_get_positions ( ) . shape [ 0 ] total_changed = 0 _change_since_opt = 0 removed_poses = [ ] added_poses0 = [ ] added_poses = [ ] nr = 1 # Check removal on the first loop for _ in range ( max_iter ) : if ( nr != 0 ) or ( always_check_remove ) : nr , rposes = remove_bad_particles ( st , * * kwargs ) na , aposes = add_missing_particles ( st , * * kwargs ) current_changed = na + nr removed_poses . extend ( rposes ) added_poses0 . extend ( aposes ) total_changed += current_changed _change_since_opt += current_changed if current_changed == 0 : break elif _change_since_opt > max_npart : _change_since_opt *= 0 CLOG . info ( 'Start add_subtract optimization.' ) opt . do_levmarq ( st , opt . name_globals ( st , remove_params = st . get ( 'psf' ) . params ) , max_iter = 1 , run_length = 4 , num_eig_dirs = 3 , max_mem = max_mem , eig_update_frequency = 2 , rz_order = 0 , use_accel = True ) CLOG . info ( 'After optimization:\t{:.6}' . format ( st . error ) ) # Optimize the added particles' radii: for p in added_poses0 : i = st . obj_closest_particle ( p ) opt . do_levmarq_particles ( st , np . array ( [ i ] ) , max_iter = 2 , damping = 0.3 ) added_poses . append ( st . obj_get_positions ( ) [ i ] ) return total_changed , np . array ( removed_poses ) , np . array ( added_poses )
2331	def normal_noise ( points ) : return np . random . rand ( 1 ) * np . random . randn ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
13140	def read ( self ) : if not self . __content__ : self . __retriever__ = self . __resolver__ . resolve ( self . uri ) self . __content__ , self . __mimetype__ = self . __retriever__ . read ( self . uri ) return self . __content__
2244	def argflag ( key , argv = None ) : if argv is None : # nocover argv = sys . argv keys = [ key ] if isinstance ( key , six . string_types ) else key flag = any ( k in argv for k in keys ) return flag
2180	def fetch_access_token ( self , url , verifier = None , * * request_kwargs ) : if verifier : self . _client . client . verifier = verifier if not getattr ( self . _client . client , "verifier" , None ) : raise VerifierMissing ( "No client verifier has been set." ) token = self . _fetch_token ( url , * * request_kwargs ) log . debug ( "Resetting verifier attribute, should not be used anymore." ) self . _client . client . verifier = None return token
9861	def sync_update_info ( self , * _ ) : loop = asyncio . get_event_loop ( ) task = loop . create_task ( self . update_info ( ) ) loop . run_until_complete ( task )
12402	def require ( self , req ) : reqs = req if isinstance ( req , list ) else [ req ] for req in reqs : if not isinstance ( req , BumpRequirement ) : req = BumpRequirement ( req ) req . required = True req . required_by = self self . requirements . append ( req )
11530	def __get_rev ( self , key , version , * * kwa ) : if '_doc' in kwa : doc = kwa [ '_doc' ] else : if type ( version ) is int : if version == 0 : order = pymongo . ASCENDING elif version == - 1 : order = pymongo . DESCENDING doc = self . _collection . find_one ( { 'k' : key } , sort = [ [ 'd' , order ] ] ) elif type ( version ) is datetime : ver = self . __round_time ( version ) doc = self . _collection . find_one ( { 'k' : key , 'd' : ver } ) if doc is None : raise KeyError ( 'Supplied key `{0}` or version `{1}` does not exist' . format ( key , str ( version ) ) ) coded_val = doc [ 'v' ] return pickle . loads ( coded_val )
2648	def make_rundir ( path ) : try : if not os . path . exists ( path ) : os . makedirs ( path ) prev_rundirs = glob ( os . path . join ( path , "[0-9]*" ) ) current_rundir = os . path . join ( path , '000' ) if prev_rundirs : # Since we globbed on files named as 0-9 x = sorted ( [ int ( os . path . basename ( x ) ) for x in prev_rundirs ] ) [ - 1 ] current_rundir = os . path . join ( path , '{0:03}' . format ( x + 1 ) ) os . makedirs ( current_rundir ) logger . debug ( "Parsl run initializing in rundir: {0}" . format ( current_rundir ) ) return os . path . abspath ( current_rundir ) except Exception as e : logger . error ( "Failed to create a run directory" ) logger . error ( "Error: {0}" . format ( e ) ) raise
2942	def _add_notify ( self , task_spec ) : if task_spec . name in self . task_specs : raise KeyError ( 'Duplicate task spec name: ' + task_spec . name ) self . task_specs [ task_spec . name ] = task_spec task_spec . id = len ( self . task_specs )
5046	def _enroll_users ( cls , request , enterprise_customer , emails , mode , course_id = None , program_details = None , notify = True ) : pending_messages = [ ] if course_id : succeeded , pending , failed = cls . enroll_users_in_course ( enterprise_customer = enterprise_customer , course_id = course_id , course_mode = mode , emails = emails , ) all_successes = succeeded + pending if notify : enterprise_customer . notify_enrolled_learners ( catalog_api_user = request . user , course_id = course_id , users = all_successes , ) if succeeded : pending_messages . append ( cls . get_success_enrollment_message ( succeeded , course_id ) ) if failed : pending_messages . append ( cls . get_failed_enrollment_message ( failed , course_id ) ) if pending : pending_messages . append ( cls . get_pending_enrollment_message ( pending , course_id ) ) if program_details : succeeded , pending , failed = cls . enroll_users_in_program ( enterprise_customer = enterprise_customer , program_details = program_details , course_mode = mode , emails = emails , ) all_successes = succeeded + pending if notify : cls . notify_program_learners ( enterprise_customer = enterprise_customer , program_details = program_details , users = all_successes ) program_identifier = program_details . get ( 'title' , program_details . get ( 'uuid' , _ ( 'the program' ) ) ) if succeeded : pending_messages . append ( cls . get_success_enrollment_message ( succeeded , program_identifier ) ) if failed : pending_messages . append ( cls . get_failed_enrollment_message ( failed , program_identifier ) ) if pending : pending_messages . append ( cls . get_pending_enrollment_message ( pending , program_identifier ) ) cls . send_messages ( request , pending_messages )
10924	def fit_comp ( new_comp , old_comp , * * kwargs ) : #resetting the category to ilm: new_cat = new_comp . category new_comp . category = 'ilm' fake_s = states . ImageState ( Image ( old_comp . get ( ) . copy ( ) ) , [ new_comp ] , pad = 0 , mdl = mdl . SmoothFieldModel ( ) ) do_levmarq ( fake_s , new_comp . params , * * kwargs ) new_comp . category = new_cat
331	def model_best ( y1 , y2 , samples = 1000 , progressbar = True ) : y = np . concatenate ( ( y1 , y2 ) ) mu_m = np . mean ( y ) mu_p = 0.000001 * 1 / np . std ( y ) ** 2 sigma_low = np . std ( y ) / 1000 sigma_high = np . std ( y ) * 1000 with pm . Model ( ) as model : group1_mean = pm . Normal ( 'group1_mean' , mu = mu_m , tau = mu_p , testval = y1 . mean ( ) ) group2_mean = pm . Normal ( 'group2_mean' , mu = mu_m , tau = mu_p , testval = y2 . mean ( ) ) group1_std = pm . Uniform ( 'group1_std' , lower = sigma_low , upper = sigma_high , testval = y1 . std ( ) ) group2_std = pm . Uniform ( 'group2_std' , lower = sigma_low , upper = sigma_high , testval = y2 . std ( ) ) nu = pm . Exponential ( 'nu_minus_two' , 1 / 29. , testval = 4. ) + 2. returns_group1 = pm . StudentT ( 'group1' , nu = nu , mu = group1_mean , lam = group1_std ** - 2 , observed = y1 ) returns_group2 = pm . StudentT ( 'group2' , nu = nu , mu = group2_mean , lam = group2_std ** - 2 , observed = y2 ) diff_of_means = pm . Deterministic ( 'difference of means' , group2_mean - group1_mean ) pm . Deterministic ( 'difference of stds' , group2_std - group1_std ) pm . Deterministic ( 'effect size' , diff_of_means / pm . math . sqrt ( ( group1_std ** 2 + group2_std ** 2 ) / 2 ) ) pm . Deterministic ( 'group1_annual_volatility' , returns_group1 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group2_annual_volatility' , returns_group2 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group1_sharpe' , returns_group1 . distribution . mean / returns_group1 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group2_sharpe' , returns_group2 . distribution . mean / returns_group2 . distribution . variance ** .5 * np . sqrt ( 252 ) ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
5375	def simple_pattern_exists_in_gcs ( file_pattern , credentials = None ) : if '*' not in file_pattern : return _file_exists_in_gcs ( file_pattern , credentials ) if not file_pattern . startswith ( 'gs://' ) : raise ValueError ( 'file name must start with gs://' ) gcs_service = _get_storage_service ( credentials ) bucket_name , prefix = file_pattern [ len ( 'gs://' ) : ] . split ( '/' , 1 ) if '*' in bucket_name : raise ValueError ( 'Wildcards may not appear in the bucket name' ) # There is a '*' in prefix because we checked there's one in file_pattern # and there isn't one in bucket_name. Hence it must be in prefix. assert '*' in prefix prefix_no_wildcard = prefix [ : prefix . index ( '*' ) ] request = gcs_service . objects ( ) . list ( bucket = bucket_name , prefix = prefix_no_wildcard ) response = request . execute ( ) if 'items' not in response : return False items_list = [ i [ 'name' ] for i in response [ 'items' ] ] return any ( fnmatch . fnmatch ( i , prefix ) for i in items_list )
10043	def create_blueprint ( endpoints ) : from invenio_records_ui . views import create_url_rule blueprint = Blueprint ( 'invenio_deposit_ui' , __name__ , static_folder = '../static' , template_folder = '../templates' , url_prefix = '' , ) @ blueprint . errorhandler ( PIDDeletedError ) def tombstone_errorhandler ( error ) : """Render tombstone page.""" return render_template ( current_app . config [ 'DEPOSIT_UI_TOMBSTONE_TEMPLATE' ] , pid = error . pid , record = error . record or { } , ) , 410 for endpoint , options in ( endpoints or { } ) . items ( ) : options = deepcopy ( options ) options . pop ( 'jsonschema' , None ) options . pop ( 'schemaform' , None ) blueprint . add_url_rule ( * * create_url_rule ( endpoint , * * options ) ) @ blueprint . route ( '/deposit' ) @ login_required def index ( ) : """List user deposits.""" return render_template ( current_app . config [ 'DEPOSIT_UI_INDEX_TEMPLATE' ] ) @ blueprint . route ( '/deposit/new' ) @ login_required def new ( ) : """Create new deposit.""" deposit_type = request . values . get ( 'type' ) return render_template ( current_app . config [ 'DEPOSIT_UI_NEW_TEMPLATE' ] , record = { '_deposit' : { 'id' : None } } , jsonschema = current_deposit . jsonschemas [ deposit_type ] , schemaform = current_deposit . schemaforms [ deposit_type ] , ) return blueprint
7590	def fields_checker ( fields ) : ## make sure fields will work if isinstance ( fields , int ) : fields = str ( fields ) if isinstance ( fields , str ) : if "," in fields : fields = [ str ( i ) for i in fields . split ( "," ) ] else : fields = [ str ( fields ) ] elif isinstance ( fields , ( tuple , list ) ) : fields = [ str ( i ) for i in fields ] else : raise IPyradWarningExit ( "fields not properly formatted" ) ## do not allow zero in fields fields = [ i for i in fields if i != '0' ] return fields
12513	def _crop_img_to ( image , slices , copy = True ) : img = check_img ( image ) data = img . get_data ( ) affine = img . get_affine ( ) cropped_data = data [ slices ] if copy : cropped_data = cropped_data . copy ( ) linear_part = affine [ : 3 , : 3 ] old_origin = affine [ : 3 , 3 ] new_origin_voxel = np . array ( [ s . start for s in slices ] ) new_origin = old_origin + linear_part . dot ( new_origin_voxel ) new_affine = np . eye ( 4 ) new_affine [ : 3 , : 3 ] = linear_part new_affine [ : 3 , 3 ] = new_origin new_img = nib . Nifti1Image ( cropped_data , new_affine ) return new_img
1526	def add_context ( self , err_context , succ_context = None ) : self . err_context = err_context self . succ_context = succ_context
2251	def color_text ( text , color ) : if color is None : return text try : import pygments import pygments . console if sys . platform . startswith ( 'win32' ) : # nocover # Hack on win32 to support colored output import colorama colorama . init ( ) ansi_text = pygments . console . colorize ( color , text ) return ansi_text except ImportError : # nocover import warnings warnings . warn ( 'pygments is not installed, text will not be colored' ) return text
5065	def filter_audit_course_modes ( enterprise_customer , course_modes ) : audit_modes = getattr ( settings , 'ENTERPRISE_COURSE_ENROLLMENT_AUDIT_MODES' , [ 'audit' ] ) if not enterprise_customer . enable_audit_enrollment : return [ course_mode for course_mode in course_modes if course_mode [ 'mode' ] not in audit_modes ] return course_modes
7447	def _step6func ( self , samples , noreverse , force , randomseed , ipyclient , * * kwargs ) : ## Get sample objects from list of strings samples = _get_samples ( self , samples ) ## remove samples that aren't ready csamples = self . _samples_precheck ( samples , 6 , force ) ## print CLI header if self . _headers : print ( "\n Step 6: Clustering at {} similarity across {} samples" . format ( self . paramsdict [ "clust_threshold" ] , len ( csamples ) ) ) ## Check if all/none in the right state if not csamples : raise IPyradError ( FIRST_RUN_5 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 6 for i in csamples ] ) : print ( DATABASE_EXISTS . format ( len ( samples ) ) ) return ## run if this point is reached. We no longer check for existing ## h5 file, since checking Sample states should suffice. assemble . cluster_across . run ( self , csamples , noreverse , force , randomseed , ipyclient , * * kwargs )
1032	def b16decode ( s , casefold = False ) : if casefold : s = s . upper ( ) if re . search ( '[^0-9A-F]' , s ) : raise TypeError ( 'Non-base16 digit found' ) return binascii . unhexlify ( s )
12786	def _effective_filename ( self ) : # type: () -> str # same logic for the configuration filename. First, check if we were # initialized with a filename... config_filename = '' if self . filename : config_filename = self . filename # ... next, take the value from the environment env_filename = getenv ( self . env_filename_name ) if env_filename : self . _log . info ( 'Configuration filename was overridden with %r ' 'by the environment variable %s.' , env_filename , self . env_filename_name ) config_filename = env_filename return config_filename
8085	def nostroke ( self ) : c = self . _canvas . strokecolor self . _canvas . strokecolor = None return c
13800	def guid ( * args ) : t = float ( time . time ( ) * 1000 ) r = float ( random . random ( ) * 10000000000000 ) a = random . random ( ) * 10000000000000 data = str ( t ) + ' ' + str ( r ) + ' ' + str ( a ) + ' ' + str ( args ) data = hashlib . md5 ( data . encode ( ) ) . hexdigest ( ) [ : 10 ] return data
5426	def _group_tasks_by_jobid ( tasks ) : ret = collections . defaultdict ( list ) for t in tasks : ret [ t . get_field ( 'job-id' ) ] . append ( t ) return ret
11405	def record_drop_duplicate_fields ( record ) : out = { } position = 0 tags = sorted ( record . keys ( ) ) for tag in tags : fields = record [ tag ] out [ tag ] = [ ] current_fields = set ( ) for full_field in fields : field = ( tuple ( full_field [ 0 ] ) , ) + full_field [ 1 : 4 ] if field not in current_fields : current_fields . add ( field ) position += 1 out [ tag ] . append ( full_field [ : 4 ] + ( position , ) ) return out
3873	async def leave_conversation ( self , conv_id ) : logger . info ( 'Leaving conversation: {}' . format ( conv_id ) ) await self . _conv_dict [ conv_id ] . leave ( ) del self . _conv_dict [ conv_id ]
1860	def LODS ( cpu , dest , src ) : src_reg = { 8 : 'SI' , 32 : 'ESI' , 64 : 'RSI' } [ cpu . address_bit_size ] base , _ , ty = cpu . get_descriptor ( cpu . DS ) src_addr = cpu . read_register ( src_reg ) + base size = dest . size arg0 = cpu . read_int ( src_addr , size ) dest . write ( arg0 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment )
1323	def SetActive ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : handle = self . NativeWindowHandle if IsIconic ( handle ) : ret = ShowWindow ( handle , SW . Restore ) elif not IsWindowVisible ( handle ) : ret = ShowWindow ( handle , SW . Show ) ret = SetForegroundWindow ( handle ) # may fail if foreground windows's process is not python time . sleep ( waitTime ) return ret return False
11420	def record_xml_output ( rec , tags = None , order_fn = None ) : if tags is None : tags = [ ] if isinstance ( tags , str ) : tags = [ tags ] if tags and '001' not in tags : # Add the missing controlfield. tags . append ( '001' ) marcxml = [ '<record>' ] # Add the tag 'tag' to each field in rec[tag] fields = [ ] if rec is not None : for tag in rec : if not tags or tag in tags : for field in rec [ tag ] : fields . append ( ( tag , field ) ) if order_fn is None : record_order_fields ( fields ) else : record_order_fields ( fields , order_fn ) for field in fields : marcxml . append ( field_xml_output ( field [ 1 ] , field [ 0 ] ) ) marcxml . append ( '</record>' ) return '\n' . join ( marcxml )
5586	def output_cleaned ( self , process_data ) : if self . METADATA [ "data_type" ] == "raster" : if is_numpy_or_masked_array ( process_data ) : return process_data elif is_numpy_or_masked_array_with_tags ( process_data ) : data , tags = process_data return self . output_cleaned ( data ) , tags elif self . METADATA [ "data_type" ] == "vector" : return list ( process_data )
8394	def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] == "help" : show_help ( ) return 0 elif argv [ 0 ] == "check" : return check_main ( argv [ 1 : ] ) elif argv [ 0 ] == "list" : return list_main ( argv [ 1 : ] ) elif argv [ 0 ] == "write" : return write_main ( argv [ 1 : ] ) else : print ( u"Don't understand {!r}" . format ( " " . join ( argv ) ) ) show_help ( ) return 1
9181	def _validate_subjects ( cursor , model ) : subject_vocab = [ term [ 0 ] for term in acquire_subject_vocabulary ( cursor ) ] subjects = model . metadata . get ( 'subjects' , [ ] ) invalid_subjects = [ s for s in subjects if s not in subject_vocab ] if invalid_subjects : raise exceptions . InvalidMetadata ( 'subjects' , invalid_subjects )
10647	def create_component ( self , name , description = None ) : new_comp = Component ( name , self . gl , description = description ) new_comp . set_parent_path ( self . path ) self . components . append ( new_comp ) return new_comp
4229	def make_formatter ( format_name ) : if "json" in format_name : from json import dumps import datetime def jsonhandler ( obj ) : obj . isoformat ( ) if isinstance ( obj , ( datetime . datetime , datetime . date ) ) else obj if format_name == "prettyjson" : def jsondumps ( data ) : return dumps ( data , default = jsonhandler , indent = 2 , separators = ( ',' , ': ' ) ) else : def jsondumps ( data ) : return dumps ( data , default = jsonhandler ) def jsonify ( data ) : if isinstance ( data , dict ) : print ( jsondumps ( data ) ) elif isinstance ( data , list ) : print ( jsondumps ( [ device . _asdict ( ) for device in data ] ) ) else : print ( dumps ( { 'result' : data } ) ) return jsonify else : def printer ( data ) : if isinstance ( data , dict ) : print ( data ) else : for row in data : print ( row ) return printer
737	def _mergeFiles ( key , chunkCount , outputFile , fields ) : title ( ) # Open all chun files files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] # Open output file with FileRecordStream ( outputFile , write = True , fields = fields ) as o : # Open all chunk files files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] records = [ f . getNextRecord ( ) for f in files ] # This loop will run until all files are exhausted while not all ( r is None for r in records ) : # Cleanup None values (files that were exhausted) indices = [ i for i , r in enumerate ( records ) if r is not None ] records = [ records [ i ] for i in indices ] files = [ files [ i ] for i in indices ] # Find the current record r = min ( records , key = itemgetter ( * key ) ) # Write it to the file o . appendRecord ( r ) # Find the index of file that produced the current record index = records . index ( r ) # Read a new record from the file records [ index ] = files [ index ] . getNextRecord ( ) # Cleanup chunk files for i , f in enumerate ( files ) : f . close ( ) os . remove ( 'chunk_%d.csv' % i )
3766	def Z_from_virial_pressure_form ( P , * args ) : return 1 + P * sum ( [ coeff * P ** i for i , coeff in enumerate ( args ) ] )
3166	def cancel ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/cancel-send' ) )
8890	def serialize ( self ) : # NOTE: code adapted from https://github.com/django/django/blob/master/django/forms/models.py#L75 opts = self . _meta data = { } for f in opts . concrete_fields : if f . attname in self . morango_fields_not_to_serialize : continue if f . attname in self . _morango_internal_fields_not_to_serialize : continue # case if model is morango mptt if f . attname in getattr ( self , '_internal_mptt_fields_not_to_serialize' , '_internal_fields_not_to_serialize' ) : continue if hasattr ( f , 'value_from_object_json_compatible' ) : data [ f . attname ] = f . value_from_object_json_compatible ( self ) else : data [ f . attname ] = f . value_from_object ( self ) return data
7152	def one ( prompt , * args , * * kwargs ) : indicator = '‣' if sys . version_info < ( 3 , 0 ) : indicator = '>' def go_back ( picker ) : return None , - 1 options , verbose_options = prepare_options ( args ) idx = kwargs . get ( 'idx' , 0 ) picker = Picker ( verbose_options , title = prompt , indicator = indicator , default_index = idx ) picker . register_custom_handler ( ord ( 'h' ) , go_back ) picker . register_custom_handler ( curses . KEY_LEFT , go_back ) with stdout_redirected ( sys . stderr ) : option , index = picker . start ( ) if index == - 1 : raise QuestionnaireGoBack if kwargs . get ( 'return_index' , False ) : # `one` was called by a special client, e.g. `many` return index return options [ index ]
6758	def set_site_specifics ( self , site ) : r = self . local_renderer site_data = self . genv . sites [ site ] . copy ( ) r . env . site = site if self . verbose : print ( 'set_site_specifics.data:' ) pprint ( site_data , indent = 4 ) # Remove local namespace settings from the global namespace # by converting <satchel_name>_<variable_name> to <variable_name>. local_ns = { } for k , v in list ( site_data . items ( ) ) : if k . startswith ( self . name + '_' ) : _k = k [ len ( self . name + '_' ) : ] local_ns [ _k ] = v del site_data [ k ] r . env . update ( local_ns ) r . env . update ( site_data )
3931	def _auth_with_refresh_token ( session , refresh_token ) : # Make a token request. token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'grant_type' : 'refresh_token' , 'refresh_token' : refresh_token , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ]
10562	def _normalize_metadata ( metadata ) : metadata = str ( metadata ) metadata = metadata . lower ( ) metadata = re . sub ( r'\/\s*\d+' , '' , metadata ) # Remove "/<totaltracks>" from track number. metadata = re . sub ( r'^0+([0-9]+)' , r'\1' , metadata ) # Remove leading zero(s) from track number. metadata = re . sub ( r'^\d+\.+' , '' , metadata ) # Remove dots from track number. metadata = re . sub ( r'[^\w\s]' , '' , metadata ) # Remove any non-words. metadata = re . sub ( r'\s+' , ' ' , metadata ) # Reduce multiple spaces to a single space. metadata = re . sub ( r'^\s+' , '' , metadata ) # Remove leading space. metadata = re . sub ( r'\s+$' , '' , metadata ) # Remove trailing space. metadata = re . sub ( r'^the\s+' , '' , metadata , re . I ) # Remove leading "the". return metadata
11444	def parse ( self , path_to_xml = None ) : if not path_to_xml : if not self . path : self . logger . error ( "No path defined!" ) return path_to_xml = self . path root = self . _clean_xml ( path_to_xml ) # See first of this XML is clean or OAI request if root . tag . lower ( ) == 'collection' : tree = ET . ElementTree ( root ) self . records = element_tree_collection_to_records ( tree ) elif root . tag . lower ( ) == 'record' : new_root = ET . Element ( 'collection' ) new_root . append ( root ) tree = ET . ElementTree ( new_root ) self . records = element_tree_collection_to_records ( tree ) else : # We have an OAI request header_subs = get_request_subfields ( root ) records = root . find ( 'ListRecords' ) if records is None : records = root . find ( 'GetRecord' ) if records is None : raise ValueError ( "Cannot find ListRecords or GetRecord!" ) tree = ET . ElementTree ( records ) for record , is_deleted in element_tree_oai_records ( tree , header_subs ) : if is_deleted : # It was OAI deleted. Create special record self . deleted_records . append ( self . create_deleted_record ( record ) ) else : self . records . append ( record )
10001	def get_nodes_with ( self , obj ) : result = set ( ) if nx . __version__ [ 0 ] == "1" : nodes = self . nodes_iter ( ) else : nodes = self . nodes for node in nodes : if node [ OBJ ] == obj : result . add ( node ) return result
725	def get ( self , number ) : if not number in self . _patterns : raise IndexError ( "Invalid number" ) return self . _patterns [ number ]
7687	def multi_segment ( annotation , sr = 22050 , length = None , * * kwargs ) : # Pentatonic scale, because why not PENT = [ 1 , 32. / 27 , 4. / 3 , 3. / 2 , 16. / 9 ] DURATION = 0.1 h_int , _ = hierarchy_flatten ( annotation ) if length is None : length = int ( sr * ( max ( np . max ( _ ) for _ in h_int ) + 1. / DURATION ) + 1 ) y = 0.0 for ints , ( oc , scale ) in zip ( h_int , product ( range ( 3 , 3 + len ( h_int ) ) , PENT ) ) : click = mkclick ( 440.0 * scale * oc , sr = sr , duration = DURATION ) y = y + filter_kwargs ( mir_eval . sonify . clicks , np . unique ( ints ) , fs = sr , length = length , click = click ) return y
3163	def create ( self , workflow_id , email_id , data ) : self . workflow_id = workflow_id self . email_id = email_id if 'email_address' not in data : raise KeyError ( 'The automation email queue must have an email_address' ) check_email ( data [ 'email_address' ] ) response = self . _mc_client . _post ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'queue' ) , data = data ) if response is not None : self . subscriber_hash = response [ 'id' ] else : self . subscriber_hash = None return response
4173	def window_visu ( N = 51 , name = 'hamming' , * * kargs ) : # get the default parameters mindB = kargs . pop ( 'mindB' , - 100 ) maxdB = kargs . pop ( 'maxdB' , None ) norm = kargs . pop ( 'norm' , True ) # create a window object w = Window ( N , name , * * kargs ) # plot the time and frequency windows w . plot_time_freq ( mindB = mindB , maxdB = maxdB , norm = norm )
8822	def main ( notify , hour , minute ) : # Read the config file and get the admin context config_opts = [ '--config-file' , '/etc/neutron/neutron.conf' ] config . init ( config_opts ) # Have to load the billing module _after_ config is parsed so # that we get the right network strategy network_strategy . STRATEGY . load ( ) billing . PUBLIC_NETWORK_ID = network_strategy . STRATEGY . get_public_net_id ( ) config . setup_logging ( ) context = neutron_context . get_admin_context ( ) # A query to get all IPAddress objects from the db query = context . session . query ( models . IPAddress ) ( period_start , period_end ) = billing . calc_periods ( hour , minute ) full_day_ips = billing . build_full_day_ips ( query , period_start , period_end ) partial_day_ips = billing . build_partial_day_ips ( query , period_start , period_end ) if notify : # '==================== Full Day =============================' for ipaddress in full_day_ips : click . echo ( 'start: {}, end: {}' . format ( period_start , period_end ) ) payload = billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = period_start , end_time = period_end ) billing . do_notify ( context , billing . IP_EXISTS , payload ) # '==================== Part Day =============================' for ipaddress in partial_day_ips : click . echo ( 'start: {}, end: {}' . format ( period_start , period_end ) ) payload = billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = ipaddress . allocated_at , end_time = period_end ) billing . do_notify ( context , billing . IP_EXISTS , payload ) else : click . echo ( 'Case 1 ({}):\n' . format ( len ( full_day_ips ) ) ) for ipaddress in full_day_ips : pp ( billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = period_start , end_time = period_end ) ) click . echo ( '\n===============================================\n' ) click . echo ( 'Case 2 ({}):\n' . format ( len ( partial_day_ips ) ) ) for ipaddress in partial_day_ips : pp ( billing . build_payload ( ipaddress , billing . IP_EXISTS , start_time = ipaddress . allocated_at , end_time = period_end ) )
10737	def path_from_keywords ( keywords , into = 'path' ) : subdirs = [ ] def prepare_string ( s ) : s = str ( s ) s = re . sub ( '[][{},*"' + f"'{os.sep}]" , '_' , s ) #replace characters that make bash life difficult by underscore if into == 'file' : s = s . replace ( '_' , ' ' ) #Remove underscore because they will be used as separator if ' ' in s : s = s . title ( ) s = s . replace ( ' ' , '' ) return s if isinstance ( keywords , set ) : keywords_list = sorted ( keywords ) for property in keywords_list : subdirs . append ( prepare_string ( property ) ) else : keywords_list = sorted ( keywords . items ( ) ) for property , value in keywords_list : # @reservedassignment if Bool . valid ( value ) : subdirs . append ( ( '' if value else ( 'not_' if into == 'path' else 'not' ) ) + prepare_string ( property ) ) #elif String.valid(value): # subdirs.append(prepare_string(value)) elif ( Float | Integer ) . valid ( value ) : subdirs . append ( '{}{}' . format ( prepare_string ( property ) , prepare_string ( value ) ) ) else : subdirs . append ( '{}{}{}' . format ( prepare_string ( property ) , '_' if into == 'path' else '' , prepare_string ( value ) ) ) if into == 'path' : out = os . path . join ( * subdirs ) else : out = '_' . join ( subdirs ) return out
2100	def read ( self , * args , * * kwargs ) : if 'actor' in kwargs : kwargs [ 'actor' ] = kwargs . pop ( 'actor' ) r = super ( Resource , self ) . read ( * args , * * kwargs ) if 'results' in r : for d in r [ 'results' ] : self . _promote_actor ( d ) else : self . _promote_actor ( d ) return r
3381	def shared_np_array ( shape , data = None , integer = False ) : size = np . prod ( shape ) if integer : array = Array ( ctypes . c_int64 , int ( size ) ) np_array = np . frombuffer ( array . get_obj ( ) , dtype = "int64" ) else : array = Array ( ctypes . c_double , int ( size ) ) np_array = np . frombuffer ( array . get_obj ( ) ) np_array = np_array . reshape ( shape ) if data is not None : if len ( shape ) != len ( data . shape ) : raise ValueError ( "`data` must have the same dimensions" "as the created array." ) same = all ( x == y for x , y in zip ( shape , data . shape ) ) if not same : raise ValueError ( "`data` must have the same shape" "as the created array." ) np_array [ : ] = data return np_array
6849	def needs_initrole ( self , stop_on_error = False ) : ret = False target_host_present = self . is_present ( ) if not target_host_present : default_host_present = self . is_present ( self . env . default_hostname ) if default_host_present : if self . verbose : print ( 'Target host missing and default host present so host init required.' ) ret = True else : if self . verbose : print ( 'Target host missing but default host also missing, ' 'so no host init required.' ) # if stop_on_error: # raise Exception( # 'Both target and default hosts missing! ' # 'Is the machine turned on and plugged into the network?') else : if self . verbose : print ( 'Target host is present so no host init required.' ) return ret
13189	async def _upload_to_mongodb ( collection , jsonld ) : document = { 'data' : jsonld } query = { 'data.reportNumber' : jsonld [ 'reportNumber' ] } await collection . update ( query , document , upsert = True , multi = False )
3261	def get_style ( self , name , workspace = None ) : styles = self . get_styles ( names = name , workspaces = workspace ) return self . _return_first_item ( styles )
3239	def get_group ( group_name , users = True , client = None , * * kwargs ) : # First, make the initial call to get the details for the group: result = client . get_group ( GroupName = group_name , * * kwargs ) # If we care about the user details, then fetch them: if users : if result . get ( 'IsTruncated' ) : kwargs_to_send = { 'GroupName' : group_name } kwargs_to_send . update ( kwargs ) user_list = result [ 'Users' ] kwargs_to_send [ 'Marker' ] = result [ 'Marker' ] result [ 'Users' ] = user_list + _get_users_for_group ( client , * * kwargs_to_send ) else : result . pop ( 'Users' , None ) result . pop ( 'IsTruncated' , None ) result . pop ( 'Marker' , None ) return result
11846	def add_thing ( self , thing , location = None ) : if not isinstance ( thing , Thing ) : thing = Agent ( thing ) assert thing not in self . things , "Don't add the same thing twice" thing . location = location or self . default_location ( thing ) self . things . append ( thing ) if isinstance ( thing , Agent ) : thing . performance = 0 self . agents . append ( thing )
4525	def get ( self , position = 0 ) : n = len ( self ) if n == 1 : return self [ 0 ] pos = position if self . length and self . autoscale : pos *= len ( self ) pos /= self . length pos *= self . scale pos += self . offset if not self . continuous : if not self . serpentine : return self [ int ( pos % n ) ] # We want a color sequence of length 2n-2 # e.g. for n=5: a b c d | e d c b | a b c d ... m = ( 2 * n ) - 2 pos %= m if pos < n : return self [ int ( pos ) ] else : return self [ int ( m - pos ) ] if self . serpentine : pos %= ( 2 * n ) if pos > n : pos = ( 2 * n ) - pos else : pos %= n # p is a number in [0, n): scale it to be in [0, n-1) pos *= n - 1 pos /= n index = int ( pos ) fade = pos - index if not fade : return self [ index ] r1 , g1 , b1 = self [ index ] r2 , g2 , b2 = self [ ( index + 1 ) % len ( self ) ] dr , dg , db = r2 - r1 , g2 - g1 , b2 - b1 return r1 + fade * dr , g1 + fade * dg , b1 + fade * db
5738	def enqueue ( self , f , * args , * * kwargs ) : task = Task ( uuid4 ( ) . hex , f , args , kwargs ) self . storage . put_task ( task ) return self . enqueue_task ( task )
997	def printState ( self , aState ) : def formatRow ( var , i ) : s = '' for c in range ( self . numberOfCols ) : if c > 0 and c % 10 == 0 : s += ' ' s += str ( var [ c , i ] ) s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatRow ( aState , i )
11355	def record_xml_output ( rec , pretty = True ) : from . html_utils import MathMLParser ret = etree . tostring ( rec , xml_declaration = False ) # Special MathML handling ret = re . sub ( "(&lt;)(([\/]?{0}))" . format ( "|[\/]?" . join ( MathMLParser . mathml_elements ) ) , '<\g<2>' , ret ) ret = re . sub ( "&gt;" , '>' , ret ) if pretty : # We are doing our own prettyfication as etree pretty_print is too insane. ret = ret . replace ( '</datafield>' , ' </datafield>\n' ) ret = re . sub ( r'<datafield(.*?)>' , r' <datafield\1>\n' , ret ) ret = ret . replace ( '</subfield>' , '</subfield>\n' ) ret = ret . replace ( '<subfield' , ' <subfield' ) ret = ret . replace ( 'record>' , 'record>\n' ) return ret
3077	def has_credentials ( self ) : if not self . credentials : return False # Is the access token expired? If so, do we have an refresh token? elif ( self . credentials . access_token_expired and not self . credentials . refresh_token ) : return False else : return True
13749	def many_to_one ( clsname , * * kw ) : @ declared_attr def m2o ( cls ) : cls . _references ( ( cls . __name__ , clsname ) ) return relationship ( clsname , * * kw ) return m2o
6391	def encode ( self , word , max_length = 8 ) : # Lowercase input & filter unknown characters word = '' . join ( char for char in word . lower ( ) if char in self . _initial_phones ) if not word : word = '÷' # Perform initial eudex coding of each character values = [ self . _initial_phones [ word [ 0 ] ] ] values += [ self . _trailing_phones [ char ] for char in word [ 1 : ] ] # Right-shift by one to determine if second instance should be skipped shifted_values = [ _ >> 1 for _ in values ] condensed_values = [ values [ 0 ] ] for n in range ( 1 , len ( shifted_values ) ) : if shifted_values [ n ] != shifted_values [ n - 1 ] : condensed_values . append ( values [ n ] ) # Add padding after first character & trim beyond max_length values = ( [ condensed_values [ 0 ] ] + [ 0 ] * max ( 0 , max_length - len ( condensed_values ) ) + condensed_values [ 1 : max_length ] ) # Combine individual character values into eudex hash hash_value = 0 for val in values : hash_value = ( hash_value << 8 ) | val return hash_value
3043	def _expires_in ( self ) : if self . token_expiry : now = _UTCNOW ( ) if self . token_expiry > now : time_delta = self . token_expiry - now # TODO(orestica): return time_delta.total_seconds() # once dropping support for Python 2.6 return time_delta . days * 86400 + time_delta . seconds else : return 0
12880	def _fill ( self , size ) : try : for i in range ( size ) : self . buffer . append ( self . source . next ( ) ) except StopIteration : self . buffer . append ( ( EndOfFile , EndOfFile ) ) self . len = len ( self . buffer )
4763	def soft_fail ( msg = '' ) : global _soft_ctx if _soft_ctx : global _soft_err _soft_err . append ( 'Fail: %s!' % msg if msg else 'Fail!' ) return fail ( msg )
11377	def _normalize_article_dir_with_dtd ( self , path ) : if exists ( join ( path , 'resolved_main.xml' ) ) : return main_xml_content = open ( join ( path , 'main.xml' ) ) . read ( ) arts = [ 'art501.dtd' , 'art510.dtd' , 'art520.dtd' , 'art540.dtd' ] tmp_extracted = 0 for art in arts : if art in main_xml_content : self . _extract_correct_dtd_package ( art . split ( '.' ) [ 0 ] , path ) tmp_extracted = 1 if not tmp_extracted : message = "It looks like the path " + path message += "does not contain an art501, art510, art520 or art540 in main.xml file" self . logger . error ( message ) raise ValueError ( message ) command = [ "xmllint" , "--format" , "--loaddtd" , join ( path , 'main.xml' ) , "--output" , join ( path , 'resolved_main.xml' ) ] dummy , dummy , cmd_err = run_shell_command ( command ) if cmd_err : message = "Error in cleaning %s: %s" % ( join ( path , 'main.xml' ) , cmd_err ) self . logger . error ( message ) raise ValueError ( message )
12396	def get_method ( self , * args , * * kwargs ) : for method in self . gen_methods ( * args , * * kwargs ) : return method msg = 'No method was found for %r on %r.' raise self . DispatchError ( msg % ( ( args , kwargs ) , self . inst ) )
5670	def temporal_network ( gtfs , start_time_ut = None , end_time_ut = None , route_type = None ) : events_df = gtfs . get_transit_events ( start_time_ut = start_time_ut , end_time_ut = end_time_ut , route_type = route_type ) events_df . drop ( 'to_seq' , 1 , inplace = True ) events_df . drop ( 'shape_id' , 1 , inplace = True ) events_df . drop ( 'duration' , 1 , inplace = True ) events_df . drop ( 'route_id' , 1 , inplace = True ) events_df . rename ( columns = { 'from_seq' : "seq" } , inplace = True ) return events_df
13877	def CopyDirectory ( source_dir , target_dir , override = False ) : _AssertIsLocal ( source_dir ) _AssertIsLocal ( target_dir ) if override and IsDir ( target_dir ) : DeleteDirectory ( target_dir , skip_on_error = False ) import shutil shutil . copytree ( source_dir , target_dir )
10122	def _kwargs ( self ) : return dict ( color = self . color , velocity = self . velocity , colors = self . colors )
6926	def cursor ( self , handle , dictcursor = False ) : if handle in self . cursors : return self . cursors [ handle ] else : if dictcursor : self . cursors [ handle ] = self . connection . cursor ( cursor_factory = psycopg2 . extras . DictCursor ) else : self . cursors [ handle ] = self . connection . cursor ( ) return self . cursors [ handle ]
13412	def changeLogType ( self ) : logType = self . selectedType ( ) programs = self . logList . get ( logType ) [ 0 ] default = self . logList . get ( logType ) [ 1 ] if logType in self . logList : self . programName . clear ( ) self . programName . addItems ( programs ) self . programName . setCurrentIndex ( programs . index ( default ) )
7900	def process_configuration_success ( self , stanza ) : _unused = stanza self . configured = True self . handler . room_configured ( )
6886	def mdwarf_subtype_from_sdsscolor ( ri_color , iz_color ) : # calculate the spectral type index and the spectral type spread of the # object. sti is calculated by fitting a line to the locus in r-i and i-z # space for M dwarfs in West+ 2007 if np . isfinite ( ri_color ) and np . isfinite ( iz_color ) : obj_sti = 0.875274 * ri_color + 0.483628 * ( iz_color + 0.00438 ) obj_sts = - 0.483628 * ri_color + 0.875274 * ( iz_color + 0.00438 ) else : obj_sti = np . nan obj_sts = np . nan # possible M star if sti is >= 0.666 but <= 3.4559 if ( np . isfinite ( obj_sti ) and np . isfinite ( obj_sts ) and ( obj_sti > 0.666 ) and ( obj_sti < 3.4559 ) ) : # decide which M subclass object this is if ( ( obj_sti > 0.6660 ) and ( obj_sti < 0.8592 ) ) : m_class = 'M0' if ( ( obj_sti > 0.8592 ) and ( obj_sti < 1.0822 ) ) : m_class = 'M1' if ( ( obj_sti > 1.0822 ) and ( obj_sti < 1.2998 ) ) : m_class = 'M2' if ( ( obj_sti > 1.2998 ) and ( obj_sti < 1.6378 ) ) : m_class = 'M3' if ( ( obj_sti > 1.6378 ) and ( obj_sti < 2.0363 ) ) : m_class = 'M4' if ( ( obj_sti > 2.0363 ) and ( obj_sti < 2.2411 ) ) : m_class = 'M5' if ( ( obj_sti > 2.2411 ) and ( obj_sti < 2.4126 ) ) : m_class = 'M6' if ( ( obj_sti > 2.4126 ) and ( obj_sti < 2.9213 ) ) : m_class = 'M7' if ( ( obj_sti > 2.9213 ) and ( obj_sti < 3.2418 ) ) : m_class = 'M8' if ( ( obj_sti > 3.2418 ) and ( obj_sti < 3.4559 ) ) : m_class = 'M9' else : m_class = None return m_class , obj_sti , obj_sts
616	def expGenerator ( args ) : # ----------------------------------------------------------------- # Parse command line options # parser = OptionParser ( ) parser . set_usage ( "%prog [options] --description='{json object with args}'\n" + "%prog [options] --descriptionFromFile='{filename}'\n" + "%prog [options] --showSchema" ) parser . add_option ( "--description" , dest = "description" , help = "Tells ExpGenerator to generate an experiment description.py and " "permutations.py file using the given JSON formatted experiment " "description string." ) parser . add_option ( "--descriptionFromFile" , dest = 'descriptionFromFile' , help = "Tells ExpGenerator to open the given filename and use it's " "contents as the JSON formatted experiment description." ) parser . add_option ( "--claDescriptionTemplateFile" , dest = 'claDescriptionTemplateFile' , default = 'claDescriptionTemplate.tpl' , help = "The file containing the template description file for " " ExpGenerator [default: %default]" ) parser . add_option ( "--showSchema" , action = "store_true" , dest = "showSchema" , help = "Prints the JSON schemas for the --description arg." ) parser . add_option ( "--version" , dest = 'version' , default = 'v2' , help = "Generate the permutations file for this version of hypersearch." " Possible choices are 'v1' and 'v2' [default: %default]." ) parser . add_option ( "--outDir" , dest = "outDir" , default = None , help = "Where to generate experiment. If not specified, " "then a temp directory will be created" ) ( options , remainingArgs ) = parser . parse_args ( args ) #print("OPTIONS=%s" % (str(options))) # ----------------------------------------------------------------- # Check for unprocessed args # if len ( remainingArgs ) > 0 : raise _InvalidCommandArgException ( _makeUsageErrorStr ( "Unexpected command-line args: <%s>" % ( ' ' . join ( remainingArgs ) , ) , parser . get_usage ( ) ) ) # ----------------------------------------------------------------- # Check for use of mutually-exclusive options # activeOptions = filter ( lambda x : getattr ( options , x ) != None , ( 'description' , 'showSchema' ) ) if len ( activeOptions ) > 1 : raise _InvalidCommandArgException ( _makeUsageErrorStr ( ( "The specified command options are " + "mutually-exclusive: %s" ) % ( activeOptions , ) , parser . get_usage ( ) ) ) # ----------------------------------------------------------------- # Process requests # if options . showSchema : _handleShowSchemaOption ( ) elif options . description : _handleDescriptionOption ( options . description , options . outDir , parser . get_usage ( ) , hsVersion = options . version , claDescriptionTemplateFile = options . claDescriptionTemplateFile ) elif options . descriptionFromFile : _handleDescriptionFromFileOption ( options . descriptionFromFile , options . outDir , parser . get_usage ( ) , hsVersion = options . version , claDescriptionTemplateFile = options . claDescriptionTemplateFile ) else : raise _InvalidCommandArgException ( _makeUsageErrorStr ( "Error in validating command options. No option " "provided:\n" , parser . get_usage ( ) ) )
8999	def _dump_knitting_pattern ( self , file ) : knitting_pattern_set = self . __on_dump ( ) knitting_pattern = knitting_pattern_set . patterns . at ( 0 ) layout = GridLayout ( knitting_pattern ) builder = AYABPNGBuilder ( * layout . bounding_box ) builder . set_colors_in_grid ( layout . walk_instructions ( ) ) builder . write_to_file ( file )
840	def closestTrainingPattern ( self , inputPattern , cat ) : dist = self . _getDistances ( inputPattern ) sorted = dist . argsort ( ) for patIdx in sorted : patternCat = self . _categoryList [ patIdx ] # If closest pattern belongs to desired category, return it if patternCat == cat : if self . useSparseMemory : closestPattern = self . _Memory . getRow ( int ( patIdx ) ) else : closestPattern = self . _M [ patIdx ] return closestPattern # No patterns were found! return None
5412	def build_action ( name = None , image_uri = None , commands = None , entrypoint = None , environment = None , pid_namespace = None , flags = None , port_mappings = None , mounts = None , labels = None ) : return { 'name' : name , 'imageUri' : image_uri , 'commands' : commands , 'entrypoint' : entrypoint , 'environment' : environment , 'pidNamespace' : pid_namespace , 'flags' : flags , 'portMappings' : port_mappings , 'mounts' : mounts , 'labels' : labels , }
3199	def delete ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _delete ( url = self . _build_path ( workflow_id , 'emails' , email_id ) )
5772	def _bcrypt_verify ( certificate_or_public_key , signature , data , hash_algorithm , rsa_pss_padding = False ) : if hash_algorithm == 'raw' : digest = data else : hash_constant = { 'md5' : BcryptConst . BCRYPT_MD5_ALGORITHM , 'sha1' : BcryptConst . BCRYPT_SHA1_ALGORITHM , 'sha256' : BcryptConst . BCRYPT_SHA256_ALGORITHM , 'sha384' : BcryptConst . BCRYPT_SHA384_ALGORITHM , 'sha512' : BcryptConst . BCRYPT_SHA512_ALGORITHM } [ hash_algorithm ] digest = getattr ( hashlib , hash_algorithm ) ( data ) . digest ( ) padding_info = null ( ) flags = 0 if certificate_or_public_key . algorithm == 'rsa' : if rsa_pss_padding : flags = BcryptConst . BCRYPT_PAD_PSS padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PSS_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) # This has to be assigned to a variable to prevent cffi from gc'ing it hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . cbSalt = len ( digest ) else : flags = BcryptConst . BCRYPT_PAD_PKCS1 padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PKCS1_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) # This has to be assigned to a variable to prevent cffi from gc'ing it if hash_algorithm == 'raw' : padding_info_struct . pszAlgId = null ( ) else : hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) else : # Windows doesn't use the ASN.1 Sequence for DSA/ECDSA signatures, # so we have to convert it here for the verification to work try : signature = algos . DSASignature . load ( signature ) . to_p1363 ( ) except ( ValueError , OverflowError , TypeError ) : raise SignatureError ( 'Signature is invalid' ) res = bcrypt . BCryptVerifySignature ( certificate_or_public_key . key_handle , padding_info , digest , len ( digest ) , signature , len ( signature ) , flags ) failure = res == BcryptConst . STATUS_INVALID_SIGNATURE failure = failure or res == BcryptConst . STATUS_INVALID_PARAMETER if failure : raise SignatureError ( 'Signature is invalid' ) handle_error ( res )
5470	def _prepare_summary_table ( rows ) : if not rows : return [ ] # We either group on the job-name (if present) or fall back to the job-id key_field = 'job-name' if key_field not in rows [ 0 ] : key_field = 'job-id' # Group each of the rows based on (job-name or job-id, status) grouped = collections . defaultdict ( lambda : collections . defaultdict ( lambda : [ ] ) ) for row in rows : grouped [ row . get ( key_field , '' ) ] [ row . get ( 'status' , '' ) ] += [ row ] # Now that we have the rows grouped, create a summary table. # Use the original table as the driver in order to preserve the order. new_rows = [ ] for job_key in sorted ( grouped . keys ( ) ) : group = grouped . get ( job_key , None ) canonical_status = [ 'RUNNING' , 'SUCCESS' , 'FAILURE' , 'CANCEL' ] # Written this way to ensure that if somehow a new status is introduced, # it shows up in our output. for status in canonical_status + sorted ( group . keys ( ) ) : if status not in group : continue task_count = len ( group [ status ] ) del group [ status ] if task_count : summary_row = collections . OrderedDict ( ) summary_row [ key_field ] = job_key summary_row [ 'status' ] = status summary_row [ 'task-count' ] = task_count new_rows . append ( summary_row ) return new_rows
4646	def exists ( self ) : query = ( "SELECT name FROM sqlite_master " + "WHERE type='table' AND name=?" , ( self . __tablename__ , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False
11518	def generate_upload_token ( self , token , item_id , filename , checksum = None ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemid' ] = item_id parameters [ 'filename' ] = filename if checksum is not None : parameters [ 'checksum' ] = checksum response = self . request ( 'midas.upload.generatetoken' , parameters ) return response [ 'token' ]
3581	def _get_objects ( self , interface , parent_path = '/org/bluez' ) : # Iterate through all the objects in bluez's DBus hierarchy and return # any that implement the requested interface under the specified path. parent_path = parent_path . lower ( ) objects = [ ] for opath , interfaces in iteritems ( self . _bluez . GetManagedObjects ( ) ) : if interface in interfaces . keys ( ) and opath . lower ( ) . startswith ( parent_path ) : objects . append ( self . _bus . get_object ( 'org.bluez' , opath ) ) return objects
9211	def cli ( url , user_agent ) : kwargs = { } if user_agent : kwargs [ 'user_agent' ] = user_agent archive_url = capture ( url , * * kwargs ) click . echo ( archive_url )
12635	def levenshtein_analysis ( self , field_weights = None ) : if field_weights is None : if not isinstance ( self . field_weights , dict ) : raise ValueError ( 'Expected a dict for `field_weights` parameter, ' 'got {}' . format ( type ( self . field_weights ) ) ) key_dicoms = list ( self . dicom_groups . keys ( ) ) file_dists = calculate_file_distances ( key_dicoms , field_weights , self . _dist_method_cls ) return file_dists
1532	def get_execution_state ( self , topologyName , callback = None ) : if callback : self . execution_state_watchers [ topologyName ] . append ( callback ) else : execution_state_path = self . get_execution_state_path ( topologyName ) with open ( execution_state_path ) as f : data = f . read ( ) executionState = ExecutionState ( ) executionState . ParseFromString ( data ) return executionState
3524	def intercom ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return IntercomNode ( )
13598	def tick ( self ) : self . current += 1 if self . current == self . factor : sys . stdout . write ( '+' ) sys . stdout . flush ( ) self . current = 0
4185	def window_flattop ( N , mode = 'symmetric' , precision = None ) : assert mode in [ 'periodic' , 'symmetric' ] t = arange ( 0 , N ) # FIXME: N=1 for mode = periodic ? if mode == 'periodic' : x = 2 * pi * t / float ( N ) else : if N == 1 : return ones ( 1 ) x = 2 * pi * t / float ( N - 1 ) a0 = 0.21557895 a1 = 0.41663158 a2 = 0.277263158 a3 = 0.083578947 a4 = 0.006947368 if precision == 'octave' : #to compare with octave, same as above but less precise d = 4.6402 a0 = 1. / d a1 = 1.93 / d a2 = 1.29 / d a3 = 0.388 / d a4 = 0.0322 / d w = a0 - a1 * cos ( x ) + a2 * cos ( 2 * x ) - a3 * cos ( 3 * x ) + a4 * cos ( 4 * x ) return w
7761	def xml_elements_equal ( element1 , element2 , ignore_level1_cdata = False ) : # pylint: disable-msg=R0911 if None in ( element1 , element2 ) or element1 . tag != element2 . tag : return False attrs1 = element1 . items ( ) attrs1 . sort ( ) attrs2 = element2 . items ( ) attrs2 . sort ( ) if not ignore_level1_cdata : if element1 . text != element2 . text : return False if attrs1 != attrs2 : return False if len ( element1 ) != len ( element2 ) : return False for child1 , child2 in zip ( element1 , element2 ) : if child1 . tag != child2 . tag : return False if not ignore_level1_cdata : if element1 . text != element2 . text : return False if not xml_elements_equal ( child1 , child2 ) : return False return True
10700	def paginate_link_tag ( item ) : a_tag = Page . default_link_tag ( item ) if item [ 'type' ] == 'current_page' : return make_html_tag ( 'li' , a_tag , * * { 'class' : 'blue white-text' } ) return make_html_tag ( 'li' , a_tag )
6573	def formatter ( self , api_client , data , newval ) : url_map = data . get ( "audioUrlMap" ) audio_url = data . get ( "audioUrl" ) # Only an audio URL, not a quality map. This happens for most of the # mobile client tokens and some of the others now. In this case # substitute the empirically determined default values in the format # used by the rest of the function so downstream consumers continue to # work. if audio_url and not url_map : url_map = { BaseAPIClient . HIGH_AUDIO_QUALITY : { "audioUrl" : audio_url , "bitrate" : 64 , "encoding" : "aacplus" , } } elif not url_map : # No audio url available (e.g. ad tokens) return None valid_audio_formats = [ BaseAPIClient . HIGH_AUDIO_QUALITY , BaseAPIClient . MED_AUDIO_QUALITY , BaseAPIClient . LOW_AUDIO_QUALITY ] # Only iterate over sublist, starting at preferred audio quality, or # from the beginning of the list if nothing is found. Ensures that the # bitrate used will always be the same or lower quality than was # specified to prevent audio from skipping for slow connections. preferred_quality = api_client . default_audio_quality if preferred_quality in valid_audio_formats : i = valid_audio_formats . index ( preferred_quality ) valid_audio_formats = valid_audio_formats [ i : ] for quality in valid_audio_formats : audio_url = url_map . get ( quality ) if audio_url : return audio_url [ self . field ] return audio_url [ self . field ] if audio_url else None
5174	def events ( self , * * kwargs ) : return self . __api . events ( query = EqualsOperator ( "report" , self . hash_ ) , * * kwargs )
13790	def MessageSetItemDecoder ( extensions_by_number ) : type_id_tag_bytes = encoder . TagBytes ( 2 , wire_format . WIRETYPE_VARINT ) message_tag_bytes = encoder . TagBytes ( 3 , wire_format . WIRETYPE_LENGTH_DELIMITED ) item_end_tag_bytes = encoder . TagBytes ( 1 , wire_format . WIRETYPE_END_GROUP ) local_ReadTag = ReadTag local_DecodeVarint = _DecodeVarint local_SkipField = SkipField def DecodeItem ( buffer , pos , end , message , field_dict ) : message_set_item_start = pos type_id = - 1 message_start = - 1 message_end = - 1 # Technically, type_id and message can appear in any order, so we need # a little loop here. while 1 : ( tag_bytes , pos ) = local_ReadTag ( buffer , pos ) if tag_bytes == type_id_tag_bytes : ( type_id , pos ) = local_DecodeVarint ( buffer , pos ) elif tag_bytes == message_tag_bytes : ( size , message_start ) = local_DecodeVarint ( buffer , pos ) pos = message_end = message_start + size elif tag_bytes == item_end_tag_bytes : break else : pos = SkipField ( buffer , pos , end , tag_bytes ) if pos == - 1 : raise _DecodeError ( 'Missing group end tag.' ) if pos > end : raise _DecodeError ( 'Truncated message.' ) if type_id == - 1 : raise _DecodeError ( 'MessageSet item missing type_id.' ) if message_start == - 1 : raise _DecodeError ( 'MessageSet item missing message.' ) extension = extensions_by_number . get ( type_id ) if extension is not None : value = field_dict . get ( extension ) if value is None : value = field_dict . setdefault ( extension , extension . message_type . _concrete_class ( ) ) if value . _InternalParse ( buffer , message_start , message_end ) != message_end : # The only reason _InternalParse would return early is if it encountered # an end-group tag. raise _DecodeError ( 'Unexpected end-group tag.' ) else : if not message . _unknown_fields : message . _unknown_fields = [ ] message . _unknown_fields . append ( ( MESSAGE_SET_ITEM_TAG , buffer [ message_set_item_start : pos ] ) ) return pos return DecodeItem
4331	def loudness ( self , gain_db = - 10.0 , reference_level = 65.0 ) : if not is_number ( gain_db ) : raise ValueError ( 'gain_db must be a number.' ) if not is_number ( reference_level ) : raise ValueError ( 'reference_level must be a number' ) if reference_level > 75 or reference_level < 50 : raise ValueError ( 'reference_level must be between 50 and 75' ) effect_args = [ 'loudness' , '{:f}' . format ( gain_db ) , '{:f}' . format ( reference_level ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'loudness' ) return self
12552	def write_mhd_file ( filename , data , shape = None , meta_dict = None ) : # check its extension ext = get_extension ( filename ) fname = op . basename ( filename ) if ext != '.mhd' or ext != '.raw' : mhd_filename = fname + '.mhd' raw_filename = fname + '.raw' elif ext == '.mhd' : mhd_filename = fname raw_filename = remove_ext ( fname ) + '.raw' elif ext == '.raw' : mhd_filename = remove_ext ( fname ) + '.mhd' raw_filename = fname else : raise ValueError ( '`filename` extension {} from {} is not recognised. ' 'Expected .mhd or .raw.' . format ( ext , filename ) ) # default values if meta_dict is None : meta_dict = { } if shape is None : shape = data . shape # prepare the default header meta_dict [ 'ObjectType' ] = meta_dict . get ( 'ObjectType' , 'Image' ) meta_dict [ 'BinaryData' ] = meta_dict . get ( 'BinaryData' , 'True' ) meta_dict [ 'BinaryDataByteOrderMSB' ] = meta_dict . get ( 'BinaryDataByteOrderMSB' , 'False' ) meta_dict [ 'ElementType' ] = meta_dict . get ( 'ElementType' , NUMPY_TO_MHD_TYPE [ data . dtype . type ] ) meta_dict [ 'NDims' ] = meta_dict . get ( 'NDims' , str ( len ( shape ) ) ) meta_dict [ 'DimSize' ] = meta_dict . get ( 'DimSize' , ' ' . join ( [ str ( i ) for i in shape ] ) ) meta_dict [ 'ElementDataFile' ] = meta_dict . get ( 'ElementDataFile' , raw_filename ) # target files mhd_filename = op . join ( op . dirname ( filename ) , mhd_filename ) raw_filename = op . join ( op . dirname ( filename ) , raw_filename ) # write the header write_meta_header ( mhd_filename , meta_dict ) # write the data dump_raw_data ( raw_filename , data ) return mhd_filename , raw_filename
5833	def create_ml_configuration_from_datasets ( self , dataset_ids ) : available_columns = self . search_template_client . get_available_columns ( dataset_ids ) # Create a search template from dataset ids search_template = self . search_template_client . create ( dataset_ids , available_columns ) return self . create_ml_configuration ( search_template , available_columns , dataset_ids )
6932	def colormagdiagram_cplist ( cplist , outpkl , color_mag1 = [ 'gaiamag' , 'sdssg' ] , color_mag2 = [ 'kmag' , 'kmag' ] , yaxis_mag = [ 'gaia_absmag' , 'rpmj' ] ) : # first, we'll collect all of the info cplist_objectids = [ ] cplist_mags = [ ] cplist_colors = [ ] for cpf in cplist : cpd = _read_checkplot_picklefile ( cpf ) cplist_objectids . append ( cpd [ 'objectid' ] ) thiscp_mags = [ ] thiscp_colors = [ ] for cm1 , cm2 , ym in zip ( color_mag1 , color_mag2 , yaxis_mag ) : if ( ym in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ ym ] is not None ) : thiscp_mags . append ( cpd [ 'objectinfo' ] [ ym ] ) else : thiscp_mags . append ( np . nan ) if ( cm1 in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ cm1 ] is not None and cm2 in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ cm2 ] is not None ) : thiscp_colors . append ( cpd [ 'objectinfo' ] [ cm1 ] - cpd [ 'objectinfo' ] [ cm2 ] ) else : thiscp_colors . append ( np . nan ) cplist_mags . append ( thiscp_mags ) cplist_colors . append ( thiscp_colors ) # convert these to arrays cplist_objectids = np . array ( cplist_objectids ) cplist_mags = np . array ( cplist_mags ) cplist_colors = np . array ( cplist_colors ) # prepare the outdict cmddict = { 'objectids' : cplist_objectids , 'mags' : cplist_mags , 'colors' : cplist_colors , 'color_mag1' : color_mag1 , 'color_mag2' : color_mag2 , 'yaxis_mag' : yaxis_mag } # save the pickled figure and dict for fast retrieval later with open ( outpkl , 'wb' ) as outfd : pickle . dump ( cmddict , outfd , pickle . HIGHEST_PROTOCOL ) plt . close ( 'all' ) return cmddict
3502	def assess_products ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : warn ( 'use assess_component instead' , DeprecationWarning ) return assess_component ( model , reaction , 'products' , flux_coefficient_cutoff , solver )
8692	def init ( self ) : # ignore 400 (IndexAlreadyExistsException) when creating an index self . es . indices . create ( index = self . params [ 'index' ] , ignore = 400 )
8086	def strokewidth ( self , w = None ) : if w is not None : self . _canvas . strokewidth = w else : return self . _canvas . strokewidth
12301	def instantiate ( repo , validator_name = None , filename = None , rulesfiles = None ) : default_validators = repo . options . get ( 'validator' , { } ) validators = { } if validator_name is not None : # Handle the case validator is specified.. if validator_name in default_validators : validators = { validator_name : default_validators [ validator_name ] } else : validators = { validator_name : { 'files' : [ ] , 'rules' : { } , 'rules-files' : [ ] } } else : validators = default_validators #========================================= # Insert the file names #========================================= if filename is not None : matching_files = repo . find_matching_files ( [ filename ] ) if len ( matching_files ) == 0 : print ( "Filename could not be found" , filename ) raise Exception ( "Invalid filename pattern" ) for v in validators : validators [ v ] [ 'files' ] = matching_files else : # Instantiate the files from the patterns specified for v in validators : if 'files' not in validators [ v ] : validators [ v ] [ 'files' ] = [ ] elif len ( validators [ v ] [ 'files' ] ) > 0 : matching_files = repo . find_matching_files ( validators [ v ] [ 'files' ] ) validators [ v ] [ 'files' ] = matching_files #========================================= # Insert the rules files.. #========================================= if rulesfiles is not None : # Command lines... matching_files = repo . find_matching_files ( [ rulesfiles ] ) if len ( matching_files ) == 0 : print ( "Could not find matching rules files ({}) for {}" . format ( rulesfiles , v ) ) raise Exception ( "Invalid rules" ) for v in validators : validators [ v ] [ 'rules-files' ] = matching_files else : # Instantiate the files from the patterns specified for v in validators : if 'rules-files' not in validators [ v ] : validators [ v ] [ 'rules-files' ] = [ ] else : rulesfiles = validators [ v ] [ 'rules-files' ] matching_files = repo . find_matching_files ( rulesfiles ) validators [ v ] [ 'rules-files' ] = matching_files return validators
9591	def set_window_size ( self , width , height , window_handle = 'current' ) : self . _execute ( Command . SET_WINDOW_SIZE , { 'width' : int ( width ) , 'height' : int ( height ) , 'window_handle' : window_handle } )
11673	def make_stacked ( self ) : if self . stacked : return self . _boundaries = bounds = np . r_ [ 0 , np . cumsum ( self . n_pts ) ] self . stacked_features = stacked = np . vstack ( self . features ) self . features = np . array ( [ stacked [ bounds [ i - 1 ] : bounds [ i ] ] for i in xrange ( 1 , len ( bounds ) ) ] , dtype = object ) self . stacked = True
4922	def retrieve ( self , request , pk = None ) : # pylint: disable=invalid-name catalog_api = CourseCatalogApiClient ( request . user ) catalog = catalog_api . get_catalog ( pk ) self . ensure_data_exists ( request , catalog , error_message = ( "Unable to fetch API response for given catalog from endpoint '/catalog/{pk}/'. " "The resource you are looking for does not exist." . format ( pk = pk ) ) ) serializer = self . serializer_class ( catalog ) return Response ( serializer . data )
1410	def filter_bolts ( table , header ) : bolts_info = [ ] for row in table : if row [ 0 ] == 'bolt' : bolts_info . append ( row ) return bolts_info , header
5506	def screenshot ( url , * args , * * kwargs ) : phantomscript = os . path . join ( os . path . dirname ( __file__ ) , 'take_screenshot.js' ) directory = kwargs . get ( 'save_dir' , '/tmp' ) image_name = kwargs . get ( 'image_name' , None ) or _image_name_from_url ( url ) ext = kwargs . get ( 'format' , 'png' ) . lower ( ) save_path = os . path . join ( directory , image_name ) + '.' + ext crop_to_visible = kwargs . get ( 'crop_to_visible' , False ) cmd_args = [ 'phantomjs' , '--ssl-protocol=any' , phantomscript , url , '--width' , str ( kwargs [ 'width' ] ) , '--height' , str ( kwargs [ 'height' ] ) , '--useragent' , str ( kwargs [ 'user_agent' ] ) , '--dir' , directory , '--ext' , ext , '--name' , str ( image_name ) , ] if crop_to_visible : cmd_args . append ( '--croptovisible' ) # TODO: # - quality # - renderafter # - maxexecutiontime # - resourcetimeout output = subprocess . Popen ( cmd_args , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] return Screenshot ( save_path , directory , image_name + '.' + ext , ext )
1695	def filter ( self , filter_function ) : from heronpy . streamlet . impl . filterbolt import FilterStreamlet filter_streamlet = FilterStreamlet ( filter_function , self ) self . _add_child ( filter_streamlet ) return filter_streamlet
7924	def is_ipv6_available ( ) : try : socket . socket ( socket . AF_INET6 ) . close ( ) except ( socket . error , AttributeError ) : return False return True
13475	def start ( self ) : assert not self . has_started ( ) , "called start() on an active GeventLoop" self . _stop_event = Event ( ) # note that we don't use safe_greenlets.spawn because we take care of it in _loop by ourselves self . _greenlet = gevent . spawn ( self . _loop )
8723	def display ( content ) : if isinstance ( content , gp . GPServer ) : IPython . display . display ( GPAuthWidget ( content ) ) elif isinstance ( content , gp . GPTask ) : IPython . display . display ( GPTaskWidget ( content ) ) elif isinstance ( content , gp . GPJob ) : IPython . display . display ( GPJobWidget ( content ) ) else : IPython . display . display ( content )
10257	def get_causal_sink_nodes ( graph : BELGraph , func ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_sink ( graph , node ) }
2440	def add_annotator ( self , doc , annotator ) : # Each annotator marks the start of a new annotation object. # FIXME: this state does not make sense self . reset_annotations ( ) if validations . validate_annotator ( annotator ) : doc . add_annotation ( annotation . Annotation ( annotator = annotator ) ) return True else : raise SPDXValueError ( 'Annotation::Annotator' )
3527	def piwik ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return PiwikNode ( )
10419	def group_dict_set ( iterator : Iterable [ Tuple [ A , B ] ] ) -> Mapping [ A , Set [ B ] ] : d = defaultdict ( set ) for key , value in iterator : d [ key ] . add ( value ) return dict ( d )
2149	def modify ( self , pk = None , create_on_missing = False , * * kwargs ) : # Create the resource if needed. if pk is None and create_on_missing : try : self . get ( * * copy . deepcopy ( kwargs ) ) except exc . NotFound : return self . create ( * * kwargs ) # Modify everything except notification type and configuration config_item = self . _separate ( kwargs ) notification_type = kwargs . pop ( 'notification_type' , None ) debug . log ( 'Modify everything except notification type and' ' configuration' , header = 'details' ) part_result = super ( Resource , self ) . modify ( pk = pk , create_on_missing = create_on_missing , * * kwargs ) # Modify notification type and configuration if notification_type is None or notification_type == part_result [ 'notification_type' ] : for item in part_result [ 'notification_configuration' ] : if item not in config_item or not config_item [ item ] : to_add = part_result [ 'notification_configuration' ] [ item ] if not ( to_add == '$encrypted$' and item in Resource . encrypted_fields ) : config_item [ item ] = to_add if notification_type is None : kwargs [ 'notification_type' ] = part_result [ 'notification_type' ] else : kwargs [ 'notification_type' ] = notification_type self . _configuration ( kwargs , config_item ) debug . log ( 'Modify notification type and configuration' , header = 'details' ) result = super ( Resource , self ) . modify ( pk = pk , create_on_missing = create_on_missing , * * kwargs ) # Update 'changed' field to give general changed info if 'changed' in result and 'changed' in part_result : result [ 'changed' ] = result [ 'changed' ] or part_result [ 'changed' ] return result
8812	def get_used_ips ( session , * * kwargs ) : LOG . debug ( "Getting used IPs..." ) with session . begin ( ) : query = session . query ( models . Subnet . segment_id , func . count ( models . IPAddress . address ) ) query = query . group_by ( models . Subnet . segment_id ) query = _filter ( query , * * kwargs ) reuse_window = timeutils . utcnow ( ) - datetime . timedelta ( seconds = cfg . CONF . QUARK . ipam_reuse_after ) # NOTE(asadoughi): This is an outer join instead of a regular join # to include subnets with zero IP addresses in the database. query = query . outerjoin ( models . IPAddress , and_ ( models . Subnet . id == models . IPAddress . subnet_id , or_ ( not_ ( models . IPAddress . lock_id . is_ ( None ) ) , models . IPAddress . _deallocated . is_ ( None ) , models . IPAddress . _deallocated == 0 , models . IPAddress . deallocated_at > reuse_window ) ) ) query = query . outerjoin ( models . IPPolicyCIDR , and_ ( models . Subnet . ip_policy_id == models . IPPolicyCIDR . ip_policy_id , models . IPAddress . address >= models . IPPolicyCIDR . first_ip , models . IPAddress . address <= models . IPPolicyCIDR . last_ip ) ) # NOTE(asadoughi): (address is allocated) OR # (address is deallocated and not inside subnet's IP policy) query = query . filter ( or_ ( models . IPAddress . _deallocated . is_ ( None ) , models . IPAddress . _deallocated == 0 , models . IPPolicyCIDR . id . is_ ( None ) ) ) ret = ( ( segment_id , address_count ) for segment_id , address_count in query . all ( ) ) return dict ( ret )
5505	def save ( url , * args , * * kwargs ) : device = heimdallDevice ( kwargs . get ( 'device' , None ) ) kwargs [ 'width' ] = kwargs . get ( 'width' , None ) or device . width kwargs [ 'height' ] = kwargs . get ( 'height' , None ) or device . height kwargs [ 'user_agent' ] = kwargs . get ( 'user_agent' , None ) or device . user_agent screenshot_image = screenshot ( url , * * kwargs ) if kwargs . get ( 'optimize' ) : image = Image . open ( screenshot_image . path ) image . save ( screenshot_image . path , optimize = True ) return screenshot_image
9739	def get_2d_markers ( self , component_info = None , data = None , component_position = None , index = None ) : return self . _get_2d_markers ( data , component_info , component_position , index = index )
8595	def create_group ( self , group ) : data = json . dumps ( self . _create_group_dict ( group ) ) response = self . _perform_request ( url = '/um/groups' , method = 'POST' , data = data ) return response
2762	def get_all_certificates ( self ) : data = self . get_data ( "certificates" ) certificates = list ( ) for jsoned in data [ 'certificates' ] : cert = Certificate ( * * jsoned ) cert . token = self . token certificates . append ( cert ) return certificates
3378	def assert_optimal ( model , message = 'optimization failed' ) : status = model . solver . status if status != OPTIMAL : exception_cls = OPTLANG_TO_EXCEPTIONS_DICT . get ( status , OptimizationError ) raise exception_cls ( "{} ({})" . format ( message , status ) )
11706	def reproduce_sexually ( self , egg_donor , sperm_donor ) : egg_word = random . choice ( egg_donor . genome ) egg = self . generate_gamete ( egg_word ) sperm_word = random . choice ( sperm_donor . genome ) sperm = self . generate_gamete ( sperm_word ) self . genome = list ( set ( egg + sperm ) ) # Eliminate duplicates self . parents = [ egg_donor . name , sperm_donor . name ] self . generation = max ( egg_donor . generation , sperm_donor . generation ) + 1 sum_ = egg_donor . divinity + sperm_donor . divinity self . divinity = int ( npchoice ( divinities , 1 , p = p_divinity [ sum_ ] ) [ 0 ] )
213	def from_0to1 ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) : heatmaps = HeatmapsOnImage ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) heatmaps . min_value = min_value heatmaps . max_value = max_value return heatmaps
5994	def set_colorbar ( cb_ticksize , cb_fraction , cb_pad , cb_tick_values , cb_tick_labels ) : if cb_tick_values is None and cb_tick_labels is None : cb = plt . colorbar ( fraction = cb_fraction , pad = cb_pad ) elif cb_tick_values is not None and cb_tick_labels is not None : cb = plt . colorbar ( fraction = cb_fraction , pad = cb_pad , ticks = cb_tick_values ) cb . ax . set_yticklabels ( cb_tick_labels ) else : raise exc . PlottingException ( 'Only 1 entry of cb_tick_values or cb_tick_labels was input. You must either supply' 'both the values and labels, or neither.' ) cb . ax . tick_params ( labelsize = cb_ticksize )
10374	def get_cutoff ( value : float , cutoff : Optional [ float ] = None ) -> int : cutoff = cutoff if cutoff is not None else 0 if value > cutoff : return 1 if value < ( - 1 * cutoff ) : return - 1 return 0
11123	def remove_directory ( self , relativePath , removeFromSystem = False ) : # get parent directory info relativePath = os . path . normpath ( relativePath ) parentDirInfoDict , errorMessage = self . get_parent_directory_info ( relativePath ) assert parentDirInfoDict is not None , errorMessage # split path path , name = os . path . split ( relativePath ) if dict . __getitem__ ( parentDirInfoDict , 'directories' ) . get ( name , None ) is None : raise Exception ( "'%s' is not a registered directory in repository relative path '%s'" % ( name , path ) ) # remove from system if removeFromSystem : # remove files for rp in self . walk_files_relative_path ( relativePath = relativePath ) : ap = os . path . join ( self . __path , relativePath , rp ) if not os . path . isfile ( ap ) : continue if not os . path . exists ( ap ) : continue if os . path . isfile ( ap ) : os . remove ( ap ) # remove directories for rp in self . walk_directories_relative_path ( relativePath = relativePath ) : ap = os . path . join ( self . __path , relativePath , rp ) if not os . path . isdir ( ap ) : continue if not os . path . exists ( ap ) : continue if not len ( os . listdir ( ap ) ) : os . rmdir ( ap ) # pop directory from repo dict . __getitem__ ( parentDirInfoDict , 'directories' ) . pop ( name , None ) ap = os . path . join ( self . __path , relativePath ) if not os . path . isdir ( ap ) : if not len ( os . listdir ( ap ) ) : os . rmdir ( ap ) # save repository self . save ( )
3244	def get_security_group ( security_group , flags = FLAGS . ALL , * * kwargs ) : result = registry . build_out ( flags , start_with = security_group , pass_datastructure = True , * * kwargs ) result . pop ( 'security_group_rules' , [ ] ) return result
3441	def rename_genes ( cobra_model , rename_dict ) : recompute_reactions = set ( ) # need to recomptue related genes remove_genes = [ ] for old_name , new_name in iteritems ( rename_dict ) : # undefined if there a value matches a different key # because dict is unordered try : gene_index = cobra_model . genes . index ( old_name ) except ValueError : gene_index = None old_gene_present = gene_index is not None new_gene_present = new_name in cobra_model . genes if old_gene_present and new_gene_present : old_gene = cobra_model . genes . get_by_id ( old_name ) # Added in case not renaming some genes: if old_gene is not cobra_model . genes . get_by_id ( new_name ) : remove_genes . append ( old_gene ) recompute_reactions . update ( old_gene . _reaction ) elif old_gene_present and not new_gene_present : # rename old gene to new gene gene = cobra_model . genes [ gene_index ] # trick DictList into updating index cobra_model . genes . _dict . pop ( gene . id ) # ugh gene . id = new_name cobra_model . genes [ gene_index ] = gene elif not old_gene_present and new_gene_present : pass else : # if not old gene_present and not new_gene_present # the new gene's _model will be set by repair # This would add genes from rename_dict # that are not associated with a rxn # cobra_model.genes.append(Gene(new_name)) pass cobra_model . repair ( ) class Renamer ( NodeTransformer ) : def visit_Name ( self , node ) : node . id = rename_dict . get ( node . id , node . id ) return node gene_renamer = Renamer ( ) for rxn , rule in iteritems ( get_compiled_gene_reaction_rules ( cobra_model ) ) : if rule is not None : rxn . _gene_reaction_rule = ast2str ( gene_renamer . visit ( rule ) ) for rxn in recompute_reactions : rxn . gene_reaction_rule = rxn . _gene_reaction_rule for i in remove_genes : cobra_model . genes . remove ( i )
7649	def _open ( name_or_fdesc , mode = 'r' , fmt = 'auto' ) : open_map = { 'jams' : open , 'json' : open , 'jamz' : gzip . open , 'gz' : gzip . open } # If we've been given an open descriptor, do the right thing if hasattr ( name_or_fdesc , 'read' ) or hasattr ( name_or_fdesc , 'write' ) : yield name_or_fdesc elif isinstance ( name_or_fdesc , six . string_types ) : # Infer the opener from the extension if fmt == 'auto' : _ , ext = os . path . splitext ( name_or_fdesc ) # Pull off the extension separator ext = ext [ 1 : ] else : ext = fmt try : ext = ext . lower ( ) # Force text mode if we're using gzip if ext in [ 'jamz' , 'gz' ] and 't' not in mode : mode = '{:s}t' . format ( mode ) with open_map [ ext ] ( name_or_fdesc , mode = mode ) as fdesc : yield fdesc except KeyError : raise ParameterError ( 'Unknown JAMS extension ' 'format: "{:s}"' . format ( ext ) ) else : # Don't know how to handle this. Raise a parameter error raise ParameterError ( 'Invalid filename or ' 'descriptor: {}' . format ( name_or_fdesc ) )
3565	def write_value ( self , value , write_type = 0 ) : data = NSData . dataWithBytes_length_ ( value , len ( value ) ) self . _device . _peripheral . writeValue_forCharacteristic_type_ ( data , self . _characteristic , write_type )
13450	def field_value ( self , admin_model , instance , field_name ) : _ , _ , value = lookup_field ( field_name , instance , admin_model ) return value
4468	def deserialize ( encoded , * * kwargs ) : params = jsonpickle . decode ( encoded , * * kwargs ) return __reconstruct ( params )
5809	def parse_handshake_messages ( data ) : pointer = 0 data_len = len ( data ) while pointer < data_len : length = int_from_bytes ( data [ pointer + 1 : pointer + 4 ] ) yield ( data [ pointer : pointer + 1 ] , data [ pointer + 4 : pointer + 4 + length ] ) pointer += 4 + length
13309	def fmt ( a , b ) : return 100 * np . min ( [ a , b ] , axis = 0 ) . sum ( ) / np . max ( [ a , b ] , axis = 0 ) . sum ( )
5165	def __intermediate_proto ( self , interface , address ) : # proto defaults to static address_proto = address . pop ( 'proto' , 'static' ) if 'proto' not in interface : return address_proto else : # allow override on interface level return interface . pop ( 'proto' )
10746	def get_default_fields ( self ) : field_names = self . _meta . get_all_field_names ( ) if 'id' in field_names : field_names . remove ( 'id' ) return field_names
7110	def fit ( self , X , y ) : trainer = pycrfsuite . Trainer ( verbose = True ) for xseq , yseq in zip ( X , y ) : trainer . append ( xseq , yseq ) trainer . set_params ( self . params ) if self . filename : filename = self . filename else : filename = 'model.tmp' trainer . train ( filename ) tagger = pycrfsuite . Tagger ( ) tagger . open ( filename ) self . estimator = tagger
3934	def _get_session_cookies ( session , access_token ) : headers = { 'Authorization' : 'Bearer {}' . format ( access_token ) } try : r = session . get ( ( 'https://accounts.google.com/accounts/OAuthLogin' '?source=hangups&issueuberauth=1' ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'OAuthLogin request failed: {}' . format ( e ) ) uberauth = r . text try : r = session . get ( ( 'https://accounts.google.com/MergeSession?' 'service=mail&' 'continue=http://www.google.com&uberauth={}' ) . format ( uberauth ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'MergeSession request failed: {}' . format ( e ) ) cookies = session . cookies . get_dict ( domain = '.google.com' ) if cookies == { } : raise GoogleAuthError ( 'Failed to find session cookies' ) return cookies
3811	def get_request_header ( self ) : # resource is allowed to be null if it's not available yet (the Chrome # client does this for the first getentitybyid call) if self . _client_id is not None : self . _request_header . client_identifier . resource = self . _client_id return self . _request_header
9215	def t_istringapostrophe_css_string ( self , t ) : t . lexer . lineno += t . value . count ( '\n' ) return t
3621	def get_adapter ( self , model ) : if not self . is_registered ( model ) : raise RegistrationError ( '{} is not registered with Algolia engine' . format ( model ) ) return self . __registered_models [ model ]
13773	def init_logs ( path = None , target = None , logger_name = 'root' , level = logging . DEBUG , maxBytes = 1 * 1024 * 1024 , backupCount = 5 , application_name = 'default' , server_hostname = None , fields = None ) : log_file = os . path . abspath ( os . path . join ( path , target ) ) logger = logging . getLogger ( logger_name ) logger . setLevel ( level ) handler = logging . handlers . RotatingFileHandler ( log_file , maxBytes = maxBytes , backupCount = backupCount ) handler . setLevel ( level ) handler . setFormatter ( JsonFormatter ( application_name = application_name , server_hostname = server_hostname , fields = fields ) ) logger . addHandler ( handler )
2411	def initialize_dictionaries ( self , p_set ) : success = False if not ( hasattr ( p_set , '_type' ) ) : error_message = "needs to be an essay set of the train type." log . exception ( error_message ) raise util_functions . InputError ( p_set , error_message ) if not ( p_set . _type == "train" ) : error_message = "needs to be an essay set of the train type." log . exception ( error_message ) raise util_functions . InputError ( p_set , error_message ) div_length = len ( p_set . _essay_sets ) if div_length == 0 : div_length = 1 #Ensures that even with a large amount of input textual features, training time stays reasonable max_feats2 = int ( math . floor ( 200 / div_length ) ) for i in xrange ( 0 , len ( p_set . _essay_sets ) ) : self . _extractors . append ( FeatureExtractor ( ) ) self . _extractors [ i ] . initialize_dictionaries ( p_set . _essay_sets [ i ] , max_feats2 = max_feats2 ) self . _initialized = True success = True return success
6893	def parallel_starfeatures ( lclist , outdir , lc_catalog_pickle , neighbor_radius_arcsec , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None , nworkers = NCPUS ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None # make sure to make the output directory if it doesn't exist if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : lclist = lclist [ : maxobjects ] # read in the kdtree pickle with open ( lc_catalog_pickle , 'rb' ) as infd : kdt_dict = pickle . load ( infd ) kdt = kdt_dict [ 'kdtree' ] objlist = kdt_dict [ 'objects' ] [ 'objectid' ] objlcfl = kdt_dict [ 'objects' ] [ 'lcfname' ] tasks = [ ( x , outdir , kdt , objlist , objlcfl , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat ) for x in lclist ] with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( _starfeatures_worker , tasks ) results = [ x for x in resultfutures ] resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( lclist , results ) } return resdict
7212	def layers ( self ) : layers = [ self . _layer_def ( style ) for style in self . styles ] return layers
2650	def send ( self , message_type , task_id , message ) : x = 0 try : buffer = pickle . dumps ( ( self . source_id , # Identifier for manager int ( time . time ( ) ) , # epoch timestamp message_type , message ) ) except Exception as e : print ( "Exception during pickling {}" . format ( e ) ) return try : x = self . sock . sendto ( buffer , ( self . ip , self . port ) ) except socket . timeout : print ( "Could not send message within timeout limit" ) return False return x
12652	def generate_config ( output_directory ) : if not op . isdir ( output_directory ) : os . makedirs ( output_directory ) config_file = op . join ( output_directory , "config.ini" ) open_file = open ( config_file , "w" ) open_file . write ( "[BOOL]\nManualNIfTIConv=0\n" ) open_file . close ( ) return config_file
5743	def update_running_pids ( old_procs ) : new_procs = [ ] for proc in old_procs : if proc . poll ( ) is None and check_pid ( proc . pid ) : publisher . debug ( str ( proc . pid ) + ' is alive' ) new_procs . append ( proc ) else : try : publisher . debug ( str ( proc . pid ) + ' is gone' ) os . kill ( proc . pid , signal . SIGKILL ) except : # the process is just already gone pass return new_procs
3039	def has_scopes ( self , scopes ) : scopes = _helpers . string_to_scopes ( scopes ) return set ( scopes ) . issubset ( self . scopes )
13280	def child_end_handler ( self , scache ) : desc = self . desc desc_level = scache . desc_level breadth = desc_level . __len__ ( ) desc [ 'breadth' ] = breadth desc [ 'breadth_path' ] . append ( breadth ) desc_level . append ( desc )
6240	def render_lights_debug ( self , camera_matrix , projection ) : self . ctx . enable ( moderngl . BLEND ) self . ctx . blend_func = moderngl . SRC_ALPHA , moderngl . ONE_MINUS_SRC_ALPHA for light in self . point_lights : m_mv = matrix44 . multiply ( light . matrix , camera_matrix ) light_size = light . radius self . debug_shader [ "m_proj" ] . write ( projection . tobytes ( ) ) self . debug_shader [ "m_mv" ] . write ( m_mv . astype ( 'f4' ) . tobytes ( ) ) self . debug_shader [ "size" ] . value = light_size self . unit_cube . render ( self . debug_shader , mode = moderngl . LINE_STRIP ) self . ctx . disable ( moderngl . BLEND )
11121	def get_file_relative_path_by_name ( self , name , skip = 0 ) : if skip is None : paths = [ ] else : paths = None for path , info in self . walk_files_info ( ) : _ , n = os . path . split ( path ) if n == name : if skip is None : paths . append ( path ) elif skip > 0 : skip -= 1 else : paths = path break return paths
977	def _newRepresentationOK ( self , newRep , newIndex ) : if newRep . size != self . w : return False if ( newIndex < self . minIndex - 1 ) or ( newIndex > self . maxIndex + 1 ) : raise ValueError ( "newIndex must be within one of existing indices" ) # A binary representation of newRep. We will use this to test containment newRepBinary = numpy . array ( [ False ] * self . n ) newRepBinary [ newRep ] = True # Midpoint midIdx = self . _maxBuckets / 2 # Start by checking the overlap at minIndex runningOverlap = self . _countOverlap ( self . bucketMap [ self . minIndex ] , newRep ) if not self . _overlapOK ( self . minIndex , newIndex , overlap = runningOverlap ) : return False # Compute running overlaps all the way to the midpoint for i in range ( self . minIndex + 1 , midIdx + 1 ) : # This is the bit that is going to change newBit = ( i - 1 ) % self . w # Update our running overlap if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 # Verify our rules if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False # At this point, runningOverlap contains the overlap for midIdx # Compute running overlaps all the way to maxIndex for i in range ( midIdx + 1 , self . maxIndex + 1 ) : # This is the bit that is going to change newBit = i % self . w # Update our running overlap if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 # Verify our rules if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False return True
11784	def check_example ( self , example ) : if self . values : for a in self . attrs : if example [ a ] not in self . values [ a ] : raise ValueError ( 'Bad value %s for attribute %s in %s' % ( example [ a ] , self . attrnames [ a ] , example ) )
11999	def _decode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : verify_signature = data [ - algorithm [ 'hash_size' ] : ] data = data [ : - algorithm [ 'hash_size' ] ] signature = self . _hmac_generate ( data , algorithm , key ) if not const_equal ( verify_signature , signature ) : raise Exception ( 'Invalid signature' ) return data elif algorithm [ 'type' ] == 'aes' : return self . _aes_decrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . loads ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . _zlib_decompress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
7353	def NetMHC ( alleles , default_peptide_lengths = [ 9 ] , program_name = "netMHC" ) : # run NetMHC's help command and parse discriminating substrings out of # the resulting str output with open ( os . devnull , 'w' ) as devnull : help_output = check_output ( [ program_name , "-h" ] , stderr = devnull ) help_output_str = help_output . decode ( "ascii" , "ignore" ) substring_to_netmhc_class = { "-listMHC" : NetMHC4 , "--Alleles" : NetMHC3 , } successes = [ ] for substring , netmhc_class in substring_to_netmhc_class . items ( ) : if substring in help_output_str : successes . append ( netmhc_class ) if len ( successes ) > 1 : raise SystemError ( "Command %s is valid for multiple NetMHC versions. " "This is likely an mhctools bug." % program_name ) if len ( successes ) == 0 : raise SystemError ( "Command %s is not a valid way of calling any NetMHC software." % program_name ) netmhc_class = successes [ 0 ] return netmhc_class ( alleles = alleles , default_peptide_lengths = default_peptide_lengths , program_name = program_name )
5088	def ecommerce_coupon_url ( self , instance ) : if not instance . entitlement_id : return "N/A" return format_html ( '<a href="{base_url}/coupons/{id}" target="_blank">View coupon "{id}" details</a>' , base_url = settings . ECOMMERCE_PUBLIC_URL_ROOT , id = instance . entitlement_id )
4542	def apply ( self , function ) : for cut in self . cuts : value = self . read ( cut ) function ( value ) self . write ( cut , value )
756	def createExperimentInferenceDir ( cls , experimentDir ) : path = cls . getExperimentInferenceDirPath ( experimentDir ) cls . makeDirectory ( path ) return path
4466	def __reconstruct ( params ) : if isinstance ( params , dict ) : if '__class__' in params : cls = params [ '__class__' ] data = __reconstruct ( params [ 'params' ] ) return cls ( * * data ) else : data = dict ( ) for key , value in six . iteritems ( params ) : data [ key ] = __reconstruct ( value ) return data elif isinstance ( params , ( list , tuple ) ) : return [ __reconstruct ( v ) for v in params ] else : return params
3066	def _apply_user_agent ( headers , user_agent ) : if user_agent is not None : if 'user-agent' in headers : headers [ 'user-agent' ] = ( user_agent + ' ' + headers [ 'user-agent' ] ) else : headers [ 'user-agent' ] = user_agent return headers
8842	def indent ( self ) : if not self . tab_always_indent : super ( PyIndenterMode , self ) . indent ( ) else : cursor = self . editor . textCursor ( ) assert isinstance ( cursor , QtGui . QTextCursor ) if cursor . hasSelection ( ) : self . indent_selection ( cursor ) else : # simply insert indentation at the cursor position tab_len = self . editor . tab_length cursor . beginEditBlock ( ) if self . editor . use_spaces_instead_of_tabs : cursor . insertText ( tab_len * " " ) else : cursor . insertText ( '\t' ) cursor . endEditBlock ( ) self . editor . setTextCursor ( cursor )
10247	def enrich_pubmed_citations ( graph : BELGraph , manager : Manager ) -> Set [ str ] : pmids = get_pubmed_identifiers ( graph ) pmid_data , errors = get_citations_by_pmids ( manager = manager , pmids = pmids ) for u , v , k in filter_edges ( graph , has_pubmed ) : pmid = graph [ u ] [ v ] [ k ] [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) if pmid not in pmid_data : log . warning ( 'Missing data for PubMed identifier: %s' , pmid ) errors . add ( pmid ) continue graph [ u ] [ v ] [ k ] [ CITATION ] . update ( pmid_data [ pmid ] ) return errors
10739	def log_calls ( function ) : def wrapper ( self , * args , * * kwargs ) : self . log . log ( group = function . __name__ , message = 'Enter' ) function ( self , * args , * * kwargs ) self . log . log ( group = function . __name__ , message = 'Exit' ) return wrapper
6517	def execute_reports ( config , path , collector , on_report_finish = None , output_file = None ) : reports = get_reports ( ) for report in config . get ( 'requested_reports' , [ ] ) : if report . get ( 'type' ) and report [ 'type' ] in reports : cfg = config . get ( 'report' , { } ) . get ( report [ 'type' ] , { } ) cfg . update ( report ) reporter = reports [ report [ 'type' ] ] ( cfg , path , output_file = output_file , ) reporter . produce ( collector ) if on_report_finish : on_report_finish ( report )
7675	def pprint_jobject ( obj , * * kwargs ) : obj_simple = { k : v for k , v in six . iteritems ( obj . __json__ ) if v } string = json . dumps ( obj_simple , * * kwargs ) # Suppress braces and quotes string = re . sub ( r'[{}"]' , '' , string ) # Kill trailing commas string = re . sub ( r',\n' , '\n' , string ) # Kill blank lines string = re . sub ( r'^\s*$' , '' , string ) return string
9410	def _create_struct ( data , session ) : out = Struct ( ) for name in data . dtype . names : item = data [ name ] # Extract values that are cells (they are doubly wrapped). if isinstance ( item , np . ndarray ) and item . dtype . kind == 'O' : item = item . squeeze ( ) . tolist ( ) out [ name ] = _extract ( item , session ) return out
4732	def generate_rt_pic ( process_data , para_meter , scale ) : pic_path = para_meter [ 'filename' ] + '.png' plt . figure ( figsize = ( 5.6 * scale , 3.2 * scale ) ) for key in process_data . keys ( ) : plt . plot ( process_data [ key ] [ : , 0 ] , process_data [ key ] [ : , 1 ] , label = str ( key ) ) plt . title ( para_meter [ 'title' ] ) plt . xlabel ( para_meter [ 'x_axis_name' ] ) plt . ylabel ( para_meter [ 'y_axis_name' ] ) plt . legend ( loc = 'upper left' ) plt . savefig ( pic_path ) return pic_path
1368	def register_on_message ( self , msg_builder ) : message = msg_builder ( ) Log . debug ( "In register_on_message(): %s" % message . DESCRIPTOR . full_name ) self . registered_message_map [ message . DESCRIPTOR . full_name ] = msg_builder
11777	def replicated_dataset ( dataset , weights , n = None ) : n = n or len ( dataset . examples ) result = copy . copy ( dataset ) result . examples = weighted_replicate ( dataset . examples , weights , n ) return result
11930	def deploy_blog ( ) : logger . info ( deploy_blog . __doc__ ) # `rsync -aqu path/to/res/* .` call ( 'rsync -aqu ' + join ( dirname ( __file__ ) , 'res' , '*' ) + ' .' , shell = True ) logger . success ( 'Done' ) logger . info ( 'Please edit config.toml to meet your needs' )
12740	def _parse_corporations ( self , datafield , subfield , roles = [ "any" ] ) : if len ( datafield ) != 3 : raise ValueError ( "datafield parameter have to be exactly 3 chars long!" ) if len ( subfield ) != 1 : raise ValueError ( "Bad subfield specification - subield have to be 3 chars long!" ) parsed_corporations = [ ] for corporation in self . get_subfields ( datafield , subfield ) : other_subfields = corporation . other_subfields # check if corporation have at least one of the roles specified in # 'roles' parameter of function if "4" in other_subfields and roles != [ "any" ] : corp_roles = other_subfields [ "4" ] # list of role parameters relevant = any ( map ( lambda role : role in roles , corp_roles ) ) # skip non-relevant corporations if not relevant : continue name = "" place = "" date = "" name = corporation if "c" in other_subfields : place = "," . join ( other_subfields [ "c" ] ) if "d" in other_subfields : date = "," . join ( other_subfields [ "d" ] ) parsed_corporations . append ( Corporation ( name , place , date ) ) return parsed_corporations
8144	def flip ( self , axis = HORIZONTAL ) : if axis == HORIZONTAL : self . img = self . img . transpose ( Image . FLIP_LEFT_RIGHT ) if axis == VERTICAL : self . img = self . img . transpose ( Image . FLIP_TOP_BOTTOM )
3100	def loadfile ( filename , cache = None ) : _SECRET_NAMESPACE = 'oauth2client:secrets#ns' if not cache : return _loadfile ( filename ) obj = cache . get ( filename , namespace = _SECRET_NAMESPACE ) if obj is None : client_type , client_info = _loadfile ( filename ) obj = { client_type : client_info } cache . set ( filename , obj , namespace = _SECRET_NAMESPACE ) return next ( six . iteritems ( obj ) )
11748	def _bundle_exists ( self , path ) : for attached_bundle in self . _attached_bundles : if path == attached_bundle . path : return True return False
2429	def reset_document ( self ) : # FIXME: this state does not make sense self . doc_version_set = False self . doc_comment_set = False self . doc_namespace_set = False self . doc_data_lics_set = False self . doc_name_set = False self . doc_spdx_id_set = False
2864	def readS8 ( self , register ) : result = self . readU8 ( register ) if result > 127 : result -= 256 return result
12542	def get_attributes ( self , attributes , default = '' ) : if isinstance ( attributes , str ) : attributes = [ attributes ] attrs = [ getattr ( self , attr , default ) for attr in attributes ] if len ( attrs ) == 1 : return attrs [ 0 ] return tuple ( attrs )
7834	def make_submit ( self , keep_types = False ) : result = Form ( "submit" ) for field in self . fields : if field . type == "fixed" : continue if not field . values : if field . required : raise ValueError ( "Required field with no value!" ) continue if keep_types : result . add_field ( field . name , field . values , field . type ) else : result . add_field ( field . name , field . values ) return result
7508	def _finalize_stats ( self , ipyclient ) : ## print stats file location: #print(STATSOUT.format(opr(self.files.stats))) ## print finished tree information --------------------- print ( FINALTREES . format ( opr ( self . trees . tree ) ) ) ## print bootstrap information -------------------------- if self . params . nboots : ## get consensus, map values to tree edges, record stats file self . _compute_tree_stats ( ipyclient ) ## print bootstrap info print ( BOOTTREES . format ( opr ( self . trees . cons ) , opr ( self . trees . boots ) ) ) ## print the ASCII tree only if its small if len ( self . samples ) < 20 : if self . params . nboots : wctre = ete3 . Tree ( self . trees . cons , format = 0 ) wctre . ladderize ( ) print ( wctre . get_ascii ( show_internal = True , attributes = [ "dist" , "name" ] ) ) print ( "" ) else : qtre = ete3 . Tree ( self . trees . tree , format = 0 ) qtre . ladderize ( ) #qtre = toytree.tree(self.trees.tree, format=0) #qtre.tree.unroot() print ( qtre . get_ascii ( ) ) print ( "" ) ## print PDF filename & tips ----------------------------- docslink = "https://toytree.readthedocs.io/" citelink = "https://ipyrad.readthedocs.io/tetrad.html" print ( LINKS . format ( docslink , citelink ) )
9579	def read_cell_array ( fd , endian , header ) : array = [ list ( ) for i in range ( header [ 'dims' ] [ 0 ] ) ] for row in range ( header [ 'dims' ] [ 0 ] ) : for col in range ( header [ 'dims' ] [ 1 ] ) : # read the matrix header and array vheader , next_pos , fd_var = read_var_header ( fd , endian ) varray = read_var_array ( fd_var , endian , vheader ) array [ row ] . append ( varray ) # move on to next field fd . seek ( next_pos ) # pack and return the array if header [ 'dims' ] [ 0 ] == 1 : return squeeze ( array [ 0 ] ) return squeeze ( array )
8592	def restore_snapshot ( self , datacenter_id , volume_id , snapshot_id ) : data = { 'snapshotId' : snapshot_id } response = self . _perform_request ( url = '/datacenters/%s/volumes/%s/restore-snapshot' % ( datacenter_id , volume_id ) , method = 'POST-ACTION' , data = urlencode ( data ) ) return response
3183	def update ( self , store_id , data ) : self . store_id = store_id return self . _mc_client . _patch ( url = self . _build_path ( store_id ) , data = data )
13044	def main ( ) : config = Config ( ) pipes_dir = config . get ( 'pipes' , 'directory' ) pipes_config = config . get ( 'pipes' , 'config_file' ) pipes_config_path = os . path . join ( config . config_dir , pipes_config ) if not os . path . exists ( pipes_config_path ) : print_error ( "Please configure the named pipes first" ) return workers = create_pipe_workers ( pipes_config_path , pipes_dir ) if workers : for worker in workers : worker . start ( ) try : for worker in workers : worker . join ( ) except KeyboardInterrupt : print_notification ( "Shutting down" ) for worker in workers : worker . terminate ( ) worker . join ( )
1774	def invalidate_cache ( cpu , address , size ) : cache = cpu . instruction_cache for offset in range ( size ) : if address + offset in cache : del cache [ address + offset ]
7086	def _single_true ( iterable ) : # return True if exactly one true found iterator = iter ( iterable ) # consume from "i" until first true or it's exhausted has_true = any ( iterator ) # carry on consuming until another true value / exhausted has_another_true = any ( iterator ) return has_true and not has_another_true
13168	def children ( self , name = None , reverse = False ) : elems = self . _children if reverse : elems = reversed ( elems ) for elem in elems : if name is None or elem . tagname == name : yield elem
2330	def computeGaussKernel ( x ) : xnorm = np . power ( euclidean_distances ( x , x ) , 2 ) return np . exp ( - xnorm / ( 2.0 ) )
6381	def dist_jaro_winkler ( src , tar , qval = 1 , mode = 'winkler' , long_strings = False , boost_threshold = 0.7 , scaling_factor = 0.1 , ) : return JaroWinkler ( ) . dist ( src , tar , qval , mode , long_strings , boost_threshold , scaling_factor )
6619	def poll ( self ) : finished_procs = [ p for p in self . running_procs if p . poll ( ) is not None ] self . running_procs = collections . deque ( [ p for p in self . running_procs if p not in finished_procs ] ) for proc in finished_procs : stdout , stderr = proc . communicate ( ) ## proc.communicate() returns (stdout, stderr) when ## self.pipe = True. Otherwise they are (None, None) finished_pids = [ p . pid for p in finished_procs ] self . finished_pids . extend ( finished_pids ) logger = logging . getLogger ( __name__ ) messages = 'Running: {}, Finished: {}' . format ( len ( self . running_procs ) , len ( self . finished_pids ) ) logger . info ( messages ) return finished_pids
11481	def _create_folder ( local_folder , parent_folder_id ) : new_folder = session . communicator . create_folder ( session . token , os . path . basename ( local_folder ) , parent_folder_id ) return new_folder [ 'folder_id' ]
10970	def get_group_name ( id_group ) : group = Group . query . get ( id_group ) if group is not None : return group . name
4321	def biquad ( self , b , a ) : if not isinstance ( b , list ) : raise ValueError ( 'b must be a list.' ) if not isinstance ( a , list ) : raise ValueError ( 'a must be a list.' ) if len ( b ) != 3 : raise ValueError ( 'b must be a length 3 list.' ) if len ( a ) != 3 : raise ValueError ( 'a must be a length 3 list.' ) if not all ( [ is_number ( b_val ) for b_val in b ] ) : raise ValueError ( 'all elements of b must be numbers.' ) if not all ( [ is_number ( a_val ) for a_val in a ] ) : raise ValueError ( 'all elements of a must be numbers.' ) effect_args = [ 'biquad' , '{:f}' . format ( b [ 0 ] ) , '{:f}' . format ( b [ 1 ] ) , '{:f}' . format ( b [ 2 ] ) , '{:f}' . format ( a [ 0 ] ) , '{:f}' . format ( a [ 1 ] ) , '{:f}' . format ( a [ 2 ] ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'biquad' ) return self
8939	def _zipped ( self , docs_base ) : with pushd ( docs_base ) : with tempfile . NamedTemporaryFile ( prefix = 'pythonhosted-' , delete = False ) as ziphandle : pass zip_name = shutil . make_archive ( ziphandle . name , 'zip' ) notify . info ( "Uploading {:.1f} MiB from '{}' to '{}'..." . format ( os . path . getsize ( zip_name ) / 1024.0 , zip_name , self . target ) ) with io . open ( zip_name , 'rb' ) as zipread : try : yield zipread finally : os . remove ( ziphandle . name ) os . remove ( ziphandle . name + '.zip' )
13220	def settings ( self ) : stmt = "select {fields} from pg_settings" . format ( fields = ', ' . join ( SETTINGS_FIELDS ) ) settings = [ ] for row in self . _iter_results ( stmt ) : row [ 'setting' ] = self . _vartype_map [ row [ 'vartype' ] ] ( row [ 'setting' ] ) settings . append ( Settings ( * * row ) ) return settings
10510	def imagecapture ( self , window_name = None , out_file = None , x = 0 , y = 0 , width = None , height = None ) : if not out_file : out_file = tempfile . mktemp ( '.png' , 'ldtp_' ) else : out_file = os . path . expanduser ( out_file ) ### Windows compatibility if _ldtp_windows_env : if width == None : width = - 1 if height == None : height = - 1 if window_name == None : window_name = '' ### Windows compatibility - End data = self . _remote_imagecapture ( window_name , x , y , width , height ) f = open ( out_file , 'wb' ) f . write ( b64decode ( data ) ) f . close ( ) return out_file
12864	def days_in_month ( year , month ) : eom = _days_per_month [ month - 1 ] if is_leap_year ( year ) and month == 2 : eom += 1 return eom
11419	def record_move_subfield ( rec , tag , subfield_position , new_subfield_position , field_position_global = None , field_position_local = None ) : subfields = record_get_subfields ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) try : subfield = subfields . pop ( subfield_position ) subfields . insert ( new_subfield_position , subfield ) except IndexError : raise InvenioBibRecordFieldError ( "There is no subfield with position '%d'." % subfield_position )
9413	def _setup_log ( ) : try : handler = logging . StreamHandler ( stream = sys . stdout ) except TypeError : # pragma: no cover handler = logging . StreamHandler ( strm = sys . stdout ) log = get_log ( ) log . addHandler ( handler ) log . setLevel ( logging . INFO ) log . propagate = False
7065	def delete_ec2_nodes ( instance_id_list , client = None ) : if not client : client = boto3 . client ( 'ec2' ) resp = client . terminate_instances ( InstanceIds = instance_id_list ) return resp
13231	def get_def_macros ( tex_source ) : macros = { } for match in DEF_PATTERN . finditer ( tex_source ) : macros [ match . group ( 'name' ) ] = match . group ( 'content' ) return macros
10857	def _i2p ( self , ind , coord ) : return '-' . join ( [ self . param_prefix , str ( ind ) , coord ] )
11806	def viterbi_segment ( text , P ) : # best[i] = best probability for text[0:i] # words[i] = best word ending at position i n = len ( text ) words = [ '' ] + list ( text ) best = [ 1.0 ] + [ 0.0 ] * n ## Fill in the vectors best, words via dynamic programming for i in range ( n + 1 ) : for j in range ( 0 , i ) : w = text [ j : i ] if P [ w ] * best [ i - len ( w ) ] >= best [ i ] : best [ i ] = P [ w ] * best [ i - len ( w ) ] words [ i ] = w ## Now recover the sequence of best words sequence = [ ] i = len ( words ) - 1 while i > 0 : sequence [ 0 : 0 ] = [ words [ i ] ] i = i - len ( words [ i ] ) ## Return sequence of best words and overall probability return sequence , best [ - 1 ]
10990	def finish_state ( st , desc = 'finish-state' , invert = 'guess' ) : for minmass in [ None , 0 ] : for _ in range ( 3 ) : npart , poses = addsub . add_subtract_locally ( st , region_depth = 7 , minmass = minmass , invert = invert ) if npart == 0 : break opt . finish ( st , n_loop = 1 , separate_psf = True , desc = desc , dowarn = False ) opt . burn ( st , mode = 'polish' , desc = desc , n_loop = 2 , dowarn = False ) d = opt . finish ( st , desc = desc , n_loop = 4 , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
7164	def add_intent ( self , name , lines , reload_cache = False ) : self . intents . add ( name , lines , reload_cache ) self . padaos . add_intent ( name , lines ) self . must_train = True
13747	def get_item ( self , hash_key , start = 0 , extra_attrs = None ) : table = self . get_table ( ) try : item = table . get_item ( hash_key = hash_key ) except DynamoDBKeyNotFoundError : item = None if item is None : item = self . create_item ( hash_key = hash_key , start = start , extra_attrs = extra_attrs , ) return item
6714	def _install_from_scratch ( python_cmd , use_sudo ) : with cd ( "/tmp" ) : download ( EZ_SETUP_URL ) command = '%(python_cmd)s ez_setup.py' % locals ( ) if use_sudo : run_as_root ( command ) else : run ( command ) run ( 'rm -f ez_setup.py' )
6336	def dist_abs ( self , src , tar , * args , * * kwargs ) : return self . dist ( src , tar , * args , * * kwargs )
10339	def spia_matrices_to_excel ( spia_matrices : Mapping [ str , pd . DataFrame ] , path : str ) -> None : writer = pd . ExcelWriter ( path , engine = 'xlsxwriter' ) for relation , df in spia_matrices . items ( ) : df . to_excel ( writer , sheet_name = relation , index = False ) # Save excel writer . save ( )
45	def compute_geometric_median ( X , eps = 1e-5 ) : y = np . mean ( X , 0 ) while True : D = scipy . spatial . distance . cdist ( X , [ y ] ) nonzeros = ( D != 0 ) [ : , 0 ] Dinv = 1 / D [ nonzeros ] Dinvs = np . sum ( Dinv ) W = Dinv / Dinvs T = np . sum ( W * X [ nonzeros ] , 0 ) num_zeros = len ( X ) - np . sum ( nonzeros ) if num_zeros == 0 : y1 = T elif num_zeros == len ( X ) : return y else : R = ( T - y ) * Dinvs r = np . linalg . norm ( R ) rinv = 0 if r == 0 else num_zeros / r y1 = max ( 0 , 1 - rinv ) * T + min ( 1 , rinv ) * y if scipy . spatial . distance . euclidean ( y , y1 ) < eps : return y1 y = y1
4936	def strfdelta ( tdelta , fmt = '{D:02}d {H:02}h {M:02}m {S:02}s' , input_type = 'timedelta' ) : # Convert tdelta to integer seconds. if input_type == 'timedelta' : remainder = int ( tdelta . total_seconds ( ) ) elif input_type in [ 's' , 'seconds' ] : remainder = int ( tdelta ) elif input_type in [ 'm' , 'minutes' ] : remainder = int ( tdelta ) * 60 elif input_type in [ 'h' , 'hours' ] : remainder = int ( tdelta ) * 3600 elif input_type in [ 'd' , 'days' ] : remainder = int ( tdelta ) * 86400 elif input_type in [ 'w' , 'weeks' ] : remainder = int ( tdelta ) * 604800 else : raise ValueError ( 'input_type is not valid. Valid input_type strings are: "timedelta", "s", "m", "h", "d", "w"' ) f = Formatter ( ) desired_fields = [ field_tuple [ 1 ] for field_tuple in f . parse ( fmt ) ] possible_fields = ( 'W' , 'D' , 'H' , 'M' , 'S' ) constants = { 'W' : 604800 , 'D' : 86400 , 'H' : 3600 , 'M' : 60 , 'S' : 1 } values = { } for field in possible_fields : if field in desired_fields and field in constants : values [ field ] , remainder = divmod ( remainder , constants [ field ] ) return f . format ( fmt , * * values )
2652	def execute_no_wait ( self , cmd , walltime = 2 , envs = { } ) : # Execute the command stdin , stdout , stderr = self . ssh_client . exec_command ( self . prepend_envs ( cmd , envs ) , bufsize = - 1 , timeout = walltime ) return None , stdout , stderr
2049	def create_contract ( self , price = 0 , address = None , caller = None , balance = 0 , init = None , gas = None ) : expected_address = self . create_account ( self . new_address ( sender = caller ) ) if address is None : address = expected_address elif caller is not None and address != expected_address : raise EthereumError ( f"Error: contract created from address {hex(caller)} with nonce {self.get_nonce(caller)} was expected to be at address {hex(expected_address)}, but create_contract was called with address={hex(address)}" ) self . start_transaction ( 'CREATE' , address , price , init , caller , balance , gas = gas ) self . _process_pending_transaction ( ) return address
1531	def get_pplan ( self , topologyName , callback = None ) : if callback : self . pplan_watchers [ topologyName ] . append ( callback ) else : pplan_path = self . get_pplan_path ( topologyName ) with open ( pplan_path ) as f : data = f . read ( ) pplan = PhysicalPlan ( ) pplan . ParseFromString ( data ) return pplan
7825	def feature_uri ( uri ) : def decorator ( class_ ) : """Returns a decorated class""" if "_pyxmpp_feature_uris" not in class_ . __dict__ : class_ . _pyxmpp_feature_uris = set ( ) class_ . _pyxmpp_feature_uris . add ( uri ) return class_ return decorator
13101	def main ( ) : config = Config ( ) core = HostSearch ( ) hosts = core . get_hosts ( tags = [ '!nessus' ] , up = True ) hosts = [ host for host in hosts ] host_ips = "," . join ( [ str ( host . address ) for host in hosts ] ) url = config . get ( 'nessus' , 'host' ) access = config . get ( 'nessus' , 'access_key' ) secret = config . get ( 'nessus' , 'secret_key' ) template_name = config . get ( 'nessus' , 'template_name' ) nessus = Nessus ( access , secret , url , template_name ) scan_id = nessus . create_scan ( host_ips ) nessus . start_scan ( scan_id ) for host in hosts : host . add_tag ( 'nessus' ) host . save ( ) Logger ( ) . log ( "nessus" , "Nessus scan started on {} hosts" . format ( len ( hosts ) ) , { 'scanned_hosts' : len ( hosts ) } )
10432	def selectrowindex ( self , window_name , object_name , row_index ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) count = len ( object_handle . AXRows ) if row_index < 0 or row_index > count : raise LdtpServerException ( 'Row index out of range: %d' % row_index ) cell = object_handle . AXRows [ row_index ] if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : # Selected pass return 1
8979	def _path ( self , path ) : mode , encoding = self . _mode_and_encoding_for_open ( ) with open ( path , mode , encoding = encoding ) as file : self . __dump_to_file ( file )
7249	def cancel ( self , workflow_id ) : self . logger . debug ( 'Canceling workflow: ' + workflow_id ) url = '%(wf_url)s/%(wf_id)s/cancel' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id } r = self . gbdx_connection . post ( url , data = '' ) r . raise_for_status ( )
6402	def fingerprint ( self , phrase , joiner = ' ' ) : phrase = unicode_normalize ( 'NFKD' , text_type ( phrase . strip ( ) . lower ( ) ) ) phrase = '' . join ( [ c for c in phrase if c . isalnum ( ) or c . isspace ( ) ] ) phrase = joiner . join ( sorted ( list ( set ( phrase . split ( ) ) ) ) ) return phrase
5549	def get_hash ( x ) : if isinstance ( x , str ) : return hash ( x ) elif isinstance ( x , dict ) : return hash ( yaml . dump ( x ) )
2519	def p_file_comments_on_lics ( self , f_term , predicate ) : try : for _ , _ , comment in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_license_comment ( self . doc , six . text_type ( comment ) ) except CardinalityError : self . more_than_one_error ( 'file comments on license' )
10423	def pair_is_consistent ( graph : BELGraph , u : BaseEntity , v : BaseEntity ) -> Optional [ str ] : relations = { data [ RELATION ] for data in graph [ u ] [ v ] . values ( ) } if 1 != len ( relations ) : return return list ( relations ) [ 0 ]
10161	def py_hash ( key , num_buckets ) : b , j = - 1 , 0 if num_buckets < 1 : raise ValueError ( 'num_buckets must be a positive number' ) while j < num_buckets : b = int ( j ) key = ( ( key * long ( 2862933555777941757 ) ) + 1 ) & 0xffffffffffffffff j = float ( b + 1 ) * ( float ( 1 << 31 ) / float ( ( key >> 33 ) + 1 ) ) return int ( b )
394	def cross_entropy_reward_loss ( logits , actions , rewards , name = None ) : cross_entropy = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = actions , logits = logits , name = name ) return tf . reduce_sum ( tf . multiply ( cross_entropy , rewards ) )
2063	def is_declared ( self , expression_var ) : if not isinstance ( expression_var , Variable ) : raise ValueError ( f'Expression must be a Variable (not a {type(expression_var)})' ) return any ( expression_var is x for x in self . get_declared_variables ( ) )
6786	def get_component_funcs ( self , components = None ) : current_tp = self . get_current_thumbprint ( components = components ) or { } previous_tp = self . get_previous_thumbprint ( components = components ) or { } if self . verbose : print ( 'Current thumbprint:' ) pprint ( current_tp , indent = 4 ) print ( 'Previous thumbprint:' ) pprint ( previous_tp , indent = 4 ) differences = list ( iter_dict_differences ( current_tp , previous_tp ) ) if self . verbose : print ( 'Differences:' ) pprint ( differences , indent = 4 ) component_order = get_component_order ( [ k for k , ( _ , _ ) in differences ] ) if self . verbose : print ( 'component_order:' ) pprint ( component_order , indent = 4 ) plan_funcs = list ( get_deploy_funcs ( component_order , current_tp , previous_tp ) ) return component_order , plan_funcs
4672	def newWallet ( self , pwd ) : if self . created ( ) : raise WalletExists ( "You already have created a wallet!" ) self . store . unlock ( pwd )
13680	def get_translated_data ( self ) : j = { } for k in self . data : d = { } for l in self . data [ k ] : d [ self . translation_keys [ l ] ] = self . data [ k ] [ l ] j [ k ] = d return j
2639	def cancel ( self , job_ids ) : statuses = [ ] for job_id in job_ids : try : self . delete_instance ( job_id ) statuses . append ( True ) self . provisioned_blocks -= 1 except Exception : statuses . append ( False ) return statuses
7150	def decode ( cls , phrase ) : phrase = phrase . split ( " " ) out = "" for i in range ( len ( phrase ) // 3 ) : word1 , word2 , word3 = phrase [ 3 * i : 3 * i + 3 ] w1 = cls . word_list . index ( word1 ) w2 = cls . word_list . index ( word2 ) % cls . n w3 = cls . word_list . index ( word3 ) % cls . n x = w1 + cls . n * ( ( w2 - w1 ) % cls . n ) + cls . n * cls . n * ( ( w3 - w2 ) % cls . n ) out += endian_swap ( "%08x" % x ) return out
1625	def FindStartOfExpressionInLine ( line , endpos , stack ) : i = endpos while i >= 0 : char = line [ i ] if char in ')]}' : # Found end of expression, push to expression stack stack . append ( char ) elif char == '>' : # Found potential end of template argument list. # # Ignore it if it's a "->" or ">=" or "operator>" if ( i > 0 and ( line [ i - 1 ] == '-' or Match ( r'\s>=\s' , line [ i - 1 : ] ) or Search ( r'\boperator\s*$' , line [ 0 : i ] ) ) ) : i -= 1 else : stack . append ( '>' ) elif char == '<' : # Found potential start of template argument list if i > 0 and line [ i - 1 ] == '<' : # Left shift operator i -= 1 else : # If there is a matching '>', we can pop the expression stack. # Otherwise, ignore this '<' since it must be an operator. if stack and stack [ - 1 ] == '>' : stack . pop ( ) if not stack : return ( i , None ) elif char in '([{' : # Found start of expression. # # If there are any unmatched '>' on the stack, they must be # operators. Remove those. while stack and stack [ - 1 ] == '>' : stack . pop ( ) if not stack : return ( - 1 , None ) if ( ( char == '(' and stack [ - 1 ] == ')' ) or ( char == '[' and stack [ - 1 ] == ']' ) or ( char == '{' and stack [ - 1 ] == '}' ) ) : stack . pop ( ) if not stack : return ( i , None ) else : # Mismatched parentheses return ( - 1 , None ) elif char == ';' : # Found something that look like end of statements. If we are currently # expecting a '<', the matching '>' must have been an operator, since # template argument list should not contain statements. while stack and stack [ - 1 ] == '>' : stack . pop ( ) if not stack : return ( - 1 , None ) i -= 1 return ( - 1 , stack )
9056	def sample ( self , random_state = None ) : from numpy_sugar import epsilon from numpy_sugar . linalg import sum2diag from numpy_sugar . random import multivariate_normal if random_state is None : random_state = RandomState ( ) m = self . _mean . value ( ) K = self . _cov . value ( ) . copy ( ) sum2diag ( K , + epsilon . small , out = K ) return self . _lik . sample ( multivariate_normal ( m , K , random_state ) , random_state )
13213	def rename ( self , from_name , to_name ) : log . info ( 'renaming database from %s to %s' % ( from_name , to_name ) ) self . _run_stmt ( 'alter database %s rename to %s' % ( from_name , to_name ) )
1707	def run ( command , data = None , timeout = None , kill_timeout = None , env = None , cwd = None ) : command = expand_args ( command ) history = [ ] for c in command : if len ( history ) : # due to broken pipe problems pass only first 10 KiB data = history [ - 1 ] . std_out [ 0 : 10 * 1024 ] cmd = Command ( c ) try : out , err = cmd . run ( data , timeout , kill_timeout , env , cwd ) status_code = cmd . returncode except OSError as e : out , err = '' , u"\n" . join ( [ e . strerror , traceback . format_exc ( ) ] ) status_code = 127 r = Response ( process = cmd ) r . command = c r . std_out = out r . std_err = err r . status_code = status_code history . append ( r ) r = history . pop ( ) r . history = history return r
1122	def action ( inner_rule , loc = None ) : def decorator ( mapper ) : @ llrule ( loc , inner_rule . expected ) def outer_rule ( parser ) : result = inner_rule ( parser ) if result is unmatched : return result if isinstance ( result , tuple ) : return mapper ( parser , * result ) else : return mapper ( parser , result ) return outer_rule return decorator
2855	def setup ( self , pin , mode ) : self . _setup_pin ( pin , mode ) self . mpsse_write_gpio ( )
3743	def ViswanathNatarajan2 ( T , A , B ) : mu = exp ( A + B / T ) mu = mu / 1000. mu = mu * 10 return mu
857	def _updateSequenceInfo ( self , r ) : # Get current sequence id (if any) newSequence = False sequenceId = ( r [ self . _sequenceIdIdx ] if self . _sequenceIdIdx is not None else None ) if sequenceId != self . _currSequence : # verify that the new sequence didn't show up before if sequenceId in self . _sequences : raise Exception ( 'Broken sequence: %s, record: %s' % ( sequenceId , r ) ) # add the finished sequence to the set of sequence self . _sequences . add ( self . _currSequence ) self . _currSequence = sequenceId # Verify that the reset is consistent (if there is one) if self . _resetIdx : assert r [ self . _resetIdx ] == 1 newSequence = True else : # Check the reset reset = False if self . _resetIdx : reset = r [ self . _resetIdx ] if reset == 1 : newSequence = True # If it's still the same old sequence make sure the time flows forward if not newSequence : if self . _timeStampIdx and self . _currTime is not None : t = r [ self . _timeStampIdx ] if t < self . _currTime : raise Exception ( 'No time travel. Early timestamp for record: %s' % r ) if self . _timeStampIdx : self . _currTime = r [ self . _timeStampIdx ]
12469	def smart_str ( value , encoding = 'utf-8' , errors = 'strict' ) : if not IS_PY3 and isinstance ( value , unicode ) : # noqa return value . encode ( encoding , errors ) return str ( value )
12638	def merge_groups ( self , indices ) : try : merged = merge_dict_of_lists ( self . dicom_groups , indices , pop_later = True , copy = True ) self . dicom_groups = merged except IndexError : raise IndexError ( 'Index out of range to merge DICOM groups.' )
11436	def _record_sort_by_indicators ( record ) : for tag , fields in record . items ( ) : record [ tag ] = _fields_sort_by_indicators ( fields )
4214	def get_all_keyring ( ) : _load_plugins ( ) viable_classes = KeyringBackend . get_viable_backends ( ) rings = util . suppress_exceptions ( viable_classes , exceptions = TypeError ) return list ( rings )
2120	def associate_success_node ( self , parent , child = None , * * kwargs ) : return self . _assoc_or_create ( 'success' , parent , child , * * kwargs )
370	def flip_axis ( x , axis = 1 , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : x = np . asarray ( x ) . swapaxes ( axis , 0 ) x = x [ : : - 1 , ... ] x = x . swapaxes ( 0 , axis ) return x else : return x else : x = np . asarray ( x ) . swapaxes ( axis , 0 ) x = x [ : : - 1 , ... ] x = x . swapaxes ( 0 , axis ) return x
1487	def _load_class ( cls , d ) : for k , v in d . items ( ) : if isinstance ( k , tuple ) : typ , k = k if typ == 'property' : v = property ( * v ) elif typ == 'staticmethod' : v = staticmethod ( v ) # pylint: disable=redefined-variable-type elif typ == 'classmethod' : v = classmethod ( v ) setattr ( cls , k , v ) return cls
477	def moses_multi_bleu ( hypotheses , references , lowercase = False ) : if np . size ( hypotheses ) == 0 : return np . float32 ( 0.0 ) # Get MOSES multi-bleu script try : multi_bleu_path , _ = urllib . request . urlretrieve ( "https://raw.githubusercontent.com/moses-smt/mosesdecoder/" "master/scripts/generic/multi-bleu.perl" ) os . chmod ( multi_bleu_path , 0o755 ) except Exception : # pylint: disable=W0702 tl . logging . info ( "Unable to fetch multi-bleu.perl script, using local." ) metrics_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) bin_dir = os . path . abspath ( os . path . join ( metrics_dir , ".." , ".." , "bin" ) ) multi_bleu_path = os . path . join ( bin_dir , "tools/multi-bleu.perl" ) # Dump hypotheses and references to tempfiles hypothesis_file = tempfile . NamedTemporaryFile ( ) hypothesis_file . write ( "\n" . join ( hypotheses ) . encode ( "utf-8" ) ) hypothesis_file . write ( b"\n" ) hypothesis_file . flush ( ) reference_file = tempfile . NamedTemporaryFile ( ) reference_file . write ( "\n" . join ( references ) . encode ( "utf-8" ) ) reference_file . write ( b"\n" ) reference_file . flush ( ) # Calculate BLEU using multi-bleu script with open ( hypothesis_file . name , "r" ) as read_pred : bleu_cmd = [ multi_bleu_path ] if lowercase : bleu_cmd += [ "-lc" ] bleu_cmd += [ reference_file . name ] try : bleu_out = subprocess . check_output ( bleu_cmd , stdin = read_pred , stderr = subprocess . STDOUT ) bleu_out = bleu_out . decode ( "utf-8" ) bleu_score = re . search ( r"BLEU = (.+?)," , bleu_out ) . group ( 1 ) bleu_score = float ( bleu_score ) except subprocess . CalledProcessError as error : if error . output is not None : tl . logging . warning ( "multi-bleu.perl script returned non-zero exit code" ) tl . logging . warning ( error . output ) bleu_score = np . float32 ( 0.0 ) # Close temp files hypothesis_file . close ( ) reference_file . close ( ) return np . float32 ( bleu_score )
3912	def show_message ( self , message_str ) : if self . _message_handle is not None : self . _message_handle . cancel ( ) self . _message_handle = asyncio . get_event_loop ( ) . call_later ( self . _MESSAGE_DELAY_SECS , self . _clear_message ) self . _message = message_str self . _update ( )
12171	def count ( self , event ) : return len ( self . _listeners [ event ] ) + len ( self . _once [ event ] )
1714	def emit ( self , op_code , * args ) : self . tape . append ( OP_CODES [ op_code ] ( * args ) )
3806	def calculate ( self , T , P , zs , ws , method ) : if method == SIMPLE : ks = [ i ( T , P ) for i in self . ThermalConductivityGases ] return mixing_simple ( zs , ks ) elif method == LINDSAY_BROMLEY : ks = [ i ( T , P ) for i in self . ThermalConductivityGases ] mus = [ i ( T , P ) for i in self . ViscosityGases ] return Lindsay_Bromley ( T = T , ys = zs , ks = ks , mus = mus , Tbs = self . Tbs , MWs = self . MWs ) else : raise Exception ( 'Method not valid' )
2609	def _extract_buffers ( obj , threshold = MAX_BYTES ) : buffers = [ ] if isinstance ( obj , CannedObject ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : nbytes = _nbytes ( buf ) if nbytes > threshold : # buffer larger than threshold, prevent pickling obj . buffers [ i ] = None buffers . append ( buf ) # buffer too small for separate send, coerce to bytes # because pickling buffer objects just results in broken pointers elif isinstance ( buf , memoryview ) : obj . buffers [ i ] = buf . tobytes ( ) elif isinstance ( buf , buffer ) : obj . buffers [ i ] = bytes ( buf ) return buffers
8852	def on_new ( self ) : interpreter , pyserver , args = self . _get_backend_parameters ( ) self . setup_editor ( self . tabWidget . create_new_document ( extension = '.py' , interpreter = interpreter , server_script = pyserver , args = args ) ) self . actionRun . setDisabled ( True ) self . actionConfigure_run . setDisabled ( True )
8502	def as_live ( self ) : key = self . get_key ( ) default = pyconfig . get ( key ) if default : default = repr ( default ) else : default = self . _default ( ) or NotSet ( ) return "%s = %s" % ( key , default )
3436	def optimize ( self , objective_sense = None , raise_error = False ) : original_direction = self . objective . direction self . objective . direction = { "maximize" : "max" , "minimize" : "min" } . get ( objective_sense , original_direction ) self . slim_optimize ( ) solution = get_solution ( self , raise_error = raise_error ) self . objective . direction = original_direction return solution
599	def computeRawAnomalyScore ( activeColumns , prevPredictedColumns ) : nActiveColumns = len ( activeColumns ) if nActiveColumns > 0 : # Test whether each element of a 1-D array is also present in a second # array. Sum to get the total # of columns that are active and were # predicted. score = numpy . in1d ( activeColumns , prevPredictedColumns ) . sum ( ) # Get the percent of active columns that were NOT predicted, that is # our anomaly score. score = ( nActiveColumns - score ) / float ( nActiveColumns ) else : # There are no active columns. score = 0.0 return score
1205	def target_optimizer_arguments ( self ) : variables = self . target_network . get_variables ( ) + [ variable for name in sorted ( self . target_distributions ) for variable in self . target_distributions [ name ] . get_variables ( ) ] source_variables = self . network . get_variables ( ) + [ variable for name in sorted ( self . distributions ) for variable in self . distributions [ name ] . get_variables ( ) ] arguments = dict ( time = self . global_timestep , variables = variables , source_variables = source_variables ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . target_network . get_variables ( ) + [ variable for name in sorted ( self . global_model . target_distributions ) for variable in self . global_model . target_distributions [ name ] . get_variables ( ) ] return arguments
8733	def parse_timedelta ( str ) : deltas = ( _parse_timedelta_part ( part . strip ( ) ) for part in str . split ( ',' ) ) return sum ( deltas , datetime . timedelta ( ) )
7219	def update ( self , task_name , task_json ) : r = self . gbdx_connection . put ( self . _base_url + '/' + task_name , json = task_json ) raise_for_status ( r ) return r . json ( )
7073	def magbin_varind_gridsearch_worker ( task ) : simbasedir , gridpoint , magbinmedian = task try : res = get_recovered_variables_for_magbin ( simbasedir , magbinmedian , stetson_stdev_min = gridpoint [ 0 ] , inveta_stdev_min = gridpoint [ 1 ] , iqr_stdev_min = gridpoint [ 2 ] , statsonly = True ) return res except Exception as e : LOGEXCEPTION ( 'failed to get info for %s' % gridpoint ) return None
5316	def setup ( self , colormode = None , colorpalette = None , extend_colors = False ) : if colormode : self . colormode = colormode if colorpalette : if extend_colors : self . update_palette ( colorpalette ) else : self . colorpalette = colorpalette
848	def _convertNonNumericData ( self , spatialOutput , temporalOutput , output ) : encoders = self . encoder . getEncoderList ( ) types = self . encoder . getDecoderOutputFieldTypes ( ) for i , ( encoder , type ) in enumerate ( zip ( encoders , types ) ) : spatialData = spatialOutput [ i ] temporalData = temporalOutput [ i ] if type != FieldMetaType . integer and type != FieldMetaType . float : # TODO: Make sure that this doesn't modify any state spatialData = encoder . getScalars ( spatialData ) [ 0 ] temporalData = encoder . getScalars ( temporalData ) [ 0 ] assert isinstance ( spatialData , ( float , int ) ) assert isinstance ( temporalData , ( float , int ) ) output [ 'spatialTopDownOut' ] [ i ] = spatialData output [ 'temporalTopDownOut' ] [ i ] = temporalData
8201	def settings ( self , * * kwargs ) : for k , v in kwargs . items ( ) : setattr ( self , k , v )
2640	def runner ( incoming_q , outgoing_q ) : logger . debug ( "[RUNNER] Starting" ) def execute_task ( bufs ) : """Deserialize the buffer and execute the task. Returns the serialized result or exception. """ user_ns = locals ( ) user_ns . update ( { '__builtins__' : __builtins__ } ) f , args , kwargs = unpack_apply_message ( bufs , user_ns , copy = False ) fname = getattr ( f , '__name__' , 'f' ) prefix = "parsl_" fname = prefix + "f" argname = prefix + "args" kwargname = prefix + "kwargs" resultname = prefix + "result" user_ns . update ( { fname : f , argname : args , kwargname : kwargs , resultname : resultname } ) code = "{0} = {1}(*{2}, **{3})" . format ( resultname , fname , argname , kwargname ) try : logger . debug ( "[RUNNER] Executing: {0}" . format ( code ) ) exec ( code , user_ns , user_ns ) except Exception as e : logger . warning ( "Caught exception; will raise it: {}" . format ( e ) ) raise e else : logger . debug ( "[RUNNER] Result: {0}" . format ( user_ns . get ( resultname ) ) ) return user_ns . get ( resultname ) while True : try : # Blocking wait on the queue msg = incoming_q . get ( block = True , timeout = 10 ) except queue . Empty : # Handle case where no items were in the queue logger . debug ( "[RUNNER] Queue is empty" ) except IOError as e : logger . debug ( "[RUNNER] Broken pipe: {}" . format ( e ) ) try : # Attempt to send a stop notification to the management thread outgoing_q . put ( None ) except Exception : pass break except Exception as e : logger . debug ( "[RUNNER] Caught unknown exception: {}" . format ( e ) ) else : # Handle received message if not msg : # Empty message is a die request logger . debug ( "[RUNNER] Received exit request" ) outgoing_q . put ( None ) break else : # Received a valid message, handle it logger . debug ( "[RUNNER] Got a valid task with ID {}" . format ( msg [ "task_id" ] ) ) try : response_obj = execute_task ( msg [ 'buffer' ] ) response = { "task_id" : msg [ "task_id" ] , "result" : serialize_object ( response_obj ) } logger . debug ( "[RUNNER] Returing result: {}" . format ( deserialize_object ( response [ "result" ] ) ) ) except Exception as e : logger . debug ( "[RUNNER] Caught task exception: {}" . format ( e ) ) response = { "task_id" : msg [ "task_id" ] , "exception" : serialize_object ( e ) } outgoing_q . put ( response ) logger . debug ( "[RUNNER] Terminating" )
2540	def set_pkg_summary ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_summary_set : self . package_summary_set = True doc . package . summary = text else : raise CardinalityError ( 'Package::Summary' )
9691	def stop ( self ) : if self . receiver != None : self . receiver . join ( ) for s in self . senders . values ( ) : s . join ( )
7621	def hierarchy ( ref , est , * * kwargs ) : namespace = 'multi_segment' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_hier , ref_hier_lab = hierarchy_flatten ( ref ) est_hier , est_hier_lab = hierarchy_flatten ( est ) return mir_eval . hierarchy . evaluate ( ref_hier , ref_hier_lab , est_hier , est_hier_lab , * * kwargs )
7527	def align_and_parse ( handle , max_internal_indels = 5 , is_gbs = False ) : ## data are already chunked, read in the whole thing. bail if no data. try : with open ( handle , 'rb' ) as infile : clusts = infile . read ( ) . split ( "//\n//\n" ) ## remove any empty spots clusts = [ i for i in clusts if i ] ## Skip entirely empty chunks if not clusts : raise IPyradError except ( IOError , IPyradError ) : LOGGER . debug ( "skipping empty chunk - {}" . format ( handle ) ) return 0 ## count discarded clusters for printing to stats later highindels = 0 ## iterate over clusters sending each to muscle, splits and aligns pairs try : aligned = persistent_popen_align3 ( clusts , 200 , is_gbs ) except Exception as inst : LOGGER . debug ( "Error in handle - {} - {}" . format ( handle , inst ) ) #raise IPyradWarningExit("error hrere {}".format(inst)) aligned = [ ] ## store good alignments to be written to file refined = [ ] ## filter and trim alignments for clust in aligned : ## check for too many internal indels filtered = aligned_indel_filter ( clust , max_internal_indels ) ## reverse complement matches. No longer implemented. #filtered = overshoot_filter(clust) ## finally, add to outstack if alignment is good if not filtered : refined . append ( clust ) #"\n".join(stack)) else : highindels += 1 ## write to file after if refined : outhandle = handle . rsplit ( "." , 1 ) [ 0 ] + ".aligned" with open ( outhandle , 'wb' ) as outfile : outfile . write ( "\n//\n//\n" . join ( refined ) + "\n" ) ## remove the old tmp file log_level = logging . getLevelName ( LOGGER . getEffectiveLevel ( ) ) if not log_level == "DEBUG" : os . remove ( handle ) return highindels
6577	def from_json ( cls , api_client , data ) : self = cls ( api_client ) PandoraModel . populate_fields ( api_client , self , data ) return self
4249	def netspeed_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . netspeed_by_addr ( addr )
10246	def count_confidences ( graph : BELGraph ) -> typing . Counter [ str ] : return Counter ( ( 'None' if ANNOTATIONS not in data or 'Confidence' not in data [ ANNOTATIONS ] else list ( data [ ANNOTATIONS ] [ 'Confidence' ] ) [ 0 ] ) for _ , _ , data in graph . edges ( data = True ) if CITATION in data # don't bother with unqualified statements )
9905	def ping ( self ) : self . __validate_ping_param ( ) ping_proc = subprocrunner . SubprocessRunner ( self . __get_ping_command ( ) ) ping_proc . run ( ) return PingResult ( ping_proc . stdout , ping_proc . stderr , ping_proc . returncode )
1509	def stop_cluster ( cl_args ) : Log . info ( "Terminating cluster..." ) roles = read_and_parse_roles ( cl_args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] dist_nodes = masters . union ( slaves ) # stop all jobs if masters : try : single_master = list ( masters ) [ 0 ] jobs = get_jobs ( cl_args , single_master ) for job in jobs : job_id = job [ "ID" ] Log . info ( "Terminating job %s" % job_id ) delete_job ( cl_args , job_id , single_master ) except : Log . debug ( "Error stopping jobs" ) Log . debug ( sys . exc_info ( ) [ 0 ] ) for node in dist_nodes : Log . info ( "Terminating processes on %s" % node ) if not is_self ( node ) : cmd = "ps aux | grep heron-nomad | awk '{print \$2}' " "| xargs kill" cmd = ssh_remote_execute ( cmd , node , cl_args ) else : cmd = "ps aux | grep heron-nomad | awk '{print $2}' " "| xargs kill" Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) Log . info ( "Cleaning up directories on %s" % node ) cmd = "rm -rf /tmp/slave ; rm -rf /tmp/master" if not is_self ( node ) : cmd = ssh_remote_execute ( cmd , node , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) )
1091	def encode_basestring ( s ) : def replace ( match ) : return ESCAPE_DCT [ match . group ( 0 ) ] return '"' + ESCAPE . sub ( replace , s ) + '"'
2814	def convert_shape ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting shape ...' ) def target_layer ( x ) : import tensorflow as tf return tf . shape ( x ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
4695	def execute ( cmd = None , shell = True , echo = True ) : if echo : cij . emph ( "cij.util.execute: shell: %r, cmd: %r" % ( shell , cmd ) ) rcode = 1 stdout , stderr = ( "" , "" ) if cmd : if shell : cmd = " " . join ( cmd ) proc = Popen ( cmd , stdout = PIPE , stderr = PIPE , shell = shell , close_fds = True ) stdout , stderr = proc . communicate ( ) rcode = proc . returncode if rcode and echo : cij . warn ( "cij.util.execute: stdout: %s" % stdout ) cij . err ( "cij.util.execute: stderr: %s" % stderr ) cij . err ( "cij.util.execute: rcode: %s" % rcode ) return rcode , stdout , stderr
5838	def _data_analysis ( self , data_view_id ) : failure_message = "Error while retrieving data analysis for data view {}" . format ( data_view_id ) return self . _get_success_json ( self . _get ( routes . data_analysis ( data_view_id ) , failure_message = failure_message ) )
11099	def select_by_mtime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . mtime <= max_time return self . select_file ( filters , recursive )
4399	def adsSyncSetTimeoutEx ( port , nMs ) : # type: (int, int) -> None adsSyncSetTimeoutFct = _adsDLL . AdsSyncSetTimeoutEx cms = ctypes . c_long ( nMs ) err_code = adsSyncSetTimeoutFct ( port , cms ) if err_code : raise ADSError ( err_code )
8716	def node_heap ( self ) : log . info ( 'Heap' ) res = self . __exchange ( 'print(node.heap())' ) log . info ( res ) return int ( res . split ( '\r\n' ) [ 1 ] )
10559	def convert_cygwin_path ( path ) : try : win_path = subprocess . check_output ( [ "cygpath" , "-aw" , path ] , universal_newlines = True ) . strip ( ) except ( FileNotFoundError , subprocess . CalledProcessError ) : logger . exception ( "Call to cygpath failed." ) raise return win_path
8785	def update_port ( self , context , port_id , * * kwargs ) : LOG . info ( "update_port %s %s" % ( context . tenant_id , port_id ) ) # TODO(morgabra): Change this when we enable security groups. if kwargs . get ( "security_groups" ) : msg = 'ironic driver does not support security group operations.' raise IronicException ( msg = msg ) return { "uuid" : port_id }
992	def sample ( reader , writer , n , start = None , stop = None , tsCol = None , writeSampleOnly = True ) : rows = list ( reader ) if tsCol is not None : ts = rows [ 0 ] [ tsCol ] inc = rows [ 1 ] [ tsCol ] - ts if start is None : start = 0 if stop is None : stop = len ( rows ) - 1 initialN = stop - start + 1 # Select random rows in the sample range to delete until the desired number # of rows are left. numDeletes = initialN - n for i in xrange ( numDeletes ) : delIndex = random . randint ( start , stop - i ) del rows [ delIndex ] # Remove outside rows if specified. if writeSampleOnly : rows = rows [ start : start + n ] # Rewrite columns if tsCol is given. if tsCol is not None : ts = rows [ 0 ] [ tsCol ] # Write resulting rows. for row in rows : if tsCol is not None : row [ tsCol ] = ts ts += inc writer . appendRecord ( row )
11894	def readtxt ( filepath ) : with open ( filepath , 'rt' ) as f : lines = f . readlines ( ) return '' . join ( lines )
6988	def serial_varfeatures ( lclist , outdir , maxobjects = None , timecols = None , magcols = None , errcols = None , mindet = 1000 , lcformat = 'hat-sql' , lcformatdir = None ) : if maxobjects : lclist = lclist [ : maxobjects ] tasks = [ ( x , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) for x in lclist ] for task in tqdm ( tasks ) : result = _varfeatures_worker ( task ) return result
12894	def set_power ( self , value = False ) : power = ( yield from self . handle_set ( self . API . get ( 'power' ) , int ( value ) ) ) return bool ( power )
3658	def _destroy_image_acquirer ( self , ia ) : id_ = None if ia . device : # ia . stop_image_acquisition ( ) # ia . _release_data_streams ( ) # id_ = ia . _device . id_ # if ia . device . node_map : # if ia . _chunk_adapter : ia . _chunk_adapter . detach_buffer ( ) ia . _chunk_adapter = None self . _logger . info ( 'Detached a buffer from the chunk adapter of {0}.' . format ( id_ ) ) ia . device . node_map . disconnect ( ) self . _logger . info ( 'Disconnected the port from the NodeMap of {0}.' . format ( id_ ) ) # if ia . _device . is_open ( ) : ia . _device . close ( ) self . _logger . info ( 'Closed Device module, {0}.' . format ( id_ ) ) ia . _device = None # if id_ : self . _logger . info ( 'Destroyed the ImageAcquirer object which {0} ' 'had belonged to.' . format ( id_ ) ) else : self . _logger . info ( 'Destroyed an ImageAcquirer.' ) if self . _profiler : self . _profiler . print_diff ( ) self . _ias . remove ( ia )
12248	def sync ( self , * buckets ) : if buckets : for _bucket in buckets : for key in mimicdb . backend . smembers ( tpl . bucket % _bucket ) : mimicdb . backend . delete ( tpl . key % ( _bucket , key ) ) mimicdb . backend . delete ( tpl . bucket % _bucket ) bucket = self . get_bucket ( _bucket , force = True ) for key in bucket . list ( force = True ) : mimicdb . backend . sadd ( tpl . bucket % bucket . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( bucket . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) ) else : for bucket in mimicdb . backend . smembers ( tpl . connection ) : for key in mimicdb . backend . smembers ( tpl . bucket % bucket ) : mimicdb . backend . delete ( tpl . key % ( bucket , key ) ) mimicdb . backend . delete ( tpl . bucket % bucket ) for bucket in self . get_all_buckets ( force = True ) : for key in bucket . list ( force = True ) : mimicdb . backend . sadd ( tpl . bucket % bucket . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( bucket . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) )
11666	def _get_rhos ( X , indices , Ks , max_K , save_all_Ks , min_dist ) : logger . info ( "Getting within-bag distances..." ) if max_K >= X . n_pts . min ( ) : msg = "asked for K = {}, but there's a bag with only {} points" raise ValueError ( msg . format ( max_K , X . n_pts . min ( ) ) ) # need to throw away the closest neighbor, which will always be self # thus K=1 corresponds to column 1 in the result array which_Ks = slice ( 1 , None ) if save_all_Ks else Ks indices = plog ( indices , name = "within-bag distances" ) rhos = [ None ] * len ( X ) for i , ( idx , bag ) in enumerate ( zip ( indices , X ) ) : r = np . sqrt ( idx . nn_index ( bag , max_K + 1 ) [ 1 ] [ : , which_Ks ] ) np . maximum ( min_dist , r , out = r ) rhos [ i ] = r return rhos
9697	def check_nonce ( self , request , oauth_request ) : oauth_nonce = oauth_request [ 'oauth_nonce' ] oauth_timestamp = oauth_request [ 'oauth_timestamp' ] return check_nonce ( request , oauth_request , oauth_nonce , oauth_timestamp )
9348	def year ( past = False , min_delta = 0 , max_delta = 20 ) : return dt . date . today ( ) . year + _delta ( past , min_delta , max_delta )
1879	def VEXTRACTF128 ( cpu , dest , src , offset ) : offset = offset . read ( ) dest . write ( Operators . EXTRACT ( src . read ( ) , offset * 128 , ( offset + 1 ) * 128 ) )
434	def CNN2d ( CNN = None , second = 10 , saveable = True , name = 'cnn' , fig_idx = 3119362 ) : import matplotlib . pyplot as plt # tl.logging.info(CNN.shape) # (5, 5, 3, 64) # exit() n_mask = CNN . shape [ 3 ] n_row = CNN . shape [ 0 ] n_col = CNN . shape [ 1 ] n_color = CNN . shape [ 2 ] row = int ( np . sqrt ( n_mask ) ) col = int ( np . ceil ( n_mask / row ) ) plt . ion ( ) # active mode fig = plt . figure ( fig_idx ) count = 1 for _ir in range ( 1 , row + 1 ) : for _ic in range ( 1 , col + 1 ) : if count > n_mask : break fig . add_subplot ( col , row , count ) # tl.logging.info(CNN[:,:,:,count-1].shape, n_row, n_col) # (5, 1, 32) 5 5 # exit() # plt.imshow( # np.reshape(CNN[count-1,:,:,:], (n_row, n_col)), # cmap='gray', interpolation="nearest") # theano if n_color == 1 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col ) ) , cmap = 'gray' , interpolation = "nearest" ) elif n_color == 3 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col , n_color ) ) , cmap = 'gray' , interpolation = "nearest" ) else : raise Exception ( "Unknown n_color" ) plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) # distable tick plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )
9756	def update ( ctx , name , description , tags ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the experiment.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment . update_experiment ( user , project_name , _experiment , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment updated." ) get_experiment_details ( response )
9147	def web ( connection , host , port ) : from bio2bel . web . application import create_application app = create_application ( connection = connection ) app . run ( host = host , port = port )
1699	def log ( self ) : from heronpy . streamlet . impl . logbolt import LogStreamlet log_streamlet = LogStreamlet ( self ) self . _add_child ( log_streamlet ) return
12377	def make_response ( self , data = None ) : if data is not None : # Prepare the data for transmission. data = self . prepare ( data ) # Encode the data using a desired encoder. self . response . write ( data , serialize = True )
6734	def get_hosts_retriever ( s = None ) : s = s or env . hosts_retriever # #assert s, 'No hosts retriever specified.' if not s : return env_hosts_retriever # module_name = '.'.join(s.split('.')[:-1]) # func_name = s.split('.')[-1] # retriever = getattr(importlib.import_module(module_name), func_name) # return retriever return str_to_callable ( s ) or env_hosts_retriever
13613	def combine_filenames ( filenames , max_length = 40 ) : # Get the SHA for each file, then sha all the shas. path = None names = [ ] extension = None timestamps = [ ] shas = [ ] filenames . sort ( ) concat_names = "_" . join ( filenames ) if concat_names in COMBINED_FILENAMES_GENERATED : return COMBINED_FILENAMES_GENERATED [ concat_names ] for filename in filenames : name = os . path . basename ( filename ) if not extension : extension = os . path . splitext ( name ) [ 1 ] elif os . path . splitext ( name ) [ 1 ] != extension : raise ValueError ( "Can't combine multiple file extensions" ) for base in MEDIA_ROOTS : try : shas . append ( md5 ( os . path . join ( base , filename ) ) ) break except IOError : pass if path is None : path = os . path . dirname ( filename ) else : if len ( os . path . dirname ( filename ) ) < len ( path ) : path = os . path . dirname ( filename ) m = hashlib . md5 ( ) m . update ( "," . join ( shas ) ) new_filename = "%s-inkmd" % m . hexdigest ( ) new_filename = new_filename [ : max_length ] new_filename += extension COMBINED_FILENAMES_GENERATED [ concat_names ] = new_filename return os . path . join ( path , new_filename )
4064	def add_tags ( self , item , * tags ) : # Make sure there's a tags field, or add one try : assert item [ "data" ] [ "tags" ] except AssertionError : item [ "data" ] [ "tags" ] = list ( ) for tag in tags : item [ "data" ] [ "tags" ] . append ( { "tag" : "%s" % tag } ) # make sure everything's OK assert self . check_items ( [ item ] ) return self . update_item ( item )
10455	def check ( self , window_name , object_name ) : # FIXME: Check for object type object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) if object_handle . AXValue == 1 : # Already checked return 1 # AXPress doesn't work with Instruments # So did the following work around self . _grabfocus ( object_handle ) x , y , width , height = self . _getobjectsize ( object_handle ) # Mouse left click on the object # Note: x + width/2, y + height / 2 doesn't work self . generatemouseevent ( x + width / 2 , y + height / 2 , "b1c" ) return 1
13487	def get_or_create_index ( self , index_ratio , index_width ) : if not self . index_path . exists ( ) or not self . filepath . stat ( ) . st_mtime == self . index_path . stat ( ) . st_mtime : create_index ( self . filepath , self . index_path , index_ratio = index_ratio , index_width = index_width ) return IndexFile ( str ( self . index_path ) )
5052	def get_course_completions ( self , enterprise_customer , days ) : return PersistentCourseGrade . objects . filter ( passed_timestamp__gt = datetime . datetime . now ( ) - datetime . timedelta ( days = days ) ) . filter ( user_id__in = enterprise_customer . enterprise_customer_users . values_list ( 'user_id' , flat = True ) )
4791	def is_upper ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . upper ( ) : self . _err ( 'Expected <%s> to contain only uppercase chars, but did not.' % self . val ) return self
11051	def listen_events ( self , reconnects = 0 ) : self . log . info ( 'Listening for events from Marathon...' ) self . _attached = False def on_finished ( result , reconnects ) : # If the callback fires then the HTTP request to the event stream # went fine, but the persistent connection for the SSE stream was # dropped. Just reconnect for now- if we can't actually connect # then the errback will fire rather. self . log . warn ( 'Connection lost listening for events, ' 'reconnecting... ({reconnects} so far)' , reconnects = reconnects ) reconnects += 1 return self . listen_events ( reconnects ) def log_failure ( failure ) : self . log . failure ( 'Failed to listen for events' , failure ) return failure return self . marathon_client . get_events ( { 'event_stream_attached' : self . _sync_on_event_stream_attached , 'api_post_event' : self . _sync_on_api_post_event } ) . addCallbacks ( on_finished , log_failure , callbackArgs = [ reconnects ] )
6100	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . divide ( self . intensity , self . sigma * np . sqrt ( 2.0 * np . pi ) ) , np . exp ( - 0.5 * np . square ( np . divide ( grid_radii , self . sigma ) ) ) )
10494	def clickMouseButtonRight ( self , coord ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonRight , modFlags ) self . _postQueuedEvents ( )
2771	def get_object ( cls , api_token , id ) : load_balancer = cls ( token = api_token , id = id ) load_balancer . load ( ) return load_balancer
7751	def process_message ( self , stanza ) : stanza_type = stanza . stanza_type if stanza_type is None : stanza_type = "normal" if self . __try_handlers ( self . _message_handlers , stanza , stanza_type = stanza_type ) : return True if stanza_type not in ( "error" , "normal" ) : # try 'normal' handler additionaly to the regular handler return self . __try_handlers ( self . _message_handlers , stanza , stanza_type = "normal" ) return False
11401	def create_field ( subfields = None , ind1 = ' ' , ind2 = ' ' , controlfield_value = '' , global_position = - 1 ) : if subfields is None : subfields = [ ] ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) field = ( subfields , ind1 , ind2 , controlfield_value , global_position ) _check_field_validity ( field ) return field
9846	def resample_factor ( self , factor ) : # new number of edges N' = (N-1)*f + 1 newlengths = [ ( N - 1 ) * float ( factor ) + 1 for N in self . _len_edges ( ) ] edges = [ numpy . linspace ( start , stop , num = int ( N ) , endpoint = True ) for ( start , stop , N ) in zip ( self . _min_edges ( ) , self . _max_edges ( ) , newlengths ) ] return self . resample ( edges )
2979	def cmd_join ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . join ( )
9388	def plot_diff ( self , graphing_library = 'matplotlib' ) : diff_datasource = sorted ( set ( self . reports [ 0 ] . datasource ) & set ( self . reports [ 1 ] . datasource ) ) graphed = False for submetric in diff_datasource : baseline_csv = naarad . utils . get_default_csv ( self . reports [ 0 ] . local_location , ( submetric + '.percentiles' ) ) current_csv = naarad . utils . get_default_csv ( self . reports [ 1 ] . local_location , ( submetric + '.percentiles' ) ) if ( not ( naarad . utils . is_valid_file ( baseline_csv ) & naarad . utils . is_valid_file ( current_csv ) ) ) : continue baseline_plot = PD ( input_csv = baseline_csv , csv_column = 1 , series_name = submetric , y_label = submetric , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' , plot_label = 'baseline' , x_label = 'Percentiles' ) current_plot = PD ( input_csv = current_csv , csv_column = 1 , series_name = submetric , y_label = submetric , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' , plot_label = 'current' , x_label = 'Percentiles' ) graphed , div_file = Diff . graphing_modules [ graphing_library ] . graph_data_on_the_same_graph ( [ baseline_plot , current_plot ] , os . path . join ( self . output_directory , self . resource_path ) , self . resource_path , ( submetric + '.diff' ) ) if graphed : self . plot_files . append ( div_file ) return True
11610	def update_probability_at_read_level ( self , model = 3 ) : self . probability . reset ( ) # reset to alignment incidence matrix if model == 1 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . HAPLOGROUP , grouping_mat = self . t2t_mat ) haplogroup_sum_mat = self . allelic_expression * self . t2t_mat self . probability . multiply ( haplogroup_sum_mat , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( haplogroup_sum_mat . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 2 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . LOCUS ) self . probability . multiply ( self . allelic_expression . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( ( self . allelic_expression * self . t2t_mat ) . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 3 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( ( self . allelic_expression * self . t2t_mat ) . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 4 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . READ ) else : raise RuntimeError ( 'The read normalization model should be 1, 2, 3, or 4.' )
203	def to_heatmaps ( self , only_nonempty = False , not_none_if_no_nonempty = False ) : # TODO get rid of this deferred import from imgaug . augmentables . heatmaps import HeatmapsOnImage if not only_nonempty : return HeatmapsOnImage . from_0to1 ( self . arr , self . shape , min_value = 0.0 , max_value = 1.0 ) else : nonempty_mask = np . sum ( self . arr , axis = ( 0 , 1 ) ) > 0 + 1e-4 if np . sum ( nonempty_mask ) == 0 : if not_none_if_no_nonempty : nonempty_mask [ 0 ] = True else : return None , [ ] class_indices = np . arange ( self . arr . shape [ 2 ] ) [ nonempty_mask ] channels = self . arr [ ... , class_indices ] return HeatmapsOnImage ( channels , self . shape , min_value = 0.0 , max_value = 1.0 ) , class_indices
9177	def _dissect_roles ( metadata ) : for role_key in cnxepub . ATTRIBUTED_ROLE_KEYS : for user in metadata . get ( role_key , [ ] ) : if user [ 'type' ] != 'cnx-id' : raise ValueError ( "Archive only accepts Connexions users." ) uid = parse_user_uri ( user [ 'id' ] ) yield uid , role_key raise StopIteration ( )
7662	def slice ( self , start_time , end_time , strict = False ) : # start by trimming the annotation sliced_ann = self . trim ( start_time , end_time , strict = strict ) raw_data = sliced_ann . pop_data ( ) # now adjust the start time of the annotation and the observations it # contains. for obs in raw_data : new_time = max ( 0 , obs . time - start_time ) # if obs.time > start_time, # duration doesn't change # if obs.time < start_time, # duration shrinks by start_time - obs.time sliced_ann . append ( time = new_time , duration = obs . duration , value = obs . value , confidence = obs . confidence ) ref_time = sliced_ann . time slice_start = ref_time slice_end = ref_time + sliced_ann . duration if 'slice' not in sliced_ann . sandbox . keys ( ) : sliced_ann . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time , 'slice_start' : slice_start , 'slice_end' : slice_end } ] ) else : sliced_ann . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time , 'slice_start' : slice_start , 'slice_end' : slice_end } ) # Update the timing for the sliced annotation sliced_ann . time = max ( 0 , ref_time - start_time ) return sliced_ann
10999	def psf_slice ( self , zint , size = 11 , zoffset = 0. , getextent = False ) : # calculate the current pixel value in 1/k, making sure we are above the slab zint = max ( self . _p2k ( self . _tz ( zint ) ) , 0 ) offset = np . array ( [ zoffset * ( zint > 0 ) , 0 , 0 ] ) scale = [ self . param_dict [ self . zscale ] , 1.0 , 1.0 ] # create the coordinate vectors for where to actually calculate the tile = util . Tile ( left = 0 , size = size , centered = True ) vecs = tile . coords ( form = 'flat' ) vecs = [ self . _p2k ( s * i + o ) for i , s , o in zip ( vecs , scale , offset ) ] psf = self . psffunc ( * vecs [ : : - 1 ] , zint = zint , * * self . pack_args ( ) ) . T vec = tile . coords ( form = 'meshed' ) # create a smoothly varying point spread function by cutting off the psf # at a certain value and smoothly taking it to zero if self . cutoffval is not None and not self . cutbyval : # find the edges of the PSF edge = psf > psf . max ( ) * self . cutoffval dd = nd . morphology . distance_transform_edt ( ~ edge ) # calculate the new PSF and normalize it to the new support psf = psf * np . exp ( - dd ** 4 ) psf /= psf . sum ( ) if getextent : # the size is determined by the edge plus a 2 pad for the # exponential damping to zero at the edge size = np . array ( [ ( vec * edge ) . min ( axis = ( 1 , 2 , 3 ) ) - 2 , ( vec * edge ) . max ( axis = ( 1 , 2 , 3 ) ) + 2 , ] ) . T return psf , vec , size return psf , vec # perform a cut by value instead if self . cutoffval is not None and self . cutbyval : cutval = self . cutoffval * psf . max ( ) dd = ( psf - cutval ) / cutval dd [ dd > 0 ] = 0. # calculate the new PSF and normalize it to the new support psf = psf * np . exp ( - ( dd / self . cutfallrate ) ** 4 ) psf /= psf . sum ( ) # let the small values determine the edges edge = psf > cutval * self . cutedgeval if getextent : # the size is determined by the edge plus a 2 pad for the # exponential damping to zero at the edge size = np . array ( [ ( vec * edge ) . min ( axis = ( 1 , 2 , 3 ) ) - 2 , ( vec * edge ) . max ( axis = ( 1 , 2 , 3 ) ) + 2 , ] ) . T return psf , vec , size return psf , vec return psf , vec
6429	def stem ( self , word ) : lowered = word . lower ( ) if lowered [ - 3 : ] == 'ies' and lowered [ - 4 : - 3 ] not in { 'e' , 'a' } : return word [ : - 3 ] + ( 'Y' if word [ - 1 : ] . isupper ( ) else 'y' ) if lowered [ - 2 : ] == 'es' and lowered [ - 3 : - 2 ] not in { 'a' , 'e' , 'o' } : return word [ : - 1 ] if lowered [ - 1 : ] == 's' and lowered [ - 2 : - 1 ] not in { 'u' , 's' } : return word [ : - 1 ] return word
736	def _sortChunk ( records , key , chunkIndex , fields ) : title ( additional = '(key=%s, chunkIndex=%d)' % ( str ( key ) , chunkIndex ) ) assert len ( records ) > 0 # Sort the current records records . sort ( key = itemgetter ( * key ) ) # Write to a chunk file if chunkIndex is not None : filename = 'chunk_%d.csv' % chunkIndex with FileRecordStream ( filename , write = True , fields = fields ) as o : for r in records : o . appendRecord ( r ) assert os . path . getsize ( filename ) > 0 return records
6824	def restart ( self ) : n = 60 sleep_n = int ( self . env . max_restart_wait_minutes / 10. * 60 ) for _ in xrange ( n ) : self . stop ( ) if self . dryrun or not self . is_running ( ) : break print ( 'Waiting for supervisor to stop (%i of %i)...' % ( _ , n ) ) time . sleep ( sleep_n ) self . start ( ) for _ in xrange ( n ) : if self . dryrun or self . is_running ( ) : return print ( 'Waiting for supervisor to start (%i of %i)...' % ( _ , n ) ) time . sleep ( sleep_n ) raise Exception ( 'Failed to restart service %s!' % self . name )
11409	def record_add_fields ( rec , tag , fields , field_position_local = None , field_position_global = None ) : if field_position_local is None and field_position_global is None : for field in fields : record_add_field ( rec , tag , ind1 = field [ 1 ] , ind2 = field [ 2 ] , subfields = field [ 0 ] , controlfield_value = field [ 3 ] ) else : fields . reverse ( ) for field in fields : record_add_field ( rec , tag , ind1 = field [ 1 ] , ind2 = field [ 2 ] , subfields = field [ 0 ] , controlfield_value = field [ 3 ] , field_position_local = field_position_local , field_position_global = field_position_global ) return field_position_local
842	def getPartitionId ( self , i ) : if ( i < 0 ) or ( i >= self . _numPatterns ) : raise RuntimeError ( "index out of bounds" ) partitionId = self . _partitionIdList [ i ] if partitionId == numpy . inf : return None else : return partitionId
3820	async def create_conversation ( self , create_conversation_request ) : response = hangouts_pb2 . CreateConversationResponse ( ) await self . _pb_request ( 'conversations/createconversation' , create_conversation_request , response ) return response
10121	def from_dict ( cls , spec ) : spec = spec . copy ( ) center = spec . pop ( 'center' , None ) radius = spec . pop ( 'radius' , None ) if center and radius : return cls . circle ( center , radius , * * spec ) vertices = spec . pop ( 'vertices' ) if len ( vertices ) == 2 : return cls . rectangle ( vertices , * * spec ) return cls ( vertices , * * spec )
12874	def satisfies ( guard ) : i = peek ( ) if ( i is EndOfFile ) or ( not guard ( i ) ) : fail ( [ "<satisfies predicate " + _fun_to_str ( guard ) + ">" ] ) next ( ) return i
7940	def _got_srv ( self , addrs ) : with self . lock : if not addrs : self . _dst_service = None if self . _dst_port : self . _dst_nameports = [ ( self . _dst_name , self . _dst_port ) ] else : self . _dst_nameports = [ ] self . _set_state ( "aborted" ) raise DNSError ( "Could not resolve SRV for service {0!r}" " on host {1!r} and fallback port number not given" . format ( self . _dst_service , self . _dst_name ) ) elif addrs == [ ( "." , 0 ) ] : self . _dst_nameports = [ ] self . _set_state ( "aborted" ) raise DNSError ( "Service {0!r} not available on host {1!r}" . format ( self . _dst_service , self . _dst_name ) ) else : self . _dst_nameports = addrs self . _set_state ( "resolve-hostname" )
12550	def write_meta_header ( filename , meta_dict ) : header = '' # do not use tags = meta_dict.keys() because the order of tags matters for tag in MHD_TAGS : if tag in meta_dict . keys ( ) : header += '{} = {}\n' . format ( tag , meta_dict [ tag ] ) with open ( filename , 'w' ) as f : f . write ( header )
4723	def trun_setup ( conf ) : declr = None try : with open ( conf [ "TESTPLAN_FPATH" ] ) as declr_fd : declr = yaml . safe_load ( declr_fd ) except AttributeError as exc : cij . err ( "rnr: %r" % exc ) if not declr : return None trun = copy . deepcopy ( TRUN ) trun [ "ver" ] = cij . VERSION trun [ "conf" ] = copy . deepcopy ( conf ) trun [ "res_root" ] = conf [ "OUTPUT" ] trun [ "aux_root" ] = os . sep . join ( [ trun [ "res_root" ] , "_aux" ] ) trun [ "evars" ] . update ( copy . deepcopy ( declr . get ( "evars" , { } ) ) ) os . makedirs ( trun [ "aux_root" ] ) hook_names = declr . get ( "hooks" , [ ] ) if "lock" not in hook_names : hook_names = [ "lock" ] + hook_names if hook_names [ 0 ] != "lock" : return None # Setup top-level hooks trun [ "hooks" ] = hooks_setup ( trun , trun , hook_names ) for enum , declr in enumerate ( declr [ "testsuites" ] ) : # Setup testsuites tsuite = tsuite_setup ( trun , declr , enum ) if tsuite is None : cij . err ( "main::FAILED: setting up tsuite: %r" % tsuite ) return 1 trun [ "testsuites" ] . append ( tsuite ) trun [ "progress" ] [ "UNKN" ] += len ( tsuite [ "testcases" ] ) return trun
5902	def prehook ( self , * * kwargs ) : cmd = [ 'smpd' , '-s' ] logger . info ( "Starting smpd: " + " " . join ( cmd ) ) rc = subprocess . call ( cmd ) return rc
2946	def get_ready_user_tasks ( self ) : return [ t for t in self . get_tasks ( Task . READY ) if not self . _is_engine_task ( t . task_spec ) ]
10065	def json_serializer ( pid , data , * args ) : if data is not None : response = Response ( json . dumps ( data . dumps ( ) ) , mimetype = 'application/json' ) else : response = Response ( mimetype = 'application/json' ) return response
401	def cross_entropy_seq_with_mask ( logits , target_seqs , input_mask , return_details = False , name = None ) : targets = tf . reshape ( target_seqs , [ - 1 ] ) # to one vector weights = tf . to_float ( tf . reshape ( input_mask , [ - 1 ] ) ) # to one vector like targets losses = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits = logits , labels = targets , name = name ) * weights # losses = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets, name=name)) # for TF1.0 and others loss = tf . divide ( tf . reduce_sum ( losses ) , # loss from mask. reduce_sum before element-wise mul with mask !! tf . reduce_sum ( weights ) , name = "seq_loss_with_mask" ) if return_details : return loss , losses , weights , targets else : return loss
3569	def centralManager_didDiscoverPeripheral_advertisementData_RSSI_ ( self , manager , peripheral , data , rssi ) : logger . debug ( 'centralManager_didDiscoverPeripheral_advertisementData_RSSI called' ) # Log name of device found while scanning. #logger.debug('Saw device advertised with name: {0}'.format(peripheral.name())) # Make sure the device is added to the list of devices and then update # its advertisement state. device = device_list ( ) . get ( peripheral ) if device is None : device = device_list ( ) . add ( peripheral , CoreBluetoothDevice ( peripheral ) ) device . _update_advertised ( data )
10505	def stopEventLoop ( ) : stopper = PyObjCAppHelperRunLoopStopper_wrap . currentRunLoopStopper ( ) if stopper is None : if NSApp ( ) is not None : NSApp ( ) . terminate_ ( None ) return True return False NSTimer . scheduledTimerWithTimeInterval_target_selector_userInfo_repeats_ ( 0.0 , stopper , 'performStop:' , None , False ) return True
2796	def load ( self , use_slug = False ) : identifier = None if use_slug or not self . id : identifier = self . slug else : identifier = self . id if not identifier : raise NotFoundError ( "One of self.id or self.slug must be set." ) data = self . get_data ( "images/%s" % identifier ) image_dict = data [ 'image' ] # Setting the attribute values for attr in image_dict . keys ( ) : setattr ( self , attr , image_dict [ attr ] ) return self
13777	def AddEnumDescriptor ( self , enum_desc ) : if not isinstance ( enum_desc , descriptor . EnumDescriptor ) : raise TypeError ( 'Expected instance of descriptor.EnumDescriptor.' ) self . _enum_descriptors [ enum_desc . full_name ] = enum_desc self . AddFileDescriptor ( enum_desc . file )
11730	def pvpc_calc_tcu_cp_feu_d ( df , verbose = True , convert_kwh = True ) : if 'TCU' + TARIFAS [ 0 ] not in df . columns : # Pasa de €/MWh a €/kWh: if convert_kwh : cols_mwh = [ c + t for c in COLS_PVPC for t in TARIFAS if c != 'COF' ] df [ cols_mwh ] = df [ cols_mwh ] . applymap ( lambda x : x / 1000. ) # Obtiene columnas TCU, CP, precio día gb_t = df . groupby ( lambda x : TARIFAS [ np . argmax ( [ t in x for t in TARIFAS ] ) ] , axis = 1 ) for k , g in gb_t : if verbose : print ( 'TARIFA {}' . format ( k ) ) print ( g . head ( ) ) # Cálculo de TCU df [ 'TCU{}' . format ( k ) ] = g [ k ] - g [ 'TEU{}' . format ( k ) ] # Cálculo de CP # cols_cp = [c + k for c in ['FOS', 'FOM', 'INT', 'PCAP', 'PMH', 'SAH']] cols_cp = [ c + k for c in COLS_PVPC if c not in [ '' , 'COF' , 'TEU' ] ] df [ 'CP{}' . format ( k ) ] = g [ cols_cp ] . sum ( axis = 1 ) # Cálculo de PERD --> No es posible así, ya que los valores base ya vienen con PERD # dfs_pvpc[k]['PERD{}'.format(k)] = dfs_pvpc[k]['TCU{}'.format(k)] / dfs_pvpc[k]['CP{}'.format(k)] # dfs_pvpc[k]['PERD{}'.format(k)] = dfs_pvpc[k]['INT{}'.format(k)] / 1.92 # Cálculo de FEU diario cols_k = [ 'TEU' + k , 'TCU' + k , 'COF' + k ] g = df [ cols_k ] . groupby ( 'TEU' + k ) pr = g . apply ( lambda x : x [ 'TCU' + k ] . dot ( x [ 'COF' + k ] ) / x [ 'COF' + k ] . sum ( ) ) pr . name = 'PD_' + k df = df . join ( pr , on = 'TEU' + k , rsuffix = '_r' ) df [ 'PD_' + k ] += df [ 'TEU' + k ] return df
13367	def register_proxy_type ( cls , real_type , proxy_type ) : if distob . engine is None : cls . _initial_proxy_types [ real_type ] = proxy_type elif isinstance ( distob . engine , ObjectHub ) : distob . engine . _runtime_reg_proxy_type ( real_type , proxy_type ) else : # TODO: remove next line after issue #58 in dill is fixed. distob . engine . _singleeng_reg_proxy_type ( real_type , proxy_type ) pass
4870	def to_internal_value ( self , data ) : if not isinstance ( data , list ) : message = self . error_messages [ 'not_a_list' ] . format ( input_type = type ( data ) . __name__ ) raise serializers . ValidationError ( { api_settings . NON_FIELD_ERRORS_KEY : [ message ] } ) ret = [ ] for item in data : try : validated = self . child . run_validation ( item ) except serializers . ValidationError as exc : ret . append ( exc . detail ) else : ret . append ( validated ) return ret
2811	def convert_reshape ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting reshape ...' ) if names == 'short' : tf_name = 'RESH' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if len ( inputs ) > 1 : if layers [ inputs [ 1 ] ] [ 0 ] == - 1 : print ( 'Cannot deduct batch size! It will be omitted, but result may be wrong.' ) reshape = keras . layers . Reshape ( layers [ inputs [ 1 ] + '_np' ] , name = tf_name ) layers [ scope_name ] = reshape ( layers [ inputs [ 0 ] ] ) else : if inputs [ 0 ] in layers : reshape = keras . layers . Reshape ( params [ 'shape' ] [ 1 : ] , name = tf_name ) layers [ scope_name ] = reshape ( layers [ inputs [ 0 ] ] ) else : print ( 'Skip weight matrix transpose, but result may be wrong.' )
13523	def _to_json ( self , resp ) : try : json = resp . json ( ) except ValueError as e : reason = "TMC Server did not send valid JSON: {0}" raise APIError ( reason . format ( repr ( e ) ) ) return json
5244	def missing_info ( * * kwargs ) -> str : func = kwargs . pop ( 'func' , 'unknown' ) if 'ticker' in kwargs : kwargs [ 'ticker' ] = kwargs [ 'ticker' ] . replace ( '/' , '_' ) info = utils . to_str ( kwargs , fmt = '{value}' , sep = '/' ) [ 1 : - 1 ] return f'{func}/{info}'
7625	def pattern ( ref , est , * * kwargs ) : namespace = 'pattern_jku' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_patterns = pattern_to_mireval ( ref ) est_patterns = pattern_to_mireval ( est ) return mir_eval . pattern . evaluate ( ref_patterns , est_patterns , * * kwargs )
12742	def get_ISBNs ( self ) : invalid_isbns = set ( self . get_invalid_ISBNs ( ) ) valid_isbns = [ self . _clean_isbn ( isbn ) for isbn in self [ "020a" ] if self . _clean_isbn ( isbn ) not in invalid_isbns ] if valid_isbns : return valid_isbns # this is used sometimes in czech national library return [ self . _clean_isbn ( isbn ) for isbn in self [ "901i" ] ]
311	def sortino_ratio ( returns , required_return = 0 , period = DAILY ) : return ep . sortino_ratio ( returns , required_return = required_return )
8931	def description ( _dummy_ctx , markdown = False ) : cfg = config . load ( ) markup = 'md' if markdown else 'html' description_file = cfg . rootjoin ( "build/project.{}" . format ( markup ) ) notify . banner ( "Creating {} file for Jenkins..." . format ( description_file ) ) long_description = cfg . project . long_description long_description = long_description . replace ( '\n\n' , '</p>\n<p>' ) long_description = re . sub ( r'(\W)``([^`]+)``(\W)' , r'\1<tt>\2</tt>\3' , long_description ) text = DESCRIPTION_TEMPLATES [ markup ] . format ( keywords = ', ' . join ( cfg . project . keywords ) , classifiers = '\n' . join ( cfg . project . classifiers ) , classifiers_indented = ' ' + '\n ' . join ( cfg . project . classifiers ) , packages = ', ' . join ( cfg . project . packages ) , long_description_html = '<p>{}</p>' . format ( long_description ) , ##data='\n'.join(["%s=%r" % i for i in cfg.project.iteritems()]), * * cfg ) with io . open ( description_file , 'w' , encoding = 'utf-8' ) as handle : handle . write ( text )
6521	def directories ( self , filters = None , containing = None ) : filters = compile_masks ( filters or [ r'.*' ] ) contains = compile_masks ( containing ) for dirname , files in iteritems ( self . _found ) : relpath = text_type ( Path ( dirname ) . relative_to ( self . base_path ) ) if matches_masks ( relpath , filters ) : if not contains or self . _contains ( files , contains ) : yield dirname
4227	def _check_old_config_root ( ) : # disable the check - once is enough and avoids infinite loop globals ( ) [ '_check_old_config_root' ] = lambda : None config_file_new = os . path . join ( _config_root_Linux ( ) , 'keyringrc.cfg' ) config_file_old = os . path . join ( _data_root_Linux ( ) , 'keyringrc.cfg' ) if os . path . isfile ( config_file_old ) and not os . path . isfile ( config_file_new ) : msg = ( "Keyring config exists only in the old location " "{config_file_old} and should be moved to {config_file_new} " "to work with this version of keyring." ) raise RuntimeError ( msg . format ( * * locals ( ) ) )
9971	def _get_range ( book , range_ , sheet ) : filename = None if isinstance ( book , str ) : filename = book book = opxl . load_workbook ( book , data_only = True ) elif isinstance ( book , opxl . Workbook ) : pass else : raise TypeError if _is_range_address ( range_ ) : sheet_names = [ name . upper ( ) for name in book . sheetnames ] index = sheet_names . index ( sheet . upper ( ) ) data = book . worksheets [ index ] [ range_ ] else : data = _get_namedrange ( book , range_ , sheet ) if data is None : raise ValueError ( "Named range '%s' not found in %s" % ( range_ , filename or book ) ) return data
262	def plot_returns ( perf_attrib_data , cost = None , ax = None ) : if ax is None : ax = plt . gca ( ) returns = perf_attrib_data [ 'total_returns' ] total_returns_label = 'Total returns' cumulative_returns_less_costs = _cumulative_returns_less_costs ( returns , cost ) if cost is not None : total_returns_label += ' (adjusted)' specific_returns = perf_attrib_data [ 'specific_returns' ] common_returns = perf_attrib_data [ 'common_returns' ] ax . plot ( cumulative_returns_less_costs , color = 'b' , label = total_returns_label ) ax . plot ( ep . cum_returns ( specific_returns ) , color = 'g' , label = 'Cumulative specific returns' ) ax . plot ( ep . cum_returns ( common_returns ) , color = 'r' , label = 'Cumulative common returns' ) if cost is not None : ax . plot ( - ep . cum_returns ( cost ) , color = 'k' , label = 'Cumulative cost spent' ) ax . set_title ( 'Time series of cumulative returns' ) ax . set_ylabel ( 'Returns' ) configure_legend ( ax ) return ax
13760	def _format_iso_time ( self , time ) : if isinstance ( time , str ) : return time elif isinstance ( time , datetime ) : return time . strftime ( '%Y-%m-%dT%H:%M:%S.%fZ' ) else : return None
10500	def waitForCreation ( self , timeout = 10 , notification = 'AXCreated' ) : callback = AXCallbacks . returnElemCallback retelem = None args = ( retelem , ) return self . waitFor ( timeout , notification , callback = callback , args = args )
8108	def search ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_SEARCH return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
2029	def CALLDATACOPY ( self , mem_offset , data_offset , size ) : if issymbolic ( size ) : if solver . can_be_true ( self . _constraints , size <= len ( self . data ) + 32 ) : self . constraints . add ( size <= len ( self . data ) + 32 ) raise ConcretizeArgument ( 3 , policy = 'SAMPLED' ) if issymbolic ( data_offset ) : if solver . can_be_true ( self . _constraints , data_offset == self . _used_calldata_size ) : self . constraints . add ( data_offset == self . _used_calldata_size ) raise ConcretizeArgument ( 2 , policy = 'SAMPLED' ) #account for calldata usage self . _use_calldata ( data_offset , size ) self . _allocate ( mem_offset , size ) for i in range ( size ) : try : c = Operators . ITEBV ( 8 , data_offset + i < len ( self . data ) , Operators . ORD ( self . data [ data_offset + i ] ) , 0 ) except IndexError : # data_offset + i is concrete and outside data c = 0 self . _store ( mem_offset + i , c )
12376	def assert_operations ( self , * args ) : if not set ( args ) . issubset ( self . allowed_operations ) : raise http . exceptions . Forbidden ( )
12114	def save ( self , filename , imdata , * * data ) : if isinstance ( imdata , numpy . ndarray ) : imdata = Image . fromarray ( numpy . uint8 ( imdata ) ) elif isinstance ( imdata , Image . Image ) : imdata . save ( self . _savepath ( filename ) )
10251	def remove_highlight_nodes ( graph : BELGraph , nodes : Optional [ Iterable [ BaseEntity ] ] = None ) -> None : for node in graph if nodes is None else nodes : if is_node_highlighted ( graph , node ) : del graph . node [ node ] [ NODE_HIGHLIGHT ]
10747	def fetch ( self , url , path , filename ) : logger . debug ( 'initializing download in ' , url ) remote_file_size = self . get_remote_file_size ( url ) if exists ( join ( path , filename ) ) : size = getsize ( join ( path , filename ) ) if size == remote_file_size : logger . error ( '%s already exists on your system' % filename ) print ( '%s already exists on your system' % filename ) return [ join ( path , filename ) , size ] logger . debug ( 'Downloading: %s' % filename ) print ( 'Downloading: %s' % filename ) fetch ( url , path ) print ( 'stored at %s' % path ) logger . debug ( 'stored at %s' % path ) return [ join ( path , filename ) , remote_file_size ]
7170	def train ( self , debug = True , force = False , single_thread = False , timeout = 20 ) : if not self . must_train and not force : return self . padaos . compile ( ) self . train_thread = Thread ( target = self . _train , kwargs = dict ( debug = debug , single_thread = single_thread , timeout = timeout ) , daemon = True ) self . train_thread . start ( ) self . train_thread . join ( timeout ) self . must_train = False return not self . train_thread . is_alive ( )
3059	def _get_backend ( filename ) : filename = os . path . abspath ( filename ) with _backends_lock : if filename not in _backends : _backends [ filename ] = _MultiprocessStorageBackend ( filename ) return _backends [ filename ]
13585	def add_link ( cls , attr , title = '' , display = '' ) : global klass_count klass_count += 1 fn_name = 'dyn_fn_%d' % klass_count cls . list_display . append ( fn_name ) if not title : title = attr . capitalize ( ) # python scoping is a bit weird with default values, if it isn't # referenced the inner function won't see it, so assign it for use _display = display def _link ( self , obj ) : field_obj = admin_obj_attr ( obj , attr ) if not field_obj : return '' text = _obj_display ( field_obj , _display ) return admin_obj_link ( field_obj , text ) _link . short_description = title _link . allow_tags = True _link . admin_order_field = attr setattr ( cls , fn_name , _link )
2467	def set_file_license_in_file ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if validations . validate_file_lics_in_file ( lic ) : self . file ( doc ) . add_lics ( lic ) return True else : raise SPDXValueError ( 'File::LicenseInFile' ) else : raise OrderError ( 'File::LicenseInFile' )
7147	def address ( addr , label = None ) : addr = str ( addr ) if _ADDR_REGEX . match ( addr ) : netbyte = bytearray ( unhexlify ( base58 . decode ( addr ) ) ) [ 0 ] if netbyte in Address . _valid_netbytes : return Address ( addr , label = label ) elif netbyte in SubAddress . _valid_netbytes : return SubAddress ( addr , label = label ) raise ValueError ( "Invalid address netbyte {nb:x}. Allowed values are: {allowed}" . format ( nb = netbyte , allowed = ", " . join ( map ( lambda b : '%02x' % b , sorted ( Address . _valid_netbytes + SubAddress . _valid_netbytes ) ) ) ) ) elif _IADDR_REGEX . match ( addr ) : return IntegratedAddress ( addr ) raise ValueError ( "Address must be either 95 or 106 characters long base58-encoded string, " "is {addr} ({len} chars length)" . format ( addr = addr , len = len ( addr ) ) )
5321	def get_data ( self , reset_device = False ) : try : if reset_device : self . _device . reset ( ) # detach kernel driver from both interfaces if attached, so we can set_configuration() for interface in [ 0 , 1 ] : if self . _device . is_kernel_driver_active ( interface ) : LOGGER . debug ( 'Detaching kernel driver for interface %d ' 'of %r on ports %r' , interface , self . _device , self . _ports ) self . _device . detach_kernel_driver ( interface ) self . _device . set_configuration ( ) # Prevent kernel message: # "usbfs: process <PID> (python) did not claim interface x before use" # This will become unnecessary once pull-request #124 for # PyUSB has been accepted and we depend on a fixed release # of PyUSB. Until then, and even with the fix applied, it # does not hurt to explicitly claim the interface. usb . util . claim_interface ( self . _device , INTERFACE ) # Turns out we don't actually need that ctrl_transfer. # Disabling this reduces number of USBErrors from ~7/30 to 0! #self._device.ctrl_transfer(bmRequestType=0x21, bRequest=0x09, # wValue=0x0201, wIndex=0x00, data_or_wLength='\x01\x01', # timeout=TIMEOUT) # Magic: Our TEMPerV1.4 likes to be asked twice. When # only asked once, it get's stuck on the next access and # requires a reset. self . _control_transfer ( COMMANDS [ 'temp' ] ) self . _interrupt_read ( ) # Turns out a whole lot of that magic seems unnecessary. #self._control_transfer(COMMANDS['ini1']) #self._interrupt_read() #self._control_transfer(COMMANDS['ini2']) #self._interrupt_read() #self._interrupt_read() # Get temperature self . _control_transfer ( COMMANDS [ 'temp' ] ) temp_data = self . _interrupt_read ( ) # Get humidity if self . _device . product == 'TEMPer1F_H1_V1.4' : humidity_data = temp_data else : humidity_data = None # Combine temperature and humidity data data = { 'temp_data' : temp_data , 'humidity_data' : humidity_data } # Be a nice citizen and undo potential interface claiming. # Also see: https://github.com/walac/pyusb/blob/master/docs/tutorial.rst#dont-be-selfish usb . util . dispose_resources ( self . _device ) return data except usb . USBError as err : if not reset_device : LOGGER . warning ( "Encountered %s, resetting %r and trying again." , err , self . _device ) return self . get_data ( True ) # Catch the permissions exception and add our message if "not permitted" in str ( err ) : raise Exception ( "Permission problem accessing USB. " "Maybe I need to run as root?" ) else : LOGGER . error ( err ) raise
6385	def pairwise_similarity_statistics ( src_collection , tar_collection , metric = sim , mean_func = amean , symmetric = False , ) : if not callable ( mean_func ) : raise ValueError ( 'mean_func must be a function' ) if not callable ( metric ) : raise ValueError ( 'metric must be a function' ) if hasattr ( src_collection , 'split' ) : src_collection = src_collection . split ( ) if not hasattr ( src_collection , '__iter__' ) : raise ValueError ( 'src_collection is neither a string nor iterable' ) if hasattr ( tar_collection , 'split' ) : tar_collection = tar_collection . split ( ) if not hasattr ( tar_collection , '__iter__' ) : raise ValueError ( 'tar_collection is neither a string nor iterable' ) src_collection = list ( src_collection ) tar_collection = list ( tar_collection ) pairwise_values = [ ] for src in src_collection : for tar in tar_collection : pairwise_values . append ( metric ( src , tar ) ) if symmetric : pairwise_values . append ( metric ( tar , src ) ) return ( max ( pairwise_values ) , min ( pairwise_values ) , mean_func ( pairwise_values ) , std ( pairwise_values , mean_func , 0 ) , )
1873	def MOVLPD ( cpu , dest , src ) : value = src . read ( ) if src . size == 64 and dest . size == 128 : value = ( dest . read ( ) & 0xffffffffffffffff0000000000000000 ) | Operators . ZEXTEND ( value , 128 ) dest . write ( value )
9658	def get_sinks ( G ) : sinks = [ ] for node in G : if not len ( list ( G . successors ( node ) ) ) : sinks . append ( node ) return sinks
8303	def find_example_dir ( ) : # Replace %s with directory to check for shoebot menus. code_stub = textwrap . dedent ( """ from pkg_resources import resource_filename, Requirement, DistributionNotFound try: print(resource_filename(Requirement.parse('shoebot'), '%s')) except DistributionNotFound: pass """ ) # Needs to run in same python env as shoebot (may be different to gedits) code = code_stub % 'share/shoebot/examples' cmd = [ "python" , "-c" , code ] p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) output , errors = p . communicate ( ) if errors : print ( 'Shoebot experienced errors searching for install and examples.' ) print ( 'Errors:\n{0}' . format ( errors . decode ( 'utf-8' ) ) ) return None else : examples_dir = output . decode ( 'utf-8' ) . strip ( ) if os . path . isdir ( examples_dir ) : return examples_dir # If user is running 'setup.py develop' then examples could be right here #code = "from pkg_resources import resource_filename, Requirement; print resource_filename(Requirement.parse('shoebot'), 'examples/')" code = code_stub % 'examples/' cmd = [ "python" , "-c" , code ] p = subprocess . Popen ( cmd , stdout = subprocess . PIPE ) output , errors = p . communicate ( ) examples_dir = output . decode ( 'utf-8' ) . strip ( ) if os . path . isdir ( examples_dir ) : return examples_dir if examples_dir : print ( 'Shoebot could not find examples at: {0}' . format ( examples_dir ) ) else : print ( 'Shoebot could not find install dir and examples.' )
9250	def filter_issues_for_tags ( self , newer_tag , older_tag ) : filtered_pull_requests = self . delete_by_time ( self . pull_requests , older_tag , newer_tag ) filtered_issues = self . delete_by_time ( self . issues , older_tag , newer_tag ) newer_tag_name = newer_tag [ "name" ] if newer_tag else None if self . options . filter_issues_by_milestone : # delete excess irrelevant issues (according milestones).Issue #22. filtered_issues = self . filter_by_milestone ( filtered_issues , newer_tag_name , self . issues ) filtered_pull_requests = self . filter_by_milestone ( filtered_pull_requests , newer_tag_name , self . pull_requests ) return filtered_issues , filtered_pull_requests
7244	def _parse_geoms ( self , * * kwargs ) : bbox = kwargs . get ( 'bbox' , None ) wkt_geom = kwargs . get ( 'wkt' , None ) geojson = kwargs . get ( 'geojson' , None ) if bbox is not None : g = box ( * bbox ) elif wkt_geom is not None : g = wkt . loads ( wkt_geom ) elif geojson is not None : g = shape ( geojson ) else : return None if self . proj is None : return g else : return self . _reproject ( g , from_proj = kwargs . get ( 'from_proj' , 'EPSG:4326' ) )
13471	def apply_changesets ( args , changesets , catalog ) : tmpdir = tempfile . mkdtemp ( ) tmp_patch = join ( tmpdir , "tmp.patch" ) tmp_lcat = join ( tmpdir , "tmp.lcat" ) for node in changesets : remove ( tmp_patch ) copy ( node . mfile [ 'changeset' ] [ 'filename' ] , tmp_patch ) logging . info ( "mv %s %s" % ( catalog , tmp_lcat ) ) shutil . move ( catalog , tmp_lcat ) cmd = args . patch_cmd . replace ( "$in1" , tmp_lcat ) . replace ( "$patch" , tmp_patch ) . replace ( "$out" , catalog ) logging . info ( "Patch: %s" % cmd ) subprocess . check_call ( cmd , shell = True ) shutil . rmtree ( tmpdir , ignore_errors = True )
12109	def _review_all ( self , launchers ) : # Run review of launch args if necessary if self . launch_args is not None : proceed = self . review_args ( self . launch_args , show_repr = True , heading = 'Meta Arguments' ) if not proceed : return False reviewers = [ self . review_args , self . review_command , self . review_launcher ] for ( count , launcher ) in enumerate ( launchers ) : # Run reviews for all launchers if desired... if not all ( reviewer ( launcher ) for reviewer in reviewers ) : print ( "\n == Aborting launch ==" ) return False # But allow the user to skip these extra reviews if len ( launchers ) != 1 and count < len ( launchers ) - 1 : skip_remaining = self . input_options ( [ 'Y' , 'n' , 'quit' ] , '\nSkip remaining reviews?' , default = 'y' ) if skip_remaining == 'y' : break elif skip_remaining == 'quit' : return False if self . input_options ( [ 'y' , 'N' ] , 'Execute?' , default = 'n' ) != 'y' : return False else : return self . _launch_all ( launchers )
131	def is_partly_within_image ( self , image ) : return not self . is_out_of_image ( image , fully = True , partly = False )
7303	def set_mongonaut_base ( self ) : if hasattr ( self , "app_label" ) : # prevents us from calling this multiple times return None self . app_label = self . kwargs . get ( 'app_label' ) self . document_name = self . kwargs . get ( 'document_name' ) # TODO Allow this to be assigned via url variable self . models_name = self . kwargs . get ( 'models_name' , 'models' ) # import the models file self . model_name = "{0}.{1}" . format ( self . app_label , self . models_name ) self . models = import_module ( self . model_name )
10324	def microcanonical_averages_arrays ( microcanonical_averages ) : ret = dict ( ) for n , microcanonical_average in enumerate ( microcanonical_averages ) : assert n == microcanonical_average [ 'n' ] if n == 0 : num_edges = microcanonical_average [ 'M' ] num_sites = microcanonical_average [ 'N' ] spanning_cluster = ( 'spanning_cluster' in microcanonical_average ) ret [ 'max_cluster_size' ] = np . empty ( num_edges + 1 ) ret [ 'max_cluster_size_ci' ] = np . empty ( ( num_edges + 1 , 2 ) ) if spanning_cluster : ret [ 'spanning_cluster' ] = np . empty ( num_edges + 1 ) ret [ 'spanning_cluster_ci' ] = np . empty ( ( num_edges + 1 , 2 ) ) ret [ 'moments' ] = np . empty ( ( 5 , num_edges + 1 ) ) ret [ 'moments_ci' ] = np . empty ( ( 5 , num_edges + 1 , 2 ) ) ret [ 'max_cluster_size' ] [ n ] = microcanonical_average [ 'max_cluster_size' ] ret [ 'max_cluster_size_ci' ] [ n ] = ( microcanonical_average [ 'max_cluster_size_ci' ] ) if spanning_cluster : ret [ 'spanning_cluster' ] [ n ] = ( microcanonical_average [ 'spanning_cluster' ] ) ret [ 'spanning_cluster_ci' ] [ n ] = ( microcanonical_average [ 'spanning_cluster_ci' ] ) ret [ 'moments' ] [ : , n ] = microcanonical_average [ 'moments' ] ret [ 'moments_ci' ] [ : , n ] = microcanonical_average [ 'moments_ci' ] # normalize by number of sites for key in ret : if 'spanning_cluster' in key : continue ret [ key ] /= num_sites ret [ 'M' ] = num_edges ret [ 'N' ] = num_sites return ret
3151	def update ( self , list_id , webhook_id , data ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) , data = data )
13039	def overview ( ) : search = Credential . search ( ) search . aggs . bucket ( 'password_count' , 'terms' , field = 'secret' , order = { '_count' : 'desc' } , size = 20 ) . metric ( 'username_count' , 'cardinality' , field = 'username' ) . metric ( 'host_count' , 'cardinality' , field = 'host_ip' ) . metric ( 'top_hits' , 'top_hits' , docvalue_fields = [ 'username' ] , size = 100 ) response = search . execute ( ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( "Secret" , "Count" , "Hosts" , "Users" , "Usernames" ) ) print_line ( "-" * 100 ) for entry in response . aggregations . password_count . buckets : usernames = [ ] for creds in entry . top_hits : usernames . append ( creds . username [ 0 ] ) usernames = list ( set ( usernames ) ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( entry . key , entry . doc_count , entry . host_count . value , entry . username_count . value , usernames ) )
11348	def is_instance ( self ) : ret = False val = self . callback if self . is_class ( ) : return False ret = not inspect . isfunction ( val ) and not inspect . ismethod ( val ) # if is_py2: # ret = isinstance(val, types.InstanceType) or hasattr(val, '__dict__') \ # and not (hasattr(val, 'func_name') or hasattr(val, 'im_func')) # # else: # ret = not inspect.isfunction(val) and not inspect.ismethod(val) return ret
6318	def _find_last_of ( self , path , finders ) : found_path = None for finder in finders : result = finder . find ( path ) if result : found_path = result return found_path
6465	def size ( self ) : for fd in range ( 3 ) : cr = self . _ioctl_GWINSZ ( fd ) if cr : break if not cr : try : fd = os . open ( os . ctermid ( ) , os . O_RDONLY ) cr = self . _ioctl_GWINSZ ( fd ) os . close ( fd ) except Exception : pass if not cr : env = os . environ cr = ( env . get ( 'LINES' , 25 ) , env . get ( 'COLUMNS' , 80 ) ) return int ( cr [ 1 ] ) , int ( cr [ 0 ] )
167	def is_fully_within_image ( self , image , default = False ) : if len ( self . coords ) == 0 : return default return np . all ( self . get_pointwise_inside_image_mask ( image ) )
7444	def _step2func ( self , samples , force , ipyclient ) : ## print header if self . _headers : print ( "\n Step 2: Filtering reads " ) ## If no samples in this assembly then it means you skipped step1, if not self . samples . keys ( ) : raise IPyradWarningExit ( FIRST_RUN_1 ) ## Get sample objects from list of strings, if API. samples = _get_samples ( self , samples ) if not force : ## print warning and skip if all are finished if all ( [ i . stats . state >= 2 for i in samples ] ) : print ( EDITS_EXIST . format ( len ( samples ) ) ) return ## Run samples through rawedit assemble . rawedit . run2 ( self , samples , force , ipyclient )
2437	def add_review_date ( self , doc , reviewed ) : if len ( doc . reviews ) != 0 : if not self . review_date_set : self . review_date_set = True date = utils . datetime_from_iso_format ( reviewed ) if date is not None : doc . reviews [ - 1 ] . review_date = date return True else : raise SPDXValueError ( 'Review::ReviewDate' ) else : raise CardinalityError ( 'Review::ReviewDate' ) else : raise OrderError ( 'Review::ReviewDate' )
8872	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : for t in self . regex : m = t . Regex . search ( l ) if m != None : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}' . format ( str ( truePosition + 1 ) ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . failed = True self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) raise DirectiveException ( self )
12658	def append_dict_values ( list_of_dicts , keys = None ) : if keys is None : keys = list ( list_of_dicts [ 0 ] . keys ( ) ) dict_of_lists = DefaultOrderedDict ( list ) for d in list_of_dicts : for k in keys : dict_of_lists [ k ] . append ( d [ k ] ) return dict_of_lists
1432	def is_grouping_sane ( cls , gtype ) : if gtype == cls . SHUFFLE or gtype == cls . ALL or gtype == cls . LOWEST or gtype == cls . NONE : return True elif isinstance ( gtype , cls . FIELDS ) : return gtype . gtype == topology_pb2 . Grouping . Value ( "FIELDS" ) and gtype . fields is not None elif isinstance ( gtype , cls . CUSTOM ) : return gtype . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) and gtype . python_serialized is not None else : #pylint: disable=fixme #TODO: DIRECT are not supported yet return False
13794	def handle_map_doc ( self , document ) : # This uses the stored set of functions, sorted by order of addition. for function in sorted ( self . functions . values ( ) , key = lambda x : x [ 0 ] ) : try : # It has to be run through ``list``, because it may be a # generator function. yield [ list ( function ( document ) ) ] except Exception , exc : # Otherwise, return an empty list and log the event. yield [ ] self . log ( repr ( exc ) )
10701	def set_state ( _id , body ) : url = DEVICE_URL % _id if "mode" in body : url = MODES_URL % _id arequest = requests . put ( url , headers = HEADERS , data = json . dumps ( body ) ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "State not accepted. " + status_code ) return False
6815	def enable_mods ( self ) : r = self . local_renderer for mod_name in r . env . mods_enabled : with self . settings ( warn_only = True ) : self . enable_mod ( mod_name )
9222	def reverse_guard ( lst ) : rev = { '<' : '>=' , '>' : '=<' , '>=' : '<' , '=<' : '>' } return [ rev [ l ] if l in rev else l for l in lst ]
13717	def request ( self , batch , attempt = 0 ) : try : q = self . api . new_queue ( ) for msg in batch : q . add ( msg [ 'event' ] , msg [ 'value' ] , source = msg [ 'source' ] ) q . submit ( ) except : if attempt > self . retries : raise self . request ( batch , attempt + 1 )
5675	def get_shape_distance_between_stops ( self , trip_I , from_stop_seq , to_stop_seq ) : query_template = "SELECT shape_break FROM stop_times WHERE trip_I={trip_I} AND seq={seq} " stop_seqs = [ from_stop_seq , to_stop_seq ] shape_breaks = [ ] for seq in stop_seqs : q = query_template . format ( seq = seq , trip_I = trip_I ) shape_breaks . append ( self . conn . execute ( q ) . fetchone ( ) ) query_template = "SELECT max(d) - min(d) " "FROM shapes JOIN trips ON(trips.shape_id=shapes.shape_id) " "WHERE trip_I={trip_I} AND shapes.seq>={from_stop_seq} AND shapes.seq<={to_stop_seq};" distance_query = query_template . format ( trip_I = trip_I , from_stop_seq = from_stop_seq , to_stop_seq = to_stop_seq ) return self . conn . execute ( distance_query ) . fetchone ( ) [ 0 ]
10010	def get_command_names ( ) : ret = [ ] for f in os . listdir ( COMMAND_MODULE_PATH ) : if os . path . isfile ( os . path . join ( COMMAND_MODULE_PATH , f ) ) and f . endswith ( COMMAND_MODULE_SUFFIX ) : ret . append ( f [ : - len ( COMMAND_MODULE_SUFFIX ) ] ) return ret
1215	def _wait_state ( self , state , reward , terminal ) : while state == [ None ] or not state : state , terminal , reward = self . _execute ( dict ( key = 0 ) ) return state , terminal , reward
8046	def parse_definitions ( self , class_ , all = False ) : while self . current is not None : self . log . debug ( "parsing definition list, current token is %r (%s)" , self . current . kind , self . current . value , ) self . log . debug ( "got_newline: %s" , self . stream . got_logical_newline ) if all and self . current . value == "__all__" : self . parse_all ( ) elif ( self . current . kind == tk . OP and self . current . value == "@" and self . stream . got_logical_newline ) : self . consume ( tk . OP ) self . parse_decorators ( ) elif self . current . value in [ "def" , "class" ] : yield self . parse_definition ( class_ . _nest ( self . current . value ) ) elif self . current . kind == tk . INDENT : self . consume ( tk . INDENT ) for definition in self . parse_definitions ( class_ ) : yield definition elif self . current . kind == tk . DEDENT : self . consume ( tk . DEDENT ) return elif self . current . value == "from" : self . parse_from_import_statement ( ) else : self . stream . move ( )
10895	def set_filter ( self , slices , values ) : self . filters = [ [ sl , values [ sl ] ] for sl in slices ]
2578	def submit ( self , func , * args , executors = 'all' , fn_hash = None , cache = False , * * kwargs ) : if self . cleanup_called : raise ValueError ( "Cannot submit to a DFK that has been cleaned up" ) task_id = self . task_count self . task_count += 1 if isinstance ( executors , str ) and executors . lower ( ) == 'all' : choices = list ( e for e in self . executors if e != 'data_manager' ) elif isinstance ( executors , list ) : choices = executors executor = random . choice ( choices ) # Transform remote input files to data futures args , kwargs = self . _add_input_deps ( executor , args , kwargs ) task_def = { 'depends' : None , 'executor' : executor , 'func' : func , 'func_name' : func . __name__ , 'args' : args , 'kwargs' : kwargs , 'fn_hash' : fn_hash , 'memoize' : cache , 'callback' : None , 'exec_fu' : None , 'checkpoint' : None , 'fail_count' : 0 , 'fail_history' : [ ] , 'env' : None , 'status' : States . unsched , 'id' : task_id , 'time_submitted' : None , 'time_returned' : None , 'app_fu' : None } if task_id in self . tasks : raise DuplicateTaskError ( "internal consistency error: Task {0} already exists in task list" . format ( task_id ) ) else : self . tasks [ task_id ] = task_def # Get the dep count and a list of dependencies for the task dep_cnt , depends = self . _gather_all_deps ( args , kwargs ) self . tasks [ task_id ] [ 'depends' ] = depends # Extract stdout and stderr to pass to AppFuture: task_stdout = kwargs . get ( 'stdout' ) task_stderr = kwargs . get ( 'stderr' ) logger . info ( "Task {} submitted for App {}, waiting on tasks {}" . format ( task_id , task_def [ 'func_name' ] , [ fu . tid for fu in depends ] ) ) self . tasks [ task_id ] [ 'task_launch_lock' ] = threading . Lock ( ) app_fu = AppFuture ( tid = task_id , stdout = task_stdout , stderr = task_stderr ) self . tasks [ task_id ] [ 'app_fu' ] = app_fu app_fu . add_done_callback ( partial ( self . handle_app_update , task_id ) ) self . tasks [ task_id ] [ 'status' ] = States . pending logger . debug ( "Task {} set to pending state with AppFuture: {}" . format ( task_id , task_def [ 'app_fu' ] ) ) # at this point add callbacks to all dependencies to do a launch_if_ready # call whenever a dependency completes. # we need to be careful about the order of setting the state to pending, # adding the callbacks, and caling launch_if_ready explicitly once always below. # I think as long as we call launch_if_ready once after setting pending, then # we can add the callback dependencies at any point: if the callbacks all fire # before then, they won't cause a launch, but the one below will. if they fire # after we set it pending, then the last one will cause a launch, and the # explicit one won't. for d in depends : def callback_adapter ( dep_fut ) : self . launch_if_ready ( task_id ) try : d . add_done_callback ( callback_adapter ) except Exception as e : logger . error ( "add_done_callback got an exception {} which will be ignored" . format ( e ) ) self . launch_if_ready ( task_id ) return task_def [ 'app_fu' ]
1636	def CheckForFunctionLengths ( filename , clean_lines , linenum , function_state , error ) : lines = clean_lines . lines line = lines [ linenum ] joined_line = '' starting_func = False regexp = r'(\w(\w|::|\*|\&|\s)*)\(' # decls * & space::name( ... match_result = Match ( regexp , line ) if match_result : # If the name is all caps and underscores, figure it's a macro and # ignore it, unless it's TEST or TEST_F. function_name = match_result . group ( 1 ) . split ( ) [ - 1 ] if function_name == 'TEST' or function_name == 'TEST_F' or ( not Match ( r'[A-Z_]+$' , function_name ) ) : starting_func = True if starting_func : body_found = False for start_linenum in range ( linenum , clean_lines . NumLines ( ) ) : start_line = lines [ start_linenum ] joined_line += ' ' + start_line . lstrip ( ) if Search ( r'(;|})' , start_line ) : # Declarations and trivial functions body_found = True break # ... ignore elif Search ( r'{' , start_line ) : body_found = True function = Search ( r'((\w|:)*)\(' , line ) . group ( 1 ) if Match ( r'TEST' , function ) : # Handle TEST... macros parameter_regexp = Search ( r'(\(.*\))' , joined_line ) if parameter_regexp : # Ignore bad syntax function += parameter_regexp . group ( 1 ) else : function += '()' function_state . Begin ( function ) break if not body_found : # No body for the function (or evidence of a non-function) was found. error ( filename , linenum , 'readability/fn_size' , 5 , 'Lint failed to find start of function body.' ) elif Match ( r'^\}\s*$' , line ) : # function end function_state . Check ( error , filename , linenum ) function_state . End ( ) elif not Match ( r'^\s*$' , line ) : function_state . Count ( )
1741	def add_inputs ( self , xs ) : states = [ ] cur = self for x in xs : cur = cur . add_input ( x ) states . append ( cur ) return states
7962	def _close ( self ) : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) if self . _socket is None : return try : self . _socket . shutdown ( socket . SHUT_RDWR ) except socket . error : pass self . _socket . close ( ) self . _socket = None self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
6049	def set_defaults ( key ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( phase , new_value ) : new_value = new_value or [ ] for item in new_value : # noinspection PyTypeChecker galaxy = new_value [ item ] if isinstance ( item , str ) else item galaxy . redshift = galaxy . redshift or conf . instance . general . get ( "redshift" , key , float ) return func ( phase , new_value ) return wrapper return decorator
12206	def raise_for_status ( response ) : for err_name in web_exceptions . __all__ : err = getattr ( web_exceptions , err_name ) if err . status_code == response . status : payload = dict ( headers = response . headers , reason = response . reason , ) if issubclass ( err , web_exceptions . _HTTPMove ) : # pylint: disable=protected-access raise err ( response . headers [ 'Location' ] , * * payload ) raise err ( * * payload )
7419	def sample_cleanup ( data , sample ) : umap1file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap1.fastq" ) umap2file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap2.fastq" ) unmapped = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) samplesam = os . path . join ( data . dirs . refmapping , sample . name + ".sam" ) split1 = os . path . join ( data . dirs . edits , sample . name + "-split1.fastq" ) split2 = os . path . join ( data . dirs . edits , sample . name + "-split2.fastq" ) refmap_derep = os . path . join ( data . dirs . edits , sample . name + "-refmap_derep.fastq" ) for f in [ umap1file , umap2file , unmapped , samplesam , split1 , split2 , refmap_derep ] : try : os . remove ( f ) except : pass
11828	def exact_sqrt ( n2 ) : n = int ( math . sqrt ( n2 ) ) assert n * n == n2 return n
7686	def downbeat ( annotation , sr = 22050 , length = None , * * kwargs ) : beat_click = mkclick ( 440 * 2 , sr = sr ) downbeat_click = mkclick ( 440 * 3 , sr = sr ) intervals , values = annotation . to_interval_values ( ) beats , downbeats = [ ] , [ ] for time , value in zip ( intervals [ : , 0 ] , values ) : if value [ 'position' ] == 1 : downbeats . append ( time ) else : beats . append ( time ) if length is None : length = int ( sr * np . max ( intervals ) ) + len ( beat_click ) + 1 y = filter_kwargs ( mir_eval . sonify . clicks , np . asarray ( beats ) , fs = sr , length = length , click = beat_click ) y += filter_kwargs ( mir_eval . sonify . clicks , np . asarray ( downbeats ) , fs = sr , length = length , click = downbeat_click ) return y
5972	def generate_submit_scripts ( templates , prefix = None , deffnm = 'md' , jobname = 'MD' , budget = None , mdrun_opts = None , walltime = 1.0 , jobarray_string = None , startdir = None , npme = None , * * kwargs ) : if not jobname [ 0 ] . isalpha ( ) : jobname = 'MD_' + jobname wmsg = "To make the jobname legal it must start with a letter: changed to {0!r}" . format ( jobname ) logger . warn ( wmsg ) warnings . warn ( wmsg , category = AutoCorrectionWarning ) if prefix is None : prefix = "" if mdrun_opts is not None : mdrun_opts = '"' + str ( mdrun_opts ) + '"' # TODO: could test if quotes already present dirname = kwargs . pop ( 'dirname' , os . path . curdir ) wt = Timedelta ( hours = walltime ) walltime = wt . strftime ( "%h:%M:%S" ) wall_hours = wt . ashours def write_script ( template ) : submitscript = os . path . join ( dirname , prefix + os . path . basename ( template ) ) logger . info ( "Setting up queuing system script {submitscript!r}..." . format ( * * vars ( ) ) ) # These substitution rules are documented for the user in the module doc string qsystem = detect_queuing_system ( template ) if qsystem is not None and ( qsystem . name == 'Slurm' ) : cbook . edit_txt ( template , [ ( '^ *DEFFNM=' , '(?<==)(.*)' , deffnm ) , ( '^#.*(-J)' , '((?<=-J\s))\s*\w+' , jobname ) , ( '^#.*(-A|account_no)' , '((?<=-A\s)|(?<=account_no\s))\s*\w+' , budget ) , ( '^#.*(-t)' , '(?<=-t\s)(\d+:\d+:\d+)' , walltime ) , ( '^ *WALL_HOURS=' , '(?<==)(.*)' , wall_hours ) , ( '^ *STARTDIR=' , '(?<==)(.*)' , startdir ) , ( '^ *NPME=' , '(?<==)(.*)' , npme ) , ( '^ *MDRUN_OPTS=' , '(?<==)("")' , mdrun_opts ) , # only replace literal "" ( '^# JOB_ARRAY_PLACEHOLDER' , '^.*$' , jobarray_string ) , ] , newname = submitscript ) ext = os . path . splitext ( submitscript ) [ 1 ] else : cbook . edit_txt ( template , [ ( '^ *DEFFNM=' , '(?<==)(.*)' , deffnm ) , ( '^#.*(-N|job_name)' , '((?<=-N\s)|(?<=job_name\s))\s*\w+' , jobname ) , ( '^#.*(-A|account_no)' , '((?<=-A\s)|(?<=account_no\s))\s*\w+' , budget ) , ( '^#.*(-l walltime|wall_clock_limit)' , '(?<==)(\d+:\d+:\d+)' , walltime ) , ( '^ *WALL_HOURS=' , '(?<==)(.*)' , wall_hours ) , ( '^ *STARTDIR=' , '(?<==)(.*)' , startdir ) , ( '^ *NPME=' , '(?<==)(.*)' , npme ) , ( '^ *MDRUN_OPTS=' , '(?<==)("")' , mdrun_opts ) , # only replace literal "" ( '^# JOB_ARRAY_PLACEHOLDER' , '^.*$' , jobarray_string ) , ] , newname = submitscript ) ext = os . path . splitext ( submitscript ) [ 1 ] if ext in ( '.sh' , '.csh' , '.bash' ) : os . chmod ( submitscript , 0o755 ) return submitscript return [ write_script ( template ) for template in config . get_templates ( templates ) ]
1192	def translate ( pat ) : i , n = 0 , len ( pat ) res = '' while i < n : c = pat [ i ] i = i + 1 if c == '*' : res = res + '.*' elif c == '?' : res = res + '.' elif c == '[' : j = i if j < n and pat [ j ] == '!' : j = j + 1 if j < n and pat [ j ] == ']' : j = j + 1 while j < n and pat [ j ] != ']' : j = j + 1 if j >= n : res = res + '\\[' else : stuff = pat [ i : j ] . replace ( '\\' , '\\\\' ) i = j + 1 if stuff [ 0 ] == '!' : stuff = '^' + stuff [ 1 : ] elif stuff [ 0 ] == '^' : stuff = '\\' + stuff res = '%s[%s]' % ( res , stuff ) else : res = res + re . escape ( c ) return res + '\Z(?ms)'
12947	def saveToExternal ( self , redisCon ) : if type ( redisCon ) == dict : conn = redis . Redis ( * * redisCon ) elif hasattr ( conn , '__class__' ) and issubclass ( conn . __class__ , redis . Redis ) : conn = redisCon else : raise ValueError ( 'saveToExternal "redisCon" param must either be a dictionary of connection parameters, or redis.Redis, or extension thereof' ) saver = self . saver # Fetch next PK from external forceID = saver . _getNextID ( conn ) # Redundant because of changes in save method myCopy = self . copy ( False ) return saver . save ( myCopy , usePipeline = True , forceID = forceID , conn = conn )
6513	def _most_popular_gender ( self , name , counter ) : if name not in self . names : return self . unknown_value max_count , max_tie = ( 0 , 0 ) best = self . names [ name ] . keys ( ) [ 0 ] for gender , country_values in self . names [ name ] . items ( ) : count , tie = counter ( country_values ) if count > max_count or ( count == max_count and tie > max_tie ) : max_count , max_tie , best = count , tie , gender return best if max_count > 0 else self . unknown_value
5680	def get_trip_counts_per_day ( self ) : query = "SELECT date, count(*) AS number_of_trips FROM day_trips GROUP BY date" # this yields the actual data trip_counts_per_day = pd . read_sql_query ( query , self . conn , index_col = "date" ) # the rest is simply code for filling out "gaps" in the time span # (necessary for some visualizations) max_day = trip_counts_per_day . index . max ( ) min_day = trip_counts_per_day . index . min ( ) min_date = datetime . datetime . strptime ( min_day , '%Y-%m-%d' ) max_date = datetime . datetime . strptime ( max_day , '%Y-%m-%d' ) num_days = ( max_date - min_date ) . days dates = [ min_date + datetime . timedelta ( days = x ) for x in range ( num_days + 1 ) ] trip_counts = [ ] date_strings = [ ] for date in dates : date_string = date . strftime ( "%Y-%m-%d" ) date_strings . append ( date_string ) try : value = trip_counts_per_day . loc [ date_string , 'number_of_trips' ] except KeyError : # set value to 0 if dsut is not present, i.e. when no trips # take place on that day value = 0 trip_counts . append ( value ) # check that all date_strings are included (move this to tests?) for date_string in trip_counts_per_day . index : assert date_string in date_strings data = { "date" : dates , "date_str" : date_strings , "trip_counts" : trip_counts } return pd . DataFrame ( data )
6991	def cp2png ( checkplotin , extrarows = None ) : if checkplotin . endswith ( '.gz' ) : outfile = checkplotin . replace ( '.pkl.gz' , '.png' ) else : outfile = checkplotin . replace ( '.pkl' , '.png' ) return checkplot_pickle_to_png ( checkplotin , outfile , extrarows = extrarows )
7326	def with_continuations ( * * c ) : if len ( c ) : keys , k = zip ( * c . items ( ) ) else : keys , k = tuple ( [ ] ) , tuple ( [ ] ) def d ( f ) : return C ( lambda kself , * conts : lambda * args : f ( * args , self = kself , * * dict ( zip ( keys , conts ) ) ) ) ( * k ) return d
3717	def economic_status ( CASRN , Method = None , AvailableMethods = False ) : # pragma: no cover load_economic_data ( ) CASi = CAS2int ( CASRN ) def list_methods ( ) : methods = [ ] methods . append ( 'Combined' ) if CASRN in _EPACDRDict : methods . append ( EPACDR ) if CASRN in _ECHATonnageDict : methods . append ( ECHA ) if CASi in HPV_data . index : methods . append ( OECD ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section if Method == EPACDR : status = 'US public: ' + str ( _EPACDRDict [ CASRN ] ) elif Method == ECHA : status = _ECHATonnageDict [ CASRN ] elif Method == OECD : status = 'OECD HPV Chemicals' elif Method == 'Combined' : status = [ ] if CASRN in _EPACDRDict : status += [ 'US public: ' + str ( _EPACDRDict [ CASRN ] ) ] if CASRN in _ECHATonnageDict : status += _ECHATonnageDict [ CASRN ] if CASi in HPV_data . index : status += [ 'OECD HPV Chemicals' ] elif Method == NONE : status = None else : raise Exception ( 'Failure in in function' ) return status
8689	def _construct_key ( self , values ) : key = { } for column , value in zip ( self . keys . columns , values ) : key . update ( { column . name : value } ) return key
705	def recordModelProgress ( self , modelID , modelParams , modelParamsHash , results , completed , completionReason , matured , numRecords ) : if results is None : metricResult = None else : metricResult = results [ 1 ] . values ( ) [ 0 ] # Update our database. errScore = self . _resultsDB . update ( modelID = modelID , modelParams = modelParams , modelParamsHash = modelParamsHash , metricResult = metricResult , completed = completed , completionReason = completionReason , matured = matured , numRecords = numRecords ) # Log message. self . logger . debug ( 'Received progress on model %d: completed: %s, ' 'cmpReason: %s, numRecords: %d, errScore: %s' , modelID , completed , completionReason , numRecords , errScore ) # Log best so far. ( bestModelID , bestResult ) = self . _resultsDB . bestModelIdAndErrScore ( ) self . logger . debug ( 'Best err score seen so far: %s on model %s' % ( bestResult , bestModelID ) )
12589	def treefall ( iterable ) : num_elems = len ( iterable ) for i in range ( num_elems , - 1 , - 1 ) : for c in combinations ( iterable , i ) : yield c
10429	def getrowcount ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) return len ( object_handle . AXRows )
7862	def handle_tls_connected_event ( self , event ) : if self . settings [ "tls_verify_peer" ] : valid = self . settings [ "tls_verify_callback" ] ( event . stream , event . peer_certificate ) if not valid : raise SSLError ( "Certificate verification failed" ) event . stream . tls_established = True with event . stream . lock : event . stream . _restart_stream ( )
8921	def _get_version ( self ) : if "version" in self . document . attrib : value = self . document . attrib [ "version" ] . lower ( ) if value in allowed_versions [ self . params [ 'service' ] ] : self . params [ "version" ] = value else : raise OWSInvalidParameterValue ( "Version %s is not supported" % value , value = "version" ) elif self . _get_request_type ( ) == "getcapabilities" : self . params [ "version" ] = None else : raise OWSMissingParameterValue ( 'Parameter "version" is missing' , value = "version" ) return self . params [ "version" ]
7776	def __from_xml ( self , value ) : n = value . children vns = get_node_ns ( value ) while n : if n . type != 'element' : n = n . next continue ns = get_node_ns ( n ) if ( ns and vns and ns . getContent ( ) != vns . getContent ( ) ) : n = n . next continue if n . name == 'POBOX' : self . pobox = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name in ( 'EXTADR' , 'EXTADD' ) : self . extadr = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'STREET' : self . street = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'LOCALITY' : self . locality = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'REGION' : self . region = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'PCODE' : self . pcode = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'CTRY' : self . ctry = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name in ( "HOME" , "WORK" , "POSTAL" , "PARCEL" , "DOM" , "INTL" , "PREF" ) : self . type . append ( n . name . lower ( ) ) n = n . next if self . type == [ ] : self . type = [ "intl" , "postal" , "parcel" , "work" ] elif "dom" in self . type and "intl" in self . type : raise ValueError ( "Both 'dom' and 'intl' specified in vcard ADR" )
3485	def _create_parameter ( model , pid , value , sbo = None , constant = True , units = None , flux_udef = None ) : parameter = model . createParameter ( ) # type: libsbml.Parameter parameter . setId ( pid ) parameter . setValue ( value ) parameter . setConstant ( constant ) if sbo : parameter . setSBOTerm ( sbo ) if units : parameter . setUnits ( flux_udef . getId ( ) )
6058	def bin_up_array_2d_using_mean ( array_2d , bin_up_factor ) : padded_array_2d = pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = array_2d , bin_up_factor = bin_up_factor ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = 0.0 for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 value += padded_array_2d [ padded_y , padded_x ] binned_array_2d [ y , x ] = value / ( bin_up_factor ** 2.0 ) return binned_array_2d
9015	def _row ( self , values ) : row_id = self . _to_id ( values [ ID ] ) row = self . _spec . new_row ( row_id , values , self ) if SAME_AS in values : self . _delay_inheritance ( row , self . _to_id ( values [ SAME_AS ] ) ) self . _delay_instructions ( row ) self . _id_cache [ row_id ] = row return row
11295	def make_request_data ( self , zipcode , city , state ) : data = { 'key' : self . api_key , 'postalcode' : str ( zipcode ) , 'city' : city , 'state' : state } data = ZipTaxClient . _clean_request_data ( data ) return data
11642	def yaml_write_data ( yaml_data , filename ) : with open ( filename , 'w' ) as fd : yaml . dump ( yaml_data , fd , default_flow_style = False ) return True return False
5643	def get_min_visit_time ( self ) : if not self . visit_events : return float ( 'inf' ) else : return min ( self . visit_events , key = lambda event : event . arr_time_ut ) . arr_time_ut
5817	def _write_callback ( connection_id , data_buffer , data_length_pointer ) : try : self = _connection_refs . get ( connection_id ) if not self : socket = _socket_refs . get ( connection_id ) else : socket = self . _socket if not self and not socket : return 0 data_length = deref ( data_length_pointer ) data = bytes_from_buffer ( data_buffer , data_length ) if self and not self . _done_handshake : self . _client_hello += data error = None try : sent = socket . send ( data ) except ( socket_ . error ) as e : error = e . errno if error is not None and error != errno . EAGAIN : if error == errno . ECONNRESET or error == errno . EPIPE : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort if sent != data_length : pointer_set ( data_length_pointer , sent ) return SecurityConst . errSSLWouldBlock return 0 except ( KeyboardInterrupt ) as e : self . _exception = e return SecurityConst . errSSLPeerUserCancelled
11421	def field_xml_output ( field , tag ) : marcxml = [ ] if field [ 3 ] : marcxml . append ( ' <controlfield tag="%s">%s</controlfield>' % ( tag , MathMLParser . html_to_text ( field [ 3 ] ) ) ) else : marcxml . append ( ' <datafield tag="%s" ind1="%s" ind2="%s">' % ( tag , field [ 1 ] , field [ 2 ] ) ) marcxml += [ _subfield_xml_output ( subfield ) for subfield in field [ 0 ] ] marcxml . append ( ' </datafield>' ) return '\n' . join ( marcxml )
6656	def calibrateEB ( variances , sigma2 ) : if ( sigma2 <= 0 or min ( variances ) == max ( variances ) ) : return ( np . maximum ( variances , 0 ) ) sigma = np . sqrt ( sigma2 ) eb_prior = gfit ( variances , sigma ) # Set up a partial execution of the function part = functools . partial ( gbayes , g_est = eb_prior , sigma = sigma ) if len ( variances ) >= 200 : # Interpolate to speed up computations: calib_x = np . percentile ( variances , np . arange ( 0 , 102 , 2 ) ) calib_y = list ( map ( part , calib_x ) ) calib_all = np . interp ( variances , calib_x , calib_y ) else : calib_all = list ( map ( part , variances ) ) return np . asarray ( calib_all )
771	def __constructMetricsModules ( self , metricSpecs ) : if not metricSpecs : return self . __metricSpecs = metricSpecs for spec in metricSpecs : if not InferenceElement . validate ( spec . inferenceElement ) : raise ValueError ( "Invalid inference element for metric spec: %r" % spec ) self . __metrics . append ( metrics . getModule ( spec ) ) self . __metricLabels . append ( spec . getLabel ( ) )
3816	async def _add_channel_services ( self ) : logger . info ( 'Adding channel services...' ) # Based on what Hangouts for Chrome does over 2 requests, this is # trimmed down to 1 request that includes the bare minimum to make # things work. services = [ "babel" , "babel_presence_last_seen" ] map_list = [ dict ( p = json . dumps ( { "3" : { "1" : { "1" : service } } } ) ) for service in services ] await self . _channel . send_maps ( map_list ) logger . info ( 'Channel services added' )
3048	def _implicit_credentials_from_files ( ) : credentials_filename = _get_environment_variable_file ( ) if not credentials_filename : credentials_filename = _get_well_known_file ( ) if os . path . isfile ( credentials_filename ) : extra_help = ( ' (produced automatically when running' ' "gcloud auth login" command)' ) else : credentials_filename = None else : extra_help = ( ' (pointed to by ' + GOOGLE_APPLICATION_CREDENTIALS + ' environment variable)' ) if not credentials_filename : return # If we can read the credentials from a file, we don't need to know # what environment we are in. SETTINGS . env_name = DEFAULT_ENV_NAME try : return _get_application_default_credential_from_file ( credentials_filename ) except ( ApplicationDefaultCredentialsError , ValueError ) as error : _raise_exception_for_reading_json ( credentials_filename , extra_help , error )
4422	async def seek ( self , pos : int ) : await self . _lavalink . ws . send ( op = 'seek' , guildId = self . guild_id , position = pos )
11851	def add_walls ( self ) : for x in range ( self . width ) : self . add_thing ( Wall ( ) , ( x , 0 ) ) self . add_thing ( Wall ( ) , ( x , self . height - 1 ) ) for y in range ( self . height ) : self . add_thing ( Wall ( ) , ( 0 , y ) ) self . add_thing ( Wall ( ) , ( self . width - 1 , y ) )
5863	def add_organization_course ( organization_data , course_key ) : _validate_course_key ( course_key ) _validate_organization_data ( organization_data ) data . create_organization_course ( organization = organization_data , course_key = course_key )
10683	def S ( self , T ) : result = self . Sref for Tmax in sorted ( [ float ( TT ) for TT in self . _Cp_records . keys ( ) ] ) : result += self . _Cp_records [ str ( Tmax ) ] . S ( T ) if T <= Tmax : return result + self . S_mag ( T ) # Extrapolate beyond the upper limit by using a constant heat capacity. Tmax = max ( [ float ( TT ) for TT in self . _Cp_records . keys ( ) ] ) result += self . Cp ( Tmax ) * math . log ( T / Tmax ) return result + self . S_mag ( T )
4006	def pty_fork ( * args ) : updated_env = copy ( os . environ ) updated_env . update ( get_docker_env ( ) ) args += ( updated_env , ) executable = args [ 0 ] demote_fn = demote_to_user ( get_config_value ( constants . CONFIG_MAC_USERNAME_KEY ) ) child_pid , pty_fd = pty . fork ( ) if child_pid == 0 : demote_fn ( ) os . execle ( _executable_path ( executable ) , * args ) else : child_process = psutil . Process ( child_pid ) terminal = os . fdopen ( pty_fd , 'r' , 0 ) with streaming_to_client ( ) : while child_process . status ( ) == 'running' : output = terminal . read ( 1 ) log_to_client ( output ) _ , exit_code = os . waitpid ( child_pid , 0 ) if exit_code != 0 : raise subprocess . CalledProcessError ( exit_code , ' ' . join ( args [ : - 1 ] ) )
7018	def parallel_concat_lcdir ( lcbasedir , objectidlist , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True , nworkers = 32 , maxworkertasks = 1000 ) : if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) tasks = [ ( lcbasedir , x , { 'aperture' : aperture , 'postfix' : postfix , 'sortby' : sortby , 'normalize' : normalize , 'outdir' : outdir , 'recursive' : recursive } ) for x in objectidlist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel_concat_worker , tasks ) pool . close ( ) pool . join ( ) return { x : y for ( x , y ) in zip ( objectidlist , results ) }
12341	def _set_path ( self , path ) : import os . path self . path = os . path . abspath ( path ) self . dirname = os . path . dirname ( path ) self . basename = os . path . basename ( path )
3176	def update ( self , campaign_id , feedback_id , data ) : self . campaign_id = campaign_id self . feedback_id = feedback_id if 'message' not in data : raise KeyError ( 'The campaign feedback must have a message' ) return self . _mc_client . _patch ( url = self . _build_path ( campaign_id , 'feedback' , feedback_id ) , data = data )
13051	def import_nmap ( result , tag , check_function = all_hosts , import_services = False ) : host_search = HostSearch ( arguments = False ) service_search = ServiceSearch ( ) parser = NmapParser ( ) report = parser . parse_fromstring ( result ) imported_hosts = 0 imported_services = 0 for nmap_host in report . hosts : if check_function ( nmap_host ) : imported_hosts += 1 host = host_search . id_to_object ( nmap_host . address ) host . status = nmap_host . status host . add_tag ( tag ) if nmap_host . os_fingerprinted : host . os = nmap_host . os_fingerprint if nmap_host . hostnames : host . hostname . extend ( nmap_host . hostnames ) if import_services : for service in nmap_host . services : imported_services += 1 serv = Service ( * * service . get_dict ( ) ) serv . address = nmap_host . address service_id = service_search . object_to_id ( serv ) if service_id : # Existing object, save the banner and script results. serv_old = Service . get ( service_id ) if service . banner : serv_old . banner = service . banner # TODO implement # if service.script_results: # serv_old.script_results.extend(service.script_results) serv_old . save ( ) else : # New object serv . address = nmap_host . address serv . save ( ) if service . state == 'open' : host . open_ports . append ( service . port ) if service . state == 'closed' : host . closed_ports . append ( service . port ) if service . state == 'filtered' : host . filtered_ports . append ( service . port ) host . save ( ) if imported_hosts : print_success ( "Imported {} hosts, with tag {}" . format ( imported_hosts , tag ) ) else : print_error ( "No hosts found" ) return { 'hosts' : imported_hosts , 'services' : imported_services }
8125	def draw_cornu_bezier ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd , scale , rot ) : s = None for j in range ( 0 , 5 ) : # travel along the function two points at a time (at time t and t2) # the first time through we'll need to get both points # after that we only need the second point because the old second point # becomes the new first point t = j * .2 t2 = t + .2 curvetime = t0 + t * ( t1 - t0 ) curvetime2 = t0 + t2 * ( t1 - t0 ) Dt = ( curvetime2 - curvetime ) * scale if not s : # get first point # avoid calling this again: the next time though x,y will equal x3, y3 s , c = eval_cornu ( curvetime ) s *= flip s -= s0 c -= c0 # calculate derivative of fresnel function at point to get tangent slope # just take the integrand of the fresnel function dx1 = cos ( pow ( curvetime , 2 ) + ( flip * rot ) ) dy1 = flip * sin ( pow ( curvetime , 2 ) + ( flip * rot ) ) # x,y = first point on function x = ( ( c * cs - s * ss ) + x0 ) y = ( ( s * cs + c * ss ) + y0 ) #evaluate the fresnel further along the function to look ahead to the next point s2 , c2 = eval_cornu ( curvetime2 ) s2 *= flip s2 -= s0 c2 -= c0 dx2 = cos ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) dy2 = flip * sin ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) # x3, y3 = second point on function x3 = ( ( c2 * cs - s2 * ss ) + x0 ) y3 = ( ( s2 * cs + c2 * ss ) + y0 ) # calculate control points x1 = ( x + ( ( Dt / 3.0 ) * dx1 ) ) y1 = ( y + ( ( Dt / 3.0 ) * dy1 ) ) x2 = ( x3 - ( ( Dt / 3.0 ) * dx2 ) ) y2 = ( y3 - ( ( Dt / 3.0 ) * dy2 ) ) if cmd == 'moveto' : print_pt ( x , y , cmd ) cmd = 'curveto' print_crv ( x1 , y1 , x2 , y2 , x3 , y3 ) dx1 , dy1 = dx2 , dy2 x , y = x3 , y3 return cmd
12351	def restore ( self , image , wait = True ) : return self . _action ( 'restore' , image = image , wait = wait )
5519	def append ( self , name , data , start ) : for throttle in self . throttles . values ( ) : getattr ( throttle , name ) . append ( data , start )
4904	def populate_data_sharing_consent ( apps , schema_editor ) : DataSharingConsent = apps . get_model ( 'consent' , 'DataSharingConsent' ) EnterpriseCourseEnrollment = apps . get_model ( 'enterprise' , 'EnterpriseCourseEnrollment' ) User = apps . get_model ( 'auth' , 'User' ) for enrollment in EnterpriseCourseEnrollment . objects . all ( ) : user = User . objects . get ( pk = enrollment . enterprise_customer_user . user_id ) data_sharing_consent , __ = DataSharingConsent . objects . get_or_create ( username = user . username , enterprise_customer = enrollment . enterprise_customer_user . enterprise_customer , course_id = enrollment . course_id , ) if enrollment . consent_granted is not None : data_sharing_consent . granted = enrollment . consent_granted else : # Check UDSCA instead. consent_state = enrollment . enterprise_customer_user . data_sharing_consent . first ( ) if consent_state is not None : data_sharing_consent . granted = consent_state . state in [ 'enabled' , 'external' ] else : data_sharing_consent . granted = False data_sharing_consent . save ( )
81	def ImpulseNoise ( p = 0 , name = None , deterministic = False , random_state = None ) : return SaltAndPepper ( p = p , per_channel = True , name = name , deterministic = deterministic , random_state = random_state )
4204	def rlevinson ( a , efinal ) : a = numpy . array ( a ) realdata = numpy . isrealobj ( a ) assert a [ 0 ] == 1 , 'First coefficient of the prediction polynomial must be unity' p = len ( a ) if p < 2 : raise ValueError ( 'Polynomial should have at least two coefficients' ) if realdata == True : U = numpy . zeros ( ( p , p ) ) # This matrix will have the prediction # polynomials of orders 1:p else : U = numpy . zeros ( ( p , p ) , dtype = complex ) U [ : , p - 1 ] = numpy . conj ( a [ - 1 : : - 1 ] ) # Prediction coefficients of order p p = p - 1 e = numpy . zeros ( p ) # First we find the prediction coefficients of smaller orders and form the # Matrix U # Initialize the step down e [ - 1 ] = efinal # Prediction error of order p # Step down for k in range ( p - 1 , 0 , - 1 ) : [ a , e [ k - 1 ] ] = levdown ( a , e [ k ] ) U [ : , k ] = numpy . concatenate ( ( numpy . conj ( a [ - 1 : : - 1 ] . transpose ( ) ) , [ 0 ] * ( p - k ) ) ) e0 = e [ 0 ] / ( 1. - abs ( a [ 1 ] ** 2 ) ) #% Because a[1]=1 (true polynomial) U [ 0 , 0 ] = 1 #% Prediction coefficient of zeroth order kr = numpy . conj ( U [ 0 , 1 : ] ) #% The reflection coefficients kr = kr . transpose ( ) #% To make it into a column vector # % Once we have the matrix U and the prediction error at various orders, we can # % use this information to find the autocorrelation coefficients. R = numpy . zeros ( 1 , dtype = complex ) #% Initialize recursion k = 1 R0 = e0 # To take care of the zero indexing problem R [ 0 ] = - numpy . conj ( U [ 0 , 1 ] ) * R0 # R[1]=-a1[1]*R[0] # Actual recursion for k in range ( 1 , p ) : r = - sum ( numpy . conj ( U [ k - 1 : : - 1 , k ] ) * R [ - 1 : : - 1 ] ) - kr [ k ] * e [ k - 1 ] R = numpy . insert ( R , len ( R ) , r ) # Include R(0) and make it a column vector. Note the dot transpose #R = [R0 R].'; R = numpy . insert ( R , 0 , e0 ) return R , U , kr , e
1730	def match ( self , string , pos ) : return self . pat . match ( string , int ( pos ) )
11177	def parsestr ( self , argstr ) : argv = shlex . split ( argstr , comments = True ) if len ( argv ) != 1 : raise BadNumberOfArguments ( 1 , len ( argv ) ) arg = argv [ 0 ] lower = arg . lower ( ) if lower in self . true : return True if lower in self . false : return False raise BadArgument ( arg , "Allowed values are " + self . allowed + '.' )
9630	def render_to_message ( self , extra_context = None , * * kwargs ) : if extra_context is None : extra_context = { } # Ensure our custom headers are added to the underlying message class. kwargs . setdefault ( 'headers' , { } ) . update ( self . headers ) context = self . get_context_data ( * * extra_context ) return self . message_class ( subject = self . render_subject ( context ) , body = self . render_body ( context ) , * * kwargs )
1604	def run_metrics ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) spouts = result [ 'physical_plan' ] [ 'spouts' ] . keys ( ) bolts = result [ 'physical_plan' ] [ 'bolts' ] . keys ( ) components = spouts + bolts cname = cl_args [ 'component' ] if cname : if cname in components : components = [ cname ] else : Log . error ( 'Unknown component: \'%s\'' % cname ) raise except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False cresult = [ ] for comp in components : try : metrics = tracker_access . get_component_metrics ( comp , cluster , env , topology , role ) except : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False stat , header = to_table ( metrics ) cresult . append ( ( comp , stat , header ) ) for i , ( comp , stat , header ) in enumerate ( cresult ) : if i != 0 : print ( '' ) print ( '\'%s\' metrics:' % comp ) print ( tabulate ( stat , headers = header ) ) return True
5963	def plot ( self , * * kwargs ) : columns = kwargs . pop ( 'columns' , Ellipsis ) # slice for everything maxpoints = kwargs . pop ( 'maxpoints' , self . maxpoints_default ) transform = kwargs . pop ( 'transform' , lambda x : x ) # default is identity transformation method = kwargs . pop ( 'method' , "mean" ) ax = kwargs . pop ( 'ax' , None ) if columns is Ellipsis or columns is None : columns = numpy . arange ( self . array . shape [ 0 ] ) if len ( columns ) == 0 : raise MissingDataError ( "plot() needs at least one column of data" ) if len ( self . array . shape ) == 1 or self . array . shape [ 0 ] == 1 : # special case: plot against index; plot would do this automatically but # we'll just produce our own xdata and pretend that this was X all along a = numpy . ravel ( self . array ) X = numpy . arange ( len ( a ) ) a = numpy . vstack ( ( X , a ) ) columns = [ 0 ] + [ c + 1 for c in columns ] else : a = self . array color = kwargs . pop ( 'color' , self . default_color_cycle ) try : cmap = matplotlib . cm . get_cmap ( color ) colors = cmap ( matplotlib . colors . Normalize ( ) ( numpy . arange ( len ( columns [ 1 : ] ) , dtype = float ) ) ) except TypeError : colors = cycle ( utilities . asiterable ( color ) ) if ax is None : ax = plt . gca ( ) # (decimate/smooth o slice o transform)(array) a = self . decimate ( method , numpy . asarray ( transform ( a ) ) [ columns ] , maxpoints = maxpoints ) # now deal with infs, nans etc AFTER all transformations (needed for plotting across inf/nan) ma = numpy . ma . MaskedArray ( a , mask = numpy . logical_not ( numpy . isfinite ( a ) ) ) # finally plot (each column separately to catch empty sets) for column , color in zip ( range ( 1 , len ( columns ) ) , colors ) : if len ( ma [ column ] ) == 0 : warnings . warn ( "No data to plot for column {column:d}" . format ( * * vars ( ) ) , category = MissingDataWarning ) kwargs [ 'color' ] = color ax . plot ( ma [ 0 ] , ma [ column ] , * * kwargs ) # plot all other columns in parallel return ax
6944	def jhk_to_vmag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , VJHK , VJH , VJK , VHK , VJ , VH , VK )
12138	def _expand_pattern ( self , pattern ) : ( globpattern , regexp , fields , types ) = self . _decompose_pattern ( pattern ) filelist = glob . glob ( globpattern ) expansion = [ ] for fname in filelist : if fields == [ ] : expansion . append ( ( fname , { } ) ) continue match = re . match ( regexp , fname ) if match is None : continue match_items = match . groupdict ( ) . items ( ) tags = dict ( ( k , types . get ( k , str ) ( v ) ) for ( k , v ) in match_items ) expansion . append ( ( fname , tags ) ) return expansion
13247	def get_lsst_bibtex ( bibtex_filenames = None ) : logger = logging . getLogger ( __name__ ) if bibtex_filenames is None : # Default lsst-texmf bibliography files bibtex_names = KNOWN_LSSTTEXMF_BIB_NAMES else : # Sanitize filenames (remove extensions, path) bibtex_names = [ ] for filename in bibtex_filenames : name = os . path . basename ( os . path . splitext ( filename ) [ 0 ] ) if name not in KNOWN_LSSTTEXMF_BIB_NAMES : logger . warning ( '%r is not a known lsst-texmf bib file' , name ) continue bibtex_names . append ( name ) # names of bibtex files not in cache uncached_names = [ name for name in bibtex_names if name not in _LSSTTEXMF_BIB_CACHE ] if len ( uncached_names ) > 0 : # Download bibtex and put into the cache loop = asyncio . get_event_loop ( ) future = asyncio . ensure_future ( _download_lsst_bibtex ( uncached_names ) ) loop . run_until_complete ( future ) for name , text in zip ( bibtex_names , future . result ( ) ) : _LSSTTEXMF_BIB_CACHE [ name ] = text return { name : _LSSTTEXMF_BIB_CACHE [ name ] for name in bibtex_names }
7499	def resolve_ambigs ( tmpseq ) : ## iterate over the bases 'RSKWYM': [82, 83, 75, 87, 89, 77] for ambig in np . uint8 ( [ 82 , 83 , 75 , 87 , 89 , 77 ] ) : ## get all site in this ambig idx , idy = np . where ( tmpseq == ambig ) ## get the two resolutions of the ambig res1 , res2 = AMBIGS [ ambig . view ( "S1" ) ] ## randomly sample half those sites halfmask = np . random . choice ( [ True , False ] , idx . shape [ 0 ] ) ## replace ambig bases with their resolutions for i in xrange ( halfmask . shape [ 0 ] ) : if halfmask [ i ] : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res1 ) . view ( np . uint8 ) else : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res2 ) . view ( np . uint8 ) return tmpseq
6389	def _sb_short_word ( self , term , r1_prefixes = None ) : if self . _sb_r1 ( term , r1_prefixes ) == len ( term ) and self . _sb_ends_in_short_syllable ( term ) : return True return False
10016	def swap_environment_cnames ( self , from_env_name , to_env_name ) : self . ebs . swap_environment_cnames ( source_environment_name = from_env_name , destination_environment_name = to_env_name )
1259	def save_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) return component . save ( sess = self . session , save_path = save_path )
11138	def __clean_before_after ( self , stateBefore , stateAfter , keepNoneEmptyDirectory = True ) : # prepare after for faster search errors = [ ] afterDict = { } [ afterDict . setdefault ( list ( aitem ) [ 0 ] , [ ] ) . append ( aitem ) for aitem in stateAfter ] # loop before for bitem in reversed ( stateBefore ) : relaPath = list ( bitem ) [ 0 ] basename = os . path . basename ( relaPath ) btype = bitem [ relaPath ] [ 'type' ] alist = afterDict . get ( relaPath , [ ] ) aitem = [ a for a in alist if a [ relaPath ] [ 'type' ] == btype ] if len ( aitem ) > 1 : errors . append ( "Multiple '%s' of type '%s' where found in '%s', this should never had happened. Please report issue" % ( basename , btype , relaPath ) ) continue if not len ( aitem ) : removeDirs = [ ] removeFiles = [ ] if btype == 'dir' : if not len ( relaPath ) : errors . append ( "Removing main repository directory is not allowed" ) continue removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirLock ) ) elif btype == 'file' : removeFiles . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileLock % basename ) ) else : ### MUST VERIFY THAT ONCE pyrepobjectdir IS IMPLEMENTED removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) # remove files for fpath in removeFiles : if os . path . isfile ( fpath ) : try : os . remove ( fpath ) except Exception as err : errors . append ( "Unable to clean file '%s' (%s)" % ( fpath , str ( err ) ) ) # remove directories for dpath in removeDirs : if os . path . isdir ( dpath ) : if keepNoneEmptyDirectory or not len ( os . listdir ( dpath ) ) : try : shutil . rmtree ( dpath ) except Exception as err : errors . append ( "Unable to clean directory '%s' (%s)" % ( fpath , str ( err ) ) ) # return result and errors list return len ( errors ) == 0 , errors
3210	def get ( self , key , delete_if_expired = True ) : self . _update_cache_stats ( key , None ) if key in self . _CACHE : ( expiration , obj ) = self . _CACHE [ key ] if expiration > self . _now ( ) : self . _update_cache_stats ( key , 'hit' ) return obj else : if delete_if_expired : self . delete ( key ) self . _update_cache_stats ( key , 'expired' ) return None self . _update_cache_stats ( key , 'miss' ) return None
10160	def ci ( ctx ) : opts = [ '' ] # 'tox' makes no sense in Travis if os . environ . get ( 'TRAVIS' , '' ) . lower ( ) == 'true' : opts += [ 'test.pytest' ] else : opts += [ 'test.tox' ] ctx . run ( "invoke --echo --pty clean --all build --docs check --reports{}" . format ( ' ' . join ( opts ) ) )
10183	def _aggregations_delete ( aggregation_types = None , start_date = None , end_date = None ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) for a in aggregation_types : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , * * aggr_cfg . aggregator_config ) aggregator . delete ( start_date , end_date )
8781	def create_locks ( context , network_ids , addresses ) : for address in addresses : address_model = None try : address_model = _find_or_create_address ( context , network_ids , address ) lock_holder = None if address_model . lock_id : lock_holder = db_api . lock_holder_find ( context , lock_id = address_model . lock_id , name = LOCK_NAME , scope = db_api . ONE ) if not lock_holder : LOG . info ( "Creating lock holder on IPAddress %s with id %s" , address_model . address_readable , address_model . id ) db_api . lock_holder_create ( context , address_model , name = LOCK_NAME , type = "ip_address" ) except Exception : LOG . exception ( "Failed to create lock holder on IPAddress %s" , address_model ) continue context . session . flush ( )
318	def calc_bootstrap ( func , returns , * args , * * kwargs ) : n_samples = kwargs . pop ( 'n_samples' , 1000 ) out = np . empty ( n_samples ) factor_returns = kwargs . pop ( 'factor_returns' , None ) for i in range ( n_samples ) : idx = np . random . randint ( len ( returns ) , size = len ( returns ) ) returns_i = returns . iloc [ idx ] . reset_index ( drop = True ) if factor_returns is not None : factor_returns_i = factor_returns . iloc [ idx ] . reset_index ( drop = True ) out [ i ] = func ( returns_i , factor_returns_i , * args , * * kwargs ) else : out [ i ] = func ( returns_i , * args , * * kwargs ) return out
2692	def split_grafs ( lines ) : graf = [ ] for line in lines : line = line . strip ( ) if len ( line ) < 1 : if len ( graf ) > 0 : yield "\n" . join ( graf ) graf = [ ] else : graf . append ( line ) if len ( graf ) > 0 : yield "\n" . join ( graf )
7452	def writetofastq ( data , dsort , read ) : if read == 1 : rrr = "R1" else : rrr = "R2" for sname in dsort : ## skip writing if empty. Write to tmpname handle = os . path . join ( data . dirs . fastqs , "{}_{}_.fastq" . format ( sname , rrr ) ) with open ( handle , 'a' ) as out : out . write ( "" . join ( dsort [ sname ] ) )
13873	def CopyFile ( source_filename , target_filename , override = True , md5_check = False , copy_symlink = True ) : from . _exceptions import FileNotFoundError # Check override if not override and Exists ( target_filename ) : from . _exceptions import FileAlreadyExistsError raise FileAlreadyExistsError ( target_filename ) # Don't do md5 check for md5 files themselves. md5_check = md5_check and not target_filename . endswith ( '.md5' ) # If we enabled md5 checks, ignore copy of files that haven't changed their md5 contents. if md5_check : source_md5_filename = source_filename + '.md5' target_md5_filename = target_filename + '.md5' try : source_md5_contents = GetFileContents ( source_md5_filename ) except FileNotFoundError : source_md5_contents = None try : target_md5_contents = GetFileContents ( target_md5_filename ) except FileNotFoundError : target_md5_contents = None if source_md5_contents is not None and source_md5_contents == target_md5_contents and Exists ( target_filename ) : return MD5_SKIP # Copy source file _DoCopyFile ( source_filename , target_filename , copy_symlink = copy_symlink ) # If we have a source_md5, but no target_md5, create the target_md5 file if md5_check and source_md5_contents is not None and source_md5_contents != target_md5_contents : CreateFile ( target_md5_filename , source_md5_contents )
7894	def send_message ( self , body ) : m = Message ( to_jid = self . room_jid . bare ( ) , stanza_type = "groupchat" , body = body ) self . manager . stream . send ( m )
10088	def patch ( self , * args , * * kwargs ) : return super ( Deposit , self ) . patch ( * args , * * kwargs )
6458	def _m_degree ( self , term ) : mdeg = 0 last_was_vowel = False for letter in term : if letter in self . _vowels : last_was_vowel = True else : if last_was_vowel : mdeg += 1 last_was_vowel = False return mdeg
10127	def draw ( self ) : if self . enabled : self . _vertex_list . colors = self . _gl_colors self . _vertex_list . vertices = self . _gl_vertices self . _vertex_list . draw ( pyglet . gl . GL_TRIANGLES )
4319	def _parse_stat ( stat_output ) : lines = stat_output . split ( '\n' ) stat_dict = { } for line in lines : split_line = line . split ( ':' ) if len ( split_line ) == 2 : key = split_line [ 0 ] val = split_line [ 1 ] . strip ( ' ' ) try : val = float ( val ) except ValueError : val = None stat_dict [ key ] = val return stat_dict
1328	def channel_axis ( self , batch ) : axis = self . __model . channel_axis ( ) if not batch : axis = axis - 1 return axis
5932	def scale_impropers ( mol , impropers , scale , banned_lines = None ) : if banned_lines is None : banned_lines = [ ] new_impropers = [ ] for im in mol . impropers : atypes = ( im . atom1 . get_atomtype ( ) , im . atom2 . get_atomtype ( ) , im . atom3 . get_atomtype ( ) , im . atom4 . get_atomtype ( ) ) atypes = [ a . replace ( "_" , "" ) . replace ( "=" , "" ) for a in atypes ] # special-case: this is a [ dihedral ] override in molecule block, continue and don't match if im . gromacs [ 'param' ] != [ ] : for p in im . gromacs [ 'param' ] : p [ 'kpsi' ] *= scale new_impropers . append ( im ) continue for iswitch in range ( 32 ) : if ( iswitch % 2 == 0 ) : a1 = atypes [ 0 ] a2 = atypes [ 1 ] a3 = atypes [ 2 ] a4 = atypes [ 3 ] else : a1 = atypes [ 3 ] a2 = atypes [ 2 ] a3 = atypes [ 1 ] a4 = atypes [ 0 ] if ( ( iswitch // 2 ) % 2 == 1 ) : a1 = "X" if ( ( iswitch // 4 ) % 2 == 1 ) : a2 = "X" if ( ( iswitch // 8 ) % 2 == 1 ) : a3 = "X" if ( ( iswitch // 16 ) % 2 == 1 ) : a4 = "X" key = "{0}-{1}-{2}-{3}-{4}" . format ( a1 , a2 , a3 , a4 , im . gromacs [ 'func' ] ) if ( key in impropers ) : for i , imt in enumerate ( impropers [ key ] ) : imA = copy . deepcopy ( im ) param = copy . deepcopy ( imt . gromacs [ 'param' ] ) # Only check the first dihedral in a list if not impropers [ key ] [ 0 ] . line in banned_lines : for p in param : p [ 'kpsi' ] *= scale imA . gromacs [ 'param' ] = param if i == 0 : imA . comment = "; banned lines {0} found={1}\n ; parameters for types {2}-{3}-{4}-{5}-9 at LINE({6})\n" . format ( " " . join ( map ( str , banned_lines ) ) , 1 if imt . line in banned_lines else 0 , imt . atype1 , imt . atype2 , imt . atype3 , imt . atype4 , imt . line ) new_impropers . append ( imA ) break #assert(len(mol.impropers) == new_impropers) mol . impropers = new_impropers return mol
3916	def _get_date_str ( timestamp , datetimefmt , show_date = False ) : fmt = '' if show_date : fmt += '\n' + datetimefmt . get ( 'date' , '' ) + '\n' fmt += datetimefmt . get ( 'time' , '' ) return timestamp . astimezone ( tz = None ) . strftime ( fmt )
4539	def multi ( method ) : @ functools . wraps ( method ) def multi ( self , address = '' ) : values = flask . request . values address = urllib . parse . unquote_plus ( address ) if address and values and not address . endswith ( '.' ) : address += '.' result = { } for a in values or '' : try : if not self . project : raise ValueError ( 'No Project is currently loaded' ) ed = editor . Editor ( address + a , self . project ) result [ address + a ] = { 'value' : method ( self , ed , a ) } except : if self . project : traceback . print_exc ( ) result [ address + a ] = { 'error' : 'Could not multi addr %s' % a } return flask . jsonify ( result ) return multi
5420	def _get_job_resources ( args ) : logging = param_util . build_logging_param ( args . logging ) if args . logging else None timeout = param_util . timeout_in_seconds ( args . timeout ) log_interval = param_util . log_interval_in_seconds ( args . log_interval ) return job_model . Resources ( min_cores = args . min_cores , min_ram = args . min_ram , machine_type = args . machine_type , disk_size = args . disk_size , disk_type = args . disk_type , boot_disk_size = args . boot_disk_size , preemptible = args . preemptible , image = args . image , regions = args . regions , zones = args . zones , logging = logging , logging_path = None , service_account = args . service_account , scopes = args . scopes , keep_alive = args . keep_alive , cpu_platform = args . cpu_platform , network = args . network , subnetwork = args . subnetwork , use_private_address = args . use_private_address , accelerator_type = args . accelerator_type , accelerator_count = args . accelerator_count , nvidia_driver_version = args . nvidia_driver_version , timeout = timeout , log_interval = log_interval , ssh = args . ssh )
4595	def make_object ( * args , typename = None , python_path = None , datatype = None , * * kwds ) : datatype = datatype or import_symbol ( typename , python_path ) field_types = getattr ( datatype , 'FIELD_TYPES' , fields . FIELD_TYPES ) return datatype ( * args , * * fields . component ( kwds , field_types ) )
9651	def check_shastore_version ( from_store , settings ) : sprint = settings [ "sprint" ] error = settings [ "error" ] sprint ( "checking .shastore version for potential incompatibilities" , level = "verbose" ) if not from_store or 'sake version' not in from_store : errmes = [ "Since you've used this project last, a new version of " , "sake was installed that introduced backwards incompatible" , " changes. Run 'sake clean', and rebuild before continuing\n" ] errmes = " " . join ( errmes ) error ( errmes ) sys . exit ( 1 )
7479	def build_clustbits ( data , ipyclient , force ) : ## If you run this step then we clear all tmp .fa and .indel.h5 files if os . path . exists ( data . tmpdir ) : shutil . rmtree ( data . tmpdir ) os . mkdir ( data . tmpdir ) ## parallel client lbview = ipyclient . load_balanced_view ( ) start = time . time ( ) printstr = " building clusters | {} | s6 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) uhandle = os . path . join ( data . dirs . across , data . name + ".utemp" ) usort = os . path . join ( data . dirs . across , data . name + ".utemp.sort" ) async1 = "" ## skip usorting if not force and already exists if not os . path . exists ( usort ) or force : ## send sort job to engines. Sorted seeds allows us to work through ## the utemp file one locus at a time instead of reading all into mem. LOGGER . info ( "building reads file -- loading utemp file into mem" ) async1 = lbview . apply ( sort_seeds , * ( uhandle , usort ) ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async1 . ready ( ) : break else : time . sleep ( 0.1 ) ## send count seeds job to engines. async2 = lbview . apply ( count_seeds , usort ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 1 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async2 . ready ( ) : break else : time . sleep ( 0.1 ) ## wait for both to finish while printing progress timer nseeds = async2 . result ( ) ## send the clust bit building job to work and track progress async3 = lbview . apply ( sub_build_clustbits , * ( data , usort , nseeds ) ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 2 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async3 . ready ( ) : break else : time . sleep ( 0.1 ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 3 , printstr . format ( elapsed ) , spacer = data . _spacer ) print ( "" ) ## check for errors for job in [ async1 , async2 , async3 ] : try : if not job . successful ( ) : raise IPyradWarningExit ( job . result ( ) ) except AttributeError : ## If we skip usorting then async1 == "" so the call to ## successful() raises, but we can ignore it. pass
11064	def _ignore_event ( self , message ) : if hasattr ( message , 'subtype' ) and message . subtype in self . ignored_events : return True return False
3813	async def upload_image ( self , image_file , filename = None , * , return_uploaded_image = False ) : image_filename = filename or os . path . basename ( image_file . name ) image_data = image_file . read ( ) # request an upload URL res = await self . _base_request ( IMAGE_UPLOAD_URL , 'application/x-www-form-urlencoded;charset=UTF-8' , 'json' , json . dumps ( { "protocolVersion" : "0.8" , "createSessionRequest" : { "fields" : [ { "external" : { "name" : "file" , "filename" : image_filename , "put" : { } , "size" : len ( image_data ) } } ] } } ) ) try : upload_url = self . _get_upload_session_status ( res ) [ 'externalFieldTransfers' ] [ 0 ] [ 'putInfo' ] [ 'url' ] except KeyError : raise exceptions . NetworkError ( 'image upload failed: can not acquire an upload url' ) # upload the image data using the upload_url to get the upload info res = await self . _base_request ( upload_url , 'application/octet-stream' , 'json' , image_data ) try : raw_info = ( self . _get_upload_session_status ( res ) [ 'additionalInfo' ] [ 'uploader_service.GoogleRupioAdditionalInfo' ] [ 'completionInfo' ] [ 'customerSpecificInfo' ] ) image_id = raw_info [ 'photoid' ] url = raw_info [ 'url' ] except KeyError : raise exceptions . NetworkError ( 'image upload failed: can not fetch upload info' ) result = UploadedImage ( image_id = image_id , url = url ) return result if return_uploaded_image else result . image_id
3350	def _generate_index ( self ) : self . _dict = { v . id : k for k , v in enumerate ( self ) }
6475	def line ( self , p1 , p2 , resolution = 1 ) : xdiff = max ( p1 . x , p2 . x ) - min ( p1 . x , p2 . x ) ydiff = max ( p1 . y , p2 . y ) - min ( p1 . y , p2 . y ) xdir = [ - 1 , 1 ] [ int ( p1 . x <= p2 . x ) ] ydir = [ - 1 , 1 ] [ int ( p1 . y <= p2 . y ) ] r = int ( round ( max ( xdiff , ydiff ) ) ) if r == 0 : return for i in range ( ( r + 1 ) * resolution ) : x = p1 . x y = p1 . y if xdiff : x += ( float ( i ) * xdiff ) / r * xdir / resolution if ydiff : y += ( float ( i ) * ydiff ) / r * ydir / resolution yield Point ( ( x , y ) )
12530	def open_volume_file ( filepath ) : # check if the file exists if not op . exists ( filepath ) : raise IOError ( 'Could not find file {}.' . format ( filepath ) ) # define helper functions def open_nifti_file ( filepath ) : return NiftiImage ( filepath ) def open_mhd_file ( filepath ) : return MedicalImage ( filepath ) vol_data , hdr_data = load_raw_data_with_mhd ( filepath ) # TODO: convert vol_data and hdr_data into MedicalImage return vol_data , hdr_data def open_mha_file ( filepath ) : raise NotImplementedError ( 'This function has not been implemented yet.' ) # generic loader function def _load_file ( filepath , loader ) : return loader ( filepath ) # file_extension -> file loader function filext_loader = { 'nii' : open_nifti_file , 'mhd' : open_mhd_file , 'mha' : open_mha_file , } # get extension of the `filepath` ext = get_extension ( filepath ) # find the loader from `ext` loader = None for e in filext_loader : if ext in e : loader = filext_loader [ e ] if loader is None : raise ValueError ( 'Could not find a loader for file {}.' . format ( filepath ) ) return _load_file ( filepath , loader )
4754	def runlogs_to_html ( run_root ) : if not os . path . isdir ( run_root ) : return "CANNOT_LOCATE_LOGFILES" hook_enter = [ ] hook_exit = [ ] tcase = [ ] for fpath in glob . glob ( os . sep . join ( [ run_root , "*.log" ] ) ) : if "exit" in fpath : hook_exit . append ( fpath ) continue if "hook" in fpath : hook_enter . append ( fpath ) continue tcase . append ( fpath ) content = "" for fpath in hook_enter + tcase + hook_exit : content += "# BEGIN: run-log from log_fpath: %s\n" % fpath content += open ( fpath , "r" ) . read ( ) content += "# END: run-log from log_fpath: %s\n\n" % fpath return content
13261	def get_task_tree ( white_list = None ) : assert white_list is None or isinstance ( white_list , list ) , type ( white_list ) if white_list is not None : white_list = set ( item if isinstance ( item , str ) else item . __qualname__ for item in white_list ) tree = dict ( ( task . qualified_name , task ) for task in _task_list . values ( ) if white_list is None or task . qualified_name in white_list ) plugins = get_plugin_list ( ) for plugin in [ plugin for plugin in plugins . values ( ) if white_list is None or plugin . __qualname__ in white_list ] : tasks = [ func for _ , func in inspect . getmembers ( plugin ) if inspect . isfunction ( func ) and hasattr ( func , "yaz_task_config" ) ] if len ( tasks ) == 0 : continue node = tree for name in plugin . __qualname__ . split ( "." ) : if not name in node : node [ name ] = { } node = node [ name ] for func in tasks : logger . debug ( "Found task %s" , func ) node [ func . __name__ ] = Task ( plugin_class = plugin , func = func , config = func . yaz_task_config ) return tree
7092	def handle_change ( self , change ) : op = change [ 'operation' ] if op in 'append' : self . add ( len ( change [ 'value' ] ) , LatLng ( * change [ 'item' ] ) ) elif op == 'insert' : self . add ( change [ 'index' ] , LatLng ( * change [ 'item' ] ) ) elif op == 'extend' : points = [ LatLng ( * p ) for p in change [ 'items' ] ] self . addAll ( [ bridge . encode ( c ) for c in points ] ) elif op == '__setitem__' : self . set ( change [ 'index' ] , LatLng ( * change [ 'newitem' ] ) ) elif op == 'pop' : self . remove ( change [ 'index' ] ) else : raise NotImplementedError ( "Unsupported change operation {}" . format ( op ) )
3616	def _should_really_index ( self , instance ) : if self . _should_index_is_method : is_method = inspect . ismethod ( self . should_index ) try : count_args = len ( inspect . signature ( self . should_index ) . parameters ) except AttributeError : # noinspection PyDeprecation count_args = len ( inspect . getargspec ( self . should_index ) . args ) if is_method or count_args is 1 : # bound method, call with instance return self . should_index ( instance ) else : # unbound method, simply call without arguments return self . should_index ( ) else : # property/attribute/Field, evaluate as bool attr_type = type ( self . should_index ) if attr_type is DeferredAttribute : attr_value = self . should_index . __get__ ( instance , None ) elif attr_type is str : attr_value = getattr ( instance , self . should_index ) elif attr_type is property : attr_value = self . should_index . __get__ ( instance ) else : raise AlgoliaIndexError ( '{} should be a boolean attribute or a method that returns a boolean.' . format ( self . should_index ) ) if type ( attr_value ) is not bool : raise AlgoliaIndexError ( "%s's should_index (%s) should be a boolean" % ( instance . __class__ . __name__ , self . should_index ) ) return attr_value
5623	def path_exists ( path ) : if path . startswith ( ( "http://" , "https://" ) ) : try : urlopen ( path ) . info ( ) return True except HTTPError as e : if e . code == 404 : return False else : raise elif path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) for obj in bucket . objects . filter ( Prefix = key ) : if obj . key == key : return True else : return False else : logger . debug ( "%s exists: %s" , path , os . path . exists ( path ) ) return os . path . exists ( path )
925	def _aggr_mode ( inList ) : valueCounts = dict ( ) nonNone = 0 for elem in inList : if elem == SENTINEL_VALUE_FOR_MISSING_DATA : continue nonNone += 1 if elem in valueCounts : valueCounts [ elem ] += 1 else : valueCounts [ elem ] = 1 # Get the most common one if nonNone == 0 : return None # Sort by counts sortedCounts = valueCounts . items ( ) sortedCounts . sort ( cmp = lambda x , y : x [ 1 ] - y [ 1 ] , reverse = True ) return sortedCounts [ 0 ] [ 0 ]
54	def shift ( self , x = 0 , y = 0 ) : keypoints = [ keypoint . shift ( x = x , y = y ) for keypoint in self . keypoints ] return self . deepcopy ( keypoints )
8428	def gradient_n_pal ( colors , values = None , name = 'gradientn' ) : # Note: For better results across devices and media types, # it would be better to do the interpolation in # Lab color space. if values is None : colormap = mcolors . LinearSegmentedColormap . from_list ( name , colors ) else : colormap = mcolors . LinearSegmentedColormap . from_list ( name , list ( zip ( values , colors ) ) ) def _gradient_n_pal ( vals ) : return ratios_to_colors ( vals , colormap ) return _gradient_n_pal
2057	def TBH ( cpu , dest ) : # Capstone merges the two registers values into one operand, so we need to extract them back # Specifies the base register. This contains the address of the table of branch lengths. This # register is allowed to be the PC. If it is, the table immediately follows this instruction. base_addr = dest . get_mem_base_addr ( ) if dest . mem . base in ( 'PC' , 'R15' ) : base_addr = cpu . PC # Specifies the index register. This contains an integer pointing to a halfword within the table. # The offset within the table is twice the value of the index. offset = cpu . read_int ( base_addr + dest . get_mem_offset ( ) , 16 ) offset = Operators . ZEXTEND ( offset , cpu . address_bit_size ) cpu . PC += ( offset << 1 )
9246	def compound_changelog ( self ) : self . fetch_and_filter_tags ( ) tags_sorted = self . sort_tags_by_date ( self . filtered_tags ) self . filtered_tags = tags_sorted self . fetch_and_filter_issues_and_pr ( ) log = str ( self . options . frontmatter ) if self . options . frontmatter else u"" log += u"{0}\n\n" . format ( self . options . header ) if self . options . unreleased_only : log += self . generate_unreleased_section ( ) else : log += self . generate_log_for_all_tags ( ) try : with open ( self . options . base ) as fh : log += fh . read ( ) except ( TypeError , IOError ) : pass return log
10469	def launchAppByBundlePath ( bundlePath , arguments = None ) : if arguments is None : arguments = [ ] bundleUrl = NSURL . fileURLWithPath_ ( bundlePath ) workspace = AppKit . NSWorkspace . sharedWorkspace ( ) arguments_strings = list ( map ( lambda a : NSString . stringWithString_ ( str ( a ) ) , arguments ) ) arguments = NSDictionary . dictionaryWithDictionary_ ( { AppKit . NSWorkspaceLaunchConfigurationArguments : NSArray . arrayWithArray_ ( arguments_strings ) } ) return workspace . launchApplicationAtURL_options_configuration_error_ ( bundleUrl , AppKit . NSWorkspaceLaunchAllowingClassicStartup , arguments , None )
10631	def clear ( self ) : self . _compound_mfrs = self . _compound_mfrs * 0.0 self . _P = 1.0 self . _T = 25.0 self . _H = 0.0
5292	def post ( self , request , * args , * * kwargs ) : form_class = self . get_form_class ( ) form = self . get_form ( form_class ) if form . is_valid ( ) : self . object = form . save ( commit = False ) form_validated = True else : form_validated = False inlines = self . construct_inlines ( ) if all_valid ( inlines ) and form_validated : return self . forms_valid ( form , inlines ) return self . forms_invalid ( form , inlines )
2957	def _get_blockade_id_from_cwd ( self , cwd = None ) : if not cwd : cwd = os . getcwd ( ) # this follows a similar pattern as docker-compose uses parent_dir = os . path . abspath ( cwd ) basename = os . path . basename ( parent_dir ) . lower ( ) blockade_id = re . sub ( r"[^a-z0-9]" , "" , basename ) if not blockade_id : # if we can't get a valid name from CWD, use "default" blockade_id = "default" return blockade_id
6644	def availableVersions ( self ) : r = [ ] for t in self . vcs . tags ( ) : logger . debug ( "available version tag: %s" , t ) # ignore empty tags: if not len ( t . strip ( ) ) : continue try : r . append ( GitCloneVersion ( t , t , self ) ) except ValueError : logger . debug ( 'invalid version tag: %s' , t ) return r
2606	def check_memo ( self , task_id , task ) : if not self . memoize or not task [ 'memoize' ] : task [ 'hashsum' ] = None return None , None hashsum = self . make_hash ( task ) present = False result = None if hashsum in self . memo_lookup_table : present = True result = self . memo_lookup_table [ hashsum ] logger . info ( "Task %s using result from cache" , task_id ) task [ 'hashsum' ] = hashsum return present , result
12134	def write_log ( log_path , data , allow_append = True ) : append = os . path . isfile ( log_path ) islist = isinstance ( data , list ) if append and not allow_append : raise Exception ( 'Appending has been disabled' ' and file %s exists' % log_path ) if not ( islist or isinstance ( data , Args ) ) : raise Exception ( 'Can only write Args objects or dictionary' ' lists to log file.' ) specs = data if islist else data . specs if not all ( isinstance ( el , dict ) for el in specs ) : raise Exception ( 'List elements must be dictionaries.' ) log_file = open ( log_path , 'r+' ) if append else open ( log_path , 'w' ) start = int ( log_file . readlines ( ) [ - 1 ] . split ( ) [ 0 ] ) + 1 if append else 0 ascending_indices = range ( start , start + len ( data ) ) log_str = '\n' . join ( [ '%d %s' % ( tid , json . dumps ( el ) ) for ( tid , el ) in zip ( ascending_indices , specs ) ] ) log_file . write ( "\n" + log_str if append else log_str ) log_file . close ( )
6746	def topological_sort ( source ) : if isinstance ( source , dict ) : source = source . items ( ) pending = sorted ( [ ( name , set ( deps ) ) for name , deps in source ] ) # copy deps so we can modify set in-place emitted = [ ] while pending : next_pending = [ ] next_emitted = [ ] for entry in pending : name , deps = entry deps . difference_update ( emitted ) # remove deps we emitted last pass if deps : # still has deps? recheck during next pass next_pending . append ( entry ) else : # no more deps? time to emit yield name emitted . append ( name ) # <-- not required, but helps preserve original ordering next_emitted . append ( name ) # remember what we emitted for difference_update() in next pass if not next_emitted : # all entries have unmet deps, one of two things is wrong... raise ValueError ( "cyclic or missing dependancy detected: %r" % ( next_pending , ) ) pending = next_pending emitted = next_emitted
9833	def initialize ( self ) : return self . DXclasses [ self . type ] ( self . id , * * self . args )
13267	def gml_to_geojson ( el ) : if el . get ( 'srsName' ) not in ( 'urn:ogc:def:crs:EPSG::4326' , None ) : if el . get ( 'srsName' ) == 'EPSG:4326' : return _gmlv2_to_geojson ( el ) else : raise NotImplementedError ( "Unrecognized srsName %s" % el . get ( 'srsName' ) ) tag = el . tag . replace ( '{%s}' % NS_GML , '' ) if tag == 'Point' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}pos' % NS_GML ) ) [ 0 ] elif tag == 'LineString' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}posList' % NS_GML ) ) elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:exterior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) + el . xpath ( 'gml:interior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) : coordinates . append ( _reverse_gml_coords ( ring . text ) ) elif tag in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' ) : single_type = tag [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' coordinates = [ gml_to_geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member_tag , single_type ) , namespaces = NSMAP ) ] else : raise NotImplementedError return { 'type' : tag , 'coordinates' : coordinates }
6862	def drop_views ( self , name = None , site = None ) : r = self . database_renderer result = r . sudo ( "mysql --batch -v -h {db_host} " #"-u {db_root_username} -p'{db_root_password}' " "-u {db_user} -p'{db_password}' " "--execute=\"SELECT GROUP_CONCAT(CONCAT(TABLE_SCHEMA,'.',table_name) SEPARATOR ', ') AS views " "FROM INFORMATION_SCHEMA.views WHERE TABLE_SCHEMA = '{db_name}' ORDER BY table_name DESC;\"" ) result = re . findall ( r'^views[\s\t\r\n]+(.*)' , result , flags = re . IGNORECASE | re . DOTALL | re . MULTILINE ) if not result : return r . env . db_view_list = result [ 0 ] #cmd = ("mysql -v -h {db_host} -u {db_root_username} -p'{db_root_password}' " \ r . sudo ( "mysql -v -h {db_host} -u {db_user} -p'{db_password}' " "--execute=\"DROP VIEW {db_view_list} CASCADE;\"" )
4573	def hsv2rgb_spectrum ( hsv ) : h , s , v = hsv return hsv2rgb_raw ( ( ( h * 192 ) >> 8 , s , v ) )
338	def _GetNextLogCountPerToken ( token ) : global _log_counter_per_token # pylint: disable=global-variable-not-assigned _log_counter_per_token [ token ] = 1 + _log_counter_per_token . get ( token , - 1 ) return _log_counter_per_token [ token ]
9270	def get_temp_tag_for_repo_creation ( self ) : tag_date = self . tag_times_dict . get ( REPO_CREATED_TAG_NAME , None ) if not tag_date : tag_name , tag_date = self . fetcher . fetch_repo_creation_date ( ) self . tag_times_dict [ tag_name ] = timestring_to_datetime ( tag_date ) return REPO_CREATED_TAG_NAME
1071	def getaddrspec ( self ) : aslist = [ ] self . gotonext ( ) while self . pos < len ( self . field ) : if self . field [ self . pos ] == '.' : aslist . append ( '.' ) self . pos += 1 elif self . field [ self . pos ] == '"' : aslist . append ( '"%s"' % self . getquote ( ) ) elif self . field [ self . pos ] in self . atomends : break else : aslist . append ( self . getatom ( ) ) self . gotonext ( ) if self . pos >= len ( self . field ) or self . field [ self . pos ] != '@' : return '' . join ( aslist ) aslist . append ( '@' ) self . pos += 1 self . gotonext ( ) return '' . join ( aslist ) + self . getdomain ( )
13482	def sphinx_make ( * targets ) : sh ( 'make %s' % ' ' . join ( targets ) , cwd = options . paved . docs . path )
389	def sequences_get_mask ( sequences , pad_val = 0 ) : mask = np . ones_like ( sequences ) for i , seq in enumerate ( sequences ) : for i_w in reversed ( range ( len ( seq ) ) ) : if seq [ i_w ] == pad_val : mask [ i , i_w ] = 0 else : break # <-- exit the for loop, prepcess next sequence return mask
5983	def output_subplot_array ( output_path , output_filename , output_format ) : if output_format is 'show' : plt . show ( ) elif output_format is 'png' : plt . savefig ( output_path + output_filename + '.png' , bbox_inches = 'tight' ) elif output_format is 'fits' : raise exc . PlottingException ( 'You cannot output a subplots with format .fits' )
10869	def calc_pts_hg ( npts = 20 ) : pts_hg , wts_hg = np . polynomial . hermite . hermgauss ( npts * 2 ) pts_hg = pts_hg [ npts : ] wts_hg = wts_hg [ npts : ] * np . exp ( pts_hg * pts_hg ) return pts_hg , wts_hg
7787	def _try_backup_item ( self ) : if not self . _backup_state : return False item = self . cache . get_item ( self . address , self . _backup_state ) if item : self . _object_handler ( item . address , item . value , item . state ) return True else : False
10233	def _reaction_cartesion_expansion_unqualified_helper ( graph : BELGraph , u : BaseEntity , v : BaseEntity , d : dict , ) -> None : if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( reactant , product , INCREASES ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( product , reactant , d [ RELATION ] , ) elif isinstance ( u , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) for product in u . products : # Skip create increases edges between enzymes if product in enzymes : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips hasReactant, hasProduct edges if v not in u . products and v not in u . reactants : graph . add_unqualified_edge ( product , v , INCREASES ) for reactant in u . reactants : graph . add_unqualified_edge ( reactant , product , INCREASES ) elif isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( v ) for reactant in v . reactants : # Skip create increases edges between enzymes if reactant in enzymes : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips hasReactant, hasProduct edges if u not in v . products and u not in v . reactants : graph . add_unqualified_edge ( u , reactant , INCREASES ) for product in v . products : graph . add_unqualified_edge ( reactant , product , INCREASES )
11632	def codepointsInNamelist ( namFilename , unique_glyphs = False , cache = None ) : key = 'charset' if not unique_glyphs else 'ownCharset' internals_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) target = os . path . join ( internals_dir , namFilename ) result = readNamelist ( target , unique_glyphs , cache ) return result [ key ]
13604	def system ( self , cmd , fake_code = False ) : try : if self . options . dry_run : def fake_system ( cmd ) : self . print_message ( cmd ) return fake_code return fake_system ( cmd ) except AttributeError : self . logger . warnning ( "fake mode enabled," "but you don't set '--dry-run' option " "in your argparser options" ) pass return os . system ( cmd )
2894	def connect_if ( self , condition , task_spec ) : assert task_spec is not None self . outputs . append ( task_spec ) self . cond_task_specs . append ( ( condition , task_spec . name ) ) task_spec . _connect_notify ( self )
11391	def contribute_to_class ( self , cls , name ) : super ( EmbeddedMediaField , self ) . contribute_to_class ( cls , name ) register_field ( cls , self ) # add a virtual field that will create signals on any/all subclasses cls . _meta . add_virtual_field ( EmbeddedSignalCreator ( self ) )
11708	def print_parents ( self ) : if self . gender == female : title = 'Daughter' elif self . gender == male : title = 'Son' else : title = 'Child' p1 = self . parents [ 0 ] p2 = self . parents [ 1 ] template = '%s of %s, the %s, and %s, the %s.' print ( template % ( title , p1 . name , p1 . epithet , p2 . name , p2 . epithet ) )
11268	def join ( prev , sep , * args , * * kw ) : yield sep . join ( prev , * args , * * kw )
2093	def lookup_stdout ( self , pk = None , start_line = None , end_line = None , full = True ) : stdout_url = '%s%s/stdout/' % ( self . unified_job_type , pk ) payload = { 'format' : 'json' , 'content_encoding' : 'base64' , 'content_format' : 'ansi' } if start_line : payload [ 'start_line' ] = start_line if end_line : payload [ 'end_line' ] = end_line debug . log ( 'Requesting a copy of job standard output' , header = 'details' ) resp = client . get ( stdout_url , params = payload ) . json ( ) content = b64decode ( resp [ 'content' ] ) return content . decode ( 'utf-8' , 'replace' )
3982	def get_repo_of_app_or_library ( app_or_library_name ) : specs = get_specs ( ) repo_name = specs . get_app_or_lib ( app_or_library_name ) [ 'repo' ] if not repo_name : return None return Repo ( repo_name )
12484	def get_subdict ( adict , path , sep = os . sep ) : return reduce ( adict . __class__ . get , [ p for p in op . split ( sep ) if p ] , adict )
10003	def rename ( self , name ) : self . _impl . system . rename_model ( new_name = name , old_name = self . name )
11717	def create ( self , config ) : assert config [ "name" ] == self . name , "Given config is not for this template" data = self . _json_encode ( config ) headers = self . _default_headers ( ) return self . _request ( "" , ok_status = None , data = data , headers = headers )
13110	def lookup ( cls , key , get = False ) : if get : item = cls . _item_dict . get ( key ) return item . name if item else key return cls . _item_dict [ key ] . name
12611	def search_by_eid ( self , table_name , eid ) : elem = self . table ( table_name ) . get ( eid = eid ) if elem is None : raise KeyError ( 'Could not find {} with eid {}.' . format ( table_name , eid ) ) return elem
1844	def JO ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . OF , target . read ( ) , cpu . PC )
5788	def _bcrypt_encrypt ( cipher , key , data , iv , padding ) : key_handle = None try : key_handle = _bcrypt_create_key_handle ( cipher , key ) if iv is None : iv_len = 0 else : iv_len = len ( iv ) flags = 0 if padding is True : flags = BcryptConst . BCRYPT_BLOCK_PADDING out_len = new ( bcrypt , 'ULONG *' ) res = bcrypt . BCryptEncrypt ( key_handle , data , len ( data ) , null ( ) , null ( ) , 0 , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) iv_buffer = buffer_from_bytes ( iv ) if iv else null ( ) res = bcrypt . BCryptEncrypt ( key_handle , data , len ( data ) , null ( ) , iv_buffer , iv_len , buffer , buffer_len , out_len , flags ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) ) finally : if key_handle : bcrypt . BCryptDestroyKey ( key_handle )
2778	def remove_forwarding_rules ( self , forwarding_rules ) : rules_dict = [ rule . __dict__ for rule in forwarding_rules ] return self . get_data ( "load_balancers/%s/forwarding_rules/" % self . id , type = DELETE , params = { "forwarding_rules" : rules_dict } )
9499	def convert_completezip ( path ) : for filepath in path . glob ( '**/index_auto_generated.cnxml' ) : filepath . rename ( filepath . parent / 'index.cnxml' ) logger . debug ( 'removed {}' . format ( filepath ) ) for filepath in path . glob ( '**/index.cnxml.html' ) : filepath . unlink ( ) return parse_litezip ( path )
1381	def getComponentExceptionSummary ( self , tmaster , component_name , instances = [ ] , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return exception_request = tmaster_pb2 . ExceptionLogRequest ( ) exception_request . component_name = component_name if len ( instances ) > 0 : exception_request . instances . extend ( instances ) request_str = exception_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/exceptionsummary" . format ( host , port ) Log . debug ( "Creating request object." ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch exceptionsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) # Check the response code - error if it is in 400s or 500s responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) # Parse the response from tmaster. exception_response = tmaster_pb2 . ExceptionLogResponse ( ) exception_response . ParseFromString ( result . body ) if exception_response . status . status == common_pb2 . NOTOK : if exception_response . status . HasField ( "message" ) : raise tornado . gen . Return ( { "message" : exception_response . status . message } ) # Send response ret = [ ] for exception_log in exception_response . exceptions : ret . append ( { 'class_name' : exception_log . stacktrace , 'lasttime' : exception_log . lasttime , 'firsttime' : exception_log . firsttime , 'count' : str ( exception_log . count ) } ) raise tornado . gen . Return ( ret )
6766	def interfaces ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : if is_file ( '/usr/sbin/dladm' ) : res = run ( '/usr/sbin/dladm show-link' ) else : res = sudo ( '/sbin/ifconfig -s' ) return [ line . split ( ' ' ) [ 0 ] for line in res . splitlines ( ) [ 1 : ] ]
2815	def convert_avgpool ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width = params [ 'kernel_shape' ] else : height , width = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width = params [ 'strides' ] else : stride_height , stride_width = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , _ , _ = params [ 'pads' ] else : padding_h , padding_w = params [ 'padding' ] input_name = inputs [ 0 ] pad = 'valid' if height % 2 == 1 and width % 2 == 1 and height // 2 == padding_h and width // 2 == padding_w and stride_height == 1 and stride_width == 1 : pad = 'same' else : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding2D ( padding = ( padding_h , padding_w ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name # Pooling type AveragePooling2D pooling = keras . layers . AveragePooling2D ( pool_size = ( height , width ) , strides = ( stride_height , stride_width ) , padding = pad , name = tf_name , data_format = 'channels_first' ) layers [ scope_name ] = pooling ( layers [ input_name ] )
1540	def set_config ( self , config ) : if not isinstance ( config , dict ) : raise TypeError ( "Argument to set_config needs to be dict, given: %s" % str ( config ) ) self . _topology_config = config
10379	def calculate_concordance_by_annotation ( graph , annotation , key , cutoff = None ) : return { value : calculate_concordance ( subgraph , key , cutoff = cutoff ) for value , subgraph in get_subgraphs_by_annotation ( graph , annotation ) . items ( ) }
10493	def clickMouseButtonLeft ( self , coord , interval = None ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) if interval : self . _postQueuedEvents ( interval = interval ) else : self . _postQueuedEvents ( )
10133	def parse_scalar ( scalar_data , version ) : try : return hs_scalar [ version ] . parseString ( scalar_data , parseAll = True ) [ 0 ] except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse scalar: %s' % reformat_exception ( pe ) , scalar_data , 1 , pe . col ) except : LOG . debug ( 'Failing scalar data: %r (version %r)' , scalar_data , version )
555	def getAllSwarms ( self , sprintIdx ) : swarmIds = [ ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : if info [ 'sprintIdx' ] == sprintIdx : swarmIds . append ( swarmId ) return swarmIds
5899	def get_double_or_single_prec_mdrun ( ) : try : gromacs . mdrun_d ( h = True , stdout = False , stderr = False ) logger . debug ( "using double precision gromacs.mdrun_d" ) return gromacs . mdrun_d except ( AttributeError , GromacsError , OSError ) : # fall back to mdrun if no double precision binary wmsg = "No 'mdrun_d' binary found so trying 'mdrun' instead.\n" "(Note that energy minimization runs better with mdrun_d.)" logger . warn ( wmsg ) warnings . warn ( wmsg , category = AutoCorrectionWarning ) return gromacs . mdrun
1535	def init_topology ( mcs , classname , class_dict ) : if classname == 'Topology' : # Base class can't initialize protobuf return heron_options = TopologyType . get_heron_options_from_env ( ) initial_state = heron_options . get ( "cmdline.topology.initial.state" , "RUNNING" ) tmp_directory = heron_options . get ( "cmdline.topologydefn.tmpdirectory" ) if tmp_directory is None : raise RuntimeError ( "Topology definition temp directory not specified" ) topology_name = heron_options . get ( "cmdline.topology.name" , classname ) topology_id = topology_name + str ( uuid . uuid4 ( ) ) # create protobuf topology = topology_pb2 . Topology ( ) topology . id = topology_id topology . name = topology_name topology . state = topology_pb2 . TopologyState . Value ( initial_state ) topology . topology_config . CopyFrom ( TopologyType . get_topology_config_protobuf ( class_dict ) ) TopologyType . add_bolts_and_spouts ( topology , class_dict ) class_dict [ 'topology_name' ] = topology_name class_dict [ 'topology_id' ] = topology_id class_dict [ 'protobuf_topology' ] = topology class_dict [ 'topologydefn_tmpdir' ] = tmp_directory class_dict [ 'heron_runtime_options' ] = heron_options
4076	def cfg_to_args ( config ) : kwargs = { } opts_to_args = { 'metadata' : [ ( 'name' , 'name' ) , ( 'author' , 'author' ) , ( 'author-email' , 'author_email' ) , ( 'maintainer' , 'maintainer' ) , ( 'maintainer-email' , 'maintainer_email' ) , ( 'home-page' , 'url' ) , ( 'summary' , 'description' ) , ( 'description' , 'long_description' ) , ( 'download-url' , 'download_url' ) , ( 'classifier' , 'classifiers' ) , ( 'platform' , 'platforms' ) , ( 'license' , 'license' ) , ( 'keywords' , 'keywords' ) , ] , 'files' : [ ( 'packages_root' , 'package_dir' ) , ( 'packages' , 'packages' ) , ( 'modules' , 'py_modules' ) , ( 'scripts' , 'scripts' ) , ( 'package_data' , 'package_data' ) , ( 'data_files' , 'data_files' ) , ] , } opts_to_args [ 'metadata' ] . append ( ( 'requires-dist' , 'install_requires' ) ) if IS_PY2K and not which ( '3to2' ) : kwargs [ 'setup_requires' ] = [ '3to2' ] kwargs [ 'zip_safe' ] = False for section in opts_to_args : for option , argname in opts_to_args [ section ] : value = get_cfg_value ( config , section , option ) if value : kwargs [ argname ] = value if 'long_description' not in kwargs : kwargs [ 'long_description' ] = read_description_file ( config ) if 'package_dir' in kwargs : kwargs [ 'package_dir' ] = { '' : kwargs [ 'package_dir' ] } if 'keywords' in kwargs : kwargs [ 'keywords' ] = split_elements ( kwargs [ 'keywords' ] ) if 'package_data' in kwargs : kwargs [ 'package_data' ] = get_package_data ( kwargs [ 'package_data' ] ) if 'data_files' in kwargs : kwargs [ 'data_files' ] = get_data_files ( kwargs [ 'data_files' ] ) kwargs [ 'version' ] = get_version ( ) if not IS_PY2K : kwargs [ 'test_suite' ] = 'test' return kwargs
7496	def calculate ( seqnon , mapcol , nmask , tests ) : ## create empty matrices #LOGGER.info("tests[0] %s", tests[0]) #LOGGER.info('seqnon[[tests[0]]] %s', seqnon[[tests[0]]]) mats = chunk_to_matrices ( seqnon , mapcol , nmask ) ## empty svdscores for each arrangement of seqchunk svds = np . zeros ( ( 3 , 16 ) , dtype = np . float64 ) qscores = np . zeros ( 3 , dtype = np . float64 ) ranks = np . zeros ( 3 , dtype = np . float64 ) for test in range ( 3 ) : ## get svd scores svds [ test ] = np . linalg . svd ( mats [ test ] . astype ( np . float64 ) ) [ 1 ] ranks [ test ] = np . linalg . matrix_rank ( mats [ test ] . astype ( np . float64 ) ) ## get minrank, or 11 minrank = int ( min ( 11 , ranks . min ( ) ) ) for test in range ( 3 ) : qscores [ test ] = np . sqrt ( np . sum ( svds [ test , minrank : ] ** 2 ) ) ## sort to find the best qorder best = np . where ( qscores == qscores . min ( ) ) [ 0 ] #best = qscores[qscores == qscores.min()][0] bidx = tests [ best ] [ 0 ] qsnps = count_snps ( mats [ best ] [ 0 ] ) return bidx , qsnps
7826	def payload_element_name ( element_name ) : def decorator ( klass ) : """The payload_element_name decorator.""" # pylint: disable-msg=W0212,W0404 from . stanzapayload import STANZA_PAYLOAD_CLASSES from . stanzapayload import STANZA_PAYLOAD_ELEMENTS if hasattr ( klass , "_pyxmpp_payload_element_name" ) : klass . _pyxmpp_payload_element_name . append ( element_name ) else : klass . _pyxmpp_payload_element_name = [ element_name ] if element_name in STANZA_PAYLOAD_CLASSES : logger = logging . getLogger ( 'pyxmpp.payload_element_name' ) logger . warning ( "Overriding payload class for {0!r}" . format ( element_name ) ) STANZA_PAYLOAD_CLASSES [ element_name ] = klass STANZA_PAYLOAD_ELEMENTS [ klass ] . append ( element_name ) return klass return decorator
9435	def _read_a_packet ( file_h , hdrp , layers = 0 ) : raw_packet_header = file_h . read ( 16 ) if not raw_packet_header or len ( raw_packet_header ) != 16 : return None # in case the capture file is not the same endianness as ours, we have to # use the correct byte order for the packet header if hdrp [ 0 ] . byteorder == 'big' : packet_header = struct . unpack ( '>IIII' , raw_packet_header ) else : packet_header = struct . unpack ( '<IIII' , raw_packet_header ) ( timestamp , timestamp_us , capture_len , packet_len ) = packet_header raw_packet_data = file_h . read ( capture_len ) if not raw_packet_data or len ( raw_packet_data ) != capture_len : return None if layers > 0 : layers -= 1 raw_packet = linklayer . clookup ( hdrp [ 0 ] . ll_type ) ( raw_packet_data , layers = layers ) else : raw_packet = raw_packet_data packet = pcap_packet ( hdrp , timestamp , timestamp_us , capture_len , packet_len , raw_packet ) return packet
6894	def parallel_starfeatures_lcdir ( lcdir , outdir , lc_catalog_pickle , neighbor_radius_arcsec , fileglob = None , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None , nworkers = NCPUS , recursive = True ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not fileglob : fileglob = dfileglob # now find the files LOGINFO ( 'searching for %s light curves in %s ...' % ( lcformat , lcdir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcdir , fileglob ) ) else : # use recursive glob for Python 3.5+ if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcdir , '**' , fileglob ) , recursive = True ) # otherwise, use os.walk and glob else : # use os.walk to go through the directories walker = os . walk ( lcdir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) # now that we have all the files, process them if matching and len ( matching ) > 0 : LOGINFO ( 'found %s light curves, getting starfeatures...' % len ( matching ) ) return parallel_starfeatures ( matching , outdir , lc_catalog_pickle , neighbor_radius_arcsec , deredden = deredden , custom_bandpasses = custom_bandpasses , maxobjects = maxobjects , lcformat = lcformat , lcformatdir = lcformatdir , nworkers = nworkers ) else : LOGERROR ( 'no light curve files in %s format found in %s' % ( lcformat , lcdir ) ) return None
8667	def put_key ( key_name , value , description , meta , modify , add , lock , key_type , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Stashing {0} key...' . format ( key_type ) ) stash . put ( name = key_name , value = _build_dict_from_key_value ( value ) , modify = modify , metadata = _build_dict_from_key_value ( meta ) , description = description , lock = lock , key_type = key_type , add = add ) click . echo ( 'Key stashed successfully' ) except GhostError as ex : sys . exit ( ex )
12953	def _add_id_to_keys ( self , pk , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . sadd ( self . _get_ids_key ( ) , pk )
8210	def contains_point ( self , x , y , d = 2 ) : if self . path != None and len ( self . path ) > 1 and self . path . contains ( x , y ) : # If all points around the mouse are also part of the path, # this means we are somewhere INSIDE the path. # Only points near the edge (i.e. on the outline stroke) # should propagate. if not self . path . contains ( x + d , y ) or not self . path . contains ( x , y + d ) or not self . path . contains ( x - d , y ) or not self . path . contains ( x , y - d ) or not self . path . contains ( x + d , y + d ) or not self . path . contains ( x - d , y - d ) or not self . path . contains ( x + d , y - d ) or not self . path . contains ( x - d , y + d ) : return True return False
13755	def write_to_file ( file_path , contents , encoding = "utf-8" ) : with codecs . open ( file_path , "w" , encoding ) as f : f . write ( contents )
4774	def contains_sequence ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : try : for i in xrange ( len ( self . val ) - len ( items ) + 1 ) : for j in xrange ( len ( items ) ) : if self . val [ i + j ] != items [ j ] : break else : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain sequence %s, but did not.' % ( self . val , self . _fmt_items ( items ) ) )
13620	def create_patch ( self , from_tag , to_tag ) : return str ( self . _git . diff ( '{}..{}' . format ( from_tag , to_tag ) , _tty_out = False ) )
11875	def getpassword ( prompt = "Password: " ) : fd = sys . stdin . fileno ( ) old = termios . tcgetattr ( fd ) new = termios . tcgetattr ( fd ) new [ 3 ] &= ~ termios . ECHO # lflags try : termios . tcsetattr ( fd , termios . TCSADRAIN , new ) passwd = raw_input ( prompt ) finally : termios . tcsetattr ( fd , termios . TCSADRAIN , old ) return passwd
1501	def template_slave_hcl ( cl_args , masters ) : slave_config_template = "%s/standalone/templates/slave.template.hcl" % cl_args [ "config_path" ] slave_config_actual = "%s/standalone/resources/slave.hcl" % cl_args [ "config_path" ] masters_in_quotes = [ '"%s"' % master for master in masters ] template_file ( slave_config_template , slave_config_actual , { "<nomad_masters:master_port>" : ", " . join ( masters_in_quotes ) } )
10838	def interactions ( self ) : interactions = [ ] url = PATHS [ 'GET_INTERACTIONS' ] % self . id response = self . api . get ( url = url ) for interaction in response [ 'interactions' ] : interactions . append ( ResponseObject ( interaction ) ) self . __interactions = interactions return self . __interactions
3768	def zs_to_Vfs ( zs , Vms ) : vol_is = [ zi * Vmi for zi , Vmi in zip ( zs , Vms ) ] tot = sum ( vol_is ) return [ vol_i / tot for vol_i in vol_is ]
5119	def initialize ( self , nActive = 1 , queues = None , edges = None , edge_type = None ) : if queues is None and edges is None and edge_type is None : if nActive >= 1 and isinstance ( nActive , numbers . Integral ) : qs = [ q . edge [ 2 ] for q in self . edge2queue if q . edge [ 3 ] != 0 ] n = min ( nActive , len ( qs ) ) queues = np . random . choice ( qs , size = n , replace = False ) elif not isinstance ( nActive , numbers . Integral ) : msg = "If queues is None, then nActive must be an integer." raise TypeError ( msg ) else : msg = ( "If queues is None, then nActive must be a " "positive int." ) raise ValueError ( msg ) else : queues = _get_queues ( self . g , queues , edges , edge_type ) queues = [ e for e in queues if self . edge2queue [ e ] . edge [ 3 ] != 0 ] if len ( queues ) == 0 : raise QueueingToolError ( "There were no queues to initialize." ) if len ( queues ) > self . max_agents : queues = queues [ : self . max_agents ] for ei in queues : self . edge2queue [ ei ] . set_active ( ) self . num_agents [ ei ] = self . edge2queue [ ei ] . _num_total keys = [ q . _key ( ) for q in self . edge2queue if q . _time < np . infty ] self . _fancy_heap = PriorityQueue ( keys , self . nE ) self . _initialized = True
8319	def parse_tables ( self , markup ) : tables = [ ] m = re . findall ( self . re [ "table" ] , markup ) for chunk in m : table = WikipediaTable ( ) table . properties = chunk . split ( "\n" ) [ 0 ] . strip ( "{|" ) . strip ( ) self . connect_table ( table , chunk , markup ) # Tables start with "{|". # On the same line can be properties, e.g. {| border="1" # The table heading starts with "|+". # A new row in the table starts with "|-". # The end of the table is marked with "|}". row = None for chunk in chunk . split ( "\n" ) : chunk = chunk . strip ( ) if chunk . startswith ( "|+" ) : title = self . plain ( chunk . strip ( "|+" ) ) table . title = title elif chunk . startswith ( "|-" ) : if row : row . properties = chunk . strip ( "|-" ) . strip ( ) table . append ( row ) row = None elif chunk . startswith ( "|}" ) : pass elif chunk . startswith ( "|" ) or chunk . startswith ( "!" ) : row = self . parse_table_row ( chunk , row ) # Append the last row. if row : table . append ( row ) if len ( table ) > 0 : tables . append ( table ) return tables
10106	def _process_tabs ( self , tabs , current_tab , group_current_tab ) : # Update references to the current tab for t in tabs : t . current_tab = current_tab t . group_current_tab = group_current_tab # Filter out hidden tabs tabs = list ( filter ( lambda t : t . tab_visible , tabs ) ) # Sort remaining tabs in-place tabs . sort ( key = lambda t : t . weight ) return tabs
1138	def _splitext ( p , sep , altsep , extsep ) : sepIndex = p . rfind ( sep ) if altsep : altsepIndex = p . rfind ( altsep ) sepIndex = max ( sepIndex , altsepIndex ) dotIndex = p . rfind ( extsep ) if dotIndex > sepIndex : # skip all leading dots filenameIndex = sepIndex + 1 while filenameIndex < dotIndex : if p [ filenameIndex ] != extsep : return p [ : dotIndex ] , p [ dotIndex : ] filenameIndex += 1 return p , ''
11569	def run ( self ) : while not self . is_stopped ( ) : # we can get an OSError: [Errno9] Bad file descriptor when shutting down # just ignore it try : if self . arduino . inWaiting ( ) : c = self . arduino . read ( ) self . command_deque . append ( ord ( c ) ) else : time . sleep ( .1 ) except OSError : pass except IOError : self . stop ( ) self . close ( )
8083	def transform ( self , mode = None ) : if mode : self . _canvas . mode = mode return self . _canvas . mode
6740	def get_packager ( ) : # TODO: remove once fabric stops using contextlib.nested. # https://github.com/fabric/fabric/issues/1364 import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_packager = get_rc ( 'common_packager' ) if common_packager : return common_packager #TODO:cache result by current env.host_string so we can handle multiple hosts with different OSes with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run ( 'cat /etc/fedora-release' ) if ret . succeeded : common_packager = YUM else : ret = _run ( 'cat /etc/lsb-release' ) if ret . succeeded : common_packager = APT else : for pn in PACKAGERS : ret = _run ( 'which %s' % pn ) if ret . succeeded : common_packager = pn break if not common_packager : raise Exception ( 'Unable to determine packager.' ) set_rc ( 'common_packager' , common_packager ) return common_packager
10925	def reset ( self , new_damping = None ) : self . _num_iter = 0 self . _inner_run_counter = 0 self . _J_update_counter = self . update_J_frequency self . _fresh_JTJ = False self . _has_run = False if new_damping is not None : self . damping = np . array ( new_damping ) . astype ( 'float' ) self . _set_err_paramvals ( )
7771	def payload_class_for_element_name ( element_name ) : logger . debug ( " looking up payload class for element: {0!r}" . format ( element_name ) ) logger . debug ( " known: {0!r}" . format ( STANZA_PAYLOAD_CLASSES ) ) if element_name in STANZA_PAYLOAD_CLASSES : return STANZA_PAYLOAD_CLASSES [ element_name ] else : return XMLPayload
4129	def _autocov ( s , * * kwargs ) : # only remove the mean once, if needed debias = kwargs . pop ( 'debias' , True ) axis = kwargs . get ( 'axis' , - 1 ) if debias : s = _remove_bias ( s , axis ) kwargs [ 'debias' ] = False return _crosscov ( s , s , * * kwargs )
8486	def get ( self , name , default , allow_default = True ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) if name not in self . settings : if not allow_default : raise LookupError ( 'No setting "{name}"' . format ( name = name ) ) self . settings [ name ] = default return self . settings [ name ]
5982	def output_figure ( array , as_subplot , output_path , output_filename , output_format ) : if not as_subplot : if output_format is 'show' : plt . show ( ) elif output_format is 'png' : plt . savefig ( output_path + output_filename + '.png' , bbox_inches = 'tight' ) elif output_format is 'fits' : array_util . numpy_array_2d_to_fits ( array_2d = array , file_path = output_path + output_filename + '.fits' , overwrite = True )
169	def find_intersections_with ( self , other ) : import shapely . geometry geom = _convert_var_to_shapely_geometry ( other ) result = [ ] for p_start , p_end in zip ( self . coords [ : - 1 ] , self . coords [ 1 : ] ) : ls = shapely . geometry . LineString ( [ p_start , p_end ] ) intersections = ls . intersection ( geom ) intersections = list ( _flatten_shapely_collection ( intersections ) ) intersections_points = [ ] for inter in intersections : if isinstance ( inter , shapely . geometry . linestring . LineString ) : inter_start = ( inter . coords [ 0 ] [ 0 ] , inter . coords [ 0 ] [ 1 ] ) inter_end = ( inter . coords [ - 1 ] [ 0 ] , inter . coords [ - 1 ] [ 1 ] ) intersections_points . extend ( [ inter_start , inter_end ] ) else : assert isinstance ( inter , shapely . geometry . point . Point ) , ( "Expected to find shapely.geometry.point.Point or " "shapely.geometry.linestring.LineString intersection, " "actually found %s." % ( type ( inter ) , ) ) intersections_points . append ( ( inter . x , inter . y ) ) # sort by distance to start point, this makes it later on easier # to remove duplicate points inter_sorted = sorted ( intersections_points , key = lambda p : np . linalg . norm ( np . float32 ( p ) - p_start ) ) result . append ( inter_sorted ) return result
7171	def train_subprocess ( self , * args , * * kwargs ) : ret = call ( [ sys . executable , '-m' , 'padatious' , 'train' , self . cache_dir , '-d' , json . dumps ( self . serialized_args ) , '-a' , json . dumps ( args ) , '-k' , json . dumps ( kwargs ) , ] ) if ret == 2 : raise TypeError ( 'Invalid train arguments: {} {}' . format ( args , kwargs ) ) data = self . serialized_args self . clear ( ) self . apply_training_args ( data ) self . padaos . compile ( ) if ret == 0 : self . must_train = False return True elif ret == 10 : # timeout return False else : raise ValueError ( 'Training failed and returned code: {}' . format ( ret ) )
8620	def getServerStates ( pbclient = None , dc_id = None , serverid = None , servername = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc_id is None : raise ValueError ( "argument 'dc_id' must not be None" ) server = None if serverid is None : if servername is None : raise ValueError ( "one of 'serverid' or 'servername' must be specified" ) # so, arg.servername is set (to whatever) server_info = select_where ( getServerInfo ( pbclient , dc_id ) , [ 'id' , 'name' , 'state' , 'vmstate' ] , name = servername ) if len ( server_info ) > 1 : raise NameError ( "ambiguous server name '{}'" . format ( servername ) ) if len ( server_info ) == 1 : server = server_info [ 0 ] else : # get by ID may also fail if it's removed # in this case, catch exception (message 404) and be quiet for a while # unfortunately this has changed from Py2 to Py3 try : server_info = pbclient . get_server ( dc_id , serverid , 1 ) server = dict ( id = server_info [ 'id' ] , name = server_info [ 'properties' ] [ 'name' ] , state = server_info [ 'metadata' ] [ 'state' ] , vmstate = server_info [ 'properties' ] [ 'vmState' ] ) except Exception : ex = sys . exc_info ( ) [ 1 ] if ex . args [ 0 ] is not None and ex . args [ 0 ] == 404 : print ( "Server w/ ID {} not found" . format ( serverid ) ) server = None else : raise ex # end try/except # end if/else(serverid) return server
4320	def set_globals ( self , dither = False , guard = False , multithread = False , replay_gain = False , verbosity = 2 ) : if not isinstance ( dither , bool ) : raise ValueError ( 'dither must be a boolean.' ) if not isinstance ( guard , bool ) : raise ValueError ( 'guard must be a boolean.' ) if not isinstance ( multithread , bool ) : raise ValueError ( 'multithread must be a boolean.' ) if not isinstance ( replay_gain , bool ) : raise ValueError ( 'replay_gain must be a boolean.' ) if verbosity not in VERBOSITY_VALS : raise ValueError ( 'Invalid value for VERBOSITY. Must be one {}' . format ( VERBOSITY_VALS ) ) global_args = [ ] if not dither : global_args . append ( '-D' ) if guard : global_args . append ( '-G' ) if multithread : global_args . append ( '--multi-threaded' ) if replay_gain : global_args . append ( '--replay-gain' ) global_args . append ( 'track' ) global_args . append ( '-V{}' . format ( verbosity ) ) self . globals = global_args return self
8037	def get_summarizer ( self , name ) : if name in self . summarizers : pass elif name == 'lexrank' : from . import lexrank self . summarizers [ name ] = lexrank . summarize elif name == 'mcp' : from . import mcp_summ self . summarizers [ name ] = mcp_summ . summarize return self . summarizers [ name ]
12607	def find_unique ( table , sample , unique_fields = None ) : res = search_unique ( table , sample , unique_fields ) if res is not None : return res . eid else : return res
884	def reset ( self ) : self . activeCells = [ ] self . winnerCells = [ ] self . activeSegments = [ ] self . matchingSegments = [ ]
6623	def _getTarball ( url , into_directory , cache_key , origin_info = None ) : try : access_common . unpackFromCache ( cache_key , into_directory ) except KeyError as e : tok = settings . getProperty ( 'github' , 'authtoken' ) headers = { } if tok is not None : headers [ 'Authorization' ] = 'token ' + str ( tok ) logger . debug ( 'GET %s' , url ) response = requests . get ( url , allow_redirects = True , stream = True , headers = headers ) response . raise_for_status ( ) logger . debug ( 'getting file: %s' , url ) logger . debug ( 'headers: %s' , response . headers ) response . raise_for_status ( ) # github doesn't exposes hashes of the archives being downloaded as far # as I can tell :( access_common . unpackTarballStream ( stream = response , into_directory = into_directory , hash = { } , cache_key = cache_key , origin_info = origin_info )
260	def create_perf_attrib_stats ( perf_attrib , risk_exposures ) : summary = OrderedDict ( ) total_returns = perf_attrib [ 'total_returns' ] specific_returns = perf_attrib [ 'specific_returns' ] common_returns = perf_attrib [ 'common_returns' ] summary [ 'Annualized Specific Return' ] = ep . annual_return ( specific_returns ) summary [ 'Annualized Common Return' ] = ep . annual_return ( common_returns ) summary [ 'Annualized Total Return' ] = ep . annual_return ( total_returns ) summary [ 'Specific Sharpe Ratio' ] = ep . sharpe_ratio ( specific_returns ) summary [ 'Cumulative Specific Return' ] = ep . cum_returns_final ( specific_returns ) summary [ 'Cumulative Common Return' ] = ep . cum_returns_final ( common_returns ) summary [ 'Total Returns' ] = ep . cum_returns_final ( total_returns ) summary = pd . Series ( summary , name = '' ) annualized_returns_by_factor = [ ep . annual_return ( perf_attrib [ c ] ) for c in risk_exposures . columns ] cumulative_returns_by_factor = [ ep . cum_returns_final ( perf_attrib [ c ] ) for c in risk_exposures . columns ] risk_exposure_summary = pd . DataFrame ( data = OrderedDict ( [ ( 'Average Risk Factor Exposure' , risk_exposures . mean ( axis = 'rows' ) ) , ( 'Annualized Return' , annualized_returns_by_factor ) , ( 'Cumulative Return' , cumulative_returns_by_factor ) , ] ) , index = risk_exposures . columns , ) return summary , risk_exposure_summary
4123	def _twosided_zerolag ( data , zerolag ) : res = twosided ( np . insert ( data , 0 , zerolag ) ) return res
13046	def f_hierarchical_passages ( reffs , citation ) : d = OrderedDict ( ) levels = [ x for x in citation ] for cit , name in reffs : ref = cit . split ( '-' ) [ 0 ] levs = [ '%{}|{}%' . format ( levels [ i ] . name , v ) for i , v in enumerate ( ref . split ( '.' ) ) ] getFromDict ( d , levs [ : - 1 ] ) [ name ] = cit return d
12112	def _savepath ( self , filename ) : ( basename , ext ) = os . path . splitext ( filename ) basename = basename if ( ext in self . extensions ) else filename ext = ext if ( ext in self . extensions ) else self . extensions [ 0 ] savepath = os . path . abspath ( os . path . join ( self . directory , '%s%s' % ( basename , ext ) ) ) return ( tempfile . mkstemp ( ext , basename + "_" , self . directory ) [ 1 ] if self . hash_suffix else savepath )
11293	def oembed_schema ( request ) : current_domain = Site . objects . get_current ( ) . domain url_schemes = [ ] # a list of dictionaries for all the urls we can match endpoint = reverse ( 'oembed_json' ) # the public endpoint for our oembeds providers = oembed . site . get_providers ( ) for provider in providers : # first make sure this provider class is exposed at the public endpoint if not provider . provides : continue match = None if isinstance ( provider , DjangoProvider ) : # django providers define their regex_list by using urlreversing url_pattern = resolver . reverse_dict . get ( provider . _meta . named_view ) # this regex replacement is set to be non-greedy, which results # in things like /news/*/*/*/*/ -- this is more explicit if url_pattern : regex = re . sub ( r'%\(.+?\)s' , '*' , url_pattern [ 0 ] [ 0 ] [ 0 ] ) match = 'http://%s/%s' % ( current_domain , regex ) elif isinstance ( provider , HTTPProvider ) : match = provider . url_scheme else : match = provider . regex if match : url_schemes . append ( { 'type' : provider . resource_type , 'matches' : match , 'endpoint' : endpoint } ) url_schemes . sort ( key = lambda item : item [ 'matches' ] ) response = HttpResponse ( mimetype = 'application/json' ) response . write ( simplejson . dumps ( url_schemes ) ) return response
11509	def delete_item ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id response = self . request ( 'midas.item.delete' , parameters ) return response
9632	def render_subject ( self , context ) : rendered = self . subject_template . render ( unescape ( context ) ) return rendered . strip ( )
12199	def description ( self ) : if self . _description is None : text = '\n' . join ( self . __doc__ . splitlines ( ) [ 1 : ] ) . strip ( ) lines = [ ] for line in map ( str . strip , text . splitlines ( ) ) : if line and lines : lines [ - 1 ] = ' ' . join ( ( lines [ - 1 ] , line ) ) elif line : lines . append ( line ) else : lines . append ( '' ) self . _description = '\n' . join ( lines ) return self . _description
9979	def extract_params ( source ) : funcdef = find_funcdef ( source ) params = [ ] for node in ast . walk ( funcdef . args ) : if isinstance ( node , ast . arg ) : if node . arg not in params : params . append ( node . arg ) return params
12497	def as_ndarray ( arr , copy = False , dtype = None , order = 'K' ) : if order not in ( 'C' , 'F' , 'A' , 'K' , None ) : raise ValueError ( "Invalid value for 'order': {}" . format ( str ( order ) ) ) if isinstance ( arr , np . memmap ) : if dtype is None : if order in ( 'K' , 'A' , None ) : ret = np . array ( np . asarray ( arr ) , copy = True ) else : ret = np . array ( np . asarray ( arr ) , copy = True , order = order ) else : if order in ( 'K' , 'A' , None ) : # always copy (even when dtype does not change) ret = np . asarray ( arr ) . astype ( dtype ) else : # load data from disk without changing order # Changing order while reading through a memmap is incredibly # inefficient. ret = _asarray ( np . array ( arr , copy = True ) , dtype = dtype , order = order ) elif isinstance ( arr , np . ndarray ) : ret = _asarray ( arr , dtype = dtype , order = order ) # In the present cas, np.may_share_memory result is always reliable. if np . may_share_memory ( ret , arr ) and copy : # order-preserving copy ret = ret . T . copy ( ) . T if ret . flags [ 'F_CONTIGUOUS' ] else ret . copy ( ) elif isinstance ( arr , ( list , tuple ) ) : if order in ( "A" , "K" ) : ret = np . asarray ( arr , dtype = dtype ) else : ret = np . asarray ( arr , dtype = dtype , order = order ) else : raise ValueError ( "Type not handled: {}" . format ( arr . __class__ ) ) return ret
9950	def get_node ( obj , args , kwargs ) : if args is None and kwargs is None : return ( obj , ) if kwargs is None : kwargs = { } return obj , _bind_args ( obj , args , kwargs )
10218	def plot_summary_axes ( graph : BELGraph , lax , rax , logx = True ) : ntc = count_functions ( graph ) etc = count_relations ( graph ) df = pd . DataFrame . from_dict ( dict ( ntc ) , orient = 'index' ) df_ec = pd . DataFrame . from_dict ( dict ( etc ) , orient = 'index' ) df . sort_values ( 0 , ascending = True ) . plot ( kind = 'barh' , logx = logx , ax = lax ) lax . set_title ( 'Number of nodes: {}' . format ( graph . number_of_nodes ( ) ) ) df_ec . sort_values ( 0 , ascending = True ) . plot ( kind = 'barh' , logx = logx , ax = rax ) rax . set_title ( 'Number of edges: {}' . format ( graph . number_of_edges ( ) ) )
8845	def _at_block_start ( tc , line ) : if tc . atBlockStart ( ) : return True column = tc . columnNumber ( ) indentation = len ( line ) - len ( line . lstrip ( ) ) return column <= indentation
3262	def get_workspaces ( self , names = None ) : if names is None : names = [ ] elif isinstance ( names , basestring ) : names = [ s . strip ( ) for s in names . split ( ',' ) if s . strip ( ) ] data = self . get_xml ( "{}/workspaces.xml" . format ( self . service_url ) ) workspaces = [ ] workspaces . extend ( [ workspace_from_index ( self , node ) for node in data . findall ( "workspace" ) ] ) if workspaces and names : return ( [ ws for ws in workspaces if ws . name in names ] ) return workspaces
1801	def LAHF ( cpu ) : used_regs = ( cpu . SF , cpu . ZF , cpu . AF , cpu . PF , cpu . CF ) is_expression = any ( issymbolic ( x ) for x in used_regs ) def make_flag ( val , offset ) : if is_expression : return Operators . ITEBV ( 8 , val , BitVecConstant ( 8 , 1 << offset ) , BitVecConstant ( 8 , 0 ) ) else : return val << offset cpu . AH = ( make_flag ( cpu . SF , 7 ) | make_flag ( cpu . ZF , 6 ) | make_flag ( 0 , 5 ) | make_flag ( cpu . AF , 4 ) | make_flag ( 0 , 3 ) | make_flag ( cpu . PF , 2 ) | make_flag ( 1 , 1 ) | make_flag ( cpu . CF , 0 ) )
548	def __deleteOutputCache ( self , modelID ) : # If this is our output, we should close the connection if modelID == self . _modelID and self . _predictionLogger is not None : self . _predictionLogger . close ( ) del self . __predictionCache self . _predictionLogger = None self . __predictionCache = None
5079	def strip_html_tags ( text , allowed_tags = None ) : if text is None : return if allowed_tags is None : allowed_tags = ALLOWED_TAGS return bleach . clean ( text , tags = allowed_tags , attributes = [ 'id' , 'class' , 'style' , 'href' , 'title' ] , strip = True )
13506	def get_position ( self , position_id ) : url = "/2/positions/%s" % position_id return self . position_from_json ( self . _get_resource ( url ) [ "position" ] )
7099	def child_added ( self , child ) : if child . widget : # TODO: Should we keep count and remove the adapter if not all # markers request it? self . parent ( ) . init_info_window_adapter ( ) super ( AndroidMapMarker , self ) . child_added ( child )
11114	def remove_repository ( self , path = None , relatedFiles = False , relatedFolders = False , verbose = True ) : if path is not None : realPath = os . path . realpath ( os . path . expanduser ( path ) ) else : realPath = self . __path if realPath is None : if verbose : warnings . warn ( 'path is None and current Repository is not initialized!' ) return if not self . is_repository ( realPath ) : if verbose : warnings . warn ( "No repository found in '%s'!" % realPath ) return # check for security if realPath == os . path . realpath ( '/..' ) : if verbose : warnings . warn ( 'You are about to wipe out your system !!! action aboarded' ) return # get repo if path is not None : repo = Repository ( ) repo . load_repository ( realPath ) else : repo = self # delete files if relatedFiles : for relativePath in repo . walk_files_relative_path ( ) : realPath = os . path . join ( repo . path , relativePath ) if not os . path . isfile ( realPath ) : continue if not os . path . exists ( realPath ) : continue os . remove ( realPath ) # delete directories if relatedFolders : for relativePath in reversed ( list ( repo . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( repo . path , relativePath ) # protect from wiping out the system if not os . path . isdir ( realPath ) : continue if not os . path . exists ( realPath ) : continue if not len ( os . listdir ( realPath ) ) : os . rmdir ( realPath ) # delete repository os . remove ( os . path . join ( repo . path , ".pyrepinfo" ) ) for fname in ( ".pyrepstate" , ".pyreplock" ) : p = os . path . join ( repo . path , fname ) if os . path . exists ( p ) : os . remove ( p ) # remove main directory if empty if os . path . isdir ( repo . path ) : if not len ( os . listdir ( repo . path ) ) : os . rmdir ( repo . path ) # reset repository repo . __reset_repository ( )
6595	def poll ( self ) : ret = self . communicationChannel . receive_finished ( ) self . nruns -= len ( ret ) return ret
938	def _getModelPickleFilePath ( saveModelDir ) : path = os . path . join ( saveModelDir , "model.pkl" ) path = os . path . abspath ( path ) return path
9661	def find_standard_sakefile ( settings ) : error = settings [ "error" ] if settings [ "customsake" ] : custom = settings [ "customsake" ] if not os . path . isfile ( custom ) : error ( "Specified sakefile '{}' doesn't exist" , custom ) sys . exit ( 1 ) return custom # no custom specified, going over defaults in order for name in [ "Sakefile" , "Sakefile.yaml" , "Sakefile.yml" ] : if os . path . isfile ( name ) : return name error ( "Error: there is no Sakefile to read" ) sys . exit ( 1 )
13122	def id_to_object ( self , line ) : result = Range . get ( line , ignore = 404 ) if not result : result = Range ( range = line ) result . save ( ) return result
12737	def parse_amc ( source ) : lines = 0 frames = 1 frame = { } degrees = False for line in source : lines += 1 line = line . split ( '#' ) [ 0 ] . strip ( ) if not line : continue if line . startswith ( ':' ) : if line . lower ( ) . startswith ( ':deg' ) : degrees = True continue if line . isdigit ( ) : if int ( line ) != frames : raise RuntimeError ( 'frame mismatch on line {}: ' 'produced {} but file claims {}' . format ( lines , frames , line ) ) yield frame frames += 1 frame = { } continue fields = line . split ( ) frame [ fields [ 0 ] ] = list ( map ( float , fields [ 1 : ] ) )
12250	def _get_key_internal ( self , * args , * * kwargs ) : if args [ 1 ] is not None and 'force' in args [ 1 ] : key , res = super ( Bucket , self ) . _get_key_internal ( * args , * * kwargs ) if key : mimicdb . backend . sadd ( tpl . bucket % self . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( self . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) ) return key , res key = None if mimicdb . backend . sismember ( tpl . bucket % self . name , args [ 0 ] ) : key = Key ( self ) key . name = args [ 0 ] return key , None
8255	def copy ( self ) : return ColorList ( [ color ( clr . r , clr . g , clr . b , clr . a , mode = "rgb" ) for clr in self ] , name = self . name , tags = self . tags )
10219	def remove_nodes_by_function_namespace ( graph : BELGraph , func : str , namespace : Strings ) -> None : remove_filtered_nodes ( graph , function_namespace_inclusion_builder ( func , namespace ) )
2900	def get_tasks ( self , state = Task . ANY_MASK ) : return [ t for t in Task . Iterator ( self . task_tree , state ) ]
130	def is_fully_within_image ( self , image ) : return not self . is_out_of_image ( image , fully = True , partly = True )
11594	def _rc_rename ( self , src , dst ) : if src == dst : return self . rename ( src + "{" + src + "}" , src ) if not self . exists ( src ) : return self . rename ( src + "{" + src + "}" , src ) self . delete ( dst ) ktype = self . type ( src ) kttl = self . ttl ( src ) if ktype == b ( 'none' ) : return False if ktype == b ( 'string' ) : self . set ( dst , self . get ( src ) ) elif ktype == b ( 'hash' ) : self . hmset ( dst , self . hgetall ( src ) ) elif ktype == b ( 'list' ) : for k in self . lrange ( src , 0 , - 1 ) : self . rpush ( dst , k ) elif ktype == b ( 'set' ) : for k in self . smembers ( src ) : self . sadd ( dst , k ) elif ktype == b ( 'zset' ) : for k , v in self . zrange ( src , 0 , - 1 , withscores = True ) : self . zadd ( dst , v , k ) # Handle keys with an expire time set kttl = - 1 if kttl is None or kttl < 0 else int ( kttl ) if kttl != - 1 : self . expire ( dst , kttl ) return self . delete ( src )
3405	def _sample_chain ( args ) : n , idx = args # has to be this way to work in Python 2.7 center = sampler . center np . random . seed ( ( sampler . _seed + idx ) % np . iinfo ( np . int32 ) . max ) pi = np . random . randint ( sampler . n_warmup ) prev = sampler . warmup [ pi , ] prev = step ( sampler , center , prev - center , 0.95 ) n_samples = max ( sampler . n_samples , 1 ) samples = np . zeros ( ( n , center . shape [ 0 ] ) ) for i in range ( 1 , sampler . thinning * n + 1 ) : pi = np . random . randint ( sampler . n_warmup ) delta = sampler . warmup [ pi , ] - center prev = step ( sampler , prev , delta ) if sampler . problem . homogeneous and ( n_samples * sampler . thinning % sampler . nproj == 0 ) : prev = sampler . _reproject ( prev ) center = sampler . _reproject ( center ) if i % sampler . thinning == 0 : samples [ i // sampler . thinning - 1 , ] = prev center = ( ( n_samples * center ) / ( n_samples + 1 ) + prev / ( n_samples + 1 ) ) n_samples += 1 return ( sampler . retries , samples )
620	def parseSdr ( s ) : assert isinstance ( s , basestring ) sdr = [ int ( c ) for c in s if c in ( "0" , "1" ) ] if len ( sdr ) != len ( s ) : raise ValueError ( "The provided string %s is malformed. The string should " "have only 0's and 1's." ) return sdr
911	def advance ( self ) : hasMore = True try : self . __iter . next ( ) except StopIteration : self . __iter = None hasMore = False return hasMore
8316	def parse_images ( self , markup , treshold = 6 ) : images = [ ] m = re . findall ( self . re [ "image" ] , markup ) for p in m : p = self . parse_balanced_image ( p ) img = p . split ( "|" ) path = img [ 0 ] . replace ( "[[Image:" , "" ) . strip ( ) description = u"" links = { } properties = [ ] if len ( img ) > 1 : img = "|" . join ( img [ 1 : ] ) links = self . parse_links ( img ) properties = self . plain ( img ) . split ( "|" ) description = u"" # Best guess: an image description is normally # longer than six characters, properties like # "thumb" and "right" are less than six characters. if len ( properties [ - 1 ] ) > treshold : description = properties [ - 1 ] properties = properties [ : - 1 ] img = WikipediaImage ( path , description , links , properties ) images . append ( img ) markup = markup . replace ( p , "" ) return images , markup . strip ( )
8796	def serialize_groups ( self , groups ) : rules = [ ] for group in groups : rules . extend ( self . serialize_rules ( group . rules ) ) return rules
8646	def create_milestone_request ( session , project_id , bid_id , description , amount ) : milestone_request_data = { 'project_id' : project_id , 'bid_id' : bid_id , 'description' : description , 'amount' : amount , } # POST /api/projects/0.1/milestone_requests/ response = make_post_request ( session , 'milestone_requests' , json_data = milestone_request_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_request_data = json_data [ 'result' ] return MilestoneRequest ( milestone_request_data ) else : raise MilestoneRequestNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11547	def url ( self ) : if len ( self . drivers ) > 0 : return self . drivers [ 0 ] . url else : return self . _url
1226	def tf_discounted_cumulative_reward ( self , terminal , reward , discount = None , final_reward = 0.0 , horizon = 0 ) : # By default -> take Model's gamma value if discount is None : discount = self . discount # Accumulates discounted (n-step) reward (start new if terminal) def cumulate ( cumulative , reward_terminal_horizon_subtract ) : rew , is_terminal , is_over_horizon , sub = reward_terminal_horizon_subtract return tf . where ( # If terminal, start new cumulation. condition = is_terminal , x = rew , y = tf . where ( # If we are above the horizon length (H) -> subtract discounted value from H steps back. condition = is_over_horizon , x = ( rew + cumulative * discount - sub ) , y = ( rew + cumulative * discount ) ) ) # Accumulates length of episodes (starts new if terminal) def len_ ( cumulative , term ) : return tf . where ( condition = term , # Start counting from 1 after is-terminal signal x = tf . ones ( shape = ( ) , dtype = tf . int32 ) , # Otherwise, increase length by 1 y = cumulative + 1 ) # Reverse, since reward cumulation is calculated right-to-left, but tf.scan only works left-to-right. reward = tf . reverse ( tensor = reward , axis = ( 0 , ) ) # e.g. -1.0 1.0 0.5 0.0 1.0 2.0 terminal = tf . reverse ( tensor = terminal , axis = ( 0 , ) ) # e.g. F T F F F F # Store the steps until end of the episode(s) determined by the input terminal signals (True starts new count). lengths = tf . scan ( fn = len_ , elems = terminal , initializer = 0 ) # e.g. 1 1 2 3 4 5 off_horizon = tf . greater ( lengths , tf . fill ( dims = tf . shape ( lengths ) , value = horizon ) ) # e.g. F F F F T T # Calculate the horizon-subtraction value for each step. if horizon > 0 : horizon_subtractions = tf . map_fn ( lambda x : ( discount ** horizon ) * x , reward , dtype = tf . float32 ) # Shift right by size of horizon (fill rest with 0.0). horizon_subtractions = tf . concat ( [ np . zeros ( shape = ( horizon , ) ) , horizon_subtractions ] , axis = 0 ) horizon_subtractions = tf . slice ( horizon_subtractions , begin = ( 0 , ) , size = tf . shape ( reward ) ) # e.g. 0.0, 0.0, 0.0, -1.0*g^3, 1.0*g^3, 0.5*g^3 # all 0.0 if infinite horizon (special case: horizon=0) else : horizon_subtractions = tf . zeros ( shape = tf . shape ( reward ) ) # Now do the scan, each time summing up the previous step (discounted by gamma) and # subtracting the respective `horizon_subtraction`. reward = tf . scan ( fn = cumulate , elems = ( reward , terminal , off_horizon , horizon_subtractions ) , initializer = final_reward if horizon != 1 else 0.0 ) # Re-reverse again to match input sequences. return tf . reverse ( tensor = reward , axis = ( 0 , ) )
4107	def morlet ( lb , ub , n ) : if n <= 0 : raise ValueError ( "n must be strictly positive" ) x = numpy . linspace ( lb , ub , n ) psi = numpy . cos ( 5 * x ) * numpy . exp ( - x ** 2 / 2. ) return psi
12796	def parse ( self , text , key = None ) : try : data = json . loads ( text ) except ValueError as e : raise ValueError ( "%s: Value: [%s]" % ( e , text ) ) if data and key : if key not in data : raise ValueError ( "Invalid response (key %s not found): %s" % ( key , data ) ) data = data [ key ] return data
12885	def python_value ( self , value ) : if self . field_type == 'TEXT' and isinstance ( value , str ) : return self . loads ( value ) return value
10479	def _getActions ( self ) : actions = _a11y . AXUIElement . _getActions ( self ) # strip leading AX from actions - help distinguish them from attributes return [ action [ 2 : ] for action in actions ]
12424	def loads ( s , separator = DEFAULT , index_separator = DEFAULT , cls = dict , list_cls = list ) : if isinstance ( s , six . text_type ) : io = StringIO ( s ) else : io = BytesIO ( s ) return load ( fp = io , separator = separator , index_separator = index_separator , cls = cls , list_cls = list_cls , )
2321	def get_default ( self , * args , * * kwargs ) : def retrieve_param ( i ) : try : return self . __getattribute__ ( i ) except AttributeError : if i == "device" : return self . default_device else : return self . __getattribute__ ( i . upper ( ) ) if len ( args ) == 0 : if len ( kwargs ) == 1 and kwargs [ list ( kwargs . keys ( ) ) [ 0 ] ] is not None : return kwargs [ list ( kwargs . keys ( ) ) [ 0 ] ] elif len ( kwargs ) == 1 : return retrieve_param ( list ( kwargs . keys ( ) ) [ 0 ] ) else : raise TypeError ( "As dict is unordered, it is impossible to give" "the parameters in the correct order." ) else : out = [ ] for i in args : if i [ 1 ] is None : out . append ( retrieve_param ( i [ 0 ] ) ) else : out . append ( i [ 1 ] ) return out
11135	def copyto ( self , new_abspath = None , new_dirpath = None , new_dirname = None , new_basename = None , new_fname = None , new_ext = None , overwrite = False , makedirs = False ) : self . assert_exists ( ) p = self . change ( new_abspath = new_abspath , new_dirpath = new_dirpath , new_dirname = new_dirname , new_basename = new_basename , new_fname = new_fname , new_ext = new_ext , ) if p . is_not_exist_or_allow_overwrite ( overwrite = overwrite ) : # 如果两个路径不同, 才进行copy if self . abspath != p . abspath : try : shutil . copy ( self . abspath , p . abspath ) except IOError as e : if makedirs : os . makedirs ( p . parent . abspath ) shutil . copy ( self . abspath , p . abspath ) else : raise e return p
4981	def set_final_prices ( self , modes , request ) : result = [ ] for mode in modes : if mode [ 'premium' ] : mode [ 'final_price' ] = EcommerceApiClient ( request . user ) . get_course_final_price ( mode = mode , enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None , ) result . append ( mode ) return result
8029	def sizeClassifier ( path , min_size = DEFAULTS [ 'min_size' ] ) : filestat = _stat ( path ) if stat . S_ISLNK ( filestat . st_mode ) : return # Skip symlinks. if filestat . st_size < min_size : return # Skip files below the size limit return filestat . st_size
12153	def convolve ( signal , kernel ) : pad = np . ones ( len ( kernel ) / 2 ) signal = np . concatenate ( ( pad * signal [ 0 ] , signal , pad * signal [ - 1 ] ) ) signal = np . convolve ( signal , kernel , mode = 'same' ) signal = signal [ len ( pad ) : - len ( pad ) ] return signal
12410	def all_package_versions ( package ) : info = PyPI . package_info ( package ) return info and sorted ( info [ 'releases' ] . keys ( ) , key = lambda x : x . split ( ) , reverse = True ) or [ ]
10007	def get_object ( self , name ) : parts = name . split ( "." ) space = self . spaces [ parts . pop ( 0 ) ] if parts : return space . get_object ( "." . join ( parts ) ) else : return space
7607	def search_tournaments ( self , name : str , * * params : keys ) : url = self . api . TOURNAMENT params [ 'name' ] = name return self . _get_model ( url , PartialTournament , * * params )
3748	def calculate ( self , T , method ) : if method == GHARAGHEIZI : mu = Gharagheizi_gas_viscosity ( T , self . Tc , self . Pc , self . MW ) elif method == COOLPROP : mu = CoolProp_T_dependent_property ( T , self . CASRN , 'V' , 'g' ) elif method == DIPPR_PERRY_8E : mu = EQ102 ( T , * self . Perrys2_312_coeffs ) elif method == VDI_PPDS : mu = horner ( self . VDI_PPDS_coeffs , T ) elif method == YOON_THODOS : mu = Yoon_Thodos ( T , self . Tc , self . Pc , self . MW ) elif method == STIEL_THODOS : mu = Stiel_Thodos ( T , self . Tc , self . Pc , self . MW ) elif method == LUCAS_GAS : mu = lucas_gas ( T , self . Tc , self . Pc , self . Zc , self . MW , self . dipole , CASRN = self . CASRN ) elif method in self . tabular_data : mu = self . interpolate ( T , method ) return mu
5434	def parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) : job_params = [ ] for col in header : # Reserve the "-" and "--" namespace. # If the column has no leading "-", treat it as an environment variable col_type = '--env' col_value = col if col . startswith ( '-' ) : col_type , col_value = split_pair ( col , ' ' , 1 ) if col_type == '--env' : job_params . append ( job_model . EnvParam ( col_value ) ) elif col_type == '--label' : job_params . append ( job_model . LabelParam ( col_value ) ) elif col_type == '--input' or col_type == '--input-recursive' : name = input_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . InputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) elif col_type == '--output' or col_type == '--output-recursive' : name = output_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . OutputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) else : raise ValueError ( 'Unrecognized column header: %s' % col ) return job_params
4702	def create ( ) : if env ( ) : cij . err ( "cij.lnvm.create: Invalid LNVM ENV" ) return 1 nvme = cij . env_to_dict ( "NVME" , [ "DEV_NAME" ] ) lnvm = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cij . emph ( "lnvm.create: LNVM_DEV_NAME: %s" % lnvm [ "DEV_NAME" ] ) cmd = [ "nvme lnvm create -d %s -n %s -t %s -b %s -e %s -f" % ( nvme [ "DEV_NAME" ] , lnvm [ "DEV_NAME" ] , lnvm [ "DEV_TYPE" ] , lnvm [ "BGN" ] , lnvm [ "END" ] ) ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True ) if rcode : cij . err ( "cij.lnvm.create: FAILED" ) return 1 return 0
9588	def _execute ( self , command , data = None , unpack = True ) : if not data : data = { } if self . session_id is not None : data . setdefault ( 'session_id' , self . session_id ) data = self . _wrap_el ( data ) res = self . remote_invoker . execute ( command , data ) ret = WebDriverResult . from_object ( res ) ret . raise_for_status ( ) ret . value = self . _unwrap_el ( ret . value ) if not unpack : return ret return ret . value
4369	def send ( self , message , json = False , callback = None ) : pkt = dict ( type = "message" , data = message , endpoint = self . ns_name ) if json : pkt [ 'type' ] = "json" if callback : # By passing ack=True, we use the old behavior of being returned # an 'ack' packet, automatically triggered by the client-side # with no user-code being run. The emit() version of the # callback is more useful I think :) So migrate your code. pkt [ 'ack' ] = True pkt [ 'id' ] = msgid = self . socket . _get_next_msgid ( ) self . socket . _save_ack_callback ( msgid , callback ) self . socket . send_packet ( pkt )
1001	def printComputeEnd ( self , output , learn = False ) : if self . verbosity >= 3 : print "----- computeEnd summary: " print "learn:" , learn print "numBurstingCols: %s, " % ( self . infActiveState [ 't' ] . min ( axis = 1 ) . sum ( ) ) , print "curPredScore2: %s, " % ( self . _internalStats [ 'curPredictionScore2' ] ) , print "curFalsePosScore: %s, " % ( self . _internalStats [ 'curFalsePositiveScore' ] ) , print "1-curFalseNegScore: %s, " % ( 1 - self . _internalStats [ 'curFalseNegativeScore' ] ) print "numSegments: " , self . getNumSegments ( ) , print "avgLearnedSeqLength: " , self . avgLearnedSeqLength print "----- infActiveState (%d on) ------" % ( self . infActiveState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . infActiveState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . infActiveState [ 't' ] ) print "----- infPredictedState (%d on)-----" % ( self . infPredictedState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . infPredictedState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . infPredictedState [ 't' ] ) print "----- lrnActiveState (%d on) ------" % ( self . lrnActiveState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . lrnActiveState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . lrnActiveState [ 't' ] ) print "----- lrnPredictedState (%d on)-----" % ( self . lrnPredictedState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . lrnPredictedState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . lrnPredictedState [ 't' ] ) print "----- cellConfidence -----" self . printActiveIndices ( self . cellConfidence [ 't' ] , andValues = True ) if self . verbosity >= 6 : self . printConfidence ( self . cellConfidence [ 't' ] ) print "----- colConfidence -----" self . printActiveIndices ( self . colConfidence [ 't' ] , andValues = True ) print "----- cellConfidence[t-1] for currently active cells -----" cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] self . printActiveIndices ( cc , andValues = True ) if self . verbosity == 4 : print "Cells, predicted segments only:" self . printCells ( predictedOnly = True ) elif self . verbosity >= 5 : print "Cells, all segments:" self . printCells ( predictedOnly = False ) print elif self . verbosity >= 1 : print "TM: learn:" , learn print "TM: active outputs(%d):" % len ( output . nonzero ( ) [ 0 ] ) , self . printActiveIndices ( output . reshape ( self . numberOfCols , self . cellsPerColumn ) )
12308	def get_files_to_commit ( autooptions ) : workingdir = autooptions [ 'working-directory' ] includes = autooptions [ 'track' ] [ 'includes' ] excludes = autooptions [ 'track' ] [ 'excludes' ] # transform glob patterns to regular expressions # print("Includes ", includes) includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) excludes = r'|' . join ( [ fnmatch . translate ( x ) for x in excludes ] ) or r'$.' matched_files = [ ] for root , dirs , files in os . walk ( workingdir ) : # print("Looking at ", files) # exclude dirs # dirs[:] = [os.path.join(root, d) for d in dirs] dirs [ : ] = [ d for d in dirs if not re . match ( excludes , d ) ] # exclude/include files files = [ f for f in files if not re . match ( excludes , f ) ] #print("Files after excludes", files) #print(includes) files = [ f for f in files if re . match ( includes , f ) ] #print("Files after includes", files) files = [ os . path . join ( root , f ) for f in files ] matched_files . extend ( files ) return matched_files
4567	def _write ( self , filename , frames , fps , loop = 0 , palette = 256 ) : from PIL import Image images = [ ] for f in frames : data = open ( f , 'rb' ) . read ( ) images . append ( Image . open ( io . BytesIO ( data ) ) ) # GIF duration is only measured to a hundredth of a second duration = round ( 1 / fps , 2 ) im = images . pop ( 0 ) im . save ( filename , save_all = True , append_images = images , duration = duration , loop = loop , palette = palette )
13067	def make_parents ( self , collection , lang = None ) : return [ { "id" : member . id , "label" : str ( member . get_label ( lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size } for member in collection . parents if member . get_label ( ) ]
3332	def init_logging ( config ) : verbose = config . get ( "verbose" , 3 ) enable_loggers = config . get ( "enable_loggers" , [ ] ) if enable_loggers is None : enable_loggers = [ ] logger_date_format = config . get ( "logger_date_format" , "%Y-%m-%d %H:%M:%S" ) logger_format = config . get ( "logger_format" , "%(asctime)s.%(msecs)03d - <%(thread)d> %(name)-27s %(levelname)-8s: %(message)s" , ) formatter = logging . Formatter ( logger_format , logger_date_format ) # Define handlers consoleHandler = logging . StreamHandler ( sys . stdout ) # consoleHandler = logging.StreamHandler(sys.stderr) consoleHandler . setFormatter ( formatter ) # consoleHandler.setLevel(logging.DEBUG) # Add the handlers to the base logger logger = logging . getLogger ( BASE_LOGGER_NAME ) if verbose >= 4 : # --verbose logger . setLevel ( logging . DEBUG ) elif verbose == 3 : # default logger . setLevel ( logging . INFO ) elif verbose == 2 : # --quiet logger . setLevel ( logging . WARN ) # consoleHandler.setLevel(logging.WARN) elif verbose == 1 : # -qq logger . setLevel ( logging . ERROR ) # consoleHandler.setLevel(logging.WARN) else : # -qqq logger . setLevel ( logging . CRITICAL ) # consoleHandler.setLevel(logging.ERROR) # Don't call the root's handlers after our custom handlers logger . propagate = False # Remove previous handlers for hdlr in logger . handlers [ : ] : # Must iterate an array copy try : hdlr . flush ( ) hdlr . close ( ) except Exception : pass logger . removeHandler ( hdlr ) logger . addHandler ( consoleHandler ) if verbose >= 3 : for e in enable_loggers : if not e . startswith ( BASE_LOGGER_NAME + "." ) : e = BASE_LOGGER_NAME + "." + e lg = logging . getLogger ( e . strip ( ) ) lg . setLevel ( logging . DEBUG )
1546	def get_component_metrics ( component , cluster , env , topology , role ) : all_queries = metric_queries ( ) try : result = get_topology_metrics ( cluster , env , topology , component , [ ] , all_queries , [ 0 , - 1 ] , role ) return result [ "metrics" ] except Exception : Log . debug ( traceback . format_exc ( ) ) raise
10064	def process_schema ( value ) : schemas = current_app . extensions [ 'invenio-jsonschemas' ] . schemas try : return schemas [ value ] except KeyError : raise click . BadParameter ( 'Unknown schema {0}. Please use one of:\n {1}' . format ( value , '\n' . join ( schemas . keys ( ) ) ) )
13489	def update ( self , server ) : for chunk in self . __cut_to_size ( ) : server . put ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
3823	async def get_conversation ( self , get_conversation_request ) : response = hangouts_pb2 . GetConversationResponse ( ) await self . _pb_request ( 'conversations/getconversation' , get_conversation_request , response ) return response
13187	def image_path ( instance , filename ) : filename , ext = os . path . splitext ( filename . lower ( ) ) instance_id_hash = hashlib . md5 ( str ( instance . id ) ) . hexdigest ( ) filename_hash = '' . join ( random . sample ( hashlib . md5 ( filename . encode ( 'utf-8' ) ) . hexdigest ( ) , 8 ) ) return '{}/{}{}' . format ( instance_id_hash , filename_hash , ext )
976	def _newRepresentation ( self , index , newIndex ) : newRepresentation = self . bucketMap [ index ] . copy ( ) # Choose the bit we will replace in this representation. We need to shift # this bit deterministically. If this is always chosen randomly then there # is a 1 in w chance of the same bit being replaced in neighboring # representations, which is fairly high ri = newIndex % self . w # Now we choose a bit such that the overlap rules are satisfied. newBit = self . random . getUInt32 ( self . n ) newRepresentation [ ri ] = newBit while newBit in self . bucketMap [ index ] or not self . _newRepresentationOK ( newRepresentation , newIndex ) : self . numTries += 1 newBit = self . random . getUInt32 ( self . n ) newRepresentation [ ri ] = newBit return newRepresentation
3704	def Yamada_Gunn ( T , Tc , Pc , omega ) : return R * Tc / Pc * ( 0.29056 - 0.08775 * omega ) ** ( 1 + ( 1 - T / Tc ) ** ( 2 / 7. ) )
11655	def transform ( self , X , * * params ) : X = as_features ( X , stack = True ) X_new = self . transformer . transform ( X . stacked_features , * * params ) return self . _gather_outputs ( X , X_new )
11366	def _do_unzip ( zipped_file , output_directory ) : z = zipfile . ZipFile ( zipped_file ) for path in z . namelist ( ) : relative_path = os . path . join ( output_directory , path ) dirname , dummy = os . path . split ( relative_path ) try : if relative_path . endswith ( os . sep ) and not os . path . exists ( dirname ) : os . makedirs ( relative_path ) elif not os . path . exists ( relative_path ) : dirname = os . path . join ( output_directory , os . path . dirname ( path ) ) if os . path . dirname ( path ) and not os . path . exists ( dirname ) : os . makedirs ( dirname ) fd = open ( relative_path , "w" ) fd . write ( z . read ( path ) ) fd . close ( ) except IOError , e : raise e return output_directory
7531	def _plot_dag ( dag , results , snames ) : try : import matplotlib . pyplot as plt from matplotlib . dates import date2num from matplotlib . cm import gist_rainbow ## first figure is dag layout plt . figure ( "dag_layout" , figsize = ( 10 , 10 ) ) nx . draw ( dag , pos = nx . spring_layout ( dag ) , node_color = 'pink' , with_labels = True ) plt . savefig ( "./dag_layout.png" , bbox_inches = 'tight' , dpi = 200 ) ## second figure is times for steps pos = { } colors = { } for node in dag : #jobkey = "{}-{}".format(node, sample) mtd = results [ node ] . metadata start = date2num ( mtd . started ) #runtime = date2num(md.completed)# - start ## sample id to separate samples on x-axis _ , _ , sname = node . split ( "-" , 2 ) sid = snames . index ( sname ) ## 1e6 to separate on y-axis pos [ node ] = ( start + sid , start * 1e6 ) colors [ node ] = mtd . engine_id ## x just spaces out samples; ## y is start time of each job with edge leading to next job ## color is the engine that ran the job ## all jobs were submitted as 3 second wait times plt . figure ( "dag_starttimes" , figsize = ( 10 , 16 ) ) nx . draw ( dag , pos , node_list = colors . keys ( ) , node_color = colors . values ( ) , cmap = gist_rainbow , with_labels = True ) plt . savefig ( "./dag_starttimes.png" , bbox_inches = 'tight' , dpi = 200 ) except Exception as inst : LOGGER . warning ( inst )
12716	def position_rates ( self ) : return [ self . ode_obj . getPositionRate ( i ) for i in range ( self . LDOF ) ]
5823	def to_dict ( self ) : return { "type" : self . type , "name" : self . name , "group_by_key" : self . group_by_key , "role" : self . role , "units" : self . units , "options" : self . build_options ( ) }
10239	def count_citations_by_annotation ( graph : BELGraph , annotation : str ) -> Mapping [ str , typing . Counter [ str ] ] : citations = defaultdict ( lambda : defaultdict ( set ) ) for u , v , data in graph . edges ( data = True ) : if not edge_has_annotation ( data , annotation ) or CITATION not in data : continue k = data [ ANNOTATIONS ] [ annotation ] citations [ k ] [ u , v ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) ) return { k : Counter ( itt . chain . from_iterable ( v . values ( ) ) ) for k , v in citations . items ( ) }
5210	def bdp ( tickers , flds , * * kwargs ) : logger = logs . get_logger ( bdp , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) con , _ = create_connection ( ) ovrds = assist . proc_ovrds ( * * kwargs ) logger . info ( f'loading reference data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) data = con . ref ( tickers = tickers , flds = flds , ovrds = ovrds ) if not kwargs . get ( 'cache' , False ) : return [ data ] qry_data = [ ] for r , snap in data . iterrows ( ) : subset = [ r ] data_file = storage . ref_file ( ticker = snap . ticker , fld = snap . field , ext = 'pkl' , * * kwargs ) if data_file : if not files . exists ( data_file ) : qry_data . append ( data . iloc [ subset ] ) files . create_folder ( data_file , is_file = True ) data . iloc [ subset ] . to_pickle ( data_file ) return qry_data
10625	def _calculate_Hfr_coal ( self , T ) : m_C = 0 # kg/h m_H = 0 # kg/h m_O = 0 # kg/h m_N = 0 # kg/h m_S = 0 # kg/h Hfr = 0.0 # kWh/h for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) formula = compound . split ( '[' ) [ 0 ] if stoich . element_mass_fraction ( formula , 'C' ) == 1.0 : m_C += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'H' ) == 1.0 : m_H += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'O' ) == 1.0 : m_O += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'N' ) == 1.0 : m_N += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'S' ) == 1.0 : m_S += self . _compound_mfrs [ index ] else : dHfr = thermo . H ( compound , T , self . _compound_mfrs [ index ] ) Hfr += dHfr m_total = m_C + m_H + m_O + m_N + m_S # kg/h y_C = m_C / m_total y_H = m_H / m_total y_O = m_O / m_total y_N = m_N / m_total y_S = m_S / m_total hmodel = coals . DafHTy ( ) H = hmodel . calculate ( T = T + 273.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 # kWh/kg H298 = hmodel . calculate ( T = 298.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 # kWh/kg Hdaf = H - H298 + self . _DH298 # kWh/kg Hdaf *= m_total # kWh/h Hfr += Hdaf return Hfr
10273	def remove_unweighted_sources ( graph : BELGraph , key : Optional [ str ] = None ) -> None : nodes = list ( get_unweighted_sources ( graph , key = key ) ) graph . remove_nodes_from ( nodes )
10813	def search ( cls , query , q ) : return query . filter ( Group . name . like ( '%{0}%' . format ( q ) ) )
6012	def load_exposure_time_map ( exposure_time_map_path , exposure_time_map_hdu , pixel_scale , shape , exposure_time , exposure_time_map_from_inverse_noise_map , inverse_noise_map ) : exposure_time_map_options = sum ( [ exposure_time_map_from_inverse_noise_map ] ) if exposure_time is not None and exposure_time_map_path is not None : raise exc . DataException ( 'You have supplied both a exposure_time_map_path to an exposure time map and an exposure time. Only' 'one quantity should be supplied.' ) if exposure_time_map_options == 0 : if exposure_time is not None and exposure_time_map_path is None : return ExposureTimeMap . single_value ( value = exposure_time , pixel_scale = pixel_scale , shape = shape ) elif exposure_time is None and exposure_time_map_path is not None : return ExposureTimeMap . from_fits_with_pixel_scale ( file_path = exposure_time_map_path , hdu = exposure_time_map_hdu , pixel_scale = pixel_scale ) else : if exposure_time_map_from_inverse_noise_map : return ExposureTimeMap . from_exposure_time_and_inverse_noise_map ( pixel_scale = pixel_scale , exposure_time = exposure_time , inverse_noise_map = inverse_noise_map )
8480	def get ( name , default = None , allow_default = True ) : return Config ( ) . get ( name , default , allow_default = allow_default )
3064	def update_query_params ( uri , params ) : parts = urllib . parse . urlparse ( uri ) query_params = parse_unique_urlencoded ( parts . query ) query_params . update ( params ) new_query = urllib . parse . urlencode ( query_params ) new_parts = parts . _replace ( query = new_query ) return urllib . parse . urlunparse ( new_parts )
13863	def ts ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) )
3492	def _error_string ( error , k = None ) : package = error . getPackage ( ) if package == '' : package = 'core' template = 'E{} ({}): {} ({}, L{}); {}; {}' error_str = template . format ( k , error . getSeverityAsString ( ) , error . getCategoryAsString ( ) , package , error . getLine ( ) , error . getShortMessage ( ) , error . getMessage ( ) ) return error_str
1632	def CheckForBadCharacters ( filename , lines , error ) : for linenum , line in enumerate ( lines ) : if unicode_escape_decode ( '\ufffd' ) in line : error ( filename , linenum , 'readability/utf8' , 5 , 'Line contains invalid UTF-8 (or Unicode replacement character).' ) if '\0' in line : error ( filename , linenum , 'readability/nul' , 5 , 'Line contains NUL byte.' )
4131	def get_docstring_and_rest ( filename ) : with open ( filename ) as f : content = f . read ( ) node = ast . parse ( content ) if not isinstance ( node , ast . Module ) : raise TypeError ( "This function only supports modules. " "You provided {0}" . format ( node . __class__ . __name__ ) ) if node . body and isinstance ( node . body [ 0 ] , ast . Expr ) and isinstance ( node . body [ 0 ] . value , ast . Str ) : docstring_node = node . body [ 0 ] docstring = docstring_node . value . s # This get the content of the file after the docstring last line # Note: 'maxsplit' argument is not a keyword argument in python2 rest = content . split ( '\n' , docstring_node . lineno ) [ - 1 ] return docstring , rest else : raise ValueError ( ( 'Could not find docstring in file "{0}". ' 'A docstring is required by sphinx-gallery' ) . format ( filename ) )
11298	def get_all_text ( node ) : if node . nodeType == node . TEXT_NODE : return node . data else : text_string = "" for child_node in node . childNodes : text_string += get_all_text ( child_node ) return text_string
12783	def speak ( self , message ) : campfire = self . get_campfire ( ) if not isinstance ( message , Message ) : message = Message ( campfire , message ) result = self . _connection . post ( "room/%s/speak" % self . id , { "message" : message . get_data ( ) } , parse_data = True , key = "message" ) if result [ "success" ] : return Message ( campfire , result [ "data" ] ) return result [ "success" ]
190	def deepcopy ( self , line_strings = None , shape = None ) : lss = self . line_strings if line_strings is None else line_strings shape = self . shape if shape is None else shape return LineStringsOnImage ( line_strings = [ ls . deepcopy ( ) for ls in lss ] , shape = tuple ( shape ) )
12992	def line_chunker ( text , getreffs , lines = 30 ) : level = len ( text . citation ) source_reffs = [ reff . split ( ":" ) [ - 1 ] for reff in getreffs ( level = level ) ] reffs = [ ] i = 0 while i + lines - 1 < len ( source_reffs ) : reffs . append ( tuple ( [ source_reffs [ i ] + "-" + source_reffs [ i + lines - 1 ] , source_reffs [ i ] ] ) ) i += lines if i < len ( source_reffs ) : reffs . append ( tuple ( [ source_reffs [ i ] + "-" + source_reffs [ len ( source_reffs ) - 1 ] , source_reffs [ i ] ] ) ) return reffs
6212	def plane_xz ( size = ( 10 , 10 ) , resolution = ( 10 , 10 ) ) -> VAO : sx , sz = size rx , rz = resolution dx , dz = sx / rx , sz / rz # step ox , oz = - sx / 2 , - sz / 2 # start offset def gen_pos ( ) : for z in range ( rz ) : for x in range ( rx ) : yield ox + x * dx yield 0 yield oz + z * dz def gen_uv ( ) : for z in range ( rz ) : for x in range ( rx ) : yield x / ( rx - 1 ) yield 1 - z / ( rz - 1 ) def gen_normal ( ) : for _ in range ( rx * rz ) : yield 0.0 yield 1.0 yield 0.0 def gen_index ( ) : for z in range ( rz - 1 ) : for x in range ( rx - 1 ) : # quad poly left yield z * rz + x + 1 yield z * rz + x yield z * rz + x + rx # quad poly right yield z * rz + x + 1 yield z * rz + x + rx yield z * rz + x + rx + 1 pos_data = numpy . fromiter ( gen_pos ( ) , dtype = numpy . float32 ) uv_data = numpy . fromiter ( gen_uv ( ) , dtype = numpy . float32 ) normal_data = numpy . fromiter ( gen_normal ( ) , dtype = numpy . float32 ) index_data = numpy . fromiter ( gen_index ( ) , dtype = numpy . uint32 ) vao = VAO ( "plane_xz" , mode = moderngl . TRIANGLES ) vao . buffer ( pos_data , '3f' , [ 'in_position' ] ) vao . buffer ( uv_data , '2f' , [ 'in_uv' ] ) vao . buffer ( normal_data , '3f' , [ 'in_normal' ] ) vao . index_buffer ( index_data , index_element_size = 4 ) return vao
11850	def move_to ( self , thing , destination ) : thing . bump = self . some_things_at ( destination , Obstacle ) if not thing . bump : thing . location = destination for o in self . observers : o . thing_moved ( thing )
1155	def pop ( self ) : it = iter ( self ) try : value = next ( it ) except StopIteration : raise KeyError self . discard ( value ) return value
1476	def _get_ckptmgr_process ( self ) : ckptmgr_main_class = 'org.apache.heron.ckptmgr.CheckpointManager' ckptmgr_ram_mb = self . checkpoint_manager_ram / ( 1024 * 1024 ) ckptmgr_cmd = [ os . path . join ( self . heron_java_home , "bin/java" ) , '-Xms%dM' % ckptmgr_ram_mb , '-Xmx%dM' % ckptmgr_ram_mb , '-XX:+PrintCommandLineFlags' , '-verbosegc' , '-XX:+PrintGCDetails' , '-XX:+PrintGCTimeStamps' , '-XX:+PrintGCDateStamps' , '-XX:+PrintGCCause' , '-XX:+UseGCLogFileRotation' , '-XX:NumberOfGCLogFiles=5' , '-XX:GCLogFileSize=100M' , '-XX:+PrintPromotionFailure' , '-XX:+PrintTenuringDistribution' , '-XX:+PrintHeapAtGC' , '-XX:+HeapDumpOnOutOfMemoryError' , '-XX:+UseConcMarkSweepGC' , '-XX:+UseConcMarkSweepGC' , '-Xloggc:log-files/gc.ckptmgr.log' , '-Djava.net.preferIPv4Stack=true' , '-cp' , self . checkpoint_manager_classpath , ckptmgr_main_class , '-t' + self . topology_name , '-i' + self . topology_id , '-c' + self . ckptmgr_ids [ self . shard ] , '-p' + self . checkpoint_manager_port , '-f' + self . stateful_config_file , '-o' + self . override_config_file , '-g' + self . heron_internals_config_file ] retval = { } retval [ self . ckptmgr_ids [ self . shard ] ] = Command ( ckptmgr_cmd , self . shell_env ) return retval
5089	def dropHistoricalTable ( apps , schema_editor ) : table_name = 'sap_success_factors_historicalsapsuccessfactorsenterprisecus80ad' if table_name in connection . introspection . table_names ( ) : migrations . DeleteModel ( name = table_name , )
2587	def start ( self ) : start = time . time ( ) self . _kill_event = threading . Event ( ) self . procs = { } for worker_id in range ( self . worker_count ) : p = multiprocessing . Process ( target = worker , args = ( worker_id , self . uid , self . pending_task_queue , self . pending_result_queue , self . ready_worker_queue , ) ) p . start ( ) self . procs [ worker_id ] = p logger . debug ( "Manager synced with workers" ) self . _task_puller_thread = threading . Thread ( target = self . pull_tasks , args = ( self . _kill_event , ) ) self . _result_pusher_thread = threading . Thread ( target = self . push_results , args = ( self . _kill_event , ) ) self . _task_puller_thread . start ( ) self . _result_pusher_thread . start ( ) logger . info ( "Loop start" ) # TODO : Add mechanism in this loop to stop the worker pool # This might need a multiprocessing event to signal back. self . _kill_event . wait ( ) logger . critical ( "[MAIN] Received kill event, terminating worker processes" ) self . _task_puller_thread . join ( ) self . _result_pusher_thread . join ( ) for proc_id in self . procs : self . procs [ proc_id ] . terminate ( ) logger . critical ( "Terminating worker {}:{}" . format ( self . procs [ proc_id ] , self . procs [ proc_id ] . is_alive ( ) ) ) self . procs [ proc_id ] . join ( ) logger . debug ( "Worker:{} joined successfully" . format ( self . procs [ proc_id ] ) ) self . task_incoming . close ( ) self . result_outgoing . close ( ) self . context . term ( ) delta = time . time ( ) - start logger . info ( "process_worker_pool ran for {} seconds" . format ( delta ) ) return
5433	def split_pair ( pair_string , separator , nullable_idx = 1 ) : pair = pair_string . split ( separator , 1 ) if len ( pair ) == 1 : if nullable_idx == 0 : return [ None , pair [ 0 ] ] elif nullable_idx == 1 : return [ pair [ 0 ] , None ] else : raise IndexError ( 'nullable_idx should be either 0 or 1.' ) else : return pair
5891	def smart_str ( string , encoding = 'utf-8' , strings_only = False , errors = 'strict' ) : if strings_only and isinstance ( string , ( type ( None ) , int ) ) : return string # if isinstance(s, Promise): # return unicode(s).encode(encoding, errors) if isinstance ( string , str ) : try : return string . encode ( encoding , errors ) except UnicodeEncodeError : return string . encode ( 'utf-8' , errors ) elif not isinstance ( string , bytes ) : try : return str ( string ) . encode ( encoding , errors ) except UnicodeEncodeError : if isinstance ( string , Exception ) : # An Exception subclass containing non-ASCII data that doesn't # know how to print itself properly. We shouldn't raise a # further exception. return ' ' . join ( [ smart_str ( arg , encoding , strings_only , errors ) for arg in string ] ) return str ( string ) . encode ( encoding , errors ) else : return string
9819	def teardown ( self , hooks = True ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) if self . is_kubernetes : self . teardown_on_kubernetes ( hooks = hooks ) elif self . is_docker_compose : self . teardown_on_docker_compose ( ) elif self . is_docker : self . teardown_on_docker ( hooks = hooks ) elif self . is_heroku : self . teardown_on_heroku ( hooks = hooks )
2106	def _echo_setting ( key ) : value = getattr ( settings , key ) secho ( '%s: ' % key , fg = 'magenta' , bold = True , nl = False ) secho ( six . text_type ( value ) , bold = True , fg = 'white' if isinstance ( value , six . text_type ) else 'cyan' , )
11474	def login ( email = None , password = None , api_key = None , application = 'Default' , url = None , verify_ssl_certificate = True ) : try : input_ = raw_input except NameError : input_ = input if url is None : url = input_ ( 'Server URL: ' ) url = url . rstrip ( '/' ) if session . communicator is None : session . communicator = Communicator ( url ) else : session . communicator . url = url session . communicator . verify_ssl_certificate = verify_ssl_certificate if email is None : email = input_ ( 'Email: ' ) session . email = email if api_key is None : if password is None : password = getpass . getpass ( ) session . api_key = session . communicator . get_default_api_key ( session . email , password ) session . application = 'Default' else : session . api_key = api_key session . application = application return renew_token ( )
1556	def get_out_streamids ( self ) : if self . outputs is None : return set ( ) if not isinstance ( self . outputs , ( list , tuple ) ) : raise TypeError ( "Argument to outputs must be either list or tuple, given: %s" % str ( type ( self . outputs ) ) ) ret_lst = [ ] for output in self . outputs : if not isinstance ( output , ( str , Stream ) ) : raise TypeError ( "Outputs must be a list of strings or Streams, given: %s" % str ( output ) ) ret_lst . append ( Stream . DEFAULT_STREAM_ID if isinstance ( output , str ) else output . stream_id ) return set ( ret_lst )
6538	def compile_masks ( masks ) : if not masks : masks = [ ] elif not isinstance ( masks , ( list , tuple ) ) : masks = [ masks ] return [ re . compile ( mask ) for mask in masks ]
5232	def all_files ( path_name , keyword = '' , ext = '' , full_path = True , has_date = False , date_fmt = DATE_FMT ) -> list : if not os . path . exists ( path = path_name ) : return [ ] path_name = path_name . replace ( '\\' , '/' ) if keyword or ext : keyword = f'*{keyword}*' if keyword else '*' if not ext : ext = '*' files = sort_by_modified ( [ f . replace ( '\\' , '/' ) for f in glob . iglob ( f'{path_name}/{keyword}.{ext}' ) if os . path . isfile ( f ) and ( f . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] [ 0 ] != '~' ) ] ) else : files = sort_by_modified ( [ f'{path_name}/{f}' for f in os . listdir ( path = path_name ) if os . path . isfile ( f'{path_name}/{f}' ) and ( f [ 0 ] != '~' ) ] ) if has_date : files = filter_by_dates ( files , date_fmt = date_fmt ) return files if full_path else [ f . split ( '/' ) [ - 1 ] for f in files ]
13063	def semantic ( self , collection , parent = None ) : if parent is not None : collections = parent . parents [ : : - 1 ] + [ parent , collection ] else : collections = collection . parents [ : : - 1 ] + [ collection ] return filters . slugify ( "--" . join ( [ item . get_label ( ) for item in collections if item . get_label ( ) ] ) )
4586	def parse ( s ) : parts = s . replace ( ',' , ' ' ) . split ( ) if not parts : raise ValueError ( 'Cannot parse empty string' ) pieces = [ ] for part in parts : m = PART_MATCH ( part ) pieces . extend ( m . groups ( ) if m else [ part ] ) if len ( pieces ) == 1 : pieces . append ( 's' ) if len ( pieces ) % 2 : raise ValueError ( 'Malformed duration %s: %s: %s' % ( s , parts , pieces ) ) result = 0 for number , units in zip ( * [ iter ( pieces ) ] * 2 ) : number = float ( number ) if number < 0 : raise ValueError ( 'Durations cannot have negative components' ) result += number * _get_units ( units ) return result
12037	def matrixValues ( matrix , key ) : assert key in matrix . dtype . names col = matrix . dtype . names . index ( key ) values = np . empty ( len ( matrix ) ) * np . nan for i in range ( len ( matrix ) ) : values [ i ] = matrix [ i ] [ col ] return values
6217	def prepare_attrib_mapping ( self , primitive ) : buffer_info = [ ] for name , accessor in primitive . attributes . items ( ) : info = VBOInfo ( * accessor . info ( ) ) info . attributes . append ( ( name , info . components ) ) if buffer_info and buffer_info [ - 1 ] . buffer_view == info . buffer_view : if buffer_info [ - 1 ] . interleaves ( info ) : buffer_info [ - 1 ] . merge ( info ) continue buffer_info . append ( info ) return buffer_info
8140	def invert ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "RGB" ) self . img = ImageOps . invert ( self . img ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
3404	def normalize_cutoff ( model , zero_cutoff = None ) : if zero_cutoff is None : return model . tolerance else : if zero_cutoff < model . tolerance : raise ValueError ( "The chosen zero cutoff cannot be less than the model's " "tolerance value." ) else : return zero_cutoff
10433	def selectlastrow ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) cell = object_handle . AXRows [ - 1 ] if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : # Selected pass return 1
10069	def index ( method = None , delete = False ) : if method is None : return partial ( index , delete = delete ) @ wraps ( method ) def wrapper ( self_or_cls , * args , * * kwargs ) : """Send record for indexing.""" result = method ( self_or_cls , * args , * * kwargs ) try : if delete : self_or_cls . indexer . delete ( result ) else : self_or_cls . indexer . index ( result ) except RequestError : current_app . logger . exception ( 'Could not index {0}.' . format ( result ) ) return result return wrapper
13770	def render_asset ( self , name ) : result = "" if self . has_asset ( name ) : asset = self . get_asset ( name ) if asset . files : for f in asset . files : result += f . render_include ( ) + "\r\n" return result
5221	def exch_info ( ticker : str ) -> pd . Series : logger = logs . get_logger ( exch_info , level = 'debug' ) if ' ' not in ticker . strip ( ) : ticker = f'XYZ {ticker.strip()} Equity' info = param . load_info ( cat = 'exch' ) . get ( market_info ( ticker = ticker ) . get ( 'exch' , '' ) , dict ( ) ) if ( 'allday' in info ) and ( 'day' not in info ) : info [ 'day' ] = info [ 'allday' ] if any ( req not in info for req in [ 'tz' , 'allday' , 'day' ] ) : logger . error ( f'required exchange info cannot be found in {ticker} ...' ) return pd . Series ( ) for ss in ValidSessions : if ss not in info : continue info [ ss ] = [ param . to_hour ( num = s ) for s in info [ ss ] ] return pd . Series ( info )
1123	def Tok ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : return parser . _accept ( kind ) return rule
2873	def trash ( self , file ) : if self . _should_skipped_by_specs ( file ) : self . reporter . unable_to_trash_dot_entries ( file ) return volume_of_file_to_be_trashed = self . volume_of_parent ( file ) self . reporter . volume_of_file ( volume_of_file_to_be_trashed ) candidates = self . _possible_trash_directories_for ( volume_of_file_to_be_trashed ) self . try_trash_file_using_candidates ( file , volume_of_file_to_be_trashed , candidates )
3856	def unread_events ( self ) : return [ conv_event for conv_event in self . _events if conv_event . timestamp > self . latest_read_timestamp ]
13299	def install ( self , package ) : logger . debug ( 'Installing ' + package ) shell . run ( self . pip_path , 'install' , package )
7818	def _update_handlers ( self ) : handler_map = defaultdict ( list ) for i , obj in enumerate ( self . handlers ) : for dummy , handler in inspect . getmembers ( obj , callable ) : if not hasattr ( handler , "_pyxmpp_event_handled" ) : continue # pylint: disable-msg=W0212 event_class = handler . _pyxmpp_event_handled handler_map [ event_class ] . append ( ( i , handler ) ) self . _handler_map = handler_map
6023	def new_psf_with_renormalized_array ( self ) : return PSF ( array = self , pixel_scale = self . pixel_scale , renormalize = True )
993	def _getFirstOnBit ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return [ None ] else : if input < self . minval : # Don't clip periodic inputs. Out-of-range input is always an error if self . clipInput and not self . periodic : if self . verbosity > 0 : print "Clipped input %s=%.2f to minval %.2f" % ( self . name , input , self . minval ) input = self . minval else : raise Exception ( 'input (%s) less than range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) if self . periodic : # Don't clip periodic inputs. Out-of-range input is always an error if input >= self . maxval : raise Exception ( 'input (%s) greater than periodic range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) else : if input > self . maxval : if self . clipInput : if self . verbosity > 0 : print "Clipped input %s=%.2f to maxval %.2f" % ( self . name , input , self . maxval ) input = self . maxval else : raise Exception ( 'input (%s) greater than range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) if self . periodic : centerbin = int ( ( input - self . minval ) * self . nInternal / self . range ) + self . padding else : centerbin = int ( ( ( input - self . minval ) + self . resolution / 2 ) / self . resolution ) + self . padding # We use the first bit to be set in the encoded output as the bucket index minbin = centerbin - self . halfwidth return [ minbin ]
12780	def get_users ( self , sort = True ) : self . _load ( ) if sort : self . users . sort ( key = operator . itemgetter ( "name" ) ) return self . users
6378	def manhattan ( src , tar , qval = 2 , normalized = False , alphabet = None ) : return Manhattan ( ) . dist_abs ( src , tar , qval , normalized , alphabet )
1819	def SETO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF , 1 , 0 ) )
3780	def load_all_methods ( self ) : methods = [ ] Tmins , Tmaxs = [ ] , [ ] if self . CASRN in [ '7732-18-5' , '67-56-1' , '64-17-5' ] : methods . append ( TEST_METHOD_1 ) self . TEST_METHOD_1_Tmin = 200. self . TEST_METHOD_1_Tmax = 350 self . TEST_METHOD_1_coeffs = [ 1 , .002 ] Tmins . append ( self . TEST_METHOD_1_Tmin ) Tmaxs . append ( self . TEST_METHOD_1_Tmax ) if self . CASRN in [ '67-56-1' ] : methods . append ( TEST_METHOD_2 ) self . TEST_METHOD_2_Tmin = 300. self . TEST_METHOD_2_Tmax = 400 self . TEST_METHOD_2_coeffs = [ 1 , .003 ] Tmins . append ( self . TEST_METHOD_2_Tmin ) Tmaxs . append ( self . TEST_METHOD_2_Tmax ) self . all_methods = set ( methods ) if Tmins and Tmaxs : self . Tmin = min ( Tmins ) self . Tmax = max ( Tmaxs )
899	def prettyPrintSequence ( self , sequence , verbosity = 1 ) : text = "" for i in xrange ( len ( sequence ) ) : pattern = sequence [ i ] if pattern == None : text += "<reset>" if i < len ( sequence ) - 1 : text += "\n" else : text += self . patternMachine . prettyPrintPattern ( pattern , verbosity = verbosity ) return text
11407	def record_get_field_instances ( rec , tag = "" , ind1 = " " , ind2 = " " ) : if not rec : return [ ] if not tag : return rec . items ( ) else : out = [ ] ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) if '%' in tag : # Wildcard in tag. Check all possible for field_tag in rec : if _tag_matches_pattern ( field_tag , tag ) : for possible_field_instance in rec [ field_tag ] : if ( ind1 in ( '%' , possible_field_instance [ 1 ] ) and ind2 in ( '%' , possible_field_instance [ 2 ] ) ) : out . append ( possible_field_instance ) else : # Completely defined tag. Use dict for possible_field_instance in rec . get ( tag , [ ] ) : if ( ind1 in ( '%' , possible_field_instance [ 1 ] ) and ind2 in ( '%' , possible_field_instance [ 2 ] ) ) : out . append ( possible_field_instance ) return out
10756	def iso_path_slugify ( path , path_table , is_dir = False , strict = True ) : # Split the path to extract the parent and basename parent , base = split ( path ) # Get the parent in slugified form slug_parent = path_table [ parent ] # Slugify the base name if is_dir : slug_base = iso_name_slugify ( base ) [ : 8 ] else : name , ext = base . rsplit ( '.' , 1 ) if '.' in base else ( base , '' ) slug_base = '.' . join ( [ iso_name_slugify ( name ) [ : 8 ] , ext ] ) if strict : slug_base = slug_base . upper ( ) # Deduplicate slug if needed and update path_table slugs = set ( path_table . values ( ) ) path_table [ path ] = slug = join ( slug_parent , slug_base ) while slug in slugs : slug_base = iso_name_increment ( slug_base , is_dir ) path_table [ path ] = slug = join ( slug_parent , slug_base ) # Return the unique slug return slug
5804	def detect_client_auth_request ( server_handshake_bytes ) : for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0d' : return True return False
12768	def load_skeleton ( self , filename , pid_params = None ) : self . skeleton = skeleton . Skeleton ( self ) self . skeleton . load ( filename , color = ( 0.3 , 0.5 , 0.9 , 0.8 ) ) if pid_params : self . skeleton . set_pid_params ( * * pid_params ) self . skeleton . erp = 0.1 self . skeleton . cfm = 0
12491	def as_float_array ( X , copy = True , force_all_finite = True ) : if isinstance ( X , np . matrix ) or ( not isinstance ( X , np . ndarray ) and not sp . issparse ( X ) ) : return check_array ( X , [ 'csr' , 'csc' , 'coo' ] , dtype = np . float64 , copy = copy , force_all_finite = force_all_finite , ensure_2d = False ) elif sp . issparse ( X ) and X . dtype in [ np . float32 , np . float64 ] : return X . copy ( ) if copy else X elif X . dtype in [ np . float32 , np . float64 ] : # is numpy array return X . copy ( 'F' if X . flags [ 'F_CONTIGUOUS' ] else 'C' ) if copy else X else : return X . astype ( np . float32 if X . dtype == np . int32 else np . float64 )
8895	def _with_error_handling ( resp , error , mode , response_format ) : def safe_parse ( r ) : try : return APIWrapper . _parse_resp ( r , response_format ) except ( ValueError , SyntaxError ) as ex : log . error ( ex ) r . parsed = None return r if isinstance ( error , requests . HTTPError ) : if resp . status_code == 400 : # It means that request parameters were rejected by the server, # so we need to enrich standard error message # with 'ValidationErrors' # from the response resp = safe_parse ( resp ) if resp . parsed is not None : parsed_resp = resp . parsed messages = [ ] if response_format == 'xml' and parsed_resp . find ( './ValidationErrors' ) is not None : messages = [ e . find ( './Message' ) . text for e in parsed_resp . findall ( './ValidationErrors/ValidationErrorDto' ) ] elif response_format == 'json' and 'ValidationErrors' in parsed_resp : messages = [ e [ 'Message' ] for e in parsed_resp [ 'ValidationErrors' ] ] error = requests . HTTPError ( '%s: %s' % ( error , '\n\t' . join ( messages ) ) , response = resp ) elif resp . status_code == 429 : error = requests . HTTPError ( '%sToo many requests in the last minute.' % error , response = resp ) if STRICT == mode : raise error elif GRACEFUL == mode : if isinstance ( error , EmptyResponse ) : # Empty response is returned by the API occasionally, # in this case it makes sense to ignore it and retry. log . warning ( error ) resp . parsed = None return resp elif isinstance ( error , requests . HTTPError ) : # Ignoring 'Too many requests' error, # since subsequent retries will come after a delay. if resp . status_code == 429 : # Too many requests log . warning ( error ) return safe_parse ( resp ) else : raise error else : raise error else : # ignore everything, just log it and return whatever response we # have log . error ( error ) return safe_parse ( resp )
5790	def peek_openssl_error ( ) : error = libcrypto . ERR_peek_error ( ) lib = int ( ( error >> 24 ) & 0xff ) func = int ( ( error >> 12 ) & 0xfff ) reason = int ( error & 0xfff ) return ( lib , func , reason )
7623	def melody ( ref , est , * * kwargs ) : namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_times , ref_p = ref . to_event_values ( ) est_times , est_p = est . to_event_values ( ) ref_freq = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_freq = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . melody . evaluate ( ref_times , ref_freq , est_times , est_freq , * * kwargs )
9683	def toggle_laser ( self , state ) : # Send the command byte and wait 10 ms a = self . cnxn . xfer ( [ 0x03 ] ) [ 0 ] sleep ( 10e-3 ) # If state is true, turn the laser ON, else OFF if state : b = self . cnxn . xfer ( [ 0x02 ] ) [ 0 ] else : b = self . cnxn . xfer ( [ 0x03 ] ) [ 0 ] sleep ( 0.1 ) return True if a == 0xF3 and b == 0x03 else False
12450	def _get_effect_statement ( self , effect , methods ) : statements = [ ] if len ( methods ) > 0 : statement = self . _get_empty_statement ( effect ) for method in methods : if ( method [ 'conditions' ] is None or len ( method [ 'conditions' ] ) == 0 ) : statement [ 'Resource' ] . append ( method [ 'resource_arn' ] ) else : cond_statement = self . _get_empty_statement ( effect ) cond_statement [ 'Resource' ] . append ( method [ 'resource_arn' ] ) cond_statement [ 'Condition' ] = method [ 'conditions' ] statements . append ( cond_statement ) statements . append ( statement ) return statements
1433	def custom ( cls , customgrouper ) : if customgrouper is None : raise TypeError ( "Argument to custom() must be ICustomGrouping instance or classpath" ) if not isinstance ( customgrouper , ICustomGrouping ) and not isinstance ( customgrouper , str ) : raise TypeError ( "Argument to custom() must be ICustomGrouping instance or classpath" ) serialized = default_serializer . serialize ( customgrouper ) return cls . custom_serialized ( serialized , is_java = False )
869	def clear ( cls , persistent = False ) : if persistent : try : os . unlink ( cls . getPath ( ) ) except OSError , e : if e . errno != errno . ENOENT : _getLogger ( ) . exception ( "Error %s while trying to remove dynamic " "configuration file: %s" , e . errno , cls . getPath ( ) ) raise cls . _path = None
12631	def group_dicom_files ( dicom_file_paths , header_fields ) : dist = SimpleDicomFileDistance ( field_weights = header_fields ) path_list = dicom_file_paths . copy ( ) path_groups = DefaultOrderedDict ( DicomFileSet ) while len ( path_list ) > 0 : file_path1 = path_list . pop ( ) file_subgroup = [ file_path1 ] dist . set_dicom_file1 ( file_path1 ) j = len ( path_list ) - 1 while j >= 0 : file_path2 = path_list [ j ] dist . set_dicom_file2 ( file_path2 ) if dist . transform ( ) : file_subgroup . append ( file_path2 ) path_list . pop ( j ) j -= 1 path_groups [ file_path1 ] . from_set ( file_subgroup , check_if_dicoms = False ) return path_groups
9280	def passcode ( callsign ) : assert isinstance ( callsign , str ) callsign = callsign . split ( '-' ) [ 0 ] . upper ( ) code = 0x73e2 for i , char in enumerate ( callsign ) : code ^= ord ( char ) << ( 8 if not i % 2 else 0 ) return code & 0x7fff
101	def draw_text ( img , y , x , text , color = ( 0 , 255 , 0 ) , size = 25 ) : do_assert ( img . dtype in [ np . uint8 , np . float32 ] ) input_dtype = img . dtype if img . dtype == np . float32 : img = img . astype ( np . uint8 ) img = PIL_Image . fromarray ( img ) font = PIL_ImageFont . truetype ( DEFAULT_FONT_FP , size ) context = PIL_ImageDraw . Draw ( img ) context . text ( ( x , y ) , text , fill = tuple ( color ) , font = font ) img_np = np . asarray ( img ) # PIL/asarray returns read only array if not img_np . flags [ "WRITEABLE" ] : try : # this seems to no longer work with np 1.16 (or was pillow updated?) img_np . setflags ( write = True ) except ValueError as ex : if "cannot set WRITEABLE flag to True of this array" in str ( ex ) : img_np = np . copy ( img_np ) if img_np . dtype != input_dtype : img_np = img_np . astype ( input_dtype ) return img_np
8143	def rotate ( self , angle ) : #When a layer rotates, its corners will fall outside #of its defined width and height. #Thus, its bounding box needs to be expanded. #Calculate the diagonal width, and angle from the layer center. #This way we can use the layers's corners #to calculate the bounding box. from math import sqrt , pow , sin , cos , degrees , radians , asin w0 , h0 = self . img . size d = sqrt ( pow ( w0 , 2 ) + pow ( h0 , 2 ) ) d_angle = degrees ( asin ( ( w0 * 0.5 ) / ( d * 0.5 ) ) ) angle = angle % 360 if angle > 90 and angle <= 270 : d_angle += 180 w = sin ( radians ( d_angle + angle ) ) * d w = max ( w , sin ( radians ( d_angle - angle ) ) * d ) w = int ( abs ( w ) ) h = cos ( radians ( d_angle + angle ) ) * d h = max ( h , cos ( radians ( d_angle - angle ) ) * d ) h = int ( abs ( h ) ) dx = int ( ( w - w0 ) / 2 ) dy = int ( ( h - h0 ) / 2 ) d = int ( d ) #The rotation box's background color #is the mean pixel value of the rotating image. #This is the best option to avoid borders around #the rotated image. bg = ImageStat . Stat ( self . img ) . mean bg = ( int ( bg [ 0 ] ) , int ( bg [ 1 ] ) , int ( bg [ 2 ] ) , 0 ) box = Image . new ( "RGBA" , ( d , d ) , bg ) box . paste ( self . img , ( ( d - w0 ) / 2 , ( d - h0 ) / 2 ) ) box = box . rotate ( angle , INTERPOLATION ) box = box . crop ( ( ( d - w ) / 2 + 2 , ( d - h ) / 2 , d - ( d - w ) / 2 , d - ( d - h ) / 2 ) ) self . img = box #Since rotate changes the bounding box size, #update the layers' width, height, and position, #so it rotates from the center. self . x += ( self . w - w ) / 2 self . y += ( self . h - h ) / 2 self . w = w self . h = h
5504	def relative_datetime ( self ) : now = datetime . now ( timezone . utc ) tense = "from now" if self . created_at > now else "ago" return "{0} {1}" . format ( humanize . naturaldelta ( now - self . created_at ) , tense )
10641	def Re ( L : float , v : float , nu : float ) -> float : return v * L / nu
3414	def model_to_dict ( model , sort = False ) : obj = OrderedDict ( ) obj [ "metabolites" ] = list ( map ( metabolite_to_dict , model . metabolites ) ) obj [ "reactions" ] = list ( map ( reaction_to_dict , model . reactions ) ) obj [ "genes" ] = list ( map ( gene_to_dict , model . genes ) ) obj [ "id" ] = model . id _update_optional ( model , obj , _OPTIONAL_MODEL_ATTRIBUTES , _ORDERED_OPTIONAL_MODEL_KEYS ) if sort : get_id = itemgetter ( "id" ) obj [ "metabolites" ] . sort ( key = get_id ) obj [ "reactions" ] . sort ( key = get_id ) obj [ "genes" ] . sort ( key = get_id ) return obj
12133	def extract_log ( log_path , dict_type = dict ) : log_path = ( log_path if os . path . isfile ( log_path ) else os . path . join ( os . getcwd ( ) , log_path ) ) with open ( log_path , 'r' ) as log : splits = ( line . split ( ) for line in log ) uzipped = ( ( int ( split [ 0 ] ) , json . loads ( " " . join ( split [ 1 : ] ) ) ) for split in splits ) szipped = [ ( i , dict ( ( str ( k ) , v ) for ( k , v ) in d . items ( ) ) ) for ( i , d ) in uzipped ] return dict_type ( szipped )
8076	def rectmode ( self , mode = None ) : if mode in ( self . CORNER , self . CENTER , self . CORNERS ) : self . rectmode = mode return self . rectmode elif mode is None : return self . rectmode else : raise ShoebotError ( _ ( "rectmode: invalid input" ) )
12064	def lazygo ( watchFolder = '../abfs/' , reAnalyze = False , rebuildSite = False , keepGoing = True , matching = False ) : abfsKnown = [ ] while True : print ( ) pagesNeeded = [ ] for fname in glob . glob ( watchFolder + "/*.abf" ) : ID = os . path . basename ( fname ) . replace ( ".abf" , "" ) if not fname in abfsKnown : if os . path . exists ( fname . replace ( ".abf" , ".rsv" ) ) : #TODO: or something like this continue if matching and not matching in fname : continue abfsKnown . append ( fname ) if os . path . exists ( os . path . dirname ( fname ) + "/swhlab4/" + os . path . basename ( fname ) . replace ( ".abf" , "_info.pkl" ) ) and reAnalyze == False : print ( "already analyzed" , os . path . basename ( fname ) ) if rebuildSite : pagesNeeded . append ( ID ) else : handleNewABF ( fname ) pagesNeeded . append ( ID ) if len ( pagesNeeded ) : print ( " -- rebuilding index page" ) indexing . genIndex ( os . path . dirname ( fname ) , forceIDs = pagesNeeded ) if not keepGoing : return for i in range ( 50 ) : print ( '.' , end = '' ) time . sleep ( .2 )
1005	def _learnBacktrackFrom ( self , startOffset , readOnly = True ) : # How much input history have we accumulated? # The current input is always at the end of self._prevInfPatterns (at # index -1), but it is also evaluated as a potential starting point by # turning on it's start cells and seeing if it generates sufficient # predictions going forward. numPrevPatterns = len ( self . _prevLrnPatterns ) # This is an easy to use label for the current time step currentTimeStepsOffset = numPrevPatterns - 1 # Clear out any old segment updates. learnPhase2() adds to the segment # updates if we're not readOnly if not readOnly : self . segmentUpdates = { } # Status message if self . verbosity >= 3 : if readOnly : print ( "Trying to lock-on using startCell state from %d steps ago:" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) else : print ( "Locking on using startCell state from %d steps ago:" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) # Play through up to the current time step inSequence = True for offset in range ( startOffset , numPrevPatterns ) : # Copy predicted and active states into t-1 self . lrnPredictedState [ 't-1' ] [ : , : ] = self . lrnPredictedState [ 't' ] [ : , : ] self . lrnActiveState [ 't-1' ] [ : , : ] = self . lrnActiveState [ 't' ] [ : , : ] # Get the input pattern inputColumns = self . _prevLrnPatterns [ offset ] # Apply segment updates from the last set of predictions if not readOnly : self . _processSegmentUpdates ( inputColumns ) # Phase 1: # Compute activeState[t] given bottom-up and predictedState[t-1] if offset == startOffset : self . lrnActiveState [ 't' ] . fill ( 0 ) for c in inputColumns : self . lrnActiveState [ 't' ] [ c , 0 ] = 1 inSequence = True else : # Uses lrnActiveState['t-1'] and lrnPredictedState['t-1'] # computes lrnActiveState['t'] inSequence = self . _learnPhase1 ( inputColumns , readOnly = readOnly ) # Break out immediately if we fell out of sequence or reached the current # time step if not inSequence or offset == currentTimeStepsOffset : break # Phase 2: # Computes predictedState['t'] given activeState['t'] and also queues # up active segments into self.segmentUpdates, unless this is readOnly if self . verbosity >= 3 : print " backtrack: computing predictions from " , inputColumns self . _learnPhase2 ( readOnly = readOnly ) # Return whether or not this starting point was valid return inSequence
7909	def __presence_available ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_available_presence ( MucPresence ( stanza ) ) return True
1311	def KeyboardInput ( wVk : int , wScan : int , dwFlags : int = KeyboardEventFlag . KeyDown , time_ : int = 0 ) -> INPUT : return _CreateInput ( KEYBDINPUT ( wVk , wScan , dwFlags , time_ , None ) )
11198	def tzname_in_python2 ( namefunc ) : def adjust_encoding ( * args , * * kwargs ) : name = namefunc ( * args , * * kwargs ) if name is not None and not PY3 : name = name . encode ( ) return name return adjust_encoding
11453	def convert_all ( cls , records ) : out = [ "<collection>" ] for rec in records : conversion = cls ( rec ) out . append ( conversion . convert ( ) ) out . append ( "</collection>" ) return "\n" . join ( out )
12038	def matrixToDicts ( data ) : # 1D array if "float" in str ( type ( data [ 0 ] ) ) : d = { } for x in range ( len ( data ) ) : d [ data . dtype . names [ x ] ] = data [ x ] return d # 2D array l = [ ] for y in range ( len ( data ) ) : d = { } for x in range ( len ( data [ y ] ) ) : d [ data . dtype . names [ x ] ] = data [ y ] [ x ] l . append ( d ) return l
9383	def parse ( self ) : file_status = True for infile in self . infile_list : file_status = file_status and naarad . utils . is_valid_file ( infile ) if not file_status : return False status = self . parse_xml_jtl ( self . aggregation_granularity ) gc . collect ( ) return status
8741	def create_floatingip ( context , content ) : LOG . info ( 'create_floatingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) network_id = content . get ( 'floating_network_id' ) # TODO(blogan): Since the extension logic will reject any requests without # floating_network_id, is this still needed? if not network_id : raise n_exc . BadRequest ( resource = 'floating_ip' , msg = 'floating_network_id is required.' ) fixed_ip_address = content . get ( 'fixed_ip_address' ) ip_address = content . get ( 'floating_ip_address' ) port_id = content . get ( 'port_id' ) port = None port_fixed_ip = { } network = _get_network ( context , network_id ) if port_id : port = _get_port ( context , port_id ) fixed_ip = _get_fixed_ip ( context , fixed_ip_address , port ) port_fixed_ip = { port . id : { 'port' : port , 'fixed_ip' : fixed_ip } } flip = _allocate_ip ( context , network , port , ip_address , ip_types . FLOATING ) _create_flip ( context , flip , port_fixed_ip ) return v . _make_floating_ip_dict ( flip , port_id )
319	def calc_distribution_stats ( x ) : return pd . Series ( { 'mean' : np . mean ( x ) , 'median' : np . median ( x ) , 'std' : np . std ( x ) , '5%' : np . percentile ( x , 5 ) , '25%' : np . percentile ( x , 25 ) , '75%' : np . percentile ( x , 75 ) , '95%' : np . percentile ( x , 95 ) , 'IQR' : np . subtract . reduce ( np . percentile ( x , [ 75 , 25 ] ) ) , } )
12241	def camel ( theta ) : x , y = theta obj = 2 * x ** 2 - 1.05 * x ** 4 + x ** 6 / 6 + x * y + y ** 2 grad = np . array ( [ 4 * x - 4.2 * x ** 3 + x ** 5 + y , x + 2 * y ] ) return obj , grad
1058	def remove_extension ( module , name , code ) : key = ( module , name ) if ( _extension_registry . get ( key ) != code or _inverted_registry . get ( code ) != key ) : raise ValueError ( "key %s is not registered with code %s" % ( key , code ) ) del _extension_registry [ key ] del _inverted_registry [ code ] if code in _extension_cache : del _extension_cache [ code ]
7560	def get_total ( tots , node ) : if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : ## Get counts on down edges. ## How to treat polytomies here? if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = sum ( 1 for i in down_r . iter_leaves ( ) ) lendl = sum ( 1 for i in down_l . iter_leaves ( ) ) ## get count on up edge sister up_r = node . get_sisters ( ) [ 0 ] lenur = sum ( 1 for i in up_r . iter_leaves ( ) ) ## everyone else lenul = tots - ( lendr + lendl + lenur ) ## return product return lendr * lendl * lenur * lenul
10396	def unscored_nodes_iter ( self ) -> BaseEntity : for node , data in self . graph . nodes ( data = True ) : if self . tag not in data : yield node
12101	def _record_info ( self , setup_info = None ) : info_path = os . path . join ( self . root_directory , ( '%s.info' % self . batch_name ) ) if setup_info is None : try : with open ( info_path , 'r' ) as info_file : setup_info = json . load ( info_file ) except : setup_info = { } setup_info . update ( { 'end_time' : tuple ( time . localtime ( ) ) } ) else : setup_info . update ( { 'end_time' : None , 'metadata' : self . metadata } ) with open ( info_path , 'w' ) as info_file : json . dump ( setup_info , info_file , sort_keys = True , indent = 4 )
7840	def get_name ( self ) : var = self . xmlnode . prop ( "name" ) if not var : var = "" return var . decode ( "utf-8" )
9983	def replace_funcname ( source : str , name : str ) : lines = source . splitlines ( ) atok = asttokens . ASTTokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . FunctionDef ) : break i = node . first_token . index for i in range ( node . first_token . index , node . last_token . index ) : if ( atok . tokens [ i ] . type == token . NAME and atok . tokens [ i ] . string == "def" ) : break lineno , col_begin = atok . tokens [ i + 1 ] . start lineno_end , col_end = atok . tokens [ i + 1 ] . end assert lineno == lineno_end lines [ lineno - 1 ] = ( lines [ lineno - 1 ] [ : col_begin ] + name + lines [ lineno - 1 ] [ col_end : ] ) return "\n" . join ( lines ) + "\n"
5233	def all_folders ( path_name , keyword = '' , has_date = False , date_fmt = DATE_FMT ) -> list : if not os . path . exists ( path = path_name ) : return [ ] path_name = path_name . replace ( '\\' , '/' ) if keyword : folders = sort_by_modified ( [ f . replace ( '\\' , '/' ) for f in glob . iglob ( f'{path_name}/*{keyword}*' ) if os . path . isdir ( f ) and ( f . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] [ 0 ] != '~' ) ] ) else : folders = sort_by_modified ( [ f'{path_name}/{f}' for f in os . listdir ( path = path_name ) if os . path . isdir ( f'{path_name}/{f}' ) and ( f [ 0 ] != '~' ) ] ) if has_date : folders = filter_by_dates ( folders , date_fmt = date_fmt ) return folders
8354	def start_meta ( self , attrs ) : httpEquiv = None contentType = None contentTypeIndex = None tagNeedsEncodingSubstitution = False for i in range ( 0 , len ( attrs ) ) : key , value = attrs [ i ] key = key . lower ( ) if key == 'http-equiv' : httpEquiv = value elif key == 'content' : contentType = value contentTypeIndex = i if httpEquiv and contentType : # It's an interesting meta tag. match = self . CHARSET_RE . search ( contentType ) if match : if ( self . declaredHTMLEncoding is not None or self . originalEncoding == self . fromEncoding ) : # An HTML encoding was sniffed while converting # the document to Unicode, or an HTML encoding was # sniffed during a previous pass through the # document, or an encoding was specified # explicitly and it worked. Rewrite the meta tag. def rewrite ( match ) : return match . group ( 1 ) + "%SOUP-ENCODING%" newAttr = self . CHARSET_RE . sub ( rewrite , contentType ) attrs [ contentTypeIndex ] = ( attrs [ contentTypeIndex ] [ 0 ] , newAttr ) tagNeedsEncodingSubstitution = True else : # This is our first pass through the document. # Go through it again with the encoding information. newCharset = match . group ( 3 ) if newCharset and newCharset != self . originalEncoding : self . declaredHTMLEncoding = newCharset self . _feed ( self . declaredHTMLEncoding ) raise StopParsing pass tag = self . unknown_starttag ( "meta" , attrs ) if tag and tagNeedsEncodingSubstitution : tag . containsSubstitutions = True
10883	def patch_docs ( subclass , superclass ) : funcs0 = inspect . getmembers ( subclass , predicate = inspect . ismethod ) funcs1 = inspect . getmembers ( superclass , predicate = inspect . ismethod ) funcs1 = [ f [ 0 ] for f in funcs1 ] for name , func in funcs0 : if name . startswith ( '_' ) : continue if name not in funcs1 : continue if func . __doc__ is None : func = getattr ( subclass , name ) func . __func__ . __doc__ = getattr ( superclass , name ) . __func__ . __doc__
13615	def write ( ) : click . echo ( "Fantastic. Let's get started. " ) title = click . prompt ( "What's the title?" ) # Make sure that title doesn't exist. url = slugify ( title ) url = click . prompt ( "What's the URL?" , default = url ) # Make sure that title doesn't exist. click . echo ( "Got it. Creating %s..." % url ) scaffold_piece ( title , url )
3399	def add_switches_and_objective ( self ) : constraints = list ( ) big_m = max ( max ( abs ( b ) for b in r . bounds ) for r in self . model . reactions ) prob = self . model . problem for rxn in self . model . reactions : if not hasattr ( rxn , 'gapfilling_type' ) : continue indicator = prob . Variable ( name = 'indicator_{}' . format ( rxn . id ) , lb = 0 , ub = 1 , type = 'binary' ) if rxn . id in self . penalties : indicator . cost = self . penalties [ rxn . id ] else : indicator . cost = self . penalties [ rxn . gapfilling_type ] indicator . rxn_id = rxn . id self . indicators . append ( indicator ) # if z = 1 v_i is allowed non-zero # v_i - Mz <= 0 and v_i + Mz >= 0 constraint_lb = prob . Constraint ( rxn . flux_expression - big_m * indicator , ub = 0 , name = 'constraint_lb_{}' . format ( rxn . id ) , sloppy = True ) constraint_ub = prob . Constraint ( rxn . flux_expression + big_m * indicator , lb = 0 , name = 'constraint_ub_{}' . format ( rxn . id ) , sloppy = True ) constraints . extend ( [ constraint_lb , constraint_ub ] ) self . model . add_cons_vars ( self . indicators ) self . model . add_cons_vars ( constraints , sloppy = True ) self . model . objective = prob . Objective ( Zero , direction = 'min' , sloppy = True ) self . model . objective . set_linear_coefficients ( { i : 1 for i in self . indicators } ) self . update_costs ( )
907	def replaceIterationCycle ( self , phaseSpecs ) : # ----------------------------------------------------------------------- # Replace our phase manager # self . __phaseManager = _PhaseManager ( model = self . __model , phaseSpecs = phaseSpecs ) return
8306	def close ( self ) : self . process . stdout . close ( ) self . process . stderr . close ( ) self . running = False
13708	def check_ip ( self , ip ) : self . _last_result = None if is_valid_ipv4 ( ip ) : key = None if self . _use_cache : key = self . _make_cache_key ( ip ) self . _last_result = self . _cache . get ( key , version = self . _cache_version ) if self . _last_result is None : # request httpBL API error , age , threat , type = self . _request_httpbl ( ip ) if error == 127 or error == 0 : self . _last_result = { 'error' : error , 'age' : age , 'threat' : threat , 'type' : type } if self . _use_cache : self . _cache . set ( key , self . _last_result , timeout = self . _api_timeout , version = self . _cache_version ) if self . _last_result is not None and settings . CACHED_HTTPBL_USE_LOGGING : logger . info ( 'httpBL check ip: {0}; ' 'httpBL result: error: {1}, age: {2}, threat: {3}, type: {4}' . format ( ip , self . _last_result [ 'error' ] , self . _last_result [ 'age' ] , self . _last_result [ 'threat' ] , self . _last_result [ 'type' ] ) ) return self . _last_result
13409	def show ( self ) : self . parent . addLayout ( self . _logSelectLayout ) self . menuCount += 1 self . _connectSlots ( )
1610	def make_root_tuple_info ( stream_id , tuple_id ) : key = random . getrandbits ( TupleHelper . MAX_SFIXED64_RAND_BITS ) return RootTupleInfo ( stream_id = stream_id , tuple_id = tuple_id , insertion_time = time . time ( ) , key = key )
9285	def close ( self ) : self . _connected = False self . buf = b'' if self . sock is not None : self . sock . close ( )
5940	def _build_arg_list ( self , * * kwargs ) : arglist = [ ] for flag , value in kwargs . items ( ) : # XXX: check flag against allowed values flag = str ( flag ) if flag . startswith ( '_' ) : flag = flag [ 1 : ] # python-illegal keywords are '_'-quoted if not flag . startswith ( '-' ) : flag = '-' + flag # now flag is guaranteed to start with '-' if value is True : arglist . append ( flag ) # simple command line flag elif value is False : if flag . startswith ( '-no' ) : # negate a negated flag ('noX=False' --> X=True --> -X ... but who uses that?) arglist . append ( '-' + flag [ 3 : ] ) else : arglist . append ( '-no' + flag [ 1 : ] ) # gromacs switches booleans by prefixing 'no' elif value is None : pass # ignore flag = None else : try : arglist . extend ( [ flag ] + value ) # option with value list except TypeError : arglist . extend ( [ flag , value ] ) # option with single value return list ( map ( str , arglist ) )
13212	def build_jsonld ( self , url = None , code_url = None , ci_url = None , readme_url = None , license_id = None ) : jsonld = { '@context' : [ "https://raw.githubusercontent.com/codemeta/codemeta/2.0-rc/" "codemeta.jsonld" , "http://schema.org" ] , '@type' : [ 'Report' , 'SoftwareSourceCode' ] , 'language' : 'TeX' , 'reportNumber' : self . handle , 'name' : self . plain_title , 'description' : self . plain_abstract , 'author' : [ { '@type' : 'Person' , 'name' : author_name } for author_name in self . plain_authors ] , # This is a datetime.datetime; not a string. If writing to a file, # Need to convert this to a ISO 8601 string. 'dateModified' : self . revision_datetime } try : jsonld [ 'articleBody' ] = self . plain_content jsonld [ 'fileFormat' ] = 'text/plain' # MIME type of articleBody except RuntimeError : # raised by pypandoc when it can't convert the tex document self . _logger . exception ( 'Could not convert latex body to plain ' 'text for articleBody.' ) self . _logger . warning ( 'Falling back to tex source for articleBody' ) jsonld [ 'articleBody' ] = self . _tex jsonld [ 'fileFormat' ] = 'text/plain' # no mimetype for LaTeX? if url is not None : jsonld [ '@id' ] = url jsonld [ 'url' ] = url else : # Fallback to using the document handle as the ID. This isn't # entirely ideal from a linked data perspective. jsonld [ '@id' ] = self . handle if code_url is not None : jsonld [ 'codeRepository' ] = code_url if ci_url is not None : jsonld [ 'contIntegration' ] = ci_url if readme_url is not None : jsonld [ 'readme' ] = readme_url if license_id is not None : jsonld [ 'license_id' ] = None return jsonld
1951	def deprecated ( message : str ) : assert isinstance ( message , str ) , "The deprecated decorator requires a message string argument." def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : warnings . warn ( f"`{func.__qualname__}` is deprecated. {message}" , category = ManticoreDeprecationWarning , stacklevel = 2 ) return func ( * args , * * kwargs ) return wrapper return decorator
12499	def fwhm2sigma ( fwhm ) : fwhm = np . asarray ( fwhm ) return fwhm / np . sqrt ( 8 * np . log ( 2 ) )
12935	def _parse_allele_data ( self ) : # Get allele frequencies if they exist. pref_freq , frequencies = self . _parse_frequencies ( ) info_clnvar_single_tags = [ 'ALLELEID' , 'CLNSIG' , 'CLNHGVS' ] cln_data = { x . lower ( ) : self . info [ x ] if x in self . info else None for x in info_clnvar_single_tags } cln_data . update ( { 'clndisdb' : [ x . split ( ',' ) for x in self . info [ 'CLNDISDB' ] . split ( '|' ) ] if 'CLNDISDB' in self . info else [ ] } ) cln_data . update ( { 'clndn' : self . info [ 'CLNDN' ] . split ( '|' ) if 'CLNDN' in self . info else [ ] } ) cln_data . update ( { 'clnvi' : self . info [ 'CLNVI' ] . split ( ',' ) if 'CLNVI' in self . info else [ ] } ) try : sequence = self . alt_alleles [ 0 ] except IndexError : sequence = self . ref_allele allele = ClinVarAllele ( frequency = pref_freq , sequence = sequence , * * cln_data ) # A few ClinVar variants are only reported as a combination with # other variants, and no single-variant effect is proposed. Skip these. if not cln_data [ 'clnsig' ] : return [ ] return [ allele ]
11319	def update_dois ( self ) : dois = record_get_field_instances ( self . record , '024' , ind1 = "7" ) all_dois = { } for field in dois : subs = field_get_subfield_instances ( field ) subs_dict = dict ( subs ) if subs_dict . get ( 'a' ) : if subs_dict [ 'a' ] in all_dois : record_delete_field ( self . record , tag = '024' , ind1 = '7' , field_position_global = field [ 4 ] ) continue all_dois [ subs_dict [ 'a' ] ] = field
11668	def quadratic ( Ks , dim , rhos , required = None ) : # Estimated with alpha=1, beta=0: # B_{k,d,1,0} is the same as B_{k,d,0,1} in linear() # and the full estimator is # B / (n - 1) * mean(rho ^ -dim) N = rhos . shape [ 0 ] Ks = np . asarray ( Ks ) Bs = ( Ks - 1 ) / np . pi ** ( dim / 2 ) * gamma ( dim / 2 + 1 ) # shape (num_Ks,) est = Bs / ( N - 1 ) * np . mean ( rhos ** ( - dim ) , axis = 0 ) return est
13834	def PrintMessage ( self , message ) : fields = message . ListFields ( ) if self . use_index_order : fields . sort ( key = lambda x : x [ 0 ] . index ) for field , value in fields : if _IsMapEntry ( field ) : for key in sorted ( value ) : # This is slow for maps with submessage entires because it copies the # entire tree. Unfortunately this would take significant refactoring # of this file to work around. # # TODO(haberman): refactor and optimize if this becomes an issue. entry_submsg = field . message_type . _concrete_class ( key = key , value = value [ key ] ) self . PrintField ( field , entry_submsg ) elif field . label == descriptor . FieldDescriptor . LABEL_REPEATED : for element in value : self . PrintField ( field , element ) else : self . PrintField ( field , value )
650	def generateSequences ( nPatterns = 10 , patternLen = 500 , patternActivity = 50 , hubs = [ 2 , 6 ] , seqLength = [ 5 , 6 , 7 ] , nSimpleSequences = 50 , nHubSequences = 50 ) : # Create the input patterns patterns = generateCoincMatrix ( nCoinc = nPatterns , length = patternLen , activity = patternActivity ) # Create the raw sequences seqList = generateSimpleSequences ( nCoinc = nPatterns , seqLength = seqLength , nSeq = nSimpleSequences ) + generateHubSequences ( nCoinc = nPatterns , hubs = hubs , seqLength = seqLength , nSeq = nHubSequences ) # Return results return ( seqList , patterns )
2514	def p_file_depends ( self , f_term , predicate ) : for _ , _ , other_file in self . graph . triples ( ( f_term , predicate , None ) ) : name = self . get_file_name ( other_file ) if name is not None : self . builder . add_file_dep ( six . text_type ( name ) ) else : self . error = True msg = 'File depends on file with no name' self . logger . log ( msg )
3412	def _fix_type ( value ) : # Because numpy floats can not be pickled to json if isinstance ( value , string_types ) : return str ( value ) if isinstance ( value , float_ ) : return float ( value ) if isinstance ( value , bool_ ) : return bool ( value ) if isinstance ( value , set ) : return list ( value ) if isinstance ( value , dict ) : return OrderedDict ( ( key , value [ key ] ) for key in sorted ( value ) ) # handle legacy Formula type if value . __class__ . __name__ == "Formula" : return str ( value ) if value is None : return "" return value
10940	def calc_J ( self ) : del self . J self . J = np . zeros ( [ self . param_vals . size , self . data . size ] ) dp = np . zeros_like ( self . param_vals ) f0 = self . model . copy ( ) for a in range ( self . param_vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param_vals + dp , * self . func_args , * * self . func_kwargs ) grad_func = ( f1 - f0 ) / dp [ a ] #J = grad(residuals) = -grad(model) self . J [ a ] = - grad_func
466	def generate_skip_gram_batch ( data , batch_size , num_skips , skip_window , data_index = 0 ) : # global data_index # you can put data_index outside the function, then # modify the global data_index in the function without return it. # note: without using yield, this code use data_index to instead. if batch_size % num_skips != 0 : raise Exception ( "batch_size should be able to be divided by num_skips." ) if num_skips > 2 * skip_window : raise Exception ( "num_skips <= 2 * skip_window" ) batch = np . ndarray ( shape = ( batch_size ) , dtype = np . int32 ) labels = np . ndarray ( shape = ( batch_size , 1 ) , dtype = np . int32 ) span = 2 * skip_window + 1 # [ skip_window target skip_window ] buffer = collections . deque ( maxlen = span ) for _ in range ( span ) : buffer . append ( data [ data_index ] ) data_index = ( data_index + 1 ) % len ( data ) for i in range ( batch_size // num_skips ) : target = skip_window # target label at the center of the buffer targets_to_avoid = [ skip_window ] for j in range ( num_skips ) : while target in targets_to_avoid : target = random . randint ( 0 , span - 1 ) targets_to_avoid . append ( target ) batch [ i * num_skips + j ] = buffer [ skip_window ] labels [ i * num_skips + j , 0 ] = buffer [ target ] buffer . append ( data [ data_index ] ) data_index = ( data_index + 1 ) % len ( data ) return batch , labels , data_index
393	def discount_episode_rewards ( rewards = None , gamma = 0.99 , mode = 0 ) : if rewards is None : raise Exception ( "rewards should be a list" ) discounted_r = np . zeros_like ( rewards , dtype = np . float32 ) running_add = 0 for t in reversed ( xrange ( 0 , rewards . size ) ) : if mode == 0 : if rewards [ t ] != 0 : running_add = 0 running_add = running_add * gamma + rewards [ t ] discounted_r [ t ] = running_add return discounted_r
12577	def _mask_data ( self , data ) : self . _check_for_mask ( ) msk_data = self . mask . get_data ( ) if self . ndim == 3 : return data [ msk_data ] , np . where ( msk_data ) elif self . ndim == 4 : return _apply_mask_to_4d_data ( data , self . mask ) else : raise ValueError ( 'Cannot mask {} with {} dimensions using mask {}.' . format ( self , self . ndim , self . mask ) )
598	def finishLearning ( self ) : if self . _tfdr is None : raise RuntimeError ( "Temporal memory has not been initialized" ) if hasattr ( self . _tfdr , 'finishLearning' ) : self . resetSequenceStates ( ) self . _tfdr . finishLearning ( )
6739	def check_settings_for_differences ( old , new , as_bool = False , as_tri = False ) : assert not as_bool or not as_tri old = old or { } new = new or { } changes = set ( k for k in set ( new . iterkeys ( ) ) . intersection ( old . iterkeys ( ) ) if new [ k ] != old [ k ] ) if changes and as_bool : return True added_keys = set ( new . iterkeys ( ) ) . difference ( old . iterkeys ( ) ) if added_keys and as_bool : return True if not as_tri : changes . update ( added_keys ) deled_keys = set ( old . iterkeys ( ) ) . difference ( new . iterkeys ( ) ) if deled_keys and as_bool : return True if as_bool : return False if not as_tri : changes . update ( deled_keys ) if as_tri : return added_keys , changes , deled_keys return changes
7368	async def read ( response , loads = loads , encoding = None ) : ctype = response . headers . get ( 'Content-Type' , "" ) . lower ( ) try : if "application/json" in ctype : logger . info ( "decoding data as json" ) return await response . json ( encoding = encoding , loads = loads ) if "text" in ctype : logger . info ( "decoding data as text" ) return await response . text ( encoding = encoding ) except ( UnicodeDecodeError , json . JSONDecodeError ) as exc : data = await response . read ( ) raise exceptions . PeonyDecodeError ( response = response , data = data , exception = exc ) return await response . read ( )
10109	def schema ( tg ) : tables = { } for tname , table in tg . tabledict . items ( ) : t = TableSpec . from_table_metadata ( table ) tables [ t . name ] = t for at in t . many_to_many . values ( ) : tables [ at . name ] = at # We must determine the order in which tables must be created! ordered = OrderedDict ( ) i = 0 # We loop through the tables repeatedly, and whenever we find one, which has all # referenced tables already in ordered, we move it from tables to ordered. while tables and i < 100 : i += 1 for table in list ( tables . keys ( ) ) : if all ( ( ref [ 1 ] in ordered ) or ref [ 1 ] == table for ref in tables [ table ] . foreign_keys ) : # All referenced tables are already created (or self-referential). ordered [ table ] = tables . pop ( table ) break if tables : # pragma: no cover raise ValueError ( 'there seem to be cyclic dependencies between the tables' ) return list ( ordered . values ( ) )
1558	def _get_stream_schema ( fields ) : stream_schema = topology_pb2 . StreamSchema ( ) for field in fields : key = stream_schema . keys . add ( ) key . key = field key . type = topology_pb2 . Type . Value ( "OBJECT" ) return stream_schema
3207	def delete ( self , batch_id ) : self . batch_id = batch_id self . operation_status = None return self . _mc_client . _delete ( url = self . _build_path ( batch_id ) )
269	def detect_intraday ( positions , transactions , threshold = 0.25 ) : daily_txn = transactions . copy ( ) daily_txn . index = daily_txn . index . date txn_count = daily_txn . groupby ( level = 0 ) . symbol . nunique ( ) . sum ( ) daily_pos = positions . drop ( 'cash' , axis = 1 ) . replace ( 0 , np . nan ) return daily_pos . count ( axis = 1 ) . sum ( ) / txn_count < threshold
13016	def hook ( name ) : def hookTarget ( wrapped ) : if not hasattr ( wrapped , '__hook__' ) : wrapped . __hook__ = [ name ] else : wrapped . __hook__ . append ( name ) return wrapped return hookTarget
5159	def _add_tc_script ( self ) : # fill context context = dict ( tc_options = self . config . get ( 'tc_options' , [ ] ) ) # import pdb; pdb.set_trace() contents = self . _render_template ( 'tc_script.sh' , context ) self . config . setdefault ( 'files' , [ ] ) # file list might be empty # add tc_script.sh to list of included files self . _add_unique_file ( { "path" : "/tc_script.sh" , "contents" : contents , "mode" : "755" } )
5978	def mask_blurring_from_mask_and_psf_shape ( mask , psf_shape ) : blurring_mask = np . full ( mask . shape , True ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : for y1 in range ( ( - psf_shape [ 0 ] + 1 ) // 2 , ( psf_shape [ 0 ] + 1 ) // 2 ) : for x1 in range ( ( - psf_shape [ 1 ] + 1 ) // 2 , ( psf_shape [ 1 ] + 1 ) // 2 ) : if 0 <= x + x1 <= mask . shape [ 1 ] - 1 and 0 <= y + y1 <= mask . shape [ 0 ] - 1 : if mask [ y + y1 , x + x1 ] : blurring_mask [ y + y1 , x + x1 ] = False else : raise exc . MaskException ( "setup_blurring_mask extends beyond the sub_grid_size of the masks - pad the " "datas array before masking" ) return blurring_mask
535	def writeToProto ( self , proto ) : proto . implementation = self . implementation proto . steps = self . steps proto . alpha = self . alpha proto . verbosity = self . verbosity proto . maxCategoryCount = self . maxCategoryCount proto . learningMode = self . learningMode proto . inferenceMode = self . inferenceMode proto . recordNum = self . recordNum self . _sdrClassifier . write ( proto . sdrClassifier )
189	def copy ( self , line_strings = None , shape = None ) : lss = self . line_strings if line_strings is None else line_strings shape = self . shape if shape is None else shape return LineStringsOnImage ( line_strings = lss , shape = shape )
9457	def sound_touch ( self , call_params ) : path = '/' + self . api_version + '/SoundTouch/' method = 'POST' return self . request ( path , method , call_params )
9527	def to_boulderio ( infile , outfile ) : seq_reader = sequences . file_reader ( infile ) f_out = utils . open_file_write ( outfile ) for sequence in seq_reader : print ( "SEQUENCE_ID=" + sequence . id , file = f_out ) print ( "SEQUENCE_TEMPLATE=" + sequence . seq , file = f_out ) print ( "=" , file = f_out ) utils . close ( f_out )
3880	async def _handle_watermark_notification ( self , watermark_notification ) : conv_id = watermark_notification . conversation_id . id res = parsers . parse_watermark_notification ( watermark_notification ) await self . on_watermark_notification . fire ( res ) try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for watermark notification: %s' , conv_id ) else : await conv . on_watermark_notification . fire ( res )
7928	def stop ( self ) : with self . lock : for dummy in self . threads : self . queue . put ( None )
9485	def validate_litezip ( struct ) : msgs = [ ] def _fmt_err ( err ) : return ( Path ( err . filename ) , "{}:{} -- {}: {}" . format ( * ( err [ 1 : ] ) ) ) obj_by_type = { } for obj in struct : if not is_valid_identifier ( obj . id ) : msg = ( obj . file . parent , "{} is not a valid identifier" . format ( obj . id ) , ) logger . info ( "{}: {}" . format ( * msg ) ) msgs . append ( msg ) obj_by_type . setdefault ( type ( obj ) , [ ] ) . append ( obj ) for obtype in obj_by_type : content_msgs = list ( [ _fmt_err ( err ) for err in validate_content ( * obj_by_type [ obtype ] ) ] ) for msg in content_msgs : logger . info ( "{}: {}" . format ( * msg ) ) msgs . extend ( content_msgs ) return msgs
9944	def link_file ( self , path , prefixed_path , source_storage ) : # Skip this file if it was already copied earlier if prefixed_path in self . symlinked_files : return self . log ( "Skipping '%s' (already linked earlier)" % path ) # Delete the target file if needed or break if not self . delete_file ( path , prefixed_path , source_storage ) : return # The full path of the source file source_path = source_storage . path ( path ) # Finally link the file if self . dry_run : self . log ( "Pretending to link '%s'" % source_path , level = 1 ) else : self . log ( "Linking '%s'" % source_path , level = 1 ) full_path = self . storage . path ( prefixed_path ) try : os . makedirs ( os . path . dirname ( full_path ) ) except OSError : pass try : if os . path . lexists ( full_path ) : os . unlink ( full_path ) os . symlink ( source_path , full_path ) except AttributeError : import platform raise CommandError ( "Symlinking is not supported by Python %s." % platform . python_version ( ) ) except NotImplementedError : import platform raise CommandError ( "Symlinking is not supported in this " "platform (%s)." % platform . platform ( ) ) except OSError as e : raise CommandError ( e ) if prefixed_path not in self . symlinked_files : self . symlinked_files . append ( prefixed_path )
12362	def list ( self , url_components = ( ) ) : resp = self . get ( url_components ) return resp . get ( self . result_key , [ ] )
13401	def removeLogbook ( self , menu = None ) : if self . logMenuCount > 1 and menu is not None : menu . removeMenu ( ) self . logMenus . remove ( menu ) self . logMenuCount -= 1
1572	def setup ( self , context ) : myindex = context . get_partition_index ( ) self . _files_to_consume = self . _files [ myindex : : context . get_num_partitions ( ) ] self . logger . info ( "TextFileSpout files to consume %s" % self . _files_to_consume ) self . _lines_to_consume = self . _get_next_lines ( ) self . _emit_count = 0
211	def to_uint8 ( self ) : # TODO this always returns (H,W,C), even if input ndarray was originall (H,W) # does it make sense here to also return (H,W) if self.arr_was_2d? arr_0to255 = np . clip ( np . round ( self . arr_0to1 * 255 ) , 0 , 255 ) arr_uint8 = arr_0to255 . astype ( np . uint8 ) return arr_uint8
1411	def filter_spouts ( table , header ) : spouts_info = [ ] for row in table : if row [ 0 ] == 'spout' : spouts_info . append ( row ) return spouts_info , header
940	def reapVarArgsCallback ( option , optStr , value , parser ) : newValues = [ ] # Reap the args, taking care to stop before the next option or '.' gotDot = False for arg in parser . rargs : # Stop on --longname options if arg . startswith ( "--" ) and len ( arg ) > 2 : break # Stop on -b options if arg . startswith ( "-" ) and len ( arg ) > 1 : break if arg == "." : gotDot = True break newValues . append ( arg ) if not newValues : raise optparse . OptionValueError ( ( "Empty arg list for option %r expecting one or more args " "(remaining tokens: %r)" ) % ( optStr , parser . rargs ) ) del parser . rargs [ : len ( newValues ) + int ( gotDot ) ] # Retrieve the existing arg accumulator, if any value = getattr ( parser . values , option . dest , [ ] ) #print "Previous value: %r" % value if value is None : value = [ ] # Append the new args to the existing ones and save to the parser value . extend ( newValues ) setattr ( parser . values , option . dest , value )
8290	def _description ( self ) : meta = self . find ( "meta" , { "name" : "description" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : return meta [ "content" ] else : return u""
3422	def get_context ( obj ) : try : return obj . _contexts [ - 1 ] except ( AttributeError , IndexError ) : pass try : return obj . _model . _contexts [ - 1 ] except ( AttributeError , IndexError ) : pass return None
13424	def get_message ( self , message_id ) : url = "/2/messages/%s" % message_id return self . message_from_json ( self . _get_resource ( url ) [ "message" ] )
10783	def check_add_particles ( st , guess , rad = 'calc' , do_opt = True , im_change_frac = 0.2 , min_derr = '3sig' , * * kwargs ) : # FIXME does not use the **kwargs, but needs b/c called with wrong kwargs if min_derr == '3sig' : min_derr = 3 * st . sigma accepts = 0 new_poses = [ ] if rad == 'calc' : rad = guess_add_radii ( st ) message = ( '-' * 30 + 'ADDING' + '-' * 30 + '\n Z\t Y\t X\t R\t|\t ERR0\t\t ERR1' ) with log . noformat ( ) : CLOG . info ( message ) for a in range ( guess . shape [ 0 ] ) : p0 = guess [ a ] absent_err = st . error absent_d = st . residuals . copy ( ) ind = st . obj_add_particle ( p0 , rad ) if do_opt : # the slowest part of this opt . do_levmarq_particles ( st , ind , damping = 1.0 , max_iter = 1 , run_length = 3 , eig_update = False , include_rad = False ) present_err = st . error present_d = st . residuals . copy ( ) dont_kill = should_particle_exist ( absent_err , present_err , absent_d , present_d , im_change_frac = im_change_frac , min_derr = min_derr ) if dont_kill : accepts += 1 p = tuple ( st . obj_get_positions ( ) [ ind ] . ravel ( ) ) r = tuple ( st . obj_get_radii ( ) [ ind ] . ravel ( ) ) new_poses . append ( p ) part_msg = '%2.2f\t%3.2f\t%3.2f\t%3.2f\t|\t%4.3f \t%4.3f' % ( p + r + ( absent_err , st . error ) ) with log . noformat ( ) : CLOG . info ( part_msg ) else : st . obj_remove_particle ( ind ) if np . abs ( absent_err - st . error ) > 1e-4 : raise RuntimeError ( 'updates not exact?' ) return accepts , new_poses
5241	def market_normal ( self , session , after_open , before_close ) -> Session : logger = logs . get_logger ( self . market_normal ) if session not in self . exch : return SessNA ss = self . exch [ session ] s_time = shift_time ( ss [ 0 ] , int ( after_open ) + 1 ) e_time = shift_time ( ss [ - 1 ] , - int ( before_close ) ) request_cross = pd . Timestamp ( s_time ) >= pd . Timestamp ( e_time ) session_cross = pd . Timestamp ( ss [ 0 ] ) >= pd . Timestamp ( ss [ 1 ] ) if request_cross and ( not session_cross ) : logger . warning ( f'end time {e_time} is earlier than {s_time} ...' ) return SessNA return Session ( s_time , e_time )
7709	def handle_authorized_event ( self , event ) : self . server = event . authorized_jid . bare ( ) if "versioning" in self . server_features : if self . roster is not None and self . roster . version is not None : version = self . roster . version else : version = u"" else : version = None self . request_roster ( version )
10702	def get_modes ( _id ) : url = MODES_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
6297	def release ( self , buffer = True ) : for key , vao in self . vaos : vao . release ( ) if buffer : for buff in self . buffers : buff . buffer . release ( ) if self . _index_buffer : self . _index_buffer . release ( )
4073	def eval_environ ( value ) : def eval_environ_str ( value ) : parts = value . split ( ';' ) if len ( parts ) < 2 : return value expr = parts [ 1 ] . lstrip ( ) if not re . match ( "^((\\w+(\\.\\w+)?|'.*?'|\".*?\")\\s+" '(in|==|!=|not in)\\s+' "(\\w+(\\.\\w+)?|'.*?'|\".*?\")" '(\\s+(or|and)\\s+)?)+$' , expr ) : raise ValueError ( 'bad environment marker: %r' % expr ) expr = re . sub ( r"(platform\.\w+)" , r"\1()" , expr ) return parts [ 0 ] if eval ( expr ) else '' if isinstance ( value , list ) : new_value = [ ] for element in value : element = eval_environ_str ( element ) if element : new_value . append ( element ) elif isinstance ( value , str ) : new_value = eval_environ_str ( value ) else : new_value = value return new_value
10515	def verifyscrollbarvertical ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXOrientation == "AXVerticalOrientation" : return 1 except : pass return 0
5769	def _advapi32_load_key ( key_object , key_info , container ) : key_type = 'public' if isinstance ( key_info , keys . PublicKeyInfo ) else 'private' algo = key_info . algorithm if algo == 'rsa' : provider = Advapi32Const . MS_ENH_RSA_AES_PROV else : provider = Advapi32Const . MS_ENH_DSS_DH_PROV context_handle = None key_handle = None try : context_handle = open_context_handle ( provider , verify_only = key_type == 'public' ) blob = _advapi32_create_blob ( key_info , key_type , algo ) buffer_ = buffer_from_bytes ( blob ) key_handle_pointer = new ( advapi32 , 'HCRYPTKEY *' ) res = advapi32 . CryptImportKey ( context_handle , buffer_ , len ( blob ) , null ( ) , 0 , key_handle_pointer ) handle_error ( res ) key_handle = unwrap ( key_handle_pointer ) output = container ( key_handle , key_object ) output . context_handle = context_handle if algo == 'rsa' : ex_blob = _advapi32_create_blob ( key_info , key_type , algo , signing = False ) ex_buffer = buffer_from_bytes ( ex_blob ) ex_key_handle_pointer = new ( advapi32 , 'HCRYPTKEY *' ) res = advapi32 . CryptImportKey ( context_handle , ex_buffer , len ( ex_blob ) , null ( ) , 0 , ex_key_handle_pointer ) handle_error ( res ) output . ex_key_handle = unwrap ( ex_key_handle_pointer ) return output except ( Exception ) : if key_handle : advapi32 . CryptDestroyKey ( key_handle ) if context_handle : close_context_handle ( context_handle ) raise
7602	def get_known_tournaments ( self , * * params : tournamentfilter ) : url = self . api . TOURNAMENT + '/known' return self . _get_model ( url , PartialTournament , * * params )
5759	def get_jenkins_job_urls ( rosdistro_name , jenkins_url , release_build_name , targets ) : urls = { } for target in targets : view_name = get_release_view_name ( rosdistro_name , release_build_name , target . os_name , target . os_code_name , target . arch ) base_url = jenkins_url + '/view/%s/job/%s__{pkg}__' % ( view_name , view_name ) if target . arch == 'source' : urls [ target ] = base_url + '%s_%s__source' % ( target . os_name , target . os_code_name ) else : urls [ target ] = base_url + '%s_%s_%s__binary' % ( target . os_name , target . os_code_name , target . arch ) return urls
2187	def ensure ( self , func , * args , * * kwargs ) : data = self . tryload ( ) if data is None : data = func ( * args , * * kwargs ) self . save ( data ) return data
6485	def do_search ( request , course_id = None ) : # Setup search environment SearchInitializer . set_search_enviroment ( request = request , course_id = course_id ) results = { "error" : _ ( "Nothing to search" ) } status_code = 500 search_term = request . POST . get ( "search_string" , None ) try : if not search_term : raise ValueError ( _ ( 'No search term provided for search' ) ) size , from_ , page = _process_pagination_values ( request ) # Analytics - log search request track . emit ( 'edx.course.search.initiated' , { "search_term" : search_term , "page_size" : size , "page_number" : page , } ) results = perform_search ( search_term , user = request . user , size = size , from_ = from_ , course_id = course_id ) status_code = 200 # Analytics - log search results before sending to browser track . emit ( 'edx.course.search.results_displayed' , { "search_term" : search_term , "page_size" : size , "page_number" : page , "results_count" : results [ "total" ] , } ) except ValueError as invalid_err : results = { "error" : six . text_type ( invalid_err ) } log . debug ( six . text_type ( invalid_err ) ) except QueryParseError : results = { "error" : _ ( 'Your query seems malformed. Check for unmatched quotes.' ) } # Allow for broad exceptions here - this is an entry point from external reference except Exception as err : # pylint: disable=broad-except results = { "error" : _ ( 'An error occurred when searching for "{search_string}"' ) . format ( search_string = search_term ) } log . exception ( 'Search view exception when searching for %s for user %s: %r' , search_term , request . user . id , err ) return JsonResponse ( results , status = status_code )
10292	def expand_internal ( universe : BELGraph , graph : BELGraph , edge_predicates : EdgePredicates = None ) -> None : edge_filter = and_edge_predicates ( edge_predicates ) for u , v in itt . product ( graph , repeat = 2 ) : if graph . has_edge ( u , v ) or not universe . has_edge ( u , v ) : continue rs = defaultdict ( list ) for key , data in universe [ u ] [ v ] . items ( ) : if not edge_filter ( universe , u , v , key ) : continue rs [ data [ RELATION ] ] . append ( ( key , data ) ) if 1 == len ( rs ) : relation = list ( rs ) [ 0 ] for key , data in rs [ relation ] : graph . add_edge ( u , v , key = key , * * data ) else : log . debug ( 'Multiple relationship types found between %s and %s' , u , v )
10290	def enrich_variants ( graph : BELGraph , func : Union [ None , str , Iterable [ str ] ] = None ) : if func is None : func = { PROTEIN , RNA , MIRNA , GENE } nodes = list ( get_nodes_by_function ( graph , func ) ) for u in nodes : parent = u . get_parent ( ) if parent is None : continue if parent not in graph : graph . add_has_variant ( parent , u )
12155	def list_move_to_front ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . insert ( 0 , value ) return l
5309	def check_hex ( value ) : length = len ( value ) if length not in ( 3 , 6 ) : raise ValueError ( 'Hex string #{} is too long' . format ( value ) ) regex = r'[0-9a-f]{{{length}}}' . format ( length = length ) if not re . search ( regex , value , re . I ) : raise ValueError ( 'Invalid Hex String: #{}' . format ( value ) )
12841	def _close ( self , conn ) : super ( PooledAIODatabase , self ) . _close ( conn ) for waiter in self . _waiters : if not waiter . done ( ) : logger . debug ( 'Release a waiter' ) waiter . set_result ( True ) break
9947	def new_space ( self , name = None , bases = None , formula = None , refs = None ) : space = self . _impl . model . currentspace = self . _impl . new_space ( name = name , bases = get_impls ( bases ) , formula = formula , refs = refs ) return space . interface
355	def save_npz_dict ( save_list = None , name = 'model.npz' , sess = None ) : if sess is None : raise ValueError ( "session is None." ) if save_list is None : save_list = [ ] save_list_names = [ tensor . name for tensor in save_list ] save_list_var = sess . run ( save_list ) save_var_dict = { save_list_names [ idx ] : val for idx , val in enumerate ( save_list_var ) } np . savez ( name , * * save_var_dict ) save_list_var = None save_var_dict = None del save_list_var del save_var_dict logging . info ( "[*] Model saved in npz_dict %s" % name )
6469	def consume ( self , istream , ostream , batch = False ) : datapoints = [ ] # List of 2-tuples if batch : sleep = max ( 0.01 , self . option . sleep ) fd = istream . fileno ( ) while True : try : if select . select ( [ fd ] , [ ] , [ ] , sleep ) : try : line = istream . readline ( ) if line == '' : break datapoints . append ( self . consume_line ( line ) ) except ValueError : continue if self . option . sort_by_column : datapoints = sorted ( datapoints , key = itemgetter ( self . option . sort_by_column - 1 ) ) if len ( datapoints ) > 1 : datapoints = datapoints [ - self . maximum_points : ] self . update ( [ dp [ 0 ] for dp in datapoints ] , [ dp [ 1 ] for dp in datapoints ] ) self . render ( ostream ) time . sleep ( sleep ) except KeyboardInterrupt : break else : for line in istream : try : datapoints . append ( self . consume_line ( line ) ) except ValueError : pass if self . option . sort_by_column : datapoints = sorted ( datapoints , key = itemgetter ( self . option . sort_by_column - 1 ) ) self . update ( [ dp [ 0 ] for dp in datapoints ] , [ dp [ 1 ] for dp in datapoints ] ) self . render ( ostream )
8659	def filter_by ( zips = _zips , * * kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k , v in kwargs . items ( ) ] ) ]
7821	def challenge ( self , challenge ) : # pylint: disable=R0911 if not challenge : logger . debug ( "Empty challenge" ) return Failure ( "bad-challenge" ) if self . _server_first_message : return self . _final_challenge ( challenge ) match = SERVER_FIRST_MESSAGE_RE . match ( challenge ) if not match : logger . debug ( "Bad challenge syntax: {0!r}" . format ( challenge ) ) return Failure ( "bad-challenge" ) self . _server_first_message = challenge mext = match . group ( "mext" ) if mext : logger . debug ( "Unsupported extension received: {0!r}" . format ( mext ) ) return Failure ( "bad-challenge" ) nonce = match . group ( "nonce" ) if not nonce . startswith ( self . _c_nonce ) : logger . debug ( "Nonce does not start with our nonce" ) return Failure ( "bad-challenge" ) salt = match . group ( "salt" ) try : salt = a2b_base64 ( salt ) except ValueError : logger . debug ( "Bad base64 encoding for salt: {0!r}" . format ( salt ) ) return Failure ( "bad-challenge" ) iteration_count = match . group ( "iteration_count" ) try : iteration_count = int ( iteration_count ) except ValueError : logger . debug ( "Bad iteration_count: {0!r}" . format ( iteration_count ) ) return Failure ( "bad-challenge" ) return self . _make_response ( nonce , salt , iteration_count )
4304	def _get_valid_formats ( ) : if NO_SOX : return [ ] so = subprocess . check_output ( [ 'sox' , '-h' ] ) if type ( so ) is not str : so = str ( so , encoding = 'UTF-8' ) so = so . split ( '\n' ) idx = [ i for i in range ( len ( so ) ) if 'AUDIO FILE FORMATS:' in so [ i ] ] [ 0 ] formats = so [ idx ] . split ( ' ' ) [ 3 : ] return formats
13849	def get_time ( filename ) : ts = os . stat ( filename ) . st_mtime return datetime . datetime . utcfromtimestamp ( ts )
5360	def execute_batch_tasks ( self , tasks_cls , big_delay = 0 , small_delay = 0 , wait_for_threads = True ) : def _split_tasks ( tasks_cls ) : """ we internally distinguish between tasks executed by backend and tasks executed with no specific backend. """ backend_t = [ ] global_t = [ ] for t in tasks_cls : if t . is_backend_task ( t ) : backend_t . append ( t ) else : global_t . append ( t ) return backend_t , global_t backend_tasks , global_tasks = _split_tasks ( tasks_cls ) logger . debug ( 'backend_tasks = %s' % ( backend_tasks ) ) logger . debug ( 'global_tasks = %s' % ( global_tasks ) ) threads = [ ] # stopper won't be set unless wait_for_threads is True stopper = threading . Event ( ) # launching threads for tasks by backend if len ( backend_tasks ) > 0 : repos_backend = self . _get_repos_by_backend ( ) for backend in repos_backend : # Start new Threads and add them to the threads list to complete t = TasksManager ( backend_tasks , backend , stopper , self . config , small_delay ) threads . append ( t ) t . start ( ) # launch thread for global tasks if len ( global_tasks ) > 0 : # FIXME timer is applied to all global_tasks, does it make sense? # All tasks are executed in the same thread sequentially gt = TasksManager ( global_tasks , "Global tasks" , stopper , self . config , big_delay ) threads . append ( gt ) gt . start ( ) if big_delay > 0 : when = datetime . now ( ) + timedelta ( seconds = big_delay ) when_str = when . strftime ( '%a, %d %b %Y %H:%M:%S %Z' ) logger . info ( "%s will be executed on %s" % ( global_tasks , when_str ) ) if wait_for_threads : time . sleep ( 1 ) # Give enough time create and run all threads stopper . set ( ) # All threads must stop in the next iteration # Wait for all threads to complete for t in threads : t . join ( ) # Checking for exceptions in threads to log them self . __check_queue_for_errors ( ) logger . debug ( "[thread:main] All threads (and their tasks) are finished" )
6186	def get_last_commit_line ( git_path = None ) : if git_path is None : git_path = GIT_PATH output = check_output ( [ git_path , "log" , "--pretty=format:'%ad %h %s'" , "--date=short" , "-n1" ] ) return output . strip ( ) [ 1 : - 1 ]
7426	def refmap_stats ( data , sample ) : ## shorter names mapf = os . path . join ( data . dirs . refmapping , sample . name + "-mapped-sorted.bam" ) umapf = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) ## get from unmapped cmd1 = [ ipyrad . bins . samtools , "flagstat" , umapf ] proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) result1 = proc1 . communicate ( ) [ 0 ] ## get from mapped cmd2 = [ ipyrad . bins . samtools , "flagstat" , mapf ] proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE ) result2 = proc2 . communicate ( ) [ 0 ] ## store results ## If PE, samtools reports the _actual_ number of reads mapped, both ## R1 and R2, so here if PE divide the results by 2 to stay consistent ## with how we've been reporting R1 and R2 as one "read pair" if "pair" in data . paramsdict [ "datatype" ] : sample . stats [ "refseq_unmapped_reads" ] = int ( result1 . split ( ) [ 0 ] ) / 2 sample . stats [ "refseq_mapped_reads" ] = int ( result2 . split ( ) [ 0 ] ) / 2 else : sample . stats [ "refseq_unmapped_reads" ] = int ( result1 . split ( ) [ 0 ] ) sample . stats [ "refseq_mapped_reads" ] = int ( result2 . split ( ) [ 0 ] ) sample_cleanup ( data , sample )
2245	def hzcat ( args , sep = '' ) : import unicodedata if '\n' in sep or '\r' in sep : raise ValueError ( '`sep` cannot contain newline characters' ) # TODO: ensure unicode data works correctly for python2 args = [ unicodedata . normalize ( 'NFC' , ensure_unicode ( val ) ) for val in args ] arglines = [ a . split ( '\n' ) for a in args ] height = max ( map ( len , arglines ) ) # Do vertical padding arglines = [ lines + [ '' ] * ( height - len ( lines ) ) for lines in arglines ] # Initialize output all_lines = [ '' for _ in range ( height ) ] width = 0 n_args = len ( args ) for sx , lines in enumerate ( arglines ) : # Concatenate the new string for lx , line in enumerate ( lines ) : all_lines [ lx ] += line # Find the new maximum horizontal width width = max ( width , max ( map ( len , all_lines ) ) ) if sx < n_args - 1 : # Horizontal padding on all but last iter for lx , line in list ( enumerate ( all_lines ) ) : residual = width - len ( line ) all_lines [ lx ] = line + ( ' ' * residual ) + sep width += len ( sep ) # Clean up trailing whitespace all_lines = [ line . rstrip ( ' ' ) for line in all_lines ] ret = '\n' . join ( all_lines ) return ret
4483	def create_file ( self , path , fp , force = False , update = False ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) # all paths are assumed to be absolute path = norm_remote_path ( path ) directory , fname = os . path . split ( path ) directories = directory . split ( os . path . sep ) # navigate to the right parent object for our file parent = self for directory in directories : # skip empty directory names if directory : parent = parent . create_folder ( directory , exist_ok = True ) url = parent . _new_file_url # When uploading a large file (>a few MB) that already exists # we sometimes get a ConnectionError instead of a status == 409. connection_error = False # peek at the file to check if it is an empty file which needs special # handling in requests. If we pass a file like object to data that # turns out to be of length zero then no file is created on the OSF. # See: https://github.com/osfclient/osfclient/pull/135 if file_empty ( fp ) : response = self . _put ( url , params = { 'name' : fname } , data = b'' ) else : try : response = self . _put ( url , params = { 'name' : fname } , data = fp ) except ConnectionError : connection_error = True if connection_error or response . status_code == 409 : if not force and not update : # one-liner to get file size from file pointer from # https://stackoverflow.com/a/283719/2680824 file_size_bytes = get_local_file_size ( fp ) large_file_cutoff = 2 ** 20 # 1 MB in bytes if connection_error and file_size_bytes < large_file_cutoff : msg = ( "There was a connection error which might mean {} " + "already exists. Try again with the `--force` flag " + "specified." ) . format ( path ) raise RuntimeError ( msg ) else : # note in case of connection error, we are making an inference here raise FileExistsError ( path ) else : # find the upload URL for the file we are trying to update for file_ in self . files : if norm_remote_path ( file_ . path ) == path : if not force : if checksum ( path ) == file_ . hashes . get ( 'md5' ) : # If the hashes are equal and force is False, # we're done here break # in the process of attempting to upload the file we # moved through it -> reset read position to beginning # of the file fp . seek ( 0 ) file_ . update ( fp ) break else : raise RuntimeError ( "Could not create a new file at " "({}) nor update it." . format ( path ) )
7612	def get_clan_image ( self , obj : BaseAttrDict ) : try : badge_id = obj . clan . badge_id except AttributeError : try : badge_id = obj . badge_id except AttributeError : return 'https://i.imgur.com/Y3uXsgj.png' if badge_id is None : return 'https://i.imgur.com/Y3uXsgj.png' for i in self . constants . alliance_badges : if i . id == badge_id : return 'https://royaleapi.github.io/cr-api-assets/badges/' + i . name + '.png'
6865	def get_time_flux_errs_from_Ames_lightcurve ( infile , lctype , cadence_min = 2 ) : warnings . warn ( "Use the astrotess.read_tess_fitslc and " "astrotess.consolidate_tess_fitslc functions instead of this function. " "This function will be removed in astrobase v0.4.2." , FutureWarning ) if lctype not in ( 'PDCSAP' , 'SAP' ) : raise ValueError ( 'unknown light curve type requested: %s' % lctype ) hdulist = pyfits . open ( infile ) main_hdr = hdulist [ 0 ] . header lc_hdr = hdulist [ 1 ] . header lc = hdulist [ 1 ] . data if ( ( 'Ames' not in main_hdr [ 'ORIGIN' ] ) or ( 'LIGHTCURVE' not in lc_hdr [ 'EXTNAME' ] ) ) : raise ValueError ( 'could not understand input LC format. ' 'Is it a TESS TOI LC file?' ) time = lc [ 'TIME' ] flux = lc [ '{:s}_FLUX' . format ( lctype ) ] err_flux = lc [ '{:s}_FLUX_ERR' . format ( lctype ) ] # REMOVE POINTS FLAGGED WITH: # attitude tweaks, safe mode, coarse/earth pointing, argabrithening events, # reaction wheel desaturation events, cosmic rays in optimal aperture # pixels, manual excludes, discontinuities, stray light from Earth or Moon # in camera FoV. # (Note: it's not clear to me what a lot of these mean. Also most of these # columns are probably not correctly propagated right now.) sel = ( lc [ 'QUALITY' ] == 0 ) sel &= np . isfinite ( time ) sel &= np . isfinite ( flux ) sel &= np . isfinite ( err_flux ) sel &= ~ np . isnan ( time ) sel &= ~ np . isnan ( flux ) sel &= ~ np . isnan ( err_flux ) sel &= ( time != 0 ) sel &= ( flux != 0 ) sel &= ( err_flux != 0 ) time = time [ sel ] flux = flux [ sel ] err_flux = err_flux [ sel ] # ensure desired cadence lc_cadence_diff = np . abs ( np . nanmedian ( np . diff ( time ) ) * 24 * 60 - cadence_min ) if lc_cadence_diff > 1.0e-2 : raise ValueError ( 'the light curve is not at the required cadence specified: %.2f' % cadence_min ) fluxmedian = np . nanmedian ( flux ) flux /= fluxmedian err_flux /= fluxmedian return time , flux , err_flux
11083	def help ( self , msg , args ) : output = [ ] if len ( args ) == 0 : commands = sorted ( self . _bot . dispatcher . commands . items ( ) , key = itemgetter ( 0 ) ) commands = filter ( lambda x : x [ 1 ] . is_subcmd is False , commands ) # Filter commands if auth is enabled, hide_admin_commands is enabled, and user is not admin if self . _should_filter_help_commands ( msg . user ) : commands = filter ( lambda x : x [ 1 ] . admin_only is False , commands ) for name , cmd in commands : output . append ( self . _get_short_help_for_command ( name ) ) else : name = '!' + args [ 0 ] output = [ self . _get_help_for_command ( name ) ] return '\n' . join ( output )
4582	def construct ( cls , project , * , run = None , name = None , data = None , * * desc ) : from . failed import Failed exception = desc . pop ( '_exception' , None ) if exception : a = Failed ( project . layout , desc , exception ) else : try : a = cls ( project . layout , * * desc ) a . _set_runner ( run or { } ) except Exception as e : if cls . FAIL_ON_EXCEPTION : raise a = Failed ( project . layout , desc , e ) a . name = name a . data = data return a
3595	def details ( self , packageName ) : path = DETAILS_URL + "?doc={}" . format ( requests . utils . quote ( packageName ) ) data = self . executeRequestApi2 ( path ) return utils . parseProtobufObj ( data . payload . detailsResponse . docV2 )
3583	def _print_tree ( self ) : # This is based on the bluez sample code get-managed-objects.py. objects = self . _bluez . GetManagedObjects ( ) for path in objects . keys ( ) : print ( "[ %s ]" % ( path ) ) interfaces = objects [ path ] for interface in interfaces . keys ( ) : if interface in [ "org.freedesktop.DBus.Introspectable" , "org.freedesktop.DBus.Properties" ] : continue print ( " %s" % ( interface ) ) properties = interfaces [ interface ] for key in properties . keys ( ) : print ( " %s = %s" % ( key , properties [ key ] ) )
6105	def masses_of_galaxies_within_circles_in_units ( self , radius : dim . Length , unit_mass = 'angular' , critical_surface_density = None ) : return list ( map ( lambda galaxy : galaxy . mass_within_circle_in_units ( radius = radius , unit_mass = unit_mass , kpc_per_arcsec = self . kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . galaxies ) )
8458	def shell ( cmd , check = True , stdin = None , stdout = None , stderr = None ) : return subprocess . run ( cmd , shell = True , check = check , stdin = stdin , stdout = stdout , stderr = stderr )
2983	def cmd_events ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) if opts . json : outf = None _write = puts if opts . output is not None : outf = open ( opts . output , "w" ) _write = outf . write try : delim = "" logs = b . get_audit ( ) . read_logs ( as_json = False ) _write ( '{"events": [' ) _write ( os . linesep ) for l in logs : _write ( delim + l ) delim = "," + os . linesep _write ( os . linesep ) _write ( ']}' ) finally : if opts . output is not None : outf . close ( ) else : puts ( colored . blue ( columns ( [ "EVENT" , 10 ] , [ "TARGET" , 16 ] , [ "STATUS" , 8 ] , [ "TIME" , 16 ] , [ "MESSAGE" , 25 ] ) ) ) logs = b . get_audit ( ) . read_logs ( as_json = True ) for l in logs : puts ( columns ( [ l [ 'event' ] , 10 ] , [ str ( [ str ( t ) for t in l [ 'targets' ] ] ) , 16 ] , [ l [ 'status' ] , 8 ] , [ str ( l [ 'timestamp' ] ) , 16 ] , [ l [ 'message' ] , 25 ] ) )
7742	def _prepare_pending ( self ) : if not self . _unprepared_pending : return for handler in list ( self . _unprepared_pending ) : self . _configure_io_handler ( handler ) self . check_events ( )
13697	def try_read_file ( s ) : try : with open ( s , 'r' ) as f : data = f . read ( ) except FileNotFoundError : # Not a file name. return s except EnvironmentError as ex : print_err ( '\nFailed to read file: {}\n {}' . format ( s , ex ) ) return None return data
3594	def search ( self , query ) : if self . authSubToken is None : raise LoginError ( "You need to login before executing any request" ) path = SEARCH_URL + "?c=3&q={}" . format ( requests . utils . quote ( query ) ) # FIXME: not sure if this toc call should be here self . toc ( ) data = self . executeRequestApi2 ( path ) if utils . hasPrefetch ( data ) : response = data . preFetch [ 0 ] . response else : response = data resIterator = response . payload . listResponse . doc return list ( map ( utils . parseProtobufObj , resIterator ) )
1884	def concretize ( self , symbolic , policy , maxcount = 7 ) : assert self . constraints == self . platform . constraints symbolic = self . migrate_expression ( symbolic ) vals = [ ] if policy == 'MINMAX' : vals = self . _solver . minmax ( self . _constraints , symbolic ) elif policy == 'MAX' : vals = self . _solver . max ( self . _constraints , symbolic ) elif policy == 'MIN' : vals = self . _solver . min ( self . _constraints , symbolic ) elif policy == 'SAMPLED' : m , M = self . _solver . minmax ( self . _constraints , symbolic ) vals += [ m , M ] if M - m > 3 : if self . _solver . can_be_true ( self . _constraints , symbolic == ( m + M ) // 2 ) : vals . append ( ( m + M ) // 2 ) if M - m > 100 : for i in ( 0 , 1 , 2 , 5 , 32 , 64 , 128 , 320 ) : if self . _solver . can_be_true ( self . _constraints , symbolic == m + i ) : vals . append ( m + i ) if maxcount <= len ( vals ) : break if M - m > 1000 and maxcount > len ( vals ) : vals += self . _solver . get_all_values ( self . _constraints , symbolic , maxcnt = maxcount - len ( vals ) , silent = True ) elif policy == 'ONE' : vals = [ self . _solver . get_value ( self . _constraints , symbolic ) ] else : assert policy == 'ALL' vals = solver . get_all_values ( self . _constraints , symbolic , maxcnt = maxcount , silent = True ) return tuple ( set ( vals ) )
576	def tick ( self ) : # Run activities whose time has come for act in self . __activities : if not act . iteratorHolder [ 0 ] : continue try : next ( act . iteratorHolder [ 0 ] ) except StopIteration : act . cb ( ) if act . repeating : act . iteratorHolder [ 0 ] = iter ( xrange ( act . period ) ) else : act . iteratorHolder [ 0 ] = None return True
6172	def _select_manager ( backend_name ) : if backend_name == 'RedisBackend' : lock_manager = _LockManagerRedis elif backend_name == 'DatabaseBackend' : lock_manager = _LockManagerDB else : raise NotImplementedError return lock_manager
12790	def create_from_settings ( settings ) : return Connection ( settings [ "url" ] , settings [ "base_url" ] , settings [ "user" ] , settings [ "password" ] , authorizations = settings [ "authorizations" ] , debug = settings [ "debug" ] )
2759	def get_all_load_balancers ( self ) : data = self . get_data ( "load_balancers" ) load_balancers = list ( ) for jsoned in data [ 'load_balancers' ] : load_balancer = LoadBalancer ( * * jsoned ) load_balancer . token = self . token load_balancer . health_check = HealthCheck ( * * jsoned [ 'health_check' ] ) load_balancer . sticky_sessions = StickySesions ( * * jsoned [ 'sticky_sessions' ] ) forwarding_rules = list ( ) for rule in jsoned [ 'forwarding_rules' ] : forwarding_rules . append ( ForwardingRule ( * * rule ) ) load_balancer . forwarding_rules = forwarding_rules load_balancers . append ( load_balancer ) return load_balancers
8335	def findPreviousSibling ( self , name = None , attrs = { } , text = None , * * kwargs ) : return self . _findOne ( self . findPreviousSiblings , name , attrs , text , * * kwargs )
5327	def sha_github_file ( cls , config , repo_file , repository_api , repository_branch ) : repo_file_sha = None cfg = config . get_conf ( ) github_token = cfg [ 'sortinghat' ] [ 'identities_api_token' ] headers = { "Authorization" : "token " + github_token } url_dir = repository_api + "/git/trees/" + repository_branch logger . debug ( "Gettting sha data from tree: %s" , url_dir ) raw_repo_file_info = requests . get ( url_dir , headers = headers ) raw_repo_file_info . raise_for_status ( ) for rfile in raw_repo_file_info . json ( ) [ 'tree' ] : if rfile [ 'path' ] == repo_file : logger . debug ( "SHA found: %s, " , rfile [ "sha" ] ) repo_file_sha = rfile [ "sha" ] break return repo_file_sha
9552	def _apply_value_checks ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for field_name , check , code , message , modulus in self . _value_checks : if i % modulus == 0 : # support sampling fi = self . _field_names . index ( field_name ) if fi < len ( r ) : # only apply checks if there is a value value = r [ fi ] try : check ( value ) except ValueError : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . __name__ , check . __doc__ ) if context is not None : p [ 'context' ] = context yield p
3050	def from_stream ( credential_filename ) : if credential_filename and os . path . isfile ( credential_filename ) : try : return _get_application_default_credential_from_file ( credential_filename ) except ( ApplicationDefaultCredentialsError , ValueError ) as error : extra_help = ( ' (provided as parameter to the ' 'from_stream() method)' ) _raise_exception_for_reading_json ( credential_filename , extra_help , error ) else : raise ApplicationDefaultCredentialsError ( 'The parameter passed to the from_stream() ' 'method should point to a file.' )
282	def plot_holdings ( returns , positions , legend_loc = 'best' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) positions = positions . copy ( ) . drop ( 'cash' , axis = 'columns' ) df_holdings = positions . replace ( 0 , np . nan ) . count ( axis = 1 ) df_holdings_by_month = df_holdings . resample ( '1M' ) . mean ( ) df_holdings . plot ( color = 'steelblue' , alpha = 0.6 , lw = 0.5 , ax = ax , * * kwargs ) df_holdings_by_month . plot ( color = 'orangered' , lw = 2 , ax = ax , * * kwargs ) ax . axhline ( df_holdings . values . mean ( ) , color = 'steelblue' , ls = '--' , lw = 3 ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) leg = ax . legend ( [ 'Daily holdings' , 'Average daily holdings, by month' , 'Average daily holdings, overall' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . set_title ( 'Total holdings' ) ax . set_ylabel ( 'Holdings' ) ax . set_xlabel ( '' ) return ax
2119	def _parent_filter ( self , parent , relationship , * * kwargs ) : if parent is None or relationship is None : return { } parent_filter_kwargs = { } query_params = ( ( self . _reverse_rel_name ( relationship ) , parent ) , ) parent_filter_kwargs [ 'query' ] = query_params if kwargs . get ( 'workflow_job_template' , None ) is None : parent_data = self . read ( pk = parent ) [ 'results' ] [ 0 ] parent_filter_kwargs [ 'workflow_job_template' ] = parent_data [ 'workflow_job_template' ] return parent_filter_kwargs
2286	def graph_evaluation ( data , adj_matrix , gpu = None , gpu_id = 0 , * * kwargs ) : gpu = SETTINGS . get_default ( gpu = gpu ) device = 'cuda:{}' . format ( gpu_id ) if gpu else 'cpu' obs = th . FloatTensor ( data ) . to ( device ) cgnn = CGNN_model ( adj_matrix , data . shape [ 0 ] , gpu_id = gpu_id , * * kwargs ) . to ( device ) cgnn . reset_parameters ( ) return cgnn . run ( obs , * * kwargs )
6931	def xmatch_cpdir_external_catalogs ( cpdir , xmatchpkl , cpfileglob = 'checkplot-*.pkl*' , xmatchradiusarcsec = 2.0 , updateexisting = True , resultstodir = None ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return xmatch_cplist_external_catalogs ( cplist , xmatchpkl , xmatchradiusarcsec = xmatchradiusarcsec , updateexisting = updateexisting , resultstodir = resultstodir )
13677	def prepare ( self ) : result_files = self . collect_files ( ) chain = self . prepare_handlers_chain if chain is None : # default handlers chain = [ LessCompilerPrepareHandler ( ) ] for prepare_handler in chain : result_files = prepare_handler . prepare ( result_files , self ) return result_files
9873	def CherryPyWSGIServer ( bind_addr , wsgi_app , numthreads = 10 , server_name = None , max = - 1 , request_queue_size = 5 , timeout = 10 , shutdown_timeout = 5 ) : max_threads = max if max_threads < 0 : max_threads = 0 return Rocket ( bind_addr , 'wsgi' , { 'wsgi_app' : wsgi_app } , min_threads = numthreads , max_threads = max_threads , queue_size = request_queue_size , timeout = timeout )
7836	def register_disco_cache_fetchers ( cache_suite , stream ) : tmp = stream class DiscoInfoCacheFetcher ( DiscoCacheFetcherBase ) : """Cache fetcher for DiscoInfo.""" stream = tmp disco_class = DiscoInfo class DiscoItemsCacheFetcher ( DiscoCacheFetcherBase ) : """Cache fetcher for DiscoItems.""" stream = tmp disco_class = DiscoItems cache_suite . register_fetcher ( DiscoInfo , DiscoInfoCacheFetcher ) cache_suite . register_fetcher ( DiscoItems , DiscoItemsCacheFetcher )
1816	def SETNO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF == False , 1 , 0 ) )
11640	def json_get_data ( filename ) : with open ( filename ) as fp : json_data = json . load ( fp ) return json_data return False
2980	def cmd_logs ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) puts ( b . logs ( opts . container ) . decode ( encoding = 'UTF-8' ) )
11504	def delete_folder ( self , token , folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id response = self . request ( 'midas.folder.delete' , parameters ) return response
732	def _generate ( self ) : n = self . _n w = self . _w assert type ( w ) is int , "List for w not supported" for i in xrange ( n / w ) : pattern = set ( xrange ( i * w , ( i + 1 ) * w ) ) self . _patterns [ i ] = pattern
1030	def b32encode ( s ) : parts = [ ] quanta , leftover = divmod ( len ( s ) , 5 ) # Pad the last quantum with zero bits if necessary if leftover : s += ( '\0' * ( 5 - leftover ) ) quanta += 1 for i in range ( quanta ) : # c1 and c2 are 16 bits wide, c3 is 8 bits wide. The intent of this # code is to process the 40 bits in units of 5 bits. So we take the 1 # leftover bit of c1 and tack it onto c2. Then we take the 2 leftover # bits of c2 and tack them onto c3. The shifts and masks are intended # to give us values of exactly 5 bits in width. c1 , c2 , c3 = struct . unpack ( '!HHB' , s [ i * 5 : ( i + 1 ) * 5 ] ) c2 += ( c1 & 1 ) << 16 # 17 bits wide c3 += ( c2 & 3 ) << 8 # 10 bits wide parts . extend ( [ _b32tab [ c1 >> 11 ] , # bits 1 - 5 _b32tab [ ( c1 >> 6 ) & 0x1f ] , # bits 6 - 10 _b32tab [ ( c1 >> 1 ) & 0x1f ] , # bits 11 - 15 _b32tab [ c2 >> 12 ] , # bits 16 - 20 (1 - 5) _b32tab [ ( c2 >> 7 ) & 0x1f ] , # bits 21 - 25 (6 - 10) _b32tab [ ( c2 >> 2 ) & 0x1f ] , # bits 26 - 30 (11 - 15) _b32tab [ c3 >> 5 ] , # bits 31 - 35 (1 - 5) _b32tab [ c3 & 0x1f ] , # bits 36 - 40 (1 - 5) ] ) encoded = EMPTYSTRING . join ( parts ) # Adjust for any leftover partial quanta if leftover == 1 : return encoded [ : - 6 ] + '======' elif leftover == 2 : return encoded [ : - 4 ] + '====' elif leftover == 3 : return encoded [ : - 3 ] + '===' elif leftover == 4 : return encoded [ : - 1 ] + '=' return encoded
8774	def _load_worker_plugin_with_module ( self , module , version ) : classes = inspect . getmembers ( module , inspect . isclass ) loaded = 0 for cls_name , cls in classes : if hasattr ( cls , 'versions' ) : if version not in cls . versions : continue else : continue if issubclass ( cls , base_worker . QuarkAsyncPluginBase ) : LOG . debug ( "Loading plugin %s" % cls_name ) plugin = cls ( ) self . plugins . append ( plugin ) loaded += 1 LOG . debug ( "Found %d possible plugins and loaded %d" % ( len ( classes ) , loaded ) )
2852	def mpsse_set_clock ( self , clock_hz , adaptive = False , three_phase = False ) : # Disable clock divisor by 5 to enable faster speeds on FT232H. self . _write ( '\x8A' ) # Turn on/off adaptive clocking. if adaptive : self . _write ( '\x96' ) else : self . _write ( '\x97' ) # Turn on/off three phase clock (needed for I2C). # Also adjust the frequency for three-phase clocking as specified in section 2.2.4 # of this document: # http://www.ftdichip.com/Support/Documents/AppNotes/AN_255_USB%20to%20I2C%20Example%20using%20the%20FT232H%20and%20FT201X%20devices.pdf if three_phase : self . _write ( '\x8C' ) else : self . _write ( '\x8D' ) # Compute divisor for requested clock. # Use equation from section 3.8.1 of: # http://www.ftdichip.com/Support/Documents/AppNotes/AN_108_Command_Processor_for_MPSSE_and_MCU_Host_Bus_Emulation_Modes.pdf # Note equation is using 60mhz master clock instead of 12mhz. divisor = int ( math . ceil ( ( 30000000.0 - float ( clock_hz ) ) / float ( clock_hz ) ) ) & 0xFFFF if three_phase : divisor = int ( divisor * ( 2.0 / 3.0 ) ) logger . debug ( 'Setting clockspeed with divisor value {0}' . format ( divisor ) ) # Send command to set divisor from low and high byte values. self . _write ( str ( bytearray ( ( 0x86 , divisor & 0xFF , ( divisor >> 8 ) & 0xFF ) ) ) )
810	def _finishLearning ( self ) : if self . _doSphering : self . _finishSphering ( ) self . _knn . finishLearning ( ) # Compute leave-one-out validation accuracy if # we actually received non-trivial partition info self . _accuracy = None
12457	def iteritems ( data , * * kwargs ) : return iter ( data . items ( * * kwargs ) ) if IS_PY3 else data . iteritems ( * * kwargs )
10632	def get_compound_mfr ( self , compound ) : if compound in self . material . compounds : return self . _compound_mfrs [ self . material . get_compound_index ( compound ) ] else : return 0.0
9450	def cancel_scheduled_hangup ( self , call_params ) : path = '/' + self . api_version + '/CancelScheduledHangup/' method = 'POST' return self . request ( path , method , call_params )
7814	def from_file ( cls , filename ) : with open ( filename , "r" ) as pem_file : data = pem . readPemFromFile ( pem_file ) return cls . from_der_data ( data )
1990	def rm ( self , key ) : path = os . path . join ( self . uri , key ) os . remove ( path )
2351	def register ( ) : registerDriver ( ISelenium , Selenium , class_implements = [ Firefox , Chrome , Ie , Edge , Opera , Safari , BlackBerry , PhantomJS , Android , Remote , EventFiringWebDriver , ] , )
975	def _createBucket ( self , index ) : if index < self . minIndex : if index == self . minIndex - 1 : # Create a new representation that has exactly w-1 overlapping bits # as the min representation self . bucketMap [ index ] = self . _newRepresentation ( self . minIndex , index ) self . minIndex = index else : # Recursively create all the indices above and then this index self . _createBucket ( index + 1 ) self . _createBucket ( index ) else : if index == self . maxIndex + 1 : # Create a new representation that has exactly w-1 overlapping bits # as the max representation self . bucketMap [ index ] = self . _newRepresentation ( self . maxIndex , index ) self . maxIndex = index else : # Recursively create all the indices below and then this index self . _createBucket ( index - 1 ) self . _createBucket ( index )
2784	def get_timeout ( self ) : timeout_str = os . environ . get ( REQUEST_TIMEOUT_ENV_VAR ) if timeout_str : try : return float ( timeout_str ) except : self . _log . error ( 'Failed parsing the request read timeout of ' '"%s". Please use a valid float number!' % timeout_str ) return None
12946	def copy ( self , copyPrimaryKey = False , copyValues = False ) : cpy = self . __class__ ( * * self . asDict ( copyPrimaryKey , forStorage = False ) ) if copyValues is True : for fieldName in cpy . FIELDS : setattr ( cpy , fieldName , copy . deepcopy ( getattr ( cpy , fieldName ) ) ) return cpy
10526	def get_google_playlist_songs ( self , playlist , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : logger . info ( "Loading Google Music playlist songs..." ) google_playlist = self . get_google_playlist ( playlist ) if not google_playlist : return [ ] , [ ] playlist_song_ids = [ track [ 'trackId' ] for track in google_playlist [ 'tracks' ] ] playlist_songs = [ song for song in self . api . get_all_songs ( ) if song [ 'id' ] in playlist_song_ids ] matched_songs , filtered_songs = filter_google_songs ( playlist_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Filtered {0} Google playlist songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} Google playlist songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs
7231	def create_from_wkt ( self , wkt , item_type , ingest_source , * * attributes ) : # verify the "depth" of the attributes is single layer geojson = load_wkt ( wkt ) . __geo_interface__ vector = { 'type' : "Feature" , 'geometry' : geojson , 'properties' : { 'item_type' : item_type , 'ingest_source' : ingest_source , 'attributes' : attributes } } return self . create ( vector ) [ 0 ]
10894	def filtered_image ( self , im ) : q = np . fft . fftn ( im ) for k , v in self . filters : q [ k ] -= v return np . real ( np . fft . ifftn ( q ) )
11687	def changeset_info ( changeset ) : keys = [ tag . attrib . get ( 'k' ) for tag in changeset . getchildren ( ) ] keys += [ 'id' , 'user' , 'uid' , 'bbox' , 'created_at' ] values = [ tag . attrib . get ( 'v' ) for tag in changeset . getchildren ( ) ] values += [ changeset . get ( 'id' ) , changeset . get ( 'user' ) , changeset . get ( 'uid' ) , get_bounds ( changeset ) , changeset . get ( 'created_at' ) ] return dict ( zip ( keys , values ) )
5719	def _convert_path ( path , name ) : table = os . path . splitext ( path ) [ 0 ] table = table . replace ( os . path . sep , '__' ) if name is not None : table = '___' . join ( [ table , name ] ) table = re . sub ( '[^0-9a-zA-Z_]+' , '_' , table ) table = table . lower ( ) return table
3333	def dynamic_instantiate_middleware ( name , args , expand = None ) : def _expand ( v ) : """Replace some string templates with defined values.""" if expand and compat . is_basestring ( v ) and v . lower ( ) in expand : return expand [ v ] return v try : the_class = dynamic_import_class ( name ) inst = None if type ( args ) in ( tuple , list ) : args = tuple ( map ( _expand , args ) ) inst = the_class ( * args ) else : assert type ( args ) is dict args = { k : _expand ( v ) for k , v in args . items ( ) } inst = the_class ( * * args ) _logger . debug ( "Instantiate {}({}) => {}" . format ( name , args , inst ) ) except Exception : _logger . exception ( "ERROR: Instantiate {}({}) => {}" . format ( name , args , inst ) ) return inst
7669	def slice ( self , start_time , end_time , strict = False ) : sliced_array = AnnotationArray ( ) for ann in self : sliced_array . append ( ann . slice ( start_time , end_time , strict = strict ) ) return sliced_array
11924	def render_to ( path , template , * * data ) : try : renderer . render_to ( path , template , * * data ) except JinjaTemplateNotFound as e : logger . error ( e . __doc__ + ', Template: %r' % template ) sys . exit ( e . exit_code )
2760	def get_load_balancer ( self , id ) : return LoadBalancer . get_object ( api_token = self . token , id = id )
3627	def normalize_cols ( table ) : longest_row_len = max ( [ len ( row ) for row in table ] ) for row in table : while len ( row ) < longest_row_len : row . append ( '' ) return table
9126	def _make_session ( connection : Optional [ str ] = None ) -> Session : if connection is None : connection = get_global_connection ( ) engine = create_engine ( connection ) create_all ( engine ) session_cls = sessionmaker ( bind = engine ) session = session_cls ( ) return session
4155	def save_file ( self ) : with open ( self . write_file , 'w' ) as out_nb : json . dump ( self . work_notebook , out_nb , indent = 2 )
1389	def get_machines ( self ) : if self . physical_plan : stmgrs = list ( self . physical_plan . stmgrs ) return map ( lambda s : s . host_name , stmgrs ) return [ ]
8944	def pushd ( path ) : saved = os . getcwd ( ) os . chdir ( path ) try : yield saved finally : os . chdir ( saved )
4444	def get_suggestions ( self , prefix , fuzzy = False , num = 10 , with_scores = False , with_payloads = False ) : args = [ AutoCompleter . SUGGET_COMMAND , self . key , prefix , 'MAX' , num ] if fuzzy : args . append ( AutoCompleter . FUZZY ) if with_scores : args . append ( AutoCompleter . WITHSCORES ) if with_payloads : args . append ( AutoCompleter . WITHPAYLOADS ) ret = self . redis . execute_command ( * args ) results = [ ] if not ret : return results parser = SuggestionParser ( with_scores , with_payloads , ret ) return [ s for s in parser ]
9421	def _read_header ( self , handle ) : header_data = unrarlib . RARHeaderDataEx ( ) try : res = unrarlib . RARReadHeaderEx ( handle , ctypes . byref ( header_data ) ) rarinfo = RarInfo ( header = header_data ) except unrarlib . ArchiveEnd : return None except unrarlib . MissingPassword : raise RuntimeError ( "Archive is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for Archive" ) except unrarlib . UnrarException as e : raise BadRarFile ( str ( e ) ) return rarinfo
7867	def expire ( self ) : with self . _lock : logger . debug ( "expdict.expire. timeouts: {0!r}" . format ( self . _timeouts ) ) next_timeout = None for k in self . _timeouts . keys ( ) : ret = self . _expire_item ( k ) if ret is not None : if next_timeout is None : next_timeout = ret else : next_timeout = min ( next_timeout , ret ) return next_timeout
10029	def add_arguments ( parser ) : parser . add_argument ( '-o' , '--old-environment' , help = 'Old environment name' , required = True ) parser . add_argument ( '-n' , '--new-environment' , help = 'New environment name' , required = True )
11395	def load_class ( path ) : package , klass = path . rsplit ( '.' , 1 ) module = import_module ( package ) return getattr ( module , klass )
9379	def detect_timestamp_format ( timestamp ) : time_formats = { 'epoch' : re . compile ( r'^[0-9]{10}$' ) , 'epoch_ms' : re . compile ( r'^[0-9]{13}$' ) , 'epoch_fraction' : re . compile ( r'^[0-9]{10}\.[0-9]{3,9}$' ) , '%Y-%m-%d %H:%M:%S' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y-%m-%dT%H:%M:%S' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y-%m-%d_%H:%M:%S' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y-%m-%d %H:%M:%S.%f' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y-%m-%dT%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y-%m-%d_%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y%m%d %H:%M:%S' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y%m%dT%H:%M:%S' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y%m%d_%H:%M:%S' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y%m%d %H:%M:%S.%f' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y%m%dT%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y%m%d_%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%H:%M:%S' : re . compile ( r'^[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%H:%M:%S.%f' : re . compile ( r'^[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y-%m-%dT%H:%M:%S.%f%z' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+[+-][0-9]{4}$' ) } for time_format in time_formats : if re . match ( time_formats [ time_format ] , timestamp ) : return time_format return 'unknown'
11486	def _descend_folder_for_id ( parsed_path , folder_id ) : if len ( parsed_path ) == 0 : return folder_id session . token = verify_credentials ( ) base_folder = session . communicator . folder_get ( session . token , folder_id ) cur_folder_id = - 1 for path_part in parsed_path : cur_folder_id = base_folder [ 'folder_id' ] cur_children = session . communicator . folder_children ( session . token , cur_folder_id ) for inner_folder in cur_children [ 'folders' ] : if inner_folder [ 'name' ] == path_part : base_folder = session . communicator . folder_get ( session . token , inner_folder [ 'folder_id' ] ) cur_folder_id = base_folder [ 'folder_id' ] break else : return - 1 return cur_folder_id
825	def expValue ( self , pred ) : if len ( pred ) == 1 : return pred . keys ( ) [ 0 ] return sum ( [ x * p for x , p in pred . items ( ) ] )
3432	def add_groups ( self , group_list ) : def existing_filter ( group ) : if group . id in self . groups : LOGGER . warning ( "Ignoring group '%s' since it already exists." , group . id ) return False return True if isinstance ( group_list , string_types ) or hasattr ( group_list , "id" ) : warn ( "need to pass in a list" ) group_list = [ group_list ] pruned = DictList ( filter ( existing_filter , group_list ) ) for group in pruned : group . _model = self for member in group . members : # If the member is not associated with the model, add it if isinstance ( member , Metabolite ) : if member not in self . metabolites : self . add_metabolites ( [ member ] ) if isinstance ( member , Reaction ) : if member not in self . reactions : self . add_reactions ( [ member ] ) # TODO(midnighter): `add_genes` method does not exist. # if isinstance(member, Gene): # if member not in self.genes: # self.add_genes([member]) self . groups += [ group ]
2019	def SMOD ( self , a , b ) : s0 , s1 = to_signed ( a ) , to_signed ( b ) sign = Operators . ITEBV ( 256 , s0 < 0 , - 1 , 1 ) try : result = ( Operators . ABS ( s0 ) % Operators . ABS ( s1 ) ) * sign except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , s1 == 0 , 0 , result )
2352	def root ( self ) : if self . _root is None and self . _root_locator is not None : return self . page . find_element ( * self . _root_locator ) return self . _root
11240	def copy_web_file_to_local ( file_path , target_path ) : response = urllib . request . urlopen ( file_path ) f = open ( target_path , 'w' ) f . write ( response . read ( ) ) f . close ( )
7763	def _move_session_handler ( handlers ) : index = 0 for i , handler in enumerate ( handlers ) : if isinstance ( handler , SessionHandler ) : index = i break if index : handlers [ : index + 1 ] = [ handlers [ index ] ] + handlers [ : index ]
12078	def save ( self , callit = "misc" , closeToo = True , fullpath = False ) : if fullpath is False : fname = self . abf . outPre + "plot_" + callit + ".jpg" else : fname = callit if not os . path . exists ( os . path . dirname ( fname ) ) : os . mkdir ( os . path . dirname ( fname ) ) plt . savefig ( fname ) self . log . info ( "saved [%s]" , os . path . basename ( fname ) ) if closeToo : plt . close ( )
5043	def send_messages ( cls , http_request , message_requests ) : deduplicated_messages = set ( message_requests ) for msg_type , text in deduplicated_messages : message_function = getattr ( messages , msg_type ) message_function ( http_request , text )
7293	def get_form ( self ) : self . set_fields ( ) if self . post_data_dict is not None : self . set_post_data ( ) return self . form
11404	def filter_field_instances ( field_instances , filter_subcode , filter_value , filter_mode = 'e' ) : matched = [ ] if filter_mode == 'e' : to_match = ( filter_subcode , filter_value ) for instance in field_instances : if to_match in instance [ 0 ] : matched . append ( instance ) elif filter_mode == 's' : for instance in field_instances : for subfield in instance [ 0 ] : if subfield [ 0 ] == filter_subcode and subfield [ 1 ] . find ( filter_value ) > - 1 : matched . append ( instance ) break elif filter_mode == 'r' : reg_exp = re . compile ( filter_value ) for instance in field_instances : for subfield in instance [ 0 ] : if subfield [ 0 ] == filter_subcode and reg_exp . match ( subfield [ 1 ] ) is not None : matched . append ( instance ) break return matched
3949	def execute ( self , using = None ) : if not using : using = self . get_connection ( ) inserted_entities = { } for klass in self . orders : number = self . quantities [ klass ] if klass not in inserted_entities : inserted_entities [ klass ] = [ ] for i in range ( 0 , number ) : entity = self . entities [ klass ] . execute ( using , inserted_entities ) inserted_entities [ klass ] . append ( entity ) return inserted_entities
6434	def dist_eudex ( src , tar , weights = 'exponential' , max_length = 8 ) : return Eudex ( ) . dist ( src , tar , weights , max_length )
9639	def emit ( self , record ) : try : if self . max_messages : p = self . redis_client . pipeline ( ) p . rpush ( self . key , self . format ( record ) ) p . ltrim ( self . key , - self . max_messages , - 1 ) p . execute ( ) else : self . redis_client . rpush ( self . key , self . format ( record ) ) except redis . RedisError : pass
7210	def stdout ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stdout.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stdout." ) wf = self . workflow . get ( self . id ) stdout_list = [ ] for task in wf [ 'tasks' ] : stdout_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stdout' : self . workflow . get_stdout ( self . id , task [ 'id' ] ) } ) return stdout_list
4260	def load_exif ( album ) : if not hasattr ( album . gallery , "exifCache" ) : _restore_cache ( album . gallery ) cache = album . gallery . exifCache for media in album . medias : if media . type == "image" : key = os . path . join ( media . path , media . filename ) if key in cache : media . exif = cache [ key ]
4430	async def _remove ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'Nothing queued.' ) if index > len ( player . queue ) or index < 1 : return await ctx . send ( f'Index has to be **between** 1 and {len(player.queue)}' ) index -= 1 removed = player . queue . pop ( index ) await ctx . send ( f'Removed **{removed.title}** from the queue.' )
9065	def value ( self ) : if not self . _fix [ "beta" ] : self . _update_beta ( ) if not self . _fix [ "scale" ] : self . _update_scale ( ) return self . lml ( )
2968	def _sm_relieve_pain ( self , * args , * * kwargs ) : _logger . info ( "Ending the degradation for blockade %s" % self . _blockade_name ) self . _do_reset_all ( ) # set a timer for the next pain event millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
12776	def forward_dynamics ( self , torques , start = 0 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , torque in enumerate ( torques ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) self . skeleton . add_torques ( torque ) self . ode_world . step ( self . dt ) yield self . ode_contactgroup . empty ( )
685	def getTotalw ( self ) : w = sum ( [ field . w for field in self . fields ] ) return w
3639	def squad ( self , squad_id = 0 , persona_id = None ) : method = 'GET' url = 'squad/%s/user/%s' % ( squad_id , persona_id or self . persona_id ) # pinEvents events = [ self . pin . event ( 'page_view' , 'Hub - Squads' ) ] self . pin . send ( events ) # TODO: ability to return other info than players only rc = self . __request__ ( method , url ) # pinEvents events = [ self . pin . event ( 'page_view' , 'Squad Details' ) , self . pin . event ( 'page_view' , 'Squads - Squad Overview' ) ] self . pin . send ( events ) return [ itemParse ( i ) for i in rc . get ( 'players' , ( ) ) ]
418	def delete_datasets ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) self . db . Dataset . delete_many ( kwargs ) logging . info ( "[Database] Delete Dataset SUCCESS" )
6211	def fit ( self , trX , trY , batch_size = 64 , n_epochs = 1 , len_filter = LenFilter ( ) , snapshot_freq = 1 , path = None ) : if len_filter is not None : trX , trY = len_filter . filter ( trX , trY ) trY = standardize_targets ( trY , cost = self . cost ) n = 0. t = time ( ) costs = [ ] for e in range ( n_epochs ) : epoch_costs = [ ] for xmb , ymb in self . iterator . iterXY ( trX , trY ) : c = self . _train ( xmb , ymb ) epoch_costs . append ( c ) n += len ( ymb ) if self . verbose >= 2 : n_per_sec = n / ( time ( ) - t ) n_left = len ( trY ) - n % len ( trY ) time_left = n_left / n_per_sec sys . stdout . write ( "\rEpoch %d Seen %d samples Avg cost %0.4f Time left %d seconds" % ( e , n , np . mean ( epoch_costs [ - 250 : ] ) , time_left ) ) sys . stdout . flush ( ) costs . extend ( epoch_costs ) status = "Epoch %d Seen %d samples Avg cost %0.4f Time elapsed %d seconds" % ( e , n , np . mean ( epoch_costs [ - 250 : ] ) , time ( ) - t ) if self . verbose >= 2 : sys . stdout . write ( "\r" + status ) sys . stdout . flush ( ) sys . stdout . write ( "\n" ) elif self . verbose == 1 : print ( status ) if path and e % snapshot_freq == 0 : save ( self , "{0}.{1}" . format ( path , e ) ) return costs
1402	def setTopologyInfo ( self , topology ) : # Execution state is the most basic info. # If there is no execution state, just return # as the rest of the things don't matter. if not topology . execution_state : Log . info ( "No execution state found for: " + topology . name ) return Log . info ( "Setting topology info for topology: " + topology . name ) has_physical_plan = True if not topology . physical_plan : has_physical_plan = False Log . info ( "Setting topology info for topology: " + topology . name ) has_packing_plan = True if not topology . packing_plan : has_packing_plan = False has_tmaster_location = True if not topology . tmaster : has_tmaster_location = False has_scheduler_location = True if not topology . scheduler_location : has_scheduler_location = False topologyInfo = { "name" : topology . name , "id" : topology . id , "logical_plan" : None , "physical_plan" : None , "packing_plan" : None , "execution_state" : None , "tmaster_location" : None , "scheduler_location" : None , } executionState = self . extract_execution_state ( topology ) executionState [ "has_physical_plan" ] = has_physical_plan executionState [ "has_packing_plan" ] = has_packing_plan executionState [ "has_tmaster_location" ] = has_tmaster_location executionState [ "has_scheduler_location" ] = has_scheduler_location executionState [ "status" ] = topology . get_status ( ) topologyInfo [ "metadata" ] = self . extract_metadata ( topology ) topologyInfo [ "runtime_state" ] = self . extract_runtime_state ( topology ) topologyInfo [ "execution_state" ] = executionState topologyInfo [ "logical_plan" ] = self . extract_logical_plan ( topology ) topologyInfo [ "physical_plan" ] = self . extract_physical_plan ( topology ) topologyInfo [ "packing_plan" ] = self . extract_packing_plan ( topology ) topologyInfo [ "tmaster_location" ] = self . extract_tmaster ( topology ) topologyInfo [ "scheduler_location" ] = self . extract_scheduler_location ( topology ) self . topologyInfos [ ( topology . name , topology . state_manager_name ) ] = topologyInfo
2366	def walk ( self , * types ) : requested = types if len ( types ) > 0 else [ SuiteFile , ResourceFile , SuiteFolder , Testcase , Keyword ] for thing in self . robot_files : if thing . __class__ in requested : yield thing if isinstance ( thing , SuiteFolder ) : for child in thing . walk ( ) : if child . __class__ in requested : yield child else : for child in thing . walk ( * types ) : yield child
11643	def transform ( self , X ) : X = check_array ( X ) X_rbf = np . empty_like ( X ) if self . copy else X X_in = X if not self . squared : np . power ( X_in , 2 , out = X_rbf ) X_in = X_rbf if self . scale_by_median : scale = self . median_ if self . squared else self . median_ ** 2 gamma = self . gamma * scale else : gamma = self . gamma np . multiply ( X_in , - gamma , out = X_rbf ) np . exp ( X_rbf , out = X_rbf ) return X_rbf
13741	def cached_httpbl_exempt ( view_func ) : # We could just do view_func.cached_httpbl_exempt = True, but decorators # are nicer if they don't have side-effects, so we return a new # function. def wrapped_view ( * args , * * kwargs ) : return view_func ( * args , * * kwargs ) wrapped_view . cached_httpbl_exempt = True return wraps ( view_func , assigned = available_attrs ( view_func ) ) ( wrapped_view )
2160	def _format_id ( self , payload ) : if 'id' in payload : return str ( payload [ 'id' ] ) if 'results' in payload : return ' ' . join ( [ six . text_type ( item [ 'id' ] ) for item in payload [ 'results' ] ] ) raise MultipleRelatedError ( 'Could not serialize output with id format.' )
20	def pretty_eta ( seconds_left ) : minutes_left = seconds_left // 60 seconds_left %= 60 hours_left = minutes_left // 60 minutes_left %= 60 days_left = hours_left // 24 hours_left %= 24 def helper ( cnt , name ) : return "{} {}{}" . format ( str ( cnt ) , name , ( 's' if cnt > 1 else '' ) ) if days_left > 0 : msg = helper ( days_left , 'day' ) if hours_left > 0 : msg += ' and ' + helper ( hours_left , 'hour' ) return msg if hours_left > 0 : msg = helper ( hours_left , 'hour' ) if minutes_left > 0 : msg += ' and ' + helper ( minutes_left , 'minute' ) return msg if minutes_left > 0 : return helper ( minutes_left , 'minute' ) return 'less than a minute'
12349	def create ( self , name , region , size , image , ssh_keys = None , backups = None , ipv6 = None , private_networking = None , wait = True ) : if ssh_keys and not isinstance ( ssh_keys , ( list , tuple ) ) : raise TypeError ( "ssh_keys must be a list" ) resp = self . post ( name = name , region = region , size = size , image = image , ssh_keys = ssh_keys , private_networking = private_networking , backups = backups , ipv6 = ipv6 ) droplet = self . get ( resp [ self . singular ] [ 'id' ] ) if wait : droplet . wait ( ) # HACK sometimes the IP address doesn't return correctly droplet = self . get ( resp [ self . singular ] [ 'id' ] ) return droplet
2964	def insert_rule ( self , chain , src = None , dest = None , target = None ) : if not chain : raise ValueError ( "Invalid chain" ) if not target : raise ValueError ( "Invalid target" ) if not ( src or dest ) : raise ValueError ( "Need src, dest, or both" ) args = [ "-I" , chain ] if src : args += [ "-s" , src ] if dest : args += [ "-d" , dest ] args += [ "-j" , target ] self . call ( * args )
2214	def repr2 ( data , * * kwargs ) : custom_extensions = kwargs . get ( 'extensions' , None ) _return_info = kwargs . get ( '_return_info' , False ) kwargs [ '_root_info' ] = _rectify_root_info ( kwargs . get ( '_root_info' , None ) ) outstr = None _leaf_info = None if custom_extensions : func = custom_extensions . lookup ( data ) if func is not None : outstr = func ( data , * * kwargs ) if outstr is None : if isinstance ( data , dict ) : outstr , _leaf_info = _format_dict ( data , * * kwargs ) elif isinstance ( data , ( list , tuple , set , frozenset ) ) : outstr , _leaf_info = _format_list ( data , * * kwargs ) if outstr is None : # check any globally registered functions for special formatters func = _FORMATTER_EXTENSIONS . lookup ( data ) if func is not None : outstr = func ( data , * * kwargs ) else : outstr = _format_object ( data , * * kwargs ) if _return_info : _leaf_info = _rectify_leaf_info ( _leaf_info ) return outstr , _leaf_info else : return outstr
5000	def require_at_least_one_query_parameter ( * query_parameter_names ) : def outer_wrapper ( view ) : """ Allow the passing of parameters to require_at_least_one_query_parameter. """ @ wraps ( view ) def wrapper ( request , * args , * * kwargs ) : """ Checks for the existence of the specified query parameters, raises a ValidationError if none of them were included in the request. """ requirement_satisfied = False for query_parameter_name in query_parameter_names : query_parameter_values = request . query_params . getlist ( query_parameter_name ) kwargs [ query_parameter_name ] = query_parameter_values if query_parameter_values : requirement_satisfied = True if not requirement_satisfied : raise ValidationError ( detail = 'You must provide at least one of the following query parameters: {params}.' . format ( params = ', ' . join ( query_parameter_names ) ) ) return view ( request , * args , * * kwargs ) return wrapper return outer_wrapper
6068	def tabulate_integral ( self , grid , tabulate_bins ) : eta_min = 1.0e-4 eta_max = 1.05 * np . max ( self . grid_to_elliptical_radii ( grid ) ) minimum_log_eta = np . log10 ( eta_min ) maximum_log_eta = np . log10 ( eta_max ) bin_size = ( maximum_log_eta - minimum_log_eta ) / ( tabulate_bins - 1 ) return eta_min , eta_max , minimum_log_eta , maximum_log_eta , bin_size
11044	def create_marathon_acme ( client_creator , cert_store , acme_email , allow_multiple_certs , marathon_addrs , marathon_timeout , sse_timeout , mlb_addrs , group , reactor ) : marathon_client = MarathonClient ( marathon_addrs , timeout = marathon_timeout , sse_kwargs = { 'timeout' : sse_timeout } , reactor = reactor ) marathon_lb_client = MarathonLbClient ( mlb_addrs , reactor = reactor ) return MarathonAcme ( marathon_client , group , cert_store , marathon_lb_client , client_creator , reactor , acme_email , allow_multiple_certs )
11839	def set_board ( self , board = None ) : if board is None : board = random_boggle ( ) self . board = board self . neighbors = boggle_neighbors ( len ( board ) ) self . found = { } for i in range ( len ( board ) ) : lo , hi = self . wordlist . bounds [ board [ i ] ] self . find ( lo , hi , i , [ ] , '' ) return self
5078	def get_current_course_run ( course , users_active_course_runs ) : current_course_run = None filtered_course_runs = [ ] all_course_runs = course [ 'course_runs' ] if users_active_course_runs : current_course_run = get_closest_course_run ( users_active_course_runs ) else : for course_run in all_course_runs : if is_course_run_enrollable ( course_run ) and is_course_run_upgradeable ( course_run ) : filtered_course_runs . append ( course_run ) if not filtered_course_runs : # Consider all runs if there were not any enrollable/upgradeable ones. filtered_course_runs = all_course_runs if filtered_course_runs : current_course_run = get_closest_course_run ( filtered_course_runs ) return current_course_run
6153	def fir_remez_bsf ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = bandstop_order ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , d_pass , d_stop , fsamp = fs ) # Bump up the order by N_bump to bring down the final d_pass & d_stop # Initially make sure the number of taps is even so N_bump needs to be odd if np . mod ( n , 2 ) != 0 : n += 1 N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 , maxiter = 25 , grid_density = 16 ) print ( 'N_bump must be odd to maintain odd filter length' ) print ( 'Remez filter taps = %d.' % N_taps ) return b
8719	def backup ( self , path ) : log . info ( 'Backing up in ' + path ) # List file to backup files = self . file_list ( ) # then download each of then self . prepare ( ) for f in files : self . read_file ( f [ 0 ] , os . path . join ( path , f [ 0 ] ) )
8833	def soft_equals ( a , b ) : if isinstance ( a , str ) or isinstance ( b , str ) : return str ( a ) == str ( b ) if isinstance ( a , bool ) or isinstance ( b , bool ) : return bool ( a ) is bool ( b ) return a == b
6329	def gng_importer ( self , corpus_file ) : with c_open ( corpus_file , 'r' , encoding = 'utf-8' ) as gng : for line in gng : line = line . rstrip ( ) . split ( '\t' ) words = line [ 0 ] . split ( ) self . _add_to_ngcorpus ( self . ngcorpus , words , int ( line [ 2 ] ) )
9610	def execute ( self , command , data = { } ) : method , uri = command try : path = self . _formatter . format_map ( uri , data ) body = self . _formatter . get_unused_kwargs ( ) url = "{0}{1}" . format ( self . _url , path ) return self . _request ( method , url , body ) except KeyError as err : LOGGER . debug ( 'Endpoint {0} is missing argument {1}' . format ( uri , err ) ) raise
239	def create_position_tear_sheet ( returns , positions , show_and_plot_top_pos = 2 , hide_positions = False , return_fig = False , sector_mappings = None , transactions = None , estimate_intraday = 'infer' ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) if hide_positions : show_and_plot_top_pos = 0 vertical_sections = 7 if sector_mappings is not None else 6 fig = plt . figure ( figsize = ( 14 , vertical_sections * 6 ) ) gs = gridspec . GridSpec ( vertical_sections , 3 , wspace = 0.5 , hspace = 0.5 ) ax_exposures = plt . subplot ( gs [ 0 , : ] ) ax_top_positions = plt . subplot ( gs [ 1 , : ] , sharex = ax_exposures ) ax_max_median_pos = plt . subplot ( gs [ 2 , : ] , sharex = ax_exposures ) ax_holdings = plt . subplot ( gs [ 3 , : ] , sharex = ax_exposures ) ax_long_short_holdings = plt . subplot ( gs [ 4 , : ] ) ax_gross_leverage = plt . subplot ( gs [ 5 , : ] , sharex = ax_exposures ) positions_alloc = pos . get_percent_alloc ( positions ) plotting . plot_exposures ( returns , positions , ax = ax_exposures ) plotting . show_and_plot_top_positions ( returns , positions_alloc , show_and_plot = show_and_plot_top_pos , hide_positions = hide_positions , ax = ax_top_positions ) plotting . plot_max_median_position_concentration ( positions , ax = ax_max_median_pos ) plotting . plot_holdings ( returns , positions_alloc , ax = ax_holdings ) plotting . plot_long_short_holdings ( returns , positions_alloc , ax = ax_long_short_holdings ) plotting . plot_gross_leverage ( returns , positions , ax = ax_gross_leverage ) if sector_mappings is not None : sector_exposures = pos . get_sector_exposures ( positions , sector_mappings ) if len ( sector_exposures . columns ) > 1 : sector_alloc = pos . get_percent_alloc ( sector_exposures ) sector_alloc = sector_alloc . drop ( 'cash' , axis = 'columns' ) ax_sector_alloc = plt . subplot ( gs [ 6 , : ] , sharex = ax_exposures ) plotting . plot_sector_allocations ( returns , sector_alloc , ax = ax_sector_alloc ) for ax in fig . axes : plt . setp ( ax . get_xticklabels ( ) , visible = True ) if return_fig : return fig
8094	def edges ( s , edges , alpha = 1.0 , weighted = False , directed = False ) : p = s . _ctx . BezierPath ( ) if directed and s . stroke : pd = s . _ctx . BezierPath ( ) if weighted and s . fill : pw = [ s . _ctx . BezierPath ( ) for i in range ( 11 ) ] # Draw the edges in a single BezierPath for speed. # Weighted edges are divided into ten BezierPaths, # depending on their weight rounded between 0 and 10. if len ( edges ) == 0 : return for e in edges : try : s2 = e . node1 . graph . styles [ e . node1 . style ] except : s2 = s if s2 . edge : s2 . edge ( s2 , p , e , alpha ) if directed and s . stroke : s2 . edge_arrow ( s2 , pd , e , radius = 10 ) if weighted and s . fill : s2 . edge ( s2 , pw [ int ( e . weight * 10 ) ] , e , alpha ) s . _ctx . autoclosepath ( False ) s . _ctx . nofill ( ) s . _ctx . nostroke ( ) # All weighted edges use the default fill. if weighted and s . fill : r = e . node1 . __class__ ( None ) . r s . _ctx . stroke ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * 0.65 * alpha ) for w in range ( 1 , len ( pw ) ) : s . _ctx . strokewidth ( r * w * 0.1 ) s . _ctx . drawpath ( pw [ w ] . copy ( ) ) # All edges use the default stroke. if s . stroke : s . _ctx . strokewidth ( s . strokewidth ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * 0.65 * alpha ) s . _ctx . drawpath ( p . copy ( ) ) if directed and s . stroke : #clr = s._ctx.stroke().copy() clr = s . _ctx . color ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * 0.65 * alpha ) clr . a *= 1.3 s . _ctx . stroke ( clr ) s . _ctx . drawpath ( pd . copy ( ) ) for e in edges : try : s2 = self . styles [ e . node1 . style ] except : s2 = s if s2 . edge_label : s2 . edge_label ( s2 , e , alpha )
328	def extract_interesting_date_ranges ( returns ) : returns_dupe = returns . copy ( ) returns_dupe . index = returns_dupe . index . map ( pd . Timestamp ) ranges = OrderedDict ( ) for name , ( start , end ) in PERIODS . items ( ) : try : period = returns_dupe . loc [ start : end ] if len ( period ) == 0 : continue ranges [ name ] = period except BaseException : continue return ranges
7441	def set_params ( self , param , newvalue ) : ## this includes current params and some legacy params for conversion legacy_params = [ "edit_cutsites" , "trim_overhang" ] current_params = self . paramsdict . keys ( ) allowed_params = current_params + legacy_params ## require parameter recognition #if not ((param in range(50)) or \ # (param in [str(i) for i in range(50)]) or \ # (param in allowed_params)): if not param in allowed_params : raise IPyradParamsError ( "Parameter key not recognized: {}" . format ( param ) ) ## make string param = str ( param ) ## get index if param is keyword arg (this index is now zero based!) if len ( param ) < 3 : param = self . paramsdict . keys ( ) [ int ( param ) ] ## run assertions on new param try : self = _paramschecker ( self , param , newvalue ) except Exception as inst : raise IPyradWarningExit ( BAD_PARAMETER . format ( param , inst , newvalue ) )
6669	def task_or_dryrun ( * args , * * kwargs ) : invoked = bool ( not args or kwargs ) task_class = kwargs . pop ( "task_class" , WrappedCallableTask ) # if invoked: # func, args = args[0], () # else: func , args = args [ 0 ] , ( ) def wrapper ( func ) : return task_class ( func , * args , * * kwargs ) wrapper . is_task_or_dryrun = True wrapper . wrapped = func return wrapper if invoked else wrapper ( func )
2617	def read_state_file ( self , state_file ) : try : fh = open ( state_file , 'r' ) state = json . load ( fh ) self . vpc_id = state [ 'vpcID' ] self . sg_id = state [ 'sgID' ] self . sn_ids = state [ 'snIDs' ] self . instances = state [ 'instances' ] except Exception as e : logger . debug ( "Caught exception while reading state file: {0}" . format ( e ) ) raise e logger . debug ( "Done reading state from the local state file." )
2589	def shutdown ( self , block = False ) : x = self . executor . shutdown ( wait = block ) logger . debug ( "Done with executor shutdown" ) return x
4822	def get_course_details ( self , course_id ) : try : return self . client . course ( course_id ) . get ( ) except ( SlumberBaseException , ConnectionError , Timeout ) as exc : LOGGER . exception ( 'Failed to retrieve course enrollment details for course [%s] due to: [%s]' , course_id , str ( exc ) ) return { }
4078	def write_py2k_header ( file_list ) : if not isinstance ( file_list , list ) : file_list = [ file_list ] python_re = re . compile ( br"^(#!.*\bpython)(.*)([\r\n]+)$" ) coding_re = re . compile ( br"coding[:=]\s*([-\w.]+)" ) new_line_re = re . compile ( br"([\r\n]+)$" ) version_3 = LooseVersion ( '3' ) for file in file_list : if not os . path . getsize ( file ) : continue rewrite_needed = False python_found = False coding_found = False lines = [ ] f = open ( file , 'rb' ) try : while len ( lines ) < 2 : line = f . readline ( ) match = python_re . match ( line ) if match : python_found = True version = LooseVersion ( match . group ( 2 ) . decode ( ) or '2' ) try : version_test = version >= version_3 except TypeError : version_test = True if version_test : line = python_re . sub ( br"\g<1>2\g<3>" , line ) rewrite_needed = True elif coding_re . search ( line ) : coding_found = True lines . append ( line ) if not coding_found : match = new_line_re . search ( lines [ 0 ] ) newline = match . group ( 1 ) if match else b"\n" line = b"# -*- coding: utf-8 -*-" + newline lines . insert ( 1 if python_found else 0 , line ) rewrite_needed = True if rewrite_needed : lines += f . readlines ( ) finally : f . close ( ) if rewrite_needed : f = open ( file , 'wb' ) try : f . writelines ( lines ) finally : f . close ( )
3142	def create ( self , data ) : if 'name' not in data : raise KeyError ( 'The file must have a name' ) if 'file_data' not in data : raise KeyError ( 'The file must have file_data' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . file_id = response [ 'id' ] else : self . file_id = None return response
1669	def FlagCxx14Features ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] include = Match ( r'\s*#\s*include\s+[<"]([^<"]+)[">]' , line ) # Flag unapproved C++14 headers. if include and include . group ( 1 ) in ( 'scoped_allocator' , 'shared_mutex' ) : error ( filename , linenum , 'build/c++14' , 5 , ( '<%s> is an unapproved C++14 header.' ) % include . group ( 1 ) )
6959	def query_radecl ( ra , decl , filtersystem = 'sloan_2mass' , field_deg2 = 1.0 , usebinaries = True , extinction_sigma = 0.1 , magnitude_limit = 26.0 , maglim_filtercol = 4 , trilegal_version = 1.6 , extraparams = None , forcefetch = False , cachedir = '~/.astrobase/trilegal-cache' , verbose = True , timeout = 60.0 , refresh = 150.0 , maxtimeout = 700.0 ) : # convert the ra/decl to gl, gb radecl = SkyCoord ( ra = ra * u . degree , dec = decl * u . degree ) gl = radecl . galactic . l . degree gb = radecl . galactic . b . degree return query_galcoords ( gl , gb , filtersystem = filtersystem , field_deg2 = field_deg2 , usebinaries = usebinaries , extinction_sigma = extinction_sigma , magnitude_limit = magnitude_limit , maglim_filtercol = maglim_filtercol , trilegal_version = trilegal_version , extraparams = extraparams , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout )
8546	def delete_datacenter ( self , datacenter_id ) : response = self . _perform_request ( url = '/datacenters/%s' % ( datacenter_id ) , method = 'DELETE' ) return response
7933	def _connect ( self , server = None , port = None ) : if self . me . node or self . me . resource : raise Value ( "Component JID may have only domain defined" ) if not server : server = self . server if not port : port = self . port if not server or not port : raise ValueError ( "Server or port not given" ) Stream . _connect ( self , server , port , None , self . me )
6435	def sim_eudex ( src , tar , weights = 'exponential' , max_length = 8 ) : return Eudex ( ) . sim ( src , tar , weights , max_length )
4034	def ib64_patched ( self , attrsD , contentparams ) : if attrsD . get ( "mode" , "" ) == "base64" : return 0 if self . contentparams [ "type" ] . startswith ( "text/" ) : return 0 if self . contentparams [ "type" ] . endswith ( "+xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/json" ) : return 0 return 0
11641	def yaml_get_data ( filename ) : with open ( filename , 'rb' ) as fd : yaml_data = yaml . load ( fd ) return yaml_data return False
4071	def split_multiline ( value ) : return [ element for element in ( line . strip ( ) for line in value . split ( '\n' ) ) if element ]
10994	def randomize_parameters ( self , ptp = 0.2 , fourier = False , vmin = None , vmax = None ) : if vmin is not None and vmax is not None : ptp = vmax - vmin elif vmax is not None and vmin is None : vmin = vmax - ptp elif vmin is not None and vmax is None : vmax = vmin + ptp else : vmax = 1.0 vmin = vmax - ptp self . set_values ( self . category + '-scale' , 1.0 ) self . set_values ( self . category + '-off' , 0.0 ) for k , v in iteritems ( self . poly_params ) : norm = ( self . zorder + 1.0 ) * 2 self . set_values ( k , ptp * ( np . random . rand ( ) - 0.5 ) / norm ) for i , p in enumerate ( self . barnes_params ) : N = len ( p ) if fourier : t = ( ( np . random . rand ( N ) - 0.5 ) + 1.j * ( np . random . rand ( N ) - 0.5 ) ) / ( np . arange ( N ) + 1 ) q = np . real ( np . fft . ifftn ( t ) ) / ( i + 1 ) else : t = ptp * np . sqrt ( N ) * ( np . random . rand ( N ) - 0.5 ) q = np . cumsum ( t ) / ( i + 1 ) q = ptp * q / q . ptp ( ) / len ( self . barnes_params ) q -= q . mean ( ) self . set_values ( p , q ) self . _norm_stat = [ ptp , vmin ] if self . shape : self . initialize ( ) if self . _parent : param = self . category + '-scale' self . trigger_update ( param , self . get_values ( param ) )
1113	def _split_line ( self , data_list , line_num , text ) : # if blank line or context separator, just add it to the output list if not line_num : data_list . append ( ( line_num , text ) ) return # if line text doesn't need wrapping, just add it to the output list size = len ( text ) max = self . _wrapcolumn if ( size <= max ) or ( ( size - ( text . count ( '\0' ) * 3 ) ) <= max ) : data_list . append ( ( line_num , text ) ) return # scan text looking for the wrap point, keeping track if the wrap # point is inside markers i = 0 n = 0 mark = '' while n < max and i < size : if text [ i ] == '\0' : i += 1 mark = text [ i ] i += 1 elif text [ i ] == '\1' : i += 1 mark = '' else : i += 1 n += 1 # wrap point is inside text, break it up into separate lines line1 = text [ : i ] line2 = text [ i : ] # if wrap point is inside markers, place end marker at end of first # line and start marker at beginning of second line because each # line will have its own table tag markup around it. if mark : line1 = line1 + '\1' line2 = '\0' + mark + line2 # tack on first line onto the output list data_list . append ( ( line_num , line1 ) ) # use this routine again to wrap the remaining text self . _split_line ( data_list , '>' , line2 )
12436	def traverse ( cls , request , params = None ) : # Attempt to parse the path using a pattern. result = cls . parse ( request . path ) if result is None : # No parsing was requested; no-op. return cls , { } elif not result : # Parsing failed; raise 404. raise http . exceptions . NotFound ( ) # Partition out the result. resource , data , rest = result if params : # Append params to data. data . update ( params ) if resource is None : # No traversal; return parameters. return cls , data # Modify the path appropriately. if data . get ( 'path' ) is not None : request . path = data . pop ( 'path' ) elif rest is not None : request . path = rest # Send us through traversal again. result = resource . traverse ( request , params = data ) return result
9094	def add_namespace_to_graph ( self , graph : BELGraph ) -> Namespace : namespace = self . upload_bel_namespace ( ) graph . namespace_url [ namespace . keyword ] = namespace . url # Add this manager as an annotation, too self . _add_annotation_to_graph ( graph ) return namespace
10603	def calculate ( self , * * state ) : T = state [ 'T' ] y_C = state [ 'y_C' ] y_H = state [ 'y_H' ] y_O = state [ 'y_O' ] y_N = state [ 'y_N' ] y_S = state [ 'y_S' ] a = self . _calc_a ( y_C , y_H , y_O , y_N , y_S ) / 1000 # kg/mol result = ( R / a ) * ( 380 * self . _calc_g0 ( 380 / T ) + 3600 * self . _calc_g0 ( 1800 / T ) ) return result
245	def get_low_liquidity_transactions ( transactions , market_data , last_n_days = None ) : txn_daily_w_bar = daily_txns_with_bar_data ( transactions , market_data ) txn_daily_w_bar . index . name = 'date' txn_daily_w_bar = txn_daily_w_bar . reset_index ( ) if last_n_days is not None : md = txn_daily_w_bar . date . max ( ) - pd . Timedelta ( days = last_n_days ) txn_daily_w_bar = txn_daily_w_bar [ txn_daily_w_bar . date > md ] bar_consumption = txn_daily_w_bar . assign ( max_pct_bar_consumed = ( txn_daily_w_bar . amount / txn_daily_w_bar . volume ) * 100 ) . sort_values ( 'max_pct_bar_consumed' , ascending = False ) max_bar_consumption = bar_consumption . groupby ( 'symbol' ) . first ( ) return max_bar_consumption [ [ 'date' , 'max_pct_bar_consumed' ] ]
456	def ternary_operation ( x ) : g = tf . get_default_graph ( ) with g . gradient_override_map ( { "Sign" : "Identity" } ) : threshold = _compute_threshold ( x ) x = tf . sign ( tf . add ( tf . sign ( tf . add ( x , threshold ) ) , tf . sign ( tf . add ( x , - threshold ) ) ) ) return x
1407	def emit ( self , tup , tup_id = None , stream = Stream . DEFAULT_STREAM_ID , direct_task = None , need_task_ids = False ) : # first check whether this tuple is sane self . pplan_helper . check_output_schema ( stream , tup ) # get custom grouping target task ids; get empty list if not custom grouping custom_target_task_ids = self . pplan_helper . choose_tasks_for_custom_grouping ( stream , tup ) self . pplan_helper . context . invoke_hook_emit ( tup , stream , None ) data_tuple = tuple_pb2 . HeronDataTuple ( ) data_tuple . key = 0 if direct_task is not None : if not isinstance ( direct_task , int ) : raise TypeError ( "direct_task argument needs to be an integer, given: %s" % str ( type ( direct_task ) ) ) # performing emit-direct data_tuple . dest_task_ids . append ( direct_task ) elif custom_target_task_ids is not None : # for custom grouping for task_id in custom_target_task_ids : data_tuple . dest_task_ids . append ( task_id ) if tup_id is not None : tuple_info = TupleHelper . make_root_tuple_info ( stream , tup_id ) if self . acking_enabled : # this message is rooted root = data_tuple . roots . add ( ) root . taskid = self . pplan_helper . my_task_id root . key = tuple_info . key self . in_flight_tuples [ tuple_info . key ] = tuple_info else : self . immediate_acks . append ( tuple_info ) tuple_size_in_bytes = 0 start_time = time . time ( ) # Serialize for obj in tup : serialized = self . serializer . serialize ( obj ) data_tuple . values . append ( serialized ) tuple_size_in_bytes += len ( serialized ) serialize_latency_ns = ( time . time ( ) - start_time ) * system_constants . SEC_TO_NS self . spout_metrics . serialize_data_tuple ( stream , serialize_latency_ns ) super ( SpoutInstance , self ) . admit_data_tuple ( stream_id = stream , data_tuple = data_tuple , tuple_size_in_bytes = tuple_size_in_bytes ) self . total_tuples_emitted += 1 self . spout_metrics . update_emit_count ( stream ) if need_task_ids : sent_task_ids = custom_target_task_ids or [ ] if direct_task is not None : sent_task_ids . append ( direct_task ) return sent_task_ids
4502	def clear ( self ) : self . _desc = { } for key , value in merge . DEFAULT_PROJECT . items ( ) : if key not in self . _HIDDEN : self . _desc [ key ] = type ( value ) ( )
13323	def format_objects ( objects , children = False , columns = None , header = True ) : columns = columns or ( 'NAME' , 'TYPE' , 'PATH' ) objects = sorted ( objects , key = _type_and_name ) data = [ ] for obj in objects : if isinstance ( obj , cpenv . VirtualEnvironment ) : data . append ( get_info ( obj ) ) modules = obj . get_modules ( ) if children and modules : for mod in modules : data . append ( get_info ( mod , indent = 2 , root = obj . path ) ) else : data . append ( get_info ( obj ) ) maxes = [ len ( max ( col , key = len ) ) for col in zip ( * data ) ] tmpl = '{:%d} {:%d} {:%d}' % tuple ( maxes ) lines = [ ] if header : lines . append ( '\n' + bold_blue ( tmpl . format ( * columns ) ) ) for obj_data in data : lines . append ( tmpl . format ( * obj_data ) ) return '\n' . join ( lines )
9118	def _add_admin ( self , app , * * kwargs ) : from flask_admin import Admin from flask_admin . contrib . sqla import ModelView admin = Admin ( app , * * kwargs ) for flask_admin_model in self . flask_admin_models : if isinstance ( flask_admin_model , tuple ) : # assume its a 2 tuple if len ( flask_admin_model ) != 2 : raise TypeError model , view = flask_admin_model admin . add_view ( view ( model , self . session ) ) else : admin . add_view ( ModelView ( flask_admin_model , self . session ) ) return admin
12001	def _unsign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = '' if algorithm [ 'salt_size' ] : key_salt = data [ - algorithm [ 'salt_size' ] : ] data = data [ : - algorithm [ 'salt_size' ] ] key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _decode ( data , algorithm , key ) return data
8746	def get_floatingips_count ( context , filters = None ) : LOG . info ( 'get_floatingips_count for tenant %s filters %s' % ( context . tenant_id , filters ) ) if filters is None : filters = { } filters [ '_deallocated' ] = False filters [ 'address_type' ] = ip_types . FLOATING count = db_api . ip_address_count_all ( context , filters ) LOG . info ( 'Found %s floating ips for tenant %s' % ( count , context . tenant_id ) ) return count
1408	def _is_continue_to_work ( self ) : if not self . _is_topology_running ( ) : return False max_spout_pending = self . pplan_helper . context . get_cluster_config ( ) . get ( api_constants . TOPOLOGY_MAX_SPOUT_PENDING ) if not self . acking_enabled and self . output_helper . is_out_queue_available ( ) : return True elif self . acking_enabled and self . output_helper . is_out_queue_available ( ) and len ( self . in_flight_tuples ) < max_spout_pending : return True elif self . acking_enabled and not self . in_stream . is_empty ( ) : return True else : return False
10136	def _detect_or_validate ( self , val ) : if isinstance ( val , list ) or isinstance ( val , dict ) or isinstance ( val , SortableDict ) or isinstance ( val , Grid ) : # Project Haystack 3.0 type. self . _assert_version ( VER_3_0 )
10356	def random_by_nodes ( graph : BELGraph , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.9 assert 0 < percentage <= 1 nodes = graph . nodes ( ) n = int ( len ( nodes ) * percentage ) subnodes = random . sample ( nodes , n ) result = graph . subgraph ( subnodes ) update_node_helper ( graph , result ) return result
920	def critical ( self , msg , * args , * * kwargs ) : self . _baseLogger . critical ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )
902	def updateAnomalyLikelihoods ( anomalyScores , params , verbosity = 0 ) : if verbosity > 3 : print ( "In updateAnomalyLikelihoods." ) print ( "Number of anomaly scores:" , len ( anomalyScores ) ) print ( "First 20:" , anomalyScores [ 0 : min ( 20 , len ( anomalyScores ) ) ] ) print ( "Params:" , params ) if len ( anomalyScores ) == 0 : raise ValueError ( "Must have at least one anomalyScore" ) if not isValidEstimatorParams ( params ) : raise ValueError ( "'params' is not a valid params structure" ) # For backward compatibility. if "historicalLikelihoods" not in params : params [ "historicalLikelihoods" ] = [ 1.0 ] # Compute moving averages of these new scores using the previous values # as well as likelihood for these scores using the old estimator historicalValues = params [ "movingAverage" ] [ "historicalValues" ] total = params [ "movingAverage" ] [ "total" ] windowSize = params [ "movingAverage" ] [ "windowSize" ] aggRecordList = numpy . zeros ( len ( anomalyScores ) , dtype = float ) likelihoods = numpy . zeros ( len ( anomalyScores ) , dtype = float ) for i , v in enumerate ( anomalyScores ) : newAverage , historicalValues , total = ( MovingAverage . compute ( historicalValues , total , v [ 2 ] , windowSize ) ) aggRecordList [ i ] = newAverage likelihoods [ i ] = tailProbability ( newAverage , params [ "distribution" ] ) # Filter the likelihood values. First we prepend the historical likelihoods # to the current set. Then we filter the values. We peel off the likelihoods # to return and the last windowSize values to store for later. likelihoods2 = params [ "historicalLikelihoods" ] + list ( likelihoods ) filteredLikelihoods = _filterLikelihoods ( likelihoods2 ) likelihoods [ : ] = filteredLikelihoods [ - len ( likelihoods ) : ] historicalLikelihoods = likelihoods2 [ - min ( windowSize , len ( likelihoods2 ) ) : ] # Update the estimator newParams = { "distribution" : params [ "distribution" ] , "movingAverage" : { "historicalValues" : historicalValues , "total" : total , "windowSize" : windowSize , } , "historicalLikelihoods" : historicalLikelihoods , } assert len ( newParams [ "historicalLikelihoods" ] ) <= windowSize if verbosity > 3 : print ( "Number of likelihoods:" , len ( likelihoods ) ) print ( "First 20 likelihoods:" , likelihoods [ 0 : min ( 20 , len ( likelihoods ) ) ] ) print ( "Leaving updateAnomalyLikelihoods." ) return ( likelihoods , aggRecordList , newParams )
5077	def get_closest_course_run ( course_runs ) : if len ( course_runs ) == 1 : return course_runs [ 0 ] now = datetime . datetime . now ( pytz . UTC ) # course runs with no start date should be considered last. never = now - datetime . timedelta ( days = 3650 ) return min ( course_runs , key = lambda x : abs ( get_course_run_start ( x , never ) - now ) )
5749	def aggregate_history ( self , ip , days_limit = None ) : first_date = None last_date = None prec_asn = None prec_block = None for entry in self . history ( ip , days_limit ) : if entry is None : continue date , asn , block = entry if first_date is None : last_date = date first_date = date prec_asn = asn prec_block = block elif prec_asn == asn and prec_block == block : first_date = date else : yield first_date , last_date , prec_asn , prec_block last_date = date first_date = date prec_asn = asn prec_block = block if first_date is not None : yield first_date , last_date , prec_asn , prec_block
8764	def get_public_net_id ( self ) : for id , net_params in self . strategy . iteritems ( ) : if id == CONF . QUARK . public_net_id : return id return None
4330	def gain ( self , gain_db = 0.0 , normalize = True , limiter = False , balance = None ) : if not is_number ( gain_db ) : raise ValueError ( "gain_db must be a number." ) if not isinstance ( normalize , bool ) : raise ValueError ( "normalize must be a boolean." ) if not isinstance ( limiter , bool ) : raise ValueError ( "limiter must be a boolean." ) if balance not in [ None , 'e' , 'B' , 'b' ] : raise ValueError ( "balance must be one of None, 'e', 'B', or 'b'." ) effect_args = [ 'gain' ] if balance is not None : effect_args . append ( '-{}' . format ( balance ) ) if normalize : effect_args . append ( '-n' ) if limiter : effect_args . append ( '-l' ) effect_args . append ( '{:f}' . format ( gain_db ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'gain' ) return self
8626	def delete_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_delete_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8230	def size ( self , w = None , h = None ) : if not w : w = self . _canvas . width if not h : h = self . _canvas . height if not w and not h : return ( self . _canvas . width , self . _canvas . height ) # FIXME: Updating in all these places seems a bit hacky w , h = self . _canvas . set_size ( ( w , h ) ) self . _namespace [ 'WIDTH' ] = w self . _namespace [ 'HEIGHT' ] = h self . WIDTH = w # Added to make evolution example work self . HEIGHT = h
3243	def get_rules ( security_group , * * kwargs ) : rules = security_group . pop ( 'security_group_rules' , [ ] ) for rule in rules : rule [ 'ip_protocol' ] = rule . pop ( 'protocol' ) rule [ 'from_port' ] = rule . pop ( 'port_range_max' ) rule [ 'to_port' ] = rule . pop ( 'port_range_min' ) rule [ 'cidr_ip' ] = rule . pop ( 'remote_ip_prefix' ) rule [ 'rule_type' ] = rule . pop ( 'direction' ) security_group [ 'rules' ] = sorted ( rules ) return security_group
13813	def MessageToJson ( message , including_default_value_fields = False ) : js = _MessageToJsonObject ( message , including_default_value_fields ) return json . dumps ( js , indent = 2 )
11831	def path ( self ) : node , path_back = self , [ ] while node : path_back . append ( node ) node = node . parent return list ( reversed ( path_back ) )
473	def build_words_dataset ( words = None , vocabulary_size = 50000 , printable = True , unk_key = 'UNK' ) : if words is None : raise Exception ( "words : list of str or byte" ) count = [ [ unk_key , - 1 ] ] count . extend ( collections . Counter ( words ) . most_common ( vocabulary_size - 1 ) ) dictionary = dict ( ) for word , _ in count : dictionary [ word ] = len ( dictionary ) data = list ( ) unk_count = 0 for word in words : if word in dictionary : index = dictionary [ word ] else : index = 0 # dictionary['UNK'] unk_count += 1 data . append ( index ) count [ 0 ] [ 1 ] = unk_count reverse_dictionary = dict ( zip ( dictionary . values ( ) , dictionary . keys ( ) ) ) if printable : tl . logging . info ( 'Real vocabulary size %d' % len ( collections . Counter ( words ) . keys ( ) ) ) tl . logging . info ( 'Limited vocabulary size {}' . format ( vocabulary_size ) ) if len ( collections . Counter ( words ) . keys ( ) ) < vocabulary_size : raise Exception ( "len(collections.Counter(words).keys()) >= vocabulary_size , the limited vocabulary_size must be less than or equal to the read vocabulary_size" ) return data , count , dictionary , reverse_dictionary
10186	def _aggregations_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_aggs ) : for cfg in ep . load ( ) ( ) : if cfg [ 'aggregation_name' ] not in self . enabled_aggregations : continue elif cfg [ 'aggregation_name' ] in result : raise DuplicateAggregationError ( 'Duplicate aggregation {0} in entry point ' '{1}' . format ( cfg [ 'event_type' ] , ep . name ) ) # Update the default configuration with env/overlay config. cfg . update ( self . enabled_aggregations [ cfg [ 'aggregation_name' ] ] or { } ) result [ cfg [ 'aggregation_name' ] ] = cfg return result
5559	def _unflatten_tree ( flat ) : tree = { } for key , value in flat . items ( ) : path = key . split ( "/" ) # we are at the end of a branch if len ( path ) == 1 : tree [ key ] = value # there are more branches else : # create new dict if not path [ 0 ] in tree : tree [ path [ 0 ] ] = _unflatten_tree ( { "/" . join ( path [ 1 : ] ) : value } ) # add keys to existing dict else : branch = _unflatten_tree ( { "/" . join ( path [ 1 : ] ) : value } ) if not path [ 1 ] in tree [ path [ 0 ] ] : tree [ path [ 0 ] ] [ path [ 1 ] ] = branch [ path [ 1 ] ] else : tree [ path [ 0 ] ] [ path [ 1 ] ] . update ( branch [ path [ 1 ] ] ) return tree
11661	def transform ( self , X ) : self . _check_fitted ( ) X = as_features ( X , stack = True ) assignments = self . kmeans_fit_ . predict ( X . stacked_features ) return self . _group_assignments ( X , assignments )
38	def dict_gather ( comm , d , op = 'mean' , assert_all_have_data = True ) : if comm is None : return d alldicts = comm . allgather ( d ) size = comm . size k2li = defaultdict ( list ) for d in alldicts : for ( k , v ) in d . items ( ) : k2li [ k ] . append ( v ) result = { } for ( k , li ) in k2li . items ( ) : if assert_all_have_data : assert len ( li ) == size , "only %i out of %i MPI workers have sent '%s'" % ( len ( li ) , size , k ) if op == 'mean' : result [ k ] = np . mean ( li , axis = 0 ) elif op == 'sum' : result [ k ] = np . sum ( li , axis = 0 ) else : assert 0 , op return result
13185	def get_default_tag ( app ) : view_func = get_view_function ( app , request . path , request . method ) if view_func : return view_func . __name__
3057	def _load_credentials_file ( credentials_file ) : try : credentials_file . seek ( 0 ) data = json . load ( credentials_file ) except Exception : logger . warning ( 'Credentials file could not be loaded, will ignore and ' 'overwrite.' ) return { } if data . get ( 'file_version' ) != 2 : logger . warning ( 'Credentials file is not version 2, will ignore and ' 'overwrite.' ) return { } credentials = { } for key , encoded_credential in iteritems ( data . get ( 'credentials' , { } ) ) : try : credential_json = base64 . b64decode ( encoded_credential ) credential = client . Credentials . new_from_json ( credential_json ) credentials [ key ] = credential except : logger . warning ( 'Invalid credential {0} in file, ignoring.' . format ( key ) ) return credentials
6072	def luminosity_within_ellipse_in_units ( self , major_axis : dim . Length , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if self . has_light_profile : return sum ( map ( lambda p : p . luminosity_within_ellipse_in_units ( major_axis = major_axis , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) , self . light_profiles ) ) else : return None
1281	def block_code ( self , code , lang = None ) : code = code . rstrip ( '\n' ) if not lang : code = escape ( code , smart_amp = False ) return '<pre><code>%s\n</code></pre>\n' % code code = escape ( code , quote = True , smart_amp = False ) return '<pre><code class="lang-%s">%s\n</code></pre>\n' % ( lang , code )
12856	def merge_text ( events ) : text = [ ] for obj in events : if obj [ 'type' ] == TEXT : text . append ( obj [ 'text' ] ) else : if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) } text . clear ( ) yield obj if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) }
6775	def force_stop_and_purge ( self ) : r = self . local_renderer self . stop ( ) with settings ( warn_only = True ) : r . sudo ( 'killall rabbitmq-server' ) with settings ( warn_only = True ) : r . sudo ( 'killall beam.smp' ) #TODO:explicitly delete all subfolders, star-delete doesn't work r . sudo ( 'rm -Rf /var/lib/rabbitmq/mnesia/*' )
4019	def _dusty_vm_exists ( ) : existing_vms = check_output_demoted ( [ 'VBoxManage' , 'list' , 'vms' ] ) for line in existing_vms . splitlines ( ) : if '"{}"' . format ( constants . VM_MACHINE_NAME ) in line : return True return False
8479	def potential ( self , value ) : if value : self . _potential = True else : self . _potential = False
5725	def write ( self , mi_cmd_to_write , timeout_sec = DEFAULT_GDB_TIMEOUT_SEC , raise_error_on_timeout = True , read_response = True , ) : self . verify_valid_gdb_subprocess ( ) if timeout_sec < 0 : self . logger . warning ( "timeout_sec was negative, replacing with 0" ) timeout_sec = 0 # Ensure proper type of the mi command if type ( mi_cmd_to_write ) in [ str , unicode ] : pass elif type ( mi_cmd_to_write ) == list : mi_cmd_to_write = "\n" . join ( mi_cmd_to_write ) else : raise TypeError ( "The gdb mi command must a be str or list. Got " + str ( type ( mi_cmd_to_write ) ) ) self . logger . debug ( "writing: %s" , mi_cmd_to_write ) if not mi_cmd_to_write . endswith ( "\n" ) : mi_cmd_to_write_nl = mi_cmd_to_write + "\n" else : mi_cmd_to_write_nl = mi_cmd_to_write if USING_WINDOWS : # select not implemented in windows for pipes # assume it's always ready outputready = [ self . stdin_fileno ] else : _ , outputready , _ = select . select ( [ ] , self . write_list , [ ] , timeout_sec ) for fileno in outputready : if fileno == self . stdin_fileno : # ready to write self . gdb_process . stdin . write ( mi_cmd_to_write_nl . encode ( ) ) # don't forget to flush for Python3, otherwise gdb won't realize there is data # to evaluate, and we won't get a response self . gdb_process . stdin . flush ( ) else : self . logger . error ( "got unexpected fileno %d" % fileno ) if read_response is True : return self . get_gdb_response ( timeout_sec = timeout_sec , raise_error_on_timeout = raise_error_on_timeout ) else : return [ ]
13308	def gmv ( a , b ) : return np . exp ( np . square ( np . log ( a ) - np . log ( b ) ) . mean ( ) )
39	def discount ( x , gamma ) : assert x . ndim >= 1 return scipy . signal . lfilter ( [ 1 ] , [ 1 , - gamma ] , x [ : : - 1 ] , axis = 0 ) [ : : - 1 ]
10735	def ld_to_dl ( ld ) : if ld : keys = list ( ld [ 0 ] ) dl = { key : [ d [ key ] for d in ld ] for key in keys } return dl else : return { }
3589	def set_color ( self , r , g , b ) : # See more details on the bulb's protocol from this guide: # https://learn.adafruit.com/reverse-engineering-a-bluetooth-low-energy-light-bulb/overview command = '\x58\x01\x03\x01\xFF\x00{0}{1}{2}' . format ( chr ( r & 0xFF ) , chr ( g & 0xFF ) , chr ( b & 0xFF ) ) self . _color . write_value ( command )
13172	def last ( self , name = None ) : for c in self . children ( name , reverse = True ) : return c
6537	def mod_sys_path ( paths ) : old_path = sys . path sys . path = paths + sys . path try : yield finally : sys . path = old_path
9115	def fs_cleansed_attachments ( self ) : if exists ( self . fs_cleansed_attachment_container ) : return [ join ( self . fs_cleansed_attachment_container , attachment ) for attachment in listdir ( self . fs_cleansed_attachment_container ) ] else : return [ ]
4649	def json ( self ) : if not self . _is_constructed ( ) or self . _is_require_reconstruction ( ) : self . constructTx ( ) return dict ( self )
257	def print_round_trip_stats ( round_trips , hide_pos = False ) : stats = gen_round_trip_stats ( round_trips ) print_table ( stats [ 'summary' ] , float_format = '{:.2f}' . format , name = 'Summary stats' ) print_table ( stats [ 'pnl' ] , float_format = '${:.2f}' . format , name = 'PnL stats' ) print_table ( stats [ 'duration' ] , float_format = '{:.2f}' . format , name = 'Duration stats' ) print_table ( stats [ 'returns' ] * 100 , float_format = '{:.2f}%' . format , name = 'Return stats' ) if not hide_pos : stats [ 'symbols' ] . columns = stats [ 'symbols' ] . columns . map ( format_asset ) print_table ( stats [ 'symbols' ] * 100 , float_format = '{:.2f}%' . format , name = 'Symbol stats' )
5618	def multipart_to_singleparts ( geom ) : if isinstance ( geom , base . BaseGeometry ) : if hasattr ( geom , "geoms" ) : for subgeom in geom : yield subgeom else : yield geom
4215	def name ( cls ) : parent , sep , mod_name = cls . __module__ . rpartition ( '.' ) mod_name = mod_name . replace ( '_' , ' ' ) return ' ' . join ( [ mod_name , cls . __name__ ] )
7706	def load_roster ( self , source ) : try : tree = ElementTree . parse ( source ) except ElementTree . ParseError , err : raise ValueError ( "Invalid roster format: {0}" . format ( err ) ) roster = Roster . from_xml ( tree . getroot ( ) ) for item in roster : item . verify_roster_result ( True ) self . roster = roster
5614	def segmentize_geometry ( geometry , segmentize_value ) : if geometry . geom_type != "Polygon" : raise TypeError ( "segmentize geometry type must be Polygon" ) return Polygon ( LinearRing ( [ p # pick polygon linestrings for l in map ( lambda x : LineString ( [ x [ 0 ] , x [ 1 ] ] ) , zip ( geometry . exterior . coords [ : - 1 ] , geometry . exterior . coords [ 1 : ] ) ) # interpolate additional points in between and don't forget end point for p in [ l . interpolate ( segmentize_value * i ) . coords [ 0 ] for i in range ( int ( l . length / segmentize_value ) ) ] + [ l . coords [ 1 ] ] ] ) )
731	def _getW ( self ) : w = self . _w if type ( w ) is list : return w [ self . _random . getUInt32 ( len ( w ) ) ] else : return w
3634	def search ( self , ctype , level = None , category = None , assetId = None , defId = None , min_price = None , max_price = None , min_buy = None , max_buy = None , league = None , club = None , position = None , zone = None , nationality = None , rare = False , playStyle = None , start = 0 , page_size = itemsPerPage [ 'transferMarket' ] , fast = False ) : # TODO: add "search" alias # TODO: generator method = 'GET' url = 'transfermarket' # pinEvents if start == 0 : events = [ self . pin . event ( 'page_view' , 'Hub - Transfers' ) , self . pin . event ( 'page_view' , 'Transfer Market Search' ) ] self . pin . send ( events , fast = fast ) params = { 'start' : start , 'num' : page_size , 'type' : ctype , # "type" namespace is reserved in python } if level : params [ 'lev' ] = level if category : params [ 'cat' ] = category if assetId : params [ 'maskedDefId' ] = assetId if defId : params [ 'definitionId' ] = defId if min_price : params [ 'micr' ] = min_price if max_price : params [ 'macr' ] = max_price if min_buy : params [ 'minb' ] = min_buy if max_buy : params [ 'maxb' ] = max_buy if league : params [ 'leag' ] = league if club : params [ 'team' ] = club if position : params [ 'pos' ] = position if zone : params [ 'zone' ] = zone if nationality : params [ 'nat' ] = nationality if rare : params [ 'rare' ] = 'SP' if playStyle : params [ 'playStyle' ] = playStyle rc = self . __request__ ( method , url , params = params , fast = fast ) # pinEvents if start == 0 : events = [ self . pin . event ( 'page_view' , 'Transfer Market Results - List View' ) , self . pin . event ( 'page_view' , 'Item - Detail View' ) ] self . pin . send ( events , fast = fast ) return [ itemParse ( i ) for i in rc . get ( 'auctionInfo' , ( ) ) ]
2377	def list_rules ( self ) : for rule in sorted ( self . all_rules , key = lambda rule : rule . name ) : print ( rule ) if self . args . verbose : for line in rule . doc . split ( "\n" ) : print ( " " , line )
4803	def raises ( self , ex ) : if not callable ( self . val ) : raise TypeError ( 'val must be callable' ) if not issubclass ( ex , BaseException ) : raise TypeError ( 'given arg must be exception' ) return AssertionBuilder ( self . val , self . description , self . kind , ex )
4245	def _gethostbyname ( self , hostname ) : if self . _databaseType in const . IPV6_EDITIONS : response = socket . getaddrinfo ( hostname , 0 , socket . AF_INET6 ) family , socktype , proto , canonname , sockaddr = response [ 0 ] address , port , flow , scope = sockaddr return address else : return socket . gethostbyname ( hostname )
3552	def _state_changed ( self , state ) : logger . debug ( 'Adapter state change: {0}' . format ( state ) ) # Handle when powered on. if state == 5 : self . _powered_off . clear ( ) self . _powered_on . set ( ) # Handle when powered off. elif state == 4 : self . _powered_on . clear ( ) self . _powered_off . set ( )
8174	def goal ( self , x , y , z , d = 50.0 ) : return ( x - self . x ) / d , ( y - self . y ) / d , ( z - self . z ) / d
6569	def signature_matches ( func , args = ( ) , kwargs = { } ) : try : sig = inspect . signature ( func ) sig . bind ( * args , * * kwargs ) except TypeError : return False else : return True
12656	def dictify ( a_named_tuple ) : return dict ( ( s , getattr ( a_named_tuple , s ) ) for s in a_named_tuple . _fields )
7335	async def upload_media ( self , file_ , media_type = None , media_category = None , chunked = None , size_limit = None , * * params ) : if isinstance ( file_ , str ) : url = urlparse ( file_ ) if url . scheme . startswith ( 'http' ) : media = await self . _session . get ( file_ ) else : path = urlparse ( file_ ) . path . strip ( " \"'" ) media = await utils . execute ( open ( path , 'rb' ) ) elif hasattr ( file_ , 'read' ) or isinstance ( file_ , bytes ) : media = file_ else : raise TypeError ( "upload_media input must be a file object or a " "filename or binary data or an aiohttp request" ) media_size = await utils . get_size ( media ) if chunked is not None : size_test = False else : size_test = await self . _size_test ( media_size , size_limit ) if isinstance ( media , aiohttp . ClientResponse ) : # send the content of the response media = media . content if chunked or ( size_test and chunked is None ) : args = media , media_size , file_ , media_type , media_category response = await self . _chunked_upload ( * args , * * params ) else : response = await self . upload . media . upload . post ( media = media , * * params ) if not hasattr ( file_ , 'read' ) and not getattr ( media , 'closed' , True ) : media . close ( ) return response
1628	def GetIndentLevel ( line ) : indent = Match ( r'^( *)\S' , line ) if indent : return len ( indent . group ( 1 ) ) else : return 0
7986	def registration_success ( self , stanza ) : _unused = stanza self . lock . acquire ( ) try : self . state_change ( "registered" , self . registration_form ) if ( 'FORM_TYPE' in self . registration_form and self . registration_form [ 'FORM_TYPE' ] . value == 'jabber:iq:register' ) : if 'username' in self . registration_form : self . my_jid = JID ( self . registration_form [ 'username' ] . value , self . my_jid . domain , self . my_jid . resource ) if 'password' in self . registration_form : self . password = self . registration_form [ 'password' ] . value self . registration_callback = None self . _post_connect ( ) finally : self . lock . release ( )
13490	def reconcile ( self , server ) : if not self . challenge . exists ( server ) : raise Exception ( 'Challenge does not exist on server' ) existing = MapRouletteTaskCollection . from_server ( server , self . challenge ) same = [ ] new = [ ] changed = [ ] deleted = [ ] # reconcile the new tasks with the existing tasks: for task in self . tasks : # if the task exists on the server... if task . identifier in [ existing_task . identifier for existing_task in existing . tasks ] : # and they are equal... if task == existing . get_by_identifier ( task . identifier ) : # add to 'same' list same . append ( task ) # if they are not equal, add to 'changed' list else : changed . append ( task ) # if the task does not exist on the server, add to 'new' list else : new . append ( task ) # next, check for tasks on the server that don't exist in the new collection... for task in existing . tasks : if task . identifier not in [ task . identifier for task in self . tasks ] : # ... and add those to the 'deleted' list. deleted . append ( task ) # update the server with new, changed, and deleted tasks if new : newCollection = MapRouletteTaskCollection ( self . challenge , tasks = new ) newCollection . create ( server ) if changed : changedCollection = MapRouletteTaskCollection ( self . challenge , tasks = changed ) changedCollection . update ( server ) if deleted : deletedCollection = MapRouletteTaskCollection ( self . challenge , tasks = deleted ) for task in deletedCollection . tasks : task . status = 'deleted' deletedCollection . update ( server ) # return same, new, changed and deleted tasks return { 'same' : same , 'new' : new , 'changed' : changed , 'deleted' : deleted }
4713	def hooks_setup ( trun , parent , hnames = None ) : hooks = { "enter" : [ ] , "exit" : [ ] } if hnames is None : # Nothing to do, just return the struct return hooks for hname in hnames : # Fill out paths for med in HOOK_PATTERNS : for ptn in HOOK_PATTERNS [ med ] : fpath = os . sep . join ( [ trun [ "conf" ] [ "HOOKS" ] , ptn % hname ] ) if not os . path . exists ( fpath ) : continue hook = hook_setup ( parent , fpath ) if not hook : continue hooks [ med ] . append ( hook ) if not hooks [ "enter" ] + hooks [ "exit" ] : cij . err ( "rnr:hooks_setup:FAIL { hname: %r has no files }" % hname ) return None return hooks
8745	def get_floatingips ( context , filters = None , fields = None , sorts = [ 'id' ] , limit = None , marker = None , page_reverse = False ) : LOG . info ( 'get_floatingips for tenant %s filters %s fields %s' % ( context . tenant_id , filters , fields ) ) floating_ips = _get_ips_by_type ( context , ip_types . FLOATING , filters = filters , fields = fields ) return [ v . _make_floating_ip_dict ( flip ) for flip in floating_ips ]
7221	def ingest_vectors ( self , output_port_value ) : # append two tasks to self['definition']['tasks'] ingest_task = Task ( 'IngestItemJsonToVectorServices' ) ingest_task . inputs . items = output_port_value ingest_task . impersonation_allowed = True stage_task = Task ( 'StageDataToS3' ) stage_task . inputs . destination = 's3://{vector_ingest_bucket}/{recipe_id}/{run_id}/{task_name}' stage_task . inputs . data = ingest_task . outputs . result . value self . definition [ 'tasks' ] . append ( ingest_task . generate_task_workflow_json ( ) ) self . definition [ 'tasks' ] . append ( stage_task . generate_task_workflow_json ( ) )
5092	def _login ( self , email , password ) : response = requests . post ( urljoin ( self . ENDPOINT , 'sessions' ) , json = { 'email' : email , 'password' : password , 'platform' : 'ios' , 'token' : binascii . hexlify ( os . urandom ( 64 ) ) . decode ( 'utf8' ) } , headers = self . _headers ) response . raise_for_status ( ) access_token = response . json ( ) [ 'access_token' ] self . _headers [ 'Authorization' ] = 'Token token=%s' % access_token
11670	def _get_Ks ( self ) : Ks = as_integer_type ( self . Ks ) if Ks . ndim != 1 : raise TypeError ( "Ks should be 1-dim, got shape {}" . format ( Ks . shape ) ) if Ks . min ( ) < 1 : raise ValueError ( "Ks should be positive; got {}" . format ( Ks . min ( ) ) ) return Ks
1705	def extract_common_args ( command , parser , cl_args ) : try : # do not pop like cli because ``topologies`` subcommand still needs it cluster_role_env = cl_args [ 'cluster/[role]/[env]' ] config_path = cl_args [ 'config_path' ] except KeyError : # if some of the arguments are not found, print error and exit subparser = config . get_subparser ( parser , command ) print ( subparser . format_help ( ) ) return dict ( ) cluster = config . get_heron_cluster ( cluster_role_env ) config_path = config . get_heron_cluster_conf_dir ( cluster , config_path ) new_cl_args = dict ( ) try : cluster_tuple = config . parse_cluster_role_env ( cluster_role_env , config_path ) new_cl_args [ 'cluster' ] = cluster_tuple [ 0 ] new_cl_args [ 'role' ] = cluster_tuple [ 1 ] new_cl_args [ 'environ' ] = cluster_tuple [ 2 ] new_cl_args [ 'config_path' ] = config_path except Exception as e : Log . error ( "Unable to get valid topology location: %s" , str ( e ) ) return dict ( ) cl_args . update ( new_cl_args ) return cl_args
4833	def traverse_pagination ( response , endpoint , content_filter_query , query_params ) : results = response . get ( 'results' , [ ] ) page = 1 while response . get ( 'next' ) : page += 1 response = endpoint ( ) . post ( content_filter_query , * * dict ( query_params , page = page ) ) results += response . get ( 'results' , [ ] ) return results
4449	def load_document ( self , id ) : fields = self . redis . hgetall ( id ) if six . PY3 : f2 = { to_string ( k ) : to_string ( v ) for k , v in fields . items ( ) } fields = f2 try : del fields [ 'id' ] except KeyError : pass return Document ( id = id , * * fields )
4924	def get_required_query_params ( self , request ) : email = get_request_value ( request , self . REQUIRED_PARAM_EMAIL , '' ) enterprise_name = get_request_value ( request , self . REQUIRED_PARAM_ENTERPRISE_NAME , '' ) number_of_codes = get_request_value ( request , self . OPTIONAL_PARAM_NUMBER_OF_CODES , '' ) if not ( email and enterprise_name ) : raise CodesAPIRequestError ( self . get_missing_params_message ( [ ( self . REQUIRED_PARAM_EMAIL , bool ( email ) ) , ( self . REQUIRED_PARAM_ENTERPRISE_NAME , bool ( enterprise_name ) ) , ] ) ) return email , enterprise_name , number_of_codes
5240	def market_close ( self , session , mins ) -> Session : if session not in self . exch : return SessNA end_time = self . exch [ session ] [ - 1 ] return Session ( shift_time ( end_time , - int ( mins ) + 1 ) , end_time )
9796	def get ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) try : response = PolyaxonClient ( ) . experiment_group . get_experiment_group ( user , project_name , _group ) cache . cache ( config_manager = GroupManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_group_details ( response )
6081	def potential_of_galaxies_from_grid ( grid , galaxies ) : if galaxies : return sum ( map ( lambda g : g . potential_from_grid ( grid ) , galaxies ) ) else : return np . full ( ( grid . shape [ 0 ] ) , 0.0 )
5957	def merge_ndx ( * args ) : ndxs = [ ] struct = None for fname in args : if fname . endswith ( '.ndx' ) : ndxs . append ( fname ) else : if struct is not None : raise ValueError ( "only one structure file supported" ) struct = fname fd , multi_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'multi_' ) os . close ( fd ) atexit . register ( os . unlink , multi_ndx ) if struct : make_ndx = registry [ 'Make_ndx' ] ( f = struct , n = ndxs , o = multi_ndx ) else : make_ndx = registry [ 'Make_ndx' ] ( n = ndxs , o = multi_ndx ) _ , _ , _ = make_ndx ( input = [ 'q' ] , stdout = False , stderr = False ) return multi_ndx
4776	def does_not_contain_duplicates ( self ) : try : if len ( self . val ) == len ( set ( self . val ) ) : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to not contain duplicates, but did.' % self . val )
12071	def update ( self , tids , info ) : outputs_dir = os . path . join ( info [ 'root_directory' ] , 'streams' ) pattern = '%s_*_tid_*{tid}.o.{tid}*' % info [ 'batch_name' ] flist = os . listdir ( outputs_dir ) try : outputs = [ ] for tid in tids : matches = fnmatch . filter ( flist , pattern . format ( tid = tid ) ) if len ( matches ) != 1 : self . warning ( "No unique output file for tid %d" % tid ) contents = open ( os . path . join ( outputs_dir , matches [ 0 ] ) , 'r' ) . read ( ) outputs . append ( self . output_extractor ( contents ) ) self . _next_val = self . _update_state ( outputs ) self . trace . append ( ( outputs , self . _next_val ) ) except : self . warning ( "Cannot load required output files. Cannot continue." ) self . _next_val = StopIteration
12545	def div_img ( img1 , div2 ) : if is_img ( div2 ) : return img1 . get_data ( ) / div2 . get_data ( ) elif isinstance ( div2 , ( float , int ) ) : return img1 . get_data ( ) / div2 else : raise NotImplementedError ( 'Cannot divide {}({}) by ' '{}({})' . format ( type ( img1 ) , img1 , type ( div2 ) , div2 ) )
13433	def showfig ( fig , aspect = "auto" ) : ax = fig . gca ( ) # Swap y axis if needed alim = list ( ax . axis ( ) ) if alim [ 3 ] < alim [ 2 ] : temp = alim [ 2 ] alim [ 2 ] = alim [ 3 ] alim [ 3 ] = temp ax . axis ( alim ) ax . set_aspect ( aspect ) fig . show ( )
11237	def sendreturn ( gen , value ) : try : gen . send ( value ) except StopIteration as e : return stopiter_value ( e ) else : raise RuntimeError ( 'generator did not return as expected' )
6675	def umask ( self , use_sudo = False ) : func = use_sudo and run_as_root or self . run return func ( 'umask' )
11796	def min_conflicts_value ( csp , var , current ) : return argmin_random_tie ( csp . domains [ var ] , lambda val : csp . nconflicts ( var , val , current ) )
6325	def encode ( self , text ) : text = text_type ( text ) if '\x00' in text : text = text . replace ( '\x00' , ' ' ) minval = Fraction ( 0 ) maxval = Fraction ( 1 ) for char in text + '\x00' : prob_range = self . _probs [ char ] delta = maxval - minval maxval = minval + prob_range [ 1 ] * delta minval = minval + prob_range [ 0 ] * delta # I tried without the /2 just to check. Doesn't work. # Keep scaling up until the error range is >= 1. That # gives me the minimum number of bits needed to resolve # down to the end-of-data character. delta = ( maxval - minval ) / 2 nbits = long ( 0 ) while delta < 1 : nbits += 1 delta *= 2 # The below condition shouldn't ever be false if nbits == 0 : # pragma: no cover return 0 , 0 # using -1 instead of /2 avg = ( maxval + minval ) * 2 ** ( nbits - 1 ) # Could return a rational instead ... # the division truncation is deliberate return avg . numerator // avg . denominator , nbits
229	def compute_style_factor_exposures ( positions , risk_factor ) : positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) style_factor_exposure = positions_wo_cash . multiply ( risk_factor ) . divide ( gross_exposure , axis = 'index' ) tot_style_factor_exposure = style_factor_exposure . sum ( axis = 'columns' , skipna = True ) return tot_style_factor_exposure
3171	def create ( self , store_id , data ) : self . store_id = store_id if 'id' not in data : raise KeyError ( 'The store customer must have an id' ) if 'email_address' not in data : raise KeyError ( 'The store customer must have an email_address' ) check_email ( data [ 'email_address' ] ) if 'opt_in_status' not in data : raise KeyError ( 'The store customer must have an opt_in_status' ) if data [ 'opt_in_status' ] not in [ True , False ] : raise TypeError ( 'The opt_in_status must be True or False' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'customers' ) , data = data ) if response is not None : self . customer_id = response [ 'id' ] else : self . customer_id = None return response
10610	def _calculate_H ( self , T ) : if self . isCoal : return self . _calculate_Hfr_coal ( T ) H = 0.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) dH = thermo . H ( compound , T , self . _compound_masses [ index ] ) H = H + dH return H
9554	def _apply_record_length_checks ( self , i , r , summarize = False , context = None ) : for code , message , modulus in self . _record_length_checks : if i % modulus == 0 : # support sampling if len ( r ) != len ( self . _field_names ) : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'length' ] = len ( r ) if context is not None : p [ 'context' ] = context yield p
7100	def on_marker ( self , marker ) : mid , pos = marker self . marker = Marker ( __id__ = mid ) mapview = self . parent ( ) # Save ref mapview . markers [ mid ] = self # Required so the packer can pass the id self . marker . setTag ( mid ) # If we have a child widget we must configure the map to use the # custom adapter for w in self . child_widgets ( ) : mapview . init_info_window_adapter ( ) break d = self . declaration if d . show_info : self . set_show_info ( d . show_info ) #: Can free the options now del self . options
11413	def record_replace_field ( rec , tag , new_field , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) replaced = False for position , field in enumerate ( rec [ tag ] ) : if field [ 4 ] == field_position_global : rec [ tag ] [ position ] = new_field replaced = True if not replaced : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the global field position '%d'." % ( tag , field_position_global ) ) else : try : rec [ tag ] [ field_position_local ] = new_field except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
274	def get_symbol_rets ( symbol , start = None , end = None ) : return SETTINGS [ 'returns_func' ] ( symbol , start = start , end = end )
1952	def perm ( lst , func ) : for i in range ( 1 , 2 ** len ( lst ) ) : yield [ func ( item ) if ( 1 << j ) & i else item for ( j , item ) in enumerate ( lst ) ]
4224	def _load_keyring_class ( keyring_name ) : module_name , sep , class_name = keyring_name . rpartition ( '.' ) __import__ ( module_name ) module = sys . modules [ module_name ] return getattr ( module , class_name )
4266	def set_meta ( target , keys , overwrite = False ) : if not os . path . exists ( target ) : sys . stderr . write ( "The target {} does not exist.\n" . format ( target ) ) sys . exit ( 1 ) if len ( keys ) < 2 or len ( keys ) % 2 > 0 : sys . stderr . write ( "Need an even number of arguments.\n" ) sys . exit ( 1 ) if os . path . isdir ( target ) : descfile = os . path . join ( target , 'index.md' ) else : descfile = os . path . splitext ( target ) [ 0 ] + '.md' if os . path . exists ( descfile ) and not overwrite : sys . stderr . write ( "Description file '{}' already exists. " "Use --overwrite to overwrite it.\n" . format ( descfile ) ) sys . exit ( 2 ) with open ( descfile , "w" ) as fp : for i in range ( len ( keys ) // 2 ) : k , v = keys [ i * 2 : ( i + 1 ) * 2 ] fp . write ( "{}: {}\n" . format ( k . capitalize ( ) , v ) ) print ( "{} metadata key(s) written to {}" . format ( len ( keys ) // 2 , descfile ) )
4170	def zpk2ss ( z , p , k ) : import scipy . signal return scipy . signal . zpk2ss ( z , p , k )
8017	async def websocket_disconnect ( self , message ) : # set this flag so as to ensure we don't send a downstream `websocket.close` message due to all # child applications closing. self . closing = True # inform all children await self . send_upstream ( message ) await super ( ) . websocket_disconnect ( message )
11214	def compare_signature ( expected : Union [ str , bytes ] , actual : Union [ str , bytes ] ) -> bool : expected = util . to_bytes ( expected ) actual = util . to_bytes ( actual ) return hmac . compare_digest ( expected , actual )
4656	def broadcast ( self ) : # Sign if not signed if not self . _is_signed ( ) : self . sign ( ) # Cannot broadcast an empty transaction if "operations" not in self or not self [ "operations" ] : log . warning ( "No operations in transaction! Returning" ) return # Obtain JS ret = self . json ( ) # Debugging mode does not broadcast if self . blockchain . nobroadcast : log . warning ( "Not broadcasting anything!" ) self . clear ( ) return ret # Broadcast try : if self . blockchain . blocking : ret = self . blockchain . rpc . broadcast_transaction_synchronous ( ret , api = "network_broadcast" ) ret . update ( * * ret . get ( "trx" , { } ) ) else : self . blockchain . rpc . broadcast_transaction ( ret , api = "network_broadcast" ) except Exception as e : raise e finally : self . clear ( ) return ret
1675	def _ExpandDirectories ( filenames ) : expanded = set ( ) for filename in filenames : if not os . path . isdir ( filename ) : expanded . add ( filename ) continue for root , _ , files in os . walk ( filename ) : for loopfile in files : fullname = os . path . join ( root , loopfile ) if fullname . startswith ( '.' + os . path . sep ) : fullname = fullname [ len ( '.' + os . path . sep ) : ] expanded . add ( fullname ) filtered = [ ] for filename in expanded : if os . path . splitext ( filename ) [ 1 ] [ 1 : ] in GetAllExtensions ( ) : filtered . append ( filename ) return filtered
8856	def on_current_tab_changed ( self ) : self . menuEdit . clear ( ) self . menuModes . clear ( ) self . menuPanels . clear ( ) editor = self . tabWidget . current_widget ( ) self . menuEdit . setEnabled ( editor is not None ) self . menuModes . setEnabled ( editor is not None ) self . menuPanels . setEnabled ( editor is not None ) self . actionSave . setEnabled ( editor is not None ) self . actionSave_as . setEnabled ( editor is not None ) self . actionConfigure_run . setEnabled ( editor is not None ) self . actionRun . setEnabled ( editor is not None ) if editor is not None : self . setup_mnu_edit ( editor ) self . setup_mnu_modes ( editor ) self . setup_mnu_panels ( editor ) self . widgetOutline . set_editor ( editor ) self . _update_status_bar ( editor )
6205	def populations_slices ( particles , num_pop_list ) : slices = [ ] i_prev = 0 for num_pop in num_pop_list : slices . append ( slice ( i_prev , i_prev + num_pop ) ) i_prev += num_pop return slices
4050	def last_modified_version ( self , * * kwargs ) : self . items ( * * kwargs ) return int ( self . request . headers . get ( "last-modified-version" , 0 ) )
7061	def sqs_delete_queue ( queue_url , client = None ) : if not client : client = boto3 . client ( 'sqs' ) try : client . delete_queue ( QueueUrl = queue_url ) return True except Exception as e : LOGEXCEPTION ( 'could not delete the specified queue: %s' % ( queue_url , ) ) return False
3683	def calculate ( self , T , method ) : if method == WAGNER_MCGARRY : Psat = Wagner_original ( T , self . WAGNER_MCGARRY_Tc , self . WAGNER_MCGARRY_Pc , * self . WAGNER_MCGARRY_coefs ) elif method == WAGNER_POLING : Psat = Wagner ( T , self . WAGNER_POLING_Tc , self . WAGNER_POLING_Pc , * self . WAGNER_POLING_coefs ) elif method == ANTOINE_EXTENDED_POLING : Psat = TRC_Antoine_extended ( T , * self . ANTOINE_EXTENDED_POLING_coefs ) elif method == ANTOINE_POLING : A , B , C = self . ANTOINE_POLING_coefs Psat = Antoine ( T , A , B , C , base = 10.0 ) elif method == DIPPR_PERRY_8E : Psat = EQ101 ( T , * self . Perrys2_8_coeffs ) elif method == VDI_PPDS : Psat = Wagner ( T , self . VDI_PPDS_Tc , self . VDI_PPDS_Pc , * self . VDI_PPDS_coeffs ) elif method == COOLPROP : Psat = PropsSI ( 'P' , 'T' , T , 'Q' , 0 , self . CASRN ) elif method == BOILING_CRITICAL : Psat = boiling_critical_relation ( T , self . Tb , self . Tc , self . Pc ) elif method == LEE_KESLER_PSAT : Psat = Lee_Kesler ( T , self . Tc , self . Pc , self . omega ) elif method == AMBROSE_WALTON : Psat = Ambrose_Walton ( T , self . Tc , self . Pc , self . omega ) elif method == SANJARI : Psat = Sanjari ( T , self . Tc , self . Pc , self . omega ) elif method == EDALAT : Psat = Edalat ( T , self . Tc , self . Pc , self . omega ) elif method == EOS : Psat = self . eos [ 0 ] . Psat ( T ) elif method in self . tabular_data : Psat = self . interpolate ( T , method ) return Psat
9327	def valid_content_type ( self , content_type , accept ) : accept_tokens = accept . replace ( ' ' , '' ) . split ( ';' ) content_type_tokens = content_type . replace ( ' ' , '' ) . split ( ';' ) return ( all ( elem in content_type_tokens for elem in accept_tokens ) and ( content_type_tokens [ 0 ] == 'application/vnd.oasis.taxii+json' or content_type_tokens [ 0 ] == 'application/vnd.oasis.stix+json' ) )
5130	def find ( self , s ) : pSet = [ s ] parent = self . _leader [ s ] while parent != self . _leader [ parent ] : pSet . append ( parent ) parent = self . _leader [ parent ] if len ( pSet ) > 1 : for a in pSet : self . _leader [ a ] = parent return parent
2300	def predict_undirected_graph ( self , data ) : graph = Graph ( ) for idx_i , i in enumerate ( data . columns ) : for idx_j , j in enumerate ( data . columns [ idx_i + 1 : ] ) : score = self . predict ( data [ i ] . values , data [ j ] . values ) if abs ( score ) > 0.001 : graph . add_edge ( i , j , weight = score ) return graph
9594	def execute_script ( self , script , * args ) : return self . _execute ( Command . EXECUTE_SCRIPT , { 'script' : script , 'args' : list ( args ) } )
6315	def _add_resource_descriptions_to_pools ( self , meta_list ) : if not meta_list : return for meta in meta_list : getattr ( resources , meta . resource_type ) . add ( meta )
1837	def JGE ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , ( cpu . SF == cpu . OF ) , target . read ( ) , cpu . PC )
4441	async def _find ( self , ctx , * , query ) : if not query . startswith ( 'ytsearch:' ) and not query . startswith ( 'scsearch:' ) : query = 'ytsearch:' + query results = await self . bot . lavalink . get_tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found' ) tracks = results [ 'tracks' ] [ : 10 ] # First 10 results o = '' for index , track in enumerate ( tracks , start = 1 ) : track_title = track [ "info" ] [ "title" ] track_uri = track [ "info" ] [ "uri" ] o += f'`{index}.` [{track_title}]({track_uri})\n' embed = discord . Embed ( color = discord . Color . blurple ( ) , description = o ) await ctx . send ( embed = embed )
11652	def get_version ( self ) : if ( self . name is not None and self . version is not None and self . version . startswith ( ":versiontools:" ) ) : return ( self . __get_live_version ( ) or self . __get_frozen_version ( ) or self . __fail_to_get_any_version ( ) ) else : return self . __base . get_version ( self )
849	def getOutputElementCount ( self , name ) : if name == "resetOut" : print ( "WARNING: getOutputElementCount should not have been called with " "resetOut" ) return 1 elif name == "sequenceIdOut" : print ( "WARNING: getOutputElementCount should not have been called with " "sequenceIdOut" ) return 1 elif name == "dataOut" : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'dataOut' " "on a RecordSensor node, but the encoder has not " "been set" ) return self . encoder . getWidth ( ) elif name == "sourceOut" : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'sourceOut' " "on a RecordSensor node, " "but the encoder has not been set" ) return len ( self . encoder . getDescription ( ) ) elif name == "bucketIdxOut" : return 1 elif name == "actValueOut" : return 1 elif name == "categoryOut" : return self . numCategories elif name == 'spatialTopDownOut' or name == 'temporalTopDownOut' : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'sourceOut' " "on a RecordSensor node, " "but the encoder has not been set" ) return len ( self . encoder . getDescription ( ) ) else : raise Exception ( "Unknown output %s" % name )
8837	def merge ( * args ) : ret = [ ] for arg in args : if isinstance ( arg , list ) or isinstance ( arg , tuple ) : ret += list ( arg ) else : ret . append ( arg ) return ret
1984	def load_value ( self , key , binary = False ) : with self . load_stream ( key , binary = binary ) as s : return s . read ( )
8164	def inheritFromContext ( self , ignore = ( ) ) : for canvas_attr , grob_attr in STATES . items ( ) : if canvas_attr in ignore : continue setattr ( self , grob_attr , getattr ( self . _bot . _canvas , canvas_attr ) )
13643	def command_list ( ) : from cliez . conf import COMPONENT_ROOT root = COMPONENT_ROOT if root is None : sys . stderr . write ( "cliez.conf.COMPONENT_ROOT not set.\n" ) sys . exit ( 2 ) pass if not os . path . exists ( root ) : sys . stderr . write ( "please set a valid path for `cliez.conf.COMPONENT_ROOT`\n" ) sys . exit ( 2 ) pass try : path = os . listdir ( os . path . join ( root , 'components' ) ) return [ f [ : - 3 ] for f in path if f . endswith ( '.py' ) and f != '__init__.py' ] except FileNotFoundError : return [ ]
2972	def from_dict ( name , values ) : # determine the number of instances of this container count = 1 count_value = values . get ( 'count' , 1 ) if isinstance ( count_value , int ) : count = max ( count_value , 1 ) def with_index ( name , idx ) : if name and idx : return '%s_%d' % ( name , idx ) return name def get_instance ( n , idx = None ) : return BlockadeContainerConfig ( with_index ( n , idx ) , values [ 'image' ] , command = values . get ( 'command' ) , links = values . get ( 'links' ) , volumes = values . get ( 'volumes' ) , publish_ports = values . get ( 'ports' ) , expose_ports = values . get ( 'expose' ) , environment = values . get ( 'environment' ) , hostname = values . get ( 'hostname' ) , dns = values . get ( 'dns' ) , start_delay = values . get ( 'start_delay' , 0 ) , neutral = values . get ( 'neutral' , False ) , holy = values . get ( 'holy' , False ) , container_name = with_index ( values . get ( 'container_name' ) , idx ) , cap_add = values . get ( 'cap_add' ) ) if count == 1 : yield get_instance ( name ) else : for idx in range ( 1 , count + 1 ) : # TODO: configurable name/index format yield get_instance ( name , idx )
3682	def logP ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in CRClogPDict . index : methods . append ( CRC ) if CASRN in SyrresDict2 . index : methods . append ( SYRRES ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CRC : return float ( CRClogPDict . at [ CASRN , 'logP' ] ) elif Method == SYRRES : return float ( SyrresDict2 . at [ CASRN , 'logP' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
13785	def generate ( length = DEFAULT_LENGTH ) : return '' . join ( random . SystemRandom ( ) . choice ( ALPHABET ) for _ in range ( length ) )
8274	def colors ( self , n = 10 , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) colors = colorlist ( ) for i in _range ( n ) : r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s colors . append ( rng ( clr , d ) ) return colors
5247	def public ( function ) : orig_func = function while isinstance ( orig_func , partial ) : orig_func = orig_func . func set_view_func_public ( orig_func ) return function
5592	def tiles_from_bbox ( self , geometry , zoom ) : for tile in self . tile_pyramid . tiles_from_bbox ( geometry , zoom ) : yield self . tile ( * tile . id )
10323	def spanning_2d_grid ( length ) : ret = nx . grid_2d_graph ( length + 2 , length ) for i in range ( length ) : # side 0 ret . node [ ( 0 , i ) ] [ 'span' ] = 0 ret [ ( 0 , i ) ] [ ( 1 , i ) ] [ 'span' ] = 0 # side 1 ret . node [ ( length + 1 , i ) ] [ 'span' ] = 1 ret [ ( length + 1 , i ) ] [ ( length , i ) ] [ 'span' ] = 1 return ret
265	def _cumulative_returns_less_costs ( returns , costs ) : if costs is None : return ep . cum_returns ( returns ) return ep . cum_returns ( returns - costs )
4878	def validate ( self , data ) : # pylint: disable=arguments-differ lms_user_id = data . get ( 'lms_user_id' ) tpa_user_id = data . get ( 'tpa_user_id' ) user_email = data . get ( 'user_email' ) if not lms_user_id and not tpa_user_id and not user_email : raise serializers . ValidationError ( 'At least one of the following fields must be specified and map to an EnterpriseCustomerUser: ' 'lms_user_id, tpa_user_id, user_email' ) return data
776	def __getDBNameForVersion ( cls , dbVersion ) : # DB Name prefix for the given version prefix = cls . __getDBNamePrefixForVersion ( dbVersion ) # DB Name suffix suffix = Configuration . get ( 'nupic.cluster.database.nameSuffix' ) # Replace dash and dot with underscore (e.g. 'ec2-user' or ec2.user will break SQL) suffix = suffix . replace ( "-" , "_" ) suffix = suffix . replace ( "." , "_" ) # Create the name of the database for the given DB version dbName = '%s_%s' % ( prefix , suffix ) return dbName
8358	def shoebot_example ( * * shoebot_kwargs ) : def decorator ( f ) : def run ( ) : from shoebot import ShoebotInstallError # https://github.com/shoebot/shoebot/issues/206 print ( " Shoebot - %s:" % f . __name__ . replace ( "_" , " " ) ) try : import shoebot outputfile = "/tmp/shoebot-%s.png" % f . __name__ bot = shoebot . create_bot ( outputfile = outputfile ) f ( bot ) bot . finish ( ) print ( ' [passed] : %s' % outputfile ) print ( '' ) except ShoebotInstallError as e : print ( ' [failed]' , e . args [ 0 ] ) print ( '' ) except Exception : print ( ' [failed] - traceback:' ) for line in traceback . format_exc ( ) . splitlines ( ) : print ( ' %s' % line ) print ( '' ) return run return decorator
8703	def download_file ( self , filename ) : res = self . __exchange ( 'send("{filename}")' . format ( filename = filename ) ) if ( 'unexpected' in res ) or ( 'stdin' in res ) : log . error ( 'Unexpected error downloading file: %s' , res ) raise Exception ( 'Unexpected error downloading file' ) #tell device we are ready to receive self . __write ( 'C' ) #we should get a NUL terminated filename to start with sent_filename = self . __expect ( NUL ) . strip ( ) log . info ( 'receiveing ' + sent_filename ) #ACK to start download self . __write ( ACK , True ) buf = '' data = '' chunk , buf = self . __read_chunk ( buf ) #read chunks until we get an empty which is the end while chunk != '' : self . __write ( ACK , True ) data = data + chunk chunk , buf = self . __read_chunk ( buf ) return data
9030	def _expand_consumed_mesh ( self , mesh , mesh_index , row_position , passed ) : if not mesh . is_produced ( ) : return row = mesh . producing_row position = Point ( row_position . x + mesh . index_in_producing_row - mesh_index , row_position . y - INSTRUCTION_HEIGHT ) self . _expand ( row , position , passed )
481	def main_restore_embedding_layer ( ) : # Step 1: Build the embedding matrix and load the existing embedding matrix. vocabulary_size = 50000 embedding_size = 128 model_file_name = "model_word2vec_50k_128" batch_size = None print ( "Load existing embedding matrix and dictionaries" ) all_var = tl . files . load_npy_to_any ( name = model_file_name + '.npy' ) data = all_var [ 'data' ] count = all_var [ 'count' ] dictionary = all_var [ 'dictionary' ] reverse_dictionary = all_var [ 'reverse_dictionary' ] tl . nlp . save_vocab ( count , name = 'vocab_' + model_file_name + '.txt' ) del all_var , data , count load_params = tl . files . load_npz ( name = model_file_name + '.npz' ) x = tf . placeholder ( tf . int32 , shape = [ batch_size ] ) emb_net = tl . layers . EmbeddingInputlayer ( x , vocabulary_size , embedding_size , name = 'emb' ) # sess.run(tf.global_variables_initializer()) sess . run ( tf . global_variables_initializer ( ) ) tl . files . assign_params ( sess , [ load_params [ 0 ] ] , emb_net ) emb_net . print_params ( ) emb_net . print_layers ( ) # Step 2: Input word(s), output the word vector(s). word = b'hello' word_id = dictionary [ word ] print ( 'word_id:' , word_id ) words = [ b'i' , b'am' , b'tensor' , b'layer' ] word_ids = tl . nlp . words_to_word_ids ( words , dictionary , _UNK ) context = tl . nlp . word_ids_to_words ( word_ids , reverse_dictionary ) print ( 'word_ids:' , word_ids ) print ( 'context:' , context ) vector = sess . run ( emb_net . outputs , feed_dict = { x : [ word_id ] } ) print ( 'vector:' , vector . shape ) vectors = sess . run ( emb_net . outputs , feed_dict = { x : word_ids } ) print ( 'vectors:' , vectors . shape )
5463	def ddel_tasks ( provider , user_ids = None , job_ids = None , task_ids = None , labels = None , create_time_min = None , create_time_max = None ) : # Delete the requested jobs deleted_tasks , error_messages = provider . delete_jobs ( user_ids , job_ids , task_ids , labels , create_time_min , create_time_max ) # Emit any errors canceling jobs for msg in error_messages : print ( msg ) return deleted_tasks
6540	def read_file ( filepath ) : with _FILE_CACHE_LOCK : if filepath not in _FILE_CACHE : _FILE_CACHE [ filepath ] = _read_file ( filepath ) return _FILE_CACHE [ filepath ]
4501	def project ( * descs , root_file = None ) : load . ROOT_FILE = root_file desc = merge . merge ( merge . DEFAULT_PROJECT , * descs ) path = desc . get ( 'path' , '' ) if root_file : project_path = os . path . dirname ( root_file ) if path : path += ':' + project_path else : path = project_path with load . extender ( path ) : desc = recurse . recurse ( desc ) project = construct . construct ( * * desc ) project . desc = desc return project
3137	def create ( self , data ) : self . app_id = None if 'client_id' not in data : raise KeyError ( 'The authorized app must have a client_id' ) if 'client_secret' not in data : raise KeyError ( 'The authorized app must have a client_secret' ) return self . _mc_client . _post ( url = self . _build_path ( ) , data = data )
9576	def read_header ( fd , endian ) : flag_class , nzmax = read_elements ( fd , endian , [ 'miUINT32' ] ) header = { 'mclass' : flag_class & 0x0FF , 'is_logical' : ( flag_class >> 9 & 1 ) == 1 , 'is_global' : ( flag_class >> 10 & 1 ) == 1 , 'is_complex' : ( flag_class >> 11 & 1 ) == 1 , 'nzmax' : nzmax } header [ 'dims' ] = read_elements ( fd , endian , [ 'miINT32' ] ) header [ 'n_dims' ] = len ( header [ 'dims' ] ) if header [ 'n_dims' ] != 2 : raise ParseError ( 'Only matrices with dimension 2 are supported.' ) header [ 'name' ] = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) return header
1237	def from_spec ( spec , kwargs = None ) : network = util . get_object ( obj = spec , default_object = LayeredNetwork , kwargs = kwargs ) assert isinstance ( network , Network ) return network
12599	def get_sheet_list ( xl_path : str ) -> List : wb = read_xl ( xl_path ) if hasattr ( wb , 'sheetnames' ) : return wb . sheetnames else : return wb . sheet_names ( )
2559	def create_reg_message ( self ) : msg = { 'parsl_v' : PARSL_VERSION , 'python_v' : "{}.{}.{}" . format ( sys . version_info . major , sys . version_info . minor , sys . version_info . micro ) , 'os' : platform . system ( ) , 'hname' : platform . node ( ) , 'dir' : os . getcwd ( ) , } b_msg = json . dumps ( msg ) . encode ( 'utf-8' ) return b_msg
7766	def _close_stream ( self ) : self . stream . close ( ) if self . stream . transport in self . _ml_handlers : self . _ml_handlers . remove ( self . stream . transport ) self . main_loop . remove_handler ( self . stream . transport ) self . stream = None self . uplink = None
11288	def get_request_subfields ( root ) : request = root . find ( 'request' ) responsedate = root . find ( 'responseDate' ) subs = [ ( "9" , request . text ) , ( "h" , responsedate . text ) , ( "m" , request . attrib [ "metadataPrefix" ] ) ] return subs
3116	def oauth2_callback ( request ) : if 'error' in request . GET : reason = request . GET . get ( 'error_description' , request . GET . get ( 'error' , '' ) ) reason = html . escape ( reason ) return http . HttpResponseBadRequest ( 'Authorization failed {0}' . format ( reason ) ) try : encoded_state = request . GET [ 'state' ] code = request . GET [ 'code' ] except KeyError : return http . HttpResponseBadRequest ( 'Request missing state or authorization code' ) try : server_csrf = request . session [ _CSRF_KEY ] except KeyError : return http . HttpResponseBadRequest ( 'No existing session for this flow.' ) try : state = json . loads ( encoded_state ) client_csrf = state [ 'csrf_token' ] return_url = state [ 'return_url' ] except ( ValueError , KeyError ) : return http . HttpResponseBadRequest ( 'Invalid state parameter.' ) if client_csrf != server_csrf : return http . HttpResponseBadRequest ( 'Invalid CSRF token.' ) flow = _get_flow_for_token ( client_csrf , request ) if not flow : return http . HttpResponseBadRequest ( 'Missing Oauth2 flow.' ) try : credentials = flow . step2_exchange ( code ) except client . FlowExchangeError as exchange_error : return http . HttpResponseBadRequest ( 'An error has occurred: {0}' . format ( exchange_error ) ) get_storage ( request ) . put ( credentials ) signals . oauth2_authorized . send ( sender = signals . oauth2_authorized , request = request , credentials = credentials ) return shortcuts . redirect ( return_url )
12485	def get_dict_leaves ( data ) : result = [ ] if isinstance ( data , dict ) : for item in data . values ( ) : result . extend ( get_dict_leaves ( item ) ) elif isinstance ( data , list ) : result . extend ( data ) else : result . append ( data ) return result
10337	def build_spia_matrices ( nodes : Set [ str ] ) -> Dict [ str , pd . DataFrame ] : nodes = list ( sorted ( nodes ) ) # Create sheets of the excel in the given order matrices = OrderedDict ( ) for relation in KEGG_RELATIONS : matrices [ relation ] = pd . DataFrame ( 0 , index = nodes , columns = nodes ) return matrices
7795	def unregister_fetcher ( self , object_class ) : self . _lock . acquire ( ) try : cache = self . _caches . get ( object_class ) if not cache : return cache . set_fetcher ( None ) finally : self . _lock . release ( )
1349	def write_error_response ( self , message ) : self . set_status ( 404 ) response = self . make_error_response ( str ( message ) ) now = time . time ( ) spent = now - self . basehandler_starttime response [ constants . RESPONSE_KEY_EXECUTION_TIME ] = spent self . write_json_response ( response )
10230	def flatten_list_abundance ( node : ListAbundance ) -> ListAbundance : return node . __class__ ( list ( chain . from_iterable ( ( flatten_list_abundance ( member ) . members if isinstance ( member , ListAbundance ) else [ member ] ) for member in node . members ) ) )
3113	def _validate ( self , value ) : _LOGGER . info ( 'validate: Got type %s' , type ( value ) ) if value is not None and not isinstance ( value , client . Flow ) : raise TypeError ( 'Property {0} must be convertible to a flow ' 'instance; received: {1}.' . format ( self . _name , value ) )
3280	def resolve_provider ( self , path ) : # Find DAV provider that matches the share share = None lower_path = path . lower ( ) for r in self . sorted_share_list : # @@: Case sensitivity should be an option of some sort here; # os.path.normpath might give the preferred case for a filename. if r == "/" : share = r break elif lower_path == r or lower_path . startswith ( r + "/" ) : share = r break if share is None : return None , None return share , self . provider_map . get ( share )
3840	async def set_typing ( self , set_typing_request ) : response = hangouts_pb2 . SetTypingResponse ( ) await self . _pb_request ( 'conversations/settyping' , set_typing_request , response ) return response
4070	def upload ( self ) : result = { "success" : [ ] , "failure" : [ ] , "unchanged" : [ ] } self . _create_prelim ( ) for item in self . payload : if "key" not in item : result [ "failure" ] . append ( item ) continue attach = str ( self . basedir . joinpath ( item [ "filename" ] ) ) authdata = self . _get_auth ( attach , item [ "key" ] , md5 = item . get ( "md5" , None ) ) # no need to keep going if the file exists if authdata . get ( "exists" ) : result [ "unchanged" ] . append ( item ) continue self . _upload_file ( authdata , attach , item [ "key" ] ) result [ "success" ] . append ( item ) return result
11052	def sync ( self ) : self . log . info ( 'Starting a sync...' ) def log_success ( result ) : self . log . info ( 'Sync completed successfully' ) return result def log_failure ( failure ) : self . log . failure ( 'Sync failed' , failure , LogLevel . error ) return failure return ( self . marathon_client . get_apps ( ) . addCallback ( self . _apps_acme_domains ) . addCallback ( self . _filter_new_domains ) . addCallback ( self . _issue_certs ) . addCallbacks ( log_success , log_failure ) )
9427	def getinfo ( self , name ) : rarinfo = self . NameToInfo . get ( name ) if rarinfo is None : raise KeyError ( 'There is no item named %r in the archive' % name ) return rarinfo
2124	def associate_always_node ( self , parent , child = None , * * kwargs ) : return self . _assoc_or_create ( 'always' , parent , child , * * kwargs )
7580	def get_evanno_table ( self , kvalues , max_var_multiple = 0 , quiet = False ) : ## do not allow bad vals if max_var_multiple : if max_var_multiple < 1 : raise ValueError ( 'max_variance_multiplier must be >1' ) table = _get_evanno_table ( self , kvalues , max_var_multiple , quiet ) return table
7785	def error ( self , error_data ) : if not self . active : return if not self . _try_backup_item ( ) : self . _error_handler ( self . address , error_data ) self . cache . invalidate_object ( self . address ) self . _deactivate ( )
13888	def DeleteDirectory ( directory , skip_on_error = False ) : _AssertIsLocal ( directory ) import shutil def OnError ( fn , path , excinfo ) : ''' Remove the read-only flag and try to remove again. On Windows, rmtree fails when trying to remove a read-only file. This fix it! Another case: Read-only directories return True in os.access test. It seems that read-only directories has it own flag (looking at the property windows on Explorer). ''' if IsLink ( path ) : return if fn is os . remove and os . access ( path , os . W_OK ) : raise # Make the file WRITEABLE and executes the original delete function (osfunc) import stat os . chmod ( path , stat . S_IWRITE ) fn ( path ) try : if not os . path . isdir ( directory ) : if skip_on_error : return from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( directory ) shutil . rmtree ( directory , onerror = OnError ) except : if not skip_on_error : raise
1101	def context_diff ( a , b , fromfile = '' , tofile = '' , fromfiledate = '' , tofiledate = '' , n = 3 , lineterm = '\n' ) : prefix = dict ( insert = '+ ' , delete = '- ' , replace = '! ' , equal = ' ' ) started = False for group in SequenceMatcher ( None , a , b ) . get_grouped_opcodes ( n ) : if not started : started = True # fromdate = '\t{}'.format(fromfiledate) if fromfiledate else '' fromdate = '\t%s' % ( fromfiledate ) if fromfiledate else '' # todate = '\t{}'.format(tofiledate) if tofiledate else '' todate = '\t%s' % ( tofiledate ) if tofiledate else '' # yield '*** {}{}{}'.format(fromfile, fromdate, lineterm) yield '*** %s%s%s' % ( fromfile , fromdate , lineterm ) # yield '--- {}{}{}'.format(tofile, todate, lineterm) yield '--- %s%s%s' % ( tofile , todate , lineterm ) first , last = group [ 0 ] , group [ - 1 ] yield '***************' + lineterm file1_range = _format_range_context ( first [ 1 ] , last [ 2 ] ) # yield '*** {} ****{}'.format(file1_range, lineterm) yield '*** %s ****%s' % ( file1_range , lineterm ) if any ( tag in ( 'replace' , 'delete' ) for tag , _ , _ , _ , _ in group ) : for tag , i1 , i2 , _ , _ in group : if tag != 'insert' : for line in a [ i1 : i2 ] : yield prefix [ tag ] + line file2_range = _format_range_context ( first [ 3 ] , last [ 4 ] ) # yield '--- {} ----{}'.format(file2_range, lineterm) yield '--- %s ----%s' % ( file2_range , lineterm ) if any ( tag in ( 'replace' , 'insert' ) for tag , _ , _ , _ , _ in group ) : for tag , _ , _ , j1 , j2 in group : if tag != 'delete' : for line in b [ j1 : j2 ] : yield prefix [ tag ] + line
9858	def create_url ( self , path , params = { } , opts = { } ) : if opts : warnings . warn ( '`opts` has been deprecated. Use `params` instead.' , DeprecationWarning , stacklevel = 2 ) params = params or opts if self . _shard_strategy == SHARD_STRATEGY_CRC : crc = zlib . crc32 ( path . encode ( 'utf-8' ) ) & 0xffffffff index = crc % len ( self . _domains ) # Deterministically choose domain domain = self . _domains [ index ] elif self . _shard_strategy == SHARD_STRATEGY_CYCLE : domain = self . _domains [ self . _shard_next_index ] self . _shard_next_index = ( self . _shard_next_index + 1 ) % len ( self . _domains ) else : domain = self . _domains [ 0 ] scheme = "https" if self . _use_https else "http" url_obj = UrlHelper ( domain , path , scheme , sign_key = self . _sign_key , include_library_param = self . _include_library_param , params = params ) return str ( url_obj )
7957	def _continue_tls_handshake ( self ) : try : logger . debug ( " do_handshake()" ) self . _socket . do_handshake ( ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_READ : self . _tls_state = "want_read" logger . debug ( " want_read" ) self . _state_cond . notify ( ) return elif err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : self . _tls_state = "want_write" logger . debug ( " want_write" ) self . _write_queue . appendleft ( TLSHandshake ) return else : raise self . _tls_state = "connected" self . _set_state ( "connected" ) self . _auth_properties [ 'security-layer' ] = "TLS" if "tls-unique" in CHANNEL_BINDING_TYPES : try : # pylint: disable=E1103 tls_unique = self . _socket . get_channel_binding ( "tls-unique" ) except ValueError : pass else : self . _auth_properties [ 'channel-binding' ] = { "tls-unique" : tls_unique } try : cipher = self . _socket . cipher ( ) except AttributeError : # SSLSocket.cipher doesn't work on PyPy cipher = "unknown" cert = get_certificate_from_ssl_socket ( self . _socket ) self . event ( TLSConnectedEvent ( cipher , cert ) )
13586	def add_object ( cls , attr , title = '' , display = '' ) : global klass_count klass_count += 1 fn_name = 'dyn_fn_%d' % klass_count cls . list_display . append ( fn_name ) if not title : title = attr . capitalize ( ) # python scoping is a bit weird with default values, if it isn't # referenced the inner function won't see it, so assign it for use _display = display def _ref ( self , obj ) : field_obj = admin_obj_attr ( obj , attr ) if not field_obj : return '' return _obj_display ( field_obj , _display ) _ref . short_description = title _ref . allow_tags = True _ref . admin_order_field = attr setattr ( cls , fn_name , _ref )
759	def appendInputWithSimilarValues ( inputs ) : numInputs = len ( inputs ) for i in xrange ( numInputs ) : input = inputs [ i ] for j in xrange ( len ( input ) - 1 ) : if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput = copy . deepcopy ( input ) newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) break
9168	def post_publication_processing ( event , cursor ) : module_ident , ident_hash = event . module_ident , event . ident_hash celery_app = get_current_registry ( ) . celery_app # Check baking is not already queued. cursor . execute ( 'SELECT result_id::text ' 'FROM document_baking_result_associations ' 'WHERE module_ident = %s' , ( module_ident , ) ) for result in cursor . fetchall ( ) : state = celery_app . AsyncResult ( result [ 0 ] ) . state if state in ( 'QUEUED' , 'STARTED' , 'RETRY' ) : logger . debug ( 'Already queued module_ident={} ident_hash={}' . format ( module_ident , ident_hash ) ) return logger . debug ( 'Queued for processing module_ident={} ident_hash={}' . format ( module_ident , ident_hash ) ) recipe_ids = _get_recipe_ids ( module_ident , cursor ) update_module_state ( cursor , module_ident , 'processing' , recipe_ids [ 0 ] ) # Commit the state change before preceding. cursor . connection . commit ( ) # Start of task # FIXME Looking up the task isn't the most clear usage here. task_name = 'cnxpublishing.subscribers.baking_processor' baking_processor = celery_app . tasks [ task_name ] result = baking_processor . delay ( module_ident , ident_hash ) baking_processor . backend . store_result ( result . id , None , 'QUEUED' ) # Save the mapping between a celery task and this event. track_baking_proc_state ( result , module_ident , cursor )
12680	def get_formatted_messages ( self , formats , label , context ) : format_templates = { } for fmt in formats : # conditionally turn off autoescaping for .txt extensions in format if fmt . endswith ( ".txt" ) : context . autoescape = False format_templates [ fmt ] = render_to_string ( ( "notification/%s/%s" % ( label , fmt ) , "notification/%s" % fmt ) , context_instance = context ) return format_templates
8243	def guess_name ( clr ) : clr = Color ( clr ) if clr . is_transparent : return "transparent" if clr . is_black : return "black" if clr . is_white : return "white" if clr . is_black : return "black" for name in named_colors : try : r , g , b = named_colors [ name ] except : continue if r == clr . r and g == clr . g and b == clr . b : return name for shade in shades : if clr in shade : return shade . name + " " + clr . nearest_hue ( ) break return clr . nearest_hue ( )
3460	def single_reaction_deletion ( model , reaction_list = None , method = "fba" , solution = None , processes = None , * * kwargs ) : return _multi_deletion ( model , 'reaction' , element_lists = _element_lists ( model . reactions , reaction_list ) , method = method , solution = solution , processes = processes , * * kwargs )
3890	def markdown ( tag ) : return ( MARKDOWN_START . format ( tag = tag ) , MARKDOWN_END . format ( tag = tag ) )
4524	def save ( self , project_file = '' ) : self . _request_project_file ( project_file ) data_file . dump ( self . desc . as_dict ( ) , self . project_file )
1228	def tf_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : # Mean loss per instance loss_per_instance = self . fn_loss_per_instance ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) # Returns no-op. updated = self . memory . update_batch ( loss_per_instance = loss_per_instance ) with tf . control_dependencies ( control_inputs = ( updated , ) ) : loss = tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 ) # Loss without regularization summary. if 'losses' in self . summary_labels : tf . contrib . summary . scalar ( name = 'loss-without-regularization' , tensor = loss ) # Regularization losses. losses = self . fn_regularization_losses ( states = states , internals = internals , update = update ) if len ( losses ) > 0 : loss += tf . add_n ( inputs = [ losses [ name ] for name in sorted ( losses ) ] ) if 'regularization' in self . summary_labels : for name in sorted ( losses ) : tf . contrib . summary . scalar ( name = ( 'regularization/' + name ) , tensor = losses [ name ] ) # Total loss summary. if 'losses' in self . summary_labels or 'total-loss' in self . summary_labels : tf . contrib . summary . scalar ( name = 'total-loss' , tensor = loss ) return loss
5509	def get_permissions ( self , path ) : path = pathlib . PurePosixPath ( path ) parents = filter ( lambda p : p . is_parent ( path ) , self . permissions ) perm = min ( parents , key = lambda p : len ( path . relative_to ( p . path ) . parts ) , default = Permission ( ) , ) return perm
10038	def pick_coda_from_letter ( letter ) : try : __ , __ , coda = split_phonemes ( letter , onset = False , nucleus = False , coda = True ) except ValueError : return None else : return coda
1059	def update_wrapper ( wrapper , wrapped , assigned = WRAPPER_ASSIGNMENTS , updated = WRAPPER_UPDATES ) : for attr in assigned : setattr ( wrapper , attr , getattr ( wrapped , attr ) ) for attr in updated : getattr ( wrapper , attr ) . update ( getattr ( wrapped , attr , { } ) ) # Return the wrapper so this can be used as a decorator via partial() return wrapper
4688	def get_shared_secret ( priv , pub ) : pub_point = pub . point ( ) priv_point = int ( repr ( priv ) , 16 ) res = pub_point * priv_point res_hex = "%032x" % res . x ( ) # Zero padding res_hex = "0" * ( 64 - len ( res_hex ) ) + res_hex return res_hex
6487	def _translate_hits ( es_response ) : def translate_result ( result ) : """ Any conversion from ES result syntax into our search engine syntax """ translated_result = copy . copy ( result ) data = translated_result . pop ( "_source" ) translated_result . update ( { "data" : data , "score" : translated_result [ "_score" ] } ) return translated_result def translate_facet ( result ) : """ Any conversion from ES facet syntax into our search engine sytax """ terms = { term [ "term" ] : term [ "count" ] for term in result [ "terms" ] } return { "terms" : terms , "total" : result [ "total" ] , "other" : result [ "other" ] , } results = [ translate_result ( hit ) for hit in es_response [ "hits" ] [ "hits" ] ] response = { "took" : es_response [ "took" ] , "total" : es_response [ "hits" ] [ "total" ] , "max_score" : es_response [ "hits" ] [ "max_score" ] , "results" : results , } if "facets" in es_response : response [ "facets" ] = { facet : translate_facet ( es_response [ "facets" ] [ facet ] ) for facet in es_response [ "facets" ] } return response
1687	def _CollapseStrings ( elided ) : if _RE_PATTERN_INCLUDE . match ( elided ) : return elided # Remove escaped characters first to make quote/single quote collapsing # basic. Things that look like escaped characters shouldn't occur # outside of strings and chars. elided = _RE_PATTERN_CLEANSE_LINE_ESCAPES . sub ( '' , elided ) # Replace quoted strings and digit separators. Both single quotes # and double quotes are processed in the same loop, otherwise # nested quotes wouldn't work. collapsed = '' while True : # Find the first quote character match = Match ( r'^([^\'"]*)([\'"])(.*)$' , elided ) if not match : collapsed += elided break head , quote , tail = match . groups ( ) if quote == '"' : # Collapse double quoted strings second_quote = tail . find ( '"' ) if second_quote >= 0 : collapsed += head + '""' elided = tail [ second_quote + 1 : ] else : # Unmatched double quote, don't bother processing the rest # of the line since this is probably a multiline string. collapsed += elided break else : # Found single quote, check nearby text to eliminate digit separators. # # There is no special handling for floating point here, because # the integer/fractional/exponent parts would all be parsed # correctly as long as there are digits on both sides of the # separator. So we are fine as long as we don't see something # like "0.'3" (gcc 4.9.0 will not allow this literal). if Search ( r'\b(?:0[bBxX]?|[1-9])[0-9a-fA-F]*$' , head ) : match_literal = Match ( r'^((?:\'?[0-9a-zA-Z_])*)(.*)$' , "'" + tail ) collapsed += head + match_literal . group ( 1 ) . replace ( "'" , '' ) elided = match_literal . group ( 2 ) else : second_quote = tail . find ( '\'' ) if second_quote >= 0 : collapsed += head + "''" elided = tail [ second_quote + 1 : ] else : # Unmatched single quote collapsed += elided break return collapsed
11922	def paginate_dataframe ( self , dataframe ) : if self . paginator is None : return None return self . paginator . paginate_dataframe ( dataframe , self . request , view = self )
11821	def create ( self , name , value ) : if value is None : raise ValueError ( 'Setting value cannot be `None`.' ) model = Setting . get_model_for_value ( value ) # Call `create()` method on the super class to avoid recursion. obj = super ( SettingQuerySet , model . objects . all ( ) ) . create ( name = name , value = value ) return obj
11587	def object ( self , infotype , key ) : redisent = self . redises [ self . _getnodenamefor ( key ) + '_slave' ] return getattr ( redisent , 'object' ) ( infotype , key )
2412	def gen_feats ( self , p_set ) : if self . _initialized != True : error_message = "Dictionaries have not been initialized." log . exception ( error_message ) raise util_functions . InputError ( p_set , error_message ) textual_features = [ ] for i in xrange ( 0 , len ( p_set . _essay_sets ) ) : textual_features . append ( self . _extractors [ i ] . gen_feats ( p_set . _essay_sets [ i ] ) ) textual_matrix = numpy . concatenate ( textual_features , axis = 1 ) predictor_matrix = numpy . array ( p_set . _numeric_features ) print textual_matrix . shape print predictor_matrix . shape overall_matrix = numpy . concatenate ( ( textual_matrix , predictor_matrix ) , axis = 1 ) return overall_matrix . copy ( )
9567	def byteswap ( fmt , data , offset = 0 ) : data = BytesIO ( data ) data . seek ( offset ) data_swapped = BytesIO ( ) for f in fmt : swapped = data . read ( int ( f ) ) [ : : - 1 ] data_swapped . write ( swapped ) return data_swapped . getvalue ( )
8894	def add_to_deleted_models ( sender , instance = None , * args , * * kwargs ) : if issubclass ( sender , SyncableModel ) : instance . _update_deleted_models ( )
13525	def error ( code : int , * args , * * kwargs ) -> HedgehogCommandError : # TODO add proper error code if code == FAILED_COMMAND and len ( args ) >= 1 and args [ 0 ] == "Emergency Shutdown activated" : return EmergencyShutdown ( * args , * * kwargs ) return _errors [ code ] ( * args , * * kwargs )
8586	def get_attached_cdrom ( self , datacenter_id , server_id , cdrom_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/cdroms/%s' % ( datacenter_id , server_id , cdrom_id ) ) return response
6762	def write_pgpass ( self , name = None , site = None , use_sudo = 0 , root = 0 ) : r = self . database_renderer ( name = name , site = site ) root = int ( root ) use_sudo = int ( use_sudo ) r . run ( 'touch {pgpass_path}' ) if '~' in r . env . pgpass_path : r . run ( 'chmod {pgpass_chmod} {pgpass_path}' ) else : r . sudo ( 'chmod {pgpass_chmod} {pgpass_path}' ) if root : r . env . shell_username = r . env . get ( 'db_root_username' , 'postgres' ) r . env . shell_password = r . env . get ( 'db_root_password' , 'password' ) else : r . env . shell_username = r . env . db_user r . env . shell_password = r . env . db_password r . append ( '{db_host}:{port}:*:{shell_username}:{shell_password}' , r . env . pgpass_path , use_sudo = use_sudo )
9622	def buttons ( self ) : return [ name for name , value in rController . _buttons . items ( ) if self . gamepad . wButtons & value == value ]
10404	def canonical_statistics_dtype ( spanning_cluster = True ) : fields = list ( ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability' , 'float64' ) , ] ) fields . extend ( [ ( 'max_cluster_size' , 'float64' ) , ( 'moments' , '(5,)float64' ) , ] ) return _ndarray_dtype ( fields )
5103	def draw_graph ( self , line_kwargs = None , scatter_kwargs = None , * * kwargs ) : if not HAS_MATPLOTLIB : raise ImportError ( "Matplotlib is required to draw the graph." ) fig = plt . figure ( figsize = kwargs . get ( 'figsize' , ( 7 , 7 ) ) ) ax = fig . gca ( ) mpl_kwargs = { 'line_kwargs' : line_kwargs , 'scatter_kwargs' : scatter_kwargs , 'pos' : kwargs . get ( 'pos' ) } line_kwargs , scatter_kwargs = self . lines_scatter_args ( * * mpl_kwargs ) edge_collection = LineCollection ( * * line_kwargs ) ax . add_collection ( edge_collection ) ax . scatter ( * * scatter_kwargs ) if hasattr ( ax , 'set_facecolor' ) : ax . set_facecolor ( kwargs . get ( 'bgcolor' , [ 1 , 1 , 1 , 1 ] ) ) else : ax . set_axis_bgcolor ( kwargs . get ( 'bgcolor' , [ 1 , 1 , 1 , 1 ] ) ) ax . get_xaxis ( ) . set_visible ( False ) ax . get_yaxis ( ) . set_visible ( False ) if 'fname' in kwargs : # savefig needs a positional argument for some reason new_kwargs = { k : v for k , v in kwargs . items ( ) if k in SAVEFIG_KWARGS } fig . savefig ( kwargs [ 'fname' ] , * * new_kwargs ) else : plt . ion ( ) plt . show ( )
4274	def thumbnail ( self ) : if self . _thumbnail : # stop if it is already set return self . _thumbnail # Test the thumbnail from the Markdown file. thumbnail = self . meta . get ( 'thumbnail' , [ '' ] ) [ 0 ] if thumbnail and isfile ( join ( self . src_path , thumbnail ) ) : self . _thumbnail = url_from_path ( join ( self . name , get_thumb ( self . settings , thumbnail ) ) ) self . logger . debug ( "Thumbnail for %r : %s" , self , self . _thumbnail ) return self . _thumbnail else : # find and return the first landscape image for f in self . medias : ext = splitext ( f . filename ) [ 1 ] if ext . lower ( ) in self . settings [ 'img_extensions' ] : # Use f.size if available as it is quicker (in cache), but # fallback to the size of src_path if dst_path is missing size = f . size if size is None : size = get_size ( f . src_path ) if size [ 'width' ] > size [ 'height' ] : self . _thumbnail = ( url_quote ( self . name ) + '/' + f . thumbnail ) self . logger . debug ( "Use 1st landscape image as thumbnail for %r : %s" , self , self . _thumbnail ) return self . _thumbnail # else simply return the 1st media file if not self . _thumbnail and self . medias : for media in self . medias : if media . thumbnail is not None : self . _thumbnail = ( url_quote ( self . name ) + '/' + media . thumbnail ) break else : self . logger . warning ( "No thumbnail found for %r" , self ) return None self . logger . debug ( "Use the 1st image as thumbnail for %r : %s" , self , self . _thumbnail ) return self . _thumbnail # use the thumbnail of their sub-directories if not self . _thumbnail : for path , album in self . gallery . get_albums ( self . path ) : if album . thumbnail : self . _thumbnail = ( url_quote ( self . name ) + '/' + album . thumbnail ) self . logger . debug ( "Using thumbnail from sub-directory for %r : %s" , self , self . _thumbnail ) return self . _thumbnail self . logger . error ( 'Thumbnail not found for %r' , self ) return None
13265	def get_plugin_instance ( plugin_class , * args , * * kwargs ) : assert issubclass ( plugin_class , BasePlugin ) , type ( plugin_class ) global _yaz_plugin_instance_cache qualname = plugin_class . __qualname__ if not qualname in _yaz_plugin_instance_cache : plugin_class = get_plugin_list ( ) [ qualname ] _yaz_plugin_instance_cache [ qualname ] = plugin = plugin_class ( * args , * * kwargs ) # find any yaz.dependency decorators, and call them when necessary funcs = [ func for _ , func in inspect . getmembers ( plugin ) if inspect . ismethod ( func ) and hasattr ( func , "yaz_dependency_config" ) ] for func in funcs : signature = inspect . signature ( func ) assert all ( parameter . kind is parameter . POSITIONAL_OR_KEYWORD and issubclass ( parameter . annotation , BasePlugin ) for parameter in signature . parameters . values ( ) ) , "All parameters for {} must type hint to a BasePlugin" . format ( func ) func ( * [ get_plugin_instance ( parameter . annotation ) for parameter in signature . parameters . values ( ) ] ) return _yaz_plugin_instance_cache [ qualname ]
7982	def auth_finish ( self , _unused ) : self . lock . acquire ( ) try : self . __logger . debug ( "Authenticated" ) self . authenticated = True self . state_change ( "authorized" , self . my_jid ) self . _post_auth ( ) finally : self . lock . release ( )
1992	def _get_id ( self ) : id_ = self . _last_id . value self . _last_id . value += 1 return id_
9940	def set_options ( self , * * options ) : self . interactive = options [ 'interactive' ] self . verbosity = options [ 'verbosity' ] self . symlink = options [ 'link' ] self . clear = options [ 'clear' ] self . dry_run = options [ 'dry_run' ] ignore_patterns = options [ 'ignore_patterns' ] if options [ 'use_default_ignore_patterns' ] : ignore_patterns += [ 'CVS' , '.*' , '*~' ] self . ignore_patterns = list ( set ( ignore_patterns ) ) self . post_process = options [ 'post_process' ]
8227	def _makeColorableInstance ( self , clazz , args , kwargs ) : kwargs = dict ( kwargs ) fill = kwargs . get ( 'fill' , self . _canvas . fillcolor ) if not isinstance ( fill , Color ) : fill = Color ( fill , mode = 'rgb' , color_range = 1 ) kwargs [ 'fill' ] = fill stroke = kwargs . get ( 'stroke' , self . _canvas . strokecolor ) if not isinstance ( stroke , Color ) : stroke = Color ( stroke , mode = 'rgb' , color_range = 1 ) kwargs [ 'stroke' ] = stroke kwargs [ 'strokewidth' ] = kwargs . get ( 'strokewidth' , self . _canvas . strokewidth ) inst = clazz ( self , * args , * * kwargs ) return inst
12957	def _get_key_for_index ( self , indexedField , val ) : # If provided an IRField, use the toIndex from that (to support compat_ methods if hasattr ( indexedField , 'toIndex' ) : val = indexedField . toIndex ( val ) else : # Otherwise, look up the indexed field from the model val = self . fields [ indexedField ] . toIndex ( val ) return '' . join ( [ INDEXED_REDIS_PREFIX , self . keyName , ':idx:' , indexedField , ':' , val ] )
12194	def _validate_first_message ( cls , msg ) : data = cls . _unpack_message ( msg ) logger . debug ( data ) if data != cls . RTM_HANDSHAKE : raise SlackApiError ( 'Unexpected response: {!r}' . format ( data ) ) logger . info ( 'Joined real-time messaging.' )
9238	def user_and_project_from_git ( self , options , arg0 = None , arg1 = None ) : user , project = self . user_project_from_option ( options , arg0 , arg1 ) if user and project : return user , project try : remote = subprocess . check_output ( [ 'git' , 'config' , '--get' , 'remote.{0}.url' . format ( options . git_remote ) ] ) except subprocess . CalledProcessError : return None , None except WindowsError : print ( "git binary not found." ) exit ( 1 ) else : return self . user_project_from_remote ( remote )
1445	def register_metric ( self , name , metric , time_bucket_in_sec ) : if name in self . metrics_map : raise RuntimeError ( "Another metric has already been registered with name: %s" % name ) Log . debug ( "Register metric: %s, with interval: %s" , name , str ( time_bucket_in_sec ) ) self . metrics_map [ name ] = metric if time_bucket_in_sec in self . time_bucket_in_sec_to_metrics_name : self . time_bucket_in_sec_to_metrics_name [ time_bucket_in_sec ] . append ( name ) else : self . time_bucket_in_sec_to_metrics_name [ time_bucket_in_sec ] = [ name ] self . _register_timer_task ( time_bucket_in_sec )
13758	def split_path ( path ) : result_parts = [ ] #todo: check loops while path != "/" : parts = os . path . split ( path ) if parts [ 1 ] == path : result_parts . insert ( 0 , parts [ 1 ] ) break elif parts [ 0 ] == path : result_parts . insert ( 0 , parts [ 0 ] ) break else : path = parts [ 0 ] result_parts . insert ( 0 , parts [ 1 ] ) return result_parts
5044	def notify_program_learners ( cls , enterprise_customer , program_details , users ) : program_name = program_details . get ( 'title' ) program_branding = program_details . get ( 'type' ) program_uuid = program_details . get ( 'uuid' ) lms_root_url = get_configuration_value_for_site ( enterprise_customer . site , 'LMS_ROOT_URL' , settings . LMS_ROOT_URL ) program_path = urlquote ( '/dashboard/programs/{program_uuid}/?tpa_hint={tpa_hint}' . format ( program_uuid = program_uuid , tpa_hint = enterprise_customer . identity_provider , ) ) destination_url = '{site}/{login_or_register}?next={program_path}' . format ( site = lms_root_url , login_or_register = '{login_or_register}' , program_path = program_path ) program_type = 'program' program_start = get_earliest_start_date_from_program ( program_details ) with mail . get_connection ( ) as email_conn : for user in users : login_or_register = 'register' if isinstance ( user , PendingEnterpriseCustomerUser ) else 'login' destination_url = destination_url . format ( login_or_register = login_or_register ) send_email_notification_message ( user = user , enrolled_in = { 'name' : program_name , 'url' : destination_url , 'type' : program_type , 'start' : program_start , 'branding' : program_branding , } , enterprise_customer = enterprise_customer , email_connection = email_conn )
635	def computeActivity ( self , activePresynapticCells , connectedPermanence ) : numActiveConnectedSynapsesForSegment = [ 0 ] * self . _nextFlatIdx numActivePotentialSynapsesForSegment = [ 0 ] * self . _nextFlatIdx threshold = connectedPermanence - EPSILON for cell in activePresynapticCells : for synapse in self . _synapsesForPresynapticCell [ cell ] : flatIdx = synapse . segment . flatIdx numActivePotentialSynapsesForSegment [ flatIdx ] += 1 if synapse . permanence > threshold : numActiveConnectedSynapsesForSegment [ flatIdx ] += 1 return ( numActiveConnectedSynapsesForSegment , numActivePotentialSynapsesForSegment )
5478	def get_operation_full_job_id ( op ) : job_id = op . get_field ( 'job-id' ) task_id = op . get_field ( 'task-id' ) if task_id : return '%s.%s' % ( job_id , task_id ) else : return job_id
12899	def get_play_status ( self ) : status = yield from self . handle_int ( self . API . get ( 'status' ) ) return self . PLAY_STATES . get ( status )
9618	def UnPlug ( self , force = False ) : if force : _xinput . UnPlugForce ( c_uint ( self . id ) ) else : _xinput . UnPlug ( c_uint ( self . id ) ) while self . id not in self . available_ids ( ) : if self . id == 0 : break
6194	def datafile_from_hash ( hash_ , prefix , path ) : pattern = '%s_%s*.h*' % ( prefix , hash_ ) datafiles = list ( path . glob ( pattern ) ) if len ( datafiles ) == 0 : raise NoMatchError ( 'No matches for "%s"' % pattern ) if len ( datafiles ) > 1 : raise MultipleMatchesError ( 'More than 1 match for "%s"' % pattern ) return datafiles [ 0 ]
1670	def ProcessFileData ( filename , file_extension , lines , error , extra_check_functions = None ) : lines = ( [ '// marker so line numbers and indices both start at 1' ] + lines + [ '// marker so line numbers end in a known way' ] ) include_state = _IncludeState ( ) function_state = _FunctionState ( ) nesting_state = NestingState ( ) ResetNolintSuppressions ( ) CheckForCopyright ( filename , lines , error ) ProcessGlobalSuppresions ( lines ) RemoveMultiLineComments ( filename , lines , error ) clean_lines = CleansedLines ( lines ) if file_extension in GetHeaderExtensions ( ) : CheckForHeaderGuard ( filename , clean_lines , error ) for line in range ( clean_lines . NumLines ( ) ) : ProcessLine ( filename , file_extension , clean_lines , line , include_state , function_state , nesting_state , error , extra_check_functions ) FlagCxx11Features ( filename , clean_lines , line , error ) nesting_state . CheckCompletedBlocks ( filename , error ) CheckForIncludeWhatYouUse ( filename , clean_lines , include_state , error ) # Check that the .cc file has included its header if it exists. if _IsSourceExtension ( file_extension ) : CheckHeaderFileIncluded ( filename , include_state , error ) # We check here rather than inside ProcessLine so that we see raw # lines rather than "cleaned" lines. CheckForBadCharacters ( filename , lines , error ) CheckForNewlineAtEOF ( filename , lines , error )
1515	def scp_package ( package_file , destinations , cl_args ) : pids = [ ] for dest in destinations : if is_self ( dest ) : continue Log . info ( "Server: %s" % dest ) file_path = "/tmp/heron.tar.gz" dest_file_path = "%s:%s" % ( dest , file_path ) remote_cmd = "rm -rf ~/.heron && mkdir ~/.heron " "&& tar -xzvf %s -C ~/.heron --strip-components 1" % ( file_path ) cmd = '%s && %s' % ( scp_cmd ( package_file , dest_file_path , cl_args ) , ssh_remote_execute ( remote_cmd , dest , cl_args ) ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) pids . append ( { "pid" : pid , "dest" : dest } ) errors = [ ] for entry in pids : pid = entry [ "pid" ] return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : errors . append ( "Failed to scp package to %s with error:\n%s" % ( entry [ "dest" ] , output [ 1 ] ) ) if errors : for error in errors : Log . error ( error ) sys . exit ( - 1 ) Log . info ( "Done distributing packages" )
12285	def add ( self , repo ) : key = self . key ( repo . username , repo . reponame ) repo . key = key self . repos [ key ] = repo return key
5832	def get ( self , data_view_id ) : failure_message = "Dataview get failed" return self . _get_success_json ( self . _get ( 'v1/data_views/' + data_view_id , None , failure_message = failure_message ) ) [ 'data' ] [ 'data_view' ]
4989	def redirect ( self , request , * args , * * kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( * * kwargs ) resource_id = course_key or course_run_id or program_uuid # Replace enterprise UUID and resource ID with '{}', to easily match with a path in RouterView.VIEWS. Example: # /enterprise/fake-uuid/course/course-v1:cool+course+2017/enroll/ -> /enterprise/{}/course/{}/enroll/ path = re . sub ( '{}|{}' . format ( enterprise_customer_uuid , re . escape ( resource_id ) ) , '{}' , request . path ) # Remove course_key from kwargs if it exists because delegate views are not expecting it. kwargs . pop ( 'course_key' , None ) return self . VIEWS [ path ] . as_view ( ) ( request , * args , * * kwargs )
10665	def stoichiometry_coefficient ( compound , element ) : stoichiometry = parse_compound ( compound . strip ( ) ) . count ( ) return stoichiometry [ element ]
8949	def warning ( msg ) : _flush ( ) sys . stderr . write ( "\033[1;7;33;40mWARNING: {}\033[0m\n" . format ( msg ) ) sys . stderr . flush ( )
3270	def md_entry ( node ) : key = None value = None if 'key' in node . attrib : key = node . attrib [ 'key' ] else : key = None if key in [ 'time' , 'elevation' ] or key . startswith ( 'custom_dimension' ) : value = md_dimension_info ( key , node . find ( "dimensionInfo" ) ) elif key == 'DynamicDefaultValues' : value = md_dynamic_default_values_info ( key , node . find ( "DynamicDefaultValues" ) ) elif key == 'JDBC_VIRTUAL_TABLE' : value = md_jdbc_virtual_table ( key , node . find ( "virtualTable" ) ) else : value = node . text if None in [ key , value ] : return None else : return ( key , value )
5429	def _name_for_command ( command ) : lines = command . splitlines ( ) for line in lines : line = line . strip ( ) if line and not line . startswith ( '#' ) and line != '\\' : return os . path . basename ( re . split ( r'\s' , line ) [ 0 ] ) return 'command'
185	def almost_equals ( self , other , max_distance = 1e-4 , points_per_edge = 8 ) : if self . label != other . label : return False return self . coords_almost_equals ( other , max_distance = max_distance , points_per_edge = points_per_edge )
11269	def substitute ( prev , * args , * * kw ) : template_obj = string . Template ( * args , * * kw ) for data in prev : yield template_obj . substitute ( data )
13228	def get_installation_token ( installation_id , integration_jwt ) : api_root = 'https://api.github.com' url = '{root}/installations/{id_:d}/access_tokens' . format ( api_root = api_root , id_ = installation_id ) headers = { 'Authorization' : 'Bearer {0}' . format ( integration_jwt . decode ( 'utf-8' ) ) , 'Accept' : 'application/vnd.github.machine-man-preview+json' } resp = requests . post ( url , headers = headers ) resp . raise_for_status ( ) return resp . json ( )
6659	def _bias_correction ( V_IJ , inbag , pred_centered , n_trees ) : n_train_samples = inbag . shape [ 0 ] n_var = np . mean ( np . square ( inbag [ 0 : n_trees ] ) . mean ( axis = 1 ) . T . view ( ) - np . square ( inbag [ 0 : n_trees ] . mean ( axis = 1 ) ) . T . view ( ) ) boot_var = np . square ( pred_centered ) . sum ( axis = 1 ) / n_trees bias_correction = n_train_samples * n_var * boot_var / n_trees V_IJ_unbiased = V_IJ - bias_correction return V_IJ_unbiased
1049	def print_exception ( etype , value , tb , limit = None , file = None ) : if file is None : # TODO: Use sys.stderr when that's implemented. file = open ( '/dev/stderr' , 'w' ) #file = sys.stderr if tb : _print ( file , 'Traceback (most recent call last):' ) print_tb ( tb , limit , file ) lines = format_exception_only ( etype , value ) for line in lines : _print ( file , line , '' )
11388	def parse ( self ) : if self . parsed : return self . callbacks = { } # search for main and any main_* callable objects regex = re . compile ( "^{}_?" . format ( self . function_name ) , flags = re . I ) mains = set ( ) body = self . body ast_tree = ast . parse ( self . body , self . path ) for n in ast_tree . body : if hasattr ( n , 'name' ) : if regex . match ( n . name ) : mains . add ( n . name ) if hasattr ( n , 'value' ) : ns = n . value if hasattr ( ns , 'id' ) : if regex . match ( ns . id ) : mains . add ( ns . id ) if hasattr ( n , 'targets' ) : ns = n . targets [ 0 ] if hasattr ( ns , 'id' ) : if regex . match ( ns . id ) : mains . add ( ns . id ) if hasattr ( n , 'names' ) : for ns in n . names : if hasattr ( ns , 'name' ) : if regex . match ( ns . name ) : mains . add ( ns . name ) if getattr ( ns , 'asname' , None ) : if regex . match ( ns . asname ) : mains . add ( ns . asname ) if len ( mains ) > 0 : module = self . module for function_name in mains : cb = getattr ( module , function_name , None ) if cb and callable ( cb ) : self . callbacks [ function_name ] = cb else : raise ParseError ( "no main function found" ) self . parsed = True return len ( self . callbacks ) > 0
6344	def idf ( self , term , transform = None ) : docs_with_term = 0 docs = self . docs_of_words ( ) for doc in docs : doc_set = set ( doc ) if transform : transformed_doc = [ ] for word in doc_set : transformed_doc . append ( transform ( word ) ) doc_set = set ( transformed_doc ) if term in doc_set : docs_with_term += 1 if docs_with_term == 0 : return float ( 'inf' ) return log10 ( len ( docs ) / docs_with_term )
4888	def update_enterprise_courses ( self , enterprise_customer , course_container_key = 'results' , * * kwargs ) : enterprise_context = { 'tpa_hint' : enterprise_customer and enterprise_customer . identity_provider , 'enterprise_id' : enterprise_customer and str ( enterprise_customer . uuid ) , } enterprise_context . update ( * * kwargs ) courses = [ ] for course in self . data [ course_container_key ] : courses . append ( self . update_course ( course , enterprise_customer , enterprise_context ) ) self . data [ course_container_key ] = courses
12827	def add_data ( self , data ) : if not self . _data : self . _data = { } self . _data . update ( data )
13738	def _real_time_thread ( self ) : while self . ws_client . connected ( ) : if self . die : break if self . pause : sleep ( 5 ) continue message = self . ws_client . receive ( ) if message is None : break message_type = message [ 'type' ] if message_type == 'error' : continue if message [ 'sequence' ] <= self . sequence : continue if message_type == 'open' : self . _handle_open ( message ) elif message_type == 'match' : self . _handle_match ( message ) elif message_type == 'done' : self . _handle_done ( message ) elif message_type == 'change' : self . _handle_change ( message ) else : continue self . ws_client . disconnect ( )
3095	def http ( self , * args , * * kwargs ) : return self . credentials . authorize ( transport . get_http_object ( * args , * * kwargs ) )
2228	def hash_data ( data , hasher = NoParam , base = NoParam , types = False , hashlen = NoParam , convert = False ) : if convert and isinstance ( data , six . string_types ) : # nocover try : data = json . dumps ( data ) except TypeError as ex : # import warnings # warnings.warn('Unable to encode input as json due to: {!r}'.format(ex)) pass base = _rectify_base ( base ) hashlen = _rectify_hashlen ( hashlen ) hasher = _rectify_hasher ( hasher ) ( ) # Feed the data into the hasher _update_hasher ( hasher , data , types = types ) # Get the hashed representation text = _digest_hasher ( hasher , hashlen , base ) return text
5312	def resolve_modifier_to_ansi_code ( modifiername , colormode ) : if colormode == terminal . NO_COLORS : # return empty string if colors are disabled return '' , '' try : start_code , end_code = ansi . MODIFIERS [ modifiername ] except KeyError : raise ColorfulError ( 'the modifier "{0}" is unknown. Use one of: {1}' . format ( modifiername , ansi . MODIFIERS . keys ( ) ) ) else : return ansi . ANSI_ESCAPE_CODE . format ( code = start_code ) , ansi . ANSI_ESCAPE_CODE . format ( code = end_code )
10480	def _performAction ( self , action ) : try : _a11y . AXUIElement . _performAction ( self , 'AX%s' % action ) except _a11y . ErrorUnsupported as e : sierra_ver = '10.12' if mac_ver ( ) [ 0 ] < sierra_ver : raise e else : pass
8526	def find_match ( self ) : for pattern , callback in self . rules : match = pattern . match ( self . source , pos = self . pos ) if not match : continue try : node = callback ( match ) except IgnoredMatchException : pass else : self . seen . append ( node ) return match raise NoMatchException ( 'None of the known patterns match for {}' '' . format ( self . source [ self . pos : ] ) )
5821	def version ( self ) : ver = Version ( ) ver . conn = self . conn ver . attrs = { # Parent params 'service_id' : self . attrs [ 'id' ] , } ver . save ( ) return ver
12936	def add ( self , * names ) : def decorator ( blok ) : for name in names or ( blok . __name__ , ) : self [ name ] = blok return blok return decorator
12169	def _dispatch ( self , event , listener , * args , * * kwargs ) : if ( asyncio . iscoroutinefunction ( listener ) or isinstance ( listener , functools . partial ) and asyncio . iscoroutinefunction ( listener . func ) ) : return self . _dispatch_coroutine ( event , listener , * args , * * kwargs ) return self . _dispatch_function ( event , listener , * args , * * kwargs )
7385	def group_theta ( self , group ) : for i , g in enumerate ( self . nodes . keys ( ) ) : if g == group : break return i * self . major_angle
13211	def _parse_revision_date ( self ) : doc_datetime = None # First try to parse the \date command in the latex. # \date is ignored for draft documents. if not self . is_draft : date_command = LatexCommand ( 'date' , { 'name' : 'content' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( date_command . parse ( self . _tex ) ) command_content = parsed [ 'content' ] . strip ( ) except StopIteration : command_content = None self . _logger . warning ( 'lsstdoc has no date command' ) # Try to parse a date from the \date command if command_content is not None and command_content != r'\today' : try : doc_datetime = datetime . datetime . strptime ( command_content , '%Y-%m-%d' ) # Assume LSST project time (Pacific) project_tz = timezone ( 'US/Pacific' ) localized_datetime = project_tz . localize ( doc_datetime ) # Normalize to UTC doc_datetime = localized_datetime . astimezone ( pytz . utc ) self . _revision_datetime_source = 'tex' except ValueError : self . _logger . warning ( 'Could not parse a datetime from ' 'lsstdoc date command: %r' , command_content ) # Fallback to getting the datetime from Git if doc_datetime is None : content_extensions = ( 'tex' , 'bib' , 'pdf' , 'png' , 'jpg' ) try : doc_datetime = get_content_commit_date ( content_extensions , root_dir = self . _root_dir ) self . _revision_datetime_source = 'git' except RuntimeError : self . _logger . warning ( 'Could not get a datetime from the Git ' 'repository at %r' , self . _root_dir ) # Final fallback to the current datetime if doc_datetime is None : doc_datetime = pytz . utc . localize ( datetime . datetime . now ( ) ) self . _revision_datetime_source = 'now' self . _datetime = doc_datetime
9071	def _initialize ( self ) : if self . _mean is None or self . _cov is None : return Q = self . _cov [ "QS" ] [ 0 ] [ 0 ] S = self . _cov [ "QS" ] [ 1 ] if S . size > 0 : self . tau [ : ] = 1 / npsum ( ( Q * sqrt ( S ) ) ** 2 , axis = 1 ) else : self . tau [ : ] = 0.0 self . eta [ : ] = self . _mean self . eta [ : ] *= self . tau
6609	def failed_runids ( self , runids ) : # remove failed clusterprocids from self.clusterprocids_finished # so that len(self.clusterprocids_finished)) becomes the number # of the successfully finished jobs for i in runids : try : self . clusterprocids_finished . remove ( i ) except ValueError : pass
5590	def tile ( self , zoom , row , col ) : tile = self . tile_pyramid . tile ( zoom , row , col ) return BufferedTile ( tile , pixelbuffer = self . pixelbuffer )
7893	def leave ( self ) : if self . joined : p = MucPresence ( to_jid = self . room_jid , stanza_type = "unavailable" ) self . manager . stream . send ( p )
4309	def _validate_num_channels ( input_filepath_list , combine_type ) : channels = [ file_info . channels ( f ) for f in input_filepath_list ] if not core . all_equal ( channels ) : raise IOError ( "Input files do not have the same number of channels. The " "{} combine type requires that all files have the same " "number of channels" . format ( combine_type ) )
4746	def pkill ( ) : if env ( ) : return 1 cmd = [ "ps -aux | grep fio | grep -v grep" ] status , _ , _ = cij . ssh . command ( cmd , shell = True , echo = False ) if not status : status , _ , _ = cij . ssh . command ( [ "pkill -f fio" ] , shell = True ) if status : return 1 return 0
9385	def convert_to_G ( self , word ) : value = 0.0 if word [ - 1 ] == 'G' or word [ - 1 ] == 'g' : value = float ( word [ : - 1 ] ) elif word [ - 1 ] == 'M' or word [ - 1 ] == 'm' : value = float ( word [ : - 1 ] ) / 1000.0 elif word [ - 1 ] == 'K' or word [ - 1 ] == 'k' : value = float ( word [ : - 1 ] ) / 1000.0 / 1000.0 else : # No unit value = float ( word ) / 1000.0 / 1000.0 / 1000.0 return str ( value )
3311	def do_PROPPATCH ( self , environ , start_response ) : path = environ [ "PATH_INFO" ] res = self . _davProvider . get_resource_inst ( path , environ ) # Only accept Depth: 0 (but assume this, if omitted) environ . setdefault ( "HTTP_DEPTH" , "0" ) if environ [ "HTTP_DEPTH" ] != "0" : self . _fail ( HTTP_BAD_REQUEST , "Depth must be '0'." ) if res is None : self . _fail ( HTTP_NOT_FOUND ) self . _evaluate_if_headers ( res , environ ) self . _check_write_permission ( res , "0" , environ ) # Parse request requestEL = util . parse_xml_body ( environ ) if requestEL . tag != "{DAV:}propertyupdate" : self . _fail ( HTTP_BAD_REQUEST ) # Create a list of update request tuples: (name, value) propupdatelist = [ ] for ppnode in requestEL : propupdatemethod = None if ppnode . tag == "{DAV:}remove" : propupdatemethod = "remove" elif ppnode . tag == "{DAV:}set" : propupdatemethod = "set" else : self . _fail ( HTTP_BAD_REQUEST , "Unknown tag (expected 'set' or 'remove')." ) for propnode in ppnode : if propnode . tag != "{DAV:}prop" : self . _fail ( HTTP_BAD_REQUEST , "Unknown tag (expected 'prop')." ) for propertynode in propnode : propvalue = None if propupdatemethod == "remove" : propvalue = None # Mark as 'remove' if len ( propertynode ) > 0 : # 14.23: All the XML elements in a 'prop' XML # element inside of a 'remove' XML element MUST be # empty self . _fail ( HTTP_BAD_REQUEST , "prop element must be empty for 'remove'." , ) else : propvalue = propertynode propupdatelist . append ( ( propertynode . tag , propvalue ) ) # Apply updates in SIMULATION MODE and create a result list (name, # result) successflag = True writeresultlist = [ ] for ( name , propvalue ) in propupdatelist : try : res . set_property_value ( name , propvalue , dry_run = True ) except Exception as e : writeresult = as_DAVError ( e ) else : writeresult = "200 OK" writeresultlist . append ( ( name , writeresult ) ) successflag = successflag and writeresult == "200 OK" # Generate response list of 2-tuples (name, value) # <value> is None on success, or an instance of DAVError propResponseList = [ ] responsedescription = [ ] if not successflag : # If dry run failed: convert all OK to FAILED_DEPENDENCY. for ( name , result ) in writeresultlist : if result == "200 OK" : result = DAVError ( HTTP_FAILED_DEPENDENCY ) elif isinstance ( result , DAVError ) : responsedescription . append ( result . get_user_info ( ) ) propResponseList . append ( ( name , result ) ) else : # Dry-run succeeded: set properties again, this time in 'real' mode # In theory, there should be no exceptions thrown here, but this is # real live... for ( name , propvalue ) in propupdatelist : try : res . set_property_value ( name , propvalue , dry_run = False ) # Set value to None, so the response xml contains empty tags propResponseList . append ( ( name , None ) ) except Exception as e : e = as_DAVError ( e ) propResponseList . append ( ( name , e ) ) responsedescription . append ( e . get_user_info ( ) ) # Generate response XML multistatusEL = xml_tools . make_multistatus_el ( ) href = res . get_href ( ) util . add_property_response ( multistatusEL , href , propResponseList ) if responsedescription : etree . SubElement ( multistatusEL , "{DAV:}responsedescription" ) . text = "\n" . join ( responsedescription ) # Send response return util . send_multi_status_response ( environ , start_response , multistatusEL )
9652	def get_sha ( a_file , settings = None ) : if settings : error = settings [ "error" ] else : error = ERROR_FN try : BLOCKSIZE = 65536 hasher = hashlib . sha1 ( ) with io . open ( a_file , "rb" ) as fh : buf = fh . read ( BLOCKSIZE ) while len ( buf ) > 0 : hasher . update ( buf ) buf = fh . read ( BLOCKSIZE ) the_hash = hasher . hexdigest ( ) except IOError : errmes = "File '{}' could not be read! Exiting!" . format ( a_file ) error ( errmes ) sys . exit ( 1 ) except : errmes = "Unspecified error returning sha1 hash. Exiting!" error ( errmes ) sys . exit ( 1 ) return the_hash
12056	def ftp_folder_match ( ftp , localFolder , deleteStuff = True ) : for fname in glob . glob ( localFolder + "/*.*" ) : ftp_upload ( ftp , fname ) return
1891	def _solver_version ( self ) -> Version : self . _reset ( ) if self . _received_version is None : self . _send ( '(get-info :version)' ) self . _received_version = self . _recv ( ) key , version = shlex . split ( self . _received_version [ 1 : - 1 ] ) return Version ( * map ( int , version . split ( '.' ) ) )
8401	def rescale_mid ( x , to = ( 0 , 1 ) , _from = None , mid = 0 ) : array_like = True try : len ( x ) except TypeError : array_like = False x = [ x ] if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) if _from is None : _from = np . array ( [ np . min ( x ) , np . max ( x ) ] ) else : _from = np . asarray ( _from ) if ( zero_range ( _from ) or zero_range ( to ) ) : out = np . repeat ( np . mean ( to ) , len ( x ) ) else : extent = 2 * np . max ( np . abs ( _from - mid ) ) out = ( x - mid ) / extent * np . diff ( to ) + np . mean ( to ) if not array_like : out = out [ 0 ] return out
3482	def _get_doc_from_filename ( filename ) : if isinstance ( filename , string_types ) : if ( "win" in platform ) and ( len ( filename ) < 260 ) and os . path . exists ( filename ) : # path (win) doc = libsbml . readSBMLFromFile ( filename ) # noqa: E501 type: libsbml.SBMLDocument elif ( "win" not in platform ) and os . path . exists ( filename ) : # path other doc = libsbml . readSBMLFromFile ( filename ) # noqa: E501 type: libsbml.SBMLDocument else : # string representation if "<sbml" not in filename : raise IOError ( "The file with 'filename' does not exist, " "or is not an SBML string. Provide the path to " "an existing SBML file or a valid SBML string " "representation: \n%s" , filename ) doc = libsbml . readSBMLFromString ( filename ) # noqa: E501 type: libsbml.SBMLDocument elif hasattr ( filename , "read" ) : # file handle doc = libsbml . readSBMLFromString ( filename . read ( ) ) # noqa: E501 type: libsbml.SBMLDocument else : raise CobraSBMLError ( "Input type '%s' for 'filename' is not supported." " Provide a path, SBML str, " "or file handle." , type ( filename ) ) return doc
865	def _readConfigFile ( cls , filename , path = None ) : outputProperties = dict ( ) # Get the path to the config files. if path is None : filePath = cls . findConfigFile ( filename ) else : filePath = os . path . join ( path , filename ) # ------------------------------------------------------------------ # Read in the config file try : if filePath is not None : try : # Use warn since console log level is set to warning _getLoggerBase ( ) . debug ( "Loading config file: %s" , filePath ) with open ( filePath , 'r' ) as inp : contents = inp . read ( ) except Exception : raise RuntimeError ( "Expected configuration file at %s" % filePath ) else : # If the file was not found in the normal search paths, which includes # checking the NTA_CONF_PATH, we'll try loading it from pkg_resources. try : contents = resource_string ( "nupic.support" , filename ) except Exception as resourceException : # We expect these to be read, and if they don't exist we'll just use # an empty configuration string. if filename in [ USER_CONFIG , CUSTOM_CONFIG ] : contents = '<configuration/>' else : raise resourceException elements = ElementTree . XML ( contents ) if elements . tag != 'configuration' : raise RuntimeError ( "Expected top-level element to be 'configuration' " "but got '%s'" % ( elements . tag ) ) # ------------------------------------------------------------------ # Add in each property found propertyElements = elements . findall ( './property' ) for propertyItem in propertyElements : propInfo = dict ( ) # Parse this property element propertyAttributes = list ( propertyItem ) for propertyAttribute in propertyAttributes : propInfo [ propertyAttribute . tag ] = propertyAttribute . text # Get the name name = propInfo . get ( 'name' , None ) # value is allowed to be empty string if 'value' in propInfo and propInfo [ 'value' ] is None : value = '' else : value = propInfo . get ( 'value' , None ) if value is None : if 'novalue' in propInfo : # Placeholder "novalue" properties are intended to be overridden # via dynamic configuration or another configuration layer. continue else : raise RuntimeError ( "Missing 'value' element within the property " "element: => %s " % ( str ( propInfo ) ) ) # The value is allowed to contain substitution tags of the form # ${env.VARNAME}, which should be substituted with the corresponding # environment variable values restOfValue = value value = '' while True : # Find the beginning of substitution tag pos = restOfValue . find ( '${env.' ) if pos == - 1 : # No more environment variable substitutions value += restOfValue break # Append prefix to value accumulator value += restOfValue [ 0 : pos ] # Find the end of current substitution tag varTailPos = restOfValue . find ( '}' , pos ) if varTailPos == - 1 : raise RuntimeError ( "Trailing environment variable tag delimiter '}'" " not found in %r" % ( restOfValue ) ) # Extract environment variable name from tag varname = restOfValue [ pos + 6 : varTailPos ] if varname not in os . environ : raise RuntimeError ( "Attempting to use the value of the environment" " variable %r, which is not defined" % ( varname ) ) envVarValue = os . environ [ varname ] value += envVarValue restOfValue = restOfValue [ varTailPos + 1 : ] # Check for errors if name is None : raise RuntimeError ( "Missing 'name' element within following property " "element:\n => %s " % ( str ( propInfo ) ) ) propInfo [ 'value' ] = value outputProperties [ name ] = propInfo return outputProperties except Exception : _getLoggerBase ( ) . exception ( "Error while parsing configuration file: %s." , filePath ) raise
4481	def storage ( self , provider = 'osfstorage' ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : provides = self . _get_attribute ( store , 'attributes' , 'provider' ) if provides == provider : return Storage ( store , self . session ) raise RuntimeError ( "Project has no storage " "provider '{}'" . format ( provider ) )
10190	def tell ( self , message , sender = no_sender ) : if sender is not no_sender and not isinstance ( sender , ActorRef ) : raise ValueError ( "Sender must be actor reference" ) self . _cell . send_message ( message , sender )
12984	def keywords ( func ) : @ wraps ( func ) def decorator ( * args , * * kwargs ) : idx = 0 if inspect . ismethod ( func ) else 1 if len ( args ) > idx : if isinstance ( args [ idx ] , ( dict , composite ) ) : for key in args [ idx ] : kwargs [ key ] = args [ idx ] [ key ] args = args [ : idx ] return func ( * args , * * kwargs ) return decorator
11704	def set_inherited_traits ( self , egg_donor , sperm_donor ) : if type ( egg_donor ) == str : self . reproduce_asexually ( egg_donor , sperm_donor ) else : self . reproduce_sexually ( egg_donor , sperm_donor )
11933	def load_widgets ( context , * * kwargs ) : _soft = kwargs . pop ( '_soft' , False ) try : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] except KeyError : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] = { } for alias , template_name in kwargs . items ( ) : if _soft and alias in widgets : continue with context . render_context . push ( { BLOCK_CONTEXT_KEY : BlockContext ( ) } ) : blocks = resolve_blocks ( template_name , context ) widgets [ alias ] = blocks return ''
5835	def __convert_response_to_configuration ( self , result_blob , dataset_ids ) : builder = DataViewBuilder ( ) builder . dataset_ids ( dataset_ids ) for i , ( k , v ) in enumerate ( result_blob [ 'descriptors' ] . items ( ) ) : try : descriptor = self . __snake_case ( v [ 0 ] ) print ( json . dumps ( descriptor ) ) descriptor [ 'descriptor_key' ] = k builder . add_raw_descriptor ( descriptor ) except IndexError : pass for i , ( k , v ) in enumerate ( result_blob [ 'types' ] . items ( ) ) : builder . set_role ( k , v . lower ( ) ) return builder . build ( )
9200	def _sort_lows_and_highs ( func ) : @ functools . wraps ( func ) def wrapper ( * args , * * kwargs ) : for low , high , mult in func ( * args , * * kwargs ) : if low < high : yield low , high , mult else : yield high , low , mult return wrapper
13788	def open ( name = None , fileobj = None , closefd = True ) : return Guesser ( ) . open ( name = name , fileobj = fileobj , closefd = closefd )
8262	def repeat ( self , n = 2 , oscillate = False , callback = None ) : colorlist = ColorList ( ) colors = ColorList . copy ( self ) for i in _range ( n ) : colorlist . extend ( colors ) if oscillate : colors = colors . reverse ( ) if callback : colors = callback ( colors ) return colorlist
668	def sample ( self , rgen ) : x = rgen . poisson ( self . lambdaParameter ) return x , self . logDensity ( x )
3650	def num2hex ( self , num ) : temp = '' for i in range ( 0 , 4 ) : x = self . hexChars [ ( num >> ( i * 8 + 4 ) ) & 0x0F ] y = self . hexChars [ ( num >> ( i * 8 ) ) & 0x0F ] temp += ( x + y ) return temp
11829	def expand ( self , problem ) : return [ self . child_node ( problem , action ) for action in problem . actions ( self . state ) ]
135	def change_first_point_by_coords ( self , x , y , max_distance = 1e-4 , raise_if_too_far_away = True ) : if len ( self . exterior ) == 0 : raise Exception ( "Cannot reorder polygon points, because it contains no points." ) closest_idx , closest_dist = self . find_closest_point_index ( x = x , y = y , return_distance = True ) if max_distance is not None and closest_dist > max_distance : if not raise_if_too_far_away : return self . deepcopy ( ) closest_point = self . exterior [ closest_idx , : ] raise Exception ( "Closest found point (%.9f, %.9f) exceeds max_distance of %.9f exceeded" % ( closest_point [ 0 ] , closest_point [ 1 ] , closest_dist ) ) return self . change_first_point_by_index ( closest_idx )
485	def _getCommonSteadyDBArgsDict ( ) : return dict ( creator = pymysql , host = Configuration . get ( 'nupic.cluster.database.host' ) , port = int ( Configuration . get ( 'nupic.cluster.database.port' ) ) , user = Configuration . get ( 'nupic.cluster.database.user' ) , passwd = Configuration . get ( 'nupic.cluster.database.passwd' ) , charset = 'utf8' , use_unicode = True , setsession = [ 'SET AUTOCOMMIT = 1' ] )
1586	def _handle_state_change_msg ( self , new_helper ) : assert self . my_pplan_helper is not None assert self . my_instance is not None and self . my_instance . py_class is not None if self . my_pplan_helper . get_topology_state ( ) != new_helper . get_topology_state ( ) : # handle state change # update the pplan_helper self . my_pplan_helper = new_helper if new_helper . is_topology_running ( ) : if not self . is_instance_started : self . start_instance_if_possible ( ) self . my_instance . py_class . invoke_activate ( ) elif new_helper . is_topology_paused ( ) : self . my_instance . py_class . invoke_deactivate ( ) else : raise RuntimeError ( "Unexpected TopologyState update: %s" % new_helper . get_topology_state ( ) ) else : Log . info ( "Topology state remains the same." )
12590	def get_reliabledictionary_list ( client , application_name , service_name ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) for dictionary in service . get_dictionaries ( ) : print ( dictionary . name )
692	def _initializeEncoders ( self , encoderSpec ) : #Initializing scalar encoder if self . encoderType in [ 'adaptiveScalar' , 'scalar' ] : if 'minval' in encoderSpec : self . minval = encoderSpec . pop ( 'minval' ) else : self . minval = None if 'maxval' in encoderSpec : self . maxval = encoderSpec . pop ( 'maxval' ) else : self . maxval = None self . encoder = adaptive_scalar . AdaptiveScalarEncoder ( name = 'AdaptiveScalarEncoder' , w = self . w , n = self . n , minval = self . minval , maxval = self . maxval , periodic = False , forced = True ) #Initializing category encoder elif self . encoderType == 'category' : self . encoder = sdr_category . SDRCategoryEncoder ( name = 'categoryEncoder' , w = self . w , n = self . n ) #Initializing date encoder elif self . encoderType in [ 'date' , 'datetime' ] : self . encoder = date . DateEncoder ( name = 'dateEncoder' ) else : raise RuntimeError ( 'Error in constructing class object. Either encoder type' 'or dataType must be specified' )
1818	def SETNZ ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF == False , 1 , 0 ) )
9946	def cur_space ( self , name = None ) : if name is None : return self . _impl . model . currentspace . interface else : self . _impl . model . currentspace = self . _impl . spaces [ name ] return self . cur_space ( )
8684	def load ( self , origin_passphrase , keys = None , key_file = None ) : # TODO: Handle keys not dict or key_file not json self . _assert_valid_stash ( ) # Check if both or none are provided (ahh, the mighty xor) if not ( bool ( keys ) ^ bool ( key_file ) ) : raise GhostError ( 'You must either provide a path to an exported stash file ' 'or a list of key dicts to import' ) if key_file : with open ( key_file ) as stash_file : keys = json . loads ( stash_file . read ( ) ) # If the passphrases are the same, there's no reason to decrypt # and re-encrypt. We can simply pass the value. decrypt = origin_passphrase != self . passphrase if decrypt : # TODO: The fact that we need to create a stub stash just to # decrypt means we should probably have some encryptor class. stub = Stash ( TinyDBStorage ( 'stub' ) , origin_passphrase ) # TODO: Handle existing keys when loading for key in keys : self . put ( name = key [ 'name' ] , value = stub . _decrypt ( key [ 'value' ] ) if decrypt else key [ 'value' ] , metadata = key [ 'metadata' ] , description = key [ 'description' ] , lock = key . get ( 'lock' ) , key_type = key . get ( 'type' ) , encrypt = decrypt )
13347	def cmd ( ) : if platform == 'win' : return [ 'cmd.exe' , '/K' ] elif platform == 'linux' : ppid = os . getppid ( ) ppid_cmdline_file = '/proc/{0}/cmdline' . format ( ppid ) try : with open ( ppid_cmdline_file ) as f : cmd = f . read ( ) if cmd . endswith ( '\x00' ) : cmd = cmd [ : - 1 ] cmd = cmd . split ( '\x00' ) return cmd + [ binpath ( 'subshell.sh' ) ] except : cmd = 'bash' else : cmd = 'bash' return [ cmd , binpath ( 'subshell.sh' ) ]
1346	def gradient ( self , image , label ) : _ , gradient = self . predictions_and_gradient ( image , label ) return gradient
138	def to_shapely_line_string ( self , closed = False , interpolate = 0 ) : return _convert_points_to_shapely_line_string ( self . exterior , closed = closed , interpolate = interpolate )
9835	def __general ( self ) : while 1 : # main loop try : tok = self . __peek ( ) # only peek, apply_parser() will consume except DXParserNoTokens : # save previous DXInitObject # (kludge in here as the last level-2 parser usually does not return # via the object parser) if self . currentobject and self . currentobject not in self . objects : self . objects . append ( self . currentobject ) return # stop parsing and finish # decision branches for all level-1 parsers: # (the only way to get out of the lower level parsers!) if tok . iscode ( 'COMMENT' ) : self . set_parser ( 'comment' ) # switch the state elif tok . iscode ( 'WORD' ) and tok . equals ( 'object' ) : self . set_parser ( 'object' ) # switch the state elif self . __parser is self . __general : # Either a level-2 parser screwed up or some level-1 # construct is not implemented. (Note: this elif can # be only reached at the beginning or after comments; # later we never formally switch back to __general # (would create inifinite loop) raise DXParseError ( 'Unknown level-1 construct at ' + str ( tok ) ) self . apply_parser ( )
6624	def availableVersions ( self ) : r = [ ] for t in self . _getTags ( ) : logger . debug ( "available version tag: %s" , t ) # ignore empty tags: if not len ( t [ 0 ] . strip ( ) ) : continue try : r . append ( GithubComponentVersion ( t [ 0 ] , t [ 0 ] , url = t [ 1 ] , name = self . name , cache_key = None ) ) except ValueError : logger . debug ( 'invalid version tag: %s' , t ) return r
5802	def _convert_filetime_to_timestamp ( filetime ) : hundreds_nano_seconds = struct . unpack ( b'>Q' , struct . pack ( b'>LL' , filetime . dwHighDateTime , filetime . dwLowDateTime ) ) [ 0 ] seconds_since_1601 = hundreds_nano_seconds / 10000000 return seconds_since_1601 - 11644473600
5171	def auto_client ( cls , host , server , ca_path = None , ca_contents = None , cert_path = None , cert_contents = None , key_path = None , key_contents = None ) : # client defaults client = { "mode" : "p2p" , "nobind" : True , "resolv_retry" : "infinite" , "tls_client" : True } # remote port = server . get ( 'port' ) or 1195 client [ 'remote' ] = [ { 'host' : host , 'port' : port } ] # proto if server . get ( 'proto' ) == 'tcp-server' : client [ 'proto' ] = 'tcp-client' else : client [ 'proto' ] = 'udp' # determine if pull must be True if 'server' in server or 'server_bridge' in server : client [ 'pull' ] = True # tls_client if 'tls_server' not in server or not server [ 'tls_server' ] : client [ 'tls_client' ] = False # ns_cert_type ns_cert_type = { None : '' , '' : '' , 'client' : 'server' } client [ 'ns_cert_type' ] = ns_cert_type [ server . get ( 'ns_cert_type' ) ] # remote_cert_tls remote_cert_tls = { None : '' , '' : '' , 'client' : 'server' } client [ 'remote_cert_tls' ] = remote_cert_tls [ server . get ( 'remote_cert_tls' ) ] copy_keys = [ 'name' , 'dev_type' , 'dev' , 'comp_lzo' , 'auth' , 'cipher' , 'ca' , 'cert' , 'key' , 'pkcs12' , 'mtu_disc' , 'mtu_test' , 'fragment' , 'mssfix' , 'keepalive' , 'persist_tun' , 'mute' , 'persist_key' , 'script_security' , 'user' , 'group' , 'log' , 'mute_replay_warnings' , 'secret' , 'reneg_sec' , 'tls_timeout' , 'tls_cipher' , 'float' , 'fast_io' , 'verb' ] for key in copy_keys : if key in server : client [ key ] = server [ key ] files = cls . _auto_client_files ( client , ca_path , ca_contents , cert_path , cert_contents , key_path , key_contents ) return { 'openvpn' : [ client ] , 'files' : files }
6383	def fingerprint ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _letters ) start = word [ 0 : 1 ] consonant_part = '' vowel_part = '' # add consonants & vowels to to separate strings # (omitting the first char & duplicates) for char in word [ 1 : ] : if char != start : if char in self . _vowels : if char not in vowel_part : vowel_part += char elif char not in consonant_part : consonant_part += char # return the first char followed by consonants followed by vowels return start + consonant_part + vowel_part
7691	def validate ( schema_file = None , jams_files = None ) : schema = load_json ( schema_file ) for jams_file in jams_files : try : jams = load_json ( jams_file ) jsonschema . validate ( jams , schema ) print '{:s} was successfully validated' . format ( jams_file ) except jsonschema . ValidationError as exc : print '{:s} was NOT successfully validated' . format ( jams_file ) print exc
13155	def nt_cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , * * kwargs ) : with ( yield from cls . get_cursor ( _CursorType . NAMEDTUPLE ) ) as c : return ( yield from func ( cls , c , * args , * * kwargs ) ) return wrapper
137	def to_shapely_polygon ( self ) : # load shapely lazily, which makes the dependency more optional import shapely . geometry return shapely . geometry . Polygon ( [ ( point [ 0 ] , point [ 1 ] ) for point in self . exterior ] )
7724	def __init ( self , affiliation , role , jid = None , nick = None , actor = None , reason = None ) : if not affiliation : affiliation = None elif affiliation not in affiliations : raise ValueError ( "Bad affiliation" ) self . affiliation = affiliation if not role : role = None elif role not in roles : raise ValueError ( "Bad role" ) self . role = role if jid : self . jid = JID ( jid ) else : self . jid = None if actor : self . actor = JID ( actor ) else : self . actor = None self . nick = nick self . reason = reason
6203	def em_rates_from_E_DA_mix ( em_rates_tot , E_values ) : em_rates_d , em_rates_a = [ ] , [ ] for em_rate_tot , E_value in zip ( em_rates_tot , E_values ) : em_rate_di , em_rate_ai = em_rates_from_E_DA ( em_rate_tot , E_value ) em_rates_d . append ( em_rate_di ) em_rates_a . append ( em_rate_ai ) return em_rates_d , em_rates_a
3065	def _add_query_parameter ( url , name , value ) : if value is None : return url else : return update_query_params ( url , { name : value } )
9867	async def rt_unsubscribe ( self ) : if self . _subscription_id is None : _LOGGER . error ( "Not subscribed." ) return await self . _tibber_control . sub_manager . unsubscribe ( self . _subscription_id )
12966	def allOnlyIndexedFields ( self ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultipleOnlyIndexedFields ( matchedKeys ) return IRQueryableList ( [ ] , mdl = self . mdl )
7714	def add_item ( self , jid , name = None , groups = None , callback = None , error_callback = None ) : # pylint: disable=R0913 if jid in self . roster : raise ValueError ( "{0!r} already in the roster" . format ( jid ) ) item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
9712	def heappushpop_max ( heap , item ) : if heap and heap [ 0 ] > item : # if item >= heap[0], it will be popped immediately after pushed item , heap [ 0 ] = heap [ 0 ] , item _siftup_max ( heap , 0 ) return item
2455	def set_pkg_licenses_concluded ( self , doc , licenses ) : self . assert_package_exists ( ) if not self . package_conc_lics_set : self . package_conc_lics_set = True if validations . validate_lics_conc ( licenses ) : doc . package . conc_lics = licenses return True else : raise SPDXValueError ( 'Package::ConcludedLicenses' ) else : raise CardinalityError ( 'Package::ConcludedLicenses' )
4504	def put_edit ( self , f , * args , * * kwds ) : self . put_nowait ( functools . partial ( f , * args , * * kwds ) )
6004	def setup_random_seed ( seed ) : if seed == - 1 : seed = np . random . randint ( 0 , int ( 1e9 ) ) # Use one seed, so all regions have identical column non-uniformity. np . random . seed ( seed )
10801	def _distance_matrix ( self , a , b ) : def sq ( x ) : return ( x * x ) # matrix = np.sum(map(lambda a,b: sq(a[:,None] - b[None,:]), a.T, # b.T), axis=0) # A faster version than above: matrix = sq ( a [ : , 0 ] [ : , None ] - b [ : , 0 ] [ None , : ] ) for x , y in zip ( a . T [ 1 : ] , b . T [ 1 : ] ) : matrix += sq ( x [ : , None ] - y [ None , : ] ) return matrix
11151	def md5file ( abspath , nbytes = 0 , chunk_size = DEFAULT_CHUNK_SIZE ) : return get_file_fingerprint ( abspath , hashlib . md5 , nbytes = nbytes , chunk_size = chunk_size )
13497	def clone ( self ) : t = Tag ( self . version . major , self . version . minor , self . version . patch ) if self . revision is not None : t . revision = self . revision . clone ( ) return t
7198	def describe_images ( self , idaho_image_results ) : results = idaho_image_results [ 'results' ] # filter only idaho images: results = [ r for r in results if 'IDAHOImage' in r [ 'type' ] ] self . logger . debug ( 'Describing %s IDAHO images.' % len ( results ) ) # figure out which catids are represented in this set of images catids = set ( [ r [ 'properties' ] [ 'catalogID' ] for r in results ] ) description = { } for catid in catids : # images associated with a single catid description [ catid ] = { } description [ catid ] [ 'parts' ] = { } images = [ r for r in results if r [ 'properties' ] [ 'catalogID' ] == catid ] for image in images : description [ catid ] [ 'sensorPlatformName' ] = image [ 'properties' ] [ 'sensorPlatformName' ] part = int ( image [ 'properties' ] [ 'vendorDatasetIdentifier' ] . split ( ':' ) [ 1 ] [ - 3 : ] ) color = image [ 'properties' ] [ 'colorInterpretation' ] bucket = image [ 'properties' ] [ 'tileBucketName' ] identifier = image [ 'identifier' ] boundstr = image [ 'properties' ] [ 'footprintWkt' ] try : description [ catid ] [ 'parts' ] [ part ] except : description [ catid ] [ 'parts' ] [ part ] = { } description [ catid ] [ 'parts' ] [ part ] [ color ] = { } description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'id' ] = identifier description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'bucket' ] = bucket description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'boundstr' ] = boundstr return description
8166	def reload_functions ( self ) : with LiveExecution . lock : if self . edited_source : tree = ast . parse ( self . edited_source ) for f in [ n for n in ast . walk ( tree ) if isinstance ( n , ast . FunctionDef ) ] : self . ns [ f . name ] . __code__ = meta . decompiler . compile_func ( f , self . filename , self . ns ) . __code__
9131	def ls ( cls , session : Optional [ Session ] = None ) -> List [ 'Action' ] : if session is None : session = _make_session ( ) actions = session . query ( cls ) . order_by ( cls . created . desc ( ) ) . all ( ) session . close ( ) return actions
2365	def RobotFactory ( path , parent = None ) : if os . path . isdir ( path ) : return SuiteFolder ( path , parent ) else : rf = RobotFile ( path , parent ) for table in rf . tables : if isinstance ( table , TestcaseTable ) : rf . __class__ = SuiteFile return rf rf . __class__ = ResourceFile return rf
1596	def format_mtime ( mtime ) : now = datetime . now ( ) dt = datetime . fromtimestamp ( mtime ) return '%s %2d %5s' % ( dt . strftime ( '%b' ) , dt . day , dt . year if dt . year != now . year else dt . strftime ( '%H:%M' ) )
3180	def update ( self , batch_webhook_id , data ) : self . batch_webhook_id = batch_webhook_id if 'url' not in data : raise KeyError ( 'The batch webhook must have a valid url' ) return self . _mc_client . _patch ( url = self . _build_path ( batch_webhook_id ) , data = data )
12242	def bohachevsky1 ( theta ) : x , y = theta obj = x ** 2 + 2 * y ** 2 - 0.3 * np . cos ( 3 * np . pi * x ) - 0.4 * np . cos ( 4 * np . pi * y ) + 0.7 grad = np . array ( [ 2 * x + 0.3 * np . sin ( 3 * np . pi * x ) * 3 * np . pi , 4 * y + 0.4 * np . sin ( 4 * np . pi * y ) * 4 * np . pi , ] ) return obj , grad
1036	def chain ( self , expanded_from ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = expanded_from )
6410	def lehmer_mean ( nums , exp = 2 ) : return sum ( x ** exp for x in nums ) / sum ( x ** ( exp - 1 ) for x in nums )
11328	def select ( options = None ) : if not options : return None width = len ( str ( len ( options ) ) ) for x , option in enumerate ( options ) : sys . stdout . write ( '{:{width}}) {}\n' . format ( x + 1 , option , width = width ) ) sys . stdout . write ( '{:>{width}} ' . format ( '#?' , width = width + 1 ) ) sys . stdout . flush ( ) if sys . stdin . isatty ( ) : # regular prompt try : response = raw_input ( ) . strip ( ) except ( EOFError , KeyboardInterrupt ) : # handle ctrl-d, ctrl-c response = '' else : # try connecting to current tty, when using pipes sys . stdin = open ( "/dev/tty" ) try : response = '' while True : response += sys . stdin . read ( 1 ) if response . endswith ( '\n' ) : break except ( EOFError , KeyboardInterrupt ) : sys . stdout . flush ( ) pass try : response = int ( response ) - 1 except ValueError : return None if response < 0 or response >= len ( options ) : return None return options [ response ]
6225	def move_state ( self , direction , activate ) : if direction == RIGHT : self . _xdir = POSITIVE if activate else STILL elif direction == LEFT : self . _xdir = NEGATIVE if activate else STILL elif direction == FORWARD : self . _zdir = NEGATIVE if activate else STILL elif direction == BACKWARD : self . _zdir = POSITIVE if activate else STILL elif direction == UP : self . _ydir = POSITIVE if activate else STILL elif direction == DOWN : self . _ydir = NEGATIVE if activate else STILL
5684	def day_start_ut ( self , ut ) : # set timezone to the one of gtfs old_tz = self . set_current_process_time_zone ( ) ut = time . mktime ( time . localtime ( ut ) [ : 3 ] + ( 12 , 00 , 0 , 0 , 0 , - 1 ) ) - 43200 set_process_timezone ( old_tz ) return ut
7905	def forget ( self , rs ) : try : del self . rooms [ rs . room_jid . bare ( ) . as_unicode ( ) ] except KeyError : pass
8355	def _subMSChar ( self , orig ) : sub = self . MS_CHARS . get ( orig ) if type ( sub ) == types . TupleType : if self . smartQuotesTo == 'xml' : sub = '&#x%s;' % sub [ 1 ] else : sub = '&%s;' % sub [ 0 ] return sub
4080	def get_languages ( ) -> set : try : languages = cache [ 'languages' ] except KeyError : languages = LanguageTool . _get_languages ( ) cache [ 'languages' ] = languages return languages
11953	def _parse_dumb_push_output ( self , string ) : stack = 0 json_list = [ ] tmp_json = '' for char in string : if not char == '\r' and not char == '\n' : tmp_json += char if char == '{' : stack += 1 elif char == '}' : stack -= 1 if stack == 0 : if not len ( tmp_json ) == 0 : json_list . append ( tmp_json ) tmp_json = '' return json_list
4223	def init_backend ( limit = None ) : # save the limit for the chainer to honor backend . _limit = limit # get all keyrings passing the limit filter keyrings = filter ( limit , backend . get_all_keyring ( ) ) set_keyring ( load_env ( ) or load_config ( ) or max ( keyrings , default = fail . Keyring ( ) , key = backend . by_priority ) )
13858	def curl ( self , url , post ) : try : req = urllib2 . Request ( url ) req . add_header ( "Content-type" , "application/xml" ) data = urllib2 . urlopen ( req , post . encode ( 'utf-8' ) ) . read ( ) except urllib2 . URLError , v : raise AmbientSMSError ( v ) return dictFromXml ( data )
4868	def to_representation ( self , instance ) : updated_course_run = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( updated_course_run [ 'key' ] ) return updated_course_run
10077	def create ( cls , data , id_ = None ) : data . setdefault ( '$schema' , current_jsonschemas . path_to_url ( current_app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] ) ) if '_deposit' not in data : id_ = id_ or uuid . uuid4 ( ) cls . deposit_minter ( id_ , data ) data [ '_deposit' ] . setdefault ( 'owners' , list ( ) ) if current_user and current_user . is_authenticated : creator_id = int ( current_user . get_id ( ) ) if creator_id not in data [ '_deposit' ] [ 'owners' ] : data [ '_deposit' ] [ 'owners' ] . append ( creator_id ) data [ '_deposit' ] [ 'created_by' ] = creator_id return super ( Deposit , cls ) . create ( data , id_ = id_ )
12483	def filter_list ( lst , pattern ) : if is_fnmatch_regex ( pattern ) and not is_regex ( pattern ) : #use fnmatch log . info ( 'Using fnmatch for {0}' . format ( pattern ) ) filst = fnmatch . filter ( lst , pattern ) else : #use re log . info ( 'Using regex match for {0}' . format ( pattern ) ) filst = match_list ( lst , pattern ) if filst : filst . sort ( ) return filst
10897	def get_scale_from_raw ( raw , scaled ) : t0 , t1 = scaled . min ( ) , scaled . max ( ) r0 , r1 = float ( raw . min ( ) ) , float ( raw . max ( ) ) rmin = ( t1 * r0 - t0 * r1 ) / ( t1 - t0 ) rmax = ( r1 - r0 ) / ( t1 - t0 ) + rmin return ( rmin , rmax )
5358	def es_version ( self , url ) : try : res = self . grimoire_con . get ( url ) res . raise_for_status ( ) major = res . json ( ) [ 'version' ] [ 'number' ] . split ( "." ) [ 0 ] except Exception : logger . error ( "Error retrieving Elasticsearch version: " + url ) raise return major
12173	def htmlABF ( ID , group , d , folder , overwrite = False ) : fname = folder + "/swhlab4/%s_index.html" % ID if overwrite is False and os . path . exists ( fname ) : return html = TEMPLATES [ 'abf' ] html = html . replace ( "~ID~" , ID ) html = html . replace ( "~CONTENT~" , htmlABFcontent ( ID , group , d ) ) print ( " <- writing [%s]" % os . path . basename ( fname ) ) with open ( fname , 'w' ) as f : f . write ( html ) return
4360	def _receiver_loop ( self ) : while True : rawdata = self . get_server_msg ( ) if not rawdata : continue # or close the connection ? try : pkt = packet . decode ( rawdata , self . json_loads ) except ( ValueError , KeyError , Exception ) as e : self . error ( 'invalid_packet' , "There was a decoding error when dealing with packet " "with event: %s... (%s)" % ( rawdata [ : 20 ] , e ) ) continue if pkt [ 'type' ] == 'heartbeat' : # This is already dealth with in put_server_msg() when # any incoming raw data arrives. continue if pkt [ 'type' ] == 'disconnect' and pkt [ 'endpoint' ] == '' : # On global namespace, we kill everything. self . kill ( detach = True ) continue endpoint = pkt [ 'endpoint' ] if endpoint not in self . namespaces : self . error ( "no_such_namespace" , "The endpoint you tried to connect to " "doesn't exist: %s" % endpoint , endpoint = endpoint ) continue elif endpoint in self . active_ns : pkt_ns = self . active_ns [ endpoint ] else : new_ns_class = self . namespaces [ endpoint ] pkt_ns = new_ns_class ( self . environ , endpoint , request = self . request ) # This calls initialize() on all the classes and mixins, etc.. # in the order of the MRO for cls in type ( pkt_ns ) . __mro__ : if hasattr ( cls , 'initialize' ) : cls . initialize ( pkt_ns ) # use this instead of __init__, # for less confusion self . active_ns [ endpoint ] = pkt_ns retval = pkt_ns . process_packet ( pkt ) # Has the client requested an 'ack' with the reply parameters ? if pkt . get ( 'ack' ) == "data" and pkt . get ( 'id' ) : if type ( retval ) is tuple : args = list ( retval ) else : args = [ retval ] returning_ack = dict ( type = 'ack' , ackId = pkt [ 'id' ] , args = args , endpoint = pkt . get ( 'endpoint' , '' ) ) self . send_packet ( returning_ack ) # Now, are we still connected ? if not self . connected : self . kill ( detach = True ) # ?? what,s the best clean-up # when its not a # user-initiated disconnect return
5392	def _task_directory ( self , job_id , task_id , task_attempt ) : dir_name = 'task' if task_id is None else str ( task_id ) if task_attempt : dir_name = '%s.%s' % ( dir_name , task_attempt ) return self . _provider_root ( ) + '/' + job_id + '/' + dir_name
8630	def get_projects ( session , query ) : # GET /api/projects/0.1/projects response = make_get_request ( session , 'projects' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11596	def _rc_keys ( self , pattern = '*' ) : result = [ ] for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result . extend ( redisent . keys ( pattern ) ) return result
7634	def __load_jams_schema ( ) : schema_file = os . path . join ( SCHEMA_DIR , 'jams_schema.json' ) jams_schema = None with open ( resource_filename ( __name__ , schema_file ) , mode = 'r' ) as fdesc : jams_schema = json . load ( fdesc ) if jams_schema is None : raise JamsError ( 'Unable to load JAMS schema' ) return jams_schema
12262	def add ( self , operator , * args ) : if isinstance ( operator , str ) : op = getattr ( proxops , operator ) ( * args ) elif isinstance ( operator , proxops . ProximalOperatorBaseClass ) : op = operator else : raise ValueError ( "operator must be a string or a subclass of ProximalOperator" ) self . operators . append ( op ) return self
6351	def _phonetic_numbers ( self , phonetic ) : phonetic_array = phonetic . split ( '-' ) # for names with spaces in them result = ' ' . join ( [ self . _pnums_with_leading_space ( i ) [ 1 : ] for i in phonetic_array ] ) return result
2921	def _restart ( self , my_task ) : if not my_task . _has_state ( Task . WAITING ) : raise WorkflowException ( my_task , "Cannot refire a task that is not" "in WAITING state" ) # Check state of existing call and abort it (save history) if my_task . _get_internal_data ( 'task_id' ) is not None : if not hasattr ( my_task , 'async_call' ) : task_id = my_task . _get_internal_data ( 'task_id' ) my_task . async_call = default_app . AsyncResult ( task_id ) my_task . deserialized = True my_task . async_call . state # manually refresh async_call = my_task . async_call if async_call . state == 'FAILED' : pass elif async_call . state in [ 'RETRY' , 'PENDING' , 'STARTED' ] : async_call . revoke ( ) LOG . info ( "Celery task '%s' was in %s state and was revoked" % ( async_call . state , async_call ) ) elif async_call . state == 'SUCCESS' : LOG . warning ( "Celery task '%s' succeeded, but a refire was " "requested" % async_call ) self . _clear_celery_task_data ( my_task ) # Retrigger return self . _start ( my_task )
13341	def expand_dims ( a , axis ) : if hasattr ( a , 'expand_dims' ) and hasattr ( type ( a ) , '__array_interface__' ) : return a . expand_dims ( axis ) else : return np . expand_dims ( a , axis )
12963	def getPrimaryKeys ( self , sortByAge = False ) : conn = self . _get_connection ( ) # Apply filters, and return object numFilters = len ( self . filters ) numNotFilters = len ( self . notFilters ) if numFilters + numNotFilters == 0 : # No filters, get all. conn = self . _get_connection ( ) matchedKeys = conn . smembers ( self . _get_ids_key ( ) ) elif numNotFilters == 0 : # Only Inclusive if numFilters == 1 : # Only one filter, get members of that index key ( filterFieldName , filterValue ) = self . filters [ 0 ] matchedKeys = conn . smembers ( self . _get_key_for_index ( filterFieldName , filterValue ) ) else : # Several filters, intersect the index keys indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] matchedKeys = conn . sinter ( indexKeys ) else : # Some negative filters present notIndexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . notFilters ] if numFilters == 0 : # Only negative, diff against all keys matchedKeys = conn . sdiff ( self . _get_ids_key ( ) , * notIndexKeys ) else : # Negative and positive. Use pipeline, find all positive intersections, and remove negative matches indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] tempKey = self . _getTempKey ( ) pipeline = conn . pipeline ( ) pipeline . sinterstore ( tempKey , * indexKeys ) pipeline . sdiff ( tempKey , * notIndexKeys ) pipeline . delete ( tempKey ) matchedKeys = pipeline . execute ( ) [ 1 ] # sdiff matchedKeys = [ int ( _key ) for _key in matchedKeys ] if sortByAge is False : return list ( matchedKeys ) else : matchedKeys = list ( matchedKeys ) matchedKeys . sort ( ) return matchedKeys
5195	def main ( ) : app = OutstationApplication ( ) _log . debug ( 'Initialization complete. In command loop.' ) # Ad-hoc tests can be inserted here if desired. See outstation_cmd.py for examples. app . shutdown ( ) _log . debug ( 'Exiting.' ) exit ( )
13381	def env_to_dict ( env , pathsep = os . pathsep ) : out_dict = { } for k , v in env . iteritems ( ) : if pathsep in v : out_dict [ k ] = v . split ( pathsep ) else : out_dict [ k ] = v return out_dict
7006	def plot_training_results ( classifier , classlabels , outfile ) : if isinstance ( classifier , str ) and os . path . exists ( classifier ) : with open ( classifier , 'rb' ) as infd : clfdict = pickle . load ( infd ) elif isinstance ( classifier , dict ) : clfdict = classifier else : LOGERROR ( "can't figure out the input classifier arg" ) return None confmatrix = clfdict [ 'best_confmatrix' ] overall_feature_importances = clfdict [ 'best_classifier' ] . feature_importances_ feature_importances_per_tree = np . array ( [ tree . feature_importances_ for tree in clfdict [ 'best_classifier' ] . estimators_ ] ) stdev_feature_importances = np . std ( feature_importances_per_tree , axis = 0 ) feature_names = np . array ( clfdict [ 'feature_names' ] ) plt . figure ( figsize = ( 6.4 * 3.0 , 4.8 ) ) # confusion matrix plt . subplot ( 121 ) classes = np . array ( classlabels ) plt . imshow ( confmatrix , interpolation = 'nearest' , cmap = plt . cm . Blues ) tick_marks = np . arange ( len ( classes ) ) plt . xticks ( tick_marks , classes ) plt . yticks ( tick_marks , classes ) plt . title ( 'evaluation set confusion matrix' ) plt . ylabel ( 'predicted class' ) plt . xlabel ( 'actual class' ) thresh = confmatrix . max ( ) / 2. for i , j in itertools . product ( range ( confmatrix . shape [ 0 ] ) , range ( confmatrix . shape [ 1 ] ) ) : plt . text ( j , i , confmatrix [ i , j ] , horizontalalignment = "center" , color = "white" if confmatrix [ i , j ] > thresh else "black" ) # feature importances plt . subplot ( 122 ) features = np . array ( feature_names ) sorted_ind = np . argsort ( overall_feature_importances ) [ : : - 1 ] features = features [ sorted_ind ] feature_names = feature_names [ sorted_ind ] overall_feature_importances = overall_feature_importances [ sorted_ind ] stdev_feature_importances = stdev_feature_importances [ sorted_ind ] plt . bar ( np . arange ( 0 , features . size ) , overall_feature_importances , yerr = stdev_feature_importances , width = 0.8 , color = 'grey' ) plt . xticks ( np . arange ( 0 , features . size ) , features , rotation = 90 ) plt . yticks ( [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1.0 ] ) plt . xlim ( - 0.75 , features . size - 1.0 + 0.75 ) plt . ylim ( 0.0 , 0.9 ) plt . ylabel ( 'relative importance' ) plt . title ( 'relative importance of features' ) plt . subplots_adjust ( wspace = 0.1 ) plt . savefig ( outfile , bbox_inches = 'tight' , dpi = 100 ) plt . close ( 'all' ) return outfile
2409	def dump_model_to_file ( prompt_string , feature_ext , classifier , text , score , model_path ) : model_file = { 'prompt' : prompt_string , 'extractor' : feature_ext , 'model' : classifier , 'text' : text , 'score' : score } pickle . dump ( model_file , file = open ( model_path , "w" ) )
9918	def validate_key ( self , key ) : try : confirmation = models . EmailConfirmation . objects . select_related ( "email__user" ) . get ( key = key ) except models . EmailConfirmation . DoesNotExist : raise serializers . ValidationError ( _ ( "The provided verification key is invalid." ) ) if confirmation . is_expired : raise serializers . ValidationError ( _ ( "That verification code has expired." ) ) # Cache confirmation instance self . _confirmation = confirmation return key
6157	def FIR_header ( fname_out , h ) : M = len ( h ) N = 3 # Coefficients per line f = open ( fname_out , 'wt' ) f . write ( '//define a FIR coefficient Array\n\n' ) f . write ( '#include <stdint.h>\n\n' ) f . write ( '#ifndef M_FIR\n' ) f . write ( '#define M_FIR %d\n' % M ) f . write ( '#endif\n' ) f . write ( '/************************************************************************/\n' ) f . write ( '/* FIR Filter Coefficients */\n' ) f . write ( 'float32_t h_FIR[M_FIR] = {' ) kk = 0 for k in range ( M ) : # k_mod = k % M if ( kk < N - 1 ) and ( k < M - 1 ) : f . write ( '%15.12f,' % h [ k ] ) kk += 1 elif ( kk == N - 1 ) & ( k < M - 1 ) : f . write ( '%15.12f,\n' % h [ k ] ) if k < M : f . write ( ' ' ) kk = 0 else : f . write ( '%15.12f' % h [ k ] ) f . write ( '};\n' ) f . write ( '/************************************************************************/\n' ) f . close ( )
1850	def LOOPNZ ( cpu , target ) : counter_name = { 16 : 'CX' , 32 : 'ECX' , 64 : 'RCX' } [ cpu . address_bit_size ] counter = cpu . write_register ( counter_name , cpu . read_register ( counter_name ) - 1 ) cpu . PC = Operators . ITEBV ( cpu . address_bit_size , counter != 0 , ( cpu . PC + target . read ( ) ) & ( ( 1 << target . size ) - 1 ) , cpu . PC + cpu . instruction . size )
9764	def cluster ( node ) : cluster_client = PolyaxonClient ( ) . cluster if node : try : node_config = cluster_client . get_node ( node ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load node `{}` info.' . format ( node ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_node_info ( node_config ) else : try : cluster_config = cluster_client . get_cluster ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load cluster info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_cluster_info ( cluster_config )
4335	def oops ( self ) : effect_args = [ 'oops' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'oops' ) return self
5459	def find_task_descriptor ( self , task_id ) : # It is not guaranteed that the index will be task_id - 1 when --tasks is # used with a min/max range. for task_descriptor in self . task_descriptors : if task_descriptor . task_metadata . get ( 'task-id' ) == task_id : return task_descriptor return None
1108	def get_grouped_opcodes ( self , n = 3 ) : codes = self . get_opcodes ( ) if not codes : codes = [ ( "equal" , 0 , 1 , 0 , 1 ) ] # Fixup leading and trailing groups if they show no changes. if codes [ 0 ] [ 0 ] == 'equal' : tag , i1 , i2 , j1 , j2 = codes [ 0 ] codes [ 0 ] = tag , max ( i1 , i2 - n ) , i2 , max ( j1 , j2 - n ) , j2 if codes [ - 1 ] [ 0 ] == 'equal' : tag , i1 , i2 , j1 , j2 = codes [ - 1 ] codes [ - 1 ] = tag , i1 , min ( i2 , i1 + n ) , j1 , min ( j2 , j1 + n ) nn = n + n group = [ ] for tag , i1 , i2 , j1 , j2 in codes : # End the current group and start a new one whenever # there is a large range with no changes. if tag == 'equal' and i2 - i1 > nn : group . append ( ( tag , i1 , min ( i2 , i1 + n ) , j1 , min ( j2 , j1 + n ) ) ) yield group group = [ ] i1 , j1 = max ( i1 , i2 - n ) , max ( j1 , j2 - n ) group . append ( ( tag , i1 , i2 , j1 , j2 ) ) if group and not ( len ( group ) == 1 and group [ 0 ] [ 0 ] == 'equal' ) : yield group
3988	def parallel_task_queue ( pool_size = multiprocessing . cpu_count ( ) ) : task_queue = TaskQueue ( pool_size ) yield task_queue task_queue . execute ( )
5742	def service_start ( service = None , param = None ) : if service is not None : to_run = [ "python" , service ] if param is not None : to_run += param return subprocess . Popen ( to_run ) return False
10350	def lint_directory ( source , target ) : for path in os . listdir ( source ) : if not path . endswith ( '.bel' ) : continue log . info ( 'linting: %s' , path ) with open ( os . path . join ( source , path ) ) as i , open ( os . path . join ( target , path ) , 'w' ) as o : lint_file ( i , o )
4396	def adsSyncWriteByNameEx ( port , address , data_name , value , data_type ) : # type: (int, AmsAddr, str, Any, Type) -> None # Get the handle of the PLC-variable handle = adsSyncReadWriteReqEx2 ( port , address , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING , ) # Write the value of a PLC-variable, via handle adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_VALBYHND , handle , value , data_type ) # Release the handle of the PLC-variable adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_RELEASEHND , 0 , handle , PLCTYPE_UDINT )
9412	def _is_simple_numeric ( data ) : for item in data : if isinstance ( item , set ) : item = list ( item ) if isinstance ( item , list ) : if not _is_simple_numeric ( item ) : return False elif not isinstance ( item , ( int , float , complex ) ) : return False return True
1559	def component_id ( self ) : if isinstance ( self . _component_id , HeronComponentSpec ) : if self . _component_id . name is None : # HeronComponentSpec instance's name attribute might not be available until # TopologyType metaclass finally sets it. This statement is to support __eq__(), # __hash__() and __str__() methods with safety, as raising Exception is not # appropriate this case. return "<No name available for HeronComponentSpec yet, uuid: %s>" % self . _component_id . uuid return self . _component_id . name elif isinstance ( self . _component_id , str ) : return self . _component_id else : raise ValueError ( "Component Id for this GlobalStreamId is not properly set: <%s:%s>" % ( str ( type ( self . _component_id ) ) , str ( self . _component_id ) ) )
8981	def _set_pixel_and_convert_color ( self , x , y , color ) : if color is None : return color = self . _convert_color_to_rrggbb ( color ) self . _set_pixel ( x , y , color )
12709	def world_to_body ( self , position ) : return np . array ( self . ode_body . getPosRelPoint ( tuple ( position ) ) )
13150	def log_state ( entity , state ) : p = { 'on' : entity , 'state' : state } _log ( TYPE_CODES . STATE , p )
12465	def read_config ( filename , args ) : # Initial vars config = defaultdict ( dict ) splitter = operator . methodcaller ( 'split' , ' ' ) converters = { __script__ : { 'env' : safe_path , 'pre_requirements' : splitter , } , 'pip' : { 'allow_external' : splitter , 'allow_unverified' : splitter , } } default = copy . deepcopy ( CONFIG ) sections = set ( iterkeys ( default ) ) # Append download-cache for old pip versions if int ( getattr ( pip , '__version__' , '1.x' ) . split ( '.' ) [ 0 ] ) < 6 : default [ 'pip' ] [ 'download_cache' ] = safe_path ( os . path . expanduser ( os . path . join ( '~' , '.{0}' . format ( __script__ ) , 'pip-cache' ) ) ) # Expand user and environ vars in config filename is_default = filename == DEFAULT_CONFIG filename = os . path . expandvars ( os . path . expanduser ( filename ) ) # Read config if it exists on disk if not is_default and not os . path . isfile ( filename ) : print_error ( 'Config file does not exist at {0!r}' . format ( filename ) ) return None parser = ConfigParser ( ) try : parser . read ( filename ) except ConfigParserError : print_error ( 'Cannot parse config file at {0!r}' . format ( filename ) ) return None # Apply config for each possible section for section in sections : if not parser . has_section ( section ) : continue items = parser . items ( section ) # Make auto convert here for integers and boolean values for key , value in items : try : value = int ( value ) except ( TypeError , ValueError ) : try : value = bool ( strtobool ( value ) ) except ValueError : pass if section in converters and key in converters [ section ] : value = converters [ section ] [ key ] ( value ) config [ section ] [ key ] = value # Update config with default values if necessary for section , data in iteritems ( default ) : if section not in config : config [ section ] = data else : for key , value in iteritems ( data ) : config [ section ] . setdefault ( key , value ) # Update bootstrap config from parsed args keys = set ( ( 'env' , 'hook' , 'install_dev_requirements' , 'ignore_activated' , 'pre_requirements' , 'quiet' , 'recreate' , 'requirements' ) ) for key in keys : value = getattr ( args , key ) config [ __script__ ] . setdefault ( key , value ) if key == 'pre_requirements' and not value : continue if value is not None : config [ __script__ ] [ key ] = value return config
10005	def clear_descendants ( self , source , clear_source = True ) : removed = self . cellgraph . clear_descendants ( source , clear_source ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
5006	def handle_enterprise_logistration ( backend , user , * * kwargs ) : request = backend . strategy . request enterprise_customer = get_enterprise_customer_for_running_pipeline ( request , { 'backend' : backend . name , 'kwargs' : kwargs } ) if enterprise_customer is None : # This pipeline element is not being activated as a part of an Enterprise logistration return # proceed with the creation of a link between the user and the enterprise customer, then exit. enterprise_customer_user , _ = EnterpriseCustomerUser . objects . update_or_create ( enterprise_customer = enterprise_customer , user_id = user . id ) enterprise_customer_user . update_session ( request )
9704	def checkSerial ( self ) : for item in self . rxSerial ( self . _TUN . _tun . mtu ) : # print("about to send: {0}".format(item)) try : self . _TUN . _tun . write ( item ) except pytun . Error as error : print ( "pytun error writing: {0}" . format ( item ) ) print ( error )
1246	def disconnect ( self ) : # If we are not connected, return error. if not self . socket : logging . warning ( "No active socket to close!" ) return # Close our socket. self . socket . close ( ) self . socket = None
3219	def get_network_acls ( vpc , * * conn ) : route_tables = describe_network_acls ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) nacl_ids = [ ] for r in route_tables : nacl_ids . append ( r [ "NetworkAclId" ] ) return nacl_ids
6460	def _ends_in_doubled_cons ( self , term ) : return ( len ( term ) > 1 and term [ - 1 ] not in self . _vowels and term [ - 2 ] == term [ - 1 ] )
2311	def b_fit_score ( self , x , y ) : x = np . reshape ( minmax_scale ( x ) , ( - 1 , 1 ) ) y = np . reshape ( minmax_scale ( y ) , ( - 1 , 1 ) ) poly = PolynomialFeatures ( degree = self . degree ) poly_x = poly . fit_transform ( x ) poly_x [ : , 1 ] = 0 poly_x [ : , 2 ] = 0 regressor = LinearRegression ( ) regressor . fit ( poly_x , y ) y_predict = regressor . predict ( poly_x ) error = mean_squared_error ( y_predict , y ) return error
2911	def _find_ancestor_from_name ( self , name ) : if self . parent is None : return None if self . parent . get_name ( ) == name : return self . parent return self . parent . _find_ancestor_from_name ( name )
5923	def get_configuration ( filename = CONFIGNAME ) : global cfg , configuration # very iffy --- most of the whole config mod should a class #: :data:`cfg` is the instance of :class:`GMXConfigParser` that makes all #: global configuration data accessible cfg = GMXConfigParser ( filename = filename ) # update module-level cfg globals ( ) . update ( cfg . configuration ) # update configdir, templatesdir ... configuration = cfg . configuration # update module-level configuration return cfg
4200	def _thumbnail_div ( full_dir , fname , snippet , is_backref = False ) : thumb = os . path . join ( full_dir , 'images' , 'thumb' , 'sphx_glr_%s_thumb.png' % fname [ : - 3 ] ) ref_name = os . path . join ( full_dir , fname ) . replace ( os . path . sep , '_' ) template = BACKREF_THUMBNAIL_TEMPLATE if is_backref else THUMBNAIL_TEMPLATE return template . format ( snippet = snippet , thumbnail = thumb , ref_name = ref_name )
534	def _getRegions ( self ) : def makeRegion ( name , r ) : """Wrap a engine region with a nupic.engine_internal.Region Also passes the containing nupic.engine_internal.Network network in _network. This function is passed a value wrapper to the CollectionWrapper """ r = Region ( r , self ) #r._network = self return r regions = CollectionWrapper ( engine_internal . Network . getRegions ( self ) , makeRegion ) return regions
8912	def includeme ( config ) : settings = config . registry . settings if asbool ( settings . get ( 'twitcher.rpcinterface' , True ) ) : LOGGER . debug ( 'Twitcher XML-RPC Interface enabled.' ) # include twitcher config config . include ( 'twitcher.config' ) # using basic auth config . include ( 'twitcher.basicauth' ) # pyramid xml-rpc # http://docs.pylonsproject.org/projects/pyramid-rpc/en/latest/xmlrpc.html config . include ( 'pyramid_rpc.xmlrpc' ) config . include ( 'twitcher.db' ) config . add_xmlrpc_endpoint ( 'api' , '/RPC2' ) # register xmlrpc methods config . add_xmlrpc_method ( RPCInterface , attr = 'generate_token' , endpoint = 'api' , method = 'generate_token' ) config . add_xmlrpc_method ( RPCInterface , attr = 'revoke_token' , endpoint = 'api' , method = 'revoke_token' ) config . add_xmlrpc_method ( RPCInterface , attr = 'revoke_all_tokens' , endpoint = 'api' , method = 'revoke_all_tokens' ) config . add_xmlrpc_method ( RPCInterface , attr = 'register_service' , endpoint = 'api' , method = 'register_service' ) config . add_xmlrpc_method ( RPCInterface , attr = 'unregister_service' , endpoint = 'api' , method = 'unregister_service' ) config . add_xmlrpc_method ( RPCInterface , attr = 'get_service_by_name' , endpoint = 'api' , method = 'get_service_by_name' ) config . add_xmlrpc_method ( RPCInterface , attr = 'get_service_by_url' , endpoint = 'api' , method = 'get_service_by_url' ) config . add_xmlrpc_method ( RPCInterface , attr = 'clear_services' , endpoint = 'api' , method = 'clear_services' ) config . add_xmlrpc_method ( RPCInterface , attr = 'list_services' , endpoint = 'api' , method = 'list_services' )
3740	def omega ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ 'LK' , 'DEFINITION' ] ) : def list_methods ( ) : methods = [ ] if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) : methods . append ( 'PSRK' ) if CASRN in _crit_PassutDanner . index and not np . isnan ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) : methods . append ( 'PD' ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'omega' ] ) : methods . append ( 'YAWS' ) Tcrit , Pcrit = Tc ( CASRN ) , Pc ( CASRN ) if Tcrit and Pcrit : if Tb ( CASRN ) : methods . append ( 'LK' ) if VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tcrit * 0.7 ) : methods . append ( 'DEFINITION' ) # TODO: better integration if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section if Method == 'PSRK' : _omega = float ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) elif Method == 'PD' : _omega = float ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) elif Method == 'YAWS' : _omega = float ( _crit_Yaws . at [ CASRN , 'omega' ] ) elif Method == 'LK' : _omega = LK_omega ( Tb ( CASRN ) , Tc ( CASRN ) , Pc ( CASRN ) ) elif Method == 'DEFINITION' : P = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tc ( CASRN ) * 0.7 ) _omega = - log10 ( P / Pc ( CASRN ) ) - 1.0 elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
11548	def guess_array_memory_usage ( bam_readers , dtype , use_strand = False ) : ARRAY_COUNT = 5 if not isinstance ( bam_readers , list ) : bam_readers = [ bam_readers ] if isinstance ( dtype , basestring ) : dtype = NUMPY_DTYPES . get ( dtype , None ) use_strand = use_strand + 1 #if false, factor of 1, if true, factor of 2 dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = None , force_dtype = False ) if not [ dt for dt in dtypes if dt is not None ] : #found no info from idx dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = dtype or numpy . uint64 , force_dtype = True ) elif dtype : dtypes = [ dtype if dt else None for dt in dtypes ] read_groups = [ ] no_read_group = False for bam in bam_readers : rgs = bam . get_read_groups ( ) if rgs : for rg in rgs : if rg not in read_groups : read_groups . append ( rg ) else : no_read_group = True read_groups = len ( read_groups ) + no_read_group max_ref_size = 0 array_byte_overhead = sys . getsizeof ( numpy . zeros ( ( 0 ) , dtype = numpy . uint64 ) ) array_count = ARRAY_COUNT * use_strand * read_groups for bam in bam_readers : for i , ( name , length ) in enumerate ( bam . get_references ( ) ) : if dtypes [ i ] is not None : max_ref_size = max ( max_ref_size , ( length + length * dtypes [ i ] ( ) . nbytes * array_count + ( array_byte_overhead * ( array_count + 1 ) ) ) ) return max_ref_size
2949	def execute ( self , task , script , * * kwargs ) : locals ( ) . update ( kwargs ) exec ( script )
12225	def convertGribToTiff ( listeFile , listParam , listLevel , liststep , grid , startDate , endDate , outFolder ) : dicoValues = { } for l in listeFile : grbs = pygrib . open ( l ) grbs . seek ( 0 ) index = 1 for j in range ( len ( listLevel ) , 0 , - 1 ) : for i in range ( len ( listParam ) - 1 , - 1 , - 1 ) : grb = grbs [ index ] p = grb . name . replace ( ' ' , '_' ) if grb . level != 0 : l = str ( grb . level ) + '_' + grb . typeOfLevel else : l = grb . typeOfLevel if p + '_' + l not in dicoValues . keys ( ) : dicoValues [ p + '_' + l ] = [ ] dicoValues [ p + '_' + l ] . append ( grb . values ) shape = grb . values . shape lat , lon = grb . latlons ( ) geoparam = ( lon . min ( ) , lat . max ( ) , grid , grid ) index += 1 nbJour = ( endDate - startDate ) . days + 1 #on joute des arrayNan si il manque des fichiers for s in range ( 0 , ( len ( liststep ) * nbJour - len ( listeFile ) ) ) : for k in dicoValues . keys ( ) : dicoValues [ k ] . append ( np . full ( shape , np . nan ) ) #On écrit pour chacune des variables dans un fichier for i in range ( len ( dicoValues . keys ( ) ) - 1 , - 1 , - 1 ) : dictParam = dict ( ( k , dicoValues [ dicoValues . keys ( ) [ i ] ] [ k ] ) for k in range ( 0 , len ( dicoValues [ dicoValues . keys ( ) [ i ] ] ) ) ) sorted ( dictParam . items ( ) , key = lambda x : x [ 0 ] ) outputImg = outFolder + '/' + dicoValues . keys ( ) [ i ] + '_' + startDate . strftime ( '%Y%M%d' ) + '_' + endDate . strftime ( '%Y%M%d' ) + '.tif' writeTiffFromDicoArray ( dictParam , outputImg , shape , geoparam ) for f in listeFile : os . remove ( f )
7903	def set_handlers ( self , priority = 10 ) : self . stream . set_message_handler ( "groupchat" , self . __groupchat_message , None , priority ) self . stream . set_message_handler ( "error" , self . __error_message , None , priority ) self . stream . set_presence_handler ( "available" , self . __presence_available , None , priority ) self . stream . set_presence_handler ( "unavailable" , self . __presence_unavailable , None , priority ) self . stream . set_presence_handler ( "error" , self . __presence_error , None , priority )
11327	def autodiscover ( ) : import imp from django . conf import settings for app in settings . INSTALLED_APPS : try : app_path = __import__ ( app , { } , { } , [ app . split ( '.' ) [ - 1 ] ] ) . __path__ except AttributeError : continue try : imp . find_module ( 'oembed_providers' , app_path ) except ImportError : continue __import__ ( "%s.oembed_providers" % app )
3602	def get_user ( self ) : token = self . authenticator . create_token ( self . extra ) user_id = self . extra . get ( 'id' ) return FirebaseUser ( self . email , token , self . provider , user_id )
10391	def workflow_aggregate ( graph : BELGraph , node : BaseEntity , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , aggregator : Optional [ Callable [ [ Iterable [ float ] ] , float ] ] = None , ) -> Optional [ float ] : runners = workflow ( graph , node , key = key , tag = tag , default_score = default_score , runs = runs ) scores = [ runner . get_final_score ( ) for runner in runners ] if not scores : log . warning ( 'Unable to run the heat diffusion workflow for %s' , node ) return if aggregator is None : return np . average ( scores ) return aggregator ( scores )
7987	def request_software_version ( stanza_processor , target_jid , callback , error_callback = None ) : stanza = Iq ( to_jid = target_jid , stanza_type = "get" ) payload = VersionPayload ( ) stanza . set_payload ( payload ) def wrapper ( stanza ) : """Wrapper for the user-provided `callback` that extracts the payload from stanza received.""" payload = stanza . get_payload ( VersionPayload ) if payload is None : if error_callback : error_callback ( stanza ) else : logger . warning ( "Invalid version query response." ) else : callback ( payload ) stanza_processor . set_response_handlers ( stanza , wrapper , error_callback ) stanza_processor . send ( stanza )
1893	def _reset ( self , constraints = None ) : if self . _proc is None : self . _start_proc ( ) else : if self . support_reset : self . _send ( "(reset)" ) for cfg in self . _init : self . _send ( cfg ) else : self . _stop_proc ( ) self . _start_proc ( ) if constraints is not None : self . _send ( constraints )
8087	def font ( self , fontpath = None , fontsize = None ) : if fontpath is not None : self . _canvas . fontfile = fontpath else : return self . _canvas . fontfile if fontsize is not None : self . _canvas . fontsize = fontsize
3633	def cardInfo ( self , resource_id ) : # TODO: add referer to headers (futweb) base_id = baseId ( resource_id ) if base_id in self . players : return self . players [ base_id ] else : # not a player? url = '{0}{1}.json' . format ( card_info_url , base_id ) return requests . get ( url , timeout = self . timeout ) . json ( )
4100	def aic_eigen ( s , N ) : import numpy as np kaic = [ ] n = len ( s ) for k in range ( 0 , n - 1 ) : ak = 1. / ( n - k ) * np . sum ( s [ k + 1 : ] ) gk = np . prod ( s [ k + 1 : ] ** ( 1. / ( n - k ) ) ) kaic . append ( - 2. * ( n - k ) * N * np . log ( gk / ak ) + 2. * k * ( 2. * n - k ) ) return kaic
8982	def _set_pixel ( self , x , y , color ) : if not self . is_in_bounds ( x , y ) : return rgb = self . _convert_rrggbb_to_image_color ( color ) x -= self . _min_x y -= self . _min_y self . _image . putpixel ( ( x , y ) , rgb )
13301	def df_quantile ( df , nb = 100 ) : quantiles = np . linspace ( 0 , 1. , nb ) res = pd . DataFrame ( ) for q in quantiles : res = res . append ( df . quantile ( q ) , ignore_index = True ) return res
3319	def create ( self , path , lock ) : self . _lock . acquire_write ( ) try : # We expect only a lock definition, not an existing lock assert lock . get ( "token" ) is None assert lock . get ( "expire" ) is None , "Use timeout instead of expire" assert path and "/" in path # Normalize root: /foo/bar org_path = path path = normalize_lock_root ( path ) lock [ "root" ] = path # Normalize timeout from ttl to expire-date timeout = float ( lock . get ( "timeout" ) ) if timeout is None : timeout = LockStorageDict . LOCK_TIME_OUT_DEFAULT elif timeout < 0 or timeout > LockStorageDict . LOCK_TIME_OUT_MAX : timeout = LockStorageDict . LOCK_TIME_OUT_MAX lock [ "timeout" ] = timeout lock [ "expire" ] = time . time ( ) + timeout validate_lock ( lock ) token = generate_lock_token ( ) lock [ "token" ] = token # Store lock self . _dict [ token ] = lock # Store locked path reference key = "URL2TOKEN:{}" . format ( path ) if key not in self . _dict : self . _dict [ key ] = [ token ] else : # Note: Shelve dictionary returns copies, so we must reassign # values: tokList = self . _dict [ key ] tokList . append ( token ) self . _dict [ key ] = tokList self . _flush ( ) _logger . debug ( "LockStorageDict.set({!r}): {}" . format ( org_path , lock_string ( lock ) ) ) return lock finally : self . _lock . release ( )
11798	def suppose ( self , var , value ) : self . support_pruning ( ) removals = [ ( var , a ) for a in self . curr_domains [ var ] if a != value ] self . curr_domains [ var ] = [ value ] return removals
243	def daily_txns_with_bar_data ( transactions , market_data ) : transactions . index . name = 'date' txn_daily = pd . DataFrame ( transactions . assign ( amount = abs ( transactions . amount ) ) . groupby ( [ 'symbol' , pd . TimeGrouper ( 'D' ) ] ) . sum ( ) [ 'amount' ] ) txn_daily [ 'price' ] = market_data [ 'price' ] . unstack ( ) txn_daily [ 'volume' ] = market_data [ 'volume' ] . unstack ( ) txn_daily = txn_daily . reset_index ( ) . set_index ( 'date' ) return txn_daily
9402	def _exist ( self , name ) : cmd = 'exist("%s")' % name resp = self . _engine . eval ( cmd , silent = True ) . strip ( ) exist = int ( resp . split ( ) [ - 1 ] ) if exist == 0 : msg = 'Value "%s" does not exist in Octave workspace' raise Oct2PyError ( msg % name ) return exist
10207	def file_download_event_builder ( event , sender_app , obj = None , * * kwargs ) : event . update ( dict ( # When: timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , # What: bucket_id = str ( obj . bucket_id ) , file_id = str ( obj . file_id ) , file_key = obj . key , size = obj . file . size , referrer = request . referrer , # Who: * * get_user ( ) ) ) return event
11367	def locate ( pattern , root = os . curdir ) : for path , dummy , files in os . walk ( os . path . abspath ( root ) ) : for filename in fnmatch . filter ( files , pattern ) : yield os . path . join ( path , filename )
5468	def get_event_of_type ( op , event_type ) : events = get_events ( op ) if not events : return None return [ e for e in events if e . get ( 'details' , { } ) . get ( '@type' ) == event_type ]
5050	def from_children ( cls , program_uuid , * children ) : if not children or any ( child is None for child in children ) : return None granted = all ( ( child . granted for child in children ) ) exists = any ( ( child . exists for child in children ) ) usernames = set ( [ child . username for child in children ] ) enterprises = set ( [ child . enterprise_customer for child in children ] ) if not len ( usernames ) == len ( enterprises ) == 1 : raise InvalidProxyConsent ( 'Children used to create a bulk proxy consent object must ' 'share a single common username and EnterpriseCustomer.' ) username = children [ 0 ] . username enterprise_customer = children [ 0 ] . enterprise_customer return cls ( enterprise_customer = enterprise_customer , username = username , program_uuid = program_uuid , exists = exists , granted = granted , child_consents = children )
633	def createSynapse ( self , segment , presynapticCell , permanence ) : idx = len ( segment . _synapses ) synapse = Synapse ( segment , presynapticCell , permanence , self . _nextSynapseOrdinal ) self . _nextSynapseOrdinal += 1 segment . _synapses . add ( synapse ) self . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) self . _numSynapses += 1 return synapse
6745	def iter_sites ( sites = None , site = None , renderer = None , setter = None , no_secure = False , verbose = None ) : if verbose is None : verbose = get_verbose ( ) hostname = get_current_hostname ( ) target_sites = env . available_sites_by_host . get ( hostname , None ) if sites is None : site = site or env . SITE or ALL if site == ALL : sites = list ( six . iteritems ( env . sites ) ) else : sys . stderr . flush ( ) sites = [ ( site , env . sites . get ( site ) ) ] renderer = renderer #or render_remote_paths env_default = save_env ( ) for _site , site_data in sorted ( sites ) : if no_secure and _site . endswith ( '_secure' ) : continue # Only load site configurations that are allowed for this host. if target_sites is None : pass else : assert isinstance ( target_sites , ( tuple , list ) ) if _site not in target_sites : if verbose : print ( 'Skipping site %s because not in among target sites.' % _site ) continue env . update ( env_default ) env . update ( env . sites . get ( _site , { } ) ) env . SITE = _site if callable ( renderer ) : renderer ( ) if setter : setter ( _site ) yield _site , site_data # Revert modified keys. env . update ( env_default ) # Remove keys that were added, not simply updated. added_keys = set ( env ) . difference ( env_default ) for key in added_keys : # Don't remove internally maintained variables, because these are used to cache hostnames # used by iter_sites(). if key . startswith ( '_' ) : continue del env [ key ]
10463	def menuitemenabled ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) if menu_handle . AXEnabled : return 1 except LdtpServerException : pass return 0
5172	def _auto_client_files ( cls , client , ca_path = None , ca_contents = None , cert_path = None , cert_contents = None , key_path = None , key_contents = None ) : files = [ ] if ca_path and ca_contents : client [ 'ca' ] = ca_path files . append ( dict ( path = ca_path , contents = ca_contents , mode = DEFAULT_FILE_MODE ) ) if cert_path and cert_contents : client [ 'cert' ] = cert_path files . append ( dict ( path = cert_path , contents = cert_contents , mode = DEFAULT_FILE_MODE ) ) if key_path and key_contents : client [ 'key' ] = key_path files . append ( dict ( path = key_path , contents = key_contents , mode = DEFAULT_FILE_MODE , ) ) return files
6193	def add ( self , num_particles , D ) : self . _plist += self . _generate ( num_particles , D , box = self . box , rs = self . rs )
12453	def config_to_args ( config ) : result = [ ] for key , value in iteritems ( config ) : if value is False : continue key = '--{0}' . format ( key . replace ( '_' , '-' ) ) if isinstance ( value , ( list , set , tuple ) ) : for item in value : result . extend ( ( key , smart_str ( item ) ) ) elif value is not True : result . extend ( ( key , smart_str ( value ) ) ) else : result . append ( key ) return tuple ( result )
13023	def select ( self , sql_string , cols , * args , * * kwargs ) : working_columns = None if kwargs . get ( 'columns' ) is not None : working_columns = kwargs . pop ( 'columns' ) query = self . _assemble_select ( sql_string , cols , * args , * kwargs ) return self . _execute ( query , working_columns = working_columns )
5507	def _image_name_from_url ( url ) : find = r'https?://|[^\w]' replace = '_' return re . sub ( find , replace , url ) . strip ( '_' )
8198	def angle ( x1 , y1 , x2 , y2 ) : sign = 1.0 usign = ( x1 * y2 - y1 * x2 ) if usign < 0 : sign = - 1.0 num = x1 * x2 + y1 * y2 den = hypot ( x1 , y1 ) * hypot ( x2 , y2 ) ratio = min ( max ( num / den , - 1.0 ) , 1.0 ) return sign * degrees ( acos ( ratio ) )
1550	def _get_spout ( self ) : spout = topology_pb2 . Spout ( ) spout . comp . CopyFrom ( self . _get_base_component ( ) ) # Add output streams self . _add_out_streams ( spout ) return spout
5005	def get_enterprise_customer_for_running_pipeline ( request , pipeline ) : # pylint: disable=invalid-name sso_provider_id = request . GET . get ( 'tpa_hint' ) if pipeline : sso_provider_id = Registry . get_from_pipeline ( pipeline ) . provider_id return get_enterprise_customer_for_sso ( sso_provider_id )
9822	def list ( page ) : # pylint:disable=redefined-builtin user = AuthConfigManager . get_value ( 'username' ) if not user : Printer . print_error ( 'Please login first. `polyaxon login --help`' ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_projects ( user , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get list of projects.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Projects for current user' ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No projects found for current user' ) objects = list_dicts_to_tabulate ( [ o . to_light_dict ( humanize_values = True , exclude_attrs = [ 'uuid' , 'experiment_groups' , 'experiments' , 'description' , 'num_experiments' , 'num_independent_experiments' , 'num_experiment_groups' , 'num_jobs' , 'num_builds' , 'unique_name' ] ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Projects:" ) dict_tabulate ( objects , is_list_dict = True )
4083	def check ( self , text : str , srctext = None ) -> [ Match ] : root = self . _get_root ( self . _url , self . _encode ( text , srctext ) ) return [ Match ( e . attrib ) for e in root if e . tag == 'error' ]
2598	def can_sequence ( obj ) : if istype ( obj , sequence_types ) : t = type ( obj ) return t ( [ can ( i ) for i in obj ] ) else : return obj
747	def anomalyGetLabels ( self , start , end ) : return self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabels ( start , end )
4091	def addSources ( self , * sources ) : self . _sources . extend ( sources ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB source(s): %s' % ', ' . join ( [ str ( x ) for x in self . _sources ] ) ) return self
12692	def write_tersoff_potential ( parameters ) : lines = [ ] for ( e1 , e2 , e3 ) , params in parameters . items ( ) : if len ( params ) != 14 : raise ValueError ( 'tersoff three body potential expects 14 parameters' ) lines . append ( ' ' . join ( [ e1 , e2 , e3 ] + [ '{:16.8g}' . format ( _ ) for _ in params ] ) ) return '\n' . join ( lines )
5139	def process_token ( self , kind , string , start , end , line ) : if self . current_block . is_comment : if kind == tokenize . COMMENT : self . current_block . add ( string , start , end , line ) else : self . new_noncomment ( start [ 0 ] , end [ 0 ] ) else : if kind == tokenize . COMMENT : self . new_comment ( string , start , end , line ) else : self . current_block . add ( string , start , end , line )
3605	def get ( self , url , name , params = None , headers = None , connection = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) return make_get_request ( endpoint , params , headers , connection = connection )
2068	def get_cars_data ( ) : df = pd . read_csv ( 'source_data/cars/car.data.txt' ) X = df . reindex ( columns = [ x for x in df . columns . values if x != 'class' ] ) y = df . reindex ( columns = [ 'class' ] ) y = preprocessing . LabelEncoder ( ) . fit_transform ( y . values . reshape ( - 1 , ) ) mapping = [ { 'col' : 'buying' , 'mapping' : [ ( 'vhigh' , 0 ) , ( 'high' , 1 ) , ( 'med' , 2 ) , ( 'low' , 3 ) ] } , { 'col' : 'maint' , 'mapping' : [ ( 'vhigh' , 0 ) , ( 'high' , 1 ) , ( 'med' , 2 ) , ( 'low' , 3 ) ] } , { 'col' : 'doors' , 'mapping' : [ ( '2' , 0 ) , ( '3' , 1 ) , ( '4' , 2 ) , ( '5more' , 3 ) ] } , { 'col' : 'persons' , 'mapping' : [ ( '2' , 0 ) , ( '4' , 1 ) , ( 'more' , 2 ) ] } , { 'col' : 'lug_boot' , 'mapping' : [ ( 'small' , 0 ) , ( 'med' , 1 ) , ( 'big' , 2 ) ] } , { 'col' : 'safety' , 'mapping' : [ ( 'high' , 0 ) , ( 'med' , 1 ) , ( 'low' , 2 ) ] } , ] return X , y , mapping
7155	def raw ( prompt , * args , * * kwargs ) : go_back = kwargs . get ( 'go_back' , '<' ) type_ = kwargs . get ( 'type' , str ) default = kwargs . get ( 'default' , '' ) with stdout_redirected ( sys . stderr ) : while True : try : if kwargs . get ( 'secret' , False ) : answer = getpass . getpass ( prompt ) elif sys . version_info < ( 3 , 0 ) : answer = raw_input ( prompt ) else : answer = input ( prompt ) if not answer : answer = default if answer == go_back : raise QuestionnaireGoBack return type_ ( answer ) except ValueError : eprint ( '\n`{}` is not a valid `{}`\n' . format ( answer , type_ ) )
9014	def _fill_pattern_collection ( self , pattern_collection , values ) : pattern = values . get ( PATTERNS , [ ] ) for pattern_to_parse in pattern : parsed_pattern = self . _pattern ( pattern_to_parse ) pattern_collection . append ( parsed_pattern )
6356	def dist_strcmp95 ( src , tar , long_strings = False ) : return Strcmp95 ( ) . dist ( src , tar , long_strings )
8553	def delete_ipblock ( self , ipblock_id ) : response = self . _perform_request ( url = '/ipblocks/' + ipblock_id , method = 'DELETE' ) return response
2827	def convert_hardtanh ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting hardtanh (clip) ...' ) def target_layer ( x , max_val = float ( params [ 'max_val' ] ) , min_val = float ( params [ 'min_val' ] ) ) : return tf . minimum ( max_val , tf . maximum ( min_val , x ) ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
7758	def send ( self , stanza ) : if self . uplink : self . uplink . send ( stanza ) else : raise NoRouteError ( "No route for stanza" )
10222	def get_nift_values ( ) -> Mapping [ str , str ] : r = get_bel_resource ( NIFT ) return { name . lower ( ) : name for name in r [ 'Values' ] }
3121	def make_signed_jwt ( signer , payload , key_id = None ) : header = { 'typ' : 'JWT' , 'alg' : 'RS256' } if key_id is not None : header [ 'kid' ] = key_id segments = [ _helpers . _urlsafe_b64encode ( _helpers . _json_encode ( header ) ) , _helpers . _urlsafe_b64encode ( _helpers . _json_encode ( payload ) ) , ] signing_input = b'.' . join ( segments ) signature = signer . sign ( signing_input ) segments . append ( _helpers . _urlsafe_b64encode ( signature ) ) logger . debug ( str ( segments ) ) return b'.' . join ( segments )
11278	def parse_address_list ( addrs ) : for addr in addrs . split ( ',' ) : elem = addr . split ( '-' ) if len ( elem ) == 1 : # a number yield int ( elem [ 0 ] ) elif len ( elem ) == 2 : # a range inclusive start , end = list ( map ( int , elem ) ) for i in range ( start , end + 1 ) : yield i else : # more than one hyphen raise ValueError ( 'format error in %s' % addr )
2665	def readinto ( self , buf , * * kwargs ) : self . i2c . readfrom_into ( self . device_address , buf , * * kwargs ) if self . _debug : print ( "i2c_device.readinto:" , [ hex ( i ) for i in buf ] )
8672	def list_keys ( key_name , max_suggestions , cutoff , jsonify , locked , key_type , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase , quiet = jsonify ) try : keys = stash . list ( key_name = key_name , max_suggestions = max_suggestions , cutoff = cutoff , locked_only = locked , key_type = key_type ) except GhostError as ex : sys . exit ( ex ) if jsonify : click . echo ( json . dumps ( keys , indent = 4 , sort_keys = True ) ) elif not keys : click . echo ( 'The stash is empty. Go on, put some keys in there...' ) else : click . echo ( 'Listing all keys...' ) click . echo ( _prettify_list ( keys ) )
10773	def add_node ( self , node , offset ) : # calculate x,y from offset considering axis start and end points width = self . end [ 0 ] - self . start [ 0 ] height = self . end [ 1 ] - self . start [ 1 ] node . x = self . start [ 0 ] + ( width * offset ) node . y = self . start [ 1 ] + ( height * offset ) self . nodes [ node . ID ] = node
4429	async def _queue ( self , ctx , page : int = 1 ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'There\'s nothing in the queue! Why not queue something?' ) items_per_page = 10 pages = math . ceil ( len ( player . queue ) / items_per_page ) start = ( page - 1 ) * items_per_page end = start + items_per_page queue_list = '' for index , track in enumerate ( player . queue [ start : end ] , start = start ) : queue_list += f'`{index + 1}.` [**{track.title}**]({track.uri})\n' embed = discord . Embed ( colour = discord . Color . blurple ( ) , description = f'**{len(player.queue)} tracks**\n\n{queue_list}' ) embed . set_footer ( text = f'Viewing page {page}/{pages}' ) await ctx . send ( embed = embed )
11912	def fail ( message = None , exit_status = None ) : print ( 'Error:' , message , file = sys . stderr ) sys . exit ( exit_status or 1 )
7476	def inserted_indels ( indels , ocatg ) : ## return copy with indels inserted newcatg = np . zeros ( ocatg . shape , dtype = np . uint32 ) ## iterate over loci and make extensions for indels for iloc in xrange ( ocatg . shape [ 0 ] ) : ## get indels indices indidx = np . where ( indels [ iloc , : ] ) [ 0 ] if np . any ( indidx ) : ## which new (empty) rows will be added allrows = np . arange ( ocatg . shape [ 1 ] ) mask = np . ones ( allrows . shape [ 0 ] , dtype = np . bool_ ) for idx in indidx : mask [ idx ] = False not_idx = allrows [ mask == 1 ] ## fill in new data into all other spots newcatg [ iloc ] [ not_idx ] = ocatg [ iloc , : not_idx . shape [ 0 ] ] else : newcatg [ iloc ] = ocatg [ iloc ] return newcatg
12365	def get ( self , id ) : info = super ( Images , self ) . get ( id ) return ImageActions ( self . api , parent = self , * * info )
1264	def sanity_check_states ( states_spec ) : # Leave incoming states dict intact. states = copy . deepcopy ( states_spec ) # Unique state shortform. is_unique = ( 'shape' in states ) if is_unique : states = dict ( state = states ) # Normalize states. for name , state in states . items ( ) : # Convert int to unary tuple. if isinstance ( state [ 'shape' ] , int ) : state [ 'shape' ] = ( state [ 'shape' ] , ) # Set default type to float. if 'type' not in state : state [ 'type' ] = 'float' return states , is_unique
4930	def transform_courserun_title ( self , content_metadata_item ) : title = content_metadata_item . get ( 'title' ) or '' course_run_start = content_metadata_item . get ( 'start' ) if course_run_start : if course_available_for_enrollment ( content_metadata_item ) : title += ' ({starts}: {:%B %Y})' . format ( parse_lms_api_datetime ( course_run_start ) , starts = _ ( 'Starts' ) ) else : title += ' ({:%B %Y} - {enrollment_closed})' . format ( parse_lms_api_datetime ( course_run_start ) , enrollment_closed = _ ( 'Enrollment Closed' ) ) title_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : title_with_locales . append ( { 'locale' : locale , 'value' : title } ) return title_with_locales
5180	def base_url ( self ) : return '{proto}://{host}:{port}{url_path}' . format ( proto = self . protocol , host = self . host , port = self . port , url_path = self . url_path , )
6407	def hmean ( nums ) : if len ( nums ) < 1 : raise AttributeError ( 'hmean requires at least one value' ) elif len ( nums ) == 1 : return nums [ 0 ] else : for i in range ( 1 , len ( nums ) ) : if nums [ 0 ] != nums [ i ] : break else : return nums [ 0 ] if 0 in nums : if nums . count ( 0 ) > 1 : return float ( 'nan' ) return 0 return len ( nums ) / sum ( 1 / i for i in nums )
9003	def _compute_scale ( self , instruction_id , svg_dict ) : bbox = list ( map ( float , svg_dict [ "svg" ] [ "@viewBox" ] . split ( ) ) ) scale = self . _zoom / ( bbox [ 3 ] - bbox [ 1 ] ) self . _symbol_id_to_scale [ instruction_id ] = scale
7756	def _set_response_handlers ( self , stanza , res_handler , err_handler , timeout_handler = None , timeout = None ) : # pylint: disable-msg=R0913 self . fix_out_stanza ( stanza ) to_jid = stanza . to_jid if to_jid : to_jid = unicode ( to_jid ) if timeout_handler : def callback ( dummy1 , dummy2 ) : """Wrapper for the timeout handler to make it compatible with the `ExpiringDictionary` """ timeout_handler ( ) self . _iq_response_handlers . set_item ( ( stanza . stanza_id , to_jid ) , ( res_handler , err_handler ) , timeout , callback ) else : self . _iq_response_handlers . set_item ( ( stanza . stanza_id , to_jid ) , ( res_handler , err_handler ) , timeout )
9864	def get_home ( self , home_id ) : if home_id not in self . _all_home_ids : _LOGGER . error ( "Could not find any Tibber home with id: %s" , home_id ) return None if home_id not in self . _homes . keys ( ) : self . _homes [ home_id ] = TibberHome ( home_id , self ) return self . _homes [ home_id ]
398	def sigmoid_cross_entropy ( output , target , name = None ) : return tf . reduce_mean ( tf . nn . sigmoid_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )
12861	def add_months ( self , month_int ) : month_int += self . month while month_int > 12 : self = BusinessDate . add_years ( self , 1 ) month_int -= 12 while month_int < 1 : self = BusinessDate . add_years ( self , - 1 ) month_int += 12 l = monthrange ( self . year , month_int ) [ 1 ] return BusinessDate . from_ymd ( self . year , month_int , min ( l , self . day ) )
3439	def _escape_str_id ( id_str ) : for c in ( "'" , '"' ) : if id_str . startswith ( c ) and id_str . endswith ( c ) and id_str . count ( c ) == 2 : id_str = id_str . strip ( c ) for char , escaped_char in _renames : id_str = id_str . replace ( char , escaped_char ) return id_str
4374	def get_messages_payload ( self , socket , timeout = None ) : try : msgs = socket . get_multiple_client_msgs ( timeout = timeout ) data = self . encode_payload ( msgs ) except Empty : data = "" return data
3198	def start ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _post ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'actions/start' ) )
13313	def _activate ( self ) : old_syspath = set ( sys . path ) site . addsitedir ( self . site_path ) site . addsitedir ( self . bin_path ) new_syspaths = set ( sys . path ) - old_syspath for path in new_syspaths : sys . path . remove ( path ) sys . path . insert ( 1 , path ) if not hasattr ( sys , 'real_prefix' ) : sys . real_prefix = sys . prefix sys . prefix = self . path
68	def copy ( self , x1 = None , y1 = None , x2 = None , y2 = None , label = None ) : return BoundingBox ( x1 = self . x1 if x1 is None else x1 , x2 = self . x2 if x2 is None else x2 , y1 = self . y1 if y1 is None else y1 , y2 = self . y2 if y2 is None else y2 , label = self . label if label is None else label )
1301	def mouse_event ( dwFlags : int , dx : int , dy : int , dwData : int , dwExtraInfo : int ) -> None : ctypes . windll . user32 . mouse_event ( dwFlags , dx , dy , dwData , dwExtraInfo )
12239	def beale ( theta ) : x , y = theta A = 1.5 - x + x * y B = 2.25 - x + x * y ** 2 C = 2.625 - x + x * y ** 3 obj = A ** 2 + B ** 2 + C ** 2 grad = np . array ( [ 2 * A * ( y - 1 ) + 2 * B * ( y ** 2 - 1 ) + 2 * C * ( y ** 3 - 1 ) , 2 * A * x + 4 * B * x * y + 6 * C * x * y ** 2 ] ) return obj , grad
11792	def lcv ( var , assignment , csp ) : return sorted ( csp . choices ( var ) , key = lambda val : csp . nconflicts ( var , val , assignment ) )
7535	def muscle_chunker ( data , sample ) : ## log our location for debugging LOGGER . info ( "inside muscle_chunker" ) ## only chunk up denovo data, refdata has its own chunking method which ## makes equal size chunks, instead of uneven chunks like in denovo if data . paramsdict [ "assembly_method" ] != "reference" : ## get the number of clusters clustfile = os . path . join ( data . dirs . clusts , sample . name + ".clust.gz" ) with iter ( gzip . open ( clustfile , 'rb' ) ) as clustio : nloci = sum ( 1 for i in clustio if "//" in i ) // 2 #tclust = clustio.read().count("//")//2 optim = ( nloci // 20 ) + ( nloci % 20 ) LOGGER . info ( "optim for align chunks: %s" , optim ) ## write optim clusters to each tmp file clustio = gzip . open ( clustfile , 'rb' ) inclusts = iter ( clustio . read ( ) . strip ( ) . split ( "//\n//\n" ) ) ## splitting loci so first file is smaller and last file is bigger inc = optim // 10 for idx in range ( 10 ) : ## how big is this chunk? this = optim + ( idx * inc ) left = nloci - this if idx == 9 : ## grab everything left grabchunk = list ( itertools . islice ( inclusts , int ( 1e9 ) ) ) else : ## grab next chunks-worth of data grabchunk = list ( itertools . islice ( inclusts , this ) ) nloci = left ## write the chunk to file tmpfile = os . path . join ( data . tmpdir , sample . name + "_chunk_{}.ali" . format ( idx ) ) with open ( tmpfile , 'wb' ) as out : out . write ( "//\n//\n" . join ( grabchunk ) ) ## write the chunk to file #grabchunk = list(itertools.islice(inclusts, left)) #if grabchunk: # tmpfile = os.path.join(data.tmpdir, sample.name+"_chunk_9.ali") # with open(tmpfile, 'a') as out: # out.write("\n//\n//\n".join(grabchunk)) clustio . close ( )
13563	def repack ( self ) : items = self . grouped_filter ( ) . order_by ( 'rank' ) . select_for_update ( ) for count , item in enumerate ( items ) : item . rank = count + 1 item . save ( rerank = False )
10134	def add_item ( self , key , value , after = False , index = None , pos_key = None , replace = True ) : if self . _validate_fn : self . _validate_fn ( value ) if ( index is not None ) and ( pos_key is not None ) : raise ValueError ( 'Either specify index or pos_key, not both.' ) elif pos_key is not None : try : index = self . index ( pos_key ) except ValueError : raise KeyError ( '%r not found' % pos_key ) if after and ( index is not None ) : # insert inserts *before* index, so increment by one. index += 1 if key in self . _values : if not replace : raise KeyError ( '%r is duplicate' % key ) if index is not None : # We are re-locating. del self [ key ] else : # We are updating self . _values [ key ] = value return if index is not None : # Place at given position self . _order . insert ( index , key ) else : # Place at end self . _order . append ( key ) self . _values [ key ] = value
8929	def pylint ( ctx , skip_tests = False , skip_root = False , reports = False ) : cfg = config . load ( ) add_dir2pypath ( cfg . project_root ) if not os . path . exists ( cfg . testjoin ( '__init__.py' ) ) : add_dir2pypath ( cfg . testjoin ( ) ) namelist = set ( ) for package in cfg . project . get ( 'packages' , [ ] ) : if '.' not in package : namelist . add ( cfg . srcjoin ( package ) ) for module in cfg . project . get ( 'py_modules' , [ ] ) : namelist . add ( module + '.py' ) if not skip_tests : test_py = antglob . FileSet ( cfg . testdir , '**/*.py' ) test_py = [ cfg . testjoin ( i ) for i in test_py ] if test_py : namelist |= set ( test_py ) if not skip_root : root_py = antglob . FileSet ( '.' , '*.py' ) if root_py : namelist |= set ( root_py ) namelist = set ( [ i [ len ( os . getcwd ( ) ) + 1 : ] if i . startswith ( os . getcwd ( ) + os . sep ) else i for i in namelist ] ) cmd = 'pylint' cmd += ' "{}"' . format ( '" "' . join ( sorted ( namelist ) ) ) cmd += ' --reports={0}' . format ( 'y' if reports else 'n' ) for cfgfile in ( '.pylintrc' , 'pylint.rc' , 'pylint.cfg' , 'project.d/pylint.cfg' ) : if os . path . exists ( cfgfile ) : cmd += ' --rcfile={0}' . format ( cfgfile ) break try : shell . run ( cmd , report_error = False , runner = ctx . run ) notify . info ( "OK - No problems found by pylint." ) except exceptions . Failure as exc : # Check bit flags within pylint return code if exc . result . return_code & 32 : # Usage error (internal error in this code) notify . error ( "Usage error, bad arguments in {}?!" . format ( repr ( cmd ) ) ) raise else : bits = { 1 : "fatal" , 2 : "error" , 4 : "warning" , 8 : "refactor" , 16 : "convention" , } notify . warning ( "Some messages of type {} issued by pylint." . format ( ", " . join ( [ text for bit , text in bits . items ( ) if exc . result . return_code & bit ] ) ) ) if exc . result . return_code & 3 : notify . error ( "Exiting due to fatal / error message." ) raise
2155	def set_or_reset_runtime_param ( self , key , value ) : if self . _runtime . has_option ( 'general' , key ) : self . _runtime = self . _new_parser ( ) if value is None : return settings . _runtime . set ( 'general' , key . replace ( 'tower_' , '' ) , six . text_type ( value ) )
7813	def _decode_alt_names ( self , alt_names ) : for alt_name in alt_names : tname = alt_name . getName ( ) comp = alt_name . getComponent ( ) if tname == "dNSName" : key = "DNS" value = _decode_asn1_string ( comp ) elif tname == "uniformResourceIdentifier" : key = "URI" value = _decode_asn1_string ( comp ) elif tname == "otherName" : oid = comp . getComponentByName ( "type-id" ) value = comp . getComponentByName ( "value" ) if oid == XMPPADDR_OID : key = "XmppAddr" value = der_decoder . decode ( value , asn1Spec = UTF8String ( ) ) [ 0 ] value = _decode_asn1_string ( value ) elif oid == SRVNAME_OID : key = "SRVName" value = der_decoder . decode ( value , asn1Spec = IA5String ( ) ) [ 0 ] value = _decode_asn1_string ( value ) else : logger . debug ( "Unknown other name: {0}" . format ( oid ) ) continue else : logger . debug ( "Unsupported general name: {0}" . format ( tname ) ) continue self . alt_names [ key ] . append ( value )
8887	def fit ( self , x , y = None ) : x = iter2array ( x , dtype = ( MoleculeContainer , CGRContainer ) ) if self . __head_less : warn ( f'{self.__class__.__name__} configured to head less mode. fit unusable' ) return self self . _reset ( ) self . __prepare ( x ) return self
8687	def get ( self , key_name ) : result = self . db . search ( Query ( ) . name == key_name ) if not result : return { } return result [ 0 ]
1549	def set_logging_level ( cl_args ) : if 'verbose' in cl_args and cl_args [ 'verbose' ] : configure ( logging . DEBUG ) else : configure ( logging . INFO )
2604	def close ( self ) : if self . reuse : logger . debug ( "Ipcontroller not shutting down: reuse enabled" ) return if self . mode == "manual" : logger . debug ( "Ipcontroller not shutting down: Manual mode" ) return try : pgid = os . getpgid ( self . proc . pid ) os . killpg ( pgid , signal . SIGTERM ) time . sleep ( 0.2 ) os . killpg ( pgid , signal . SIGKILL ) try : self . proc . wait ( timeout = 1 ) x = self . proc . returncode if x == 0 : logger . debug ( "Controller exited with {0}" . format ( x ) ) else : logger . error ( "Controller exited with {0}. May require manual cleanup" . format ( x ) ) except subprocess . TimeoutExpired : logger . warn ( "Ipcontroller process:{0} cleanup failed. May require manual cleanup" . format ( self . proc . pid ) ) except Exception as e : logger . warn ( "Failed to kill the ipcontroller process[{0}]: {1}" . format ( self . proc . pid , e ) )
8063	def do_set ( self , line ) : try : name , value = [ part . strip ( ) for part in line . split ( '=' ) ] if name not in self . bot . _vars : self . print_response ( 'No such variable %s enter vars to see available vars' % name ) return variable = self . bot . _vars [ name ] variable . value = variable . sanitize ( value . strip ( ';' ) ) success , msg = self . bot . canvas . sink . var_changed ( name , variable . value ) if success : print ( '{}={}' . format ( name , variable . value ) , file = self . stdout ) else : print ( '{}\n' . format ( msg ) , file = self . stdout ) except Exception as e : print ( 'Invalid Syntax.' , e ) return
3590	def get_provider ( ) : global _provider # Set the provider based on the current platform. if _provider is None : if sys . platform . startswith ( 'linux' ) : # Linux platform from . bluez_dbus . provider import BluezProvider _provider = BluezProvider ( ) elif sys . platform == 'darwin' : # Mac OSX platform from . corebluetooth . provider import CoreBluetoothProvider _provider = CoreBluetoothProvider ( ) else : # Unsupported platform raise RuntimeError ( 'Sorry the {0} platform is not supported by the BLE library!' . format ( sys . platform ) ) return _provider
13470	def copy ( src , dst ) : ( szip , dzip ) = ( src . endswith ( ".zip" ) , dst . endswith ( ".zip" ) ) logging . info ( "Copy: %s => %s" % ( src , dst ) ) if szip and dzip : #If both zipped, we can simply use copy shutil . copy2 ( src , dst ) elif szip : with zipfile . ZipFile ( src , mode = 'r' ) as z : tmpdir = tempfile . mkdtemp ( ) try : z . extractall ( tmpdir ) if len ( z . namelist ( ) ) != 1 : raise RuntimeError ( "The zip file '%s' should only have one " "compressed file" % src ) tmpfile = join ( tmpdir , z . namelist ( ) [ 0 ] ) try : os . remove ( dst ) except OSError : pass shutil . move ( tmpfile , dst ) finally : shutil . rmtree ( tmpdir , ignore_errors = True ) elif dzip : with zipfile . ZipFile ( dst , mode = 'w' , compression = ZIP_DEFLATED ) as z : z . write ( src , arcname = basename ( src ) ) else : #None of them are zipped shutil . copy2 ( src , dst )
11983	async def upload_file ( self , bucket , file , uploadpath = None , key = None , ContentType = None , * * kw ) : is_filename = False if hasattr ( file , 'read' ) : if hasattr ( file , 'seek' ) : file . seek ( 0 ) file = file . read ( ) size = len ( file ) elif key : size = len ( file ) else : is_filename = True size = os . stat ( file ) . st_size key = os . path . basename ( file ) assert key , 'key not available' if not ContentType : ContentType , _ = mimetypes . guess_type ( key ) if uploadpath : if not uploadpath . endswith ( '/' ) : uploadpath = '%s/' % uploadpath key = '%s%s' % ( uploadpath , key ) params = dict ( Bucket = bucket , Key = key ) if not ContentType : ContentType = 'application/octet-stream' params [ 'ContentType' ] = ContentType if size > MULTI_PART_SIZE and is_filename : resp = await _multipart ( self , file , params ) elif is_filename : with open ( file , 'rb' ) as fp : params [ 'Body' ] = fp . read ( ) resp = await self . put_object ( * * params ) else : params [ 'Body' ] = file resp = await self . put_object ( * * params ) if 'Key' not in resp : resp [ 'Key' ] = key if 'Bucket' not in resp : resp [ 'Bucket' ] = bucket return resp
6666	def update_merge ( d , u ) : import collections for k , v in u . items ( ) : if isinstance ( v , collections . Mapping ) : r = update_merge ( d . get ( k , dict ( ) ) , v ) d [ k ] = r else : d [ k ] = u [ k ] return d
4102	def generate_gallery_rst ( app ) : try : plot_gallery = eval ( app . builder . config . plot_gallery ) except TypeError : plot_gallery = bool ( app . builder . config . plot_gallery ) gallery_conf . update ( app . config . sphinx_gallery_conf ) gallery_conf . update ( plot_gallery = plot_gallery ) gallery_conf . update ( abort_on_example_error = app . builder . config . abort_on_example_error ) # this assures I can call the config in other places app . config . sphinx_gallery_conf = gallery_conf app . config . html_static_path . append ( glr_path_static ( ) ) clean_gallery_out ( app . builder . outdir ) examples_dirs = gallery_conf [ 'examples_dirs' ] gallery_dirs = gallery_conf [ 'gallery_dirs' ] if not isinstance ( examples_dirs , list ) : examples_dirs = [ examples_dirs ] if not isinstance ( gallery_dirs , list ) : gallery_dirs = [ gallery_dirs ] mod_examples_dir = os . path . relpath ( gallery_conf [ 'mod_example_dir' ] , app . builder . srcdir ) seen_backrefs = set ( ) for examples_dir , gallery_dir in zip ( examples_dirs , gallery_dirs ) : examples_dir = os . path . relpath ( examples_dir , app . builder . srcdir ) gallery_dir = os . path . relpath ( gallery_dir , app . builder . srcdir ) for workdir in [ examples_dir , gallery_dir , mod_examples_dir ] : if not os . path . exists ( workdir ) : os . makedirs ( workdir ) # we create an index.rst with all examples fhindex = open ( os . path . join ( gallery_dir , 'index.rst' ) , 'w' ) # Here we don't use an os.walk, but we recurse only twice: flat is # better than nested. fhindex . write ( generate_dir_rst ( examples_dir , gallery_dir , gallery_conf , seen_backrefs ) ) for directory in sorted ( os . listdir ( examples_dir ) ) : if os . path . isdir ( os . path . join ( examples_dir , directory ) ) : src_dir = os . path . join ( examples_dir , directory ) target_dir = os . path . join ( gallery_dir , directory ) fhindex . write ( generate_dir_rst ( src_dir , target_dir , gallery_conf , seen_backrefs ) ) fhindex . flush ( )
5389	def _datetime_in_range ( self , dt , dt_min = None , dt_max = None ) : # The pipelines API stores operation create-time with second granularity. # We mimic this behavior in the local provider by truncating to seconds. dt = dt . replace ( microsecond = 0 ) if dt_min : dt_min = dt_min . replace ( microsecond = 0 ) else : dt_min = dsub_util . replace_timezone ( datetime . datetime . min , pytz . utc ) if dt_max : dt_max = dt_max . replace ( microsecond = 0 ) else : dt_max = dsub_util . replace_timezone ( datetime . datetime . max , pytz . utc ) return dt_min <= dt <= dt_max
10992	def _calc_ilm_order ( imshape ) : zorder = int ( imshape [ 0 ] / 6.25 ) + 1 l_npts = int ( imshape [ 1 ] / 42.5 ) + 1 npts = ( ) for a in range ( l_npts ) : if a < 5 : npts += ( int ( imshape [ 2 ] * [ 59 , 39 , 29 , 19 , 14 ] [ a ] / 512. ) + 1 , ) else : npts += ( int ( imshape [ 2 ] * 11 / 512. ) + 1 , ) return npts , zorder
5465	def _get_action_by_name ( op , name ) : actions = get_actions ( op ) for action in actions : if action . get ( 'name' ) == name : return action
6174	def reset_lock ( self ) : redis_key = self . CELERY_LOCK . format ( task_id = self . task_identifier ) self . celery_self . backend . client . delete ( redis_key )
9954	def custom_showwarning ( message , category , filename = "" , lineno = - 1 , file = None , line = None ) : if file is None : file = sys . stderr if file is None : # sys.stderr is None when run with pythonw.exe: # warnings get lost return text = "%s: %s\n" % ( category . __name__ , message ) try : file . write ( text ) except OSError : # the file (probably stderr) is invalid - this warning gets lost. pass
4497	def project ( self , project_id ) : type_ = self . guid ( project_id ) url = self . _build_url ( type_ , project_id ) if type_ in Project . _types : return Project ( self . _json ( self . _get ( url ) , 200 ) , self . session ) raise OSFException ( '{} is unrecognized type {}. Clone supports projects and registrations' . format ( project_id , type_ ) )
9416	def to_value ( cls , instance ) : if not isinstance ( instance , OctaveUserClass ) or not instance . _attrs : return dict ( ) # Bootstrap a MatlabObject from scipy.io # From https://github.com/scipy/scipy/blob/93a0ea9e5d4aba1f661b6bb0e18f9c2d1fce436a/scipy/io/matlab/mio5.py#L435-L443 # and https://github.com/scipy/scipy/blob/93a0ea9e5d4aba1f661b6bb0e18f9c2d1fce436a/scipy/io/matlab/mio5_params.py#L224 dtype = [ ] values = [ ] for attr in instance . _attrs : dtype . append ( ( str ( attr ) , object ) ) values . append ( getattr ( instance , attr ) ) struct = np . array ( [ tuple ( values ) ] , dtype ) return MatlabObject ( struct , instance . _name )
12944	def save ( self , cascadeSave = True ) : saver = IndexedRedisSave ( self . __class__ ) return saver . save ( self , cascadeSave = cascadeSave )
12601	def _check_cols ( df , col_names ) : for col in col_names : if not hasattr ( df , col ) : raise AttributeError ( "DataFrame does not have a '{}' column, got {}." . format ( col , df . columns ) )
10918	def find_best_step ( err_vals ) : if np . all ( np . isnan ( err_vals ) ) : raise ValueError ( 'All err_vals are nans!' ) return np . nanargmin ( err_vals )
2485	def to_special_value ( self , value ) : if isinstance ( value , utils . NoAssert ) : return self . spdx_namespace . noassertion elif isinstance ( value , utils . SPDXNone ) : return self . spdx_namespace . none else : return Literal ( value )
5286	def post ( self , request , * args , * * kwargs ) : formset = self . construct_formset ( ) if formset . is_valid ( ) : return self . formset_valid ( formset ) else : return self . formset_invalid ( formset )
10036	def execute ( helper , config , args ) : environment_name = args . environment ( events , next_token ) = helper . describe_events ( environment_name , start_time = datetime . now ( ) . isoformat ( ) ) # swap C-Names for event in events : print ( ( "[" + event [ 'Severity' ] + "] " + event [ 'Message' ] ) )
12350	def get ( self , id ) : info = self . _get_droplet_info ( id ) return DropletActions ( self . api , self , * * info )
9996	def del_space ( self , name ) : if name not in self . spaces : raise ValueError ( "Space '%s' does not exist" % name ) if name in self . static_spaces : space = self . static_spaces [ name ] if space . is_derived : raise ValueError ( "%s has derived spaces" % repr ( space . interface ) ) else : self . static_spaces . del_item ( name ) self . model . spacegraph . remove_node ( space ) self . inherit ( ) self . model . spacegraph . update_subspaces ( self ) # TODO: Destroy space elif name in self . dynamic_spaces : # TODO: Destroy space self . dynamic_spaces . del_item ( name ) else : raise ValueError ( "Derived cells cannot be deleted" )
3488	def _parse_notes_dict ( sbase ) : notes = sbase . getNotesString ( ) if notes and len ( notes ) > 0 : pattern = r"<p>\s*(\w+\s*\w*)\s*:\s*([\w|\s]+)<" matches = re . findall ( pattern , notes ) d = { k . strip ( ) : v . strip ( ) for ( k , v ) in matches } return { k : v for k , v in d . items ( ) if len ( v ) > 0 } else : return { }
13464	def add_memory ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) form = MemoryForm ( request . POST or None , request . FILES or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . user = request . user instance . event = event instance . save ( ) msg = "Your thoughts were added. " if request . FILES : photo_list = request . FILES . getlist ( 'photos' ) photo_count = len ( photo_list ) for upload_file in photo_list : process_upload ( upload_file , instance , form , event , request ) if photo_count > 1 : msg += "{} images were added and should appear soon." . format ( photo_count ) else : msg += "{} image was added and should appear soon." . format ( photo_count ) messages . success ( request , msg ) return HttpResponseRedirect ( '../' ) return render ( request , 'happenings/add_memories.html' , { 'form' : form , 'event' : event } )
3849	async def fetch ( self , method , url , params = None , headers = None , data = None ) : logger . debug ( 'Sending request %s %s:\n%r' , method , url , data ) for retry_num in range ( MAX_RETRIES ) : try : async with self . fetch_raw ( method , url , params = params , headers = headers , data = data ) as res : async with async_timeout . timeout ( REQUEST_TIMEOUT ) : body = await res . read ( ) logger . debug ( 'Received response %d %s:\n%r' , res . status , res . reason , body ) except asyncio . TimeoutError : error_msg = 'Request timed out' except aiohttp . ServerDisconnectedError as err : error_msg = 'Server disconnected error: {}' . format ( err ) except ( aiohttp . ClientError , ValueError ) as err : error_msg = 'Request connection error: {}' . format ( err ) else : break logger . info ( 'Request attempt %d failed: %s' , retry_num , error_msg ) else : logger . info ( 'Request failed after %d attempts' , MAX_RETRIES ) raise exceptions . NetworkError ( error_msg ) if res . status != 200 : logger . info ( 'Request returned unexpected status: %d %s' , res . status , res . reason ) raise exceptions . NetworkError ( 'Request return unexpected status: {}: {}' . format ( res . status , res . reason ) ) return FetchResponse ( res . status , body )
13190	def json_doc_to_xml ( json_obj , lang = 'en' , custom_namespace = None ) : if 'meta' not in json_obj : raise Exception ( "This function requires a conforming Open511 JSON document with a 'meta' section." ) json_obj = dict ( json_obj ) meta = json_obj . pop ( 'meta' ) elem = get_base_open511_element ( lang = lang , version = meta . pop ( 'version' ) ) pagination = json_obj . pop ( 'pagination' , None ) json_struct_to_xml ( json_obj , elem , custom_namespace = custom_namespace ) if pagination : elem . append ( json_struct_to_xml ( pagination , 'pagination' , custom_namespace = custom_namespace ) ) json_struct_to_xml ( meta , elem ) return elem
5290	def construct_inlines ( self ) : inline_formsets = [ ] for inline_class in self . get_inlines ( ) : inline_instance = inline_class ( self . model , self . request , self . object , self . kwargs , self ) inline_formset = inline_instance . construct_formset ( ) inline_formsets . append ( inline_formset ) return inline_formsets
4857	def deprecated ( extra ) : def decorator ( func ) : """ Return a decorated function that emits a deprecation warning on use. """ @ wraps ( func ) def wrapper ( * args , * * kwargs ) : """ Wrap the function. """ message = 'You called the deprecated function `{function}`. {extra}' . format ( function = func . __name__ , extra = extra ) frame = inspect . currentframe ( ) . f_back warnings . warn_explicit ( message , category = DeprecationWarning , filename = inspect . getfile ( frame . f_code ) , lineno = frame . f_lineno ) return func ( * args , * * kwargs ) return wrapper return decorator
6591	def receive ( self ) : ret = [ ] # a list of (pkgid, result) while True : if self . runid_pkgidx_map : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret . extend ( self . _collect_all_finished_pkgidx_result_pairs ( ) ) if not self . runid_pkgidx_map : break time . sleep ( self . sleep ) ret = sorted ( ret , key = itemgetter ( 0 ) ) return ret
10081	def publish ( self , pid = None , id_ = None ) : pid = pid or self . pid if not pid . is_registered ( ) : raise PIDInvalidAction ( ) self [ '_deposit' ] [ 'status' ] = 'published' if self [ '_deposit' ] . get ( 'pid' ) is None : # First publishing self . _publish_new ( id_ = id_ ) else : # Update after edit record = self . _publish_edited ( ) record . commit ( ) self . commit ( ) return self
8920	def _get_request_type ( self ) : value = self . document . tag . lower ( ) if value in allowed_request_types [ self . params [ 'service' ] ] : self . params [ "request" ] = value else : raise OWSInvalidParameterValue ( "Request type %s is not supported" % value , value = "request" ) return self . params [ "request" ]
7866	def set_item ( self , key , value , timeout = None , timeout_callback = None ) : with self . _lock : logger . debug ( "expdict.__setitem__({0!r}, {1!r}, {2!r}, {3!r})" . format ( key , value , timeout , timeout_callback ) ) if not timeout : timeout = self . _default_timeout self . _timeouts [ key ] = ( time . time ( ) + timeout , timeout_callback ) return dict . __setitem__ ( self , key , value )
5100	def _dict2dict ( adj_dict ) : item = adj_dict . popitem ( ) adj_dict [ item [ 0 ] ] = item [ 1 ] if not isinstance ( item [ 1 ] , dict ) : new_dict = { } for key , value in adj_dict . items ( ) : new_dict [ key ] = { v : { } for v in value } adj_dict = new_dict return adj_dict
839	def getClosest ( self , inputPattern , topKCategories = 3 ) : inferenceResult = numpy . zeros ( max ( self . _categoryList ) + 1 ) dist = self . _getDistances ( inputPattern ) sorted = dist . argsort ( ) validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) for j in sorted [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ j ] ] += 1.0 winner = inferenceResult . argmax ( ) topNCats = [ ] for i in range ( topKCategories ) : topNCats . append ( ( self . _categoryList [ sorted [ i ] ] , dist [ sorted [ i ] ] ) ) return winner , dist , topNCats
6114	def resized_scaled_array_from_array ( self , new_shape , new_centre_pixels = None , new_centre_arcsec = None ) : if new_centre_pixels is None and new_centre_arcsec is None : new_centre = ( - 1 , - 1 ) # In Numba, the input origin must be the same image type as the origin, thus we cannot # pass 'None' and instead use the tuple (-1, -1). elif new_centre_pixels is not None and new_centre_arcsec is None : new_centre = new_centre_pixels elif new_centre_pixels is None and new_centre_arcsec is not None : new_centre = self . arc_second_coordinates_to_pixel_coordinates ( arc_second_coordinates = new_centre_arcsec ) else : raise exc . DataException ( 'You have supplied two centres (pixels and arc-seconds) to the resize scaled' 'array function' ) return self . new_with_array ( array = array_util . resized_array_2d_from_array_2d_and_resized_shape ( array_2d = self , resized_shape = new_shape , origin = new_centre ) )
2562	def recv_task_request_from_workers ( self ) : info = MPI . Status ( ) comm . recv ( source = MPI . ANY_SOURCE , tag = TASK_REQUEST_TAG , status = info ) worker_rank = info . Get_source ( ) logger . info ( "Received task request from worker:{}" . format ( worker_rank ) ) return worker_rank
3972	def _composed_service_dict ( service_spec ) : compose_dict = service_spec . plain_dict ( ) _apply_env_overrides ( env_overrides_for_app_or_service ( service_spec . name ) , compose_dict ) compose_dict . setdefault ( 'volumes' , [ ] ) . append ( _get_cp_volume_mount ( service_spec . name ) ) compose_dict [ 'container_name' ] = "dusty_{}_1" . format ( service_spec . name ) return compose_dict
7142	def balance ( self , unlocked = False ) : return self . _backend . balances ( account = self . index ) [ 1 if unlocked else 0 ]
1140	def fill ( text , width = 70 , * * kwargs ) : w = TextWrapper ( width = width , * * kwargs ) return w . fill ( text )
1334	def backward ( self , gradient , image = None , strict = True ) : assert self . has_gradient ( ) assert gradient . ndim == 1 if image is None : image = self . __original_image assert not strict or self . in_bounds ( image ) self . _total_gradient_calls += 1 gradient = self . __model . backward ( gradient , image ) assert gradient . shape == image . shape return gradient
10217	def prerender ( graph : BELGraph ) -> Mapping [ str , Mapping [ str , Any ] ] : import bio2bel_hgnc from bio2bel_hgnc . models import HumanGene graph : BELGraph = graph . copy ( ) enrich_protein_and_rna_origins ( graph ) collapse_all_variants ( graph ) genes : Set [ Gene ] = get_nodes_by_function ( graph , GENE ) hgnc_symbols = { gene . name for gene in genes if gene . namespace . lower ( ) == 'hgnc' } result = { } hgnc_manager = bio2bel_hgnc . Manager ( ) human_genes = ( hgnc_manager . session . query ( HumanGene . symbol , HumanGene . location ) . filter ( HumanGene . symbol . in_ ( hgnc_symbols ) ) . all ( ) ) for human_gene in human_genes : result [ human_gene . symbol ] = { 'name' : human_gene . symbol , 'chr' : ( human_gene . location . split ( 'q' ) [ 0 ] if 'q' in human_gene . location else human_gene . location . split ( 'p' ) [ 0 ] ) , } df = get_df ( ) for _ , ( gene_id , symbol , start , stop ) in df [ df [ 'Symbol' ] . isin ( hgnc_symbols ) ] . iterrows ( ) : result [ symbol ] [ 'start' ] = start result [ symbol ] [ 'stop' ] = stop return result
3690	def solve_T ( self , P , V , quick = True ) : a , b = self . a , self . b if quick : x1 = - 1.j * 1.7320508075688772 + 1. x2 = V - b x3 = x2 / R x4 = V + b x5 = ( 1.7320508075688772 * ( x2 * x2 * ( - 4. * P * P * P * x3 + 27. * a * a / ( V * V * x4 * x4 ) ) / ( R * R ) ) ** 0.5 - 9. * a * x3 / ( V * x4 ) + 0j ) ** ( 1. / 3. ) return ( 3.3019272488946263 * ( 11.537996562459266 * P * x3 / ( x1 * x5 ) + 1.2599210498948732 * x1 * x5 ) ** 2 / 144.0 ) . real else : return ( ( - ( - 1 / 2 + sqrt ( 3 ) * 1j / 2 ) * ( sqrt ( 729 * ( - V * a + a * b ) ** 2 / ( R * V ** 2 + R * V * b ) ** 2 + 108 * ( - P * V + P * b ) ** 3 / R ** 3 ) / 2 + 27 * ( - V * a + a * b ) / ( 2 * ( R * V ** 2 + R * V * b ) ) + 0j ) ** ( 1 / 3 ) / 3 + ( - P * V + P * b ) / ( R * ( - 1 / 2 + sqrt ( 3 ) * 1j / 2 ) * ( sqrt ( 729 * ( - V * a + a * b ) ** 2 / ( R * V ** 2 + R * V * b ) ** 2 + 108 * ( - P * V + P * b ) ** 3 / R ** 3 ) / 2 + 27 * ( - V * a + a * b ) / ( 2 * ( R * V ** 2 + R * V * b ) ) + 0j ) ** ( 1 / 3 ) ) ) ** 2 ) . real
9528	def pbkdf2 ( password , salt , iterations , dklen = 0 , digest = None ) : if digest is None : digest = settings . CRYPTOGRAPHY_DIGEST if not dklen : dklen = digest . digest_size password = force_bytes ( password ) salt = force_bytes ( salt ) kdf = PBKDF2HMAC ( algorithm = digest , length = dklen , salt = salt , iterations = iterations , backend = settings . CRYPTOGRAPHY_BACKEND ) return kdf . derive ( password )
6923	def aovhm_theta ( times , mags , errs , frequency , nharmonics , magvariance ) : period = 1.0 / frequency ndet = times . size two_nharmonics = nharmonics + nharmonics # phase with test period phasedseries = phase_magseries_with_errs ( times , mags , errs , period , times [ 0 ] , sort = True , wrap = False ) # get the phased quantities phase = phasedseries [ 'phase' ] pmags = phasedseries [ 'mags' ] perrs = phasedseries [ 'errs' ] # this is sqrt(1.0/errs^2) -> the weights pweights = 1.0 / perrs # multiply by 2.0*PI (for omega*time) phase = phase * 2.0 * pi_value # this is the z complex vector z = npcos ( phase ) + 1.0j * npsin ( phase ) # multiply phase with N phase = nharmonics * phase # this is the psi complex vector psi = pmags * pweights * ( npcos ( phase ) + 1j * npsin ( phase ) ) # this is the initial value of z^n zn = 1.0 + 0.0j # this is the initial value of phi phi = pweights + 0.0j # initialize theta to zero theta_aov = 0.0 # go through all the harmonics now up to 2N for _ in range ( two_nharmonics ) : # this is <phi, phi> phi_dot_phi = npsum ( phi * phi . conjugate ( ) ) # this is the alpha_n numerator alpha = npsum ( pweights * z * phi ) # this is <phi, psi>. make sure to use npvdot and NOT npdot to get # complex conjugate of first vector as expected for complex vectors phi_dot_psi = npvdot ( phi , psi ) # make sure phi_dot_phi is not zero phi_dot_phi = npmax ( [ phi_dot_phi , 10.0e-9 ] ) # this is the expression for alpha_n alpha = alpha / phi_dot_phi # update theta_aov for this harmonic theta_aov = ( theta_aov + npabs ( phi_dot_psi ) * npabs ( phi_dot_psi ) / phi_dot_phi ) # use the recurrence relation to find the next phi phi = phi * z - alpha * zn * phi . conjugate ( ) # update z^n zn = zn * z # done with all harmonics, calculate the theta_aov for this freq # the max below makes sure that magvariance - theta_aov > zero theta_aov = ( ( ndet - two_nharmonics - 1.0 ) * theta_aov / ( two_nharmonics * npmax ( [ magvariance - theta_aov , 1.0e-9 ] ) ) ) return theta_aov
13363	def echo_via_pager ( text , color = None ) : color = resolve_color_default ( color ) if not isinstance ( text , string_types ) : text = text_type ( text ) from . _termui_impl import pager return pager ( text + '\n' , color )
12115	def fileModifiedTimestamp ( fname ) : modifiedTime = os . path . getmtime ( fname ) stamp = time . strftime ( '%Y-%m-%d' , time . localtime ( modifiedTime ) ) return stamp
11362	def convert_html_subscripts_to_latex ( text ) : text = re . sub ( "<sub>(.*?)</sub>" , r"$_{\1}$" , text ) text = re . sub ( "<sup>(.*?)</sup>" , r"$^{\1}$" , text ) return text
10049	def create_error_handlers ( blueprint ) : blueprint . errorhandler ( PIDInvalidAction ) ( create_api_errorhandler ( status = 403 , message = 'Invalid action' ) ) records_rest_error_handlers ( blueprint )
9787	def init ( project , polyaxonfile ) : user , project_name = get_project_or_local ( project ) try : project_config = PolyaxonClient ( ) . project . get_project ( user , project_name ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Make sure you have a project with this name `{}`' . format ( project ) ) Printer . print_error ( 'You can a create new project with this command: ' 'polyaxon project create ' '--name={} [--description=...] [--tags=...]' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) init_project = False if ProjectManager . is_initialized ( ) : local_project = ProjectManager . get_config ( ) click . echo ( 'Warning! This project is already initialized with the following project:' ) with clint . textui . indent ( 4 ) : clint . textui . puts ( 'User: {}' . format ( local_project . user ) ) clint . textui . puts ( 'Project: {}' . format ( local_project . name ) ) if click . confirm ( 'Would you like to override this current config?' , default = False ) : init_project = True else : init_project = True if init_project : ProjectManager . purge ( ) ProjectManager . set_config ( project_config , init = True ) Printer . print_success ( 'Project was initialized' ) else : Printer . print_header ( 'Project config was not changed.' ) init_ignore = False if IgnoreManager . is_initialized ( ) : click . echo ( 'Warning! Found a .polyaxonignore file.' ) if click . confirm ( 'Would you like to override it?' , default = False ) : init_ignore = True else : init_ignore = True if init_ignore : IgnoreManager . init_config ( ) Printer . print_success ( 'New .polyaxonignore file was created.' ) else : Printer . print_header ( '.polyaxonignore file was not changed.' ) if polyaxonfile : create_polyaxonfile ( )
7340	async def get_media_metadata ( data , path = None ) : if isinstance ( data , bytes ) : media_type = await get_type ( data , path ) else : raise TypeError ( "get_metadata input must be a bytes" ) media_category = get_category ( media_type ) _logger . info ( "media_type: %s, media_category: %s" % ( media_type , media_category ) ) return media_type , media_category
5342	def __get_dash_menu ( self , kibiter_major ) : # omenu = OrderedDict() omenu = [ ] # Start with Overview omenu . append ( self . menu_panels_common [ 'Overview' ] ) # Now the data _getsources ds_menu = self . __get_menu_entries ( kibiter_major ) # Remove the kafka and community menus, they will be included at the end kafka_menu = None community_menu = None found_kafka = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == KAFKA_NAME ] if found_kafka : kafka_menu = ds_menu . pop ( found_kafka [ 0 ] ) found_community = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == COMMUNITY_NAME ] if found_community : community_menu = ds_menu . pop ( found_community [ 0 ] ) ds_menu . sort ( key = operator . itemgetter ( 'name' ) ) omenu += ds_menu # If kafka and community are present add them before the Data Status and About if kafka_menu : omenu . append ( kafka_menu ) if community_menu : omenu . append ( community_menu ) # At the end Data Status, About omenu . append ( self . menu_panels_common [ 'Data Status' ] ) omenu . append ( self . menu_panels_common [ 'About' ] ) logger . debug ( "Menu for panels: %s" , json . dumps ( ds_menu , indent = 4 ) ) return omenu
12462	def prepare_args ( config , bootstrap ) : config = copy . deepcopy ( config ) environ = dict ( copy . deepcopy ( os . environ ) ) data = { 'env' : bootstrap [ 'env' ] , 'pip' : pip_cmd ( bootstrap [ 'env' ] , '' , return_path = True ) , 'requirements' : bootstrap [ 'requirements' ] } environ . update ( data ) if isinstance ( config , string_types ) : return config . format ( * * environ ) for key , value in iteritems ( config ) : if not isinstance ( value , string_types ) : continue config [ key ] = value . format ( * * environ ) return config_to_args ( config )
4489	def might_need_auth ( f ) : @ wraps ( f ) def wrapper ( cli_args ) : try : return_value = f ( cli_args ) except UnauthorizedException as e : config = config_from_env ( config_from_file ( ) ) username = _get_username ( cli_args , config ) if username is None : sys . exit ( "Please set a username (run `osf -h` for details)." ) else : sys . exit ( "You are not authorized to access this project." ) return return_value return wrapper
2763	def get_snapshot ( self , snapshot_id ) : return Snapshot . get_object ( api_token = self . token , snapshot_id = snapshot_id )
1773	def pop ( cpu , size ) : assert size in ( 16 , cpu . address_bit_size ) base , _ , _ = cpu . get_descriptor ( cpu . SS ) address = cpu . STACK + base value = cpu . read_int ( address , size ) cpu . STACK = cpu . STACK + size // 8 return value
7074	def variable_index_gridsearch_magbin ( simbasedir , stetson_stdev_range = ( 1.0 , 20.0 ) , inveta_stdev_range = ( 1.0 , 20.0 ) , iqr_stdev_range = ( 1.0 , 20.0 ) , ngridpoints = 32 , ngridworkers = None ) : # make the output directory where all the pkls from the variability # threshold runs will go outdir = os . path . join ( simbasedir , 'recvar-threshold-pkls' ) if not os . path . exists ( outdir ) : os . mkdir ( outdir ) # get the info from the simbasedir with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) # get the column defs for the fakelcs timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] # get the magbinmedians to use for the recovery processing magbinmedians = siminfo [ 'magrms' ] [ magcols [ 0 ] ] [ 'binned_sdssr_median' ] # generate the grids for stetson and inveta stetson_grid = np . linspace ( stetson_stdev_range [ 0 ] , stetson_stdev_range [ 1 ] , num = ngridpoints ) inveta_grid = np . linspace ( inveta_stdev_range [ 0 ] , inveta_stdev_range [ 1 ] , num = ngridpoints ) iqr_grid = np . linspace ( iqr_stdev_range [ 0 ] , iqr_stdev_range [ 1 ] , num = ngridpoints ) # generate the grid stet_inveta_iqr_grid = [ ] for stet in stetson_grid : for inveta in inveta_grid : for iqr in iqr_grid : grid_point = [ stet , inveta , iqr ] stet_inveta_iqr_grid . append ( grid_point ) # the output dict grid_results = { 'stetson_grid' : stetson_grid , 'inveta_grid' : inveta_grid , 'iqr_grid' : iqr_grid , 'stet_inveta_iqr_grid' : stet_inveta_iqr_grid , 'magbinmedians' : magbinmedians , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'simbasedir' : os . path . abspath ( simbasedir ) , 'recovery' : [ ] } # set up the pool pool = mp . Pool ( ngridworkers ) # run the grid search per magbinmedian for magbinmedian in magbinmedians : LOGINFO ( 'running stetson J-inveta grid-search ' 'for magbinmedian = %.3f...' % magbinmedian ) tasks = [ ( simbasedir , gp , magbinmedian ) for gp in stet_inveta_iqr_grid ] thisbin_results = pool . map ( magbin_varind_gridsearch_worker , tasks ) grid_results [ 'recovery' ] . append ( thisbin_results ) pool . close ( ) pool . join ( ) LOGINFO ( 'done.' ) with open ( os . path . join ( simbasedir , 'fakevar-recovery-per-magbin.pkl' ) , 'wb' ) as outfd : pickle . dump ( grid_results , outfd , pickle . HIGHEST_PROTOCOL ) return grid_results
27	def nn ( input , layers_sizes , reuse = None , flatten = False , name = "" ) : for i , size in enumerate ( layers_sizes ) : activation = tf . nn . relu if i < len ( layers_sizes ) - 1 else None input = tf . layers . dense ( inputs = input , units = size , kernel_initializer = tf . contrib . layers . xavier_initializer ( ) , reuse = reuse , name = name + '_' + str ( i ) ) if activation : input = activation ( input ) if flatten : assert layers_sizes [ - 1 ] == 1 input = tf . reshape ( input , [ - 1 ] ) return input
4453	def alias ( self , alias ) : if alias is FIELDNAME : if not self . _field : raise ValueError ( "Cannot use FIELDNAME alias with no field" ) # Chop off initial '@' alias = self . _field [ 1 : ] self . _alias = alias return self
6479	def _normalised_python ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) for x , point in enumerate ( self . points ) : y = ( point - self . minimum ) * 4.0 / self . extents * self . size . y yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
3079	def get ( http , path , root = METADATA_ROOT , recursive = None ) : url = urlparse . urljoin ( root , path ) url = _helpers . _add_query_parameter ( url , 'recursive' , recursive ) response , content = transport . request ( http , url , headers = METADATA_HEADERS ) if response . status == http_client . OK : decoded = _helpers . _from_bytes ( content ) if response [ 'content-type' ] == 'application/json' : return json . loads ( decoded ) else : return decoded else : raise http_client . HTTPException ( 'Failed to retrieve {0} from the Google Compute Engine' 'metadata service. Response:\n{1}' . format ( url , response ) )
13083	def chunk ( self , text , reffs ) : if str ( text . id ) in self . chunker : return self . chunker [ str ( text . id ) ] ( text , reffs ) return self . chunker [ "default" ] ( text , reffs )
12645	def set_aad_cache ( token , cache ) : set_config_value ( 'aad_token' , jsonpickle . encode ( token ) ) set_config_value ( 'aad_cache' , jsonpickle . encode ( cache ) )
4197	def HERMTOEP ( T0 , T , Z ) : assert len ( T ) > 0 M = len ( T ) X = numpy . zeros ( M + 1 , dtype = complex ) A = numpy . zeros ( M , dtype = complex ) P = T0 if P == 0 : raise ValueError ( "P must be different from zero" ) X [ 0 ] = Z [ 0 ] / T0 for k in range ( 0 , M ) : save = T [ k ] beta = X [ 0 ] * T [ k ] if k == 0 : temp = - save / P else : for j in range ( 0 , k ) : save = save + A [ j ] * T [ k - j - 1 ] beta = beta + X [ j + 1 ] * T [ k - j - 1 ] temp = - save / P P = P * ( 1. - ( temp . real ** 2 + temp . imag ** 2 ) ) if P <= 0 : raise ValueError ( "singular matrix" ) A [ k ] = temp alpha = ( Z [ k + 1 ] - beta ) / P if k == 0 : #print 'skipping code for k=0' X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * A [ k - j ] . conjugate ( ) continue khalf = ( k + 1 ) // 2 for j in range ( 0 , khalf ) : kj = k - j - 1 save = A [ j ] A [ j ] = save + temp * A [ kj ] . conjugate ( ) if j != kj : A [ kj ] = A [ kj ] + temp * save . conjugate ( ) X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * A [ k - j ] . conjugate ( ) return X
9264	def filter_merged_pull_requests ( self , pull_requests ) : if self . options . verbose : print ( "Fetching merge date for pull requests..." ) closed_pull_requests = self . fetcher . fetch_closed_pull_requests ( ) if not pull_requests : return [ ] pulls = copy . deepcopy ( pull_requests ) for pr in pulls : fetched_pr = None for fpr in closed_pull_requests : if fpr [ 'number' ] == pr [ 'number' ] : fetched_pr = fpr if fetched_pr : pr [ 'merged_at' ] = fetched_pr [ 'merged_at' ] closed_pull_requests . remove ( fetched_pr ) for pr in pulls : if not pr . get ( 'merged_at' ) : pulls . remove ( pr ) return pulls
4313	def silent ( input_filepath , threshold = 0.001 ) : validate_input_file ( input_filepath ) stat_dictionary = stat ( input_filepath ) mean_norm = stat_dictionary [ 'Mean norm' ] if mean_norm is not float ( 'nan' ) : if mean_norm >= threshold : return False else : return True else : return True
7429	def draw ( self , axes ) : ## create a toytree object from the treemix tree result tre = toytree . tree ( newick = self . results . tree ) tre . draw ( axes = axes , use_edge_lengths = True , tree_style = 'c' , tip_labels_align = True , edge_align_style = { "stroke-width" : 1 } ) ## get coords for admix in self . results . admixture : ## parse admix event pidx , pdist , cidx , cdist , weight = admix a = _get_admix_point ( tre , pidx , pdist ) b = _get_admix_point ( tre , cidx , cdist ) ## add line for admixture edge mark = axes . plot ( a = ( a [ 0 ] , b [ 0 ] ) , b = ( a [ 1 ] , b [ 1 ] ) , style = { "stroke-width" : 10 * weight , "stroke-opacity" : 0.95 , "stroke-linecap" : "round" } ) ## add points at admixture sink axes . scatterplot ( a = ( b [ 0 ] ) , b = ( b [ 1 ] ) , size = 8 , title = "weight: {}" . format ( weight ) , ) ## add scale bar for edge lengths axes . y . show = False axes . x . ticks . show = True axes . x . label . text = "Drift parameter" return axes
6102	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . multiply ( self . intensity_prime , np . power ( np . add ( 1 , np . power ( np . divide ( self . radius_break , grid_radii ) , self . alpha ) ) , ( self . gamma / self . alpha ) ) ) , np . exp ( np . multiply ( - self . sersic_constant , ( np . power ( np . divide ( np . add ( np . power ( grid_radii , self . alpha ) , ( self . radius_break ** self . alpha ) ) , ( self . effective_radius ** self . alpha ) ) , ( 1.0 / ( self . alpha * self . sersic_index ) ) ) ) ) ) )
3236	def list_objects_in_bucket ( * * kwargs ) : bucket = get_bucket ( * * kwargs ) if bucket : return [ o for o in bucket . list_blobs ( ) ] else : return None
10860	def param_particle ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' , 'a' ] ]
10771	def contour ( self , level ) : if not isinstance ( level , numbers . Number ) : raise TypeError ( ( "'_level' must be of type 'numbers.Number' but is " "'{:s}'" ) . format ( type ( level ) ) ) vertices = self . _contour_generator . create_contour ( level ) return self . formatter ( level , vertices )
2363	def _pre_install ( ) : # Generate the parsetab.dat file at setup time dat = join ( setup_dir , 'src' , 'hcl' , 'parsetab.dat' ) if exists ( dat ) : os . unlink ( dat ) sys . path . insert ( 0 , join ( setup_dir , 'src' ) ) import hcl from hcl . parser import HclParser parser = HclParser ( )
5561	def init_bounds ( self ) : if self . _raw [ "init_bounds" ] is None : return self . bounds else : return Bounds ( * _validate_bounds ( self . _raw [ "init_bounds" ] ) )
13810	def MakeDescriptor ( desc_proto , package = '' , build_file_if_cpp = True , syntax = None ) : if api_implementation . Type ( ) == 'cpp' and build_file_if_cpp : # The C++ implementation requires all descriptors to be backed by the same # definition in the C++ descriptor pool. To do this, we build a # FileDescriptorProto with the same definition as this descriptor and build # it into the pool. from typy . google . protobuf import descriptor_pb2 file_descriptor_proto = descriptor_pb2 . FileDescriptorProto ( ) file_descriptor_proto . message_type . add ( ) . MergeFrom ( desc_proto ) # Generate a random name for this proto file to prevent conflicts with any # imported ones. We need to specify a file name so the descriptor pool # accepts our FileDescriptorProto, but it is not important what that file # name is actually set to. proto_name = str ( uuid . uuid4 ( ) ) if package : file_descriptor_proto . name = os . path . join ( package . replace ( '.' , '/' ) , proto_name + '.proto' ) file_descriptor_proto . package = package else : file_descriptor_proto . name = proto_name + '.proto' _message . default_pool . Add ( file_descriptor_proto ) result = _message . default_pool . FindFileByName ( file_descriptor_proto . name ) if _USE_C_DESCRIPTORS : return result . message_types_by_name [ desc_proto . name ] full_message_name = [ desc_proto . name ] if package : full_message_name . insert ( 0 , package ) # Create Descriptors for enum types enum_types = { } for enum_proto in desc_proto . enum_type : full_name = '.' . join ( full_message_name + [ enum_proto . name ] ) enum_desc = EnumDescriptor ( enum_proto . name , full_name , None , [ EnumValueDescriptor ( enum_val . name , ii , enum_val . number ) for ii , enum_val in enumerate ( enum_proto . value ) ] ) enum_types [ full_name ] = enum_desc # Create Descriptors for nested types nested_types = { } for nested_proto in desc_proto . nested_type : full_name = '.' . join ( full_message_name + [ nested_proto . name ] ) # Nested types are just those defined inside of the message, not all types # used by fields in the message, so no loops are possible here. nested_desc = MakeDescriptor ( nested_proto , package = '.' . join ( full_message_name ) , build_file_if_cpp = False , syntax = syntax ) nested_types [ full_name ] = nested_desc fields = [ ] for field_proto in desc_proto . field : full_name = '.' . join ( full_message_name + [ field_proto . name ] ) enum_desc = None nested_desc = None if field_proto . HasField ( 'type_name' ) : type_name = field_proto . type_name full_type_name = '.' . join ( full_message_name + [ type_name [ type_name . rfind ( '.' ) + 1 : ] ] ) if full_type_name in nested_types : nested_desc = nested_types [ full_type_name ] elif full_type_name in enum_types : enum_desc = enum_types [ full_type_name ] # Else type_name references a non-local type, which isn't implemented field = FieldDescriptor ( field_proto . name , full_name , field_proto . number - 1 , field_proto . number , field_proto . type , FieldDescriptor . ProtoTypeToCppProtoType ( field_proto . type ) , field_proto . label , None , nested_desc , enum_desc , None , False , None , options = field_proto . options , has_default_value = False ) fields . append ( field ) desc_name = '.' . join ( full_message_name ) return Descriptor ( desc_proto . name , desc_name , None , None , fields , list ( nested_types . values ( ) ) , list ( enum_types . values ( ) ) , [ ] , options = desc_proto . options )
9758	def restart ( ctx , copy , file , u ) : # pylint:disable=redefined-builtin config = None update_code = None if file : config = rhea . read ( file ) # Check if we need to upload if u : ctx . invoke ( upload , sync = False ) update_code = True user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) try : if copy : response = PolyaxonClient ( ) . experiment . copy ( user , project_name , _experiment , config = config , update_code = update_code ) Printer . print_success ( 'Experiment was copied with id {}' . format ( response . id ) ) else : response = PolyaxonClient ( ) . experiment . restart ( user , project_name , _experiment , config = config , update_code = update_code ) Printer . print_success ( 'Experiment was restarted with id {}' . format ( response . id ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not restart experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
1098	def get_close_matches ( word , possibilities , n = 3 , cutoff = 0.6 ) : if not n > 0 : raise ValueError ( "n must be > 0: %r" % ( n , ) ) if not 0.0 <= cutoff <= 1.0 : raise ValueError ( "cutoff must be in [0.0, 1.0]: %r" % ( cutoff , ) ) result = [ ] s = SequenceMatcher ( ) s . set_seq2 ( word ) for x in possibilities : s . set_seq1 ( x ) if s . real_quick_ratio ( ) >= cutoff and s . quick_ratio ( ) >= cutoff and s . ratio ( ) >= cutoff : result . append ( ( s . ratio ( ) , x ) ) # Move the best scorers to head of list result = heapq . nlargest ( n , result ) # Strip scores for the best n matches return [ x for score , x in result ]
9173	def _formatter_callback_factory ( ) : # pragma: no cover includes = [ ] exercise_url_template = '{baseUrl}/api/exercises?q={field}:"{{itemCode}}"' settings = get_current_registry ( ) . settings exercise_base_url = settings . get ( 'embeddables.exercise.base_url' , None ) exercise_matches = [ match . split ( ',' , 1 ) for match in aslist ( settings . get ( 'embeddables.exercise.match' , '' ) , flatten = False ) ] exercise_token = settings . get ( 'embeddables.exercise.token' , None ) mathml_url = settings . get ( 'mathmlcloud.url' , None ) memcache_servers = settings . get ( 'memcache_servers' ) if memcache_servers : memcache_servers = memcache_servers . split ( ) else : memcache_servers = None if exercise_base_url and exercise_matches : mc_client = None if memcache_servers : mc_client = memcache . Client ( memcache_servers , debug = 0 ) for ( exercise_match , exercise_field ) in exercise_matches : template = exercise_url_template . format ( baseUrl = exercise_base_url , field = exercise_field ) includes . append ( exercise_callback_factory ( exercise_match , template , mc_client , exercise_token , mathml_url ) ) return includes
10226	def get_correlation_triangles ( graph : BELGraph ) -> SetOfNodeTriples : return { tuple ( sorted ( [ n , u , v ] , key = str ) ) for n in graph for u , v in itt . combinations ( graph [ n ] , 2 ) if graph . has_edge ( u , v ) }
4782	def is_close_to ( self , other , tolerance ) : self . _validate_close_to_args ( self . val , other , tolerance ) if self . val < ( other - tolerance ) or self . val > ( other + tolerance ) : if type ( self . val ) is datetime . datetime : tolerance_seconds = tolerance . days * 86400 + tolerance . seconds + tolerance . microseconds / 1000000 h , rem = divmod ( tolerance_seconds , 3600 ) m , s = divmod ( rem , 60 ) self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%d:%02d:%02d>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) , h , m , s ) ) else : self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%s>, but was not.' % ( self . val , other , tolerance ) ) return self
9707	def value_from_datadict ( self , * args , * * kwargs ) : value = super ( RichTextWidget , self ) . value_from_datadict ( * args , * * kwargs ) if value is not None : value = self . get_sanitizer ( ) ( value ) return value
12877	def many_until ( these , term ) : results = [ ] while True : stop , result = choice ( _tag ( True , term ) , _tag ( False , these ) ) if stop : return results , result else : results . append ( result )
6657	def calc_inbag ( n_samples , forest ) : if not forest . bootstrap : e_s = "Cannot calculate the inbag from a forest that has " e_s = " bootstrap=False" raise ValueError ( e_s ) n_trees = forest . n_estimators inbag = np . zeros ( ( n_samples , n_trees ) ) sample_idx = [ ] for t_idx in range ( n_trees ) : sample_idx . append ( _generate_sample_indices ( forest . estimators_ [ t_idx ] . random_state , n_samples ) ) inbag [ : , t_idx ] = np . bincount ( sample_idx [ - 1 ] , minlength = n_samples ) return inbag
6700	def apt_key_exists ( keyid ) : # Command extracted from apt-key source gpg_cmd = 'gpg --ignore-time-conflict --no-options --no-default-keyring --keyring /etc/apt/trusted.gpg' with settings ( hide ( 'everything' ) , warn_only = True ) : res = run ( '%(gpg_cmd)s --fingerprint %(keyid)s' % locals ( ) ) return res . succeeded
11952	def execute ( varsfile , templatefile , outputfile = None , configfile = None , dryrun = False , build = False , push = False , verbose = False ) : if dryrun and ( build or push ) : jocker_lgr . error ( 'dryrun requested, cannot build.' ) sys . exit ( 100 ) _set_global_verbosity_level ( verbose ) j = Jocker ( varsfile , templatefile , outputfile , configfile , dryrun , build , push ) formatted_text = j . generate ( ) if dryrun : g = j . dryrun ( formatted_text ) if build or push : j . build_image ( ) if push : j . push_image ( ) if dryrun : return g
446	def batch_with_dynamic_pad ( images_and_captions , batch_size , queue_capacity , add_summaries = True ) : enqueue_list = [ ] for image , caption in images_and_captions : caption_length = tf . shape ( caption ) [ 0 ] input_length = tf . expand_dims ( tf . subtract ( caption_length , 1 ) , 0 ) input_seq = tf . slice ( caption , [ 0 ] , input_length ) target_seq = tf . slice ( caption , [ 1 ] , input_length ) indicator = tf . ones ( input_length , dtype = tf . int32 ) enqueue_list . append ( [ image , input_seq , target_seq , indicator ] ) images , input_seqs , target_seqs , mask = tf . train . batch_join ( enqueue_list , batch_size = batch_size , capacity = queue_capacity , dynamic_pad = True , name = "batch_and_pad" ) if add_summaries : lengths = tf . add ( tf . reduce_sum ( mask , 1 ) , 1 ) tf . summary . scalar ( "caption_length/batch_min" , tf . reduce_min ( lengths ) ) tf . summary . scalar ( "caption_length/batch_max" , tf . reduce_max ( lengths ) ) tf . summary . scalar ( "caption_length/batch_mean" , tf . reduce_mean ( lengths ) ) return images , input_seqs , target_seqs , mask
2320	def check_cuda_devices ( ) : import ctypes # Some constants taken from cuda.h CUDA_SUCCESS = 0 libnames = ( 'libcuda.so' , 'libcuda.dylib' , 'cuda.dll' ) for libname in libnames : try : cuda = ctypes . CDLL ( libname ) except OSError : continue else : break else : # raise OSError("could not load any of: " + ' '.join(libnames)) return 0 nGpus = ctypes . c_int ( ) error_str = ctypes . c_char_p ( ) result = cuda . cuInit ( 0 ) if result != CUDA_SUCCESS : cuda . cuGetErrorString ( result , ctypes . byref ( error_str ) ) # print("cuInit failed with error code %d: %s" % (result, error_str.value.decode())) return 0 result = cuda . cuDeviceGetCount ( ctypes . byref ( nGpus ) ) if result != CUDA_SUCCESS : cuda . cuGetErrorString ( result , ctypes . byref ( error_str ) ) # print("cuDeviceGetCount failed with error code %d: %s" % (result, error_str.value.decode())) return 0 # print("Found %d device(s)." % nGpus.value) return nGpus . value
12440	def dispatch ( self , request , response ) : # Assert authentication and attempt to get a valid user object. self . require_authentication ( request ) # Assert accessibiltiy of the resource in question. self . require_accessibility ( request . user , request . method ) # Facilitate CORS by applying various headers. # This must be done on every request. # TODO: Provide cross_domain configuration that turns this off. self . _process_cross_domain_request ( request , response ) # Route the HTTP/1.1 request to an appropriate method. return self . route ( request , response )
6520	def files ( self , filters = None ) : filters = compile_masks ( filters or [ r'.*' ] ) for files in itervalues ( self . _found ) : for file_ in files : relpath = text_type ( Path ( file_ ) . relative_to ( self . base_path ) ) if matches_masks ( relpath , filters ) : yield file_
11318	def update_isbn ( self ) : isbns = record_get_field_instances ( self . record , '020' ) for field in isbns : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , value . replace ( "-" , "" ) . strip ( ) )
10648	def remove_component ( self , name ) : component_to_remove = None for c in self . components : if c . name == name : component_to_remove = c if component_to_remove is not None : self . components . remove ( component_to_remove )
12518	def _get_node_names ( h5file , h5path = '/' , node_type = h5py . Dataset ) : if isinstance ( h5file , str ) : _h5file = get_h5file ( h5file , mode = 'r' ) else : _h5file = h5file if not h5path . startswith ( '/' ) : h5path = '/' + h5path names = [ ] try : h5group = _h5file . require_group ( h5path ) for node in _hdf5_walk ( h5group , node_type = node_type ) : names . append ( node . name ) except : raise RuntimeError ( 'Error getting node names from {}/{}.' . format ( _h5file . filename , h5path ) ) finally : if isinstance ( h5file , str ) : _h5file . close ( ) return names
5895	def formfield ( self , * * kwargs ) : defaults = { 'form_class' : RichTextFormField , 'config' : self . config , } defaults . update ( kwargs ) return super ( RichTextField , self ) . formfield ( * * defaults )
8709	def write_lines ( self , data ) : lines = data . replace ( '\r' , '' ) . split ( '\n' ) for line in lines : self . __exchange ( line )
11059	def run ( self , start = True ) : # Fail out if setup wasn't run if not self . is_setup : raise NotSetupError # Start the web server self . webserver . start ( ) first_connect = True try : while self . runnable : if self . reconnect_needed : if not self . sc . rtm_connect ( with_team_state = start ) : return False self . reconnect_needed = False if first_connect : first_connect = False self . plugins . connect ( ) # Get all waiting events - this always returns a list try : events = self . sc . rtm_read ( ) except AttributeError : self . log . exception ( 'Something has failed in the slack rtm library. This is fatal.' ) self . runnable = False events = [ ] except : self . log . exception ( 'Unhandled exception in rtm_read()' ) self . reconnect_needed = True events = [ ] for e in events : try : self . _handle_event ( e ) except KeyboardInterrupt : # Gracefully shutdown self . runnable = False except : self . log . exception ( 'Unhandled exception in event handler' ) sleep ( 0.1 ) except KeyboardInterrupt : # On ctrl-c, just exit pass except : self . log . exception ( 'Unhandled exception' )
11233	def get_inner_template ( self , language , template_type , indentation , key , val ) : #Language specific inner templates inner_templates = { 'php' : { 'iterable' : '%s%s => array \n%s( \n%s%s),\n' % ( indentation , key , indentation , val , indentation ) , 'singular' : '%s%s => %s, \n' % ( indentation , key , val ) } , 'javascript' : { 'iterable' : '%s%s : {\n%s\n%s},\n' % ( indentation , key , val , indentation ) , 'singular' : '%s%s: %s,\n' % ( indentation , key , val ) } , 'ocaml' : { 'iterable' : '%s[| (%s, (\n%s\n%s))|] ;;\n' % ( indentation , key , val , indentation ) , 'singular' : '%s(%s, %s);\n' % ( indentation , key , val ) } } return inner_templates [ language ] [ template_type ]
8178	def can_reach ( self , node , traversable = lambda node , edge : True ) : if isinstance ( node , str ) : node = self . graph [ node ] for n in self . graph . nodes : n . _visited = False return proximity . depth_first_search ( self , visit = lambda n : node == n , traversable = traversable )
6005	def generate_poisson_noise ( image , exposure_time_map , seed = - 1 ) : setup_random_seed ( seed ) image_counts = np . multiply ( image , exposure_time_map ) return image - np . divide ( np . random . poisson ( image_counts , image . shape ) , exposure_time_map )
8226	def _makeInstance ( self , clazz , args , kwargs ) : inst = clazz ( self , * args , * * kwargs ) return inst
3254	def delete_granule ( self , coverage , store , granule_id , workspace = None ) : params = dict ( ) workspace_name = workspace if isinstance ( store , basestring ) : store_name = store else : store_name = store . name workspace_name = store . workspace . name if workspace_name is None : raise ValueError ( "Must specify workspace" ) url = build_url ( self . service_url , [ "workspaces" , workspace_name , "coveragestores" , store_name , "coverages" , coverage , "index/granules" , granule_id , ".json" ] , params ) # DELETE /workspaces/<ws>/coveragestores/<name>/coverages/<coverage>/index/granules/<granule_id>.json headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , method = 'delete' , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to delete granule from mosaic {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) # maybe return a list of all granules? return None
8608	def remove_group_user ( self , group_id , user_id ) : response = self . _perform_request ( url = '/um/groups/%s/users/%s' % ( group_id , user_id ) , method = 'DELETE' ) return response
9916	def validate_is_primary ( self , is_primary ) : # TODO: Setting 'is_primary' to 'False' should probably not be # allowed. if is_primary and not ( self . instance and self . instance . is_verified ) : raise serializers . ValidationError ( _ ( "Unverified email addresses may not be used as the " "primary address." ) ) return is_primary
2675	def build ( src , requirements = None , local_package = None , config_file = 'config.yaml' , profile_name = None , ) : # Load and parse the config file. path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) # Get the absolute path to the output directory and create it if it doesn't # already exist. dist_directory = cfg . get ( 'dist_directory' , 'dist' ) path_to_dist = os . path . join ( src , dist_directory ) mkdir ( path_to_dist ) # Combine the name of the Lambda function with the current timestamp to use # for the output filename. function_name = cfg . get ( 'function_name' ) output_filename = '{0}-{1}.zip' . format ( timestamp ( ) , function_name ) path_to_temp = mkdtemp ( prefix = 'aws-lambda' ) pip_install_to_target ( path_to_temp , requirements = requirements , local_package = local_package , ) # Hack for Zope. if 'zope' in os . listdir ( path_to_temp ) : print ( 'Zope packages detected; fixing Zope package paths to ' 'make them importable.' , ) # Touch. with open ( os . path . join ( path_to_temp , 'zope/__init__.py' ) , 'wb' ) : pass # Gracefully handle whether ".zip" was included in the filename or not. output_filename = ( '{0}.zip' . format ( output_filename ) if not output_filename . endswith ( '.zip' ) else output_filename ) # Allow definition of source code directories we want to build into our # zipped package. build_config = defaultdict ( * * cfg . get ( 'build' , { } ) ) build_source_directories = build_config . get ( 'source_directories' , '' ) build_source_directories = ( build_source_directories if build_source_directories is not None else '' ) source_directories = [ d . strip ( ) for d in build_source_directories . split ( ',' ) ] files = [ ] for filename in os . listdir ( src ) : if os . path . isfile ( filename ) : if filename == '.DS_Store' : continue if filename == config_file : continue print ( 'Bundling: %r' % filename ) files . append ( os . path . join ( src , filename ) ) elif os . path . isdir ( filename ) and filename in source_directories : print ( 'Bundling directory: %r' % filename ) files . append ( os . path . join ( src , filename ) ) # "cd" into `temp_path` directory. os . chdir ( path_to_temp ) for f in files : if os . path . isfile ( f ) : _ , filename = os . path . split ( f ) # Copy handler file into root of the packages folder. copyfile ( f , os . path . join ( path_to_temp , filename ) ) copystat ( f , os . path . join ( path_to_temp , filename ) ) elif os . path . isdir ( f ) : destination_folder = os . path . join ( path_to_temp , f [ len ( src ) + 1 : ] ) copytree ( f , destination_folder ) # Zip them together into a single file. # TODO: Delete temp directory created once the archive has been compiled. path_to_zip_file = archive ( './' , path_to_dist , output_filename ) return path_to_zip_file
2	def conv_only ( convs = [ ( 32 , 8 , 4 ) , ( 64 , 4 , 2 ) , ( 64 , 3 , 1 ) ] , * * conv_kwargs ) : def network_fn ( X ) : out = tf . cast ( X , tf . float32 ) / 255. with tf . variable_scope ( "convnet" ) : for num_outputs , kernel_size , stride in convs : out = layers . convolution2d ( out , num_outputs = num_outputs , kernel_size = kernel_size , stride = stride , activation_fn = tf . nn . relu , * * conv_kwargs ) return out return network_fn
3083	def _parse_state_value ( state , user ) : uri , token = state . rsplit ( ':' , 1 ) if xsrfutil . validate_token ( xsrf_secret_key ( ) , token , user . user_id ( ) , action_id = uri ) : return uri else : return None
3769	def none_and_length_check ( all_inputs , length = None ) : if not length : length = len ( all_inputs [ 0 ] ) for things in all_inputs : if None in things or len ( things ) != length : return False return True
7064	def sqs_delete_item ( queue_url , receipt_handle , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : client . delete_message ( QueueUrl = queue_url , ReceiptHandle = receipt_handle ) except Exception as e : LOGEXCEPTION ( 'could not delete message with receipt handle: ' '%s from queue: %s' % ( receipt_handle , queue_url ) ) if raiseonfail : raise
6534	def merge_list ( list1 , list2 ) : merged = list ( list1 ) for value in list2 : if value not in merged : merged . append ( value ) return merged
13175	def prev ( self , name = None ) : if self . parent is None or self . index is None : return None for idx in xrange ( self . index - 1 , - 1 , - 1 ) : if name is None or self . parent [ idx ] . tagname == name : return self . parent [ idx ]
2115	def status ( self , pk = None , detail = False , * * kwargs ) : # Obtain the most recent project update job = self . last_job_data ( pk , * * kwargs ) # In most cases, we probably only want to know the status of the job # and the amount of time elapsed. However, if we were asked for # verbose information, provide it. if detail : return job # Print just the information we need. return { 'elapsed' : job [ 'elapsed' ] , 'failed' : job [ 'failed' ] , 'status' : job [ 'status' ] , }
13580	def dmap ( fn , record ) : values = ( fn ( v ) for k , v in record . items ( ) ) return dict ( itertools . izip ( record , values ) )
5538	def _extract ( self , in_tile = None , in_data = None , out_tile = None ) : return self . config . output . extract_subset ( input_data_tiles = [ ( in_tile , in_data ) ] , out_tile = out_tile )
8093	def node_label ( s , node , alpha = 1.0 ) : if s . text : #s._ctx.lineheight(1) s . _ctx . font ( s . font ) s . _ctx . fontsize ( s . fontsize ) s . _ctx . nostroke ( ) s . _ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha ) # Cache an outlined label text and translate it. # This enhances the speed and avoids wiggling text. try : p = node . _textpath except : txt = node . label try : txt = unicode ( txt ) except : try : txt = txt . decode ( "utf-8" ) except : pass # Abbreviation. #root = node.graph.root #if txt != root and txt[-len(root):] == root: # txt = txt[:len(txt)-len(root)]+root[0]+"." dx , dy = 0 , 0 if s . align == 2 : #CENTER dx = - s . _ctx . textwidth ( txt , s . textwidth ) / 2 dy = s . _ctx . textheight ( txt ) / 2 node . _textpath = s . _ctx . textpath ( txt , dx , dy , width = s . textwidth ) p = node . _textpath if s . depth : try : __colors . shadow ( dx = 2 , dy = 4 , blur = 5 , alpha = 0.3 * alpha ) except : pass s . _ctx . push ( ) s . _ctx . translate ( node . x , node . y ) s . _ctx . scale ( alpha ) s . _ctx . drawpath ( p . copy ( ) ) s . _ctx . pop ( )
12956	def _rem_id_from_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _get_key_for_index ( indexedField , val ) , pk )
11492	def login_with_api_key ( self , email , api_key , application = 'Default' ) : parameters = dict ( ) parameters [ 'email' ] = BaseDriver . email = email # Cache email parameters [ 'apikey' ] = BaseDriver . apikey = api_key # Cache API key parameters [ 'appname' ] = application response = self . request ( 'midas.login' , parameters ) if 'token' in response : # normal case return response [ 'token' ] if 'mfa_token_id' : # case with multi-factor authentication return response [ 'mfa_token_id' ]
2709	def limit_sentences ( path , word_limit = 100 ) : word_count = 0 if isinstance ( path , str ) : path = json_iter ( path ) for meta in path : if not isinstance ( meta , SummarySent ) : p = SummarySent ( * * meta ) else : p = meta sent_text = p . text . strip ( ) . split ( " " ) sent_len = len ( sent_text ) if ( word_count + sent_len ) > word_limit : break else : word_count += sent_len yield sent_text , p . idx
11966	def _bin_to_dec ( ip , check = True ) : if check and not is_bin ( ip ) : raise ValueError ( '_bin_to_dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = str ( ip ) return int ( str ( ip ) , 2 )
12209	def get_cache_key ( user_or_username , size , prefix ) : if isinstance ( user_or_username , get_user_model ( ) ) : user_or_username = user_or_username . username return '%s_%s_%s' % ( prefix , user_or_username , size )
553	def readStateFromDB ( self ) : self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] # Init if no prior state yet if self . _priorStateJSON is None : swarms = dict ( ) # Fast Swarm, first and only sprint has one swarm for each field # in fixedFields if self . _hsObj . _fixedFields is not None : print self . _hsObj . _fixedFields encoderSet = [ ] for field in self . _hsObj . _fixedFields : if field == '_classifierInput' : continue encoderName = self . getEncoderKeyFromName ( field ) assert encoderName in self . _hsObj . _encoderNames , "The field '%s' " " specified in the fixedFields list is not present in this " " model." % ( field ) encoderSet . append ( encoderName ) encoderSet . sort ( ) swarms [ '.' . join ( encoderSet ) ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } # Temporal prediction search, first sprint has N swarms of 1 field each, # the predicted field may or may not be that one field. elif self . _hsObj . _searchType == HsSearchType . temporal : for encoderName in self . _hsObj . _encoderNames : swarms [ encoderName ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } # Classification prediction search, first sprint has N swarms of 1 field # each where this field can NOT be the predicted field. elif self . _hsObj . _searchType == HsSearchType . classification : for encoderName in self . _hsObj . _encoderNames : if encoderName == self . _hsObj . _predictedFieldEncoder : continue swarms [ encoderName ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } # Legacy temporal. This is either a model that uses reconstruction or # an older multi-step model that doesn't have a separate # 'classifierOnly' encoder for the predicted field. Here, the predicted # field must ALWAYS be present and the first sprint tries the predicted # field only elif self . _hsObj . _searchType == HsSearchType . legacyTemporal : swarms [ self . _hsObj . _predictedFieldEncoder ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } else : raise RuntimeError ( "Unsupported search type: %s" % ( self . _hsObj . _searchType ) ) # Initialize the state. self . _state = dict ( # The last time the state was updated by a worker. lastUpdateTime = time . time ( ) , # Set from within setSwarmState() if we detect that the sprint we just # completed did worse than a prior sprint. This stores the index of # the last good sprint. lastGoodSprint = None , # Set from within setSwarmState() if lastGoodSprint is True and all # sprints have completed. searchOver = False , # This is a summary of the active swarms - this information can also # be obtained from the swarms entry that follows, but is summarized here # for easier reference when viewing the state as presented by # log messages and prints of the hsState data structure (by # permutations_runner). activeSwarms = swarms . keys ( ) , # All the swarms that have been created so far. swarms = swarms , # All the sprints that have completed or are in progress. sprints = [ { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None } ] , # The list of encoders we have "blacklisted" because they # performed so poorly. blackListedEncoders = [ ] , ) # This will do nothing if the value of engWorkerState is not still None. self . _hsObj . _cjDAO . jobSetFieldIfEqual ( self . _hsObj . _jobID , 'engWorkerState' , json . dumps ( self . _state ) , None ) self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] assert ( self . _priorStateJSON is not None ) # Read state from the database self . _state = json . loads ( self . _priorStateJSON ) self . _dirty = False
2398	def encode_plus ( s ) : regex = r"\+" pat = re . compile ( regex ) return pat . sub ( "%2B" , s )
9345	def call ( self , args , axis = 0 , out = None , chunksize = 1024 * 1024 , * * kwargs ) : if self . altreduce is not None : ret = [ None ] else : if out is None : if self . outdtype is not None : dtype = self . outdtype else : try : dtype = numpy . result_type ( * [ args [ i ] for i in self . ins ] * 2 ) except : dtype = None out = sharedmem . empty ( numpy . broadcast ( * [ args [ i ] for i in self . ins ] * 2 ) . shape , dtype = dtype ) if axis != 0 : for i in self . ins : args [ i ] = numpy . rollaxis ( args [ i ] , axis ) out = numpy . rollaxis ( out , axis ) size = numpy . max ( [ len ( args [ i ] ) for i in self . ins ] ) with sharedmem . MapReduce ( ) as pool : def work ( i ) : sl = slice ( i , i + chunksize ) myargs = args [ : ] for j in self . ins : try : tmp = myargs [ j ] [ sl ] a , b , c = sl . indices ( len ( args [ j ] ) ) myargs [ j ] = tmp except Exception as e : print tmp print j , e pass if b == a : return None rt = self . ufunc ( * myargs , * * kwargs ) if self . altreduce is not None : return rt else : out [ sl ] = rt def reduce ( rt ) : if self . altreduce is None : return if ret [ 0 ] is None : ret [ 0 ] = rt elif rt is not None : ret [ 0 ] = self . altreduce ( ret [ 0 ] , rt ) pool . map ( work , range ( 0 , size , chunksize ) , reduce = reduce ) if self . altreduce is None : if axis != 0 : out = numpy . rollaxis ( out , 0 , axis + 1 ) return out else : return ret [ 0 ]
703	def _getStreamDef ( self , modelDescription ) : #-------------------------------------------------------------------------- # Generate the string containing the aggregation settings. aggregationPeriod = { 'days' : 0 , 'hours' : 0 , 'microseconds' : 0 , 'milliseconds' : 0 , 'minutes' : 0 , 'months' : 0 , 'seconds' : 0 , 'weeks' : 0 , 'years' : 0 , } # Honor any overrides provided in the stream definition aggFunctionsDict = { } if 'aggregation' in modelDescription [ 'streamDef' ] : for key in aggregationPeriod . keys ( ) : if key in modelDescription [ 'streamDef' ] [ 'aggregation' ] : aggregationPeriod [ key ] = modelDescription [ 'streamDef' ] [ 'aggregation' ] [ key ] if 'fields' in modelDescription [ 'streamDef' ] [ 'aggregation' ] : for ( fieldName , func ) in modelDescription [ 'streamDef' ] [ 'aggregation' ] [ 'fields' ] : aggFunctionsDict [ fieldName ] = str ( func ) # Do we have any aggregation at all? hasAggregation = False for v in aggregationPeriod . values ( ) : if v != 0 : hasAggregation = True break # Convert the aggFunctionsDict to a list aggFunctionList = aggFunctionsDict . items ( ) aggregationInfo = dict ( aggregationPeriod ) aggregationInfo [ 'fields' ] = aggFunctionList streamDef = copy . deepcopy ( modelDescription [ 'streamDef' ] ) streamDef [ 'aggregation' ] = copy . deepcopy ( aggregationInfo ) return streamDef
641	def dict ( cls ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) # Make a copy so we can update any current values obtained from environment # variables result = dict ( cls . _properties ) keys = os . environ . keys ( ) replaceKeys = filter ( lambda x : x . startswith ( cls . envPropPrefix ) , keys ) for envKey in replaceKeys : key = envKey [ len ( cls . envPropPrefix ) : ] key = key . replace ( '_' , '.' ) result [ key ] = os . environ [ envKey ] return result
10410	def finalize_canonical_averages ( number_of_nodes , ps , canonical_averages , alpha , ) : spanning_cluster = ( ( 'percolation_probability_mean' in canonical_averages . dtype . names ) and 'percolation_probability_m2' in canonical_averages . dtype . names ) # append values of p as an additional field ret = np . empty_like ( canonical_averages , dtype = finalized_canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) n = canonical_averages [ 'number_of_runs' ] sqrt_n = np . sqrt ( canonical_averages [ 'number_of_runs' ] ) ret [ 'number_of_runs' ] = n ret [ 'p' ] = ps ret [ 'alpha' ] = alpha def _transform ( original_key , final_key = None , normalize = False , transpose = False , ) : if final_key is None : final_key = original_key keys_mean = [ '{}_mean' . format ( key ) for key in [ original_key , final_key ] ] keys_std = [ '{}_m2' . format ( original_key ) , '{}_std' . format ( final_key ) , ] key_ci = '{}_ci' . format ( final_key ) # calculate sample mean ret [ keys_mean [ 1 ] ] = canonical_averages [ keys_mean [ 0 ] ] if normalize : ret [ keys_mean [ 1 ] ] /= number_of_nodes # calculate sample standard deviation array = canonical_averages [ keys_std [ 0 ] ] result = np . sqrt ( ( array . T if transpose else array ) / ( n - 1 ) ) ret [ keys_std [ 1 ] ] = ( result . T if transpose else result ) if normalize : ret [ keys_std [ 1 ] ] /= number_of_nodes # calculate standard normal confidence interval array = ret [ keys_std [ 1 ] ] scale = ( array . T if transpose else array ) / sqrt_n array = ret [ keys_mean [ 1 ] ] mean = ( array . T if transpose else array ) result = scipy . stats . t . interval ( 1 - alpha , df = n - 1 , loc = mean , scale = scale , ) ( ret [ key_ci ] [ ... , 0 ] , ret [ key_ci ] [ ... , 1 ] ) = ( [ my_array . T for my_array in result ] if transpose else result ) if spanning_cluster : _transform ( 'percolation_probability' ) _transform ( 'max_cluster_size' , 'percolation_strength' , normalize = True ) _transform ( 'moments' , normalize = True , transpose = True ) return ret
663	def getCentreAndSpreadOffsets ( spaceShape , spreadShape , stepSize = 1 ) : from nupic . math . cross import cross # ===================================================================== # Init data structures # What is the range on the X and Y offsets of the center points? shape = spaceShape # If the shape is (1,1), special case of just 1 center point if shape [ 0 ] == 1 and shape [ 1 ] == 1 : centerOffsets = [ ( 0 , 0 ) ] else : xMin = - 1 * ( shape [ 1 ] // 2 ) xMax = xMin + shape [ 1 ] - 1 xPositions = range ( stepSize * xMin , stepSize * xMax + 1 , stepSize ) yMin = - 1 * ( shape [ 0 ] // 2 ) yMax = yMin + shape [ 0 ] - 1 yPositions = range ( stepSize * yMin , stepSize * yMax + 1 , stepSize ) centerOffsets = list ( cross ( yPositions , xPositions ) ) numCenterOffsets = len ( centerOffsets ) print "centerOffsets:" , centerOffsets # What is the range on the X and Y offsets of the spread points? shape = spreadShape # If the shape is (1,1), special case of no spreading around each center # point if shape [ 0 ] == 1 and shape [ 1 ] == 1 : spreadOffsets = [ ( 0 , 0 ) ] else : xMin = - 1 * ( shape [ 1 ] // 2 ) xMax = xMin + shape [ 1 ] - 1 xPositions = range ( stepSize * xMin , stepSize * xMax + 1 , stepSize ) yMin = - 1 * ( shape [ 0 ] // 2 ) yMax = yMin + shape [ 0 ] - 1 yPositions = range ( stepSize * yMin , stepSize * yMax + 1 , stepSize ) spreadOffsets = list ( cross ( yPositions , xPositions ) ) # Put the (0,0) entry first spreadOffsets . remove ( ( 0 , 0 ) ) spreadOffsets . insert ( 0 , ( 0 , 0 ) ) numSpreadOffsets = len ( spreadOffsets ) print "spreadOffsets:" , spreadOffsets return centerOffsets , spreadOffsets
9969	def value ( self ) : if self . has_value : return self . _impl [ OBJ ] . get_value ( self . _impl [ KEY ] ) else : raise ValueError ( "Value not found" )
12121	def get_data_around ( self , timePoints , thisSweep = False , padding = 0.02 , msDeriv = 0 ) : if not np . array ( timePoints ) . shape : timePoints = [ float ( timePoints ) ] data = None for timePoint in timePoints : if thisSweep : sweep = self . currentSweep else : sweep = int ( timePoint / self . sweepInterval ) timePoint = timePoint - sweep * self . sweepInterval self . setSweep ( sweep ) if msDeriv : dx = int ( msDeriv * self . rate / 1000 ) #points per ms newData = ( self . dataY [ dx : ] - self . dataY [ : - dx ] ) * self . rate / 1000 / dx else : newData = self . dataY padPoints = int ( padding * self . rate ) pad = np . empty ( padPoints ) * np . nan Ic = timePoint * self . rate #center point (I) newData = np . concatenate ( ( pad , pad , newData , pad , pad ) ) Ic += padPoints * 2 newData = newData [ Ic - padPoints : Ic + padPoints ] newData = newData [ : int ( padPoints * 2 ) ] #TODO: omg so much trouble with this! if data is None : data = [ newData ] else : data = np . vstack ( ( data , newData ) ) #TODO: omg so much trouble with this! return data
1857	def BT ( cpu , dest , src ) : if dest . type == 'register' : cpu . CF = ( ( dest . read ( ) >> ( src . read ( ) % dest . size ) ) & 1 ) != 0 elif dest . type == 'memory' : addr , pos = cpu . _getMemoryBit ( dest , src ) base , size , ty = cpu . get_descriptor ( cpu . DS ) value = cpu . read_int ( addr + base , 8 ) cpu . CF = Operators . EXTRACT ( value , pos , 1 ) == 1 else : raise NotImplementedError ( f"Unknown operand for BT: {dest.type}" )
3427	def add_metabolites ( self , metabolite_list ) : if not hasattr ( metabolite_list , '__iter__' ) : metabolite_list = [ metabolite_list ] if len ( metabolite_list ) == 0 : return None # First check whether the metabolites exist in the model metabolite_list = [ x for x in metabolite_list if x . id not in self . metabolites ] bad_ids = [ m for m in metabolite_list if not isinstance ( m . id , string_types ) or len ( m . id ) < 1 ] if len ( bad_ids ) != 0 : raise ValueError ( 'invalid identifiers in {}' . format ( repr ( bad_ids ) ) ) for x in metabolite_list : x . _model = self self . metabolites += metabolite_list # from cameo ... to_add = [ ] for met in metabolite_list : if met . id not in self . constraints : constraint = self . problem . Constraint ( Zero , name = met . id , lb = 0 , ub = 0 ) to_add += [ constraint ] self . add_cons_vars ( to_add ) context = get_context ( self ) if context : context ( partial ( self . metabolites . __isub__ , metabolite_list ) ) for x in metabolite_list : # Do we care? context ( partial ( setattr , x , '_model' , None ) )
11671	def _flann_args ( self , X = None ) : args = { 'cores' : self . _n_jobs } if self . flann_algorithm == 'auto' : if X is None or X . dim > 5 : args [ 'algorithm' ] = 'linear' else : args [ 'algorithm' ] = 'kdtree_single' else : args [ 'algorithm' ] = self . flann_algorithm if self . flann_args : args . update ( self . flann_args ) # check that arguments are correct try : FLANNParameters ( ) . update ( args ) except AttributeError as e : msg = "flann_args contains an invalid argument:\n {}" raise TypeError ( msg . format ( e ) ) return args
10344	def load_differential_gene_expression ( path : str , gene_symbol_column : str = 'Gene.symbol' , logfc_column : str = 'logFC' , aggregator : Optional [ Callable [ [ List [ float ] ] , float ] ] = None , ) -> Mapping [ str , float ] : if aggregator is None : aggregator = np . median # Load the data frame df = pd . read_csv ( path ) # Check the columns exist in the data frame assert gene_symbol_column in df . columns assert logfc_column in df . columns # throw away columns that don't have gene symbols - these represent control sequences df = df . loc [ df [ gene_symbol_column ] . notnull ( ) , [ gene_symbol_column , logfc_column ] ] values = defaultdict ( list ) for _ , gene_symbol , log_fold_change in df . itertuples ( ) : values [ gene_symbol ] . append ( log_fold_change ) return { gene_symbol : aggregator ( log_fold_changes ) for gene_symbol , log_fold_changes in values . items ( ) }
2661	def hold_worker ( self , worker_id ) : c = self . command_client . run ( "HOLD_WORKER;{}" . format ( worker_id ) ) logger . debug ( "Sent hold request to worker: {}" . format ( worker_id ) ) return c
4766	def is_same_as ( self , other ) : if self . val is not other : self . _err ( 'Expected <%s> to be identical to <%s>, but was not.' % ( self . val , other ) ) return self
299	def plot_slippage_sweep ( returns , positions , transactions , slippage_params = ( 3 , 8 , 10 , 12 , 15 , 20 , 50 ) , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) slippage_sweep = pd . DataFrame ( ) for bps in slippage_params : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) label = str ( bps ) + " bps" slippage_sweep [ label ] = ep . cum_returns ( adj_returns , 1 ) slippage_sweep . plot ( alpha = 1.0 , lw = 0.5 , ax = ax ) ax . set_title ( 'Cumulative returns given additional per-dollar slippage' ) ax . set_ylabel ( '' ) ax . legend ( loc = 'center left' , frameon = True , framealpha = 0.5 ) return ax
13372	def expandpath ( path ) : return os . path . abspath ( os . path . expandvars ( os . path . expanduser ( path ) ) )
1664	def CheckMakePairUsesDeduction ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = _RE_PATTERN_EXPLICIT_MAKEPAIR . search ( line ) if match : error ( filename , linenum , 'build/explicit_make_pair' , 4 , # 4 = high confidence 'For C++11-compatibility, omit template arguments from make_pair' ' OR use pair directly OR if appropriate, construct a pair directly' )
11916	def render_to ( self , path , template , * * data ) : html = self . render ( template , * * data ) with open ( path , 'w' ) as f : f . write ( html . encode ( charset ) )
12888	def call ( self , path , extra = None ) : try : if not self . __webfsapi : self . __webfsapi = yield from self . get_fsapi_endpoint ( ) if not self . sid : self . sid = yield from self . create_session ( ) if not isinstance ( extra , dict ) : extra = dict ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( * * extra ) req_url = ( '%s/%s' % ( self . __webfsapi , path ) ) result = yield from self . __session . get ( req_url , params = params , timeout = self . timeout ) if result . status == 200 : text = yield from result . text ( encoding = 'utf-8' ) else : self . sid = yield from self . create_session ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( * * extra ) result = yield from self . __session . get ( req_url , params = params , timeout = self . timeout ) text = yield from result . text ( encoding = 'utf-8' ) return objectify . fromstring ( text ) except Exception as e : logging . info ( 'AFSAPI Exception: ' + traceback . format_exc ( ) ) return None
13377	def walk_up ( start_dir , depth = 20 ) : root = start_dir for i in xrange ( depth ) : contents = os . listdir ( root ) subdirs , files = [ ] , [ ] for f in contents : if os . path . isdir ( os . path . join ( root , f ) ) : subdirs . append ( f ) else : files . append ( f ) yield root , subdirs , files parent = os . path . dirname ( root ) if parent and not parent == root : root = parent else : break
7511	def select_samples ( dbsamples , samples , pidx = None ) : ## get index from dbsamples samples = [ i . name for i in samples ] if pidx : sidx = [ list ( dbsamples [ pidx ] ) . index ( i ) for i in samples ] else : sidx = [ list ( dbsamples ) . index ( i ) for i in samples ] sidx . sort ( ) return sidx
593	def _initEphemerals ( self ) : if hasattr ( self , '_sfdr' ) and self . _sfdr : self . _spatialPoolerOutput = numpy . zeros ( self . columnCount , dtype = GetNTAReal ( ) ) else : self . _spatialPoolerOutput = None # Will be filled in initInNetwork # Direct logging support (faster than node watch) self . _fpLogSPInput = None self . _fpLogSP = None self . _fpLogSPDense = None self . logPathInput = "" self . logPathOutput = "" self . logPathOutputDense = ""
12205	def url_builder ( self , endpoint , * , root = None , params = None , url_params = None ) : if root is None : root = self . ROOT scheme , netloc , path , _ , _ = urlsplit ( root ) return urlunsplit ( ( scheme , netloc , urljoin ( path , endpoint ) , urlencode ( url_params or { } ) , '' , ) ) . format ( * * params or { } )
4324	def convert ( self , samplerate = None , n_channels = None , bitdepth = None ) : bitdepths = [ 8 , 16 , 24 , 32 , 64 ] if bitdepth is not None : if bitdepth not in bitdepths : raise ValueError ( "bitdepth must be one of {}." . format ( str ( bitdepths ) ) ) self . output_format . extend ( [ '-b' , '{}' . format ( bitdepth ) ] ) if n_channels is not None : if not isinstance ( n_channels , int ) or n_channels <= 0 : raise ValueError ( "n_channels must be a positive integer." ) self . output_format . extend ( [ '-c' , '{}' . format ( n_channels ) ] ) if samplerate is not None : if not is_number ( samplerate ) or samplerate <= 0 : raise ValueError ( "samplerate must be a positive number." ) self . rate ( samplerate ) return self
375	def adjust_hue ( im , hout = 0.66 , is_offset = True , is_clip = True , is_random = False ) : hsv = rgb_to_hsv ( im ) if is_random : hout = np . random . uniform ( - hout , hout ) if is_offset : hsv [ ... , 0 ] += hout else : hsv [ ... , 0 ] = hout if is_clip : hsv [ ... , 0 ] = np . clip ( hsv [ ... , 0 ] , 0 , np . inf ) # Hao : can remove green dots rgb = hsv_to_rgb ( hsv ) return rgb
12482	def find_in_sections ( var_name , app_name ) : sections = get_sections ( app_name ) if not sections : raise ValueError ( 'No sections found in {} rcfiles.' . format ( app_name ) ) for s in sections : try : var_value = get_rcfile_variable_value ( var_name , section_name = s , app_name = app_name ) except : pass else : return s , var_value raise KeyError ( 'No variable {} has been found in {} ' 'rcfiles.' . format ( var_name , app_name ) )
622	def coordinatesFromIndex ( index , dimensions ) : coordinates = [ 0 ] * len ( dimensions ) shifted = index for i in xrange ( len ( dimensions ) - 1 , 0 , - 1 ) : coordinates [ i ] = shifted % dimensions [ i ] shifted = shifted / dimensions [ i ] coordinates [ 0 ] = shifted return coordinates
537	def run ( self ) : # ----------------------------------------------------------------------- # Load the experiment's description.py module descriptionPyModule = helpers . loadExperimentDescriptionScriptFromDir ( self . _experimentDir ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) expIface . normalizeStreamSources ( ) modelDescription = expIface . getModelDescription ( ) self . _modelControl = expIface . getModelControl ( ) # ----------------------------------------------------------------------- # Create the input data stream for this task streamDef = self . _modelControl [ 'dataset' ] from nupic . data . stream_reader import StreamReader readTimeout = 0 self . _inputSource = StreamReader ( streamDef , isBlocking = False , maxTimeout = readTimeout ) # ----------------------------------------------------------------------- #Get field statistics from the input source fieldStats = self . _getFieldStats ( ) # ----------------------------------------------------------------------- # Construct the model instance self . _model = ModelFactory . create ( modelDescription ) self . _model . setFieldStatistics ( fieldStats ) self . _model . enableLearning ( ) self . _model . enableInference ( self . _modelControl . get ( "inferenceArgs" , None ) ) # ----------------------------------------------------------------------- # Instantiate the metrics self . __metricMgr = MetricsManager ( self . _modelControl . get ( 'metrics' , None ) , self . _model . getFieldInfo ( ) , self . _model . getInferenceType ( ) ) self . __loggedMetricPatterns = self . _modelControl . get ( "loggedMetrics" , [ ] ) self . _optimizedMetricLabel = self . __getOptimizedMetricLabel ( ) self . _reportMetricLabels = matchPatterns ( self . _reportKeyPatterns , self . _getMetricLabels ( ) ) # ----------------------------------------------------------------------- # Initialize periodic activities (e.g., for model result updates) self . _periodic = self . _initPeriodicActivities ( ) # ----------------------------------------------------------------------- # Create our top-level loop-control iterator numIters = self . _modelControl . get ( 'iterationCount' , - 1 ) # Are we asked to turn off learning for a certain # of iterations near the # end? learningOffAt = None iterationCountInferOnly = self . _modelControl . get ( 'iterationCountInferOnly' , 0 ) if iterationCountInferOnly == - 1 : self . _model . disableLearning ( ) elif iterationCountInferOnly > 0 : assert numIters > iterationCountInferOnly , "when iterationCountInferOnly " "is specified, iterationCount must be greater than " "iterationCountInferOnly." learningOffAt = numIters - iterationCountInferOnly self . __runTaskMainLoop ( numIters , learningOffAt = learningOffAt ) # ----------------------------------------------------------------------- # Perform final operations for model self . _finalize ( ) return ( self . _cmpReason , None )
461	def get_random_int ( min_v = 0 , max_v = 10 , number = 5 , seed = None ) : rnd = random . Random ( ) if seed : rnd = random . Random ( seed ) # return [random.randint(min,max) for p in range(0, number)] return [ rnd . randint ( min_v , max_v ) for p in range ( 0 , number ) ]
1182	def fast_search ( self , pattern_codes ) : # pattern starts with a known prefix # <5=length> <6=skip> <7=prefix data> <overlap data> flags = pattern_codes [ 2 ] prefix_len = pattern_codes [ 5 ] prefix_skip = pattern_codes [ 6 ] # don't really know what this is good for prefix = pattern_codes [ 7 : 7 + prefix_len ] overlap = pattern_codes [ 7 + prefix_len - 1 : pattern_codes [ 1 ] + 1 ] pattern_codes = pattern_codes [ pattern_codes [ 1 ] + 1 : ] i = 0 string_position = self . string_position while string_position < self . end : while True : if ord ( self . string [ string_position ] ) != prefix [ i ] : if i == 0 : break else : i = overlap [ i ] else : i += 1 if i == prefix_len : # found a potential match self . start = string_position + 1 - prefix_len self . string_position = string_position + 1 - prefix_len + prefix_skip if flags & SRE_INFO_LITERAL : return True # matched all of pure literal pattern if self . match ( pattern_codes [ 2 * prefix_skip : ] ) : return True i = overlap [ i ] break string_position += 1 return False
916	def debug ( self , msg , * args , * * kwargs ) : self . _baseLogger . debug ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )
6999	def parallel_cp_pfdir ( pfpickledir , outdir , lcbasedir , pfpickleglob = 'periodfinding-*.pkl*' , lclistpkl = None , cprenorm = False , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , makeneighborlcs = True , fast_mode = False , gaia_max_timeout = 60.0 , gaia_mirror = None , xmatchinfo = None , xmatchradiusarcsec = 3.0 , minobservations = 99 , sigclip = 10.0 , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , skipdone = False , done_callback = None , done_callback_args = None , done_callback_kwargs = None , maxobjects = None , nworkers = 32 ) : pfpicklelist = sorted ( glob . glob ( os . path . join ( pfpickledir , pfpickleglob ) ) ) LOGINFO ( 'found %s period-finding pickles, running cp...' % len ( pfpicklelist ) ) return parallel_cp ( pfpicklelist , outdir , lcbasedir , fast_mode = fast_mode , lclistpkl = lclistpkl , nbrradiusarcsec = nbrradiusarcsec , gaia_max_timeout = gaia_max_timeout , gaia_mirror = gaia_mirror , maxnumneighbors = maxnumneighbors , makeneighborlcs = makeneighborlcs , xmatchinfo = xmatchinfo , xmatchradiusarcsec = xmatchradiusarcsec , sigclip = sigclip , minobservations = minobservations , cprenorm = cprenorm , maxobjects = maxobjects , lcformat = lcformat , lcformatdir = lcformatdir , timecols = timecols , magcols = magcols , errcols = errcols , skipdone = skipdone , nworkers = nworkers , done_callback = done_callback , done_callback_args = done_callback_args , done_callback_kwargs = done_callback_kwargs )
7749	def process_iq ( self , stanza ) : typ = stanza . stanza_type if typ in ( "result" , "error" ) : return self . _process_iq_response ( stanza ) if typ not in ( "get" , "set" ) : raise BadRequestProtocolError ( "Bad <iq/> type" ) logger . debug ( "Handling <iq type='{0}'> stanza: {1!r}" . format ( stanza , typ ) ) payload = stanza . get_payload ( None ) logger . debug ( " payload: {0!r}" . format ( payload ) ) if not payload : raise BadRequestProtocolError ( "<iq/> stanza with no child element" ) handler = self . _get_iq_handler ( typ , payload ) if not handler : payload = stanza . get_payload ( None , specialize = True ) logger . debug ( " specialized payload: {0!r}" . format ( payload ) ) if not isinstance ( payload , XMLPayload ) : handler = self . _get_iq_handler ( typ , payload ) if handler : response = handler ( stanza ) self . _process_handler_result ( response ) return True else : raise ServiceUnavailableProtocolError ( "Not implemented" )
12508	def get_3D_coordmap ( img ) : if isinstance ( img , nib . Nifti1Image ) : img = nifti2nipy ( img ) if img . ndim == 4 : from nipy . core . reference . coordinate_map import drop_io_dim cm = drop_io_dim ( img . coordmap , 3 ) else : cm = img . coordmap return cm
8097	def path ( s , graph , path ) : def end ( n ) : r = n . r * 0.35 s . _ctx . oval ( n . x - r , n . y - r , r * 2 , r * 2 ) if path and len ( path ) > 1 and s . stroke : s . _ctx . nofill ( ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a ) if s . name != DEFAULT : s . _ctx . strokewidth ( s . strokewidth ) else : s . _ctx . strokewidth ( s . strokewidth * 2 ) first = True for id in path : n = graph [ id ] if first : first = False s . _ctx . beginpath ( n . x , n . y ) end ( n ) else : s . _ctx . lineto ( n . x , n . y ) s . _ctx . endpath ( ) end ( n )
7565	def memoize ( func ) : class Memodict ( dict ) : """ just a dict""" def __getitem__ ( self , * key ) : return dict . __getitem__ ( self , key ) def __missing__ ( self , key ) : """ this makes it faster """ ret = self [ key ] = func ( * key ) return ret return Memodict ( ) . __getitem__
10118	def regular_polygon ( cls , center , radius , n_vertices , start_angle = 0 , * * kwargs ) : angles = ( np . arange ( n_vertices ) * 2 * np . pi / n_vertices ) + start_angle return cls ( center + radius * np . array ( [ np . cos ( angles ) , np . sin ( angles ) ] ) . T , * * kwargs )
2738	def assign ( self , droplet_id ) : return self . get_data ( "floating_ips/%s/actions/" % self . ip , type = POST , params = { "type" : "assign" , "droplet_id" : droplet_id } )
206	def offer ( self , p , e : Event ) : existing = self . events_scan . setdefault ( p , ( [ ] , [ ] , [ ] , [ ] ) if USE_VERTICAL else ( [ ] , [ ] , [ ] ) ) # Can use double linked-list for easy insertion at beginning/end ''' if e.type == Event.Type.END: existing.insert(0, e) else: existing.append(e) ''' existing [ e . type ] . append ( e )
11800	def infer_assignment ( self ) : self . support_pruning ( ) return dict ( ( v , self . curr_domains [ v ] [ 0 ] ) for v in self . vars if 1 == len ( self . curr_domains [ v ] ) )
10061	def schemaforms ( self ) : _schemaforms = { k : v [ 'schemaform' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'schemaform' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_SCHEMAFORM' ] , _schemaforms )
8916	def _retrieve_certificate ( self , access_token , timeout = 3 ) : logger . debug ( "Retrieve certificate with token." ) # Generate a new key pair key_pair = crypto . PKey ( ) key_pair . generate_key ( crypto . TYPE_RSA , 2048 ) private_key = crypto . dump_privatekey ( crypto . FILETYPE_PEM , key_pair ) . decode ( "utf-8" ) # Generate a certificate request using that key-pair cert_request = crypto . X509Req ( ) # Create public key object cert_request . set_pubkey ( key_pair ) # Add the public key to the request cert_request . sign ( key_pair , 'md5' ) der_cert_req = crypto . dump_certificate_request ( crypto . FILETYPE_ASN1 , cert_request ) encoded_cert_req = base64 . b64encode ( der_cert_req ) # Build the OAuth session object token = { 'access_token' : access_token , 'token_type' : 'Bearer' } client = OAuth2Session ( token = token ) response = client . post ( self . certificate_url , data = { 'certificate_request' : encoded_cert_req } , verify = False , timeout = timeout , ) if response . ok : content = "{} {}" . format ( response . text , private_key ) with open ( self . esgf_credentials , 'w' ) as fh : fh . write ( content ) logger . debug ( 'Fetched certificate successfully.' ) else : msg = "Could not get certificate: {} {}" . format ( response . status_code , response . reason ) raise Exception ( msg ) return True
631	def createSegment ( self , cell ) : cellData = self . _cells [ cell ] if len ( self . _freeFlatIdxs ) > 0 : flatIdx = self . _freeFlatIdxs . pop ( ) else : flatIdx = self . _nextFlatIdx self . _segmentForFlatIdx . append ( None ) self . _nextFlatIdx += 1 ordinal = self . _nextSegmentOrdinal self . _nextSegmentOrdinal += 1 segment = Segment ( cell , flatIdx , ordinal ) cellData . _segments . append ( segment ) self . _segmentForFlatIdx [ flatIdx ] = segment return segment
2512	def handle_pkg_lic ( self , p_term , predicate , builder_func ) : try : for _ , _ , licenses in self . graph . triples ( ( p_term , predicate , None ) ) : if ( licenses , RDF . type , self . spdx_namespace [ 'ConjunctiveLicenseSet' ] ) in self . graph : lics = self . handle_conjunctive_list ( licenses ) builder_func ( self . doc , lics ) elif ( licenses , RDF . type , self . spdx_namespace [ 'DisjunctiveLicenseSet' ] ) in self . graph : lics = self . handle_disjunctive_list ( licenses ) builder_func ( self . doc , lics ) else : try : lics = self . handle_lics ( licenses ) builder_func ( self . doc , lics ) except SPDXValueError : self . value_error ( 'PKG_SINGLE_LICS' , licenses ) except CardinalityError : self . more_than_one_error ( 'package {0}' . format ( predicate ) )
13814	def _MessageToJsonObject ( message , including_default_value_fields ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : return _WrapperMessageToJsonObject ( message ) if full_name in _WKTJSONMETHODS : return _WKTJSONMETHODS [ full_name ] [ 0 ] ( message , including_default_value_fields ) js = { } return _RegularMessageToJsonObject ( message , js , including_default_value_fields )
13801	def revoke_token ( self , token , callback ) : yield Task ( self . data_store . remove , 'tokens' , token = token ) callback ( )
4478	def split_storage ( path , default = 'osfstorage' ) : path = norm_remote_path ( path ) for provider in KNOWN_PROVIDERS : if path . startswith ( provider + '/' ) : if six . PY3 : return path . split ( '/' , maxsplit = 1 ) else : return path . split ( '/' , 1 ) return ( default , path )
12651	def where_is ( strings , pattern , n = 1 , lookup_func = re . match ) : count = 0 for idx , item in enumerate ( strings ) : if lookup_func ( pattern , item ) : count += 1 if count == n : return idx return - 1
10782	def _feature_guess ( im , rad , minmass = None , use_tp = False , trim_edge = False ) : if minmass is None : # we use 1% of the feature size mass as a cutoff; # it's easier to remove than to add minmass = rad ** 3 * 4 / 3. * np . pi * 0.01 # 0.03 is a magic number; works well if use_tp : diameter = np . ceil ( 2 * rad ) diameter += 1 - ( diameter % 2 ) df = peri . trackpy . locate ( im , int ( diameter ) , minmass = minmass ) npart = np . array ( df [ 'mass' ] ) . size guess = np . zeros ( [ npart , 3 ] ) guess [ : , 0 ] = df [ 'z' ] guess [ : , 1 ] = df [ 'y' ] guess [ : , 2 ] = df [ 'x' ] mass = df [ 'mass' ] else : guess , mass = initializers . local_max_featuring ( im , radius = rad , minmass = minmass , trim_edge = trim_edge ) npart = guess . shape [ 0 ] # I want to return these sorted by mass: inds = np . argsort ( mass ) [ : : - 1 ] # biggest mass first return guess [ inds ] . copy ( ) , npart
4686	def encrypt ( self , message ) : if not message : return None nonce = str ( random . getrandbits ( 64 ) ) try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( self . from_account [ "options" ] [ "memo_key" ] ) except KeyNotFound : # if all fails, raise exception raise MissingKeyError ( "Memo private key {} for {} could not be found" . format ( self . from_account [ "options" ] [ "memo_key" ] , self . from_account [ "name" ] ) ) if not memo_wif : raise MissingKeyError ( "Memo key for %s missing!" % self . from_account [ "name" ] ) if not hasattr ( self , "chain_prefix" ) : self . chain_prefix = self . blockchain . prefix enc = memo . encode_memo ( self . privatekey_class ( memo_wif ) , self . publickey_class ( self . to_account [ "options" ] [ "memo_key" ] , prefix = self . chain_prefix ) , nonce , message , ) return { "message" : enc , "nonce" : nonce , "from" : self . from_account [ "options" ] [ "memo_key" ] , "to" : self . to_account [ "options" ] [ "memo_key" ] , }
10103	def send ( self , email_id , recipient , email_data = None , sender = None , cc = None , bcc = None , tags = [ ] , headers = { } , esp_account = None , locale = None , email_version_name = None , inline = None , files = [ ] , timeout = None ) : if not email_data : email_data = { } # for backwards compatibility, will be removed if isinstance ( recipient , string_types ) : warnings . warn ( "Passing email directly for recipient is deprecated" , DeprecationWarning ) recipient = { 'address' : recipient } payload = { 'email_id' : email_id , 'recipient' : recipient , 'email_data' : email_data } if sender : payload [ 'sender' ] = sender if cc : if not type ( cc ) == list : logger . error ( 'kwarg cc must be type(list), got %s' % type ( cc ) ) payload [ 'cc' ] = cc if bcc : if not type ( bcc ) == list : logger . error ( 'kwarg bcc must be type(list), got %s' % type ( bcc ) ) payload [ 'bcc' ] = bcc if tags : if not type ( tags ) == list : logger . error ( 'kwarg tags must be type(list), got %s' % ( type ( tags ) ) ) payload [ 'tags' ] = tags if headers : if not type ( headers ) == dict : logger . error ( 'kwarg headers must be type(dict), got %s' % ( type ( headers ) ) ) payload [ 'headers' ] = headers if esp_account : if not isinstance ( esp_account , string_types ) : logger . error ( 'kwarg esp_account must be a string, got %s' % ( type ( esp_account ) ) ) payload [ 'esp_account' ] = esp_account if locale : if not isinstance ( locale , string_types ) : logger . error ( 'kwarg locale must be a string, got %s' % ( type ( locale ) ) ) payload [ 'locale' ] = locale if email_version_name : if not isinstance ( email_version_name , string_types ) : logger . error ( 'kwarg email_version_name must be a string, got %s' % ( type ( email_version_name ) ) ) payload [ 'version_name' ] = email_version_name if inline : payload [ 'inline' ] = self . _make_file_dict ( inline ) if files : payload [ 'files' ] = [ self . _make_file_dict ( f ) for f in files ] return self . _api_request ( self . SEND_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
4931	def transform_courserun_description ( self , content_metadata_item ) : description_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : description_with_locales . append ( { 'locale' : locale , 'value' : ( content_metadata_item [ 'full_description' ] or content_metadata_item [ 'short_description' ] or content_metadata_item [ 'title' ] or '' ) } ) return description_with_locales
1206	def from_spec ( spec , kwargs ) : env = tensorforce . util . get_object ( obj = spec , predefined_objects = tensorforce . environments . environments , kwargs = kwargs ) assert isinstance ( env , Environment ) return env
12045	def originFormat ( thing ) : if type ( thing ) is list and type ( thing [ 0 ] ) is dict : return originFormat_listOfDicts ( thing ) if type ( thing ) is list and type ( thing [ 0 ] ) is list : return originFormat_listOfDicts ( dictFlat ( thing ) ) else : print ( " !! I don't know how to format this object!" ) print ( thing )
3731	def checkCAS ( CASRN ) : try : check = CASRN [ - 1 ] CASRN = CASRN [ : : - 1 ] [ 1 : ] productsum = 0 i = 1 for num in CASRN : if num == '-' : pass else : productsum += i * int ( num ) i += 1 return ( productsum % 10 == int ( check ) ) except : return False
9282	def set_filter ( self , filter_text ) : self . filter = filter_text self . logger . info ( "Setting filter to: %s" , self . filter ) if self . _connected : self . _sendall ( "#filter %s\r\n" % self . filter )
3473	def build_reaction_string ( self , use_metabolite_names = False ) : def format ( number ) : return "" if number == 1 else str ( number ) . rstrip ( "." ) + " " id_type = 'id' if use_metabolite_names : id_type = 'name' reactant_bits = [ ] product_bits = [ ] for met in sorted ( self . _metabolites , key = attrgetter ( "id" ) ) : coefficient = self . _metabolites [ met ] name = str ( getattr ( met , id_type ) ) if coefficient >= 0 : product_bits . append ( format ( coefficient ) + name ) else : reactant_bits . append ( format ( abs ( coefficient ) ) + name ) reaction_string = ' + ' . join ( reactant_bits ) if not self . reversibility : if self . lower_bound < 0 and self . upper_bound <= 0 : reaction_string += ' <-- ' else : reaction_string += ' --> ' else : reaction_string += ' <=> ' reaction_string += ' + ' . join ( product_bits ) return reaction_string
4587	def stop ( self ) : if self . is_running : log . info ( 'Stopping' ) self . is_running = False self . __class__ . _INSTANCE = None try : self . thread and self . thread . stop ( ) except : log . error ( 'Error stopping thread' ) traceback . print_exc ( ) self . thread = None return True
6602	def collect_result ( self , package_index ) : result_fullpath = self . result_fullpath ( package_index ) # e.g., '{path}/tpd_20161129_122841_HnpcmF/results/task_00009/result.p.gz' try : with gzip . open ( result_fullpath , 'rb' ) as f : result = pickle . load ( f ) except Exception as e : logger = logging . getLogger ( __name__ ) logger . warning ( e ) return None return result
1870	def MOVSX ( cpu , op0 , op1 ) : op0 . write ( Operators . SEXTEND ( op1 . read ( ) , op1 . size , op0 . size ) )
7439	def _build_stat ( self , idx ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) newdat = pd . DataFrame ( [ self . samples [ i ] . stats_dfs [ idx ] for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' ) return newdat
515	def _avgConnectedSpanForColumn1D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 1 ) connected = self . _connectedSynapses [ columnIndex ] . nonzero ( ) [ 0 ] if connected . size == 0 : return 0 else : return max ( connected ) - min ( connected ) + 1
6976	def keplermag_to_sdssr ( keplermag , kic_sdssg , kic_sdssr ) : kic_sdssgr = kic_sdssg - kic_sdssr if kic_sdssgr < 0.8 : kepsdssr = ( keplermag - 0.2 * kic_sdssg ) / 0.8 else : kepsdssr = ( keplermag - 0.1 * kic_sdssg ) / 0.9 return kepsdssr
672	def getPredictionResults ( network , clRegionName ) : classifierRegion = network . regions [ clRegionName ] actualValues = classifierRegion . getOutputData ( "actualValues" ) probabilities = classifierRegion . getOutputData ( "probabilities" ) steps = classifierRegion . getSelf ( ) . stepsList N = classifierRegion . getSelf ( ) . maxCategoryCount results = { step : { } for step in steps } for i in range ( len ( steps ) ) : # stepProbabilities are probabilities for this prediction step only. stepProbabilities = probabilities [ i * N : ( i + 1 ) * N - 1 ] mostLikelyCategoryIdx = stepProbabilities . argmax ( ) predictedValue = actualValues [ mostLikelyCategoryIdx ] predictionConfidence = stepProbabilities [ mostLikelyCategoryIdx ] results [ steps [ i ] ] [ "predictedValue" ] = predictedValue results [ steps [ i ] ] [ "predictionConfidence" ] = predictionConfidence return results
1811	def SETNAE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
1020	def buildOverlappedSequences ( numSequences = 2 , seqLen = 5 , sharedElements = [ 3 , 4 ] , numOnBitsPerPattern = 3 , patternOverlap = 0 , seqOverlap = 0 , * * kwargs ) : # Total number of patterns used to build the sequences numSharedElements = len ( sharedElements ) numUniqueElements = seqLen - numSharedElements numPatterns = numSharedElements + numUniqueElements * numSequences # Create the table of patterns patterns = getSimplePatterns ( numOnBitsPerPattern , numPatterns , patternOverlap ) # Total number of columns required numCols = len ( patterns [ 0 ] ) # ----------------------------------------------------------------------- # Create the training sequences trainingSequences = [ ] uniquePatternIndices = range ( numSharedElements , numPatterns ) for _ in xrange ( numSequences ) : sequence = [ ] # pattern indices [0 ... numSharedElements-1] are reserved for the shared # middle sharedPatternIndices = range ( numSharedElements ) # Build up the sequence for j in xrange ( seqLen ) : if j in sharedElements : patIdx = sharedPatternIndices . pop ( 0 ) else : patIdx = uniquePatternIndices . pop ( 0 ) sequence . append ( patterns [ patIdx ] ) trainingSequences . append ( sequence ) if VERBOSITY >= 3 : print "\nTraining sequences" printAllTrainingSequences ( trainingSequences ) return ( numCols , trainingSequences )
9965	def _to_attrdict ( self , attrs = None ) : result = self . _baseattrs for attr in attrs : if hasattr ( self , attr ) : result [ attr ] = getattr ( self , attr ) . _to_attrdict ( attrs ) return result
12732	def join ( self , shape , body_a , body_b = None , name = None , * * kwargs ) : ba = self . get_body ( body_a ) bb = self . get_body ( body_b ) shape = shape . lower ( ) if name is None : name = '{}^{}^{}' . format ( ba . name , shape , bb . name if bb else '' ) self . _joints [ name ] = Joint . build ( shape , name , self , body_a = ba , body_b = bb , * * kwargs ) return self . _joints [ name ]
9925	def get_queryset ( self ) : oldest = timezone . now ( ) - app_settings . PASSWORD_RESET_EXPIRATION queryset = super ( ValidPasswordResetTokenManager , self ) . get_queryset ( ) return queryset . filter ( created_at__gt = oldest )
4887	def get_course_final_price ( self , mode , currency = '$' , enterprise_catalog_uuid = None ) : try : price_details = self . client . baskets . calculate . get ( sku = [ mode [ 'sku' ] ] , username = self . user . username , catalog = enterprise_catalog_uuid , ) except ( SlumberBaseException , ConnectionError , Timeout ) as exc : LOGGER . exception ( 'Failed to get price details for sku %s due to: %s' , mode [ 'sku' ] , str ( exc ) ) price_details = { } price = price_details . get ( 'total_incl_tax' , mode [ 'min_price' ] ) if price != mode [ 'min_price' ] : return format_price ( price , currency ) return mode [ 'original_price' ]
5184	def edges ( self , * * kwargs ) : edges = self . _query ( 'edges' , * * kwargs ) for edge in edges : identifier_source = edge [ 'source_type' ] + '[' + edge [ 'source_title' ] + ']' identifier_target = edge [ 'target_type' ] + '[' + edge [ 'target_title' ] + ']' yield Edge ( source = self . resources [ identifier_source ] , target = self . resources [ identifier_target ] , relationship = edge [ 'relationship' ] , node = edge [ 'certname' ] )
2293	def eval_entropy ( x ) : hx = 0. sx = sorted ( x ) for i , j in zip ( sx [ : - 1 ] , sx [ 1 : ] ) : delta = j - i if bool ( delta ) : hx += np . log ( np . abs ( delta ) ) hx = hx / ( len ( x ) - 1 ) + psi ( len ( x ) ) - psi ( 1 ) return hx
4351	def join ( self , room ) : self . socket . rooms . add ( self . _get_room_name ( room ) )
6525	def get_grouped_issues ( self , keyfunc = None , sortby = None ) : if not keyfunc : keyfunc = default_group if not sortby : sortby = self . DEFAULT_SORT self . _ensure_cleaned_issues ( ) return self . _group_issues ( self . _cleaned_issues , keyfunc , sortby )
4135	def check_md5sum_change ( src_file ) : src_md5 = get_md5sum ( src_file ) src_md5_file = src_file + '.md5' src_file_changed = True if os . path . exists ( src_md5_file ) : with open ( src_md5_file , 'r' ) as file_checksum : ref_md5 = file_checksum . read ( ) if src_md5 == ref_md5 : src_file_changed = False if src_file_changed : with open ( src_md5_file , 'w' ) as file_checksum : file_checksum . write ( src_md5 ) return src_file_changed
3387	def _is_redundant ( self , matrix , cutoff = None ) : cutoff = 1.0 - self . feasibility_tol # Avoid zero variances extra_col = matrix [ : , 0 ] + 1 # Avoid zero rows being correlated with constant rows extra_col [ matrix . sum ( axis = 1 ) == 0 ] = 2 corr = np . corrcoef ( np . c_ [ matrix , extra_col ] ) corr = np . tril ( corr , - 1 ) return ( np . abs ( corr ) > cutoff ) . any ( axis = 1 )
10402	def calculate_score ( self , node : BaseEntity ) -> float : score = ( self . graph . nodes [ node ] [ self . tag ] if self . tag in self . graph . nodes [ node ] else self . default_score ) for predecessor , _ , d in self . graph . in_edges ( node , data = True ) : if d [ RELATION ] in CAUSAL_INCREASE_RELATIONS : score += self . graph . nodes [ predecessor ] [ self . tag ] elif d [ RELATION ] in CAUSAL_DECREASE_RELATIONS : score -= self . graph . nodes [ predecessor ] [ self . tag ] return score
1250	def do_action ( self , action ) : temp_state = np . rot90 ( self . _state , action ) reward = self . _do_action_left ( temp_state ) self . _state = np . rot90 ( temp_state , - action ) self . _score += reward self . add_random_tile ( ) return reward
9692	def send ( self , data ) : while len ( self . senders ) >= self . window : pass self . senders [ self . new_seq_no ] = self . Sender ( self . write , self . send_lock , data , self . new_seq_no , timeout = self . sending_timeout , callback = self . send_callback , ) self . senders [ self . new_seq_no ] . start ( ) self . new_seq_no = ( self . new_seq_no + 1 ) % HDLController . MAX_SEQ_NO
1775	def CPUID ( cpu ) : # FIXME Choose conservative values and consider returning some default when eax not here conf = { 0x0 : ( 0x0000000d , 0x756e6547 , 0x6c65746e , 0x49656e69 ) , 0x1 : ( 0x000306c3 , 0x05100800 , 0x7ffafbff , 0xbfebfbff ) , 0x2 : ( 0x76035a01 , 0x00f0b5ff , 0x00000000 , 0x00c10000 ) , 0x4 : { 0x0 : ( 0x1c004121 , 0x01c0003f , 0x0000003f , 0x00000000 ) , 0x1 : ( 0x1c004122 , 0x01c0003f , 0x0000003f , 0x00000000 ) , 0x2 : ( 0x1c004143 , 0x01c0003f , 0x000001ff , 0x00000000 ) , 0x3 : ( 0x1c03c163 , 0x03c0003f , 0x00000fff , 0x00000006 ) } , 0x7 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) , 0x8 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) , 0xb : { 0x0 : ( 0x00000001 , 0x00000002 , 0x00000100 , 0x00000005 ) , 0x1 : ( 0x00000004 , 0x00000004 , 0x00000201 , 0x00000003 ) } , 0xd : { 0x0 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) , 0x1 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) } , } if cpu . EAX not in conf : logger . warning ( 'CPUID with EAX=%x not implemented @ %x' , cpu . EAX , cpu . PC ) cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = 0 , 0 , 0 , 0 return if isinstance ( conf [ cpu . EAX ] , tuple ) : cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = conf [ cpu . EAX ] return if cpu . ECX not in conf [ cpu . EAX ] : logger . warning ( 'CPUID with EAX=%x ECX=%x not implemented' , cpu . EAX , cpu . ECX ) cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = 0 , 0 , 0 , 0 return cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = conf [ cpu . EAX ] [ cpu . ECX ]
5123	def show_active ( self , * * kwargs ) : g = self . g for v in g . nodes ( ) : self . g . set_vp ( v , 'vertex_color' , [ 0 , 0 , 0 , 0.9 ] ) is_active = False my_iter = g . in_edges ( v ) if g . is_directed ( ) else g . out_edges ( v ) for e in my_iter : ei = g . edge_index [ e ] if self . edge2queue [ ei ] . _active : is_active = True break if is_active : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_active' ] ) else : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_inactive' ] ) for e in g . edges ( ) : ei = g . edge_index [ e ] if self . edge2queue [ ei ] . _active : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_active' ] ) else : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_inactive' ] ) self . draw ( update_colors = False , * * kwargs ) self . _update_all_colors ( )
8619	def getServerInfo ( pbclient = None , dc_id = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc_id is None : raise ValueError ( "argument 'dc_id' must not be None" ) # list of all found server's info server_info = [ ] # depth 1 is enough for props/meta servers = pbclient . list_servers ( dc_id , 1 ) for server in servers [ 'items' ] : props = server [ 'properties' ] info = dict ( id = server [ 'id' ] , name = props [ 'name' ] , state = server [ 'metadata' ] [ 'state' ] , vmstate = props [ 'vmState' ] ) server_info . append ( info ) # end for(servers) return server_info
1567	def invoke_hook_bolt_execute ( self , heron_tuple , execute_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_execute_info = BoltExecuteInfo ( heron_tuple = heron_tuple , executing_task_id = self . get_task_id ( ) , execute_latency_ms = execute_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_execute ( bolt_execute_info )
1915	def put ( self , state_id ) : self . _states . append ( state_id ) self . _lock . notify_all ( ) return state_id
4022	def docker_vm_is_running ( ) : running_vms = check_output_demoted ( [ 'VBoxManage' , 'list' , 'runningvms' ] ) for line in running_vms . splitlines ( ) : if '"{}"' . format ( constants . VM_MACHINE_NAME ) in line : return True return False
1969	def awake ( self , procid ) : logger . debug ( f"Remove procid:{procid} from waitlists and reestablish it in the running list" ) for wait_list in self . rwait : if procid in wait_list : wait_list . remove ( procid ) for wait_list in self . twait : if procid in wait_list : wait_list . remove ( procid ) self . timers [ procid ] = None self . running . append ( procid ) if self . _current is None : self . _current = procid
6432	def encode ( self , word ) : word = word . upper ( ) # Rule 3 word = self . _delete_consecutive_repeats ( word ) # Rule 4 # Rule 5 i = 0 while i < len ( word ) : for match_len in range ( 4 , 1 , - 1 ) : if word [ i : i + match_len ] in self . _rules [ match_len ] : repl = self . _rules [ match_len ] [ word [ i : i + match_len ] ] word = word [ : i ] + repl + word [ i + match_len : ] i += len ( repl ) break else : i += 1 word = word [ : 1 ] + word [ 1 : ] . translate ( self . _del_trans ) # Rule 6 return word
3226	def get_creds_from_kwargs ( kwargs ) : creds = { 'key_file' : kwargs . pop ( 'key_file' , None ) , 'http_auth' : kwargs . pop ( 'http_auth' , None ) , 'project' : kwargs . get ( 'project' , None ) , 'user_agent' : kwargs . pop ( 'user_agent' , None ) , 'api_version' : kwargs . pop ( 'api_version' , 'v1' ) } return ( creds , kwargs )
9509	def intersection ( self , i ) : if self . intersects ( i ) : return Interval ( max ( self . start , i . start ) , min ( self . end , i . end ) ) else : return None
7561	def get_sampled ( data , totn , node ) : ## convert tip names to ints names = sorted ( totn ) cdict = { name : idx for idx , name in enumerate ( names ) } ## skip some nodes if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : ## get counts on down edges if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = set ( cdict [ i ] for i in down_r . get_leaf_names ( ) ) lendl = set ( cdict [ i ] for i in down_l . get_leaf_names ( ) ) ## get count on up edge sister up_r = node . get_sisters ( ) [ 0 ] lenur = set ( cdict [ i ] for i in up_r . get_leaf_names ( ) ) ## everyone else lenul = set ( cdict [ i ] for i in totn ) - set . union ( lendr , lendl , lenur ) idx = 0 sampled = 0 with h5py . File ( data . database . output , 'r' ) as io5 : end = io5 [ "quartets" ] . shape [ 0 ] while 1 : ## break condition if idx >= end : break ## counts matches qrts = io5 [ "quartets" ] [ idx : idx + data . _chunksize ] for qrt in qrts : sqrt = set ( qrt ) if all ( [ sqrt . intersection ( i ) for i in [ lendr , lendl , lenur , lenul ] ] ) : sampled += 1 ## increase span idx += data . _chunksize return sampled
2569	def construct_end_message ( self ) : app_count = self . dfk . task_count site_count = len ( [ x for x in self . dfk . config . executors if x . managed ] ) app_fails = len ( [ t for t in self . dfk . tasks if self . dfk . tasks [ t ] [ 'status' ] in FINAL_FAILURE_STATES ] ) message = { 'uuid' : self . uuid , 'end' : time . time ( ) , 't_apps' : app_count , 'sites' : site_count , 'c_time' : None , 'failed' : app_fails , 'test' : self . test_mode , } return json . dumps ( message )
5856	def create_dataset_version ( self , dataset_id ) : failure_message = "Failed to create dataset version for dataset {}" . format ( dataset_id ) number = self . _get_success_json ( self . _post_json ( routes . create_dataset_version ( dataset_id ) , data = { } , failure_message = failure_message ) ) [ 'dataset_scoped_id' ] return DatasetVersion ( number = number )
5740	def main ( path , pid , queue ) : setup_logging ( ) if pid : with open ( os . path . expanduser ( pid ) , "w" ) as f : f . write ( str ( os . getpid ( ) ) ) if not path : path = os . getcwd ( ) sys . path . insert ( 0 , path ) queue = import_queue ( queue ) import psq worker = psq . Worker ( queue = queue ) worker . listen ( )
1074	def getphraselist ( self ) : plist = [ ] while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS : self . pos += 1 elif self . field [ self . pos ] == '"' : plist . append ( self . getquote ( ) ) elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) elif self . field [ self . pos ] in self . phraseends : break else : plist . append ( self . getatom ( self . phraseends ) ) return plist
9349	def date ( past = False , min_delta = 0 , max_delta = 20 ) : timedelta = dt . timedelta ( days = _delta ( past , min_delta , max_delta ) ) return dt . date . today ( ) + timedelta
10231	def list_abundance_expansion ( graph : BELGraph ) -> None : mapping = { node : flatten_list_abundance ( node ) for node in graph if isinstance ( node , ListAbundance ) } relabel_nodes ( graph , mapping , copy = False )
13127	def get_pipe ( self ) : lines = [ ] for line in sys . stdin : try : lines . append ( self . line_to_object ( line . strip ( ) ) ) except ValueError : pass except KeyError : pass return lines
5906	def create_portable_topology ( topol , struct , * * kwargs ) : _topoldir , _topol = os . path . split ( topol ) processed = kwargs . pop ( 'processed' , os . path . join ( _topoldir , 'pp_' + _topol ) ) grompp_kwargs , mdp_kwargs = filter_grompp_options ( * * kwargs ) mdp_kwargs = add_mdp_includes ( topol , mdp_kwargs ) with tempfile . NamedTemporaryFile ( suffix = '.mdp' ) as mdp : mdp . write ( '; empty mdp file\ninclude = {include!s}\n' . format ( * * mdp_kwargs ) ) mdp . flush ( ) grompp_kwargs [ 'p' ] = topol grompp_kwargs [ 'pp' ] = processed grompp_kwargs [ 'f' ] = mdp . name grompp_kwargs [ 'c' ] = struct grompp_kwargs [ 'v' ] = False try : gromacs . grompp ( * * grompp_kwargs ) finally : utilities . unlink_gmx ( 'topol.tpr' , 'mdout.mdp' ) return utilities . realpath ( processed )
11291	def json ( request , * args , * * kwargs ) : # coerce to dictionary params = dict ( request . GET . items ( ) ) callback = params . pop ( 'callback' , None ) url = params . pop ( 'url' , None ) if not url : return HttpResponseBadRequest ( 'Required parameter missing: URL' ) try : provider = oembed . site . provider_for_url ( url ) if not provider . provides : raise OEmbedMissingEndpoint ( ) except OEmbedMissingEndpoint : raise Http404 ( 'No provider found for %s' % url ) query = dict ( [ ( smart_str ( k ) , smart_str ( v ) ) for k , v in params . items ( ) if v ] ) try : resource = oembed . site . embed ( url , * * query ) except OEmbedException , e : raise Http404 ( 'Error embedding %s: %s' % ( url , str ( e ) ) ) response = HttpResponse ( mimetype = 'application/json' ) json = resource . json if callback : response . write ( '%s(%s)' % ( defaultfilters . force_escape ( callback ) , json ) ) else : response . write ( json ) return response
1290	def tf_step ( self , time , variables , * * kwargs ) : fn_loss = kwargs [ "fn_loss" ] if variables is None : variables = tf . trainable_variables return tf . gradients ( fn_loss , variables )
1255	def setup_scaffold ( self ) : if self . execution_type == "single" : global_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) # global_variables += [self.global_episode, self.global_timestep] init_op = tf . variables_initializer ( var_list = global_variables ) if self . summarizer_init_op is not None : init_op = tf . group ( init_op , self . summarizer_init_op ) if self . graph_summary is None : ready_op = tf . report_uninitialized_variables ( var_list = global_variables ) ready_for_local_init_op = None local_init_op = None else : ready_op = None ready_for_local_init_op = tf . report_uninitialized_variables ( var_list = global_variables ) local_init_op = self . graph_summary else : # Global and local variable initializers. global_variables = self . global_model . get_variables ( include_submodules = True , include_nontrainable = True ) # global_variables += [self.global_episode, self.global_timestep] local_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) init_op = tf . variables_initializer ( var_list = global_variables ) if self . summarizer_init_op is not None : init_op = tf . group ( init_op , self . summarizer_init_op ) ready_op = tf . report_uninitialized_variables ( var_list = ( global_variables + local_variables ) ) ready_for_local_init_op = tf . report_uninitialized_variables ( var_list = global_variables ) if self . graph_summary is None : local_init_op = tf . group ( tf . variables_initializer ( var_list = local_variables ) , # Synchronize values of trainable variables. * ( tf . assign ( ref = local_var , value = global_var ) for local_var , global_var in zip ( self . get_variables ( include_submodules = True ) , self . global_model . get_variables ( include_submodules = True ) ) ) ) else : local_init_op = tf . group ( tf . variables_initializer ( var_list = local_variables ) , self . graph_summary , # Synchronize values of trainable variables. * ( tf . assign ( ref = local_var , value = global_var ) for local_var , global_var in zip ( self . get_variables ( include_submodules = True ) , self . global_model . get_variables ( include_submodules = True ) ) ) ) def init_fn ( scaffold , session ) : if self . saver_spec is not None and self . saver_spec . get ( 'load' , True ) : directory = self . saver_spec [ 'directory' ] file = self . saver_spec . get ( 'file' ) if file is None : file = tf . train . latest_checkpoint ( checkpoint_dir = directory , latest_filename = None # Corresponds to argument of saver.save() in Model.save(). ) elif not os . path . isfile ( file ) : file = os . path . join ( directory , file ) if file is not None : try : scaffold . saver . restore ( sess = session , save_path = file ) session . run ( fetches = self . list_buffer_index_reset_op ) except tf . errors . NotFoundError : raise TensorForceError ( "Error: Existing checkpoint could not be loaded! Set \"load\" to false in saver_spec." ) # TensorFlow scaffold object # TODO explain what it does. self . scaffold = tf . train . Scaffold ( init_op = init_op , init_feed_dict = None , init_fn = init_fn , ready_op = ready_op , ready_for_local_init_op = ready_for_local_init_op , local_init_op = local_init_op , summary_op = None , saver = self . saver , copy_from_scaffold = None )
1332	def gradient ( self , image = None , label = None , strict = True ) : assert self . has_gradient ( ) if image is None : image = self . __original_image if label is None : label = self . __original_class assert not strict or self . in_bounds ( image ) self . _total_gradient_calls += 1 gradient = self . __model . gradient ( image , label ) assert gradient . shape == image . shape return gradient
10174	def set_bookmark ( self ) : def _success_date ( ) : bookmark = { 'date' : self . new_bookmark or datetime . datetime . utcnow ( ) . strftime ( self . doc_id_suffix ) } yield dict ( _index = self . last_index_written , _type = self . bookmark_doc_type , _source = bookmark ) if self . last_index_written : bulk ( self . client , _success_date ( ) , stats_only = True )
1330	def predictions ( self , image , strict = True , return_details = False ) : in_bounds = self . in_bounds ( image ) assert not strict or in_bounds self . _total_prediction_calls += 1 predictions = self . __model . predictions ( image ) is_adversarial , is_best , distance = self . __is_adversarial ( image , predictions , in_bounds ) assert predictions . ndim == 1 if return_details : return predictions , is_adversarial , is_best , distance else : return predictions , is_adversarial
6071	def luminosity_within_circle_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if self . has_light_profile : return sum ( map ( lambda p : p . luminosity_within_circle_in_units ( radius = radius , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) , self . light_profiles ) ) else : return None
13850	def ensure_dir_exists ( func ) : @ functools . wraps ( func ) def make_if_not_present ( ) : dir = func ( ) if not os . path . isdir ( dir ) : os . makedirs ( dir ) return dir return make_if_not_present
2456	def set_pkg_license_from_file ( self , doc , lic ) : self . assert_package_exists ( ) if validations . validate_lics_from_file ( lic ) : doc . package . licenses_from_files . append ( lic ) return True else : raise SPDXValueError ( 'Package::LicensesFromFile' )
5197	def configure_database ( db_config ) : db_config . analog [ 1 ] . clazz = opendnp3 . PointClass . Class2 db_config . analog [ 1 ] . svariation = opendnp3 . StaticAnalogVariation . Group30Var1 db_config . analog [ 1 ] . evariation = opendnp3 . EventAnalogVariation . Group32Var7 db_config . analog [ 2 ] . clazz = opendnp3 . PointClass . Class2 db_config . analog [ 2 ] . svariation = opendnp3 . StaticAnalogVariation . Group30Var1 db_config . analog [ 2 ] . evariation = opendnp3 . EventAnalogVariation . Group32Var7 db_config . binary [ 1 ] . clazz = opendnp3 . PointClass . Class2 db_config . binary [ 1 ] . svariation = opendnp3 . StaticBinaryVariation . Group1Var2 db_config . binary [ 1 ] . evariation = opendnp3 . EventBinaryVariation . Group2Var2 db_config . binary [ 2 ] . clazz = opendnp3 . PointClass . Class2 db_config . binary [ 2 ] . svariation = opendnp3 . StaticBinaryVariation . Group1Var2 db_config . binary [ 2 ] . evariation = opendnp3 . EventBinaryVariation . Group2Var2
13249	def get_url_from_entry ( entry ) : if 'url' in entry . fields : return entry . fields [ 'url' ] elif entry . type . lower ( ) == 'docushare' : return 'https://ls.st/' + entry . fields [ 'handle' ] elif 'adsurl' in entry . fields : return entry . fields [ 'adsurl' ] elif 'doi' in entry . fields : return 'https://doi.org/' + entry . fields [ 'doi' ] else : raise NoEntryUrlError ( )
9223	def away_from_zero_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] >= 3 : p = 10 ** ndigits return float ( math . floor ( ( value * p ) + math . copysign ( 0.5 , value ) ) ) / p else : return round ( value , ndigits )
13032	def serve_forever ( self , poll_interval = 0.5 ) : logger . info ( 'Starting server on {}:{}...' . format ( self . server_name , self . server_port ) ) while True : try : self . poll_once ( poll_interval ) except ( KeyboardInterrupt , SystemExit ) : break self . handle_close ( ) logger . info ( 'Server stopped.' )
1361	def get_argument_endtime ( self ) : try : endtime = self . get_argument ( constants . PARAM_ENDTIME ) return endtime except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
967	def resetVector ( x1 , x2 ) : size = len ( x1 ) for i in range ( size ) : x2 [ i ] = x1 [ i ]
10435	def gettablerowindex ( self , window_name , object_name , row_text ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) index = 0 for cell in object_handle . AXRows : if re . match ( row_text , cell . AXChildren [ 0 ] . AXValue ) : return index index += 1 raise LdtpServerException ( u"Unable to find row: %s" % row_text )
1833	def JCXZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CX == 0 , target . read ( ) , cpu . PC )
8266	def _cache ( self ) : n = self . steps # Only one color in base list. if len ( self . _colors ) == 1 : ColorList . __init__ ( self , [ self . _colors [ 0 ] for i in _range ( n ) ] ) return # Expand the base list so we can chop more accurately. colors = self . _interpolate ( self . _colors , 40 ) # Chop into left half and right half. # Make sure their ending and beginning match colors. left = colors [ : len ( colors ) / 2 ] right = colors [ len ( colors ) / 2 : ] left . append ( right [ 0 ] ) right . insert ( 0 , left [ - 1 ] ) # Calculate left and right gradient proportionally to spread. gradient = self . _interpolate ( left , int ( n * self . spread ) ) [ : - 1 ] gradient . extend ( self . _interpolate ( right , n - int ( n * self . spread ) ) [ 1 : ] ) if self . spread > 1 : gradient = gradient [ : n ] if self . spread < 0 : gradient = gradient [ - n : ] ColorList . __init__ ( self , gradient )
5120	def next_event_description ( self ) : if self . _fancy_heap . size == 0 : event_type = 'Nothing' edge_index = None else : s = [ q . _key ( ) for q in self . edge2queue ] s . sort ( ) e = s [ 0 ] [ 1 ] q = self . edge2queue [ e ] event_type = 'Arrival' if q . next_event_description ( ) == 1 else 'Departure' edge_index = q . edge [ 2 ] return event_type , edge_index
12477	def merge ( dict_1 , dict_2 ) : return dict ( ( str ( key ) , dict_1 . get ( key ) or dict_2 . get ( key ) ) for key in set ( dict_2 ) | set ( dict_1 ) )
5419	def _google_v2_parse_arguments ( args ) : if ( args . zones and args . regions ) or ( not args . zones and not args . regions ) : raise ValueError ( 'Exactly one of --regions and --zones must be specified' ) if args . machine_type and ( args . min_cores or args . min_ram ) : raise ValueError ( '--machine-type not supported together with --min-cores or --min-ram.' )
11543	def set_analog_reference ( self , reference , pin = None ) : if pin is None : self . _set_analog_reference ( reference , None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : self . _set_analog_reference ( reference , pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
7947	def set_target ( self , stream ) : with self . lock : if self . _stream : raise ValueError ( "Target stream already set" ) self . _stream = stream self . _reader = StreamReader ( stream )
2385	def from_spec_resolver ( cls , spec_resolver ) : deref = DerefValidatorDecorator ( spec_resolver ) for key , validator_callable in iteritems ( cls . validators ) : yield key , deref ( validator_callable )
5654	def makedirs ( path ) : if not os . path . isdir ( path ) : os . makedirs ( path ) return path
9322	def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : self . refresh_information ( accept ) self . refresh_collections ( accept )
13459	def create_ical ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) # convert dates to datetimes. # when we change code to datetimes, we won't have to do this. start = event . start_date start = datetime . datetime ( start . year , start . month , start . day ) if event . end_date : end = event . end_date end = datetime . datetime ( end . year , end . month , end . day ) else : end = start cal = card_me . iCalendar ( ) cal . add ( 'method' ) . value = 'PUBLISH' vevent = cal . add ( 'vevent' ) vevent . add ( 'dtstart' ) . value = start vevent . add ( 'dtend' ) . value = end vevent . add ( 'dtstamp' ) . value = datetime . datetime . now ( ) vevent . add ( 'summary' ) . value = event . name response = HttpResponse ( cal . serialize ( ) , content_type = 'text/calendar' ) response [ 'Filename' ] = 'filename.ics' response [ 'Content-Disposition' ] = 'attachment; filename=filename.ics' return response
11618	def detect ( text ) : if sys . version_info < ( 3 , 0 ) : # Verify encoding try : text = text . decode ( 'utf-8' ) except UnicodeError : pass # Brahmic schemes are all within a specific range of code points. for L in text : code = ord ( L ) if code >= BRAHMIC_FIRST_CODE_POINT : for name , start_code in BLOCKS : if start_code <= code <= BRAHMIC_LAST_CODE_POINT : return name # Romanizations if Regex . IAST_OR_KOLKATA_ONLY . search ( text ) : if Regex . KOLKATA_ONLY . search ( text ) : return Scheme . Kolkata else : return Scheme . IAST if Regex . ITRANS_ONLY . search ( text ) : return Scheme . ITRANS if Regex . SLP1_ONLY . search ( text ) : return Scheme . SLP1 if Regex . VELTHUIS_ONLY . search ( text ) : return Scheme . Velthuis if Regex . ITRANS_OR_VELTHUIS_ONLY . search ( text ) : return Scheme . ITRANS return Scheme . HK
6392	def _get_qgrams ( self , src , tar , qval = 0 , skip = 0 ) : if isinstance ( src , Counter ) and isinstance ( tar , Counter ) : return src , tar if qval > 0 : return QGrams ( src , qval , '$#' , skip ) , QGrams ( tar , qval , '$#' , skip ) return Counter ( src . strip ( ) . split ( ) ) , Counter ( tar . strip ( ) . split ( ) )
8446	def switch ( template , version ) : temple . update . update ( new_template = template , new_version = version )
12670	def create ( _ ) : endpoint = client_endpoint ( ) if not endpoint : raise CLIError ( "Connection endpoint not found. " "Before running sfctl commands, connect to a cluster using " "the 'sfctl cluster select' command." ) no_verify = no_verify_setting ( ) if security_type ( ) == 'aad' : auth = AdalAuthentication ( no_verify ) else : cert = cert_info ( ) ca_cert = ca_cert_info ( ) auth = ClientCertAuthentication ( cert , ca_cert , no_verify ) return ServiceFabricClientAPIs ( auth , base_url = endpoint )
4452	def aggregate ( self , query ) : if isinstance ( query , AggregateRequest ) : has_schema = query . _with_schema has_cursor = bool ( query . _cursor ) cmd = [ self . AGGREGATE_CMD , self . index_name ] + query . build_args ( ) elif isinstance ( query , Cursor ) : has_schema = False has_cursor = True cmd = [ self . CURSOR_CMD , 'READ' , self . index_name ] + query . build_args ( ) else : raise ValueError ( 'Bad query' , query ) raw = self . redis . execute_command ( * cmd ) if has_cursor : if isinstance ( query , Cursor ) : query . cid = raw [ 1 ] cursor = query else : cursor = Cursor ( raw [ 1 ] ) raw = raw [ 0 ] else : cursor = None if query . _with_schema : schema = raw [ 0 ] rows = raw [ 2 : ] else : schema = None rows = raw [ 1 : ] res = AggregateResult ( rows , cursor , schema ) return res
2586	def _command_server ( self , kill_event ) : logger . debug ( "[COMMAND] Command Server Starting" ) while not kill_event . is_set ( ) : try : command_req = self . command_channel . recv_pyobj ( ) logger . debug ( "[COMMAND] Received command request: {}" . format ( command_req ) ) if command_req == "OUTSTANDING_C" : outstanding = self . pending_task_queue . qsize ( ) for manager in self . _ready_manager_queue : outstanding += len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) reply = outstanding elif command_req == "WORKERS" : num_workers = 0 for manager in self . _ready_manager_queue : num_workers += self . _ready_manager_queue [ manager ] [ 'worker_count' ] reply = num_workers elif command_req == "MANAGERS" : reply = [ ] for manager in self . _ready_manager_queue : resp = { 'manager' : manager . decode ( 'utf-8' ) , 'block_id' : self . _ready_manager_queue [ manager ] [ 'block_id' ] , 'worker_count' : self . _ready_manager_queue [ manager ] [ 'worker_count' ] , 'tasks' : len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) , 'active' : self . _ready_manager_queue [ manager ] [ 'active' ] } reply . append ( resp ) elif command_req . startswith ( "HOLD_WORKER" ) : cmd , s_manager = command_req . split ( ';' ) manager = s_manager . encode ( 'utf-8' ) logger . info ( "[CMD] Received HOLD_WORKER for {}" . format ( manager ) ) if manager in self . _ready_manager_queue : self . _ready_manager_queue [ manager ] [ 'active' ] = False reply = True else : reply = False elif command_req == "SHUTDOWN" : logger . info ( "[CMD] Received SHUTDOWN command" ) kill_event . set ( ) reply = True else : reply = None logger . debug ( "[COMMAND] Reply: {}" . format ( reply ) ) self . command_channel . send_pyobj ( reply ) except zmq . Again : logger . debug ( "[COMMAND] is alive" ) continue
11294	def main ( path ) : basepath = os . path . abspath ( os . path . expanduser ( str ( path ) ) ) echo . h2 ( "Available scripts in {}" . format ( basepath ) ) echo . br ( ) for root_dir , dirs , files in os . walk ( basepath , topdown = True ) : for f in fnmatch . filter ( files , '*.py' ) : try : filepath = os . path . join ( root_dir , f ) # super edge case, this makes sure the python script won't start # an interactive console session which would cause the session # to start and not allow the for loop to complete with open ( filepath , encoding = "UTF-8" ) as fp : body = fp . read ( ) is_console = "InteractiveConsole" in body is_console = is_console or "code" in body is_console = is_console and "interact(" in body if is_console : continue s = captain . Script ( filepath ) if s . can_run_from_cli ( ) : rel_filepath = s . call_path ( basepath ) p = s . parser echo . h3 ( rel_filepath ) desc = p . description if desc : echo . indent ( desc , indent = ( " " * 4 ) ) subcommands = s . subcommands if subcommands : echo . br ( ) echo . indent ( "Subcommands:" , indent = ( " " * 4 ) ) for sc in subcommands . keys ( ) : echo . indent ( sc , indent = ( " " * 6 ) ) echo . br ( ) except captain . ParseError : pass except Exception as e : #echo.exception(e) #echo.err("Failed to parse {} because {}", f, e.message) echo . err ( "Failed to parse {}" , f ) echo . verbose ( e . message ) echo . br ( )
4193	def plot_frequencies ( self , mindB = None , maxdB = None , norm = True ) : from pylab import plot , title , xlim , grid , ylim , xlabel , ylabel # recompute the response self . compute_response ( norm = norm ) plot ( self . frequencies , self . response ) title ( "ENBW=%2.1f" % ( self . enbw ) ) ylabel ( 'Frequency response (dB)' ) xlabel ( 'Fraction of sampling frequency' ) # define the plot limits xlim ( - 0.5 , 0.5 ) y0 , y1 = ylim ( ) if mindB : y0 = mindB if maxdB is not None : y1 = maxdB else : y1 = max ( self . response ) ylim ( y0 , y1 ) grid ( True )
1813	def SETNBE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , 1 , 0 ) )
10004	def rename ( self , name ) : if is_valid_name ( name ) : if name not in self . system . models : self . name = name return True # Rename success else : # Model name already exists return False else : raise ValueError ( "Invalid name '%s'." % name )
5548	def validate_values ( config , values ) : if not isinstance ( config , dict ) : raise TypeError ( "config must be a dictionary" ) for value , vtype in values : if value not in config : raise ValueError ( "%s not given" % value ) if not isinstance ( config [ value ] , vtype ) : raise TypeError ( "%s must be %s" % ( value , vtype ) ) return True
12891	def handle_int ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u8 . text ) or None
8761	def delete_subnet ( context , id ) : LOG . info ( "delete_subnet %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : subnet = db_api . subnet_find ( context , id = id , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( subnet . network_id ) : if subnet . tenant_id == context . tenant_id : # A tenant can't delete subnets on provider network raise n_exc . NotAuthorized ( subnet_id = id ) else : # Raise a NotFound here because the foreign tenant # does not have to know about other tenant's subnet # existence. raise n_exc . SubnetNotFound ( subnet_id = id ) _delete_subnet ( context , subnet )
12700	def get_i_name ( self , num , is_oai = None ) : if num not in ( 1 , 2 ) : raise ValueError ( "`num` parameter have to be 1 or 2!" ) if is_oai is None : is_oai = self . oai_marc i_name = "ind" if not is_oai else "i" return i_name + str ( num )
10400	def done_chomping ( self ) -> bool : return self . tag in self . graph . nodes [ self . target_node ]
1239	def move ( self , external_index , new_priority ) : index = external_index + ( self . _capacity - 1 ) return self . _move ( index , new_priority )
12200	def from_jsonfile ( cls , fp , selector_handler = None , strict = False , debug = False ) : return cls . _from_jsonlines ( fp , selector_handler = selector_handler , strict = strict , debug = debug )
3012	def locked_put ( self , credentials ) : filters = { self . key_name : self . key_value } query = self . session . query ( self . model_class ) . filter_by ( * * filters ) entity = query . first ( ) if not entity : entity = self . model_class ( * * filters ) setattr ( entity , self . property_name , credentials ) self . session . add ( entity )
4458	def sort_by ( self , field , asc = True ) : self . _sortby = SortbyField ( field , asc ) return self
9955	def custom_showtraceback ( self , exc_tuple = None , filename = None , tb_offset = None , exception_only = False , running_compiled_code = False , ) : self . default_showtraceback ( exc_tuple , filename , tb_offset , exception_only = True , running_compiled_code = running_compiled_code , )
12782	def set_topic ( self , topic ) : if not topic : topic = '' result = self . _connection . put ( "room/%s" % self . id , { "room" : { "topic" : topic } } ) if result [ "success" ] : self . _load ( ) return result [ "success" ]
75	def DirectedEdgeDetect ( alpha = 0 , direction = ( 0.0 , 1.0 ) , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) direction_param = iap . handle_continuous_param ( direction , "direction" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) direction_sample = direction_param . draw_sample ( random_state = random_state_func ) deg = int ( direction_sample * 360 ) % 360 rad = np . deg2rad ( deg ) x = np . cos ( rad - 0.5 * np . pi ) y = np . sin ( rad - 0.5 * np . pi ) direction_vector = np . array ( [ x , y ] ) matrix_effect = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) for x in [ - 1 , 0 , 1 ] : for y in [ - 1 , 0 , 1 ] : if ( x , y ) != ( 0 , 0 ) : cell_vector = np . array ( [ x , y ] ) distance_deg = np . rad2deg ( ia . angle_between_vectors ( cell_vector , direction_vector ) ) distance = distance_deg / 180 similarity = ( 1 - distance ) ** 4 matrix_effect [ y + 1 , x + 1 ] = similarity matrix_effect = matrix_effect / np . sum ( matrix_effect ) matrix_effect = matrix_effect * ( - 1 ) matrix_effect [ 1 , 1 ] = 1 matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
4450	def info ( self ) : res = self . redis . execute_command ( 'FT.INFO' , self . index_name ) it = six . moves . map ( to_string , res ) return dict ( six . moves . zip ( it , it ) )
13576	def submit ( course , tid = None , pastebin = False , review = False ) : if tid is not None : return submit_exercise ( Exercise . byid ( tid ) , pastebin = pastebin , request_review = review ) else : sel = Exercise . get_selected ( ) if not sel : raise NoExerciseSelected ( ) return submit_exercise ( sel , pastebin = pastebin , request_review = review )
289	def plot_rolling_returns ( returns , factor_returns = None , live_start_date = None , logy = False , cone_std = None , legend_loc = 'best' , volatility_match = False , cone_function = timeseries . forecast_cone_bootstrap , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Cumulative returns' ) ax . set_yscale ( 'log' if logy else 'linear' ) if volatility_match and factor_returns is None : raise ValueError ( 'volatility_match requires passing of ' 'factor_returns.' ) elif volatility_match and factor_returns is not None : bmark_vol = factor_returns . loc [ returns . index ] . std ( ) returns = ( returns / returns . std ( ) ) * bmark_vol cum_rets = ep . cum_returns ( returns , 1.0 ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) if factor_returns is not None : cum_factor_returns = ep . cum_returns ( factor_returns [ cum_rets . index ] , 1.0 ) cum_factor_returns . plot ( lw = 2 , color = 'gray' , label = factor_returns . name , alpha = 0.60 , ax = ax , * * kwargs ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_cum_returns = cum_rets . loc [ cum_rets . index < live_start_date ] oos_cum_returns = cum_rets . loc [ cum_rets . index >= live_start_date ] else : is_cum_returns = cum_rets oos_cum_returns = pd . Series ( [ ] ) is_cum_returns . plot ( lw = 3 , color = 'forestgreen' , alpha = 0.6 , label = 'Backtest' , ax = ax , * * kwargs ) if len ( oos_cum_returns ) > 0 : oos_cum_returns . plot ( lw = 4 , color = 'red' , alpha = 0.6 , label = 'Live' , ax = ax , * * kwargs ) if cone_std is not None : if isinstance ( cone_std , ( float , int ) ) : cone_std = [ cone_std ] is_returns = returns . loc [ returns . index < live_start_date ] cone_bounds = cone_function ( is_returns , len ( oos_cum_returns ) , cone_std = cone_std , starting_value = is_cum_returns [ - 1 ] ) cone_bounds = cone_bounds . set_index ( oos_cum_returns . index ) for std in cone_std : ax . fill_between ( cone_bounds . index , cone_bounds [ float ( std ) ] , cone_bounds [ float ( - std ) ] , color = 'steelblue' , alpha = 0.5 ) if legend_loc is not None : ax . legend ( loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . axhline ( 1.0 , linestyle = '--' , color = 'black' , lw = 2 ) return ax
11006	def get_active_bets ( self , project_id = None ) : url = urljoin ( self . settings [ 'bets_url' ] , 'bets?state=fresh,active,accept_end&page=1&page_size=100' ) if project_id is not None : url += '&kava_project_id={}' . format ( project_id ) bets = [ ] has_next_page = True while has_next_page : res = self . _req ( url ) bets . extend ( res [ 'bets' ] [ 'results' ] ) url = res [ 'bets' ] . get ( 'next' ) has_next_page = bool ( url ) return bets
9906	def parse ( self , ping_message ) : try : # accept PingResult instance as an input if typepy . is_not_null_string ( ping_message . stdout ) : ping_message = ping_message . stdout except AttributeError : pass logger . debug ( "parsing ping result: {}" . format ( ping_message ) ) self . __parser = NullPingParser ( ) if typepy . is_null_string ( ping_message ) : logger . debug ( "ping_message is empty" ) self . __stats = PingStats ( ) return self . __stats ping_lines = _to_unicode ( ping_message ) . splitlines ( ) parser_class_list = ( LinuxPingParser , WindowsPingParser , MacOsPingParser , AlpineLinuxPingParser , ) for parser_class in parser_class_list : self . __parser = parser_class ( ) try : self . __stats = self . __parser . parse ( ping_lines ) return self . __stats except ParseError as e : if e . reason != ParseErrorReason . HEADER_NOT_FOUND : raise e except pp . ParseException : pass self . __parser = NullPingParser ( ) return self . __stats
7725	def __from_xmlnode ( self , xmlnode ) : actor = None reason = None n = xmlnode . children while n : ns = n . ns ( ) if ns and ns . getContent ( ) != MUC_USER_NS : continue if n . name == "actor" : actor = n . getContent ( ) if n . name == "reason" : reason = n . getContent ( ) n = n . next self . __init ( from_utf8 ( xmlnode . prop ( "affiliation" ) ) , from_utf8 ( xmlnode . prop ( "role" ) ) , from_utf8 ( xmlnode . prop ( "jid" ) ) , from_utf8 ( xmlnode . prop ( "nick" ) ) , from_utf8 ( actor ) , from_utf8 ( reason ) , )
12407	def cons ( collection , value ) : if isinstance ( value , collections . Mapping ) : if collection is None : collection = { } collection . update ( * * value ) elif isinstance ( value , six . string_types ) : if collection is None : collection = [ ] collection . append ( value ) elif isinstance ( value , collections . Iterable ) : if collection is None : collection = [ ] collection . extend ( value ) else : if collection is None : collection = [ ] collection . append ( value ) return collection
5226	def _to_gen_ ( iterable ) : from collections import Iterable for elm in iterable : if isinstance ( elm , Iterable ) and not isinstance ( elm , ( str , bytes ) ) : yield from flatten ( elm ) else : yield elm
13811	def GetTopLevelContainingType ( self ) : desc = self while desc . containing_type is not None : desc = desc . containing_type return desc
9583	def write_var_header ( fd , header ) : # write tag bytes, # and array flags + class and nzmax (null bytes) fd . write ( struct . pack ( 'b3xI' , etypes [ 'miUINT32' ] [ 'n' ] , 8 ) ) fd . write ( struct . pack ( 'b3x4x' , mclasses [ header [ 'mclass' ] ] ) ) # write dimensions array write_elements ( fd , 'miINT32' , header [ 'dims' ] ) # write var name write_elements ( fd , 'miINT8' , asbytes ( header [ 'name' ] ) , is_name = True )
7480	def sub_build_clustbits ( data , usort , nseeds ) : ## load FULL concat fasta file into a dict. This could cause RAM issues. ## this file has iupac codes in it, not ambigs resolved, and is gzipped. LOGGER . info ( "loading full _catcons file into memory" ) allcons = { } conshandle = os . path . join ( data . dirs . across , data . name + "_catcons.tmp" ) with gzip . open ( conshandle , 'rb' ) as iocons : cons = itertools . izip ( * [ iter ( iocons ) ] * 2 ) for namestr , seq in cons : nnn , sss = [ i . strip ( ) for i in namestr , seq ] allcons [ nnn [ 1 : ] ] = sss ## set optim to approximately 4 chunks per core. Smaller allows for a bit ## cleaner looking progress bar. 40 cores will make 160 files. optim = ( ( nseeds // ( data . cpus * 4 ) ) + ( nseeds % ( data . cpus * 4 ) ) ) LOGGER . info ( "building clustbits, optim=%s, nseeds=%s, cpus=%s" , optim , nseeds , data . cpus ) ## iterate through usort grabbing seeds and matches with open ( usort , 'rb' ) as insort : ## iterator, seed null, and seqlist null isort = iter ( insort ) loci = 0 lastseed = 0 fseqs = [ ] seqlist = [ ] seqsize = 0 while 1 : ## grab the next line try : hit , seed , ori = isort . next ( ) . strip ( ) . split ( ) except StopIteration : break try : ## if same seed, append match if seed != lastseed : ## store the last fseq, count it, and clear it if fseqs : seqlist . append ( "\n" . join ( fseqs ) ) seqsize += 1 fseqs = [ ] ## occasionally write to file if seqsize >= optim : if seqlist : loci += seqsize with open ( os . path . join ( data . tmpdir , data . name + ".chunk_{}" . format ( loci ) ) , 'w' ) as clustsout : LOGGER . debug ( "writing chunk - seqsize {} loci {} {}" . format ( seqsize , loci , clustsout . name ) ) clustsout . write ( "\n//\n//\n" . join ( seqlist ) + "\n//\n//\n" ) ## reset list and counter seqlist = [ ] seqsize = 0 ## store the new seed on top of fseq fseqs . append ( ">{}\n{}" . format ( seed , allcons [ seed ] ) ) lastseed = seed ## add match to the seed seq = allcons [ hit ] ## revcomp if orientation is reversed if ori == "-" : seq = fullcomp ( seq ) [ : : - 1 ] fseqs . append ( ">{}\n{}" . format ( hit , seq ) ) except KeyError as inst : ## Caught bad seed or hit? Log and continue. LOGGER . error ( "Bad Seed/Hit: seqsize {}\tloci {}\tseed {}\thit {}" . format ( seqsize , loci , seed , hit ) ) ## write whatever is left over to the clusts file if fseqs : seqlist . append ( "\n" . join ( fseqs ) ) seqsize += 1 loci += seqsize if seqlist : with open ( os . path . join ( data . tmpdir , data . name + ".chunk_{}" . format ( loci ) ) , 'w' ) as clustsout : clustsout . write ( "\n//\n//\n" . join ( seqlist ) + "\n//\n//\n" ) ## final progress and cleanup del allcons clustbits = glob . glob ( os . path . join ( data . tmpdir , data . name + ".chunk_*" ) ) ## return stuff return clustbits , loci
3858	def _on_watermark_notification ( self , notif ) : # Update the conversation: if self . get_user ( notif . user_id ) . is_self : logger . info ( 'latest_read_timestamp for {} updated to {}' . format ( self . id_ , notif . read_timestamp ) ) self_conversation_state = ( self . _conversation . self_conversation_state ) self_conversation_state . self_read_state . latest_read_timestamp = ( parsers . to_timestamp ( notif . read_timestamp ) ) # Update the participants' watermarks: previous_timestamp = self . _watermarks . get ( notif . user_id , datetime . datetime . min . replace ( tzinfo = datetime . timezone . utc ) ) if notif . read_timestamp > previous_timestamp : logger . info ( ( 'latest_read_timestamp for conv {} participant {}' + ' updated to {}' ) . format ( self . id_ , notif . user_id . chat_id , notif . read_timestamp ) ) self . _watermarks [ notif . user_id ] = notif . read_timestamp
4110	def ac2poly ( data ) : a , e , _c = LEVINSON ( data ) a = numpy . insert ( a , 0 , 1 ) return a , e
4010	def get_docker_client ( ) : env = get_docker_env ( ) host , cert_path , tls_verify = env [ 'DOCKER_HOST' ] , env [ 'DOCKER_CERT_PATH' ] , env [ 'DOCKER_TLS_VERIFY' ] params = { 'base_url' : host . replace ( 'tcp://' , 'https://' ) , 'timeout' : None , 'version' : 'auto' } if tls_verify and cert_path : params [ 'tls' ] = docker . tls . TLSConfig ( client_cert = ( os . path . join ( cert_path , 'cert.pem' ) , os . path . join ( cert_path , 'key.pem' ) ) , ca_cert = os . path . join ( cert_path , 'ca.pem' ) , verify = True , ssl_version = None , assert_hostname = False ) return docker . Client ( * * params )
1496	def get_sub_parts ( self , query ) : parts = [ ] num_open_braces = 0 delimiter = ',' last_starting_index = 0 for i in range ( len ( query ) ) : if query [ i ] == '(' : num_open_braces += 1 elif query [ i ] == ')' : num_open_braces -= 1 elif query [ i ] == delimiter and num_open_braces == 0 : parts . append ( query [ last_starting_index : i ] . strip ( ) ) last_starting_index = i + 1 parts . append ( query [ last_starting_index : ] . strip ( ) ) return parts
7127	def find_and_patch_entry ( soup , entry ) : link = soup . find ( "a" , { "class" : "headerlink" } , href = "#" + entry . anchor ) tag = soup . new_tag ( "a" ) tag [ "name" ] = APPLE_REF_TEMPLATE . format ( entry . type , entry . name ) if link : link . parent . insert ( 0 , tag ) return True elif entry . anchor . startswith ( "module-" ) : soup . h1 . parent . insert ( 0 , tag ) return True else : return False
11974	def convert_nm ( nm , notation = IP_DOT , inotation = IP_UNKNOWN , check = True ) : return _convert ( nm , notation , inotation , _check = check , _isnm = True )
8192	def nodes_by_category ( self , category ) : return [ n for n in self . nodes if n . category == category ]
8513	def fit_and_score_estimator ( estimator , parameters , cv , X , y = None , scoring = None , iid = True , n_jobs = 1 , verbose = 1 , pre_dispatch = '2*n_jobs' ) : scorer = check_scoring ( estimator , scoring = scoring ) n_samples = num_samples ( X ) X , y = check_arrays ( X , y , allow_lists = True , sparse_format = 'csr' , allow_nans = True ) if y is not None : if len ( y ) != n_samples : raise ValueError ( 'Target variable (y) has a different number ' 'of samples (%i) than data (X: %i samples)' % ( len ( y ) , n_samples ) ) cv = check_cv ( cv = cv , y = y , classifier = is_classifier ( estimator ) ) out = Parallel ( n_jobs = n_jobs , verbose = verbose , pre_dispatch = pre_dispatch ) ( delayed ( _fit_and_score ) ( clone ( estimator ) , X , y , scorer , train , test , verbose , parameters , fit_params = None ) for train , test in cv . split ( X , y ) ) assert len ( out ) == cv . n_splits train_scores , test_scores = [ ] , [ ] n_train_samples , n_test_samples = [ ] , [ ] for test_score , n_test , train_score , n_train , _ in out : train_scores . append ( train_score ) test_scores . append ( test_score ) n_test_samples . append ( n_test ) n_train_samples . append ( n_train ) train_scores , test_scores = map ( list , check_arrays ( train_scores , test_scores , warn_nans = True , replace_nans = True ) ) if iid : if verbose > 0 and is_msmbuilder_estimator ( estimator ) : print ( '[CV] Using MSMBuilder API n_samples averaging' ) print ( '[CV] n_train_samples: %s' % str ( n_train_samples ) ) print ( '[CV] n_test_samples: %s' % str ( n_test_samples ) ) mean_test_score = np . average ( test_scores , weights = n_test_samples ) mean_train_score = np . average ( train_scores , weights = n_train_samples ) else : mean_test_score = np . average ( test_scores ) mean_train_score = np . average ( train_scores ) grid_scores = { 'mean_test_score' : mean_test_score , 'test_scores' : test_scores , 'mean_train_score' : mean_train_score , 'train_scores' : train_scores , 'n_test_samples' : n_test_samples , 'n_train_samples' : n_train_samples } return grid_scores
11667	def linear ( Ks , dim , num_q , rhos , nus ) : return _get_linear ( Ks , dim ) ( num_q , rhos , nus )
942	def _runExperimentImpl ( options , model = None ) : json_helpers . validate ( options . privateOptions , schemaDict = g_parsedPrivateCommandLineOptionsSchema ) # Load the experiment's description.py module experimentDir = options . experimentDir descriptionPyModule = helpers . loadExperimentDescriptionScriptFromDir ( experimentDir ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) # Handle "list checkpoints" request if options . privateOptions [ 'listAvailableCheckpoints' ] : _printAvailableCheckpoints ( experimentDir ) return None # Load experiment tasks experimentTasks = expIface . getModelControl ( ) . get ( 'tasks' , [ ] ) # If the tasks list is empty, and this is a nupic environment description # file being run from the OPF, convert it to a simple OPF description file. if ( len ( experimentTasks ) == 0 and expIface . getModelControl ( ) [ 'environment' ] == OpfEnvironment . Nupic ) : expIface . convertNupicEnvToOPF ( ) experimentTasks = expIface . getModelControl ( ) . get ( 'tasks' , [ ] ) # Ensures all the source locations are either absolute paths or relative to # the nupic.datafiles package_data location. expIface . normalizeStreamSources ( ) # Extract option newSerialization = options . privateOptions [ 'newSerialization' ] # Handle listTasks if options . privateOptions [ 'listTasks' ] : print "Available tasks:" for label in [ t [ 'taskLabel' ] for t in experimentTasks ] : print "\t" , label return None # Construct the experiment instance if options . privateOptions [ 'runCheckpointName' ] : assert model is None checkpointName = options . privateOptions [ 'runCheckpointName' ] model = ModelFactory . loadFromCheckpoint ( savedModelDir = _getModelCheckpointDir ( experimentDir , checkpointName ) , newSerialization = newSerialization ) elif model is not None : print "Skipping creation of OPFExperiment instance: caller provided his own" else : modelDescription = expIface . getModelDescription ( ) model = ModelFactory . create ( modelDescription ) # Handle "create model" request if options . privateOptions [ 'createCheckpointName' ] : checkpointName = options . privateOptions [ 'createCheckpointName' ] _saveModel ( model = model , experimentDir = experimentDir , checkpointLabel = checkpointName , newSerialization = newSerialization ) return model # Build the task list # Default task execution index list is in the natural list order of the tasks taskIndexList = range ( len ( experimentTasks ) ) customTaskExecutionLabelsList = options . privateOptions [ 'taskLabels' ] if customTaskExecutionLabelsList : taskLabelsList = [ t [ 'taskLabel' ] for t in experimentTasks ] taskLabelsSet = set ( taskLabelsList ) customTaskExecutionLabelsSet = set ( customTaskExecutionLabelsList ) assert customTaskExecutionLabelsSet . issubset ( taskLabelsSet ) , ( "Some custom-provided task execution labels don't correspond " "to actual task labels: mismatched labels: %r; actual task " "labels: %r." ) % ( customTaskExecutionLabelsSet - taskLabelsSet , customTaskExecutionLabelsList ) taskIndexList = [ taskLabelsList . index ( label ) for label in customTaskExecutionLabelsList ] print "#### Executing custom task list: %r" % [ taskLabelsList [ i ] for i in taskIndexList ] # Run all experiment tasks for taskIndex in taskIndexList : task = experimentTasks [ taskIndex ] # Create a task runner and run it! taskRunner = _TaskRunner ( model = model , task = task , cmdOptions = options ) taskRunner . run ( ) del taskRunner if options . privateOptions [ 'checkpointModel' ] : _saveModel ( model = model , experimentDir = experimentDir , checkpointLabel = task [ 'taskLabel' ] , newSerialization = newSerialization ) return model
52	def on ( self , image ) : shape = normalize_shape ( image ) if shape [ 0 : 2 ] == self . shape [ 0 : 2 ] : return self . deepcopy ( ) else : keypoints = [ kp . project ( self . shape , shape ) for kp in self . keypoints ] return self . deepcopy ( keypoints , shape )
7039	def object_info ( lcc_server , objectid , db_collection_id ) : urlparams = { 'objectid' : objectid , 'collection' : db_collection_id } urlqs = urlencode ( urlparams ) url = '%s/api/object?%s' % ( lcc_server , urlqs ) try : LOGINFO ( 'getting info for %s in collection %s from %s' % ( objectid , db_collection_id , lcc_server ) ) # check if we have an API key already have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) # if not, get a new one if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) # if apikey is not None, add it in as an Authorization: Bearer [apikey] # header if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } # hit the server req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) objectinfo = json . loads ( resp . read ( ) ) [ 'result' ] return objectinfo except HTTPError as e : if e . code == 404 : LOGERROR ( 'additional info for object %s not ' 'found in collection: %s' % ( objectid , db_collection_id ) ) else : LOGERROR ( 'could not retrieve object info, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
13740	def connect ( self ) : if not self . connected ( ) : self . _ws = create_connection ( self . WS_URI ) message = { 'type' : self . WS_TYPE , 'product_id' : self . WS_PRODUCT_ID } self . _ws . send ( dumps ( message ) ) # There will be only one keep alive thread per client instance with self . _lock : if not self . _thread : thread = Thread ( target = self . _keep_alive_thread , args = [ ] ) thread . start ( )
9751	def find_fann ( ) : # FANN possible libs directories (as $LD_LIBRARY_PATH), also includes # pkgsrc framework support. if sys . platform == "win32" : dirs = sys . path for ver in dirs : if os . path . isdir ( ver ) : if find_x ( ver ) : return True raise Exception ( "Couldn't find FANN source libs!" ) else : dirs = [ '/lib' , '/usr/lib' , '/usr/lib64' , '/usr/local/lib' , '/usr/pkg/lib' ] for path in dirs : if os . path . isdir ( path ) : if find_x ( path ) : return True raise Exception ( "Couldn't find FANN source libs!" )
4432	def unregister_hook ( self , func ) : if func in self . hooks : self . hooks . remove ( func )
11007	def get_bets ( self , type = None , order_by = None , state = None , project_id = None , page = None , page_size = None ) : if page is None : page = 1 if page_size is None : page_size = 100 if state == 'all' : _states = [ ] # all states == no filter elif state == 'closed' : _states = self . CLOSED_STATES else : _states = self . ACTIVE_STATES url = urljoin ( self . settings [ 'bets_url' ] , 'bets?page={}&page_size={}' . format ( page , page_size ) ) url += '&state={}' . format ( ',' . join ( _states ) ) if type is not None : url += '&type={}' . format ( type ) if order_by in [ '-last_stake' , 'last_stake' ] : url += '&order_by={}' . format ( order_by ) if project_id is not None : url += '&kava_project_id={}' . format ( project_id ) res = self . _req ( url ) return res [ 'bets' ] [ 'results' ]
9123	def belns ( keyword : str , file : TextIO , encoding : Optional [ str ] , use_names : bool ) : directory = get_data_dir ( keyword ) obo_url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo_path = os . path . join ( directory , f'{keyword}.obo' ) obo_cache_path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo_getter = make_obo_getter ( obo_url , obo_path , preparsed_path = obo_cache_path ) graph = obo_getter ( ) convert_obo_graph_to_belns ( graph , file = file , encoding = encoding , use_names = use_names , )
1898	def _getvalue ( self , expression ) : if not issymbolic ( expression ) : return expression assert isinstance ( expression , Variable ) if isinstance ( expression , Array ) : result = bytearray ( ) for c in expression : expression_str = translate_to_smtlib ( c ) self . _send ( '(get-value (%s))' % expression_str ) response = self . _recv ( ) result . append ( int ( '0x{:s}' . format ( response . split ( expression_str ) [ 1 ] [ 3 : - 2 ] ) , 16 ) ) return bytes ( result ) else : self . _send ( '(get-value (%s))' % expression . name ) ret = self . _recv ( ) assert ret . startswith ( '((' ) and ret . endswith ( '))' ) , ret if isinstance ( expression , Bool ) : return { 'true' : True , 'false' : False } [ ret [ 2 : - 2 ] . split ( ' ' ) [ 1 ] ] elif isinstance ( expression , BitVec ) : pattern , base = self . _get_value_fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) return int ( value , base ) raise NotImplementedError ( "_getvalue only implemented for Bool and BitVec" )
7955	def getpeercert ( self ) : with self . lock : if not self . _socket or self . _tls_state != "connected" : raise ValueError ( "Not TLS-connected" ) return get_certificate_from_ssl_socket ( self . _socket )
10206	def run ( self , start_date = None , end_date = None , * * kwargs ) : start_date = self . extract_date ( start_date ) if start_date else None end_date = self . extract_date ( end_date ) if end_date else None self . validate_arguments ( start_date , end_date , * * kwargs ) agg_query = self . build_query ( start_date , end_date , * * kwargs ) query_result = agg_query . execute ( ) . to_dict ( ) res = self . process_query_result ( query_result , start_date , end_date ) return res
9575	def read_elements ( fd , endian , mtps , is_name = False ) : mtpn , num_bytes , data = read_element_tag ( fd , endian ) if mtps and mtpn not in [ etypes [ mtp ] [ 'n' ] for mtp in mtps ] : raise ParseError ( 'Got type {}, expected {}' . format ( mtpn , ' / ' . join ( '{} ({})' . format ( etypes [ mtp ] [ 'n' ] , mtp ) for mtp in mtps ) ) ) if not data : # full format, read data data = fd . read ( num_bytes ) # Seek to next 64-bit boundary mod8 = num_bytes % 8 if mod8 : fd . seek ( 8 - mod8 , 1 ) # parse data and return values if is_name : # names are stored as miINT8 bytes fmt = 's' val = [ unpack ( endian , fmt , s ) for s in data . split ( b'\0' ) if s ] if len ( val ) == 0 : val = '' elif len ( val ) == 1 : val = asstr ( val [ 0 ] ) else : val = [ asstr ( s ) for s in val ] else : fmt = etypes [ inv_etypes [ mtpn ] ] [ 'fmt' ] val = unpack ( endian , fmt , data ) return val
3927	def set_tab ( self , widget , switch = False , title = None ) : if widget not in self . _widgets : self . _widgets . append ( widget ) self . _widget_title [ widget ] = '' if switch : self . _tab_index = self . _widgets . index ( widget ) if title : self . _widget_title [ widget ] = title self . _update_tabs ( )
12326	def init ( globalvars = None , show = False ) : global config profileini = getprofileini ( ) if os . path . exists ( profileini ) : config = configparser . ConfigParser ( ) config . read ( profileini ) mgr = plugins_get_mgr ( ) mgr . update_configs ( config ) if show : for source in config : print ( "[%s] :" % ( source ) ) for k in config [ source ] : print ( " %s : %s" % ( k , config [ source ] [ k ] ) ) else : print ( "Profile does not exist. So creating one" ) if not show : update ( globalvars ) print ( "Complete init" )
5181	def _url ( self , endpoint , path = None ) : log . debug ( '_url called with endpoint: {0} and path: {1}' . format ( endpoint , path ) ) try : endpoint = ENDPOINTS [ endpoint ] except KeyError : # If we reach this we're trying to query an endpoint that doesn't # exist. This shouldn't happen unless someone made a booboo. raise APIError url = '{base_url}/{endpoint}' . format ( base_url = self . base_url , endpoint = endpoint , ) if path is not None : url = '{0}/{1}' . format ( url , quote ( path ) ) return url
10639	def extract ( self , other ) : # Extract the specified mass flow rate. if type ( other ) is float or type ( other ) is numpy . float64 or type ( other ) is numpy . float32 : return self . _extract_mfr ( other ) # Extract the specified mass flow rateof the specified compound. elif self . _is_compound_mfr_tuple ( other ) : return self . _extract_compound_mfr ( other [ 0 ] , other [ 1 ] ) # Extract all of the specified compound. elif type ( other ) is str : return self . _extract_compound ( other ) # TODO: Test # Extract all of the compounds of the specified material. elif type ( other ) is Material : return self . _extract_material ( other ) # If not one of the above, it must be an invalid argument. else : raise TypeError ( "Invalid extraction argument." )
2390	def count_list ( the_list ) : count = the_list . count result = [ ( item , count ( item ) ) for item in set ( the_list ) ] result . sort ( ) return result
4585	def animated_gif_to_colorlists ( image , container = list ) : deprecated . deprecated ( 'util.gif.animated_gif_to_colorlists' ) from PIL import ImageSequence it = ImageSequence . Iterator ( image ) return [ image_to_colorlist ( i , container ) for i in it ]
8030	def groupByContent ( paths ) : handles , results = [ ] , [ ] # Silently ignore files we don't have permission to read. hList = [ ] for path in paths : try : hList . append ( ( path , open ( path , 'rb' ) , '' ) ) except IOError : pass # TODO: Verbose-mode output here. handles . append ( hList ) while handles : # Process more blocks. more , done = compareChunks ( handles . pop ( 0 ) ) # Add the results to the top-level lists. handles . extend ( more ) results . extend ( done ) # Keep the same API as the others. return dict ( ( x [ 0 ] , x ) for x in results )
11129	def stats ( cls , traces ) : data = { } stats = { } # Group traces by key and minute for trace in traces : key = trace [ 'key' ] if key not in data : data [ key ] = [ ] stats [ key ] = { } data [ key ] . append ( trace [ 'total_time' ] ) cls . _traces . pop ( trace [ 'id' ] ) for key in data : times = data [ key ] stats [ key ] = dict ( count = len ( times ) , max = max ( times ) , min = min ( times ) , avg = sum ( times ) / len ( times ) ) return stats
6903	def angle_wrap ( angle , radians = False ) : if radians : wrapped = angle % ( 2.0 * pi_value ) if wrapped < 0.0 : wrapped = 2.0 * pi_value + wrapped else : wrapped = angle % 360.0 if wrapped < 0.0 : wrapped = 360.0 + wrapped return wrapped
3925	def _update_tabs ( self ) : text = [ ] for num , widget in enumerate ( self . _widgets ) : palette = ( 'active_tab' if num == self . _tab_index else 'inactive_tab' ) text += [ ( palette , ' {} ' . format ( self . _widget_title [ widget ] ) ) , ( 'tab_background' , ' ' ) , ] self . _tabs . set_text ( text ) self . _frame . contents [ 'body' ] = ( self . _widgets [ self . _tab_index ] , None )
10612	def _calculate_T ( self , H ) : # Create the initial guesses for temperature. x = list ( ) x . append ( self . _T ) x . append ( self . _T + 10.0 ) # Evaluate the enthalpy for the initial guesses. y = list ( ) y . append ( self . _calculate_H ( x [ 0 ] ) - H ) y . append ( self . _calculate_H ( x [ 1 ] ) - H ) # Solve for temperature. for i in range ( 2 , 50 ) : x . append ( x [ i - 1 ] - y [ i - 1 ] * ( ( x [ i - 1 ] - x [ i - 2 ] ) / ( y [ i - 1 ] - y [ i - 2 ] ) ) ) y . append ( self . _calculate_H ( x [ i ] ) - H ) if abs ( y [ i - 1 ] ) < 1.0e-5 : break return x [ len ( x ) - 1 ]
10508	def log ( self , message , level = logging . DEBUG ) : if _ldtp_debug : print ( message ) self . logger . log ( level , str ( message ) ) return 1
10565	def exclude_filepaths ( filepaths , exclude_patterns = None ) : if not exclude_patterns : return filepaths , [ ] exclude_re = re . compile ( "|" . join ( pattern for pattern in exclude_patterns ) ) included_songs = [ ] excluded_songs = [ ] for filepath in filepaths : if exclude_patterns and exclude_re . search ( filepath ) : excluded_songs . append ( filepath ) else : included_songs . append ( filepath ) return included_songs , excluded_songs
11304	def embed ( self , url , * * kwargs ) : try : # first figure out the provider provider = self . provider_for_url ( url ) except OEmbedMissingEndpoint : raise else : try : # check the database for a cached response, because of certain # race conditions that exist with get_or_create(), do a filter # lookup and just grab the first item stored_match = StoredOEmbed . objects . filter ( match = url , maxwidth = kwargs . get ( 'maxwidth' , None ) , maxheight = kwargs . get ( 'maxheight' , None ) , date_expires__gte = datetime . datetime . now ( ) ) [ 0 ] return OEmbedResource . create_json ( stored_match . response_json ) except IndexError : # query the endpoint and cache response in db # prevent None from being passed in as a GET param params = dict ( [ ( k , v ) for k , v in kwargs . items ( ) if v ] ) # request an oembed resource for the url resource = provider . request_resource ( url , * * params ) try : cache_age = int ( resource . cache_age ) if cache_age < MIN_OEMBED_TTL : cache_age = MIN_OEMBED_TTL except : cache_age = DEFAULT_OEMBED_TTL date_expires = datetime . datetime . now ( ) + datetime . timedelta ( seconds = cache_age ) stored_oembed , created = StoredOEmbed . objects . get_or_create ( match = url , maxwidth = kwargs . get ( 'maxwidth' , None ) , maxheight = kwargs . get ( 'maxheight' , None ) ) stored_oembed . response_json = resource . json stored_oembed . resource_type = resource . type stored_oembed . date_expires = date_expires if resource . content_object : stored_oembed . content_object = resource . content_object stored_oembed . save ( ) return resource
10558	def download ( self , songs , template = None ) : if not template : template = os . getcwd ( ) songnum = 0 total = len ( songs ) results = [ ] errors = { } pad = len ( str ( total ) ) for result in self . _download ( songs , template ) : song_id = songs [ songnum ] [ 'id' ] songnum += 1 downloaded , error = result if downloaded : logger . info ( "({num:>{pad}}/{total}) Successfully downloaded -- {file} ({song_id})" . format ( num = songnum , pad = pad , total = total , file = downloaded [ song_id ] , song_id = song_id ) ) results . append ( { 'result' : 'downloaded' , 'id' : song_id , 'filepath' : downloaded [ song_id ] } ) elif error : title = songs [ songnum ] . get ( 'title' , "<empty>" ) artist = songs [ songnum ] . get ( 'artist' , "<empty>" ) album = songs [ songnum ] . get ( 'album' , "<empty>" ) logger . info ( "({num:>{pad}}/{total}) Error on download -- {title} -- {artist} -- {album} ({song_id})" . format ( num = songnum , pad = pad , total = total , title = title , artist = artist , album = album , song_id = song_id ) ) results . append ( { 'result' : 'error' , 'id' : song_id , 'message' : error [ song_id ] } ) if errors : logger . info ( "\n\nThe following errors occurred:\n" ) for filepath , e in errors . items ( ) : logger . info ( "{file} | {error}" . format ( file = filepath , error = e ) ) logger . info ( "\nThese files may need to be synced again.\n" ) return results
9058	def gradient ( self ) : self . _update_approx ( ) g = self . _ep . lml_derivatives ( self . _X ) ed = exp ( - self . logitdelta ) es = exp ( self . logscale ) grad = dict ( ) grad [ "logitdelta" ] = g [ "delta" ] * ( ed / ( 1 + ed ) ) / ( 1 + ed ) grad [ "logscale" ] = g [ "scale" ] * es grad [ "beta" ] = g [ "mean" ] return grad
9808	def create_tarfile ( files , project_name ) : fd , filename = tempfile . mkstemp ( prefix = "polyaxon_{}" . format ( project_name ) , suffix = '.tar.gz' ) with tarfile . open ( filename , "w:gz" ) as tar : for f in files : tar . add ( f ) yield filename # clear os . close ( fd ) os . remove ( filename )
4001	def get_port_spec_document ( expanded_active_specs , docker_vm_ip ) : forwarding_port = 65000 port_spec = { 'docker_compose' : { } , 'nginx' : [ ] , 'hosts_file' : [ ] } host_full_addresses , host_names , stream_host_ports = set ( ) , set ( ) , set ( ) # No matter the order of apps in expanded_active_specs, we want to produce a consistent # port_spec with respect to the apps and the ports they are outputted on for app_name in sorted ( expanded_active_specs [ 'apps' ] . keys ( ) ) : app_spec = expanded_active_specs [ 'apps' ] [ app_name ] if 'host_forwarding' not in app_spec : continue port_spec [ 'docker_compose' ] [ app_name ] = [ ] for host_forwarding_spec in app_spec [ 'host_forwarding' ] : # These functions are just used for validating the set of specs all works together _add_full_addresses ( host_forwarding_spec , host_full_addresses ) if host_forwarding_spec [ 'type' ] == 'stream' : _add_stream_host_port ( host_forwarding_spec , stream_host_ports ) port_spec [ 'docker_compose' ] [ app_name ] . append ( _docker_compose_port_spec ( host_forwarding_spec , forwarding_port ) ) port_spec [ 'nginx' ] . append ( _nginx_port_spec ( host_forwarding_spec , forwarding_port , docker_vm_ip ) ) _add_host_names ( host_forwarding_spec , docker_vm_ip , port_spec , host_names ) forwarding_port += 1 return port_spec
85	def ContrastNormalization ( alpha = 1.0 , per_channel = False , name = None , deterministic = False , random_state = None ) : # placed here to avoid cyclic dependency from . import contrast as contrast_lib return contrast_lib . LinearContrast ( alpha = alpha , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
800	def modelsGetFieldsForJob ( self , jobID , fields , ignoreKilled = False ) : assert len ( fields ) >= 1 , 'fields is empty' # Form the sequence of field name strings that will go into the # request dbFields = [ self . _models . pubToDBNameDict [ x ] for x in fields ] dbFieldsStr = ',' . join ( dbFields ) query = 'SELECT model_id, %s FROM %s ' ' WHERE job_id=%%s ' % ( dbFieldsStr , self . modelsTableName ) sqlParams = [ jobID ] if ignoreKilled : query += ' AND (completion_reason IS NULL OR completion_reason != %s)' sqlParams . append ( self . CMPL_REASON_KILLED ) # Get a database connection and cursor with ConnectionFactory . get ( ) as conn : conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) if rows is None : # fetchall is defined to return a (possibly-empty) sequence of # sequences; however, we occasionally see None returned and don't know # why... self . _logger . error ( "Unexpected None result from cursor.fetchall; " "query=%r; Traceback=%r" , query , traceback . format_exc ( ) ) return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ]
12010	def getTableOfContents ( self ) : self . directory_size = self . getDirectorySize ( ) if self . directory_size > 65536 : self . directory_size += 2 self . requestContentDirectory ( ) # and find the offset from start of file where it can be found directory_start = unpack ( "i" , self . raw_bytes [ self . directory_end + 16 : self . directory_end + 20 ] ) [ 0 ] # find the data in the raw_bytes self . raw_bytes = self . raw_bytes current_start = directory_start - self . start filestart = 0 compressedsize = 0 tableOfContents = [ ] try : while True : # get file name size (n), extra len (m) and comm len (k) zip_n = unpack ( "H" , self . raw_bytes [ current_start + 28 : current_start + 28 + 2 ] ) [ 0 ] zip_m = unpack ( "H" , self . raw_bytes [ current_start + 30 : current_start + 30 + 2 ] ) [ 0 ] zip_k = unpack ( "H" , self . raw_bytes [ current_start + 32 : current_start + 32 + 2 ] ) [ 0 ] filename = self . raw_bytes [ current_start + 46 : current_start + 46 + zip_n ] # check if this is the index file filestart = unpack ( "I" , self . raw_bytes [ current_start + 42 : current_start + 42 + 4 ] ) [ 0 ] compressedsize = unpack ( "I" , self . raw_bytes [ current_start + 20 : current_start + 20 + 4 ] ) [ 0 ] uncompressedsize = unpack ( "I" , self . raw_bytes [ current_start + 24 : current_start + 24 + 4 ] ) [ 0 ] tableItem = { 'filename' : filename , 'compressedsize' : compressedsize , 'uncompressedsize' : uncompressedsize , 'filestart' : filestart } tableOfContents . append ( tableItem ) # not this file, move along current_start = current_start + 46 + zip_n + zip_m + zip_k except : pass self . tableOfContents = tableOfContents return tableOfContents
133	def clip_out_of_image ( self , image ) : # load shapely lazily, which makes the dependency more optional import shapely . geometry # if fully out of image, clip everything away, nothing remaining if self . is_out_of_image ( image , fully = True , partly = False ) : return [ ] h , w = image . shape [ 0 : 2 ] if ia . is_np_array ( image ) else image [ 0 : 2 ] poly_shapely = self . to_shapely_polygon ( ) poly_image = shapely . geometry . Polygon ( [ ( 0 , 0 ) , ( w , 0 ) , ( w , h ) , ( 0 , h ) ] ) multipoly_inter_shapely = poly_shapely . intersection ( poly_image ) if not isinstance ( multipoly_inter_shapely , shapely . geometry . MultiPolygon ) : ia . do_assert ( isinstance ( multipoly_inter_shapely , shapely . geometry . Polygon ) ) multipoly_inter_shapely = shapely . geometry . MultiPolygon ( [ multipoly_inter_shapely ] ) polygons = [ ] for poly_inter_shapely in multipoly_inter_shapely . geoms : polygons . append ( Polygon . from_shapely ( poly_inter_shapely , label = self . label ) ) # shapely changes the order of points, we try here to preserve it as # much as possible polygons_reordered = [ ] for polygon in polygons : found = False for x , y in self . exterior : closest_idx , dist = polygon . find_closest_point_index ( x = x , y = y , return_distance = True ) if dist < 1e-6 : polygon_reordered = polygon . change_first_point_by_index ( closest_idx ) polygons_reordered . append ( polygon_reordered ) found = True break ia . do_assert ( found ) # could only not find closest points if new polys are empty return polygons_reordered
5448	def make_param ( self , name , raw_uri , disk_size ) : if raw_uri . startswith ( 'https://www.googleapis.com/compute' ) : # Full Image URI should look something like: # https://www.googleapis.com/compute/v1/projects/<project>/global/images/ # But don't validate further, should the form of a valid image URI # change (v1->v2, for example) docker_path = self . _parse_image_uri ( raw_uri ) return job_model . PersistentDiskMountParam ( name , raw_uri , docker_path , disk_size , disk_type = None ) elif raw_uri . startswith ( 'file://' ) : local_path , docker_path = self . _parse_local_mount_uri ( raw_uri ) return job_model . LocalMountParam ( name , raw_uri , docker_path , local_path ) elif raw_uri . startswith ( 'gs://' ) : docker_path = self . _parse_gcs_uri ( raw_uri ) return job_model . GCSMountParam ( name , raw_uri , docker_path ) else : raise ValueError ( 'Mount parameter {} must begin with valid prefix.' . format ( raw_uri ) )
9573	def unpack ( endian , fmt , data ) : if fmt == 's' : # read data as an array of chars val = struct . unpack ( '' . join ( [ endian , str ( len ( data ) ) , 's' ] ) , data ) [ 0 ] else : # read a number of values num = len ( data ) // struct . calcsize ( fmt ) val = struct . unpack ( '' . join ( [ endian , str ( num ) , fmt ] ) , data ) if len ( val ) == 1 : val = val [ 0 ] return val
8168	def run ( self ) : with LiveExecution . lock : if self . edited_source : success , ex = self . run_tenuous ( ) if success : return self . do_exec ( self . known_good , self . ns )
193	def OneOf ( children , name = None , deterministic = False , random_state = None ) : return SomeOf ( n = 1 , children = children , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
2986	def get_cors_options ( appInstance , * dicts ) : options = DEFAULT_OPTIONS . copy ( ) options . update ( get_app_kwarg_dict ( appInstance ) ) if dicts : for d in dicts : options . update ( d ) return serialize_options ( options )
6882	def describe_lcc_csv ( lcdict , returndesc = False ) : metadata_lines = [ ] coldef_lines = [ ] if 'lcformat' in lcdict and 'lcc-csv' in lcdict [ 'lcformat' ] . lower ( ) : metadata = lcdict [ 'metadata' ] metakeys = lcdict [ 'objectinfo' ] . keys ( ) coldefs = lcdict [ 'coldefs' ] for mk in metakeys : metadata_lines . append ( '%20s | %s' % ( mk , metadata [ mk ] [ 'desc' ] ) ) for ck in lcdict [ 'columns' ] : coldef_lines . append ( 'column %02d | %8s | numpy dtype: %3s | %s' % ( coldefs [ ck ] [ 'colnum' ] , ck , coldefs [ ck ] [ 'dtype' ] , coldefs [ ck ] [ 'desc' ] ) ) desc = LCC_CSVLC_DESCTEMPLATE . format ( objectid = lcdict [ 'objectid' ] , metadata_desc = '\n' . join ( metadata_lines ) , metadata = pformat ( lcdict [ 'objectinfo' ] ) , columndefs = '\n' . join ( coldef_lines ) ) print ( desc ) if returndesc : return desc else : LOGERROR ( "this lcdict is not from an LCC CSV, can't figure it out..." ) return None
3246	def get_managed_policies ( group , * * conn ) : managed_policies = list_attached_group_managed_policies ( group [ 'GroupName' ] , * * conn ) managed_policy_names = [ ] for policy in managed_policies : managed_policy_names . append ( policy [ 'PolicyName' ] ) return managed_policy_names
1508	def add_additional_args ( parsers ) : for parser in parsers : cli_args . add_verbose ( parser ) cli_args . add_config ( parser ) parser . add_argument ( '--heron-dir' , default = config . get_heron_dir ( ) , help = 'Path to Heron home directory' )
2699	def render_ranks ( graph , ranks , dot_file = "graph.dot" ) : if dot_file : write_dot ( graph , ranks , path = dot_file )
8740	def _create_flip ( context , flip , port_fixed_ips ) : if port_fixed_ips : context . session . begin ( ) try : ports = [ val [ 'port' ] for val in port_fixed_ips . values ( ) ] flip = db_api . port_associate_ip ( context , ports , flip , port_fixed_ips . keys ( ) ) for port_id in port_fixed_ips : fixed_ip = port_fixed_ips [ port_id ] [ 'fixed_ip' ] flip = db_api . floating_ip_associate_fixed_ip ( context , flip , fixed_ip ) flip_driver = registry . DRIVER_REGISTRY . get_driver ( ) flip_driver . register_floating_ip ( flip , port_fixed_ips ) context . session . commit ( ) except Exception : context . session . rollback ( ) raise # alexm: Notify from this method for consistency with _delete_flip billing . notify ( context , billing . IP_ASSOC , flip )
2909	def _find_any ( self , task_spec ) : tasks = [ ] if self . task_spec == task_spec : tasks . append ( self ) for child in self : if child . task_spec != task_spec : continue tasks . append ( child ) return tasks
4926	def transform_title ( self , content_metadata_item ) : title_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : title_with_locales . append ( { 'locale' : locale , 'value' : content_metadata_item . get ( 'title' , '' ) } ) return title_with_locales
12020	def fasta_dict_to_file ( fasta_dict , fasta_file , line_char_limit = None ) : fasta_fp = fasta_file if isinstance ( fasta_file , str ) : fasta_fp = open ( fasta_file , 'wb' ) for key in fasta_dict : seq = fasta_dict [ key ] [ 'seq' ] if line_char_limit : seq = '\n' . join ( [ seq [ i : i + line_char_limit ] for i in range ( 0 , len ( seq ) , line_char_limit ) ] ) fasta_fp . write ( u'{0:s}\n{1:s}\n' . format ( fasta_dict [ key ] [ 'header' ] , seq ) )
13398	def check_docstring ( cls ) : docstring = inspect . getdoc ( cls ) if not docstring : breadcrumbs = " -> " . join ( t . __name__ for t in inspect . getmro ( cls ) [ : - 1 ] [ : : - 1 ] ) msg = "docstring required for plugin '%s' (%s, defined in %s)" args = ( cls . __name__ , breadcrumbs , cls . __module__ ) raise InternalCashewException ( msg % args ) max_line_length = cls . _class_settings . get ( 'max-docstring-length' ) if max_line_length : for i , line in enumerate ( docstring . splitlines ( ) ) : if len ( line ) > max_line_length : msg = "docstring line %s of %s is %s chars too long" args = ( i , cls . __name__ , len ( line ) - max_line_length ) raise Exception ( msg % args ) return docstring
1053	def extract_stack ( f = None , limit = None ) : if f is None : try : raise ZeroDivisionError except ZeroDivisionError : f = sys . exc_info ( ) [ 2 ] . tb_frame . f_back if limit is None : if hasattr ( sys , 'tracebacklimit' ) : limit = sys . tracebacklimit list = [ ] n = 0 while f is not None and ( limit is None or n < limit ) : lineno = f . f_lineno co = f . f_code filename = co . co_filename name = co . co_name linecache . checkcache ( filename ) line = linecache . getline ( filename , lineno , f . f_globals ) if line : line = line . strip ( ) else : line = None list . append ( ( filename , lineno , name , line ) ) f = f . f_back n = n + 1 list . reverse ( ) return list
5525	def grab ( self , bbox = None ) : w = Gdk . get_default_root_window ( ) if bbox is not None : g = [ bbox [ 0 ] , bbox [ 1 ] , bbox [ 2 ] - bbox [ 0 ] , bbox [ 3 ] - bbox [ 1 ] ] else : g = w . get_geometry ( ) pb = Gdk . pixbuf_get_from_window ( w , * g ) if pb . get_bits_per_sample ( ) != 8 : raise ValueError ( 'Expected 8 bits per pixel.' ) elif pb . get_n_channels ( ) != 3 : raise ValueError ( 'Expected RGB image.' ) # Read the entire buffer into a python bytes object. # read_pixel_bytes: New in version 2.32. pixel_bytes = pb . read_pixel_bytes ( ) . get_data ( ) # type: bytes width , height = g [ 2 ] , g [ 3 ] # Probably for SSE alignment reasons, the pixbuf has extra data in each line. # The args after "raw" help handle this; see # http://effbot.org/imagingbook/decoder.htm#the-raw-decoder return Image . frombytes ( 'RGB' , ( width , height ) , pixel_bytes , 'raw' , 'RGB' , pb . get_rowstride ( ) , 1 )
10393	def workflow_all_aggregate ( graph : BELGraph , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , aggregator : Optional [ Callable [ [ Iterable [ float ] ] , float ] ] = None , ) : results = { } bioprocess_nodes = list ( get_nodes_by_function ( graph , BIOPROCESS ) ) for bioprocess_node in tqdm ( bioprocess_nodes ) : subgraph = generate_mechanism ( graph , bioprocess_node , key = key ) try : results [ bioprocess_node ] = workflow_aggregate ( graph = subgraph , node = bioprocess_node , key = key , tag = tag , default_score = default_score , runs = runs , aggregator = aggregator ) except Exception : log . exception ( 'could not run on %' , bioprocess_node ) return results
10812	def query_by_user ( cls , user , with_pending = False , eager = False ) : q1 = Group . query . join ( Membership ) . filter_by ( user_id = user . get_id ( ) ) if not with_pending : q1 = q1 . filter_by ( state = MembershipState . ACTIVE ) if eager : q1 = q1 . options ( joinedload ( Group . members ) ) q2 = Group . query . join ( GroupAdmin ) . filter_by ( admin_id = user . get_id ( ) , admin_type = resolve_admin_type ( user ) ) if eager : q2 = q2 . options ( joinedload ( Group . members ) ) query = q1 . union ( q2 ) . with_entities ( Group . id ) return Group . query . filter ( Group . id . in_ ( query ) )
13406	def submitEntry ( self ) : # logType = self.logui.logType.currentText() mcclogs , physlogs = self . selectedLogs ( ) success = True if mcclogs != [ ] : if not self . acceptedUser ( "MCC" ) : QMessageBox ( ) . warning ( self , "Invalid User" , "Please enter a valid user name!" ) return fileName = self . xmlSetup ( "MCC" , mcclogs ) if fileName is None : return if not self . imagePixmap . isNull ( ) : self . prepareImages ( fileName , "MCC" ) success = self . sendToLogbook ( fileName , "MCC" ) if physlogs != [ ] : for i in range ( len ( physlogs ) ) : fileName = self . xmlSetup ( "Physics" , physlogs [ i ] ) if fileName is None : return if not self . imagePixmap . isNull ( ) : self . prepareImages ( fileName , "Physics" ) success_phys = self . sendToLogbook ( fileName , "Physics" , physlogs [ i ] ) success = success and success_phys self . done ( success )
13899	def PushPopItem ( obj , key , value ) : if key in obj : old_value = obj [ key ] obj [ key ] = value yield value obj [ key ] = old_value else : obj [ key ] = value yield value del obj [ key ]
3208	def _reformat_policy ( policy ) : policy_name = policy [ 'PolicyName' ] ret = { } ret [ 'type' ] = policy [ 'PolicyTypeName' ] attrs = policy [ 'PolicyAttributeDescriptions' ] if ret [ 'type' ] != 'SSLNegotiationPolicyType' : return policy_name , ret attributes = dict ( ) for attr in attrs : attributes [ attr [ 'AttributeName' ] ] = attr [ 'AttributeValue' ] ret [ 'protocols' ] = dict ( ) ret [ 'protocols' ] [ 'sslv2' ] = bool ( attributes . get ( 'Protocol-SSLv2' ) ) ret [ 'protocols' ] [ 'sslv3' ] = bool ( attributes . get ( 'Protocol-SSLv3' ) ) ret [ 'protocols' ] [ 'tlsv1' ] = bool ( attributes . get ( 'Protocol-TLSv1' ) ) ret [ 'protocols' ] [ 'tlsv1_1' ] = bool ( attributes . get ( 'Protocol-TLSv1.1' ) ) ret [ 'protocols' ] [ 'tlsv1_2' ] = bool ( attributes . get ( 'Protocol-TLSv1.2' ) ) ret [ 'server_defined_cipher_order' ] = bool ( attributes . get ( 'Server-Defined-Cipher-Order' ) ) ret [ 'reference_security_policy' ] = attributes . get ( 'Reference-Security-Policy' , None ) non_ciphers = [ 'Server-Defined-Cipher-Order' , 'Protocol-SSLv2' , 'Protocol-SSLv3' , 'Protocol-TLSv1' , 'Protocol-TLSv1.1' , 'Protocol-TLSv1.2' , 'Reference-Security-Policy' ] ciphers = [ ] for cipher in attributes : if attributes [ cipher ] == 'true' and cipher not in non_ciphers : ciphers . append ( cipher ) ciphers . sort ( ) ret [ 'supported_ciphers' ] = ciphers return policy_name , ret
7184	def copy_arguments_to_annotations ( args , type_comment , * , is_method = False ) : if isinstance ( type_comment , ast3 . Ellipsis ) : return expected = len ( args . args ) if args . vararg : expected += 1 expected += len ( args . kwonlyargs ) if args . kwarg : expected += 1 actual = len ( type_comment ) if isinstance ( type_comment , list ) else 1 if expected != actual : if is_method and expected - actual == 1 : pass # fine, we're just skipping `self`, `cls`, etc. else : raise ValueError ( f"number of arguments in type comment doesn't match; " + f"expected {expected}, found {actual}" ) if isinstance ( type_comment , list ) : next_value = type_comment . pop else : # If there's just one value, only one of the loops and ifs below will # be populated. We ensure this with the expected/actual length check # above. _tc = type_comment def next_value ( index : int = 0 ) -> ast3 . expr : return _tc for arg in args . args [ expected - actual : ] : ensure_no_annotation ( arg . annotation ) arg . annotation = next_value ( 0 ) if args . vararg : ensure_no_annotation ( args . vararg . annotation ) args . vararg . annotation = next_value ( 0 ) for arg in args . kwonlyargs : ensure_no_annotation ( arg . annotation ) arg . annotation = next_value ( 0 ) if args . kwarg : ensure_no_annotation ( args . kwarg . annotation ) args . kwarg . annotation = next_value ( 0 )
12275	def iso_reference_isvalid ( ref ) : ref = str ( ref ) cs_source = ref [ 4 : ] + ref [ : 4 ] return ( iso_reference_str2int ( cs_source ) % 97 ) == 1
1717	def replacement_template ( rep , source , span , npar ) : n = 0 res = '' while n < len ( rep ) - 1 : char = rep [ n ] if char == '$' : if rep [ n + 1 ] == '$' : res += '$' n += 2 continue elif rep [ n + 1 ] == '`' : # replace with string that is BEFORE match res += source [ : span [ 0 ] ] n += 2 continue elif rep [ n + 1 ] == '\'' : # replace with string that is AFTER match res += source [ span [ 1 ] : ] n += 2 continue elif rep [ n + 1 ] in DIGS : dig = rep [ n + 1 ] if n + 2 < len ( rep ) and rep [ n + 2 ] in DIGS : dig += rep [ n + 2 ] num = int ( dig ) # we will not do any replacements if we dont have this npar or dig is 0 if not num or num > len ( npar ) : res += '$' + dig else : # None - undefined has to be replaced with '' res += npar [ num - 1 ] if npar [ num - 1 ] else '' n += 1 + len ( dig ) continue res += char n += 1 if n < len ( rep ) : res += rep [ - 1 ] return res
11251	def get_percentage ( a , b , i = False , r = False ) : # Round to the second decimal if i is False and r is True : percentage = round ( 100.0 * ( float ( a ) / b ) , 2 ) # Round to the nearest whole number elif ( i is True and r is True ) or ( i is True and r is False ) : percentage = int ( round ( 100 * ( float ( a ) / b ) ) ) # A rounded number and an integer were requested if r is False : warnings . warn ( "If integer is set to True and Round is set to False, you will still get a rounded number if you pass floating point numbers as arguments." ) # A precise unrounded decimal else : percentage = 100.0 * ( float ( a ) / b ) return percentage
10975	def manage ( group_id ) : group = Group . query . get_or_404 ( group_id ) form = GroupForm ( request . form , obj = group ) if form . validate_on_submit ( ) : if group . can_edit ( current_user ) : try : group . update ( * * form . data ) flash ( _ ( 'Group "%(name)s" was updated' , name = group . name ) , 'success' ) except Exception as e : flash ( str ( e ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , group = group , ) else : flash ( _ ( 'You cannot edit group %(group_name)s' , group_name = group . name ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , group = group , )
7259	def search_point ( self , lat , lng , filters = None , startDate = None , endDate = None , types = None , type = None ) : searchAreaWkt = "POLYGON ((%s %s, %s %s, %s %s, %s %s, %s %s))" % ( lng , lat , lng , lat , lng , lat , lng , lat , lng , lat ) return self . search ( searchAreaWkt = searchAreaWkt , filters = filters , startDate = startDate , endDate = endDate , types = types )
6598	def end ( self ) : results = self . communicationChannel . receive ( ) if self . nruns != len ( results ) : import logging logger = logging . getLogger ( __name__ ) # logger.setLevel(logging.DEBUG) logger . warning ( 'too few results received: {} results received, {} expected' . format ( len ( results ) , self . nruns ) ) return results
9488	def generate_simple_call ( opcode : int , index : int ) : bs = b"" # add the opcode bs += opcode . to_bytes ( 1 , byteorder = "little" ) # Add the index if isinstance ( index , int ) : if PY36 : bs += index . to_bytes ( 1 , byteorder = "little" ) else : bs += index . to_bytes ( 2 , byteorder = "little" ) else : bs += index return bs
9770	def update ( ctx , name , description , tags ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the job.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . job . update_job ( user , project_name , _job , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Job updated." ) get_job_details ( response )
3843	def from_timestamp ( microsecond_timestamp ) : # Create datetime without losing precision from floating point (yes, this # is actually needed): return datetime . datetime . fromtimestamp ( microsecond_timestamp // 1000000 , datetime . timezone . utc ) . replace ( microsecond = ( microsecond_timestamp % 1000000 ) )
5037	def _handle_bulk_upload ( cls , enterprise_customer , manage_learners_form , request , email_list = None ) : errors = [ ] emails = set ( ) already_linked_emails = [ ] duplicate_emails = [ ] csv_file = manage_learners_form . cleaned_data [ ManageLearnersForm . Fields . BULK_UPLOAD ] if email_list : parsed_csv = [ { ManageLearnersForm . CsvColumns . EMAIL : email } for email in email_list ] else : parsed_csv = parse_csv ( csv_file , expected_columns = { ManageLearnersForm . CsvColumns . EMAIL } ) try : for index , row in enumerate ( parsed_csv ) : email = row [ ManageLearnersForm . CsvColumns . EMAIL ] try : already_linked = validate_email_to_link ( email , ignore_existing = True ) except ValidationError as exc : message = _ ( "Error at line {line}: {message}\n" ) . format ( line = index + 1 , message = exc ) errors . append ( message ) else : if already_linked : already_linked_emails . append ( ( email , already_linked . enterprise_customer ) ) elif email in emails : duplicate_emails . append ( email ) else : emails . add ( email ) except ValidationError as exc : errors . append ( exc ) if errors : manage_learners_form . add_error ( ManageLearnersForm . Fields . GENERAL_ERRORS , ValidationMessages . BULK_LINK_FAILED ) for error in errors : manage_learners_form . add_error ( ManageLearnersForm . Fields . BULK_UPLOAD , error ) return # There were no errors. Now do the actual linking: for email in emails : EnterpriseCustomerUser . objects . link_user ( enterprise_customer , email ) # Report what happened: count = len ( emails ) messages . success ( request , ungettext ( "{count} new learner was added to {enterprise_customer_name}." , "{count} new learners were added to {enterprise_customer_name}." , count ) . format ( count = count , enterprise_customer_name = enterprise_customer . name ) ) this_customer_linked_emails = [ email for email , customer in already_linked_emails if customer == enterprise_customer ] other_customer_linked_emails = [ email for email , __ in already_linked_emails if email not in this_customer_linked_emails ] if this_customer_linked_emails : messages . warning ( request , _ ( "The following learners were already associated with this Enterprise " "Customer: {list_of_emails}" ) . format ( list_of_emails = ", " . join ( this_customer_linked_emails ) ) ) if other_customer_linked_emails : messages . warning ( request , _ ( "The following learners are already associated with " "another Enterprise Customer. These learners were not " "added to {enterprise_customer_name}: {list_of_emails}" ) . format ( enterprise_customer_name = enterprise_customer . name , list_of_emails = ", " . join ( other_customer_linked_emails ) , ) ) if duplicate_emails : messages . warning ( request , _ ( "The following duplicate email addresses were not added: " "{list_of_emails}" ) . format ( list_of_emails = ", " . join ( duplicate_emails ) ) ) # Build a list of all the emails that we can act on further; that is, # emails that we either linked to this customer, or that were linked already. all_processable_emails = list ( emails ) + this_customer_linked_emails return all_processable_emails
11506	def create_item ( self , token , name , parent_id , * * kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'name' ] = name parameters [ 'parentid' ] = parent_id optional_keys = [ 'description' , 'uuid' , 'privacy' ] for key in optional_keys : if key in kwargs : parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.item.create' , parameters ) return response
4326	def delay ( self , positions ) : if not isinstance ( positions , list ) : raise ValueError ( "positions must be a a list of numbers" ) if not all ( ( is_number ( p ) and p >= 0 ) for p in positions ) : raise ValueError ( "positions must be positive nubmers" ) effect_args = [ 'delay' ] effect_args . extend ( [ '{:f}' . format ( p ) for p in positions ] ) self . effects . extend ( effect_args ) self . effects_log . append ( 'delay' ) return self
9836	def __comment ( self ) : tok = self . __consume ( ) self . DXfield . add_comment ( tok . value ( ) ) self . set_parser ( 'general' )
5854	def create_dataset ( self , name = None , description = None , public = False ) : data = { "public" : _convert_bool_to_public_value ( public ) } if name : data [ "name" ] = name if description : data [ "description" ] = description dataset = { "dataset" : data } failure_message = "Unable to create dataset" result = self . _get_success_json ( self . _post_json ( routes . create_dataset ( ) , dataset , failure_message = failure_message ) ) return _dataset_from_response_dict ( result )
13702	def _after ( self , response ) : # Ignore excluded routes. if getattr ( request , '_tracy_exclude' , False ) : return response duration = None if getattr ( request , '_tracy_start_time' , None ) : duration = monotonic ( ) - request . _tracy_start_time # Add Trace_ID header. trace_id = None if getattr ( request , '_tracy_id' , None ) : trace_id = request . _tracy_id response . headers [ trace_header_id ] = trace_id # Get the invoking client. trace_client = None if getattr ( request , '_tracy_client' , None ) : trace_client = request . _tracy_client # Extra log kwargs. d = { 'status_code' : response . status_code , 'url' : request . base_url , 'client_ip' : request . remote_addr , 'trace_name' : trace_client , 'trace_id' : trace_id , 'trace_duration' : duration } logger . info ( None , extra = d ) return response
695	def _loadDescriptionFile ( descriptionPyPath ) : global g_descriptionImportCount if not os . path . isfile ( descriptionPyPath ) : raise RuntimeError ( ( "Experiment description file %s does not exist or " + "is not a file" ) % ( descriptionPyPath , ) ) mod = imp . load_source ( "pf_description%d" % g_descriptionImportCount , descriptionPyPath ) g_descriptionImportCount += 1 if not hasattr ( mod , "descriptionInterface" ) : raise RuntimeError ( "Experiment description file %s does not define %s" % ( descriptionPyPath , "descriptionInterface" ) ) if not isinstance ( mod . descriptionInterface , exp_description_api . DescriptionIface ) : raise RuntimeError ( ( "Experiment description file %s defines %s but it " + "is not DescriptionIface-based" ) % ( descriptionPyPath , name ) ) return mod
6162	def MPSK_bb ( N_symb , Ns , M , pulse = 'rect' , alpha = 0.25 , MM = 6 ) : data = np . random . randint ( 0 , M , N_symb ) xs = np . exp ( 1j * 2 * np . pi / M * data ) x = np . hstack ( ( xs . reshape ( N_symb , 1 ) , np . zeros ( ( N_symb , int ( Ns ) - 1 ) ) ) ) x = x . flatten ( ) if pulse . lower ( ) == 'rect' : b = np . ones ( int ( Ns ) ) elif pulse . lower ( ) == 'rc' : b = rc_imp ( Ns , alpha , MM ) elif pulse . lower ( ) == 'src' : b = sqrt_rc_imp ( Ns , alpha , MM ) else : raise ValueError ( 'pulse type must be rec, rc, or src' ) x = signal . lfilter ( b , 1 , x ) if M == 4 : x = x * np . exp ( 1j * np . pi / 4 ) # For QPSK points in quadrants return x , b / float ( Ns ) , data
12097	def delete ( self , force = False , * * kwargs ) : if force : return super ( BaseActivatableModel , self ) . delete ( * * kwargs ) else : setattr ( self , self . ACTIVATABLE_FIELD_NAME , False ) return self . save ( update_fields = [ self . ACTIVATABLE_FIELD_NAME ] )
10244	def get_citation_years ( graph : BELGraph ) -> List [ Tuple [ int , int ] ] : return create_timeline ( count_citation_years ( graph ) )
3409	def knock_out ( self ) : self . functional = False for reaction in self . reactions : if not reaction . functional : reaction . bounds = ( 0 , 0 )
12710	def relative_offset_to_world ( self , offset ) : return np . array ( self . body_to_world ( offset * self . dimensions / 2 ) )
9519	def interleave ( infile_1 , infile_2 , outfile , suffix1 = None , suffix2 = None ) : seq_reader_1 = sequences . file_reader ( infile_1 ) seq_reader_2 = sequences . file_reader ( infile_2 ) f_out = utils . open_file_write ( outfile ) for seq_1 in seq_reader_1 : try : seq_2 = next ( seq_reader_2 ) except : utils . close ( f_out ) raise Error ( 'Error getting mate for sequence' , seq_1 . id , ' ... cannot continue' ) if suffix1 is not None and not seq_1 . id . endswith ( suffix1 ) : seq_1 . id += suffix1 if suffix2 is not None and not seq_2 . id . endswith ( suffix2 ) : seq_2 . id += suffix2 print ( seq_1 , file = f_out ) print ( seq_2 , file = f_out ) try : seq_2 = next ( seq_reader_2 ) except : seq_2 = None if seq_2 is not None : utils . close ( f_out ) raise Error ( 'Error getting mate for sequence' , seq_2 . id , ' ... cannot continue' ) utils . close ( f_out )
532	def getParameter ( self , paramName ) : ( setter , getter ) = self . _getParameterMethods ( paramName ) if getter is None : import exceptions raise exceptions . Exception ( "getParameter -- parameter name '%s' does not exist in region %s of type %s" % ( paramName , self . name , self . type ) ) return getter ( paramName )
13676	def add_prepare_handler ( self , prepare_handlers ) : if not isinstance ( prepare_handlers , static_bundle . BUNDLE_ITERABLE_TYPES ) : prepare_handlers = [ prepare_handlers ] if self . prepare_handlers_chain is None : self . prepare_handlers_chain = [ ] for handler in prepare_handlers : self . prepare_handlers_chain . append ( handler )
909	def __advancePhase ( self ) : self . __currentPhase = self . __phaseCycler . next ( ) self . __currentPhase . enterPhase ( ) return
4639	def set_shared_config ( cls , config ) : assert isinstance ( config , dict ) cls . _sharedInstance . config . update ( config ) # if one is already set, delete if cls . _sharedInstance . instance : cls . _sharedInstance . instance = None
7677	def hierarchy ( annotation , * * kwargs ) : htimes , hlabels = hierarchy_flatten ( annotation ) htimes = [ np . asarray ( _ ) for _ in htimes ] return mir_eval . display . hierarchy ( htimes , hlabels , * * kwargs )
3608	def put_async ( self , url , name , data , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) process_pool . apply_async ( make_put_request , args = ( endpoint , data , params , headers ) , callback = callback )
6834	def vagrant ( self , name = '' ) : r = self . local_renderer config = self . ssh_config ( name ) extra_args = self . _settings_dict ( config ) r . genv . update ( extra_args )
10385	def get_walks_exhaustive ( graph , node , length ) : if 0 == length : return ( node , ) , return tuple ( ( node , key ) + path for neighbor in graph . edge [ node ] for path in get_walks_exhaustive ( graph , neighbor , length - 1 ) if node not in path for key in graph . edge [ node ] [ neighbor ] )
6189	def get_bromo_fnames_da ( d_em_kHz , d_bg_kHz , a_em_kHz , a_bg_kHz , ID = '1+2+3+4+5+6' , t_tot = '480' , num_p = '30' , pM = '64' , t_step = 0.5e-6 , D = 1.2e-11 , dir_ = '' ) : clk_p = t_step / 32. # with t_step=0.5us -> 156.25 ns E_sim = 1. * a_em_kHz / ( a_em_kHz + d_em_kHz ) FRET_val = 100. * E_sim print ( "Simulated FRET value: %.1f%%" % FRET_val ) d_em_kHz_str = "%04d" % d_em_kHz a_em_kHz_str = "%04d" % a_em_kHz d_bg_kHz_str = "%04.1f" % d_bg_kHz a_bg_kHz_str = "%04.1f" % a_bg_kHz print ( "D: EM %s BG %s " % ( d_em_kHz_str , d_bg_kHz_str ) ) print ( "A: EM %s BG %s " % ( a_em_kHz_str , a_bg_kHz_str ) ) fname_d = ( 'ph_times_{t_tot}s_D{D}_{np}P_{pM}pM_' 'step{ts_us}us_ID{ID}_EM{em}kHz_BG{bg}kHz.npy' ) . format ( em = d_em_kHz_str , bg = d_bg_kHz_str , t_tot = t_tot , pM = pM , np = num_p , ID = ID , ts_us = t_step * 1e6 , D = D ) fname_a = ( 'ph_times_{t_tot}s_D{D}_{np}P_{pM}pM_' 'step{ts_us}us_ID{ID}_EM{em}kHz_BG{bg}kHz.npy' ) . format ( em = a_em_kHz_str , bg = a_bg_kHz_str , t_tot = t_tot , pM = pM , np = num_p , ID = ID , ts_us = t_step * 1e6 , D = D ) print ( fname_d ) print ( fname_a ) name = ( 'BroSim_E{:.1f}_dBG{:.1f}k_aBG{:.1f}k_' 'dEM{:.0f}k' ) . format ( FRET_val , d_bg_kHz , a_bg_kHz , d_em_kHz ) return dir_ + fname_d , dir_ + fname_a , name , clk_p , E_sim
6908	def galactic_to_equatorial ( gl , gb ) : gal = SkyCoord ( gl * u . degree , gl * u . degree , frame = 'galactic' ) transformed = gal . transform_to ( 'icrs' ) return transformed . ra . degree , transformed . dec . degree
10497	def doubleClickMouse ( self , coord ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) # This is a kludge: # If directed towards a Fusion VM the clickCount gets ignored and this # will be seen as a single click, so in sequence this will be a double- # click # Otherwise to a host app only this second one will count as a double- # click self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , clickCount = 2 ) self . _postQueuedEvents ( )
12623	def dir_match ( regex , wd = os . curdir ) : ls = os . listdir ( wd ) filt = re . compile ( regex ) . match return filter_list ( ls , filt )
12287	def shellcmd ( repo , args ) : with cd ( repo . rootdir ) : result = run ( args ) return result
6552	def check ( self , solution ) : return self . func ( * ( solution [ v ] for v in self . variables ) )
13005	def bruteforce ( users , domain , password , host ) : cs = CredentialSearch ( use_pipe = False ) print_notification ( "Connecting to {}" . format ( host ) ) s = Server ( host ) c = Connection ( s ) for user in users : if c . rebind ( user = "{}\\{}" . format ( domain , user . username ) , password = password , authentication = NTLM ) : print_success ( 'Success for: {}:{}' . format ( user . username , password ) ) credential = cs . find_object ( user . username , password , domain = domain , host_ip = host ) if not credential : credential = Credential ( username = user . username , secret = password , domain = domain , host_ip = host , type = "plaintext" , port = 389 ) credential . add_tag ( tag ) credential . save ( ) # Add a tag to the user object, so we dont have to bruteforce it again. user . add_tag ( tag ) user . save ( ) else : print_error ( "Fail for: {}:{}" . format ( user . username , password ) )
13700	def init_app ( self , app ) : app . config . setdefault ( "TRACY_REQUIRE_CLIENT" , False ) if not hasattr ( app , 'extensions' ) : app . extensions = { } app . extensions [ 'restpoints' ] = self app . before_request ( self . _before ) app . after_request ( self . _after )
9688	def read_bin_boundaries ( self ) : config = [ ] data = { } # Send the command byte and sleep for 10 ms self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) # Read the config variables by sending 256 empty bytes for i in range ( 30 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) # Add the bin bounds to the dictionary of data [bytes 0-29] for i in range ( 0 , 14 ) : data [ "Bin Boundary {0}" . format ( i ) ] = self . _16bit_unsigned ( config [ 2 * i ] , config [ 2 * i + 1 ] ) return data
11614	def export_posterior_probability ( self , filename , title = "Posterior Probability" ) : self . probability . save ( h5file = filename , title = title )
11759	def is_variable ( x ) : return isinstance ( x , Expr ) and not x . args and is_var_symbol ( x . op )
2720	def wait ( self , update_every_seconds = 1 ) : while self . status == u'in-progress' : sleep ( update_every_seconds ) self . load ( ) return self . status == u'completed'
9503	def remove_contained_in_list ( l ) : i = 0 l . sort ( ) while i < len ( l ) - 1 : if l [ i + 1 ] . contains ( l [ i ] ) : l . pop ( i ) elif l [ i ] . contains ( l [ i + 1 ] ) : l . pop ( i + 1 ) else : i += 1
1618	def IsCppString ( line ) : line = line . replace ( r'\\' , 'XX' ) # after this, \\" does not match to \" return ( ( line . count ( '"' ) - line . count ( r'\"' ) - line . count ( "'\"'" ) ) & 1 ) == 1
10138	def nearest ( self , ver ) : if not isinstance ( ver , Version ) : ver = Version ( ver ) if ver in OFFICIAL_VERSIONS : return ver # We might not have an exact match for that. # See if we have one that's newer than the grid we're looking at. versions = list ( OFFICIAL_VERSIONS ) versions . sort ( reverse = True ) best = None for candidate in versions : # Due to ambiguities, we might have an exact match and not know it. # '2.0' will not hash to the same value as '2.0.0', but both are # equivalent. if candidate == ver : # We can't beat this, make a note of the match for later return candidate # If we have not seen a better candidate, and this is older # then we may have to settle for that. if ( best is None ) and ( candidate < ver ) : warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug. Closest (older) version supported is %s.' % ( ver , candidate ) ) return candidate # Probably the best so far, but see if we can go closer if candidate > ver : best = candidate # Unhappy path, no best option? This should not happen. assert best is not None warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug. Closest (newer) version supported is %s.' % ( ver , best ) ) return best
7299	def get_qset ( self , queryset , q ) : if self . mongoadmin . search_fields and q : params = { } for field in self . mongoadmin . search_fields : if field == 'id' : # check to make sure this is a valid ID, otherwise we just continue if is_valid_object_id ( q ) : return queryset . filter ( pk = q ) continue search_key = "{field}__icontains" . format ( field = field ) params [ search_key ] = q queryset = queryset . filter ( * * params ) return queryset
4970	def clean ( self ) : cleaned_data = super ( EnterpriseCustomerAdminForm , self ) . clean ( ) if 'catalog' in cleaned_data and not cleaned_data [ 'catalog' ] : cleaned_data [ 'catalog' ] = None return cleaned_data
2045	def set_storage_data ( self , storage_address , offset , value ) : self . _world_state [ storage_address ] [ 'storage' ] [ offset ] = value
7914	def get_int_range_validator ( start , stop ) : def validate_int_range ( value ) : """Integer range validator.""" value = int ( value ) if value >= start and value < stop : return value raise ValueError ( "Not in <{0},{1}) range" . format ( start , stop ) ) return validate_int_range
11408	def record_delete_field ( rec , tag , ind1 = ' ' , ind2 = ' ' , field_position_global = None , field_position_local = None ) : error = _validate_record_field_positions_global ( rec ) if error : # FIXME one should write a message here. pass if tag not in rec : return False ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) deleted = [ ] newfields = [ ] if field_position_global is None and field_position_local is None : # Remove all fields with tag 'tag'. for field in rec [ tag ] : if field [ 1 ] != ind1 or field [ 2 ] != ind2 : newfields . append ( field ) else : deleted . append ( field ) rec [ tag ] = newfields elif field_position_global is not None : # Remove the field with 'field_position_global'. for field in rec [ tag ] : if ( field [ 1 ] != ind1 and field [ 2 ] != ind2 or field [ 4 ] != field_position_global ) : newfields . append ( field ) else : deleted . append ( field ) rec [ tag ] = newfields elif field_position_local is not None : # Remove the field with 'field_position_local'. try : del rec [ tag ] [ field_position_local ] except IndexError : return [ ] if not rec [ tag ] : # Tag is now empty, remove it. del rec [ tag ] return deleted
8149	def _should_run ( self , iteration , max_iterations ) : if iteration == 0 : # First frame always runs return True if max_iterations : if iteration < max_iterations : return True elif max_iterations is None : if self . _dynamic : return True else : return False return True if not self . _dynamic : return False return False
6924	def open ( self , database , user , password , host ) : try : self . connection = pg . connect ( user = user , password = password , database = database , host = host ) LOGINFO ( 'postgres connection successfully ' 'created, using DB %s, user %s' % ( database , user ) ) self . database = database self . user = user except Exception as e : LOGEXCEPTION ( 'postgres connection failed, ' 'using DB %s, user %s' % ( database , user ) ) self . database = None self . user = None
7726	def __init ( self , code ) : code = int ( code ) if code < 0 or code > 999 : raise ValueError ( "Bad status code" ) self . code = code
4231	def run_subcommand ( netgear , args ) : subcommand = args . subcommand if subcommand == "block_device" or subcommand == "allow_device" : return netgear . allow_block_device ( args . mac_addr , BLOCK if subcommand == "block_device" else ALLOW ) if subcommand == "attached_devices" : if args . verbose : return netgear . get_attached_devices_2 ( ) else : return netgear . get_attached_devices ( ) if subcommand == 'traffic_meter' : return netgear . get_traffic_meter ( ) if subcommand == 'login' : return netgear . login ( ) print ( "Unknown subcommand" )
5892	def get_urls ( self ) : urls = patterns ( '' , url ( r'^upload/$' , self . admin_site . admin_view ( self . handle_upload ) , name = 'quill-file-upload' ) , ) return urls + super ( QuillAdmin , self ) . get_urls ( )
11892	def update ( self ) : bulbs = self . _hub . get_lights ( ) if not bulbs : _LOGGER . debug ( "%s is offline, send command failed" , self . _zid ) self . _online = False
1343	def onehot_like ( a , index , value = 1 ) : x = np . zeros_like ( a ) x [ index ] = value return x
1094	def findall ( pattern , string , flags = 0 ) : return _compile ( pattern , flags ) . findall ( string ) # if sys.hexversion >= 0x02020000: # __all__.append("finditer") def finditer ( pattern , string , flags = 0 ) : """Return an iterator over all non-overlapping matches in the string. For each match, the iterator returns a match object. Empty matches are included in the result.""" return _compile ( pattern , flags ) . finditer ( string )
1203	def tf_step ( self , x , iteration , conjugate , residual , squared_residual ) : x , next_iteration , conjugate , residual , squared_residual = super ( ConjugateGradient , self ) . tf_step ( x , iteration , conjugate , residual , squared_residual ) # Ac := A * c_t A_conjugate = self . fn_x ( conjugate ) # TODO: reference? if self . damping > 0.0 : A_conjugate = [ A_conj + self . damping * conj for A_conj , conj in zip ( A_conjugate , conjugate ) ] # cAc := c_t^T * Ac conjugate_A_conjugate = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( conj * A_conj ) ) for conj , A_conj in zip ( conjugate , A_conjugate ) ] ) # \alpha := r_t^2 / cAc alpha = squared_residual / tf . maximum ( x = conjugate_A_conjugate , y = util . epsilon ) # x_{t+1} := x_t + \alpha * c_t next_x = [ t + alpha * conj for t , conj in zip ( x , conjugate ) ] # r_{t+1} := r_t - \alpha * Ac next_residual = [ res - alpha * A_conj for res , A_conj in zip ( residual , A_conjugate ) ] # r_{t+1}^2 := r_{t+1}^T * r_{t+1} next_squared_residual = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( res * res ) ) for res in next_residual ] ) # \beta = r_{t+1}^2 / r_t^2 beta = next_squared_residual / tf . maximum ( x = squared_residual , y = util . epsilon ) # c_{t+1} := r_{t+1} + \beta * c_t next_conjugate = [ res + beta * conj for res , conj in zip ( next_residual , conjugate ) ] return next_x , next_iteration , next_conjugate , next_residual , next_squared_residual
9268	def detect_link_tag_time ( self , tag ) : # if tag is nil - set current time newer_tag_time = self . get_time_of_tag ( tag ) if tag else datetime . datetime . now ( ) # if it's future release tag - set this value if tag [ "name" ] == self . options . unreleased_label and self . options . future_release : newer_tag_name = self . options . future_release newer_tag_link = self . options . future_release elif tag [ "name" ] is not self . options . unreleased_label : # put unreleased label if there is no name for the tag newer_tag_name = tag [ "name" ] newer_tag_link = newer_tag_name else : newer_tag_name = self . options . unreleased_label newer_tag_link = "HEAD" return [ newer_tag_link , newer_tag_name , newer_tag_time ]
6859	def create_database ( name , owner = None , owner_host = 'localhost' , charset = 'utf8' , collate = 'utf8_general_ci' , * * kwargs ) : with settings ( hide ( 'running' ) ) : query ( "CREATE DATABASE %(name)s CHARACTER SET %(charset)s COLLATE %(collate)s;" % { 'name' : name , 'charset' : charset , 'collate' : collate } , * * kwargs ) if owner : query ( "GRANT ALL PRIVILEGES ON %(name)s.* TO '%(owner)s'@'%(owner_host)s' WITH GRANT OPTION;" % { 'name' : name , 'owner' : owner , 'owner_host' : owner_host } , * * kwargs ) puts ( "Created MySQL database '%s'." % name )
4159	def CORRELOGRAMPSD ( X , Y = None , lag = - 1 , window = 'hamming' , norm = 'unbiased' , NFFT = 4096 , window_params = { } , correlation_method = 'xcorr' ) : N = len ( X ) assert lag < N , 'lag must be < size of input data' assert correlation_method in [ 'CORRELATION' , 'xcorr' ] if Y is None : Y = numpy . array ( X ) crosscorrelation = False else : crosscorrelation = True if NFFT is None : NFFT = N psd = numpy . zeros ( NFFT , dtype = complex ) # Window should be centered around zero. Moreover, we want only the # positive values. So, we need to use 2*lag + 1 window and keep values on # the right side. w = Window ( 2. * lag + 1 , window , * * window_params ) w = w . data [ lag + 1 : ] # compute the cross correlation if correlation_method == 'CORRELATION' : rxy = CORRELATION ( X , Y , maxlags = lag , norm = norm ) elif correlation_method == 'xcorr' : rxy , _l = xcorr ( X , Y , maxlags = lag , norm = norm ) rxy = rxy [ lag : ] # keep track of the first elt. psd [ 0 ] = rxy [ 0 ] # create the first part of the PSD psd [ 1 : lag + 1 ] = rxy [ 1 : ] * w # create the second part. # First, we need to compute the auto or cross correlation ryx if crosscorrelation is True : # compute the cross correlation if correlation_method == 'CORRELATION' : ryx = CORRELATION ( Y , X , maxlags = lag , norm = norm ) elif correlation_method == 'xcorr' : ryx , _l = xcorr ( Y , X , maxlags = lag , norm = norm ) ryx = ryx [ lag : ] #print len(ryx), len(psd[-1:NPSD-lag-1:-1]) psd [ - 1 : NFFT - lag - 1 : - 1 ] = ryx [ 1 : ] . conjugate ( ) * w else : #autocorrelation no additional correlation call required psd [ - 1 : NFFT - lag - 1 : - 1 ] = rxy [ 1 : ] . conjugate ( ) * w psd = numpy . real ( fft ( psd ) ) return psd
11574	def digital_message ( self , data ) : port = data [ 0 ] port_data = ( data [ self . MSB ] << 7 ) + data [ self . LSB ] # set all the pins for this reporting port # get the first pin number for this report pin = port * 8 for pin in range ( pin , min ( pin + 8 , self . total_pins_discovered ) ) : # shift through all the bit positions and set the digital response table with self . pymata . data_lock : # look at the previously stored value for this pin prev_data = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] # get the current value self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = port_data & 0x01 # if the values differ and callback is enabled for the pin, then send out the callback if prev_data != port_data & 0x01 : callback = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_CALLBACK ] if callback : callback ( [ self . pymata . DIGITAL , pin , self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] ] ) # determine if the latch data table needs to be updated for each pin latching_entry = self . digital_latch_table [ pin ] if latching_entry [ self . LATCH_STATE ] == self . LATCH_ARMED : if latching_entry [ self . LATCHED_THRESHOLD_TYPE ] == self . DIGITAL_LATCH_LOW : if ( port_data & 0x01 ) == 0 : if latching_entry [ self . DIGITAL_LATCH_CALLBACK ] is not None : self . digital_latch_table [ pin ] = [ 0 , 0 , 0 , 0 , None ] latching_entry [ self . DIGITAL_LATCH_CALLBACK ] ( [ self . pymata . OUTPUT | self . pymata . LATCH_MODE , pin , 0 , time . time ( ) ] ) else : updated_latch_entry = latching_entry updated_latch_entry [ self . LATCH_STATE ] = self . LATCH_LATCHED updated_latch_entry [ self . DIGITAL_LATCHED_DATA ] = self . DIGITAL_LATCH_LOW # time stamp it updated_latch_entry [ self . DIGITAL_TIME_STAMP ] = time . time ( ) else : pass elif latching_entry [ self . LATCHED_THRESHOLD_TYPE ] == self . DIGITAL_LATCH_HIGH : if port_data & 0x01 : if latching_entry [ self . DIGITAL_LATCH_CALLBACK ] is not None : self . digital_latch_table [ pin ] = [ 0 , 0 , 0 , 0 , None ] latching_entry [ self . DIGITAL_LATCH_CALLBACK ] ( [ self . pymata . OUTPUT | self . pymata . LATCH_MODE , pin , 1 , time . time ( ) ] ) else : updated_latch_entry = latching_entry updated_latch_entry [ self . LATCH_STATE ] = self . LATCH_LATCHED updated_latch_entry [ self . DIGITAL_LATCHED_DATA ] = self . DIGITAL_LATCH_HIGH # time stamp it updated_latch_entry [ self . DIGITAL_TIME_STAMP ] = time . time ( ) else : pass else : pass # get the next data bit port_data >>= 1
8885	def fit ( self , x , y = None ) : if self . _dtype is not None : iter2array ( x , dtype = self . _dtype ) else : iter2array ( x ) return self
10588	def _get_account_and_descendants_ ( self , account , result ) : result . append ( account ) for child in account . accounts : self . _get_account_and_descendants_ ( child , result )
7361	async def _connect ( self ) : logger . debug ( "connecting to the stream" ) await self . client . setup if self . session is None : self . session = self . client . _session kwargs = await self . client . headers . prepare_request ( * * self . kwargs ) request = self . client . error_handler ( self . session . request ) return await request ( timeout = 0 , * * kwargs )
12369	def rename ( self , id , name ) : return super ( DomainRecords , self ) . update ( id , name = name ) [ self . singular ]
1052	def print_stack ( f = None , limit = None , file = None ) : if f is None : try : raise ZeroDivisionError except ZeroDivisionError : f = sys . exc_info ( ) [ 2 ] . tb_frame . f_back print_list ( extract_stack ( f , limit ) , file )
5668	def stop_to_stop_network_for_route_type ( gtfs , route_type , link_attributes = None , start_time_ut = None , end_time_ut = None ) : if link_attributes is None : link_attributes = DEFAULT_STOP_TO_STOP_LINK_ATTRIBUTES assert ( route_type in route_types . TRANSIT_ROUTE_TYPES ) stops_dataframe = gtfs . get_stops_for_route_type ( route_type ) net = networkx . DiGraph ( ) _add_stops_to_net ( net , stops_dataframe ) events_df = gtfs . get_transit_events ( start_time_ut = start_time_ut , end_time_ut = end_time_ut , route_type = route_type ) if len ( net . nodes ( ) ) < 2 : assert events_df . shape [ 0 ] == 0 # group events by links, and loop over them (i.e. each link): link_event_groups = events_df . groupby ( [ 'from_stop_I' , 'to_stop_I' ] , sort = False ) for key , link_events in link_event_groups : from_stop_I , to_stop_I = key assert isinstance ( link_events , pd . DataFrame ) # 'dep_time_ut' 'arr_time_ut' 'shape_id' 'route_type' 'trip_I' 'duration' 'from_seq' 'to_seq' if link_attributes is None : net . add_edge ( from_stop_I , to_stop_I ) else : link_data = { } if "duration_min" in link_attributes : link_data [ 'duration_min' ] = float ( link_events [ 'duration' ] . min ( ) ) if "duration_max" in link_attributes : link_data [ 'duration_max' ] = float ( link_events [ 'duration' ] . max ( ) ) if "duration_median" in link_attributes : link_data [ 'duration_median' ] = float ( link_events [ 'duration' ] . median ( ) ) if "duration_avg" in link_attributes : link_data [ 'duration_avg' ] = float ( link_events [ 'duration' ] . mean ( ) ) # statistics on numbers of vehicles: if "n_vehicles" in link_attributes : link_data [ 'n_vehicles' ] = int ( link_events . shape [ 0 ] ) if "capacity_estimate" in link_attributes : link_data [ 'capacity_estimate' ] = route_types . ROUTE_TYPE_TO_APPROXIMATE_CAPACITY [ route_type ] * int ( link_events . shape [ 0 ] ) if "d" in link_attributes : from_lat = net . node [ from_stop_I ] [ 'lat' ] from_lon = net . node [ from_stop_I ] [ 'lon' ] to_lat = net . node [ to_stop_I ] [ 'lat' ] to_lon = net . node [ to_stop_I ] [ 'lon' ] distance = wgs84_distance ( from_lat , from_lon , to_lat , to_lon ) link_data [ 'd' ] = int ( distance ) if "distance_shape" in link_attributes : assert "shape_id" in link_events . columns . values found = None for i , shape_id in enumerate ( link_events [ "shape_id" ] . values ) : if shape_id is not None : found = i break if found is None : link_data [ "distance_shape" ] = None else : link_event = link_events . iloc [ found ] distance = gtfs . get_shape_distance_between_stops ( link_event [ "trip_I" ] , int ( link_event [ "from_seq" ] ) , int ( link_event [ "to_seq" ] ) ) link_data [ 'distance_shape' ] = distance if "route_I_counts" in link_attributes : link_data [ "route_I_counts" ] = link_events . groupby ( "route_I" ) . size ( ) . to_dict ( ) net . add_edge ( from_stop_I , to_stop_I , attr_dict = link_data ) return net
9072	def build_engine_session ( connection , echo = False , autoflush = None , autocommit = None , expire_on_commit = None , scopefunc = None ) : if connection is None : raise ValueError ( 'can not build engine when connection is None' ) engine = create_engine ( connection , echo = echo ) autoflush = autoflush if autoflush is not None else False autocommit = autocommit if autocommit is not None else False expire_on_commit = expire_on_commit if expire_on_commit is not None else True log . debug ( 'auto flush: %s, auto commit: %s, expire on commmit: %s' , autoflush , autocommit , expire_on_commit ) #: A SQLAlchemy session maker session_maker = sessionmaker ( bind = engine , autoflush = autoflush , autocommit = autocommit , expire_on_commit = expire_on_commit , ) #: A SQLAlchemy session object session = scoped_session ( session_maker , scopefunc = scopefunc ) return engine , session
11449	def send ( self , recipient , message ) : if self . _logindata [ 'login_rufnummer' ] is None or self . _logindata [ 'login_passwort' ] is None : err_mess = "YesssSMS: Login data required" raise self . LoginError ( err_mess ) if not recipient : raise self . NoRecipientError ( "YesssSMS: recipient number missing" ) if not isinstance ( recipient , str ) : raise ValueError ( "YesssSMS: str expected as recipient number" ) if not message : raise self . EmptyMessageError ( "YesssSMS: message is empty" ) with self . _login ( requests . Session ( ) ) as sess : sms_data = { 'to_nummer' : recipient , 'nachricht' : message } req = sess . post ( self . _websms_url , data = sms_data ) if not ( req . status_code == 200 or req . status_code == 302 ) : raise self . SMSSendingError ( "YesssSMS: error sending SMS" ) if _UNSUPPORTED_CHARS_STRING in req . text : raise self . UnsupportedCharsError ( "YesssSMS: message contains unsupported character(s)" ) if _SMS_SENDING_SUCCESSFUL_STRING not in req . text : raise self . SMSSendingError ( "YesssSMS: error sending SMS" ) sess . get ( self . _logout_url )
11473	def parse_data ( self , text , maxwidth , maxheight , template_dir , context , urlize_all_links ) : block_parser = TextBlockParser ( ) lines = text . splitlines ( ) parsed = [ ] for line in lines : if STANDALONE_URL_RE . match ( line ) : user_url = line . strip ( ) try : resource = oembed . site . embed ( user_url , maxwidth = maxwidth , maxheight = maxheight ) context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) except OEmbedException : if urlize_all_links : line = '<a href="%(LINK)s">%(LINK)s</a>' % { 'LINK' : user_url } else : context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) line = self . render_oembed ( resource , user_url , template_dir = template_dir , context = context ) else : line = block_parser . parse ( line , maxwidth , maxheight , 'inline' , context , urlize_all_links ) parsed . append ( line ) return mark_safe ( '\n' . join ( parsed ) )
13445	def create_admin ( username = 'admin' , email = 'admin@admin.com' , password = 'admin' ) : admin = User . objects . create_user ( username , email , password ) admin . is_staff = True admin . is_superuser = True admin . save ( ) return admin
3501	def assess_precursors ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : warn ( 'use assess_component instead' , DeprecationWarning ) return assess_component ( model , reaction , 'reactants' , flux_coefficient_cutoff , solver )
8039	def is_public ( self ) : if self . all is not None : return self . name in self . all else : return not self . name . startswith ( "_" )
7694	def _sasl_authenticate ( self , stream , username , authzid ) : if not stream . initiator : raise SASLAuthenticationFailed ( "Only initiating entity start" " SASL authentication" ) if stream . features is None or not self . peer_sasl_mechanisms : raise SASLNotAvailable ( "Peer doesn't support SASL" ) props = dict ( stream . auth_properties ) if not props . get ( "service-domain" ) and ( stream . peer and stream . peer . domain ) : props [ "service-domain" ] = stream . peer . domain if username is not None : props [ "username" ] = username if authzid is not None : props [ "authzid" ] = authzid if "password" in self . settings : props [ "password" ] = self . settings [ "password" ] props [ "available_mechanisms" ] = self . peer_sasl_mechanisms enabled = sasl . filter_mechanism_list ( self . settings [ 'sasl_mechanisms' ] , props , self . settings [ 'insecure_auth' ] ) if not enabled : raise SASLNotAvailable ( "None of SASL mechanism selected can be used" ) props [ "enabled_mechanisms" ] = enabled mechanism = None for mech in enabled : if mech in self . peer_sasl_mechanisms : mechanism = mech break if not mechanism : raise SASLMechanismNotAvailable ( "Peer doesn't support any of" " our SASL mechanisms" ) logger . debug ( "Our mechanism: {0!r}" . format ( mechanism ) ) stream . auth_method_used = mechanism self . authenticator = sasl . client_authenticator_factory ( mechanism ) initial_response = self . authenticator . start ( props ) if not isinstance ( initial_response , sasl . Response ) : raise SASLAuthenticationFailed ( "SASL initiation failed" ) element = ElementTree . Element ( AUTH_TAG ) element . set ( "mechanism" , mechanism ) if initial_response . data : if initial_response . encode : element . text = initial_response . encode ( ) else : element . text = initial_response . data stream . write_element ( element )
4296	def parse_config_file ( parser , stdin_args ) : config_args = [ ] # Temporary switch required args and save them to restore. required_args = [ ] for action in parser . _actions : if action . required : required_args . append ( action ) action . required = False parsed_args = parser . parse_args ( stdin_args ) # Restore required args. for action in required_args : action . required = True if not parsed_args . config_file : return config_args config = ConfigParser ( ) if not config . read ( parsed_args . config_file ) : sys . stderr . write ( 'Config file "{0}" doesn\'t exists\n' . format ( parsed_args . config_file ) ) sys . exit ( 7 ) # It isn't used anywhere. config_args = _convert_config_to_stdin ( config , parser ) return config_args
6178	def map_chunk ( func , array , out_array ) : for slice in iter_chunk_slice ( array . shape [ - 1 ] , array . chunkshape [ - 1 ] ) : out_array . append ( func ( array [ ... , slice ] ) ) return out_array
11225	def dump_OrderedDict ( self , obj , class_name = "collections.OrderedDict" ) : return { "$" + class_name : [ ( key , self . _json_convert ( value ) ) for key , value in iteritems ( obj ) ] }
8876	def compute_dosage ( expec , alt = None ) : if alt is None : return expec [ ... , - 1 ] try : return expec [ : , alt ] except NotImplementedError : alt = asarray ( alt , int ) return asarray ( expec , float ) [ : , alt ]
935	def readFromCheckpoint ( cls , checkpointDir ) : checkpointPath = cls . _getModelCheckpointFilePath ( checkpointDir ) with open ( checkpointPath , 'r' ) as f : proto = cls . getSchema ( ) . read ( f , traversal_limit_in_words = _TRAVERSAL_LIMIT_IN_WORDS ) model = cls . read ( proto ) return model
2898	def get_task ( self , id ) : tasks = [ task for task in self . get_tasks ( ) if task . id == id ] return tasks [ 0 ] if len ( tasks ) == 1 else None
10633	def get_compound_afrs ( self ) : result = self . _compound_mfrs * 1.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) result [ index ] = stoich . amount ( compound , result [ index ] ) return result
11904	def rand_blend_mask ( shape , rand = rand . uniform ( - 10 , 10 ) , * * kwargs ) : # batch, channel = shape[0], shape[3] z = rand ( shape [ 0 ] ) # seed noise = snoise2dz ( ( shape [ 1 ] , shape [ 2 ] ) , z , * * kwargs ) return noise
7626	def transcription ( ref , est , * * kwargs ) : namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_intervals , ref_p = ref . to_interval_values ( ) est_intervals , est_p = est . to_interval_values ( ) ref_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . transcription . evaluate ( ref_intervals , ref_pitches , est_intervals , est_pitches , * * kwargs )
13384	def get_store_env_tmp ( ) : tempdir = tempfile . gettempdir ( ) temp_name = 'envstore{0:0>3d}' temp_path = unipath ( tempdir , temp_name . format ( random . getrandbits ( 9 ) ) ) if not os . path . exists ( temp_path ) : return temp_path else : return get_store_env_tmp ( )
3526	def kiss_metrics ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return KissMetricsNode ( )
3577	def disconnect_devices ( self , service_uuids ) : # Get list of connected devices with specified services. cbuuids = map ( uuid_to_cbuuid , service_uuids ) for device in self . _central_manager . retrieveConnectedPeripheralsWithServices_ ( cbuuids ) : self . _central_manager . cancelPeripheralConnection_ ( device )
5517	def limit ( self , value ) : self . _limit = value self . _start = None self . _sum = 0
3693	def Tb ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ PSAT_DEFINITION ] ) : def list_methods ( ) : methods = [ ] if CASRN in CRC_inorganic_data . index and not np . isnan ( CRC_inorganic_data . at [ CASRN , 'Tb' ] ) : methods . append ( CRC_INORG ) if CASRN in CRC_organic_data . index and not np . isnan ( CRC_organic_data . at [ CASRN , 'Tb' ] ) : methods . append ( CRC_ORG ) if CASRN in Yaws_data . index : methods . append ( YAWS ) if PSAT_DEFINITION not in IgnoreMethods : try : # For some chemicals, vapor pressure range will exclude Tb VaporPressure ( CASRN = CASRN ) . solve_prop ( 101325. ) methods . append ( PSAT_DEFINITION ) except : # pragma: no cover pass if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CRC_INORG : return float ( CRC_inorganic_data . at [ CASRN , 'Tb' ] ) elif Method == CRC_ORG : return float ( CRC_organic_data . at [ CASRN , 'Tb' ] ) elif Method == YAWS : return float ( Yaws_data . at [ CASRN , 'Tb' ] ) elif Method == PSAT_DEFINITION : return VaporPressure ( CASRN = CASRN ) . solve_prop ( 101325. ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
4256	def get_compressed_filename ( self , filename ) : if not os . path . splitext ( filename ) [ 1 ] [ 1 : ] in self . suffixes_to_compress : return False file_stats = None compressed_stats = None compressed_filename = '{}.{}' . format ( filename , self . suffix ) try : file_stats = os . stat ( filename ) compressed_stats = os . stat ( compressed_filename ) except OSError : # FileNotFoundError is for Python3 only pass if file_stats and compressed_stats : return ( compressed_filename if file_stats . st_mtime > compressed_stats . st_mtime else False ) else : return compressed_filename
1861	def MOVS ( cpu , dest , src ) : base , size , ty = cpu . get_descriptor ( cpu . DS ) src_addr = src . address ( ) + base dest_addr = dest . address ( ) + base src_reg = src . mem . base dest_reg = dest . mem . base size = dest . size # Copy the data dest . write ( src . read ( ) ) #Advance EDI/ESI pointers increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
8430	def cmap_d_pal ( name = None , lut = None ) : colormap = get_cmap ( name , lut ) if not isinstance ( colormap , mcolors . ListedColormap ) : raise ValueError ( "For a discrete palette, cmap must be of type " "matplotlib.colors.ListedColormap" ) ncolors = len ( colormap . colors ) def _cmap_d_pal ( n ) : if n > ncolors : raise ValueError ( "cmap `{}` has {} colors you requested {} " "colors." . format ( name , ncolors , n ) ) if ncolors < 256 : return [ mcolors . rgb2hex ( c ) for c in colormap . colors [ : n ] ] else : # Assume these are continuous and get colors equally spaced # intervals e.g. viridis is defined with 256 colors idx = np . linspace ( 0 , ncolors - 1 , n ) . round ( ) . astype ( int ) return [ mcolors . rgb2hex ( colormap . colors [ i ] ) for i in idx ] return _cmap_d_pal
6909	def xieta_from_radecl ( inra , indecl , incenterra , incenterdecl , deg = True ) : if deg : ra = np . radians ( inra ) decl = np . radians ( indecl ) centerra = np . radians ( incenterra ) centerdecl = np . radians ( incenterdecl ) else : ra = inra decl = indecl centerra = incenterra centerdecl = incenterdecl cdecc = np . cos ( centerdecl ) sdecc = np . sin ( centerdecl ) crac = np . cos ( centerra ) srac = np . sin ( centerra ) uu = np . cos ( decl ) * np . cos ( ra ) vv = np . cos ( decl ) * np . sin ( ra ) ww = np . sin ( decl ) uun = uu * cdecc * crac + vv * cdecc * srac + ww * sdecc vvn = - uu * srac + vv * crac wwn = - uu * sdecc * crac - vv * sdecc * srac + ww * cdecc denom = vvn * vvn + wwn * wwn aunn = np . zeros_like ( uun ) aunn [ uun >= 1.0 ] = 0.0 aunn [ uun < 1.0 ] = np . arccos ( uun ) xi , eta = np . zeros_like ( aunn ) , np . zeros_like ( aunn ) xi [ ( aunn <= 0.0 ) | ( denom <= 0.0 ) ] = 0.0 eta [ ( aunn <= 0.0 ) | ( denom <= 0.0 ) ] = 0.0 sdenom = np . sqrt ( denom ) xi [ ( aunn > 0.0 ) | ( denom > 0.0 ) ] = aunn * vvn / sdenom eta [ ( aunn > 0.0 ) | ( denom > 0.0 ) ] = aunn * wwn / sdenom if deg : return np . degrees ( xi ) , np . degrees ( eta ) else : return xi , eta
3467	def functional ( self ) : if self . _model : tree , _ = parse_gpr ( self . gene_reaction_rule ) return eval_gpr ( tree , { gene . id for gene in self . genes if not gene . functional } ) return True
5307	def rgb_to_ansi16 ( r , g , b , use_bright = False ) : ansi_b = round ( b / 255.0 ) << 2 ansi_g = round ( g / 255.0 ) << 1 ansi_r = round ( r / 255.0 ) ansi = ( 90 if use_bright else 30 ) + ( ansi_b | ansi_g | ansi_r ) return ansi
7743	def _prepare_io_handler_cb ( self , handler ) : self . _anything_done = True logger . debug ( "_prepar_io_handler_cb called for {0!r}" . format ( handler ) ) self . _configure_io_handler ( handler ) self . _prepare_sources . pop ( handler , None ) return False
2955	def update ( self , containers ) : self . _containers = deepcopy ( containers ) self . __write ( containers , initialize = False )
2940	def deserialize_condition ( self , workflow , start_node ) : # Collect all information. condition = None spec_name = None for node in start_node . childNodes : if node . nodeType != minidom . Node . ELEMENT_NODE : continue if node . nodeName . lower ( ) == 'successor' : if spec_name is not None : _exc ( 'Duplicate task name %s' % spec_name ) if node . firstChild is None : _exc ( 'Successor tag without a task name' ) spec_name = node . firstChild . nodeValue elif node . nodeName . lower ( ) in _op_map : if condition is not None : _exc ( 'Multiple conditions are not yet supported' ) condition = self . deserialize_logical ( node ) else : _exc ( 'Unknown node: %s' % node . nodeName ) if condition is None : _exc ( 'Missing condition in conditional statement' ) if spec_name is None : _exc ( 'A %s has no task specified' % start_node . nodeName ) return condition , spec_name
13745	def get_table ( self ) : if hasattr ( self , '_table' ) : table = self . _table else : try : table = self . conn . get_table ( self . get_table_name ( ) ) except boto . exception . DynamoDBResponseError : if self . auto_create_table : table = self . create_table ( ) else : raise self . _table = table return table
11100	def select_by_atime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . atime <= max_time return self . select_file ( filters , recursive )
12412	def write ( self , chunk , serialize = False , format = None ) : # Ensure we're not closed. self . require_not_closed ( ) if chunk is None : # There is nothing here. return if serialize or format is not None : # Forward to the serializer to serialize the chunk # before it gets written to the response. self . serialize ( chunk , format = format ) return # `serialize` invokes write(...) if type ( chunk ) is six . binary_type : # Update the stream length. self . _length += len ( chunk ) # If passed a byte string, we hope the user encoded it properly. self . _stream . write ( chunk ) elif isinstance ( chunk , six . string_types ) : encoding = self . encoding if encoding is not None : # If passed a string, we can encode it for the user. chunk = chunk . encode ( encoding ) else : # Bail; we don't have an encoding. raise exceptions . InvalidOperation ( 'Attempting to write textual data without an encoding.' ) # Update the stream length. self . _length += len ( chunk ) # Write the encoded data into the byte stream. self . _stream . write ( chunk ) elif isinstance ( chunk , collections . Iterable ) : # If passed some kind of iterator, attempt to recurse into # oblivion. for section in chunk : self . write ( section ) else : # Bail; we have no idea what to do with this. raise exceptions . InvalidOperation ( 'Attempting to write something not recognized.' )
1320	def GetTopLevelControl ( self ) -> 'Control' : handle = self . NativeWindowHandle if handle : topHandle = GetAncestor ( handle , GAFlag . Root ) if topHandle : if topHandle == handle : return self else : return ControlFromHandle ( topHandle ) else : #self is root control pass else : control = self while True : control = control . GetParentControl ( ) handle = control . NativeWindowHandle if handle : topHandle = GetAncestor ( handle , GAFlag . Root ) return ControlFromHandle ( topHandle )
13239	def intervals ( self , range_start = datetime . datetime . min , range_end = datetime . datetime . max ) : # At the moment the algorithm works on periods split by calendar day, one at a time, # merging them if they're continuous; to avoid looping infinitely for infinitely long # periods, it splits periods as soon as they reach 60 days. # This algorithm could likely be improved to get rid of this restriction and improve # efficiency, so code should not rely on this behaviour. current_period = None max_continuous_days = 60 range_start = self . to_timezone ( range_start ) range_end = self . to_timezone ( range_end ) for period in self . _daily_periods ( range_start . date ( ) , range_end . date ( ) ) : if period . end < range_start or period . start > range_end : continue if current_period is None : current_period = period else : if ( ( ( period . start < current_period . end ) or ( period . start - current_period . end ) <= datetime . timedelta ( minutes = 1 ) ) and ( current_period . end - current_period . start ) < datetime . timedelta ( days = max_continuous_days ) ) : # Merge current_period = Period ( current_period . start , period . end ) else : yield current_period current_period = period if current_period : yield current_period
2768	def get_volume ( self , volume_id ) : return Volume . get_object ( api_token = self . token , volume_id = volume_id )
7537	def branch_assembly ( args , parsedict ) : ## Get the current assembly data = getassembly ( args , parsedict ) ## get arguments to branch command bargs = args . branch ## get new name, trim off .txt if it was accidentally added newname = bargs [ 0 ] if newname . endswith ( ".txt" ) : newname = newname [ : - 4 ] ## look for subsamples if len ( bargs ) > 1 : ## Branching and subsampling at step 6 is a bad idea, it messes up ## indexing into the hdf5 cluster file. Warn against this. if any ( [ x . stats . state == 6 for x in data . samples . values ( ) ] ) : pass ## TODODODODODO #print("wat") ## are we removing or keeping listed samples? subsamples = bargs [ 1 : ] ## drop the matching samples if bargs [ 1 ] == "-" : ## check drop names fails = [ i for i in subsamples [ 1 : ] if i not in data . samples . keys ( ) ] if any ( fails ) : raise IPyradWarningExit ( "\ \n Failed: unrecognized names requested, check spelling:\n {}" . format ( "\n " . join ( [ i for i in fails ] ) ) ) print ( " dropping {} samples" . format ( len ( subsamples ) - 1 ) ) subsamples = list ( set ( data . samples . keys ( ) ) - set ( subsamples ) ) ## If the arg after the new param name is a file that exists if os . path . exists ( bargs [ 1 ] ) : new_data = data . branch ( newname , infile = bargs [ 1 ] ) else : new_data = data . branch ( newname , subsamples ) ## keeping all samples else : new_data = data . branch ( newname , None ) print ( " creating a new branch called '{}' with {} Samples" . format ( new_data . name , len ( new_data . samples ) ) ) print ( " writing new params file to {}" . format ( "params-" + new_data . name + ".txt\n" ) ) new_data . write_params ( "params-" + new_data . name + ".txt" , force = args . force )
5645	def make_views ( cls , conn ) : conn . execute ( 'DROP VIEW IF EXISTS main.day_trips' ) conn . execute ( 'CREATE VIEW day_trips AS ' 'SELECT day_trips2.*, trips.* ' #'days.day_start_ut+trips.start_time_ds AS start_time_ut, ' #'days.day_start_ut+trips.end_time_ds AS end_time_ut ' 'FROM day_trips2 JOIN trips USING (trip_I);' ) conn . commit ( ) conn . execute ( 'DROP VIEW IF EXISTS main.day_stop_times' ) conn . execute ( 'CREATE VIEW day_stop_times AS ' 'SELECT day_trips2.*, trips.*, stop_times.*, ' #'days.day_start_ut+trips.start_time_ds AS start_time_ut, ' #'days.day_start_ut+trips.end_time_ds AS end_time_ut, ' 'day_trips2.day_start_ut+stop_times.arr_time_ds AS arr_time_ut, ' 'day_trips2.day_start_ut+stop_times.dep_time_ds AS dep_time_ut ' 'FROM day_trips2 ' 'JOIN trips USING (trip_I) ' 'JOIN stop_times USING (trip_I)' ) conn . commit ( )
3009	def has_credentials ( self ) : credentials = _credentials_from_request ( self . request ) return ( credentials and not credentials . invalid and credentials . has_scopes ( self . _get_scopes ( ) ) )
5196	def configure_stack ( ) : stack_config = asiodnp3 . OutstationStackConfig ( opendnp3 . DatabaseSizes . AllTypes ( 10 ) ) stack_config . outstation . eventBufferConfig = opendnp3 . EventBufferConfig ( ) . AllTypes ( 10 ) stack_config . outstation . params . allowUnsolicited = True stack_config . link . LocalAddr = 10 stack_config . link . RemoteAddr = 1 stack_config . link . KeepAliveTimeout = openpal . TimeDuration ( ) . Max ( ) return stack_config
6610	def getArrays ( self , tree , branchName ) : itsArray = self . _getArray ( tree , branchName ) if itsArray is None : return None , None itsCountArray = self . _getCounterArray ( tree , branchName ) return itsArray , itsCountArray
314	def rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 6 ) : if factor_returns . ndim > 1 : # Apply column-wise return factor_returns . apply ( partial ( rolling_beta , returns ) , rolling_window = rolling_window ) else : out = pd . Series ( index = returns . index ) for beg , end in zip ( returns . index [ 0 : - rolling_window ] , returns . index [ rolling_window : ] ) : out . loc [ end ] = ep . beta ( returns . loc [ beg : end ] , factor_returns . loc [ beg : end ] ) return out
10420	def count_unique_relations ( graph : BELGraph ) -> Counter : return Counter ( itt . chain . from_iterable ( get_edge_relations ( graph ) . values ( ) ) )
10961	def create_img ( ) : # 1. particles + coverslip rad = 0.5 * np . random . randn ( POS . shape [ 0 ] ) + 4.5 # 4.5 +- 0.5 px particles part = objs . PlatonicSpheresCollection ( POS , rad , zscale = 0.89 ) slab = objs . Slab ( zpos = 4.92 , angles = ( - 4.7e-3 , - 7.3e-4 ) ) objects = comp . ComponentCollection ( [ part , slab ] , category = 'obj' ) # 2. psf, ilm p = exactpsf . FixedSSChebLinePSF ( kfki = 1.07 , zslab = - 29.3 , alpha = 1.17 , n2n1 = 0.98 , sigkf = - 0.33 , zscale = 0.89 , laser_wavelength = 0.45 ) i = ilms . BarnesStreakLegPoly2P1D ( npts = ( 16 , 10 , 8 , 4 ) , zorder = 8 ) b = ilms . LegendrePoly2P1D ( order = ( 7 , 2 , 2 ) , category = 'bkg' ) off = comp . GlobalScalar ( name = 'offset' , value = - 2.11 ) mdl = models . ConfocalImageModel ( ) st = states . ImageState ( util . NullImage ( shape = [ 48 , 64 , 64 ] ) , [ objects , p , i , b , off ] , mdl = mdl , model_as_data = True ) b . update ( b . params , BKGVALS ) i . update ( i . params , ILMVALS ) im = st . model + np . random . randn ( * st . model . shape ) * 0.03 return util . Image ( im )
9541	def datetime_range_inclusive ( min , max , format ) : dmin = datetime . strptime ( min , format ) dmax = datetime . strptime ( max , format ) def checker ( v ) : dv = datetime . strptime ( v , format ) if dv < dmin or dv > dmax : raise ValueError ( v ) return checker
12426	def _expand_targets ( self , targets , base_dir = None ) : all_targets = [ ] for target in targets : target_dirs = [ p for p in [ base_dir , os . path . dirname ( target ) ] if p ] target_dir = target_dirs and os . path . join ( * target_dirs ) or '' target = os . path . basename ( target ) target_path = os . path . join ( target_dir , target ) if os . path . exists ( target_path ) : all_targets . append ( target_path ) with open ( target_path ) as fp : for line in fp : if line . startswith ( '-r ' ) : _ , new_target = line . split ( ' ' , 1 ) all_targets . extend ( self . _expand_targets ( [ new_target . strip ( ) ] , base_dir = target_dir ) ) return all_targets
5508	def worker ( f ) : @ functools . wraps ( f ) async def wrapper ( cls , connection , rest ) : try : await f ( cls , connection , rest ) except asyncio . CancelledError : connection . response ( "426" , "transfer aborted" ) connection . response ( "226" , "abort successful" ) return wrapper
5237	def get_interval ( ticker , session ) -> Session : if '_' not in session : session = f'{session}_normal_0_0' interval = Intervals ( ticker = ticker ) ss_info = session . split ( '_' ) return getattr ( interval , f'market_{ss_info.pop(1)}' ) ( * ss_info )
140	def to_keypoints ( self ) : # TODO get rid of this deferred import from imgaug . augmentables . kps import Keypoint return [ Keypoint ( x = point [ 0 ] , y = point [ 1 ] ) for point in self . exterior ]
12493	def check_array ( array , accept_sparse = None , dtype = None , order = None , copy = False , force_all_finite = True , ensure_2d = True , allow_nd = False ) : if isinstance ( accept_sparse , str ) : accept_sparse = [ accept_sparse ] if sp . issparse ( array ) : array = _ensure_sparse_format ( array , accept_sparse , dtype , order , copy , force_all_finite ) else : if ensure_2d : array = np . atleast_2d ( array ) array = np . array ( array , dtype = dtype , order = order , copy = copy ) if not allow_nd and array . ndim >= 3 : raise ValueError ( "Found array with dim %d. Expected <= 2" % array . ndim ) if force_all_finite : _assert_all_finite ( array ) return array
4555	def genVector ( width , height , x_mult = 1 , y_mult = 1 ) : center_x = ( width - 1 ) / 2 center_y = ( height - 1 ) / 2 def length ( x , y ) : dx = math . pow ( x - center_x , 2 * x_mult ) dy = math . pow ( y - center_y , 2 * y_mult ) return int ( math . sqrt ( dx + dy ) ) return [ [ length ( x , y ) for x in range ( width ) ] for y in range ( height ) ]
11229	def after ( self , dt , inc = False ) : if self . _cache_complete : gen = self . _cache else : gen = self if inc : for i in gen : if i >= dt : return i else : for i in gen : if i > dt : return i return None
3828	async def query_presence ( self , query_presence_request ) : response = hangouts_pb2 . QueryPresenceResponse ( ) await self . _pb_request ( 'presence/querypresence' , query_presence_request , response ) return response
10364	def has_degradation_increases_activity ( data : Dict ) -> bool : return part_has_modifier ( data , SUBJECT , DEGRADATION ) and part_has_modifier ( data , OBJECT , ACTIVITY )
10912	def vectorize_damping ( params , damping = 1.0 , increase_list = [ [ 'psf-' , 1e4 ] ] ) : damp_vec = np . ones ( len ( params ) ) * damping for nm , fctr in increase_list : for a in range ( damp_vec . size ) : if nm in params [ a ] : damp_vec [ a ] *= fctr return damp_vec
3541	def do_apply ( mutation_pk , dict_synonyms , backup ) : filename , mutation_id = filename_and_mutation_id_from_pk ( int ( mutation_pk ) ) update_line_numbers ( filename ) context = Context ( mutation_id = mutation_id , filename = filename , dict_synonyms = dict_synonyms , ) mutate_file ( backup = backup , context = context , ) if context . number_of_performed_mutations == 0 : raise RuntimeError ( 'No mutations performed.' )
13013	def strip_labels ( filename ) : labels = [ ] with open ( filename ) as f , open ( 'processed_labels.txt' , 'w' ) as f1 : for l in f : if l . startswith ( '#' ) : next l = l . replace ( " ." , '' ) l = l . replace ( ">\tskos:prefLabel\t" , ' ' ) l = l . replace ( "<" , '' ) l = l . replace ( ">\trdfs:label\t" , ' ' ) f1 . write ( l )
3	def make_vec_env ( env_id , env_type , num_env , seed , wrapper_kwargs = None , start_index = 0 , reward_scale = 1.0 , flatten_dict_observations = True , gamestate = None ) : wrapper_kwargs = wrapper_kwargs or { } mpi_rank = MPI . COMM_WORLD . Get_rank ( ) if MPI else 0 seed = seed + 10000 * mpi_rank if seed is not None else None logger_dir = logger . get_dir ( ) def make_thunk ( rank ) : return lambda : make_env ( env_id = env_id , env_type = env_type , mpi_rank = mpi_rank , subrank = rank , seed = seed , reward_scale = reward_scale , gamestate = gamestate , flatten_dict_observations = flatten_dict_observations , wrapper_kwargs = wrapper_kwargs , logger_dir = logger_dir ) set_global_seeds ( seed ) if num_env > 1 : return SubprocVecEnv ( [ make_thunk ( i + start_index ) for i in range ( num_env ) ] ) else : return DummyVecEnv ( [ make_thunk ( start_index ) ] )
7587	def taxon_table ( self ) : if self . tests : keys = sorted ( self . tests [ 0 ] . keys ( ) ) if isinstance ( self . tests , list ) : ld = [ [ ( key , i [ key ] ) for key in keys ] for i in self . tests ] dd = [ dict ( i ) for i in ld ] df = pd . DataFrame ( dd ) return df else : return pd . DataFrame ( pd . Series ( self . tests ) ) . T else : return None
13618	def get_branches ( self ) : return [ self . _sanitize ( branch ) for branch in self . _git . branch ( color = "never" ) . splitlines ( ) ]
2695	def parse_doc ( json_iter ) : global DEBUG for meta in json_iter : base_idx = 0 for graf_text in filter_quotes ( meta [ "text" ] , is_email = False ) : if DEBUG : print ( "graf_text:" , graf_text ) grafs , new_base_idx = parse_graf ( meta [ "id" ] , graf_text , base_idx ) base_idx = new_base_idx for graf in grafs : yield graf
12266	def docstring ( docstr ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : return func ( * args , * * kwargs ) wrapper . __doc__ = docstr return wrapper return decorator
10823	def query_requests ( cls , admin , eager = False ) : # Get direct pending request if hasattr ( admin , 'is_superadmin' ) and admin . is_superadmin : q1 = GroupAdmin . query . with_entities ( GroupAdmin . group_id ) else : q1 = GroupAdmin . query_by_admin ( admin ) . with_entities ( GroupAdmin . group_id ) q2 = Membership . query . filter ( Membership . state == MembershipState . PENDING_ADMIN , Membership . id_group . in_ ( q1 ) , ) # Get request from admin groups your are member of q3 = Membership . query_by_user ( user = admin , state = MembershipState . ACTIVE ) . with_entities ( Membership . id_group ) q4 = GroupAdmin . query . filter ( GroupAdmin . admin_type == 'Group' , GroupAdmin . admin_id . in_ ( q3 ) ) . with_entities ( GroupAdmin . group_id ) q5 = Membership . query . filter ( Membership . state == MembershipState . PENDING_ADMIN , Membership . id_group . in_ ( q4 ) ) query = q2 . union ( q5 ) return query
13449	def authed_post ( self , url , data , response_code = 200 , follow = False , headers = { } ) : if not self . authed : self . authorize ( ) response = self . client . post ( url , data , follow = follow , * * headers ) self . assertEqual ( response_code , response . status_code ) return response
9193	def _insert_file ( cursor , file , media_type ) : resource_hash = _get_file_sha1 ( file ) cursor . execute ( "SELECT fileid FROM files WHERE sha1 = %s" , ( resource_hash , ) ) try : fileid = cursor . fetchone ( ) [ 0 ] except ( IndexError , TypeError ) : cursor . execute ( "INSERT INTO files (file, media_type) " "VALUES (%s, %s)" "RETURNING fileid" , ( psycopg2 . Binary ( file . read ( ) ) , media_type , ) ) fileid = cursor . fetchone ( ) [ 0 ] return fileid , resource_hash
13590	def sigma_prime ( self ) : return _np . sqrt ( self . emit / self . beta ( self . E ) )
4292	def validate_project ( project_name ) : if '-' in project_name : return None if keyword . iskeyword ( project_name ) : return None if project_name in dir ( __builtins__ ) : return None try : __import__ ( project_name ) return None except ImportError : return project_name
7157	def assign_prompter ( self , prompter ) : if is_string ( prompter ) : if prompter not in prompters : eprint ( "Error: '{}' is not a core prompter" . format ( prompter ) ) sys . exit ( ) self . prompter = prompters [ prompter ] else : self . prompter = prompter
8665	def _prettify_list ( items ) : assert isinstance ( items , list ) keys_list = 'Available Keys:' for item in items : keys_list += '\n - {0}' . format ( item ) return keys_list
10555	def find_helping_materials ( project_id , * * kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'helpingmaterial' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ HelpingMaterial ( helping ) for helping in res ] else : return res except : # pragma: no cover raise
5169	def __netjson_protocol ( self , radio ) : htmode = radio . get ( 'htmode' ) hwmode = radio . get ( 'hwmode' , None ) if htmode . startswith ( 'HT' ) : return '802.11n' elif htmode . startswith ( 'VHT' ) : return '802.11ac' return '802.{0}' . format ( hwmode )
12237	def doublewell ( theta ) : k0 , k1 , depth = 0.01 , 100 , 0.5 shallow = 0.5 * k0 * theta ** 2 + depth deep = 0.5 * k1 * theta ** 2 obj = float ( np . minimum ( shallow , deep ) ) grad = np . where ( deep < shallow , k1 * theta , k0 * theta ) return obj , grad
3349	def geometric_fba ( model , epsilon = 1E-06 , max_tries = 200 , processes = None ) : with model : # Variables' and constraints' storage variables. consts = [ ] obj_vars = [ ] updating_vars_cons = [ ] # The first iteration. prob = model . problem add_pfba ( model ) # Minimize the solution space to a convex hull. model . optimize ( ) fva_sol = flux_variability_analysis ( model , processes = processes ) mean_flux = ( fva_sol [ "maximum" ] + fva_sol [ "minimum" ] ) . abs ( ) / 2 # Set the gFBA constraints. for rxn in model . reactions : var = prob . Variable ( "geometric_fba_" + rxn . id , lb = 0 , ub = mean_flux [ rxn . id ] ) upper_const = prob . Constraint ( rxn . flux_expression - var , ub = mean_flux [ rxn . id ] , name = "geometric_fba_upper_const_" + rxn . id ) lower_const = prob . Constraint ( rxn . flux_expression + var , lb = fva_sol . at [ rxn . id , "minimum" ] , name = "geometric_fba_lower_const_" + rxn . id ) updating_vars_cons . append ( ( rxn . id , var , upper_const , lower_const ) ) consts . extend ( [ var , upper_const , lower_const ] ) obj_vars . append ( var ) model . add_cons_vars ( consts ) # Minimize the distance between the flux distribution and center. model . objective = prob . Objective ( Zero , sloppy = True , direction = "min" ) model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } ) # Update loop variables. sol = model . optimize ( ) fva_sol = flux_variability_analysis ( model , processes = processes ) mean_flux = ( fva_sol [ "maximum" ] + fva_sol [ "minimum" ] ) . abs ( ) / 2 delta = ( fva_sol [ "maximum" ] - fva_sol [ "minimum" ] ) . max ( ) count = 1 LOGGER . debug ( "Iteration: %d; delta: %.3g; status: %s." , count , delta , sol . status ) # Following iterations that minimize the distance below threshold. while delta > epsilon and count < max_tries : for rxn_id , var , u_c , l_c in updating_vars_cons : var . ub = mean_flux [ rxn_id ] u_c . ub = mean_flux [ rxn_id ] l_c . lb = fva_sol . at [ rxn_id , "minimum" ] # Update loop variables. sol = model . optimize ( ) fva_sol = flux_variability_analysis ( model , processes = processes ) mean_flux = ( fva_sol [ "maximum" ] + fva_sol [ "minimum" ] ) . abs ( ) / 2 delta = ( fva_sol [ "maximum" ] - fva_sol [ "minimum" ] ) . max ( ) count += 1 LOGGER . debug ( "Iteration: %d; delta: %.3g; status: %s." , count , delta , sol . status ) if count == max_tries : raise RuntimeError ( "The iterations have exceeded the maximum value of {}. " "This is probably due to the increased complexity of the " "model and can lead to inaccurate results. Please set a " "different convergence tolerance and/or increase the " "maximum iterations" . format ( max_tries ) ) return sol
11339	def set_target_fahrenheit ( self , fahrenheit , mode = config . SCHEDULE_HOLD ) : temperature = fahrenheit_to_nuheat ( fahrenheit ) self . set_target_temperature ( temperature , mode )
10297	def get_undefined_namespace_names ( graph : BELGraph , namespace : str ) -> Set [ str ] : return { exc . name for _ , exc , _ in graph . warnings if isinstance ( exc , UndefinedNamespaceWarning ) and exc . namespace == namespace }
7237	def randwindow ( self , window_shape ) : row = random . randrange ( window_shape [ 0 ] , self . shape [ 1 ] ) col = random . randrange ( window_shape [ 1 ] , self . shape [ 2 ] ) return self [ : , row - window_shape [ 0 ] : row , col - window_shape [ 1 ] : col ]
13837	def ConsumeIdentifier ( self ) : result = self . token if not self . _IDENTIFIER . match ( result ) : raise self . _ParseError ( 'Expected identifier.' ) self . NextToken ( ) return result
13091	def get_interface_name ( ) : interface_name = '' interfaces = psutil . net_if_addrs ( ) for name , details in interfaces . items ( ) : for detail in details : if detail . family == socket . AF_INET : ip_address = ipaddress . ip_address ( detail . address ) if not ( ip_address . is_link_local or ip_address . is_loopback ) : interface_name = name break return interface_name
7552	def _getbins ( ) : # Return error if system is 32-bit arch. # This is straight from the python docs: # https://docs.python.org/2/library/platform.html#cross-platform if not _sys . maxsize > 2 ** 32 : _sys . exit ( "ipyrad requires 64bit architecture" ) ## get platform mac or linux _platform = _sys . platform ## get current location if 'VIRTUAL_ENV' in _os . environ : ipyrad_path = _os . environ [ 'VIRTUAL_ENV' ] else : path = _os . path . abspath ( _os . path . dirname ( __file__ ) ) ipyrad_path = _os . path . dirname ( path ) ## find bin directory ipyrad_path = _os . path . dirname ( path ) bin_path = _os . path . join ( ipyrad_path , "bin" ) ## get the correct binaries if 'linux' in _platform : vsearch = _os . path . join ( _os . path . abspath ( bin_path ) , "vsearch-linux-x86_64" ) muscle = _os . path . join ( _os . path . abspath ( bin_path ) , "muscle-linux-x86_64" ) smalt = _os . path . join ( _os . path . abspath ( bin_path ) , "smalt-linux-x86_64" ) bwa = _os . path . join ( _os . path . abspath ( bin_path ) , "bwa-linux-x86_64" ) samtools = _os . path . join ( _os . path . abspath ( bin_path ) , "samtools-linux-x86_64" ) bedtools = _os . path . join ( _os . path . abspath ( bin_path ) , "bedtools-linux-x86_64" ) qmc = _os . path . join ( _os . path . abspath ( bin_path ) , "QMC-linux-x86_64" ) else : vsearch = _os . path . join ( _os . path . abspath ( bin_path ) , "vsearch-osx-x86_64" ) muscle = _os . path . join ( _os . path . abspath ( bin_path ) , "muscle-osx-x86_64" ) smalt = _os . path . join ( _os . path . abspath ( bin_path ) , "smalt-osx-x86_64" ) bwa = _os . path . join ( _os . path . abspath ( bin_path ) , "bwa-osx-x86_64" ) samtools = _os . path . join ( _os . path . abspath ( bin_path ) , "samtools-osx-x86_64" ) bedtools = _os . path . join ( _os . path . abspath ( bin_path ) , "bedtools-osx-x86_64" ) ## only one compiled version available, works for all? qmc = _os . path . join ( _os . path . abspath ( bin_path ) , "QMC-osx-x86_64" ) # Test for existence of binaries assert _cmd_exists ( muscle ) , "muscle not found here: " + muscle assert _cmd_exists ( vsearch ) , "vsearch not found here: " + vsearch assert _cmd_exists ( smalt ) , "smalt not found here: " + smalt assert _cmd_exists ( bwa ) , "bwa not found here: " + bwa assert _cmd_exists ( samtools ) , "samtools not found here: " + samtools assert _cmd_exists ( bedtools ) , "bedtools not found here: " + bedtools #assert _cmd_exists(qmc), "wQMC not found here: "+qmc return vsearch , muscle , smalt , bwa , samtools , bedtools , qmc
166	def project ( self , from_shape , to_shape ) : coords_proj = project_coords ( self . coords , from_shape , to_shape ) return self . copy ( coords = coords_proj )
13829	def remove ( self , collection , * * kwargs ) : callback = kwargs . pop ( 'callback' ) yield Op ( self . db [ collection ] . remove , kwargs ) callback ( )
2075	def score_models ( clf , X , y , encoder , runs = 1 ) : scores = [ ] X_test = None for _ in range ( runs ) : X_test = encoder ( ) . fit_transform ( X , y ) # Some models, like logistic regression, like normalized features otherwise they underperform and/or take a long time to converge. # To be rigorous, we should have trained the normalization on each fold individually via pipelines. # See grid_search_example to learn how to do it. X_test = StandardScaler ( ) . fit_transform ( X_test ) scores . append ( cross_validate ( clf , X_test , y , n_jobs = 1 , cv = 5 ) [ 'test_score' ] ) gc . collect ( ) scores = [ y for z in [ x for x in scores ] for y in z ] return float ( np . mean ( scores ) ) , float ( np . std ( scores ) ) , scores , X_test . shape [ 1 ]
3846	def parse_typing_status_message ( p ) : return TypingStatusMessage ( conv_id = p . conversation_id . id , user_id = from_participantid ( p . sender_id ) , timestamp = from_timestamp ( p . timestamp ) , status = p . type , )
4378	def add_parent ( self , parent ) : parent . children . add ( self ) self . parents . add ( parent )
11315	def update_notes ( self ) : fields = record_get_field_instances ( self . record , '500' ) for field in fields : subs = field_get_subfields ( field ) for sub in subs . get ( 'a' , [ ] ) : sub = sub . strip ( ) # remove any spaces before/after if sub . startswith ( "*" ) and sub . endswith ( "*" ) : record_delete_field ( self . record , tag = "500" , field_position_global = field [ 4 ] )
2197	def log_part ( self ) : self . cap_stdout . seek ( self . _pos ) text = self . cap_stdout . read ( ) self . _pos = self . cap_stdout . tell ( ) self . parts . append ( text ) self . text = text
13049	def check_service ( service ) : # Try HTTP service . add_tag ( 'header_scan' ) http = False try : result = requests . head ( 'http://{}:{}' . format ( service . address , service . port ) , timeout = 1 ) print_success ( "Found http service on {}:{}" . format ( service . address , service . port ) ) service . add_tag ( 'http' ) http = True try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass if not http : # Try HTTPS try : result = requests . head ( 'https://{}:{}' . format ( service . address , service . port ) , verify = False , timeout = 3 ) service . add_tag ( 'https' ) print_success ( "Found https service on {}:{}" . format ( service . address , service . port ) ) try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass service . save ( )
13631	def _adaptToResource ( self , result ) : if result is None : return NotFound ( ) spinneretResource = ISpinneretResource ( result , None ) if spinneretResource is not None : return SpinneretResource ( spinneretResource ) renderable = IRenderable ( result , None ) if renderable is not None : return _RenderableResource ( renderable ) resource = IResource ( result , None ) if resource is not None : return resource if isinstance ( result , URLPath ) : return Redirect ( str ( result ) ) return result
11337	def connect ( self ) : for tried_connection_count in range ( CFG_FTP_CONNECTION_ATTEMPTS ) : try : self . ftp = FtpHandler ( self . config . OXFORD . URL , self . config . OXFORD . LOGIN , self . config . OXFORD . PASSWORD ) self . logger . debug ( ( "Successful connection to the " "Oxford University Press server" ) ) return except socket_timeout_exception as err : self . logger . error ( ( 'Failed to connect %d of %d times. ' 'Will sleep for %d seconds and try again.' ) % ( tried_connection_count + 1 , CFG_FTP_CONNECTION_ATTEMPTS , CFG_FTP_TIMEOUT_SLEEP_DURATION ) ) time . sleep ( CFG_FTP_TIMEOUT_SLEEP_DURATION ) except Exception as err : self . logger . error ( ( 'Failed to connect to the Oxford ' 'University Press server. %s' ) % ( err , ) ) break raise LoginException ( err )
4815	def create_feature_array ( text , n_pad = 21 ) : n = len ( text ) n_pad_2 = int ( ( n_pad - 1 ) / 2 ) text_pad = [ ' ' ] * n_pad_2 + [ t for t in text ] + [ ' ' ] * n_pad_2 x_char , x_type = [ ] , [ ] for i in range ( n_pad_2 , n_pad_2 + n ) : char_list = text_pad [ i + 1 : i + n_pad_2 + 1 ] + list ( reversed ( text_pad [ i - n_pad_2 : i ] ) ) + [ text_pad [ i ] ] char_map = [ CHARS_MAP . get ( c , 80 ) for c in char_list ] char_type = [ CHAR_TYPES_MAP . get ( CHAR_TYPE_FLATTEN . get ( c , 'o' ) , 4 ) for c in char_list ] x_char . append ( char_map ) x_type . append ( char_type ) x_char = np . array ( x_char ) . astype ( float ) x_type = np . array ( x_type ) . astype ( float ) return x_char , x_type
9186	def get_moderation ( request ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( """\ SELECT row_to_json(combined_rows) FROM ( SELECT id, created, publisher, publication_message, (select array_agg(row_to_json(pd)) from pending_documents as pd where pd.publication_id = p.id) AS models FROM publications AS p WHERE state = 'Waiting for moderation') AS combined_rows""" ) moderations = [ x [ 0 ] for x in cursor . fetchall ( ) ] return moderations
8909	def fetch_by_url ( self , url ) : service = self . collection . find_one ( { 'url' : url } ) if not service : raise ServiceNotFound return Service ( service )
3117	def oauth2_authorize ( request ) : return_url = request . GET . get ( 'return_url' , None ) if not return_url : return_url = request . META . get ( 'HTTP_REFERER' , '/' ) scopes = request . GET . getlist ( 'scopes' , django_util . oauth2_settings . scopes ) # Model storage (but not session storage) requires a logged in user if django_util . oauth2_settings . storage_model : if not request . user . is_authenticated ( ) : return redirect ( '{0}?next={1}' . format ( settings . LOGIN_URL , parse . quote ( request . get_full_path ( ) ) ) ) # This checks for the case where we ended up here because of a logged # out user but we had credentials for it in the first place else : user_oauth = django_util . UserOAuth2 ( request , scopes , return_url ) if user_oauth . has_credentials ( ) : return redirect ( return_url ) flow = _make_flow ( request = request , scopes = scopes , return_url = return_url ) auth_url = flow . step1_get_authorize_url ( ) return shortcuts . redirect ( auth_url )
5571	def execute ( mp ) : # Reading and writing data works like this: with mp . open ( "file1" , resampling = "bilinear" ) as raster_file : if raster_file . is_empty ( ) : return "empty" # This assures a transparent tile instead of a pink error tile # is returned when using mapchete serve. dem = raster_file . read ( ) return dem
1322	def MoveToCenter ( self ) -> bool : if self . IsTopLevel ( ) : rect = self . BoundingRectangle screenWidth , screenHeight = GetScreenSize ( ) x , y = ( screenWidth - rect . width ( ) ) // 2 , ( screenHeight - rect . height ( ) ) // 2 if x < 0 : x = 0 if y < 0 : y = 0 return SetWindowPos ( self . NativeWindowHandle , SWP . HWND_Top , x , y , 0 , 0 , SWP . SWP_NoSize ) return False
6823	def maint_up ( self ) : r = self . local_renderer fn = self . render_to_file ( r . env . maintenance_template , extra = { 'current_hostname' : self . current_hostname } ) r . put ( local_path = fn , remote_path = r . env . maintenance_path , use_sudo = True ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {maintenance_path}' )
179	def to_keypoints ( self ) : # TODO get rid of this deferred import from imgaug . augmentables . kps import Keypoint return [ Keypoint ( x = x , y = y ) for ( x , y ) in self . coords ]
10820	def _filter ( cls , query , state = MembershipState . ACTIVE , eager = None ) : query = query . filter_by ( state = state ) eager = eager or [ ] for field in eager : query = query . options ( joinedload ( field ) ) return query
6406	def sim ( self , src , tar ) : if src == tar : return 1.0 if not src or not tar : return 0.0 return ( len ( src ) / len ( tar ) if len ( src ) < len ( tar ) else len ( tar ) / len ( src ) )
6328	def _add_to_ngcorpus ( self , corpus , words , count ) : if words [ 0 ] not in corpus : corpus [ words [ 0 ] ] = Counter ( ) if len ( words ) == 1 : corpus [ words [ 0 ] ] [ None ] += count else : self . _add_to_ngcorpus ( corpus [ words [ 0 ] ] , words [ 1 : ] , count )
9024	def write ( self , string ) : bytes_ = string . encode ( self . _encoding ) self . _file . write ( bytes_ )
10668	def get_datetime_at_period_ix ( self , ix ) : if self . timestep_period_duration == TimePeriod . millisecond : return self . start_datetime + timedelta ( milliseconds = ix ) elif self . timestep_period_duration == TimePeriod . second : return self . start_datetime + timedelta ( seconds = ix ) elif self . timestep_period_duration == TimePeriod . minute : return self . start_datetime + timedelta ( minutes = ix ) elif self . timestep_period_duration == TimePeriod . hour : return self . start_datetime + timedelta ( hours = ix ) elif self . timestep_period_duration == TimePeriod . day : return self . start_datetime + relativedelta ( days = ix ) elif self . timestep_period_duration == TimePeriod . week : return self . start_datetime + relativedelta ( days = ix * 7 ) elif self . timestep_period_duration == TimePeriod . month : return self . start_datetime + relativedelta ( months = ix ) elif self . timestep_period_duration == TimePeriod . year : return self . start_datetime + relativedelta ( years = ix )
10332	def average_node_annotation ( graph : BELGraph , key : str , annotation : str = 'Subgraph' , aggregator : Optional [ Callable [ [ Iterable [ X ] ] , X ] ] = None , ) -> Mapping [ str , X ] : if aggregator is None : def aggregator ( x ) : """Calculates the average""" return sum ( x ) / len ( x ) result = { } for subgraph , nodes in group_nodes_by_annotation ( graph , annotation ) . items ( ) : values = [ graph . nodes [ node ] [ key ] for node in nodes if key in graph . nodes [ node ] ] result [ subgraph ] = aggregator ( values ) return result
10853	def harris_feature ( im , region_size = 5 , to_return = 'harris' , scale = 0.05 ) : ndim = im . ndim #1. Gradient of image grads = [ nd . sobel ( im , axis = i ) for i in range ( ndim ) ] #2. Corner response matrix matrix = np . zeros ( ( ndim , ndim ) + im . shape ) for a in range ( ndim ) : for b in range ( ndim ) : matrix [ a , b ] = nd . filters . gaussian_filter ( grads [ a ] * grads [ b ] , region_size ) if to_return == 'matrix' : return matrix #3. Trace, determinant trc = np . trace ( matrix , axis1 = 0 , axis2 = 1 ) det = np . linalg . det ( matrix . T ) . T if to_return == 'trace-determinant' : return trc , det else : #4. Harris detector: harris = det - scale * trc * trc return harris
7279	def play ( self ) : if not self . is_playing ( ) : self . play_pause ( ) self . _is_playing = True self . playEvent ( self )
4821	def redirect_if_blocked ( course_run_ids , user = None , ip_address = None , url = None ) : for course_run_id in course_run_ids : redirect_url = embargo_api . redirect_if_blocked ( CourseKey . from_string ( course_run_id ) , user = user , ip_address = ip_address , url = url ) if redirect_url : return redirect_url
4275	def zip ( self ) : zip_gallery = self . settings [ 'zip_gallery' ] if zip_gallery and len ( self ) > 0 : zip_gallery = zip_gallery . format ( album = self ) archive_path = join ( self . dst_path , zip_gallery ) if ( self . settings . get ( 'zip_skip_if_exists' , False ) and isfile ( archive_path ) ) : self . logger . debug ( "Archive %s already created, passing" , archive_path ) return zip_gallery archive = zipfile . ZipFile ( archive_path , 'w' , allowZip64 = True ) attr = ( 'src_path' if self . settings [ 'zip_media_format' ] == 'orig' else 'dst_path' ) for p in self : path = getattr ( p , attr ) try : archive . write ( path , os . path . split ( path ) [ 1 ] ) except OSError as e : self . logger . warn ( 'Failed to add %s to the ZIP: %s' , p , e ) archive . close ( ) self . logger . debug ( 'Created ZIP archive %s' , archive_path ) return zip_gallery
8469	def getOSName ( self ) : _system = platform . system ( ) if _system in [ self . __class__ . OS_WINDOWS , self . __class__ . OS_MAC , self . __class__ . OS_LINUX ] : if _system == self . __class__ . OS_LINUX : _dist = platform . linux_distribution ( ) [ 0 ] if _dist . lower ( ) == self . __class__ . OS_UBUNTU . lower ( ) : return self . __class__ . OS_UBUNTU elif _dist . lower ( ) == self . __class__ . OS_DEBIAN . lower ( ) : return self . __class__ . OS_DEBIAN elif _dist . lower ( ) == self . __class__ . OS_CENTOS . lower ( ) : return self . __class__ . OS_CENTOS elif _dist . lower ( ) == self . __class__ . OS_REDHAT . lower ( ) : return self . __class__ . OS_REDHAT elif _dist . lower ( ) == self . __class__ . OS_KALI . lower ( ) : return self . __class__ . OS_KALI return _system else : return None
9356	def words ( quantity = 10 , as_list = False ) : global _words if not _words : _words = ' ' . join ( get_dictionary ( 'lorem_ipsum' ) ) . lower ( ) . replace ( '\n' , '' ) _words = re . sub ( r'\.|,|;/' , '' , _words ) _words = _words . split ( ' ' ) result = random . sample ( _words , quantity ) if as_list : return result else : return ' ' . join ( result )
2349	def open ( self ) : if self . seed_url : self . driver_adapter . open ( self . seed_url ) self . wait_for_page_to_load ( ) return self raise UsageError ( "Set a base URL or URL_TEMPLATE to open this page." )
12829	def parse_conll ( self , texts : List [ str ] , retry_count : int = 0 ) -> List [ str ] : post_data = { 'texts' : texts , 'output_type' : 'conll' } try : response = requests . post ( f'http://{self.hostname}:{self.port}' , json = post_data , headers = { 'Connection' : 'close' } ) response . raise_for_status ( ) except ( requests . exceptions . ConnectionError , requests . exceptions . Timeout ) as server_error : raise ServerError ( server_error , self . hostname , self . port ) except requests . exceptions . HTTPError as http_error : raise http_error else : try : return response . json ( ) except json . JSONDecodeError as json_exception : if retry_count == self . retries : self . log_error ( response . text ) raise Exception ( 'Json Decoding error cannot parse this ' f':\n{response.text}' ) return self . parse_conll ( texts , retry_count + 1 )
4630	def _derive_y_from_x ( self , x , is_even ) : curve = ecdsa . SECP256k1 . curve # The curve equation over F_p is: # y^2 = x^3 + ax + b a , b , p = curve . a ( ) , curve . b ( ) , curve . p ( ) alpha = ( pow ( x , 3 , p ) + a * x + b ) % p beta = ecdsa . numbertheory . square_root_mod_prime ( alpha , p ) if ( beta % 2 ) == is_even : beta = p - beta return beta
10913	def find_particles_in_tile ( positions , tile ) : bools = tile . contains ( positions ) return np . arange ( bools . size ) [ bools ]
13360	def load ( self ) : if not os . path . exists ( self . path ) : return with open ( self . path , 'r' ) as f : env_data = yaml . load ( f . read ( ) ) if env_data : for env in env_data : self . add ( VirtualEnvironment ( env [ 'root' ] ) )
7653	def serialize_obj ( obj ) : if isinstance ( obj , np . integer ) : return int ( obj ) elif isinstance ( obj , np . floating ) : return float ( obj ) elif isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , list ) : return [ serialize_obj ( x ) for x in obj ] elif isinstance ( obj , Observation ) : return { k : serialize_obj ( v ) for k , v in six . iteritems ( obj . _asdict ( ) ) } return obj
165	def compute_distance ( self , other , default = None ) : # FIXME this computes distance pointwise, does not have to be identical # with the actual min distance (e.g. edge center to other's point) distances = self . compute_pointwise_distances ( other , default = [ ] ) if len ( distances ) == 0 : return default return min ( distances )
4498	def guid ( self , guid ) : return self . _json ( self . _get ( self . _build_url ( 'guids' , guid ) ) , 200 ) [ 'data' ] [ 'type' ]
7820	def flush ( self , dispatch = True ) : if dispatch : while True : event = self . dispatch ( False ) if event in ( None , QUIT ) : return event else : while True : try : self . queue . get ( False ) except Queue . Empty : return None
8649	def delete_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'delete' , } # POST /api/projects/0.1/milestone_requests/{milestone_request_id}/?action= # delete endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
5708	def process_request ( self , request ) : try : session = request . session except AttributeError : raise ImproperlyConfigured ( 'django-lockdown requires the Django ' 'sessions framework' ) # Don't lock down if django-lockdown is disabled altogether. if settings . ENABLED is False : return None # Don't lock down if the client REMOTE_ADDR matched and is part of the # exception list. if self . remote_addr_exceptions : remote_addr_exceptions = self . remote_addr_exceptions else : remote_addr_exceptions = settings . REMOTE_ADDR_EXCEPTIONS if remote_addr_exceptions : # If forwarding proxies are used they must be listed as trusted trusted_proxies = self . trusted_proxies or settings . TRUSTED_PROXIES remote_addr = request . META . get ( 'REMOTE_ADDR' ) if remote_addr in remote_addr_exceptions : return None if remote_addr in trusted_proxies : # If REMOTE_ADDR is a trusted proxy check x-forwarded-for x_forwarded_for = request . META . get ( 'HTTP_X_FORWARDED_FOR' ) if x_forwarded_for : remote_addr = x_forwarded_for . split ( ',' ) [ - 1 ] . strip ( ) if remote_addr in remote_addr_exceptions : return None # Don't lock down if the URL matches an exception pattern. if self . url_exceptions : url_exceptions = compile_url_exceptions ( self . url_exceptions ) else : url_exceptions = compile_url_exceptions ( settings . URL_EXCEPTIONS ) for pattern in url_exceptions : if pattern . search ( request . path ) : return None # Don't lock down if the URL resolves to a whitelisted view. try : resolved_path = resolve ( request . path ) except Resolver404 : pass else : if resolved_path . func in settings . VIEW_EXCEPTIONS : return None # Don't lock down if outside of the lockdown dates. if self . until_date : until_date = self . until_date else : until_date = settings . UNTIL_DATE if self . after_date : after_date = self . after_date else : after_date = settings . AFTER_DATE if until_date or after_date : locked_date = False if until_date and datetime . datetime . now ( ) < until_date : locked_date = True if after_date and datetime . datetime . now ( ) > after_date : locked_date = True if not locked_date : return None form_data = request . POST if request . method == 'POST' else None if self . form : form_class = self . form else : form_class = get_lockdown_form ( settings . FORM ) form = form_class ( data = form_data , * * self . form_kwargs ) authorized = False token = session . get ( self . session_key ) if hasattr ( form , 'authenticate' ) : if form . authenticate ( token ) : authorized = True elif token is True : authorized = True if authorized and self . logout_key and self . logout_key in request . GET : if self . session_key in session : del session [ self . session_key ] querystring = request . GET . copy ( ) del querystring [ self . logout_key ] return self . redirect ( request ) # Don't lock down if the user is already authorized for previewing. if authorized : return None if form . is_valid ( ) : if hasattr ( form , 'generate_token' ) : token = form . generate_token ( ) else : token = True session [ self . session_key ] = token return self . redirect ( request ) page_data = { 'until_date' : until_date , 'after_date' : after_date } if not hasattr ( form , 'show_form' ) or form . show_form ( ) : page_data [ 'form' ] = form if self . extra_context : page_data . update ( self . extra_context ) return render ( request , 'lockdown/form.html' , page_data )
8927	def dist ( ctx , devpi = False , egg = False , wheel = False , auto = True ) : config . load ( ) cmd = [ "python" , "setup.py" , "sdist" ] # Automatically create wheels if possible if auto : egg = sys . version_info . major == 2 try : import wheel as _ wheel = True except ImportError : wheel = False if egg : cmd . append ( "bdist_egg" ) if wheel : cmd . append ( "bdist_wheel" ) ctx . run ( "invoke clean --all build --docs test check" ) ctx . run ( ' ' . join ( cmd ) ) if devpi : ctx . run ( "devpi upload dist/*" )
4302	def create_user ( config_data ) : with chdir ( os . path . abspath ( config_data . project_directory ) ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) subprocess . check_call ( [ sys . executable , 'create_user.py' ] , env = env , stderr = subprocess . STDOUT ) for ext in [ 'py' , 'pyc' ] : try : os . remove ( 'create_user.{0}' . format ( ext ) ) except OSError : pass
10148	def from_schema_mapping ( self , schema_mapping ) : responses = { } for status , response_schema in schema_mapping . items ( ) : response = { } if response_schema . description : response [ 'description' ] = response_schema . description else : raise CorniceSwaggerException ( 'Responses must have a description.' ) for field_schema in response_schema . children : location = field_schema . name if location == 'body' : title = field_schema . __class__ . __name__ if title == 'body' : title = response_schema . __class__ . __name__ + 'Body' field_schema . title = title response [ 'schema' ] = self . definitions . from_schema ( field_schema ) elif location in ( 'header' , 'headers' ) : header_schema = self . type_converter ( field_schema ) headers = header_schema . get ( 'properties' ) if headers : # Response headers doesn't accept titles for header in headers . values ( ) : header . pop ( 'title' ) response [ 'headers' ] = headers pointer = response_schema . __class__ . __name__ if self . ref : response = self . _ref ( response , pointer ) responses [ status ] = response return responses
11882	def scanAllProcessesForMapping ( searchPortion , isExactMatch = False , ignoreCase = False ) : pids = getAllRunningPids ( ) # Since processes could disappear, we run the scan as fast as possible here with a list comprehension, then assemble the return dictionary later. mappingResults = [ scanProcessForMapping ( pid , searchPortion , isExactMatch , ignoreCase ) for pid in pids ] ret = { } for i in range ( len ( pids ) ) : if mappingResults [ i ] is not None : ret [ pids [ i ] ] = mappingResults [ i ] return ret
5011	def _call_post_with_session ( self , url , payload ) : now = datetime . datetime . utcnow ( ) if now >= self . expires_at : # Create a new session with a valid token self . session . close ( ) self . _create_session ( ) response = self . session . post ( url , data = payload ) return response . status_code , response . text
2011	def read_code ( self , address , size = 1 ) : assert address < len ( self . bytecode ) value = self . bytecode [ address : address + size ] if len ( value ) < size : value += '\x00' * ( size - len ( value ) ) # pad with null (spec) return value
8843	def unindent ( self ) : if self . tab_always_indent : cursor = self . editor . textCursor ( ) if not cursor . hasSelection ( ) : cursor . select ( cursor . LineUnderCursor ) self . unindent_selection ( cursor ) else : super ( PyIndenterMode , self ) . unindent ( )
2241	def modpath_to_modname ( modpath , hide_init = True , hide_main = False , check = True , relativeto = None ) : if check and relativeto is None : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) modpath_ = abspath ( expanduser ( modpath ) ) modpath_ = normalize_modpath ( modpath_ , hide_init = hide_init , hide_main = hide_main ) if relativeto : dpath = dirname ( abspath ( expanduser ( relativeto ) ) ) rel_modpath = relpath ( modpath_ , dpath ) else : dpath , rel_modpath = split_modpath ( modpath_ , check = check ) modname = splitext ( rel_modpath ) [ 0 ] if '.' in modname : modname , abi_tag = modname . split ( '.' ) modname = modname . replace ( '/' , '.' ) modname = modname . replace ( '\\' , '.' ) return modname
6977	def kepler_lcdict_to_pkl ( lcdict , outfile = None ) : if not outfile : outfile = '%s-keplc.pkl' % lcdict [ 'objectid' ] . replace ( ' ' , '-' ) # we're using pickle.HIGHEST_PROTOCOL here, this will make Py3 pickles # unreadable for Python 2.7 with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) return os . path . abspath ( outfile )
330	def model_returns_normal ( data , samples = 500 , progressbar = True ) : with pm . Model ( ) as model : mu = pm . Normal ( 'mean returns' , mu = 0 , sd = .01 , testval = data . mean ( ) ) sigma = pm . HalfCauchy ( 'volatility' , beta = 1 , testval = data . std ( ) ) returns = pm . Normal ( 'returns' , mu = mu , sd = sigma , observed = data ) pm . Deterministic ( 'annual volatility' , returns . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'sharpe' , returns . distribution . mean / returns . distribution . variance ** .5 * np . sqrt ( 252 ) ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
3576	def initialize ( self ) : # Setup the central manager and its delegate. self . _central_manager = CBCentralManager . alloc ( ) self . _central_manager . initWithDelegate_queue_options_ ( self . _central_delegate , None , None )
12386	def split_segments ( text , closing_paren = False ) : buf = StringIO ( ) # The segments we're building, and the combinators used to combine them. # Note that after this is complete, this should be true: # len(segments) == len(combinators) + 1 # Thus we can understand the relationship between segments and combinators # like so: # s1 (c1) s2 (c2) s3 (c3) where sN are segments and cN are combination # functions. # TODO: Figure out exactly where the querystring died and post cool # error messages about it. segments = [ ] combinators = [ ] # A flag dictating if the last character we processed was a group. # This is used to determine if the next character (being a combinator) # is allowed to last_group = False # The recursive nature of this function relies on keeping track of the # state of iteration. This iterator will be passed down to recursed calls. iterator = iter ( text ) # Detection for exclamation points. only matters for this situation: # foo=bar&!(bar=baz) last_negation = False for character in iterator : if character in COMBINATORS : if last_negation : buf . write ( constants . OPERATOR_NEGATION ) # The string representation of our segment. val = buf . getvalue ( ) reset_stringio ( buf ) if not last_group and not len ( val ) : raise ValueError ( 'Unexpected %s.' % character ) # When a group happens, the previous value is empty. if len ( val ) : segments . append ( parse_segment ( val ) ) combinators . append ( COMBINATORS [ character ] ) elif character == constants . GROUP_BEGIN : # Recursively go into the next group. if buf . tell ( ) : raise ValueError ( 'Unexpected %s' % character ) seg = split_segments ( iterator , True ) if last_negation : seg = UnarySegmentCombinator ( seg ) segments . append ( seg ) # Flag that the last entry was a grouping, so that we don't panic # when the next character is a logical combinator last_group = True continue elif character == constants . GROUP_END : # Build the segment for anything remaining, and then combine # all the segments. val = buf . getvalue ( ) # Check for unbalanced parens or an empty thing: foo=bar&();bar=baz if not buf . tell ( ) or not closing_paren : raise ValueError ( 'Unexpected %s' % character ) segments . append ( parse_segment ( val ) ) return combine ( segments , combinators ) elif character == constants . OPERATOR_NEGATION and not buf . tell ( ) : last_negation = True continue else : if last_negation : buf . write ( constants . OPERATOR_NEGATION ) if last_group : raise ValueError ( 'Unexpected %s' % character ) buf . write ( character ) last_negation = False last_group = False else : # Check and see if the iterator exited early (unbalanced parens) if closing_paren : raise ValueError ( 'Expected %s.' % constants . GROUP_END ) if not last_group : # Add the final segment. segments . append ( parse_segment ( buf . getvalue ( ) ) ) # Everything completed normally, combine all the segments into one # and return them. return combine ( segments , combinators )
3055	def from_string ( cls , key , password = 'notasecret' ) : key = _helpers . _from_bytes ( key ) # pem expects str in Py3 marker_id , key_bytes = pem . readPemBlocksFromFile ( six . StringIO ( key ) , _PKCS1_MARKER , _PKCS8_MARKER ) if marker_id == 0 : pkey = rsa . key . PrivateKey . load_pkcs1 ( key_bytes , format = 'DER' ) elif marker_id == 1 : key_info , remaining = decoder . decode ( key_bytes , asn1Spec = _PKCS8_SPEC ) if remaining != b'' : raise ValueError ( 'Unused bytes' , remaining ) pkey_info = key_info . getComponentByName ( 'privateKey' ) pkey = rsa . key . PrivateKey . load_pkcs1 ( pkey_info . asOctets ( ) , format = 'DER' ) else : raise ValueError ( 'No key could be detected.' ) return cls ( pkey )
4219	def get_preferred_collection ( self ) : bus = secretstorage . dbus_init ( ) try : if hasattr ( self , 'preferred_collection' ) : collection = secretstorage . Collection ( bus , self . preferred_collection ) else : collection = secretstorage . get_default_collection ( bus ) except exceptions . SecretStorageException as e : raise InitError ( "Failed to create the collection: %s." % e ) if collection . is_locked ( ) : collection . unlock ( ) if collection . is_locked ( ) : # User dismissed the prompt raise KeyringLocked ( "Failed to unlock the collection!" ) return collection
1276	def tf_initialize ( self , x_init , base_value , target_value , estimated_improvement ) : self . base_value = base_value if estimated_improvement is None : # TODO: Is this a good alternative? estimated_improvement = tf . abs ( x = base_value ) first_step = super ( LineSearch , self ) . tf_initialize ( x_init ) improvement = tf . divide ( x = ( target_value - self . base_value ) , y = tf . maximum ( x = estimated_improvement , y = util . epsilon ) ) last_improvement = improvement - 1.0 if self . mode == 'linear' : deltas = [ - t * self . parameter for t in x_init ] self . estimated_incr = - estimated_improvement * self . parameter elif self . mode == 'exponential' : deltas = [ - t * self . parameter for t in x_init ] return first_step + ( deltas , improvement , last_improvement , estimated_improvement )
1847	def JZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . ZF , target . read ( ) , cpu . PC )
7568	def comp ( seq ) : ## makes base to its small complement then makes upper return seq . replace ( "A" , 't' ) . replace ( 'T' , 'a' ) . replace ( 'C' , 'g' ) . replace ( 'G' , 'c' ) . replace ( 'n' , 'Z' ) . upper ( ) . replace ( "Z" , "n" )
1027	def unhex ( s ) : bits = 0 for c in s : if '0' <= c <= '9' : i = ord ( '0' ) elif 'a' <= c <= 'f' : i = ord ( 'a' ) - 10 elif 'A' <= c <= 'F' : i = ord ( 'A' ) - 10 else : break bits = bits * 16 + ( ord ( c ) - i ) return bits
11794	def mac ( csp , var , value , assignment , removals ) : return AC3 ( csp , [ ( X , var ) for X in csp . neighbors [ var ] ] , removals )
9281	def parse_header ( head ) : try : ( fromcall , path ) = head . split ( '>' , 1 ) except : raise ParseError ( "invalid packet header" ) if ( not 1 <= len ( fromcall ) <= 9 or not re . findall ( r"^[a-z0-9]{0,9}(\-[a-z0-9]{1,8})?$" , fromcall , re . I ) ) : raise ParseError ( "fromcallsign is invalid" ) path = path . split ( ',' ) if len ( path [ 0 ] ) == 0 : raise ParseError ( "no tocallsign in header" ) tocall = path [ 0 ] path = path [ 1 : ] validate_callsign ( tocall , "tocallsign" ) for digi in path : if not re . findall ( r"^[A-Z0-9\-]{1,9}\*?$" , digi , re . I ) : raise ParseError ( "invalid callsign in path" ) parsed = { 'from' : fromcall , 'to' : tocall , 'path' : path , } viacall = "" if len ( path ) >= 2 and re . match ( r"^q..$" , path [ - 2 ] ) : viacall = path [ - 1 ] parsed . update ( { 'via' : viacall } ) return parsed
12520	def _load_images_and_labels ( self , images , labels = None ) : if not isinstance ( images , ( list , tuple ) ) : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects. ' 'Got a {}.' . format ( type ( images ) ) ) if not len ( images ) > 0 : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects ' 'of size higher than 0. Got {} items.' . format ( len ( images ) ) ) if labels is not None and len ( labels ) != len ( images ) : raise ValueError ( 'Expected the same length for image set ({}) and ' 'labels list ({}).' . format ( len ( images ) , len ( labels ) ) ) first_file = images [ 0 ] if first_file : first_img = NeuroImage ( first_file ) else : raise ( 'Error reading image {}.' . format ( repr_imgs ( first_file ) ) ) for idx , image in enumerate ( images ) : try : img = NeuroImage ( image ) self . check_compatibility ( img , first_img ) except : log . exception ( 'Error reading image {}.' . format ( repr_imgs ( image ) ) ) raise else : self . items . append ( img ) self . set_labels ( labels )
2893	def get_outgoing_sequence_names ( self ) : return sorted ( [ s . name for s in list ( self . outgoing_sequence_flows_by_id . values ( ) ) ] )
9341	def MetaOrdered ( parallel , done , turnstile ) : class Ordered : def __init__ ( self , iterref ) : if parallel . master : done [ ... ] = 0 self . iterref = iterref parallel . barrier ( ) @ classmethod def abort ( self ) : turnstile . release ( ) def __enter__ ( self ) : while self . iterref != done : pass turnstile . acquire ( ) return self def __exit__ ( self , * args ) : done [ ... ] += 1 turnstile . release ( ) return Ordered
5261	def parse_10qk ( self , response ) : loader = ReportItemLoader ( response = response ) item = loader . load_item ( ) if 'doc_type' in item : doc_type = item [ 'doc_type' ] if doc_type in ( '10-Q' , '10-K' ) : return item return None
3551	def list_descriptors ( self ) : paths = self . _props . Get ( _CHARACTERISTIC_INTERFACE , 'Descriptors' ) return map ( BluezGattDescriptor , get_provider ( ) . _get_objects_by_path ( paths ) )
8069	def randomChildElement ( self , node ) : choices = [ e for e in node . childNodes if e . nodeType == e . ELEMENT_NODE ] chosen = random . choice ( choices ) if _debug : sys . stderr . write ( '%s available choices: %s\n' % ( len ( choices ) , [ e . toxml ( ) for e in choices ] ) ) sys . stderr . write ( 'Chosen: %s\n' % chosen . toxml ( ) ) return chosen
11365	def create_logger ( name , filename = None , logging_level = logging . DEBUG ) : logger = logging . getLogger ( name ) formatter = logging . Formatter ( ( '%(asctime)s - %(name)s - ' '%(levelname)-8s - %(message)s' ) ) if filename : fh = logging . FileHandler ( filename = filename ) fh . setFormatter ( formatter ) logger . addHandler ( fh ) ch = logging . StreamHandler ( ) ch . setFormatter ( formatter ) logger . addHandler ( ch ) logger . setLevel ( logging_level ) return logger
3919	async def _load ( self ) : try : conv_events = await self . _conversation . get_events ( self . _conversation . events [ 0 ] . id_ ) except ( IndexError , hangups . NetworkError ) : conv_events = [ ] if not conv_events : self . _first_loaded = True if self . _focus_position == self . POSITION_LOADING and conv_events : # If the loading indicator is still focused, and we loaded more # events, set focus on the first new event so the loaded # indicator is replaced. self . set_focus ( conv_events [ - 1 ] . id_ ) else : # Otherwise, still need to invalidate in case the loading # indicator is showing but not focused. self . _modified ( ) # Loading events can also update the watermarks. self . _refresh_watermarked_events ( ) self . _is_loading = False
7782	def update_state ( self ) : self . _lock . acquire ( ) try : now = datetime . utcnow ( ) if self . state == 'new' : self . state = 'fresh' if self . state == 'fresh' : if now > self . freshness_time : self . state = 'old' if self . state == 'old' : if now > self . expire_time : self . state = 'stale' if self . state == 'stale' : if now > self . purge_time : self . state = 'purged' self . state_value = _state_values [ self . state ] return self . state finally : self . _lock . release ( )
4270	def get_iptc_data ( filename ) : logger = logging . getLogger ( __name__ ) iptc_data = { } raw_iptc = { } # PILs IptcImagePlugin issues a SyntaxError in certain circumstances # with malformed metadata, see PIL/IptcImagePlugin.py", line 71. # ( https://github.com/python-pillow/Pillow/blob/9dd0348be2751beb2c617e32ff9985aa2f92ae5f/src/PIL/IptcImagePlugin.py#L71 ) try : img = _read_image ( filename ) raw_iptc = IptcImagePlugin . getiptcinfo ( img ) except SyntaxError : logger . info ( 'IPTC Error in %s' , filename ) # IPTC fields are catalogued in: # https://www.iptc.org/std/photometadata/specification/IPTC-PhotoMetadata # 2:05 is the IPTC title property if raw_iptc and ( 2 , 5 ) in raw_iptc : iptc_data [ "title" ] = raw_iptc [ ( 2 , 5 ) ] . decode ( 'utf-8' , errors = 'replace' ) # 2:120 is the IPTC description property if raw_iptc and ( 2 , 120 ) in raw_iptc : iptc_data [ "description" ] = raw_iptc [ ( 2 , 120 ) ] . decode ( 'utf-8' , errors = 'replace' ) # 2:105 is the IPTC headline property if raw_iptc and ( 2 , 105 ) in raw_iptc : iptc_data [ "headline" ] = raw_iptc [ ( 2 , 105 ) ] . decode ( 'utf-8' , errors = 'replace' ) return iptc_data
628	def _orderForCoordinate ( cls , coordinate ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getReal64 ( )
1404	def load_configs ( self ) : self . statemgr_config . set_state_locations ( self . configs [ STATEMGRS_KEY ] ) if EXTRA_LINKS_KEY in self . configs : for extra_link in self . configs [ EXTRA_LINKS_KEY ] : self . extra_links . append ( self . validate_extra_link ( extra_link ) )
7406	def bottom ( self ) : o = self . get_ordering_queryset ( ) . aggregate ( Max ( 'order' ) ) . get ( 'order__max' ) self . to ( o )
9915	def update ( self , instance , validated_data ) : is_primary = validated_data . pop ( "is_primary" , False ) instance = super ( EmailSerializer , self ) . update ( instance , validated_data ) if is_primary : instance . set_primary ( ) return instance
11057	def _remove_by_pk ( self , key , flush = True ) : try : del self . store [ key ] except Exception as error : pass if flush : self . flush ( )
2184	def clear ( self , cfgstr = None ) : data_fpath = self . get_fpath ( cfgstr ) if self . verbose > 0 : self . log ( '[cacher] clear cache' ) if exists ( data_fpath ) : if self . verbose > 0 : self . log ( '[cacher] removing {}' . format ( data_fpath ) ) os . remove ( data_fpath ) # Remove the metadata if it exists meta_fpath = data_fpath + '.meta' if exists ( meta_fpath ) : os . remove ( meta_fpath ) else : if self . verbose > 0 : self . log ( '[cacher] ... nothing to clear' )
8204	def snapshot ( self , target , defer = True , file_number = None ) : output_func = self . output_closure ( target , file_number ) if defer : self . _drawqueue . append ( output_func ) else : self . _drawqueue . append_immediate ( output_func )
8126	def search ( q , start = 1 , count = 10 , context = None , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO_SEARCH return YahooSearch ( q , start , count , service , context , wait , asynchronous , cached )
13322	def rem_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . discard ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
951	def showPredictions ( ) : for k in range ( 6 ) : tm . reset ( ) print "--- " + "ABCDXY" [ k ] + " ---" tm . compute ( set ( seqT [ k ] [ : ] . nonzero ( ) [ 0 ] . tolist ( ) ) , learn = False ) activeColumnsIndices = [ tm . columnForCell ( i ) for i in tm . getActiveCells ( ) ] predictedColumnIndices = [ tm . columnForCell ( i ) for i in tm . getPredictiveCells ( ) ] currentColumns = [ 1 if i in activeColumnsIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] predictedColumns = [ 1 if i in predictedColumnIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] print ( "Active cols: " + str ( np . nonzero ( currentColumns ) [ 0 ] ) ) print ( "Predicted cols: " + str ( np . nonzero ( predictedColumns ) [ 0 ] ) ) print ""
3788	def set_user_method ( self , user_methods , forced = False ) : # Accept either a string or a list of methods, and whether # or not to only consider the false methods if isinstance ( user_methods , str ) : user_methods = [ user_methods ] # The user's order matters and is retained for use by select_valid_methods self . user_methods = user_methods self . forced = forced # Validate that the user's specified methods are actual methods if set ( self . user_methods ) . difference ( self . all_methods ) : raise Exception ( "One of the given methods is not available for this mixture" ) if not self . user_methods and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) # Remove previously selected methods self . method = None self . sorted_valid_methods = [ ] self . TP_zs_ws_cached = ( None , None , None , None )
5187	def inventory ( self , * * kwargs ) : inventory = self . _query ( 'inventory' , * * kwargs ) for inv in inventory : yield Inventory ( node = inv [ 'certname' ] , time = inv [ 'timestamp' ] , environment = inv [ 'environment' ] , facts = inv [ 'facts' ] , trusted = inv [ 'trusted' ] )
9620	def main ( ) : import time print ( 'Testing controller in position 1:' ) print ( 'Running 3 x 3 seconds tests' ) # Initialise Controller con = rController ( 1 ) # Loop printing controller state and buttons held for i in range ( 3 ) : print ( 'Waiting...' ) time . sleep ( 2.5 ) print ( 'State: ' , con . gamepad ) print ( 'Buttons: ' , con . buttons ) time . sleep ( 0.5 ) print ( 'Done!' )
2038	def SWAP ( self , * operands ) : a = operands [ 0 ] b = operands [ - 1 ] return ( b , ) + operands [ 1 : - 1 ] + ( a , )
9366	def account_number ( ) : account = [ random . randint ( 1 , 9 ) for _ in range ( 20 ) ] return "" . join ( map ( str , account ) )
8148	def _load_namespace ( self , namespace , filename = None ) : from shoebot import data for name in dir ( data ) : namespace [ name ] = getattr ( data , name ) for name in dir ( self ) : if name [ 0 ] != '_' : namespace [ name ] = getattr ( self , name ) namespace [ '_ctx' ] = self # Used in older nodebox scripts. namespace [ '__file__' ] = filename
2867	def readU8 ( self , register ) : result = self . _bus . read_byte_data ( self . _address , register ) & 0xFF self . _logger . debug ( "Read 0x%02X from register 0x%02X" , result , register ) return result
12573	def put_df_as_ndarray ( self , key , df , range_values , loop_multiindex = False , unstack = False , fill_value = 0 , fill_method = None ) : idx_colnames = df . index . names #idx_colranges = [range_values[x] for x in idx_colnames] #dataset group name if not given if key is None : key = idx_colnames [ 0 ] if loop_multiindex : idx_values = df . index . get_level_values ( 0 ) . unique ( ) for idx in idx_values : vals , _ = self . _fill_missing_values ( df . xs ( ( idx , ) , level = idx_colnames [ 0 ] ) , range_values , fill_value = fill_value , fill_method = fill_method ) ds_name = str ( idx ) + '_' + '_' . join ( vals . columns ) self . _push_dfblock ( key , vals , ds_name , range_values ) return self . _handle . get_node ( '/' + str ( key ) ) #separate the dataframe into blocks, only with the first index else : if unstack : df = df . unstack ( idx_colnames [ 0 ] ) for idx in df : vals , _ = self . _fill_missing_values ( df [ idx ] , range_values , fill_value = fill_value , fill_method = fill_method ) vals = np . nan_to_num ( vals ) ds_name = '_' . join ( [ str ( x ) for x in vals . name ] ) self . _push_dfblock ( key , vals , ds_name , range_values ) return self . _handle . get_node ( '/' + str ( key ) ) #not separate the data vals , _ = self . _fill_missing_values ( df , range_values , fill_value = fill_value , fill_method = fill_method ) ds_name = self . _array_dsname return self . _push_dfblock ( key , vals , ds_name , range_values )
6090	def transform_grid ( func ) : @ wraps ( func ) def wrapper ( profile , grid , * args , * * kwargs ) : """ Parameters ---------- profile : GeometryProfile The profiles that owns the function grid : ndarray PlaneCoordinates in either cartesian or profiles coordinate system args kwargs Returns ------- A value or coordinate in the same coordinate system as those passed in. """ if not isinstance ( grid , TransformedGrid ) : return func ( profile , profile . transform_grid_to_reference_frame ( grid ) , * args , * * kwargs ) else : return func ( profile , grid , * args , * * kwargs ) return wrapper
12430	def create_nginx_config ( self ) : cfg = '# nginx config for {0}\n' . format ( self . _project_name ) if not self . _shared_hosting : # user if self . _user : cfg += 'user {0};\n' . format ( self . _user ) # misc nginx config cfg += 'worker_processes 1;\nerror_log {0}-errors.log;\n\ pid {1}_ nginx.pid;\n\n' . format ( os . path . join ( self . _log_dir , self . _project_name ) , os . path . join ( self . _var_dir , self . _project_name ) ) cfg += 'events {\n\tworker_connections 32;\n}\n\n' # http section cfg += 'http {\n' if self . _include_mimetypes : cfg += '\tinclude mime.types;\n' cfg += '\tdefault_type application/octet-stream;\n' cfg += '\tclient_max_body_size 1G;\n' cfg += '\tproxy_max_temp_file_size 0;\n' cfg += '\tproxy_buffering off;\n' cfg += '\taccess_log {0}-access.log;\n' . format ( os . path . join ( self . _log_dir , self . _project_name ) ) cfg += '\tsendfile on;\n' cfg += '\tkeepalive_timeout 65;\n' # server section cfg += '\tserver {\n' cfg += '\t\tlisten 0.0.0.0:{0};\n' . format ( self . _port ) if self . _server_name : cfg += '\t\tserver_name {0};\n' . format ( self . _server_name ) # location section cfg += '\t\tlocation / {\n' cfg += '\t\t\tuwsgi_pass unix:///{0}.sock;\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) ) cfg += '\t\t\tinclude uwsgi_params;\n' cfg += '\t\t}\n\n' # end location # error page templates cfg += '\t\terror_page 500 502 503 504 /50x.html;\n' cfg += '\t\tlocation = /50x.html {\n' cfg += '\t\t\troot html;\n' # end error page section cfg += '\t\t}\n' # end server section cfg += '\t}\n' if not self . _shared_hosting : # end http section cfg += '}\n' # create conf f = open ( self . _nginx_config , 'w' ) f . write ( cfg ) f . close ( )
3364	def load_yaml_model ( filename ) : if isinstance ( filename , string_types ) : with io . open ( filename , "r" ) as file_handle : return model_from_dict ( yaml . load ( file_handle ) ) else : return model_from_dict ( yaml . load ( filename ) )
9917	def validate ( self , data ) : user = self . _confirmation . email . user if ( app_settings . EMAIL_VERIFICATION_PASSWORD_REQUIRED and not user . check_password ( data [ "password" ] ) ) : raise serializers . ValidationError ( _ ( "The provided password is invalid." ) ) # Add email to returned data data [ "email" ] = self . _confirmation . email . email return data
7551	def _cmd_exists ( cmd ) : return _subprocess . call ( "type " + cmd , shell = True , stdout = _subprocess . PIPE , stderr = _subprocess . PIPE ) == 0
4942	def enterprise_customer_uuid ( self ) : try : enterprise_user = EnterpriseCustomerUser . objects . get ( user_id = self . user . id ) except ObjectDoesNotExist : LOGGER . warning ( 'User {} has a {} assignment but is not linked to an enterprise!' . format ( self . __class__ , self . user . id ) ) return None except MultipleObjectsReturned : LOGGER . warning ( 'User {} is linked to multiple enterprises, which is not yet supported!' . format ( self . user . id ) ) return None return str ( enterprise_user . enterprise_customer . uuid )
6364	def to_tuple ( self ) : return self . _tp , self . _tn , self . _fp , self . _fn
12571	def get ( self , key ) : node = self . get_node ( key ) if node is None : raise KeyError ( 'No object named %s in the file' % key ) if hasattr ( node , 'attrs' ) : if 'pandas_type' in node . attrs : return self . _read_group ( node ) return self . _read_array ( node )
12688	def send_now ( users , label , extra_context = None , sender = None ) : sent = False if extra_context is None : extra_context = { } notice_type = NoticeType . objects . get ( label = label ) current_language = get_language ( ) for user in users : # get user language for user from language store defined in # NOTIFICATION_LANGUAGE_MODULE setting try : language = get_notification_language ( user ) except LanguageStoreNotAvailable : language = None if language is not None : # activate the user's language activate ( language ) for backend in NOTIFICATION_BACKENDS . values ( ) : if backend . can_send ( user , notice_type ) : backend . deliver ( user , sender , notice_type , extra_context ) sent = True # reset environment to original language activate ( current_language ) return sent
6168	def from_bin ( bin_array ) : width = len ( bin_array ) bin_wgts = 2 ** np . arange ( width - 1 , - 1 , - 1 ) return int ( np . dot ( bin_array , bin_wgts ) )
3799	def Bahadori_liquid ( T , M ) : A = [ - 6.48326E-2 , 2.715015E-3 , - 1.08580E-5 , 9.853917E-9 ] B = [ 1.565612E-2 , - 1.55833E-4 , 5.051114E-7 , - 4.68030E-10 ] C = [ - 1.80304E-4 , 1.758693E-6 , - 5.55224E-9 , 5.201365E-12 ] D = [ 5.880443E-7 , - 5.65898E-9 , 1.764384E-11 , - 1.65944E-14 ] X , Y = M , T a = A [ 0 ] + B [ 0 ] * X + C [ 0 ] * X ** 2 + D [ 0 ] * X ** 3 b = A [ 1 ] + B [ 1 ] * X + C [ 1 ] * X ** 2 + D [ 1 ] * X ** 3 c = A [ 2 ] + B [ 2 ] * X + C [ 2 ] * X ** 2 + D [ 2 ] * X ** 3 d = A [ 3 ] + B [ 3 ] * X + C [ 3 ] * X ** 2 + D [ 3 ] * X ** 3 return a + b * Y + c * Y ** 2 + d * Y ** 3
10516	def verifyscrollbarhorizontal ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXOrientation == "AXHorizontalOrientation" : return 1 except : pass return 0
10019	def create_environment ( self , env_name , version_label = None , solution_stack_name = None , cname_prefix = None , description = None , option_settings = None , tier_name = 'WebServer' , tier_type = 'Standard' , tier_version = '1.1' ) : out ( "Creating environment: " + str ( env_name ) + ", tier_name:" + str ( tier_name ) + ", tier_type:" + str ( tier_type ) ) self . ebs . create_environment ( self . app_name , env_name , version_label = version_label , solution_stack_name = solution_stack_name , cname_prefix = cname_prefix , description = description , option_settings = option_settings , tier_type = tier_type , tier_name = tier_name , tier_version = tier_version )
2473	def set_lic_id ( self , doc , lic_id ) : # FIXME: this state does not make sense self . reset_extr_lics ( ) if validations . validate_extracted_lic_id ( lic_id ) : doc . add_extr_lic ( document . ExtractedLicense ( lic_id ) ) return True else : raise SPDXValueError ( 'ExtractedLicense::id' )
4985	def get ( self , request , enterprise_uuid , program_uuid ) : verify_edx_resources ( ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) context_data = get_global_context ( request , enterprise_customer ) program_details , error_code = self . get_program_details ( request , program_uuid , enterprise_customer ) if error_code : return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , ) if program_details [ 'certificate_eligible_for_program' ] : # The user is already enrolled in the program, so redirect to the program's dashboard. return redirect ( LMS_PROGRAMS_DASHBOARD_URL . format ( uuid = program_uuid ) ) # Check to see if access to any of the course runs in the program are restricted for this user. course_run_ids = [ ] for course in program_details [ 'courses' ] : for course_run in course [ 'course_runs' ] : course_run_ids . append ( course_run [ 'key' ] ) embargo_url = EmbargoApiClient . redirect_if_blocked ( course_run_ids , request . user , get_ip ( request ) , request . path ) if embargo_url : return redirect ( embargo_url ) return self . get_enterprise_program_enrollment_page ( request , enterprise_customer , program_details )
11724	def init_config ( self , app ) : config_apps = [ 'APP_' , 'RATELIMIT_' ] flask_talisman_debug_mode = [ "'unsafe-inline'" ] for k in dir ( config ) : if any ( [ k . startswith ( prefix ) for prefix in config_apps ] ) : app . config . setdefault ( k , getattr ( config , k ) ) if app . config [ 'DEBUG' ] : app . config . setdefault ( 'APP_DEFAULT_SECURE_HEADERS' , { } ) headers = app . config [ 'APP_DEFAULT_SECURE_HEADERS' ] # ensure `content_security_policy` is not set to {} if headers . get ( 'content_security_policy' ) != { } : headers . setdefault ( 'content_security_policy' , { } ) csp = headers [ 'content_security_policy' ] # ensure `default-src` is not set to [] if csp . get ( 'default-src' ) != [ ] : csp . setdefault ( 'default-src' , [ ] ) # add default `content_security_policy` value when debug csp [ 'default-src' ] += flask_talisman_debug_mode
10024	def deploy_version ( self , environment_name , version_label ) : out ( "Deploying " + str ( version_label ) + " to " + str ( environment_name ) ) self . ebs . update_environment ( environment_name = environment_name , version_label = version_label )
5162	def __intermediate_interface ( self , interface , uci_name ) : interface . update ( { '.type' : 'interface' , '.name' : uci_name , 'ifname' : interface . pop ( 'name' ) } ) if 'network' in interface : del interface [ 'network' ] if 'mac' in interface : # mac address of wireless interface must # be set in /etc/config/wireless, therfore # we can skip this in /etc/config/network if interface . get ( 'type' ) != 'wireless' : interface [ 'macaddr' ] = interface [ 'mac' ] del interface [ 'mac' ] if 'autostart' in interface : interface [ 'auto' ] = interface [ 'autostart' ] del interface [ 'autostart' ] if 'disabled' in interface : interface [ 'enabled' ] = not interface [ 'disabled' ] del interface [ 'disabled' ] if 'wireless' in interface : del interface [ 'wireless' ] if 'addresses' in interface : del interface [ 'addresses' ] return interface
12939	def clearRedisPools ( ) : global RedisPools global _redisManagedConnectionParams for pool in RedisPools . values ( ) : try : pool . disconnect ( ) except : pass for paramsList in _redisManagedConnectionParams . values ( ) : for params in paramsList : if 'connection_pool' in params : del params [ 'connection_pool' ] RedisPools . clear ( ) _redisManagedConnectionParams . clear ( )
5746	def asn ( self , ip , announce_date = None ) : assignations , announce_date , _ = self . run ( ip , announce_date ) return next ( ( assign for assign in assignations if assign is not None ) , None ) , announce_date
4712	def script_run ( trun , script ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { script: %s }" % script ) cij . emph ( "rnr:script:run:evars: %s" % script [ "evars" ] ) launchers = { ".py" : "python" , ".sh" : "source" } ext = os . path . splitext ( script [ "fpath" ] ) [ - 1 ] if not ext in launchers . keys ( ) : cij . err ( "rnr:script:run { invalid script[\"fpath\"]: %r }" % script [ "fpath" ] ) return 1 launch = launchers [ ext ] with open ( script [ "log_fpath" ] , "a" ) as log_fd : log_fd . write ( "# script_fpath: %r\n" % script [ "fpath" ] ) log_fd . flush ( ) bgn = time . time ( ) cmd = [ 'bash' , '-c' , 'CIJ_ROOT=$(cij_root) && ' 'source $CIJ_ROOT/modules/cijoe.sh && ' 'source %s && ' 'CIJ_TEST_RES_ROOT="%s" %s %s ' % ( trun [ "conf" ] [ "ENV_FPATH" ] , script [ "res_root" ] , launch , script [ "fpath" ] ) ] if trun [ "conf" ] [ "VERBOSE" ] > 1 : cij . emph ( "rnr:script:run { cmd: %r }" % " " . join ( cmd ) ) evars = os . environ . copy ( ) evars . update ( { k : str ( script [ "evars" ] [ k ] ) for k in script [ "evars" ] } ) process = Popen ( cmd , stdout = log_fd , stderr = STDOUT , cwd = script [ "res_root" ] , env = evars ) process . wait ( ) script [ "rcode" ] = process . returncode script [ "wallc" ] = time . time ( ) - bgn if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { wallc: %02f }" % script [ "wallc" ] ) cij . emph ( "rnr:script:run { rcode: %r } " % script [ "rcode" ] , script [ "rcode" ] ) return script [ "rcode" ]
2647	def bash_app ( function = None , data_flow_kernel = None , walltime = 60 , cache = False , executors = 'all' ) : from parsl . app . bash import BashApp def decorator ( func ) : def wrapper ( f ) : return BashApp ( f , data_flow_kernel = data_flow_kernel , walltime = walltime , cache = cache , executors = executors ) return wrapper ( func ) if function is not None : return decorator ( function ) return decorator
8911	def ows_security_tween_factory ( handler , registry ) : security = owssecurity_factory ( registry ) def ows_security_tween ( request ) : try : security . check_request ( request ) return handler ( request ) except OWSException as err : logger . exception ( "security check failed." ) return err except Exception as err : logger . exception ( "unknown error" ) return OWSNoApplicableCode ( "{}" . format ( err ) ) return ows_security_tween
770	def _getGroundTruth ( self , inferenceElement ) : sensorInputElement = InferenceElement . getInputElement ( inferenceElement ) if sensorInputElement is None : return None return getattr ( self . __currentGroundTruth . sensorInput , sensorInputElement )
9298	def paginate_query ( self , query , count , offset = None , sort = None ) : assert isinstance ( query , peewee . Query ) assert isinstance ( count , int ) assert isinstance ( offset , ( str , int , type ( None ) ) ) assert isinstance ( sort , ( list , set , tuple , type ( None ) ) ) # ensure our model has a primary key fields = query . model . _meta . get_primary_keys ( ) if len ( fields ) == 0 : raise peewee . ProgrammingError ( 'Cannot apply pagination on model without primary key' ) # ensure our model doesn't use a compound primary key if len ( fields ) > 1 : raise peewee . ProgrammingError ( 'Cannot apply pagination on model with compound primary key' ) # apply offset if offset is not None : query = query . where ( fields [ 0 ] >= offset ) # do we need to apply sorting? order_bys = [ ] if sort : for field , direction in sort : # does this field have a valid sort direction? if not isinstance ( direction , str ) : raise ValueError ( "Invalid sort direction on field '{}'" . format ( field ) ) direction = direction . lower ( ) . strip ( ) if direction not in [ 'asc' , 'desc' ] : raise ValueError ( "Invalid sort direction on field '{}'" . format ( field ) ) # apply sorting order_by = peewee . SQL ( field ) order_by = getattr ( order_by , direction ) ( ) order_bys += [ order_by ] # add primary key ordering after user sorting order_bys += [ fields [ 0 ] . asc ( ) ] # apply ordering and limits query = query . order_by ( * order_bys ) query = query . limit ( count ) return query
126	def Negative ( other_param , mode = "invert" , reroll_count_max = 2 ) : return ForceSign ( other_param = other_param , positive = False , mode = mode , reroll_count_max = reroll_count_max )
9725	async def set_qtm_event ( self , event = None ) : cmd = "event%s" % ( "" if event is None else " " + event ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
10712	def _translateCommands ( commands ) : for command in commands . split ( ',' ) : # each command results in 2 bytes of binary data result = [ 0 , 0 ] device , command = command . strip ( ) . upper ( ) . split ( None , 1 ) # translate the house code result [ 0 ] = houseCodes [ device [ 0 ] ] # translate the device number if there is one if len ( device ) > 1 : deviceNumber = deviceNumbers [ device [ 1 : ] ] result [ 0 ] |= deviceNumber [ 0 ] result [ 1 ] = deviceNumber [ 1 ] # translate the command result [ 1 ] |= commandCodes [ command ] # convert 2 bytes to bit strings and yield them yield ' ' . join ( map ( _strBinary , result ) )
5754	def bootstrap_paginate ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" " (Page object reference)" % bits [ 0 ] ) page = parser . compile_filter ( bits [ 1 ] ) kwargs = { } bits = bits [ 2 : ] kwarg_re = re . compile ( r'(\w+)=(.+)' ) if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to bootstrap_pagination paginate tag" ) name , value = match . groups ( ) kwargs [ name ] = parser . compile_filter ( value ) return BootstrapPaginationNode ( page , kwargs )
8712	def file_list ( self ) : log . info ( 'Listing files' ) res = self . __exchange ( LIST_FILES ) res = res . split ( '\r\n' ) # skip first and last lines res = res [ 1 : - 1 ] files = [ ] for line in res : files . append ( line . split ( '\t' ) ) return files
348	def load_nietzsche_dataset ( path = 'data' ) : logging . info ( "Load or Download nietzsche dataset > {}" . format ( path ) ) path = os . path . join ( path , 'nietzsche' ) filename = "nietzsche.txt" url = 'https://s3.amazonaws.com/text-datasets/' filepath = maybe_download_and_extract ( filename , path , url ) with open ( filepath , "r" ) as f : words = f . read ( ) return words
4408	def connected_channel ( self ) : if not self . channel_id : return None return self . _lavalink . bot . get_channel ( int ( self . channel_id ) )
5553	def _validate_zooms ( zooms ) : if isinstance ( zooms , dict ) : if any ( [ a not in zooms for a in [ "min" , "max" ] ] ) : raise MapcheteConfigError ( "min and max zoom required" ) zmin = _validate_zoom ( zooms [ "min" ] ) zmax = _validate_zoom ( zooms [ "max" ] ) if zmin > zmax : raise MapcheteConfigError ( "max zoom must not be smaller than min zoom" ) return list ( range ( zmin , zmax + 1 ) ) elif isinstance ( zooms , list ) : if len ( zooms ) == 1 : return zooms elif len ( zooms ) == 2 : zmin , zmax = sorted ( [ _validate_zoom ( z ) for z in zooms ] ) return list ( range ( zmin , zmax + 1 ) ) else : return zooms else : return [ _validate_zoom ( zooms ) ]
3712	def calculate_P ( self , T , P , method ) : if method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP ( T = T , P = P ) Vm = self . eos [ 0 ] . V_g elif method == TSONOPOULOS_EXTENDED : B = BVirial_Tsonopoulos_extended ( T , self . Tc , self . Pc , self . omega , dipole = self . dipole ) Vm = ideal_gas ( T , P ) + B elif method == TSONOPOULOS : B = BVirial_Tsonopoulos ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == ABBOTT : B = BVirial_Abbott ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == PITZER_CURL : B = BVirial_Pitzer_Curl ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == CRC_VIRIAL : a1 , a2 , a3 , a4 , a5 = self . CRC_VIRIAL_coeffs t = 298.15 / T - 1. B = ( a1 + a2 * t + a3 * t ** 2 + a4 * t ** 3 + a5 * t ** 4 ) / 1E6 Vm = ideal_gas ( T , P ) + B elif method == IDEAL : Vm = ideal_gas ( T , P ) elif method == COOLPROP : Vm = 1. / PropsSI ( 'DMOLAR' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : Vm = self . interpolate_P ( T , P , method ) return Vm
12925	def match_to_clinvar ( genome_file , clin_file ) : clin_curr_line = _next_line ( clin_file ) genome_curr_line = _next_line ( genome_file ) # Ignores all the lines that start with a hashtag while clin_curr_line . startswith ( '#' ) : clin_curr_line = _next_line ( clin_file ) while genome_curr_line . startswith ( '#' ) : genome_curr_line = _next_line ( genome_file ) # Advance through both files simultaneously to find matches while clin_curr_line and genome_curr_line : # Advance a file when positions aren't equal. clin_curr_pos = VCFLine . get_pos ( clin_curr_line ) genome_curr_pos = VCFLine . get_pos ( genome_curr_line ) try : if clin_curr_pos [ 'chrom' ] > genome_curr_pos [ 'chrom' ] : genome_curr_line = _next_line ( genome_file ) continue elif clin_curr_pos [ 'chrom' ] < genome_curr_pos [ 'chrom' ] : clin_curr_line = _next_line ( clin_file ) continue if clin_curr_pos [ 'pos' ] > genome_curr_pos [ 'pos' ] : genome_curr_line = _next_line ( genome_file ) continue elif clin_curr_pos [ 'pos' ] < genome_curr_pos [ 'pos' ] : clin_curr_line = _next_line ( clin_file ) continue except StopIteration : break # If we get here, start positions match. # Look for allele matching. genome_vcf_line = GenomeVCFLine ( vcf_line = genome_curr_line , skip_info = True ) # We can skip if genome has no allele information for this point. if not genome_vcf_line . genotype_allele_indexes : genome_curr_line = _next_line ( genome_file ) continue # Match only if ClinVar and Genome ref_alleles match. clinvar_vcf_line = ClinVarVCFLine ( vcf_line = clin_curr_line ) if not genome_vcf_line . ref_allele == clinvar_vcf_line . ref_allele : try : genome_curr_line = _next_line ( genome_file ) clin_curr_line = _next_line ( clin_file ) continue except StopIteration : break # Determine genome alleles and zygosity. Zygosity is assumed to be one # of: heterozygous, homozygous, or hemizygous. genotype_allele_indexes = genome_vcf_line . genotype_allele_indexes genome_alleles = [ genome_vcf_line . alleles [ x ] for x in genotype_allele_indexes ] if len ( genome_alleles ) == 1 : zygosity = 'Hem' elif len ( genome_alleles ) == 2 : if genome_alleles [ 0 ] . sequence == genome_alleles [ 1 ] . sequence : zygosity = 'Hom' genome_alleles = [ genome_alleles [ 0 ] ] else : zygosity = 'Het' else : raise ValueError ( 'This code only expects to work on genomes ' + 'with one or two alleles called at each ' + 'location. The following line violates this:' + str ( genome_vcf_line ) ) # Look for matches to ClinVar alleles. for genome_allele in genome_alleles : for allele in clinvar_vcf_line . alleles : if genome_allele . sequence == allele . sequence : # The 'records' attribute is specific to ClinVarAlleles. if hasattr ( allele , 'records' ) : yield ( genome_vcf_line , allele , zygosity ) # Done matching, move on. try : genome_curr_line = _next_line ( genome_file ) clin_curr_line = _next_line ( clin_file ) except StopIteration : break
10124	def flip_x ( self , center = None ) : if center is None : self . poly . flip ( ) else : self . poly . flip ( center [ 0 ] )
2498	def handle_package_has_file ( self , package , package_node ) : file_nodes = map ( self . handle_package_has_file_helper , package . files ) triples = [ ( package_node , self . spdx_namespace . hasFile , node ) for node in file_nodes ] for triple in triples : self . graph . add ( triple )
10266	def collapse_consistent_edges ( graph : BELGraph ) : for u , v in graph . edges ( ) : relation = pair_is_consistent ( graph , u , v ) if not relation : continue edges = [ ( u , v , k ) for k in graph [ u ] [ v ] ] graph . remove_edges_from ( edges ) graph . add_edge ( u , v , attr_dict = { RELATION : relation } )
11243	def add_newlines ( f , output , char ) : line_count = get_line_count ( f ) f = open ( f , 'r+' ) output = open ( output , 'r+' ) for line in range ( line_count ) : string = f . readline ( ) string = re . sub ( char , char + '\n' , string ) output . write ( string )
1297	def from_config ( config , kwargs = None ) : return util . get_object ( obj = config , predefined = tensorforce . core . optimizers . solvers . solvers , kwargs = kwargs )
9665	def clean_all ( G , settings ) : quiet = settings [ "quiet" ] recon = settings [ "recon" ] sprint = settings [ "sprint" ] error = settings [ "error" ] all_outputs = [ ] for node in G . nodes ( data = True ) : if "output" in node [ 1 ] : for item in get_all_outputs ( node [ 1 ] ) : all_outputs . append ( item ) all_outputs . append ( ".shastore" ) retcode = 0 for item in sorted ( all_outputs ) : if os . path . isfile ( item ) : if recon : sprint ( "Would remove file: {}" . format ( item ) ) continue sprint ( "Attempting to remove file '{}'" , level = "verbose" ) try : os . remove ( item ) sprint ( "Removed file" , level = "verbose" ) except : errmes = "Error: file '{}' failed to be removed" error ( errmes . format ( item ) ) retcode = 1 if not retcode and not recon : sprint ( "All clean" , color = True ) return retcode
1931	def update ( self , name : str , value = None , default = None , description : str = None ) : if name in self . _vars : description = description or self . _vars [ name ] . description default = default or self . _vars [ name ] . default elif name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default , defined = False ) v . value = value self . _vars [ name ] = v
661	def computeSaturationLevels ( outputs , outputsShape , sparseForm = False ) : # Get the outputs into a SparseBinaryMatrix if not sparseForm : outputs = outputs . reshape ( outputsShape ) spOut = SM32 ( outputs ) else : if len ( outputs ) > 0 : assert ( outputs . max ( ) < outputsShape [ 0 ] * outputsShape [ 1 ] ) spOut = SM32 ( 1 , outputsShape [ 0 ] * outputsShape [ 1 ] ) spOut . setRowFromSparse ( 0 , outputs , [ 1 ] * len ( outputs ) ) spOut . reshape ( outputsShape [ 0 ] , outputsShape [ 1 ] ) # Get the activity in each local region using the nNonZerosPerBox method # This method takes a list of the end row indices and a list of the end # column indices. # We will use regions that are 15x15, which give us about a 1/225 (.4%) resolution # on saturation. regionSize = 15 rows = xrange ( regionSize + 1 , outputsShape [ 0 ] + 1 , regionSize ) cols = xrange ( regionSize + 1 , outputsShape [ 1 ] + 1 , regionSize ) regionSums = spOut . nNonZerosPerBox ( rows , cols ) # Get all the nonzeros out - those are our saturation sums ( locations , values ) = regionSums . tolist ( ) values /= float ( regionSize * regionSize ) sat = list ( values ) # Now, to compute which are the inner regions, we will only take the ones that # are surrounded by activity above, below, left and right innerSat = [ ] locationSet = set ( locations ) for ( location , value ) in itertools . izip ( locations , values ) : ( row , col ) = location if ( row - 1 , col ) in locationSet and ( row , col - 1 ) in locationSet and ( row + 1 , col ) in locationSet and ( row , col + 1 ) in locationSet : innerSat . append ( value ) return ( sat , innerSat )
2239	def _syspath_modname_to_modpath ( modname , sys_path = None , exclude = None ) : def _isvalid ( modpath , base ) : # every directory up to the module, should have an init subdir = dirname ( modpath ) while subdir and subdir != base : if not exists ( join ( subdir , '__init__.py' ) ) : return False subdir = dirname ( subdir ) return True _fname_we = modname . replace ( '.' , os . path . sep ) candidate_fnames = [ _fname_we + '.py' , # _fname_we + '.pyc', # _fname_we + '.pyo', ] # Add extension library suffixes candidate_fnames += [ _fname_we + ext for ext in _platform_pylib_exts ( ) ] if sys_path is None : sys_path = sys . path # the empty string in sys.path indicates cwd. Change this to a '.' candidate_dpaths = [ '.' if p == '' else p for p in sys_path ] if exclude : def normalize ( p ) : if sys . platform . startswith ( 'win32' ) : # nocover return realpath ( p ) . lower ( ) else : return realpath ( p ) # Keep only the paths not in exclude real_exclude = { normalize ( p ) for p in exclude } candidate_dpaths = [ p for p in candidate_dpaths if normalize ( p ) not in real_exclude ] for dpath in candidate_dpaths : # Check for directory-based modules (has presidence over files) modpath = join ( dpath , _fname_we ) if exists ( modpath ) : if isfile ( join ( modpath , '__init__.py' ) ) : if _isvalid ( modpath , dpath ) : return modpath # If that fails, check for file-based modules for fname in candidate_fnames : modpath = join ( dpath , fname ) if isfile ( modpath ) : if _isvalid ( modpath , dpath ) : return modpath
6112	def single_value ( cls , value , shape , pixel_scale , origin = ( 0.0 , 0.0 ) ) : array = np . ones ( shape ) * value return cls ( array , pixel_scale , origin )
3787	def TP_dependent_property_derivative_P ( self , T , P , order = 1 ) : sorted_valid_methods_P = self . select_valid_methods_P ( T , P ) for method in sorted_valid_methods_P : try : return self . calculate_derivative_P ( P , T , method , order ) except : pass return None
9897	def boottime ( ) : global __boottime if __boottime is None : up = uptime ( ) if up is None : return None if __boottime is None : _boottime_linux ( ) if datetime is None : raise RuntimeError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime or time . time ( ) - up )
11426	def record_strip_empty_volatile_subfields ( rec ) : for tag in rec . keys ( ) : for field in rec [ tag ] : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield [ 1 ] [ : 9 ] != "VOLATILE:" ]
11699	def serve ( self , sock , request_handler , error_handler , debug = False , request_timeout = 60 , ssl = None , request_max_size = None , reuse_port = False , loop = None , protocol = HttpProtocol , backlog = 100 , * * kwargs ) : if debug : loop . set_debug ( debug ) server = partial ( protocol , loop = loop , connections = self . connections , signal = self . signal , request_handler = request_handler , error_handler = error_handler , request_timeout = request_timeout , request_max_size = request_max_size , ) server_coroutine = loop . create_server ( server , host = None , port = None , ssl = ssl , reuse_port = reuse_port , sock = sock , backlog = backlog ) # Instead of pulling time at the end of every request, # pull it once per minute loop . call_soon ( partial ( update_current_time , loop ) ) return server_coroutine
11731	def _compress ( self , input_str ) : compressed_bits = cStringIO . StringIO ( ) f = gzip . GzipFile ( fileobj = compressed_bits , mode = 'wb' ) f . write ( input_str ) f . close ( ) return compressed_bits . getvalue ( )
10294	def get_namespaces_with_incorrect_names ( graph : BELGraph ) -> Set [ str ] : return { exc . namespace for _ , exc , _ in graph . warnings if isinstance ( exc , ( MissingNamespaceNameWarning , MissingNamespaceRegexWarning ) ) }
4494	def upload ( args ) : osf = _setup_osf ( args ) if osf . username is None or osf . password is None : sys . exit ( 'To upload a file you need to provide a username and' ' password.' ) project = osf . project ( args . project ) storage , remote_path = split_storage ( args . destination ) store = project . storage ( storage ) if args . recursive : if not os . path . isdir ( args . source ) : raise RuntimeError ( "Expected source ({}) to be a directory when " "using recursive mode." . format ( args . source ) ) # local name of the directory that is being uploaded _ , dir_name = os . path . split ( args . source ) for root , _ , files in os . walk ( args . source ) : subdir_path = os . path . relpath ( root , args . source ) for fname in files : local_path = os . path . join ( root , fname ) with open ( local_path , 'rb' ) as fp : # build the remote path + fname name = os . path . join ( remote_path , dir_name , subdir_path , fname ) store . create_file ( name , fp , force = args . force , update = args . update ) else : with open ( args . source , 'rb' ) as fp : store . create_file ( remote_path , fp , force = args . force , update = args . update )
4154	def add_markdown_cell ( self , text ) : markdown_cell = { "cell_type" : "markdown" , "metadata" : { } , "source" : [ rst2md ( text ) ] } self . work_notebook [ "cells" ] . append ( markdown_cell )
10529	def get_projects ( limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : print ( OFFSET_WARNING ) params = dict ( limit = limit , offset = offset ) try : res = _pybossa_req ( 'get' , 'project' , params = params ) if type ( res ) . __name__ == 'list' : return [ Project ( project ) for project in res ] else : raise TypeError except : # pragma: no cover raise
10577	def get_assay ( self ) : masses_sum = sum ( self . compound_masses ) return [ m / masses_sum for m in self . compound_masses ]
5728	def _get_responses_unix ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : select_timeout = timeout_time_sec - time . time ( ) # I prefer to not pass a negative value to select if select_timeout <= 0 : select_timeout = 0 events , _ , _ = select . select ( self . read_list , [ ] , [ ] , select_timeout ) responses_list = None # to avoid infinite loop if using Python 2 try : for fileno in events : # new data is ready to read if fileno == self . stdout_fileno : self . gdb_process . stdout . flush ( ) raw_output = self . gdb_process . stdout . read ( ) stream = "stdout" elif fileno == self . stderr_fileno : self . gdb_process . stderr . flush ( ) raw_output = self . gdb_process . stderr . read ( ) stream = "stderr" else : raise ValueError ( "Developer error. Got unexpected file number %d" % fileno ) responses_list = self . _get_responses_list ( raw_output , stream ) responses += responses_list except IOError : # only occurs in python 2.7 pass if timeout_sec == 0 : # just exit immediately break elif responses_list and self . _allow_overwrite_timeout_times : # update timeout time to potentially be closer to now to avoid lengthy wait times when nothing is being output by gdb timeout_time_sec = min ( time . time ( ) + self . time_to_check_for_additional_output_sec , timeout_time_sec , ) elif time . time ( ) > timeout_time_sec : break return responses
11557	def extended_analog ( self , pin , data ) : analog_data = [ pin , data & 0x7f , ( data >> 7 ) & 0x7f , ( data >> 14 ) & 0x7f ] self . _command_handler . send_sysex ( self . _command_handler . EXTENDED_ANALOG , analog_data )
4683	def getAccounts ( self ) : pubkeys = self . getPublicKeys ( ) accounts = [ ] for pubkey in pubkeys : # Filter those keys not for our network if pubkey [ : len ( self . prefix ) ] == self . prefix : accounts . extend ( self . getAccountsFromPublicKey ( pubkey ) ) return accounts
12676	def _escape_char ( c , escape_char = ESCAPE_CHAR ) : buf = [ ] for byte in c . encode ( 'utf8' ) : buf . append ( escape_char ) buf . append ( '%X' % _ord ( byte ) ) return '' . join ( buf )
12838	async def async_connect ( self ) : if self . _async_lock is None : raise Exception ( 'Error, database not properly initialized before async connection' ) async with self . _async_lock : self . connect ( True ) return self . _state . conn
6110	def unmasked_blurred_image_of_galaxies_from_psf ( self , padded_grid_stack , psf ) : return [ padded_grid_stack . unmasked_blurred_image_from_psf_and_unmasked_image ( psf , image ) if not galaxy . has_pixelization else None for galaxy , image in zip ( self . galaxies , self . image_plane_image_1d_of_galaxies ) ]
9120	def dropbox_fileupload ( dropbox , request ) : attachment = request . POST [ 'attachment' ] attached = dropbox . add_attachment ( attachment ) return dict ( files = [ dict ( name = attached , type = attachment . type , ) ] )
10849	def set_verbosity ( self , verbosity = 'vvv' , handlers = None ) : self . verbosity = sanitize ( verbosity ) self . set_level ( v2l [ verbosity ] , handlers = handlers ) self . set_formatter ( v2f [ verbosity ] , handlers = handlers )
9595	def execute_async_script ( self , script , * args ) : return self . _execute ( Command . EXECUTE_ASYNC_SCRIPT , { 'script' : script , 'args' : list ( args ) } )
7330	def stream_request ( self , method , url , headers = None , _session = None , * args , * * kwargs ) : return StreamResponse ( method = method , url = url , client = self , headers = headers , session = _session , proxy = self . proxy , * * kwargs )
12679	def can_send ( self , user , notice_type ) : from notification . models import NoticeSetting return NoticeSetting . for_user ( user , notice_type , self . medium_id ) . send
12937	def depricated_name ( newmethod ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : warnings . simplefilter ( 'always' , DeprecationWarning ) warnings . warn ( "Function {} is depricated, please use {} instead." . format ( func . __name__ , newmethod ) , category = DeprecationWarning , stacklevel = 2 ) warnings . simplefilter ( 'default' , DeprecationWarning ) return func ( * args , * * kwargs ) return wrapper return decorator
11208	def get_config ( jid ) : acls = getattr ( settings , 'XMPP_HTTP_UPLOAD_ACCESS' , ( ( '.*' , False ) , ) ) for regex , config in acls : if isinstance ( regex , six . string_types ) : regex = [ regex ] for subex in regex : if re . search ( subex , jid ) : return config return False
8214	def gtk_mouse_button_down ( self , widget , event ) : if self . menu_enabled and event . button == 3 : menu = self . uimanager . get_widget ( '/Save as' ) menu . popup ( None , None , None , None , event . button , event . time ) else : super ( ShoebotWindow , self ) . gtk_mouse_button_down ( widget , event )
8109	def search_images ( q , start = 0 , size = "" , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_IMAGES return GoogleSearch ( q , start , service , size , wait , asynchronous , cached )
12296	def post ( repo , args = [ ] ) : mgr = plugins_get_mgr ( ) keys = mgr . search ( what = 'metadata' ) keys = keys [ 'metadata' ] if len ( keys ) == 0 : return # Incorporate pipeline information... if 'pipeline' in repo . options : for name , details in repo . options [ 'pipeline' ] . items ( ) : patterns = details [ 'files' ] matching_files = repo . find_matching_files ( patterns ) matching_files . sort ( ) details [ 'files' ] = matching_files for i , f in enumerate ( matching_files ) : r = repo . get_resource ( f ) if 'pipeline' not in r : r [ 'pipeline' ] = [ ] r [ 'pipeline' ] . append ( name + " [Step {}]" . format ( i ) ) if 'metadata-management' in repo . options : print ( "Collecting all the required metadata to post" ) metadata = repo . options [ 'metadata-management' ] # Add data repo history if 'include-data-history' in metadata and metadata [ 'include-data-history' ] : repo . package [ 'history' ] = get_history ( repo . rootdir ) # Add action history if 'include-action-history' in metadata and metadata [ 'include-action-history' ] : annotate_metadata_action ( repo ) # Add data repo history if 'include-preview' in metadata : annotate_metadata_data ( repo , task = 'preview' , patterns = metadata [ 'include-preview' ] [ 'files' ] , size = metadata [ 'include-preview' ] [ 'length' ] ) if ( ( 'include-schema' in metadata ) and metadata [ 'include-schema' ] ) : annotate_metadata_data ( repo , task = 'schema' ) if 'include-code-history' in metadata : annotate_metadata_code ( repo , files = metadata [ 'include-code-history' ] ) if 'include-platform' in metadata : annotate_metadata_platform ( repo ) if 'include-validation' in metadata : annotate_metadata_validation ( repo ) if 'include-dependencies' in metadata : annotate_metadata_dependencies ( repo ) history = repo . package . get ( 'history' , None ) if ( ( 'include-tab-diffs' in metadata ) and metadata [ 'include-tab-diffs' ] and history is not None ) : annotate_metadata_diffs ( repo ) # Insert options as well repo . package [ 'config' ] = repo . options try : for k in keys : # print("Key", k) metadatamgr = mgr . get_by_key ( 'metadata' , k ) url = metadatamgr . url o = urlparse ( url ) print ( "Posting to " , o . netloc ) response = metadatamgr . post ( repo ) if isinstance ( response , str ) : print ( "Error while posting:" , response ) elif response . status_code in [ 400 ] : content = response . json ( ) print ( "Error while posting:" ) for k in content : print ( " " , k , "- " , "," . join ( content [ k ] ) ) except NetworkError as e : print ( "Unable to reach metadata server!" ) except NetworkInvalidConfiguration as e : print ( "Invalid network configuration in the INI file" ) print ( e . message ) except Exception as e : print ( "Could not post. Unknown error" ) print ( e )
11790	def revise ( csp , Xi , Xj , removals ) : revised = False for x in csp . curr_domains [ Xi ] [ : ] : # If Xi=x conflicts with Xj=y for every possible y, eliminate Xi=x if every ( lambda y : not csp . constraints ( Xi , x , Xj , y ) , csp . curr_domains [ Xj ] ) : csp . prune ( Xi , x , removals ) revised = True return revised
739	def cPrint ( self , level , message , * args , * * kw ) : if level > self . consolePrinterVerbosity : return if len ( kw ) > 1 : raise KeyError ( "Invalid keywords for cPrint: %s" % str ( kw . keys ( ) ) ) newline = kw . get ( "newline" , True ) if len ( kw ) == 1 and 'newline' not in kw : raise KeyError ( "Invalid keyword for cPrint: %s" % kw . keys ( ) [ 0 ] ) if len ( args ) == 0 : if newline : print message else : print message , else : if newline : print message % args else : print message % args ,
7708	def handle_got_features_event ( self , event ) : server_features = set ( ) logger . debug ( "Checking roster-related features" ) if event . features . find ( FEATURE_ROSTERVER ) is not None : logger . debug ( " Roster versioning available" ) server_features . add ( "versioning" ) if event . features . find ( FEATURE_APPROVALS ) is not None : logger . debug ( " Subscription pre-approvals available" ) server_features . add ( "pre-approvals" ) self . server_features = server_features
8877	def allele_expectation ( bgen , variant_idx ) : geno = bgen [ "genotype" ] [ variant_idx ] . compute ( ) if geno [ "phased" ] : raise ValueError ( "Allele expectation is define for unphased genotypes only." ) nalleles = bgen [ "variants" ] . loc [ variant_idx , "nalleles" ] . compute ( ) . item ( ) genotypes = get_genotypes ( geno [ "ploidy" ] , nalleles ) expec = [ ] for i in range ( len ( genotypes ) ) : count = asarray ( genotypes_to_allele_counts ( genotypes [ i ] ) , float ) n = count . shape [ 0 ] expec . append ( ( count . T * geno [ "probs" ] [ i , : n ] ) . sum ( 1 ) ) return stack ( expec , axis = 0 )
8648	def reject_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'reject' , } # POST /api/projects/0.1/milestone_requests/{milestone_request_id}/?action= # reject endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotRejectedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11000	def _tz ( self , z ) : return ( z - self . param_dict [ 'psf-zslab' ] ) * self . param_dict [ self . zscale ]
1685	def RepositoryName ( self ) : fullname = self . FullName ( ) if os . path . exists ( fullname ) : project_dir = os . path . dirname ( fullname ) # If the user specified a repository path, it exists, and the file is # contained in it, use the specified repository path if _repository : repo = FileInfo ( _repository ) . FullName ( ) root_dir = project_dir while os . path . exists ( root_dir ) : # allow case insensitive compare on Windows if os . path . normcase ( root_dir ) == os . path . normcase ( repo ) : return os . path . relpath ( fullname , root_dir ) . replace ( '\\' , '/' ) one_up_dir = os . path . dirname ( root_dir ) if one_up_dir == root_dir : break root_dir = one_up_dir if os . path . exists ( os . path . join ( project_dir , ".svn" ) ) : # If there's a .svn file in the current directory, we recursively look # up the directory tree for the top of the SVN checkout root_dir = project_dir one_up_dir = os . path . dirname ( root_dir ) while os . path . exists ( os . path . join ( one_up_dir , ".svn" ) ) : root_dir = os . path . dirname ( root_dir ) one_up_dir = os . path . dirname ( one_up_dir ) prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] # Not SVN <= 1.6? Try to find a git, hg, or svn top level directory by # searching up from the current path. root_dir = current_dir = os . path . dirname ( fullname ) while current_dir != os . path . dirname ( current_dir ) : if ( os . path . exists ( os . path . join ( current_dir , ".git" ) ) or os . path . exists ( os . path . join ( current_dir , ".hg" ) ) or os . path . exists ( os . path . join ( current_dir , ".svn" ) ) ) : root_dir = current_dir current_dir = os . path . dirname ( current_dir ) if ( os . path . exists ( os . path . join ( root_dir , ".git" ) ) or os . path . exists ( os . path . join ( root_dir , ".hg" ) ) or os . path . exists ( os . path . join ( root_dir , ".svn" ) ) ) : prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] # Don't know what to do; header guard warnings may be wrong... return fullname
379	def featurewise_norm ( x , mean = None , std = None , epsilon = 1e-7 ) : if mean : x = x - mean if std : x = x / ( std + epsilon ) return x
5023	def get_integrated_channels ( self , options ) : channel_classes = self . get_channel_classes ( options . get ( 'channel' ) ) filter_kwargs = { 'active' : True , 'enterprise_customer__active' : True , } enterprise_customer = self . get_enterprise_customer ( options . get ( 'enterprise_customer' ) ) if enterprise_customer : filter_kwargs [ 'enterprise_customer' ] = enterprise_customer for channel_class in channel_classes : for integrated_channel in channel_class . objects . filter ( * * filter_kwargs ) : yield integrated_channel
6291	def add_texture_dir ( self , directory ) : dirs = list ( self . TEXTURE_DIRS ) dirs . append ( directory ) self . TEXTURE_DIRS = dirs
1306	def GetConsoleTitle ( ) -> str : arrayType = ctypes . c_wchar * MAX_PATH values = arrayType ( ) ctypes . windll . kernel32 . GetConsoleTitleW ( values , MAX_PATH ) return values . value
6830	def get_logs_between_commits ( self , a , b ) : print ( 'REAL' ) ret = self . local ( 'git --no-pager log --pretty=oneline %s...%s' % ( a , b ) , capture = True ) if self . verbose : print ( ret ) return str ( ret )
5996	def plot_border ( mask , should_plot_border , units , kpc_per_arcsec , pointsize , zoom_offset_pixels ) : if should_plot_border and mask is not None : plt . gca ( ) border_pixels = mask . masked_grid_index_to_pixel [ mask . border_pixels ] if zoom_offset_pixels is not None : border_pixels -= zoom_offset_pixels border_arcsec = mask . grid_pixels_to_grid_arcsec ( grid_pixels = border_pixels ) border_units = convert_grid_units ( array = mask , grid_arcsec = border_arcsec , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = border_units [ : , 0 ] , x = border_units [ : , 1 ] , s = pointsize , c = 'y' )
10289	def enrich_reactions ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add_has_reactant ( u , v ) for v in u . products : graph . add_has_product ( u , v )
4907	def _sync_content_metadata ( self , serialized_data , http_method ) : try : status_code , response_body = getattr ( self , '_' + http_method ) ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . course_api_path ) , serialized_data , self . CONTENT_PROVIDER_SCOPE ) except requests . exceptions . RequestException as exc : raise ClientError ( 'DegreedAPIClient request failed: {error} {message}' . format ( error = exc . __class__ . __name__ , message = str ( exc ) ) ) if status_code >= 400 : raise ClientError ( 'DegreedAPIClient request failed with status {status_code}: {message}' . format ( status_code = status_code , message = response_body ) )
7350	def predict ( self , sequences ) : with tempfile . NamedTemporaryFile ( suffix = ".fsa" , mode = "w" ) as input_fd : for ( i , sequence ) in enumerate ( sequences ) : input_fd . write ( "> %d\n" % i ) input_fd . write ( sequence ) input_fd . write ( "\n" ) input_fd . flush ( ) try : output = subprocess . check_output ( [ "netChop" , input_fd . name ] ) except subprocess . CalledProcessError as e : logging . error ( "Error calling netChop: %s:\n%s" % ( e , e . output ) ) raise parsed = self . parse_netchop ( output ) assert len ( parsed ) == len ( sequences ) , "Expected %d results but got %d" % ( len ( sequences ) , len ( parsed ) ) assert [ len ( x ) for x in parsed ] == [ len ( x ) for x in sequences ] return parsed
6139	def get_is_sim_running ( self ) : sim_info = self . simulation_info ( ) try : progress_info = sim_info [ 'simulation_info_progress' ] ret = progress_info [ 'simulation_progress_is_running' ] except KeyError : # Simulation has not been created. ret = False return ret
773	def generateStats ( filename , maxSamples = None , ) : # Mapping from field type to stats collector object statsCollectorMapping = { 'float' : FloatStatsCollector , 'int' : IntStatsCollector , 'string' : StringStatsCollector , 'datetime' : DateTimeStatsCollector , 'bool' : BoolStatsCollector , } filename = resource_filename ( "nupic.datafiles" , filename ) print "*" * 40 print "Collecting statistics for file:'%s'" % ( filename , ) dataFile = FileRecordStream ( filename ) # Initialize collector objects # statsCollectors list holds statsCollector objects for each field statsCollectors = [ ] for fieldName , fieldType , fieldSpecial in dataFile . getFields ( ) : # Find the corresponding stats collector for each field based on field type # and intialize an instance statsCollector = statsCollectorMapping [ fieldType ] ( fieldName , fieldType , fieldSpecial ) statsCollectors . append ( statsCollector ) # Now collect the stats if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : record = dataFile . getNextRecord ( ) if record is None : break for i , value in enumerate ( record ) : statsCollectors [ i ] . addValue ( value ) # stats dict holds the statistics for each field stats = { } for statsCollector in statsCollectors : statsCollector . getStats ( stats ) # We don't want to include reset field in permutations # TODO: handle reset field in a clean way if dataFile . getResetFieldIdx ( ) is not None : resetFieldName , _ , _ = dataFile . getFields ( ) [ dataFile . reset ] stats . pop ( resetFieldName ) if VERBOSITY > 0 : pprint . pprint ( stats ) return stats
9649	def determine_paths ( self , package_name = None , create_package_dir = False , dry_run = False ) : # Give preference to the environment variable here as it will not # derefrence sym links self . project_dir = Path ( os . getenv ( 'PWD' ) or os . getcwd ( ) ) # Try and work out the project name distribution = self . get_distribution ( ) if distribution : # Get name from setup.py self . project_name = distribution . get_name ( ) else : # ...failing that, use the current directory name self . project_name = self . project_dir . name # Descend into the 'src' directory to find the package # if necessary if os . path . isdir ( self . project_dir / "src" ) : package_search_dir = self . project_dir / "src" else : package_search_dir = self . project_dir created_package_dir = False if not package_name : # Lets try and work out the package_name from the project_name package_name = self . project_name . replace ( "-" , "_" ) # Now do some fuzzy matching def get_matches ( name ) : possibles = [ n for n in os . listdir ( package_search_dir ) if os . path . isdir ( package_search_dir / n ) ] return difflib . get_close_matches ( name , possibles , n = 1 , cutoff = 0.8 ) close = get_matches ( package_name ) # If no matches, try removing the first part of the package name # (e.g. django-guardian becomes guardian) if not close and "_" in package_name : short_package_name = "_" . join ( package_name . split ( "_" ) [ 1 : ] ) close = get_matches ( short_package_name ) if not close : if create_package_dir : package_dir = package_search_dir / package_name # Gets set to true even during dry run created_package_dir = True if not dry_run : print ( "Creating package directory at %s" % package_dir ) os . mkdir ( package_dir ) else : print ( "Would have created package directory at %s" % package_dir ) else : raise CommandError ( "Could not guess the package name. Specify it using --name." ) else : package_name = close [ 0 ] self . package_name = package_name self . package_dir = package_search_dir / package_name if not os . path . exists ( self . package_dir ) and not created_package_dir : raise CommandError ( "Package directory did not exist at %s. Perhaps specify it using --name" % self . package_dir )
2534	def validate_str_fields ( self , fields , optional , messages ) : for field_str in fields : field = getattr ( self , field_str ) if field is not None : # FIXME: this does not make sense??? attr = getattr ( field , '__str__' , None ) if not callable ( attr ) : messages = messages + [ '{0} must provide __str__ method.' . format ( field ) ] # Continue checking. elif not optional : messages = messages + [ 'Package {0} can not be None.' . format ( field_str ) ] return messages
5138	def process_file ( self , file ) : if sys . version_info [ 0 ] >= 3 : nxt = file . __next__ else : nxt = file . next for token in tokenize . generate_tokens ( nxt ) : self . process_token ( * token ) self . make_index ( )
11809	def index_collection ( self , filenames ) : for filename in filenames : self . index_document ( open ( filename ) . read ( ) , filename )
8628	def create_project ( session , title , description , currency , budget , jobs ) : project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs } # POST /api/projects/0.1/projects/ response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
207	def draw ( self , size = None , cmap = "jet" ) : heatmaps_uint8 = self . to_uint8 ( ) heatmaps_drawn = [ ] for c in sm . xrange ( heatmaps_uint8 . shape [ 2 ] ) : # c:c+1 here, because the additional axis is needed by imresize_single_image heatmap_c = heatmaps_uint8 [ ... , c : c + 1 ] if size is not None : heatmap_c_rs = ia . imresize_single_image ( heatmap_c , size , interpolation = "nearest" ) else : heatmap_c_rs = heatmap_c heatmap_c_rs = np . squeeze ( heatmap_c_rs ) . astype ( np . float32 ) / 255.0 if cmap is not None : # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225) import matplotlib . pyplot as plt cmap_func = plt . get_cmap ( cmap ) heatmap_cmapped = cmap_func ( heatmap_c_rs ) heatmap_cmapped = np . delete ( heatmap_cmapped , 3 , 2 ) else : heatmap_cmapped = np . tile ( heatmap_c_rs [ ... , np . newaxis ] , ( 1 , 1 , 3 ) ) heatmap_cmapped = np . clip ( heatmap_cmapped * 255 , 0 , 255 ) . astype ( np . uint8 ) heatmaps_drawn . append ( heatmap_cmapped ) return heatmaps_drawn
11910	def bump_version ( version , which = None ) : try : parts = [ int ( n ) for n in version . split ( '.' ) ] except ValueError : fail ( 'Current version is not numeric' ) if len ( parts ) != 3 : fail ( 'Current version is not semantic versioning' ) # Determine where to increment the version number PARTS = { 'major' : 0 , 'minor' : 1 , 'patch' : 2 } index = PARTS [ which ] if which in PARTS else 2 # Increment the version number at that index and set the subsequent parts # to 0. before , middle , after = parts [ : index ] , parts [ index ] , parts [ index + 1 : ] middle += 1 return '.' . join ( str ( n ) for n in before + [ middle ] + after )
931	def next ( self , record , curInputBookmark ) : # This will hold the aggregated record we return outRecord = None # This will hold the bookmark of the last input used within the # aggregated record we return. retInputBookmark = None if record is not None : # Increment input count self . _inIdx += 1 #print self._inIdx, record # Apply the filter, ignore the record if any field is unacceptable if self . _filter != None and not self . _filter [ 0 ] ( self . _filter [ 1 ] , record ) : return ( None , None ) # If no aggregation info just return as-is if self . _nullAggregation : return ( record , curInputBookmark ) # ---------------------------------------------------------------------- # Do aggregation # # Remember the very first record time stamp - it will be used as # the timestamp for all first records in all sequences to align # times for the aggregation/join of sequences. # # For a set of aggregated records, it will use the beginning of the time # window as a timestamp for the set # t = record [ self . _timeFieldIdx ] if self . _firstSequenceStartTime == None : self . _firstSequenceStartTime = t # Create initial startTime and endTime if needed if self . _startTime is None : self . _startTime = t if self . _endTime is None : self . _endTime = self . _getEndTime ( t ) assert self . _endTime > t #print 'Processing line:', i, t, endTime #from dbgp.client import brk; brk(port=9011) # ---------------------------------------------------------------------- # Does this record have a reset signal or sequence Id associated with it? # If so, see if we've reached a sequence boundary if self . _resetFieldIdx is not None : resetSignal = record [ self . _resetFieldIdx ] else : resetSignal = None if self . _sequenceIdFieldIdx is not None : currSequenceId = record [ self . _sequenceIdFieldIdx ] else : currSequenceId = None newSequence = ( resetSignal == 1 and self . _inIdx > 0 ) or self . _sequenceId != currSequenceId or self . _inIdx == 0 if newSequence : self . _sequenceId = currSequenceId # -------------------------------------------------------------------- # We end the aggregation chunk if we go past the end time # -OR- we get an out of order record (t < startTime) sliceEnded = ( t >= self . _endTime or t < self . _startTime ) # ------------------------------------------------------------------- # Time to generate a new output record? if ( newSequence or sliceEnded ) and len ( self . _slice ) > 0 : # Create aggregated record # print 'Creating aggregate record...' # Make first record timestamp as the beginning of the time period, # in case the first record wasn't falling on the beginning of the period for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break # Generate the aggregated record outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark # Reset the slice self . _slice = defaultdict ( list ) # -------------------------------------------------------------------- # Add current record to slice (Note keeping slices in memory). Each # field in the slice is a list of field values from all the sliced # records for j , f in enumerate ( self . _fields ) : index = f [ 0 ] # append the parsed field value to the proper aggregated slice field. self . _slice [ j ] . append ( record [ index ] ) self . _aggrInputBookmark = curInputBookmark # -------------------------------------------------------------------- # If we've encountered a new sequence, start aggregation over again if newSequence : # TODO: May use self._firstSequenceStartTime as a start for the new # sequence (to align all sequences) self . _startTime = t self . _endTime = self . _getEndTime ( t ) # -------------------------------------------------------------------- # If a slice just ended, re-compute the start and end time for the # next aggregated record if sliceEnded : # Did we receive an out of order record? If so, go back and iterate # till we get to the next end time boundary. if t < self . _startTime : self . _endTime = self . _firstSequenceStartTime while t >= self . _endTime : self . _startTime = self . _endTime self . _endTime = self . _getEndTime ( self . _endTime ) # If we have a record to return, do it now if outRecord is not None : return ( outRecord , retInputBookmark ) # --------------------------------------------------------------------- # Input reached EOF # Aggregate one last time in the end if necessary elif self . _slice : # Make first record timestamp as the beginning of the time period, # in case the first record wasn't falling on the beginning of the period for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) # Return aggregated record return ( outRecord , retInputBookmark )
7948	def send_stream_head ( self , stanza_namespace , stream_from , stream_to , stream_id = None , version = u'1.0' , language = None ) : # pylint: disable=R0913 with self . lock : self . _serializer = XMPPSerializer ( stanza_namespace , self . settings [ "extra_ns_prefixes" ] ) head = self . _serializer . emit_head ( stream_from , stream_to , stream_id , version , language ) self . _write ( head . encode ( "utf-8" ) )
10988	def _translate_particles ( s , max_mem = 1e9 , desc = '' , min_rad = 'calc' , max_rad = 'calc' , invert = 'guess' , rz_order = 0 , do_polish = True ) : if desc is not None : desc_trans = desc + 'translate-particles' desc_burn = desc + 'addsub_burn' desc_polish = desc + 'addsub_polish' else : desc_trans , desc_burn , desc_polish = [ None ] * 3 RLOG . info ( 'Translate Particles:' ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.1 , desc = desc_trans , max_mem = max_mem , include_rad = False , dowarn = False ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.05 , desc = desc_trans , max_mem = max_mem , include_rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) addsub . add_subtract ( s , tries = 30 , min_rad = min_rad , max_rad = max_rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'translate-addsub' ) if do_polish : RLOG . info ( 'Final Burn:' ) opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 3e-4 , desc = desc_burn , max_mem = max_mem , rz_order = rz_order , dowarn = False ) RLOG . info ( 'Final Polish:' ) d = opt . burn ( s , mode = 'polish' , n_loop = 4 , fractol = 3e-4 , desc = desc_polish , max_mem = max_mem , rz_order = rz_order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
7692	def _handle_auth_success ( self , stream , success ) : if not self . _check_authorization ( success . properties , stream ) : element = ElementTree . Element ( FAILURE_TAG ) ElementTree . SubElement ( element , SASL_QNP + "invalid-authzid" ) return True authzid = success . properties . get ( "authzid" ) if authzid : peer = JID ( success . authzid ) elif "username" in success . properties : peer = JID ( success . properties [ "username" ] , stream . me . domain ) else : # anonymous peer = None stream . set_peer_authenticated ( peer , True )
5417	def _format_task_uri ( fmt , job_metadata , task_metadata ) : values = { 'job-id' : None , 'task-id' : 'task' , 'job-name' : None , 'user-id' : None , 'task-attempt' : None } for key in values : values [ key ] = task_metadata . get ( key ) or job_metadata . get ( key ) or values [ key ] return fmt . format ( * * values )
8829	def security_group_rule_update ( context , rule , * * kwargs ) : rule . update ( kwargs ) context . session . add ( rule ) return rule
5989	def grid_stack_from_deflection_stack ( grid_stack , deflection_stack ) : if deflection_stack is not None : def minus ( grid , deflections ) : return grid - deflections return grid_stack . map_function ( minus , deflection_stack )
7804	def verify_server ( self , server_name , srv_type = 'xmpp-client' ) : server_jid = JID ( server_name ) if "XmppAddr" not in self . alt_names and "DNS" not in self . alt_names and "SRV" not in self . alt_names : return self . verify_jid_against_common_name ( server_jid ) names = [ name for name in self . alt_names . get ( "DNS" , [ ] ) if not name . startswith ( u"*." ) ] names += self . alt_names . get ( "XmppAddr" , [ ] ) for name in names : logger . debug ( "checking {0!r} against {1!r}" . format ( server_jid , name ) ) try : jid = JID ( name ) except ValueError : logger . debug ( "Not a valid JID: {0!r}" . format ( name ) ) continue if jid == server_jid : logger . debug ( "Match!" ) return True if srv_type and self . verify_jid_against_srv_name ( server_jid , srv_type ) : return True wildcards = [ name [ 2 : ] for name in self . alt_names . get ( "DNS" , [ ] ) if name . startswith ( "*." ) ] if not wildcards or not "." in server_jid . domain : return False logger . debug ( "checking {0!r} against wildcard domains: {1!r}" . format ( server_jid , wildcards ) ) server_domain = JID ( domain = server_jid . domain . split ( "." , 1 ) [ 1 ] ) for domain in wildcards : logger . debug ( "checking {0!r} against {1!r}" . format ( server_domain , domain ) ) try : jid = JID ( domain ) except ValueError : logger . debug ( "Not a valid JID: {0!r}" . format ( name ) ) continue if jid == server_domain : logger . debug ( "Match!" ) return True return False
7346	async def get_oauth_token ( consumer_key , consumer_secret , callback_uri = "oob" ) : client = BasePeonyClient ( consumer_key = consumer_key , consumer_secret = consumer_secret , api_version = "" , suffix = "" ) response = await client . api . oauth . request_token . post ( _suffix = "" , oauth_callback = callback_uri ) return parse_token ( response )
10859	def update ( self , params , values ) : #1. Figure out if we're going to do a global update, in which # case we just draw from scratch. global_update , particles = self . _update_type ( params ) # if we are doing a global update, everything must change, so # starting fresh will be faster instead of add subtract if global_update : self . set_values ( params , values ) self . initialize ( ) return # otherwise, update individual particles. delete the current versions # of the particles update the particles, and redraw them anew at the # places given by (params, values) oldargs = self . _drawargs ( ) for n in particles : self . _draw_particle ( self . pos [ n ] , * listify ( oldargs [ n ] ) , sign = - 1 ) self . set_values ( params , values ) newargs = self . _drawargs ( ) for n in particles : self . _draw_particle ( self . pos [ n ] , * listify ( newargs [ n ] ) , sign = + 1 )
881	def compute ( self , activeColumns , learn = True ) : self . activateCells ( sorted ( activeColumns ) , learn ) self . activateDendrites ( learn )
11956	def is_bin ( ip ) : try : ip = str ( ip ) if len ( ip ) != 32 : return False dec = int ( ip , 2 ) except ( TypeError , ValueError ) : return False if dec > 4294967295 or dec < 0 : return False return True
3450	def flux_variability_analysis ( model , reaction_list = None , loopless = False , fraction_of_optimum = 1.0 , pfba_factor = None , processes = None ) : if reaction_list is None : reaction_ids = [ r . id for r in model . reactions ] else : reaction_ids = [ r . id for r in model . reactions . get_by_any ( reaction_list ) ] if processes is None : processes = CONFIGURATION . processes num_reactions = len ( reaction_ids ) processes = min ( processes , num_reactions ) fva_result = DataFrame ( { "minimum" : zeros ( num_reactions , dtype = float ) , "maximum" : zeros ( num_reactions , dtype = float ) } , index = reaction_ids ) prob = model . problem with model : # Safety check before setting up FVA. model . slim_optimize ( error_value = None , message = "There is no optimal solution for the " "chosen objective!" ) # Add the previous objective as a variable to the model then set it to # zero. This also uses the fraction to create the lower/upper bound for # the old objective. # TODO: Use utility function here (fix_objective_as_constraint)? if model . solver . objective . direction == "max" : fva_old_objective = prob . Variable ( "fva_old_objective" , lb = fraction_of_optimum * model . solver . objective . value ) else : fva_old_objective = prob . Variable ( "fva_old_objective" , ub = fraction_of_optimum * model . solver . objective . value ) fva_old_obj_constraint = prob . Constraint ( model . solver . objective . expression - fva_old_objective , lb = 0 , ub = 0 , name = "fva_old_objective_constraint" ) model . add_cons_vars ( [ fva_old_objective , fva_old_obj_constraint ] ) if pfba_factor is not None : if pfba_factor < 1. : warn ( "The 'pfba_factor' should be larger or equal to 1." , UserWarning ) with model : add_pfba ( model , fraction_of_optimum = 0 ) ub = model . slim_optimize ( error_value = None ) flux_sum = prob . Variable ( "flux_sum" , ub = pfba_factor * ub ) flux_sum_constraint = prob . Constraint ( model . solver . objective . expression - flux_sum , lb = 0 , ub = 0 , name = "flux_sum_constraint" ) model . add_cons_vars ( [ flux_sum , flux_sum_constraint ] ) model . objective = Zero # This will trigger the reset as well for what in ( "minimum" , "maximum" ) : if processes > 1 : # We create and destroy a new pool here in order to set the # objective direction for all reactions. This creates a # slight overhead but seems the most clean. chunk_size = len ( reaction_ids ) // processes pool = multiprocessing . Pool ( processes , initializer = _init_worker , initargs = ( model , loopless , what [ : 3 ] ) ) for rxn_id , value in pool . imap_unordered ( _fva_step , reaction_ids , chunksize = chunk_size ) : fva_result . at [ rxn_id , what ] = value pool . close ( ) pool . join ( ) else : _init_worker ( model , loopless , what [ : 3 ] ) for rxn_id , value in map ( _fva_step , reaction_ids ) : fva_result . at [ rxn_id , what ] = value return fva_result [ [ "minimum" , "maximum" ] ]
4521	def get ( self , ring , angle ) : pixel = self . angleToPixel ( angle , ring ) return self . _get_base ( pixel )
3377	def check_solver_status ( status , raise_error = False ) : if status == OPTIMAL : return elif ( status in has_primals ) and not raise_error : warn ( "solver status is '{}'" . format ( status ) , UserWarning ) elif status is None : raise OptimizationError ( "model was not optimized yet or solver context switched" ) else : raise OptimizationError ( "solver status is '{}'" . format ( status ) )
5353	def retain_identities ( self , retention_time ) : enrich_es = self . conf [ 'es_enrichment' ] [ 'url' ] sortinghat_db = self . db current_data_source = self . get_backend ( self . backend_section ) active_data_sources = self . config . get_active_data_sources ( ) if retention_time is None : logger . debug ( "[identities retention] Retention policy disabled, no identities will be deleted." ) return if retention_time <= 0 : logger . debug ( "[identities retention] Retention time must be greater than 0." ) return retain_identities ( retention_time , enrich_es , sortinghat_db , current_data_source , active_data_sources )
5408	def _operation_status_message ( self ) : msg = None action = None if not google_v2_operations . is_done ( self . _op ) : last_event = google_v2_operations . get_last_event ( self . _op ) if last_event : msg = last_event [ 'description' ] action_id = last_event . get ( 'details' , { } ) . get ( 'actionId' ) if action_id : action = google_v2_operations . get_action_by_id ( self . _op , action_id ) else : msg = 'Pending' else : failed_events = google_v2_operations . get_failed_events ( self . _op ) if failed_events : failed_event = failed_events [ - 1 ] msg = failed_event . get ( 'details' , { } ) . get ( 'stderr' ) action_id = failed_event . get ( 'details' , { } ) . get ( 'actionId' ) if action_id : action = google_v2_operations . get_action_by_id ( self . _op , action_id ) if not msg : error = google_v2_operations . get_error ( self . _op ) if error : msg = error [ 'message' ] else : msg = 'Success' return msg , action
9664	def construct_graph ( sakefile , settings ) : verbose = settings [ "verbose" ] sprint = settings [ "sprint" ] G = nx . DiGraph ( ) sprint ( "Going to construct Graph" , level = "verbose" ) for target in sakefile : if target == "all" : # we don't want this node continue if "formula" not in sakefile [ target ] : # that means this is a meta target for atomtarget in sakefile [ target ] : if atomtarget == "help" : continue sprint ( "Adding '{}'" . format ( atomtarget ) , level = "verbose" ) data_dict = sakefile [ target ] [ atomtarget ] data_dict [ "parent" ] = target G . add_node ( atomtarget , * * data_dict ) else : sprint ( "Adding '{}'" . format ( target ) , level = "verbose" ) G . add_node ( target , * * sakefile [ target ] ) sprint ( "Nodes are built\nBuilding connections" , level = "verbose" ) for node in G . nodes ( data = True ) : sprint ( "checking node {} for dependencies" . format ( node [ 0 ] ) , level = "verbose" ) # normalize all paths in output for k , v in node [ 1 ] . items ( ) : if v is None : node [ 1 ] [ k ] = [ ] if "output" in node [ 1 ] : for index , out in enumerate ( node [ 1 ] [ 'output' ] ) : node [ 1 ] [ 'output' ] [ index ] = clean_path ( node [ 1 ] [ 'output' ] [ index ] ) if "dependencies" not in node [ 1 ] : continue sprint ( "it has dependencies" , level = "verbose" ) connects = [ ] # normalize all paths in dependencies for index , dep in enumerate ( node [ 1 ] [ 'dependencies' ] ) : dep = os . path . normpath ( dep ) shrt = "dependencies" node [ 1 ] [ 'dependencies' ] [ index ] = clean_path ( node [ 1 ] [ shrt ] [ index ] ) for node in G . nodes ( data = True ) : connects = [ ] if "dependencies" not in node [ 1 ] : continue for dep in node [ 1 ] [ 'dependencies' ] : matches = check_for_dep_in_outputs ( dep , verbose , G ) if not matches : continue for match in matches : sprint ( "Appending {} to matches" . format ( match ) , level = "verbose" ) connects . append ( match ) if connects : for connect in connects : G . add_edge ( connect , node [ 0 ] ) return G
13543	def from_server ( cls , server , slug , identifier ) : task = server . get ( 'task' , replacements = { 'slug' : slug , 'identifier' : identifier } ) return cls ( * * task )
3382	def step ( sampler , x , delta , fraction = None , tries = 0 ) : prob = sampler . problem valid = ( ( np . abs ( delta ) > sampler . feasibility_tol ) & np . logical_not ( prob . variable_fixed ) ) # permissible alphas for staying in variable bounds valphas = ( ( 1.0 - sampler . bounds_tol ) * prob . variable_bounds - x ) [ : , valid ] valphas = ( valphas / delta [ valid ] ) . flatten ( ) if prob . bounds . shape [ 0 ] > 0 : # permissible alphas for staying in constraint bounds ineqs = prob . inequalities . dot ( delta ) valid = np . abs ( ineqs ) > sampler . feasibility_tol balphas = ( ( 1.0 - sampler . bounds_tol ) * prob . bounds - prob . inequalities . dot ( x ) ) [ : , valid ] balphas = ( balphas / ineqs [ valid ] ) . flatten ( ) # combined alphas alphas = np . hstack ( [ valphas , balphas ] ) else : alphas = valphas pos_alphas = alphas [ alphas > 0.0 ] neg_alphas = alphas [ alphas <= 0.0 ] alpha_range = np . array ( [ neg_alphas . max ( ) if len ( neg_alphas ) > 0 else 0 , pos_alphas . min ( ) if len ( pos_alphas ) > 0 else 0 ] ) if fraction : alpha = alpha_range [ 0 ] + fraction * ( alpha_range [ 1 ] - alpha_range [ 0 ] ) else : alpha = np . random . uniform ( alpha_range [ 0 ] , alpha_range [ 1 ] ) p = x + alpha * delta # Numerical instabilities may cause bounds invalidation # reset sampler and sample from one of the original warmup directions # if that occurs. Also reset if we got stuck. if ( np . any ( sampler . _bounds_dist ( p ) < - sampler . bounds_tol ) or np . abs ( np . abs ( alpha_range ) . max ( ) * delta ) . max ( ) < sampler . bounds_tol ) : if tries > MAX_TRIES : raise RuntimeError ( "Can not escape sampling region, model seems" " numerically unstable :( Reporting the " "model to " "https://github.com/opencobra/cobrapy/issues " "will help us to fix this :)" ) LOGGER . info ( "found bounds infeasibility in sample, " "resetting to center" ) newdir = sampler . warmup [ np . random . randint ( sampler . n_warmup ) ] sampler . retries += 1 return step ( sampler , sampler . center , newdir - sampler . center , None , tries + 1 ) return p
11994	def set_algorithms ( self , signature = None , encryption = None , serialization = None , compression = None ) : self . signature_algorithms = self . _update_dict ( signature , self . DEFAULT_SIGNATURE ) self . encryption_algorithms = self . _update_dict ( encryption , self . DEFAULT_ENCRYPTION ) self . serialization_algorithms = self . _update_dict ( serialization , self . DEFAULT_SERIALIZATION ) self . compression_algorithms = self . _update_dict ( compression , self . DEFAULT_COMPRESSION )
3680	def T_converter ( T , current , desired ) : def range_check ( T , Tmin , Tmax ) : if T < Tmin or T > Tmax : raise Exception ( 'Temperature conversion is outside one or both scales' ) try : if current == 'ITS-90' : pass elif current == 'ITS-68' : range_check ( T , 13.999 , 4300.0001 ) T = T68_to_T90 ( T ) elif current == 'ITS-76' : range_check ( T , 4.9999 , 27.0001 ) T = T76_to_T90 ( T ) elif current == 'ITS-48' : range_check ( T , 93.149999 , 4273.15001 ) T = T48_to_T90 ( T ) elif current == 'ITS-27' : range_check ( T , 903.15 , 4273.15 ) T = T27_to_T90 ( T ) else : raise Exception ( 'Current scale not supported' ) # T should be in ITS-90 now if desired == 'ITS-90' : pass elif desired == 'ITS-68' : range_check ( T , 13.999 , 4300.0001 ) T = T90_to_T68 ( T ) elif desired == 'ITS-76' : range_check ( T , 4.9999 , 27.0001 ) T = T90_to_T76 ( T ) elif desired == 'ITS-48' : range_check ( T , 93.149999 , 4273.15001 ) T = T90_to_T48 ( T ) elif desired == 'ITS-27' : range_check ( T , 903.15 , 4273.15 ) T = T90_to_T27 ( T ) else : raise Exception ( 'Desired scale not supported' ) except ValueError : raise Exception ( 'Temperature could not be converted to desired scale' ) return float ( T )
6535	def merge_dict ( dict1 , dict2 , merge_lists = False ) : merged = dict ( dict1 ) for key , value in iteritems ( dict2 ) : if isinstance ( merged . get ( key ) , dict ) : merged [ key ] = merge_dict ( merged [ key ] , value ) elif merge_lists and isinstance ( merged . get ( key ) , list ) : merged [ key ] = merge_list ( merged [ key ] , value ) else : merged [ key ] = value return merged
188	def clip_out_of_image ( self ) : lss_cut = [ ls_clipped for ls in self . line_strings for ls_clipped in ls . clip_out_of_image ( self . shape ) ] return LineStringsOnImage ( lss_cut , shape = self . shape )
290	def plot_rolling_beta ( returns , factor_returns , legend_loc = 'best' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) ax . set_title ( "Rolling portfolio beta to " + str ( factor_returns . name ) ) ax . set_ylabel ( 'Beta' ) rb_1 = timeseries . rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 6 ) rb_1 . plot ( color = 'steelblue' , lw = 3 , alpha = 0.6 , ax = ax , * * kwargs ) rb_2 = timeseries . rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 12 ) rb_2 . plot ( color = 'grey' , lw = 3 , alpha = 0.4 , ax = ax , * * kwargs ) ax . axhline ( rb_1 . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 2 ) ax . set_xlabel ( '' ) ax . legend ( [ '6-mo' , '12-mo' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . set_ylim ( ( - 1.0 , 1.0 ) ) return ax
3878	async def _handle_conversation_delta ( self , conversation ) : conv_id = conversation . conversation_id . id conv = self . _conv_dict . get ( conv_id , None ) if conv is None : # Ignore the delta and fetch the complete conversation. await self . _get_or_fetch_conversation ( conv_id ) else : # Update conversation using the delta. conv . update_conversation ( conversation )
13844	def process_macros ( self , content : str ) -> str : def _sub ( macro ) : name = macro . group ( 'body' ) params = self . get_options ( macro . group ( 'options' ) ) return self . options [ 'macros' ] . get ( name , '' ) . format_map ( params ) return self . pattern . sub ( _sub , content )
1697	def reduce_by_window ( self , window_config , reduce_function ) : from heronpy . streamlet . impl . reducebywindowbolt import ReduceByWindowStreamlet reduce_streamlet = ReduceByWindowStreamlet ( window_config , reduce_function , self ) self . _add_child ( reduce_streamlet ) return reduce_streamlet
13257	def save ( self , entry , with_location = True , debug = False ) : entry_dict = { } if isinstance ( entry , DayOneEntry ) : # Get a dict of the DayOneEntry entry_dict = entry . as_dict ( ) else : entry_dict = entry # Set the UUID entry_dict [ 'UUID' ] = uuid . uuid4 ( ) . get_hex ( ) if with_location and not entry_dict [ 'Location' ] : entry_dict [ 'Location' ] = self . get_location ( ) # Do we have everything needed? if not all ( ( entry_dict [ 'UUID' ] , entry_dict [ 'Time Zone' ] , entry_dict [ 'Entry Text' ] ) ) : print "You must provide: Time zone, UUID, Creation Date, Entry Text" return False if debug is False : file_path = self . _file_path ( entry_dict [ 'UUID' ] ) plistlib . writePlist ( entry_dict , file_path ) else : plist = plistlib . writePlistToString ( entry_dict ) print plist return True
9084	def upload_backend ( index = 'dev' , user = None ) : get_vars ( ) use_devpi ( index = index ) with fab . lcd ( '../application' ) : fab . local ( 'make upload' )
5262	def camelcase ( string ) : string = re . sub ( r"^[\-_\.]" , '' , str ( string ) ) if not string : return string return lowercase ( string [ 0 ] ) + re . sub ( r"[\-_\.\s]([a-z])" , lambda matched : uppercase ( matched . group ( 1 ) ) , string [ 1 : ] )
6430	def sim ( self , src , tar ) : def _lcsstr_stl ( src , tar ) : """Return start positions & length for Ratcliff-Obershelp. Parameters ---------- src : str Source string for comparison tar : str Target string for comparison Returns ------- tuple The start position in the source string, start position in the target string, and length of the longest common substring of strings src and tar. """ lengths = np_zeros ( ( len ( src ) + 1 , len ( tar ) + 1 ) , dtype = np_int ) longest , src_longest , tar_longest = 0 , 0 , 0 for i in range ( 1 , len ( src ) + 1 ) : for j in range ( 1 , len ( tar ) + 1 ) : if src [ i - 1 ] == tar [ j - 1 ] : lengths [ i , j ] = lengths [ i - 1 , j - 1 ] + 1 if lengths [ i , j ] > longest : longest = lengths [ i , j ] src_longest = i tar_longest = j else : lengths [ i , j ] = 0 return src_longest - longest , tar_longest - longest , longest def _sstr_matches ( src , tar ) : """Return the sum of substring match lengths. This follows the Ratcliff-Obershelp algorithm :cite:`Ratcliff:1988`: 1. Find the length of the longest common substring in src & tar. 2. Recurse on the strings to the left & right of each this substring in src & tar. 3. Base case is a 0 length common substring, in which case, return 0. 4. Return the sum. Parameters ---------- src : str Source string for comparison tar : str Target string for comparison Returns ------- int Sum of substring match lengths """ src_start , tar_start , length = _lcsstr_stl ( src , tar ) if length == 0 : return 0 return ( _sstr_matches ( src [ : src_start ] , tar [ : tar_start ] ) + length + _sstr_matches ( src [ src_start + length : ] , tar [ tar_start + length : ] ) ) if src == tar : return 1.0 elif not src or not tar : return 0.0 return 2 * _sstr_matches ( src , tar ) / ( len ( src ) + len ( tar ) )
11837	def actions ( self , state ) : if state [ - 1 ] is not None : return [ ] # All columns filled; no successors else : col = state . index ( None ) return [ row for row in range ( self . N ) if not self . conflicted ( state , row , col ) ]
3395	def remove_genes ( cobra_model , gene_list , remove_reactions = True ) : gene_set = { cobra_model . genes . get_by_id ( str ( i ) ) for i in gene_list } gene_id_set = { i . id for i in gene_set } remover = _GeneRemover ( gene_id_set ) ast_rules = get_compiled_gene_reaction_rules ( cobra_model ) target_reactions = [ ] for reaction , rule in iteritems ( ast_rules ) : if reaction . gene_reaction_rule is None or len ( reaction . gene_reaction_rule ) == 0 : continue # reactions to remove if remove_reactions and not eval_gpr ( rule , gene_id_set ) : target_reactions . append ( reaction ) else : # if the reaction is not removed, remove the gene # from its gpr remover . visit ( rule ) new_rule = ast2str ( rule ) if new_rule != reaction . gene_reaction_rule : reaction . gene_reaction_rule = new_rule for gene in gene_set : cobra_model . genes . remove ( gene ) # remove reference to the gene in all groups associated_groups = cobra_model . get_associated_groups ( gene ) for group in associated_groups : group . remove_members ( gene ) cobra_model . remove_reactions ( target_reactions )
13415	def addlabel ( ax = None , toplabel = None , xlabel = None , ylabel = None , zlabel = None , clabel = None , cb = None , windowlabel = None , fig = None , axes = None ) : if ( axes is None ) and ( ax is not None ) : axes = ax if ( windowlabel is not None ) and ( fig is not None ) : fig . canvas . set_window_title ( windowlabel ) if fig is None : fig = _plt . gcf ( ) if fig is not None and axes is None : axes = fig . get_axes ( ) if axes == [ ] : logger . error ( 'No axes found!' ) if axes is not None : if toplabel is not None : axes . set_title ( toplabel ) if xlabel is not None : axes . set_xlabel ( xlabel ) if ylabel is not None : axes . set_ylabel ( ylabel ) if zlabel is not None : axes . set_zlabel ( zlabel ) if ( clabel is not None ) or ( cb is not None ) : if ( clabel is not None ) and ( cb is not None ) : cb . set_label ( clabel ) else : if clabel is None : logger . error ( 'Missing colorbar label' ) else : logger . error ( 'Missing colorbar instance' )
7338	def get_args ( func , skip = 0 ) : code = getattr ( func , '__code__' , None ) if code is None : code = func . __call__ . __code__ return code . co_varnames [ skip : code . co_argcount ]
10680	def Cp_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : c = ( self . _B_mag * ( 2 * tau ** 3 + 2 * tau ** 9 / 3 + 2 * tau ** 15 / 5 ) ) / self . _D_mag else : c = ( 2 * tau ** - 5 + 2 * tau ** - 15 / 3 + 2 * tau ** - 25 / 5 ) / self . _D_mag result = R * math . log ( self . beta0_mag + 1 ) * c return result
10554	def get_helping_materials ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'helpingmaterial' , params = params ) if type ( res ) . __name__ == 'list' : return [ HelpingMaterial ( helping ) for helping in res ] else : return res except : # pragma: no cover raise
12330	def run ( cmd ) : cmd = [ pipes . quote ( c ) for c in cmd ] cmd = " " . join ( cmd ) cmd += "; exit 0" # print("Running {} in {}".format(cmd, os.getcwd())) try : output = subprocess . check_output ( cmd , stderr = subprocess . STDOUT , shell = True ) except subprocess . CalledProcessError as e : output = e . output output = output . decode ( 'utf-8' ) output = output . strip ( ) return output
3139	def create ( self , store_id , data ) : self . store_id = store_id if 'id' not in data : raise KeyError ( 'The promo rule must have an id' ) if 'description' not in data : raise KeyError ( 'This promo rule must have a description' ) if 'amount' not in data : raise KeyError ( 'This promo rule must have an amount' ) if 'target' not in data : raise KeyError ( 'This promo rule must apply to a target (example per_item, total, or shipping' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'promo-rules' ) , data = data ) if response is not None : return response
7162	def format_answers ( self , fmt = 'obj' ) : fmts = ( 'obj' , 'array' , 'plain' ) if fmt not in fmts : eprint ( "Error: '{}' not in {}" . format ( fmt , fmts ) ) return def stringify ( val ) : if type ( val ) in ( list , tuple ) : return ', ' . join ( str ( e ) for e in val ) return val if fmt == 'obj' : return json . dumps ( self . answers ) elif fmt == 'array' : answers = [ [ k , v ] for k , v in self . answers . items ( ) ] return json . dumps ( answers ) elif fmt == 'plain' : answers = '\n' . join ( '{}: {}' . format ( k , stringify ( v ) ) for k , v in self . answers . items ( ) ) return answers
1910	def run ( self , procs = 1 , timeout = None , should_profile = False ) : assert not self . running , "Manticore is already running." self . _start_run ( ) self . _last_run_stats [ 'time_started' ] = time . time ( ) with self . shutdown_timeout ( timeout ) : self . _start_workers ( procs , profiling = should_profile ) self . _join_workers ( ) self . _finish_run ( profiling = should_profile )
8418	def nearest_int ( x ) : if x == 0 : return np . int64 ( 0 ) elif x > 0 : return np . int64 ( x + 0.5 ) else : return np . int64 ( x - 0.5 )
2494	def package_verif_node ( self , package ) : verif_node = BNode ( ) type_triple = ( verif_node , RDF . type , self . spdx_namespace . PackageVerificationCode ) self . graph . add ( type_triple ) value_triple = ( verif_node , self . spdx_namespace . packageVerificationCodeValue , Literal ( package . verif_code ) ) self . graph . add ( value_triple ) excl_file_nodes = map ( lambda excl : Literal ( excl ) , package . verif_exc_files ) excl_predicate = self . spdx_namespace . packageVerificationCodeExcludedFile excl_file_triples = [ ( verif_node , excl_predicate , xcl_file ) for xcl_file in excl_file_nodes ] for trp in excl_file_triples : self . graph . add ( trp ) return verif_node
3231	def get_cache_access_details ( key = None ) : from cloudaux . gcp . decorators import _GCP_CACHE return _GCP_CACHE . get_access_details ( key = key )
7107	def fit ( self , X , y , coef_init = None , intercept_init = None , sample_weight = None ) : super ( SGDClassifier , self ) . fit ( X , y , coef_init , intercept_init , sample_weight )
8336	def findPreviousSiblings ( self , name = None , attrs = { } , text = None , limit = None , * * kwargs ) : return self . _findAll ( name , attrs , text , limit , self . previousSiblingGenerator , * * kwargs )
6813	def post_deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) self . vprint ( 'post_deploy:' , service ) funcs = common . service_post_deployers . get ( service ) if funcs : self . vprint ( 'Running post-deployments for service %s...' % ( service , ) ) for func in funcs : try : func ( ) except Exception as e : print ( 'Post deployment error: %s' % e , file = sys . stderr ) print ( traceback . format_exc ( ) , file = sys . stderr )
6811	def pre_deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service_pre_deployers . get ( service ) if funcs : print ( 'Running pre-deployments for service %s...' % ( service , ) ) for func in funcs : func ( )
1999	def _method ( self , expression , * args ) : assert expression . __class__ . __mro__ [ - 1 ] is object for cls in expression . __class__ . __mro__ : sort = cls . __name__ methodname = 'visit_%s' % sort method = getattr ( self , methodname , None ) if method is not None : method ( expression , * args ) return return
9869	def cleanup_none ( self ) : for ( prop , default ) in self . defaults . items ( ) : if getattr ( self , prop ) == '_None' : setattr ( self , prop , None )
1070	def getaddrlist ( self ) : result = [ ] ad = self . getaddress ( ) while ad : result += ad ad = self . getaddress ( ) return result
3793	def setup_a_alpha_and_derivatives ( self , i , T = None ) : self . a , self . kappa , self . Tc = self . ais [ i ] , self . kappas [ i ] , self . Tcs [ i ]
6060	def numpy_array_2d_from_fits ( file_path , hdu ) : hdu_list = fits . open ( file_path ) return np . flipud ( np . array ( hdu_list [ hdu ] . data ) )
7268	def operator ( name = None , operators = None , aliases = None , kind = None ) : def delegator ( assertion , subject , expected , * args , * * kw ) : return assertion . test ( subject , expected , * args , * * kw ) def decorator ( fn ) : operator = Operator ( fn = fn , aliases = aliases , kind = kind ) _name = name if isinstance ( name , six . string_types ) else fn . __name__ operator . operators = ( _name , ) _operators = operators if isinstance ( _operators , list ) : _operators = tuple ( _operators ) if isinstance ( _operators , tuple ) : operator . operators += _operators # Register operator Engine . register ( operator ) return functools . partial ( delegator , operator ) return decorator ( name ) if inspect . isfunction ( name ) else decorator
12188	def message_is_to_me ( self , data ) : return ( data . get ( 'type' ) == 'message' and data . get ( 'text' , '' ) . startswith ( self . address_as ) )
4187	def window_riesz ( N ) : n = linspace ( - N / 2. , ( N ) / 2. , N ) w = 1 - abs ( n / ( N / 2. ) ) ** 2. return w
12204	def auto_constraints ( self , component = None ) : if not component : for table in self . tables : self . auto_constraints ( table ) return if not component . tableSchema . primaryKey : idcol = component . get_column ( term_uri ( 'id' ) ) if idcol : component . tableSchema . primaryKey = [ idcol . name ] self . _auto_foreign_keys ( component ) try : table_type = self . get_tabletype ( component ) except ValueError : # New component is not a known CLDF term, so cannot add components # automatically. TODO: We might me able to infer some based on # `xxxReference` column properties? return # auto-add foreign keys targetting the new component: for table in self . tables : self . _auto_foreign_keys ( table , component = component , table_type = table_type )
9417	def to_pointer ( cls , instance ) : return OctavePtr ( instance . _ref , instance . _name , instance . _address )
10321	def microcanonical_averages ( graph , runs = 40 , spanning_cluster = True , model = 'bond' , alpha = alpha_1sigma , copy_result = True ) : try : runs = int ( runs ) except : raise ValueError ( "runs needs to be a positive integer" ) if runs <= 0 : raise ValueError ( "runs needs to be a positive integer" ) try : alpha = float ( alpha ) except : raise ValueError ( "alpha needs to be a float in the interval (0, 1)" ) if alpha <= 0.0 or alpha >= 1.0 : raise ValueError ( "alpha needs to be a float in the interval (0, 1)" ) # initial iteration # we do not need a copy of the result dictionary since we copy the values # anyway run_iterators = [ sample_states ( graph , spanning_cluster = spanning_cluster , model = model , copy_result = False ) for _ in range ( runs ) ] ret = dict ( ) for microcanonical_ensemble in zip ( * run_iterators ) : # merge cluster statistics ret [ 'n' ] = microcanonical_ensemble [ 0 ] [ 'n' ] ret [ 'N' ] = microcanonical_ensemble [ 0 ] [ 'N' ] ret [ 'M' ] = microcanonical_ensemble [ 0 ] [ 'M' ] max_cluster_size = np . empty ( runs ) moments = np . empty ( ( runs , 5 ) ) if spanning_cluster : has_spanning_cluster = np . empty ( runs ) for r , state in enumerate ( microcanonical_ensemble ) : assert state [ 'n' ] == ret [ 'n' ] assert state [ 'N' ] == ret [ 'N' ] assert state [ 'M' ] == ret [ 'M' ] max_cluster_size [ r ] = state [ 'max_cluster_size' ] moments [ r ] = state [ 'moments' ] if spanning_cluster : has_spanning_cluster [ r ] = state [ 'has_spanning_cluster' ] ret . update ( _microcanonical_average_max_cluster_size ( max_cluster_size , alpha ) ) ret . update ( _microcanonical_average_moments ( moments , alpha ) ) if spanning_cluster : ret . update ( _microcanonical_average_spanning_cluster ( has_spanning_cluster , alpha ) ) if copy_result : yield copy . deepcopy ( ret ) else : yield ret
10907	def twoslice ( field , center = None , size = 6.0 , cmap = 'bone_r' , vmin = 0 , vmax = 1 , orientation = 'vertical' , figpad = 1.09 , off = 0.01 ) : center = center or [ i // 2 for i in field . shape ] slices = [ ] for i , c in enumerate ( center ) : blank = [ np . s_ [ : ] ] * len ( center ) blank [ i ] = c slices . append ( tuple ( blank ) ) z , y , x = [ float ( i ) for i in field . shape ] w = float ( x + z ) h = float ( y + z ) def show ( field , ax , slicer , transpose = False ) : tmp = field [ slicer ] if not transpose else field [ slicer ] . T ax . imshow ( tmp , cmap = cmap , interpolation = 'nearest' , vmin = vmin , vmax = vmax ) ax . set_xticks ( [ ] ) ax . set_yticks ( [ ] ) ax . grid ( 'off' ) if orientation . startswith ( 'v' ) : # rect = l,b,w,h log . info ( '{} {} {} {} {} {}' . format ( x , y , z , w , h , x / h ) ) r = x / h q = y / h f = 1 / ( 1 + 3 * off ) fig = pl . figure ( figsize = ( size * r , size * f ) ) ax1 = fig . add_axes ( ( off , f * ( 1 - q ) + 2 * off , f , f * q ) ) ax2 = fig . add_axes ( ( off , off , f , f * ( 1 - q ) ) ) show ( field , ax1 , slices [ 0 ] ) show ( field , ax2 , slices [ 1 ] ) else : # rect = l,b,w,h r = y / w q = x / w f = 1 / ( 1 + 3 * off ) fig = pl . figure ( figsize = ( size * f , size * r ) ) ax1 = fig . add_axes ( ( off , off , f * q , f ) ) ax2 = fig . add_axes ( ( 2 * off + f * q , off , f * ( 1 - q ) , f ) ) show ( field , ax1 , slices [ 0 ] ) show ( field , ax2 , slices [ 2 ] , transpose = True ) return fig , ax1 , ax2
8663	def generate_passphrase ( size = 12 ) : chars = string . ascii_lowercase + string . ascii_uppercase + string . digits return str ( '' . join ( random . choice ( chars ) for _ in range ( size ) ) )
11625	def generate ( grammar = None , num = 1 , output = sys . stdout , max_recursion = 10 , seed = None ) : if seed is not None : gramfuzz . rand . seed ( seed ) fuzzer = gramfuzz . GramFuzzer ( ) fuzzer . load_grammar ( grammar ) cat_group = os . path . basename ( grammar ) . replace ( ".py" , "" ) results = fuzzer . gen ( cat_group = cat_group , num = num , max_recursion = max_recursion ) for res in results : output . write ( res )
9146	def sheet ( connection , skip , file : TextIO ) : from tabulate import tabulate header = [ '' , 'Name' , 'Description' , 'Terms' , 'Relations' ] rows = [ ] for i , ( idx , name , manager ) in enumerate ( _iterate_managers ( connection , skip ) , start = 1 ) : try : if not manager . is_populated ( ) : continue except AttributeError : click . secho ( f'{name} does not implement is_populated' , fg = 'red' ) continue terms , relations = None , None if isinstance ( manager , BELNamespaceManagerMixin ) : terms = manager . _count_model ( manager . namespace_model ) if isinstance ( manager , BELManagerMixin ) : try : relations = manager . count_relations ( ) except TypeError as e : relations = str ( e ) rows . append ( ( i , name , manager . __doc__ . split ( '\n' ) [ 0 ] . strip ( ) . strip ( '.' ) , terms , relations ) ) print ( tabulate ( rows , headers = header , # tablefmt="fancy_grid", ) )
4848	def _partition_items ( self , channel_metadata_item_map ) : items_to_create = { } items_to_update = { } items_to_delete = { } transmission_map = { } export_content_ids = channel_metadata_item_map . keys ( ) # Get the items that were previously transmitted to the integrated channel. # If we are not transmitting something that was previously transmitted, # we need to delete it from the integrated channel. for transmission in self . _get_transmissions ( ) : transmission_map [ transmission . content_id ] = transmission if transmission . content_id not in export_content_ids : items_to_delete [ transmission . content_id ] = transmission . channel_metadata # Compare what is currently being transmitted to what was transmitted # previously, identifying items that need to be created or updated. for item in channel_metadata_item_map . values ( ) : content_id = item . content_id channel_metadata = item . channel_metadata transmitted_item = transmission_map . get ( content_id , None ) if transmitted_item is not None : if diff ( channel_metadata , transmitted_item . channel_metadata ) : items_to_update [ content_id ] = channel_metadata else : items_to_create [ content_id ] = channel_metadata LOGGER . info ( 'Preparing to transmit creation of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_create ) , self . enterprise_configuration , items_to_create . keys ( ) , ) LOGGER . info ( 'Preparing to transmit update of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_update ) , self . enterprise_configuration , items_to_update . keys ( ) , ) LOGGER . info ( 'Preparing to transmit deletion of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_delete ) , self . enterprise_configuration , items_to_delete . keys ( ) , ) return items_to_create , items_to_update , items_to_delete , transmission_map
7495	def chunk_to_matrices ( narr , mapcol , nmask ) : ## get seq alignment and create an empty array for filling mats = np . zeros ( ( 3 , 16 , 16 ) , dtype = np . uint32 ) ## replace ints with small ints that index their place in the ## 16x16. This no longer checks for big ints to exclude, so resolve=True ## is now the default, TODO. last_loc = - 1 for idx in xrange ( mapcol . shape [ 0 ] ) : if not nmask [ idx ] : if not mapcol [ idx ] == last_loc : i = narr [ : , idx ] mats [ 0 , ( 4 * i [ 0 ] ) + i [ 1 ] , ( 4 * i [ 2 ] ) + i [ 3 ] ] += 1 last_loc = mapcol [ idx ] ## fill the alternates x = np . uint8 ( 0 ) for y in np . array ( [ 0 , 4 , 8 , 12 ] , dtype = np . uint8 ) : for z in np . array ( [ 0 , 4 , 8 , 12 ] , dtype = np . uint8 ) : mats [ 1 , y : y + np . uint8 ( 4 ) , z : z + np . uint8 ( 4 ) ] = mats [ 0 , x ] . reshape ( 4 , 4 ) mats [ 2 , y : y + np . uint8 ( 4 ) , z : z + np . uint8 ( 4 ) ] = mats [ 0 , x ] . reshape ( 4 , 4 ) . T x += np . uint8 ( 1 ) return mats
559	def setSwarmState ( self , swarmId , newStatus ) : assert ( newStatus in [ 'active' , 'completing' , 'completed' , 'killed' ] ) # Set the swarm status swarmInfo = self . _state [ 'swarms' ] [ swarmId ] if swarmInfo [ 'status' ] == newStatus : return # If some other worker noticed it as completed, setting it to completing # is obviously old information.... if swarmInfo [ 'status' ] == 'completed' and newStatus == 'completing' : return self . _dirty = True swarmInfo [ 'status' ] = newStatus if newStatus == 'completed' : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) swarmInfo [ 'bestModelId' ] = modelId swarmInfo [ 'bestErrScore' ] = errScore # If no longer active, remove it from the activeSwarms entry if newStatus != 'active' and swarmId in self . _state [ 'activeSwarms' ] : self . _state [ 'activeSwarms' ] . remove ( swarmId ) # If new status is 'killed', kill off any running particles in that swarm if newStatus == 'killed' : self . _hsObj . killSwarmParticles ( swarmId ) # In case speculative particles are enabled, make sure we generate a new # swarm at this time if all of the swarms in the current sprint have # completed. This will insure that we don't mark the sprint as completed # before we've created all the possible swarms. sprintIdx = swarmInfo [ 'sprintIdx' ] self . isSprintActive ( sprintIdx ) # Update the sprint status. Check all the swarms that belong to this sprint. # If they are all completed, the sprint is completed. sprintInfo = self . _state [ 'sprints' ] [ sprintIdx ] statusCounts = dict ( active = 0 , completing = 0 , completed = 0 , killed = 0 ) bestModelIds = [ ] bestErrScores = [ ] for info in self . _state [ 'swarms' ] . itervalues ( ) : if info [ 'sprintIdx' ] != sprintIdx : continue statusCounts [ info [ 'status' ] ] += 1 if info [ 'status' ] == 'completed' : bestModelIds . append ( info [ 'bestModelId' ] ) bestErrScores . append ( info [ 'bestErrScore' ] ) if statusCounts [ 'active' ] > 0 : sprintStatus = 'active' elif statusCounts [ 'completing' ] > 0 : sprintStatus = 'completing' else : sprintStatus = 'completed' sprintInfo [ 'status' ] = sprintStatus # If the sprint is complete, get the best model from all of its swarms and # store that as the sprint best if sprintStatus == 'completed' : if len ( bestErrScores ) > 0 : whichIdx = numpy . array ( bestErrScores ) . argmin ( ) sprintInfo [ 'bestModelId' ] = bestModelIds [ whichIdx ] sprintInfo [ 'bestErrScore' ] = bestErrScores [ whichIdx ] else : # This sprint was empty, most likely because all particles were # killed. Give it a huge error score sprintInfo [ 'bestModelId' ] = 0 sprintInfo [ 'bestErrScore' ] = numpy . inf # See if our best err score got NO BETTER as compared to a previous # sprint. If so, stop exploring subsequent sprints (lastGoodSprint # is no longer None). bestPrior = numpy . inf for idx in range ( sprintIdx ) : if self . _state [ 'sprints' ] [ idx ] [ 'status' ] == 'completed' : ( _ , errScore ) = self . bestModelInCompletedSprint ( idx ) if errScore is None : errScore = numpy . inf else : errScore = numpy . inf if errScore < bestPrior : bestPrior = errScore if sprintInfo [ 'bestErrScore' ] >= bestPrior : self . _state [ 'lastGoodSprint' ] = sprintIdx - 1 # If ALL sprints up to the last good one are done, the search is now over if self . _state [ 'lastGoodSprint' ] is not None and not self . anyGoodSprintsActive ( ) : self . _state [ 'searchOver' ] = True
10962	def get_values ( self , params ) : return util . delistify ( [ self . param_dict [ p ] for p in util . listify ( params ) ] , params )
4438	async def _previous ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) try : await player . play_previous ( ) except lavalink . NoPreviousTrack : await ctx . send ( 'There is no previous song to play.' )
7016	def concat_write_pklc ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True ) : concatlcd = concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = aperture , sortby = sortby , normalize = normalize , recursive = recursive ) if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) outfpath = os . path . join ( outdir , '%s-%s-pklc.pkl' % ( concatlcd [ 'objectid' ] , aperture ) ) pklc = lcdict_to_pickle ( concatlcd , outfile = outfpath ) return pklc
8232	def set_callbacks ( self , * * kwargs ) : for name in self . SUPPORTED_CALLBACKS : func = kwargs . get ( name , getattr ( self , name ) ) setattr ( self , name , func )
9484	def validate_content ( * objs ) : from . main import Collection , Module validator = { Collection : cnxml . validate_collxml , Module : cnxml . validate_cnxml , } [ type ( objs [ 0 ] ) ] return validator ( * [ obj . file for obj in objs ] )
2741	def remove_tags ( self , tags ) : return self . get_data ( "firewalls/%s/tags" % self . id , type = DELETE , params = { "tags" : tags } )
3390	def validate ( self , samples ) : samples = np . atleast_2d ( samples ) prob = self . problem if samples . shape [ 1 ] == len ( self . model . reactions ) : S = create_stoichiometric_matrix ( self . model ) b = np . array ( [ self . model . constraints [ m . id ] . lb for m in self . model . metabolites ] ) bounds = np . array ( [ r . bounds for r in self . model . reactions ] ) . T elif samples . shape [ 1 ] == len ( self . model . variables ) : S = prob . equalities b = prob . b bounds = prob . variable_bounds else : raise ValueError ( "Wrong number of columns. samples must have a " "column for each flux or variable defined in the " "model!" ) feasibility = np . abs ( S . dot ( samples . T ) . T - b ) . max ( axis = 1 ) lb_error = ( samples - bounds [ 0 , ] ) . min ( axis = 1 ) ub_error = ( bounds [ 1 , ] - samples ) . min ( axis = 1 ) if ( samples . shape [ 1 ] == len ( self . model . variables ) and prob . inequalities . shape [ 0 ] ) : consts = prob . inequalities . dot ( samples . T ) lb_error = np . minimum ( lb_error , ( consts - prob . bounds [ 0 , ] ) . min ( axis = 1 ) ) ub_error = np . minimum ( ub_error , ( prob . bounds [ 1 , ] - consts ) . min ( axis = 1 ) ) valid = ( ( feasibility < self . feasibility_tol ) & ( lb_error > - self . bounds_tol ) & ( ub_error > - self . bounds_tol ) ) codes = np . repeat ( "" , valid . shape [ 0 ] ) . astype ( np . dtype ( ( str , 3 ) ) ) codes [ valid ] = "v" codes [ lb_error <= - self . bounds_tol ] = np . char . add ( codes [ lb_error <= - self . bounds_tol ] , "l" ) codes [ ub_error <= - self . bounds_tol ] = np . char . add ( codes [ ub_error <= - self . bounds_tol ] , "u" ) codes [ feasibility > self . feasibility_tol ] = np . char . add ( codes [ feasibility > self . feasibility_tol ] , "e" ) return codes
8481	def env ( key , default ) : value = os . environ . get ( key , None ) if value is not None : log . info ( ' %s = %r' , key . lower ( ) . replace ( '_' , '.' ) , value ) return value key = key . lower ( ) . replace ( '_' , '.' ) value = get ( key ) if value is not None : return value return default
4537	def wheel_helper ( pos , length , cycle_step ) : return wheel_color ( ( pos * len ( _WHEEL ) / length ) + cycle_step )
2631	def _status ( self ) : job_id_list = ' ' . join ( self . resources . keys ( ) ) cmd = "condor_q {0} -af:jr JobStatus" . format ( job_id_list ) retcode , stdout , stderr = super ( ) . execute_wait ( cmd ) """ Example output: $ condor_q 34524642.0 34524643.0 -af:jr JobStatus 34524642.0 2 34524643.0 1 """ for line in stdout . strip ( ) . split ( '\n' ) : parts = line . split ( ) job_id = parts [ 0 ] status = translate_table . get ( parts [ 1 ] , 'UNKNOWN' ) self . resources [ job_id ] [ 'status' ] = status
5252	def bdib ( self , ticker , start_datetime , end_datetime , event_type , interval , elms = None ) : elms = [ ] if not elms else elms # flush event queue in case previous call errored out logger = _get_logger ( self . debug ) while ( self . _session . tryNextEvent ( ) ) : pass # Create and fill the request for the historical data request = self . refDataService . createRequest ( 'IntradayBarRequest' ) request . set ( 'security' , ticker ) request . set ( 'eventType' , event_type ) request . set ( 'interval' , interval ) # bar interval in minutes request . set ( 'startDateTime' , start_datetime ) request . set ( 'endDateTime' , end_datetime ) for name , val in elms : request . set ( name , val ) logger . info ( 'Sending Request:\n{}' . format ( request ) ) # Send the request self . _session . sendRequest ( request , identity = self . _identity ) # Process received events data = [ ] flds = [ 'open' , 'high' , 'low' , 'close' , 'volume' , 'numEvents' ] for msg in self . _receive_events ( ) : d = msg [ 'element' ] [ 'IntradayBarResponse' ] for bar in d [ 'barData' ] [ 'barTickData' ] : data . append ( bar [ 'barTickData' ] ) data = pd . DataFrame ( data ) . set_index ( 'time' ) . sort_index ( ) . loc [ : , flds ] return data
6985	def parallel_timebin ( lclist , binsizesec , maxobjects = None , outdir = None , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , minbinelems = 7 , nworkers = NCPUS , maxworkertasks = 1000 ) : if outdir and not os . path . exists ( outdir ) : os . mkdir ( outdir ) if maxobjects is not None : lclist = lclist [ : maxobjects ] tasks = [ ( x , binsizesec , { 'outdir' : outdir , 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'minbinelems' : minbinelems } ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( timebinlc_worker , tasks ) pool . close ( ) pool . join ( ) resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( lclist , results ) } return resdict
4740	def emph ( txt , rval = None ) : if rval is None : # rval is not specified, use 'neutral' info ( txt ) elif rval == 0 : # rval is 0, by convention, this is 'good' good ( txt ) else : # any other value, considered 'bad' err ( txt )
9353	def job_title ( ) : result = random . choice ( get_dictionary ( 'job_titles' ) ) . strip ( ) result = result . replace ( '#{N}' , job_title_suffix ( ) ) return result
10325	def _binomial_pmf ( n , p ) : n = int ( n ) ret = np . empty ( n + 1 ) nmax = int ( np . round ( p * n ) ) ret [ nmax ] = 1.0 old_settings = np . seterr ( under = 'ignore' ) # seterr to known value for i in range ( nmax + 1 , n + 1 ) : ret [ i ] = ret [ i - 1 ] * ( n - i + 1.0 ) / i * p / ( 1.0 - p ) for i in range ( nmax - 1 , - 1 , - 1 ) : ret [ i ] = ret [ i + 1 ] * ( i + 1.0 ) / ( n - i ) * ( 1.0 - p ) / p np . seterr ( * * old_settings ) # reset to default return ret / ret . sum ( )
2660	def _start_local_queue_process ( self ) : comm_q = Queue ( maxsize = 10 ) self . queue_proc = Process ( target = interchange . starter , args = ( comm_q , ) , kwargs = { "client_ports" : ( self . outgoing_q . port , self . incoming_q . port , self . command_client . port ) , "worker_ports" : self . worker_ports , "worker_port_range" : self . worker_port_range , "logdir" : "{}/{}" . format ( self . run_dir , self . label ) , "suppress_failure" : self . suppress_failure , "heartbeat_threshold" : self . heartbeat_threshold , "poll_period" : self . poll_period , "logging_level" : logging . DEBUG if self . worker_debug else logging . INFO } , ) self . queue_proc . start ( ) try : ( worker_task_port , worker_result_port ) = comm_q . get ( block = True , timeout = 120 ) except queue . Empty : logger . error ( "Interchange has not completed initialization in 120s. Aborting" ) raise Exception ( "Interchange failed to start" ) self . worker_task_url = "tcp://{}:{}" . format ( self . address , worker_task_port ) self . worker_result_url = "tcp://{}:{}" . format ( self . address , worker_result_port )
6586	def retries ( max_tries , exceptions = ( Exception , ) ) : def decorator ( func ) : def function ( * args , * * kwargs ) : retries_left = max_tries while retries_left > 0 : try : retries_left -= 1 return func ( * args , * * kwargs ) except exceptions as exc : # Don't retry for PandoraExceptions - unlikely that result # will change for same set of input parameters. if isinstance ( exc , PandoraException ) : raise if retries_left > 0 : time . sleep ( delay_exponential ( 0.5 , 2 , max_tries - retries_left ) ) else : raise return function return decorator
10376	def calculate_concordance ( graph : BELGraph , key : str , cutoff : Optional [ float ] = None , use_ambiguous : bool = False ) -> float : correct , incorrect , ambiguous , _ = calculate_concordance_helper ( graph , key , cutoff = cutoff ) try : return correct / ( correct + incorrect + ( ambiguous if use_ambiguous else 0 ) ) except ZeroDivisionError : return - 1.0
8647	def accept_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'accept' , } # POST /api/projects/0.1/milestone_requests/{milestone_request_id}/?action= # accept endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotAcceptedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11370	def convert_date_from_iso_to_human ( value ) : try : year , month , day = value . split ( "-" ) except ValueError : # Not separated by "-". Space? try : year , month , day = value . split ( " " ) except ValueError : # What gives? OK, lets just return as is return value try : date_object = datetime ( int ( year ) , int ( month ) , int ( day ) ) except TypeError : return value return date_object . strftime ( "%d %b %Y" )
8295	def cliques ( graph , threshold = 3 ) : cliques = [ ] for n in graph . nodes : c = clique ( graph , n . id ) if len ( c ) >= threshold : c . sort ( ) if c not in cliques : cliques . append ( c ) return cliques
1095	def escape ( pattern ) : s = list ( pattern ) alphanum = _alphanum for i , c in enumerate ( pattern ) : if c not in alphanum : if c == "\000" : s [ i ] = "\\000" else : s [ i ] = "\\" + c return pattern [ : 0 ] . join ( s )
5167	def __intermediate_dns_search ( self , uci , address ) : # allow override if 'dns_search' in uci : return uci [ 'dns_search' ] # ignore if "proto" is none if address [ 'proto' ] == 'none' : return None dns_search = self . netjson . get ( 'dns_search' , None ) if dns_search : return ' ' . join ( dns_search )
12538	def get_unique_field_values ( dcm_file_list , field_name ) : field_values = set ( ) for dcm in dcm_file_list : field_values . add ( str ( DicomFile ( dcm ) . get_attributes ( field_name ) ) ) return field_values
5009	def create_course_completion ( self , user_id , payload ) : url = self . enterprise_configuration . sapsf_base_url + self . global_sap_config . completion_status_api_path return self . _call_post_with_user_override ( user_id , url , payload )
3529	def get_user_from_context ( context ) : try : return context [ 'user' ] except KeyError : pass try : request = context [ 'request' ] return request . user except ( KeyError , AttributeError ) : pass return None
8937	def get_pypi_auth ( configfile = '~/.pypirc' ) : pypi_cfg = ConfigParser ( ) if pypi_cfg . read ( os . path . expanduser ( configfile ) ) : try : user = pypi_cfg . get ( 'pypi' , 'username' ) pwd = pypi_cfg . get ( 'pypi' , 'password' ) return user , pwd except ConfigError : notify . warning ( "No PyPI credentials in '{}'," " will fall back to '~/.netrc'..." . format ( configfile ) ) return None
13607	def pickle ( obj , filepath ) : arr = pkl . dumps ( obj , - 1 ) with open ( filepath , 'wb' ) as f : s = 0 while s < len ( arr ) : e = min ( s + blosc . MAX_BUFFERSIZE , len ( arr ) ) carr = blosc . compress ( arr [ s : e ] , typesize = 8 ) f . write ( carr ) s = e
13241	def daily_periods ( self , range_start = datetime . date . min , range_end = datetime . date . max , exclude_dates = tuple ( ) ) : tz = self . timezone period = self . period weekdays = self . weekdays current_date = max ( range_start , self . start_date ) end_date = range_end if self . end_date : end_date = min ( end_date , self . end_date ) while current_date <= end_date : if current_date . weekday ( ) in weekdays and current_date not in exclude_dates : yield Period ( tz . localize ( datetime . datetime . combine ( current_date , period . start ) ) , tz . localize ( datetime . datetime . combine ( current_date , period . end ) ) ) current_date += datetime . timedelta ( days = 1 )
11590	def _rc_smove ( self , src , dst , value ) : if self . type ( src ) != b ( "set" ) : return self . smove ( src + "{" + src + "}" , dst , value ) if self . type ( dst ) != b ( "set" ) : return self . smove ( dst + "{" + dst + "}" , src , value ) if self . srem ( src , value ) : return 1 if self . sadd ( dst , value ) else 0 return 0
5664	def interpolate_shape_times ( shape_distances , shape_breaks , stop_times ) : shape_times = np . zeros ( len ( shape_distances ) ) shape_times [ : shape_breaks [ 0 ] ] = stop_times [ 0 ] for i in range ( len ( shape_breaks ) - 1 ) : cur_break = shape_breaks [ i ] cur_time = stop_times [ i ] next_break = shape_breaks [ i + 1 ] next_time = stop_times [ i + 1 ] if cur_break == next_break : shape_times [ cur_break ] = stop_times [ i ] else : cur_distances = shape_distances [ cur_break : next_break + 1 ] norm_distances = ( ( np . array ( cur_distances ) - float ( cur_distances [ 0 ] ) ) / float ( cur_distances [ - 1 ] - cur_distances [ 0 ] ) ) times = ( 1. - norm_distances ) * cur_time + norm_distances * next_time shape_times [ cur_break : next_break ] = times [ : - 1 ] # deal final ones separately: shape_times [ shape_breaks [ - 1 ] : ] = stop_times [ - 1 ] return list ( shape_times )
7241	def aoi ( self , * * kwargs ) : g = self . _parse_geoms ( * * kwargs ) if g is None : return self else : return self [ g ]
3753	def Ceiling ( CASRN , AvailableMethods = False , Method = None ) : # pragma: no cover def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits and ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] or _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] ) : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : if _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] : _Ceiling = ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] , 'ppm' ) elif _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] : _Ceiling = ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] , 'mg/m^3' ) elif Method == NONE : _Ceiling = None else : raise Exception ( 'Failure in in function' ) return _Ceiling
144	def copy ( self , exterior = None , label = None ) : return self . deepcopy ( exterior = exterior , label = label )
8824	def context ( self ) : if not self . _context : self . _context = context . get_admin_context ( ) return self . _context
1627	def CheckForCopyright ( filename , lines , error ) : # We'll say it should occur by line 10. Don't forget there's a # dummy line at the front. for line in range ( 1 , min ( len ( lines ) , 11 ) ) : if re . search ( r'Copyright' , lines [ line ] , re . I ) : break else : # means no copyright line was found error ( filename , 0 , 'legal/copyright' , 5 , 'No copyright message found. ' 'You should have a line: "Copyright [year] <Copyright Owner>"' )
3901	def _exception_handler ( self , _loop , context ) : # Start a graceful shutdown. self . _coroutine_queue . put ( self . _client . disconnect ( ) ) # Store the exception to be re-raised later. If the context doesn't # contain an exception, create one containing the error message. default_exception = Exception ( context . get ( 'message' ) ) self . _exception = context . get ( 'exception' , default_exception )
10922	def do_levmarq_n_directions ( s , directions , max_iter = 2 , run_length = 2 , damping = 1e-3 , collect_stats = False , marquardt_damping = True , * * kwargs ) : # normal = direction / np.sqrt(np.dot(direction, direction)) normals = np . array ( [ d / np . sqrt ( np . dot ( d , d ) ) for d in directions ] ) if np . isnan ( normals ) . any ( ) : raise ValueError ( '`directions` must not be 0s or contain nan' ) obj = OptState ( s , normals ) lo = LMOptObj ( obj , max_iter = max_iter , run_length = run_length , damping = damping , marquardt_damping = marquardt_damping , * * kwargs ) lo . do_run_1 ( ) if collect_stats : return lo . get_termination_stats ( )
6753	def local_renderer ( self ) : if not self . _local_renderer : r = self . create_local_renderer ( ) self . _local_renderer = r return self . _local_renderer
3666	def calculate ( self , T , P , zs , ws , method ) : if method == SIMPLE : Cpsms = [ i ( T ) for i in self . HeatCapacitySolids ] return mixing_simple ( zs , Cpsms ) else : raise Exception ( 'Method not valid' )
2094	def stdout ( self , pk , start_line = None , end_line = None , outfile = sys . stdout , * * kwargs ) : # resource is Unified Job Template if self . unified_job_type != self . endpoint : unified_job = self . last_job_data ( pk , * * kwargs ) pk = unified_job [ 'id' ] # resource is Unified Job, but pk not given elif not pk : unified_job = self . get ( * * kwargs ) pk = unified_job [ 'id' ] content = self . lookup_stdout ( pk , start_line , end_line ) opened = False if isinstance ( outfile , six . string_types ) : outfile = open ( outfile , 'w' ) opened = True if len ( content ) > 0 : click . echo ( content , nl = 1 , file = outfile ) if opened : outfile . close ( ) return { "changed" : False }
6522	def add_issues ( self , issues ) : if not isinstance ( issues , ( list , tuple ) ) : issues = [ issues ] with self . _lock : self . _all_issues . extend ( issues ) self . _cleaned_issues = None
7402	def to ( self , order ) : if order is None or self . order == order : # object is already at desired position return qs = self . get_ordering_queryset ( ) if self . order > order : qs . filter ( order__lt = self . order , order__gte = order ) . update ( order = F ( 'order' ) + 1 ) else : qs . filter ( order__gt = self . order , order__lte = order ) . update ( order = F ( 'order' ) - 1 ) self . order = order self . save ( )
6921	def _autocorr_func3 ( mags , lag , maglen , magmed , magstd ) : # from http://tinyurl.com/afz57c4 result = npcorrelate ( mags , mags , mode = 'full' ) result = result / npmax ( result ) return result [ int ( result . size / 2 ) : ]
2340	def GNN_instance ( x , idx = 0 , device = None , nh = 20 , * * kwargs ) : device = SETTINGS . get_default ( device = device ) xy = scale ( x ) . astype ( 'float32' ) inputx = th . FloatTensor ( xy [ : , [ 0 ] ] ) . to ( device ) target = th . FloatTensor ( xy [ : , [ 1 ] ] ) . to ( device ) GNNXY = GNN_model ( x . shape [ 0 ] , device = device , nh = nh ) . to ( device ) GNNYX = GNN_model ( x . shape [ 0 ] , device = device , nh = nh ) . to ( device ) GNNXY . reset_parameters ( ) GNNYX . reset_parameters ( ) XY = GNNXY . run ( inputx , target , * * kwargs ) YX = GNNYX . run ( target , inputx , * * kwargs ) return [ XY , YX ]
6218	def get_bbox ( self , primitive ) : accessor = primitive . attributes . get ( 'POSITION' ) return accessor . min , accessor . max
12335	def apt ( self , package_names , raise_on_error = False ) : if isinstance ( package_names , basestring ) : package_names = [ package_names ] cmd = "apt-get install -y %s" % ( ' ' . join ( package_names ) ) return self . wait ( cmd , raise_on_error = raise_on_error )
10371	def build_pmid_exclusion_filter ( pmids : Strings ) -> EdgePredicate : if isinstance ( pmids , str ) : @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : """Fail for edges with PubMed citations matching the contained PubMed identifier. :return: If the edge has a PubMed citation with the contained PubMed identifier """ return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] != pmids elif isinstance ( pmids , Iterable ) : pmids = set ( pmids ) @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : """Pass for edges with PubMed citations matching one of the contained PubMed identifiers. :return: If the edge has a PubMed citation with one of the contained PubMed identifiers """ return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] not in pmids else : raise TypeError return pmid_exclusion_filter
11824	def genetic_search ( problem , fitness_fn , ngen = 1000 , pmut = 0.1 , n = 20 ) : s = problem . initial_state states = [ problem . result ( s , a ) for a in problem . actions ( s ) ] random . shuffle ( states ) return genetic_algorithm ( states [ : n ] , problem . value , ngen , pmut )
7853	def identity_is ( self , item_category , item_type = None ) : if not item_category : raise ValueError ( "bad category" ) if not item_type : type_expr = u"" elif '"' not in item_type : type_expr = u' and @type="%s"' % ( item_type , ) elif "'" not in type : type_expr = u" and @type='%s'" % ( item_type , ) else : raise ValueError ( "Invalid type name" ) if '"' not in item_category : expr = u'd:identity[@category="%s"%s]' % ( item_category , type_expr ) elif "'" not in item_category : expr = u"d:identity[@category='%s'%s]" % ( item_category , type_expr ) else : raise ValueError ( "Invalid category name" ) l = self . xpath_ctxt . xpathEval ( to_utf8 ( expr ) ) if l : return True else : return False
7892	def join ( self , password = None , history_maxchars = None , history_maxstanzas = None , history_seconds = None , history_since = None ) : if self . joined : raise RuntimeError ( "Room is already joined" ) p = MucPresence ( to_jid = self . room_jid ) p . make_join_request ( password , history_maxchars , history_maxstanzas , history_seconds , history_since ) self . manager . stream . send ( p )
887	def _createSegment ( cls , connections , lastUsedIterationForSegment , cell , iteration , maxSegmentsPerCell ) : # Enforce maxSegmentsPerCell. while connections . numSegments ( cell ) >= maxSegmentsPerCell : leastRecentlyUsedSegment = min ( connections . segmentsForCell ( cell ) , key = lambda segment : lastUsedIterationForSegment [ segment . flatIdx ] ) connections . destroySegment ( leastRecentlyUsedSegment ) # Create the segment. segment = connections . createSegment ( cell ) # Do TM-specific bookkeeping for the segment. if segment . flatIdx == len ( lastUsedIterationForSegment ) : lastUsedIterationForSegment . append ( iteration ) elif segment . flatIdx < len ( lastUsedIterationForSegment ) : # A flatIdx was recycled. lastUsedIterationForSegment [ segment . flatIdx ] = iteration else : raise AssertionError ( "All segments should be created with the TM createSegment method." ) return segment
7138	def get_type_info ( obj ) : if isinstance ( obj , primitive_types ) : return ( 'primitive' , type ( obj ) . __name__ ) if isinstance ( obj , sequence_types ) : return ( 'sequence' , type ( obj ) . __name__ ) if isinstance ( obj , array_types ) : return ( 'array' , type ( obj ) . __name__ ) if isinstance ( obj , key_value_types ) : return ( 'key-value' , type ( obj ) . __name__ ) if isinstance ( obj , types . ModuleType ) : return ( 'module' , type ( obj ) . __name__ ) if isinstance ( obj , ( types . FunctionType , types . MethodType ) ) : return ( 'function' , type ( obj ) . __name__ ) if isinstance ( obj , type ) : if hasattr ( obj , '__dict__' ) : return ( 'class' , obj . __name__ ) if isinstance ( type ( obj ) , type ) : if hasattr ( obj , '__dict__' ) : cls_name = type ( obj ) . __name__ if cls_name == 'classobj' : cls_name = obj . __name__ return ( 'class' , '{}' . format ( cls_name ) ) if cls_name == 'instance' : cls_name = obj . __class__ . __name__ return ( 'instance' , '{} instance' . format ( cls_name ) ) return ( 'unknown' , type ( obj ) . __name__ )
3790	def property_derivative_P ( self , T , P , zs , ws , order = 1 ) : sorted_valid_methods = self . select_valid_methods ( T , P , zs , ws ) for method in sorted_valid_methods : try : return self . calculate_derivative_P ( P , T , zs , ws , method , order ) except : pass return None
823	def addInstance ( self , groundTruth , prediction , record = None , result = None ) : self . value = self . avg ( prediction )
198	def Snowflakes ( density = ( 0.005 , 0.075 ) , density_uniformity = ( 0.3 , 0.9 ) , flake_size = ( 0.2 , 0.7 ) , flake_size_uniformity = ( 0.4 , 0.8 ) , angle = ( - 30 , 30 ) , speed = ( 0.007 , 0.03 ) , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) layer = SnowflakesLayer ( density = density , density_uniformity = density_uniformity , flake_size = flake_size , flake_size_uniformity = flake_size_uniformity , angle = angle , speed = speed , blur_sigma_fraction = ( 0.0001 , 0.001 ) ) return meta . SomeOf ( ( 1 , 3 ) , children = [ layer . deepcopy ( ) for _ in range ( 3 ) ] , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
12831	def validate_xml_text ( text ) : bad_chars = __INVALID_XML_CHARS & set ( text ) if bad_chars : for offset , c in enumerate ( text ) : if c in bad_chars : raise RuntimeError ( 'invalid XML character: ' + repr ( c ) + ' at offset ' + str ( offset ) )
3979	def _get_referenced_services ( specs ) : active_services = set ( ) for app_spec in specs [ 'apps' ] . values ( ) : for service in app_spec [ 'depends' ] [ 'services' ] : active_services . add ( service ) for bundle_spec in specs [ 'bundles' ] . values ( ) : for service in bundle_spec [ 'services' ] : active_services . add ( service ) return active_services
6900	def parallel_periodicfeatures_lcdir ( pfpkl_dir , lcbasedir , outdir , pfpkl_glob = 'periodfinding-*.pkl*' , starfeaturesdir = None , fourierorder = 5 , # these are depth, duration, ingress duration transitparams = ( - 0.01 , 0.1 , 0.1 ) , # these are depth, duration, depth ratio, secphase ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None , nworkers = NCPUS , recursive = True , ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None fileglob = pfpkl_glob # now find the files LOGINFO ( 'searching for periodfinding pickles in %s ...' % pfpkl_dir ) if recursive is False : matching = glob . glob ( os . path . join ( pfpkl_dir , fileglob ) ) else : # use recursive glob for Python 3.5+ if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( pfpkl_dir , '**' , fileglob ) , recursive = True ) # otherwise, use os.walk and glob else : # use os.walk to go through the directories walker = os . walk ( pfpkl_dir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) # now that we have all the files, process them if matching and len ( matching ) > 0 : LOGINFO ( 'found %s periodfinding pickles, getting periodicfeatures...' % len ( matching ) ) return parallel_periodicfeatures ( matching , lcbasedir , outdir , starfeaturesdir = starfeaturesdir , fourierorder = fourierorder , transitparams = transitparams , ebparams = ebparams , pdiff_threshold = pdiff_threshold , sidereal_threshold = sidereal_threshold , sampling_peak_multiplier = sampling_peak_multiplier , sampling_startp = sampling_startp , sampling_endp = sampling_endp , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , sigclip = sigclip , verbose = verbose , maxobjects = maxobjects , nworkers = nworkers , ) else : LOGERROR ( 'no periodfinding pickles found in %s' % ( pfpkl_dir ) ) return None
7667	def search ( self , * * kwargs ) : results = AnnotationArray ( ) for annotation in self : if annotation . search ( * * kwargs ) : results . append ( annotation ) return results
2089	def get ( self , pk = None , * * kwargs ) : if kwargs . pop ( 'include_debug_header' , True ) : debug . log ( 'Getting the record.' , header = 'details' ) response = self . read ( pk = pk , fail_on_no_results = True , fail_on_multiple_results = True , * * kwargs ) return response [ 'results' ] [ 0 ]
3513	def clicky ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ClickyNode ( )
9174	def bake ( binder , recipe_id , publisher , message , cursor ) : recipe = _get_recipe ( recipe_id , cursor ) includes = _formatter_callback_factory ( ) binder = collate_models ( binder , ruleset = recipe , includes = includes ) def flatten_filter ( model ) : return ( isinstance ( model , cnxepub . CompositeDocument ) or ( isinstance ( model , cnxepub . Binder ) and model . metadata . get ( 'type' ) == 'composite-chapter' ) ) def only_documents_filter ( model ) : return isinstance ( model , cnxepub . Document ) and not isinstance ( model , cnxepub . CompositeDocument ) for doc in cnxepub . flatten_to ( binder , flatten_filter ) : publish_composite_model ( cursor , doc , binder , publisher , message ) for doc in cnxepub . flatten_to ( binder , only_documents_filter ) : publish_collated_document ( cursor , doc , binder ) tree = cnxepub . model_to_tree ( binder ) publish_collated_tree ( cursor , tree ) return [ ]
173	def draw_heatmap_array ( self , image_shape , alpha_lines = 1.0 , alpha_points = 1.0 , size_lines = 1 , size_points = 0 , antialiased = True , raise_if_out_of_image = False ) : heatmap_lines = self . draw_lines_heatmap_array ( image_shape , alpha = alpha_lines , size = size_lines , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) if size_points <= 0 : return heatmap_lines heatmap_points = self . draw_points_heatmap_array ( image_shape , alpha = alpha_points , size = size_points , raise_if_out_of_image = raise_if_out_of_image ) heatmap = np . dstack ( [ heatmap_lines , heatmap_points ] ) return np . max ( heatmap , axis = 2 )
2172	def new_state ( self ) : try : self . _state = self . state ( ) log . debug ( "Generated new state %s." , self . _state ) except TypeError : self . _state = self . state log . debug ( "Re-using previously supplied state %s." , self . _state ) return self . _state
1118	def _MakeParallelBenchmark ( p , work_func , * args ) : def Benchmark ( b ) : # pylint: disable=missing-docstring e = threading . Event ( ) def Target ( ) : e . wait ( ) for _ in xrange ( b . N / p ) : work_func ( * args ) threads = [ ] for _ in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . ResetTimer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark
10858	def get_update_tile ( self , params , values ) : doglobal , particles = self . _update_type ( params ) if doglobal : return self . shape . copy ( ) # 1) store the current parameters of interest values0 = self . get_values ( params ) # 2) calculate the current tileset tiles0 = [ self . _tile ( n ) for n in particles ] # 3) update to newer parameters and calculate tileset self . set_values ( params , values ) tiles1 = [ self . _tile ( n ) for n in particles ] # 4) revert parameters & return union of all tiles self . set_values ( params , values0 ) return Tile . boundingtile ( tiles0 + tiles1 )
5767	def _advapi32_interpret_rsa_key_blob ( bit_size , blob_struct , blob ) : len1 = bit_size // 8 len2 = bit_size // 16 prime1_offset = len1 prime2_offset = prime1_offset + len2 exponent1_offset = prime2_offset + len2 exponent2_offset = exponent1_offset + len2 coefficient_offset = exponent2_offset + len2 private_exponent_offset = coefficient_offset + len2 public_exponent = blob_struct . rsapubkey . pubexp modulus = int_from_bytes ( blob [ 0 : prime1_offset ] [ : : - 1 ] ) prime1 = int_from_bytes ( blob [ prime1_offset : prime2_offset ] [ : : - 1 ] ) prime2 = int_from_bytes ( blob [ prime2_offset : exponent1_offset ] [ : : - 1 ] ) exponent1 = int_from_bytes ( blob [ exponent1_offset : exponent2_offset ] [ : : - 1 ] ) exponent2 = int_from_bytes ( blob [ exponent2_offset : coefficient_offset ] [ : : - 1 ] ) coefficient = int_from_bytes ( blob [ coefficient_offset : private_exponent_offset ] [ : : - 1 ] ) private_exponent = int_from_bytes ( blob [ private_exponent_offset : private_exponent_offset + len1 ] [ : : - 1 ] ) public_key_info = keys . PublicKeyInfo ( { 'algorithm' : keys . PublicKeyAlgorithm ( { 'algorithm' : 'rsa' , } ) , 'public_key' : keys . RSAPublicKey ( { 'modulus' : modulus , 'public_exponent' : public_exponent , } ) , } ) rsa_private_key = keys . RSAPrivateKey ( { 'version' : 'two-prime' , 'modulus' : modulus , 'public_exponent' : public_exponent , 'private_exponent' : private_exponent , 'prime1' : prime1 , 'prime2' : prime2 , 'exponent1' : exponent1 , 'exponent2' : exponent2 , 'coefficient' : coefficient , } ) private_key_info = keys . PrivateKeyInfo ( { 'version' : 0 , 'private_key_algorithm' : keys . PrivateKeyAlgorithm ( { 'algorithm' : 'rsa' , } ) , 'private_key' : rsa_private_key , } ) return ( public_key_info , private_key_info )
306	def plot_round_trip_lifetimes ( round_trips , disp_amount = 16 , lsize = 18 , ax = None ) : if ax is None : ax = plt . subplot ( ) symbols_sample = round_trips . symbol . unique ( ) np . random . seed ( 1 ) sample = np . random . choice ( round_trips . symbol . unique ( ) , replace = False , size = min ( disp_amount , len ( symbols_sample ) ) ) sample_round_trips = round_trips [ round_trips . symbol . isin ( sample ) ] symbol_idx = pd . Series ( np . arange ( len ( sample ) ) , index = sample ) for symbol , sym_round_trips in sample_round_trips . groupby ( 'symbol' ) : for _ , row in sym_round_trips . iterrows ( ) : c = 'b' if row . long else 'r' y_ix = symbol_idx [ symbol ] + 0.05 ax . plot ( [ row [ 'open_dt' ] , row [ 'close_dt' ] ] , [ y_ix , y_ix ] , color = c , linewidth = lsize , solid_capstyle = 'butt' ) ax . set_yticks ( range ( disp_amount ) ) ax . set_yticklabels ( [ utils . format_asset ( s ) for s in sample ] ) ax . set_ylim ( ( - 0.5 , min ( len ( sample ) , disp_amount ) - 0.5 ) ) blue = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'b' , label = 'Long' ) red = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'r' , label = 'Short' ) leg = ax . legend ( handles = [ blue , red ] , loc = 'lower left' , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . grid ( False ) return ax
6916	def add_variability_to_fakelc_collection ( simbasedir , override_paramdists = None , overwrite_existingvar = False ) : # open the fakelcs-info.pkl infof = os . path . join ( simbasedir , 'fakelcs-info.pkl' ) with open ( infof , 'rb' ) as infd : lcinfo = pickle . load ( infd ) lclist = lcinfo [ 'lcfpath' ] varflag = lcinfo [ 'isvariable' ] vartypes = lcinfo [ 'vartype' ] vartind = 0 varinfo = { } # go through all the LCs and add the required type of variability for lc , varf , _lcind in zip ( lclist , varflag , range ( len ( lclist ) ) ) : # if this object is variable, add variability if varf : thisvartype = vartypes [ vartind ] if ( override_paramdists and isinstance ( override_paramdists , dict ) and thisvartype in override_paramdists and isinstance ( override_paramdists [ thisvartype ] , dict ) ) : thisoverride_paramdists = override_paramdists [ thisvartype ] else : thisoverride_paramdists = None varlc = add_fakelc_variability ( lc , thisvartype , override_paramdists = thisoverride_paramdists , overwrite = overwrite_existingvar ) varinfo [ varlc [ 'objectid' ] ] = { 'params' : varlc [ 'actual_varparams' ] , 'vartype' : varlc [ 'actual_vartype' ] } # update vartind vartind = vartind + 1 else : varlc = add_fakelc_variability ( lc , None , overwrite = overwrite_existingvar ) varinfo [ varlc [ 'objectid' ] ] = { 'params' : varlc [ 'actual_varparams' ] , 'vartype' : varlc [ 'actual_vartype' ] } # # done with all objects # # write the varinfo back to the dict and fakelcs-info.pkl lcinfo [ 'varinfo' ] = varinfo tempoutf = '%s.%s' % ( infof , md5 ( npr . bytes ( 4 ) ) . hexdigest ( ) [ - 8 : ] ) with open ( tempoutf , 'wb' ) as outfd : pickle . dump ( lcinfo , outfd , pickle . HIGHEST_PROTOCOL ) if os . path . exists ( tempoutf ) : shutil . copy ( tempoutf , infof ) os . remove ( tempoutf ) else : LOGEXCEPTION ( 'could not write output light curve file to dir: %s' % os . path . dirname ( tempoutf ) ) # fail here raise return lcinfo
5570	def profile ( self ) : with rasterio . open ( self . path , "r" ) as src : return deepcopy ( src . meta )
3686	def solve_T ( self , P , V , quick = True ) : def to_solve ( T ) : a_alpha = self . a_alpha_and_derivatives ( T , full = False ) P_calc = R * T / ( V - self . b ) - a_alpha / ( V * V + self . delta * V + self . epsilon ) return P_calc - P return newton ( to_solve , self . Tc * 0.5 )
230	def plot_style_factor_exposures ( tot_style_factor_exposure , factor_name = None , ax = None ) : if ax is None : ax = plt . gca ( ) if factor_name is None : factor_name = tot_style_factor_exposure . name ax . plot ( tot_style_factor_exposure . index , tot_style_factor_exposure , label = factor_name ) avg = tot_style_factor_exposure . mean ( ) ax . axhline ( avg , linestyle = '-.' , label = 'Mean = {:.3}' . format ( avg ) ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) _ , _ , y1 , y2 = plt . axis ( ) lim = max ( abs ( y1 ) , abs ( y2 ) ) ax . set ( title = 'Exposure to {}' . format ( factor_name ) , ylabel = '{} \n weighted exposure' . format ( factor_name ) , ylim = ( - lim , lim ) ) ax . legend ( frameon = True , framealpha = 0.5 ) return ax
3434	def _populate_solver ( self , reaction_list , metabolite_list = None ) : constraint_terms = AutoVivification ( ) to_add = [ ] if metabolite_list is not None : for met in metabolite_list : to_add += [ self . problem . Constraint ( Zero , name = met . id , lb = 0 , ub = 0 ) ] self . add_cons_vars ( to_add ) for reaction in reaction_list : if reaction . id not in self . variables : forward_variable = self . problem . Variable ( reaction . id ) reverse_variable = self . problem . Variable ( reaction . reverse_id ) self . add_cons_vars ( [ forward_variable , reverse_variable ] ) else : reaction = self . reactions . get_by_id ( reaction . id ) forward_variable = reaction . forward_variable reverse_variable = reaction . reverse_variable for metabolite , coeff in six . iteritems ( reaction . metabolites ) : if metabolite . id in self . constraints : constraint = self . constraints [ metabolite . id ] else : constraint = self . problem . Constraint ( Zero , name = metabolite . id , lb = 0 , ub = 0 ) self . add_cons_vars ( constraint , sloppy = True ) constraint_terms [ constraint ] [ forward_variable ] = coeff constraint_terms [ constraint ] [ reverse_variable ] = - coeff self . solver . update ( ) for reaction in reaction_list : reaction = self . reactions . get_by_id ( reaction . id ) reaction . update_variable_bounds ( ) for constraint , terms in six . iteritems ( constraint_terms ) : constraint . set_linear_coefficients ( terms )
1897	def _assert ( self , expression : Bool ) : assert isinstance ( expression , Bool ) smtlib = translate_to_smtlib ( expression ) self . _send ( '(assert %s)' % smtlib )
7919	def __from_unicode ( cls , data , check = True ) : parts1 = data . split ( u"/" , 1 ) parts2 = parts1 [ 0 ] . split ( u"@" , 1 ) if len ( parts2 ) == 2 : local = parts2 [ 0 ] domain = parts2 [ 1 ] if check : local = cls . __prepare_local ( local ) domain = cls . __prepare_domain ( domain ) else : local = None domain = parts2 [ 0 ] if check : domain = cls . __prepare_domain ( domain ) if len ( parts1 ) == 2 : resource = parts1 [ 1 ] if check : resource = cls . __prepare_resource ( parts1 [ 1 ] ) else : resource = None if not domain : raise JIDError ( "Domain is required in JID." ) return ( local , domain , resource )
4074	def get_cfg_value ( config , section , option ) : try : value = config [ section ] [ option ] except KeyError : if ( section , option ) in MULTI_OPTIONS : return [ ] else : return '' if ( section , option ) in MULTI_OPTIONS : value = split_multiline ( value ) if ( section , option ) in ENVIRON_OPTIONS : value = eval_environ ( value ) return value
12964	def all ( self , cascadeFetch = False ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultiple ( matchedKeys , cascadeFetch = cascadeFetch ) return IRQueryableList ( [ ] , mdl = self . mdl )
6902	def load_xmatch_external_catalogs ( xmatchto , xmatchkeys , outfile = None ) : outdict = { } for xc , xk in zip ( xmatchto , xmatchkeys ) : parsed_catdef = _parse_xmatch_catalog_header ( xc , xk ) if not parsed_catdef : continue ( infd , catdefdict , catcolinds , catcoldtypes , catcolnames , catcolunits ) = parsed_catdef # get the specified columns out of the catalog catarr = np . genfromtxt ( infd , usecols = catcolinds , names = xk , dtype = ',' . join ( catcoldtypes ) , comments = '#' , delimiter = '|' , autostrip = True ) infd . close ( ) catshortname = os . path . splitext ( os . path . basename ( xc ) ) [ 0 ] catshortname = catshortname . replace ( '.csv' , '' ) # # make a kdtree for this catalog # # get the ra and decl columns objra , objdecl = ( catarr [ catdefdict [ 'colra' ] ] , catarr [ catdefdict [ 'coldec' ] ] ) # get the xyz unit vectors from ra,decl cosdecl = np . cos ( np . radians ( objdecl ) ) sindecl = np . sin ( np . radians ( objdecl ) ) cosra = np . cos ( np . radians ( objra ) ) sinra = np . sin ( np . radians ( objra ) ) xyz = np . column_stack ( ( cosra * cosdecl , sinra * cosdecl , sindecl ) ) # generate the kdtree kdt = cKDTree ( xyz , copy_data = True ) # generate the outdict element for this catalog catoutdict = { 'kdtree' : kdt , 'data' : catarr , 'columns' : xk , 'colnames' : catcolnames , 'colunits' : catcolunits , 'name' : catdefdict [ 'name' ] , 'desc' : catdefdict [ 'description' ] } outdict [ catshortname ] = catoutdict if outfile is not None : # if we're on OSX, we apparently need to save the file in chunks smaller # than 2 GB to make it work right. can't load pickles larger than 4 GB # either, but 3 GB < total size < 4 GB appears to be OK when loading. # also see: https://bugs.python.org/issue24658. # fix adopted from: https://stackoverflow.com/a/38003910 if sys . platform == 'darwin' : dumpbytes = pickle . dumps ( outdict , protocol = pickle . HIGHEST_PROTOCOL ) max_bytes = 2 ** 31 - 1 with open ( outfile , 'wb' ) as outfd : for idx in range ( 0 , len ( dumpbytes ) , max_bytes ) : outfd . write ( dumpbytes [ idx : idx + max_bytes ] ) else : with open ( outfile , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outfile else : return outdict
2532	def parse_ext_doc_ref ( self , ext_doc_ref_term ) : for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'externalDocumentId' ] , None ) ) : try : self . builder . set_ext_doc_id ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'External Document ID' ) break for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'spdxDocument' ] , None ) ) : try : self . builder . set_spdx_doc_uri ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'SPDX Document URI' ) break for _s , _p , checksum in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'checksum' ] , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : try : self . builder . set_chksum ( self . doc , six . text_type ( value ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'Checksum' ) break
10977	def members ( group_id ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) s = request . args . get ( 's' , '' ) group = Group . query . get_or_404 ( group_id ) if group . can_see_members ( current_user ) : members = Membership . query_by_group ( group_id , with_invitations = True ) if q : members = Membership . search ( members , q ) if s : members = Membership . order ( members , Membership . state , s ) members = members . paginate ( page , per_page = per_page ) return render_template ( "invenio_groups/members.html" , group = group , members = members , page = page , per_page = per_page , q = q , s = s , ) flash ( _ ( 'You are not allowed to see members of this group %(group_name)s.' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
1828	def RET ( cpu , * operands ) : # TODO FIX 64Bit FIX segment N = 0 if len ( operands ) > 0 : N = operands [ 0 ] . read ( ) cpu . PC = cpu . pop ( cpu . address_bit_size ) cpu . STACK += N
2474	def set_lic_text ( self , doc , text ) : if self . has_extr_lic ( doc ) : if not self . extr_text_set : self . extr_text_set = True if validations . validate_is_free_form_text ( text ) : self . extr_lic ( doc ) . text = str_from_text ( text ) return True else : raise SPDXValueError ( 'ExtractedLicense::text' ) else : raise CardinalityError ( 'ExtractedLicense::text' ) else : raise OrderError ( 'ExtractedLicense::text' )
9659	def get_levels ( G ) : levels = [ ] ends = get_sinks ( G ) levels . append ( ends ) while get_direct_ancestors ( G , ends ) : ends = get_direct_ancestors ( G , ends ) levels . append ( ends ) levels . reverse ( ) return levels
7679	def event ( annotation , * * kwargs ) : times , values = annotation . to_interval_values ( ) if any ( values ) : labels = values else : labels = None return mir_eval . display . events ( times , labels = labels , * * kwargs )
7028	def objectid_search ( gaiaid , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g_mean_mag' , 'phot_bp_mean_mag' , 'phot_rp_mean_mag' , 'l' , 'b' , 'parallax, parallax_error' , 'pmra' , 'pmra_error' , 'pmdec' , 'pmdec_error' ) , returnformat = 'csv' , forcefetch = False , cachedir = '~/.astrobase/gaia-cache' , verbose = True , timeout = 15.0 , refresh = 2.0 , maxtimeout = 300.0 , maxtries = 3 , complete_query_later = True ) : # NOTE: here we don't resolve the table name right away. this is because # some of the GAIA mirrors use different table names, so we leave the table # name to be resolved by the lower level tap_query function. this is done by # the {{table}} construct. query = ( "select {columns} from {{table}} where " "source_id = {gaiaid}" ) formatted_query = query . format ( columns = ', ' . join ( columns ) , gaiaid = gaiaid ) return tap_query ( formatted_query , gaia_mirror = gaia_mirror , returnformat = returnformat , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , complete_query_later = complete_query_later )
4054	def collections_sub ( self , collection , * * kwargs ) : query_string = "/{t}/{u}/collections/{c}/collections" . format ( u = self . library_id , t = self . library_type , c = collection . upper ( ) ) return self . _build_query ( query_string )
12862	def add_business_days ( self , days_int , holiday_obj = None ) : res = self if days_int >= 0 : count = 0 while count < days_int : res = BusinessDate . add_days ( res , 1 ) if BusinessDate . is_business_day ( res , holiday_obj ) : count += 1 else : count = 0 while count > days_int : res = BusinessDate . add_days ( res , - 1 ) if BusinessDate . is_business_day ( res , holiday_obj ) : count -= 1 return res
9035	def walk_instructions ( self , mapping = identity ) : instructions = chain ( * self . walk_rows ( lambda row : row . instructions ) ) return map ( mapping , instructions )
4152	def ipy_notebook_skeleton ( ) : py_version = sys . version_info notebook_skeleton = { "cells" : [ ] , "metadata" : { "kernelspec" : { "display_name" : "Python " + str ( py_version [ 0 ] ) , "language" : "python" , "name" : "python" + str ( py_version [ 0 ] ) } , "language_info" : { "codemirror_mode" : { "name" : "ipython" , "version" : py_version [ 0 ] } , "file_extension" : ".py" , "mimetype" : "text/x-python" , "name" : "python" , "nbconvert_exporter" : "python" , "pygments_lexer" : "ipython" + str ( py_version [ 0 ] ) , "version" : '{0}.{1}.{2}' . format ( * sys . version_info [ : 3 ] ) } } , "nbformat" : 4 , "nbformat_minor" : 0 } return notebook_skeleton
13047	def f_i18n_citation_type ( string , lang = "eng" ) : s = " " . join ( string . strip ( "%" ) . split ( "|" ) ) return s . capitalize ( )
10533	def update_project ( project ) : try : project_id = project . id project = _forbidden_attributes ( project ) res = _pybossa_req ( 'put' , 'project' , project_id , payload = project . data ) if res . get ( 'id' ) : return Project ( res ) else : return res except : # pragma: no cover raise
5748	def history ( self , ip , days_limit = None ) : all_dates = sorted ( self . routing_db . smembers ( 'imported_dates' ) , reverse = True ) if days_limit is not None : all_dates = all_dates [ : days_limit ] return [ self . date_asn_block ( ip , date ) for date in all_dates ]
2505	def get_extr_license_text ( self , extr_lic ) : text_tripples = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'extractedText' ] , None ) ) ) if not text_tripples : self . error = True msg = 'Extracted license must have extractedText property' self . logger . log ( msg ) return if len ( text_tripples ) > 1 : self . more_than_one_error ( 'extracted license text' ) return text_tripple = text_tripples [ 0 ] _s , _p , text = text_tripple return text
4182	def window_blackman_nuttall ( N ) : a0 = 0.3635819 a1 = 0.4891775 a2 = 0.1365995 a3 = 0.0106411 return _coeff4 ( N , a0 , a1 , a2 , a3 )
386	def obj_box_horizontal_flip ( im , coords = None , is_rescale = False , is_center = False , is_random = False ) : if coords is None : coords = [ ] def _flip ( im , coords ) : im = flip_axis ( im , axis = 1 , is_random = False ) coords_new = list ( ) for coord in coords : if len ( coord ) != 4 : raise AssertionError ( "coordinate should be 4 values : [x, y, w, h]" ) if is_rescale : if is_center : # x_center' = 1 - x x = 1. - coord [ 0 ] else : # x_center' = 1 - x - w x = 1. - coord [ 0 ] - coord [ 2 ] else : if is_center : # x' = im.width - x x = im . shape [ 1 ] - coord [ 0 ] else : # x' = im.width - x - w x = im . shape [ 1 ] - coord [ 0 ] - coord [ 2 ] coords_new . append ( [ x , coord [ 1 ] , coord [ 2 ] , coord [ 3 ] ] ) return im , coords_new if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : return _flip ( im , coords ) else : return im , coords else : return _flip ( im , coords )
9611	def _request ( self , method , url , body ) : if method != 'POST' and method != 'PUT' : body = None s = Session ( ) LOGGER . debug ( 'Method: {0}, Url: {1}, Body: {2}.' . format ( method , url , body ) ) req = Request ( method , url , json = body ) prepped = s . prepare_request ( req ) res = s . send ( prepped , timeout = self . _timeout or None ) res . raise_for_status ( ) # TODO try catch return res . json ( )
10789	def guess_invert ( st ) : pos = st . obj_get_positions ( ) pxinds_ar = np . round ( pos ) . astype ( 'int' ) inim = st . ishape . translate ( - st . pad ) . contains ( pxinds_ar ) pxinds_tuple = tuple ( pxinds_ar [ inim ] . T ) pxvals = st . data [ pxinds_tuple ] invert = np . median ( pxvals ) < np . median ( st . data ) # invert if dark particles return invert
6020	def simulate_as_gaussian ( cls , shape , pixel_scale , sigma , centre = ( 0.0 , 0.0 ) , axis_ratio = 1.0 , phi = 0.0 ) : from autolens . model . profiles . light_profiles import EllipticalGaussian gaussian = EllipticalGaussian ( centre = centre , axis_ratio = axis_ratio , phi = phi , intensity = 1.0 , sigma = sigma ) grid_1d = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) gaussian_1d = gaussian . intensities_from_grid ( grid = grid_1d ) gaussian_2d = mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = gaussian_1d , shape = shape ) return PSF ( array = gaussian_2d , pixel_scale = pixel_scale , renormalize = True )
1637	def CheckComment ( line , filename , linenum , next_line_start , error ) : commentpos = line . find ( '//' ) if commentpos != - 1 : # Check if the // may be in quotes. If so, ignore it if re . sub ( r'\\.' , '' , line [ 0 : commentpos ] ) . count ( '"' ) % 2 == 0 : # Allow one space for new scopes, two spaces otherwise: if ( not ( Match ( r'^.*{ *//' , line ) and next_line_start == commentpos ) and ( ( commentpos >= 1 and line [ commentpos - 1 ] not in string . whitespace ) or ( commentpos >= 2 and line [ commentpos - 2 ] not in string . whitespace ) ) ) : error ( filename , linenum , 'whitespace/comments' , 2 , 'At least two spaces is best between code and comments' ) # Checks for common mistakes in TODO comments. comment = line [ commentpos : ] match = _RE_PATTERN_TODO . match ( comment ) if match : # One whitespace is correct; zero whitespace is handled elsewhere. leading_whitespace = match . group ( 1 ) if len ( leading_whitespace ) > 1 : error ( filename , linenum , 'whitespace/todo' , 2 , 'Too many spaces before TODO' ) username = match . group ( 2 ) if not username : error ( filename , linenum , 'readability/todo' , 2 , 'Missing username in TODO; it should look like ' '"// TODO(my_username): Stuff."' ) middle_whitespace = match . group ( 3 ) # Comparisons made explicit for correctness -- pylint: disable=g-explicit-bool-comparison if middle_whitespace != ' ' and middle_whitespace != '' : error ( filename , linenum , 'whitespace/todo' , 2 , 'TODO(my_username) should be followed by a space' ) # If the comment contains an alphanumeric character, there # should be a space somewhere between it and the // unless # it's a /// or //! Doxygen comment. if ( Match ( r'//[^ ]*\w' , comment ) and not Match ( r'(///|//\!)(\s+|$)' , comment ) ) : error ( filename , linenum , 'whitespace/comments' , 4 , 'Should have a space between // and comment' )
2285	def predict ( self , df_data , graph = None , * * kwargs ) : if graph is None : return self . create_graph_from_data ( df_data , * * kwargs ) elif isinstance ( graph , nx . DiGraph ) : return self . orient_directed_graph ( df_data , graph , * * kwargs ) elif isinstance ( graph , nx . Graph ) : return self . orient_undirected_graph ( df_data , graph , * * kwargs ) else : print ( 'Unknown Graph type' ) raise ValueError
6549	def fill_field ( self , ypos , xpos , tosend , length ) : if length < len ( tosend ) : raise FieldTruncateError ( 'length limit %d, but got "%s"' % ( length , tosend ) ) if xpos is not None and ypos is not None : self . move_to ( ypos , xpos ) self . delete_field ( ) self . send_string ( tosend )
9032	def _place_row ( self , row , position ) : self . _rows_in_grid [ row ] = RowInGrid ( row , position )
1798	def CMOVG ( cpu , dest , src ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . ZF == 0 , cpu . SF == cpu . OF ) , src . read ( ) , dest . read ( ) ) )
13314	def remove ( self ) : self . run_hook ( 'preremove' ) utils . rmtree ( self . path ) self . run_hook ( 'postremove' )
10942	def update_function ( self , param_vals ) : self . opt_obj . update_function ( param_vals ) return self . opt_obj . get_error ( )
7119	def filter_dict ( unfiltered , filter_keys ) : filtered = DotDict ( ) for k in filter_keys : filtered [ k ] = unfiltered [ k ] return filtered
4424	def get ( self , guild_id ) : if guild_id not in self . _players : p = self . _player ( lavalink = self . lavalink , guild_id = guild_id ) self . _players [ guild_id ] = p return self . _players [ guild_id ]
12223	def execute ( self , args , kwargs ) : return self . lookup_explicit ( args , kwargs ) ( * args , * * kwargs )
12444	def route ( self , request , response ) : # Ensure that we're allowed to use this HTTP method. self . require_http_allowed_method ( request ) # Retrieve the function corresponding to this HTTP method. function = getattr ( self , request . method . lower ( ) , None ) if function is None : # Server is not capable of supporting it. raise http . exceptions . NotImplemented ( ) # Delegate to the determined function to process the request. return function ( request , response )
9586	def write_numeric_array ( fd , header , array ) : # make a memory file for writing array data bd = BytesIO ( ) # write matrix header to memory file write_var_header ( bd , header ) if not isinstance ( array , basestring ) and header [ 'dims' ] [ 0 ] > 1 : # list array data in column major order array = list ( chain . from_iterable ( izip ( * array ) ) ) # write matrix data to memory file write_elements ( bd , header [ 'mtp' ] , array ) # write the variable to disk file data = bd . getvalue ( ) bd . close ( ) write_var_data ( fd , data )
10298	def get_incorrect_names ( graph : BELGraph ) -> Mapping [ str , Set [ str ] ] : return { namespace : get_incorrect_names_by_namespace ( graph , namespace ) for namespace in get_namespaces ( graph ) }
2033	def MSTORE ( self , address , value ) : if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . _allocate ( address , 32 ) self . _store ( address , value , 32 )
10644	def Sh ( L : float , h : float , D : float ) -> float : return h * L / D
6088	def scaled_noise_map_from_hyper_galaxies_and_contribution_maps ( contribution_maps , hyper_galaxies , noise_map ) : scaled_noise_maps = list ( map ( lambda hyper_galaxy , contribution_map : hyper_galaxy . hyper_noise_from_contributions ( noise_map = noise_map , contributions = contribution_map ) , hyper_galaxies , contribution_maps ) ) return noise_map + sum ( scaled_noise_maps )
9494	def compile ( code : list , consts : list , names : list , varnames : list , func_name : str = "<unknown, compiled>" , arg_count : int = 0 , kwarg_defaults : Tuple [ Any ] = ( ) , use_safety_wrapper : bool = True ) : varnames = tuple ( varnames ) consts = tuple ( consts ) names = tuple ( names ) # Flatten the code list. code = util . flatten ( code ) if arg_count > len ( varnames ) : raise CompileError ( "arg_count > len(varnames)" ) if len ( kwarg_defaults ) > len ( varnames ) : raise CompileError ( "len(kwarg_defaults) > len(varnames)" ) # Compile it. bc = compile_bytecode ( code ) dis . dis ( bc ) # Check for a final RETURN_VALUE. if PY36 : # TODO: Add Python 3.6 check pass else : if bc [ - 1 ] != tokens . RETURN_VALUE : raise CompileError ( "No default RETURN_VALUE. Add a `pyte.tokens.RETURN_VALUE` to the end of your " "bytecode if you don't need one." ) # Set default flags flags = 1 | 2 | 64 frame_data = inspect . stack ( ) [ 1 ] if sys . version_info [ 0 : 2 ] > ( 3 , 3 ) : # Validate the stack. stack_size = _simulate_stack ( dis . _get_instructions_bytes ( bc , constants = consts , names = names , varnames = varnames ) ) else : warnings . warn ( "Cannot check stack for safety." ) stack_size = 99 # Generate optimization warnings. _optimize_warn_pass ( dis . _get_instructions_bytes ( bc , constants = consts , names = names , varnames = varnames ) ) obb = types . CodeType ( arg_count , # Varnames - used for arguments. 0 , # Kwargs are not supported yet len ( varnames ) , # co_nlocals -> Non-argument local variables stack_size , # Auto-calculated flags , # 67 is default for a normal function. bc , # co_code - use the bytecode we generated. consts , # co_consts names , # co_names, used for global calls. varnames , # arguments frame_data [ 1 ] , # use <unknown, compiled> func_name , # co_name frame_data [ 2 ] , # co_firstlineno, ignore this. b'' , # https://svn.python.org/projects/python/trunk/Objects/lnotab_notes.txt ( ) , # freevars - no idea what this does ( ) # cellvars - used for nested functions - we don't use these. ) # Update globals f_globals = frame_data [ 0 ] . f_globals # Create a function type. f = types . FunctionType ( obb , f_globals ) f . __name__ = func_name f . __defaults__ = kwarg_defaults if use_safety_wrapper : def __safety_wrapper ( * args , * * kwargs ) : try : return f ( * args , * * kwargs ) except SystemError as e : if 'opcode' not in ' ' . join ( e . args ) : # Re-raise any non opcode related errors. raise msg = "Bytecode exception!" "\nFunction {} returned an invalid opcode." "\nFunction dissection:\n\n" . format ( f . __name__ ) # dis sucks and always prints to stdout # so we capture it file = io . StringIO ( ) with contextlib . redirect_stdout ( file ) : dis . dis ( f ) msg += file . getvalue ( ) raise SystemError ( msg ) from e returned_func = __safety_wrapper returned_func . wrapped = f else : returned_func = f # return the func return returned_func
7449	def combinefiles ( filepath ) : ## unpack seq files in filepath fastqs = glob . glob ( filepath ) firsts = [ i for i in fastqs if "_R1_" in i ] ## check names if not firsts : raise IPyradWarningExit ( "First read files names must contain '_R1_'." ) ## get paired reads seconds = [ ff . replace ( "_R1_" , "_R2_" ) for ff in firsts ] return zip ( firsts , seconds )
8834	def less ( a , b , * args ) : types = set ( [ type ( a ) , type ( b ) ] ) if float in types or int in types : try : a , b = float ( a ) , float ( b ) except TypeError : # NaN return False return a < b and ( not args or less ( b , * args ) )
2938	def deserialize_assign_list ( self , workflow , start_node ) : # Collect all information. assignments = [ ] for node in start_node . childNodes : if node . nodeType != minidom . Node . ELEMENT_NODE : continue if node . nodeName . lower ( ) == 'assign' : assignments . append ( self . deserialize_assign ( workflow , node ) ) else : _exc ( 'Unknown node: %s' % node . nodeName ) return assignments
3235	def list_buckets ( client = None , * * kwargs ) : buckets = client . list_buckets ( * * kwargs ) return [ b . __dict__ for b in buckets ]
526	def _inhibitColumnsLocal ( self , overlaps , density ) : activeArray = numpy . zeros ( self . _numColumns , dtype = "bool" ) for column , overlap in enumerate ( overlaps ) : if overlap >= self . _stimulusThreshold : neighborhood = self . _getColumnNeighborhood ( column ) neighborhoodOverlaps = overlaps [ neighborhood ] numBigger = numpy . count_nonzero ( neighborhoodOverlaps > overlap ) # When there is a tie, favor neighbors that are already selected as # active. ties = numpy . where ( neighborhoodOverlaps == overlap ) tiedNeighbors = neighborhood [ ties ] numTiesLost = numpy . count_nonzero ( activeArray [ tiedNeighbors ] ) numActive = int ( 0.5 + density * len ( neighborhood ) ) if numBigger + numTiesLost < numActive : activeArray [ column ] = True return activeArray . nonzero ( ) [ 0 ]
3507	def create_stoichiometric_matrix ( model , array_type = 'dense' , dtype = None ) : if array_type not in ( 'DataFrame' , 'dense' ) and not dok_matrix : raise ValueError ( 'Sparse matrices require scipy' ) if dtype is None : dtype = np . float64 array_constructor = { 'dense' : np . zeros , 'dok' : dok_matrix , 'lil' : lil_matrix , 'DataFrame' : np . zeros , } n_metabolites = len ( model . metabolites ) n_reactions = len ( model . reactions ) array = array_constructor [ array_type ] ( ( n_metabolites , n_reactions ) , dtype = dtype ) m_ind = model . metabolites . index r_ind = model . reactions . index for reaction in model . reactions : for metabolite , stoich in iteritems ( reaction . metabolites ) : array [ m_ind ( metabolite ) , r_ind ( reaction ) ] = stoich if array_type == 'DataFrame' : metabolite_ids = [ met . id for met in model . metabolites ] reaction_ids = [ rxn . id for rxn in model . reactions ] return pd . DataFrame ( array , index = metabolite_ids , columns = reaction_ids ) else : return array
9785	def bookmark ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : PolyaxonClient ( ) . build_job . bookmark ( user , project_name , _build ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not bookmark build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Build job bookmarked." )
10308	def barh ( d , plt , title = None ) : labels = sorted ( d , key = d . get ) index = range ( len ( labels ) ) plt . yticks ( index , labels ) plt . barh ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
12401	def satisfied_by_checked ( self , req ) : req_man = RequirementsManager ( [ req ] ) return any ( req_man . check ( * checked ) for checked in self . checked )
13114	def resolve_domains ( domains , disable_zone = False ) : dnsresolver = dns . resolver . Resolver ( ) ips = [ ] for domain in domains : print_notification ( "Resolving {}" . format ( domain ) ) try : result = dnsresolver . query ( domain , 'A' ) for a in result . response . answer [ 0 ] : ips . append ( str ( a ) ) if not disable_zone : ips . extend ( zone_transfer ( str ( a ) , domain ) ) except dns . resolver . NXDOMAIN as e : print_error ( e ) return ips
12286	def lookup ( username , reponame ) : mgr = plugins_get_mgr ( ) # XXX This should be generalized to all repo managers. repomgr = mgr . get ( what = 'repomanager' , name = 'git' ) repo = repomgr . lookup ( username = username , reponame = reponame ) return repo
8254	def _context ( self ) : tags1 = None for clr in self : overlap = [ ] if clr . is_black : name = "black" elif clr . is_white : name = "white" elif clr . is_grey : name = "grey" else : name = clr . nearest_hue ( primary = True ) if name == "orange" and clr . brightness < 0.6 : name = "brown" tags2 = context [ name ] if tags1 is None : tags1 = tags2 else : for tag in tags2 : if tag in tags1 : if tag not in overlap : overlap . append ( tag ) tags1 = overlap overlap . sort ( ) return overlap
2319	def autoset_settings ( set_var ) : try : devices = ast . literal_eval ( os . environ [ "CUDA_VISIBLE_DEVICES" ] ) if type ( devices ) != list and type ( devices ) != tuple : devices = [ devices ] if len ( devices ) != 0 : set_var . GPU = len ( devices ) set_var . NB_JOBS = len ( devices ) warnings . warn ( "Detecting CUDA devices : {}" . format ( devices ) ) except KeyError : set_var . GPU = check_cuda_devices ( ) set_var . NB_JOBS = set_var . GPU warnings . warn ( "Detecting {} CUDA devices." . format ( set_var . GPU ) ) if not set_var . GPU : warnings . warn ( "No GPU automatically detected. Setting SETTINGS.GPU to 0, " + "and SETTINGS.NB_JOBS to cpu_count." ) set_var . GPU = 0 set_var . NB_JOBS = multiprocessing . cpu_count ( ) return set_var
13509	def sloccount ( ) : # filter out subpackages setup = options . get ( 'setup' ) packages = options . get ( 'packages' ) if setup else None if packages : dirs = [ x for x in packages if '.' not in x ] else : dirs = [ '.' ] # sloccount has strange behaviour with directories, # can cause exception in hudson sloccount plugin. # Better to call it with file list ls = [ ] for d in dirs : ls += list ( path ( d ) . walkfiles ( ) ) #ls=list(set(ls)) files = ' ' . join ( ls ) param = options . paved . pycheck . sloccount . param sh ( 'sloccount {param} {files} | tee sloccount.sc' . format ( param = param , files = files ) )
3391	def prune_unused_metabolites ( cobra_model ) : output_model = cobra_model . copy ( ) inactive_metabolites = [ m for m in output_model . metabolites if len ( m . reactions ) == 0 ] output_model . remove_metabolites ( inactive_metabolites ) return output_model , inactive_metabolites
5058	def get_notification_subject_line ( course_name , template_configuration = None ) : stock_subject_template = _ ( 'You\'ve been enrolled in {course_name}!' ) default_subject_template = getattr ( settings , 'ENTERPRISE_ENROLLMENT_EMAIL_DEFAULT_SUBJECT_LINE' , stock_subject_template , ) if template_configuration is not None and template_configuration . subject_line : final_subject_template = template_configuration . subject_line else : final_subject_template = default_subject_template try : return final_subject_template . format ( course_name = course_name ) except KeyError : pass try : return default_subject_template . format ( course_name = course_name ) except KeyError : return stock_subject_template . format ( course_name = course_name )
9221	def mix ( self , color1 , color2 , weight = 50 , * args ) : if color1 and color2 : if isinstance ( weight , string_types ) : weight = float ( weight . strip ( '%' ) ) weight = ( ( weight / 100.0 ) * 2 ) - 1 rgb1 = self . _hextorgb ( color1 ) rgb2 = self . _hextorgb ( color2 ) alpha = 0 w1 = ( ( ( weight if weight * alpha == - 1 else weight + alpha ) / ( 1 + weight * alpha ) ) + 1 ) w1 = w1 / 2.0 w2 = 1 - w1 rgb = [ rgb1 [ 0 ] * w1 + rgb2 [ 0 ] * w2 , rgb1 [ 1 ] * w1 + rgb2 [ 1 ] * w2 , rgb1 [ 2 ] * w1 + rgb2 [ 2 ] * w2 , ] return self . _rgbatohex ( rgb ) raise ValueError ( 'Illegal color values' )
9779	def whoami ( ) : try : user = PolyaxonClient ( ) . auth . get_user ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load user info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) click . echo ( "\nUsername: {username}, Email: {email}\n" . format ( * * user . to_dict ( ) ) )
13446	def messages_from_response ( response ) : messages = [ ] if hasattr ( response , 'context' ) and response . context and 'messages' in response . context : messages = response . context [ 'messages' ] elif hasattr ( response , 'cookies' ) : # no "context" set-up or no messages item, check for message info in # the cookies morsel = response . cookies . get ( 'messages' ) if not morsel : return [ ] # use the decoder in the CookieStore to process and get a list of # messages from django . contrib . messages . storage . cookie import CookieStorage store = CookieStorage ( FakeRequest ( ) ) messages = store . _decode ( morsel . value ) else : return [ ] return [ ( m . message , m . level ) for m in messages ]
5014	def filter_queryset ( self , request , queryset , view ) : if not request . user . is_staff : filter_kwargs = { view . USER_ID_FILTER : request . user . id } queryset = queryset . filter ( * * filter_kwargs ) return queryset
5793	def _cert_details ( cert_pointer ) : data_pointer = None try : data_pointer = Security . SecCertificateCopyData ( cert_pointer ) der_cert = CFHelpers . cf_data_to_bytes ( data_pointer ) cert_hash = hashlib . sha1 ( der_cert ) . digest ( ) return ( der_cert , cert_hash ) finally : if data_pointer is not None : CoreFoundation . CFRelease ( data_pointer )
10764	def _random_token ( self , bits = 128 ) : alphabet = string . ascii_letters + string . digits + '-_' # alphabet length is 64, so each letter provides lg(64) = 6 bits num_letters = int ( math . ceil ( bits / 6.0 ) ) return '' . join ( random . choice ( alphabet ) for i in range ( num_letters ) )
3847	def parse_watermark_notification ( p ) : return WatermarkNotification ( conv_id = p . conversation_id . id , user_id = from_participantid ( p . sender_id ) , read_timestamp = from_timestamp ( p . latest_read_timestamp ) , )
6413	def ghmean ( nums ) : m_g = gmean ( nums ) m_h = hmean ( nums ) if math . isnan ( m_g ) or math . isnan ( m_h ) : return float ( 'nan' ) while round ( m_h , 12 ) != round ( m_g , 12 ) : m_g , m_h = ( m_g * m_h ) ** ( 1 / 2 ) , ( 2 * m_g * m_h ) / ( m_g + m_h ) return m_g
11155	def print_big_dir ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size_table [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size ) , p . abspath ) )
6755	def lenv ( self ) : _env = type ( env ) ( ) for _k , _v in six . iteritems ( env ) : if _k . startswith ( self . name + '_' ) : _env [ _k [ len ( self . name ) + 1 : ] ] = _v return _env
2666	def write ( self , buf , * * kwargs ) : self . i2c . writeto ( self . device_address , buf , * * kwargs ) if self . _debug : print ( "i2c_device.write:" , [ hex ( i ) for i in buf ] )
4941	def unlink_user ( self , enterprise_customer , user_email ) : try : existing_user = User . objects . get ( email = user_email ) # not capturing DoesNotExist intentionally to signal to view that link does not exist link_record = self . get ( enterprise_customer = enterprise_customer , user_id = existing_user . id ) link_record . delete ( ) if update_user : # Remove the SailThru flags for enterprise learner. update_user . delay ( sailthru_vars = { 'is_enterprise_learner' : False , 'enterprise_name' : None , } , email = user_email ) except User . DoesNotExist : # not capturing DoesNotExist intentionally to signal to view that link does not exist pending_link = PendingEnterpriseCustomerUser . objects . get ( enterprise_customer = enterprise_customer , user_email = user_email ) pending_link . delete ( ) LOGGER . info ( 'Enterprise learner {%s} successfully unlinked from Enterprise Customer {%s}' , user_email , enterprise_customer . name )
10654	def prepare_to_run ( self , clock , period_count ) : self . period_count = period_count self . _exec_year_end_datetime = clock . get_datetime_at_period_ix ( period_count ) self . _prev_year_end_datetime = clock . start_datetime self . _curr_year_end_datetime = clock . start_datetime + relativedelta ( years = 1 ) # Remove all the transactions del self . gl . transactions [ : ] for c in self . components : c . prepare_to_run ( clock , period_count ) self . negative_income_tax_total = 0
11309	def map_to_dictionary ( self , url , obj , * * kwargs ) : maxwidth = kwargs . get ( 'maxwidth' , None ) maxheight = kwargs . get ( 'maxheight' , None ) provider_url , provider_name = self . provider_from_url ( url ) mapping = { 'version' : '1.0' , 'url' : url , 'provider_name' : provider_name , 'provider_url' : provider_url , 'type' : self . resource_type } # a hook self . preprocess ( obj , mapping , * * kwargs ) # resize image if we have a photo, otherwise use the given maximums if self . resource_type == 'photo' and self . get_image ( obj ) : self . resize_photo ( obj , mapping , maxwidth , maxheight ) elif self . resource_type in ( 'video' , 'rich' , 'photo' ) : width , height = size_to_nearest ( maxwidth , maxheight , self . _meta . valid_sizes , self . _meta . force_fit ) mapping . update ( width = width , height = height ) # create a thumbnail if self . get_image ( obj ) : self . thumbnail ( obj , mapping ) # map attributes to the mapping dictionary. if the attribute is # a callable, it must have an argument signature of # (self, obj) for attr in ( 'title' , 'author_name' , 'author_url' , 'html' ) : self . map_attr ( mapping , attr , obj ) # fix any urls if 'url' in mapping : mapping [ 'url' ] = relative_to_full ( mapping [ 'url' ] , url ) if 'thumbnail_url' in mapping : mapping [ 'thumbnail_url' ] = relative_to_full ( mapping [ 'thumbnail_url' ] , url ) if 'html' not in mapping and mapping [ 'type' ] in ( 'video' , 'rich' ) : mapping [ 'html' ] = self . render_html ( obj , context = Context ( mapping ) ) # a hook self . postprocess ( obj , mapping , * * kwargs ) return mapping
4306	def play ( args ) : if args [ 0 ] . lower ( ) != "play" : args . insert ( 0 , "play" ) else : args [ 0 ] = "play" try : logger . info ( "Executing: %s" , " " . join ( args ) ) process_handle = subprocess . Popen ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) status = process_handle . wait ( ) if process_handle . stderr is not None : logger . info ( process_handle . stderr ) if status == 0 : return True else : logger . info ( "Play returned with error code %s" , status ) return False except OSError as error_msg : logger . error ( "OSError: Play failed! %s" , error_msg ) except TypeError as error_msg : logger . error ( "TypeError: %s" , error_msg ) return False
5893	def handle_upload ( self , request ) : if request . method != 'POST' : raise Http404 if request . is_ajax ( ) : try : filename = request . GET [ 'quillUploadFile' ] data = request is_raw = True except KeyError : return HttpResponseBadRequest ( "Invalid file upload." ) else : if len ( request . FILES ) != 1 : return HttpResponseBadRequest ( "Can only upload 1 file at a time." ) try : data = request . FILES [ 'quillUploadFile' ] filename = data . name is_raw = False except KeyError : return HttpResponseBadRequest ( 'Missing image `quillUploadFile`.' ) url = save_file ( data , filename , is_raw , default_storage ) response_data = { } response_data [ 'url' ] = url # Response content type needs to be text/html here or else # IE will try to download the file. return HttpResponse ( json . dumps ( response_data ) , content_type = "text/html; charset=utf-8" )
468	def sample_top ( a = None , top_k = 10 ) : if a is None : a = [ ] idx = np . argpartition ( a , - top_k ) [ - top_k : ] probs = a [ idx ] # tl.logging.info("new %f" % probs) probs = probs / np . sum ( probs ) choice = np . random . choice ( idx , p = probs ) return choice
8525	def log_callback ( wrapped_function ) : def debug_log ( message ) : """Helper to log an escaped version of the given message to DEBUG""" logger . debug ( message . encode ( 'unicode_escape' ) . decode ( ) ) @ functools . wraps ( wrapped_function ) def _wrapper ( parser , match , * * kwargs ) : func_name = wrapped_function . __name__ debug_log ( u'{func_name} <- {matched_string}' . format ( func_name = func_name , matched_string = match . group ( ) , ) ) try : result = wrapped_function ( parser , match , * * kwargs ) except IgnoredMatchException : debug_log ( u'{func_name} -> IGNORED' . format ( func_name = func_name ) ) raise debug_log ( u'{func_name} -> {result}' . format ( func_name = func_name , result = result , ) ) return result return _wrapper
1039	def column ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return column
8958	def walk ( self , * * kwargs ) : lead = '' if 'with_root' in kwargs and kwargs . pop ( 'with_root' ) : lead = self . root . rstrip ( os . sep ) + os . sep for base , dirs , files in os . walk ( self . root , * * kwargs ) : prefix = base [ len ( self . root ) : ] . lstrip ( os . sep ) bits = prefix . split ( os . sep ) if prefix else [ ] for dirname in dirs [ : ] : path = '/' . join ( bits + [ dirname ] ) inclusive = self . included ( path , is_dir = True ) if inclusive : yield lead + path + '/' elif inclusive is False : dirs . remove ( dirname ) for filename in files : path = '/' . join ( bits + [ filename ] ) if self . included ( path ) : yield lead + path
9254	def issue_line_with_user ( self , line , issue ) : if not issue . get ( "pull_request" ) or not self . options . author : return line if not issue . get ( "user" ) : line += u" (Null user)" elif self . options . username_as_tag : line += u" (@{0})" . format ( issue [ "user" ] [ "login" ] ) else : line += u" ([{0}]({1}))" . format ( issue [ "user" ] [ "login" ] , issue [ "user" ] [ "html_url" ] ) return line
12624	def recursive_dir_match ( folder_path , regex = '' ) : outlist = [ ] for root , dirs , files in os . walk ( folder_path ) : outlist . extend ( [ op . join ( root , f ) for f in dirs if re . match ( regex , f ) ] ) return outlist
11622	def _unrecognised ( chr ) : if options [ 'handleUnrecognised' ] == UNRECOGNISED_ECHO : return chr elif options [ 'handleUnrecognised' ] == UNRECOGNISED_SUBSTITUTE : return options [ 'substituteChar' ] else : raise ( KeyError , chr )
2232	def _register_numpy_extensions ( self ) : # system checks import numpy as np numpy_floating_types = ( np . float16 , np . float32 , np . float64 ) if hasattr ( np , 'float128' ) : # nocover numpy_floating_types = numpy_floating_types + ( np . float128 , ) @ self . add_iterable_check def is_object_ndarray ( data ) : # ndarrays of objects cannot be hashed directly. return isinstance ( data , np . ndarray ) and data . dtype . kind == 'O' @ self . register ( np . ndarray ) def hash_numpy_array ( data ) : """ Example: >>> import ubelt as ub >>> if not ub.modname_to_modpath('numpy'): ... raise pytest.skip() >>> import numpy as np >>> data_f32 = np.zeros((3, 3, 3), dtype=np.float64) >>> data_i64 = np.zeros((3, 3, 3), dtype=np.int64) >>> data_i32 = np.zeros((3, 3, 3), dtype=np.int32) >>> hash_f64 = _hashable_sequence(data_f32, types=True) >>> hash_i64 = _hashable_sequence(data_i64, types=True) >>> hash_i32 = _hashable_sequence(data_i64, types=True) >>> assert hash_i64 != hash_f64 >>> assert hash_i64 != hash_i32 """ if data . dtype . kind == 'O' : msg = 'directly hashing ndarrays with dtype=object is unstable' raise TypeError ( msg ) else : # tobytes() views the array in 1D (via ravel()) # encode the shape as well header = b'' . join ( _hashable_sequence ( ( len ( data . shape ) , data . shape ) ) ) dtype = b'' . join ( _hashable_sequence ( data . dtype . descr ) ) hashable = header + dtype + data . tobytes ( ) prefix = b'NDARR' return prefix , hashable @ self . register ( ( np . int64 , np . int32 , np . int16 , np . int8 ) + ( np . uint64 , np . uint32 , np . uint16 , np . uint8 ) ) def _hash_numpy_int ( data ) : return _convert_to_hashable ( int ( data ) ) @ self . register ( numpy_floating_types ) def _hash_numpy_float ( data ) : return _convert_to_hashable ( float ( data ) ) @ self . register ( np . random . RandomState ) def _hash_numpy_random_state ( data ) : """ Example: >>> import ubelt as ub >>> if not ub.modname_to_modpath('numpy'): ... raise pytest.skip() >>> import numpy as np >>> rng = np.random.RandomState(0) >>> _hashable_sequence(rng, types=True) """ hashable = b'' . join ( _hashable_sequence ( data . get_state ( ) ) ) prefix = b'RNG' return prefix , hashable
3060	def locked_get ( self ) : credential = self . _backend . locked_get ( self . _key ) if credential is not None : credential . set_store ( self ) return credential
13085	def set ( self , section , key , value ) : if not section in self . config : self . config . add_section ( section ) self . config . set ( section , key , value )
356	def save_ckpt ( sess = None , mode_name = 'model.ckpt' , save_dir = 'checkpoint' , var_list = None , global_step = None , printable = False ) : if sess is None : raise ValueError ( "session is None." ) if var_list is None : var_list = [ ] ckpt_file = os . path . join ( save_dir , mode_name ) if var_list == [ ] : var_list = tf . global_variables ( ) logging . info ( "[*] save %s n_params: %d" % ( ckpt_file , len ( var_list ) ) ) if printable : for idx , v in enumerate ( var_list ) : logging . info ( " param {:3}: {:15} {}" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) saver = tf . train . Saver ( var_list ) saver . save ( sess , ckpt_file , global_step = global_step )
8397	def gettrans ( t ) : obj = t # Make sure trans object is instantiated if isinstance ( obj , str ) : name = '{}_trans' . format ( obj ) obj = globals ( ) [ name ] ( ) if callable ( obj ) : obj = obj ( ) if isinstance ( obj , type ) : obj = obj ( ) if not isinstance ( obj , trans ) : raise ValueError ( "Could not get transform object." ) return obj
5633	def find_sections ( lines ) : sections = [ ] for line in lines : if is_heading ( line ) : sections . append ( get_heading ( line ) ) return sections
440	def print_layers ( self ) : for i , layer in enumerate ( self . all_layers ) : # logging.info(" layer %d: %s" % (i, str(layer))) logging . info ( " layer {:3}: {:20} {:15} {}" . format ( i , layer . name , str ( layer . get_shape ( ) ) , layer . dtype . name ) )
1026	def encode ( input , output , quotetabs , header = 0 ) : if b2a_qp is not None : data = input . read ( ) odata = b2a_qp ( data , quotetabs = quotetabs , header = header ) output . write ( odata ) return def write ( s , output = output , lineEnd = '\n' ) : # RFC 1521 requires that the line ending in a space or tab must have # that trailing character encoded. if s and s [ - 1 : ] in ' \t' : output . write ( s [ : - 1 ] + quote ( s [ - 1 ] ) + lineEnd ) elif s == '.' : output . write ( quote ( s ) + lineEnd ) else : output . write ( s + lineEnd ) prevline = None while 1 : line = input . readline ( ) if not line : break outline = [ ] # Strip off any readline induced trailing newline stripped = '' if line [ - 1 : ] == '\n' : line = line [ : - 1 ] stripped = '\n' # Calculate the un-length-limited encoded line for c in line : if needsquoting ( c , quotetabs , header ) : c = quote ( c ) if header and c == ' ' : outline . append ( '_' ) else : outline . append ( c ) # First, write out the previous line if prevline is not None : write ( prevline ) # Now see if we need any soft line breaks because of RFC-imposed # length limitations. Then do the thisline->prevline dance. thisline = EMPTYSTRING . join ( outline ) while len ( thisline ) > MAXLINESIZE : # Don't forget to include the soft line break `=' sign in the # length calculation! write ( thisline [ : MAXLINESIZE - 1 ] , lineEnd = '=\n' ) thisline = thisline [ MAXLINESIZE - 1 : ] # Write out the current line prevline = thisline # Write out the last line, without a trailing newline if prevline is not None : write ( prevline , lineEnd = stripped )
11931	def using ( context , alias ) : # An empty alias means look in the current widget set. if alias == '' : yield context else : try : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] except KeyError : raise template . TemplateSyntaxError ( 'No widget libraries loaded!' ) try : block_set = widgets [ alias ] except KeyError : raise template . TemplateSyntaxError ( 'No widget library loaded for alias: %r' % alias ) context . render_context . push ( ) context . render_context [ BLOCK_CONTEXT_KEY ] = block_set context . render_context [ WIDGET_CONTEXT_KEY ] = widgets yield context context . render_context . pop ( )
4632	def child ( self , offset256 ) : a = bytes ( self ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . add ( s )
7133	def add_icon ( icon_data , dest ) : with open ( os . path . join ( dest , "icon.png" ) , "wb" ) as f : f . write ( icon_data )
11857	def settings ( request ) : settings = Setting . objects . all ( ) . as_dict ( default = '' ) context = { 'SETTINGS' : settings , } return context
8548	def delete_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) , method = 'DELETE' ) return response
7082	def fourier_sinusoidal_func ( fourierparams , times , mags , errs ) : period , epoch , famps , fphases = fourierparams # figure out the order from the length of the Fourier param list forder = len ( famps ) # phase the times with this period iphase = ( times - epoch ) / period iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] # calculate all the individual terms of the series fseries = [ famps [ x ] * np . cos ( 2.0 * np . pi * x * phase + fphases [ x ] ) for x in range ( forder ) ] # this is the zeroth order coefficient - a constant equal to median mag modelmags = np . median ( mags ) # sum the series for fo in fseries : modelmags += fo return modelmags , phase , ptimes , pmags , perrs
3623	def __pre_delete_receiver ( self , instance , * * kwargs ) : logger . debug ( 'RECEIVE pre_delete FOR %s' , instance . __class__ ) self . delete_record ( instance )
1243	def sample_minibatch ( self , batch_size ) : pool_size = len ( self ) if pool_size == 0 : return [ ] delta_p = self . _memory [ 0 ] / batch_size chosen_idx = [ ] # if all priorities sum to ~0 choose randomly otherwise random sample if abs ( self . _memory [ 0 ] ) < util . epsilon : chosen_idx = np . random . randint ( self . _capacity - 1 , self . _capacity - 1 + len ( self ) , size = batch_size ) . tolist ( ) else : for i in xrange ( batch_size ) : lower = max ( i * delta_p , 0 ) upper = min ( ( i + 1 ) * delta_p , self . _memory [ 0 ] ) p = random . uniform ( lower , upper ) chosen_idx . append ( self . _sample_with_priority ( p ) ) return [ ( i , self . _memory [ i ] ) for i in chosen_idx ]
12274	def iso_reference_str2int ( n ) : n = n . upper ( ) numbers = [ ] for c in n : iso_reference_valid_char ( c ) if c in ISO_REFERENCE_VALID_NUMERIC : numbers . append ( c ) else : numbers . append ( str ( iso_reference_char2int ( c ) ) ) return int ( '' . join ( numbers ) )
5731	def advance_past_string_with_gdb_escapes ( self , chars_to_remove_gdb_escape = None ) : if chars_to_remove_gdb_escape is None : chars_to_remove_gdb_escape = [ '"' ] buf = "" while True : c = self . raw_text [ self . index ] self . index += 1 logging . debug ( "%s" , fmt_cyan ( c ) ) if c == "\\" : # We are on a backslash and there is another character after the backslash # to parse. Handle this case specially since gdb escaped it for us # Get the next char that is being escaped c2 = self . raw_text [ self . index ] self . index += 1 # only store the escaped character in the buffer; don't store the backslash # (don't leave it escaped) buf += c2 elif c == '"' : # Quote is closed. Exit (and don't include the end quote). break else : # capture this character, and keep capturing buf += c return buf
7702	def get_items_by_name ( self , name , case_sensitive = True ) : if not case_sensitive and name : name = name . lower ( ) result = [ ] for item in self . _items : if item . name == name : result . append ( item ) elif item . name is None : continue elif not case_sensitive and item . name . lower ( ) == name : result . append ( item ) return result
3320	def refresh ( self , token , timeout ) : assert token in self . _dict , "Lock must exist" assert timeout == - 1 or timeout > 0 if timeout < 0 or timeout > LockStorageDict . LOCK_TIME_OUT_MAX : timeout = LockStorageDict . LOCK_TIME_OUT_MAX self . _lock . acquire_write ( ) try : # Note: shelve dictionary returns copies, so we must reassign # values: lock = self . _dict [ token ] lock [ "timeout" ] = timeout lock [ "expire" ] = time . time ( ) + timeout self . _dict [ token ] = lock self . _flush ( ) finally : self . _lock . release ( ) return lock
13642	def check_confirmations_or_resend ( self , use_open_peers = False , * * kw ) : if self . confirmations ( ) == 0 : self . send ( use_open_peers , * * kw )
4049	def fulltext_item ( self , itemkey , * * kwargs ) : query_string = "/{t}/{u}/items/{itemkey}/fulltext" . format ( t = self . library_type , u = self . library_id , itemkey = itemkey ) return self . _build_query ( query_string )
8015	async def dispatch_downstream ( self , message , steam_name ) : handler = getattr ( self , get_handler_name ( message ) , None ) if handler : await handler ( message , stream_name = steam_name ) else : # if there is not handler then just pass the message further downstream. await self . base_send ( message )
12866	def startup ( self , app ) : self . database . init_async ( app . loop ) if not self . cfg . connection_manual : app . middlewares . insert ( 0 , self . _middleware )
11199	def _validate_fromutc_inputs ( f ) : @ wraps ( f ) def fromutc ( self , dt ) : if not isinstance ( dt , datetime ) : raise TypeError ( "fromutc() requires a datetime argument" ) if dt . tzinfo is not self : raise ValueError ( "dt.tzinfo is not self" ) return f ( self , dt ) return fromutc
12662	def load_mask ( image , allow_empty = True ) : img = check_img ( image , make_it_3d = True ) values = np . unique ( img . get_data ( ) ) if len ( values ) == 1 : # We accept a single value if it is not 0 (full true mask). if values [ 0 ] == 0 and not allow_empty : raise ValueError ( 'Given mask is invalid because it masks all data' ) elif len ( values ) == 2 : # If there are 2 different values, one of them must be 0 (background) if 0 not in values : raise ValueError ( 'Background of the mask must be represented with 0.' ' Given mask contains: {}.' . format ( values ) ) elif len ( values ) != 2 : # If there are more than 2 values, the mask is invalid raise ValueError ( 'Given mask is not made of 2 values: {}. ' 'Cannot interpret as true or false' . format ( values ) ) return nib . Nifti1Image ( as_ndarray ( get_img_data ( img ) , dtype = bool ) , img . get_affine ( ) , img . get_header ( ) )
533	def setParameter ( self , paramName , value ) : ( setter , getter ) = self . _getParameterMethods ( paramName ) if setter is None : import exceptions raise exceptions . Exception ( "setParameter -- parameter name '%s' does not exist in region %s of type %s" % ( paramName , self . name , self . type ) ) setter ( paramName , value )
12498	def xfm_atlas_to_functional ( atlas_filepath , anatbrain_filepath , meanfunc_filepath , atlas2anat_nonlin_xfm_filepath , is_atlas2anat_inverted , anat2func_lin_xfm_filepath , atlasinanat_out_filepath , atlasinfunc_out_filepath , interp = 'nn' , rewrite = True , parallel = False ) : if is_atlas2anat_inverted : # I already have the inverted fields I need anat_to_mni_nl_inv = atlas2anat_nonlin_xfm_filepath else : # I am creating the inverted fields then...need output file path: output_dir = op . abspath ( op . dirname ( atlasinanat_out_filepath ) ) ext = get_extension ( atlas2anat_nonlin_xfm_filepath ) anat_to_mni_nl_inv = op . join ( output_dir , remove_ext ( op . basename ( atlas2anat_nonlin_xfm_filepath ) ) + '_inv' + ext ) # setup the commands to be called invwarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'invwarp' ) applywarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'applywarp' ) fslsub_cmd = op . join ( '${FSLDIR}' , 'bin' , 'fsl_sub' ) # add fsl_sub before the commands if parallel : invwarp_cmd = fslsub_cmd + ' ' + invwarp_cmd applywarp_cmd = fslsub_cmd + ' ' + applywarp_cmd # create the inverse fields if rewrite or ( not is_atlas2anat_inverted and not op . exists ( anat_to_mni_nl_inv ) ) : log . debug ( 'Creating {}.\n' . format ( anat_to_mni_nl_inv ) ) cmd = invwarp_cmd + ' ' cmd += '-w {} ' . format ( atlas2anat_nonlin_xfm_filepath ) cmd += '-o {} ' . format ( anat_to_mni_nl_inv ) cmd += '-r {} ' . format ( anatbrain_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) # transform the atlas to anatomical space if rewrite or not op . exists ( atlasinanat_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinanat_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlas_filepath ) cmd += '--ref={} ' . format ( anatbrain_filepath ) cmd += '--warp={} ' . format ( anat_to_mni_nl_inv ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinanat_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) # transform the atlas to functional space if rewrite or not op . exists ( atlasinfunc_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinfunc_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlasinanat_out_filepath ) cmd += '--ref={} ' . format ( meanfunc_filepath ) cmd += '--premat={} ' . format ( anat2func_lin_xfm_filepath ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinfunc_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd )
4451	def search ( self , query ) : args , query = self . _mk_query_args ( query ) st = time . time ( ) res = self . redis . execute_command ( self . SEARCH_CMD , * args ) return Result ( res , not query . _no_content , duration = ( time . time ( ) - st ) * 1000.0 , has_payload = query . _with_payloads )
4670	def setKeys ( self , loadkeys ) : log . debug ( "Force setting of private keys. Not using the wallet database!" ) if isinstance ( loadkeys , dict ) : loadkeys = list ( loadkeys . values ( ) ) elif not isinstance ( loadkeys , ( list , set ) ) : loadkeys = [ loadkeys ] for wif in loadkeys : pub = self . publickey_from_wif ( wif ) self . store . add ( str ( wif ) , pub )
646	def generateVectors ( numVectors = 100 , length = 500 , activity = 50 ) : vectors = [ ] coinc = numpy . zeros ( length , dtype = 'int32' ) indexList = range ( length ) for i in xrange ( numVectors ) : coinc [ : ] = 0 coinc [ random . sample ( indexList , activity ) ] = 1 vectors . append ( coinc . copy ( ) ) return vectors
8333	def findPrevious ( self , name = None , attrs = { } , text = None , * * kwargs ) : return self . _findOne ( self . findAllPrevious , name , attrs , text , * * kwargs )
3136	def get ( self , * * queryparams ) : return self . _mc_client . _get ( url = self . _build_path ( ) , * * queryparams )
1313	def ControlFromPoint ( x : int , y : int ) -> Control : element = _AutomationClient . instance ( ) . IUIAutomation . ElementFromPoint ( ctypes . wintypes . POINT ( x , y ) ) return Control . CreateControlFromElement ( element )
9698	def deliveries ( self ) : key = make_key ( event = self . object . event , owner_name = self . object . owner . username , identifier = self . object . identifier ) return redis . lrange ( key , 0 , 20 )
8264	def swarm ( self , x , y , r = 100 ) : sc = _ctx . stroke ( 0 , 0 , 0 , 0 ) sw = _ctx . strokewidth ( 0 ) _ctx . push ( ) _ctx . transform ( _ctx . CORNER ) _ctx . translate ( x , y ) for i in _range ( r * 3 ) : clr = choice ( self ) . copy ( ) clr . alpha -= 0.5 * random ( ) _ctx . fill ( clr ) clr = choice ( self ) _ctx . stroke ( clr ) _ctx . strokewidth ( 10 * random ( ) ) _ctx . rotate ( 360 * random ( ) ) r2 = r * 0.5 * random ( ) _ctx . oval ( r * random ( ) , 0 , r2 , r2 ) _ctx . pop ( ) _ctx . strokewidth ( sw ) if sc is None : _ctx . nostroke ( ) else : _ctx . stroke ( sc )
9700	def worker ( wrapped , dkwargs , hash_value = None , * args , * * kwargs ) : if "event" not in dkwargs : msg = "djwebhooks.decorators.redis_hook requires an 'event' argument in the decorator." raise TypeError ( msg ) event = dkwargs [ 'event' ] if "owner" not in kwargs : msg = "djwebhooks.senders.redis_callable requires an 'owner' argument in the decorated function." raise TypeError ( msg ) owner = kwargs [ 'owner' ] if "identifier" not in kwargs : msg = "djwebhooks.senders.orm_callable requires an 'identifier' argument in the decorated function." raise TypeError ( msg ) identifier = kwargs [ 'identifier' ] senderobj = DjangoRQSenderable ( wrapped , dkwargs , hash_value , WEBHOOK_ATTEMPTS , * args , * * kwargs ) # Add the webhook object just so it's around # TODO - error handling if this can't be found senderobj . webhook_target = WebhookTarget . objects . get ( event = event , owner = owner , identifier = identifier ) # Get the target url and add it senderobj . url = senderobj . webhook_target . target_url # Get the payload. This overides the senderobj.payload property. senderobj . payload = senderobj . get_payload ( ) # Get the creator and add it to the payload. senderobj . payload [ 'owner' ] = getattr ( kwargs [ 'owner' ] , WEBHOOK_OWNER_FIELD ) # get the event and add it to the payload senderobj . payload [ 'event' ] = dkwargs [ 'event' ] return senderobj . send ( )
2136	def disassociate_notification_template ( self , workflow , notification_template , status ) : return self . _disassoc ( 'notification_templates_%s' % status , workflow , notification_template )
12711	def add_force ( self , force , relative = False , position = None , relative_position = None ) : b = self . ode_body if relative_position is not None : op = b . addRelForceAtRelPos if relative else b . addForceAtRelPos op ( force , relative_position ) elif position is not None : op = b . addRelForceAtPos if relative else b . addForceAtPos op ( force , position ) else : op = b . addRelForce if relative else b . addForce op ( force )
395	def log_weight ( probs , weights , name = 'log_weight' ) : with tf . variable_scope ( name ) : exp_v = tf . reduce_mean ( tf . log ( probs ) * weights ) return exp_v
6457	def dist ( src , tar , method = sim_levenshtein ) : if callable ( method ) : return 1 - method ( src , tar ) else : raise AttributeError ( 'Unknown distance function: ' + str ( method ) )
4038	def default_headers ( self ) : _headers = { "User-Agent" : "Pyzotero/%s" % __version__ , "Zotero-API-Version" : "%s" % __api_version__ , } if self . api_key : _headers [ "Authorization" ] = "Bearer %s" % self . api_key return _headers
3186	def create ( self , conversation_id , data ) : self . conversation_id = conversation_id if 'from_email' not in data : raise KeyError ( 'The conversation message must have a from_email' ) check_email ( data [ 'from_email' ] ) if 'read' not in data : raise KeyError ( 'The conversation message must have a read' ) if data [ 'read' ] not in [ True , False ] : raise TypeError ( 'The conversation message read must be True or False' ) response = self . _mc_client . _post ( url = self . _build_path ( conversation_id , 'messages' ) , data = data ) if response is not None : self . message_id = response [ 'id' ] else : self . message_id = None return response
2906	def _assign_new_thread_id ( self , recursive = True ) : self . __class__ . thread_id_pool += 1 self . thread_id = self . __class__ . thread_id_pool if not recursive : return self . thread_id for child in self : child . thread_id = self . thread_id return self . thread_id
6001	def pix_to_sub ( self ) : pix_to_sub = [ [ ] for _ in range ( self . pixels ) ] for regular_pixel , pix_pixel in enumerate ( self . sub_to_pix ) : pix_to_sub [ pix_pixel ] . append ( regular_pixel ) return pix_to_sub
9548	def add_unique_check ( self , key , code = UNIQUE_CHECK_FAILED , message = MESSAGES [ UNIQUE_CHECK_FAILED ] ) : if isinstance ( key , basestring ) : assert key in self . _field_names , 'unexpected field name: %s' % key else : for f in key : assert f in self . _field_names , 'unexpected field name: %s' % key t = key , code , message self . _unique_checks . append ( t )
4906	def delete_course_completion ( self , user_id , payload ) : # pylint: disable=unused-argument return self . _delete ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . completion_status_api_path ) , payload , self . COMPLETION_PROVIDER_SCOPE )
499	def _deleteRecordsFromKNN ( self , recordsToDelete ) : prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) idsToDelete = ( [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] ) nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
9333	def full_like ( array , value , dtype = None ) : shared = empty_like ( array , dtype ) shared [ : ] = value return shared
3508	def nullspace ( A , atol = 1e-13 , rtol = 0 ) : A = np . atleast_2d ( A ) u , s , vh = np . linalg . svd ( A ) tol = max ( atol , rtol * s [ 0 ] ) nnz = ( s >= tol ) . sum ( ) ns = vh [ nnz : ] . conj ( ) . T return ns
6693	def get_or_create_bucket ( self , name ) : from boto . s3 import connection if self . dryrun : print ( 'boto.connect_s3().create_bucket(%s)' % repr ( name ) ) else : conn = connection . S3Connection ( self . genv . aws_access_key_id , self . genv . aws_secret_access_key ) bucket = conn . create_bucket ( name ) return bucket
8332	def findNextSiblings ( self , name = None , attrs = { } , text = None , limit = None , * * kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextSiblingGenerator , * * kwargs )
11878	def getProcessOwner ( pid ) : try : ownerUid = os . stat ( '/proc/' + str ( pid ) ) . st_uid except : return None try : ownerName = pwd . getpwuid ( ownerUid ) . pw_name except : ownerName = None return { 'uid' : ownerUid , 'name' : ownerName }
13334	def home_resolver ( resolver , path ) : from . api import get_home_path path = unipath ( get_home_path ( ) , path ) if is_environment ( path ) : return VirtualEnvironment ( path ) raise ResolveError
12661	def convert_sav ( inputfile , outputfile = None , method = 'rpy2' , otype = 'csv' ) : assert ( os . path . isfile ( inputfile ) ) assert ( method == 'rpy2' or method == 'savread' ) if method == 'rpy2' : df = sav_to_pandas_rpy2 ( inputfile ) elif method == 'savread' : df = sav_to_pandas_savreader ( inputfile ) otype_exts = { 'csv' : '.csv' , 'hdf' : '.h5' , 'stata' : '.dta' , 'json' : '.json' , 'pickle' : '.pickle' , 'excel' : '.xls' , 'html' : '.html' } if outputfile is None : outputfile = inputfile . replace ( path ( inputfile ) . ext , '' ) outputfile = add_extension_if_needed ( outputfile , otype_exts [ otype ] ) if otype == 'csv' : df . to_csv ( outputfile ) elif otype == 'hdf' : df . to_hdf ( outputfile , os . path . basename ( outputfile ) ) elif otype == 'stata' : df . to_stata ( outputfile ) elif otype == 'json' : df . to_json ( outputfile ) elif otype == 'pickle' : df . to_pickle ( outputfile ) elif otype == 'excel' : df . to_excel ( outputfile ) elif otype == 'html' : df . to_html ( outputfile ) else : df . to_csv ( outputfile )
10855	def sphere_analytical_gaussian_trim ( dr , a , alpha = 0.2765 , cut = 1.6 ) : m = np . abs ( dr ) <= cut # only compute on the relevant scales rr = dr [ m ] t = - rr / ( alpha * np . sqrt ( 2 ) ) q = 0.5 * ( 1 + erf ( t ) ) - np . sqrt ( 0.5 / np . pi ) * ( alpha / ( rr + a + 1e-10 ) ) * np . exp ( - t * t ) # fill in the grid, inside the interpolation and outside where values are constant ans = 0 * dr ans [ m ] = q ans [ dr > cut ] = 0 ans [ dr < - cut ] = 1 return ans
8474	def getConfig ( self , section = None ) : data = { } if section is None : for s in self . config . sections ( ) : if '/' in s : # Subsection parent , _s = s . split ( '/' ) data [ parent ] [ _s ] = dict ( self . config . items ( s ) ) else : data [ s ] = dict ( self . config . items ( s ) ) else : # Only one section will be returned data = dict ( self . config . items ( section ) ) return data
7883	def _make_prefixed ( self , name , is_element , declared_prefixes , declarations ) : namespace , name = self . _split_qname ( name , is_element ) if namespace is None : prefix = None elif namespace in declared_prefixes : prefix = declared_prefixes [ namespace ] elif namespace in self . _prefixes : prefix = self . _prefixes [ namespace ] declarations [ namespace ] = prefix declared_prefixes [ namespace ] = prefix else : if is_element : prefix = None else : prefix = self . _make_prefix ( declared_prefixes ) declarations [ namespace ] = prefix declared_prefixes [ namespace ] = prefix if prefix : return prefix + u":" + name else : return name
8580	def create_server ( self , datacenter_id , server ) : data = json . dumps ( self . _create_server_dict ( server ) ) response = self . _perform_request ( url = '/datacenters/%s/servers' % ( datacenter_id ) , method = 'POST' , data = data ) return response
11866	def sum_out ( self , var , bn ) : vars = [ X for X in self . vars if X != var ] cpt = dict ( ( event_values ( e , vars ) , sum ( self . p ( extend ( e , var , val ) ) for val in bn . variable_values ( var ) ) ) for e in all_events ( vars , bn , { } ) ) return Factor ( vars , cpt )
13866	def fromtsms ( ts , tzin = None , tzout = None ) : if ts is None : return None when = datetime . utcfromtimestamp ( ts / 1000 ) . replace ( microsecond = ts % 1000 * 1000 ) when = when . replace ( tzinfo = tzin or utc ) return totz ( when , tzout )
3217	def get_subnets ( vpc , * * conn ) : subnets = describe_subnets ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , * * conn ) s_ids = [ ] for s in subnets : s_ids . append ( s [ "SubnetId" ] ) return s_ids
12146	def analyzeSingle ( abfFname ) : assert os . path . exists ( abfFname ) and abfFname . endswith ( ".abf" ) ABFfolder , ABFfname = os . path . split ( abfFname ) abfID = os . path . splitext ( ABFfname ) [ 0 ] IN = INDEX ( ABFfolder ) IN . analyzeABF ( abfID ) IN . scan ( ) IN . html_single_basic ( [ abfID ] , overwrite = True ) IN . html_single_plot ( [ abfID ] , overwrite = True ) IN . scan ( ) IN . html_index ( ) return
4142	def _numpy_cholesky ( A , B ) : L = numpy . linalg . cholesky ( A ) # A=L*numpy.transpose(L).conjugate() # Ly = b y = numpy . linalg . solve ( L , B ) # Ux = y x = numpy . linalg . solve ( L . transpose ( ) . conjugate ( ) , y ) return x , L
351	def download_file_from_google_drive ( ID , destination ) : def save_response_content ( response , destination , chunk_size = 32 * 1024 ) : total_size = int ( response . headers . get ( 'content-length' , 0 ) ) with open ( destination , "wb" ) as f : for chunk in tqdm ( response . iter_content ( chunk_size ) , total = total_size , unit = 'B' , unit_scale = True , desc = destination ) : if chunk : # filter out keep-alive new chunks f . write ( chunk ) def get_confirm_token ( response ) : for key , value in response . cookies . items ( ) : if key . startswith ( 'download_warning' ) : return value return None URL = "https://docs.google.com/uc?export=download" session = requests . Session ( ) response = session . get ( URL , params = { 'id' : ID } , stream = True ) token = get_confirm_token ( response ) if token : params = { 'id' : ID , 'confirm' : token } response = session . get ( URL , params = params , stream = True ) save_response_content ( response , destination )
13283	def _parse_whitespace_argument ( source , name ) : # First match the command name itself so that we find the argument # *after* the command command_pattern = r'\\(' + name + r')(?:[\s{[%])' command_match = re . search ( command_pattern , source ) if command_match is not None : # Trim `source` so we only look after the command source = source [ command_match . end ( 1 ) : ] # Find the whitespace-delimited argument itself. pattern = r'(?P<content>\S+)(?:[ %\t\n]+)' match = re . search ( pattern , source ) if match is None : message = ( 'When parsing {}, did not find whitespace-delimited command ' 'argument' ) raise CommandParserError ( message . format ( name ) ) content = match . group ( 'content' ) content . strip ( ) return content
9335	def copy ( a ) : shared = anonymousmemmap ( a . shape , dtype = a . dtype ) shared [ : ] = a [ : ] return shared
7961	def disconnect ( self ) : logger . debug ( "TCPTransport.disconnect()" ) with self . lock : if self . _socket is None : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) return if self . _hup or not self . _serializer : self . _close ( ) else : self . send_stream_tail ( )
1841	def JNO ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . OF , target . read ( ) , cpu . PC )
1575	def hex_escape ( bin_str ) : printable = string . ascii_letters + string . digits + string . punctuation + ' ' return '' . join ( ch if ch in printable else r'0x{0:02x}' . format ( ord ( ch ) ) for ch in bin_str )
2570	def send_UDP_message ( self , message ) : x = 0 if self . tracking_enabled : try : proc = udp_messenger ( self . domain_name , self . UDP_IP , self . UDP_PORT , self . sock_timeout , message ) self . procs . append ( proc ) except Exception as e : logger . debug ( "Usage tracking failed: {}" . format ( e ) ) else : x = - 1 return x
3120	def value_to_string ( self , obj ) : value = self . _get_val_from_obj ( obj ) return self . get_prep_value ( value )
4410	async def disconnect ( self ) : if not self . is_connected : return await self . stop ( ) ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , None )
7318	def parsemail ( raw_message ) : message = email . parser . Parser ( ) . parsestr ( raw_message ) # Detect encoding detected = chardet . detect ( bytearray ( raw_message , "utf-8" ) ) encoding = detected [ "encoding" ] print ( ">>> encoding {}" . format ( encoding ) ) for part in message . walk ( ) : if part . get_content_maintype ( ) == 'multipart' : continue part . set_charset ( encoding ) # Extract recipients addrs = email . utils . getaddresses ( message . get_all ( "TO" , [ ] ) ) + email . utils . getaddresses ( message . get_all ( "CC" , [ ] ) ) + email . utils . getaddresses ( message . get_all ( "BCC" , [ ] ) ) recipients = [ x [ 1 ] for x in addrs ] message . __delitem__ ( "bcc" ) message . __setitem__ ( 'Date' , email . utils . formatdate ( ) ) sender = message [ "from" ] return ( message , sender , recipients )
3559	def find_service ( self , uuid ) : for service in self . list_services ( ) : if service . uuid == uuid : return service return None
10276	def get_neurommsig_scores ( graph : BELGraph , genes : List [ Gene ] , annotation : str = 'Subgraph' , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None , preprocess : bool = False ) -> Optional [ Mapping [ str , float ] ] : if preprocess : graph = neurommsig_graph_preprocessor . run ( graph ) if not any ( gene in graph for gene in genes ) : logger . debug ( 'no genes mapping to graph' ) return subgraphs = get_subgraphs_by_annotation ( graph , annotation = annotation ) return get_neurommsig_scores_prestratified ( subgraphs = subgraphs , genes = genes , ora_weight = ora_weight , hub_weight = hub_weight , top_percent = top_percent , topology_weight = topology_weight , )
13329	def remove ( path ) : r = cpenv . resolve ( path ) if isinstance ( r . resolved [ 0 ] , cpenv . VirtualEnvironment ) : EnvironmentCache . discard ( r . resolved [ 0 ] ) EnvironmentCache . save ( )
10535	def get_categories ( limit = 20 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) try : res = _pybossa_req ( 'get' , 'category' , params = params ) if type ( res ) . __name__ == 'list' : return [ Category ( category ) for category in res ] else : raise TypeError except : raise
9069	def _df ( self ) : if not self . _restricted : return self . nsamples return self . nsamples - self . _X [ "tX" ] . shape [ 1 ]
4667	def refresh ( self ) : dict . __init__ ( self , self . blockchain . rpc . get_object ( self . identifier ) , blockchain_instance = self . blockchain , )
2073	def convert_input ( X ) : if not isinstance ( X , pd . DataFrame ) : if isinstance ( X , list ) : X = pd . DataFrame ( X ) elif isinstance ( X , ( np . generic , np . ndarray ) ) : X = pd . DataFrame ( X ) elif isinstance ( X , csr_matrix ) : X = pd . DataFrame ( X . todense ( ) ) elif isinstance ( X , pd . Series ) : X = pd . DataFrame ( X ) else : raise ValueError ( 'Unexpected input type: %s' % ( str ( type ( X ) ) ) ) X = X . apply ( lambda x : pd . to_numeric ( x , errors = 'ignore' ) ) return X
10780	def diffusion ( diffusion_constant = 0.2 , exposure_time = 0.05 , samples = 200 ) : radius = 5 psfsize = np . array ( [ 2.0 , 1.0 , 3.0 ] ) # create a base image of one particle s0 = init . create_single_particle_state ( imsize = 4 * radius , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) # add up a bunch of trajectories finalimage = 0 * s0 . get_model_image ( ) [ s0 . inner ] position = 0 * s0 . obj . pos [ 0 ] for i in xrange ( samples ) : offset = np . sqrt ( 6 * diffusion_constant * exposure_time ) * np . random . randn ( 3 ) s0 . obj . pos [ 0 ] = np . array ( s0 . image . shape ) / 2 + offset s0 . reset ( ) finalimage += s0 . get_model_image ( ) [ s0 . inner ] position += s0 . obj . pos [ 0 ] finalimage /= float ( samples ) position /= float ( samples ) # place that into a new image at the expected parameters s = init . create_single_particle_state ( imsize = 4 * radius , sigma = 0.05 , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) s . reset ( ) # measure the true inferred parameters return s , finalimage , position
3490	def _parse_annotations ( sbase ) : annotation = { } # SBO term if sbase . isSetSBOTerm ( ) : # FIXME: correct handling of annotations annotation [ "sbo" ] = sbase . getSBOTermID ( ) # RDF annotation cvterms = sbase . getCVTerms ( ) if cvterms is None : return annotation for cvterm in cvterms : # type: libsbml.CVTerm for k in range ( cvterm . getNumResources ( ) ) : # FIXME: read and store the qualifier uri = cvterm . getResourceURI ( k ) match = URL_IDENTIFIERS_PATTERN . match ( uri ) if not match : LOGGER . warning ( "%s does not conform to " "http(s)://identifiers.org/collection/id" , uri ) continue provider , identifier = match . group ( 1 ) , match . group ( 2 ) if provider in annotation : if isinstance ( annotation [ provider ] , string_types ) : annotation [ provider ] = [ annotation [ provider ] ] # FIXME: use a list if identifier not in annotation [ provider ] : annotation [ provider ] . append ( identifier ) else : # FIXME: always in list annotation [ provider ] = identifier return annotation
4388	def adsPortCloseEx ( port ) : # type: (int) -> None port_close_ex = _adsDLL . AdsPortCloseEx port_close_ex . restype = ctypes . c_long error_code = port_close_ex ( port ) if error_code : raise ADSError ( error_code )
1370	def get_subparser ( parser , command ) : # pylint: disable=protected-access subparsers_actions = [ action for action in parser . _actions if isinstance ( action , argparse . _SubParsersAction ) ] # there will probably only be one subparser_action, # but better save than sorry for subparsers_action in subparsers_actions : # get all subparsers for choice , subparser in subparsers_action . choices . items ( ) : if choice == command : return subparser return None
9078	def make_df_getter ( data_url : str , data_path : str , * * kwargs ) -> Callable [ [ Optional [ str ] , bool , bool ] , pd . DataFrame ] : download_function = make_downloader ( data_url , data_path ) def get_df ( url : Optional [ str ] = None , cache : bool = True , force_download : bool = False ) -> pd . DataFrame : """Get the data as a pandas DataFrame. :param url: The URL (or file path) to download. :param cache: If true, the data is downloaded to the file system, else it is loaded from the internet :param force_download: If true, overwrites a previously cached file """ if url is None and cache : url = download_function ( force_download = force_download ) return pd . read_csv ( url or data_url , * * kwargs ) return get_df
5083	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , grade = None , is_passing = False ) : completed_timestamp = None course_completed = False if completed_date is not None : completed_timestamp = parse_datetime_to_epoch_millis ( completed_date ) course_completed = is_passing sapsf_user_id = enterprise_enrollment . enterprise_customer_user . get_remote_id ( ) if sapsf_user_id is not None : SapSuccessFactorsLearnerDataTransmissionAudit = apps . get_model ( # pylint: disable=invalid-name 'sap_success_factors' , 'SapSuccessFactorsLearnerDataTransmissionAudit' ) # We return two records here, one with the course key and one with the course run id, to account for # uncertainty about the type of content (course vs. course run) that was sent to the integrated channel. return [ SapSuccessFactorsLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , sapsf_user_id = sapsf_user_id , course_id = parse_course_key ( enterprise_enrollment . course_id ) , course_completed = course_completed , completed_timestamp = completed_timestamp , grade = grade , ) , SapSuccessFactorsLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , sapsf_user_id = sapsf_user_id , course_id = enterprise_enrollment . course_id , course_completed = course_completed , completed_timestamp = completed_timestamp , grade = grade , ) , ] else : LOGGER . debug ( 'No learner data was sent for user [%s] because an SAP SuccessFactors user ID could not be found.' , enterprise_enrollment . enterprise_customer_user . username )
13468	def normalize_slice ( slice_obj , length ) : if isinstance ( slice_obj , slice ) : start , stop , step = slice_obj . start , slice_obj . stop , slice_obj . step if start is None : start = 0 if stop is None : stop = length if step is None : step = 1 if start < 0 : start += length if stop < 0 : stop += length elif isinstance ( slice_obj , int ) : start = slice_obj if start < 0 : start += length stop = start + 1 step = 1 else : raise TypeError if ( 0 <= start <= length ) and ( 0 <= stop <= length ) : return start , stop , step raise IndexError
2656	def makedirs ( self , path , mode = 511 , exist_ok = False ) : if exist_ok is False and self . isdir ( path ) : raise OSError ( 'Target directory {} already exists' . format ( path ) ) self . execute_wait ( 'mkdir -p {}' . format ( path ) ) self . sftp_client . chmod ( path , mode )
10698	def get ( self , key , default = None ) : if self . in_memory : return self . _memory_db . get ( key , default ) else : db = self . _read_file ( ) return db . get ( key , default )
9188	def admin_print_styles ( request ) : styles = [ ] # This fetches all recipes that have been used to successfully bake a # current book plus all default recipes that have not yet been used # as well as "bad" books that are not "current" state, but would otherwise # be the latest/current for that book with db_connect ( cursor_factory = DictCursor ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( """\ WITH latest AS (SELECT print_style, recipe, count(*), count(nullif(stateid, 1)) as bad FROM modules m WHERE portal_type = 'Collection' AND recipe IS NOT NULL AND ( baked IS NOT NULL OR ( baked IS NULL AND stateid not in (1,8) ) ) AND ARRAY [major_version, minor_version] = ( SELECT max(ARRAY[major_version,minor_version]) FROM modules where m.uuid= uuid) GROUP BY print_style, recipe ), defaults AS (SELECT print_style, fileid AS recipe FROM default_print_style_recipes d WHERE not exists (SELECT 1 FROM latest WHERE latest.recipe = d.fileid) ) SELECT coalesce(ps.print_style, '(custom)') as print_style, ps.title, coalesce(ps.recipe_type, 'web') as type, ps.revised, ps.tag, ps.commit_id, la.count, la.bad FROM latest la LEFT JOIN print_style_recipes ps ON la.print_style = ps.print_style AND la.recipe = ps.fileid UNION ALL SELECT ps.print_style, ps.title, ps.recipe_type, ps.revised, ps.tag, ps.commit_id, 0 AS count, 0 AS bad FROM defaults de JOIN print_style_recipes ps ON de.print_style = ps.print_style AND de.recipe = ps.fileid ORDER BY revised desc NULLS LAST, print_style """ ) for row in cursor . fetchall ( ) : styles . append ( { 'print_style' : row [ 'print_style' ] , 'title' : row [ 'title' ] , 'type' : row [ 'type' ] , 'revised' : row [ 'revised' ] , 'tag' : row [ 'tag' ] , 'commit_id' : row [ 'commit_id' ] , 'number' : row [ 'count' ] , 'bad' : row [ 'bad' ] , 'link' : request . route_path ( 'admin-print-style-single' , style = row [ 'print_style' ] ) } ) return { 'styles' : styles }
10904	def compare_data_model_residuals ( s , tile , data_vmin = 'calc' , data_vmax = 'calc' , res_vmin = - 0.1 , res_vmax = 0.1 , edgepts = 'calc' , do_imshow = True , data_cmap = plt . cm . bone , res_cmap = plt . cm . RdBu ) : # This could be modified to alpha the borderline... or to embiggen # the image and slice it more finely residuals = s . residuals [ tile . slicer ] . squeeze ( ) data = s . data [ tile . slicer ] . squeeze ( ) model = s . model [ tile . slicer ] . squeeze ( ) if data . ndim != 2 : raise ValueError ( 'tile does not give a 2D slice' ) im = np . zeros ( [ data . shape [ 0 ] , data . shape [ 1 ] , 4 ] ) if data_vmin == 'calc' : data_vmin = 0.5 * ( data . min ( ) + model . min ( ) ) if data_vmax == 'calc' : data_vmax = 0.5 * ( data . max ( ) + model . max ( ) ) #1. Get masks: upper_mask , center_mask , lower_mask = trisect_image ( im . shape , edgepts ) #2. Get colorbar'd images gm = data_cmap ( center_data ( model , data_vmin , data_vmax ) ) dt = data_cmap ( center_data ( data , data_vmin , data_vmax ) ) rs = res_cmap ( center_data ( residuals , res_vmin , res_vmax ) ) for a in range ( 4 ) : im [ : , : , a ] [ upper_mask ] = rs [ : , : , a ] [ upper_mask ] im [ : , : , a ] [ center_mask ] = gm [ : , : , a ] [ center_mask ] im [ : , : , a ] [ lower_mask ] = dt [ : , : , a ] [ lower_mask ] if do_imshow : return plt . imshow ( im ) else : return im
427	def augment_with_ngrams ( unigrams , unigram_vocab_size , n_buckets , n = 2 ) : def get_ngrams ( n ) : return list ( zip ( * [ unigrams [ i : ] for i in range ( n ) ] ) ) def hash_ngram ( ngram ) : bytes_ = array . array ( 'L' , ngram ) . tobytes ( ) hash_ = int ( hashlib . sha256 ( bytes_ ) . hexdigest ( ) , 16 ) return unigram_vocab_size + hash_ % n_buckets return unigrams + [ hash_ngram ( ngram ) for i in range ( 2 , n + 1 ) for ngram in get_ngrams ( i ) ]
12254	def list ( self , * args , * * kwargs ) : if kwargs . pop ( 'force' , None ) : headers = kwargs . get ( 'headers' , args [ 4 ] if len ( args ) > 4 else None ) or dict ( ) headers [ 'force' ] = True kwargs [ 'headers' ] = headers for key in super ( Bucket , self ) . list ( * args , * * kwargs ) : yield key else : prefix = kwargs . get ( 'prefix' , args [ 0 ] if args else '' ) for key in mimicdb . backend . smembers ( tpl . bucket % self . name ) : if key . startswith ( prefix ) : k = Key ( self , key ) meta = mimicdb . backend . hgetall ( tpl . key % ( self . name , key ) ) if meta : k . _load_meta ( meta [ 'size' ] , meta [ 'md5' ] ) yield k
7169	def remove_entity ( self , name ) : self . entities . remove ( name ) self . padaos . remove_entity ( name )
5225	def flatten ( iterable , maps = None , unique = False ) -> list : if iterable is None : return [ ] if maps is None : maps = dict ( ) if isinstance ( iterable , ( str , int , float ) ) : return [ maps . get ( iterable , iterable ) ] else : x = [ maps . get ( item , item ) for item in _to_gen_ ( iterable ) ] return list ( set ( x ) ) if unique else x
8878	def find_libname ( self , name ) : names = [ "{}.lib" , "lib{}.lib" , "{}lib.lib" ] names = [ n . format ( name ) for n in names ] dirs = self . get_library_dirs ( ) for d in dirs : for n in names : if exists ( join ( d , n ) ) : return n [ : - 4 ] msg = "Could not find the {} library." . format ( name ) raise ValueError ( msg )
8035	def summarize ( text , char_limit , sentence_filter = None , debug = False ) : debug_info = { } sents = list ( tools . sent_splitter_ja ( text ) ) words_list = [ # pulp variables should be utf-8 encoded w . encode ( 'utf-8' ) for s in sents for w in tools . word_segmenter_ja ( s ) ] tf = collections . Counter ( ) for words in words_list : for w in words : tf [ w ] += 1.0 if sentence_filter is not None : valid_indices = [ i for i , s in enumerate ( sents ) if sentence_filter ( s ) ] sents = [ sents [ i ] for i in valid_indices ] words_list = [ words_list [ i ] for i in valid_indices ] sent_ids = [ str ( i ) for i in range ( len ( sents ) ) ] # sentence id sent_id2len = dict ( ( id_ , len ( s ) ) for id_ , s in zip ( sent_ids , sents ) ) # c word_contain = dict ( ) # a for id_ , words in zip ( sent_ids , words_list ) : word_contain [ id_ ] = collections . defaultdict ( lambda : 0 ) for w in words : word_contain [ id_ ] [ w ] = 1 prob = pulp . LpProblem ( 'summarize' , pulp . LpMaximize ) # x sent_vars = pulp . LpVariable . dicts ( 'sents' , sent_ids , 0 , 1 , pulp . LpBinary ) # z word_vars = pulp . LpVariable . dicts ( 'words' , tf . keys ( ) , 0 , 1 , pulp . LpBinary ) # first, set objective function: sum(w*z) prob += pulp . lpSum ( [ tf [ w ] * word_vars [ w ] for w in tf ] ) # next, add constraints # limit summary length: sum(c*x) <= K prob += pulp . lpSum ( [ sent_id2len [ id_ ] * sent_vars [ id_ ] for id_ in sent_ids ] ) <= char_limit , 'lengthRequirement' # for each term, sum(a*x) <= z for w in tf : prob += pulp . lpSum ( [ word_contain [ id_ ] [ w ] * sent_vars [ id_ ] for id_ in sent_ids ] ) >= word_vars [ w ] , 'z:{}' . format ( w ) prob . solve ( ) # print("Status:", pulp.LpStatus[prob.status]) sent_indices = [ ] for v in prob . variables ( ) : # print v.name, "=", v.varValue if v . name . startswith ( 'sents' ) and v . varValue == 1 : sent_indices . append ( int ( v . name . split ( '_' ) [ - 1 ] ) ) return [ sents [ i ] for i in sent_indices ] , debug_info
6224	def _gl_look_at ( self , pos , target , up ) : z = vector . normalise ( pos - target ) x = vector . normalise ( vector3 . cross ( vector . normalise ( up ) , z ) ) y = vector3 . cross ( z , x ) translate = matrix44 . create_identity ( ) translate [ 3 ] [ 0 ] = - pos . x translate [ 3 ] [ 1 ] = - pos . y translate [ 3 ] [ 2 ] = - pos . z rotate = matrix44 . create_identity ( ) rotate [ 0 ] [ 0 ] = x [ 0 ] # -- X rotate [ 1 ] [ 0 ] = x [ 1 ] rotate [ 2 ] [ 0 ] = x [ 2 ] rotate [ 0 ] [ 1 ] = y [ 0 ] # -- Y rotate [ 1 ] [ 1 ] = y [ 1 ] rotate [ 2 ] [ 1 ] = y [ 2 ] rotate [ 0 ] [ 2 ] = z [ 0 ] # -- Z rotate [ 1 ] [ 2 ] = z [ 1 ] rotate [ 2 ] [ 2 ] = z [ 2 ] return matrix44 . multiply ( translate , rotate )
10372	def node_has_namespace ( node : BaseEntity , namespace : str ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns == namespace
5031	def get ( self , request , template_id , view_type ) : template = get_object_or_404 ( EnrollmentNotificationEmailTemplate , pk = template_id ) if view_type not in self . view_type_contexts : return HttpResponse ( status = 404 ) base_context = self . view_type_contexts [ view_type ] . copy ( ) base_context . update ( { 'user_name' : self . get_user_name ( request ) } ) return HttpResponse ( template . render_html_template ( base_context ) , content_type = 'text/html' )
5659	def _validate_danglers ( self ) : for query , warning in zip ( DANGLER_QUERIES , DANGLER_WARNINGS ) : dangler_count = self . gtfs . execute_custom_query ( query ) . fetchone ( ) [ 0 ] if dangler_count > 0 : if self . verbose : print ( str ( dangler_count ) + " " + warning ) self . warnings_container . add_warning ( warning , self . location , count = dangler_count )
8472	def setup ( ) : # # Check if dir is writable # if not os.access(AtomShieldsScanner.HOME, os.W_OK): # AtomShieldsScanner.HOME = os.path.expanduser("~/.atomshields") # AtomShieldsScanner.CHECKERS_DIR = os.path.join(AtomShieldsScanner.HOME, "checkers") # AtomShieldsScanner.REPORTS_DIR = os.path.join(AtomShieldsScanner.HOME, "reports") if not os . path . isdir ( AtomShieldsScanner . CHECKERS_DIR ) : os . makedirs ( AtomShieldsScanner . CHECKERS_DIR ) if not os . path . isdir ( AtomShieldsScanner . REPORTS_DIR ) : os . makedirs ( AtomShieldsScanner . REPORTS_DIR ) # Copy all checkers for f in AtomShieldsScanner . _getFiles ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , "checkers" ) , "*.py" ) : AtomShieldsScanner . installChecker ( f ) # Copy all reports for f in AtomShieldsScanner . _getFiles ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , "reports" ) , "*.py" ) : AtomShieldsScanner . installReport ( f ) AtomShieldsScanner . _executeMassiveMethod ( path = AtomShieldsScanner . CHECKERS_DIR , method = "install" , args = { } ) config_dir = os . path . dirname ( AtomShieldsScanner . CONFIG_PATH ) if not os . path . isdir ( config_dir ) : os . makedirs ( config_dir )
2128	def set_display_columns ( self , set_true = [ ] , set_false = [ ] ) : for i in range ( len ( self . fields ) ) : if self . fields [ i ] . name in set_true : self . fields [ i ] . display = True elif self . fields [ i ] . name in set_false : self . fields [ i ] . display = False
323	def gen_drawdown_table ( returns , top = 10 ) : df_cum = ep . cum_returns ( returns , 1.0 ) drawdown_periods = get_top_drawdowns ( returns , top = top ) df_drawdowns = pd . DataFrame ( index = list ( range ( top ) ) , columns = [ 'Net drawdown in %' , 'Peak date' , 'Valley date' , 'Recovery date' , 'Duration' ] ) for i , ( peak , valley , recovery ) in enumerate ( drawdown_periods ) : if pd . isnull ( recovery ) : df_drawdowns . loc [ i , 'Duration' ] = np . nan else : df_drawdowns . loc [ i , 'Duration' ] = len ( pd . date_range ( peak , recovery , freq = 'B' ) ) df_drawdowns . loc [ i , 'Peak date' ] = ( peak . to_pydatetime ( ) . strftime ( '%Y-%m-%d' ) ) df_drawdowns . loc [ i , 'Valley date' ] = ( valley . to_pydatetime ( ) . strftime ( '%Y-%m-%d' ) ) if isinstance ( recovery , float ) : df_drawdowns . loc [ i , 'Recovery date' ] = recovery else : df_drawdowns . loc [ i , 'Recovery date' ] = ( recovery . to_pydatetime ( ) . strftime ( '%Y-%m-%d' ) ) df_drawdowns . loc [ i , 'Net drawdown in %' ] = ( ( df_cum . loc [ peak ] - df_cum . loc [ valley ] ) / df_cum . loc [ peak ] ) * 100 df_drawdowns [ 'Peak date' ] = pd . to_datetime ( df_drawdowns [ 'Peak date' ] ) df_drawdowns [ 'Valley date' ] = pd . to_datetime ( df_drawdowns [ 'Valley date' ] ) df_drawdowns [ 'Recovery date' ] = pd . to_datetime ( df_drawdowns [ 'Recovery date' ] ) return df_drawdowns
9896	def uptime ( ) : if __boottime is not None : return time . time ( ) - __boottime return { 'amiga' : _uptime_amiga , 'aros12' : _uptime_amiga , 'beos5' : _uptime_beos , 'cygwin' : _uptime_linux , 'darwin' : _uptime_osx , 'haiku1' : _uptime_beos , 'linux' : _uptime_linux , 'linux-armv71' : _uptime_linux , 'linux2' : _uptime_linux , 'mac' : _uptime_mac , 'minix3' : _uptime_minix , 'riscos' : _uptime_riscos , 'sunos5' : _uptime_solaris , 'syllable' : _uptime_syllable , 'win32' : _uptime_windows , 'wince' : _uptime_windows } . get ( sys . platform , _uptime_bsd ) ( ) or _uptime_bsd ( ) or _uptime_plan9 ( ) or _uptime_linux ( ) or _uptime_windows ( ) or _uptime_solaris ( ) or _uptime_beos ( ) or _uptime_amiga ( ) or _uptime_riscos ( ) or _uptime_posix ( ) or _uptime_syllable ( ) or _uptime_mac ( ) or _uptime_osx ( )
8124	def draw_cornu_flat ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd ) : for j in range ( 0 , 100 ) : t = j * .01 s , c = eval_cornu ( t0 + t * ( t1 - t0 ) ) s *= flip s -= s0 c -= c0 #print '%', c, s x = c * cs - s * ss y = s * cs + c * ss print_pt ( x0 + x , y0 + y , cmd ) cmd = 'lineto' return cmd
8105	def callback ( self , * incoming ) : message = incoming [ 0 ] if message : address , command = message [ 0 ] , message [ 2 ] profile = self . get_profile ( address ) if profile is not None : try : getattr ( profile , command ) ( self , message ) except AttributeError : pass
5520	def check_codes ( self , expected_codes , received_code , info ) : if not any ( map ( received_code . matches , expected_codes ) ) : raise errors . StatusCodeError ( expected_codes , received_code , info )
5132	def generate_transition_matrix ( g , seed = None ) : g = _test_graph ( g ) if isinstance ( seed , numbers . Integral ) : np . random . seed ( seed ) nV = g . number_of_nodes ( ) mat = np . zeros ( ( nV , nV ) ) for v in g . nodes ( ) : ind = [ e [ 1 ] for e in sorted ( g . out_edges ( v ) ) ] deg = len ( ind ) if deg == 1 : mat [ v , ind ] = 1 elif deg > 1 : probs = np . ceil ( np . random . rand ( deg ) * 100 ) / 100. if np . isclose ( np . sum ( probs ) , 0 ) : probs [ np . random . randint ( deg ) ] = 1 mat [ v , ind ] = probs / np . sum ( probs ) return mat
6147	def freqz_cas ( sos , w ) : Ns , Mcol = sos . shape w , Hcas = signal . freqz ( sos [ 0 , : 3 ] , sos [ 0 , 3 : ] , w ) for k in range ( 1 , Ns ) : w , Htemp = signal . freqz ( sos [ k , : 3 ] , sos [ k , 3 : ] , w ) Hcas *= Htemp return w , Hcas
11231	def replace ( self , * * kwargs ) : new_kwargs = { "interval" : self . _interval , "count" : self . _count , "dtstart" : self . _dtstart , "freq" : self . _freq , "until" : self . _until , "wkst" : self . _wkst , "cache" : False if self . _cache is None else True } new_kwargs . update ( self . _original_rule ) new_kwargs . update ( kwargs ) return rrule ( * * new_kwargs )
9769	def delete ( ctx ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if not click . confirm ( "Are sure you want to delete job `{}`" . format ( _job ) ) : click . echo ( 'Existing without deleting job.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . job . delete_job ( user , project_name , _job ) # Purge caching JobManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Job `{}` was delete successfully" . format ( _job ) )
8841	def jsonLogic ( tests , data = None ) : # You've recursed to a primitive, stop! if tests is None or not isinstance ( tests , dict ) : return tests data = data or { } operator = list ( tests . keys ( ) ) [ 0 ] values = tests [ operator ] # Easy syntax for unary operators, like {"var": "x"} instead of strict # {"var": ["x"]} if not isinstance ( values , list ) and not isinstance ( values , tuple ) : values = [ values ] # Recursion! values = [ jsonLogic ( val , data ) for val in values ] if operator == 'var' : return get_var ( data , * values ) if operator == 'missing' : return missing ( data , * values ) if operator == 'missing_some' : return missing_some ( data , * values ) if operator not in operations : raise ValueError ( "Unrecognized operation %s" % operator ) return operations [ operator ] ( * values )
7657	def append ( self , time = None , duration = None , value = None , confidence = None ) : self . data . add ( Observation ( time = float ( time ) , duration = float ( duration ) , value = value , confidence = confidence ) )
12657	def merge_dict_of_lists ( adict , indices , pop_later = True , copy = True ) : def check_indices ( idxs , x ) : for i in chain ( * idxs ) : if i < 0 or i >= x : raise IndexError ( "Given indices are out of dict range." ) check_indices ( indices , len ( adict ) ) rdict = adict . copy ( ) if copy else adict dict_keys = list ( rdict . keys ( ) ) for i , j in zip ( * indices ) : rdict [ dict_keys [ i ] ] . extend ( rdict [ dict_keys [ j ] ] ) if pop_later : for i , j in zip ( * indices ) : rdict . pop ( dict_keys [ j ] , '' ) return rdict
895	def getPredictiveCells ( self ) : previousCell = None predictiveCells = [ ] for segment in self . activeSegments : if segment . cell != previousCell : predictiveCells . append ( segment . cell ) previousCell = segment . cell return predictiveCells
12764	def attach ( self , frame_no ) : assert not self . joints for label , j in self . channels . items ( ) : target = self . targets . get ( label ) if target is None : continue if self . visibility [ frame_no , j ] < 0 : continue if np . linalg . norm ( self . velocities [ frame_no , j ] ) > 10 : continue joint = ode . BallJoint ( self . world . ode_world , self . jointgroup ) joint . attach ( self . bodies [ label ] . ode_body , target . ode_body ) joint . setAnchor1Rel ( [ 0 , 0 , 0 ] ) joint . setAnchor2Rel ( self . offsets [ label ] ) joint . setParam ( ode . ParamCFM , self . cfms [ frame_no , j ] ) joint . setParam ( ode . ParamERP , self . erp ) joint . name = label self . joints [ label ] = joint self . _frame_no = frame_no
13223	def dinner ( self , message = "Dinner is served" , shout : bool = False ) : return self . helper . output ( message , shout )
11986	async def _upload_file ( self , full_path ) : rel_path = os . path . relpath ( full_path , self . folder ) key = s3_key ( os . path . join ( self . key , rel_path ) ) ct = self . content_types . get ( key . split ( '.' ) [ - 1 ] ) with open ( full_path , 'rb' ) as fp : file = fp . read ( ) try : await self . botocore . upload_file ( self . bucket , file , key = key , ContentType = ct ) except Exception as exc : LOGGER . error ( 'Could not upload "%s": %s' , key , exc ) self . failures [ key ] = self . all . pop ( full_path ) return size = self . all . pop ( full_path ) self . success [ key ] = size self . total_size += size percentage = 100 * ( 1 - len ( self . all ) / self . total_files ) message = '{0:.0f}% completed - uploaded "{1}" - {2}' . format ( percentage , key , convert_bytes ( size ) ) LOGGER . info ( message )
3645	def tradepileDelete ( self , trade_id ) : # item_id instead of trade_id? method = 'DELETE' url = 'trade/%s' % trade_id self . __request__ ( method , url ) # returns nothing # TODO: validate status code return True
22	def get_wrapper_by_name ( env , classname ) : currentenv = env while True : if classname == currentenv . class_name ( ) : return currentenv elif isinstance ( currentenv , gym . Wrapper ) : currentenv = currentenv . env else : raise ValueError ( "Couldn't find wrapper named %s" % classname )
11363	def download_file ( from_url , to_filename = None , chunk_size = 1024 * 8 , retry_count = 3 ) : if not to_filename : to_filename = get_temporary_file ( ) session = requests . Session ( ) adapter = requests . adapters . HTTPAdapter ( max_retries = retry_count ) session . mount ( from_url , adapter ) response = session . get ( from_url , stream = True ) with open ( to_filename , 'wb' ) as fd : for chunk in response . iter_content ( chunk_size ) : fd . write ( chunk ) return to_filename
12076	def frameAndSave ( abf , tag = "" , dataType = "plot" , saveAsFname = False , closeWhenDone = True ) : print ( "closeWhenDone" , closeWhenDone ) plt . tight_layout ( ) plt . subplots_adjust ( top = .93 , bottom = .07 ) plt . annotate ( tag , ( .01 , .99 ) , xycoords = 'figure fraction' , ha = 'left' , va = 'top' , family = 'monospace' , size = 10 , alpha = .5 ) msgBot = "%s [%s]" % ( abf . ID , abf . protocomment ) plt . annotate ( msgBot , ( .01 , .01 ) , xycoords = 'figure fraction' , ha = 'left' , va = 'bottom' , family = 'monospace' , size = 10 , alpha = .5 ) fname = tag . lower ( ) . replace ( " " , '_' ) + ".jpg" fname = dataType + "_" + fname plt . tight_layout ( ) if IMAGE_SAVE : abf . log . info ( "saving [%s]" , fname ) try : if saveAsFname : saveAs = os . path . abspath ( saveAsFname ) else : saveAs = os . path . abspath ( abf . outPre + fname ) if not os . path . exists ( abf . outFolder ) : os . mkdir ( abf . outFolder ) plt . savefig ( saveAs ) except Exception as E : abf . log . error ( "saving [%s] failed! 'pip install pillow'?" , fname ) print ( E ) if IMAGE_SHOW == True : if closeWhenDone == False : print ( "NOT SHOWING (because closeWhenDone==True and showing would mess things up)" ) else : abf . log . info ( "showing [%s]" , fname ) plt . show ( ) if closeWhenDone : print ( "closing figure" ) plt . close ( 'all' )
12311	def record ( self , localStreamName , pathToFile , * * kwargs ) : return self . protocol . execute ( 'record' , localStreamName = localStreamName , pathToFile = pathToFile , * * kwargs )
3045	def _refresh ( self , http ) : if not self . store : self . _do_refresh_request ( http ) else : self . store . acquire_lock ( ) try : new_cred = self . store . locked_get ( ) if ( new_cred and not new_cred . invalid and new_cred . access_token != self . access_token and not new_cred . access_token_expired ) : logger . info ( 'Updated access_token read from Storage' ) self . _updateFromCredential ( new_cred ) else : self . _do_refresh_request ( http ) finally : self . store . release_lock ( )
4263	def filter_nomedia ( album , settings = None ) : nomediapath = os . path . join ( album . src_path , ".nomedia" ) if os . path . isfile ( nomediapath ) : if os . path . getsize ( nomediapath ) == 0 : logger . info ( "Ignoring album '%s' because of present 0-byte " ".nomedia file" , album . name ) # subdirs have been added to the gallery already, remove them # there, too _remove_albums_with_subdirs ( album . gallery . albums , [ album . path ] ) try : os . rmdir ( album . dst_path ) except OSError as e : # directory was created and populated with images in a # previous run => keep it pass # cannot set albums => empty subdirs so that no albums are # generated album . subdirs = [ ] album . medias = [ ] else : with open ( nomediapath , "r" ) as nomediaFile : logger . info ( "Found a .nomedia file in %s, ignoring its " "entries" , album . name ) ignored = nomediaFile . read ( ) . split ( "\n" ) album . medias = [ media for media in album . medias if media . src_filename not in ignored ] album . subdirs = [ dirname for dirname in album . subdirs if dirname not in ignored ] # subdirs have been added to the gallery already, remove # them there, too _remove_albums_with_subdirs ( album . gallery . albums , ignored , album . path + os . path . sep )
9886	def _call_multi_fortran_z ( self , names , data_types , rec_nums , dim_sizes , input_type_code , func , epoch = False , data_offset = None , epoch16 = False ) : # isolate input type code variables from total supplied types idx , = np . where ( data_types == input_type_code ) if len ( idx ) > 0 : # read all data of a given type at once max_rec = rec_nums [ idx ] . max ( ) sub_names = np . array ( names ) [ idx ] sub_sizes = dim_sizes [ idx ] status , data = func ( self . fname , sub_names . tolist ( ) , sub_sizes , sub_sizes . sum ( ) , max_rec , len ( sub_names ) ) if status == 0 : # account for quirks of CDF data storage for certain types if data_offset is not None : data = data . astype ( int ) idx , idy , = np . where ( data < 0 ) data [ idx , idy ] += data_offset if epoch : # account for difference in seconds between # CDF epoch and python's epoch, leap year in there # (datetime(1971,1,2) - # datetime(1,1,1)).total_seconds()*1000 data -= 62167219200000 data = data . astype ( '<M8[ms]' ) if epoch16 : data [ 0 : : 2 , : ] -= 62167219200 data = data [ 0 : : 2 , : ] * 1E9 + data [ 1 : : 2 , : ] / 1.E3 data = data . astype ( 'datetime64[ns]' ) sub_sizes /= 2 # all data of a type has been loaded and tweaked as necessary # parse through returned array to break out the individual variables # as appropriate self . _process_return_multi_z ( data , sub_names , sub_sizes ) else : raise IOError ( fortran_cdf . statusreporter ( status ) )
5971	def MD ( dirname = 'MD' , * * kwargs ) : logger . info ( "[{dirname!s}] Setting up MD..." . format ( * * vars ( ) ) ) kwargs . setdefault ( 'struct' , 'MD_POSRES/md.gro' ) kwargs . setdefault ( 'qname' , 'MD_GMX' ) return _setup_MD ( dirname , * * kwargs )
5937	def transform_args ( self , * args , * * kwargs ) : options = [ ] for option , value in kwargs . items ( ) : if not option . startswith ( '-' ) : # heuristic for turning key=val pairs into options # (fails for commands such as 'find' -- then just use args) if len ( option ) == 1 : option = '-' + option # POSIX style else : option = '--' + option # GNU option if value is True : options . append ( option ) continue elif value is False : raise ValueError ( 'A False value is ambiguous for option {0!r}' . format ( option ) ) if option [ : 2 ] == '--' : options . append ( option + '=' + str ( value ) ) # GNU option else : options . extend ( ( option , str ( value ) ) ) # POSIX style return options + list ( args )
4773	def contains_only ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : extra = [ ] for i in self . val : if i not in items : extra . append ( i ) if extra : self . _err ( 'Expected <%s> to contain only %s, but did contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( extra ) ) ) missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to contain only %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
4052	def dump ( self , itemkey , filename = None , path = None ) : if not filename : filename = self . item ( itemkey ) [ "data" ] [ "filename" ] if path : pth = os . path . join ( path , filename ) else : pth = filename file = self . file ( itemkey ) if self . snapshot : self . snapshot = False pth = pth + ".zip" with open ( pth , "wb" ) as f : f . write ( file )
8883	def predict ( self , X ) : # Check is fit had been called check_is_fitted ( self , [ 'inverse_influence_matrix' ] ) # Check that X have correct shape X = check_array ( X ) return self . __find_leverages ( X , self . inverse_influence_matrix ) <= self . threshold_value
9843	def __tokenize ( self , string ) : for m in self . dx_regex . finditer ( string . strip ( ) ) : code = m . lastgroup text = m . group ( m . lastgroup ) tok = Token ( code , text ) if not tok . iscode ( 'WHITESPACE' ) : self . tokens . append ( tok )
12406	def serialize ( self , data = None ) : if data is not None and self . response is not None : # Set the content type. self . response [ 'Content-Type' ] = self . media_types [ 0 ] # Write the encoded and prepared data to the response. self . response . write ( data ) # Return the serialized data. # This has normally been transformed by a base class. return data
7562	def _run_qmc ( self , boot ) : ## build command self . _tmp = os . path . join ( self . dirs , ".tmptre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . _tmp ] ## run it proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : raise IPyradWarningExit ( res [ 1 ] ) ## parse tmp file written by qmc into a tree and rename it with open ( self . _tmp , 'r' ) as intree : tre = ete3 . Tree ( intree . read ( ) . strip ( ) ) names = tre . get_leaves ( ) for name in names : name . name = self . samples [ int ( name . name ) ] tmptre = tre . write ( format = 9 ) ## save the tree to file if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmptre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmptre ) ## save the file self . _save ( )
5727	def _get_responses_windows ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : try : self . gdb_process . stdout . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stdout . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stdout . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stdout" ) except IOError : pass try : self . gdb_process . stderr . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stderr . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stderr . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stderr" ) except IOError : pass if time . time ( ) > timeout_time_sec : break return responses
10581	def calculate ( self , * * state ) : super ( ) . calculate ( * * state ) mm_average = 0.0 for compound , molefraction in state [ "x" ] . items ( ) : mm_average += molefraction * mm ( compound ) mm_average /= 1000.0 return mm_average * state [ "P" ] / R / state [ "T" ]
4283	def generate_video ( source , outname , settings , options = None ) : logger = logging . getLogger ( __name__ ) # Don't transcode if source is in the required format and # has fitting datedimensions, copy instead. converter = settings [ 'video_converter' ] w_src , h_src = video_size ( source , converter = converter ) w_dst , h_dst = settings [ 'video_size' ] logger . debug ( 'Video size: %i, %i -> %i, %i' , w_src , h_src , w_dst , h_dst ) base , src_ext = splitext ( source ) base , dst_ext = splitext ( outname ) if dst_ext == src_ext and w_src <= w_dst and h_src <= h_dst : logger . debug ( 'Video is smaller than the max size, copying it instead' ) shutil . copy ( source , outname ) return # http://stackoverflow.com/questions/8218363/maintaining-ffmpeg-aspect-ratio # + I made a drawing on paper to figure this out if h_dst * w_src < h_src * w_dst : # biggest fitting dimension is height resize_opt = [ '-vf' , "scale=trunc(oh*a/2)*2:%i" % h_dst ] else : # biggest fitting dimension is width resize_opt = [ '-vf' , "scale=%i:trunc(ow/a/2)*2" % w_dst ] # do not resize if input dimensions are smaller than output dimensions if w_src <= w_dst and h_src <= h_dst : resize_opt = [ ] # Encoding options improved, thanks to # http://ffmpeg.org/trac/ffmpeg/wiki/vpxEncodingGuide cmd = [ converter , '-i' , source , '-y' ] # -y to overwrite output files if options is not None : cmd += options cmd += resize_opt + [ outname ] logger . debug ( 'Processing video: %s' , ' ' . join ( cmd ) ) check_subprocess ( cmd , source , outname )
9443	def call ( self , call_params ) : path = '/' + self . api_version + '/Call/' method = 'POST' return self . request ( path , method , call_params )
10590	def report ( self , format = ReportFormat . printout , output_path = None ) : rpt = GlsRpt ( self , output_path ) return rpt . render ( format )
1321	def Maximize ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : return self . ShowWindow ( SW . ShowMaximized , waitTime ) return False
1459	def load_pex ( path_to_pex , include_deps = True ) : abs_path_to_pex = os . path . abspath ( path_to_pex ) Log . debug ( "Add a pex to the path: %s" % abs_path_to_pex ) if abs_path_to_pex not in sys . path : sys . path . insert ( 0 , os . path . dirname ( abs_path_to_pex ) ) # add dependencies to path if include_deps : for dep in _get_deps_list ( abs_path_to_pex ) : to_join = os . path . join ( os . path . dirname ( abs_path_to_pex ) , dep ) if to_join not in sys . path : Log . debug ( "Add a new dependency to the path: %s" % dep ) sys . path . insert ( 0 , to_join ) Log . debug ( "Python path: %s" % str ( sys . path ) )
969	def _extractCallingMethodArgs ( ) : import inspect import copy callingFrame = inspect . stack ( ) [ 1 ] [ 0 ] argNames , _ , _ , frameLocalVarDict = inspect . getargvalues ( callingFrame ) argNames . remove ( "self" ) args = copy . copy ( frameLocalVarDict ) for varName in frameLocalVarDict : if varName not in argNames : args . pop ( varName ) return args
11654	def fit ( self , X , y = None , * * params ) : X = as_features ( X , stack = True ) self . transformer . fit ( X . stacked_features , y , * * params ) return self
6419	def dist ( self , src , tar , probs = None ) : if src == tar : return 0.0 if probs is None : # lacking a reasonable dictionary, train on the strings themselves self . _coder . train ( src + tar ) else : self . _coder . set_probs ( probs ) src_comp = self . _coder . encode ( src ) [ 1 ] tar_comp = self . _coder . encode ( tar ) [ 1 ] concat_comp = self . _coder . encode ( src + tar ) [ 1 ] concat_comp2 = self . _coder . encode ( tar + src ) [ 1 ] return ( min ( concat_comp , concat_comp2 ) - min ( src_comp , tar_comp ) ) / max ( src_comp , tar_comp )
3991	def _nginx_stream_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_proxy_string ( port_spec , bridge_ip ) ) server_string_spec += "\t }\n" return server_string_spec
4364	def encode ( data , json_dumps = default_json_dumps ) : payload = '' msg = str ( MSG_TYPES [ data [ 'type' ] ] ) if msg in [ '0' , '1' ] : # '1::' [path] [query] msg += '::' + data [ 'endpoint' ] if 'qs' in data and data [ 'qs' ] != '' : msg += ':' + data [ 'qs' ] elif msg == '2' : # heartbeat msg += '::' elif msg in [ '3' , '4' , '5' ] : # '3:' [id ('+')] ':' [endpoint] ':' [data] # '4:' [id ('+')] ':' [endpoint] ':' [json] # '5:' [id ('+')] ':' [endpoint] ':' [json encoded event] # The message id is an incremental integer, required for ACKs. # If the message id is followed by a +, the ACK is not handled by # socket.io, but by the user instead. if msg == '3' : payload = data [ 'data' ] if msg == '4' : payload = json_dumps ( data [ 'data' ] ) if msg == '5' : d = { } d [ 'name' ] = data [ 'name' ] if 'args' in data and data [ 'args' ] != [ ] : d [ 'args' ] = data [ 'args' ] payload = json_dumps ( d ) if 'id' in data : msg += ':' + str ( data [ 'id' ] ) if data [ 'ack' ] == 'data' : msg += '+' msg += ':' else : msg += '::' if 'endpoint' not in data : data [ 'endpoint' ] = '' if payload != '' : msg += data [ 'endpoint' ] + ':' + payload else : msg += data [ 'endpoint' ] elif msg == '6' : # '6:::' [id] '+' [data] msg += '::' + data . get ( 'endpoint' , '' ) + ':' + str ( data [ 'ackId' ] ) if 'args' in data and data [ 'args' ] != [ ] : msg += '+' + json_dumps ( data [ 'args' ] ) elif msg == '7' : # '7::' [endpoint] ':' [reason] '+' [advice] msg += ':::' if 'reason' in data and data [ 'reason' ] != '' : msg += str ( ERROR_REASONS [ data [ 'reason' ] ] ) if 'advice' in data and data [ 'advice' ] != '' : msg += '+' + str ( ERROR_ADVICES [ data [ 'advice' ] ] ) msg += data [ 'endpoint' ] # NoOp, used to close a poll after the polling duration time elif msg == '8' : msg += '::' return msg
3371	def get_solver_name ( mip = False , qp = False ) : if len ( solvers ) == 0 : raise SolverNotFound ( "no solvers installed" ) # Those lists need to be updated as optlang implements more solvers mip_order = [ "gurobi" , "cplex" , "glpk" ] lp_order = [ "glpk" , "cplex" , "gurobi" ] qp_order = [ "gurobi" , "cplex" ] if mip is False and qp is False : for solver_name in lp_order : if solver_name in solvers : return solver_name # none of them are in the list order - so return the first one return list ( solvers ) [ 0 ] elif qp : # mip does not yet matter for this determination for solver_name in qp_order : if solver_name in solvers : return solver_name raise SolverNotFound ( "no qp-capable solver found" ) else : for solver_name in mip_order : if solver_name in solvers : return solver_name raise SolverNotFound ( "no mip-capable solver found" )
9370	def person_inn ( ) : mask11 = [ 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] mask12 = [ 3 , 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] # get the 11th digit of the INN weighted11 = [ v * mask11 [ i ] for i , v in enumerate ( inn [ : - 2 ] ) ] inn [ 10 ] = sum ( weighted11 ) % 11 % 10 # get the 12th digit of the INN weighted12 = [ v * mask12 [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 11 ] = sum ( weighted12 ) % 11 % 10 return "" . join ( map ( str , inn ) )
6949	def jhk_to_sdssr ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSR_JHK , SDSSR_JH , SDSSR_JK , SDSSR_HK , SDSSR_J , SDSSR_H , SDSSR_K )
4605	def upgrade ( self ) : # pragma: no cover assert callable ( self . blockchain . upgrade_account ) return self . blockchain . upgrade_account ( account = self )
2782	def destroy ( self ) : return self . get_data ( "domains/%s/records/%s" % ( self . domain , self . id ) , type = DELETE , )
4563	def to_type_constructor ( value , python_path = None ) : if not value : return value if callable ( value ) : return { 'datatype' : value } value = to_type ( value ) typename = value . get ( 'typename' ) if typename : r = aliases . resolve ( typename ) try : value [ 'datatype' ] = importer . import_symbol ( r , python_path = python_path ) del value [ 'typename' ] except Exception as e : value [ '_exception' ] = e return value
6817	def create_local_renderer ( self ) : r = super ( ApacheSatchel , self ) . create_local_renderer ( ) # Dynamically set values based on target operating system. os_version = self . os_version apache_specifics = r . env . specifics [ os_version . type ] [ os_version . distro ] r . env . update ( apache_specifics ) return r
7083	def fourier_sinusoidal_residual ( fourierparams , times , mags , errs ) : modelmags , phase , ptimes , pmags , perrs = ( fourier_sinusoidal_func ( fourierparams , times , mags , errs ) ) # this is now a weighted residual taking into account the measurement err return ( pmags - modelmags ) / perrs
5438	def validate_submit_args_or_fail ( job_descriptor , provider_name , input_providers , output_providers , logging_providers ) : job_resources = job_descriptor . job_resources job_params = job_descriptor . job_params task_descriptors = job_descriptor . task_descriptors # Validate logging file provider. _validate_providers ( [ job_resources . logging ] , 'logging' , logging_providers , provider_name ) # Validate job input and output file providers _validate_providers ( job_params [ 'inputs' ] , 'input' , input_providers , provider_name ) _validate_providers ( job_params [ 'outputs' ] , 'output' , output_providers , provider_name ) # Validate input and output file providers. for task_descriptor in task_descriptors : _validate_providers ( task_descriptor . task_params [ 'inputs' ] , 'input' , input_providers , provider_name ) _validate_providers ( task_descriptor . task_params [ 'outputs' ] , 'output' , output_providers , provider_name )
9878	def _coincidences ( value_counts , value_domain , dtype = np . float64 ) : value_counts_matrices = value_counts . reshape ( value_counts . shape + ( 1 , ) ) pairable = np . maximum ( np . sum ( value_counts , axis = 1 ) , 2 ) diagonals = np . tile ( np . eye ( len ( value_domain ) ) , ( len ( value_counts ) , 1 , 1 ) ) * value_counts . reshape ( ( value_counts . shape [ 0 ] , 1 , value_counts . shape [ 1 ] ) ) unnormalized_coincidences = value_counts_matrices * value_counts_matrices . transpose ( ( 0 , 2 , 1 ) ) - diagonals return np . sum ( np . divide ( unnormalized_coincidences , ( pairable - 1 ) . reshape ( ( - 1 , 1 , 1 ) ) , dtype = dtype ) , axis = 0 )
6136	def add_model_string ( self , model_str , position = 1 , file_id = None ) : if file_id is None : file_id = self . make_unique_id ( 'inlined_input' ) ret_data = self . file_create ( File . from_string ( model_str , position , file_id ) ) return ret_data
6213	def load ( self ) : self . path = self . find_scene ( self . meta . path ) if not self . path : raise ValueError ( "Scene '{}' not found" . format ( self . meta . path ) ) self . scene = Scene ( self . path ) # Load gltf json file if self . path . suffix == '.gltf' : self . load_gltf ( ) # Load binary gltf file if self . path . suffix == '.glb' : self . load_glb ( ) self . meta . check_version ( ) self . meta . check_extensions ( self . supported_extensions ) self . load_images ( ) self . load_samplers ( ) self . load_textures ( ) self . load_materials ( ) self . load_meshes ( ) self . load_nodes ( ) self . scene . calc_scene_bbox ( ) self . scene . prepare ( ) return self . scene
2174	def token_from_fragment ( self , authorization_response ) : self . _client . parse_request_uri_response ( authorization_response , state = self . _state ) self . token = self . _client . token return self . token
8720	def operation_upload ( uploader , sources , verify , do_compile , do_file , do_restart ) : sources , destinations = destination_from_source ( sources ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : if do_compile : uploader . file_remove ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) uploader . write_file ( filename , dst , verify ) #init.lua is not allowed to be compiled if do_compile and dst != 'init.lua' : uploader . file_compile ( dst ) uploader . file_remove ( dst ) if do_file : uploader . file_do ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) elif do_file : uploader . file_do ( dst ) else : raise Exception ( 'Error preparing nodemcu for reception' ) else : raise Exception ( 'You must specify a destination filename for each file you want to upload.' ) if do_restart : uploader . node_restart ( ) log . info ( 'All done!' )
2335	def clr ( M , * * kwargs ) : R = np . zeros ( M . shape ) Id = [ [ 0 , 0 ] for i in range ( M . shape [ 0 ] ) ] for i in range ( M . shape [ 0 ] ) : mu_i = np . mean ( M [ i , : ] ) sigma_i = np . std ( M [ i , : ] ) Id [ i ] = [ mu_i , sigma_i ] for i in range ( M . shape [ 0 ] ) : for j in range ( i + 1 , M . shape [ 0 ] ) : z_i = np . max ( [ 0 , ( M [ i , j ] - Id [ i ] [ 0 ] ) / Id [ i ] [ 0 ] ] ) z_j = np . max ( [ 0 , ( M [ i , j ] - Id [ j ] [ 0 ] ) / Id [ j ] [ 0 ] ] ) R [ i , j ] = np . sqrt ( z_i ** 2 + z_j ** 2 ) R [ j , i ] = R [ i , j ] # Symmetric return R
6028	def neighbors_from_pixelization ( self , pixels , ridge_points ) : return pixelization_util . voronoi_neighbors_from_pixels_and_ridge_points ( pixels = pixels , ridge_points = np . asarray ( ridge_points ) )
6101	def intensities_from_grid_radii ( self , grid_radii ) : np . seterr ( all = 'ignore' ) return np . multiply ( self . intensity , np . exp ( np . multiply ( - self . sersic_constant , np . add ( np . power ( np . divide ( grid_radii , self . effective_radius ) , 1. / self . sersic_index ) , - 1 ) ) ) )
12788	def load ( self , reload = False , require_load = False ) : # type: (bool, bool) -> None if reload : # pragma: no cover self . config = None # only load the config if necessary (or explicitly requested) if self . config : # pragma: no cover self . _log . debug ( 'Returning cached config instance. Use ' '``reload=True`` to avoid caching!' ) return path = self . _effective_path ( ) config_filename = self . _effective_filename ( ) # Next, use the resolved path to find the filenames. Keep track of # which files we loaded in order to inform the user. self . _active_path = [ join ( _ , config_filename ) for _ in path ] for dirname in path : conf_name = join ( dirname , config_filename ) readable = self . check_file ( conf_name ) if readable : action = 'Updating' if self . _loaded_files else 'Loading initial' self . _log . info ( '%s config from %s' , action , conf_name ) self . read ( conf_name ) if conf_name == expanduser ( "~/.%s/%s/%s" % ( self . group_name , self . app_name , self . filename ) ) : self . _log . warning ( "DEPRECATION WARNING: The file " "'%s/.%s/%s/app.ini' was loaded. The XDG " "Basedir standard requires this file to be in " "'%s/.config/%s/%s/app.ini'! This location " "will no longer be parsed in a future version of " "config_resolver! You can already (and should) move " "the file!" , expanduser ( "~" ) , self . group_name , self . app_name , expanduser ( "~" ) , self . group_name , self . app_name ) self . _loaded_files . append ( conf_name ) if not self . _loaded_files and not require_load : self . _log . warning ( "No config file named %s found! Search path was %r" , config_filename , path ) elif not self . _loaded_files and require_load : raise IOError ( "No config file named %s found! Search path " "was %r" % ( config_filename , path ) )
5021	def get_enterprise_customer_from_catalog_id ( catalog_id ) : try : return str ( EnterpriseCustomerCatalog . objects . get ( pk = catalog_id ) . enterprise_customer . uuid ) except EnterpriseCustomerCatalog . DoesNotExist : return None
7819	def dispatch ( self , block = False , timeout = None ) : logger . debug ( " dispatching..." ) try : event = self . queue . get ( block , timeout ) except Queue . Empty : logger . debug ( " queue empty" ) return None try : logger . debug ( " event: {0!r}" . format ( event ) ) if event is QUIT : return QUIT handlers = list ( self . _handler_map [ None ] ) klass = event . __class__ if klass in self . _handler_map : handlers += self . _handler_map [ klass ] logger . debug ( " handlers: {0!r}" . format ( handlers ) ) # to restore the original order of handler objects handlers . sort ( key = lambda x : x [ 0 ] ) for dummy , handler in handlers : logger . debug ( u" passing the event to: {0!r}" . format ( handler ) ) result = handler ( event ) if isinstance ( result , Event ) : self . queue . put ( result ) elif result and event is not QUIT : return event return event finally : self . queue . task_done ( )
7163	def answer_display ( self , s = '' ) : padding = len ( max ( self . questions . keys ( ) , key = len ) ) + 5 for key in list ( self . answers . keys ( ) ) : s += '{:>{}} : {}\n' . format ( key , padding , self . answers [ key ] ) return s
2182	def rebuild_auth ( self , prepared_request , response ) : if "Authorization" in prepared_request . headers : # If we get redirected to a new host, we should strip out # any authentication headers. prepared_request . headers . pop ( "Authorization" , True ) prepared_request . prepare_auth ( self . auth ) return
12519	def mask ( self , image ) : if image is None : self . _mask = None try : mask = load_mask ( image ) except Exception as exc : raise Exception ( 'Could not load mask image {}.' . format ( image ) ) from exc else : self . _mask = mask
4350	def vol ( self , gain , gain_type = 'amplitude' , limiter_gain = None ) : if not is_number ( gain ) : raise ValueError ( 'gain must be a number.' ) if limiter_gain is not None : if ( not is_number ( limiter_gain ) or limiter_gain <= 0 or limiter_gain >= 1 ) : raise ValueError ( 'limiter gain must be a positive number less than 1' ) if gain_type in [ 'amplitude' , 'power' ] and gain < 0 : raise ValueError ( "If gain_type = amplitude or power, gain must be positive." ) effect_args = [ 'vol' ] effect_args . append ( '{:f}' . format ( gain ) ) if gain_type == 'amplitude' : effect_args . append ( 'amplitude' ) elif gain_type == 'power' : effect_args . append ( 'power' ) elif gain_type == 'db' : effect_args . append ( 'dB' ) else : raise ValueError ( 'gain_type must be one of amplitude power or db' ) if limiter_gain is not None : if gain_type in [ 'amplitude' , 'power' ] and gain > 1 : effect_args . append ( '{:f}' . format ( limiter_gain ) ) elif gain_type == 'db' and gain > 0 : effect_args . append ( '{:f}' . format ( limiter_gain ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'vol' ) return self
11988	async def connect ( self ) : if not self . _consumer : waiter = self . _waiter = asyncio . Future ( ) try : address = self . _websocket_host ( ) self . logger . info ( 'Connect to %s' , address ) self . _consumer = await self . http . get ( address ) if self . _consumer . status_code != 101 : raise PusherError ( "Could not connect to websocket" ) except Exception as exc : waiter . set_exception ( exc ) raise else : await waiter return self . _consumer
5063	def get_course_track_selection_url ( course_run , query_parameters ) : try : course_root = reverse ( 'course_modes_choose' , kwargs = { 'course_id' : course_run [ 'key' ] } ) except KeyError : LOGGER . exception ( "KeyError while parsing course run data.\nCourse Run: \n[%s]" , course_run , ) raise url = '{}{}' . format ( settings . LMS_ROOT_URL , course_root ) course_run_url = update_query_parameters ( url , query_parameters ) return course_run_url
6226	def rot_state ( self , x , y ) : if self . last_x is None : self . last_x = x if self . last_y is None : self . last_y = y x_offset = self . last_x - x y_offset = self . last_y - y self . last_x = x self . last_y = y x_offset *= self . mouse_sensitivity y_offset *= self . mouse_sensitivity self . yaw -= x_offset self . pitch += y_offset if self . pitch > 85.0 : self . pitch = 85.0 if self . pitch < - 85.0 : self . pitch = - 85.0 self . _update_yaw_and_pitch ( )
11813	def present_results ( self , query_text , n = 10 ) : self . present ( self . query ( query_text , n ) )
9893	def _uptime_plan9 ( ) : # Apparently Plan 9 only has Python 2.2, which I'm not prepared to # support. Maybe some Linuxes implement /dev/time, though, someone was # talking about it somewhere. try : # The time file holds one 32-bit number representing the sec- # onds since start of epoch and three 64-bit numbers, repre- # senting nanoseconds since start of epoch, clock ticks, and # clock frequency. # -- cons(3) f = open ( '/dev/time' , 'r' ) s , ns , ct , cf = f . read ( ) . split ( ) f . close ( ) return float ( ct ) / float ( cf ) except ( IOError , ValueError ) : return None
8031	def compareChunks ( handles , chunk_size = CHUNK_SIZE ) : chunks = [ ( path , fh , fh . read ( chunk_size ) ) for path , fh , _ in handles ] more , done = [ ] , [ ] # While there are combinations not yet tried... while chunks : # Compare the first chunk to all successive chunks matches , non_matches = [ chunks [ 0 ] ] , [ ] for chunk in chunks [ 1 : ] : if matches [ 0 ] [ 2 ] == chunk [ 2 ] : matches . append ( chunk ) else : non_matches . append ( chunk ) # Check for EOF or obviously unique files if len ( matches ) == 1 or matches [ 0 ] [ 2 ] == "" : for x in matches : x [ 1 ] . close ( ) done . append ( [ x [ 0 ] for x in matches ] ) else : more . append ( matches ) chunks = non_matches return more , done
12875	def not_followed_by ( parser ) : @ tri def not_followed_by_block ( ) : failed = object ( ) result = optional ( tri ( parser ) , failed ) if result != failed : fail ( [ "not " + _fun_to_str ( parser ) ] ) choice ( not_followed_by_block )
11936	def display ( self ) : if not self . is_group ( ) : return self . _display return ( ( force_text ( k ) , v ) for k , v in self . _display )
405	def pixel_wise_softmax ( x , name = 'pixel_wise_softmax' ) : with tf . name_scope ( name ) : return tf . nn . softmax ( x )
4188	def window_riemann ( N ) : n = linspace ( - N / 2. , ( N ) / 2. , N ) w = sin ( n / float ( N ) * 2. * pi ) / ( n / float ( N ) * 2. * pi ) return w
5584	def prepare_path ( self , tile ) : makedirs ( os . path . dirname ( self . get_path ( tile ) ) )
10725	def _variant_levels ( level , variant ) : return ( level + variant , level + variant ) if variant != 0 else ( variant , level )
13402	def selectedLogs ( self ) : mcclogs = [ ] physlogs = [ ] for i in range ( len ( self . logMenus ) ) : logType = self . logMenus [ i ] . selectedType ( ) log = self . logMenus [ i ] . selectedProgram ( ) if logType == "MCC" : if log not in mcclogs : mcclogs . append ( log ) elif logType == "Physics" : if log not in physlogs : physlogs . append ( log ) return mcclogs , physlogs
4000	def _mount_repo ( repo , wait_for_server = False ) : check_call_on_vm ( 'sudo mkdir -p {}' . format ( repo . vm_path ) ) if wait_for_server : for i in range ( 0 , 10 ) : try : _run_mount_command ( repo ) return except CalledProcessError as e : if 'Connection refused' in e . output : logging . info ( 'Failed to mount repo; waiting for nfsd to restart' ) time . sleep ( 1 ) else : logging . info ( e . output ) raise e log_to_client ( 'Failed to mount repo {}' . format ( repo . short_name ) ) raise RuntimeError ( 'Unable to mount repo with NFS' ) else : _run_mount_command ( repo )
339	def log_every_n ( level , msg , n , * args ) : count = _GetNextLogCountPerToken ( _GetFileAndLine ( ) ) log_if ( level , msg , not ( count % n ) , * args )
7114	def fit ( self , X , y ) : #################### # Data Loader #################### word_vector_transformer = WordVectorTransformer ( padding = 'max' ) X = word_vector_transformer . fit_transform ( X ) X = LongTensor ( X ) self . word_vector_transformer = word_vector_transformer y_transformer = LabelEncoder ( ) y = y_transformer . fit_transform ( y ) y = torch . from_numpy ( y ) self . y_transformer = y_transformer dataset = CategorizedDataset ( X , y ) dataloader = DataLoader ( dataset , batch_size = self . batch_size , shuffle = True , num_workers = 4 ) #################### # Model #################### KERNEL_SIZES = self . kernel_sizes NUM_KERNEL = self . num_kernel EMBEDDING_DIM = self . embedding_dim model = TextCNN ( vocab_size = word_vector_transformer . get_vocab_size ( ) , embedding_dim = EMBEDDING_DIM , output_size = len ( self . y_transformer . classes_ ) , kernel_sizes = KERNEL_SIZES , num_kernel = NUM_KERNEL ) if USE_CUDA : model = model . cuda ( ) #################### # Train #################### EPOCH = self . epoch LR = self . lr loss_function = nn . CrossEntropyLoss ( ) optimizer = optim . Adam ( model . parameters ( ) , lr = LR ) for epoch in range ( EPOCH ) : losses = [ ] for i , data in enumerate ( dataloader ) : X , y = data X , y = Variable ( X ) , Variable ( y ) optimizer . zero_grad ( ) model . train ( ) output = model ( X ) loss = loss_function ( output , y ) losses . append ( loss . data . tolist ( ) [ 0 ] ) loss . backward ( ) optimizer . step ( ) if i % 100 == 0 : print ( "[%d/%d] mean_loss : %0.2f" % ( epoch , EPOCH , np . mean ( losses ) ) ) losses = [ ] self . model = model
1684	def Begin ( self , function_name ) : self . in_a_function = True self . lines_in_function = 0 self . current_function = function_name
8315	def parse_links ( self , markup ) : links = [ ] m = re . findall ( self . re [ "link" ] , markup ) for link in m : # We don't like [[{{{1|Universe (disambiguation)}}}]] if link . find ( "{" ) >= 0 : link = re . sub ( "\{{1,3}[0-9]{0,2}\|" , "" , link ) link = link . replace ( "{" , "" ) link = link . replace ( "}" , "" ) link = link . split ( "|" ) link [ 0 ] = link [ 0 ] . split ( "#" ) page = link [ 0 ] [ 0 ] . strip ( ) #anchor = u"" #display = u"" #if len(link[0]) > 1: # anchor = link[0][1].strip() #if len(link) > 1: # display = link[1].strip() if not page in links : links . append ( page ) #links[page] = WikipediaLink(page, anchor, display) links . sort ( ) return links
5980	def bin_up_mask_2d ( mask_2d , bin_up_factor ) : padded_array_2d = array_util . pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = mask_2d , bin_up_factor = bin_up_factor , pad_value = True ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = True for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 if padded_array_2d [ padded_y , padded_x ] == False : value = False binned_array_2d [ y , x ] = value return binned_array_2d
7253	def status ( self , order_id ) : self . logger . debug ( 'Get status of order ' + order_id ) url = '%(base_url)s/order/%(order_id)s' % { 'base_url' : self . base_url , 'order_id' : order_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( ) . get ( "acquisitions" , { } )
637	def read ( cls , proto ) : #pylint: disable=W0212 protoCells = proto . cells connections = cls ( len ( protoCells ) ) for cellIdx , protoCell in enumerate ( protoCells ) : protoCell = protoCells [ cellIdx ] protoSegments = protoCell . segments connections . _cells [ cellIdx ] = CellData ( ) segments = connections . _cells [ cellIdx ] . _segments for segmentIdx , protoSegment in enumerate ( protoSegments ) : segment = Segment ( cellIdx , connections . _nextFlatIdx , connections . _nextSegmentOrdinal ) segments . append ( segment ) connections . _segmentForFlatIdx . append ( segment ) connections . _nextFlatIdx += 1 connections . _nextSegmentOrdinal += 1 synapses = segment . _synapses protoSynapses = protoSegment . synapses for synapseIdx , protoSynapse in enumerate ( protoSynapses ) : presynapticCell = protoSynapse . presynapticCell synapse = Synapse ( segment , presynapticCell , protoSynapse . permanence , ordinal = connections . _nextSynapseOrdinal ) connections . _nextSynapseOrdinal += 1 synapses . add ( synapse ) connections . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) connections . _numSynapses += 1 #pylint: enable=W0212 return connections
5093	def refresh_maps ( self ) : for robot in self . robots : resp2 = ( requests . get ( urljoin ( self . ENDPOINT , 'users/me/robots/{}/maps' . format ( robot . serial ) ) , headers = self . _headers ) ) resp2 . raise_for_status ( ) self . _maps . update ( { robot . serial : resp2 . json ( ) } )
1565	def invoke_hook_spout_ack ( self , message_id , complete_latency_ns ) : if len ( self . task_hooks ) > 0 : spout_ack_info = SpoutAckInfo ( message_id = message_id , spout_task_id = self . get_task_id ( ) , complete_latency_ms = complete_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . spout_ack ( spout_ack_info )
8677	def ssh ( key_name , no_tunnel , stash , passphrase , backend ) : # TODO: find_executable or raise def execute ( command ) : try : click . echo ( 'Executing: {0}' . format ( ' ' . join ( command ) ) ) subprocess . check_call ( ' ' . join ( command ) , shell = True ) except subprocess . CalledProcessError : sys . exit ( 1 ) stash = _get_stash ( backend , stash , passphrase ) key = stash . get ( key_name ) if key : _assert_is_ssh_type_key ( key ) else : sys . exit ( 'Key `{0}` not found' . format ( key_name ) ) conn_info = key [ 'value' ] ssh_key_path = conn_info . get ( 'ssh_key_path' ) ssh_key = conn_info . get ( 'ssh_key' ) proxy_key_path = conn_info . get ( 'proxy_key_path' ) proxy_key = conn_info . get ( 'proxy_key' ) id_file = _write_tmp ( ssh_key ) if ssh_key else ssh_key_path conn_info [ 'ssh_key_path' ] = id_file if conn_info . get ( 'proxy' ) : proxy_id_file = _write_tmp ( proxy_key ) if proxy_key else proxy_key_path conn_info [ 'proxy_key_path' ] = proxy_id_file ssh_command = _build_ssh_command ( conn_info , no_tunnel ) try : execute ( ssh_command ) finally : # If they're not equal, that means we've created a temp one which # should be deleted, else, it's a path to an existing key file. if id_file != ssh_key_path : click . echo ( 'Removing temp ssh key file: {0}...' . format ( id_file ) ) os . remove ( id_file ) if conn_info . get ( 'proxy' ) and proxy_id_file != proxy_key_path : click . echo ( 'Removing temp proxy key file: {0}...' . format ( proxy_id_file ) ) os . remove ( proxy_id_file )
5194	def Process ( self , info , values ) : visitor_class_types = { opendnp3 . ICollectionIndexedBinary : VisitorIndexedBinary , opendnp3 . ICollectionIndexedDoubleBitBinary : VisitorIndexedDoubleBitBinary , opendnp3 . ICollectionIndexedCounter : VisitorIndexedCounter , opendnp3 . ICollectionIndexedFrozenCounter : VisitorIndexedFrozenCounter , opendnp3 . ICollectionIndexedAnalog : VisitorIndexedAnalog , opendnp3 . ICollectionIndexedBinaryOutputStatus : VisitorIndexedBinaryOutputStatus , opendnp3 . ICollectionIndexedAnalogOutputStatus : VisitorIndexedAnalogOutputStatus , opendnp3 . ICollectionIndexedTimeAndInterval : VisitorIndexedTimeAndInterval } visitor_class = visitor_class_types [ type ( values ) ] visitor = visitor_class ( ) values . Foreach ( visitor ) for index , value in visitor . index_and_value : log_string = 'SOEHandler.Process {0}\theaderIndex={1}\tdata_type={2}\tindex={3}\tvalue={4}' _log . debug ( log_string . format ( info . gv , info . headerIndex , type ( values ) . __name__ , index , value ) )
12787	def check_file ( self , filename ) : # type: (str) -> bool if not exists ( filename ) : return False # Check if the file is version-compatible with this instance. new_config = ConfigResolverBase ( ) new_config . read ( filename ) if self . version and not new_config . has_option ( 'meta' , 'version' ) : # self.version is set, so we MUST have a version in the file! raise NoVersionError ( "The config option 'meta.version' is missing in {}. The " "application expects version {}!" . format ( filename , self . version ) ) elif not self . version and new_config . has_option ( 'meta' , 'version' ) : # Automatically "lock-in" a version number if one is found. # This prevents loading a chain of config files with incompatible # version numbers! self . version = StrictVersion ( new_config . get ( 'meta' , 'version' ) ) self . _log . info ( '%r contains a version number, but the config ' 'instance was not created with a version ' 'restriction. Will set version number to "%s" to ' 'prevent accidents!' , filename , self . version ) elif self . version : # This instance expected a certain version. We need to check the # version in the file and compare. file_version = new_config . get ( 'meta' , 'version' ) major , minor , _ = StrictVersion ( file_version ) . version expected_major , expected_minor , _ = self . version . version if expected_major != major : self . _log . error ( 'Invalid major version number in %r. Expected %r, got %r!' , abspath ( filename ) , str ( self . version ) , file_version ) return False if expected_minor != minor : self . _log . warning ( 'Mismatching minor version number in %r. ' 'Expected %r, got %r!' , abspath ( filename ) , str ( self . version ) , file_version ) return True return True
12616	def is_img ( obj ) : try : get_data = getattr ( obj , 'get_data' ) get_affine = getattr ( obj , 'get_affine' ) return isinstance ( get_data , collections . Callable ) and isinstance ( get_affine , collections . Callable ) except AttributeError : return False
1962	def sys_rt_sigaction ( self , signum , act , oldact ) : return self . sys_sigaction ( signum , act , oldact )
284	def plot_drawdown_periods ( returns , top = 10 , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) df_drawdowns = timeseries . gen_drawdown_table ( returns , top = top ) df_cum_rets . plot ( ax = ax , * * kwargs ) lim = ax . get_ylim ( ) colors = sns . cubehelix_palette ( len ( df_drawdowns ) ) [ : : - 1 ] for i , ( peak , recovery ) in df_drawdowns [ [ 'Peak date' , 'Recovery date' ] ] . iterrows ( ) : if pd . isnull ( recovery ) : recovery = returns . index [ - 1 ] ax . fill_between ( ( peak , recovery ) , lim [ 0 ] , lim [ 1 ] , alpha = .4 , color = colors [ i ] ) ax . set_ylim ( lim ) ax . set_title ( 'Top %i drawdown periods' % top ) ax . set_ylabel ( 'Cumulative returns' ) ax . legend ( [ 'Portfolio' ] , loc = 'upper left' , frameon = True , framealpha = 0.5 ) ax . set_xlabel ( '' ) return ax
4638	def shared_blockchain_instance ( self ) : if not self . _sharedInstance . instance : klass = self . get_instance_class ( ) self . _sharedInstance . instance = klass ( * * self . _sharedInstance . config ) return self . _sharedInstance . instance
3360	def elements ( self ) : tmp_formula = self . formula if tmp_formula is None : return { } # necessary for some old pickles which use the deprecated # Formula class tmp_formula = str ( self . formula ) # commonly occurring characters in incorrectly constructed formulas if "*" in tmp_formula : warn ( "invalid character '*' found in formula '%s'" % self . formula ) tmp_formula = tmp_formula . replace ( "*" , "" ) if "(" in tmp_formula or ")" in tmp_formula : warn ( "invalid formula (has parenthesis) in '%s'" % self . formula ) return None composition = { } parsed = element_re . findall ( tmp_formula ) for ( element , count ) in parsed : if count == '' : count = 1 else : try : count = float ( count ) int_count = int ( count ) if count == int_count : count = int_count else : warn ( "%s is not an integer (in formula %s)" % ( count , self . formula ) ) except ValueError : warn ( "failed to parse %s (in formula %s)" % ( count , self . formula ) ) return None if element in composition : composition [ element ] += count else : composition [ element ] = count return composition
10944	def reset ( self , new_region_size = None , do_calc_size = True , new_damping = None , new_max_mem = None ) : if new_region_size is not None : self . region_size = new_region_size if new_max_mem != None : self . max_mem = new_max_mem if do_calc_size : self . region_size = calc_particle_group_region_size ( self . state , region_size = self . region_size , max_mem = self . max_mem ) self . stats = [ ] self . particle_groups = separate_particles_into_groups ( self . state , self . region_size , doshift = 'rand' ) if new_damping is not None : self . _kwargs . update ( { 'damping' : new_damping } ) if self . save_J : if len ( self . particle_groups ) > 90 : CLOG . warn ( 'Attempting to create many open files. Consider increasing max_mem and/or region_size to avoid crashes.' ) self . _tempfiles = [ ] self . _has_saved_J = [ ] for a in range ( len ( self . particle_groups ) ) : #TemporaryFile is automatically deleted for _ in [ 'j' , 'tile' ] : self . _tempfiles . append ( tempfile . TemporaryFile ( dir = os . getcwd ( ) ) ) self . _has_saved_J . append ( False )
11433	def _shift_field_positions_global ( record , start , delta = 1 ) : if not delta : return for tag , fields in record . items ( ) : newfields = [ ] for field in fields : if field [ 4 ] < start : newfields . append ( field ) else : # Increment the global field position by delta. newfields . append ( tuple ( list ( field [ : 4 ] ) + [ field [ 4 ] + delta ] ) ) record [ tag ] = newfields
11314	def update_hidden_notes ( self ) : if not self . tag_as_cern : notes = record_get_field_instances ( self . record , tag = "595" ) for field in notes : for dummy , value in field [ 0 ] : if value == "CDS" : self . tag_as_cern = True record_delete_fields ( self . record , tag = "595" )
9164	def includeme ( config ) : api_key_authn_policy = APIKeyAuthenticationPolicy ( ) config . include ( 'openstax_accounts' ) openstax_authn_policy = config . registry . getUtility ( IOpenstaxAccountsAuthenticationPolicy ) # Set up api & user authentication policies. policies = [ api_key_authn_policy , openstax_authn_policy ] authn_policy = MultiAuthenticationPolicy ( policies ) config . set_authentication_policy ( authn_policy ) # Set up the authorization policy. authz_policy = ACLAuthorizationPolicy ( ) config . set_authorization_policy ( authz_policy )
10711	def models_preparing ( app ) : def wrapper ( resource , parent ) : if isinstance ( resource , DeclarativeMeta ) : resource = ListResource ( resource ) if not getattr ( resource , '__parent__' , None ) : resource . __parent__ = parent return resource resources_preparing_factory ( app , wrapper )
9859	def set_parameter ( self , key , value ) : if value is None or isinstance ( value , ( int , float , bool ) ) : value = str ( value ) if key . endswith ( '64' ) : value = urlsafe_b64encode ( value . encode ( 'utf-8' ) ) value = value . replace ( b ( '=' ) , b ( '' ) ) self . _parameters [ key ] = value
4375	def encode_payload ( self , messages ) : if not messages or messages [ 0 ] is None : return '' if len ( messages ) == 1 : return messages [ 0 ] . encode ( 'utf-8' ) payload = u'' . join ( [ ( u'\ufffd%d\ufffd%s' % ( len ( p ) , p ) ) for p in messages if p is not None ] ) # FIXME: why is it so that we must filter None from here ? How # is it even possible that a None gets in there ? return payload . encode ( 'utf-8' )
7395	def get_publication ( context , id ) : pbl = Publication . objects . filter ( pk = int ( id ) ) if len ( pbl ) < 1 : return '' pbl [ 0 ] . links = pbl [ 0 ] . customlink_set . all ( ) pbl [ 0 ] . files = pbl [ 0 ] . customfile_set . all ( ) return render_template ( 'publications/publication.html' , context [ 'request' ] , { 'publication' : pbl [ 0 ] } )
5630	def _postreceive ( self ) : digest = self . _get_digest ( ) if digest is not None : sig_parts = _get_header ( 'X-Hub-Signature' ) . split ( '=' , 1 ) if not isinstance ( digest , six . text_type ) : digest = six . text_type ( digest ) if ( len ( sig_parts ) < 2 or sig_parts [ 0 ] != 'sha1' or not hmac . compare_digest ( sig_parts [ 1 ] , digest ) ) : abort ( 400 , 'Invalid signature' ) event_type = _get_header ( 'X-Github-Event' ) data = request . get_json ( ) if data is None : abort ( 400 , 'Request body must contain json' ) self . _logger . info ( '%s (%s)' , _format_event ( event_type , data ) , _get_header ( 'X-Github-Delivery' ) ) for hook in self . _hooks . get ( event_type , [ ] ) : hook ( data ) return '' , 204
4506	def find_serial_devices ( self ) : if self . devices is not None : return self . devices self . devices = { } hardware_id = "(?i)" + self . hardware_id # forces case insensitive for ports in serial . tools . list_ports . grep ( hardware_id ) : port = ports [ 0 ] try : id = self . get_device_id ( port ) ver = self . _get_device_version ( port ) except : log . debug ( 'Error getting device_id for %s, %s' , port , self . baudrate ) if True : raise continue if getattr ( ports , '__len__' , lambda : 0 ) ( ) : log . debug ( 'Multi-port device %s:%s:%s with %s ports found' , self . hardware_id , id , ver , len ( ports ) ) if id < 0 : log . debug ( 'Serial device %s:%s:%s with id %s < 0' , self . hardware_id , id , ver ) else : self . devices [ id ] = port , ver return self . devices
767	def getMetrics ( self ) : result = { } for metricObj , label in zip ( self . __metrics , self . __metricLabels ) : value = metricObj . getMetric ( ) result [ label ] = value [ 'value' ] return result
11947	def jocker ( test_options = None ) : version = ver_check ( ) options = test_options or docopt ( __doc__ , version = version ) _set_global_verbosity_level ( options . get ( '--verbose' ) ) jocker_lgr . debug ( options ) jocker_run ( options )
5578	def driver_from_file ( input_file ) : file_ext = os . path . splitext ( input_file ) [ 1 ] . split ( "." ) [ 1 ] if file_ext not in _file_ext_to_driver ( ) : raise MapcheteDriverError ( "no driver could be found for file extension %s" % file_ext ) driver = _file_ext_to_driver ( ) [ file_ext ] if len ( driver ) > 1 : warnings . warn ( DeprecationWarning ( "more than one driver for file found, taking %s" % driver [ 0 ] ) ) return driver [ 0 ]
10604	def create_entity ( self , name , gl_structure , description = None ) : new_entity = Entity ( name , gl_structure , description = description ) self . entities . append ( new_entity ) return new_entity
10963	def set_shape ( self , shape , inner ) : if self . shape != shape or self . inner != inner : self . shape = shape self . inner = inner self . initialize ( )
12602	def col_values ( df , col_name ) : _check_cols ( df , [ col_name ] ) if 'O' in df [ col_name ] or pd . np . issubdtype ( df [ col_name ] . dtype , str ) : # if the column is of strings return [ nom . lower ( ) for nom in df [ pd . notnull ( df ) ] [ col_name ] if not pd . isnull ( nom ) ] else : return [ nom for nom in df [ pd . notnull ( df ) ] [ col_name ] if not pd . isnull ( nom ) ]
13224	def main ( ) : parser = argparse . ArgumentParser ( description = 'Discover and ingest metadata from document sources, ' 'including lsstdoc-based LaTeX documents and ' 'reStructuredText-based technotes. Metadata can be ' 'upserted into the LSST Projectmeta MongoDB.' ) parser . add_argument ( '--ltd-product' , dest = 'ltd_product_url' , help = 'URL of an LSST the Docs product ' '(https://keeper.lsst.codes/products/<slug>). If provided, ' 'only this document will be ingested.' ) parser . add_argument ( '--github-token' , help = 'GitHub personal access token.' ) parser . add_argument ( '--mongodb-uri' , help = 'MongoDB connection URI. If provided, metadata will be loaded ' 'into the Projectmeta database. Omit this argument to just ' 'test the ingest pipeline.' ) parser . add_argument ( '--mongodb-db' , default = 'lsstprojectmeta' , help = 'Name of MongoDB database' ) parser . add_argument ( '--mongodb-collection' , default = 'resources' , help = 'Name of the MongoDB collection for projectmeta resources' ) args = parser . parse_args ( ) # Configure the root logger stream_handler = logging . StreamHandler ( ) stream_formatter = logging . Formatter ( '%(asctime)s %(levelname)8s %(name)s | %(message)s' ) stream_handler . setFormatter ( stream_formatter ) root_logger = logging . getLogger ( ) root_logger . addHandler ( stream_handler ) root_logger . setLevel ( logging . WARNING ) # Configure app logger app_logger = logging . getLogger ( 'lsstprojectmeta' ) app_logger . setLevel ( logging . DEBUG ) if args . mongodb_uri is not None : mongo_client = AsyncIOMotorClient ( args . mongodb_uri , ssl = True ) collection = mongo_client [ args . mongodb_db ] [ args . mongodb_collection ] else : collection = None loop = asyncio . get_event_loop ( ) if args . ltd_product_url is not None : # Run single technote loop . run_until_complete ( run_single_ltd_doc ( args . ltd_product_url , args . github_token , collection ) ) else : # Run bulk technote processing loop . run_until_complete ( run_bulk_etl ( args . github_token , collection ) )
12915	def filelist ( self ) : if len ( self . _filelist ) == 0 : for item in self . _data : if isinstance ( self . _data [ item ] , filetree ) : self . _filelist . extend ( self . _data [ item ] . filelist ( ) ) else : self . _filelist . append ( self . _data [ item ] ) return self . _filelist
987	def mmPrettyPrintSequenceCellRepresentations ( self , sortby = "Column" ) : self . _mmComputeTransitionTraces ( ) table = PrettyTable ( [ "Pattern" , "Column" , "predicted=>active cells" ] ) for sequenceLabel , predictedActiveCells in ( self . _mmData [ "predictedActiveCellsForSequence" ] . iteritems ( ) ) : cellsForColumn = self . mapCellsToColumns ( predictedActiveCells ) for column , cells in cellsForColumn . iteritems ( ) : table . add_row ( [ sequenceLabel , column , list ( cells ) ] ) return table . get_string ( sortby = sortby ) . encode ( "utf-8" )
1638	def CheckSpacing ( filename , clean_lines , linenum , nesting_state , error ) : # Don't use "elided" lines here, otherwise we can't check commented lines. # Don't want to use "raw" either, because we don't want to check inside C++11 # raw strings, raw = clean_lines . lines_without_raw_strings line = raw [ linenum ] # Before nixing comments, check if the line is blank for no good # reason. This includes the first line after a block is opened, and # blank lines at the end of a function (ie, right before a line like '}' # # Skip all the blank line checks if we are immediately inside a # namespace body. In other words, don't issue blank line warnings # for this block: # namespace { # # } # # A warning about missing end of namespace comments will be issued instead. # # Also skip blank line checks for 'extern "C"' blocks, which are formatted # like namespaces. if ( IsBlankLine ( line ) and not nesting_state . InNamespaceBody ( ) and not nesting_state . InExternC ( ) ) : elided = clean_lines . elided prev_line = elided [ linenum - 1 ] prevbrace = prev_line . rfind ( '{' ) # TODO(unknown): Don't complain if line before blank line, and line after, # both start with alnums and are indented the same amount. # This ignores whitespace at the start of a namespace block # because those are not usually indented. if prevbrace != - 1 and prev_line [ prevbrace : ] . find ( '}' ) == - 1 : # OK, we have a blank line at the start of a code block. Before we # complain, we check if it is an exception to the rule: The previous # non-empty line has the parameters of a function header that are indented # 4 spaces (because they did not fit in a 80 column line when placed on # the same line as the function name). We also check for the case where # the previous line is indented 6 spaces, which may happen when the # initializers of a constructor do not fit into a 80 column line. exception = False if Match ( r' {6}\w' , prev_line ) : # Initializer list? # We are looking for the opening column of initializer list, which # should be indented 4 spaces to cause 6 space indentation afterwards. search_position = linenum - 2 while ( search_position >= 0 and Match ( r' {6}\w' , elided [ search_position ] ) ) : search_position -= 1 exception = ( search_position >= 0 and elided [ search_position ] [ : 5 ] == ' :' ) else : # Search for the function arguments or an initializer list. We use a # simple heuristic here: If the line is indented 4 spaces; and we have a # closing paren, without the opening paren, followed by an opening brace # or colon (for initializer lists) we assume that it is the last line of # a function header. If we have a colon indented 4 spaces, it is an # initializer list. exception = ( Match ( r' {4}\w[^\(]*\)\s*(const\s*)?(\{\s*$|:)' , prev_line ) or Match ( r' {4}:' , prev_line ) ) if not exception : error ( filename , linenum , 'whitespace/blank_line' , 2 , 'Redundant blank line at the start of a code block ' 'should be deleted.' ) # Ignore blank lines at the end of a block in a long if-else # chain, like this: # if (condition1) { # // Something followed by a blank line # # } else if (condition2) { # // Something else # } if linenum + 1 < clean_lines . NumLines ( ) : next_line = raw [ linenum + 1 ] if ( next_line and Match ( r'\s*}' , next_line ) and next_line . find ( '} else ' ) == - 1 ) : error ( filename , linenum , 'whitespace/blank_line' , 3 , 'Redundant blank line at the end of a code block ' 'should be deleted.' ) matched = Match ( r'\s*(public|protected|private):' , prev_line ) if matched : error ( filename , linenum , 'whitespace/blank_line' , 3 , 'Do not leave a blank line after "%s:"' % matched . group ( 1 ) ) # Next, check comments next_line_start = 0 if linenum + 1 < clean_lines . NumLines ( ) : next_line = raw [ linenum + 1 ] next_line_start = len ( next_line ) - len ( next_line . lstrip ( ) ) CheckComment ( line , filename , linenum , next_line_start , error ) # get rid of comments and strings line = clean_lines . elided [ linenum ] # You shouldn't have spaces before your brackets, except maybe after # 'delete []' or 'return []() {};' if Search ( r'\w\s+\[' , line ) and not Search ( r'(?:delete|return)\s+\[' , line ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Extra space before [' ) # In range-based for, we wanted spaces before and after the colon, but # not around "::" tokens that might appear. if ( Search ( r'for *\(.*[^:]:[^: ]' , line ) or Search ( r'for *\(.*[^: ]:[^:]' , line ) ) : error ( filename , linenum , 'whitespace/forcolon' , 2 , 'Missing space around colon in range-based for loop' )
9637	def format ( self , record ) : data = record . _raw . copy ( ) # serialize the datetime date as utc string data [ 'time' ] = data [ 'time' ] . isoformat ( ) # stringify exception data if data . get ( 'traceback' ) : data [ 'traceback' ] = self . formatException ( data [ 'traceback' ] ) return json . dumps ( data )
9945	def copy_file ( self , path , prefixed_path , source_storage ) : # Skip this file if it was already copied earlier if prefixed_path in self . copied_files : return self . log ( "Skipping '%s' (already copied earlier)" % path ) # Delete the target file if needed or break if not self . delete_file ( path , prefixed_path , source_storage ) : return # The full path of the source file source_path = source_storage . path ( path ) # Finally start copying if self . dry_run : self . log ( "Pretending to copy '%s'" % source_path , level = 1 ) else : self . log ( "Copying '%s'" % source_path , level = 1 ) with source_storage . open ( path ) as source_file : self . storage . save ( prefixed_path , source_file ) self . copied_files . append ( prefixed_path )
2112	def parse_requirements ( filename ) : reqs = [ ] version_spec_in_play = None # Iterate over each line in the requirements file. for line in open ( filename , 'r' ) . read ( ) . strip ( ) . split ( '\n' ) : # Sanity check: Is this an empty line? # If so, do nothing. if not line . strip ( ) : continue # If this is just a plain requirement (not a comment), then # add it to the requirements list. if not line . startswith ( '#' ) : reqs . append ( line ) continue # "Header" comments take the form of "=== Python {op} {version} ===", # and make the requirement only matter for those versions. # If this line is a header comment, parse it. match = re . search ( r'^# === [Pp]ython (?P<op>[<>=]{1,2}) ' r'(?P<major>[\d])\.(?P<minor>[\d]+) ===[\s]*$' , line ) if match : version_spec_in_play = match . groupdict ( ) for key in ( 'major' , 'minor' ) : version_spec_in_play [ key ] = int ( version_spec_in_play [ key ] ) continue # If this is a comment that otherwise looks like a package, then it # should be a package applying only to the current version spec. # # We can identify something that looks like a package by a lack # of any spaces. if ' ' not in line [ 1 : ] . strip ( ) and version_spec_in_play : package = line [ 1 : ] . strip ( ) # Sanity check: Is our version of Python one of the ones currently # in play? op = version_spec_in_play [ 'op' ] vspec = ( version_spec_in_play [ 'major' ] , version_spec_in_play [ 'minor' ] ) if '=' in op and sys . version_info [ 0 : 2 ] == vspec : reqs . append ( package ) elif '>' in op and sys . version_info [ 0 : 2 ] > vspec : reqs . append ( package ) elif '<' in op and sys . version_info [ 0 : 2 ] < vspec : reqs . append ( package ) # Okay, we should have an entire list of requirements now. return reqs
8362	def encode ( self , o ) : # This is for extremely simple cases and benchmarks. if isinstance ( o , basestring ) : if isinstance ( o , str ) : _encoding = self . encoding if ( _encoding is not None and not ( _encoding == 'utf-8' ) ) : o = o . decode ( _encoding ) if self . ensure_ascii : return encode_basestring_ascii ( o ) else : return encode_basestring ( o ) # This doesn't pass the iterator directly to ''.join() because the # exceptions aren't as detailed. The list call should be roughly # equivalent to the PySequence_Fast that ''.join() would do. chunks = list ( self . iterencode ( o ) ) return '' . join ( chunks )
13619	def get_current_branch ( self ) : return next ( ( self . _sanitize ( branch ) for branch in self . _git . branch ( color = "never" ) . splitlines ( ) if branch . startswith ( '*' ) ) , None )
12352	def rebuild ( self , image , wait = True ) : return self . _action ( 'rebuild' , image = image , wait = wait )
8732	def divide_timedelta_float ( td , divisor ) : # td is comprised of days, seconds, microseconds dsm = [ getattr ( td , attr ) for attr in ( 'days' , 'seconds' , 'microseconds' ) ] dsm = map ( lambda elem : elem / divisor , dsm ) return datetime . timedelta ( * dsm )
12649	def is_valid_regex ( string ) : try : re . compile ( string ) is_valid = True except re . error : is_valid = False return is_valid
2551	def unescape ( data ) : cc = re . compile ( r'&(?:(?:#(\d+))|([^;]+));' ) result = [ ] m = cc . search ( data ) while m : result . append ( data [ 0 : m . start ( ) ] ) d = m . group ( 1 ) if d : d = int ( d ) result . append ( unichr ( d ) ) else : d = _unescape . get ( m . group ( 2 ) , ord ( '?' ) ) result . append ( unichr ( d ) ) data = data [ m . end ( ) : ] m = cc . search ( data ) result . append ( data ) return '' . join ( result )
6119	def circular_anti_annular ( cls , shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , outer_radius_2_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_anti_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , outer_radius_2_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
8588	def start_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/start' % ( datacenter_id , server_id ) , method = 'POST-ACTION' ) return response
6779	def get_deploy_funcs ( components , current_thumbprint , previous_thumbprint , preview = False ) : for component in components : funcs = manifest_deployers . get ( component , [ ] ) for func_name in funcs : #TODO:remove this after burlap.* naming prefix bug fixed if func_name . startswith ( 'burlap.' ) : print ( 'skipping %s' % func_name ) continue takes_diff = manifest_deployers_takes_diff . get ( func_name , False ) func = resolve_deployer ( func_name ) current = current_thumbprint . get ( component ) last = previous_thumbprint . get ( component ) if takes_diff : yield func_name , partial ( func , last = last , current = current ) else : yield func_name , partial ( func )
2502	def to_special_value ( self , value ) : if value == self . spdx_namespace . none : return utils . SPDXNone ( ) elif value == self . spdx_namespace . noassertion : return utils . NoAssert ( ) elif value == self . spdx_namespace . unknown : return utils . UnKnown ( ) else : return value
8158	def sql ( self , sql ) : self . _cur . execute ( sql ) if sql . lower ( ) . find ( "select" ) >= 0 : matches = [ ] for r in self . _cur : matches . append ( r ) return matches
8277	def fseq ( self , client , message ) : client . last_frame = client . current_frame client . current_frame = message [ 3 ]
11120	def get_file_relative_path_by_id ( self , id ) : for path , info in self . walk_files_info ( ) : if info [ 'id' ] == id : return path # none was found return None
4874	def validate_lms_user_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) try : # Ensure the given user is associated with the enterprise. return models . EnterpriseCustomerUser . objects . get ( user_id = value , enterprise_customer = enterprise_customer ) except models . EnterpriseCustomerUser . DoesNotExist : pass return None
13098	def wait ( self ) : try : self . relay . wait ( ) self . responder . wait ( ) except KeyboardInterrupt : print_notification ( "Stopping" ) finally : self . terminate_processes ( )
3420	def create_mat_dict ( model ) : rxns = model . reactions mets = model . metabolites mat = OrderedDict ( ) mat [ "mets" ] = _cell ( [ met_id for met_id in create_mat_metabolite_id ( model ) ] ) mat [ "metNames" ] = _cell ( mets . list_attr ( "name" ) ) mat [ "metFormulas" ] = _cell ( [ str ( m . formula ) for m in mets ] ) try : mat [ "metCharge" ] = array ( mets . list_attr ( "charge" ) ) * 1. except TypeError : # can't have any None entries for charge, or this will fail pass mat [ "genes" ] = _cell ( model . genes . list_attr ( "id" ) ) # make a matrix for rxnGeneMat # reactions are rows, genes are columns rxn_gene = scipy_sparse . dok_matrix ( ( len ( model . reactions ) , len ( model . genes ) ) ) if min ( rxn_gene . shape ) > 0 : for i , reaction in enumerate ( model . reactions ) : for gene in reaction . genes : rxn_gene [ i , model . genes . index ( gene ) ] = 1 mat [ "rxnGeneMat" ] = rxn_gene mat [ "grRules" ] = _cell ( rxns . list_attr ( "gene_reaction_rule" ) ) mat [ "rxns" ] = _cell ( rxns . list_attr ( "id" ) ) mat [ "rxnNames" ] = _cell ( rxns . list_attr ( "name" ) ) mat [ "subSystems" ] = _cell ( rxns . list_attr ( "subsystem" ) ) stoich_mat = create_stoichiometric_matrix ( model ) mat [ "S" ] = stoich_mat if stoich_mat is not None else [ [ ] ] # multiply by 1 to convert to float, working around scipy bug # https://github.com/scipy/scipy/issues/4537 mat [ "lb" ] = array ( rxns . list_attr ( "lower_bound" ) ) * 1. mat [ "ub" ] = array ( rxns . list_attr ( "upper_bound" ) ) * 1. mat [ "b" ] = array ( mets . list_attr ( "_bound" ) ) * 1. mat [ "c" ] = array ( rxns . list_attr ( "objective_coefficient" ) ) * 1. mat [ "rev" ] = array ( rxns . list_attr ( "reversibility" ) ) * 1 mat [ "description" ] = str ( model . id ) return mat
4347	def tempo ( self , factor , audio_type = None , quick = False ) : if not is_number ( factor ) or factor <= 0 : raise ValueError ( "factor must be a positive number" ) if factor < 0.5 or factor > 2 : logger . warning ( "Using an extreme time stretching factor. " "Quality of results will be poor" ) if abs ( factor - 1.0 ) <= 0.1 : logger . warning ( "For this stretch factor, " "the stretch effect has better performance." ) if audio_type not in [ None , 'm' , 's' , 'l' ] : raise ValueError ( "audio_type must be one of None, 'm', 's', or 'l'." ) if not isinstance ( quick , bool ) : raise ValueError ( "quick must be a boolean." ) effect_args = [ 'tempo' ] if quick : effect_args . append ( '-q' ) if audio_type is not None : effect_args . append ( '-{}' . format ( audio_type ) ) effect_args . append ( '{:f}' . format ( factor ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'tempo' ) return self
10825	def search ( cls , query , q ) : query = query . join ( User ) . filter ( User . email . like ( '%{0}%' . format ( q ) ) , ) return query
11003	def psffunc ( self , * args , * * kwargs ) : if self . polychromatic : func = psfcalc . calculate_polychrome_linescan_psf else : func = psfcalc . calculate_linescan_psf return func ( * args , * * kwargs )
1491	def get_serializer ( context ) : cluster_config = context . get_cluster_config ( ) serializer_clsname = cluster_config . get ( constants . TOPOLOGY_SERIALIZER_CLASSNAME , None ) if serializer_clsname is None : return PythonSerializer ( ) else : try : topo_pex_path = context . get_topology_pex_path ( ) pex_loader . load_pex ( topo_pex_path ) serializer_cls = pex_loader . import_and_get_class ( topo_pex_path , serializer_clsname ) serializer = serializer_cls ( ) return serializer except Exception as e : raise RuntimeError ( "Error with loading custom serializer class: %s, with error message: %s" % ( serializer_clsname , str ( e ) ) )
10909	def missing_particle ( separation = 0.0 , radius = RADIUS , SNR = 20 ) : # create a base image of one particle s = init . create_two_particle_state ( imsize = 6 * radius + 4 , axis = 'x' , sigma = 1.0 / SNR , delta = separation , radius = radius , stateargs = { 'varyn' : True } , psfargs = { 'error' : 1e-6 } ) s . obj . typ [ 1 ] = 0. s . reset ( ) return s , s . obj . pos . copy ( )
11168	def _add_option ( self , option ) : if option . name in self . options : raise ValueError ( 'name already in use' ) if option . abbreviation in self . abbreviations : raise ValueError ( 'abbreviation already in use' ) if option . name in [ arg . name for arg in self . positional_args ] : raise ValueError ( 'name already in use by a positional argument' ) self . options [ option . name ] = option if option . abbreviation : self . abbreviations [ option . abbreviation ] = option self . option_order . append ( option . name )
2468	def set_file_license_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_license_comment_set : self . file_license_comment_set = True if validations . validate_file_lics_comment ( text ) : self . file ( doc ) . license_comment = str_from_text ( text ) else : raise SPDXValueError ( 'File::LicenseComment' ) else : raise CardinalityError ( 'File::LicenseComment' ) else : raise OrderError ( 'File::LicenseComment' )
6748	def collect_genv ( self , include_local = True , include_global = True ) : e = type ( self . genv ) ( ) if include_global : e . update ( self . genv ) if include_local : for k , v in self . lenv . items ( ) : e [ '%s_%s' % ( self . obj . name . lower ( ) , k ) ] = v return e
6582	def play_station ( self , station ) : for song in iterate_forever ( station . get_playlist ) : try : self . play ( song ) except StopIteration : self . stop ( ) return
7744	def _timeout_cb ( self , method ) : self . _anything_done = True logger . debug ( "_timeout_cb() called for: {0!r}" . format ( method ) ) result = method ( ) # pylint: disable=W0212 rec = method . _pyxmpp_recurring if rec : self . _prepare_pending ( ) return True if rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) tag = glib . timeout_add ( int ( result * 1000 ) , self . _timeout_cb , method ) self . _timer_sources [ method ] = tag else : self . _timer_sources . pop ( method , None ) self . _prepare_pending ( ) return False
1772	def push ( cpu , value , size ) : assert size in ( 8 , 16 , cpu . address_bit_size ) cpu . STACK = cpu . STACK - size // 8 base , _ , _ = cpu . get_descriptor ( cpu . read_register ( 'SS' ) ) address = cpu . STACK + base cpu . write_int ( address , value , size )
385	def parse_darknet_ann_list_to_cls_box ( annotations ) : class_list = [ ] bbox_list = [ ] for ann in annotations : class_list . append ( ann [ 0 ] ) bbox_list . append ( ann [ 1 : ] ) return class_list , bbox_list
9470	def conference_list_members ( self , call_params ) : path = '/' + self . api_version + '/ConferenceListMembers/' method = 'POST' return self . request ( path , method , call_params )
728	def numberMapForBits ( self , bits ) : numberMap = dict ( ) for bit in bits : numbers = self . numbersForBit ( bit ) for number in numbers : if not number in numberMap : numberMap [ number ] = set ( ) numberMap [ number ] . add ( bit ) return numberMap
11556	def enable_digital_reporting ( self , pin ) : port = pin // 8 command = [ self . _command_handler . REPORT_DIGITAL + port , self . REPORTING_ENABLE ] self . _command_handler . send_command ( command )
3764	def Parachor ( MW , rhol , rhog , sigma ) : rhol , rhog = rhol * 1000. , rhog * 1000. # Convert kg/m^3 to g/m^3 return sigma ** 0.25 * MW / ( rhol - rhog )
9040	def add_instruction ( self , specification ) : instruction = self . as_instruction ( specification ) self . _type_to_instruction [ instruction . type ] = instruction
12989	def setup_notebook ( debug = False ) : output_notebook ( INLINE , hide_banner = True ) if debug : _setup_logging ( logging . DEBUG ) logging . debug ( 'Running notebook in debug mode.' ) else : _setup_logging ( logging . WARNING ) # If JUPYTERHUB_SERVICE_PREFIX environment variable isn't set, # this means that you're running JupyterHub not with Hub in k8s, # and not using run_local.sh (which sets it to empty). if 'JUPYTERHUB_SERVICE_PREFIX' not in os . environ : global jupyter_proxy_url jupyter_proxy_url = 'localhost:8888' logging . info ( 'Setting jupyter proxy to local mode.' )
7681	def piano_roll ( annotation , * * kwargs ) : times , midi = annotation . to_interval_values ( ) return mir_eval . display . piano_roll ( times , midi = midi , * * kwargs )
4426	async def _play ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) query = query . strip ( '<>' ) if not url_rx . match ( query ) : query = f'ytsearch:{query}' tracks = await self . bot . lavalink . get_tracks ( query ) if not tracks : return await ctx . send ( 'Nothing found!' ) embed = discord . Embed ( color = discord . Color . blurple ( ) ) if 'list' in query and 'ytsearch:' not in query : for track in tracks : player . add ( requester = ctx . author . id , track = track ) embed . title = 'Playlist enqueued!' embed . description = f'Imported {len(tracks)} tracks from the playlist!' await ctx . send ( embed = embed ) else : track_title = tracks [ 0 ] [ "info" ] [ "title" ] track_uri = tracks [ 0 ] [ "info" ] [ "uri" ] embed . title = "Track enqueued!" embed . description = f'[{track_title}]({track_uri})' player . add ( requester = ctx . author . id , track = tracks [ 0 ] ) if not player . is_playing : await player . play ( )
7342	def set_debug ( ) : logging . basicConfig ( level = logging . WARNING ) peony . logger . setLevel ( logging . DEBUG )
3177	def create ( self , list_id , data ) : self . list_id = list_id if 'name' not in data : raise KeyError ( 'The list merge field must have a name' ) if 'type' not in data : raise KeyError ( 'The list merge field must have a type' ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'merge-fields' ) , data = data ) if response is not None : self . merge_id = response [ 'merge_id' ] else : self . merge_id = None return response
11876	def getch ( ) : try : termios . tcsetattr ( _fd , termios . TCSANOW , _new_settings ) ch = sys . stdin . read ( 1 ) finally : termios . tcsetattr ( _fd , termios . TCSADRAIN , _old_settings ) return ch
8027	def groupBy ( groups_in , classifier , fun_desc = '?' , keep_uniques = False , * args , * * kwargs ) : groups , count , group_count = { } , 0 , len ( groups_in ) for pos , paths in enumerate ( groups_in . values ( ) ) : out . write ( "Subdividing group %d of %d by %s... (%d files examined, %d " "in current group)" % ( pos + 1 , group_count , fun_desc , count , len ( paths ) ) ) for key , group in classifier ( paths , * args , * * kwargs ) . items ( ) : groups . setdefault ( key , set ( ) ) . update ( group ) count += len ( group ) if not keep_uniques : # Return only the groups with more than one file. groups = dict ( [ ( x , groups [ x ] ) for x in groups if len ( groups [ x ] ) > 1 ] ) out . write ( "Found %s sets of files with identical %s. (%d files examined)" % ( len ( groups ) , fun_desc , count ) , newline = True ) return groups
528	def _getInputNeighborhood ( self , centerInput ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions ) else : return topology . neighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions )
8011	def from_request ( cls , request , webhook_id = PAYPAL_WEBHOOK_ID ) : headers = fix_django_headers ( request . META ) assert headers try : body = request . body . decode ( request . encoding or "utf-8" ) except Exception : body = "(error decoding body)" ip = request . META [ "REMOTE_ADDR" ] obj = cls . objects . create ( headers = headers , body = body , remote_ip = ip ) try : obj . valid = obj . verify ( PAYPAL_WEBHOOK_ID ) if obj . valid : # Process the item (do not save it, it'll get saved below) obj . process ( save = False ) except Exception as e : max_length = WebhookEventTrigger . _meta . get_field ( "exception" ) . max_length obj . exception = str ( e ) [ : max_length ] obj . traceback = format_exc ( ) finally : obj . save ( ) return obj
13496	def bump ( self , target ) : if target == 'patch' : return Version ( self . major , self . minor , self . patch + 1 ) if target == 'minor' : return Version ( self . major , self . minor + 1 , 0 ) if target == 'major' : return Version ( self . major + 1 , 0 , 0 ) return self . clone ( )
5518	def clone ( self ) : return StreamThrottle ( read = self . read . clone ( ) , write = self . write . clone ( ) )
13296	def decode_jsonld ( jsonld_text ) : decoder = json . JSONDecoder ( object_pairs_hook = _decode_object_pairs ) return decoder . decode ( jsonld_text )
7400	def up ( self ) : self . swap ( self . get_ordering_queryset ( ) . filter ( order__lt = self . order ) . order_by ( '-order' ) )
5332	def get_panels ( config ) : task = TaskPanels ( config ) task . execute ( ) task = TaskPanelsMenu ( config ) task . execute ( ) logging . info ( "Panels creation finished!" )
872	def _setPath ( cls ) : cls . _path = os . path . join ( os . environ [ 'NTA_DYNAMIC_CONF_DIR' ] , cls . customFileName )
8770	def _lswitch_select_open ( self , context , switches = None , * * kwargs ) : if switches is not None : for res in switches [ "results" ] : count = res [ "_relations" ] [ "LogicalSwitchStatus" ] [ "lport_count" ] if ( self . limits [ 'max_ports_per_switch' ] == 0 or count < self . limits [ 'max_ports_per_switch' ] ) : return res [ "uuid" ] return None
209	def invert ( self ) : arr_inv = HeatmapsOnImage . from_0to1 ( 1 - self . arr_0to1 , shape = self . shape , min_value = self . min_value , max_value = self . max_value ) arr_inv . arr_was_2d = self . arr_was_2d return arr_inv
7794	def register_fetcher ( self , object_class , fetcher_class ) : self . _lock . acquire ( ) try : cache = self . _caches . get ( object_class ) if not cache : cache = Cache ( self . max_items , self . default_freshness_period , self . default_expiration_period , self . default_purge_period ) self . _caches [ object_class ] = cache cache . set_fetcher ( fetcher_class ) finally : self . _lock . release ( )
4301	def setup_database ( config_data ) : with chdir ( config_data . project_directory ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) commands = [ ] commands . append ( [ sys . executable , '-W' , 'ignore' , 'manage.py' , 'migrate' ] , ) if config_data . verbose : sys . stdout . write ( 'Database setup commands: {0}\n' . format ( ', ' . join ( [ ' ' . join ( cmd ) for cmd in commands ] ) ) ) for command in commands : try : output = subprocess . check_output ( command , env = env , stderr = subprocess . STDOUT ) sys . stdout . write ( output . decode ( 'utf-8' ) ) except subprocess . CalledProcessError as e : # pragma: no cover if config_data . verbose : sys . stdout . write ( e . output . decode ( 'utf-8' ) ) raise if not config_data . no_user : sys . stdout . write ( 'Creating admin user\n' ) if config_data . noinput : create_user ( config_data ) else : subprocess . check_call ( ' ' . join ( [ sys . executable , '-W' , 'ignore' , 'manage.py' , 'createsuperuser' ] ) , shell = True , stderr = subprocess . STDOUT )
3720	def ionic_strength ( mis , zis ) : return 0.5 * sum ( [ mi * zi * zi for mi , zi in zip ( mis , zis ) ] )
1591	def _setup_custom_grouping ( self , topology ) : for i in range ( len ( topology . bolts ) ) : for in_stream in topology . bolts [ i ] . inputs : if in_stream . stream . component_name == self . my_component_name and in_stream . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) : # this bolt takes my output in custom grouping manner if in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) : custom_grouping_obj = default_serializer . deserialize ( in_stream . custom_grouping_object ) if isinstance ( custom_grouping_obj , str ) : pex_loader . load_pex ( self . topology_pex_abs_path ) grouping_cls = pex_loader . import_and_get_class ( self . topology_pex_abs_path , custom_grouping_obj ) custom_grouping_obj = grouping_cls ( ) assert isinstance ( custom_grouping_obj , ICustomGrouping ) self . custom_grouper . add ( in_stream . stream . id , self . _get_taskids_for_component ( topology . bolts [ i ] . comp . name ) , custom_grouping_obj , self . my_component_name ) elif in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "JAVA_OBJECT" ) : raise NotImplementedError ( "Java-serialized custom grouping is not yet supported " "for python topology" ) else : raise ValueError ( "Unrecognized custom grouping type found: %s" % str ( in_stream . type ) )
4482	def storages ( self ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : yield Storage ( store , self . session )
6427	def dist ( self , src , tar ) : if src == tar : return 0.0 src = src . encode ( 'utf-8' ) tar = tar . encode ( 'utf-8' ) src_comp = bz2 . compress ( src , self . _level ) [ 10 : ] tar_comp = bz2 . compress ( tar , self . _level ) [ 10 : ] concat_comp = bz2 . compress ( src + tar , self . _level ) [ 10 : ] concat_comp2 = bz2 . compress ( tar + src , self . _level ) [ 10 : ] return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
10520	def oneup ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarvertical ( window_name , object_name ) : raise LdtpServerException ( 'Object not vertical scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 minValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue <= 0 : raise LdtpServerException ( 'Minimum limit reached' ) object_handle . AXValue -= minValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to decrease scrollbar' )
5673	def from_directory_as_inmemory_db ( cls , gtfs_directory ) : # this import is here to avoid circular imports (which turned out to be a problem) from gtfspy . import_gtfs import import_gtfs conn = sqlite3 . connect ( ":memory:" ) import_gtfs ( gtfs_directory , conn , preserve_connection = True , print_progress = False ) return cls ( conn )
12727	def stop_erps ( self , stop_erps ) : _set_params ( self . ode_obj , 'StopERP' , stop_erps , self . ADOF + self . LDOF )
7958	def handle_read ( self ) : with self . lock : logger . debug ( "handle_read()" ) if self . _eof or self . _socket is None : return if self . _state == "tls-handshake" : while True : logger . debug ( "tls handshake read..." ) self . _continue_tls_handshake ( ) logger . debug ( " state: {0}" . format ( self . _tls_state ) ) if self . _tls_state != "want_read" : break elif self . _tls_state == "connected" : while self . _socket and not self . _eof : logger . debug ( "tls socket read..." ) try : data = self . _socket . read ( 4096 ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_READ : break elif err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : break else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING_ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( "Connection reset by peer" ) data = None else : raise self . _feed_reader ( data ) else : while self . _socket and not self . _eof : logger . debug ( "raw socket read..." ) try : data = self . _socket . recv ( 4096 ) except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue elif err . args [ 0 ] in BLOCKING_ERRORS : break elif err . args [ 0 ] == errno . ECONNRESET : logger . warning ( "Connection reset by peer" ) data = None else : raise self . _feed_reader ( data )
5634	def make_toc ( sections , maxdepth = 0 ) : if not sections : return [ ] outer = min ( n for n , t in sections ) refs = [ ] for ind , sec in sections : if maxdepth and ind - outer + 1 > maxdepth : continue ref = sec . lower ( ) ref = ref . replace ( '`' , '' ) ref = ref . replace ( ' ' , '-' ) ref = ref . replace ( '?' , '' ) refs . append ( " " * ( ind - outer ) + "- [%s](#%s)" % ( sec , ref ) ) return refs
885	def activatePredictedColumn ( self , column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) : return self . _activatePredictedColumn ( self . connections , self . _random , columnActiveSegments , prevActiveCells , prevWinnerCells , self . numActivePotentialSynapsesForSegment , self . maxNewSynapseCount , self . initialPermanence , self . permanenceIncrement , self . permanenceDecrement , self . maxSynapsesPerSegment , learn )
2281	def to_csv ( self , fname_radical , * * kwargs ) : if self . data is not None : self . data . to_csv ( fname_radical + '_data.csv' , index = False , * * kwargs ) pd . DataFrame ( self . adjacency_matrix ) . to_csv ( fname_radical + '_target.csv' , index = False , * * kwargs ) else : raise ValueError ( "Graph has not yet been generated. \ Use self.generate() to do so." )
13524	def safe_joinall ( greenlets , timeout = None , raise_error = False ) : greenlets = list ( greenlets ) try : gevent . joinall ( greenlets , timeout = timeout , raise_error = raise_error ) except gevent . GreenletExit : [ greenlet . kill ( ) for greenlet in greenlets if not greenlet . ready ( ) ] raise return greenlets
12105	def _qsub_block ( self , output_dir , error_dir , tid_specs ) : processes = [ ] job_names = [ ] for ( tid , spec ) in tid_specs : job_name = "%s_%s_tid_%d" % ( self . batch_name , self . job_timestamp , tid ) job_names . append ( job_name ) cmd_args = self . command ( self . command . _formatter ( spec ) , tid , self . _launchinfo ) popen_args = self . _qsub_args ( [ ( "-e" , error_dir ) , ( '-N' , job_name ) , ( "-o" , output_dir ) ] , cmd_args ) p = subprocess . Popen ( popen_args , stdout = subprocess . PIPE ) ( stdout , stderr ) = p . communicate ( ) self . debug ( stdout ) if p . poll ( ) != 0 : raise EnvironmentError ( "qsub command exit with code: %d" % p . poll ( ) ) processes . append ( p ) self . message ( "Invoked qsub for %d commands" % len ( processes ) ) if ( self . reduction_fn is not None ) or self . dynamic : self . _qsub_collate_and_launch ( output_dir , error_dir , job_names )
5935	def irecarray_to_py ( a ) : pytypes = [ pyify ( typestr ) for name , typestr in a . dtype . descr ] def convert_record ( r ) : return tuple ( [ converter ( value ) for converter , value in zip ( pytypes , r ) ] ) return ( convert_record ( r ) for r in a )
2177	def authorized ( self ) : if self . _client . client . signature_method == SIGNATURE_RSA : # RSA only uses resource_owner_key return bool ( self . _client . client . resource_owner_key ) else : # other methods of authentication use all three pieces return ( bool ( self . _client . client . client_secret ) and bool ( self . _client . client . resource_owner_key ) and bool ( self . _client . client . resource_owner_secret ) )
3530	def get_identity ( context , prefix = None , identity_func = None , user = None ) : if prefix is not None : try : return context [ '%s_identity' % prefix ] except KeyError : pass try : return context [ 'analytical_identity' ] except KeyError : pass if getattr ( settings , 'ANALYTICAL_AUTO_IDENTIFY' , True ) : try : if user is None : user = get_user_from_context ( context ) if get_user_is_authenticated ( user ) : if identity_func is not None : return identity_func ( user ) else : return user . get_username ( ) except ( KeyError , AttributeError ) : pass return None
5540	def open ( self , input_id , * * kwargs ) : if not isinstance ( input_id , str ) : return input_id . open ( self . tile , * * kwargs ) if input_id not in self . params [ "input" ] : raise ValueError ( "%s not found in config as input file" % input_id ) return self . params [ "input" ] [ input_id ] . open ( self . tile , * * kwargs )
11905	def snoise2d ( size , z = 0.0 , scale = 0.05 , octaves = 1 , persistence = 0.25 , lacunarity = 2.0 ) : import noise data = np . empty ( size , dtype = 'float32' ) for y in range ( size [ 0 ] ) : for x in range ( size [ 1 ] ) : v = noise . snoise3 ( x * scale , y * scale , z , octaves = octaves , persistence = persistence , lacunarity = lacunarity ) data [ x , y ] = v data = data * 0.5 + 0.5 if __debug__ : assert data . min ( ) >= 0. and data . max ( ) <= 1.0 return data
7071	def recall ( ntp , nfn ) : if ( ntp + nfn ) > 0 : return ntp / ( ntp + nfn ) else : return np . nan
462	def exit_tensorflow ( sess = None , port = 6006 ) : text = "[TL] Close tensorboard and nvidia-process if available" text2 = "[TL] Close tensorboard and nvidia-process not yet supported by this function (tl.ops.exit_tf) on " if sess is not None : sess . close ( ) if _platform == "linux" or _platform == "linux2" : tl . logging . info ( 'linux: %s' % text ) os . system ( 'nvidia-smi' ) os . system ( 'fuser ' + port + '/tcp -k' ) # kill tensorboard 6006 os . system ( "nvidia-smi | grep python |awk '{print $3}'|xargs kill" ) # kill all nvidia-smi python process _exit ( ) elif _platform == "darwin" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( "lsof -i tcp:" + str ( port ) + " | grep -v PID | awk '{print $2}' | xargs kill" , shell = True ) # kill tensorboard elif _platform == "win32" : raise NotImplementedError ( "this function is not supported on the Windows platform" ) else : tl . logging . info ( text2 + _platform )
5334	def get_params_parser ( ) : parser = argparse . ArgumentParser ( add_help = False ) parser . add_argument ( '-g' , '--debug' , dest = 'debug' , action = 'store_true' , help = argparse . SUPPRESS ) parser . add_argument ( "--arthur" , action = 'store_true' , dest = 'arthur' , help = "Enable arthur to collect raw data" ) parser . add_argument ( "--raw" , action = 'store_true' , dest = 'raw' , help = "Activate raw task" ) parser . add_argument ( "--enrich" , action = 'store_true' , dest = 'enrich' , help = "Activate enrich task" ) parser . add_argument ( "--identities" , action = 'store_true' , dest = 'identities' , help = "Activate merge identities task" ) parser . add_argument ( "--panels" , action = 'store_true' , dest = 'panels' , help = "Activate panels task" ) parser . add_argument ( "--cfg" , dest = 'cfg_path' , help = "Configuration file path" ) parser . add_argument ( "--backends" , dest = 'backend_sections' , default = [ ] , nargs = '*' , help = "Backend sections to execute" ) if len ( sys . argv ) == 1 : parser . print_help ( ) sys . exit ( 1 ) return parser
2820	def convert_instancenorm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting instancenorm ...' ) if names == 'short' : tf_name = 'IN' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) assert ( len ( inputs ) == 3 ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) # Use previously taken constants if inputs [ - 2 ] + '_np' in layers : gamma = layers [ inputs [ - 2 ] + '_np' ] else : gamma = weights [ weights_name ] . numpy ( ) if inputs [ - 1 ] + '_np' in layers : beta = layers [ inputs [ - 1 ] + '_np' ] else : beta = weights [ bias_name ] . numpy ( ) def target_layer ( x , epsilon = params [ 'epsilon' ] , gamma = gamma , beta = beta ) : layer = tf . contrib . layers . instance_norm ( x , param_initializers = { 'beta' : tf . constant_initializer ( beta ) , 'gamma' : tf . constant_initializer ( gamma ) } , epsilon = epsilon , data_format = 'NCHW' , trainable = False ) return layer lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
3899	def main ( ) : parser = argparse . ArgumentParser ( ) parser . add_argument ( 'protofilepath' ) args = parser . parse_args ( ) out_file = compile_protofile ( args . protofilepath ) with open ( out_file , 'rb' ) as proto_file : # pylint: disable=no-member file_descriptor_set = descriptor_pb2 . FileDescriptorSet . FromString ( proto_file . read ( ) ) # pylint: enable=no-member for file_descriptor in file_descriptor_set . file : # Build dict of location tuples locations = { } for location in file_descriptor . source_code_info . location : locations [ tuple ( location . path ) ] = location # Add comment to top print ( make_comment ( 'This file was automatically generated from {} and ' 'should not be edited directly.' . format ( args . protofilepath ) ) ) # Generate documentation for index , message_desc in enumerate ( file_descriptor . message_type ) : generate_message_doc ( message_desc , locations , ( 4 , index ) ) for index , enum_desc in enumerate ( file_descriptor . enum_type ) : generate_enum_doc ( enum_desc , locations , ( 5 , index ) )
12047	def msgDict ( d , matching = None , sep1 = "=" , sep2 = "\n" , sort = True , cantEndWith = None ) : msg = "" if "record" in str ( type ( d ) ) : keys = d . dtype . names else : keys = d . keys ( ) if sort : keys = sorted ( keys ) for key in keys : if key [ 0 ] == "_" : continue if matching : if not key in matching : continue if cantEndWith and key [ - len ( cantEndWith ) ] == cantEndWith : continue if 'float' in str ( type ( d [ key ] ) ) : s = "%.02f" % d [ key ] else : s = str ( d [ key ] ) if "object" in s : s = '<object>' msg += key + sep1 + s + sep2 return msg . strip ( )
5848	def get_preferred_credentials ( api_key , site , cred_file = DEFAULT_CITRINATION_CREDENTIALS_FILE ) : profile_api_key , profile_site = get_credentials_from_file ( cred_file ) if api_key is None : api_key = os . environ . get ( citr_env_vars . CITRINATION_API_KEY ) if api_key is None or len ( api_key ) == 0 : api_key = profile_api_key if site is None : site = os . environ . get ( citr_env_vars . CITRINATION_SITE ) if site is None or len ( site ) == 0 : site = profile_site if site is None : site = "https://citrination.com" return api_key , site
13859	def contents ( self , f , text ) : text += self . _read ( f . abs_path ) + "\r\n" return text
1611	def ParseNolintSuppressions ( filename , raw_line , linenum , error ) : matched = Search ( r'\bNOLINT(NEXTLINE)?\b(\([^)]+\))?' , raw_line ) if matched : if matched . group ( 1 ) : suppressed_line = linenum + 1 else : suppressed_line = linenum category = matched . group ( 2 ) if category in ( None , '(*)' ) : # => "suppress all" _error_suppressions . setdefault ( None , set ( ) ) . add ( suppressed_line ) else : if category . startswith ( '(' ) and category . endswith ( ')' ) : category = category [ 1 : - 1 ] if category in _ERROR_CATEGORIES : _error_suppressions . setdefault ( category , set ( ) ) . add ( suppressed_line ) elif category not in _LEGACY_ERROR_CATEGORIES : error ( filename , linenum , 'readability/nolint' , 5 , 'Unknown NOLINT error category: %s' % category )
6590	def put ( self , package ) : pkgidx = self . workingArea . put_package ( package ) logger = logging . getLogger ( __name__ ) logger . info ( 'submitting {}' . format ( self . workingArea . package_relpath ( pkgidx ) ) ) runid = self . dispatcher . run ( self . workingArea , pkgidx ) self . runid_pkgidx_map [ runid ] = pkgidx return pkgidx
11167	def keys ( self ) : return self . options . keys ( ) + [ p . name for p in self . positional_args ]
12535	def copy_files_to_other_folder ( self , output_folder , rename_files = True , mkdir = True , verbose = False ) : import shutil if not os . path . exists ( output_folder ) : os . mkdir ( output_folder ) if not rename_files : for dcmf in self . items : outf = os . path . join ( output_folder , os . path . basename ( dcmf ) ) if verbose : print ( '{} -> {}' . format ( dcmf , outf ) ) shutil . copyfile ( dcmf , outf ) else : n_pad = len ( self . items ) + 2 for idx , dcmf in enumerate ( self . items ) : outf = '{number:0{width}d}.dcm' . format ( width = n_pad , number = idx ) outf = os . path . join ( output_folder , outf ) if verbose : print ( '{} -> {}' . format ( dcmf , outf ) ) shutil . copyfile ( dcmf , outf )
3911	def _on_event ( self , _ ) : # TODO: handle adding new conversations self . sort ( key = lambda conv_button : conv_button . last_modified , reverse = True )
9882	def alpha ( reliability_data = None , value_counts = None , value_domain = None , level_of_measurement = 'interval' , dtype = np . float64 ) : if ( reliability_data is None ) == ( value_counts is None ) : raise ValueError ( "Either reliability_data or value_counts must be provided, but not both." ) # Don't know if it's a list or numpy array. If it's the latter, the truth value is ambiguous. So, ask for None. if value_counts is None : if type ( reliability_data ) is not np . ndarray : reliability_data = np . array ( reliability_data ) value_domain = value_domain or np . unique ( reliability_data [ ~ np . isnan ( reliability_data ) ] ) value_counts = _reliability_data_to_value_counts ( reliability_data , value_domain ) else : # elif reliability_data is None if value_domain : assert value_counts . shape [ 1 ] == len ( value_domain ) , "The value domain should be equal to the number of columns of value_counts." else : value_domain = tuple ( range ( value_counts . shape [ 1 ] ) ) distance_metric = _distance_metric ( level_of_measurement ) o = _coincidences ( value_counts , value_domain , dtype = dtype ) n_v = np . sum ( o , axis = 0 ) n = np . sum ( n_v ) e = _random_coincidences ( value_domain , n , n_v ) d = _distances ( value_domain , distance_metric , n_v ) return 1 - np . sum ( o * d ) / np . sum ( e * d )
7983	def registration_error ( self , stanza ) : self . lock . acquire ( ) try : err = stanza . get_error ( ) ae = err . xpath_eval ( "e:*" , { "e" : "jabber:iq:auth:error" } ) if ae : ae = ae [ 0 ] . name else : ae = err . get_condition ( ) . name raise RegistrationError ( "Authentication error condition: %s" % ( ae , ) ) finally : self . lock . release ( )
142	def from_shapely ( polygon_shapely , label = None ) : # load shapely lazily, which makes the dependency more optional import shapely . geometry ia . do_assert ( isinstance ( polygon_shapely , shapely . geometry . Polygon ) ) # polygon_shapely.exterior can be None if the polygon was instantiated without points if polygon_shapely . exterior is None or len ( polygon_shapely . exterior . coords ) == 0 : return Polygon ( [ ] , label = label ) exterior = np . float32 ( [ [ x , y ] for ( x , y ) in polygon_shapely . exterior . coords ] ) return Polygon ( exterior , label = label )
768	def getMetricDetails ( self , metricLabel ) : try : metricIndex = self . __metricLabels . index ( metricLabel ) except IndexError : return None return self . __metrics [ metricIndex ] . getMetric ( )
7956	def _initiate_starttls ( self , * * kwargs ) : if self . _tls_state == "connected" : raise RuntimeError ( "Already TLS-connected" ) kwargs [ "do_handshake_on_connect" ] = False logger . debug ( "Wrapping the socket into ssl" ) self . _socket = ssl . wrap_socket ( self . _socket , * * kwargs ) self . _set_state ( "tls-handshake" ) self . _continue_tls_handshake ( )
6270	def swap_buffers ( self ) : if not self . window . context : return self . frames += 1 self . window . flip ( ) self . window . dispatch_events ( )
6063	def mass_within_ellipse_in_units ( self , major_axis , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : self . check_units_of_radius_and_critical_surface_density ( radius = major_axis , critical_surface_density = critical_surface_density ) profile = self . new_profile_with_units_converted ( unit_length = major_axis . unit_length , unit_mass = 'angular' , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) mass_angular = dim . Mass ( value = quad ( profile . mass_integral , a = 0.0 , b = major_axis , args = ( self . axis_ratio , ) ) [ 0 ] , unit_mass = 'angular' ) return mass_angular . convert ( unit_mass = unit_mass , critical_surface_density = critical_surface_density )
3762	def Tt ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in Staveley_data . index : methods . append ( STAVELEY ) if Tm ( CASRN ) : methods . append ( MELTING ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == STAVELEY : Tt = Staveley_data . at [ CASRN , "Tt68" ] elif Method == MELTING : Tt = Tm ( CASRN ) elif Method == NONE : Tt = None else : raise Exception ( 'Failure in in function' ) return Tt
6783	def lock ( self ) : self . init ( ) r = self . local_renderer if self . file_exists ( r . env . lockfile_path ) : raise exceptions . AbortDeployment ( 'Lock file %s exists. Perhaps another deployment is currently underway?' % r . env . lockfile_path ) else : self . vprint ( 'Locking %s.' % r . env . lockfile_path ) r . env . hostname = socket . gethostname ( ) r . run_or_local ( 'echo "{hostname}" > {lockfile_path}' )
1043	def generic_visit ( self , node ) : for field_name in node . _fields : setattr ( node , field_name , self . visit ( getattr ( node , field_name ) ) ) return node
4983	def get ( self , request , enterprise_uuid , course_id ) : # Check to see if access to the course run is restricted for this user. embargo_url = EmbargoApiClient . redirect_if_blocked ( [ course_id ] , request . user , get_ip ( request ) , request . path ) if embargo_url : return redirect ( embargo_url ) enterprise_customer , course , course_run , modes = self . get_base_details ( request , enterprise_uuid , course_id ) enterprise_customer_user = get_enterprise_customer_user ( request . user . id , enterprise_uuid ) data_sharing_consent = DataSharingConsent . objects . proxied_get ( username = enterprise_customer_user . username , course_id = course_id , enterprise_customer = enterprise_customer ) enrollment_client = EnrollmentApiClient ( ) enrolled_course = enrollment_client . get_course_enrollment ( request . user . username , course_id ) try : enterprise_course_enrollment = EnterpriseCourseEnrollment . objects . get ( enterprise_customer_user__enterprise_customer = enterprise_customer , enterprise_customer_user__user_id = request . user . id , course_id = course_id ) except EnterpriseCourseEnrollment . DoesNotExist : enterprise_course_enrollment = None if enrolled_course and enterprise_course_enrollment : # The user is already enrolled in the course through the Enterprise Customer, so redirect to the course # info page. return redirect ( LMS_COURSEWARE_URL . format ( course_id = course_id ) ) return self . get_enterprise_course_enrollment_page ( request , enterprise_customer , course , course_run , modes , enterprise_course_enrollment , data_sharing_consent , )
9310	def amz_cano_path ( self , path ) : safe_chars = '/~' qs = '' fixed_path = path if '?' in fixed_path : fixed_path , qs = fixed_path . split ( '?' , 1 ) fixed_path = posixpath . normpath ( fixed_path ) fixed_path = re . sub ( '/+' , '/' , fixed_path ) if path . endswith ( '/' ) and not fixed_path . endswith ( '/' ) : fixed_path += '/' full_path = fixed_path # If Python 2, switch to working entirely in str as quote() has problems # with Unicode if PY2 : full_path = full_path . encode ( 'utf-8' ) safe_chars = safe_chars . encode ( 'utf-8' ) qs = qs . encode ( 'utf-8' ) # S3 seems to require unquoting first. 'host' service is used in # amz_testsuite tests if self . service in [ 's3' , 'host' ] : full_path = unquote ( full_path ) full_path = quote ( full_path , safe = safe_chars ) if qs : qm = b'?' if PY2 else '?' full_path = qm . join ( ( full_path , qs ) ) if PY2 : full_path = unicode ( full_path ) return full_path
8600	def add_share ( self , group_id , resource_id , * * kwargs ) : properties = { } for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/groups/%s/shares/%s' % ( group_id , resource_id ) , method = 'POST' , data = json . dumps ( data ) ) return response
399	def binary_cross_entropy ( output , target , epsilon = 1e-8 , name = 'bce_loss' ) : # with ops.op_scope([output, target], name, "bce_loss") as name: # output = ops.convert_to_tensor(output, name="preds") # target = ops.convert_to_tensor(targets, name="target") # with tf.name_scope(name): return tf . reduce_mean ( tf . reduce_sum ( - ( target * tf . log ( output + epsilon ) + ( 1. - target ) * tf . log ( 1. - output + epsilon ) ) , axis = 1 ) , name = name )
4961	def paginated_list ( object_list , page , page_size = 25 ) : paginator = CustomPaginator ( object_list , page_size ) try : object_list = paginator . page ( page ) except PageNotAnInteger : object_list = paginator . page ( 1 ) except EmptyPage : object_list = paginator . page ( paginator . num_pages ) page_range = [ ] page_num = object_list . number # If there are 10 or fewer pages, display links to every page. # Otherwise, do some fancy if paginator . num_pages <= 10 : page_range = range ( paginator . num_pages ) else : # Insert "smart" pagination links, so that there are always ON_ENDS # links at either end of the list of pages, and there are always # ON_EACH_SIDE links at either end of the "current page" link. if page_num > ( PAGES_ON_EACH_SIDE + PAGES_ON_ENDS + 1 ) : page_range . extend ( range ( 1 , PAGES_ON_ENDS + 1 ) ) page_range . append ( DOT ) page_range . extend ( range ( page_num - PAGES_ON_EACH_SIDE , page_num + 1 ) ) else : page_range . extend ( range ( 1 , page_num + 1 ) ) if page_num < ( paginator . num_pages - PAGES_ON_EACH_SIDE - PAGES_ON_ENDS ) : page_range . extend ( range ( page_num + 1 , page_num + PAGES_ON_EACH_SIDE + 1 ) ) page_range . append ( DOT ) page_range . extend ( range ( paginator . num_pages + 1 - PAGES_ON_ENDS , paginator . num_pages + 1 ) ) else : page_range . extend ( range ( page_num + 1 , paginator . num_pages + 1 ) ) # Override page range to implement custom smart links. object_list . paginator . page_range = page_range return object_list
6484	def _process_field_values ( request ) : return { field_key : request . POST [ field_key ] for field_key in request . POST if field_key in course_discovery_filter_fields ( ) }
5253	def assemble_one ( asmcode , pc = 0 , fork = DEFAULT_FORK ) : try : instruction_table = instruction_tables [ fork ] asmcode = asmcode . strip ( ) . split ( ' ' ) instr = instruction_table [ asmcode [ 0 ] . upper ( ) ] if pc : instr . pc = pc if instr . operand_size > 0 : assert len ( asmcode ) == 2 instr . operand = int ( asmcode [ 1 ] , 0 ) return instr except : raise AssembleError ( "Something wrong at pc %d" % pc )
13171	def iter ( self , name = None ) : for c in self . _children : if name is None or c . tagname == name : yield c for gc in c . find ( name ) : yield gc
4554	def pointOnCircle ( cx , cy , radius , angle ) : angle = math . radians ( angle ) - ( math . pi / 2 ) x = cx + radius * math . cos ( angle ) if x < cx : x = math . ceil ( x ) else : x = math . floor ( x ) y = cy + radius * math . sin ( angle ) if y < cy : y = math . ceil ( y ) else : y = math . floor ( y ) return ( int ( x ) , int ( y ) )
5007	def get_user_from_social_auth ( tpa_provider , tpa_username ) : user_social_auth = UserSocialAuth . objects . select_related ( 'user' ) . filter ( user__username = tpa_username , provider = tpa_provider . backend_name ) . first ( ) return user_social_auth . user if user_social_auth else None
2176	def request ( self , method , url , data = None , headers = None , withhold_token = False , client_id = None , client_secret = None , * * kwargs ) : if not is_secure_transport ( url ) : raise InsecureTransportError ( ) if self . token and not withhold_token : log . debug ( "Invoking %d protected resource request hooks." , len ( self . compliance_hook [ "protected_request" ] ) , ) for hook in self . compliance_hook [ "protected_request" ] : log . debug ( "Invoking hook %s." , hook ) url , headers , data = hook ( url , headers , data ) log . debug ( "Adding token %s to request." , self . token ) try : url , headers , data = self . _client . add_token ( url , http_method = method , body = data , headers = headers ) # Attempt to retrieve and save new access token if expired except TokenExpiredError : if self . auto_refresh_url : log . debug ( "Auto refresh is set, attempting to refresh at %s." , self . auto_refresh_url , ) # We mustn't pass auth twice. auth = kwargs . pop ( "auth" , None ) if client_id and client_secret and ( auth is None ) : log . debug ( 'Encoding client_id "%s" with client_secret as Basic auth credentials.' , client_id , ) auth = requests . auth . HTTPBasicAuth ( client_id , client_secret ) token = self . refresh_token ( self . auto_refresh_url , auth = auth , * * kwargs ) if self . token_updater : log . debug ( "Updating token to %s using %s." , token , self . token_updater ) self . token_updater ( token ) url , headers , data = self . _client . add_token ( url , http_method = method , body = data , headers = headers ) else : raise TokenUpdated ( token ) else : raise log . debug ( "Requesting url %s using method %s." , url , method ) log . debug ( "Supplying headers %s and data %s" , headers , data ) log . debug ( "Passing through key word arguments %s." , kwargs ) return super ( OAuth2Session , self ) . request ( method , url , headers = headers , data = data , * * kwargs )
13219	def shell ( self , expect = pexpect ) : dsn = self . connection_dsn ( ) log . debug ( 'connection string: %s' % dsn ) child = expect . spawn ( 'psql "%s"' % dsn ) if self . _connect_args [ 'password' ] is not None : child . expect ( 'Password: ' ) child . sendline ( self . _connect_args [ 'password' ] ) child . interact ( )
3734	def load_included_indentifiers ( self , file_name ) : self . restrict_identifiers = True included_identifiers = set ( ) with open ( file_name ) as f : [ included_identifiers . add ( int ( line ) ) for line in f ] self . included_identifiers = included_identifiers
6453	def dist ( self , src , tar ) : if tar == src : return 0.0 if not src or not tar : return 1.0 max_length = max ( len ( src ) , len ( tar ) ) return self . dist_abs ( src , tar ) / max_length
9724	async def load_project ( self , project_path ) : cmd = "loadproject %s" % project_path return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
3531	def is_internal_ip ( context , prefix = None ) : try : request = context [ 'request' ] remote_ip = request . META . get ( 'HTTP_X_FORWARDED_FOR' , '' ) if not remote_ip : remote_ip = request . META . get ( 'REMOTE_ADDR' , '' ) if not remote_ip : return False internal_ips = None if prefix is not None : internal_ips = getattr ( settings , '%s_INTERNAL_IPS' % prefix , None ) if internal_ips is None : internal_ips = getattr ( settings , 'ANALYTICAL_INTERNAL_IPS' , None ) if internal_ips is None : internal_ips = getattr ( settings , 'INTERNAL_IPS' , None ) return remote_ip in ( internal_ips or [ ] ) except ( KeyError , AttributeError ) : return False
11603	def parse_byteranges ( cls , environ ) : r = [ ] s = environ . get ( cls . header_range , '' ) . replace ( ' ' , '' ) . lower ( ) if s : l = s . split ( '=' ) if len ( l ) == 2 : unit , vals = tuple ( l ) if unit == 'bytes' and vals : gen_rng = ( tuple ( rng . split ( '-' ) ) for rng in vals . split ( ',' ) if '-' in rng ) for start , end in gen_rng : if start or end : r . append ( ( int ( start ) if start else None , int ( end ) if end else None ) ) return r
7183	def parse_type_comment ( type_comment ) : try : result = ast3 . parse ( type_comment , '<type_comment>' , 'eval' ) except SyntaxError : raise ValueError ( f"invalid type comment: {type_comment!r}" ) from None assert isinstance ( result , ast3 . Expression ) return result . body
3491	def _sbase_annotations ( sbase , annotation ) : if not annotation or len ( annotation ) == 0 : return # standardize annotations annotation_data = deepcopy ( annotation ) for key , value in annotation_data . items ( ) : # handling of non-string annotations (e.g. integers) if isinstance ( value , ( float , int ) ) : value = str ( value ) if isinstance ( value , string_types ) : annotation_data [ key ] = [ ( "is" , value ) ] for key , value in annotation_data . items ( ) : for idx , item in enumerate ( value ) : if isinstance ( item , string_types ) : value [ idx ] = ( "is" , item ) # set metaId meta_id = "meta_{}" . format ( sbase . getId ( ) ) sbase . setMetaId ( meta_id ) # rdf_items = [] for provider , data in iteritems ( annotation_data ) : # set SBOTerm if provider in [ "SBO" , "sbo" ] : if provider == "SBO" : LOGGER . warning ( "'SBO' provider is deprecated, " "use 'sbo' provider instead" ) sbo_term = data [ 0 ] [ 1 ] _check ( sbase . setSBOTerm ( sbo_term ) , "Setting SBOTerm: {}" . format ( sbo_term ) ) # FIXME: sbo should also be written as CVTerm continue for item in data : qualifier_str , entity = item [ 0 ] , item [ 1 ] qualifier = QUALIFIER_TYPES . get ( qualifier_str , None ) if qualifier is None : qualifier = libsbml . BQB_IS LOGGER . error ( "Qualifier type is not supported on " "annotation: '{}'" . format ( qualifier_str ) ) qualifier_type = libsbml . BIOLOGICAL_QUALIFIER if qualifier_str . startswith ( "bqm_" ) : qualifier_type = libsbml . MODEL_QUALIFIER cv = libsbml . CVTerm ( ) # type: libsbml.CVTerm cv . setQualifierType ( qualifier_type ) if qualifier_type == libsbml . BIOLOGICAL_QUALIFIER : cv . setBiologicalQualifierType ( qualifier ) elif qualifier_type == libsbml . MODEL_QUALIFIER : cv . setModelQualifierType ( qualifier ) else : raise CobraSBMLError ( 'Unsupported qualifier: ' '%s' % qualifier ) resource = "%s/%s/%s" % ( URL_IDENTIFIERS_PREFIX , provider , entity ) cv . addResource ( resource ) _check ( sbase . addCVTerm ( cv ) , "Setting cvterm: {}, resource: {}" . format ( cv , resource ) )
6359	def sim ( self , src , tar ) : if src == tar : return 1.0 elif not src or not tar : return 0.0 return len ( self . lcsstr ( src , tar ) ) / max ( len ( src ) , len ( tar ) )
5635	def doc2md ( docstr , title , min_level = 1 , more_info = False , toc = True , maxdepth = 0 ) : text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 shiftlevel = 0 if level < min_level : shiftlevel = min_level - level level = min_level sections = [ ( lev + shiftlevel , tit ) for lev , tit in sections ] head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] , shiftlevel ) if more_info : return ( md , sections ) else : return "\n" . join ( md )
3516	def woopra ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return WoopraNode ( )
13289	def get_content_commit_date ( extensions , acceptance_callback = None , root_dir = '.' ) : logger = logging . getLogger ( __name__ ) def _null_callback ( _ ) : return True if acceptance_callback is None : acceptance_callback = _null_callback # Cache the repo object for each query root_dir = os . path . abspath ( root_dir ) repo = git . repo . base . Repo ( path = root_dir , search_parent_directories = True ) # Iterate over all files with all file extensions, looking for the # newest commit datetime. newest_datetime = None iters = [ _iter_filepaths_with_extension ( ext , root_dir = root_dir ) for ext in extensions ] for content_path in itertools . chain ( * iters ) : content_path = os . path . abspath ( os . path . join ( root_dir , content_path ) ) if acceptance_callback ( content_path ) : logger . debug ( 'Found content path %r' , content_path ) try : commit_datetime = read_git_commit_timestamp_for_file ( content_path , repo = repo ) logger . debug ( 'Commit timestamp of %r is %s' , content_path , commit_datetime ) except IOError : logger . warning ( 'Count not get commit for %r, skipping' , content_path ) continue if not newest_datetime or commit_datetime > newest_datetime : # Seed initial newest_datetime # or set a newer newest_datetime newest_datetime = commit_datetime logger . debug ( 'Newest commit timestamp is %s' , newest_datetime ) logger . debug ( 'Final commit timestamp is %s' , newest_datetime ) if newest_datetime is None : raise RuntimeError ( 'No content files found in {}' . format ( root_dir ) ) return newest_datetime
8410	def best_units ( self , sequence ) : # Read # [(0.9, 's'), # (9, 'm)] # as, break ranges between 0.9 seconds (inclusive) # and 9 minutes are represented in seconds. And so on. ts_range = self . value ( max ( sequence ) ) - self . value ( min ( sequence ) ) package = self . determine_package ( sequence [ 0 ] ) if package == 'pandas' : cuts = [ ( 0.9 , 'us' ) , ( 0.9 , 'ms' ) , ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = NANOSECONDS base_units = 'ns' else : cuts = [ ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = SECONDS base_units = 'ms' for size , units in reversed ( cuts ) : if ts_range >= size * denomination [ units ] : return units return base_units
8099	def copy ( self , graph ) : s = styles ( graph ) s . guide = self . guide . copy ( graph ) dict . __init__ ( s , [ ( v . name , v . copy ( ) ) for v in self . values ( ) ] ) return s
3006	def get_storage ( request ) : storage_model = oauth2_settings . storage_model user_property = oauth2_settings . storage_model_user_property credentials_property = oauth2_settings . storage_model_credentials_property if storage_model : module_name , class_name = storage_model . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) storage_model_class = getattr ( module , class_name ) return storage . DjangoORMStorage ( storage_model_class , user_property , request . user , credentials_property ) else : # use session return dictionary_storage . DictionaryStorage ( request . session , key = _CREDENTIALS_KEY )
13900	def db_to_specifier ( db_string ) : local_match = PLAIN_RE . match ( db_string ) remote_match = URL_RE . match ( db_string ) # If this looks like a local specifier: if local_match : return 'local:' + local_match . groupdict ( ) [ 'database' ] # If this looks like a remote specifier: elif remote_match : # Just a fancy way of getting 3 variables in 2 lines... hostname , portnum , database = map ( remote_match . groupdict ( ) . get , ( 'hostname' , 'portnum' , 'database' ) ) local_url = settings . _ ( 'COUCHDB_SERVER' , 'http://127.0.0.1:5984/' ) localhost , localport = urlparse . urlparse ( local_url ) [ 1 ] . split ( ':' ) # If it's the local server, then return a local specifier. if ( localhost == hostname ) and ( localport == portnum ) : return 'local:' + database # Otherwise, prepare and return the remote specifier. return 'remote:%s:%s:%s' % ( hostname , portnum , database ) # Throw a wobbly. raise ValueError ( 'Invalid database string: %r' % ( db_string , ) )
8449	def not_has_branch ( branch ) : if _has_branch ( branch ) : msg = 'Cannot proceed while {} branch exists; remove and try again.' . format ( branch ) raise temple . exceptions . ExistingBranchError ( msg )
4226	def _data_root_Linux ( ) : fallback = os . path . expanduser ( '~/.local/share' ) root = os . environ . get ( 'XDG_DATA_HOME' , None ) or fallback return os . path . join ( root , 'python_keyring' )
10453	def waittillguinotexist ( self , window_name , object_name = '' , guiTimeOut = 30 ) : timeout = 0 while timeout < guiTimeOut : if not self . guiexist ( window_name , object_name ) : return 1 # Wait 1 second before retrying time . sleep ( 1 ) timeout += 1 # Object and/or window still appears within the timeout period return 0
181	def to_polygon ( self ) : from . polys import Polygon return Polygon ( self . coords , label = self . label )
4761	def assert_that ( val , description = '' ) : global _soft_ctx if _soft_ctx : return AssertionBuilder ( val , description , 'soft' ) return AssertionBuilder ( val , description )
12021	def add_line_error ( self , line_data , error_info , log_level = logging . ERROR ) : if not error_info : return try : line_data [ 'line_errors' ] . append ( error_info ) except KeyError : line_data [ 'line_errors' ] = [ error_info ] except TypeError : # no line_data pass try : self . logger . log ( log_level , Gff3 . error_format . format ( current_line_num = line_data [ 'line_index' ] + 1 , error_type = error_info [ 'error_type' ] , message = error_info [ 'message' ] , line = line_data [ 'line_raw' ] . rstrip ( ) ) ) except AttributeError : # no logger pass
584	def _deleteRecordsFromKNN ( self , recordsToDelete ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) idsToDelete = [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] nProtos = knn . _numPatterns knn . removeIds ( idsToDelete ) assert knn . _numPatterns == nProtos - len ( idsToDelete )
9053	def posteriori_mean ( self ) : from numpy_sugar . linalg import rsolve Sigma = self . posteriori_covariance ( ) eta = self . _ep . _posterior . eta return dot ( Sigma , eta + rsolve ( GLMM . covariance ( self ) , self . mean ( ) ) )
7790	def update_item ( self , item ) : self . _lock . acquire ( ) try : state = item . update_state ( ) self . _items_list . sort ( ) if item . state == 'purged' : self . _purged += 1 if self . _purged > 0.25 * self . max_items : self . purge_items ( ) return state finally : self . _lock . release ( )
11767	def weighted_sample_with_replacement ( seq , weights , n ) : sample = weighted_sampler ( seq , weights ) return [ sample ( ) for s in range ( n ) ]
10388	def calculate_average_scores_on_graph ( graph : BELGraph , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , use_tqdm : bool = False , ) : subgraphs = generate_bioprocess_mechanisms ( graph , key = key ) scores = calculate_average_scores_on_subgraphs ( subgraphs , key = key , tag = tag , default_score = default_score , runs = runs , use_tqdm = use_tqdm ) return scores
8602	def get_user ( self , user_id , depth = 1 ) : response = self . _perform_request ( '/um/users/%s?depth=%s' % ( user_id , str ( depth ) ) ) return response
458	def alphas ( shape , alpha_value , name = None ) : with ops . name_scope ( name , "alphas" , [ shape ] ) as name : alpha_tensor = convert_to_tensor ( alpha_value ) alpha_dtype = dtypes . as_dtype ( alpha_tensor . dtype ) . base_dtype if not isinstance ( shape , ops . Tensor ) : try : shape = constant_op . _tensor_shape_tensor_conversion_function ( tensor_shape . TensorShape ( shape ) ) except ( TypeError , ValueError ) : shape = ops . convert_to_tensor ( shape , dtype = dtypes . int32 ) if not shape . _shape_tuple ( ) : shape = reshape ( shape , [ - 1 ] ) # Ensure it's a vector try : output = constant ( alpha_value , shape = shape , dtype = alpha_dtype , name = name ) except ( TypeError , ValueError ) : output = fill ( shape , constant ( alpha_value , dtype = alpha_dtype ) , name = name ) if output . dtype . base_dtype != alpha_dtype : raise AssertionError ( "Dtypes do not corresponds: %s and %s" % ( output . dtype . base_dtype , alpha_dtype ) ) return output
6933	def colormagdiagram_cpdir ( cpdir , outpkl , cpfileglob = 'checkplot*.pkl*' , color_mag1 = [ 'gaiamag' , 'sdssg' ] , color_mag2 = [ 'kmag' , 'kmag' ] , yaxis_mag = [ 'gaia_absmag' , 'rpmj' ] ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return colormagdiagram_cplist ( cplist , outpkl , color_mag1 = color_mag1 , color_mag2 = color_mag2 , yaxis_mag = yaxis_mag )
6482	def _load_class ( class_path , default ) : if class_path is None : return default component = class_path . rsplit ( '.' , 1 ) result_processor = getattr ( importlib . import_module ( component [ 0 ] ) , component [ 1 ] , default ) if len ( component ) > 1 else default return result_processor
3987	def _move_temp_binary_to_path ( tmp_binary_path ) : # pylint: disable=E1101 binary_path = _get_binary_location ( ) if not binary_path . endswith ( constants . DUSTY_BINARY_NAME ) : raise RuntimeError ( 'Refusing to overwrite binary {}' . format ( binary_path ) ) st = os . stat ( binary_path ) permissions = st . st_mode owner = st . st_uid group = st . st_gid shutil . move ( tmp_binary_path , binary_path ) os . chown ( binary_path , owner , group ) os . chmod ( binary_path , permissions ) return binary_path
6772	def install_required ( self , type = None , service = None , list_only = 0 , * * kwargs ) : # pylint: disable=redefined-builtin r = self . local_renderer list_only = int ( list_only ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE_TYPES , 'Unknown package type: %s' % ( type , ) lst = [ ] if type : types = [ type ] else : types = PACKAGE_TYPES for _type in types : if _type == SYSTEM : content = '\n' . join ( self . list_required ( type = _type , service = service ) ) if list_only : lst . extend ( _ for _ in content . split ( '\n' ) if _ . strip ( ) ) if self . verbose : print ( 'content:' , content ) break fd , fn = tempfile . mkstemp ( ) fout = open ( fn , 'w' ) fout . write ( content ) fout . close ( ) self . install_custom ( fn = fn ) else : raise NotImplementedError return lst
882	def activateCells ( self , activeColumns , learn = True ) : prevActiveCells = self . activeCells prevWinnerCells = self . winnerCells self . activeCells = [ ] self . winnerCells = [ ] segToCol = lambda segment : int ( segment . cell / self . cellsPerColumn ) identity = lambda x : x for columnData in groupby2 ( activeColumns , identity , self . activeSegments , segToCol , self . matchingSegments , segToCol ) : ( column , activeColumns , columnActiveSegments , columnMatchingSegments ) = columnData if activeColumns is not None : if columnActiveSegments is not None : cellsToAdd = self . activatePredictedColumn ( column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) self . activeCells += cellsToAdd self . winnerCells += cellsToAdd else : ( cellsToAdd , winnerCell ) = self . burstColumn ( column , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) self . activeCells += cellsToAdd self . winnerCells . append ( winnerCell ) else : if learn : self . punishPredictedColumn ( column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells )
5523	def upload_stream ( self , destination , * , offset = 0 ) : return self . get_stream ( "STOR " + str ( destination ) , "1xx" , offset = offset , )
153	def min_item ( self ) : if self . is_empty ( ) : raise ValueError ( "Tree is empty" ) node = self . _root while node . left is not None : node = node . left return node . key , node . value
12836	def render_vars ( self ) : return { 'records' : [ { 'message' : record . getMessage ( ) , 'time' : dt . datetime . fromtimestamp ( record . created ) . strftime ( '%H:%M:%S' ) , } for record in self . handler . records ] }
2154	def _read ( self , fp , fpname ) : # Attempt to read the file using the superclass implementation. # # Check the permissions of the file we are considering reading # if the file exists and the permissions expose it to reads from # other users, raise a warning if os . path . isfile ( fpname ) : file_permission = os . stat ( fpname ) if fpname != os . path . join ( tower_dir , 'tower_cli.cfg' ) and ( ( file_permission . st_mode & stat . S_IRGRP ) or ( file_permission . st_mode & stat . S_IROTH ) ) : warnings . warn ( 'File {0} readable by group or others.' . format ( fpname ) , RuntimeWarning ) # If it doesn't work because there's no section header, then # create a section header and call the superclass implementation # again. try : return configparser . ConfigParser . _read ( self , fp , fpname ) except configparser . MissingSectionHeaderError : fp . seek ( 0 ) string = '[general]\n%s' % fp . read ( ) flo = StringIO ( string ) # flo == file-like object return configparser . ConfigParser . _read ( self , flo , fpname )
3963	def start_local_env ( recreate_containers ) : assembled_spec = spec_assembler . get_assembled_specs ( ) required_absent_assets = virtualbox . required_absent_assets ( assembled_spec ) if required_absent_assets : raise RuntimeError ( 'Assets {} are specified as required but are not set. Set them with `dusty assets set`' . format ( required_absent_assets ) ) docker_ip = virtualbox . get_docker_vm_ip ( ) # Stop will fail if we've never written a Composefile before if os . path . exists ( constants . COMPOSEFILE_PATH ) : try : stop_apps_or_services ( rm_containers = recreate_containers ) except CalledProcessError as e : log_to_client ( "WARNING: docker-compose stop failed" ) log_to_client ( str ( e ) ) daemon_warnings . clear_namespace ( 'disk' ) df_info = virtualbox . get_docker_vm_disk_info ( as_dict = True ) if 'M' in df_info [ 'free' ] or 'K' in df_info [ 'free' ] : warning_msg = 'VM is low on disk. Available disk: {}' . format ( df_info [ 'free' ] ) daemon_warnings . warn ( 'disk' , warning_msg ) log_to_client ( warning_msg ) log_to_client ( "Compiling together the assembled specs" ) active_repos = spec_assembler . get_all_repos ( active_only = True , include_specs_repo = False ) log_to_client ( "Compiling the port specs" ) port_spec = port_spec_compiler . get_port_spec_document ( assembled_spec , docker_ip ) log_to_client ( "Compiling the nginx config" ) docker_bridge_ip = virtualbox . get_docker_bridge_ip ( ) nginx_config = nginx_compiler . get_nginx_configuration_spec ( port_spec , docker_bridge_ip ) log_to_client ( "Creating setup and script bash files" ) make_up_command_files ( assembled_spec , port_spec ) log_to_client ( "Compiling docker-compose config" ) compose_config = compose_compiler . get_compose_dict ( assembled_spec , port_spec ) log_to_client ( "Saving port forwarding to hosts file" ) hosts . update_hosts_file_from_port_spec ( port_spec ) log_to_client ( "Configuring NFS" ) nfs . configure_nfs ( ) log_to_client ( "Saving updated nginx config to the VM" ) nginx . update_nginx_from_config ( nginx_config ) log_to_client ( "Saving Docker Compose config and starting all containers" ) compose . update_running_containers_from_spec ( compose_config , recreate_containers = recreate_containers ) log_to_client ( "Your local environment is now started!" )
13621	def one ( func , n = 0 ) : def _one ( result ) : if _isSequenceTypeNotText ( result ) and len ( result ) > n : return func ( result [ n ] ) return None return maybe ( _one )
6286	def supports_file ( cls , meta ) : path = Path ( meta . path ) for ext in cls . file_extensions : if path . suffixes [ : len ( ext ) ] == ext : return True return False
5765	def _decrypt_encrypted_data ( encryption_algorithm_info , encrypted_content , password ) : decrypt_func = crypto_funcs [ encryption_algorithm_info . encryption_cipher ] # Modern, PKCS#5 PBES2-based encryption if encryption_algorithm_info . kdf == 'pbkdf2' : if encryption_algorithm_info . encryption_cipher == 'rc5' : raise ValueError ( pretty_message ( ''' PBES2 encryption scheme utilizing RC5 encryption is not supported ''' ) ) enc_key = pbkdf2 ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . key_length ) enc_iv = encryption_algorithm_info . encryption_iv plaintext = decrypt_func ( enc_key , encrypted_content , enc_iv ) elif encryption_algorithm_info . kdf == 'pbkdf1' : derived_output = pbkdf1 ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . key_length + 8 ) enc_key = derived_output [ 0 : 8 ] enc_iv = derived_output [ 8 : 16 ] plaintext = decrypt_func ( enc_key , encrypted_content , enc_iv ) elif encryption_algorithm_info . kdf == 'pkcs12_kdf' : enc_key = pkcs12_kdf ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . key_length , 1 # ID 1 is for generating a key ) # Since RC4 is a stream cipher, we don't use an IV if encryption_algorithm_info . encryption_cipher == 'rc4' : plaintext = decrypt_func ( enc_key , encrypted_content ) else : enc_iv = pkcs12_kdf ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . encryption_block_size , 2 # ID 2 is for generating an IV ) plaintext = decrypt_func ( enc_key , encrypted_content , enc_iv ) return plaintext
1022	def createTMs ( includeCPP = True , includePy = True , numCols = 100 , cellsPerCol = 4 , activationThreshold = 3 , minThreshold = 3 , newSynapseCount = 3 , initialPerm = 0.6 , permanenceInc = 0.1 , permanenceDec = 0.0 , globalDecay = 0.0 , pamLength = 0 , checkSynapseConsistency = True , maxInfBacktrack = 0 , maxLrnBacktrack = 0 , * * kwargs ) : # Keep these fixed: connectedPerm = 0.5 tms = dict ( ) if includeCPP : if VERBOSITY >= 2 : print "Creating BacktrackingTMCPP instance" cpp_tm = BacktrackingTMCPP ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , initialPerm = initialPerm , connectedPerm = connectedPerm , minThreshold = minThreshold , newSynapseCount = newSynapseCount , permanenceInc = permanenceInc , permanenceDec = permanenceDec , activationThreshold = activationThreshold , globalDecay = globalDecay , burnIn = 1 , seed = SEED , verbosity = VERBOSITY , checkSynapseConsistency = checkSynapseConsistency , collectStats = True , pamLength = pamLength , maxInfBacktrack = maxInfBacktrack , maxLrnBacktrack = maxLrnBacktrack , ) # Ensure we are copying over learning states for TMDiff cpp_tm . retrieveLearningStates = True tms [ 'CPP' ] = cpp_tm if includePy : if VERBOSITY >= 2 : print "Creating PY TM instance" py_tm = BacktrackingTM ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , initialPerm = initialPerm , connectedPerm = connectedPerm , minThreshold = minThreshold , newSynapseCount = newSynapseCount , permanenceInc = permanenceInc , permanenceDec = permanenceDec , activationThreshold = activationThreshold , globalDecay = globalDecay , burnIn = 1 , seed = SEED , verbosity = VERBOSITY , collectStats = True , pamLength = pamLength , maxInfBacktrack = maxInfBacktrack , maxLrnBacktrack = maxLrnBacktrack , ) tms [ 'PY ' ] = py_tm return tms
12858	def from_date ( datetime_date ) : return BusinessDate . from_ymd ( datetime_date . year , datetime_date . month , datetime_date . day )
8196	def click ( self , node ) : if not self . has_node ( node . id ) : return if node == self . root : return self . _dx , self . _dy = self . offset ( node ) self . previous = self . root . id self . load ( node . id )
13650	def get_fuel_price_trends ( self , latitude : float , longitude : float , fuel_types : List [ str ] ) -> PriceTrends : response = requests . post ( '{}/prices/trends/' . format ( API_URL_BASE ) , json = { 'location' : { 'latitude' : latitude , 'longitude' : longitude , } , 'fueltypes' : [ { 'code' : type } for type in fuel_types ] , } , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) data = response . json ( ) return PriceTrends ( variances = [ Variance . deserialize ( variance ) for variance in data [ 'Variances' ] ] , average_prices = [ AveragePrice . deserialize ( avg_price ) for avg_price in data [ 'AveragePrices' ] ] )
3953	def get_last_result ( self ) : # Retrieve the conversion register value, convert to a signed int, and # return it. result = self . _device . readList ( ADS1x15_POINTER_CONVERSION , 2 ) return self . _conversion_value ( result [ 1 ] , result [ 0 ] )
9913	def _create ( cls , model_class , * args , * * kwargs ) : manager = cls . _get_manager ( model_class ) return manager . create_user ( * args , * * kwargs )
2950	def _start ( self , my_task , force = False ) : # If the threshold was already reached, there is nothing else to do. if my_task . _has_state ( Task . COMPLETED ) : return True , None if my_task . _has_state ( Task . READY ) : return True , None # Check whether we may fire. if self . split_task is None : return self . _check_threshold_unstructured ( my_task , force ) return self . _check_threshold_structured ( my_task , force )
9992	def _new_dynspace ( self , name = None , bases = None , formula = None , refs = None , arguments = None , source = None , ) : if name is None : name = self . spacenamer . get_next ( self . namespace ) if name in self . namespace : raise ValueError ( "Name '%s' already exists." % name ) if not is_valid_name ( name ) : raise ValueError ( "Invalid name '%s'." % name ) space = RootDynamicSpaceImpl ( parent = self , name = name , formula = formula , refs = refs , source = source , arguments = arguments , ) space . is_derived = False self . _set_space ( space ) if bases : # i.e. not [] dynbase = self . _get_dynamic_base ( bases ) space . _dynbase = dynbase dynbase . _dynamic_subs . append ( space ) return space
6910	def generate_transit_lightcurve ( times , mags = None , errs = None , paramdists = { 'transitperiod' : sps . uniform ( loc = 0.1 , scale = 49.9 ) , 'transitdepth' : sps . uniform ( loc = 1.0e-4 , scale = 2.0e-2 ) , 'transitduration' : sps . uniform ( loc = 0.01 , scale = 0.29 ) } , magsarefluxes = False , ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) # choose the epoch epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) # choose the period, depth, duration period = paramdists [ 'transitperiod' ] . rvs ( size = 1 ) depth = paramdists [ 'transitdepth' ] . rvs ( size = 1 ) duration = paramdists [ 'transitduration' ] . rvs ( size = 1 ) # figure out the ingress duration ingduration = npr . random ( ) * ( 0.5 * duration - 0.05 * duration ) + 0.05 * duration # fix the transit depth if it needs to be flipped if magsarefluxes and depth < 0.0 : depth = - depth elif not magsarefluxes and depth > 0.0 : depth = - depth # generate the model modelmags , phase , ptimes , pmags , perrs = ( transits . trapezoid_transit_func ( [ period , epoch , depth , duration , ingduration ] , times , mags , errs ) ) # resort in original time order timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] # return a dict with everything modeldict = { 'vartype' : 'planet' , 'params' : { x : np . asscalar ( y ) for x , y in zip ( [ 'transitperiod' , 'transitepoch' , 'transitdepth' , 'transitduration' , 'ingressduration' ] , [ period , epoch , depth , duration , ingduration ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , # these are standard keys that help with later characterization of # variability as a function period, variability amplitude, object mag, # ndet, etc. 'varperiod' : period , 'varamplitude' : depth } return modeldict
10092	def _parse_response ( self , response ) : if not self . _raise_errors : return response is_4xx_error = str ( response . status_code ) [ 0 ] == '4' is_5xx_error = str ( response . status_code ) [ 0 ] == '5' content = response . content if response . status_code == 403 : raise AuthenticationError ( content ) elif is_4xx_error : raise APIError ( content ) elif is_5xx_error : raise ServerError ( content ) return response
4362	def _heartbeat ( self ) : interval = self . config [ 'heartbeat_interval' ] while self . connected : gevent . sleep ( interval ) # TODO: this process could use a timeout object like the disconnect # timeout thing, and ONLY send packets when none are sent! # We would do that by calling timeout.set() for a "sending" # timeout. If we're sending 100 messages a second, there is # no need to push some heartbeats in there also. self . put_client_msg ( "2::" )
9225	def permutations_with_replacement ( iterable , r = None ) : pool = tuple ( iterable ) n = len ( pool ) r = n if r is None else r for indices in itertools . product ( range ( n ) , repeat = r ) : yield list ( pool [ i ] for i in indices )
1075	def _days_in_month ( year , month ) : assert 1 <= month <= 12 , month if month == 2 and _is_leap ( year ) : return 29 return _DAYS_IN_MONTH [ month ]
268	def print_table ( table , name = None , float_format = None , formatters = None , header_rows = None ) : if isinstance ( table , pd . Series ) : table = pd . DataFrame ( table ) if name is not None : table . columns . name = name html = table . to_html ( float_format = float_format , formatters = formatters ) if header_rows is not None : # Count the number of columns for the text to span n_cols = html . split ( '<thead>' ) [ 1 ] . split ( '</thead>' ) [ 0 ] . count ( '<th>' ) # Generate the HTML for the extra rows rows = '' for name , value in header_rows . items ( ) : rows += ( '\n <tr style="text-align: right;"><th>%s</th>' + '<td colspan=%d>%s</td></tr>' ) % ( name , n_cols , value ) # Inject the new HTML html = html . replace ( '<thead>' , '<thead>' + rows ) display ( HTML ( html ) )
7739	def check_bidi ( data ) : has_l = False has_ral = False for char in data : if stringprep . in_table_d1 ( char ) : has_ral = True elif stringprep . in_table_d2 ( char ) : has_l = True if has_l and has_ral : raise StringprepError ( "Both RandALCat and LCat characters present" ) if has_ral and ( not stringprep . in_table_d1 ( data [ 0 ] ) or not stringprep . in_table_d1 ( data [ - 1 ] ) ) : raise StringprepError ( "The first and the last character must" " be RandALCat" ) return data
11455	def get_config_item ( cls , key , kb_name , allow_substring = True ) : config_dict = cls . kbs . get ( kb_name , None ) if config_dict : if key in config_dict : return config_dict [ key ] elif allow_substring : res = [ v for k , v in config_dict . items ( ) if key in k ] if res : return res [ 0 ] return key
2884	def connect ( self , callback , * args , * * kwargs ) : if self . is_connected ( callback ) : raise AttributeError ( 'callback is already connected' ) if self . hard_subscribers is None : self . hard_subscribers = [ ] self . hard_subscribers . append ( ( callback , args , kwargs ) )
7838	def set_node ( self , node ) : if node is None : if self . xmlnode . hasProp ( "node" ) : self . xmlnode . unsetProp ( "node" ) return node = unicode ( node ) self . xmlnode . setProp ( "node" , node . encode ( "utf-8" ) )
10365	def has_translocation_increases_activity ( data : Dict ) -> bool : return part_has_modifier ( data , SUBJECT , TRANSLOCATION ) and part_has_modifier ( data , OBJECT , ACTIVITY )
2291	def orient_directed_graph ( self , data , dag , alg = 'HC' ) : alg_dic = { 'HC' : hill_climbing , 'HCr' : hill_climbing_with_removal , 'tabu' : tabu_search , 'EHC' : exploratory_hill_climbing } return alg_dic [ alg ] ( data , dag , nh = self . nh , nb_runs = self . nb_runs , gpu = self . gpu , nb_jobs = self . nb_jobs , lr = self . lr , train_epochs = self . train_epochs , test_epochs = self . test_epochs , verbose = self . verbose )
1973	def sys_openat ( self , dirfd , buf , flags , mode ) : if issymbolic ( dirfd ) : logger . debug ( "Ask to read from a symbolic directory file descriptor!!" ) # Constrain to a valid fd and one past the end of fds self . constraints . add ( dirfd >= 0 ) self . constraints . add ( dirfd <= len ( self . files ) ) raise ConcretizeArgument ( self , 0 ) if issymbolic ( buf ) : logger . debug ( "Ask to read to a symbolic buffer" ) raise ConcretizeArgument ( self , 1 ) return super ( ) . sys_openat ( dirfd , buf , flags , mode )
8412	def numeric_to_timedelta ( self , numerics ) : if self . package == 'pandas' : return [ self . type ( int ( x * self . factor ) , units = 'ns' ) for x in numerics ] else : return [ self . type ( seconds = x * self . factor ) for x in numerics ]
6543	def exec_command ( self , cmdstr ) : if self . is_terminated : raise TerminatedError ( "this TerminalClient instance has been terminated" ) log . debug ( "sending command: %s" , cmdstr ) c = Command ( self . app , cmdstr ) start = time . time ( ) c . execute ( ) elapsed = time . time ( ) - start log . debug ( "elapsed execution: {0}" . format ( elapsed ) ) self . status = Status ( c . status_line ) return c
1872	def RDTSC ( cpu ) : val = cpu . icount cpu . RAX = val & 0xffffffff cpu . RDX = ( val >> 32 ) & 0xffffffff
2367	def _load ( self , path ) : self . tables = [ ] current_table = DefaultTable ( self ) with Utf8Reader ( path ) as f : # N.B. the caller should be catching errors self . raw_text = f . read ( ) f . _file . seek ( 0 ) # bleh; wish this wasn't a private property matcher = Matcher ( re . IGNORECASE ) for linenumber , raw_text in enumerate ( f . readlines ( ) ) : linenumber += 1 # start counting at 1 rather than zero # this mimics what the robot TSV reader does -- # it replaces non-breaking spaces with regular spaces, # and then strips trailing whitespace raw_text = raw_text . replace ( u'\xA0' , ' ' ) raw_text = raw_text . rstrip ( ) # FIXME: I'm keeping line numbers but throwing away # where each cell starts. I should be preserving that # (though to be fair, robot is throwing that away so # I'll have to write my own splitter if I want to save # the character position) cells = TxtReader . split_row ( raw_text ) _heading_regex = r'^\s*\*+\s*(.*?)[ *]*$' if matcher ( _heading_regex , cells [ 0 ] ) : # we've found the start of a new table table_name = matcher . group ( 1 ) current_table = tableFactory ( self , linenumber , table_name , raw_text ) self . tables . append ( current_table ) else : current_table . append ( Row ( linenumber , raw_text , cells ) )
8997	def url ( self , url , encoding = "UTF-8" ) : import urllib . request with urllib . request . urlopen ( url ) as file : webpage_content = file . read ( ) webpage_content = webpage_content . decode ( encoding ) return self . string ( webpage_content )
5699	def write_stats_as_csv ( gtfs , path_to_csv , re_write = False ) : stats_dict = get_stats ( gtfs ) # check if file exist if re_write : os . remove ( path_to_csv ) #if not os.path.isfile(path_to_csv): # is_new = True #else: # is_new = False is_new = True mode = 'r' if os . path . exists ( path_to_csv ) else 'w+' with open ( path_to_csv , mode ) as csvfile : for line in csvfile : if line : is_new = False else : is_new = True with open ( path_to_csv , 'a' ) as csvfile : if ( sys . version_info > ( 3 , 0 ) ) : delimiter = u"," else : delimiter = b"," statswriter = csv . writer ( csvfile , delimiter = delimiter ) # write column names if if is_new : statswriter . writerow ( [ key for key in sorted ( stats_dict . keys ( ) ) ] ) row_to_write = [ ] # write stats row sorted by column name for key in sorted ( stats_dict . keys ( ) ) : row_to_write . append ( stats_dict [ key ] ) statswriter . writerow ( row_to_write )
13420	def validate ( cls , definition ) : schema_path = os . path . join ( os . path . dirname ( __file__ ) , '../../schema/mapper_definition_schema.json' ) with open ( schema_path , 'r' ) as jsonfp : schema = json . load ( jsonfp ) # Validation of JSON schema jsonschema . validate ( definition , schema ) # Validation of JSON properties relations assert definition [ 'main_key' ] in definition [ 'supported_keys' ] , '\'main_key\' must be contained in \'supported_keys\'' assert set ( definition . get ( 'list_valued_keys' , [ ] ) ) <= set ( definition [ 'supported_keys' ] ) , '\'list_valued_keys\' must be a subset of \'supported_keys\'' assert set ( definition . get ( 'disjoint' , [ ] ) ) <= set ( definition . get ( 'list_valued_keys' , [ ] ) ) , '\'disjoint\' must be a subset of \'list_valued_keys\'' assert set ( definition . get ( 'key_synonyms' , { } ) . values ( ) ) <= set ( definition [ 'supported_keys' ] ) , '\'The values of the \'key_synonyms\' mapping must be in \'supported_keys\''
3067	def clean_headers ( headers ) : clean = { } try : for k , v in six . iteritems ( headers ) : if not isinstance ( k , six . binary_type ) : k = str ( k ) if not isinstance ( v , six . binary_type ) : v = str ( v ) clean [ _helpers . _to_bytes ( k ) ] = _helpers . _to_bytes ( v ) except UnicodeEncodeError : from oauth2client . client import NonAsciiHeaderError raise NonAsciiHeaderError ( k , ': ' , v ) return clean
4029	def load ( domain_name = "" ) : cj = http . cookiejar . CookieJar ( ) for cookie_fn in [ chrome , firefox ] : try : for cookie in cookie_fn ( domain_name = domain_name ) : cj . set_cookie ( cookie ) except BrowserCookieError : pass return cj
8701	def close ( self ) : try : if self . baud != self . start_baud : self . __set_baudrate ( self . start_baud ) self . _port . flush ( ) self . __clear_buffers ( ) except serial . serialutil . SerialException : pass log . debug ( 'closing port' ) self . _port . close ( )
10412	def summarize_node_filter ( graph : BELGraph , node_filters : NodePredicates ) -> None : passed = count_passed_node_filter ( graph , node_filters ) print ( '{}/{} nodes passed' . format ( passed , graph . number_of_nodes ( ) ) )
13248	def get_bibliography ( lsst_bib_names = None , bibtex = None ) : bibtex_data = get_lsst_bibtex ( bibtex_filenames = lsst_bib_names ) # Parse with pybtex into BibliographyData instances pybtex_data = [ pybtex . database . parse_string ( _bibtex , 'bibtex' ) for _bibtex in bibtex_data . values ( ) ] # Also parse local bibtex content if bibtex is not None : pybtex_data . append ( pybtex . database . parse_string ( bibtex , 'bibtex' ) ) # Merge BibliographyData bib = pybtex_data [ 0 ] if len ( pybtex_data ) > 1 : for other_bib in pybtex_data [ 1 : ] : for key , entry in other_bib . entries . items ( ) : bib . add_entry ( key , entry ) return bib
1117	def make_table ( self , fromlines , tolines , fromdesc = '' , todesc = '' , context = False , numlines = 5 ) : # make unique anchor prefixes so that multiple tables may exist # on the same page without conflict. self . _make_prefix ( ) # change tabs to spaces before it gets more difficult after we insert # markup fromlines , tolines = self . _tab_newline_replace ( fromlines , tolines ) # create diffs iterator which generates side by side from/to data if context : context_lines = numlines else : context_lines = None diffs = _mdiff ( fromlines , tolines , context_lines , linejunk = self . _linejunk , charjunk = self . _charjunk ) # set up iterator to wrap lines that exceed desired width if self . _wrapcolumn : diffs = self . _line_wrapper ( diffs ) # collect up from/to lines and flags into lists (also format the lines) fromlist , tolist , flaglist = self . _collect_lines ( diffs ) # process change flags, generating middle column of next anchors/links fromlist , tolist , flaglist , next_href , next_id = self . _convert_flags ( fromlist , tolist , flaglist , context , numlines ) s = [ ] fmt = ' <tr><td class="diff_next"%s>%s</td>%s' + '<td class="diff_next">%s</td>%s</tr>\n' for i in range ( len ( flaglist ) ) : if flaglist [ i ] is None : # mdiff yields None on separator lines skip the bogus ones # generated for the first line if i > 0 : s . append ( ' </tbody> \n <tbody>\n' ) else : s . append ( fmt % ( next_id [ i ] , next_href [ i ] , fromlist [ i ] , next_href [ i ] , tolist [ i ] ) ) if fromdesc or todesc : header_row = '<thead><tr>%s%s%s%s</tr></thead>' % ( '<th class="diff_next"><br /></th>' , '<th colspan="2" class="diff_header">%s</th>' % fromdesc , '<th class="diff_next"><br /></th>' , '<th colspan="2" class="diff_header">%s</th>' % todesc ) else : header_row = '' table = self . _table_template % dict ( data_rows = '' . join ( s ) , header_row = header_row , prefix = self . _prefix [ 1 ] ) return table . replace ( '\0+' , '<span class="diff_add">' ) . replace ( '\0-' , '<span class="diff_sub">' ) . replace ( '\0^' , '<span class="diff_chg">' ) . replace ( '\1' , '</span>' ) . replace ( '\t' , '&nbsp;' )
12451	def deref ( self , data ) : # We have to make a deepcopy here to create a proper JSON # compatible object, otherwise `json.dumps` fails when it # hits jsonref.JsonRef objects. deref = copy . deepcopy ( jsonref . JsonRef . replace_refs ( data ) ) # Write out JSON version because we might want this. self . write_template ( deref , filename = 'swagger.json' ) return deref
11499	def get_community_by_id ( self , community_id , token = None ) : parameters = dict ( ) parameters [ 'id' ] = community_id if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.get' , parameters ) return response
5469	def get_last_update ( op ) : last_update = get_end_time ( op ) if not last_update : last_event = get_last_event ( op ) if last_event : last_update = last_event [ 'timestamp' ] if not last_update : last_update = get_create_time ( op ) return last_update
3520	def snapengage ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return SnapEngageNode ( )
4086	def _process_events ( self , events ) : for f , callback , transferred , key , ov in events : try : self . _logger . debug ( 'Invoking event callback {}' . format ( callback ) ) value = callback ( transferred , key , ov ) except OSError : self . _logger . warning ( 'Event callback failed' , exc_info = sys . exc_info ( ) ) else : f . set_result ( value )
13173	def parents ( self , name = None ) : p = self . parent while p is not None : if name is None or p . tagname == name : yield p p = p . parent
4015	def consume ( consumer_id ) : global _consumers consumer = _consumers [ consumer_id ] client = get_docker_client ( ) try : status = client . inspect_container ( consumer . container_id ) [ 'State' ] [ 'Status' ] except Exception as e : status = 'unknown' new_logs = client . logs ( consumer . container_id , stdout = True , stderr = True , stream = False , timestamps = False , since = calendar . timegm ( consumer . offset . timetuple ( ) ) ) updated_consumer = Consumer ( consumer . container_id , datetime . utcnow ( ) ) _consumers [ str ( consumer_id ) ] = updated_consumer response = jsonify ( { 'logs' : new_logs , 'status' : status } ) response . headers [ 'Access-Control-Allow-Origin' ] = '*' response . headers [ 'Access-Control-Allow-Methods' ] = 'GET, POST' return response
1398	def extract_scheduler_location ( self , topology ) : schedulerLocation = { "name" : None , "http_endpoint" : None , "job_page_link" : None , } if topology . scheduler_location : schedulerLocation [ "name" ] = topology . scheduler_location . topology_name schedulerLocation [ "http_endpoint" ] = topology . scheduler_location . http_endpoint schedulerLocation [ "job_page_link" ] = topology . scheduler_location . job_page_link [ 0 ] if len ( topology . scheduler_location . job_page_link ) > 0 else "" return schedulerLocation
7918	def _validate_ip_address ( family , address ) : try : info = socket . getaddrinfo ( address , 0 , family , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) except socket . gaierror , err : logger . debug ( "gaierror: {0} for {1!r}" . format ( err , address ) ) raise ValueError ( "Bad IP address" ) if not info : logger . debug ( "getaddrinfo result empty" ) raise ValueError ( "Bad IP address" ) addr = info [ 0 ] [ 4 ] logger . debug ( " got address: {0!r}" . format ( addr ) ) try : return socket . getnameinfo ( addr , socket . NI_NUMERICHOST ) [ 0 ] except socket . gaierror , err : logger . debug ( "gaierror: {0} for {1!r}" . format ( err , addr ) ) raise ValueError ( "Bad IP address" )
7780	def _process_rfc2425_record ( self , data ) : label , value = data . split ( ":" , 1 ) value = value . replace ( "\\n" , "\n" ) . replace ( "\\N" , "\n" ) psplit = label . lower ( ) . split ( ";" ) name = psplit [ 0 ] params = psplit [ 1 : ] if u"." in name : name = name . split ( "." , 1 ) [ 1 ] name = name . upper ( ) if name in ( u"X-DESC" , u"X-JABBERID" ) : name = name [ 2 : ] if not self . components . has_key ( name ) : return if params : params = dict ( [ p . split ( "=" , 1 ) for p in params ] ) cl , tp = self . components [ name ] if tp in ( "required" , "optional" ) : if self . content . has_key ( name ) : raise ValueError ( "Duplicate %s" % ( name , ) ) try : self . content [ name ] = cl ( name , value , params ) except Empty : pass elif tp == "multi" : if not self . content . has_key ( name ) : self . content [ name ] = [ ] try : self . content [ name ] . append ( cl ( name , value , params ) ) except Empty : pass else : return
3278	def get_resource_inst ( self , path , environ ) : _logger . info ( "get_resource_inst('%s')" % path ) self . _count_get_resource_inst += 1 root = RootCollection ( environ ) return root . resolve ( "" , path )
7125	def get_download_total ( rows ) : headers = rows . pop ( 0 ) index = headers . index ( 'download_count' ) total_downloads = sum ( int ( row [ index ] ) for row in rows ) rows . insert ( 0 , headers ) return total_downloads , index
5380	def build_pipeline_args ( cls , project , script , job_params , task_params , reserved_labels , preemptible , logging_uri , scopes , keep_alive ) : # For the Pipelines API, envs and file inputs are all "inputs". inputs = { } inputs . update ( { SCRIPT_VARNAME : script } ) inputs . update ( { var . name : var . value for var in job_params [ 'envs' ] | task_params [ 'envs' ] if var . value } ) inputs . update ( { var . name : var . uri for var in job_params [ 'inputs' ] | task_params [ 'inputs' ] if not var . recursive and var . value } ) # Remove wildcard references for non-recursive output. When the pipelines # controller generates a delocalize call, it must point to a bare directory # for patterns. The output param OUTFILE=gs://bucket/path/*.bam should # delocalize with a call similar to: # gsutil cp /mnt/data/output/gs/bucket/path/*.bam gs://bucket/path/ outputs = { } for var in job_params [ 'outputs' ] | task_params [ 'outputs' ] : if var . recursive or not var . value : continue if '*' in var . uri . basename : outputs [ var . name ] = var . uri . path else : outputs [ var . name ] = var . uri labels = { } labels . update ( { label . name : label . value if label . value else '' for label in ( reserved_labels | job_params [ 'labels' ] | task_params [ 'labels' ] ) } ) # pyformat: disable args = { 'pipelineArgs' : { 'projectId' : project , 'resources' : { 'preemptible' : preemptible , } , 'inputs' : inputs , 'outputs' : outputs , 'labels' : labels , 'serviceAccount' : { 'email' : 'default' , 'scopes' : scopes , } , # Pass the user-specified GCS destination for pipeline logging. 'logging' : { 'gcsPath' : logging_uri } , } } # pyformat: enable if keep_alive : args [ 'pipelineArgs' ] [ 'keep_vm_alive_on_failure_duration' ] = '%ss' % keep_alive return args
7423	def ref_muscle_chunker ( data , sample ) : LOGGER . info ( 'entering ref_muscle_chunker' ) ## Get regions, which will be a giant list of 5-tuples, of which we're ## only really interested in the first three: (chrom, start, end) position. regions = bedtools_merge ( data , sample ) if len ( regions ) > 0 : ## this calls bam_region_to_fasta a billion times get_overlapping_reads ( data , sample , regions ) else : msg = "No reads mapped to reference sequence - {}" . format ( sample . name ) LOGGER . warn ( msg )
13778	def FindFileContainingSymbol ( self , symbol ) : symbol = _NormalizeFullyQualifiedName ( symbol ) try : return self . _descriptors [ symbol ] . file except KeyError : pass try : return self . _enum_descriptors [ symbol ] . file except KeyError : pass try : file_proto = self . _internal_db . FindFileContainingSymbol ( symbol ) except KeyError as error : if self . _descriptor_db : file_proto = self . _descriptor_db . FindFileContainingSymbol ( symbol ) else : raise error if not file_proto : raise KeyError ( 'Cannot find a file containing %s' % symbol ) return self . _ConvertFileProtoToFileDescriptor ( file_proto )
10234	def _get_catalysts_in_reaction ( reaction : Reaction ) -> Set [ BaseAbundance ] : return { reactant for reactant in reaction . reactants if reactant in reaction . products }
8599	def get_share ( self , group_id , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/shares/%s?depth=%s' % ( group_id , resource_id , str ( depth ) ) ) return response
2674	def init ( src , minimal = False ) : templates_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ ) ) , 'project_templates' , ) for filename in os . listdir ( templates_path ) : if ( minimal and filename == 'event.json' ) or filename . endswith ( '.pyc' ) : continue dest_path = os . path . join ( templates_path , filename ) if not os . path . isdir ( dest_path ) : copy ( dest_path , src )
5452	def ensure_task_params_are_complete ( task_descriptors ) : for task_desc in task_descriptors : for param in [ 'labels' , 'envs' , 'inputs' , 'outputs' , 'input-recursives' , 'output-recursives' ] : if not task_desc . task_params . get ( param ) : task_desc . task_params [ param ] = set ( )
2482	def parse ( self , data ) : try : return self . yacc . parse ( data , lexer = self . lex ) except : return None
5176	def fact ( self , name ) : facts = self . facts ( name = name ) return next ( fact for fact in facts )
10084	def discard ( self , pid = None ) : pid = pid or self . pid with db . session . begin_nested ( ) : before_record_update . send ( current_app . _get_current_object ( ) , record = self ) _ , record = self . fetch_published ( ) self . model . json = deepcopy ( record . model . json ) self . model . json [ '$schema' ] = self . build_deposit_schema ( record ) flag_modified ( self . model , 'json' ) db . session . merge ( self . model ) after_record_update . send ( current_app . _get_current_object ( ) , record = self ) return self . __class__ ( self . model . json , model = self . model )
13386	def upstream_url ( self , uri ) : return self . application . options . upstream + self . request . uri
12691	def write_table_pair_potential ( func , dfunc = None , bounds = ( 1.0 , 10.0 ) , samples = 1000 , tollerance = 1e-6 , keyword = 'PAIR' ) : r_min , r_max = bounds if dfunc is None : dfunc = lambda r : ( func ( r + tollerance ) - func ( r - tollerance ) ) / ( 2 * tollerance ) i = np . arange ( 1 , samples + 1 ) r = np . linspace ( r_min , r_max , samples ) forces = func ( r ) energies = dfunc ( r ) lines = [ '%d %f %f %f\n' % ( index , radius , force , energy ) for index , radius , force , energy in zip ( i , r , forces , energies ) ] return "%s\nN %d\n\n" % ( keyword , samples ) + '' . join ( lines )
7246	def launch ( self , workflow ) : # hit workflow api try : r = self . gbdx_connection . post ( self . workflows_url , json = workflow ) try : r . raise_for_status ( ) except : print ( "GBDX API Status Code: %s" % r . status_code ) print ( "GBDX API Response: %s" % r . text ) r . raise_for_status ( ) workflow_id = r . json ( ) [ 'id' ] return workflow_id except TypeError : self . logger . debug ( 'Workflow not launched!' )
13693	def register ( self , service , name = '' ) : try : is_model = issubclass ( service , orb . Model ) except StandardError : is_model = False # expose an ORB table dynamically as a service if is_model : self . services [ service . schema ( ) . dbname ( ) ] = ( ModelService , service ) else : super ( OrbApiFactory , self ) . register ( service , name = name )
13207	def _parse_author ( self ) : command = LatexCommand ( 'author' , { 'name' : 'authors' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no author' ) self . _authors = [ ] return try : content = parsed [ 'authors' ] except KeyError : self . _logger . warning ( 'lsstdoc has no author' ) self . _authors = [ ] return # Clean content content = content . replace ( '\n' , ' ' ) content = content . replace ( '~' , ' ' ) content = content . strip ( ) # Split content into list of individual authors authors = [ ] for part in content . split ( ',' ) : part = part . strip ( ) for split_part in part . split ( 'and ' ) : split_part = split_part . strip ( ) if len ( split_part ) > 0 : authors . append ( split_part ) self . _authors = authors
2279	def retrieve_adjacency_matrix ( graph , order_nodes = None , weight = False ) : if isinstance ( graph , np . ndarray ) : return graph elif isinstance ( graph , nx . DiGraph ) : if order_nodes is None : order_nodes = graph . nodes ( ) if not weight : return np . array ( nx . adjacency_matrix ( graph , order_nodes , weight = None ) . todense ( ) ) else : return np . array ( nx . adjacency_matrix ( graph , order_nodes ) . todense ( ) ) else : raise TypeError ( "Only networkx.DiGraph and np.ndarray (adjacency matrixes) are supported." )
1125	def Rule ( name , loc = None ) : @ llrule ( loc , lambda parser : getattr ( parser , name ) . expected ( parser ) ) def rule ( parser ) : return getattr ( parser , name ) ( ) return rule
2388	def spell_correct ( string ) : # Create a temp file so that aspell could be used # By default, tempfile will delete this file when the file handle is closed. f = tempfile . NamedTemporaryFile ( mode = 'w' ) f . write ( string ) f . flush ( ) f_path = os . path . abspath ( f . name ) try : p = os . popen ( aspell_path + " -a < " + f_path + " --sug-mode=ultra" ) # Aspell returns a list of incorrect words with the above flags incorrect = p . readlines ( ) p . close ( ) except Exception : log . exception ( "aspell process failed; could not spell check" ) # Return original string if aspell fails return string , 0 , string finally : f . close ( ) incorrect_words = list ( ) correct_spelling = list ( ) for i in range ( 1 , len ( incorrect ) ) : if ( len ( incorrect [ i ] ) > 10 ) : #Reformat aspell output to make sense match = re . search ( ":" , incorrect [ i ] ) if hasattr ( match , "start" ) : begstring = incorrect [ i ] [ 2 : match . start ( ) ] begmatch = re . search ( " " , begstring ) begword = begstring [ 0 : begmatch . start ( ) ] sugstring = incorrect [ i ] [ match . start ( ) + 2 : ] sugmatch = re . search ( "," , sugstring ) if hasattr ( sugmatch , "start" ) : sug = sugstring [ 0 : sugmatch . start ( ) ] incorrect_words . append ( begword ) correct_spelling . append ( sug ) #Create markup based on spelling errors newstring = string markup_string = string already_subbed = [ ] for i in range ( 0 , len ( incorrect_words ) ) : sub_pat = r"\b" + incorrect_words [ i ] + r"\b" sub_comp = re . compile ( sub_pat ) newstring = re . sub ( sub_comp , correct_spelling [ i ] , newstring ) if incorrect_words [ i ] not in already_subbed : markup_string = re . sub ( sub_comp , '<bs>' + incorrect_words [ i ] + "</bs>" , markup_string ) already_subbed . append ( incorrect_words [ i ] ) return newstring , len ( incorrect_words ) , markup_string
8116	def line_line_intersection ( x1 , y1 , x2 , y2 , x3 , y3 , x4 , y4 , infinite = False ) : # Based on: P. Bourke, http://local.wasp.uwa.edu.au/~pbourke/geometry/lineline2d/ ua = ( x4 - x3 ) * ( y1 - y3 ) - ( y4 - y3 ) * ( x1 - x3 ) ub = ( x2 - x1 ) * ( y1 - y3 ) - ( y2 - y1 ) * ( x1 - x3 ) d = ( y4 - y3 ) * ( x2 - x1 ) - ( x4 - x3 ) * ( y2 - y1 ) if d == 0 : if ua == ub == 0 : # The lines are coincident return [ ] else : # The lines are parallel. return [ ] ua /= float ( d ) ub /= float ( d ) if not infinite and not ( 0 <= ua <= 1 and 0 <= ub <= 1 ) : # Intersection point is not within both line segments. return None , None return [ ( x1 + ua * ( x2 - x1 ) , y1 + ua * ( y2 - y1 ) ) ]
8408	def expand_range_distinct ( range , expand = ( 0 , 0 , 0 , 0 ) , zero_width = 1 ) : if len ( expand ) == 2 : expand = tuple ( expand ) * 2 lower = expand_range ( range , expand [ 0 ] , expand [ 1 ] , zero_width ) [ 0 ] upper = expand_range ( range , expand [ 2 ] , expand [ 3 ] , zero_width ) [ 1 ] return ( lower , upper )
9500	def _get_instructions_bytes ( code , varnames = None , names = None , constants = None , cells = None , linestarts = None , line_offset = 0 ) : labels = dis . findlabels ( code ) extended_arg = 0 starts_line = None free = None # enumerate() is not an option, since we sometimes process # multiple elements on a single pass through the loop n = len ( code ) i = 0 while i < n : op = code [ i ] offset = i if linestarts is not None : starts_line = linestarts . get ( i , None ) if starts_line is not None : starts_line += line_offset is_jump_target = i in labels i = i + 1 arg = None argval = None argrepr = '' if op >= dis . HAVE_ARGUMENT : arg = code [ i ] + code [ i + 1 ] * 256 + extended_arg extended_arg = 0 i = i + 2 if op == dis . EXTENDED_ARG : extended_arg = arg * 65536 # Set argval to the dereferenced value of the argument when # availabe, and argrepr to the string representation of argval. # _disassemble_bytes needs the string repr of the # raw name index for LOAD_GLOBAL, LOAD_CONST, etc. argval = arg if op in dis . hasconst : argval , argrepr = dis . _get_const_info ( arg , constants ) elif op in dis . hasname : argval , argrepr = dis . _get_name_info ( arg , names ) elif op in dis . hasjrel : argval = i + arg argrepr = "to " + repr ( argval ) elif op in dis . haslocal : argval , argrepr = dis . _get_name_info ( arg , varnames ) elif op in dis . hascompare : argval = dis . cmp_op [ arg ] argrepr = argval elif op in dis . hasfree : argval , argrepr = dis . _get_name_info ( arg , cells ) elif op in dis . hasnargs : argrepr = "%d positional, %d keyword pair" % ( code [ i - 2 ] , code [ i - 1 ] ) yield dis . Instruction ( dis . opname [ op ] , op , arg , argval , argrepr , offset , starts_line , is_jump_target )
1107	def get_opcodes ( self ) : if self . opcodes is not None : return self . opcodes i = j = 0 self . opcodes = answer = [ ] for ai , bj , size in self . get_matching_blocks ( ) : # invariant: we've pumped out correct diffs to change # a[:i] into b[:j], and the next matching block is # a[ai:ai+size] == b[bj:bj+size]. So we need to pump # out a diff to change a[i:ai] into b[j:bj], pump out # the matching block, and move (i,j) beyond the match tag = '' if i < ai and j < bj : tag = 'replace' elif i < ai : tag = 'delete' elif j < bj : tag = 'insert' if tag : answer . append ( ( tag , i , ai , j , bj ) ) i , j = ai + size , bj + size # the list of matching blocks is terminated by a # sentinel with size 0 if size : answer . append ( ( 'equal' , ai , i , bj , j ) ) return answer
143	def exterior_almost_equals ( self , other , max_distance = 1e-6 , points_per_edge = 8 ) : if isinstance ( other , list ) : other = Polygon ( np . float32 ( other ) ) elif ia . is_np_array ( other ) : other = Polygon ( other ) else : assert isinstance ( other , Polygon ) other = other return self . to_line_string ( closed = True ) . coords_almost_equals ( other . to_line_string ( closed = True ) , max_distance = max_distance , points_per_edge = points_per_edge )
4916	def entitlements ( self , request , pk = None ) : # pylint: disable=invalid-name,unused-argument enterprise_customer_user = self . get_object ( ) instance = { "entitlements" : enterprise_customer_user . entitlements } serializer = serializers . EnterpriseCustomerUserEntitlementSerializer ( instance , context = { 'request' : request } ) return Response ( serializer . data )
13672	def init_build ( self , asset , builder ) : if not self . abs_path : rel_path = utils . prepare_path ( self . rel_bundle_path ) self . abs_bundle_path = utils . prepare_path ( [ builder . config . input_dir , rel_path ] ) self . abs_path = True self . input_dir = builder . config . input_dir
1881	def constrain ( self , constraint ) : constraint = self . migrate_expression ( constraint ) self . _constraints . add ( constraint )
1131	def urldefrag ( url ) : if '#' in url : s , n , p , a , q , frag = urlparse ( url ) defrag = urlunparse ( ( s , n , p , a , q , '' ) ) return defrag , frag else : return url , ''
12140	def load_table ( self , table ) : items , data_keys = [ ] , None for key , filename in table . items ( ) : data_dict = self . filetype . data ( filename [ 0 ] ) current_keys = tuple ( sorted ( data_dict . keys ( ) ) ) values = [ data_dict [ k ] for k in current_keys ] if data_keys is None : data_keys = current_keys elif data_keys != current_keys : raise Exception ( "Data keys are inconsistent" ) items . append ( ( key , values ) ) return Table ( items , kdims = table . kdims , vdims = data_keys )
3643	def quickSell ( self , item_id ) : method = 'DELETE' url = 'item' if not isinstance ( item_id , ( list , tuple ) ) : item_id = ( item_id , ) item_id = ( str ( i ) for i in item_id ) params = { 'itemIds' : ',' . join ( item_id ) } self . __request__ ( method , url , params = params ) # {"items":[{"id":280607437106}],"totalCredits":18136} return True
6615	def receive_all ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . receive ( )
11193	def metadata ( proto_dataset_uri , relpath_in_dataset , key , value ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri , config_path = CONFIG_PATH ) proto_dataset . add_item_metadata ( handle = relpath_in_dataset , key = key , value = value )
6532	def get_local_config ( project_path , use_cache = True ) : pyproject_path = os . path . join ( project_path , 'pyproject.toml' ) if os . path . exists ( pyproject_path ) : with open ( pyproject_path , 'r' ) as config_file : config = pytoml . load ( config_file ) config = config . get ( 'tool' , { } ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
3014	def _to_json ( self , strip , to_serialize = None ) : if to_serialize is None : to_serialize = copy . copy ( self . __dict__ ) pkcs12_val = to_serialize . get ( _PKCS12_KEY ) if pkcs12_val is not None : to_serialize [ _PKCS12_KEY ] = base64 . b64encode ( pkcs12_val ) return super ( ServiceAccountCredentials , self ) . _to_json ( strip , to_serialize = to_serialize )
3932	def _auth_with_code ( session , authorization_code ) : # Make a token request. token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'code' : authorization_code , 'grant_type' : 'authorization_code' , 'redirect_uri' : 'urn:ietf:wg:oauth:2.0:oob' , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ] , res [ 'refresh_token' ]
12613	def is_unique ( self , table_name , sample , unique_fields = None ) : try : eid = find_unique ( self . table ( table_name ) , sample = sample , unique_fields = unique_fields ) except : return False else : return eid is not None
3870	async def get_events ( self , event_id = None , max_events = 50 ) : if event_id is None : # If no event_id is provided, return the newest events in this # conversation. conv_events = self . _events [ - 1 * max_events : ] else : # If event_id is provided, return the events we have that are # older, or request older events if event_id corresponds to the # oldest event we have. conv_event = self . get_event ( event_id ) if self . _events [ 0 ] . id_ != event_id : conv_events = self . _events [ self . _events . index ( conv_event ) + 1 : ] else : logger . info ( 'Loading events for conversation {} before {}' . format ( self . id_ , conv_event . timestamp ) ) res = await self . _client . get_conversation ( hangouts_pb2 . GetConversationRequest ( request_header = self . _client . get_request_header ( ) , conversation_spec = hangouts_pb2 . ConversationSpec ( conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) ) , include_event = True , max_events_per_conversation = max_events , event_continuation_token = self . _event_cont_token ) ) # Certain fields of conversation_state are not populated by # SyncRecentConversations. This is the case with the # user_read_state fields which are all set to 0 but for the # 'self' user. Update here so these fields get populated on the # first call to GetConversation. if res . conversation_state . HasField ( 'conversation' ) : self . update_conversation ( res . conversation_state . conversation ) self . _event_cont_token = ( res . conversation_state . event_continuation_token ) conv_events = [ self . _wrap_event ( event ) for event in res . conversation_state . event ] logger . info ( 'Loaded {} events for conversation {}' . format ( len ( conv_events ) , self . id_ ) ) # Iterate though the events newest to oldest. for conv_event in reversed ( conv_events ) : # Add event as the new oldest event, unless we already have # it. if conv_event . id_ not in self . _events_dict : self . _events . insert ( 0 , conv_event ) self . _events_dict [ conv_event . id_ ] = conv_event else : # If this happens, there's probably a bug. logger . info ( 'Conversation %s ignoring duplicate event %s' , self . id_ , conv_event . id_ ) return conv_events
12576	def set_mask ( self , mask_img ) : mask = load_mask ( mask_img , allow_empty = True ) check_img_compatibility ( self . img , mask , only_check_3d = True ) # this will raise an exception if something is wrong self . mask = mask
11115	def save ( self ) : # open file repoInfoPath = os . path . join ( self . __path , ".pyrepinfo" ) try : fdinfo = open ( repoInfoPath , 'wb' ) except Exception as e : raise Exception ( "unable to open repository info for saving (%s)" % e ) # save repository try : pickle . dump ( self , fdinfo , protocol = 2 ) except Exception as e : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) raise Exception ( "Unable to save repository info (%s)" % e ) finally : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) # save timestamp repoTimePath = os . path . join ( self . __path , ".pyrepstate" ) try : self . __state = ( "%.6f" % time . time ( ) ) . encode ( ) with open ( repoTimePath , 'wb' ) as fdtime : fdtime . write ( self . __state ) fdtime . flush ( ) os . fsync ( fdtime . fileno ( ) ) except Exception as e : raise Exception ( "unable to open repository time stamp for saving (%s)" % e )
7755	def set_response_handlers ( self , stanza , res_handler , err_handler , timeout_handler = None , timeout = None ) : # pylint: disable-msg=R0913 self . lock . acquire ( ) try : self . _set_response_handlers ( stanza , res_handler , err_handler , timeout_handler , timeout ) finally : self . lock . release ( )
9834	def parse ( self , DXfield ) : self . DXfield = DXfield # OpenDX.field (used by comment parser) self . currentobject = None # containers for data self . objects = [ ] # | self . tokens = [ ] # token buffer with open ( self . filename , 'r' ) as self . dxfile : self . use_parser ( 'general' ) # parse the whole file and populate self.objects # assemble field from objects for o in self . objects : if o . type == 'field' : # Almost ignore the field object; VMD, for instance, # does not write components. To make this work # seamlessly I have to think harder how to organize # and use the data, eg preping the field object # properly and the initializing. Probably should also # check uniqueness of ids etc. DXfield . id = o . id continue c = o . initialize ( ) self . DXfield . add ( c . component , c ) # free space del self . currentobject , self . objects
12927	def _parse_allele_data ( self ) : return [ Allele ( sequence = x ) for x in [ self . ref_allele ] + self . alt_alleles ]
13273	def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , np . generic ) : return np . asscalar ( obj ) # Let the base class default method raise the TypeError return json . JSONEncoder ( self , obj )
11958	def is_dec ( ip ) : try : dec = int ( str ( ip ) ) except ValueError : return False if dec > 4294967295 or dec < 0 : return False return True
11338	def schedule_mode ( self , mode ) : modes = [ config . SCHEDULE_RUN , config . SCHEDULE_TEMPORARY_HOLD , config . SCHEDULE_HOLD ] if mode not in modes : raise Exception ( "Invalid mode. Please use one of: {}" . format ( modes ) ) self . set_data ( { "ScheduleMode" : mode } )
13222	def lunch ( self , message = "Time for lunch" , shout : bool = False ) : return self . helper . output ( message , shout )
4112	def rc2ac ( k , R0 ) : [ a , efinal ] = rc2poly ( k , R0 ) R , u , kr , e = rlevinson ( a , efinal ) return R
11443	def _compare_lists ( list1 , list2 , custom_cmp ) : if len ( list1 ) != len ( list2 ) : return False for element1 , element2 in zip ( list1 , list2 ) : if not custom_cmp ( element1 , element2 ) : return False return True
4255	def compress ( self , filename ) : compressed_filename = self . get_compressed_filename ( filename ) if not compressed_filename : return self . do_compress ( filename , compressed_filename )
12824	def _exec ( self , globals_dict = None ) : globals_dict = globals_dict or { } globals_dict . setdefault ( '__builtins__' , { } ) exec ( self . _code , globals_dict ) return globals_dict
8527	def add_child ( self , child ) : if not isinstance ( child , ChildMixin ) : raise TypeError ( 'Requires instance of TreeElement. ' 'Got {}' . format ( type ( child ) ) ) child . parent = self self . _children . append ( child )
1725	def eval ( self , expression , use_compilation_plan = False ) : code = 'PyJsEvalResult = eval(%s)' % json . dumps ( expression ) self . execute ( code , use_compilation_plan = use_compilation_plan ) return self [ 'PyJsEvalResult' ]
7094	def init_options ( self ) : self . options = GoogleMapOptions ( ) d = self . declaration self . set_map_type ( d . map_type ) if d . ambient_mode : self . set_ambient_mode ( d . ambient_mode ) if ( d . camera_position or d . camera_zoom or d . camera_tilt or d . camera_bearing ) : self . update_camera ( ) if d . map_bounds : self . set_map_bounds ( d . map_bounds ) if not d . show_compass : self . set_show_compass ( d . show_compass ) if not d . show_zoom_controls : self . set_show_zoom_controls ( d . show_zoom_controls ) if not d . show_toolbar : self . set_show_toolbar ( d . show_toolbar ) if d . lite_mode : self . set_lite_mode ( d . lite_mode ) if not d . rotate_gestures : self . set_rotate_gestures ( d . rotate_gestures ) if not d . scroll_gestures : self . set_scroll_gestures ( d . scroll_gestures ) if not d . tilt_gestures : self . set_tilt_gestures ( d . tilt_gestures ) if not d . zoom_gestures : self . set_zoom_gestures ( d . zoom_gestures ) if d . min_zoom : self . set_min_zoom ( d . min_zoom ) if d . max_zoom : self . set_max_zoom ( d . max_zoom )
1473	def _get_healthmgr_cmd ( self ) : healthmgr_main_class = 'org.apache.heron.healthmgr.HealthManager' healthmgr_cmd = [ os . path . join ( self . heron_java_home , 'bin/java' ) , # We could not rely on the default -Xmx setting, which could be very big, # for instance, the default -Xmx in Twitter mesos machine is around 18GB '-Xmx1024M' , '-XX:+PrintCommandLineFlags' , '-verbosegc' , '-XX:+PrintGCDetails' , '-XX:+PrintGCTimeStamps' , '-XX:+PrintGCDateStamps' , '-XX:+PrintGCCause' , '-XX:+UseGCLogFileRotation' , '-XX:NumberOfGCLogFiles=5' , '-XX:GCLogFileSize=100M' , '-XX:+PrintPromotionFailure' , '-XX:+PrintTenuringDistribution' , '-XX:+PrintHeapAtGC' , '-XX:+HeapDumpOnOutOfMemoryError' , '-XX:+UseConcMarkSweepGC' , '-XX:+PrintCommandLineFlags' , '-Xloggc:log-files/gc.healthmgr.log' , '-Djava.net.preferIPv4Stack=true' , '-cp' , self . health_manager_classpath , healthmgr_main_class , "--cluster" , self . cluster , "--role" , self . role , "--environment" , self . environment , "--topology_name" , self . topology_name , "--metricsmgr_port" , self . metrics_manager_port ] return Command ( healthmgr_cmd , self . shell_env )
12355	def delete ( self , wait = True ) : resp = self . parent . delete ( self . id ) if wait : self . wait ( ) return resp
11841	def TraceAgent ( agent ) : old_program = agent . program def new_program ( percept ) : action = old_program ( percept ) print '%s perceives %s and does %s' % ( agent , percept , action ) return action agent . program = new_program return agent
10755	def iso_name_increment ( name , is_dir = False , max_length = 8 ) : # Split the extension if needed if not is_dir and '.' in name : name , ext = name . rsplit ( '.' ) ext = '.{}' . format ( ext ) else : ext = '' # Find the position of the last letter for position , char in reversed ( list ( enumerate ( name ) ) ) : if char not in string . digits : break # Extract the numbers and the text from the name base , tag = name [ : position + 1 ] , name [ position + 1 : ] tag = str ( int ( tag or 0 ) + 1 ) # Crop the text if the numbers are too long if len ( tag ) + len ( base ) > max_length : base = base [ : max_length - len ( tag ) ] # Return the name with the extension return '' . join ( [ base , tag , ext ] )
1207	def setup ( app ) : global _is_sphinx _is_sphinx = True app . add_config_value ( 'no_underscore_emphasis' , False , 'env' ) app . add_source_parser ( '.md' , M2RParser ) app . add_directive ( 'mdinclude' , MdInclude )
3331	def release ( self ) : me = currentThread ( ) self . __condition . acquire ( ) try : if self . __writer is me : # We are the writer, take one nesting depth away. self . __writercount -= 1 if not self . __writercount : # No more write locks; take our writer position away and # notify waiters of the new circumstances. self . __writer = None self . __condition . notifyAll ( ) elif me in self . __readers : # We are a reader currently, take one nesting depth away. self . __readers [ me ] -= 1 if not self . __readers [ me ] : # No more read locks, take our reader position away. del self . __readers [ me ] if not self . __readers : # No more readers, notify waiters of the new # circumstances. self . __condition . notifyAll ( ) else : raise ValueError ( "Trying to release unheld lock" ) finally : self . __condition . release ( )
8980	def temporary_path ( self , extension = "" ) : path = NamedTemporaryFile ( delete = False , suffix = extension ) . name self . path ( path ) return path
11150	def get_text_fingerprint ( text , hash_meth , encoding = "utf-8" ) : # pragma: no cover m = hash_meth ( ) m . update ( text . encode ( encoding ) ) return m . hexdigest ( )
8857	def on_run ( self ) : filename = self . tabWidget . current_widget ( ) . file . path wd = os . path . dirname ( filename ) args = Settings ( ) . get_run_config_for_file ( filename ) self . interactiveConsole . start_process ( Settings ( ) . interpreter , args = [ filename ] + args , cwd = wd ) self . dockWidget . show ( ) self . actionRun . setEnabled ( False ) self . actionConfigure_run . setEnabled ( False )
6716	def bootstrap ( self , force = 0 ) : force = int ( force ) if self . has_pip ( ) and not force : return r = self . local_renderer if r . env . bootstrap_method == GET_PIP : r . sudo ( 'curl --silent --show-error --retry 5 https://bootstrap.pypa.io/get-pip.py | python' ) elif r . env . bootstrap_method == EZ_SETUP : r . run ( 'wget http://peak.telecommunity.com/dist/ez_setup.py -O /tmp/ez_setup.py' ) with self . settings ( warn_only = True ) : r . sudo ( 'python /tmp/ez_setup.py -U setuptools' ) r . sudo ( 'easy_install -U pip' ) elif r . env . bootstrap_method == PYTHON_PIP : r . sudo ( 'apt-get install -y python-pip' ) else : raise NotImplementedError ( 'Unknown pip bootstrap method: %s' % r . env . bootstrap_method ) r . sudo ( 'pip {quiet_flag} install --upgrade pip' ) r . sudo ( 'pip {quiet_flag} install --upgrade virtualenv' )
11886	def send_command ( self , command ) : # use lock to make TCP send/receive thread safe with self . _lock : try : self . _socket . send ( command . encode ( "utf8" ) ) result = self . receive ( ) # hub may send "status"/"new" messages that should be ignored while result . startswith ( "S" ) or result . startswith ( "NEW" ) : _LOGGER . debug ( "!Got response: %s" , result ) result = self . receive ( ) _LOGGER . debug ( "Received: %s" , result ) return result except socket . error as error : _LOGGER . error ( "Error sending command: %s" , error ) # try re-connecting socket self . connect ( ) return ""
10116	def append ( self , key , value = MARKER , replace = True ) : return self . add_item ( key , value , replace = replace )
5547	def _get_zoom ( zoom , input_raster , pyramid_type ) : if not zoom : minzoom = 1 maxzoom = get_best_zoom_level ( input_raster , pyramid_type ) elif len ( zoom ) == 1 : minzoom = zoom [ 0 ] maxzoom = zoom [ 0 ] elif len ( zoom ) == 2 : if zoom [ 0 ] < zoom [ 1 ] : minzoom = zoom [ 0 ] maxzoom = zoom [ 1 ] else : minzoom = zoom [ 1 ] maxzoom = zoom [ 0 ] return minzoom , maxzoom
4893	def _collect_certificate_data ( self , enterprise_enrollment ) : if self . certificates_api is None : self . certificates_api = CertificatesApiClient ( self . user ) course_id = enterprise_enrollment . course_id username = enterprise_enrollment . enterprise_customer_user . user . username try : certificate = self . certificates_api . get_course_certificate ( course_id , username ) completed_date = certificate . get ( 'created_date' ) if completed_date : completed_date = parse_datetime ( completed_date ) else : completed_date = timezone . now ( ) # For consistency with _collect_grades_data, we only care about Pass/Fail grades. This could change. is_passing = certificate . get ( 'is_passing' ) grade = self . grade_passing if is_passing else self . grade_failing except HttpNotFoundError : completed_date = None grade = self . grade_incomplete is_passing = False return completed_date , grade , is_passing
11092	def select_dir ( self , filters = all_true , recursive = True ) : for p in self . select ( filters , recursive ) : if p . is_dir ( ) : yield p
3671	def identify_phase ( T , P , Tm = None , Tb = None , Tc = None , Psat = None ) : if Tm and T <= Tm : return 's' elif Tc and T >= Tc : # No special return value for the critical point return 'g' elif Psat : # Do not allow co-existence of phases; transition to 'l' directly under if P <= Psat : return 'g' elif P > Psat : return 'l' elif Tb : # Crude attempt to model phases without Psat # Treat Tb as holding from 90 kPa to 110 kPa if 9E4 < P < 1.1E5 : if T < Tb : return 'l' else : return 'g' elif P > 1.1E5 and T <= Tb : # For the higher-pressure case, it is definitely liquid if under Tb # Above the normal boiling point, impossible to say - return None return 'l' else : return None else : return None
8235	def split_complementary ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) clr = clr . complement colors . append ( clr . rotate_ryb ( - 30 ) . lighten ( 0.1 ) ) colors . append ( clr . rotate_ryb ( 30 ) . lighten ( 0.1 ) ) return colors
8823	def start_rpc_listeners ( self ) : self . _setup_rpc ( ) if not self . endpoints : return [ ] self . conn = n_rpc . create_connection ( ) self . conn . create_consumer ( self . topic , self . endpoints , fanout = False ) return self . conn . consume_in_threads ( )
258	def perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True ) : ( returns , positions , factor_returns , factor_loadings ) = _align_and_warn ( returns , positions , factor_returns , factor_loadings , transactions = transactions , pos_in_dollars = pos_in_dollars ) # Note that we convert positions to percentages *after* the checks # above, since get_turnover() expects positions in dollars. positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . perf_attrib ( returns , positions , factor_returns , factor_loadings )
2333	def predict_dataset ( self , df ) : if len ( list ( df . columns ) ) == 2 : df . columns = [ "A" , "B" ] if self . model is None : raise AssertionError ( "Model has not been trained before predictions" ) df2 = DataFrame ( ) for idx , row in df . iterrows ( ) : df2 = df2 . append ( row , ignore_index = True ) df2 = df2 . append ( { 'A' : row [ "B" ] , 'B' : row [ "A" ] } , ignore_index = True ) return predict . predict ( deepcopy ( df2 ) , deepcopy ( self . model ) ) [ : : 2 ]
1061	def unquote ( s ) : if len ( s ) > 1 : if s . startswith ( '"' ) and s . endswith ( '"' ) : return s [ 1 : - 1 ] . replace ( '\\\\' , '\\' ) . replace ( '\\"' , '"' ) if s . startswith ( '<' ) and s . endswith ( '>' ) : return s [ 1 : - 1 ] return s
9876	def _ordinal_metric ( _v1 , _v2 , i1 , i2 , n_v ) : if i1 > i2 : i1 , i2 = i2 , i1 return ( np . sum ( n_v [ i1 : ( i2 + 1 ) ] ) - ( n_v [ i1 ] + n_v [ i2 ] ) / 2 ) ** 2
4704	def memcopy ( self , stream , offset = 0 , length = float ( "inf" ) ) : data = [ ord ( i ) for i in list ( stream ) ] size = min ( length , len ( data ) , self . m_size ) buff = cast ( self . m_buf , POINTER ( c_uint8 ) ) for i in range ( size ) : buff [ offset + i ] = data [ i ]
5642	def compute_pseudo_connections ( transit_connections , start_time_dep , end_time_dep , transfer_margin , walk_network , walk_speed ) : # A pseudo-connection should be created after (each) arrival to a transit_connection's arrival stop. pseudo_connection_set = set ( ) # use a set to ignore possible duplicates for c in transit_connections : if start_time_dep <= c . departure_time <= end_time_dep : walk_arr_stop = c . departure_stop walk_arr_time = c . departure_time - transfer_margin for _ , walk_dep_stop , data in walk_network . edges ( nbunch = [ walk_arr_stop ] , data = True ) : walk_dep_time = walk_arr_time - data [ 'd_walk' ] / float ( walk_speed ) if walk_dep_time > end_time_dep or walk_dep_time < start_time_dep : continue pseudo_connection = Connection ( walk_dep_stop , walk_arr_stop , walk_dep_time , walk_arr_time , Connection . WALK_TRIP_ID , Connection . WALK_SEQ , is_walk = True ) pseudo_connection_set . add ( pseudo_connection ) return pseudo_connection_set
12698	def _parse_control_fields ( self , fields , tag_id = "tag" ) : for field in fields : params = field . params # skip tags without parameters if tag_id not in params : continue self . controlfields [ params [ tag_id ] ] = field . getContent ( ) . strip ( )
6307	def load_effects_classes ( self ) : self . effect_classes = [ ] for _ , cls in inspect . getmembers ( self . effect_module ) : if inspect . isclass ( cls ) : if cls == Effect : continue if issubclass ( cls , Effect ) : self . effect_classes . append ( cls ) self . effect_class_map [ cls . __name__ ] = cls cls . _name = "{}.{}" . format ( self . effect_module_name , cls . __name__ )
11062	def send_im ( self , user , text ) : if isinstance ( user , SlackUser ) : user = user . id channelid = self . _find_im_channel ( user ) else : channelid = user . id self . send_message ( channelid , text )
9140	def find_best_label_for_type ( labels , language , labeltype ) : typelabels = [ l for l in labels if l . type == labeltype ] if not typelabels : return False if language == 'any' : return typelabels [ 0 ] exact = filter_labels_by_language ( typelabels , language ) if exact : return exact [ 0 ] inexact = filter_labels_by_language ( typelabels , language , True ) if inexact : return inexact [ 0 ] return False
10119	def circle ( cls , center , radius , n_vertices = 50 , * * kwargs ) : return cls . regular_polygon ( center , radius , n_vertices , * * kwargs )
12750	def set_pid_params ( self , * args , * * kwargs ) : for joint in self . joints : joint . target_angles = [ None ] * joint . ADOF joint . controllers = [ pid ( * args , * * kwargs ) for i in range ( joint . ADOF ) ]
8836	def minus ( * args ) : if len ( args ) == 1 : return - to_numeric ( args [ 0 ] ) return to_numeric ( args [ 0 ] ) - to_numeric ( args [ 1 ] )
7187	def remove_function_signature_type_comment ( body ) : for node in body . children : if node . type == token . INDENT : prefix = node . prefix . lstrip ( ) if prefix . startswith ( '# type: ' ) : node . prefix = '\n' . join ( prefix . split ( '\n' ) [ 1 : ] ) break
2643	def filepath ( self ) : if hasattr ( self , 'local_path' ) : return self . local_path if self . scheme in [ 'ftp' , 'http' , 'https' , 'globus' ] : return self . filename elif self . scheme in [ 'file' ] : return self . path else : raise Exception ( 'Cannot return filepath for unknown scheme {}' . format ( self . scheme ) )
11765	def k_in_row ( self , board , move , player , ( delta_x , delta_y ) ) : x , y = move n = 0 # n is number of moves in row while board . get ( ( x , y ) ) == player : n += 1 x , y = x + delta_x , y + delta_y x , y = move while board . get ( ( x , y ) ) == player : n += 1 x , y = x - delta_x , y - delta_y n -= 1 # Because we counted move itself twice return n >= self . k
874	def initStateFrom ( self , particleId , particleState , newBest ) : # Get the update best position and result? if newBest : ( bestResult , bestPosition ) = self . _resultsDB . getParticleBest ( particleId ) else : bestResult = bestPosition = None # Replace with the position and velocity of each variable from # saved state varStates = particleState [ 'varStates' ] for varName in varStates . keys ( ) : varState = copy . deepcopy ( varStates [ varName ] ) if newBest : varState [ 'bestResult' ] = bestResult if bestPosition is not None : varState [ 'bestPosition' ] = bestPosition [ varName ] self . permuteVars [ varName ] . setState ( varState )
793	def jobGetModelIDs ( self , jobID ) : rows = self . _getMatchingRowsWithRetries ( self . _models , dict ( job_id = jobID ) , [ 'model_id' ] ) return [ r [ 0 ] for r in rows ]
880	def create ( modelConfig , logLevel = logging . ERROR ) : logger = ModelFactory . __getLogger ( ) logger . setLevel ( logLevel ) logger . debug ( "ModelFactory returning Model from dict: %s" , modelConfig ) modelClass = None if modelConfig [ 'model' ] == "HTMPrediction" : modelClass = HTMPredictionModel elif modelConfig [ 'model' ] == "TwoGram" : modelClass = TwoGramModel elif modelConfig [ 'model' ] == "PreviousValue" : modelClass = PreviousValueModel else : raise Exception ( "ModelFactory received unsupported Model type: %s" % modelConfig [ 'model' ] ) return modelClass ( * * modelConfig [ 'modelParams' ] )
7856	def __response ( self , stanza ) : try : d = self . disco_class ( stanza . get_query ( ) ) self . got_it ( d ) except ValueError , e : self . error ( e )
256	def gen_round_trip_stats ( round_trips ) : stats = { } stats [ 'pnl' ] = agg_all_long_short ( round_trips , 'pnl' , PNL_STATS ) stats [ 'summary' ] = agg_all_long_short ( round_trips , 'pnl' , SUMMARY_STATS ) stats [ 'duration' ] = agg_all_long_short ( round_trips , 'duration' , DURATION_STATS ) stats [ 'returns' ] = agg_all_long_short ( round_trips , 'returns' , RETURN_STATS ) stats [ 'symbols' ] = round_trips . groupby ( 'symbol' ) [ 'returns' ] . agg ( RETURN_STATS ) . T return stats
4946	def send_course_enrollment_statement ( lrs_configuration , course_enrollment ) : user_details = LearnerInfoSerializer ( course_enrollment . user ) course_details = CourseInfoSerializer ( course_enrollment . course ) statement = LearnerCourseEnrollmentStatement ( course_enrollment . user , course_enrollment . course , user_details . data , course_details . data , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
10793	def separate_particles_into_groups ( s , region_size = 40 , bounds = None ) : imtile = ( s . oshape . translate ( - s . pad ) if bounds is None else util . Tile ( bounds [ 0 ] , bounds [ 1 ] ) ) # does all particle including out of image, is that correct? region = util . Tile ( region_size , dim = s . dim ) trange = np . ceil ( imtile . shape . astype ( 'float' ) / region . shape ) translations = util . Tile ( trange ) . coords ( form = 'vector' ) translations = translations . reshape ( - 1 , translations . shape [ - 1 ] ) groups = [ ] positions = s . obj_get_positions ( ) for v in translations : tmptile = region . copy ( ) . translate ( region . shape * v - s . pad ) groups . append ( find_particles_in_tile ( positions , tmptile ) ) return [ g for g in groups if len ( g ) > 0 ]
11054	def rm_fwd_refs ( obj ) : for stack , key in obj . _backrefs_flat : # Unpack stack backref_key , parent_schema_name , parent_field_name = stack # Get parent info parent_schema = obj . _collections [ parent_schema_name ] parent_key_store = parent_schema . _pk_to_storage ( key ) parent_object = parent_schema . load ( parent_key_store ) if parent_object is None : continue # Remove forward references if parent_object . _fields [ parent_field_name ] . _list : getattr ( parent_object , parent_field_name ) . remove ( obj ) else : parent_field_object = parent_object . _fields [ parent_field_name ] setattr ( parent_object , parent_field_name , parent_field_object . _gen_default ( ) ) # Save parent_object . save ( )
827	def _getInputValue ( self , obj , fieldName ) : if isinstance ( obj , dict ) : if not fieldName in obj : knownFields = ", " . join ( key for key in obj . keys ( ) if not key . startswith ( "_" ) ) raise ValueError ( "Unknown field name '%s' in input record. Known fields are '%s'.\n" "This could be because input headers are mislabeled, or because " "input data rows do not contain a value for '%s'." % ( fieldName , knownFields , fieldName ) ) return obj [ fieldName ] else : return getattr ( obj , fieldName )
13300	def upgrade ( self , package ) : logger . debug ( 'Upgrading ' + package ) shell . run ( self . pip_path , 'install' , '--upgrade' , '--no-deps' , package ) shell . run ( self . pip_path , 'install' , package )
13474	def _loop ( self ) : while True : try : with uncaught_greenlet_exception_context ( ) : self . _loop_callback ( ) except gevent . GreenletExit : break if self . _stop_event . wait ( self . _interval ) : break self . _clear ( )
5536	def write ( self , process_tile , data ) : if isinstance ( process_tile , tuple ) : process_tile = self . config . process_pyramid . tile ( * process_tile ) elif not isinstance ( process_tile , BufferedTile ) : raise ValueError ( "invalid process_tile type: %s" % type ( process_tile ) ) if self . config . mode not in [ "continue" , "overwrite" ] : raise ValueError ( "cannot write output in current process mode" ) if self . config . mode == "continue" and ( self . config . output . tiles_exist ( process_tile ) ) : message = "output exists, not overwritten" logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = False , write_msg = message ) elif data is None : message = "output empty, nothing written" logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = False , write_msg = message ) else : with Timer ( ) as t : self . config . output . write ( process_tile = process_tile , data = data ) message = "output written in %s" % t logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = True , write_msg = message )
10677	def Cp ( self , T ) : result = 0.0 for c , e in zip ( self . _coefficients , self . _exponents ) : result += c * T ** e return result
9935	def find ( self , path , all = False ) : matches = [ ] for prefix , root in self . locations : if root not in searched_locations : searched_locations . append ( root ) matched_path = self . find_location ( root , path , prefix ) if matched_path : if not all : return matched_path matches . append ( matched_path ) return matches
720	def getOptimizationMetricInfo ( cls , searchJobParams ) : if searchJobParams [ "hsVersion" ] == "v2" : search = HypersearchV2 ( searchParams = searchJobParams ) else : raise RuntimeError ( "Unsupported hypersearch version \"%s\"" % ( searchJobParams [ "hsVersion" ] ) ) info = search . getOptimizationMetricInfo ( ) return info
10685	def G_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : g = 1 - ( self . _A_mag / tau + self . _B_mag * ( tau ** 3 / 6 + tau ** 9 / 135 + tau ** 15 / 600 ) ) / self . _D_mag else : g = - ( tau ** - 5 / 10 + tau ** - 15 / 315 + tau ** - 25 / 1500 ) / self . _D_mag return R * T * math . log ( self . beta0_mag + 1 ) * g
4948	def export ( self ) : content_metadata_export = { } content_metadata_items = self . enterprise_api . get_content_metadata ( self . enterprise_customer ) LOGGER . info ( 'Retrieved content metadata for enterprise [%s]' , self . enterprise_customer . name ) for item in content_metadata_items : transformed = self . _transform_item ( item ) LOGGER . info ( 'Exporting content metadata item with plugin configuration [%s]: [%s]' , self . enterprise_configuration , json . dumps ( transformed , indent = 4 ) , ) content_metadata_item_export = ContentMetadataItemExport ( item , transformed ) content_metadata_export [ content_metadata_item_export . content_id ] = content_metadata_item_export return OrderedDict ( sorted ( content_metadata_export . items ( ) ) )
3834	async def send_offnetwork_invitation ( self , send_offnetwork_invitation_request ) : response = hangouts_pb2 . SendOffnetworkInvitationResponse ( ) await self . _pb_request ( 'devices/sendoffnetworkinvitation' , send_offnetwork_invitation_request , response ) return response
8071	def not_found ( url , wait = 10 ) : try : connection = open ( url , wait ) except HTTP404NotFound : return True except : return False return False
12198	def to_cldf ( self , dest , mdname = 'cldf-metadata.json' ) : dest = Path ( dest ) if not dest . exists ( ) : dest . mkdir ( ) data = self . read ( ) if data [ self . source_table_name ] : sources = Sources ( ) for src in data [ self . source_table_name ] : sources . add ( Source ( src [ 'genre' ] , src [ 'id' ] , * * { k : v for k , v in src . items ( ) if k not in [ 'id' , 'genre' ] } ) ) sources . write ( dest / self . dataset . properties . get ( 'dc:source' , 'sources.bib' ) ) for table_type , items in data . items ( ) : try : table = self . dataset [ table_type ] table . common_props [ 'dc:extent' ] = table . write ( [ self . retranslate ( table , item ) for item in items ] , base = dest ) except KeyError : assert table_type == self . source_table_name , table_type return self . dataset . write_metadata ( dest / mdname )
5308	def hex_to_rgb ( value ) : value = value . lstrip ( '#' ) check_hex ( value ) length = len ( value ) step = int ( length / 3 ) return tuple ( int ( value [ i : i + step ] , 16 ) for i in range ( 0 , length , step ) )
2568	def construct_start_message ( self ) : uname = getpass . getuser ( ) . encode ( 'latin1' ) hashed_username = hashlib . sha256 ( uname ) . hexdigest ( ) [ 0 : 10 ] hname = socket . gethostname ( ) . encode ( 'latin1' ) hashed_hostname = hashlib . sha256 ( hname ) . hexdigest ( ) [ 0 : 10 ] message = { 'uuid' : self . uuid , 'uname' : hashed_username , 'hname' : hashed_hostname , 'test' : self . test_mode , 'parsl_v' : self . parsl_version , 'python_v' : self . python_version , 'os' : platform . system ( ) , 'os_v' : platform . release ( ) , 'start' : time . time ( ) } return json . dumps ( message )
149	def clip_out_of_image ( self ) : polys_cut = [ poly . clip_out_of_image ( self . shape ) for poly in self . polygons if poly . is_partly_within_image ( self . shape ) ] polys_cut_flat = [ poly for poly_lst in polys_cut for poly in poly_lst ] # TODO use deepcopy() here return PolygonsOnImage ( polys_cut_flat , shape = self . shape )
11232	def run_excel_to_html ( ) : # Capture commandline arguments. prog='' argument must # match the command name in setup.py entry_points parser = argparse . ArgumentParser ( prog = 'excel_to_html' ) parser . add_argument ( '-p' , nargs = '?' , help = 'Path to an excel file for conversion.' ) parser . add_argument ( '-s' , nargs = '?' , help = 'The name of a sheet in our excel file. Defaults to "Sheet1".' , ) parser . add_argument ( '-css' , nargs = '?' , help = 'Space separated css classes to append to the table.' ) parser . add_argument ( '-m' , action = 'store_true' , help = 'Merge, attempt to combine merged cells.' ) parser . add_argument ( '-c' , nargs = '?' , help = 'Caption for creating an accessible table.' ) parser . add_argument ( '-d' , nargs = '?' , help = 'Two strings separated by a | character. The first string \ is for the html "summary" attribute and the second string is for the html "details" attribute. \ both values must be provided and nothing more.' , ) parser . add_argument ( '-r' , action = 'store_true' , help = 'Row headers. Does the table have row headers?' ) args = parser . parse_args ( ) inputs = { 'p' : args . p , 's' : args . s , 'css' : args . css , 'm' : args . m , 'c' : args . c , 'd' : args . d , 'r' : args . r , } p = inputs [ 'p' ] s = inputs [ 's' ] if inputs [ 's' ] else 'Sheet1' css = inputs [ 'css' ] if inputs [ 'css' ] else '' m = inputs [ 'm' ] if inputs [ 'm' ] else False c = inputs [ 'c' ] if inputs [ 'c' ] else '' d = inputs [ 'd' ] . split ( '|' ) if inputs [ 'd' ] else [ ] r = inputs [ 'r' ] if inputs [ 'r' ] else False html = fp . excel_to_html ( p , sheetname = s , css_classes = css , caption = c , details = d , row_headers = r , merge = m ) print ( html )
7351	def parse_netchop ( netchop_output ) : line_iterator = iter ( netchop_output . decode ( ) . split ( "\n" ) ) scores = [ ] for line in line_iterator : if "pos" in line and 'AA' in line and 'score' in line : scores . append ( [ ] ) if "----" not in next ( line_iterator ) : raise ValueError ( "Dashes expected" ) line = next ( line_iterator ) while '-------' not in line : score = float ( line . split ( ) [ 3 ] ) scores [ - 1 ] . append ( score ) line = next ( line_iterator ) return scores
5960	def _tcorrel ( self , nstep = 100 , * * kwargs ) : t = self . array [ 0 , : : nstep ] r = gromacs . collections . Collection ( [ numkit . timeseries . tcorrel ( t , Y , nstep = 1 , * * kwargs ) for Y in self . array [ 1 : , : : nstep ] ] ) return r
2451	def set_pkg_down_location ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_down_location_set : self . package_down_location_set = True doc . package . download_location = location return True else : raise CardinalityError ( 'Package::DownloadLocation' )
1312	def HardwareInput ( uMsg : int , param : int = 0 ) -> INPUT : return _CreateInput ( HARDWAREINPUT ( uMsg , param & 0xFFFF , param >> 16 & 0xFFFF ) )
1570	def submit_fatjar ( cl_args , unknown_args , tmp_dir ) : # execute main of the topology to create the topology definition topology_file = cl_args [ 'topology-file-name' ] main_class = cl_args [ 'topology-class-name' ] res = execute . heron_class ( class_name = main_class , lib_jars = config . get_heron_libs ( jars . topology_jars ( ) ) , extra_jars = [ topology_file ] , args = tuple ( unknown_args ) , java_defines = cl_args [ 'topology_main_jvm_property' ] ) result . render ( res ) if not result . is_successful ( res ) : err_context = ( "Failed to create topology definition " "file when executing class '%s' of file '%s'" ) % ( main_class , topology_file ) res . add_context ( err_context ) return res results = launch_topologies ( cl_args , topology_file , tmp_dir ) return results
12383	def create_project ( self ) : if os . path . exists ( self . _py ) : prj_dir = os . path . join ( self . _app_dir , self . _project_name ) if os . path . exists ( prj_dir ) : if self . _force : logging . warn ( 'Removing existing project' ) shutil . rmtree ( prj_dir ) else : logging . warn ( 'Found existing project; not creating (use --force to overwrite)' ) return logging . info ( 'Creating project' ) p = subprocess . Popen ( 'cd {0} ; {1} startproject {2} > /dev/null' . format ( self . _app_dir , self . _ve_dir + os . sep + self . _project_name + os . sep + 'bin' + os . sep + 'django-admin.py' , self . _project_name ) , shell = True ) os . waitpid ( p . pid , 0 ) else : logging . error ( 'Unable to find Python interpreter in virtualenv' ) return
3654	def start_image_acquisition ( self ) : if not self . _create_ds_at_connection : self . _setup_data_streams ( ) # num_required_buffers = self . _num_buffers for data_stream in self . _data_streams : try : num_buffers = data_stream . buffer_announce_min if num_buffers < num_required_buffers : num_buffers = num_required_buffers except InvalidParameterException as e : num_buffers = num_required_buffers self . _logger . debug ( e , exc_info = True ) if data_stream . defines_payload_size ( ) : buffer_size = data_stream . payload_size else : buffer_size = self . device . node_map . PayloadSize . value raw_buffers = self . _create_raw_buffers ( num_buffers , buffer_size ) buffer_tokens = self . _create_buffer_tokens ( raw_buffers ) self . _announced_buffers = self . _announce_buffers ( data_stream = data_stream , _buffer_tokens = buffer_tokens ) self . _queue_announced_buffers ( data_stream = data_stream , buffers = self . _announced_buffers ) # Reset the number of images to acquire. try : acq_mode = self . device . node_map . AcquisitionMode . value if acq_mode == 'Continuous' : num_images_to_acquire = - 1 elif acq_mode == 'SingleFrame' : num_images_to_acquire = 1 elif acq_mode == 'MultiFrame' : num_images_to_acquire = self . device . node_map . AcquisitionFrameCount . value else : num_images_to_acquire = - 1 except LogicalErrorException as e : # The node doesn't exist. num_images_to_acquire = - 1 self . _logger . debug ( e , exc_info = True ) self . _num_images_to_acquire = num_images_to_acquire try : # We're ready to start image acquisition. Lock the device's # transport layer related features: self . device . node_map . TLParamsLocked . value = 1 except LogicalErrorException : # SFNC < 2.0 pass # Start image acquisition. self . _is_acquiring_images = True for data_stream in self . _data_streams : data_stream . start_acquisition ( ACQ_START_FLAGS_LIST . ACQ_START_FLAGS_DEFAULT , self . _num_images_to_acquire ) # if self . thread_image_acquisition : self . thread_image_acquisition . start ( ) # self . device . node_map . AcquisitionStart . execute ( ) self . _logger . info ( '{0} started image acquisition.' . format ( self . _device . id_ ) ) if self . _profiler : self . _profiler . print_diff ( )
10258	def count_top_centrality ( graph : BELGraph , number : Optional [ int ] = 30 ) -> Mapping [ BaseEntity , int ] : dd = nx . betweenness_centrality ( graph ) dc = Counter ( dd ) return dict ( dc . most_common ( number ) )
843	def _addPartitionId ( self , index , partitionId = None ) : if partitionId is None : self . _partitionIdList . append ( numpy . inf ) else : self . _partitionIdList . append ( partitionId ) indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( index ) self . _partitionIdMap [ partitionId ] = indices
9255	def generate_log_for_tag ( self , pull_requests , issues , newer_tag , older_tag_name ) : newer_tag_link , newer_tag_name , newer_tag_time = self . detect_link_tag_time ( newer_tag ) github_site = "https://github.com" or self . options . github_endpoint project_url = "{0}/{1}/{2}" . format ( github_site , self . options . user , self . options . project ) log = self . generate_header ( newer_tag_name , newer_tag_link , newer_tag_time , older_tag_name , project_url ) if self . options . issues : # Generate issues: log += self . issues_to_log ( issues , pull_requests ) if self . options . include_pull_request : # Generate pull requests: log += self . generate_sub_section ( pull_requests , self . options . merge_prefix ) return log
8898	def _deserialize_from_store ( profile ) : # we first serialize to avoid deserialization merge conflicts _serialize_into_store ( profile ) fk_cache = { } with transaction . atomic ( ) : syncable_dict = _profile_models [ profile ] excluded_list = [ ] # iterate through classes which are in foreign key dependency order for model_name , klass_model in six . iteritems ( syncable_dict ) : # handle cases where a class has a single FK reference to itself self_ref_fk = _self_referential_fk ( klass_model ) query = Q ( model_name = klass_model . morango_model_name ) for klass in klass_model . morango_model_dependencies : query |= Q ( model_name = klass . morango_model_name ) if self_ref_fk : clean_parents = Store . objects . filter ( dirty_bit = False , profile = profile ) . filter ( query ) . char_ids_list ( ) dirty_children = Store . objects . filter ( dirty_bit = True , profile = profile ) . filter ( Q ( _self_ref_fk__in = clean_parents ) | Q ( _self_ref_fk = '' ) ) . filter ( query ) # keep iterating until size of dirty_children is 0 while len ( dirty_children ) > 0 : for store_model in dirty_children : try : app_model = store_model . _deserialize_store_model ( fk_cache ) if app_model : with mute_signals ( signals . pre_save , signals . post_save ) : app_model . save ( update_dirty_bit_to = False ) # we update a store model after we have deserialized it to be able to mark it as a clean parent store_model . dirty_bit = False store_model . save ( update_fields = [ 'dirty_bit' ] ) except exceptions . ValidationError : # if the app model did not validate, we leave the store dirty bit set excluded_list . append ( store_model . id ) # update lists with new clean parents and dirty children clean_parents = Store . objects . filter ( dirty_bit = False , profile = profile ) . filter ( query ) . char_ids_list ( ) dirty_children = Store . objects . filter ( dirty_bit = True , profile = profile , _self_ref_fk__in = clean_parents ) . filter ( query ) else : # array for holding db values from the fields of each model for this class db_values = [ ] fields = klass_model . _meta . fields for store_model in Store . objects . filter ( model_name = model_name , profile = profile , dirty_bit = True ) : try : app_model = store_model . _deserialize_store_model ( fk_cache ) # if the model was not deleted add its field values to the list if app_model : for f in fields : value = getattr ( app_model , f . attname ) db_value = f . get_db_prep_value ( value , connection ) db_values . append ( db_value ) except exceptions . ValidationError : # if the app model did not validate, we leave the store dirty bit set excluded_list . append ( store_model . id ) if db_values : # number of rows to update num_of_rows = len ( db_values ) // len ( fields ) # create '%s' placeholders for a single row placeholder_tuple = tuple ( [ '%s' for _ in range ( len ( fields ) ) ] ) # create list of the '%s' tuple placeholders based on number of rows to update placeholder_list = [ str ( placeholder_tuple ) for _ in range ( num_of_rows ) ] with connection . cursor ( ) as cursor : DBBackend . _bulk_insert_into_app_models ( cursor , klass_model . _meta . db_table , fields , db_values , placeholder_list ) # clear dirty bit for all store models for this profile except for models that did not validate Store . objects . exclude ( id__in = excluded_list ) . filter ( profile = profile , dirty_bit = True ) . update ( dirty_bit = False )
5613	def reproject_geometry ( geometry , src_crs = None , dst_crs = None , error_on_clip = False , validity_check = True , antimeridian_cutting = False ) : src_crs = _validated_crs ( src_crs ) dst_crs = _validated_crs ( dst_crs ) def _reproject_geom ( geometry , src_crs , dst_crs ) : if geometry . is_empty : return geometry else : out_geom = to_shape ( transform_geom ( src_crs . to_dict ( ) , dst_crs . to_dict ( ) , mapping ( geometry ) , antimeridian_cutting = antimeridian_cutting ) ) return _repair ( out_geom ) if validity_check else out_geom # return repaired geometry if no reprojection needed if src_crs == dst_crs or geometry . is_empty : return _repair ( geometry ) # geometry needs to be clipped to its CRS bounds elif ( dst_crs . is_epsg_code and # just in case for an CRS with EPSG code dst_crs . get ( "init" ) in CRS_BOUNDS and # if CRS has defined bounds dst_crs . get ( "init" ) != "epsg:4326" # and is not WGS84 (does not need clipping) ) : wgs84_crs = CRS ( ) . from_epsg ( 4326 ) # get dst_crs boundaries crs_bbox = box ( * CRS_BOUNDS [ dst_crs . get ( "init" ) ] ) # reproject geometry to WGS84 geometry_4326 = _reproject_geom ( geometry , src_crs , wgs84_crs ) # raise error if geometry has to be clipped if error_on_clip and not geometry_4326 . within ( crs_bbox ) : raise RuntimeError ( "geometry outside target CRS bounds" ) # clip geometry dst_crs boundaries and return return _reproject_geom ( crs_bbox . intersection ( geometry_4326 ) , wgs84_crs , dst_crs ) # return without clipping if destination CRS does not have defined bounds else : return _reproject_geom ( geometry , src_crs , dst_crs )
6725	def get_or_create ( name = None , group = None , config = None , extra = 0 , verbose = 0 , backend_opts = None ) : require ( 'vm_type' , 'vm_group' ) backend_opts = backend_opts or { } verbose = int ( verbose ) extra = int ( extra ) if config : config_fn = common . find_template ( config ) config = yaml . load ( open ( config_fn ) ) env . update ( config ) env . vm_type = ( env . vm_type or '' ) . lower ( ) assert env . vm_type , 'No VM type specified.' group = group or env . vm_group assert group , 'No VM group specified.' ret = exists ( name = name , group = group ) if not extra and ret : if verbose : print ( 'VM %s:%s exists.' % ( name , group ) ) return ret today = datetime . date . today ( ) release = int ( '%i%02i%02i' % ( today . year , today . month , today . day ) ) if not name : existing_instances = list_instances ( group = group , release = release , verbose = verbose ) name = env . vm_name_template . format ( index = len ( existing_instances ) + 1 ) if env . vm_type == EC2 : return get_or_create_ec2_instance ( name = name , group = group , release = release , verbose = verbose , backend_opts = backend_opts ) else : raise NotImplementedError
316	def perf_stats ( returns , factor_returns = None , positions = None , transactions = None , turnover_denom = 'AGB' ) : stats = pd . Series ( ) for stat_func in SIMPLE_STAT_FUNCS : stats [ STAT_FUNC_NAMES [ stat_func . __name__ ] ] = stat_func ( returns ) if positions is not None : stats [ 'Gross leverage' ] = gross_lev ( positions ) . mean ( ) if transactions is not None : stats [ 'Daily turnover' ] = get_turnover ( positions , transactions , turnover_denom ) . mean ( ) if factor_returns is not None : for stat_func in FACTOR_STAT_FUNCS : res = stat_func ( returns , factor_returns ) stats [ STAT_FUNC_NAMES [ stat_func . __name__ ] ] = res return stats
7868	def _expire_item ( self , key ) : ( timeout , callback ) = self . _timeouts [ key ] now = time . time ( ) if timeout <= now : item = dict . pop ( self , key ) del self . _timeouts [ key ] if callback : try : callback ( key , item ) except TypeError : try : callback ( key ) except TypeError : callback ( ) return None else : return timeout - now
3379	def add_lp_feasibility ( model ) : obj_vars = [ ] prob = model . problem for met in model . metabolites : s_plus = prob . Variable ( "s_plus_" + met . id , lb = 0 ) s_minus = prob . Variable ( "s_minus_" + met . id , lb = 0 ) model . add_cons_vars ( [ s_plus , s_minus ] ) model . constraints [ met . id ] . set_linear_coefficients ( { s_plus : 1.0 , s_minus : - 1.0 } ) obj_vars . append ( s_plus ) obj_vars . append ( s_minus ) model . objective = prob . Objective ( Zero , sloppy = True , direction = "min" ) model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } )
2576	def _add_input_deps ( self , executor , args , kwargs ) : # Return if the task is _*_stage_in if executor == 'data_manager' : return args , kwargs inputs = kwargs . get ( 'inputs' , [ ] ) for idx , f in enumerate ( inputs ) : if isinstance ( f , File ) and f . is_remote ( ) : inputs [ idx ] = self . data_manager . stage_in ( f , executor ) for kwarg , f in kwargs . items ( ) : if isinstance ( f , File ) and f . is_remote ( ) : kwargs [ kwarg ] = self . data_manager . stage_in ( f , executor ) newargs = list ( args ) for idx , f in enumerate ( newargs ) : if isinstance ( f , File ) and f . is_remote ( ) : newargs [ idx ] = self . data_manager . stage_in ( f , executor ) return tuple ( newargs ) , kwargs
10953	def set_model ( self , mdl ) : self . mdl = mdl self . mdl . check_inputs ( self . comps ) for c in self . comps : setattr ( self , '_comp_' + c . category , c )
5887	def extract ( self , url = None , raw_html = None ) : crawl_candidate = CrawlCandidate ( self . config , url , raw_html ) return self . __crawl ( crawl_candidate )
2713	def get_object ( cls , api_token , snapshot_id ) : snapshot = cls ( token = api_token , id = snapshot_id ) snapshot . load ( ) return snapshot
2808	def convert_matmul ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting matmul ...' ) if names == 'short' : tf_name = 'MMUL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if len ( inputs ) == 1 : weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = False , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] ) elif len ( inputs ) == 2 : weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = False , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] ) else : raise AssertionError ( 'Cannot convert matmul layer' )
7589	def call_fastq_dump_on_SRRs ( self , srr , outname , paired ) : ## build command for fastq-dumping fd_cmd = [ "fastq-dump" , srr , "--accession" , outname , "--outdir" , self . workdir , "--gzip" , ] if paired : fd_cmd += [ "--split-files" ] ## call fq dump command proc = sps . Popen ( fd_cmd , stderr = sps . STDOUT , stdout = sps . PIPE ) o , e = proc . communicate ( ) ## delete the stupid temp sra file from the place ## that it is very hard-coded to be written to, and ## LEFT IN, for some crazy reason. srafile = os . path . join ( self . workdir , "sra" , srr + ".sra" ) if os . path . exists ( srafile ) : os . remove ( srafile )
182	def to_heatmap ( self , image_shape , size_lines = 1 , size_points = 0 , antialiased = True , raise_if_out_of_image = False ) : from . heatmaps import HeatmapsOnImage return HeatmapsOnImage ( self . draw_heatmap_array ( image_shape , size_lines = size_lines , size_points = size_points , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) , shape = image_shape )
8640	def retract_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'retract' } # POST /api/projects/0.1/bids/{bid_id}/?action=revoke endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRetractedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8074	def image ( self , path , x , y , width = None , height = None , alpha = 1.0 , data = None , draw = True , * * kwargs ) : return self . Image ( path , x , y , width , height , alpha , data , * * kwargs )
9162	def processor ( ) : # pragma: no cover registry = get_current_registry ( ) settings = registry . settings connection_string = settings [ CONNECTION_STRING ] channels = _get_channels ( settings ) # Code adapted from # http://initd.org/psycopg/docs/advanced.html#asynchronous-notifications with psycopg2 . connect ( connection_string ) as conn : conn . set_isolation_level ( ISOLATION_LEVEL_AUTOCOMMIT ) with conn . cursor ( ) as cursor : for channel in channels : cursor . execute ( 'LISTEN {}' . format ( channel ) ) logger . debug ( 'Waiting for notifications on channel "{}"' . format ( channel ) ) registry . notify ( ChannelProcessingStartUpEvent ( ) ) rlist = [ conn ] # wait until ready for reading wlist = [ ] # wait until ready for writing xlist = [ ] # wait for an "exceptional condition" timeout = 5 while True : if select . select ( rlist , wlist , xlist , timeout ) != ( [ ] , [ ] , [ ] ) : conn . poll ( ) while conn . notifies : notif = conn . notifies . pop ( 0 ) logger . debug ( 'Got NOTIFY: pid={} channel={} payload={}' . format ( notif . pid , notif . channel , notif . payload ) ) event = create_pg_notify_event ( notif ) try : registry . notify ( event ) except Exception : logger . exception ( 'Logging an uncaught exception' )
10957	def _calc_loglikelihood ( self , model = None , tile = None ) : if model is None : res = self . residuals else : res = model - self . _data [ tile . slicer ] sig , isig = self . sigma , 1.0 / self . sigma nlogs = - np . log ( np . sqrt ( 2 * np . pi ) * sig ) * res . size return - 0.5 * isig * isig * np . dot ( res . flat , res . flat ) + nlogs
3019	def from_json ( cls , json_data ) : if not isinstance ( json_data , dict ) : json_data = json . loads ( _helpers . _from_bytes ( json_data ) ) private_key_pkcs8_pem = None pkcs12_val = json_data . get ( _PKCS12_KEY ) password = None if pkcs12_val is None : private_key_pkcs8_pem = json_data [ '_private_key_pkcs8_pem' ] signer = crypt . Signer . from_string ( private_key_pkcs8_pem ) else : # NOTE: This assumes that private_key_pkcs8_pem is not also # in the serialized data. This would be very incorrect # state. pkcs12_val = base64 . b64decode ( pkcs12_val ) password = json_data [ '_private_key_password' ] signer = crypt . Signer . from_string ( pkcs12_val , password ) credentials = cls ( json_data [ '_service_account_email' ] , signer , scopes = json_data [ '_scopes' ] , private_key_id = json_data [ '_private_key_id' ] , client_id = json_data [ 'client_id' ] , user_agent = json_data [ '_user_agent' ] , * * json_data [ '_kwargs' ] ) if private_key_pkcs8_pem is not None : credentials . _private_key_pkcs8_pem = private_key_pkcs8_pem if pkcs12_val is not None : credentials . _private_key_pkcs12 = pkcs12_val if password is not None : credentials . _private_key_password = password credentials . invalid = json_data [ 'invalid' ] credentials . access_token = json_data [ 'access_token' ] credentials . token_uri = json_data [ 'token_uri' ] credentials . revoke_uri = json_data [ 'revoke_uri' ] token_expiry = json_data . get ( 'token_expiry' , None ) if token_expiry is not None : credentials . token_expiry = datetime . datetime . strptime ( token_expiry , client . EXPIRY_FORMAT ) return credentials
696	def getModelIDFromParamsHash ( self , paramsHash ) : entryIdx = self . _paramsHashToIndexes . get ( paramsHash , None ) if entryIdx is not None : return self . _allResults [ entryIdx ] [ 'modelID' ] else : return None
2853	def mpsse_read_gpio ( self ) : # Send command to read low byte and high byte. self . _write ( '\x81\x83' ) # Wait for 2 byte response. data = self . _poll_read ( 2 ) # Assemble response into 16 bit value. low_byte = ord ( data [ 0 ] ) high_byte = ord ( data [ 1 ] ) logger . debug ( 'Read MPSSE GPIO low byte = {0:02X} and high byte = {1:02X}' . format ( low_byte , high_byte ) ) return ( high_byte << 8 ) | low_byte
5288	def forms_valid ( self , form , inlines ) : response = self . form_valid ( form ) for formset in inlines : formset . save ( ) return response
9237	def open ( self ) : if self . is_open : return try : os . chdir ( self . working_directory ) if self . chroot_directory : os . chroot ( self . chroot_directory ) os . setgid ( self . gid ) os . setuid ( self . uid ) os . umask ( self . umask ) except OSError as err : raise DaemonError ( 'Setting up Environment failed: {0}' . format ( err ) ) if self . prevent_core : try : resource . setrlimit ( resource . RLIMIT_CORE , ( 0 , 0 ) ) except Exception as err : raise DaemonError ( 'Could not disable core files: {0}' . format ( err ) ) if self . detach_process : try : if os . fork ( ) > 0 : os . _exit ( 0 ) except OSError as err : raise DaemonError ( 'First fork failed: {0}' . format ( err ) ) os . setsid ( ) try : if os . fork ( ) > 0 : os . _exit ( 0 ) except OSError as err : raise DaemonError ( 'Second fork failed: {0}' . format ( err ) ) for ( signal_number , handler ) in self . _signal_handler_map . items ( ) : signal . signal ( signal_number , handler ) close_filenos ( self . _files_preserve ) redirect_stream ( sys . stdin , self . stdin ) redirect_stream ( sys . stdout , self . stdout ) redirect_stream ( sys . stderr , self . stderr ) if self . pidfile : self . pidfile . acquire ( ) self . _is_open = True
5943	def autoconvert ( s ) : if type ( s ) is not str : return s for converter in int , float , str : # try them in increasing order of lenience try : s = [ converter ( i ) for i in s . split ( ) ] if len ( s ) == 1 : return s [ 0 ] else : return numpy . array ( s ) except ( ValueError , AttributeError ) : pass raise ValueError ( "Failed to autoconvert {0!r}" . format ( s ) )
6699	def get_selections ( ) : with settings ( hide ( 'stdout' ) ) : res = run_as_root ( 'dpkg --get-selections' ) selections = dict ( ) for line in res . splitlines ( ) : package , status = line . split ( ) selections . setdefault ( status , list ( ) ) . append ( package ) return selections
11308	def get_image ( self , obj ) : if self . _meta . image_field : return getattr ( obj , self . _meta . image_field )
11526	def create_small_thumbnail ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemId' ] = item_id response = self . request ( 'midas.thumbnailcreator.create.small.thumbnail' , parameters ) return response
7750	def __try_handlers ( self , handler_list , stanza , stanza_type = None ) : # pylint: disable=W0212 if stanza_type is None : stanza_type = stanza . stanza_type payload = stanza . get_all_payload ( ) classes = [ p . __class__ for p in payload ] keys = [ ( p . __class__ , p . handler_key ) for p in payload ] for handler in handler_list : type_filter = handler . _pyxmpp_stanza_handled [ 1 ] class_filter = handler . _pyxmpp_payload_class_handled extra_filter = handler . _pyxmpp_payload_key if type_filter != stanza_type : continue if class_filter : if extra_filter is None and class_filter not in classes : continue if extra_filter and ( class_filter , extra_filter ) not in keys : continue response = handler ( stanza ) if self . _process_handler_result ( response ) : return True return False
9312	def generate_key ( cls , secret_key , region , service , date , intermediates = False ) : init_key = ( 'AWS4' + secret_key ) . encode ( 'utf-8' ) date_key = cls . sign_sha256 ( init_key , date ) region_key = cls . sign_sha256 ( date_key , region ) service_key = cls . sign_sha256 ( region_key , service ) key = cls . sign_sha256 ( service_key , 'aws4_request' ) if intermediates : return ( key , date_key , region_key , service_key ) else : return key
2364	def append ( self , linenumber , raw_text , cells ) : self . rows . append ( Row ( linenumber , raw_text , cells ) )
7207	def execute ( self ) : # if not self.tasks: # raise WorkflowError('Workflow contains no tasks, and cannot be executed.') # for task in self.tasks: # self.definition['tasks'].append( task.generate_task_workflow_json() ) self . generate_workflow_description ( ) # hit batch workflow endpoint if batch values if self . batch_values : self . id = self . workflow . launch_batch_workflow ( self . definition ) # use regular workflow endpoint if no batch values else : self . id = self . workflow . launch ( self . definition ) return self . id
8216	def hide_variables_window ( self ) : if self . var_window is not None : self . var_window . window . destroy ( ) self . var_window = None
4634	def derive_private_key ( self , sequence ) : encoded = "%s %d" % ( str ( self ) , sequence ) a = bytes ( encoded , "ascii" ) s = hashlib . sha256 ( hashlib . sha512 ( a ) . digest ( ) ) . digest ( ) return PrivateKey ( hexlify ( s ) . decode ( "ascii" ) , prefix = self . pubkey . prefix )
5294	def get_params_for_field ( self , field_name , sort_type = None ) : if not sort_type : if self . initial_sort == field_name : sort_type = 'desc' if self . initial_sort_type == 'asc' else 'asc' else : sort_type = 'asc' self . initial_params [ self . sort_param_name ] = self . sort_fields [ field_name ] self . initial_params [ self . sort_type_param_name ] = sort_type return '?%s' % self . initial_params . urlencode ( )
5348	def compose_bugzilla ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'bugzilla' ] ) > 0 ] : if 'bugzilla' not in projects [ p ] : projects [ p ] [ 'bugzilla' ] = [ ] urls = [ url [ 'query_url' ] for url in data [ p ] [ 'bugzilla' ] if url [ 'query_url' ] not in projects [ p ] [ 'bugzilla' ] ] projects [ p ] [ 'bugzilla' ] += urls return projects
7257	def get_address_coords ( self , address ) : url = "https://maps.googleapis.com/maps/api/geocode/json?&address=" + address r = requests . get ( url ) r . raise_for_status ( ) results = r . json ( ) [ 'results' ] lat = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lat' ] lng = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lng' ] return lat , lng
13295	def convert_lsstdoc_tex ( content , to_fmt , deparagraph = False , mathjax = False , smart = True , extra_args = None ) : augmented_content = '\n' . join ( ( LSSTDOC_MACROS , content ) ) return convert_text ( augmented_content , 'latex' , to_fmt , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args )
1655	def IsDerivedFunction ( clean_lines , linenum ) : # Scan back a few lines for start of current function for i in xrange ( linenum , max ( - 1 , linenum - 10 ) , - 1 ) : match = Match ( r'^([^()]*\w+)\(' , clean_lines . elided [ i ] ) if match : # Look for "override" after the matching closing parenthesis line , _ , closing_paren = CloseExpression ( clean_lines , i , len ( match . group ( 1 ) ) ) return ( closing_paren >= 0 and Search ( r'\boverride\b' , line [ closing_paren : ] ) ) return False
10839	def edit ( self , text , media = None , utc = None , now = None ) : url = PATHS [ 'EDIT' ] % self . id post_data = "text=%s&" % text if now : post_data += "now=%s&" % now if utc : post_data += "utc=%s&" % utc if media : media_format = "media[%s]=%s&" for media_type , media_item in media . iteritems ( ) : post_data += media_format % ( media_type , media_item ) response = self . api . post ( url = url , data = post_data ) return Update ( api = self . api , raw_response = response [ 'update' ] )
3256	def mosaic_coverages ( self , store ) : params = dict ( ) url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "coveragestores" , store . name , "coverages.json" ] , params ) # GET /workspaces/<ws>/coveragestores/<name>/coverages.json headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to get mosaic coverages {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp . json ( )
2683	def get_function_config ( cfg ) : function_name = cfg . get ( 'function_name' ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) try : return client . get_function ( FunctionName = function_name ) except client . exceptions . ResourceNotFoundException as e : if 'Function not found' in str ( e ) : return False
12882	def main ( world_cls , referee_cls , gui_cls , gui_actor_cls , ai_actor_cls , theater_cls = PygletTheater , default_host = DEFAULT_HOST , default_port = DEFAULT_PORT , argv = None ) : import sys , os , docopt , nonstdlib exe_name = os . path . basename ( sys . argv [ 0 ] ) usage = main . __doc__ . format ( * * locals ( ) ) . strip ( ) args = docopt . docopt ( usage , argv or sys . argv [ 1 : ] ) num_guis = int ( args [ '<num_guis>' ] or 1 ) num_ais = int ( args [ '<num_ais>' ] or 0 ) host , port = args [ '--host' ] , int ( args [ '--port' ] ) logging . basicConfig ( format = '%(levelname)s: %(name)s: %(message)s' , level = nonstdlib . verbosity ( args [ '--verbose' ] ) , ) # Use the given game objects and command line arguments to play a game! if args [ 'debug' ] : print ( """\ ****************************** KNOWN BUG WARNING ****************************** In debug mode, every message produced by the logging system gets printed twice. I know vaguely why this is happening, but as of yet I've not been able to fix it. In the mean time, don't let this confuse you! *******************************************************************************""" ) game = MultiplayerDebugger ( world_cls , referee_cls , gui_cls , gui_actor_cls , num_guis , ai_actor_cls , num_ais , theater_cls , host , port ) else : game = theater_cls ( ) ai_actors = [ ai_actor_cls ( ) for i in range ( num_ais ) ] if args [ 'sandbox' ] : game . gui = gui_cls ( ) game . initial_stage = UniplayerGameStage ( world_cls ( ) , referee_cls ( ) , gui_actor_cls ( ) , ai_actors ) game . initial_stage . successor = PostgameSplashStage ( ) if args [ 'client' ] : game . gui = gui_cls ( ) game . initial_stage = ClientConnectionStage ( world_cls ( ) , gui_actor_cls ( ) , host , port ) if args [ 'server' ] : game . initial_stage = ServerConnectionStage ( world_cls ( ) , referee_cls ( ) , num_guis , ai_actors , host , port ) game . play ( )
13610	def load_gitconfig ( self ) : gitconfig_path = os . path . expanduser ( '~/.gitconfig' ) if os . path . exists ( gitconfig_path ) : parser = Parser ( ) parser . read ( gitconfig_path ) parser . sections ( ) return parser pass
10333	def group_nodes_by_annotation_filtered ( graph : BELGraph , node_predicates : NodePredicates = None , annotation : str = 'Subgraph' , ) -> Mapping [ str , Set [ BaseEntity ] ] : node_filter = concatenate_node_predicates ( node_predicates ) return { key : { node for node in nodes if node_filter ( graph , node ) } for key , nodes in group_nodes_by_annotation ( graph , annotation ) . items ( ) }
7882	def _make_prefix ( self , declared_prefixes ) : used_prefixes = set ( self . _prefixes . values ( ) ) used_prefixes |= set ( declared_prefixes . values ( ) ) while True : prefix = u"ns{0}" . format ( self . _next_id ) self . _next_id += 1 if prefix not in used_prefixes : break return prefix
13880	def MoveFile ( source_filename , target_filename ) : _AssertIsLocal ( source_filename ) _AssertIsLocal ( target_filename ) import shutil shutil . move ( source_filename , target_filename )
5516	def append ( self , data , start ) : if self . _limit is not None and self . _limit > 0 : if self . _start is None : self . _start = start if start - self . _start > self . reset_rate : self . _sum -= round ( ( start - self . _start ) * self . _limit ) self . _start = start self . _sum += len ( data )
8115	def distance ( x0 , y0 , x1 , y1 ) : return sqrt ( pow ( x1 - x0 , 2 ) + pow ( y1 - y0 , 2 ) )
12265	def wrap ( f_df , xref , size = 1 ) : memoized_f_df = lrucache ( lambda x : f_df ( restruct ( x , xref ) ) , size ) objective = compose ( first , memoized_f_df ) gradient = compose ( destruct , second , memoized_f_df ) return objective , gradient
4394	def adsSyncReadReqEx2 ( port , address , index_group , index_offset , data_type , return_ctypes = False ) : # type: (int, AmsAddr, int, int, Type, bool) -> Any sync_read_request = _adsDLL . AdsSyncReadReqEx2 ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) index_group_c = ctypes . c_ulong ( index_group ) index_offset_c = ctypes . c_ulong ( index_offset ) if data_type == PLCTYPE_STRING : data = ( STRING_BUFFER * PLCTYPE_STRING ) ( ) else : data = data_type ( ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . c_ulong ( ctypes . sizeof ( data ) ) bytes_read = ctypes . c_ulong ( ) bytes_read_pointer = ctypes . pointer ( bytes_read ) error_code = sync_read_request ( port , ams_address_pointer , index_group_c , index_offset_c , data_length , data_pointer , bytes_read_pointer , ) if error_code : raise ADSError ( error_code ) # If we're reading a value of predetermined size (anything but a string), # validate that the correct number of bytes were read if data_type != PLCTYPE_STRING and bytes_read . value != data_length . value : raise RuntimeError ( "Insufficient data (expected {0} bytes, {1} were read)." . format ( data_length . value , bytes_read . value ) ) if return_ctypes : return data if data_type == PLCTYPE_STRING : return data . value . decode ( "utf-8" ) if type ( data_type ) . __name__ == "PyCArrayType" : return [ i for i in data ] if hasattr ( data , "value" ) : return data . value return data
8321	def parse_important ( self , markup ) : important = [ ] table_titles = [ table . title for table in self . tables ] m = re . findall ( self . re [ "bold" ] , markup ) for bold in m : bold = self . plain ( bold ) if not bold in table_titles : important . append ( bold . lower ( ) ) return important
10132	def parse_grid ( grid_data ) : try : # Split the grid up. grid_parts = NEWLINE_RE . split ( grid_data ) if len ( grid_parts ) < 2 : raise ZincParseException ( 'Malformed grid received' , grid_data , 1 , 1 ) # Grid and column metadata are the first two lines. grid_meta_str = grid_parts . pop ( 0 ) col_meta_str = grid_parts . pop ( 0 ) # First element is the grid metadata ver_match = VERSION_RE . match ( grid_meta_str ) if ver_match is None : raise ZincParseException ( 'Could not determine version from %r' % grid_meta_str , grid_data , 1 , 1 ) version = Version ( ver_match . group ( 1 ) ) # Now parse the rest of the grid accordingly try : grid_meta = hs_gridMeta [ version ] . parseString ( grid_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse grid metadata: %s' % pe , grid_data , 1 , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse grid meta: %r' , grid_meta_str ) raise try : col_meta = hs_cols [ version ] . parseString ( col_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse column metadata: %s' % reformat_exception ( pe , 2 ) , grid_data , 2 , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse column meta: %r' , col_meta_str ) raise row_grammar = hs_row [ version ] def _parse_row ( row_num_and_data ) : ( row_num , row ) = row_num_and_data line_num = row_num + 3 try : return dict ( zip ( col_meta . keys ( ) , row_grammar . parseString ( row , parseAll = True ) [ 0 ] . asList ( ) ) ) except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse row: %s' % reformat_exception ( pe , line_num ) , grid_data , line_num , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse row: %r' , row ) raise g = Grid ( version = grid_meta . pop ( 'ver' ) , metadata = grid_meta , columns = list ( col_meta . items ( ) ) ) g . extend ( map ( _parse_row , filter ( lambda gp : bool ( gp [ 1 ] ) , enumerate ( grid_parts ) ) ) ) return g except : LOG . debug ( 'Failing grid: %r' , grid_data ) raise
5919	def center_fit ( self , * * kwargs ) : kwargs . setdefault ( 's' , self . tpr ) kwargs . setdefault ( 'n' , self . ndx ) kwargs [ 'f' ] = self . xtc kwargs . setdefault ( 'o' , self . outfile ( self . infix_filename ( None , self . xtc , '_centfit' , 'xtc' ) ) ) force = kwargs . pop ( 'force' , self . force ) logger . info ( "Centering and fitting trajectory {f!r}..." . format ( * * kwargs ) ) with utilities . in_dir ( self . dirname ) : if not self . check_file_exists ( kwargs [ 'o' ] , resolve = "indicate" , force = force ) : trj_fitandcenter ( * * kwargs ) logger . info ( "Centered and fit trajectory: {o!r}." . format ( * * kwargs ) ) return { 'tpr' : self . rp ( kwargs [ 's' ] ) , 'xtc' : self . rp ( kwargs [ 'o' ] ) }
2943	def validate ( self ) : results = [ ] from . . specs import Join def recursive_find_loop ( task , history ) : current = history [ : ] current . append ( task ) if isinstance ( task , Join ) : if task in history : msg = "Found loop with '%s': %s then '%s' again" % ( task . name , '->' . join ( [ p . name for p in history ] ) , task . name ) raise Exception ( msg ) for predecessor in task . inputs : recursive_find_loop ( predecessor , current ) for parent in task . inputs : recursive_find_loop ( parent , current ) for task_id , task in list ( self . task_specs . items ( ) ) : # Check for cyclic waits try : recursive_find_loop ( task , [ ] ) except Exception as exc : results . append ( exc . __str__ ( ) ) # Check for disconnected tasks if not task . inputs and task . name not in [ 'Start' , 'Root' ] : if task . outputs : results . append ( "Task '%s' is disconnected (no inputs)" % task . name ) else : LOG . debug ( "Task '%s' is not being used" % task . name ) return results
7217	def get_definition ( self , task_name ) : r = self . gbdx_connection . get ( self . _base_url + '/' + task_name ) raise_for_status ( r ) return r . json ( )
694	def loadExperimentDescriptionScriptFromDir ( experimentDir ) : descriptionScriptPath = os . path . join ( experimentDir , "description.py" ) module = _loadDescriptionFile ( descriptionScriptPath ) return module
2191	def expired ( self , cfgstr = None , product = None ) : products = self . _rectify_products ( product ) certificate = self . _get_certificate ( cfgstr = cfgstr ) if certificate is None : # We dont have a certificate, so we are expired is_expired = True elif products is None : # We dont have a product to check, so assume not expired is_expired = False elif not all ( map ( os . path . exists , products ) ) : # We are expired if the expected product does not exist is_expired = True else : # We are expired if the hash of the existing product data # does not match the expected hash in the certificate product_file_hash = self . _product_file_hash ( products ) certificate_hash = certificate . get ( 'product_file_hash' , None ) is_expired = product_file_hash != certificate_hash return is_expired
13630	def _renderResource ( resource , request ) : meth = getattr ( resource , 'render_' + nativeString ( request . method ) , None ) if meth is None : try : allowedMethods = resource . allowedMethods except AttributeError : allowedMethods = _computeAllowedMethods ( resource ) raise UnsupportedMethod ( allowedMethods ) return meth ( request )
3452	def find_essential_genes ( model , threshold = None , processes = None ) : if threshold is None : threshold = model . slim_optimize ( error_value = None ) * 1E-02 deletions = single_gene_deletion ( model , method = 'fba' , processes = processes ) essential = deletions . loc [ deletions [ 'growth' ] . isna ( ) | ( deletions [ 'growth' ] < threshold ) , : ] . index return { model . genes . get_by_id ( g ) for ids in essential for g in ids }
11862	def weighted_sample ( bn , e ) : w = 1 event = dict ( e ) # boldface x in Fig. 14.15 for node in bn . nodes : Xi = node . variable if Xi in e : w *= node . p ( e [ Xi ] , event ) else : event [ Xi ] = node . sample ( event ) return event , w
111	def warn_deprecated ( msg , stacklevel = 2 ) : import warnings warnings . warn ( msg , category = DeprecationWarning , stacklevel = stacklevel )
7386	def find_node_group_membership ( self , node ) : for group , nodelist in self . nodes . items ( ) : if node in nodelist : return group
10162	def setup ( app ) : lexer = MarkdownLexer ( ) for alias in lexer . aliases : app . add_lexer ( alias , lexer ) return dict ( version = __version__ )
2757	def get_all_floating_ips ( self ) : data = self . get_data ( "floating_ips" ) floating_ips = list ( ) for jsoned in data [ 'floating_ips' ] : floating_ip = FloatingIP ( * * jsoned ) floating_ip . token = self . token floating_ips . append ( floating_ip ) return floating_ips
10552	def _forbidden_attributes ( obj ) : for key in list ( obj . data . keys ( ) ) : if key in list ( obj . reserved_keys . keys ( ) ) : obj . data . pop ( key ) return obj
7453	def collate_files ( data , sname , tmp1s , tmp2s ) : ## out handle out1 = os . path . join ( data . dirs . fastqs , "{}_R1_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out1 , 'w' ) ) ## build cmd cmd1 = [ 'cat' ] for tmpfile in tmp1s : cmd1 += [ tmpfile ] ## compression function proc = sps . Popen ( [ 'which' , 'pigz' ] , stderr = sps . PIPE , stdout = sps . PIPE ) . communicate ( ) if proc [ 0 ] . strip ( ) : compress = [ "pigz" ] else : compress = [ "gzip" ] ## call cmd proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R1 %s" , err ) proc1 . stdout . close ( ) out . close ( ) ## then cleanup for tmpfile in tmp1s : os . remove ( tmpfile ) if 'pair' in data . paramsdict [ "datatype" ] : ## out handle out2 = os . path . join ( data . dirs . fastqs , "{}_R2_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out2 , 'w' ) ) ## build cmd cmd1 = [ 'cat' ] for tmpfile in tmp2s : cmd1 += [ tmpfile ] ## call cmd proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R2 %s" , err ) proc1 . stdout . close ( ) out . close ( ) ## then cleanup for tmpfile in tmp2s : os . remove ( tmpfile )
2739	def get_object ( cls , api_token , firewall_id ) : firewall = cls ( token = api_token , id = firewall_id ) firewall . load ( ) return firewall
6474	def apply_function ( self , points ) : if not self . option . function : return points if np is None : raise ImportError ( 'numpy is not available' ) if ':' in self . option . function : function , arguments = self . option . function . split ( ':' , 1 ) arguments = arguments . split ( ',' ) else : function = self . option . function arguments = [ ] # Resolve arguments arguments = list ( map ( self . _function_argument , arguments ) ) # Resolve function filter_function = FUNCTION . get ( function ) if filter_function is None : raise TypeError ( 'Invalid function "%s"' % ( function , ) ) else : # We wrap in ``list()`` to consume generators and iterators, as # ``np.array`` doesn't do this for us. return filter_function ( np . array ( list ( points ) ) , * arguments )
6597	def receive ( self ) : ret = self . communicationChannel . receive_all ( ) self . nruns -= len ( ret ) if self . nruns > 0 : import logging logger = logging . getLogger ( __name__ ) logger . warning ( 'too few results received: {} results received, {} more expected' . format ( len ( ret ) , self . nruns ) ) elif self . nruns < 0 : import logging logger = logging . getLogger ( __name__ ) logger . warning ( 'too many results received: {} results received, {} too many' . format ( len ( ret ) , - self . nruns ) ) return ret
13555	def delete_shifts ( self , shifts ) : url = "/2/shifts/?%s" % urlencode ( { 'ids' : "," . join ( str ( s ) for s in shifts ) } ) data = self . _delete_resource ( url ) return data
4058	def _bib_processor ( self , retrieved ) : items = [ ] for bib in retrieved . entries : items . append ( bib [ "content" ] [ 0 ] [ "value" ] ) self . url_params = None return items
502	def _labelToCategoryNumber ( self , label ) : if label not in self . saved_categories : self . saved_categories . append ( label ) return pow ( 2 , self . saved_categories . index ( label ) )
5929	def getLogLevel ( self , section , option ) : return logging . getLevelName ( self . get ( section , option ) . upper ( ) )
2113	def modify ( self , pk = None , create_on_missing = False , * * kwargs ) : # Associated with issue #52, the organization can't be modified # with the 'modify' command. This would create confusion about # whether its flag is an identifier versus a field to modify. if 'job_timeout' in kwargs and 'timeout' not in kwargs : kwargs [ 'timeout' ] = kwargs . pop ( 'job_timeout' ) return super ( Resource , self ) . write ( pk , create_on_missing = create_on_missing , force_on_exists = True , * * kwargs )
2710	def make_sentence ( sent_text ) : lex = [ ] idx = 0 for word in sent_text : if len ( word ) > 0 : if ( idx > 0 ) and not ( word [ 0 ] in ",.:;!?-\"'" ) : lex . append ( " " ) lex . append ( word ) idx += 1 return "" . join ( lex )
4558	def make_segments ( strip , length ) : if len ( strip ) % length : raise ValueError ( 'The length of strip must be a multiple of length' ) s = [ ] try : while True : s . append ( s [ - 1 ] . next ( length ) if s else Segment ( strip , length ) ) except ValueError : return s
7415	def copy ( self ) : cp = copy . deepcopy ( self ) cp . genotypes = allel . GenotypeArray ( self . genotypes , copy = True ) return cp
4824	def get_course_modes ( self , course_id ) : details = self . get_course_details ( course_id ) modes = details . get ( 'course_modes' , [ ] ) return self . _sort_course_modes ( [ mode for mode in modes if mode [ 'slug' ] not in EXCLUDED_COURSE_MODES ] )
2219	def register ( self , type ) : def _decorator ( func ) : if isinstance ( type , tuple ) : for t in type : self . func_registry [ t ] = func else : self . func_registry [ type ] = func return func return _decorator
12726	def stop_cfms ( self , stop_cfms ) : _set_params ( self . ode_obj , 'StopCFM' , stop_cfms , self . ADOF + self . LDOF )
6428	def encode ( self , word , lang = 'en' ) : if lang == 'es' : return self . _phonetic_spanish . encode ( self . _spanish_metaphone . encode ( word ) ) word = self . _soundex . encode ( self . _metaphone . encode ( word ) ) word = word [ 0 ] . translate ( self . _trans ) + word [ 1 : ] return word
7994	def _send ( self , stanza ) : self . fix_out_stanza ( stanza ) element = stanza . as_xml ( ) self . _write_element ( element )
2571	def send_message ( self ) : start = time . time ( ) message = None if not self . initialized : message = self . construct_start_message ( ) self . initialized = True else : message = self . construct_end_message ( ) self . send_UDP_message ( message ) end = time . time ( ) return end - start
12215	def get_frame_locals ( stepback = 0 ) : with Frame ( stepback = stepback ) as frame : locals_dict = frame . f_locals return locals_dict
12714	def connect_to ( self , joint , other_body , offset = ( 0 , 0 , 0 ) , other_offset = ( 0 , 0 , 0 ) , * * kwargs ) : anchor = self . world . move_next_to ( self , other_body , offset , other_offset ) self . world . join ( joint , self , other_body , anchor = anchor , * * kwargs )
13062	def get_siblings ( self , objectId , subreference , passage ) : reffs = [ reff for reff , _ in self . get_reffs ( objectId ) ] if subreference in reffs : index = reffs . index ( subreference ) # Not the first item and not the last one if 0 < index < len ( reffs ) - 1 : return reffs [ index - 1 ] , reffs [ index + 1 ] elif index == 0 and index < len ( reffs ) - 1 : return None , reffs [ 1 ] elif index > 0 and index == len ( reffs ) - 1 : return reffs [ index - 1 ] , None else : return None , None else : return passage . siblingsId
12048	def determineProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 5000 ) #it should be in the first 5k of the file f . close ( ) protoComment = "unknown" if b"SWHLab4[" in raw : protoComment = raw . split ( b"SWHLab4[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] elif b"SWH[" in raw : protoComment = raw . split ( b"SWH[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] else : protoComment = "?" if not type ( protoComment ) is str : protoComment = protoComment . decode ( "utf-8" ) return protoComment
7968	def _run_io_threads ( self , handler ) : reader = ReadingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) writter = WrittingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) self . io_threads += [ reader , writter ] reader . start ( ) writter . start ( )
5368	def _get_storage_service ( credentials ) : if credentials is None : credentials = oauth2client . client . GoogleCredentials . get_application_default ( ) return discovery . build ( 'storage' , 'v1' , credentials = credentials )
1917	def fork ( self , state , expression , policy = 'ALL' , setstate = None ) : assert isinstance ( expression , Expression ) if setstate is None : setstate = lambda x , y : None # Find a set of solutions for expression solutions = state . concretize ( expression , policy ) if not solutions : raise ExecutorError ( "Forking on unfeasible constraint set" ) if len ( solutions ) == 1 : setstate ( state , solutions [ 0 ] ) return state logger . info ( "Forking. Policy: %s. Values: %s" , policy , ', ' . join ( f'0x{sol:x}' for sol in solutions ) ) self . _publish ( 'will_fork_state' , state , expression , solutions , policy ) # Build and enqueue a state for each solution children = [ ] for new_value in solutions : with state as new_state : new_state . constrain ( expression == new_value ) # and set the PC of the new state to the concrete pc-dest #(or other register or memory address to concrete) setstate ( new_state , new_value ) self . _publish ( 'did_fork_state' , new_state , expression , new_value , policy ) # enqueue new_state state_id = self . enqueue ( new_state ) # maintain a list of children for logging purpose children . append ( state_id ) logger . info ( "Forking current state into states %r" , children ) return None
12157	def list_order_by ( l , firstItems ) : l = list ( l ) for item in firstItems [ : : - 1 ] : #backwards if item in l : l . remove ( item ) l . insert ( 0 , item ) return l
4314	def validate_input_file ( input_filepath ) : if not os . path . exists ( input_filepath ) : raise IOError ( "input_filepath {} does not exist." . format ( input_filepath ) ) ext = file_extension ( input_filepath ) if ext not in VALID_FORMATS : logger . info ( "Valid formats: %s" , " " . join ( VALID_FORMATS ) ) logger . warning ( "This install of SoX cannot process .{} files." . format ( ext ) )
3449	def _init_worker ( model , loopless , sense ) : global _model global _loopless _model = model _model . solver . objective . direction = sense _loopless = loopless
10387	def build_database ( manager : pybel . Manager , annotation_url : Optional [ str ] = None ) -> None : annotation_url = annotation_url or NEUROMMSIG_DEFAULT_URL annotation = manager . get_namespace_by_url ( annotation_url ) if annotation is None : raise RuntimeError ( 'no graphs in database with given annotation' ) networks = get_networks_using_annotation ( manager , annotation ) dtis = ... for network in networks : graph = network . as_bel ( ) scores = epicom_on_graph ( graph , dtis ) for ( drug_name , subgraph_name ) , score in scores . items ( ) : drug_model = get_drug_model ( manager , drug_name ) subgraph_model = manager . get_annotation_entry ( annotation_url , subgraph_name ) score_model = Score ( network = network , annotation = subgraph_model , drug = drug_model , score = score ) manager . session . add ( score_model ) t = time . time ( ) logger . info ( 'committing scores' ) manager . session . commit ( ) logger . info ( 'committed scores in %.2f seconds' , time . time ( ) - t )
11934	def auto_widget ( field ) : # Auto-detect info = { 'widget' : field . field . widget . __class__ . __name__ , 'field' : field . field . __class__ . __name__ , 'name' : field . name , } return [ fmt . format ( * * info ) for fmt in ( '{field}_{widget}_{name}' , '{field}_{name}' , '{widget}_{name}' , '{field}_{widget}' , '{name}' , '{widget}' , '{field}' , ) ]
7005	def apply_rf_classifier ( classifier , varfeaturesdir , outpickle , maxobjects = None ) : if isinstance ( classifier , str ) and os . path . exists ( classifier ) : with open ( classifier , 'rb' ) as infd : clfdict = pickle . load ( infd ) elif isinstance ( classifier , dict ) : clfdict = classifier else : LOGERROR ( "can't figure out the input classifier arg" ) return None # get the features to extract from clfdict if 'feature_names' not in clfdict : LOGERROR ( "feature_names not present in classifier input, " "can't figure out which ones to extract from " "varfeature pickles in %s" % varfeaturesdir ) return None # get the feature labeltype, pklglob, and maxobjects from classifier's # collect_kwargs elem. featurestouse = clfdict [ 'feature_names' ] pklglob = clfdict [ 'collect_kwargs' ] [ 'pklglob' ] magcol = clfdict [ 'magcol' ] # extract the features used by the classifier from the varfeatures pickles # in varfeaturesdir using the pklglob provided featfile = os . path . join ( os . path . dirname ( outpickle ) , 'actual-collected-features.pkl' ) features = collect_nonperiodic_features ( varfeaturesdir , magcol , featfile , pklglob = pklglob , featurestouse = featurestouse , maxobjects = maxobjects ) # now use the trained classifier on these features bestclf = clfdict [ 'best_classifier' ] predicted_labels = bestclf . predict ( features [ 'features_array' ] ) # FIXME: do we need to use the probability calibration curves to fix these # probabilities? probably. figure out how to do this. predicted_label_probs = bestclf . predict_proba ( features [ 'features_array' ] ) outdict = { 'features' : features , 'featfile' : featfile , 'classifier' : clfdict , 'predicted_labels' : predicted_labels , 'predicted_label_probs' : predicted_label_probs , } with open ( outpickle , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outdict
4650	def appendSigner ( self , accounts , permission ) : assert permission in self . permission_types , "Invalid permission" if self . blockchain . wallet . locked ( ) : raise WalletLocked ( ) if not isinstance ( accounts , ( list , tuple , set ) ) : accounts = [ accounts ] for account in accounts : # Now let's actually deal with the accounts if account not in self . signing_accounts : # is the account an instance of public key? if isinstance ( account , self . publickey_class ) : self . appendWif ( self . blockchain . wallet . getPrivateKeyForPublicKey ( str ( account ) ) ) # ... or should we rather obtain the keys from an account name else : accountObj = self . account_class ( account , blockchain_instance = self . blockchain ) required_treshold = accountObj [ permission ] [ "weight_threshold" ] keys = self . _fetchkeys ( accountObj , permission , required_treshold = required_treshold ) # If we couldn't find an active key, let's try overwrite it # with an owner key if not keys and permission != "owner" : keys . extend ( self . _fetchkeys ( accountObj , "owner" , required_treshold = required_treshold ) ) for x in keys : self . appendWif ( x [ 0 ] ) self . signing_accounts . append ( account )
3258	def get_resources ( self , names = None , stores = None , workspaces = None ) : stores = self . get_stores ( names = stores , workspaces = workspaces ) resources = [ ] for s in stores : try : resources . extend ( s . get_resources ( ) ) except FailedRequestError : continue if names is None : names = [ ] elif isinstance ( names , basestring ) : names = [ s . strip ( ) for s in names . split ( ',' ) if s . strip ( ) ] if resources and names : return ( [ resource for resource in resources if resource . name in names ] ) return resources
2254	def argunique ( items , key = None ) : # yield from unique(range(len(items)), key=lambda i: items[i]) if key is None : return unique ( range ( len ( items ) ) , key = lambda i : items [ i ] ) else : return unique ( range ( len ( items ) ) , key = lambda i : key ( items [ i ] ) )
7599	def get_popular_players ( self , * * params : keys ) : url = self . api . POPULAR + '/players' return self . _get_model ( url , PartialPlayerClan , * * params )
6150	def fir_remez_lpf ( f_pass , f_stop , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = lowpass_order ( f_pass , f_stop , d_pass , d_stop , fsamp = fs ) # Bump up the order by N_bump to bring down the final d_pass & d_stop N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) print ( 'Remez filter taps = %d.' % N_taps ) return b
7719	def xpath_eval ( self , expr ) : ctxt = common_doc . xpathNewContext ( ) ctxt . setContextNode ( self . xmlnode ) ctxt . xpathRegisterNs ( "muc" , self . ns . getContent ( ) ) ret = ctxt . xpathEval ( to_utf8 ( expr ) ) ctxt . xpathFreeContext ( ) return ret
4475	def sample_clip_indices ( filename , n_samples , sr ) : with psf . SoundFile ( str ( filename ) , mode = 'r' ) as soundf : # Measure required length of fragment n_target = int ( np . ceil ( n_samples * soundf . samplerate / float ( sr ) ) ) # Raise exception if source is too short if len ( soundf ) < n_target : raise RuntimeError ( 'Source {} (length={})' . format ( filename , len ( soundf ) ) + ' must be at least the length of the input ({})' . format ( n_target ) ) # Draw a starting point at random in the background waveform start = np . random . randint ( 0 , 1 + len ( soundf ) - n_target ) stop = start + n_target return start , stop
9106	def dropbox_editor_factory ( request ) : dropbox = dropbox_factory ( request ) if is_equal ( dropbox . editor_token , request . matchdict [ 'editor_token' ] . encode ( 'utf-8' ) ) : return dropbox else : raise HTTPNotFound ( 'invalid editor token' )
411	def minibatches ( inputs = None , targets = None , batch_size = None , allow_dynamic_batch_size = False , shuffle = False ) : if len ( inputs ) != len ( targets ) : raise AssertionError ( "The length of inputs and targets should be equal" ) if shuffle : indices = np . arange ( len ( inputs ) ) np . random . shuffle ( indices ) # for start_idx in range(0, len(inputs) - batch_size + 1, batch_size): # chulei: handling the case where the number of samples is not a multiple of batch_size, avoiding wasting samples for start_idx in range ( 0 , len ( inputs ) , batch_size ) : end_idx = start_idx + batch_size if end_idx > len ( inputs ) : if allow_dynamic_batch_size : end_idx = len ( inputs ) else : break if shuffle : excerpt = indices [ start_idx : end_idx ] else : excerpt = slice ( start_idx , end_idx ) if ( isinstance ( inputs , list ) or isinstance ( targets , list ) ) and ( shuffle == True ) : # zsdonghao: for list indexing when shuffle==True yield [ inputs [ i ] for i in excerpt ] , [ targets [ i ] for i in excerpt ] else : yield inputs [ excerpt ] , targets [ excerpt ]
10475	def _sendKeyWithModifiers ( self , keychr , modifiers , globally = False ) : if not self . _isSingleCharacter ( keychr ) : raise ValueError ( 'Please provide only one character to send' ) if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) modFlags = self . _pressModifiers ( modifiers , globally = globally ) # Press the non-modifier key self . _sendKey ( keychr , modFlags , globally = globally ) # Release the modifiers self . _releaseModifiers ( modifiers , globally = globally ) # Post the queued keypresses: self . _postQueuedEvents ( )
2131	def get ( self , pk = None , * * kwargs ) : if kwargs . pop ( 'include_debug_header' , True ) : debug . log ( 'Getting the role record.' , header = 'details' ) data , self . endpoint = self . data_endpoint ( kwargs ) response = self . read ( pk = pk , fail_on_no_results = True , fail_on_multiple_results = True , * * data ) item_dict = response [ 'results' ] [ 0 ] self . configure_display ( item_dict ) return item_dict
13349	def launch ( prompt_prefix = None ) : if prompt_prefix : os . environ [ 'PROMPT' ] = prompt ( prompt_prefix ) subprocess . call ( cmd ( ) , env = os . environ . data )
13765	def insert ( self , index , value ) : self . _list . insert ( index , value ) self . _sync ( )
3946	def _decode_repeated_field ( message , field , value_list ) : if field . type == FieldDescriptor . TYPE_MESSAGE : for value in value_list : decode ( getattr ( message , field . name ) . add ( ) , value ) else : try : for value in value_list : if field . type == FieldDescriptor . TYPE_BYTES : value = base64 . b64decode ( value ) getattr ( message , field . name ) . append ( value ) except ( ValueError , TypeError ) as e : # ValueError: invalid enum value, negative unsigned int value, or # invalid base64 # TypeError: mismatched type logger . warning ( 'Message %r ignoring repeated field %s: %s' , message . __class__ . __name__ , field . name , e ) # Ignore any values already decoded by clearing list message . ClearField ( field . name )
110	def imshow ( image , backend = IMSHOW_BACKEND_DEFAULT ) : do_assert ( backend in [ "matplotlib" , "cv2" ] , "Expected backend 'matplotlib' or 'cv2', got %s." % ( backend , ) ) if backend == "cv2" : image_bgr = image if image . ndim == 3 and image . shape [ 2 ] in [ 3 , 4 ] : image_bgr = image [ ... , 0 : 3 ] [ ... , : : - 1 ] win_name = "imgaug-default-window" cv2 . namedWindow ( win_name , cv2 . WINDOW_NORMAL ) cv2 . imshow ( win_name , image_bgr ) cv2 . waitKey ( 0 ) cv2 . destroyWindow ( win_name ) else : # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225) import matplotlib . pyplot as plt dpi = 96 h , w = image . shape [ 0 ] / dpi , image . shape [ 1 ] / dpi w = max ( w , 6 ) # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly) fig , ax = plt . subplots ( figsize = ( w , h ) , dpi = dpi ) fig . canvas . set_window_title ( "imgaug.imshow(%s)" % ( image . shape , ) ) ax . imshow ( image , cmap = "gray" ) # cmap is only activate for grayscale images plt . show ( )
10154	def _extract_transform_colander_schema ( self , args ) : schema = args . get ( 'schema' , colander . MappingSchema ( ) ) if not isinstance ( schema , colander . Schema ) : schema = schema ( ) schema = schema . clone ( ) for transformer in self . schema_transformers : schema = transformer ( schema , args ) return schema
11203	def picknthweekday ( year , month , dayofweek , hour , minute , whichweek ) : first = datetime . datetime ( year , month , 1 , hour , minute ) # This will work if dayofweek is ISO weekday (1-7) or Microsoft-style (0-6), # Because 7 % 7 = 0 weekdayone = first . replace ( day = ( ( dayofweek - first . isoweekday ( ) ) % 7 ) + 1 ) wd = weekdayone + ( ( whichweek - 1 ) * ONEWEEK ) if ( wd . month != month ) : wd -= ONEWEEK return wd
9104	def dropbox_post_factory ( request ) : try : max_age = int ( request . registry . settings . get ( 'post_token_max_age_seconds' ) ) except Exception : max_age = 300 try : drop_id = parse_post_token ( token = request . matchdict [ 'token' ] , secret = request . registry . settings [ 'post_secret' ] , max_age = max_age ) except SignatureExpired : raise HTTPGone ( 'dropbox expired' ) except Exception : # don't be too specific on the reason for the error raise HTTPNotFound ( 'no such dropbox' ) dropbox = request . registry . settings [ 'dropbox_container' ] . get_dropbox ( drop_id ) if dropbox . status_int >= 20 : raise HTTPGone ( 'dropbox already in processing, no longer accepts data' ) return dropbox
6720	def init ( self ) : r = self . local_renderer # if self.virtualenv_exists(): # print('virtualenv exists') # return print ( 'Creating new virtual environment...' ) with self . settings ( warn_only = True ) : cmd = '[ ! -d {virtualenv_dir} ] && virtualenv --no-site-packages {virtualenv_dir} || true' if self . is_local : r . run_or_local ( cmd ) else : r . sudo ( cmd )
9440	def request ( self , path , method = None , data = { } ) : if not path : raise ValueError ( 'Invalid path parameter' ) if method and method not in [ 'GET' , 'POST' , 'DELETE' , 'PUT' ] : raise NotImplementedError ( 'HTTP %s method not implemented' % method ) if path [ 0 ] == '/' : uri = self . url + path else : uri = self . url + '/' + path if APPENGINE : return json . loads ( self . _appengine_fetch ( uri , data , method ) ) return json . loads ( self . _urllib2_fetch ( uri , data , method ) )
8162	def publish_event ( event_t , data = None , extra_channels = None , wait = None ) : event = Event ( event_t , data ) pubsub . publish ( "shoebot" , event ) for channel_name in extra_channels or [ ] : pubsub . publish ( channel_name , event ) if wait is not None : channel = pubsub . subscribe ( wait ) channel . listen ( wait )
1456	def valid_path ( path ) : # check if the suffic of classpath suffix exists as directory if path . endswith ( '*' ) : Log . debug ( 'Checking classpath entry suffix as directory: %s' , path [ : - 1 ] ) if os . path . isdir ( path [ : - 1 ] ) : return True return False # check if the classpath entry is a directory Log . debug ( 'Checking classpath entry as directory: %s' , path ) if os . path . isdir ( path ) : return True else : # check if the classpath entry is a file Log . debug ( 'Checking classpath entry as file: %s' , path ) if os . path . isfile ( path ) : return True return False
4365	def decode ( rawstr , json_loads = default_json_loads ) : decoded_msg = { } try : # Handle decoding in Python<3. rawstr = rawstr . decode ( 'utf-8' ) except AttributeError : pass split_data = rawstr . split ( ":" , 3 ) msg_type = split_data [ 0 ] msg_id = split_data [ 1 ] endpoint = split_data [ 2 ] data = '' if msg_id != '' : if "+" in msg_id : msg_id = msg_id . split ( '+' ) [ 0 ] decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = 'data' else : decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = True # common to every message msg_type_id = int ( msg_type ) if msg_type_id in MSG_VALUES : decoded_msg [ 'type' ] = MSG_VALUES [ int ( msg_type ) ] else : raise Exception ( "Unknown message type: %s" % msg_type ) decoded_msg [ 'endpoint' ] = endpoint if len ( split_data ) > 3 : data = split_data [ 3 ] if msg_type == "0" : # disconnect pass elif msg_type == "1" : # connect decoded_msg [ 'qs' ] = data elif msg_type == "2" : # heartbeat pass elif msg_type == "3" : # message decoded_msg [ 'data' ] = data elif msg_type == "4" : # json msg decoded_msg [ 'data' ] = json_loads ( data ) elif msg_type == "5" : # event try : data = json_loads ( data ) except ValueError : print ( "Invalid JSON event message" , data ) decoded_msg [ 'args' ] = [ ] else : decoded_msg [ 'name' ] = data . pop ( 'name' ) if 'args' in data : decoded_msg [ 'args' ] = data [ 'args' ] else : decoded_msg [ 'args' ] = [ ] elif msg_type == "6" : # ack if '+' in data : ackId , data = data . split ( '+' ) decoded_msg [ 'ackId' ] = int ( ackId ) decoded_msg [ 'args' ] = json_loads ( data ) else : decoded_msg [ 'ackId' ] = int ( data ) decoded_msg [ 'args' ] = [ ] elif msg_type == "7" : # error if '+' in data : reason , advice = data . split ( '+' ) decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( reason ) ] decoded_msg [ 'advice' ] = ADVICES_VALUES [ int ( advice ) ] else : decoded_msg [ 'advice' ] = '' if data != '' : decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( data ) ] else : decoded_msg [ 'reason' ] = '' elif msg_type == "8" : # noop pass return decoded_msg
4	def parse_unknown_args ( args ) : retval = { } preceded_by_key = False for arg in args : if arg . startswith ( '--' ) : if '=' in arg : key = arg . split ( '=' ) [ 0 ] [ 2 : ] value = arg . split ( '=' ) [ 1 ] retval [ key ] = value else : key = arg [ 2 : ] preceded_by_key = True elif preceded_by_key : retval [ key ] = arg preceded_by_key = False return retval
8022	def cast ( cls , fx_spot , domestic_curve = None , foreign_curve = None ) : assert domestic_curve . origin == foreign_curve . origin return cls ( fx_spot , domestic_curve = domestic_curve , foreign_curve = foreign_curve )
11159	def execute_pyfile ( self , py_exe = None ) : # pragma: no cover import subprocess self . assert_is_dir_and_exists ( ) if py_exe is None : if six . PY2 : py_exe = "python2" elif six . PY3 : py_exe = "python3" for p in self . select_by_ext ( ".py" ) : subprocess . Popen ( '%s "%s"' % ( py_exe , p . abspath ) )
11696	def verify_editor ( self ) : powerful_editors = [ 'josm' , 'level0' , 'merkaartor' , 'qgis' , 'arcgis' , 'upload.py' , 'osmapi' , 'Services_OpenStreetMap' ] if self . editor is not None : for editor in powerful_editors : if editor in self . editor . lower ( ) : self . powerfull_editor = True break if 'iD' in self . editor : trusted_hosts = [ 'www.openstreetmap.org/id' , 'www.openstreetmap.org/edit' , 'improveosm.org' , 'strava.github.io/iD' , 'preview.ideditor.com/release' , 'preview.ideditor.com/master' , 'hey.mapbox.com/iD-internal' , 'projets.pavie.info/id-indoor' , 'maps.mapcat.com/edit' , 'id.softek.ir' ] if self . host . split ( '://' ) [ - 1 ] . strip ( '/' ) not in trusted_hosts : self . label_suspicious ( 'Unknown iD instance' ) else : self . powerfull_editor = True self . label_suspicious ( 'Software editor was not declared' )
12819	def _file_size ( self , field ) : size = 0 try : handle = open ( self . _files [ field ] , "r" ) size = os . fstat ( handle . fileno ( ) ) . st_size handle . close ( ) except : size = 0 self . _file_lengths [ field ] = size return self . _file_lengths [ field ]
507	def removeLabels ( self , start = None , end = None , labelFilter = None ) : if len ( self . _recordsCache ) == 0 : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for " "'removeLabels'. Model has no saved records." ) try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = self . _recordsCache [ - 1 ] . ROWID startID = self . _recordsCache [ 0 ] . ROWID clippedStart = 0 if start is None else max ( 0 , start - startID ) clippedEnd = len ( self . _recordsCache ) if end is None else max ( 0 , min ( len ( self . _recordsCache ) , end - startID ) ) if clippedEnd <= clippedStart : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for " "'removeLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'clippedRequestRange' : { 'startRecordID' : clippedStart , 'endRecordID' : clippedEnd } , 'validRange' : { 'startRecordID' : startID , 'endRecordID' : self . _recordsCache [ len ( self . _recordsCache ) - 1 ] . ROWID } , 'numRecordsStored' : len ( self . _recordsCache ) } ) # Remove records within the cache recordsToDelete = [ ] for state in self . _recordsCache [ clippedStart : clippedEnd ] : if labelFilter is not None : if labelFilter in state . anomalyLabel : state . anomalyLabel . remove ( labelFilter ) else : state . anomalyLabel = [ ] state . setByUser = False recordsToDelete . append ( state ) self . _deleteRecordsFromKNN ( recordsToDelete ) # Remove records not in cache self . _deleteRangeFromKNN ( start , end ) # Recompute [clippedEnd, ...) for state in self . _recordsCache [ clippedEnd : ] : self . _classifyState ( state )
4651	def appendWif ( self , wif ) : if wif : try : self . privatekey_class ( wif ) self . wifs . add ( wif ) except Exception : raise InvalidWifError
5274	def find ( self , y ) : node = self . root while True : edge = self . _edgeLabel ( node , node . parent ) if edge . startswith ( y ) : return node . idx i = 0 while ( i < len ( edge ) and edge [ i ] == y [ 0 ] ) : y = y [ 1 : ] i += 1 if i != 0 : if i == len ( edge ) and y != '' : pass else : return - 1 node = node . _get_transition_link ( y [ 0 ] ) if not node : return - 1
10367	def complex_increases_activity ( graph : BELGraph , u : BaseEntity , v : BaseEntity , key : str ) -> bool : return ( isinstance ( u , ( ComplexAbundance , NamedComplexAbundance ) ) and complex_has_member ( graph , u , v ) and part_has_modifier ( graph [ u ] [ v ] [ key ] , OBJECT , ACTIVITY ) )
4470	def _get_param_names ( cls ) : init = cls . __init__ args , varargs = inspect . getargspec ( init ) [ : 2 ] if varargs is not None : raise RuntimeError ( 'BaseTransformer objects cannot have varargs' ) args . pop ( 0 ) args . sort ( ) return args
449	def batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , data_format , name = None ) : with ops . name_scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : inv = math_ops . rsqrt ( variance + variance_epsilon ) if scale is not None : inv *= scale a = math_ops . cast ( inv , x . dtype ) b = math_ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) # Return a * x + b with customized data_format. # Currently TF doesn't have bias_scale, and tensorRT has bug in converting tf.nn.bias_add # So we reimplemted them to allow make the model work with tensorRT. # See https://github.com/tensorlayer/openpose-plus/issues/75 for more details. df = { 'channels_first' : 'NCHW' , 'channels_last' : 'NHWC' } return _bias_add ( _bias_scale ( x , a , df [ data_format ] ) , b , df [ data_format ] )
13134	def import_domaindump ( ) : parser = argparse . ArgumentParser ( description = "Imports users, groups and computers result files from the ldapdomaindump tool, will resolve the names from domain_computers output for IPs" ) parser . add_argument ( "files" , nargs = '+' , help = "The domaindump files to import" ) arguments = parser . parse_args ( ) domain_users_file = '' domain_groups_file = '' computer_count = 0 user_count = 0 stats = { } for filename in arguments . files : if filename . endswith ( 'domain_computers.json' ) : print_notification ( 'Parsing domain computers' ) computer_count = parse_domain_computers ( filename ) if computer_count : stats [ 'hosts' ] = computer_count print_success ( "{} hosts imported" . format ( computer_count ) ) elif filename . endswith ( 'domain_users.json' ) : domain_users_file = filename elif filename . endswith ( 'domain_groups.json' ) : domain_groups_file = filename if domain_users_file : print_notification ( "Parsing domain users" ) user_count = parse_domain_users ( domain_users_file , domain_groups_file ) if user_count : print_success ( "{} users imported" . format ( user_count ) ) stats [ 'users' ] = user_count Logger ( ) . log ( "import_domaindump" , 'Imported domaindump, found {} user, {} systems' . format ( user_count , computer_count ) , stats )
3032	def credentials_from_code ( client_id , client_secret , scope , code , redirect_uri = 'postmessage' , http = None , user_agent = None , token_uri = oauth2client . GOOGLE_TOKEN_URI , auth_uri = oauth2client . GOOGLE_AUTH_URI , revoke_uri = oauth2client . GOOGLE_REVOKE_URI , device_uri = oauth2client . GOOGLE_DEVICE_URI , token_info_uri = oauth2client . GOOGLE_TOKEN_INFO_URI , pkce = False , code_verifier = None ) : flow = OAuth2WebServerFlow ( client_id , client_secret , scope , redirect_uri = redirect_uri , user_agent = user_agent , auth_uri = auth_uri , token_uri = token_uri , revoke_uri = revoke_uri , device_uri = device_uri , token_info_uri = token_info_uri , pkce = pkce , code_verifier = code_verifier ) credentials = flow . step2_exchange ( code , http = http ) return credentials
7470	def fill_dups_arr ( data ) : ## build the duplicates array duplefiles = glob . glob ( os . path . join ( data . tmpdir , "duples_*.tmp.npy" ) ) duplefiles . sort ( key = lambda x : int ( x . rsplit ( "_" , 1 ) [ - 1 ] [ : - 8 ] ) ) ## enter the duplicates filter into super h5 array io5 = h5py . File ( data . clust_database , 'r+' ) dfilter = io5 [ "duplicates" ] ## enter all duple arrays into full duplicates array init = 0 for dupf in duplefiles : end = int ( dupf . rsplit ( "_" , 1 ) [ - 1 ] [ : - 8 ] ) inarr = np . load ( dupf ) dfilter [ init : end ] = inarr init += end - init #os.remove(dupf) #del inarr ## continued progress bar LOGGER . info ( "all duplicates: %s" , dfilter [ : ] . sum ( ) ) io5 . close ( )
9948	def new_space_from_excel ( self , book , range_ , sheet = None , name = None , names_row = None , param_cols = None , space_param_order = None , cells_param_order = None , transpose = False , names_col = None , param_rows = None , ) : space = self . _impl . new_space_from_excel ( book , range_ , sheet , name , names_row , param_cols , space_param_order , cells_param_order , transpose , names_col , param_rows , ) return get_interfaces ( space )
3299	def xml_to_bytes ( element , pretty_print = False ) : if use_lxml : xml = etree . tostring ( element , encoding = "UTF-8" , xml_declaration = True , pretty_print = pretty_print ) else : xml = etree . tostring ( element , encoding = "UTF-8" ) if not xml . startswith ( b"<?xml " ) : xml = b'<?xml version="1.0" encoding="utf-8" ?>\n' + xml assert xml . startswith ( b"<?xml " ) # ET should prepend an encoding header return xml
7874	def get_payload ( self , payload_class , payload_key = None , specialize = False ) : if self . _payload is None : self . decode_payload ( ) if payload_class is None : if self . _payload : payload = self . _payload [ 0 ] if specialize and isinstance ( payload , XMLPayload ) : klass = payload_class_for_element_name ( payload . element . tag ) if klass is not XMLPayload : payload = klass . from_xml ( payload . element ) self . _payload [ 0 ] = payload return payload else : return None # pylint: disable=W0212 elements = payload_class . _pyxmpp_payload_element_name for i , payload in enumerate ( self . _payload ) : if isinstance ( payload , XMLPayload ) : if payload_class is not XMLPayload : if payload . xml_element_name not in elements : continue payload = payload_class . from_xml ( payload . element ) elif not isinstance ( payload , payload_class ) : continue if payload_key is not None and payload_key != payload . handler_key ( ) : continue self . _payload [ i ] = payload return payload return None
3739	def Hf_g ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in ATcT_g . index : methods . append ( ATCT_G ) if CASRN in TRC_gas_data . index and not np . isnan ( TRC_gas_data . at [ CASRN , 'Hf' ] ) : methods . append ( TRC ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ATCT_G : _Hfg = float ( ATcT_g . at [ CASRN , 'Hf_298K' ] ) elif Method == TRC : _Hfg = float ( TRC_gas_data . at [ CASRN , 'Hf' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Hfg
8476	def _executeMassiveMethod ( path , method , args = None , classArgs = None ) : response = { } if args is None : args = { } if classArgs is None : classArgs = { } sys . path . append ( path ) exclude = [ "__init__.py" , "base.py" ] for f in AtomShieldsScanner . _getFiles ( path , "*.py" , exclude = exclude ) : try : instance = AtomShieldsScanner . _getClassInstance ( path = f , args = classArgs ) if instance is not None : if callable ( method ) : args [ "instance" ] = instance output = method ( * * args ) response [ instance . __class__ . NAME ] = output else : if hasattr ( instance , method ) : output = getattr ( instance , method ) ( * * args ) response [ instance . __class__ . NAME ] = output else : continue except Exception as e : AtomShieldsScanner . _debug ( "[!] %s" % e ) sys . path . remove ( path ) return response
3701	def solubility_eutectic ( T , Tm , Hm , Cpl = 0 , Cps = 0 , gamma = 1 ) : dCp = Cpl - Cps x = exp ( - Hm / R / T * ( 1 - T / Tm ) + dCp * ( Tm - T ) / R / T - dCp / R * log ( Tm / T ) ) / gamma return x
7656	def validate ( self , strict = True ) : valid = True try : jsonschema . validate ( self . __json__ , self . __schema__ ) except jsonschema . ValidationError as invalid : if strict : raise SchemaError ( str ( invalid ) ) else : warnings . warn ( str ( invalid ) ) valid = False return valid
10877	def calculate_linescan_psf ( x , y , z , normalize = False , kfki = 0.889 , zint = 100. , polar_angle = 0. , wrap = True , * * kwargs ) : #0. Set up vecs if wrap : xpts = vec_to_halfvec ( x ) ypts = vec_to_halfvec ( y ) x3 , y3 , z3 = np . meshgrid ( xpts , ypts , z , indexing = 'ij' ) else : x3 , y3 , z3 = np . meshgrid ( x , y , z , indexing = 'ij' ) rho3 = np . sqrt ( x3 * x3 + y3 * y3 ) #1. Hilm if wrap : y2 , z2 = np . meshgrid ( ypts , z , indexing = 'ij' ) hilm0 = calculate_linescan_ilm_psf ( y2 , z2 , zint = zint , polar_angle = polar_angle , * * kwargs ) if ypts [ 0 ] == 0 : hilm = np . append ( hilm0 [ - 1 : 0 : - 1 ] , hilm0 , axis = 0 ) else : hilm = np . append ( hilm0 [ : : - 1 ] , hilm0 , axis = 0 ) else : y2 , z2 = np . meshgrid ( y , z , indexing = 'ij' ) hilm = calculate_linescan_ilm_psf ( y2 , z2 , zint = zint , polar_angle = polar_angle , * * kwargs ) #2. Hdet if wrap : #Lambda function that ignores its args but still returns correct values func = lambda * args : get_hsym_asym ( rho3 * kfki , z3 * kfki , zint = kfki * zint , get_hdet = True , * * kwargs ) [ 0 ] hdet = wrap_and_calc_psf ( xpts , ypts , z , func ) else : hdet , toss = get_hsym_asym ( rho3 * kfki , z3 * kfki , zint = kfki * zint , get_hdet = True , * * kwargs ) if normalize : hilm /= hilm . sum ( ) hdet /= hdet . sum ( ) for a in range ( x . size ) : hdet [ a ] *= hilm return hdet if normalize else hdet / hdet . sum ( )
4570	def load ( file , use_yaml = None ) : if isinstance ( file , str ) : fp = open ( file ) filename = file else : fp = file filename = getattr ( fp , 'name' , '' ) try : return loads ( fp . read ( ) , use_yaml , filename ) except Exception as e : e . args = ( 'There was a error in the data file' , filename ) + e . args raise
4448	def delete_document ( self , doc_id , conn = None ) : if conn is None : conn = self . redis return conn . execute_command ( self . DEL_CMD , self . index_name , doc_id )
9806	def deploy ( file , manager_path , check , dry_run ) : # pylint:disable=redefined-builtin config = read_deployment_config ( file ) manager = DeployManager ( config = config , filepath = file , manager_path = manager_path , dry_run = dry_run ) exception = None if check : manager . check ( ) Printer . print_success ( 'Polyaxon deployment file is valid.' ) else : try : manager . install ( ) except Exception as e : Printer . print_error ( 'Polyaxon could not be installed.' ) exception = e if exception : Printer . print_error ( 'Error message `{}`.' . format ( exception ) )
12778	def render ( self , dt ) : for frame in self . _frozen : for body in frame : self . draw_body ( body ) for body in self . world . bodies : self . draw_body ( body ) if hasattr ( self . world , 'markers' ) : # draw line between anchor1 and anchor2 for marker joints. window . glColor4f ( 0.9 , 0.1 , 0.1 , 0.9 ) window . glLineWidth ( 3 ) for j in self . world . markers . joints . values ( ) : window . glBegin ( window . GL_LINES ) window . glVertex3f ( * j . getAnchor ( ) ) window . glVertex3f ( * j . getAnchor2 ( ) ) window . glEnd ( )
8491	def _parse_hosts ( self , hosts ) : # Default host if hosts is None : return # If it's a string, we allow comma separated strings if isinstance ( hosts , six . string_types ) : # Split comma-separated list hosts = [ host . strip ( ) for host in hosts . split ( ',' ) ] # Split host and port hosts = [ host . split ( ':' ) for host in hosts ] # Coerce ports to int hosts = [ ( host [ 0 ] , int ( host [ 1 ] ) ) for host in hosts ] # The python-etcd client explicitly checks for a tuple type return tuple ( hosts )
8889	def _self_referential_fk ( klass_model ) : for f in klass_model . _meta . concrete_fields : if f . related_model : if issubclass ( klass_model , f . related_model ) : return f . attname return None
9723	async def save ( self , filename , overwrite = False ) : cmd = "save %s%s" % ( filename , " overwrite" if overwrite else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
11884	def scanAllProcessesForOpenFile ( searchPortion , isExactMatch = True , ignoreCase = False ) : pids = getAllRunningPids ( ) # Since processes could disappear, we run the scan as fast as possible here with a list comprehension, then assemble the return dictionary later. mappingResults = [ scanProcessForOpenFile ( pid , searchPortion , isExactMatch , ignoreCase ) for pid in pids ] ret = { } for i in range ( len ( pids ) ) : if mappingResults [ i ] is not None : ret [ pids [ i ] ] = mappingResults [ i ] return ret
6477	def render ( self , stream ) : encoding = self . option . encoding or self . term . encoding or "utf8" if self . option . color : ramp = self . color_ramp ( self . size . y ) [ : : - 1 ] else : ramp = None if self . cycle >= 1 and self . lines : stream . write ( self . term . csi ( 'cuu' , self . lines ) ) zero = int ( self . null / 4 ) # Zero crossing lines = 0 for y in range ( self . screen . size . y ) : if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) for x in range ( self . screen . size . x ) : point = Point ( ( x , y ) ) if point in self . screen : value = self . screen [ point ] if isinstance ( value , int ) : stream . write ( chr ( self . base + value ) . encode ( encoding ) ) else : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( self . term . csi_wrap ( value . encode ( encoding ) , 'bold' ) ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) else : stream . write ( b' ' ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'rmul' ) ) if ramp : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( b'\n' ) lines += 1 stream . flush ( ) self . cycle = self . cycle + 1 self . lines = lines
3542	def popen_streaming_output ( cmd , callback , timeout = None ) : if os . name == 'nt' : # pragma: no cover process = subprocess . Popen ( shlex . split ( cmd ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) stdout = process . stdout else : master , slave = os . openpty ( ) process = subprocess . Popen ( shlex . split ( cmd , posix = True ) , stdout = slave , stderr = slave ) stdout = os . fdopen ( master ) os . close ( slave ) def kill ( process_ ) : """Kill the specified process on Timer completion""" try : process_ . kill ( ) except OSError : pass # python 2-3 agnostic process timer timer = Timer ( timeout , kill , [ process ] ) timer . setDaemon ( True ) timer . start ( ) while process . returncode is None : try : if os . name == 'nt' : # pragma: no cover line = stdout . readline ( ) # windows gives readline() raw stdout as a b'' # need to decode it line = line . decode ( "utf-8" ) if line : # ignore empty strings and None callback ( line . rstrip ( ) ) else : while True : line = stdout . readline ( ) if not line : break callback ( line . rstrip ( ) ) except ( IOError , OSError ) : # This seems to happen on some platforms, including TravisCI. # It seems like it's ok to just let this pass here, you just # won't get as nice feedback. pass if not timer . is_alive ( ) : raise TimeoutError ( "subprocess running command '{}' timed out after {} seconds" . format ( cmd , timeout ) ) process . poll ( ) # we have returned from the subprocess cancel the timer if it is running timer . cancel ( ) return process . returncode
9083	def get_by_uri ( self , uri ) : if not is_uri ( uri ) : raise ValueError ( '%s is not a valid URI.' % uri ) # Check if there's a provider that's more likely to have the URI csuris = [ csuri for csuri in self . concept_scheme_uri_map . keys ( ) if uri . startswith ( csuri ) ] for csuri in csuris : c = self . get_provider ( csuri ) . get_by_uri ( uri ) if c : return c # Check all providers for p in self . providers . values ( ) : c = p . get_by_uri ( uri ) if c : return c return False
11012	def write ( context ) : config = context . obj title = click . prompt ( 'Title' ) author = click . prompt ( 'Author' , default = config . get ( 'DEFAULT_AUTHOR' ) ) slug = slugify ( title ) creation_date = datetime . now ( ) basename = '{:%Y-%m-%d}_{}.md' . format ( creation_date , slug ) meta = ( ( 'Title' , title ) , ( 'Date' , '{:%Y-%m-%d %H:%M}:00' . format ( creation_date ) ) , ( 'Modified' , '{:%Y-%m-%d %H:%M}:00' . format ( creation_date ) ) , ( 'Author' , author ) , ) file_content = '' for key , value in meta : file_content += '{}: {}\n' . format ( key , value ) file_content += '\n\n' file_content += 'Text...\n\n' file_content += '![image description]({filename}/images/my-photo.jpg)\n\n' file_content += 'Text...\n\n' os . makedirs ( config [ 'CONTENT_DIR' ] , exist_ok = True ) path = os . path . join ( config [ 'CONTENT_DIR' ] , basename ) with click . open_file ( path , 'w' ) as f : f . write ( file_content ) click . echo ( path ) click . launch ( path )
10093	def templates ( self , timeout = None ) : return self . _api_request ( self . TEMPLATES_ENDPOINT , self . HTTP_GET , timeout = timeout )
860	def getTemporalDelay ( inferenceElement , key = None ) : # ----------------------------------------------------------------------- # For next step prediction, we shift by 1 if inferenceElement in ( InferenceElement . prediction , InferenceElement . encodings ) : return 1 # ----------------------------------------------------------------------- # For classification, anomaly scores, the inferences immediately succeed the # inputs if inferenceElement in ( InferenceElement . anomalyScore , InferenceElement . anomalyLabel , InferenceElement . classification , InferenceElement . classConfidences ) : return 0 # ----------------------------------------------------------------------- # For multistep prediction, the delay is based on the key in the inference # dictionary if inferenceElement in ( InferenceElement . multiStepPredictions , InferenceElement . multiStepBestPredictions ) : return int ( key ) # ----------------------------------------------------------------------- # default: return 0 return 0
3656	def add_cti_file ( self , file_path : str ) : if not os . path . exists ( file_path ) : self . _logger . warning ( 'Attempted to add {0} which does not exist.' . format ( file_path ) ) if file_path not in self . _cti_files : self . _cti_files . append ( file_path ) self . _logger . info ( 'Added {0} to the CTI file list.' . format ( file_path ) )
8942	def upload ( self , docs_base , release ) : return getattr ( self , '_to_' + self . target ) ( docs_base , release )
6717	def has_virtualenv ( self ) : with self . settings ( warn_only = True ) : ret = self . run_or_local ( 'which virtualenv' ) . strip ( ) return bool ( ret )
11043	def listen ( self , reactor , endpoint_description ) : endpoint = serverFromString ( reactor , endpoint_description ) return endpoint . listen ( Site ( self . app . resource ( ) ) )
2425	def set_doc_name ( self , doc , name ) : if not self . doc_name_set : doc . name = name self . doc_name_set = True return True else : raise CardinalityError ( 'Document::Name' )
6518	def is_excluded ( self , path ) : relpath = path . relative_to ( self . base_path ) . as_posix ( ) return matches_masks ( relpath , self . excludes )
1434	def custom_serialized ( cls , serialized , is_java = True ) : if not isinstance ( serialized , bytes ) : raise TypeError ( "Argument to custom_serialized() must be " "a serialized Python class as bytes, given: %s" % str ( serialized ) ) if not is_java : return cls . CUSTOM ( gtype = topology_pb2 . Grouping . Value ( "CUSTOM" ) , python_serialized = serialized ) else : raise NotImplementedError ( "Custom grouping implemented in Java for Python topology" "is not yet supported." )
7573	def progressbar ( njobs , finished , msg = "" , spacer = " " ) : if njobs : progress = 100 * ( finished / float ( njobs ) ) else : progress = 100 hashes = '#' * int ( progress / 5. ) nohash = ' ' * int ( 20 - len ( hashes ) ) if not ipyrad . __interactive__ : msg = msg . rsplit ( "|" , 2 ) [ 0 ] args = [ spacer , hashes + nohash , int ( progress ) , msg ] print ( "\r{}[{}] {:>3}% {} " . format ( * args ) , end = "" ) sys . stdout . flush ( )
13602	def warn_message ( self , message , fh = None , prefix = "[warn]:" , suffix = "..." ) : msg = prefix + message + suffix fh = fh or sys . stdout if fh is sys . stdout : termcolor . cprint ( msg , color = "yellow" ) else : fh . write ( msg ) pass
5391	def _delocalize_logging_command ( self , logging_path , user_project ) : # Get the logging prefix (everything up to ".log") logging_prefix = os . path . splitext ( logging_path . uri ) [ 0 ] # Set the provider-specific mkdir and file copy commands if logging_path . file_provider == job_model . P_LOCAL : mkdir_cmd = 'mkdir -p "%s"\n' % os . path . dirname ( logging_prefix ) cp_cmd = 'cp' elif logging_path . file_provider == job_model . P_GCS : mkdir_cmd = '' if user_project : cp_cmd = 'gsutil -u {} -mq cp' . format ( user_project ) else : cp_cmd = 'gsutil -mq cp' else : assert False # Construct the copy command copy_logs_cmd = textwrap . dedent ( """\ local cp_cmd="{cp_cmd}" local prefix="{prefix}" """ ) . format ( cp_cmd = cp_cmd , prefix = logging_prefix ) # Build up the command body = textwrap . dedent ( """\ {mkdir_cmd} {copy_logs_cmd} """ ) . format ( mkdir_cmd = mkdir_cmd , copy_logs_cmd = copy_logs_cmd ) return body
12380	def put ( self , request , response ) : if self . slug is None : # Mass-PUT is not implemented. raise http . exceptions . NotImplemented ( ) # Check if the resource exists. target = self . read ( ) # Deserialize and clean the incoming object. data = self . _clean ( target , self . request . read ( deserialize = True ) ) if target is not None : # Ensure we're allowed to update the resource. self . assert_operations ( 'update' ) try : # Delegate to `update` to create the item. self . update ( target , data ) except AttributeError : # No read method defined. raise http . exceptions . NotImplemented ( ) # Build the response object. self . make_response ( target ) else : # Ensure we're allowed to create the resource. self . assert_operations ( 'create' ) # Delegate to `create` to create the item. target = self . create ( data ) # Build the response object. self . response . status = http . client . CREATED self . make_response ( target )
4397	def adsSyncAddDeviceNotificationReqEx ( port , adr , data_name , pNoteAttrib , callback , user_handle = None ) : # type: (int, AmsAddr, str, NotificationAttrib, Callable, int) -> Tuple[int, int] global callback_store if NOTEFUNC is None : raise TypeError ( "Callback function type can't be None" ) adsSyncAddDeviceNotificationReqFct = _adsDLL . AdsSyncAddDeviceNotificationReqEx pAmsAddr = ctypes . pointer ( adr . amsAddrStruct ( ) ) hnl = adsSyncReadWriteReqEx2 ( port , adr , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING ) nIndexGroup = ctypes . c_ulong ( ADSIGRP_SYM_VALBYHND ) nIndexOffset = ctypes . c_ulong ( hnl ) attrib = pNoteAttrib . notificationAttribStruct ( ) pNotification = ctypes . c_ulong ( ) nHUser = ctypes . c_ulong ( hnl ) if user_handle is not None : nHUser = ctypes . c_ulong ( user_handle ) adsSyncAddDeviceNotificationReqFct . argtypes = [ ctypes . c_ulong , ctypes . POINTER ( SAmsAddr ) , ctypes . c_ulong , ctypes . c_ulong , ctypes . POINTER ( SAdsNotificationAttrib ) , NOTEFUNC , ctypes . c_ulong , ctypes . POINTER ( ctypes . c_ulong ) , ] adsSyncAddDeviceNotificationReqFct . restype = ctypes . c_long def wrapper ( addr , notification , user ) : # type: (AmsAddr, SAdsNotificationHeader, int) -> Callable[[SAdsNotificationHeader, str], None] return callback ( notification , data_name ) c_callback = NOTEFUNC ( wrapper ) err_code = adsSyncAddDeviceNotificationReqFct ( port , pAmsAddr , nIndexGroup , nIndexOffset , ctypes . byref ( attrib ) , c_callback , nHUser , ctypes . byref ( pNotification ) , ) if err_code : raise ADSError ( err_code ) callback_store [ pNotification . value ] = c_callback return ( pNotification . value , hnl )
5112	def animate ( self , out = None , t = None , line_kwargs = None , scatter_kwargs = None , * * kwargs ) : if not self . _initialized : msg = ( "Network has not been initialized. " "Call '.initialize()' first." ) raise QueueingToolError ( msg ) if not HAS_MATPLOTLIB : msg = "Matplotlib is necessary to animate a simulation." raise ImportError ( msg ) self . _update_all_colors ( ) kwargs . setdefault ( 'bgcolor' , self . colors [ 'bgcolor' ] ) fig = plt . figure ( figsize = kwargs . get ( 'figsize' , ( 7 , 7 ) ) ) ax = fig . gca ( ) mpl_kwargs = { 'line_kwargs' : line_kwargs , 'scatter_kwargs' : scatter_kwargs , 'pos' : kwargs . get ( 'pos' ) } line_args , scat_args = self . g . lines_scatter_args ( * * mpl_kwargs ) lines = LineCollection ( * * line_args ) lines = ax . add_collection ( lines ) scatt = ax . scatter ( * * scat_args ) t = np . infty if t is None else t now = self . _t def update ( frame_number ) : if t is not None : if self . _t > now + t : return False self . _simulate_next_event ( slow = True ) lines . set_color ( line_args [ 'colors' ] ) scatt . set_edgecolors ( scat_args [ 'edgecolors' ] ) scatt . set_facecolor ( scat_args [ 'c' ] ) if hasattr ( ax , 'set_facecolor' ) : ax . set_facecolor ( kwargs [ 'bgcolor' ] ) else : ax . set_axis_bgcolor ( kwargs [ 'bgcolor' ] ) ax . get_xaxis ( ) . set_visible ( False ) ax . get_yaxis ( ) . set_visible ( False ) animation_args = { 'fargs' : None , 'event_source' : None , 'init_func' : None , 'frames' : None , 'blit' : False , 'interval' : 10 , 'repeat' : None , 'func' : update , 'repeat_delay' : None , 'fig' : fig , 'save_count' : None , } for key , value in kwargs . items ( ) : if key in animation_args : animation_args [ key ] = value animation = FuncAnimation ( * * animation_args ) if 'filename' not in kwargs : plt . ioff ( ) plt . show ( ) else : save_args = { 'filename' : None , 'writer' : None , 'fps' : None , 'dpi' : None , 'codec' : None , 'bitrate' : None , 'extra_args' : None , 'metadata' : None , 'extra_anim' : None , 'savefig_kwargs' : None } for key , value in kwargs . items ( ) : if key in save_args : save_args [ key ] = value animation . save ( * * save_args )
7087	def get_epochs_given_midtimes_and_period ( t_mid , period , err_t_mid = None , t0_fixed = None , t0_percentile = None , verbose = False ) : kwargarr = np . array ( [ isinstance ( err_t_mid , np . ndarray ) , t0_fixed , t0_percentile ] ) if not _single_true ( kwargarr ) and not np . all ( ~ kwargarr . astype ( bool ) ) : raise AssertionError ( 'can have at most one of err_t_mid, t0_fixed, t0_percentile' ) t_mid = t_mid [ np . isfinite ( t_mid ) ] N_midtimes = len ( t_mid ) if t0_fixed : t0 = t0_fixed elif isinstance ( err_t_mid , np . ndarray ) : # get the weighted average. then round it to the nearest transit epoch. t0_avg = np . average ( t_mid , weights = 1 / err_t_mid ** 2 ) t0_options = np . arange ( min ( t_mid ) , max ( t_mid ) + period , period ) t0 = t0_options [ np . argmin ( np . abs ( t0_options - t0_avg ) ) ] else : if not t0_percentile : # if there are an odd number of times, take the median time as # epoch=0. elif there are an even number of times, take the lower # of the two middle times as epoch=0. if N_midtimes % 2 == 1 : t0 = np . median ( t_mid ) else : t0 = t_mid [ int ( N_midtimes / 2 ) ] else : t0 = np . sort ( t_mid ) [ int ( N_midtimes * t0_percentile / 100 ) ] epoch = ( t_mid - t0 ) / period # do not convert numpy entries to actual ints, because np.nan is float type int_epoch = np . round ( epoch , 0 ) if verbose : LOGINFO ( 'epochs before rounding' ) LOGINFO ( '\n{:s}' . format ( repr ( epoch ) ) ) LOGINFO ( 'epochs after rounding' ) LOGINFO ( '\n{:s}' . format ( repr ( int_epoch ) ) ) return int_epoch , t0
5466	def get_action_environment ( op , name ) : action = _get_action_by_name ( op , name ) if action : return action . get ( 'environment' )
5816	def _read_remaining ( socket ) : output = b'' old_timeout = socket . gettimeout ( ) try : socket . settimeout ( 0.0 ) output += socket . recv ( 8192 ) except ( socket_ . error ) : pass finally : socket . settimeout ( old_timeout ) return output
9229	def fetch_closed_pull_requests ( self ) : pull_requests = [ ] verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project if verbose : print ( "Fetching closed pull requests..." ) page = 1 while page > 0 : if verbose > 2 : print ( "." , end = "" ) if self . options . release_branch : rc , data = gh . repos [ user ] [ repo ] . pulls . get ( page = page , per_page = PER_PAGE_NUMBER , state = 'closed' , base = self . options . release_branch ) else : rc , data = gh . repos [ user ] [ repo ] . pulls . get ( page = page , per_page = PER_PAGE_NUMBER , state = 'closed' , ) if rc == 200 : pull_requests . extend ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) if verbose > 2 : print ( "." ) if verbose > 1 : print ( "\tfetched {} closed pull requests." . format ( len ( pull_requests ) ) ) return pull_requests
11190	def show ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) click . secho ( readme_content )
1593	def prepare ( self , context ) : for stream_id , targets in self . targets . items ( ) : for target in targets : target . prepare ( context , stream_id )
3302	def _get_checked_path ( path , config , must_exist = True , allow_none = True ) : if path in ( None , "" ) : if allow_none : return None raise ValueError ( "Invalid path {!r}" . format ( path ) ) # Evaluate path relative to the folder of the config file (if any) config_file = config . get ( "_config_file" ) if config_file and not os . path . isabs ( path ) : path = os . path . normpath ( os . path . join ( os . path . dirname ( config_file ) , path ) ) else : path = os . path . abspath ( path ) if must_exist and not os . path . exists ( path ) : raise ValueError ( "Invalid path {!r}" . format ( path ) ) return path
12459	def main ( * args ) : # Create parser, read arguments from direct input or command line with disable_error_handler ( ) : args = parse_args ( args or sys . argv [ 1 : ] ) # Read current config from file and command line arguments config = read_config ( args . config , args ) if config is None : return True bootstrap = config [ __script__ ] # Check pre-requirements if not check_pre_requirements ( bootstrap [ 'pre_requirements' ] ) : return True # Create virtual environment env_args = prepare_args ( config [ 'virtualenv' ] , bootstrap ) if not create_env ( bootstrap [ 'env' ] , env_args , bootstrap [ 'recreate' ] , bootstrap [ 'ignore_activated' ] , bootstrap [ 'quiet' ] ) : # Exit if couldn't create virtual environment return True # And install library or project here pip_args = prepare_args ( config [ 'pip' ] , bootstrap ) if not install ( bootstrap [ 'env' ] , bootstrap [ 'requirements' ] , pip_args , bootstrap [ 'ignore_activated' ] , bootstrap [ 'install_dev_requirements' ] , bootstrap [ 'quiet' ] ) : # Exist if couldn't install requirements into venv return True # Run post-bootstrap hook run_hook ( bootstrap [ 'hook' ] , bootstrap , bootstrap [ 'quiet' ] ) # All OK! if not bootstrap [ 'quiet' ] : print_message ( 'All OK!' ) # False means everything went alright, exit code: 0 return False
4522	def run ( function , * args , use_subprocess = False , daemon = True , * * kwds ) : if use_subprocess : Creator , Queue = multiprocessing . Process , multiprocessing . Queue else : Creator , Queue = threading . Thread , queue . Queue input , output = Queue ( ) , Queue ( ) args = input , output , function , args sub = Creator ( target = _run_locally , args = args , kwargs = kwds , daemon = daemon ) sub . start ( ) return sub , input , output
1458	def _get_deps_list ( abs_path_to_pex ) : pex = zipfile . ZipFile ( abs_path_to_pex , mode = 'r' ) deps = list ( set ( [ re . match ( egg_regex , i ) . group ( 1 ) for i in pex . namelist ( ) if re . match ( egg_regex , i ) is not None ] ) ) return deps
7446	def _step5func ( self , samples , force , ipyclient ) : ## print header if self . _headers : print ( "\n Step 5: Consensus base calling " ) ## Get sample objects from list of strings samples = _get_samples ( self , samples ) ## Check if all/none in the right state if not self . _samples_precheck ( samples , 5 , force ) : raise IPyradError ( FIRST_RUN_4 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 5 for i in samples ] ) : print ( CONSENS_EXIST . format ( len ( samples ) ) ) return ## pass samples to rawedit assemble . consens_se . run ( self , samples , force , ipyclient )
11161	def autopep8 ( self , * * kwargs ) : # pragma: no cover self . assert_is_dir_and_exists ( ) for p in self . select_by_ext ( ".py" ) : with open ( p . abspath , "rb" ) as f : code = f . read ( ) . decode ( "utf-8" ) formatted_code = autopep8 . fix_code ( code , * * kwargs ) with open ( p . abspath , "wb" ) as f : f . write ( formatted_code . encode ( "utf-8" ) )
2846	def close ( self ) : if self . _ctx is not None : ftdi . free ( self . _ctx ) self . _ctx = None
8189	def eigenvector_centrality ( self , normalized = True , reversed = True , rating = { } , start = None , iterations = 100 , tolerance = 0.0001 ) : ec = proximity . eigenvector_centrality ( self , normalized , reversed , rating , start , iterations , tolerance ) for id , w in ec . iteritems ( ) : self [ id ] . _eigenvalue = w return ec
13836	def _MergeMessageField ( self , tokenizer , message , field ) : is_map_entry = _IsMapEntry ( field ) if tokenizer . TryConsume ( '<' ) : end_token = '>' else : tokenizer . Consume ( '{' ) end_token = '}' if field . label == descriptor . FieldDescriptor . LABEL_REPEATED : if field . is_extension : sub_message = message . Extensions [ field ] . add ( ) elif is_map_entry : # pylint: disable=protected-access sub_message = field . message_type . _concrete_class ( ) else : sub_message = getattr ( message , field . name ) . add ( ) else : if field . is_extension : sub_message = message . Extensions [ field ] else : sub_message = getattr ( message , field . name ) sub_message . SetInParent ( ) while not tokenizer . TryConsume ( end_token ) : if tokenizer . AtEnd ( ) : raise tokenizer . ParseErrorPreviousToken ( 'Expected "%s".' % ( end_token , ) ) self . _MergeField ( tokenizer , sub_message ) if is_map_entry : value_cpptype = field . message_type . fields_by_name [ 'value' ] . cpp_type if value_cpptype == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : value = getattr ( message , field . name ) [ sub_message . key ] value . MergeFrom ( sub_message . value ) else : getattr ( message , field . name ) [ sub_message . key ] = sub_message . value
8256	def _darkest ( self ) : min , n = ( 1.0 , 1.0 , 1.0 ) , 3.0 for clr in self : if clr . r + clr . g + clr . b < n : min , n = clr , clr . r + clr . g + clr . b return min
5320	def find_ports ( device ) : bus_id = device . bus dev_id = device . address for dirent in os . listdir ( USB_SYS_PREFIX ) : matches = re . match ( USB_PORTS_STR + '$' , dirent ) if matches : bus_str = readattr ( dirent , 'busnum' ) if bus_str : busnum = float ( bus_str ) else : busnum = None dev_str = readattr ( dirent , 'devnum' ) if dev_str : devnum = float ( dev_str ) else : devnum = None if busnum == bus_id and devnum == dev_id : return str ( matches . groups ( ) [ 1 ] )
2188	def _get_certificate ( self , cfgstr = None ) : certificate = self . cacher . tryload ( cfgstr = cfgstr ) return certificate
4339	def pitch ( self , n_semitones , quick = False ) : if not is_number ( n_semitones ) : raise ValueError ( "n_semitones must be a positive number" ) if n_semitones < - 12 or n_semitones > 12 : logger . warning ( "Using an extreme pitch shift. " "Quality of results will be poor" ) if not isinstance ( quick , bool ) : raise ValueError ( "quick must be a boolean." ) effect_args = [ 'pitch' ] if quick : effect_args . append ( '-q' ) effect_args . append ( '{:f}' . format ( n_semitones * 100. ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'pitch' ) return self
2832	def get_platform_pwm ( * * keywords ) : plat = Platform . platform_detect ( ) if plat == Platform . RASPBERRY_PI : import RPi . GPIO return RPi_PWM_Adapter ( RPi . GPIO , * * keywords ) elif plat == Platform . BEAGLEBONE_BLACK : import Adafruit_BBIO . PWM return BBIO_PWM_Adapter ( Adafruit_BBIO . PWM , * * keywords ) elif plat == Platform . UNKNOWN : raise RuntimeError ( 'Could not determine platform.' )
8859	def calltips ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] # encoding = request_data['encoding'] encoding = 'utf-8' # use jedi to get call signatures script = jedi . Script ( code , line , column , path , encoding ) signatures = script . call_signatures ( ) for sig in signatures : results = ( str ( sig . module_name ) , str ( sig . name ) , [ p . description for p in sig . params ] , sig . index , sig . bracket_start , column ) # todo: add support for multiple signatures, for that we need a custom # widget for showing calltips. return results return [ ]
766	def clean ( s ) : lines = [ l . rstrip ( ) for l in s . split ( '\n' ) ] return '\n' . join ( lines )
392	def keypoint_random_resize ( image , annos , mask = None , zoom_range = ( 0.8 , 1.2 ) ) : height = image . shape [ 0 ] width = image . shape [ 1 ] _min , _max = zoom_range scalew = np . random . uniform ( _min , _max ) scaleh = np . random . uniform ( _min , _max ) neww = int ( width * scalew ) newh = int ( height * scaleh ) dst = cv2 . resize ( image , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) if mask is not None : mask = cv2 . resize ( mask , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) # adjust meta data adjust_joint_list = [ ] for joint in annos : # TODO : speed up with affine transform adjust_joint = [ ] for point in joint : if point [ 0 ] < - 100 or point [ 1 ] < - 100 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue adjust_joint . append ( ( int ( point [ 0 ] * scalew + 0.5 ) , int ( point [ 1 ] * scaleh + 0.5 ) ) ) adjust_joint_list . append ( adjust_joint ) if mask is not None : return dst , adjust_joint_list , mask else : return dst , adjust_joint_list , None
9377	def calculate_stats ( data_list , stats_to_calculate = [ 'mean' , 'std' ] , percentiles_to_calculate = [ ] ) : stats_to_numpy_method_map = { 'mean' : numpy . mean , 'avg' : numpy . mean , 'std' : numpy . std , 'standard_deviation' : numpy . std , 'median' : numpy . median , 'min' : numpy . amin , 'max' : numpy . amax } calculated_stats = { } calculated_percentiles = { } if len ( data_list ) == 0 : return calculated_stats , calculated_percentiles for stat in stats_to_calculate : if stat in stats_to_numpy_method_map . keys ( ) : calculated_stats [ stat ] = stats_to_numpy_method_map [ stat ] ( data_list ) else : logger . error ( "Unsupported stat : " + str ( stat ) ) for percentile in percentiles_to_calculate : if isinstance ( percentile , float ) or isinstance ( percentile , int ) : calculated_percentiles [ percentile ] = numpy . percentile ( data_list , percentile ) else : logger . error ( "Unsupported percentile requested (should be int or float): " + str ( percentile ) ) return calculated_stats , calculated_percentiles
3797	def setup_a_alpha_and_derivatives ( self , i , T = None ) : self . a , self . Tc , self . omega = self . ais [ i ] , self . Tcs [ i ] , self . omegas [ i ]
8153	def simple_traceback ( ex , source ) : exc_type , exc_value , exc_tb = sys . exc_info ( ) exc = traceback . format_exception ( exc_type , exc_value , exc_tb ) source_arr = source . splitlines ( ) # Defaults... exc_location = exc [ - 2 ] for i , err in enumerate ( exc ) : if 'exec source in ns' in err : exc_location = exc [ i + 1 ] break # extract line number from traceback fn = exc_location . split ( ',' ) [ 0 ] [ 8 : - 1 ] line_number = int ( exc_location . split ( ',' ) [ 1 ] . replace ( 'line' , '' ) . strip ( ) ) # Build error messages err_msgs = [ ] # code around the error err_where = ' ' . join ( exc [ i - 1 ] . split ( ',' ) [ 1 : ] ) . strip ( ) # 'line 37 in blah" err_msgs . append ( 'Error in the Shoebot script at %s:' % err_where ) for i in xrange ( max ( 0 , line_number - 5 ) , line_number ) : if fn == "<string>" : line = source_arr [ i ] else : line = linecache . getline ( fn , i + 1 ) err_msgs . append ( '%s: %s' % ( i + 1 , line . rstrip ( ) ) ) err_msgs . append ( ' %s^ %s' % ( len ( str ( i ) ) * ' ' , exc [ - 1 ] . rstrip ( ) ) ) err_msgs . append ( '' ) # traceback err_msgs . append ( exc [ 0 ] . rstrip ( ) ) for err in exc [ 3 : ] : err_msgs . append ( err . rstrip ( ) ) return '\n' . join ( err_msgs )
9547	def add_record_predicate ( self , record_predicate , code = RECORD_PREDICATE_FALSE , message = MESSAGES [ RECORD_PREDICATE_FALSE ] , modulus = 1 ) : assert callable ( record_predicate ) , 'record predicate must be a callable function' t = record_predicate , code , message , modulus self . _record_predicates . append ( t )
7323	def sendmail ( message , sender , recipients , config_filename ) : # Read config file from disk to get SMTP server host, port, username if not hasattr ( sendmail , "host" ) : config = configparser . RawConfigParser ( ) config . read ( config_filename ) sendmail . host = config . get ( "smtp_server" , "host" ) sendmail . port = config . getint ( "smtp_server" , "port" ) sendmail . username = config . get ( "smtp_server" , "username" ) sendmail . security = config . get ( "smtp_server" , "security" ) print ( ">>> Read SMTP server configuration from {}" . format ( config_filename ) ) print ( ">>> host = {}" . format ( sendmail . host ) ) print ( ">>> port = {}" . format ( sendmail . port ) ) print ( ">>> username = {}" . format ( sendmail . username ) ) print ( ">>> security = {}" . format ( sendmail . security ) ) # Prompt for password if not hasattr ( sendmail , "password" ) : if sendmail . security == "Dummy" or sendmail . username == "None" : sendmail . password = None else : prompt = ">>> password for {} on {}: " . format ( sendmail . username , sendmail . host ) sendmail . password = getpass . getpass ( prompt ) # Connect to SMTP server if sendmail . security == "SSL/TLS" : smtp = smtplib . SMTP_SSL ( sendmail . host , sendmail . port ) elif sendmail . security == "STARTTLS" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) smtp . ehlo ( ) smtp . starttls ( ) smtp . ehlo ( ) elif sendmail . security == "Never" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) elif sendmail . security == "Dummy" : smtp = smtp_dummy . SMTP_dummy ( ) else : raise configparser . Error ( "Unrecognized security type: {}" . format ( sendmail . security ) ) # Send credentials if sendmail . username != "None" : smtp . login ( sendmail . username , sendmail . password ) # Send message. Note that we can't use the elegant # "smtp.send_message(message)" because that's python3 only smtp . sendmail ( sender , recipients , message . as_string ( ) ) smtp . close ( )
7049	def massradius ( age , planetdist , coremass , mass = 'massjupiter' , radius = 'radiusjupiter' ) : MR = { 0.3 : MASSESRADII_0_3GYR , 1.0 : MASSESRADII_1_0GYR , 4.5 : MASSESRADII_4_5GYR } if age not in MR : print ( 'given age not in Fortney 2007, returning...' ) return massradius = MR [ age ] if ( planetdist in massradius ) and ( coremass in massradius [ planetdist ] ) : print ( 'getting % Gyr M-R for planet dist %s AU, ' 'core mass %s Mearth...' % ( age , planetdist , coremass ) ) massradrelation = massradius [ planetdist ] [ coremass ] outdict = { 'mass' : array ( massradrelation [ mass ] ) , 'radius' : array ( massradrelation [ radius ] ) } return outdict
1659	def CheckCasts ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] # Check to see if they're using an conversion function cast. # I just try to capture the most common basic types, though there are more. # Parameterless conversion functions, such as bool(), are allowed as they are # probably a member operator declaration or default constructor. match = Search ( r'(\bnew\s+(?:const\s+)?|\S<\s*(?:const\s+)?)?\b' r'(int|float|double|bool|char|int32|uint32|int64|uint64)' r'(\([^)].*)' , line ) expecting_function = ExpectingFunctionArgs ( clean_lines , linenum ) if match and not expecting_function : matched_type = match . group ( 2 ) # matched_new_or_template is used to silence two false positives: # - New operators # - Template arguments with function types # # For template arguments, we match on types immediately following # an opening bracket without any spaces. This is a fast way to # silence the common case where the function type is the first # template argument. False negative with less-than comparison is # avoided because those operators are usually followed by a space. # # function<double(double)> // bracket + no space = false positive # value < double(42) // bracket + space = true positive matched_new_or_template = match . group ( 1 ) # Avoid arrays by looking for brackets that come after the closing # parenthesis. if Match ( r'\([^()]+\)\s*\[' , match . group ( 3 ) ) : return # Other things to ignore: # - Function pointers # - Casts to pointer types # - Placement new # - Alias declarations matched_funcptr = match . group ( 3 ) if ( matched_new_or_template is None and not ( matched_funcptr and ( Match ( r'\((?:[^() ]+::\s*\*\s*)?[^() ]+\)\s*\(' , matched_funcptr ) or matched_funcptr . startswith ( '(*)' ) ) ) and not Match ( r'\s*using\s+\S+\s*=\s*' + matched_type , line ) and not Search ( r'new\(\S+\)\s*' + matched_type , line ) ) : error ( filename , linenum , 'readability/casting' , 4 , 'Using deprecated casting style. ' 'Use static_cast<%s>(...) instead' % matched_type ) if not expecting_function : CheckCStyleCast ( filename , clean_lines , linenum , 'static_cast' , r'\((int|float|double|bool|char|u?int(16|32|64))\)' , error ) # This doesn't catch all cases. Consider (const char * const)"hello". # # (char *) "foo" should always be a const_cast (reinterpret_cast won't # compile). if CheckCStyleCast ( filename , clean_lines , linenum , 'const_cast' , r'\((char\s?\*+\s?)\)\s*"' , error ) : pass else : # Check pointer casts for other than string constants CheckCStyleCast ( filename , clean_lines , linenum , 'reinterpret_cast' , r'\((\w+\s?\*+\s?)\)' , error ) # In addition, we look for people taking the address of a cast. This # is dangerous -- casts can assign to temporaries, so the pointer doesn't # point where you think. # # Some non-identifier character is required before the '&' for the # expression to be recognized as a cast. These are casts: # expression = &static_cast<int*>(temporary()); # function(&(int*)(temporary())); # # This is not a cast: # reference_type&(int* function_param); match = Search ( r'(?:[^\w]&\(([^)*][^)]*)\)[\w(])|' r'(?:[^\w]&(static|dynamic|down|reinterpret)_cast\b)' , line ) if match : # Try a better error message when the & is bound to something # dereferenced by the casted pointer, as opposed to the casted # pointer itself. parenthesis_error = False match = Match ( r'^(.*&(?:static|dynamic|down|reinterpret)_cast\b)<' , line ) if match : _ , y1 , x1 = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) if x1 >= 0 and clean_lines . elided [ y1 ] [ x1 ] == '(' : _ , y2 , x2 = CloseExpression ( clean_lines , y1 , x1 ) if x2 >= 0 : extended_line = clean_lines . elided [ y2 ] [ x2 : ] if y2 < clean_lines . NumLines ( ) - 1 : extended_line += clean_lines . elided [ y2 + 1 ] if Match ( r'\s*(?:->|\[)' , extended_line ) : parenthesis_error = True if parenthesis_error : error ( filename , linenum , 'readability/casting' , 4 , ( 'Are you taking an address of something dereferenced ' 'from a cast? Wrapping the dereferenced expression in ' 'parentheses will make the binding more obvious' ) ) else : error ( filename , linenum , 'runtime/casting' , 4 , ( 'Are you taking an address of a cast? ' 'This is dangerous: could be a temp var. ' 'Take the address before doing the cast, rather than after' ) )
10541	def get_tasks ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'task' , params = params ) if type ( res ) . __name__ == 'list' : return [ Task ( task ) for task in res ] else : return res except : # pragma: no cover raise
9759	def statuses ( ctx , job , page ) : def get_experiment_statuses ( ) : try : response = PolyaxonClient ( ) . experiment . get_statuses ( user , project_name , _experiment , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could get status for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for experiment `{}`.' . format ( _experiment ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for experiment `{}`.' . format ( _experiment ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'experiment' , None ) dict_tabulate ( objects , is_list_dict = True ) def get_experiment_job_statuses ( ) : try : response = PolyaxonClient ( ) . experiment_job . get_statuses ( user , project_name , _experiment , _job , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get status for job `{}`.' . format ( job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for Job `{}`.' . format ( _job ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for job `{}`.' . format ( _job ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'job' , None ) dict_tabulate ( objects , is_list_dict = True ) page = page or 1 user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_statuses ( ) else : get_experiment_statuses ( )
516	def _avgConnectedSpanForColumn2D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 2 ) connected = self . _connectedSynapses [ columnIndex ] ( rows , cols ) = connected . reshape ( self . _inputDimensions ) . nonzero ( ) if rows . size == 0 and cols . size == 0 : return 0 rowSpan = rows . max ( ) - rows . min ( ) + 1 colSpan = cols . max ( ) - cols . min ( ) + 1 return numpy . average ( [ rowSpan , colSpan ] )
2307	def reset_parameters ( self ) : stdv = 1. / math . sqrt ( self . weight . size ( 1 ) ) self . weight . data . uniform_ ( - stdv , stdv ) if self . bias is not None : self . bias . data . uniform_ ( - stdv , stdv )
8222	def do_toggle_variables ( self , action ) : self . show_vars = action . get_active ( ) if self . show_vars : self . show_variables_window ( ) else : self . hide_variables_window ( )
13393	def notify_client ( notifier_uri , client_id , status_code , message = None ) : payload = { "client_id" : client_id , "result" : { "response" : { "status_code" : status_code } } } if message is not None : payload [ "result" ] [ "response" ] [ "message" ] = message response = requests . post ( notifier_uri , json = payload ) if response . status_code != 201 : sys . stderr . write ( "failed to notify client: {}\n" . format ( payload ) ) sys . stderr . flush ( )
2137	def create ( self , fail_on_found = False , force_on_exists = False , * * kwargs ) : if kwargs . get ( 'parent' , None ) : parent_data = self . set_child_endpoint ( parent = kwargs [ 'parent' ] , inventory = kwargs . get ( 'inventory' , None ) ) kwargs [ 'inventory' ] = parent_data [ 'inventory' ] elif 'inventory' not in kwargs : raise exc . UsageError ( 'To create a group, you must provide a parent inventory or parent group.' ) return super ( Resource , self ) . create ( fail_on_found = fail_on_found , force_on_exists = force_on_exists , * * kwargs )
12044	def where_cross ( data , threshold ) : Is = np . where ( data > threshold ) [ 0 ] Is = np . concatenate ( ( [ 0 ] , Is ) ) Ds = Is [ : - 1 ] - Is [ 1 : ] + 1 return Is [ np . where ( Ds ) [ 0 ] + 1 ]
10776	def finalize ( self , result = None ) : if not self . settings_path : # short circuit if no settings file can be found return from django . test . utils import teardown_test_environment from django . db import connection from django . conf import settings self . call_plugins_method ( 'beforeDestroyTestDb' , settings , connection ) try : connection . creation . destroy_test_db ( self . old_db , verbosity = self . verbosity , ) except Exception : # If we can't tear down the test DB, don't worry about it. pass self . call_plugins_method ( 'afterDestroyTestDb' , settings , connection ) self . call_plugins_method ( 'beforeTeardownTestEnv' , settings , teardown_test_environment ) teardown_test_environment ( ) self . call_plugins_method ( 'afterTeardownTestEnv' , settings )
5296	def get_end_date ( self , obj ) : obj_date = getattr ( obj , self . get_end_date_field ( ) ) try : obj_date = obj_date . date ( ) except AttributeError : # It's a date rather than datetime, so we use it as is pass return obj_date
2475	def set_lic_name ( self , doc , name ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_name_set : self . extr_lic_name_set = True if validations . validate_extr_lic_name ( name ) : self . extr_lic ( doc ) . full_name = name return True else : raise SPDXValueError ( 'ExtractedLicense::Name' ) else : raise CardinalityError ( 'ExtractedLicense::Name' ) else : raise OrderError ( 'ExtractedLicense::Name' )
4059	def _citation_processor ( self , retrieved ) : items = [ ] for cit in retrieved . entries : items . append ( cit [ "content" ] [ 0 ] [ "value" ] ) self . url_params = None return items
4289	def generate_media_pages ( gallery ) : writer = PageWriter ( gallery . settings , index_title = gallery . title ) for album in gallery . albums . values ( ) : medias = album . medias next_medias = medias [ 1 : ] + [ None ] previous_medias = [ None ] + medias [ : - 1 ] # The media group allows us to easily get next and previous links media_groups = zip ( medias , next_medias , previous_medias ) for media_group in media_groups : writer . write ( album , media_group )
9144	def drop ( connection , skip ) : for idx , name , manager in _iterate_managers ( connection , skip ) : click . secho ( f'dropping {name}' , fg = 'cyan' , bold = True ) manager . drop_all ( )
7713	def handle_roster_push ( self , stanza ) : if self . server is None and stanza . from_jid : logger . debug ( u"Server address not known, cannot verify roster push" " from {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) if self . server and stanza . from_jid and stanza . from_jid != self . server : logger . debug ( u"Roster push from invalid source: {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) payload = stanza . get_payload ( RosterPayload ) if len ( payload ) != 1 : logger . warning ( "Bad roster push received ({0} items)" . format ( len ( payload ) ) ) return stanza . make_error_response ( u"bad-request" ) if self . roster is None : logger . debug ( "Dropping roster push - no roster here" ) return True item = payload [ 0 ] item . verify_roster_push ( True ) old_item = self . roster . get ( item . jid ) if item . subscription == "remove" : if old_item : self . roster . remove_item ( item . jid ) else : self . roster . add_item ( item , replace = True ) self . _event_queue . put ( RosterUpdatedEvent ( self , old_item , item ) ) return stanza . make_result_response ( )
9411	def _encode ( data , convert_to_float ) : ctf = convert_to_float # Handle variable pointer. if isinstance ( data , ( OctaveVariablePtr ) ) : return _encode ( data . value , ctf ) # Handle a user defined object. if isinstance ( data , OctaveUserClass ) : return _encode ( OctaveUserClass . to_value ( data ) , ctf ) # Handle a function pointer. if isinstance ( data , ( OctaveFunctionPtr , MatlabFunction ) ) : raise Oct2PyError ( 'Cannot write Octave functions' ) # Handle matlab objects. if isinstance ( data , MatlabObject ) : view = data . view ( np . ndarray ) out = MatlabObject ( data , data . classname ) for name in out . dtype . names : out [ name ] = _encode ( view [ name ] , ctf ) return out # Handle pandas series and dataframes if isinstance ( data , ( DataFrame , Series ) ) : return _encode ( data . values , ctf ) # Extract and encode values from dict-like objects. if isinstance ( data , dict ) : out = dict ( ) for ( key , value ) in data . items ( ) : out [ key ] = _encode ( value , ctf ) return out # Send None as nan. if data is None : return np . NaN # Sets are treated like lists. if isinstance ( data , set ) : return _encode ( list ( data ) , ctf ) # Lists can be interpreted as numeric arrays or cell arrays. if isinstance ( data , list ) : if _is_simple_numeric ( data ) : return _encode ( np . array ( data ) , ctf ) return _encode ( tuple ( data ) , ctf ) # Tuples are handled as cells. if isinstance ( data , tuple ) : obj = np . empty ( len ( data ) , dtype = object ) for ( i , item ) in enumerate ( data ) : obj [ i ] = _encode ( item , ctf ) return obj # Sparse data must be floating type. if isinstance ( data , spmatrix ) : return data . astype ( np . float64 ) # Return other data types unchanged. if not isinstance ( data , np . ndarray ) : return data # Extract and encode data from object-like arrays. if data . dtype . kind in 'OV' : out = np . empty ( data . size , dtype = data . dtype ) for ( i , item ) in enumerate ( data . ravel ( ) ) : if data . dtype . names : for name in data . dtype . names : out [ i ] [ name ] = _encode ( item [ name ] , ctf ) else : out [ i ] = _encode ( item , ctf ) return out . reshape ( data . shape ) # Complex 128 is the highest supported by savemat. if data . dtype . name == 'complex256' : return data . astype ( np . complex128 ) # Convert to float if applicable. if ctf and data . dtype . kind in 'ui' : return data . astype ( np . float64 ) # Return standard array. return data
11116	def create_package ( self , path = None , name = None , mode = None ) : # check mode assert mode in ( None , 'w' , 'w:' , 'w:gz' , 'w:bz2' ) , 'unkown archive mode %s' % str ( mode ) if mode is None : mode = 'w:bz2' mode = 'w:' # get root if path is None : root = os . path . split ( self . __path ) [ 0 ] elif path . strip ( ) in ( '' , '.' ) : root = os . getcwd ( ) else : root = os . path . realpath ( os . path . expanduser ( path ) ) assert os . path . isdir ( root ) , 'absolute path %s is not a valid directory' % path # get name if name is None : ext = mode . split ( ":" ) if len ( ext ) == 2 : if len ( ext [ 1 ] ) : ext = "." + ext [ 1 ] else : ext = '.tar' else : ext = '.tar' name = os . path . split ( self . __path ) [ 1 ] + ext # save repository self . save ( ) # create tar file tarfilePath = os . path . join ( root , name ) try : tarHandler = tarfile . TarFile . open ( tarfilePath , mode = mode ) except Exception as e : raise Exception ( "Unable to create package (%s)" % e ) # walk directory and create empty directories for directory in sorted ( list ( self . walk_directories_relative_path ( ) ) ) : t = tarfile . TarInfo ( directory ) t . type = tarfile . DIRTYPE tarHandler . addfile ( t ) # walk files and add to tar for file in self . walk_files_relative_path ( ) : tarHandler . add ( os . path . join ( self . __path , file ) , arcname = file ) # save repository .pyrepinfo tarHandler . add ( os . path . join ( self . __path , ".pyrepinfo" ) , arcname = ".pyrepinfo" ) # close tar file tarHandler . close ( )
10880	def listify ( a ) : if a is None : return [ ] elif not isinstance ( a , ( tuple , list , np . ndarray ) ) : return [ a ] return list ( a )
8800	def run_migrations_offline ( ) : context . configure ( url = neutron_config . database . connection ) with context . begin_transaction ( ) : context . run_migrations ( )
13362	def prompt ( text , default = None , hide_input = False , confirmation_prompt = False , type = None , value_proc = None , prompt_suffix = ': ' , show_default = True , err = False ) : result = None def prompt_func ( text ) : f = hide_input and hidden_prompt_func or visible_prompt_func try : # Write the prompt separately so that we get nice # coloring through colorama on Windows echo ( text , nl = False , err = err ) return f ( '' ) except ( KeyboardInterrupt , EOFError ) : # getpass doesn't print a newline if the user aborts input with ^C. # Allegedly this behavior is inherited from getpass(3). # A doc bug has been filed at https://bugs.python.org/issue24711 if hide_input : echo ( None , err = err ) raise Abort ( ) if value_proc is None : value_proc = convert_type ( type , default ) prompt = _build_prompt ( text , prompt_suffix , show_default , default ) while 1 : while 1 : value = prompt_func ( prompt ) if value : break # If a default is set and used, then the confirmation # prompt is always skipped because that's the only thing # that really makes sense. elif default is not None : return default try : result = value_proc ( value ) except UsageError as e : echo ( 'Error: %s' % e . message , err = err ) continue if not confirmation_prompt : return result while 1 : value2 = prompt_func ( 'Repeat for confirmation: ' ) if value2 : break if value == value2 : return result echo ( 'Error: the two entered values do not match' , err = err )
465	def set_gpu_fraction ( gpu_fraction = 0.3 ) : tl . logging . info ( "[TL]: GPU MEM Fraction %f" % gpu_fraction ) gpu_options = tf . GPUOptions ( per_process_gpu_memory_fraction = gpu_fraction ) sess = tf . Session ( config = tf . ConfigProto ( gpu_options = gpu_options ) ) return sess
2696	def get_tiles ( graf , size = 3 ) : keeps = list ( filter ( lambda w : w . word_id > 0 , graf ) ) keeps_len = len ( keeps ) for i in iter ( range ( 0 , keeps_len - 1 ) ) : w0 = keeps [ i ] for j in iter ( range ( i + 1 , min ( keeps_len , i + 1 + size ) ) ) : w1 = keeps [ j ] if ( w1 . idx - w0 . idx ) <= size : yield ( w0 . root , w1 . root , )
2130	def list ( self , * * kwargs ) : data , self . endpoint = self . data_endpoint ( kwargs ) r = super ( Resource , self ) . list ( * * data ) # Change display settings and data format for human consumption self . configure_display ( r ) return r
3824	async def get_entity_by_id ( self , get_entity_by_id_request ) : response = hangouts_pb2 . GetEntityByIdResponse ( ) await self . _pb_request ( 'contacts/getentitybyid' , get_entity_by_id_request , response ) return response
5502	def timeline ( ctx , pager , limit , twtfile , sorting , timeout , porcelain , source , cache , force_update ) : if source : source_obj = ctx . obj [ "conf" ] . get_source_by_nick ( source ) if not source_obj : logger . debug ( "Not following {0}, trying as URL" . format ( source ) ) source_obj = Source ( source , source ) sources = [ source_obj ] else : sources = ctx . obj [ "conf" ] . following tweets = [ ] if cache : try : with Cache . discover ( update_interval = ctx . obj [ "conf" ] . timeline_update_interval ) as cache : force_update = force_update or not cache . is_valid if force_update : tweets = get_remote_tweets ( sources , limit , timeout , cache ) else : logger . debug ( "Multiple calls to 'timeline' within {0} seconds. Skipping update" . format ( cache . update_interval ) ) # Behold, almighty list comprehensions! (I might have gone overboard here…) tweets = list ( chain . from_iterable ( [ cache . get_tweets ( source . url ) for source in sources ] ) ) except OSError as e : logger . debug ( e ) tweets = get_remote_tweets ( sources , limit , timeout ) else : tweets = get_remote_tweets ( sources , limit , timeout ) if twtfile and not source : source = Source ( ctx . obj [ "conf" ] . nick , ctx . obj [ "conf" ] . twturl , file = twtfile ) tweets . extend ( get_local_tweets ( source , limit ) ) if not tweets : return tweets = sort_and_truncate_tweets ( tweets , sorting , limit ) if pager : click . echo_via_pager ( style_timeline ( tweets , porcelain ) ) else : click . echo ( style_timeline ( tweets , porcelain ) )
11427	def record_make_all_subfields_volatile ( rec ) : for tag in rec . keys ( ) : for field_position , field in enumerate ( rec [ tag ] ) : for subfield_position , subfield in enumerate ( field [ 0 ] ) : if subfield [ 1 ] [ : 9 ] != "VOLATILE:" : record_modify_subfield ( rec , tag , subfield [ 0 ] , "VOLATILE:" + subfield [ 1 ] , subfield_position , field_position_local = field_position )
957	def _genLoggingFilePath ( ) : appName = os . path . splitext ( os . path . basename ( sys . argv [ 0 ] ) ) [ 0 ] or 'UnknownApp' appLogDir = os . path . abspath ( os . path . join ( os . environ [ 'NTA_LOG_DIR' ] , 'numenta-logs-%s' % ( os . environ [ 'USER' ] , ) , appName ) ) appLogFileName = '%s-%s-%s.log' % ( appName , long ( time . mktime ( time . gmtime ( ) ) ) , os . getpid ( ) ) return os . path . join ( appLogDir , appLogFileName )
8848	def mousePressEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mousePressEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if e . button ( ) == QtCore . Qt . LeftButton : self . open_file_requested . emit ( usd . filename , usd . line )
12152	def html_single_plot ( self , abfID , launch = False , overwrite = False ) : if type ( abfID ) is str : abfID = [ abfID ] for thisABFid in cm . abfSort ( abfID ) : parentID = cm . parent ( self . groups , thisABFid ) saveAs = os . path . abspath ( "%s/%s_plot.html" % ( self . folder2 , parentID ) ) if overwrite is False and os . path . basename ( saveAs ) in self . files2 : continue filesByType = cm . filesByType ( self . groupFiles [ parentID ] ) html = "" html += '<div style="background-color: #DDDDFF;">' html += '<span class="title">intrinsic properties for: %s</span></br>' % parentID html += '<code>%s</code>' % os . path . abspath ( self . folder1 + "/" + parentID + ".abf" ) html += '</div>' for fname in filesByType [ 'plot' ] : html += self . htmlFor ( fname ) print ( "creating" , saveAs , '...' ) style . save ( html , saveAs , launch = launch )
12079	def figure_sweeps ( self , offsetX = 0 , offsetY = 0 ) : self . log . debug ( "creating overlayed sweeps plot" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) self . setColorBySweep ( ) plt . plot ( self . abf . sweepX2 + sweep * offsetX , self . abf . sweepY + sweep * offsetY , * * self . kwargs ) if offsetX : self . marginX = .05 self . decorate ( )
7254	def heartbeat ( self ) : url = '%s/heartbeat' % self . base_url # Auth is not required to hit the heartbeat r = requests . get ( url ) try : return r . json ( ) == "ok" except : return False
3155	def update ( self , list_id , segment_id , data ) : return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'segments' , segment_id ) , data = data )
12855	def subtree ( events ) : stack = 0 for obj in events : if obj [ 'type' ] == ENTER : stack += 1 elif obj [ 'type' ] == EXIT : if stack == 0 : break stack -= 1 yield obj
1777	def TEST ( cpu , src1 , src2 ) : # Defined Flags: szp temp = src1 . read ( ) & src2 . read ( ) cpu . SF = ( temp & ( 1 << ( src1 . size - 1 ) ) ) != 0 cpu . ZF = temp == 0 cpu . PF = cpu . _calculate_parity_flag ( temp ) cpu . CF = False cpu . OF = False
11031	def get_json_field ( self , field , * * kwargs ) : d = self . request ( 'GET' , headers = { 'Accept' : 'application/json' } , * * kwargs ) d . addCallback ( raise_for_status ) d . addCallback ( raise_for_header , 'Content-Type' , 'application/json' ) d . addCallback ( json_content ) d . addCallback ( self . _get_json_field , field ) return d
1054	def seed ( self , a = None ) : super ( Random , self ) . seed ( a ) self . gauss_next = None
3150	def get ( self , list_id , webhook_id ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _get ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) )
7846	def add_item ( self , jid , node = None , name = None , action = None ) : return DiscoItem ( self , jid , node , name , action )
12259	def simplex ( x , rho ) : # sort the elements in descending order u = np . flipud ( np . sort ( x . ravel ( ) ) ) lambdas = ( 1 - np . cumsum ( u ) ) / ( 1. + np . arange ( u . size ) ) ix = np . where ( u + lambdas > 0 ) [ 0 ] . max ( ) return np . maximum ( x + lambdas [ ix ] , 0 )
11766	def update ( x , * * entries ) : if isinstance ( x , dict ) : x . update ( entries ) else : x . __dict__ . update ( entries ) return x
8292	def sorted ( list , cmp = None , reversed = False ) : list = [ x for x in list ] list . sort ( cmp ) if reversed : list . reverse ( ) return list
6965	def get ( self ) : # add the reviewed key to the current dict if it doesn't exist # this will hold all the reviewed objects for the frontend if 'reviewed' not in self . currentproject : self . currentproject [ 'reviewed' ] = { } # just returns the current project as JSON self . write ( self . currentproject )
6357	def encode ( self , word ) : def _to_regex ( pattern , left_match = True ) : new_pattern = '' replacements = { '#' : '[AEIOU]+' , ':' : '[BCDFGHJKLMNPQRSTVWXYZ]*' , '^' : '[BCDFGHJKLMNPQRSTVWXYZ]' , '.' : '[BDVGJLMNTWZ]' , '%' : '(ER|E|ES|ED|ING|ELY)' , '+' : '[EIY]' , ' ' : '^' , } for char in pattern : new_pattern += ( replacements [ char ] if char in replacements else char ) if left_match : new_pattern += '$' if '^' not in pattern : new_pattern = '^.*' + new_pattern else : new_pattern = '^' + new_pattern . replace ( '^' , '$' ) if '$' not in new_pattern : new_pattern += '.*$' return new_pattern word = word . upper ( ) pron = '' pos = 0 while pos < len ( word ) : left_orig = word [ : pos ] right_orig = word [ pos : ] first = word [ pos ] if word [ pos ] in self . _rules else ' ' for rule in self . _rules [ first ] : left , match , right , out = rule if right_orig . startswith ( match ) : if left : l_pattern = _to_regex ( left , left_match = True ) if right : r_pattern = _to_regex ( right , left_match = False ) if ( not left or re_match ( l_pattern , left_orig ) ) and ( not right or re_match ( r_pattern , right_orig [ len ( match ) : ] ) ) : pron += out pos += len ( match ) break else : pron += word [ pos ] pos += 1 return pron
4741	def paths_from_env ( prefix = None , names = None ) : def expand_path ( path ) : """Expands variables in 'path' and turns it into absolute path""" return os . path . abspath ( os . path . expanduser ( os . path . expandvars ( path ) ) ) if prefix is None : prefix = "CIJ" if names is None : names = [ "ROOT" , "ENVS" , "TESTPLANS" , "TESTCASES" , "TESTSUITES" , "MODULES" , "HOOKS" , "TEMPLATES" ] conf = { v : os . environ . get ( "_" . join ( [ prefix , v ] ) ) for v in names } for env in ( e for e in conf . keys ( ) if e [ : len ( prefix ) ] in names and conf [ e ] ) : conf [ env ] = expand_path ( conf [ env ] ) if not os . path . exists ( conf [ env ] ) : err ( "%s_%s: %r, does not exist" % ( prefix , env , conf [ env ] ) ) return conf
1465	def get_command_handlers ( ) : return { 'activate' : activate , 'config' : hconfig , 'deactivate' : deactivate , 'help' : cli_help , 'kill' : kill , 'restart' : restart , 'submit' : submit , 'update' : update , 'version' : version }
9010	def index_of_first_produced_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_produced_meshes else : self . _raise_not_found_error ( ) return index
3327	def refresh ( self , token , timeout = None ) : if timeout is None : timeout = LockManager . LOCK_TIME_OUT_DEFAULT return self . storage . refresh ( token , timeout )
4005	def streaming_to_client ( ) : for handler in client_logger . handlers : if hasattr ( handler , 'append_newlines' ) : break else : handler = None old_propagate = client_logger . propagate client_logger . propagate = False if handler is not None : old_append = handler . append_newlines handler . append_newlines = False yield client_logger . propagate = old_propagate if handler is not None : handler . append_newlines = old_append
4311	def _build_input_args ( input_filepath_list , input_format_list ) : if len ( input_format_list ) != len ( input_filepath_list ) : raise ValueError ( "input_format_list & input_filepath_list are not the same size" ) input_args = [ ] zipped = zip ( input_filepath_list , input_format_list ) for input_file , input_fmt in zipped : input_args . extend ( input_fmt ) input_args . append ( input_file ) return input_args
12467	def run_hook ( hook , config , quiet = False ) : if not hook : return True if not quiet : print_message ( '== Step 3. Run post-bootstrap hook ==' ) result = not run_cmd ( prepare_args ( hook , config ) , echo = not quiet , fail_silently = True , shell = True ) if not quiet : print_message ( ) return result
3670	def Wilson ( xs , params ) : gammas = [ ] cmps = range ( len ( xs ) ) for i in cmps : tot1 = log ( sum ( [ params [ i ] [ j ] * xs [ j ] for j in cmps ] ) ) tot2 = 0. for j in cmps : tot2 += params [ j ] [ i ] * xs [ j ] / sum ( [ params [ j ] [ k ] * xs [ k ] for k in cmps ] ) gamma = exp ( 1. - tot1 - tot2 ) gammas . append ( gamma ) return gammas
7993	def _restart_stream ( self ) : self . _input_state = "restart" self . _output_state = "restart" self . features = None self . transport . restart ( ) if self . initiator : self . _send_stream_start ( self . stream_id )
8129	def suggest_spelling ( q , wait = 10 , asynchronous = False , cached = False ) : return YahooSpelling ( q , wait , asynchronous , cached )
11124	def move_directory ( self , relativePath , relativeDestination , replace = False , verbose = True ) : # normalize path relativePath = os . path . normpath ( relativePath ) relativeDestination = os . path . normpath ( relativeDestination ) # get files and directories filesInfo = list ( self . walk_files_info ( relativePath = relativePath ) ) dirsPath = list ( self . walk_directories_relative_path ( relativePath = relativePath ) ) dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage # remove directory info only self . remove_directory ( relativePath = relativePath , removeFromSystem = False ) # create new relative path self . add_directory ( relativeDestination ) # move files for RP , info in filesInfo : source = os . path . join ( self . __path , relativePath , RP ) destination = os . path . join ( self . __path , relativeDestination , RP ) # add directory newDirRP , fileName = os . path . split ( os . path . join ( relativeDestination , RP ) ) dirInfoDict = self . add_directory ( newDirRP ) # move file if os . path . isfile ( destination ) : if replace : os . remove ( destination ) if verbose : warnings . warn ( "file '%s' is copied replacing existing one in destination '%s'." % ( fileName , newDirRP ) ) else : if verbose : warnings . warn ( "file '%s' is not copied because the same file exists in destination '%s'." % ( fileName , destination ) ) continue os . rename ( source , destination ) # set file information dict . __getitem__ ( dirInfoDict , "files" ) [ fileName ] = info # save repository self . save ( )
10076	def commit ( self , * args , * * kwargs ) : return super ( Deposit , self ) . commit ( * args , * * kwargs )
4835	def get_paginated_catalog_courses ( self , catalog_id , querystring = None ) : return self . _load_data ( self . CATALOGS_COURSES_ENDPOINT . format ( catalog_id ) , default = [ ] , querystring = querystring , traverse_pagination = False , many = False , )
3655	def stop_image_acquisition ( self ) : if self . is_acquiring_images : # self . _is_acquiring_images = False # if self . thread_image_acquisition . is_running : # TODO self . thread_image_acquisition . stop ( ) with MutexLocker ( self . thread_image_acquisition ) : # self . device . node_map . AcquisitionStop . execute ( ) try : # Unlock TLParamsLocked in order to allow full device # configuration: self . device . node_map . TLParamsLocked . value = 0 except LogicalErrorException : # SFNC < 2.0 pass for data_stream in self . _data_streams : # Stop image acquisition. try : data_stream . stop_acquisition ( ACQ_STOP_FLAGS_LIST . ACQ_STOP_FLAGS_KILL ) except ( ResourceInUseException , TimeoutException ) as e : self . _logger . error ( e , exc_info = True ) # Flash the queue for image acquisition process. data_stream . flush_buffer_queue ( ACQ_QUEUE_TYPE_LIST . ACQ_QUEUE_ALL_DISCARD ) for event_manager in self . _event_new_buffer_managers : event_manager . flush_event_queue ( ) if self . _create_ds_at_connection : self . _release_buffers ( ) else : self . _release_data_streams ( ) # self . _has_acquired_1st_image = False # self . _chunk_adapter . detach_buffer ( ) # self . _logger . info ( '{0} stopped image acquisition.' . format ( self . _device . id_ ) ) if self . _profiler : self . _profiler . print_diff ( )
11787	def add ( self , o ) : self . smooth_for ( o ) self . dictionary [ o ] += 1 self . n_obs += 1 self . sampler = None
3285	def end_write ( self , with_errors ) : if not with_errors : commands . add ( self . provider . ui , self . provider . repo , self . localHgPath )
8229	def ximport ( self , libName ) : # from Nodebox lib = __import__ ( libName ) self . _namespace [ libName ] = lib lib . _ctx = self return lib
2491	def add_file_dependencies_helper ( self , doc_file ) : subj_triples = list ( self . graph . triples ( ( None , self . spdx_namespace . fileName , Literal ( doc_file . name ) ) ) ) if len ( subj_triples ) != 1 : raise InvalidDocumentError ( 'Could not find dependency subject {0}' . format ( doc_file . name ) ) subject_node = subj_triples [ 0 ] [ 0 ] for dependency in doc_file . dependencies : dep_triples = list ( self . graph . triples ( ( None , self . spdx_namespace . fileName , Literal ( dependency ) ) ) ) if len ( dep_triples ) == 1 : dep_node = dep_triples [ 0 ] [ 0 ] dep_triple = ( subject_node , self . spdx_namespace . fileDependency , dep_node ) self . graph . add ( dep_triple ) else : print ( 'Warning could not resolve file dependency {0} -> {1}' . format ( doc_file . name , dependency ) )
8055	def trusted_cmd ( f ) : def run_cmd ( self , line ) : if self . trusted : f ( self , line ) else : print ( "Sorry cannot do %s here." % f . __name__ [ 3 : ] ) global trusted_cmds trusted_cmds . add ( f . __name__ ) run_cmd . __doc__ = f . __doc__ return run_cmd
7783	def _deactivate ( self ) : self . cache . remove_fetcher ( self ) if self . active : self . _deactivated ( )
4514	def fillCircle ( self , x0 , y0 , r , color = None ) : md . fill_circle ( self . set , x0 , y0 , r , color )
12236	def objective ( param_scales = ( 1 , 1 ) , xstar = None , seed = None ) : ndim = len ( param_scales ) def decorator ( func ) : @ wraps ( func ) def wrapper ( theta ) : return func ( theta ) def param_init ( ) : np . random . seed ( seed ) return np . random . randn ( ndim , ) * np . array ( param_scales ) wrapper . ndim = ndim wrapper . param_init = param_init wrapper . xstar = xstar return wrapper return decorator
3503	def add_loopless ( model , zero_cutoff = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) internal = [ i for i , r in enumerate ( model . reactions ) if not r . boundary ] s_int = create_stoichiometric_matrix ( model ) [ : , numpy . array ( internal ) ] n_int = nullspace ( s_int ) . T max_bound = max ( max ( abs ( b ) for b in r . bounds ) for r in model . reactions ) prob = model . problem # Add indicator variables and new constraints to_add = [ ] for i in internal : rxn = model . reactions [ i ] # indicator variable a_i indicator = prob . Variable ( "indicator_" + rxn . id , type = "binary" ) # -M*(1 - a_i) <= v_i <= M*a_i on_off_constraint = prob . Constraint ( rxn . flux_expression - max_bound * indicator , lb = - max_bound , ub = 0 , name = "on_off_" + rxn . id ) # -(max_bound + 1) * a_i + 1 <= G_i <= -(max_bound + 1) * a_i + 1000 delta_g = prob . Variable ( "delta_g_" + rxn . id ) delta_g_range = prob . Constraint ( delta_g + ( max_bound + 1 ) * indicator , lb = 1 , ub = max_bound , name = "delta_g_range_" + rxn . id ) to_add . extend ( [ indicator , on_off_constraint , delta_g , delta_g_range ] ) model . add_cons_vars ( to_add ) # Add nullspace constraints for G_i for i , row in enumerate ( n_int ) : name = "nullspace_constraint_" + str ( i ) nullspace_constraint = prob . Constraint ( Zero , lb = 0 , ub = 0 , name = name ) model . add_cons_vars ( [ nullspace_constraint ] ) coefs = { model . variables [ "delta_g_" + model . reactions [ ridx ] . id ] : row [ i ] for i , ridx in enumerate ( internal ) if abs ( row [ i ] ) > zero_cutoff } model . constraints [ name ] . set_linear_coefficients ( coefs )
2967	def _sm_stop_from_no_pain ( self , * args , * * kwargs ) : # Just stop the timer. It is possible that it was too late and the # timer is about to run _logger . info ( "Stopping chaos for blockade %s" % self . _blockade_name ) self . _timer . cancel ( )
5154	def get_copy ( dict_ , key , default = None ) : value = dict_ . get ( key , default ) if value : return deepcopy ( value ) return value
10882	def aN ( a , dim = 3 , dtype = 'int' ) : if not hasattr ( a , '__iter__' ) : return np . array ( [ a ] * dim , dtype = dtype ) return np . array ( a ) . astype ( dtype )
6738	def get_last_modified_timestamp ( path , ignore = None ) : ignore = ignore or [ ] if not isinstance ( path , six . string_types ) : return ignore_str = '' if ignore : assert isinstance ( ignore , ( tuple , list ) ) ignore_str = ' ' . join ( "! -name '%s'" % _ for _ in ignore ) cmd = 'find "' + path + '" ' + ignore_str + ' -type f -printf "%T@ %p\n" | sort -n | tail -1 | cut -f 1 -d " "' #'find '+path+' -type f -printf "%T@ %p\n" | sort -n | tail -1 | cut -d " " -f1 ret = subprocess . check_output ( cmd , shell = True ) # Note, we round now to avoid rounding errors later on where some formatters # use different decimal contexts. try : ret = round ( float ( ret ) , 2 ) except ValueError : return return ret
1771	def visualize ( self ) : if os . path . isfile ( self . workspace ) : t = threading . Thread ( target = self . highlight_from_file , args = ( self . workspace , ) ) elif os . path . isdir ( self . workspace ) : t = threading . Thread ( target = self . highlight_from_dir , args = ( self . workspace , ) ) t . start ( )
7907	def __error_message ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_error_message ( stanza ) return True
8739	def _allocate_from_v6_subnet ( self , context , net_id , subnet , port_id , reuse_after , ip_address = None , * * kwargs ) : LOG . info ( "Attempting to allocate a v6 address - [{0}]" . format ( utils . pretty_kwargs ( network_id = net_id , subnet = subnet , port_id = port_id , ip_address = ip_address ) ) ) if ip_address : LOG . info ( "IP %s explicitly requested, deferring to standard " "allocation" % ip_address ) return self . _allocate_from_subnet ( context , net_id = net_id , subnet = subnet , port_id = port_id , reuse_after = reuse_after , ip_address = ip_address , * * kwargs ) else : mac = kwargs . get ( "mac_address" ) if mac : mac = kwargs [ "mac_address" ] . get ( "address" ) if subnet and subnet [ "ip_policy" ] : ip_policy_cidrs = subnet [ "ip_policy" ] . get_cidrs_ip_set ( ) else : ip_policy_cidrs = netaddr . IPSet ( [ ] ) for tries , ip_address in enumerate ( generate_v6 ( mac , port_id , subnet [ "cidr" ] ) ) : LOG . info ( "Attempt {0} of {1}" . format ( tries + 1 , CONF . QUARK . v6_allocation_attempts ) ) if tries > CONF . QUARK . v6_allocation_attempts - 1 : LOG . info ( "Exceeded v6 allocation attempts, bailing" ) raise ip_address_failure ( net_id ) ip_address = netaddr . IPAddress ( ip_address ) . ipv6 ( ) LOG . info ( "Generated a new v6 address {0}" . format ( str ( ip_address ) ) ) if ( ip_policy_cidrs is not None and ip_address in ip_policy_cidrs ) : LOG . info ( "Address {0} excluded by policy" . format ( str ( ip_address ) ) ) continue try : with context . session . begin ( ) : address = db_api . ip_address_create ( context , address = ip_address , subnet_id = subnet [ "id" ] , version = subnet [ "ip_version" ] , network_id = net_id , address_type = kwargs . get ( 'address_type' , ip_types . FIXED ) ) return address except db_exception . DBDuplicateEntry : # This shouldn't ever happen, since we hold a unique MAC # address from the previous IPAM step. LOG . info ( "{0} exists but was already " "allocated" . format ( str ( ip_address ) ) ) LOG . debug ( "Duplicate entry found when inserting subnet_id" " %s ip_address %s" , subnet [ "id" ] , ip_address )
10237	def _generate_citation_dict ( graph : BELGraph ) -> Mapping [ str , Mapping [ Tuple [ BaseEntity , BaseEntity ] , str ] ] : results = defaultdict ( lambda : defaultdict ( set ) ) for u , v , data in graph . edges ( data = True ) : if CITATION not in data : continue results [ data [ CITATION ] [ CITATION_TYPE ] ] [ u , v ] . add ( data [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) return dict ( results )
4865	def validate_username ( self , value ) : try : self . user = User . objects . get ( username = value ) except User . DoesNotExist : raise serializers . ValidationError ( "User does not exist" ) return value
97	def quokka_bounding_boxes ( size = None , extract = None ) : # TODO get rid of this deferred import from imgaug . augmentables . bbs import BoundingBox , BoundingBoxesOnImage left , top = 0 , 0 if extract is not None : bb_extract = _quokka_normalize_extract ( extract ) left = bb_extract . x1 top = bb_extract . y1 with open ( QUOKKA_ANNOTATIONS_FP , "r" ) as f : json_dict = json . load ( f ) bbs = [ ] for bb_dict in json_dict [ "bounding_boxes" ] : bbs . append ( BoundingBox ( x1 = bb_dict [ "x1" ] - left , y1 = bb_dict [ "y1" ] - top , x2 = bb_dict [ "x2" ] - left , y2 = bb_dict [ "y2" ] - top ) ) if extract is not None : shape = ( bb_extract . height , bb_extract . width , 3 ) else : shape = ( 643 , 960 , 3 ) bbsoi = BoundingBoxesOnImage ( bbs , shape = shape ) if size is not None : shape_resized = _compute_resized_shape ( shape , size ) bbsoi = bbsoi . on ( shape_resized ) return bbsoi
10267	def collapse_nodes_with_same_names ( graph : BELGraph ) -> None : survivor_mapping = defaultdict ( set ) # Collapse mapping dict victims = set ( ) # Things already mapped while iterating it = tqdm ( itt . combinations ( graph , r = 2 ) , total = graph . number_of_nodes ( ) * ( graph . number_of_nodes ( ) - 1 ) / 2 ) for a , b in it : if b in victims : continue a_name , b_name = a . get ( NAME ) , b . get ( NAME ) if not a_name or not b_name or a_name . lower ( ) != b_name . lower ( ) : continue if a . keys ( ) != b . keys ( ) : # not same version (might have variants) continue # Ensure that the values in the keys are also the same for k in set ( a . keys ( ) ) - { NAME , NAMESPACE } : if a [ k ] != b [ k ] : # something different continue survivor_mapping [ a ] . add ( b ) # Keep track of things that has been already mapped victims . add ( b ) collapse_nodes ( graph , survivor_mapping )
13567	def linspacestep ( start , stop , step = 1 ) : # Find an integer number of steps numsteps = _np . int ( ( stop - start ) / step ) # Do a linspace over the new range # that has the correct endpoint return _np . linspace ( start , start + step * numsteps , numsteps + 1 )
2372	def variables ( self ) : for table in self . tables : if isinstance ( table , VariableTable ) : # FIXME: settings have statements, variables have rows WTF? :-( for statement in table . rows : if statement [ 0 ] != "" : yield statement
763	def createRecordSensor ( network , name , dataSource ) : # Specific type of region. Possible options can be found in /nupic/regions/ regionType = "py.RecordSensor" # Creates a json from specified dictionary. regionParams = json . dumps ( { "verbosity" : _VERBOSITY } ) network . addRegion ( name , regionType , regionParams ) # getSelf returns the actual region, instead of a region wrapper sensorRegion = network . regions [ name ] . getSelf ( ) # Specify how RecordSensor encodes input values sensorRegion . encoder = createEncoder ( ) # Specify which sub-encoder should be used for "actValueOut" network . regions [ name ] . setParameter ( "predictedField" , "consumption" ) # Specify the dataSource as a file record stream instance sensorRegion . dataSource = dataSource return sensorRegion
13357	def moyennes_glissantes ( df , sur = 8 , rep = 0.75 ) : return pd . rolling_mean ( df , window = sur , min_periods = rep * sur )
8137	def brightness ( self , value = 1.0 ) : b = ImageEnhance . Brightness ( self . img ) self . img = b . enhance ( value )
13651	def get_reference_data ( self , modified_since : Optional [ datetime . datetime ] = None ) -> GetReferenceDataResponse : if modified_since is None : modified_since = datetime . datetime ( year = 2010 , month = 1 , day = 1 ) response = requests . get ( '{}/lovs' . format ( API_URL_BASE ) , headers = { 'if-modified-since' : self . _format_dt ( modified_since ) , * * self . _get_headers ( ) , } , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) # return response.text return GetReferenceDataResponse . deserialize ( response . json ( ) )
4956	def get_object ( self , name , description ) : return Activity ( id = X_API_ACTIVITY_COURSE , definition = ActivityDefinition ( name = LanguageMap ( { 'en-US' : ( name or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , description = LanguageMap ( { 'en-US' : ( description or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , ) , )
4697	def cat_file ( path ) : cmd = [ "cat" , path ] status , stdout , _ = cij . ssh . command ( cmd , shell = True , echo = True ) if status : raise RuntimeError ( "cij.nvme.env: cat %s failed" % path ) return stdout . strip ( )
12289	def bootstrap_datapackage ( repo , force = False , options = None , noinput = False ) : print ( "Bootstrapping datapackage" ) # get the directory tsprefix = datetime . now ( ) . date ( ) . isoformat ( ) # Initial data package json package = OrderedDict ( [ ( 'title' , '' ) , ( 'description' , '' ) , ( 'username' , repo . username ) , ( 'reponame' , repo . reponame ) , ( 'name' , str ( repo ) ) , ( 'title' , "" ) , ( 'description' , "" ) , ( 'keywords' , [ ] ) , ( 'resources' , [ ] ) , ( 'creator' , getpass . getuser ( ) ) , ( 'createdat' , datetime . now ( ) . isoformat ( ) ) , ( 'remote-url' , repo . remoteurl ) ] ) if options is not None : package [ 'title' ] = options [ 'title' ] package [ 'description' ] = options [ 'description' ] else : if noinput : raise IncompleteParameters ( "Option field with title and description" ) for var in [ 'title' , 'description' ] : value = '' while value in [ '' , None ] : value = input ( 'Your Repo ' + var . title ( ) + ": " ) if len ( value ) == 0 : print ( "{} cannot be empty. Please re-enter." . format ( var . title ( ) ) ) package [ var ] = value # Now store the package... ( handle , filename ) = tempfile . mkstemp ( ) with open ( filename , 'w' ) as fd : fd . write ( json . dumps ( package , indent = 4 ) ) repo . package = package return filename
7512	def padnames ( names ) : ## get longest name longname_len = max ( len ( i ) for i in names ) ## Padding distance between name and seq. padding = 5 ## add pad to names pnames = [ name + " " * ( longname_len - len ( name ) + padding ) for name in names ] snppad = "//" + " " * ( longname_len - 2 + padding ) return np . array ( pnames ) , snppad
6216	def buffers_exist ( self ) : for buff in self . buffers : if not buff . is_separate_file : continue path = self . path . parent / buff . uri if not os . path . exists ( path ) : raise FileNotFoundError ( "Buffer {} referenced in {} not found" . format ( path , self . path ) )
13429	def get_sites ( self ) : url = "/2/sites" data = self . _get_resource ( url ) sites = [ ] for entry in data [ 'sites' ] : sites . append ( self . site_from_json ( entry ) ) return sites
2323	def forward ( self , pred , target ) : loss = th . FloatTensor ( [ 0 ] ) for i in range ( 1 , self . moments ) : mk_pred = th . mean ( th . pow ( pred , i ) , 0 ) mk_tar = th . mean ( th . pow ( target , i ) , 0 ) loss . add_ ( th . mean ( ( mk_pred - mk_tar ) ** 2 ) ) # L2 return loss
11255	def attrdict ( prev , attr_names ) : if isinstance ( attr_names , dict ) : for obj in prev : attr_values = dict ( ) for name in attr_names . keys ( ) : if hasattr ( obj , name ) : attr_values [ name ] = getattr ( obj , name ) else : attr_values [ name ] = attr_names [ name ] yield attr_values else : for obj in prev : attr_values = dict ( ) for name in attr_names : if hasattr ( obj , name ) : attr_values [ name ] = getattr ( obj , name ) yield attr_values
12090	def proto_01_13_steps025dual ( abf = exampleABF ) : swhlab . ap . detect ( abf ) standard_groupingForInj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot_values ( abf , feature , continuous = False ) #plot AP info swhlab . plot . save ( abf , tag = 'A_' + feature ) f1 = swhlab . ap . getAvgBySweep ( abf , 'freq' , None , 1 ) f2 = swhlab . ap . getAvgBySweep ( abf , 'freq' , 1 , None ) f1 = np . nan_to_num ( f1 ) f2 = np . nan_to_num ( f2 ) Xs = abf . clampValues ( abf . dataX [ int ( abf . protoSeqX [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = "gain function" , xlabel = "command current (pA)" , ylabel = "average inst. freq. (Hz)" ) pylab . plot ( Xs , f1 , '.-' , ms = 20 , alpha = .5 , label = "step 1" , color = 'b' ) pylab . plot ( Xs , f2 , '.-' , ms = 20 , alpha = .5 , label = "step 2" , color = 'r' ) pylab . legend ( loc = 'upper left' ) pylab . axis ( [ Xs [ 0 ] , Xs [ - 1 ] , None , None ] ) swhlab . plot . save ( abf , tag = 'gain' )
13548	def update ( dst , src ) : stack = [ ( dst , src ) ] def isdict ( o ) : return hasattr ( o , 'keys' ) while stack : current_dst , current_src = stack . pop ( ) for key in current_src : if key not in current_dst : current_dst [ key ] = current_src [ key ] else : if isdict ( current_src [ key ] ) and isdict ( current_dst [ key ] ) : stack . append ( ( current_dst [ key ] , current_src [ key ] ) ) else : current_dst [ key ] = current_src [ key ] return dst
31	def adjust_shape ( placeholder , data ) : if not isinstance ( data , np . ndarray ) and not isinstance ( data , list ) : return data if isinstance ( data , list ) : data = np . array ( data ) placeholder_shape = [ x or - 1 for x in placeholder . shape . as_list ( ) ] assert _check_shape ( placeholder_shape , data . shape ) , 'Shape of data {} is not compatible with shape of the placeholder {}' . format ( data . shape , placeholder_shape ) return np . reshape ( data , placeholder_shape )
9466	def conference_record_stop ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStop/' method = 'POST' return self . request ( path , method , call_params )
6180	def merge_DA_ph_times ( ph_times_d , ph_times_a ) : ph_times = np . hstack ( [ ph_times_d , ph_times_a ] ) a_em = np . hstack ( [ np . zeros ( ph_times_d . size , dtype = np . bool ) , np . ones ( ph_times_a . size , dtype = np . bool ) ] ) index_sort = ph_times . argsort ( ) return ph_times [ index_sort ] , a_em [ index_sort ]
1180	def _create_regs ( self , state ) : regs = [ ( state . start , state . string_position ) ] for group in range ( self . re . groups ) : mark_index = 2 * group if mark_index + 1 < len ( state . marks ) and state . marks [ mark_index ] is not None and state . marks [ mark_index + 1 ] is not None : regs . append ( ( state . marks [ mark_index ] , state . marks [ mark_index + 1 ] ) ) else : regs . append ( ( - 1 , - 1 ) ) return tuple ( regs )
7209	def cancel ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot cancel.' ) if self . batch_values : self . workflow . batch_workflow_cancel ( self . id ) else : self . workflow . cancel ( self . id )
317	def perf_stats_bootstrap ( returns , factor_returns = None , return_stats = True , * * kwargs ) : bootstrap_values = OrderedDict ( ) for stat_func in SIMPLE_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns ) if factor_returns is not None : for stat_func in FACTOR_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns , factor_returns = factor_returns ) bootstrap_values = pd . DataFrame ( bootstrap_values ) if return_stats : stats = bootstrap_values . apply ( calc_distribution_stats ) return stats . T [ [ 'mean' , 'median' , '5%' , '95%' ] ] else : return bootstrap_values
4169	def zpk2tf ( z , p , k ) : import scipy . signal b , a = scipy . signal . zpk2tf ( z , p , k ) return b , a
9592	def set_window_position ( self , x , y , window_handle = 'current' ) : self . _execute ( Command . SET_WINDOW_POSITION , { 'x' : int ( x ) , 'y' : int ( y ) , 'window_handle' : window_handle } )
3839	async def set_presence ( self , set_presence_request ) : response = hangouts_pb2 . SetPresenceResponse ( ) await self . _pb_request ( 'presence/setpresence' , set_presence_request , response ) return response
280	def plot_annual_returns ( returns , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) x_axis_formatter = FuncFormatter ( utils . percentage ) ax . xaxis . set_major_formatter ( FuncFormatter ( x_axis_formatter ) ) ax . tick_params ( axis = 'x' , which = 'major' ) ann_ret_df = pd . DataFrame ( ep . aggregate_returns ( returns , 'yearly' ) ) ax . axvline ( 100 * ann_ret_df . values . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 4 , alpha = 0.7 ) ( 100 * ann_ret_df . sort_index ( ascending = False ) ) . plot ( ax = ax , kind = 'barh' , alpha = 0.70 , * * kwargs ) ax . axvline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Year' ) ax . set_xlabel ( 'Returns' ) ax . set_title ( "Annual returns" ) ax . legend ( [ 'Mean' ] , frameon = True , framealpha = 0.5 ) return ax
12070	def tryLoadingFrom ( tryPath , moduleName = 'swhlab' ) : if not 'site-packages' in swhlab . __file__ : print ( "loaded custom swhlab module from" , os . path . dirname ( swhlab . __file__ ) ) return # no need to warn if it's already outside. while len ( tryPath ) > 5 : sp = tryPath + "/swhlab/" # imaginary swhlab module path if os . path . isdir ( sp ) and os . path . exists ( sp + "/__init__.py" ) : if not os . path . dirname ( tryPath ) in sys . path : sys . path . insert ( 0 , os . path . dirname ( tryPath ) ) print ( "#" * 80 ) print ( "# WARNING: using site-packages swhlab module" ) print ( "#" * 80 ) tryPath = os . path . dirname ( tryPath ) return
12685	def pods ( self ) : # Return empty list if xml_tree is not defined (error Result object) if not self . xml_tree : return [ ] # Create a Pod object for every pod group in xml return [ Pod ( elem ) for elem in self . xml_tree . findall ( 'pod' ) ]
6436	def gen_fibonacci ( ) : num_a , num_b = 1 , 2 while True : yield num_a num_a , num_b = num_b , num_a + num_b
1068	def getaddrlist ( self , name ) : raw = [ ] for h in self . getallmatchingheaders ( name ) : if h [ 0 ] in ' \t' : raw . append ( h ) else : if raw : raw . append ( ', ' ) i = h . find ( ':' ) if i > 0 : addr = h [ i + 1 : ] raw . append ( addr ) alladdrs = '' . join ( raw ) a = AddressList ( alladdrs ) return a . addresslist
7524	def _collapse_outgroup ( tree , taxdicts ) : ## check that all tests have the same outgroup outg = taxdicts [ 0 ] [ "p4" ] if not all ( [ i [ "p4" ] == outg for i in taxdicts ] ) : raise Exception ( "no good" ) ## prune tree, keep only one sample from outgroup tre = ete . Tree ( tree . write ( format = 1 ) ) #tree.copy(method="deepcopy") alltax = [ i for i in tre . get_leaf_names ( ) if i not in outg ] alltax += [ outg [ 0 ] ] tre . prune ( alltax ) tre . search_nodes ( name = outg [ 0 ] ) [ 0 ] . name = "outgroup" tre . ladderize ( ) ## remove other ougroups from taxdicts taxd = copy . deepcopy ( taxdicts ) newtaxdicts = [ ] for test in taxd : #test["p4"] = [outg[0]] test [ "p4" ] = [ "outgroup" ] newtaxdicts . append ( test ) return tre , newtaxdicts
8424	def husl_palette ( n_colors = 6 , h = .01 , s = .9 , l = .65 ) : hues = np . linspace ( 0 , 1 , n_colors + 1 ) [ : - 1 ] hues += h hues %= 1 hues *= 359 s *= 99 l *= 99 palette = [ husl . husl_to_rgb ( h_i , s , l ) for h_i in hues ] return palette
4745	def dev_get_chunk ( dev_name , state , pugrp = None , punit = None ) : rprt = dev_get_rprt ( dev_name , pugrp , punit ) if not rprt : return None return next ( ( d for d in rprt if d [ "cs" ] == state ) , None )
6881	def _parse_csv_header_lcc_csv_v1 ( headerlines ) : # the first three lines indicate the format name, comment char, separator commentchar = headerlines [ 1 ] separator = headerlines [ 2 ] headerlines = [ x . lstrip ( '%s ' % commentchar ) for x in headerlines [ 3 : ] ] # next, find the indices of the various LC sections metadatastart = headerlines . index ( 'OBJECT METADATA' ) columnstart = headerlines . index ( 'COLUMN DEFINITIONS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) metadata = ' ' . join ( headerlines [ metadatastart + 1 : columnstart - 1 ] ) columns = ' ' . join ( headerlines [ columnstart + 1 : lcstart - 1 ] ) metadata = json . loads ( metadata ) columns = json . loads ( columns ) return metadata , columns , separator
13726	def set_connection ( host = None , database = None , user = None , password = None ) : c . CONNECTION [ 'HOST' ] = host c . CONNECTION [ 'DATABASE' ] = database c . CONNECTION [ 'USER' ] = user c . CONNECTION [ 'PASSWORD' ] = password
9210	def capture ( target_url , user_agent = "archiveis (https://github.com/pastpages/archiveis)" , proxies = { } ) : # Put together the URL that will save our request domain = "http://archive.vn" save_url = urljoin ( domain , "/submit/" ) # Configure the request headers headers = { 'User-Agent' : user_agent , "host" : "archive.vn" , } # Request a unique identifier for our activity logger . debug ( "Requesting {}" . format ( domain + "/" ) ) get_kwargs = dict ( timeout = 120 , allow_redirects = True , headers = headers , ) if proxies : get_kwargs [ 'proxies' ] = proxies response = requests . get ( domain + "/" , * * get_kwargs ) response . raise_for_status ( ) # It will need to be parsed from the homepage response headers html = str ( response . content ) try : unique_id = html . split ( 'name="submitid' , 1 ) [ 1 ] . split ( 'value="' , 1 ) [ 1 ] . split ( '"' , 1 ) [ 0 ] logger . debug ( "Unique identifier: {}" . format ( unique_id ) ) except IndexError : logger . warn ( "Unable to extract unique identifier from archive.is. Submitting without it." ) unique_id = None # Send the capture request to archive.is with the unique id included data = { "url" : target_url , "anyway" : 1 , } if unique_id : data . update ( { "submitid" : unique_id } ) post_kwargs = dict ( timeout = 120 , allow_redirects = True , headers = headers , data = data ) if proxies : post_kwargs [ 'proxies' ] = proxies logger . debug ( "Requesting {}" . format ( save_url ) ) response = requests . post ( save_url , * * post_kwargs ) response . raise_for_status ( ) # There are a couple ways the header can come back if 'Refresh' in response . headers : memento = str ( response . headers [ 'Refresh' ] ) . split ( ';url=' ) [ 1 ] logger . debug ( "Memento from Refresh header: {}" . format ( memento ) ) return memento if 'Location' in response . headers : memento = response . headers [ 'Location' ] logger . debug ( "Memento from Location header: {}" . format ( memento ) ) return memento logger . debug ( "Memento not found in response headers. Inspecting history." ) for i , r in enumerate ( response . history ) : logger . debug ( "Inspecting history request #{}" . format ( i ) ) logger . debug ( r . headers ) if 'Location' in r . headers : memento = r . headers [ 'Location' ] logger . debug ( "Memento from the Location header of {} history response: {}" . format ( i + 1 , memento ) ) return memento # If there's nothing at this point, throw an error logger . error ( "No memento returned by archive.is" ) logger . error ( "Status code: {}" . format ( response . status_code ) ) logger . error ( response . headers ) logger . error ( response . text ) raise Exception ( "No memento returned by archive.is" )
10054	def put ( self , pid , record ) : try : ids = [ data [ 'id' ] for data in json . loads ( request . data . decode ( 'utf-8' ) ) ] except KeyError : raise WrongFile ( ) record . files . sort_by ( * ids ) record . commit ( ) db . session . commit ( ) return self . make_response ( obj = record . files , pid = pid , record = record )
11856	def extender ( self , edge ) : ( j , k , B , _ , _ ) = edge for ( i , j , A , alpha , B1b ) in self . chart [ j ] : if B1b and B == B1b [ 0 ] : self . add_edge ( [ i , k , A , alpha + [ edge ] , B1b [ 1 : ] ] )
879	def __getLogger ( cls ) : if cls . __logger is None : cls . __logger = opf_utils . initLogger ( cls ) return cls . __logger
8246	def morguefile ( query , n = 10 , top = 10 ) : from web import morguefile images = morguefile . search ( query ) [ : top ] path = choice ( images ) . download ( thumbnail = True , wait = 10 ) return ColorList ( path , n , name = query )
4735	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.pci.env: invalid SSH environment" ) return 1 pci = cij . env_to_dict ( PREFIX , REQUIRED ) pci [ "BUS_PATH" ] = "/sys/bus/pci" pci [ "DEV_PATH" ] = os . sep . join ( [ pci [ "BUS_PATH" ] , "devices" , pci [ "DEV_NAME" ] ] ) cij . env_export ( PREFIX , EXPORTED , pci ) return 0
11130	def start ( self ) : with self . _status_lock : if self . _running : raise RuntimeError ( "Already running" ) self . _running = True # Cannot re-use Observer after stopped self . _observer = Observer ( ) self . _observer . schedule ( self . _event_handler , self . _directory_location , recursive = True ) self . _observer . start ( ) # Load all in directory afterwards to ensure no undetected changes between loading all and observing self . _origin_mapped_data = self . _load_all_in_directory ( )
944	def getCheckpointParentDir ( experimentDir ) : baseDir = os . path . join ( experimentDir , "savedmodels" ) baseDir = os . path . abspath ( baseDir ) return baseDir
5949	def filename ( self , filename = None , ext = None , set_default = False , use_my_ext = False ) : if filename is None : if not hasattr ( self , '_filename' ) : self . _filename = None # add attribute to class if self . _filename : filename = self . _filename else : raise ValueError ( "A file name is required because no default file name was defined." ) my_ext = None else : filename , my_ext = os . path . splitext ( filename ) if set_default : # replaces existing default file name self . _filename = filename if my_ext and use_my_ext : ext = my_ext if ext is not None : if ext . startswith ( os . extsep ) : ext = ext [ 1 : ] # strip a dot to avoid annoying mistakes if ext != "" : filename = filename + os . extsep + ext return filename
6441	def sim_euclidean ( src , tar , qval = 2 , alphabet = None ) : return Euclidean ( ) . sim ( src , tar , qval , alphabet )
7200	def create_leaflet_viewer ( self , idaho_image_results , filename ) : description = self . describe_images ( idaho_image_results ) if len ( description ) > 0 : functionstring = '' for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : num_images = len ( list ( part . keys ( ) ) ) partname = None if num_images == 1 : # there is only one image, use the PAN partname = [ p for p in list ( part . keys ( ) ) ] [ 0 ] pan_image_id = '' elif num_images == 2 : # there are two images in this part, use the multi (or pansharpen) partname = [ p for p in list ( part . keys ( ) ) if p is not 'PAN' ] [ 0 ] pan_image_id = part [ 'PAN' ] [ 'id' ] if not partname : self . logger . debug ( "Cannot find part for idaho image." ) continue bandstr = { 'RGBN' : '0,1,2' , 'WORLDVIEW_8_BAND' : '4,2,1' , 'PAN' : '0' } . get ( partname , '0,1,2' ) part_boundstr_wkt = part [ partname ] [ 'boundstr' ] part_polygon = from_wkt ( part_boundstr_wkt ) bucketname = part [ partname ] [ 'bucket' ] image_id = part [ partname ] [ 'id' ] W , S , E , N = part_polygon . bounds functionstring += "addLayerToMap('%s','%s',%s,%s,%s,%s,'%s');\n" % ( bucketname , image_id , W , S , E , N , pan_image_id ) __location__ = os . path . realpath ( os . path . join ( os . getcwd ( ) , os . path . dirname ( __file__ ) ) ) try : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) . decode ( "utf8" ) except AttributeError : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) data = data . replace ( 'FUNCTIONSTRING' , functionstring ) data = data . replace ( 'CENTERLAT' , str ( S ) ) data = data . replace ( 'CENTERLON' , str ( W ) ) data = data . replace ( 'BANDS' , bandstr ) data = data . replace ( 'TOKEN' , self . gbdx_connection . access_token ) with codecs . open ( filename , 'w' , 'utf8' ) as outputfile : self . logger . debug ( "Saving %s" % filename ) outputfile . write ( data ) else : print ( 'No items returned.' )
12807	def incoming ( self , messages ) : if self . _observers : campfire = self . _room . get_campfire ( ) for message in messages : for observer in self . _observers : observer ( Message ( campfire , message ) )
8	def observation_input ( ob_space , batch_size = None , name = 'Ob' ) : placeholder = observation_placeholder ( ob_space , batch_size , name ) return placeholder , encode_observation ( ob_space , placeholder )
9145	def clear ( skip ) : for name in sorted ( MODULES ) : if name in skip : continue click . secho ( f'clearing cache for {name}' , fg = 'cyan' , bold = True ) clear_cache ( name )
1382	def register_watch ( self , callback ) : RETRY_COUNT = 5 # Retry in case UID is previously # generated, just in case... for _ in range ( RETRY_COUNT ) : # Generate a random UUID. uid = uuid . uuid4 ( ) if uid not in self . watches : Log . info ( "Registering a watch with uid: " + str ( uid ) ) try : callback ( self ) except Exception as e : Log . error ( "Caught exception while triggering callback: " + str ( e ) ) Log . debug ( traceback . format_exc ( ) ) return None self . watches [ uid ] = callback return uid return None
12448	def from_cookie_string ( self , cookie_string ) : for key_value in cookie_string . split ( ';' ) : if '=' in key_value : key , value = key_value . split ( '=' , 1 ) else : key = key_value strip_key = key . strip ( ) if strip_key and strip_key . lower ( ) not in COOKIE_ATTRIBUTE_NAMES : self [ strip_key ] = value . strip ( )
990	def scale ( reader , writer , column , start , stop , multiple ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( multiple ) ( row [ column ] ) * multiple writer . appendRecord ( row )
12798	def _fetch ( self , method , url = None , post_data = None , parse_data = True , key = None , parameters = None , listener = None , full_return = False ) : headers = self . get_headers ( ) headers [ "Content-Type" ] = "application/json" handlers = [ ] debuglevel = int ( self . _settings [ "debug" ] ) handlers . append ( urllib2 . HTTPHandler ( debuglevel = debuglevel ) ) if hasattr ( httplib , "HTTPS" ) : handlers . append ( urllib2 . HTTPSHandler ( debuglevel = debuglevel ) ) handlers . append ( urllib2 . HTTPCookieProcessor ( cookielib . CookieJar ( ) ) ) password_url = self . _get_password_url ( ) if password_url and "Authorization" not in headers : pwd_manager = urllib2 . HTTPPasswordMgrWithDefaultRealm ( ) pwd_manager . add_password ( None , password_url , self . _settings [ "user" ] , self . _settings [ "password" ] ) handlers . append ( HTTPBasicAuthHandler ( pwd_manager ) ) opener = urllib2 . build_opener ( * handlers ) if post_data is not None : post_data = json . dumps ( post_data ) uri = self . _url ( url , parameters ) request = RESTRequest ( uri , method = method , headers = headers ) if post_data is not None : request . add_data ( post_data ) response = None try : response = opener . open ( request ) body = response . read ( ) if password_url and password_url not in self . _settings [ "authorizations" ] and request . has_header ( "Authorization" ) : self . _settings [ "authorizations" ] [ password_url ] = request . get_header ( "Authorization" ) except urllib2 . HTTPError as e : if e . code == 401 : raise AuthenticationError ( "Access denied while trying to access %s" % uri ) elif e . code == 404 : raise ConnectionError ( "URL not found: %s" % uri ) else : raise except urllib2 . URLError as e : raise ConnectionError ( "Error while fetching from %s: %s" % ( uri , e ) ) finally : if response : response . close ( ) opener . close ( ) data = None if parse_data : if not key : key = string . split ( url , "/" ) [ 0 ] data = self . parse ( body , key ) if full_return : info = response . info ( ) if response else None status = int ( string . split ( info [ "status" ] ) [ 0 ] ) if ( info and "status" in info ) else None return { "success" : ( status >= 200 and status < 300 ) , "data" : data , "info" : info , "body" : body } return data
1656	def IsOutOfLineMethodDefinition ( clean_lines , linenum ) : # Scan back a few lines for start of current function for i in xrange ( linenum , max ( - 1 , linenum - 10 ) , - 1 ) : if Match ( r'^([^()]*\w+)\(' , clean_lines . elided [ i ] ) : return Match ( r'^[^()]*\w+::\w+\(' , clean_lines . elided [ i ] ) is not None return False
7832	def _new_from_xml ( cls , xmlnode ) : child = xmlnode . children fields = [ ] while child : if child . type != "element" or child . ns ( ) . content != DATAFORM_NS : pass elif child . name == "field" : fields . append ( Field . _new_from_xml ( child ) ) child = child . next return cls ( fields )
5625	def relative_path ( path = None , base_dir = None ) : if path_is_remote ( path ) or not os . path . isabs ( path ) : return path else : return os . path . relpath ( path , base_dir )
3438	def merge ( self , right , prefix_existing = None , inplace = True , objective = 'left' ) : if inplace : new_model = self else : new_model = self . copy ( ) new_model . id = '{}_{}' . format ( self . id , right . id ) new_reactions = deepcopy ( right . reactions ) if prefix_existing is not None : existing = new_reactions . query ( lambda rxn : rxn . id in self . reactions ) for reaction in existing : reaction . id = '{}{}' . format ( prefix_existing , reaction . id ) new_model . add_reactions ( new_reactions ) interface = new_model . problem new_vars = [ interface . Variable . clone ( v ) for v in right . variables if v . name not in new_model . variables ] new_model . add_cons_vars ( new_vars ) new_cons = [ interface . Constraint . clone ( c , model = new_model . solver ) for c in right . constraints if c . name not in new_model . constraints ] new_model . add_cons_vars ( new_cons , sloppy = True ) new_model . objective = dict ( left = self . objective , right = right . objective , sum = self . objective . expression + right . objective . expression ) [ objective ] return new_model
6681	def move ( self , source , destination , use_sudo = False ) : func = use_sudo and run_as_root or self . run func ( '/bin/mv {0} {1}' . format ( quote ( source ) , quote ( destination ) ) )
2761	def get_certificate ( self , id ) : return Certificate . get_object ( api_token = self . token , cert_id = id )
9419	def format_docstring ( * args , * * kwargs ) : def decorator ( func ) : func . __doc__ = getdoc ( func ) . format ( * args , * * kwargs ) return func return decorator
512	def _updateMinDutyCyclesLocal ( self ) : for column in xrange ( self . _numColumns ) : neighborhood = self . _getColumnNeighborhood ( column ) maxActiveDuty = self . _activeDutyCycles [ neighborhood ] . max ( ) maxOverlapDuty = self . _overlapDutyCycles [ neighborhood ] . max ( ) self . _minOverlapDutyCycles [ column ] = ( maxOverlapDuty * self . _minPctOverlapDutyCycles )
4310	def _build_input_format_list ( input_filepath_list , input_volumes = None , input_format = None ) : n_inputs = len ( input_filepath_list ) input_format_list = [ ] for _ in range ( n_inputs ) : input_format_list . append ( [ ] ) # Adjust length of input_volumes list if input_volumes is None : vols = [ 1 ] * n_inputs else : n_volumes = len ( input_volumes ) if n_volumes < n_inputs : logger . warning ( 'Volumes were only specified for %s out of %s files.' 'The last %s files will remain at their original volumes.' , n_volumes , n_inputs , n_inputs - n_volumes ) vols = input_volumes + [ 1 ] * ( n_inputs - n_volumes ) elif n_volumes > n_inputs : logger . warning ( '%s volumes were specified but only %s input files exist.' 'The last %s volumes will be ignored.' , n_volumes , n_inputs , n_volumes - n_inputs ) vols = input_volumes [ : n_inputs ] else : vols = [ v for v in input_volumes ] # Adjust length of input_format list if input_format is None : fmts = [ [ ] for _ in range ( n_inputs ) ] else : n_fmts = len ( input_format ) if n_fmts < n_inputs : logger . warning ( 'Input formats were only specified for %s out of %s files.' 'The last %s files will remain unformatted.' , n_fmts , n_inputs , n_inputs - n_fmts ) fmts = [ f for f in input_format ] fmts . extend ( [ [ ] for _ in range ( n_inputs - n_fmts ) ] ) elif n_fmts > n_inputs : logger . warning ( '%s Input formats were specified but only %s input files exist' '. The last %s formats will be ignored.' , n_fmts , n_inputs , n_fmts - n_inputs ) fmts = input_format [ : n_inputs ] else : fmts = [ f for f in input_format ] for i , ( vol , fmt ) in enumerate ( zip ( vols , fmts ) ) : input_format_list [ i ] . extend ( [ '-v' , '{}' . format ( vol ) ] ) input_format_list [ i ] . extend ( fmt ) return input_format_list
13759	def _create_api_uri ( self , * parts ) : return urljoin ( self . API_URI , '/' . join ( map ( quote , parts ) ) )
9939	def find_in_app ( self , app , path ) : storage = self . storages . get ( app , None ) if storage : # only try to find a file if the source dir actually exists if storage . exists ( path ) : matched_path = storage . path ( path ) if matched_path : return matched_path
433	def draw_boxes_and_labels_to_image ( image , classes , coords , scores , classes_list , is_center = True , is_rescale = True , save_name = None ) : if len ( coords ) != len ( classes ) : raise AssertionError ( "number of coordinates and classes are equal" ) if len ( scores ) > 0 and len ( scores ) != len ( classes ) : raise AssertionError ( "number of scores and classes are equal" ) # don't change the original image, and avoid error https://stackoverflow.com/questions/30249053/python-opencv-drawing-errors-after-manipulating-array-with-numpy image = image . copy ( ) imh , imw = image . shape [ 0 : 2 ] thick = int ( ( imh + imw ) // 430 ) for i , _v in enumerate ( coords ) : if is_center : x , y , x2 , y2 = tl . prepro . obj_box_coord_centroid_to_upleft_butright ( coords [ i ] ) else : x , y , x2 , y2 = coords [ i ] if is_rescale : # scale back to pixel unit if the coords are the portion of width and high x , y , x2 , y2 = tl . prepro . obj_box_coord_scale_to_pixelunit ( [ x , y , x2 , y2 ] , ( imh , imw ) ) cv2 . rectangle ( image , ( int ( x ) , int ( y ) ) , ( int ( x2 ) , int ( y2 ) ) , # up-left and botton-right [ 0 , 255 , 0 ] , thick ) cv2 . putText ( image , classes_list [ classes [ i ] ] + ( ( " %.2f" % ( scores [ i ] ) ) if ( len ( scores ) != 0 ) else " " ) , ( int ( x ) , int ( y ) ) , # button left 0 , 1.5e-3 * imh , # bigger = larger font [ 0 , 0 , 256 ] , # self.meta['colors'][max_indx], int ( thick / 2 ) + 1 ) # bold if save_name is not None : # cv2.imwrite('_my.png', image) save_image ( image , save_name ) # if len(coords) == 0: # tl.logging.info("draw_boxes_and_labels_to_image: no bboxes exist, cannot draw !") return image
11056	def ensure_backrefs ( obj , fields = None ) : for ref in _collect_refs ( obj , fields ) : updated = ref [ 'value' ] . _update_backref ( ref [ 'field_instance' ] . _backref_field_name , obj , ref [ 'field_name' ] , ) if updated : logging . debug ( 'Updated reference {}:{}:{}:{}:{}' . format ( obj . _name , obj . _primary_key , ref [ 'field_name' ] , ref [ 'value' ] . _name , ref [ 'value' ] . _primary_key , ) )
4357	def remove_namespace ( self , namespace ) : if namespace in self . active_ns : del self . active_ns [ namespace ] if len ( self . active_ns ) == 0 and self . connected : self . kill ( detach = True )
10167	def get_md_status ( self , line ) : ret = { } splitted = split ( '\W+' , line ) if len ( splitted ) < 7 : ret [ 'available' ] = None ret [ 'used' ] = None ret [ 'config' ] = None else : # The final 2 entries on this line: [n/m] [UUUU_] # [n/m] means that ideally the array would have n devices however, currently, m devices are in use. # Obviously when m >= n then things are good. ret [ 'available' ] = splitted [ - 4 ] ret [ 'used' ] = splitted [ - 3 ] # [UUUU_] represents the status of each device, either U for up or _ for down. ret [ 'config' ] = splitted [ - 2 ] return ret
3410	def remove_from_model ( self , model = None , make_dependent_reactions_nonfunctional = True ) : warn ( "Use cobra.manipulation.remove_genes instead" ) if model is not None : if model != self . _model : raise Exception ( "%s is a member of %s, not %s" % ( repr ( self ) , repr ( self . _model ) , repr ( model ) ) ) if self . _model is None : raise Exception ( '%s is not in a model' % repr ( self ) ) if make_dependent_reactions_nonfunctional : gene_state = 'False' else : gene_state = 'True' the_gene_re = re . compile ( '(^|(?<=( |\()))%s(?=( |\)|$))' % re . escape ( self . id ) ) # remove reference to the gene in all groups associated_groups = self . _model . get_associated_groups ( self ) for group in associated_groups : group . remove_members ( self ) self . _model . genes . remove ( self ) self . _model = None for the_reaction in list ( self . _reaction ) : the_reaction . _gene_reaction_rule = the_gene_re . sub ( gene_state , the_reaction . gene_reaction_rule ) the_reaction . _genes . remove ( self ) # Now, deactivate the reaction if its gene association evaluates # to False the_gene_reaction_relation = the_reaction . gene_reaction_rule for other_gene in the_reaction . _genes : other_gene_re = re . compile ( '(^|(?<=( |\()))%s(?=( |\)|$))' % re . escape ( other_gene . id ) ) the_gene_reaction_relation = other_gene_re . sub ( 'True' , the_gene_reaction_relation ) if not eval ( the_gene_reaction_relation ) : the_reaction . lower_bound = 0 the_reaction . upper_bound = 0 self . _reaction . clear ( )
11089	def _sort_by ( key ) : @ staticmethod def sort_by ( p_list , reverse = False ) : return sorted ( p_list , key = lambda p : getattr ( p , key ) , reverse = reverse , ) return sort_by
11475	def renew_token ( ) : session . token = session . communicator . login_with_api_key ( session . email , session . api_key , application = session . application ) if len ( session . token ) < 10 : # HACK to check for mfa being enabled one_time_pass = getpass . getpass ( 'One-Time Password: ' ) session . token = session . communicator . mfa_otp_login ( session . token , one_time_pass ) return session . token
2789	def snapshot ( self , name ) : return self . get_data ( "volumes/%s/snapshots/" % self . id , type = POST , params = { "name" : name } )
3444	def load_json_model ( filename ) : if isinstance ( filename , string_types ) : with open ( filename , "r" ) as file_handle : return model_from_dict ( json . load ( file_handle ) ) else : return model_from_dict ( json . load ( filename ) )
11384	def module ( self ) : # we have to guard this value because: # https://thingspython.wordpress.com/2010/09/27/another-super-wrinkle-raising-typeerror/ if not hasattr ( self , '_module' ) : if "__main__" in sys . modules : mod = sys . modules [ "__main__" ] path = self . normalize_path ( mod . __file__ ) if os . path . splitext ( path ) == os . path . splitext ( self . path ) : self . _module = mod else : # http://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path self . _module = imp . load_source ( 'captain_script' , self . path ) #self._module = imp.load_source(self.module_name, self.path) return self . _module
1479	def _wait_process_std_out_err ( self , name , process ) : proc . stream_process_stdout ( process , stdout_log_fn ( name ) ) process . wait ( )
8550	def update_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value if attr == 'source_mac' : data [ 'sourceMac' ] = value elif attr == 'source_ip' : data [ 'sourceIp' ] = value elif attr == 'target_ip' : data [ 'targetIp' ] = value elif attr == 'port_range_start' : data [ 'portRangeStart' ] = value elif attr == 'port_range_end' : data [ 'portRangeEnd' ] = value elif attr == 'icmp_type' : data [ 'icmpType' ] = value elif attr == 'icmp_code' : data [ 'icmpCode' ] = value else : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
8987	def last_produced_mesh ( self ) : for instruction in reversed ( self . instructions ) : if instruction . produces_meshes ( ) : return instruction . last_produced_mesh raise IndexError ( "{} produces no meshes" . format ( self ) )
13021	def process_columns ( self , columns ) : if type ( columns ) == list : self . columns = columns elif type ( columns ) == str : self . columns = [ c . strip ( ) for c in columns . split ( ) ] elif type ( columns ) == IntEnum : self . columns = [ str ( c ) for c in columns ] else : raise RawlException ( "Unknown format for columns" )
5581	def _get_contour_values ( min_val , max_val , base = 0 , interval = 100 ) : i = base out = [ ] if min_val < base : while i >= min_val : i -= interval while i <= max_val : if i >= min_val : out . append ( i ) i += interval return out
5934	def to_int64 ( a ) : # build new dtype and replace i4 --> i8 def promote_i4 ( typestr ) : if typestr [ 1 : ] == 'i4' : typestr = typestr [ 0 ] + 'i8' return typestr dtype = [ ( name , promote_i4 ( typestr ) ) for name , typestr in a . dtype . descr ] return a . astype ( dtype )
5270	def _get_word_start_index ( self , idx ) : i = 0 for _idx in self . word_starts [ 1 : ] : if idx < _idx : return i else : i += 1 return i
12387	def parse_segment ( text ) : if not len ( text ) : return NoopQuerySegment ( ) q = QuerySegment ( ) # First we need to split the segment into key/value pairs. This is done # by attempting to split the sequence for each equality comparison. Then # discard any that did not split properly. Then chose the smallest key # (greedily chose the first comparator we encounter in the string) # followed by the smallest value (greedily chose the largest comparator # possible.) # translate into [('=', 'foo=bar')] equalities = zip ( constants . OPERATOR_EQUALITIES , itertools . repeat ( text ) ) # Translate into [('=', ['foo', 'bar'])] equalities = map ( lambda x : ( x [ 0 ] , x [ 1 ] . split ( x [ 0 ] , 1 ) ) , equalities ) # Remove unsplit entries and translate into [('=': ['foo', 'bar'])] # Note that the result from this stage is iterated over twice. equalities = list ( filter ( lambda x : len ( x [ 1 ] ) > 1 , equalities ) ) # Get the smallest key and use the length of that to remove other items key_len = len ( min ( ( x [ 1 ] [ 0 ] for x in equalities ) , key = len ) ) equalities = filter ( lambda x : len ( x [ 1 ] [ 0 ] ) == key_len , equalities ) # Get the smallest value length. thus we have the earliest key and the # smallest value. op , ( key , value ) = min ( equalities , key = lambda x : len ( x [ 1 ] [ 1 ] ) ) key , directive = parse_directive ( key ) if directive : op = constants . OPERATOR_EQUALITY_FALLBACK q . directive = directive # Process negation. This comes in both foo.not= and foo!= forms. path = key . split ( constants . SEP_PATH ) last = path [ - 1 ] # Check for != if last . endswith ( constants . OPERATOR_NEGATION ) : last = last [ : - 1 ] q . negated = not q . negated # Check for foo.not= if last == constants . PATH_NEGATION : path . pop ( - 1 ) q . negated = not q . negated q . values = value . split ( constants . SEP_VALUE ) # Check for suffixed operators (foo.gte=bar). Prioritize suffixed # entries over actual equality checks. if path [ - 1 ] in constants . OPERATOR_SUFFIXES : # The case where foo.gte<=bar, which obviously makes no sense. if op not in constants . OPERATOR_FALLBACK : raise ValueError ( 'Both path-style operator and equality style operator ' 'provided. Please provide only a single style operator.' ) q . operator = constants . OPERATOR_SUFFIX_MAP [ path [ - 1 ] ] path . pop ( - 1 ) else : q . operator = constants . OPERATOR_EQUALITY_MAP [ op ] if not len ( path ) : raise ValueError ( 'No attribute navigation path provided.' ) q . path = path return q
4620	def unlock ( self , password ) : self . password = password if self . config_key in self . config and self . config [ self . config_key ] : self . _decrypt_masterpassword ( ) else : self . _new_masterpassword ( password ) self . _save_encrypted_masterpassword ( )
1373	def defaults_cluster_role_env ( cluster_role_env ) : if len ( cluster_role_env [ 1 ] ) == 0 and len ( cluster_role_env [ 2 ] ) == 0 : return ( cluster_role_env [ 0 ] , getpass . getuser ( ) , ENVIRON ) return ( cluster_role_env [ 0 ] , cluster_role_env [ 1 ] , cluster_role_env [ 2 ] )
8111	def search_blogs ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_BLOGS return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
8516	def _assert_all_finite ( X ) : X = np . asanyarray ( X ) # First try an O(n) time, O(1) space solution for the common case that # everything is finite; fall back to O(n) space np.isfinite to prevent # false positives from overflow in sum method if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : raise ValueError ( "Input contains NaN, infinity" " or a value too large for %r." % X . dtype )
12715	def positions ( self ) : return [ self . ode_obj . getPosition ( i ) for i in range ( self . LDOF ) ]
8722	def operation_list ( uploader ) : files = uploader . file_list ( ) for f in files : log . info ( "{file:30s} {size}" . format ( file = f [ 0 ] , size = f [ 1 ] ) )
13115	def parse_ips ( ips , netmask , include_public ) : hs = HostSearch ( ) rs = RangeSearch ( ) ranges = [ ] ips = list ( set ( ips ) ) included_ips = [ ] print_success ( "Found {} ips" . format ( len ( ips ) ) ) for ip in ips : ip_address = ipaddress . ip_address ( ip ) if include_public or ip_address . is_private : # To stop the screen filling with ranges. if len ( ips ) < 15 : print_success ( "Found ip: {}" . format ( ip ) ) host = hs . id_to_object ( ip ) host . add_tag ( 'dns_discover' ) host . save ( ) r = str ( ipaddress . IPv4Network ( "{}/{}" . format ( ip , netmask ) , strict = False ) ) ranges . append ( r ) included_ips . append ( ip ) else : print_notification ( "Excluding ip {}" . format ( ip ) ) ranges = list ( set ( ranges ) ) print_success ( "Found {} ranges" . format ( len ( ranges ) ) ) for rng in ranges : # To stop the screen filling with ranges. if len ( ranges ) < 15 : print_success ( "Found range: {}" . format ( rng ) ) r = rs . id_to_object ( rng ) r . add_tag ( 'dns_discover' ) r . save ( ) stats = { } stats [ 'ips' ] = included_ips stats [ 'ranges' ] = ranges return stats
10200	def run ( self ) : return elasticsearch . helpers . bulk ( self . client , self . actionsiter ( ) , stats_only = True , chunk_size = 50 )
12108	def _launch_all ( self , launchers ) : for launcher in launchers : print ( "== Launching %s ==" % launcher . batch_name ) launcher ( ) return True
3281	def compute_digest_response ( self , realm , user_name , method , uri , nonce , cnonce , qop , nc , environ ) : def md5h ( data ) : return md5 ( compat . to_bytes ( data ) ) . hexdigest ( ) def md5kd ( secret , data ) : return md5h ( secret + ":" + data ) A1 = self . domain_controller . digest_auth_user ( realm , user_name , environ ) if not A1 : return False A2 = method + ":" + uri if qop : res = md5kd ( A1 , nonce + ":" + nc + ":" + cnonce + ":" + qop + ":" + md5h ( A2 ) ) else : res = md5kd ( A1 , nonce + ":" + md5h ( A2 ) ) return res
2153	def config_from_environment ( ) : kwargs = { } for k in CONFIG_OPTIONS : env = 'TOWER_' + k . upper ( ) v = os . getenv ( env , None ) if v is not None : kwargs [ k ] = v return kwargs
7322	def addattachments ( message , template_path ) : if 'attachment' not in message : return message , 0 message = make_message_multipart ( message ) attachment_filepaths = message . get_all ( 'attachment' , failobj = [ ] ) template_parent_dir = os . path . dirname ( template_path ) for attachment_filepath in attachment_filepaths : attachment_filepath = os . path . expanduser ( attachment_filepath . strip ( ) ) if not attachment_filepath : continue if not os . path . isabs ( attachment_filepath ) : # Relative paths are relative to the template's parent directory attachment_filepath = os . path . join ( template_parent_dir , attachment_filepath ) normalized_path = os . path . abspath ( attachment_filepath ) # Check that the attachment exists if not os . path . exists ( normalized_path ) : print ( "Error: can't find attachment " + normalized_path ) sys . exit ( 1 ) filename = os . path . basename ( normalized_path ) with open ( normalized_path , "rb" ) as attachment : part = email . mime . application . MIMEApplication ( attachment . read ( ) , Name = filename ) part . add_header ( 'Content-Disposition' , 'attachment; filename="{}"' . format ( filename ) ) message . attach ( part ) print ( ">>> attached {}" . format ( normalized_path ) ) del message [ 'attachment' ] return message , len ( attachment_filepaths )
7115	def config_sources ( app , environment , cluster , configs_dirs , app_dir , local = False , build = False ) : sources = [ # Machine-specific ( configs_dirs , 'hostname' ) , ( configs_dirs , 'hostname-local' ) , ( configs_dirs , 'hostname-build' ) , # Global ( configs_dirs , 'common' ) , # Environment + Cluster ( configs_dirs , 'common-%s' % environment ) , ( configs_dirs , 'common-%s-%s' % ( environment , cluster ) ) , ( configs_dirs , 'common-local' ) , ( configs_dirs , 'common-build' ) , # Machine-specific overrides ( configs_dirs , 'common-overrides' ) , # Application-specific ( [ app_dir ] , '%s-default' % app ) , ( [ app_dir ] , '%s-%s' % ( app , environment ) ) , ( [ app_dir ] , '%s-%s-%s' % ( app , environment , cluster ) ) , ( configs_dirs , app ) , ( configs_dirs , '%s-%s' % ( app , environment ) ) , ( configs_dirs , '%s-%s-%s' % ( app , environment , cluster ) ) , ( [ app_dir ] , '%s-local' % app ) , ( [ app_dir ] , '%s-build' % app ) , ( configs_dirs , '%s-local' % app ) , ( configs_dirs , '%s-build' % app ) , # Machine-specific application override ( configs_dirs , '%s-overrides' % app ) , ] # Filter out build sources if not requested if not build : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-build' ) ] # Filter out local sources if not build and not local if not local : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-local' ) ] return available_sources ( sources )
7577	def _get_clumpp_table ( self , kpop , max_var_multiple , quiet ) : ## concat results for k=x reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : return "no result files found" clumphandle = os . path . join ( self . workdir , "tmp.clumppparams.txt" ) self . clumppparams . kpop = kpop self . clumppparams . c = ninds self . clumppparams . r = nreps with open ( clumphandle , 'w' ) as tmp_c : tmp_c . write ( self . clumppparams . _asfile ( ) ) ## create CLUMPP args string outfile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) indfile = os . path . join ( self . workdir , "{}-K-{}.indfile" . format ( self . name , kpop ) ) miscfile = os . path . join ( self . workdir , "{}-K-{}.miscfile" . format ( self . name , kpop ) ) cmd = [ "CLUMPP" , clumphandle , "-i" , indfile , "-o" , outfile , "-j" , miscfile , "-r" , str ( nreps ) , "-c" , str ( ninds ) , "-k" , str ( kpop ) ] ## call clumpp proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) _ = proc . communicate ( ) ## cleanup for rfile in [ indfile , miscfile ] : if os . path . exists ( rfile ) : os . remove ( rfile ) ## parse clumpp results file ofile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) if os . path . exists ( ofile ) : csvtable = pd . read_csv ( ofile , delim_whitespace = True , header = None ) table = csvtable . loc [ : , 5 : ] ## apply names to cols and rows table . columns = range ( table . shape [ 1 ] ) table . index = self . labels if not quiet : sys . stderr . write ( "[K{}] {}/{} results permuted across replicates (max_var={}).\n" . format ( kpop , nreps , nreps + excluded , max_var_multiple ) ) return table else : sys . stderr . write ( "No files ready for {}-K-{} in {}\n" . format ( self . name , kpop , self . workdir ) ) return
3673	def draw_2d ( self , width = 300 , height = 300 , Hs = False ) : # pragma: no cover try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mol = self . rdkitmol_Hs else : mol = self . rdkitmol return Draw . MolToImage ( mol , size = ( width , height ) ) except : return 'Rdkit is required for this feature.'
11628	def clear_sent_messages ( self , offset = None ) : if offset is None : offset = getattr ( settings , 'MAILQUEUE_CLEAR_OFFSET' , defaults . MAILQUEUE_CLEAR_OFFSET ) if type ( offset ) is int : offset = datetime . timedelta ( hours = offset ) delete_before = timezone . now ( ) - offset self . filter ( sent = True , last_attempt__lte = delete_before ) . delete ( )
3206	def get ( self , batch_id , * * queryparams ) : self . batch_id = batch_id self . operation_status = None return self . _mc_client . _get ( url = self . _build_path ( batch_id ) , * * queryparams )
7943	def _start_connect ( self ) : family , addr = self . _dst_addrs . pop ( 0 ) self . _socket = socket . socket ( family , socket . SOCK_STREAM ) self . _socket . setblocking ( False ) self . _dst_addr = addr self . _family = family try : self . _socket . connect ( addr ) except socket . error , err : logger . debug ( "Connect error: {0}" . format ( err ) ) if err . args [ 0 ] in BLOCKING_ERRORS : self . _set_state ( "connecting" ) self . _write_queue . append ( ContinueConnect ( ) ) self . _write_queue_cond . notify ( ) self . event ( ConnectingEvent ( addr ) ) return elif self . _dst_addrs : self . _set_state ( "connect" ) return elif self . _dst_nameports : self . _set_state ( "resolve-hostname" ) return else : self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( ) raise self . _connected ( )
7314	def search ( self ) : try : filters = json . loads ( self . query ) except ValueError : return False result = self . model_query if 'filter' in filters . keys ( ) : result = self . parse_filter ( filters [ 'filter' ] ) if 'sort' in filters . keys ( ) : result = result . order_by ( * self . sort ( filters [ 'sort' ] ) ) return result
6098	def luminosity_within_ellipse_in_units ( self , major_axis , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if not isinstance ( major_axis , dim . Length ) : major_axis = dim . Length ( major_axis , 'arcsec' ) profile = self . new_profile_with_units_converted ( unit_length = major_axis . unit_length , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) luminosity = quad ( profile . luminosity_integral , a = 0.0 , b = major_axis , args = ( self . axis_ratio , ) ) [ 0 ] return dim . Luminosity ( luminosity , unit_luminosity )
11143	def is_name_allowed ( self , path ) : assert isinstance ( path , basestring ) , "given path must be a string" name = os . path . basename ( path ) if not len ( name ) : return False , "empty name is not allowed" # exact match for em in [ self . __repoLock , self . __repoFile , self . __dirInfo , self . __dirLock ] : if name == em : return False , "name '%s' is reserved for pyrep internal usage" % em # pattern match for pm in [ self . __fileInfo , self . __fileLock ] : #,self.__objectDir]: if name == pm or ( name . endswith ( pm [ 3 : ] ) and name . startswith ( '.' ) ) : return False , "name pattern '%s' is not allowed as result may be reserved for pyrep internal usage" % pm # name is ok return True , None
1379	def print_build_info ( zipped_pex = False ) : if zipped_pex : release_file = get_zipped_heron_release_file ( ) else : release_file = get_heron_release_file ( ) with open ( release_file ) as release_info : release_map = yaml . load ( release_info ) release_items = sorted ( release_map . items ( ) , key = lambda tup : tup [ 0 ] ) for key , value in release_items : print ( "%s : %s" % ( key , value ) )
9214	def t_t_isopen ( self , t ) : if t . value [ 0 ] == '"' : t . lexer . push_state ( 'istringquotes' ) elif t . value [ 0 ] == '\'' : t . lexer . push_state ( 'istringapostrophe' ) return t
5339	def __create_dashboard_menu ( self , dash_menu , kibiter_major ) : logger . info ( "Adding dashboard menu" ) if kibiter_major == "6" : menu_resource = ".kibana/doc/metadashboard" mapping_resource = ".kibana/_mapping/doc" mapping = { "dynamic" : "true" } menu = { 'metadashboard' : dash_menu } else : menu_resource = ".kibana/metadashboard/main" mapping_resource = ".kibana/_mapping/metadashboard" mapping = { "dynamic" : "true" } menu = dash_menu menu_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , menu_resource ) mapping_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , mapping_resource ) logger . debug ( "Adding mapping for metadashboard" ) res = self . grimoire_con . put ( mapping_url , data = json . dumps ( mapping ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create mapping for Kibiter menu." ) res = self . grimoire_con . post ( menu_url , data = json . dumps ( menu ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create Kibiter menu." ) logger . error ( res . json ( ) ) raise
651	def sameTMParams ( tp1 , tp2 ) : result = True for param in [ "numberOfCols" , "cellsPerColumn" , "initialPerm" , "connectedPerm" , "minThreshold" , "newSynapseCount" , "permanenceInc" , "permanenceDec" , "permanenceMax" , "globalDecay" , "activationThreshold" , "doPooling" , "segUpdateValidDuration" , "burnIn" , "pamLength" , "maxAge" ] : if getattr ( tp1 , param ) != getattr ( tp2 , param ) : print param , "is different" print getattr ( tp1 , param ) , "vs" , getattr ( tp2 , param ) result = False return result
431	def save_image ( image , image_path = '_temp.png' ) : try : # RGB imageio . imwrite ( image_path , image ) except Exception : # Greyscale imageio . imwrite ( image_path , image [ : , : , 0 ] )
4217	def delete_password ( self , service , username ) : if not self . connected ( service ) : # the user pressed "cancel" when prompted to unlock their keyring. raise PasswordDeleteError ( "Cancelled by user" ) if not self . iface . hasEntry ( self . handle , service , username , self . appid ) : raise PasswordDeleteError ( "Password not found" ) self . iface . removeEntry ( self . handle , service , username , self . appid )
10058	def records ( ) : import pkg_resources from dojson . contrib . marc21 import marc21 from dojson . contrib . marc21 . utils import create_record , split_blob from flask_login import login_user , logout_user from invenio_accounts . models import User from invenio_deposit . api import Deposit users = User . query . all ( ) # pkg resources the demodata data_path = pkg_resources . resource_filename ( 'invenio_records' , 'data/marc21/bibliographic.xml' ) with open ( data_path ) as source : with current_app . test_request_context ( ) : indexer = RecordIndexer ( ) with db . session . begin_nested ( ) : for index , data in enumerate ( split_blob ( source . read ( ) ) , start = 1 ) : login_user ( users [ index % len ( users ) ] ) # do translate record = marc21 . do ( create_record ( data ) ) # create record indexer . index ( Deposit . create ( record ) ) logout_user ( ) db . session . commit ( )
26	def flatten_grads ( var_list , grads ) : return tf . concat ( [ tf . reshape ( grad , [ U . numel ( v ) ] ) for ( v , grad ) in zip ( var_list , grads ) ] , 0 )
4569	def dump ( data , file = sys . stdout , use_yaml = None , * * kwds ) : if use_yaml is None : use_yaml = ALWAYS_DUMP_YAML def dump ( fp ) : if use_yaml : yaml . safe_dump ( data , stream = fp , * * kwds ) else : json . dump ( data , fp , indent = 4 , sort_keys = True , * * kwds ) if not isinstance ( file , str ) : return dump ( file ) if os . path . isabs ( file ) : parent = os . path . dirname ( file ) if not os . path . exists ( parent ) : os . makedirs ( parent , exist_ok = True ) with open ( file , 'w' ) as fp : return dump ( fp )
12473	def add_extension_if_needed ( filepath , ext , check_if_exists = False ) : if not filepath . endswith ( ext ) : filepath += ext if check_if_exists : if not op . exists ( filepath ) : raise IOError ( 'File not found: ' + filepath ) return filepath
3761	def draw_2d ( self , Hs = False ) : # pragma: no cover try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mols = [ i . rdkitmol_Hs for i in self . Chemicals ] else : mols = [ i . rdkitmol for i in self . Chemicals ] return Draw . MolsToImage ( mols ) except : return 'Rdkit is required for this feature.'
6235	def get_time ( self ) -> float : if self . paused : return self . pause_time return mixer . music . get_pos ( ) / 1000.0
9929	def authenticate ( username , password , service = 'login' , encoding = 'utf-8' , resetcred = True ) : if sys . version_info >= ( 3 , ) : if isinstance ( username , str ) : username = username . encode ( encoding ) if isinstance ( password , str ) : password = password . encode ( encoding ) if isinstance ( service , str ) : service = service . encode ( encoding ) @ conv_func def my_conv ( n_messages , messages , p_response , app_data ) : """Simple conversation function that responds to any prompt where the echo is off with the supplied password""" # Create an array of n_messages response objects addr = calloc ( n_messages , sizeof ( PamResponse ) ) p_response [ 0 ] = cast ( addr , POINTER ( PamResponse ) ) for i in range ( n_messages ) : if messages [ i ] . contents . msg_style == PAM_PROMPT_ECHO_OFF : pw_copy = strdup ( password ) p_response . contents [ i ] . resp = cast ( pw_copy , c_char_p ) p_response . contents [ i ] . resp_retcode = 0 return 0 handle = PamHandle ( ) conv = PamConv ( my_conv , 0 ) retval = pam_start ( service , username , byref ( conv ) , byref ( handle ) ) if retval != 0 : # TODO: This is not an authentication error, something # has gone wrong starting up PAM return False retval = pam_authenticate ( handle , 0 ) auth_success = ( retval == 0 ) # Re-initialize credentials (for Kerberos users, etc) # Don't check return code of pam_setcred(), it shouldn't matter # if this fails if auth_success and resetcred : retval = pam_setcred ( handle , PAM_REINITIALIZE_CRED ) pam_end ( handle , retval ) return auth_success
11411	def record_delete_subfield ( rec , tag , subfield_code , ind1 = ' ' , ind2 = ' ' ) : ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) for field in rec . get ( tag , [ ] ) : if field [ 1 ] == ind1 and field [ 2 ] == ind2 : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield_code != subfield [ 0 ] ]
5639	def _temporal_distance_pdf ( self ) : temporal_distance_split_points_ordered , norm_cdf = self . _temporal_distance_cdf ( ) delta_peak_loc_to_probability_mass = { } non_delta_peak_split_points = [ temporal_distance_split_points_ordered [ 0 ] ] non_delta_peak_densities = [ ] for i in range ( 0 , len ( temporal_distance_split_points_ordered ) - 1 ) : left = temporal_distance_split_points_ordered [ i ] right = temporal_distance_split_points_ordered [ i + 1 ] width = right - left prob_mass = norm_cdf [ i + 1 ] - norm_cdf [ i ] if width == 0.0 : delta_peak_loc_to_probability_mass [ left ] = prob_mass else : non_delta_peak_split_points . append ( right ) non_delta_peak_densities . append ( prob_mass / float ( width ) ) assert ( len ( non_delta_peak_densities ) == len ( non_delta_peak_split_points ) - 1 ) return numpy . array ( non_delta_peak_split_points ) , numpy . array ( non_delta_peak_densities ) , delta_peak_loc_to_probability_mass
9462	def conference_hangup ( self , call_params ) : path = '/' + self . api_version + '/ConferenceHangup/' method = 'POST' return self . request ( path , method , call_params )
4531	def construct ( cls , project , * * desc ) : return cls ( project . drivers , maker = project . maker , * * desc )
11598	def prepare ( self ) : # Create a collection for the attributes and elements of # this instance. attributes , elements = OrderedDict ( ) , [ ] # Initialize the namespace map. nsmap = dict ( [ self . meta . namespace ] ) # Iterate through all declared items. for name , item in self . _items . items ( ) : if isinstance ( item , Attribute ) : # Prepare the item as an attribute. attributes [ name ] = item . prepare ( self ) elif isinstance ( item , Element ) : # Update the nsmap. nsmap . update ( [ item . namespace ] ) # Prepare the item as an element. elements . append ( item ) # Return the collected attributes and elements return attributes , elements , nsmap
13653	def Text ( name , encoding = None ) : def _match ( request , value ) : return name , query . Text ( value , encoding = contentEncoding ( request . requestHeaders , encoding ) ) return _match
873	def getState ( self ) : varStates = dict ( ) for varName , var in self . permuteVars . iteritems ( ) : varStates [ varName ] = var . getState ( ) return dict ( id = self . particleId , genIdx = self . genIdx , swarmId = self . swarmId , varStates = varStates )
3649	def messages ( self ) : method = 'GET' url = 'activeMessage' rc = self . __request__ ( method , url ) # try: # return rc['activeMessage'] # except: # raise UnknownError('Invalid activeMessage response') # is it even possible? return rc [ 'activeMessage' ]
5340	def __remove_dashboard_menu ( self , kibiter_major ) : logger . info ( "Removing old dashboard menu, if any" ) if kibiter_major == "6" : metadashboard = ".kibana/doc/metadashboard" else : metadashboard = ".kibana/metadashboard/main" menu_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , metadashboard ) self . grimoire_con . delete ( menu_url )
13053	def nmap_scan ( ) : # Create the search and config objects hs = HostSearch ( ) config = Config ( ) # Static options to be able to figure out what options to use depending on the input the user gives. nmap_types = [ 'top10' , 'top100' , 'custom' , 'top1000' , 'all' ] options = { 'top10' : '--top-ports 10' , 'top100' : '--top-ports 100' , 'custom' : config . get ( 'nmap' , 'options' ) , 'top1000' : '--top-ports 1000' , 'all' : '-p-' } # Create an argument parser hs_parser = hs . argparser argparser = argparse . ArgumentParser ( parents = [ hs_parser ] , conflict_handler = 'resolve' , description = "Scans hosts from the database using nmap, any arguments that are not in the help are passed to nmap" ) argparser . add_argument ( 'type' , metavar = 'type' , help = 'The number of ports to scan: top10, top100, custom, top1000 (default) or all' , type = str , choices = nmap_types , default = 'top1000' , const = 'top1000' , nargs = '?' ) arguments , extra_nmap_args = argparser . parse_known_args ( ) # Fix the tags for the search tags = nmap_types [ nmap_types . index ( arguments . type ) : ] tags = [ "!nmap_" + tag for tag in tags ] hosts = hs . get_hosts ( tags = tags ) hosts = [ host for host in hosts ] # Create the nmap arguments nmap_args = [ ] nmap_args . extend ( extra_nmap_args ) nmap_args . extend ( options [ arguments . type ] . split ( ' ' ) ) # Run nmap print_notification ( "Running nmap with args: {} on {} hosts(s)" . format ( nmap_args , len ( hosts ) ) ) if len ( hosts ) : result = nmap ( nmap_args , [ str ( h . address ) for h in hosts ] ) # Import the nmap result for host in hosts : host . add_tag ( "nmap_{}" . format ( arguments . type ) ) host . save ( ) print_notification ( "Nmap done, importing results" ) stats = import_nmap ( result , "nmap_{}" . format ( arguments . type ) , check_function = all_hosts , import_services = True ) stats [ 'scanned_hosts' ] = len ( hosts ) stats [ 'type' ] = arguments . type Logger ( ) . log ( 'nmap_scan' , "Performed nmap {} scan on {} hosts" . format ( arguments . type , len ( hosts ) ) , stats ) else : print_notification ( "No hosts found" )
13020	def _execute ( self , query , commit = False , working_columns = None ) : log . debug ( "RawlBase._execute()" ) result = [ ] if working_columns is None : working_columns = self . columns with RawlConnection ( self . dsn ) as conn : query_id = random . randrange ( 9999 ) curs = conn . cursor ( ) try : log . debug ( "Executing(%s): %s" % ( query_id , query . as_string ( curs ) ) ) except : log . exception ( "LOGGING EXCEPTION LOL" ) curs . execute ( query ) log . debug ( "Executed" ) if commit == True : log . debug ( "COMMIT(%s)" % query_id ) conn . commit ( ) log . debug ( "curs.rowcount: %s" % curs . rowcount ) if curs . rowcount > 0 : #result = curs.fetchall() # Process the results into a dict and stuff it in a RawlResult # object. Then append that object to result result_rows = curs . fetchall ( ) for row in result_rows : i = 0 row_dict = { } for col in working_columns : try : #log.debug("row_dict[%s] = row[%s] which is %s" % (col, i, row[i])) # For aliased columns, we need to get rid of the dot col = col . replace ( '.' , '_' ) row_dict [ col ] = row [ i ] except IndexError : pass i += 1 log . debug ( "Appending dict to result: %s" % row_dict ) rr = RawlResult ( working_columns , row_dict ) result . append ( rr ) curs . close ( ) return result
5061	def get_enterprise_customer_for_user ( auth_user ) : EnterpriseCustomerUser = apps . get_model ( 'enterprise' , 'EnterpriseCustomerUser' ) # pylint: disable=invalid-name try : return EnterpriseCustomerUser . objects . get ( user_id = auth_user . id ) . enterprise_customer # pylint: disable=no-member except EnterpriseCustomerUser . DoesNotExist : return None
11241	def get_line_count ( fname ) : i = 0 with open ( fname ) as f : for i , l in enumerate ( f ) : pass return i + 1
1359	def get_argument_instance ( self ) : try : instance = self . get_argument ( constants . PARAM_INSTANCE ) return instance except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
1953	def input_from_cons ( constupl , datas ) : def make_chr ( c ) : try : return chr ( c ) except Exception : return c newset = constraints_to_constraintset ( constupl ) ret = '' for data in datas : for c in data : ret += make_chr ( solver . get_value ( newset , c ) ) return ret
12187	async def handle_message ( self , message , filters ) : data = self . _unpack_message ( message ) logger . debug ( data ) if data . get ( 'type' ) == 'error' : raise SlackApiError ( data . get ( 'error' , { } ) . get ( 'msg' , str ( data ) ) ) elif self . message_is_to_me ( data ) : text = data [ 'text' ] [ len ( self . address_as ) : ] . strip ( ) if text == 'help' : return self . _respond ( channel = data [ 'channel' ] , text = self . _instruction_list ( filters ) , ) elif text == 'version' : return self . _respond ( channel = data [ 'channel' ] , text = self . VERSION , ) for _filter in filters : if _filter . matches ( data ) : logger . debug ( 'Response triggered' ) async for response in _filter : self . _respond ( channel = data [ 'channel' ] , text = response )
6687	def install ( packages , repos = None , yes = None , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , six . string_types ) : options = [ options ] if not isinstance ( packages , six . string_types ) : packages = " " . join ( packages ) if repos : for repo in repos : options . append ( '--enablerepo=%(repo)s' % locals ( ) ) options = " " . join ( options ) if isinstance ( yes , str ) : run_as_root ( 'yes %(yes)s | %(manager)s %(options)s install %(packages)s' % locals ( ) ) else : run_as_root ( '%(manager)s %(options)s install %(packages)s' % locals ( ) )
5753	def get_page_url ( page_num , current_app , url_view_name , url_extra_args , url_extra_kwargs , url_param_name , url_get_params , url_anchor ) : if url_view_name is not None : # Add page param to the kwargs list. Overrides any previously set parameter of the same name. url_extra_kwargs [ url_param_name ] = page_num try : url = reverse ( url_view_name , args = url_extra_args , kwargs = url_extra_kwargs , current_app = current_app ) except NoReverseMatch as e : # Attempt to load view from application root, allowing the use of non-namespaced view names if your view is defined in the root application if settings . SETTINGS_MODULE : if django . VERSION < ( 1 , 9 , 0 ) : separator = '.' else : separator = ':' # Namespace separator changed to colon after 1.8 project_name = settings . SETTINGS_MODULE . split ( '.' ) [ 0 ] try : url = reverse ( project_name + separator + url_view_name , args = url_extra_args , kwargs = url_extra_kwargs , current_app = current_app ) except NoReverseMatch : raise e # Raise the original exception so the error message doesn't confusingly include something the Developer didn't add to the view name themselves else : raise e # We can't determine the project name so just re-throw the exception else : url = '' url_get_params = url_get_params or QueryDict ( url ) url_get_params = url_get_params . copy ( ) url_get_params [ url_param_name ] = str ( page_num ) if len ( url_get_params ) > 0 : if not isinstance ( url_get_params , QueryDict ) : tmp = QueryDict ( mutable = True ) tmp . update ( url_get_params ) url_get_params = tmp url += '?' + url_get_params . urlencode ( ) if ( url_anchor is not None ) : url += '#' + url_anchor return url
8924	def verify ( self ) : value = self . get ( 'verify' , 'true' ) if isinstance ( value , bool ) : verify = value elif value . lower ( ) == 'true' : verify = True elif value . lower ( ) == 'false' : verify = False else : verify = value return verify
7638	def find_with_extension ( in_dir , ext , depth = 3 , sort = True ) : assert depth >= 1 ext = ext . strip ( os . extsep ) match = list ( ) for n in range ( 1 , depth + 1 ) : wildcard = os . path . sep . join ( [ "*" ] * n ) search_path = os . path . join ( in_dir , os . extsep . join ( [ wildcard , ext ] ) ) match += glob . glob ( search_path ) if sort : match . sort ( ) return match
1087	def concat ( a , b ) : if not hasattr ( a , '__getitem__' ) : msg = "'%s' object can't be concatenated" % type ( a ) . __name__ raise TypeError ( msg ) return a + b
11471	def upload ( self , filename , location = '' ) : current_folder = self . _ftp . pwd ( ) self . mkdir ( location ) self . cd ( location ) fl = open ( filename , 'rb' ) filename = filename . split ( '/' ) [ - 1 ] self . _ftp . storbinary ( 'STOR %s' % filename , fl ) fl . close ( ) self . cd ( current_folder )
7637	def smkdirs ( dpath , mode = 0o777 ) : if not os . path . exists ( dpath ) : os . makedirs ( dpath , mode = mode )
3049	def _get_implicit_credentials ( cls ) : # Environ checks (in order). environ_checkers = [ cls . _implicit_credentials_from_files , cls . _implicit_credentials_from_gae , cls . _implicit_credentials_from_gce , ] for checker in environ_checkers : credentials = checker ( ) if credentials is not None : return credentials # If no credentials, fail. raise ApplicationDefaultCredentialsError ( ADC_HELP_MSG )
754	def setLoggedMetrics ( self , metricNames ) : if metricNames is None : self . __metricNames = set ( [ ] ) else : self . __metricNames = set ( metricNames )
2982	def cmd_add ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . add_container ( opts . containers )
5710	def get ( self , profile_id ) : if profile_id not in self . _profiles : try : self . _profiles [ profile_id ] = self . _get_profile ( profile_id ) except ( ValueError , IOError ) as e : six . raise_from ( RegistryError ( e ) , e ) return self . _profiles [ profile_id ]
6712	def tunnel ( self , local_port , remote_port ) : r = self . local_renderer r . env . tunnel_local_port = local_port r . env . tunnel_remote_port = remote_port r . local ( ' ssh -i {key_filename} -L {tunnel_local_port}:localhost:{tunnel_remote_port} {user}@{host_string} -N' )
8809	def delete_mac_address_range ( context , id ) : LOG . info ( "delete_mac_address_range %s for tenant %s" % ( id , context . tenant_id ) ) if not context . is_admin : raise n_exc . NotAuthorized ( ) with context . session . begin ( ) : mar = db_api . mac_address_range_find ( context , id = id , scope = db_api . ONE ) if not mar : raise q_exc . MacAddressRangeNotFound ( mac_address_range_id = id ) _delete_mac_address_range ( context , mar )
7596	def search_tournaments ( self , * * params : keys ) : url = self . api . TOURNAMENT + '/search' return self . _get_model ( url , PartialClan , * * params )
4743	def exists ( ) : if env ( ) : cij . err ( "cij.nvm.exists: Invalid NVMe ENV." ) return 1 nvm = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ '[[ -b "%s" ]]' % nvm [ "DEV_PATH" ] ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True , echo = False ) return rcode
3058	def _write_credentials_file ( credentials_file , credentials ) : data = { 'file_version' : 2 , 'credentials' : { } } for key , credential in iteritems ( credentials ) : credential_json = credential . to_json ( ) encoded_credential = _helpers . _from_bytes ( base64 . b64encode ( _helpers . _to_bytes ( credential_json ) ) ) data [ 'credentials' ] [ key ] = encoded_credential credentials_file . seek ( 0 ) json . dump ( data , credentials_file ) credentials_file . truncate ( )
12591	def get_reliabledictionary_schema ( client , application_name , service_name , dictionary_name , output_file = None ) : cluster = Cluster . from_sfclient ( client ) dictionary = cluster . get_application ( application_name ) . get_service ( service_name ) . get_dictionary ( dictionary_name ) result = json . dumps ( dictionary . get_information ( ) , indent = 4 ) if ( output_file == None ) : output_file = "{}-{}-{}-schema-output.json" . format ( application_name , service_name , dictionary_name ) with open ( output_file , "w" ) as output : output . write ( result ) print ( 'Printed schema information to: ' + output_file ) print ( result )
10794	def create_comparison_state ( image , position , radius = 5.0 , snr = 20 , method = 'constrained-cubic' , extrapad = 2 , zscale = 1.0 ) : # first pad the image slightly since they are pretty small image = common . pad ( image , extrapad , 0 ) # place that into a new image at the expected parameters s = init . create_single_particle_state ( imsize = np . array ( image . shape ) , sigma = 1.0 / snr , radius = radius , psfargs = { 'params' : np . array ( [ 2.0 , 1.0 , 3.0 ] ) , 'error' : 1e-6 , 'threads' : 2 } , objargs = { 'method' : method } , stateargs = { 'sigmapad' : False , 'pad' : 4 , 'zscale' : zscale } ) s . obj . pos [ 0 ] = position + s . pad + extrapad s . reset ( ) s . model_to_true_image ( ) timage = 1 - np . pad ( image , s . pad , mode = 'constant' , constant_values = 0 ) timage = s . psf . execute ( timage ) return s , timage [ s . inner ]
1623	def FindEndOfExpressionInLine ( line , startpos , stack ) : for i in xrange ( startpos , len ( line ) ) : char = line [ i ] if char in '([{' : # Found start of parenthesized expression, push to expression stack stack . append ( char ) elif char == '<' : # Found potential start of template argument list if i > 0 and line [ i - 1 ] == '<' : # Left shift operator if stack and stack [ - 1 ] == '<' : stack . pop ( ) if not stack : return ( - 1 , None ) elif i > 0 and Search ( r'\boperator\s*$' , line [ 0 : i ] ) : # operator<, don't add to stack continue else : # Tentative start of template argument list stack . append ( '<' ) elif char in ')]}' : # Found end of parenthesized expression. # # If we are currently expecting a matching '>', the pending '<' # must have been an operator. Remove them from expression stack. while stack and stack [ - 1 ] == '<' : stack . pop ( ) if not stack : return ( - 1 , None ) if ( ( stack [ - 1 ] == '(' and char == ')' ) or ( stack [ - 1 ] == '[' and char == ']' ) or ( stack [ - 1 ] == '{' and char == '}' ) ) : stack . pop ( ) if not stack : return ( i + 1 , None ) else : # Mismatched parentheses return ( - 1 , None ) elif char == '>' : # Found potential end of template argument list. # Ignore "->" and operator functions if ( i > 0 and ( line [ i - 1 ] == '-' or Search ( r'\boperator\s*$' , line [ 0 : i - 1 ] ) ) ) : continue # Pop the stack if there is a matching '<'. Otherwise, ignore # this '>' since it must be an operator. if stack : if stack [ - 1 ] == '<' : stack . pop ( ) if not stack : return ( i + 1 , None ) elif char == ';' : # Found something that look like end of statements. If we are currently # expecting a '>', the matching '<' must have been an operator, since # template argument list should not contain statements. while stack and stack [ - 1 ] == '<' : stack . pop ( ) if not stack : return ( - 1 , None ) # Did not find end of expression or unbalanced parentheses on this line return ( - 1 , stack )
1799	def CMOVO ( cpu , dest , src ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF , src . read ( ) , dest . read ( ) ) )
7579	def result_files ( self ) : reps = OPJ ( self . workdir , self . name + "-K-*-rep-*_f" ) repfiles = glob . glob ( reps ) return repfiles
10319	def _microcanonical_average_max_cluster_size ( max_cluster_size , alpha ) : ret = dict ( ) runs = max_cluster_size . size sqrt_n = np . sqrt ( runs ) max_cluster_size_sample_mean = max_cluster_size . mean ( ) ret [ 'max_cluster_size' ] = max_cluster_size_sample_mean max_cluster_size_sample_std = max_cluster_size . std ( ddof = 1 ) if max_cluster_size_sample_std : old_settings = np . seterr ( all = 'raise' ) ret [ 'max_cluster_size_ci' ] = scipy . stats . t . interval ( 1 - alpha , df = runs - 1 , loc = max_cluster_size_sample_mean , scale = max_cluster_size_sample_std / sqrt_n ) np . seterr ( * * old_settings ) else : ret [ 'max_cluster_size_ci' ] = ( max_cluster_size_sample_mean * np . ones ( 2 ) ) return ret
10044	def default_view_method ( pid , record , template = None ) : record_viewed . send ( current_app . _get_current_object ( ) , pid = pid , record = record , ) deposit_type = request . values . get ( 'type' ) return render_template ( template , pid = pid , record = record , jsonschema = current_deposit . jsonschemas [ deposit_type ] , schemaform = current_deposit . schemaforms [ deposit_type ] , )
11738	def attach_bp ( self , bp , description = '' ) : if not isinstance ( bp , Blueprint ) : raise InvalidBlueprint ( 'Blueprints attached to the bundle must be of type {0}' . format ( Blueprint ) ) self . blueprints . append ( ( bp , description ) )
4594	def make_matrix_coord_map ( dx , dy , serpentine = True , offset = 0 , rotation = 0 , y_flip = False ) : result = [ ] for y in range ( dy ) : if not serpentine or y % 2 == 0 : result . append ( [ ( dx * y ) + x + offset for x in range ( dx ) ] ) else : result . append ( [ dx * ( y + 1 ) - 1 - x + offset for x in range ( dx ) ] ) result = rotate_and_flip ( result , rotation , y_flip ) return result
171	def draw_lines_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , antialiased = True , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_lines_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
10238	def count_citations ( graph : BELGraph , * * annotations ) -> Counter : citations = defaultdict ( set ) annotation_dict_filter = build_edge_data_filter ( annotations ) for u , v , _ , d in filter_edges ( graph , annotation_dict_filter ) : if CITATION not in d : continue citations [ u , v ] . add ( ( d [ CITATION ] [ CITATION_TYPE ] , d [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) ) return Counter ( itt . chain . from_iterable ( citations . values ( ) ) )
8604	def update_user ( self , user_id , * * kwargs ) : properties = { } for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/users/%s' % user_id , method = 'PUT' , data = json . dumps ( data ) ) return response
3741	def omega_mixture ( omegas , zs , CASRNs = None , Method = None , AvailableMethods = False ) : def list_methods ( ) : methods = [ ] if none_and_length_check ( [ zs , omegas ] ) : methods . append ( 'SIMPLE' ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'SIMPLE' : _omega = mixing_simple ( zs , omegas ) elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
12632	def copy_groups_to_folder ( dicom_groups , folder_path , groupby_field_name ) : if dicom_groups is None or not dicom_groups : raise ValueError ( 'Expected a boyle.dicom.sets.DicomFileSet.' ) if not os . path . exists ( folder_path ) : os . makedirs ( folder_path , exist_ok = False ) for dcmg in dicom_groups : if groupby_field_name is not None and len ( groupby_field_name ) > 0 : dfile = DicomFile ( dcmg ) dir_name = '' for att in groupby_field_name : dir_name = os . path . join ( dir_name , dfile . get_attributes ( att ) ) dir_name = str ( dir_name ) else : dir_name = os . path . basename ( dcmg ) group_folder = os . path . join ( folder_path , dir_name ) os . makedirs ( group_folder , exist_ok = False ) log . debug ( 'Copying files to {}.' . format ( group_folder ) ) import shutil dcm_files = dicom_groups [ dcmg ] for srcf in dcm_files : destf = os . path . join ( group_folder , os . path . basename ( srcf ) ) while os . path . exists ( destf ) : destf += '+' shutil . copy2 ( srcf , destf )
870	def getCustomDict ( cls ) : if not os . path . exists ( cls . getPath ( ) ) : return dict ( ) properties = Configuration . _readConfigFile ( os . path . basename ( cls . getPath ( ) ) , os . path . dirname ( cls . getPath ( ) ) ) values = dict ( ) for propName in properties : if 'value' in properties [ propName ] : values [ propName ] = properties [ propName ] [ 'value' ] return values
3729	def critical_surface ( Tc = None , Pc = None , Vc = None , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if ( Tc and Pc ) or ( Tc and Vc ) or ( Pc and Vc ) : methods . append ( IHMELS ) methods . append ( MEISSNER ) methods . append ( GRIGORAS ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section if Method == IHMELS : Third = Ihmels ( Tc = Tc , Pc = Pc , Vc = Vc ) elif Method == MEISSNER : Third = Meissner ( Tc = Tc , Pc = Pc , Vc = Vc ) elif Method == GRIGORAS : Third = Grigoras ( Tc = Tc , Pc = Pc , Vc = Vc ) elif Method == NONE : Third = None else : raise Exception ( 'Failure in in function' ) return Third
511	def _updateMinDutyCyclesGlobal ( self ) : self . _minOverlapDutyCycles . fill ( self . _minPctOverlapDutyCycles * self . _overlapDutyCycles . max ( ) )
8906	def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if self . collection . count_documents ( { 'name' : name } ) > 0 : name = namesgenerator . get_random_name ( retry = True ) # check if service is already registered if self . collection . count_documents ( { 'name' : name } ) > 0 : if overwrite : self . collection . delete_one ( { 'name' : name } ) else : raise Exception ( "service name already registered." ) self . collection . insert_one ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
8610	def get_resource ( self , resource_type , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/resources/%s/%s?depth=%s' % ( resource_type , resource_id , str ( depth ) ) ) return response
581	def _setRandomEncoderResolution ( minResolution = 0.001 ) : encoder = ( model_params . MODEL_PARAMS [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] [ "value" ] ) if encoder [ "type" ] == "RandomDistributedScalarEncoder" : rangePadding = abs ( _INPUT_MAX - _INPUT_MIN ) * 0.2 minValue = _INPUT_MIN - rangePadding maxValue = _INPUT_MAX + rangePadding resolution = max ( minResolution , ( maxValue - minValue ) / encoder . pop ( "numBuckets" ) ) encoder [ "resolution" ] = resolution
572	def rCopy ( d , f = identityConversion , discardNoneKeys = True , deepCopy = True ) : # Optionally deep copy the dict. if deepCopy : d = copy . deepcopy ( d ) newDict = { } toCopy = [ ( k , v , newDict , ( ) ) for k , v in d . iteritems ( ) ] while len ( toCopy ) > 0 : k , v , d , prevKeys = toCopy . pop ( ) prevKeys = prevKeys + ( k , ) if isinstance ( v , dict ) : d [ k ] = dict ( ) toCopy [ 0 : 0 ] = [ ( innerK , innerV , d [ k ] , prevKeys ) for innerK , innerV in v . iteritems ( ) ] else : #print k, v, prevKeys newV = f ( v , prevKeys ) if not discardNoneKeys or newV is not None : d [ k ] = newV return newDict
9760	def resources ( ctx , job , gpu ) : def get_experiment_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment . resources ( user , project_name , _experiment , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) def get_experiment_job_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment_job . resources ( user , project_name , _experiment , _job , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_resources ( ) else : get_experiment_resources ( )
4027	def create_local_copy ( cookie_file ) : # if type of cookie_file is a list, use the first element in the list if isinstance ( cookie_file , list ) : cookie_file = cookie_file [ 0 ] # check if cookie file exists if os . path . exists ( cookie_file ) : # copy to random name in tmp folder tmp_cookie_file = tempfile . NamedTemporaryFile ( suffix = '.sqlite' ) . name open ( tmp_cookie_file , 'wb' ) . write ( open ( cookie_file , 'rb' ) . read ( ) ) return tmp_cookie_file else : raise BrowserCookieError ( 'Can not find cookie file at: ' + cookie_file )
3251	def save ( self , obj , content_type = "application/xml" ) : rest_url = obj . href data = obj . message ( ) headers = { "Content-type" : content_type , "Accept" : content_type } logger . debug ( "{} {}" . format ( obj . save_method , obj . href ) ) resp = self . http_request ( rest_url , method = obj . save_method . lower ( ) , data = data , headers = headers ) if resp . status_code not in ( 200 , 201 ) : raise FailedRequestError ( 'Failed to save to Geoserver catalog: {}, {}' . format ( resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp
4664	def detail ( self , * args , * * kwargs ) : prefix = kwargs . pop ( "prefix" , default_prefix ) # remove dublicates kwargs [ "votes" ] = list ( set ( kwargs [ "votes" ] ) ) # # Sort votes # kwargs["votes"] = sorted( # kwargs["votes"], # key=lambda x: float(x.split(":")[1]), # ) return OrderedDict ( [ ( "memo_key" , PublicKey ( kwargs [ "memo_key" ] , prefix = prefix ) ) , ( "voting_account" , ObjectId ( kwargs [ "voting_account" ] , "account" ) ) , ( "num_witness" , Uint16 ( kwargs [ "num_witness" ] ) ) , ( "num_committee" , Uint16 ( kwargs [ "num_committee" ] ) ) , ( "votes" , Array ( [ VoteId ( o ) for o in kwargs [ "votes" ] ] ) ) , ( "extensions" , Set ( [ ] ) ) , ] )
7128	def inv_entry_to_path ( data ) : path_tuple = data [ 2 ] . split ( "#" ) if len ( path_tuple ) > 1 : path_str = "#" . join ( ( path_tuple [ 0 ] , path_tuple [ - 1 ] ) ) else : path_str = data [ 2 ] return path_str
9523	def scaffolds_to_contigs ( infile , outfile , number_contigs = False ) : seq_reader = sequences . file_reader ( infile ) fout = utils . open_file_write ( outfile ) for seq in seq_reader : contigs = seq . contig_coords ( ) counter = 1 for contig in contigs : if number_contigs : name = seq . id + '.' + str ( counter ) counter += 1 else : name = '.' . join ( [ seq . id , str ( contig . start + 1 ) , str ( contig . end + 1 ) ] ) print ( sequences . Fasta ( name , seq [ contig . start : contig . end + 1 ] ) , file = fout ) utils . close ( fout )
439	def print_params ( self , details = True , session = None ) : for i , p in enumerate ( self . all_params ) : if details : try : val = p . eval ( session = session ) logging . info ( " param {:3}: {:20} {:15} {} (mean: {:<18}, median: {:<18}, std: {:<18}) " . format ( i , p . name , str ( val . shape ) , p . dtype . name , val . mean ( ) , np . median ( val ) , val . std ( ) ) ) except Exception as e : logging . info ( str ( e ) ) raise Exception ( "Hint: print params details after tl.layers.initialize_global_variables(sess) " "or use network.print_params(False)." ) else : logging . info ( " param {:3}: {:20} {:15} {}" . format ( i , p . name , str ( p . get_shape ( ) ) , p . dtype . name ) ) logging . info ( " num of params: %d" % self . count_params ( ) )
8170	def separation ( self , r = 10 ) : vx = vy = vz = 0 for b in self . boids : if b != self : if abs ( self . x - b . x ) < r : vx += ( self . x - b . x ) if abs ( self . y - b . y ) < r : vy += ( self . y - b . y ) if abs ( self . z - b . z ) < r : vz += ( self . z - b . z ) return vx , vy , vz
7211	def stderr ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stderr.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stderr." ) wf = self . workflow . get ( self . id ) stderr_list = [ ] for task in wf [ 'tasks' ] : stderr_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stderr' : self . workflow . get_stderr ( self . id , task [ 'id' ] ) } ) return stderr_list
12825	def handle_extends ( self , text ) : match = self . re_extends . match ( text ) if match : extra_text = self . re_extends . sub ( '' , text , count = 1 ) blocks = self . get_blocks ( extra_text ) path = os . path . join ( self . base_dir , match . group ( 'path' ) ) with open ( path , encoding = 'utf-8' ) as fp : return self . replace_blocks_in_extends ( fp . read ( ) , blocks ) else : return None
4382	def is_allowed ( self , role , method , resource ) : return ( role , method , resource ) in self . _allowed
4903	def link_to_modal ( link_text , index , autoescape = True ) : # pylint: disable=unused-argument link = ( '<a' ' href="#!"' ' class="text-underline view-course-details-link"' ' id="view-course-details-link-{index}"' ' data-toggle="modal"' ' data-target="#course-details-modal-{index}"' '>{link_text}</a>' ) . format ( index = index , link_text = link_text , ) return mark_safe ( link )
11897	def _create_index_files ( root_dir , force_no_processing = False ) : # Initialise list of created file paths to build up as we make them created_files = [ ] # Walk the root dir downwards, creating index files as we go for here , dirs , files in os . walk ( root_dir ) : print ( 'Processing %s' % here ) # Sort the subdirectories by name dirs = sorted ( dirs ) # Get image files - all files in the directory matching IMAGE_FILE_REGEX image_files = [ f for f in files if re . match ( IMAGE_FILE_REGEX , f ) ] # Sort the image files by name image_files = sorted ( image_files ) # Create this directory's index file and add its name to the created # files list created_files . append ( _create_index_file ( root_dir , here , image_files , dirs , force_no_processing ) ) # Return the list of created files return created_files
2859	def transfer ( self , data ) : #check for hardware limit of FT232H and similar MPSSE chips if ( len ( data ) > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) # Build command to read and write SPI data. command = 0x30 | ( self . lsbfirst << 3 ) | ( self . read_clock_ve << 2 ) | self . write_clock_ve logger . debug ( 'SPI transfer with command {0:2X}.' . format ( command ) ) # Compute length low and high bytes. # NOTE: Must actually send length minus one because the MPSSE engine # considers 0 a length of 1 and FFFF a length of 65536 data1 = data [ : len ( data ) / 2 ] data2 = data [ len ( data ) / 2 : ] len_low1 = ( len ( data1 ) - 1 ) & 0xFF len_high1 = ( ( len ( data1 ) - 1 ) >> 8 ) & 0xFF len_low2 = ( len ( data2 ) - 1 ) & 0xFF len_high2 = ( ( len ( data2 ) - 1 ) >> 8 ) & 0xFF payload1 = '' payload2 = '' #start command set self . _assert_cs ( ) # Perform twice to prevent error from hardware defect/limits # Send command and length, then data, split into two commands, handle for length 1 if len ( data1 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low1 , len_high1 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data1 ) ) ) payload1 = self . _ft232h . _poll_read ( len ( data1 ) ) if len ( data2 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low2 , len_high2 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data2 ) ) ) payload2 = self . _ft232h . _poll_read ( len ( data2 ) ) #self._ft232h._write('\x87') self . _deassert_cs ( ) # Read response bytes. return bytearray ( payload1 + payload2 )
10676	def Cp ( compound_string , T , mass = 1.0 ) : formula , phase = _split_compound_string_ ( compound_string ) TK = T + 273.15 compound = compounds [ formula ] result = compound . Cp ( phase , TK ) return _finalise_result_ ( compound , result , mass )
8444	def update ( check , enter_parameters , version ) : if check : if temple . update . up_to_date ( version = version ) : print ( 'Temple package is up to date' ) else : msg = ( 'This temple package is out of date with the latest template.' ' Update your package by running "temple update" and commiting changes.' ) raise temple . exceptions . NotUpToDateWithTemplateError ( msg ) else : temple . update . update ( new_version = version , enter_parameters = enter_parameters )
4701	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.lnvm.env: invalid SSH environment" ) return 1 lnvm = cij . env_to_dict ( PREFIX , REQUIRED ) nvme = cij . env_to_dict ( "NVME" , [ "DEV_NAME" ] ) if "BGN" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_BGN" ) return 1 if "END" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_END" ) return 1 if "DEV_TYPE" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_DEV_TYPE" ) return 1 lnvm [ "DEV_NAME" ] = "%sb%03de%03d" % ( nvme [ "DEV_NAME" ] , int ( lnvm [ "BGN" ] ) , int ( lnvm [ "END" ] ) ) lnvm [ "DEV_PATH" ] = "/dev/%s" % lnvm [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , lnvm ) return 0
4124	def data_cosine ( N = 1024 , A = 0.1 , sampling = 1024. , freq = 200 ) : t = arange ( 0 , float ( N ) / sampling , 1. / sampling ) x = cos ( 2. * pi * t * freq ) + A * randn ( t . size ) return x
3781	def calculate ( self , T , method ) : if method == TEST_METHOD_1 : prop = self . TEST_METHOD_1_coeffs [ 0 ] + self . TEST_METHOD_1_coeffs [ 1 ] * T elif method == TEST_METHOD_2 : prop = self . TEST_METHOD_2_coeffs [ 0 ] + self . TEST_METHOD_2_coeffs [ 1 ] * T elif method in self . tabular_data : prop = self . interpolate ( T , method ) return prop
7233	def aggregate_query ( self , searchAreaWkt , agg_def , query = None , start_date = None , end_date = None , count = 10 , index = default_index ) : geojson = load_wkt ( searchAreaWkt ) . __geo_interface__ aggs_str = str ( agg_def ) # could be string or AggregationDef params = { "count" : count , "aggs" : aggs_str } if query : params [ 'query' ] = query if start_date : params [ 'start_date' ] = start_date if end_date : params [ 'end_date' ] = end_date url = self . aggregations_by_index_url % index if index else self . aggregations_url r = self . gbdx_connection . post ( url , params = params , json = geojson ) r . raise_for_status ( ) return r . json ( object_pairs_hook = OrderedDict ) [ 'aggregations' ]
11101	def select_by_ctime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . ctime <= max_time return self . select_file ( filters , recursive )
6688	def groupinstall ( group , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , str ) : options = [ options ] options = " " . join ( options ) run_as_root ( '%(manager)s %(options)s groupinstall "%(group)s"' % locals ( ) , pty = False )
5703	def get_vehicle_hours_by_type ( gtfs , route_type ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT * , SUM(end_time_ds - start_time_ds)/3600 as vehicle_hours_type" " FROM" " (SELECT * FROM day_trips as q1" " INNER JOIN" " (SELECT route_I, type FROM routes) as q2" " ON q1.route_I = q2.route_I" " WHERE type = {route_type}" " AND date = '{day}')" . format ( day = day , route_type = route_type ) ) df = gtfs . execute_custom_query_pandas ( query ) return df [ 'vehicle_hours_type' ] . item ( )
2721	def get_object ( cls , api_token , droplet_id ) : droplet = cls ( token = api_token , id = droplet_id ) droplet . load ( ) return droplet
2774	def save ( self ) : forwarding_rules = [ rule . __dict__ for rule in self . forwarding_rules ] data = { 'name' : self . name , 'region' : self . region [ 'slug' ] , 'forwarding_rules' : forwarding_rules , 'redirect_http_to_https' : self . redirect_http_to_https } if self . tag : data [ 'tag' ] = self . tag else : data [ 'droplet_ids' ] = self . droplet_ids if self . algorithm : data [ "algorithm" ] = self . algorithm if self . health_check : data [ 'health_check' ] = self . health_check . __dict__ if self . sticky_sessions : data [ 'sticky_sessions' ] = self . sticky_sessions . __dict__ return self . get_data ( "load_balancers/%s/" % self . id , type = PUT , params = data )
1878	def MOVSS ( cpu , dest , src ) : if dest . type == 'register' and src . type == 'register' : assert dest . size == 128 and src . size == 128 dest . write ( dest . read ( ) & ~ 0xffffffff | src . read ( ) & 0xffffffff ) elif dest . type == 'memory' : assert src . type == 'register' dest . write ( Operators . EXTRACT ( src . read ( ) , 0 , dest . size ) ) else : assert src . type == 'memory' and dest . type == 'register' assert src . size == 32 and dest . size == 128 dest . write ( Operators . ZEXTEND ( src . read ( ) , 128 ) )
5946	def in_dir ( directory , create = True ) : startdir = os . getcwd ( ) try : try : os . chdir ( directory ) logger . debug ( "Working in {directory!r}..." . format ( * * vars ( ) ) ) except OSError as err : if create and err . errno == errno . ENOENT : os . makedirs ( directory ) os . chdir ( directory ) logger . info ( "Working in {directory!r} (newly created)..." . format ( * * vars ( ) ) ) else : logger . exception ( "Failed to start working in {directory!r}." . format ( * * vars ( ) ) ) raise yield os . getcwd ( ) finally : os . chdir ( startdir )
8861	def defined_names ( request_data ) : global _old_definitions ret_val = [ ] path = request_data [ 'path' ] toplvl_definitions = jedi . names ( request_data [ 'code' ] , path , 'utf-8' ) for d in toplvl_definitions : definition = _extract_def ( d , path ) if d . type != 'import' : ret_val . append ( definition ) ret_val = [ d . to_dict ( ) for d in ret_val ] return ret_val
8391	def check_visitors ( cls ) : for name in dir ( cls ) : if name . startswith ( "visit_" ) : if name [ 6 : ] not in CLASS_NAMES : raise Exception ( u"Method {} doesn't correspond to a node class" . format ( name ) ) return cls
4148	def centerdc_gen ( self ) : for a in range ( 0 , self . N ) : yield ( a - self . N / 2 ) * self . df
2828	def convert_selu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting selu ...' ) if names == 'short' : tf_name = 'SELU' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) selu = keras . layers . Activation ( 'selu' , name = tf_name ) layers [ scope_name ] = selu ( layers [ inputs [ 0 ] ] )
11997	def verify_signature ( self , data ) : data = self . _remove_magic ( data ) data = urlsafe_nopadding_b64decode ( data ) options = self . _read_header ( data ) data = self . _add_magic ( data ) self . _unsign_data ( data , options )
1875	def PSUBB ( cpu , dest , src ) : result = [ ] value_a = dest . read ( ) value_b = src . read ( ) for i in reversed ( range ( 0 , dest . size , 8 ) ) : a = Operators . EXTRACT ( value_a , i , 8 ) b = Operators . EXTRACT ( value_b , i , 8 ) result . append ( ( a - b ) & 0xff ) dest . write ( Operators . CONCAT ( 8 * len ( result ) , * result ) )
9672	def iteration ( self ) : i = 0 conv = np . inf old_conv = - np . inf conv_list = [ ] m = self . original # If the original data input is in pandas DataFrame format if isinstance ( self . original , pd . DataFrame ) : ipfn_method = self . ipfn_df elif isinstance ( self . original , np . ndarray ) : ipfn_method = self . ipfn_np self . original = self . original . astype ( 'float64' ) else : print ( 'Data input instance not recognized' ) sys . exit ( 0 ) while ( ( i <= self . max_itr and conv > self . conv_rate ) and ( i <= self . max_itr and abs ( conv - old_conv ) > self . rate_tolerance ) ) : old_conv = conv m , conv = ipfn_method ( m , self . aggregates , self . dimensions , self . weight_col ) conv_list . append ( conv ) i += 1 converged = 1 if i <= self . max_itr : if not conv > self . conv_rate : print ( 'ipfn converged: convergence_rate below threshold' ) elif not abs ( conv - old_conv ) > self . rate_tolerance : print ( 'ipfn converged: convergence_rate not updating or below rate_tolerance' ) else : print ( 'Maximum iterations reached' ) converged = 0 # Handle the verbose if self . verbose == 0 : return m elif self . verbose == 1 : return m , converged elif self . verbose == 2 : return m , converged , pd . DataFrame ( { 'iteration' : range ( i ) , 'conv' : conv_list } ) . set_index ( 'iteration' ) else : print ( 'wrong verbose input, return None' ) sys . exit ( 0 )
12748	def load_skel ( self , source , * * kwargs ) : logging . info ( '%s: parsing skeleton configuration' , source ) if hasattr ( source , 'read' ) : p = parser . parse ( source , self . world , self . jointgroup , * * kwargs ) else : with open ( source ) as handle : p = parser . parse ( handle , self . world , self . jointgroup , * * kwargs ) self . bodies = p . bodies self . joints = p . joints self . set_pid_params ( kp = 0.999 / self . world . dt )
11262	def sub ( prev , pattern , repl , * args , * * kw ) : count = 0 if 'count' not in kw else kw . pop ( 'count' ) pattern_obj = re . compile ( pattern , * args , * * kw ) for s in prev : yield pattern_obj . sub ( repl , s , count = count )
7770	def base_handlers_factory ( self ) : tls_handler = StreamTLSHandler ( self . settings ) sasl_handler = StreamSASLHandler ( self . settings ) session_handler = SessionHandler ( ) binding_handler = ResourceBindingHandler ( self . settings ) return [ tls_handler , sasl_handler , binding_handler , session_handler ]
7238	def iterwindows ( self , count = 64 , window_shape = ( 256 , 256 ) ) : if count is None : while True : yield self . randwindow ( window_shape ) else : for i in xrange ( count ) : yield self . randwindow ( window_shape )
10561	def _mutagen_fields_to_single_value ( metadata ) : return dict ( ( k , v [ 0 ] ) for k , v in metadata . items ( ) if v )
1014	def _getBestMatchingSegment ( self , c , i , activeState ) : maxActivity , which = self . minThreshold , - 1 for j , s in enumerate ( self . cells [ c ] [ i ] ) : activity = self . _getSegmentActivityLevel ( s , activeState , connectedSynapsesOnly = False ) if activity >= maxActivity : maxActivity , which = activity , j if which == - 1 : return None else : return self . cells [ c ] [ i ] [ which ]
6690	def repolist ( status = '' , media = None ) : manager = MANAGER with settings ( hide ( 'running' , 'stdout' ) ) : if media : repos = run_as_root ( "%(manager)s repolist %(status)s | sed '$d' | sed -n '/repo id/,$p'" % locals ( ) ) else : repos = run_as_root ( "%(manager)s repolist %(status)s | sed '/Media\\|Debug/d' | sed '$d' | sed -n '/repo id/,$p'" % locals ( ) ) return [ line . split ( ' ' ) [ 0 ] for line in repos . splitlines ( ) [ 1 : ] ]
701	def firstNonFullGeneration ( self , swarmId , minNumParticles ) : if not swarmId in self . _swarmNumParticlesPerGeneration : return None numPsPerGen = self . _swarmNumParticlesPerGeneration [ swarmId ] numPsPerGen = numpy . array ( numPsPerGen ) firstNonFull = numpy . where ( numPsPerGen < minNumParticles ) [ 0 ] if len ( firstNonFull ) == 0 : return len ( numPsPerGen ) else : return firstNonFull [ 0 ]
492	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) dbConn = self . _pool . connection ( shareable = False ) connWrap = ConnectionWrapper ( dbConn = dbConn , cursor = dbConn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
10346	def merge_namespaces ( input_locations , output_path , namespace_name , namespace_keyword , namespace_domain , author_name , citation_name , namespace_description = None , namespace_species = None , namespace_version = None , namespace_query_url = None , namespace_created = None , author_contact = None , author_copyright = None , citation_description = None , citation_url = None , citation_version = None , citation_date = None , case_sensitive = True , delimiter = '|' , cacheable = True , functions = None , value_prefix = '' , sort_key = None , check_keywords = True ) : results = get_merged_namespace_names ( input_locations , check_keywords = check_keywords ) with open ( output_path , 'w' ) as file : write_namespace ( namespace_name = namespace_name , namespace_keyword = namespace_keyword , namespace_domain = namespace_domain , author_name = author_name , citation_name = citation_name , values = results , namespace_species = namespace_species , namespace_description = namespace_description , namespace_query_url = namespace_query_url , namespace_version = namespace_version , namespace_created = namespace_created , author_contact = author_contact , author_copyright = author_copyright , citation_description = citation_description , citation_url = citation_url , citation_version = citation_version , citation_date = citation_date , case_sensitive = case_sensitive , delimiter = delimiter , cacheable = cacheable , functions = functions , value_prefix = value_prefix , sort_key = sort_key , file = file )
13656	def routedResource ( f , routerAttribute = 'router' ) : return wraps ( f ) ( lambda * a , * * kw : getattr ( f ( * a , * * kw ) , routerAttribute ) . resource ( ) )
4881	def delete_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . filter ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH ) . delete ( )
10762	def _wait_for_connection ( self , port ) : connected = False max_tries = 10 num_tries = 0 wait_time = 0.5 while not connected or num_tries >= max_tries : time . sleep ( wait_time ) try : af = socket . AF_INET addr = ( '127.0.0.1' , port ) sock = socket . socket ( af , socket . SOCK_STREAM ) sock . connect ( addr ) except socket . error : if sock : sock . close ( ) num_tries += 1 continue connected = True if not connected : print ( "Error connecting to sphinx searchd" , file = sys . stderr )
1750	def mappings ( self ) : result = [ ] for m in self . maps : if isinstance ( m , AnonMap ) : result . append ( ( m . start , m . end , m . perms , 0 , '' ) ) elif isinstance ( m , FileMap ) : result . append ( ( m . start , m . end , m . perms , m . _offset , m . _filename ) ) else : result . append ( ( m . start , m . end , m . perms , 0 , m . name ) ) return sorted ( result )
11141	def load_repository ( self , path , verbose = True , ntrials = 3 ) : assert isinstance ( ntrials , int ) , "ntrials must be integer" assert ntrials > 0 , "ntrials must be >0" repo = None for _trial in range ( ntrials ) : try : self . __load_repository ( path = path , verbose = True ) except Exception as err1 : try : from . OldRepository import Repository REP = Repository ( path ) except Exception as err2 : #traceback.print_exc() error = "Unable to load repository using neiher new style (%s) nor old style (%s)" % ( err1 , err2 ) if self . DEBUG_PRINT_FAILED_TRIALS : print ( "Trial %i failed in Repository.%s (%s). Set Repository.DEBUG_PRINT_FAILED_TRIALS to False to mute" % ( _trial , inspect . stack ( ) [ 1 ] [ 3 ] , str ( error ) ) ) else : error = None repo = REP break else : error = None repo = self break # check and return assert error is None , error return repo
5677	def get_trip_trajectories_within_timespan ( self , start , end , use_shapes = True , filter_name = None ) : trips = [ ] trip_df = self . get_tripIs_active_in_range ( start , end ) print ( "gtfs_viz.py: fetched " + str ( len ( trip_df ) ) + " trip ids" ) shape_cache = { } # loop over all trips: for row in trip_df . itertuples ( ) : trip_I = row . trip_I day_start_ut = row . day_start_ut shape_id = row . shape_id trip = { } name , route_type = self . get_route_name_and_type_of_tripI ( trip_I ) trip [ 'route_type' ] = int ( route_type ) trip [ 'name' ] = str ( name ) if filter_name and ( name != filter_name ) : continue stop_lats = [ ] stop_lons = [ ] stop_dep_times = [ ] shape_breaks = [ ] stop_seqs = [ ] # get stop_data and store it: stop_time_df = self . get_trip_stop_time_data ( trip_I , day_start_ut ) for stop_row in stop_time_df . itertuples ( ) : stop_lats . append ( float ( stop_row . lat ) ) stop_lons . append ( float ( stop_row . lon ) ) stop_dep_times . append ( float ( stop_row . dep_time_ut ) ) try : stop_seqs . append ( int ( stop_row . seq ) ) except TypeError : stop_seqs . append ( None ) if use_shapes : try : shape_breaks . append ( int ( stop_row . shape_break ) ) except ( TypeError , ValueError ) : shape_breaks . append ( None ) if use_shapes : # get shape data (from cache, if possible) if shape_id not in shape_cache : shape_cache [ shape_id ] = shapes . get_shape_points2 ( self . conn . cursor ( ) , shape_id ) shape_data = shape_cache [ shape_id ] # noinspection PyBroadException try : trip [ 'times' ] = shapes . interpolate_shape_times ( shape_data [ 'd' ] , shape_breaks , stop_dep_times ) trip [ 'lats' ] = shape_data [ 'lats' ] trip [ 'lons' ] = shape_data [ 'lons' ] start_break = shape_breaks [ 0 ] end_break = shape_breaks [ - 1 ] trip [ 'times' ] = trip [ 'times' ] [ start_break : end_break + 1 ] trip [ 'lats' ] = trip [ 'lats' ] [ start_break : end_break + 1 ] trip [ 'lons' ] = trip [ 'lons' ] [ start_break : end_break + 1 ] except : # In case interpolation fails: trip [ 'times' ] = stop_dep_times trip [ 'lats' ] = stop_lats trip [ 'lons' ] = stop_lons else : trip [ 'times' ] = stop_dep_times trip [ 'lats' ] = stop_lats trip [ 'lons' ] = stop_lons trips . append ( trip ) return { "trips" : trips }
6369	def recall ( self ) : if self . _tp + self . _fn == 0 : return float ( 'NaN' ) return self . _tp / ( self . _tp + self . _fn )
4053	def all_collections ( self , collid = None ) : all_collections = [ ] def subcoll ( clct ) : """ recursively add collections to a flat master list """ all_collections . append ( clct ) if clct [ "meta" ] . get ( "numCollections" , 0 ) > 0 : # add collection to master list & recur with all child # collections [ subcoll ( c ) for c in self . everything ( self . collections_sub ( clct [ "data" ] [ "key" ] ) ) ] # select all top-level collections or a specific collection and # children if collid : toplevel = [ self . collection ( collid ) ] else : toplevel = self . everything ( self . collections_top ( ) ) [ subcoll ( collection ) for collection in toplevel ] return all_collections
4409	async def connect ( self , channel_id : int ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , str ( channel_id ) )
6890	def read_csv_lightcurve ( lcfile ) : # read in the file first if '.gz' in os . path . basename ( lcfile ) : LOGINFO ( 'reading gzipped K2 LC: %s' % lcfile ) infd = gzip . open ( lcfile , 'rb' ) else : LOGINFO ( 'reading K2 LC: %s' % lcfile ) infd = open ( lcfile , 'rb' ) lctext = infd . read ( ) . decode ( ) infd . close ( ) # figure out the header and get the LC columns lcstart = lctext . index ( '# LIGHTCURVE\n' ) lcheader = lctext [ : lcstart + 12 ] lccolumns = lctext [ lcstart + 13 : ] . split ( '\n' ) lccolumns = [ x . split ( ',' ) for x in lccolumns if len ( x ) > 0 ] # initialize the lcdict and parse the CSV header lcdict = _parse_csv_header ( lcheader ) # tranpose the LC rows into columns lccolumns = list ( zip ( * lccolumns ) ) # write the columns to the dict for colind , col in enumerate ( lcdict [ 'columns' ] ) : # this picks out the caster to use when reading each column using the # definitions in the lcutils.COLUMNDEFS dictionary lcdict [ col . lower ( ) ] = np . array ( [ COLUMNDEFS [ col ] [ 2 ] ( x ) for x in lccolumns [ colind ] ] ) lcdict [ 'columns' ] = [ x . lower ( ) for x in lcdict [ 'columns' ] ] return lcdict
4792	def is_unicode ( self ) : if type ( self . val ) is not unicode : self . _err ( 'Expected <%s> to be unicode, but was <%s>.' % ( self . val , type ( self . val ) . __name__ ) ) return self
8851	def open_file ( self , path , line = None ) : editor = None if path : interpreter , pyserver , args = self . _get_backend_parameters ( ) editor = self . tabWidget . open_document ( path , None , interpreter = interpreter , server_script = pyserver , args = args ) if editor : self . setup_editor ( editor ) self . recent_files_manager . open_file ( path ) self . menu_recents . update_actions ( ) if line is not None : TextHelper ( self . tabWidget . current_widget ( ) ) . goto_line ( line ) return editor
4464	def load_jam_audio ( jam_in , audio_file , validate = True , strict = True , fmt = 'auto' , * * kwargs ) : if isinstance ( jam_in , jams . JAMS ) : jam = jam_in else : jam = jams . load ( jam_in , validate = validate , strict = strict , fmt = fmt ) y , sr = librosa . load ( audio_file , * * kwargs ) if jam . file_metadata . duration is None : jam . file_metadata . duration = librosa . get_duration ( y = y , sr = sr ) return jam_pack ( jam , _audio = dict ( y = y , sr = sr ) )
374	def illumination ( x , gamma = 1. , contrast = 1. , saturation = 1. , is_random = False ) : if is_random : if not ( len ( gamma ) == len ( contrast ) == len ( saturation ) == 2 ) : raise AssertionError ( "if is_random = True, the arguments are (min, max)" ) ## random change brightness # small --> brighter illum_settings = np . random . randint ( 0 , 3 ) # 0-brighter, 1-darker, 2 keep normal if illum_settings == 0 : # brighter gamma = np . random . uniform ( gamma [ 0 ] , 1.0 ) # (.5, 1.0) elif illum_settings == 1 : # darker gamma = np . random . uniform ( 1.0 , gamma [ 1 ] ) # (1.0, 5.0) else : gamma = 1 im_ = brightness ( x , gamma = gamma , gain = 1 , is_random = False ) # tl.logging.info("using contrast and saturation") image = PIL . Image . fromarray ( im_ ) # array -> PIL contrast_adjust = PIL . ImageEnhance . Contrast ( image ) image = contrast_adjust . enhance ( np . random . uniform ( contrast [ 0 ] , contrast [ 1 ] ) ) #0.3,0.9)) saturation_adjust = PIL . ImageEnhance . Color ( image ) image = saturation_adjust . enhance ( np . random . uniform ( saturation [ 0 ] , saturation [ 1 ] ) ) # (0.7,1.0)) im_ = np . array ( image ) # PIL -> array else : im_ = brightness ( x , gamma = gamma , gain = 1 , is_random = False ) image = PIL . Image . fromarray ( im_ ) # array -> PIL contrast_adjust = PIL . ImageEnhance . Contrast ( image ) image = contrast_adjust . enhance ( contrast ) saturation_adjust = PIL . ImageEnhance . Color ( image ) image = saturation_adjust . enhance ( saturation ) im_ = np . array ( image ) # PIL -> array return np . asarray ( im_ )
6335	def sim ( self , src , tar , * args , * * kwargs ) : return 1.0 - self . dist ( src , tar , * args , * * kwargs )
13559	def get_top_assets ( self ) : images = self . get_all_images ( ) [ 0 : 14 ] video = [ ] if supports_video : video = self . eventvideo_set . all ( ) [ 0 : 10 ] return list ( chain ( images , video ) ) [ 0 : 15 ]
302	def plot_daily_volume ( returns , transactions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) daily_txn = txn . get_txn_vol ( transactions ) daily_txn . txn_shares . plot ( alpha = 1.0 , lw = 0.5 , ax = ax , * * kwargs ) ax . axhline ( daily_txn . txn_shares . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 , alpha = 1.0 ) ax . set_title ( 'Daily trading volume' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_ylabel ( 'Amount of shares traded' ) ax . set_xlabel ( '' ) return ax
10740	def add_runtime ( function ) : def wrapper ( * args , * * kwargs ) : pr = cProfile . Profile ( ) pr . enable ( ) output = function ( * args , * * kwargs ) pr . disable ( ) return pr , output return wrapper
8406	def zero_range ( x , tol = np . finfo ( float ) . eps * 100 ) : try : if len ( x ) == 1 : return True except TypeError : return True if len ( x ) != 2 : raise ValueError ( 'x must be length 1 or 2' ) # Deals with array_likes that have non-standard indices x = tuple ( x ) # datetime - pandas, cpython if isinstance ( x [ 0 ] , ( pd . Timestamp , datetime . datetime ) ) : # date2num include timezone info, .toordinal() does not x = date2num ( x ) # datetime - numpy elif isinstance ( x [ 0 ] , np . datetime64 ) : return x [ 0 ] == x [ 1 ] # timedelta - pandas, cpython elif isinstance ( x [ 0 ] , ( pd . Timedelta , datetime . timedelta ) ) : x = x [ 0 ] . total_seconds ( ) , x [ 1 ] . total_seconds ( ) # timedelta - numpy elif isinstance ( x [ 0 ] , np . timedelta64 ) : return x [ 0 ] == x [ 1 ] elif not isinstance ( x [ 0 ] , ( float , int , np . number ) ) : raise TypeError ( "zero_range objects cannot work with objects " "of type '{}'" . format ( type ( x [ 0 ] ) ) ) if any ( np . isnan ( x ) ) : return np . nan if x [ 0 ] == x [ 1 ] : return True if all ( np . isinf ( x ) ) : return False m = np . abs ( x ) . min ( ) if m == 0 : return False return np . abs ( ( x [ 0 ] - x [ 1 ] ) / m ) < tol
4484	def copyfileobj ( fsrc , fdst , total , length = 16 * 1024 ) : with tqdm ( unit = 'bytes' , total = total , unit_scale = True ) as pbar : while 1 : buf = fsrc . read ( length ) if not buf : break fdst . write ( buf ) pbar . update ( len ( buf ) )
9067	def _lml_optimal_scale ( self ) : assert self . _optimal [ "scale" ] n = len ( self . _y ) lml = - self . _df * log2pi - self . _df - n * log ( self . scale ) lml -= sum ( npsum ( log ( D ) ) for D in self . _D ) return lml / 2
5813	def detect_other_protocol ( server_handshake_bytes ) : if server_handshake_bytes [ 0 : 5 ] == b'HTTP/' : return 'HTTP' if server_handshake_bytes [ 0 : 4 ] == b'220 ' : if re . match ( b'^[^\r\n]*ftp' , server_handshake_bytes , re . I ) : return 'FTP' else : return 'SMTP' if server_handshake_bytes [ 0 : 4 ] == b'220-' : return 'FTP' if server_handshake_bytes [ 0 : 4 ] == b'+OK ' : return 'POP3' if server_handshake_bytes [ 0 : 4 ] == b'* OK' or server_handshake_bytes [ 0 : 9 ] == b'* PREAUTH' : return 'IMAP' return None
10816	def is_member ( self , user , with_pending = False ) : m = Membership . get ( self , user ) if m is not None : if with_pending : return True elif m . state == MembershipState . ACTIVE : return True return False
11263	def wildcard ( prev , pattern , * args , * * kw ) : import fnmatch inv = 'inv' in kw and kw . pop ( 'inv' ) pattern_obj = re . compile ( fnmatch . translate ( pattern ) , * args , * * kw ) if not inv : for data in prev : if pattern_obj . match ( data ) : yield data else : for data in prev : if not pattern_obj . match ( data ) : yield data
2217	def _list_itemstrs ( list_ , * * kwargs ) : items = list ( list_ ) kwargs [ '_return_info' ] = True _tups = [ repr2 ( item , * * kwargs ) for item in items ] itemstrs = [ t [ 0 ] for t in _tups ] max_height = max ( [ t [ 1 ] [ 'max_height' ] for t in _tups ] ) if _tups else 0 _leaf_info = { 'max_height' : max_height + 1 , } sort = kwargs . get ( 'sort' , None ) if sort is None : # Force orderings on sets. sort = isinstance ( list_ , ( set , frozenset ) ) if sort : itemstrs = _sort_itemstrs ( items , itemstrs ) return itemstrs , _leaf_info
8779	def _try_allocate ( self , context , segment_id , network_id ) : LOG . info ( "Attempting to allocate segment for network %s " "segment_id %s segment_type %s" % ( network_id , segment_id , self . segment_type ) ) filter_dict = { "segment_id" : segment_id , "segment_type" : self . segment_type , "do_not_use" : False } available_ranges = db_api . segment_allocation_range_find ( context , scope = db_api . ALL , * * filter_dict ) available_range_ids = [ r [ "id" ] for r in available_ranges ] try : with context . session . begin ( subtransactions = True ) : # Search for any deallocated segment ids for the # given segment. filter_dict = { "deallocated" : True , "segment_id" : segment_id , "segment_type" : self . segment_type , "segment_allocation_range_ids" : available_range_ids } # NOTE(morgabra) We select 100 deallocated segment ids from # the table here, and then choose 1 randomly. This is to help # alleviate the case where an uncaught exception might leave # an allocation active on a remote service but we do not have # a record of it locally. If we *do* end up choosing a # conflicted id, the caller should simply allocate another one # and mark them all as reserved. If a single object has # multiple reservations on the same segment, they will not be # deallocated, and the operator must resolve the conficts # manually. allocations = db_api . segment_allocation_find ( context , lock_mode = True , * * filter_dict ) . limit ( 100 ) . all ( ) if allocations : allocation = random . choice ( allocations ) # Allocate the chosen segment. update_dict = { "deallocated" : False , "deallocated_at" : None , "network_id" : network_id } allocation = db_api . segment_allocation_update ( context , allocation , * * update_dict ) LOG . info ( "Allocated segment %s for network %s " "segment_id %s segment_type %s" % ( allocation [ "id" ] , network_id , segment_id , self . segment_type ) ) return allocation except Exception : LOG . exception ( "Error in segment reallocation." ) LOG . info ( "Cannot find reallocatable segment for network %s " "segment_id %s segment_type %s" % ( network_id , segment_id , self . segment_type ) )
13215	def dump ( self , name , filename ) : if not self . exists ( name ) : raise DatabaseError ( 'database %s does not exist!' ) log . info ( 'dumping %s to %s' % ( name , filename ) ) self . _run_cmd ( 'pg_dump' , '--verbose' , '--blobs' , '--format=custom' , '--file=%s' % filename , name )
9959	def restore_python ( self ) : orig = self . orig_settings sys . setrecursionlimit ( orig [ "sys.recursionlimit" ] ) if "sys.tracebacklimit" in orig : sys . tracebacklimit = orig [ "sys.tracebacklimit" ] else : if hasattr ( sys , "tracebacklimit" ) : del sys . tracebacklimit if "showwarning" in orig : warnings . showwarning = orig [ "showwarning" ] orig . clear ( ) threading . stack_size ( )
5213	def intraday ( ticker , dt , session = '' , * * kwargs ) -> pd . DataFrame : from xbbg . core import intervals cur_data = bdib ( ticker = ticker , dt = dt , typ = kwargs . get ( 'typ' , 'TRADE' ) ) if cur_data . empty : return pd . DataFrame ( ) fmt = '%H:%M:%S' ss = intervals . SessNA ref = kwargs . get ( 'ref' , None ) exch = pd . Series ( ) if ref is None else const . exch_info ( ticker = ref ) if session : ss = intervals . get_interval ( ticker = kwargs . get ( 'ref' , ticker ) , session = session ) start_time = kwargs . get ( 'start_time' , None ) end_time = kwargs . get ( 'end_time' , None ) if ss != intervals . SessNA : start_time = pd . Timestamp ( ss . start_time ) . strftime ( fmt ) end_time = pd . Timestamp ( ss . end_time ) . strftime ( fmt ) if start_time and end_time : kw = dict ( start_time = start_time , end_time = end_time ) if not exch . empty : cur_tz = cur_data . index . tz res = cur_data . tz_convert ( exch . tz ) . between_time ( * * kw ) if kwargs . get ( 'keep_tz' , False ) : res = res . tz_convert ( cur_tz ) return pd . DataFrame ( res ) return pd . DataFrame ( cur_data . between_time ( * * kw ) ) return cur_data
10720	def get_parser ( ) : parser = argparse . ArgumentParser ( ) parser . add_argument ( "package" , choices = arg_map . keys ( ) , help = "designates the package to test" ) parser . add_argument ( "--ignore" , help = "ignore these files" ) return parser
6125	def plot_image ( image , plot_origin = True , mask = None , extract_array_from_mask = False , zoom_around_mask = False , should_plot_border = False , positions = None , as_subplot = False , units = 'arcsec' , kpc_per_arcsec = None , figsize = ( 7 , 7 ) , aspect = 'square' , cmap = 'jet' , norm = 'linear' , norm_min = None , norm_max = None , linthresh = 0.05 , linscale = 0.01 , cb_ticksize = 10 , cb_fraction = 0.047 , cb_pad = 0.01 , cb_tick_values = None , cb_tick_labels = None , title = 'Image' , titlesize = 16 , xlabelsize = 16 , ylabelsize = 16 , xyticksize = 16 , mask_pointsize = 10 , position_pointsize = 30 , grid_pointsize = 1 , output_path = None , output_format = 'show' , output_filename = 'image' ) : origin = get_origin ( array = image , plot_origin = plot_origin ) array_plotters . plot_array ( array = image , origin = origin , mask = mask , extract_array_from_mask = extract_array_from_mask , zoom_around_mask = zoom_around_mask , should_plot_border = should_plot_border , positions = positions , as_subplot = as_subplot , units = units , kpc_per_arcsec = kpc_per_arcsec , figsize = figsize , aspect = aspect , cmap = cmap , norm = norm , norm_min = norm_min , norm_max = norm_max , linthresh = linthresh , linscale = linscale , cb_ticksize = cb_ticksize , cb_fraction = cb_fraction , cb_pad = cb_pad , cb_tick_values = cb_tick_values , cb_tick_labels = cb_tick_labels , title = title , titlesize = titlesize , xlabelsize = xlabelsize , ylabelsize = ylabelsize , xyticksize = xyticksize , mask_pointsize = mask_pointsize , position_pointsize = position_pointsize , grid_pointsize = grid_pointsize , output_path = output_path , output_format = output_format , output_filename = output_filename )
4547	def bresenham_line ( setter , x0 , y0 , x1 , y1 , color = None , colorFunc = None ) : steep = abs ( y1 - y0 ) > abs ( x1 - x0 ) if steep : x0 , y0 = y0 , x0 x1 , y1 = y1 , x1 if x0 > x1 : x0 , x1 = x1 , x0 y0 , y1 = y1 , y0 dx = x1 - x0 dy = abs ( y1 - y0 ) err = dx / 2 if y0 < y1 : ystep = 1 else : ystep = - 1 count = 0 for x in range ( x0 , x1 + 1 ) : if colorFunc : color = colorFunc ( count ) count += 1 if steep : setter ( y0 , x , color ) else : setter ( x , y0 , color ) err -= dy if err < 0 : y0 += ystep err += dx
1965	def sys_mmap_pgoff ( self , address , size , prot , flags , fd , offset ) : return self . sys_mmap2 ( address , size , prot , flags , fd , offset )
10658	def amount_fractions ( masses ) : n = amounts ( masses ) n_total = sum ( n . values ( ) ) return { compound : n [ compound ] / n_total for compound in n . keys ( ) }
10749	def validate_sceneInfo ( self ) : if self . sceneInfo . prefix not in self . __satellitesMap : logger . error ( 'Google Downloader: Prefix of %s (%s) is invalid' % ( self . sceneInfo . name , self . sceneInfo . prefix ) ) raise WrongSceneNameError ( 'Google Downloader: Prefix of %s (%s) is invalid' % ( self . sceneInfo . name , self . sceneInfo . prefix ) )
8508	def _get_dataset ( self , X , y = None ) : from pylearn2 . datasets import DenseDesignMatrix X = np . asarray ( X ) assert X . ndim > 1 if y is not None : y = self . _get_labels ( y ) if X . ndim == 2 : return DenseDesignMatrix ( X = X , y = y ) return DenseDesignMatrix ( topo_view = X , y = y )
602	def addHistogram ( self , data , position = 111 , xlabel = None , ylabel = None , bins = None ) : ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . hist ( data , bins = bins , color = "green" , alpha = 0.8 ) plt . draw ( )
7287	def set_form_fields ( self , form_field_dict , parent_key = None , field_type = None ) : for form_key , field_value in form_field_dict . items ( ) : form_key = make_key ( parent_key , form_key ) if parent_key is not None else form_key if isinstance ( field_value , tuple ) : set_list_class = False base_key = form_key # Style list fields if ListField in ( field_value . field_type , field_type ) : # Nested lists/embedded docs need special care to get # styles to work out nicely. if parent_key is None or ListField == field_value . field_type : if field_type != EmbeddedDocumentField : field_value . widget . attrs [ 'class' ] += ' listField {0}' . format ( form_key ) set_list_class = True else : field_value . widget . attrs [ 'class' ] += ' listField' # Compute number value for list key list_keys = [ field_key for field_key in self . form . fields . keys ( ) if has_digit ( field_key ) ] key_int = 0 while form_key in list_keys : key_int += 1 form_key = make_key ( form_key , key_int ) if parent_key is not None : # Get the base key for our embedded field class valid_base_keys = [ model_key for model_key in self . model_map_dict . keys ( ) if not model_key . startswith ( "_" ) ] while base_key not in valid_base_keys and base_key : base_key = make_key ( base_key , exclude_last_string = True ) # We need to remove the trailing number from the key # so that grouping will occur on the front end when we have a list. embedded_key_class = None if set_list_class : field_value . widget . attrs [ 'class' ] += " listField" . format ( base_key ) embedded_key_class = make_key ( field_key , exclude_last_string = True ) field_value . widget . attrs [ 'class' ] += " embeddedField" # Setting the embedded key correctly allows to visually nest the # embedded documents on the front end. if base_key == parent_key : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( base_key ) else : field_value . widget . attrs [ 'class' ] += ' {0} {1}' . format ( base_key , parent_key ) if embedded_key_class is not None : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( embedded_key_class ) default_value = self . get_field_value ( form_key ) # Style embedded documents if isinstance ( default_value , list ) and len ( default_value ) > 0 : key_index = int ( form_key . split ( "_" ) [ - 1 ] ) new_base_key = make_key ( form_key , exclude_last_string = True ) for list_value in default_value : # Note, this is copied every time so each widget gets a different class list_widget = deepcopy ( field_value . widget ) new_key = make_key ( new_base_key , six . text_type ( key_index ) ) list_widget . attrs [ 'class' ] += " {0}" . format ( make_key ( base_key , key_index ) ) self . set_form_field ( list_widget , field_value . document_field , new_key , list_value ) key_index += 1 else : self . set_form_field ( field_value . widget , field_value . document_field , form_key , default_value ) elif isinstance ( field_value , dict ) : self . set_form_fields ( field_value , form_key , field_value . get ( "_field_type" , None ) )
7857	def __error ( self , stanza ) : try : self . error ( stanza . get_error ( ) ) except ProtocolError : from . . error import StanzaErrorNode self . error ( StanzaErrorNode ( "undefined-condition" ) )
6679	def getmtime ( self , path , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' ) ) : return int ( func ( 'stat -c %%Y "%(path)s" ' % locals ( ) ) . strip ( ) )
1183	def push_new_context ( self , pattern_offset ) : child_context = _MatchContext ( self . state , self . pattern_codes [ self . code_position + pattern_offset : ] ) self . state . context_stack . append ( child_context ) return child_context
12438	def deserialize ( self , request = None , text = None , format = None ) : if isinstance ( self , Resource ) : if not request : # Ensure we have a response object. request = self . _request Deserializer = None if format : # An explicit format was given; do not attempt to auto-detect # a deserializer. Deserializer = self . meta . deserializers [ format ] if not Deserializer : # Determine an appropriate deserializer to use by # introspecting the request object and looking at # the `Content-Type` header. media_ranges = request . get ( 'Content-Type' ) if media_ranges : # Parse the media ranges and determine the deserializer # that is the closest match. media_types = six . iterkeys ( self . _deserializer_map ) media_type = mimeparse . best_match ( media_types , media_ranges ) if media_type : format = self . _deserializer_map [ media_type ] Deserializer = self . meta . deserializers [ format ] else : # Client didn't provide a content-type; we're supposed # to auto-detect. # TODO: Implement this. pass if Deserializer : try : # Attempt to deserialize the data using the determined # deserializer. deserializer = Deserializer ( ) data = deserializer . deserialize ( request = request , text = text ) return data , deserializer except ValueError : # Failed to deserialize the data. pass # Failed to determine a deserializer; or failed to deserialize. raise http . exceptions . UnsupportedMediaType ( )
3196	def delete_permanent ( self , list_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' , subscriber_hash , 'actions' , 'delete-permanent' ) )
9196	def includeme ( config ) : global cache_manager settings = config . registry . settings cache_manager = CacheManager ( * * parse_cache_config_options ( settings ) )
2101	def log ( s , header = '' , file = sys . stderr , nl = 1 , * * kwargs ) : # Sanity check: If we are not in verbose mode, this is a no-op. if not settings . verbose : return # Construct multi-line string to stderr if header is provided. if header : word_arr = s . split ( ' ' ) multi = [ ] word_arr . insert ( 0 , '%s:' % header . upper ( ) ) i = 0 while i < len ( word_arr ) : to_add = [ '***' ] count = 3 while count <= 79 : count += len ( word_arr [ i ] ) + 1 if count <= 79 : to_add . append ( word_arr [ i ] ) i += 1 if i == len ( word_arr ) : break # Handle corner case of extra-long word longer than 75 characters. if len ( to_add ) == 1 : to_add . append ( word_arr [ i ] ) i += 1 if i != len ( word_arr ) : count -= len ( word_arr [ i ] ) + 1 to_add . append ( '*' * ( 78 - count ) ) multi . append ( ' ' . join ( to_add ) ) s = '\n' . join ( multi ) lines = len ( multi ) else : lines = 1 # If `nl` is an int greater than the number of rows of a message, # add the appropriate newlines to the output. if isinstance ( nl , int ) and nl > lines : s += '\n' * ( nl - lines ) # Output to stderr. return secho ( s , file = file , * * kwargs )
5211	def bds ( tickers , flds , * * kwargs ) : logger = logs . get_logger ( bds , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) con , _ = create_connection ( ) ovrds = assist . proc_ovrds ( * * kwargs ) logger . info ( f'loading block data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) data = con . bulkref ( tickers = tickers , flds = flds , ovrds = ovrds ) if not kwargs . get ( 'cache' , False ) : return [ data ] qry_data = [ ] for ( ticker , fld ) , grp in data . groupby ( [ 'ticker' , 'field' ] ) : data_file = storage . ref_file ( ticker = ticker , fld = fld , ext = 'pkl' , has_date = kwargs . get ( 'has_date' , True ) , * * kwargs ) if data_file : if not files . exists ( data_file ) : qry_data . append ( grp ) files . create_folder ( data_file , is_file = True ) grp . reset_index ( drop = True ) . to_pickle ( data_file ) return qry_data
5275	def _edgeLabel ( self , node , parent ) : return self . word [ node . idx + parent . depth : node . idx + node . depth ]
3525	def uservoice ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return UserVoiceNode ( )
6620	def wait ( self ) : finished_pids = [ ] while self . running_procs : finished_pids . extend ( self . poll ( ) ) return finished_pids
2813	def convert_unsqueeze ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting unsqueeze ...' ) if names == 'short' : tf_name = 'UNSQ' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) def target_layer ( x ) : import keras return keras . backend . expand_dims ( x ) lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name + 'E' ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
7427	def refmap_init ( data , sample , force ) : ## make some persistent file handles for the refmap reads files sample . files . unmapped_reads = os . path . join ( data . dirs . edits , "{}-refmap_derep.fastq" . format ( sample . name ) ) sample . files . mapped_reads = os . path . join ( data . dirs . refmapping , "{}-mapped-sorted.bam" . format ( sample . name ) )
9262	def filter_by_include_labels ( self , issues ) : if not self . options . include_labels : return copy . deepcopy ( issues ) filtered_issues = [ ] include_labels = set ( self . options . include_labels ) for issue in issues : labels = [ label [ "name" ] for label in issue [ "labels" ] ] if include_labels . intersection ( labels ) : filtered_issues . append ( issue ) return filtered_issues
11207	def gettz_db_metadata ( ) : warnings . warn ( "zoneinfo.gettz_db_metadata() will be removed in future " "versions, to use the dateutil-provided zoneinfo files, " "ZoneInfoFile object and query the 'metadata' attribute " "instead. See the documentation for details." , DeprecationWarning ) if len ( _CLASS_ZONE_INSTANCE ) == 0 : _CLASS_ZONE_INSTANCE . append ( ZoneInfoFile ( getzoneinfofile_stream ( ) ) ) return _CLASS_ZONE_INSTANCE [ 0 ] . metadata
7243	def geotiff ( self , * * kwargs ) : if 'proj' not in kwargs : kwargs [ 'proj' ] = self . proj return to_geotiff ( self , * * kwargs )
11463	def connect ( self ) : self . _ftp . connect ( ) self . _ftp . login ( user = self . _username , passwd = self . _passwd )
3119	def get_prep_value ( self , value ) : if value is None : return None else : return encoding . smart_text ( base64 . b64encode ( jsonpickle . encode ( value ) . encode ( ) ) )
2706	def mh_digest ( data ) : num_perm = 512 m = MinHash ( num_perm ) for d in data : m . update ( d . encode ( 'utf8' ) ) return m
13144	def pack_triples_numpy ( triples ) : if len ( triples ) == 0 : return np . array ( [ ] , dtype = np . int64 ) return np . stack ( list ( map ( _transform_triple_numpy , triples ) ) , axis = 0 )
2238	def _extension_module_tags ( ) : import sysconfig tags = [ ] if six . PY2 : # see also 'SHLIB_EXT' multiarch = sysconfig . get_config_var ( 'MULTIARCH' ) if multiarch is not None : tags . append ( multiarch ) else : # handle PEP 3149 -- ABI version tagged .so files # ABI = application binary interface tags . append ( sysconfig . get_config_var ( 'SOABI' ) ) tags . append ( 'abi3' ) # not sure why this one is valid but it is tags = [ t for t in tags if t ] return tags
10518	def setmin ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) object_handle . AXValue = 0 return 1
2630	def scale_in ( self , blocks = 0 , machines = 0 , strategy = None ) : count = 0 instances = self . client . servers . list ( ) for instance in instances [ 0 : machines ] : print ( "Deleting : " , instance ) instance . delete ( ) count += 1 return count
8252	def image_to_rgb ( self , path , n = 10 ) : from PIL import Image img = Image . open ( path ) p = img . getdata ( ) f = lambda p : choice ( p ) for i in _range ( n ) : rgba = f ( p ) rgba = _list ( rgba ) if len ( rgba ) == 3 : rgba . append ( 255 ) r , g , b , a = [ v / 255.0 for v in rgba ] clr = color ( r , g , b , a , mode = "rgb" ) self . append ( clr )
1937	def get_abi ( self , hsh : bytes ) -> Dict [ str , Any ] : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) if sig is not None : return dict ( self . _function_abi_items_by_signature [ sig ] ) item = self . _fallback_function_abi_item if item is not None : return dict ( item ) # An item describing the default fallback function. return { 'payable' : False , 'stateMutability' : 'nonpayable' , 'type' : 'fallback' }
894	def mapCellsToColumns ( self , cells ) : cellsForColumns = defaultdict ( set ) for cell in cells : column = self . columnForCell ( cell ) cellsForColumns [ column ] . add ( cell ) return cellsForColumns
11015	def publish ( context ) : header ( 'Recording changes...' ) run ( 'git add -A' ) header ( 'Displaying changes...' ) run ( 'git -c color.status=always status' ) if not click . confirm ( '\nContinue publishing' ) : run ( 'git reset HEAD --' ) abort ( context ) header ( 'Saving changes...' ) try : run ( 'git commit -m "{message}"' . format ( message = 'Publishing {}' . format ( choose_commit_emoji ( ) ) ) , capture = True ) except subprocess . CalledProcessError as e : if 'nothing to commit' not in e . stdout : raise else : click . echo ( 'Nothing to commit.' ) header ( 'Pushing to GitHub...' ) branch = get_branch ( ) run ( 'git push origin {branch}:{branch}' . format ( branch = branch ) ) pr_link = get_pr_link ( branch ) if pr_link : click . launch ( pr_link )
8562	def list_loadbalancers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
13812	def FindMethodByName ( self , name ) : for method in self . methods : if name == method . name : return method return None
4327	def downsample ( self , factor = 2 ) : if not isinstance ( factor , int ) or factor < 1 : raise ValueError ( 'factor must be a positive integer.' ) effect_args = [ 'downsample' , '{}' . format ( factor ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'downsample' ) return self
12004	def _read_header ( self , data ) : # pylint: disable=W0212 version = self . _read_version ( data ) version_info = self . _get_version_info ( version ) header_data = data [ : version_info [ 'header_size' ] ] header = version_info [ 'header' ] header = header . _make ( unpack ( version_info [ 'header_format' ] , header_data ) ) header = dict ( header . _asdict ( ) ) flags = list ( "{0:0>8b}" . format ( header [ 'flags' ] ) ) flags = dict ( version_info [ 'flags' ] . _make ( flags ) . _asdict ( ) ) flags = dict ( ( i , bool ( int ( j ) ) ) for i , j in flags . iteritems ( ) ) header [ 'flags' ] = flags timestamp = None if flags [ 'timestamp' ] : ts_start = version_info [ 'header_size' ] ts_end = ts_start + version_info [ 'timestamp_size' ] timestamp_data = data [ ts_start : ts_end ] timestamp = unpack ( version_info [ 'timestamp_format' ] , timestamp_data ) [ 0 ] header [ 'info' ] = { 'timestamp' : timestamp } return header
6443	def _cond_n ( self , word , suffix_len ) : if len ( word ) - suffix_len >= 3 : if word [ - suffix_len - 3 ] == 's' : if len ( word ) - suffix_len >= 4 : return True else : return True return False
12182	async def execute_method ( self , method , * * params ) : url = self . url_builder ( method , url_params = params ) logger . info ( 'Executing method %r' , method ) response = await aiohttp . get ( url ) logger . info ( 'Status: %r' , response . status ) if response . status == 200 : json = await response . json ( ) logger . debug ( '...with JSON %r' , json ) if json . get ( 'ok' ) : return json raise SlackApiError ( json [ 'error' ] ) else : raise_for_status ( response )
6524	def get_issues ( self , sortby = None ) : self . _ensure_cleaned_issues ( ) return self . _sort_issues ( self . _cleaned_issues , sortby )
1513	def wait_for_master_to_start ( single_master ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/status/leader" % single_master ) if r . status_code == 200 : break except : Log . debug ( sys . exc_info ( ) [ 0 ] ) Log . info ( "Waiting for cluster to come up... %s" % i ) time . sleep ( 1 ) if i > 10 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
9204	def path_to_node ( tree , path ) : if path is None : return None node = tree for key in path : node = child_by_key ( node , key ) return node
10313	def calculate_betweenness_centality ( graph : BELGraph , number_samples : int = CENTRALITY_SAMPLES ) -> Counter : try : res = nx . betweenness_centrality ( graph , k = number_samples ) except Exception : res = nx . betweenness_centrality ( graph ) return Counter ( res )
2685	def fate ( name ) : return cached_download ( 'http://fate.ffmpeg.org/fate-suite/' + name , os . path . join ( 'fate-suite' , name . replace ( '/' , os . path . sep ) ) )
1939	def get_func_return_types ( self , hsh : bytes ) -> str : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) abi = self . get_abi ( hsh ) outputs = abi . get ( 'outputs' ) return '()' if outputs is None else SolidityMetadata . tuple_signature_for_components ( outputs )
3454	def add_SBO ( model ) : for r in model . reactions : # don't annotate already annotated reactions if r . annotation . get ( "sbo" ) : continue # only doing exchanges if len ( r . metabolites ) != 1 : continue met_id = list ( r . _metabolites ) [ 0 ] . id if r . id . startswith ( "EX_" ) and r . id == "EX_" + met_id : r . annotation [ "sbo" ] = "SBO:0000627" elif r . id . startswith ( "DM_" ) and r . id == "DM_" + met_id : r . annotation [ "sbo" ] = "SBO:0000628"
11920	def get_object ( self ) : dataframe = self . filter_dataframe ( self . get_dataframe ( ) ) assert self . lookup_url_kwarg in self . kwargs , ( 'Expected view %s to be called with a URL keyword argument ' 'named "%s". Fix your URL conf, or set the `.lookup_field` ' 'attribute on the view correctly.' % ( self . __class__ . __name__ , self . lookup_url_kwarg ) ) try : obj = self . index_row ( dataframe ) except ( IndexError , KeyError , ValueError ) : raise Http404 # May raise a permission denied self . check_object_permissions ( self . request , obj ) return obj
115	def map_batches ( self , batches , chunksize = None ) : assert isinstance ( batches , list ) , ( "Expected to get a list as 'batches', got type %s. " + "Call imap_batches() if you use generators." ) % ( type ( batches ) , ) return self . pool . map ( _Pool_starworker , self . _handle_batch_ids ( batches ) , chunksize = chunksize )
12848	def add_safety_checks ( meta , members ) : for member_name , member_value in members . items ( ) : members [ member_name ] = meta . add_safety_check ( member_name , member_value )
13164	def format_value ( value ) : value_id = id ( value ) if value_id in recursion_breaker . processed : return u'<recursion>' recursion_breaker . processed . add ( value_id ) try : if isinstance ( value , six . binary_type ) : # suppose, all byte strings are in unicode # don't know if everybody in the world uses anything else? return u"'{0}'" . format ( value . decode ( 'utf-8' ) ) elif isinstance ( value , six . text_type ) : return u"u'{0}'" . format ( value ) elif isinstance ( value , ( list , tuple ) ) : # long lists or lists with multiline items # will be shown vertically values = list ( map ( format_value , value ) ) result = serialize_list ( u'[' , values , delimiter = u',' ) + u']' return force_unicode ( result ) elif isinstance ( value , dict ) : items = six . iteritems ( value ) # format each key/value pair as a text, # calling format_value recursively items = ( tuple ( map ( format_value , item ) ) for item in items ) items = list ( items ) # sort by keys for readability items . sort ( ) # for each item value items = [ serialize_text ( u'{0}: ' . format ( key ) , item_value ) for key , item_value in items ] # and serialize these pieces as a list, enclosing # them into a curve brackets result = serialize_list ( u'{' , items , delimiter = u',' ) + u'}' return force_unicode ( result ) return force_unicode ( repr ( value ) ) finally : recursion_breaker . processed . remove ( value_id )
12792	def post ( self , url = None , post_data = { } , parse_data = False , key = None , parameters = None , listener = None ) : return self . _fetch ( "POST" , url , post_data = post_data , parse_data = parse_data , key = key , parameters = parameters , listener = listener , full_return = True )
1450	def get_all_file_state_managers ( conf ) : state_managers = [ ] state_locations = conf . get_state_locations_of_type ( "file" ) for location in state_locations : name = location [ 'name' ] rootpath = os . path . expanduser ( location [ 'rootpath' ] ) LOG . info ( "Connecting to file state with rootpath: " + rootpath ) state_manager = FileStateManager ( name , rootpath ) state_managers . append ( state_manager ) return state_managers
11683	def _readblock ( self ) : block = '' while not self . _stop : line = self . _readline ( ) if line == '.' : break block += line return block
13673	def add_file ( self , * args ) : for file_path in args : self . files . append ( FilePath ( file_path , self ) )
8212	def draw_freehand ( self ) : if _ctx . _ns [ "mousedown" ] : x , y = mouse ( ) if self . show_grid : x , y = self . grid . snap ( x , y ) if self . freehand_move == True : cmd = MOVETO self . freehand_move = False else : cmd = LINETO # Add a new LINETO to the path, # except when starting to draw, # then a MOVETO is added to the path. pt = PathElement ( ) if cmd != MOVETO : pt . freehand = True # Used when mixed with curve drawing. else : pt . freehand = False pt . cmd = cmd pt . x = x pt . y = y pt . ctrl1 = Point ( x , y ) pt . ctrl2 = Point ( x , y ) self . _points . append ( pt ) # Draw the current location of the cursor. r = 4 _ctx . nofill ( ) _ctx . stroke ( self . handle_color ) _ctx . oval ( pt . x - r , pt . y - r , r * 2 , r * 2 ) _ctx . fontsize ( 9 ) _ctx . fill ( self . handle_color ) _ctx . text ( " (" + str ( int ( pt . x ) ) + ", " + str ( int ( pt . y ) ) + ")" , pt . x + r , pt . y ) self . _dirty = True else : # Export the updated drawing, # remember to do a MOVETO on the next interaction. self . freehand_move = True if self . _dirty : self . _points [ - 1 ] . freehand = False self . export_svg ( ) self . _dirty = False
5648	def _write_stop_to_stop_network_edges ( net , file_name , data = True , fmt = None ) : if fmt is None : fmt = "edg" if fmt == "edg" : if data : networkx . write_edgelist ( net , file_name , data = True ) else : networkx . write_edgelist ( net , file_name ) elif fmt == "csv" : with open ( file_name , 'w' ) as f : # writing out the header edge_iter = net . edges_iter ( data = True ) _ , _ , edg_data = next ( edge_iter ) edg_data_keys = list ( sorted ( edg_data . keys ( ) ) ) header = ";" . join ( [ "from_stop_I" , "to_stop_I" ] + edg_data_keys ) f . write ( header ) for from_node_I , to_node_I , data in net . edges_iter ( data = True ) : f . write ( "\n" ) values = [ str ( from_node_I ) , str ( to_node_I ) ] data_values = [ ] for key in edg_data_keys : if key == "route_I_counts" : route_I_counts_string = str ( data [ key ] ) . replace ( " " , "" ) [ 1 : - 1 ] data_values . append ( route_I_counts_string ) else : data_values . append ( str ( data [ key ] ) ) all_values = values + data_values f . write ( ";" . join ( all_values ) )
13151	def log_update ( entity , update ) : p = { 'on' : entity , 'update' : update } _log ( TYPE_CODES . UPDATE , p )
12	def smooth ( y , radius , mode = 'two_sided' , valid_only = False ) : assert mode in ( 'two_sided' , 'causal' ) if len ( y ) < 2 * radius + 1 : return np . ones_like ( y ) * y . mean ( ) elif mode == 'two_sided' : convkernel = np . ones ( 2 * radius + 1 ) out = np . convolve ( y , convkernel , mode = 'same' ) / np . convolve ( np . ones_like ( y ) , convkernel , mode = 'same' ) if valid_only : out [ : radius ] = out [ - radius : ] = np . nan elif mode == 'causal' : convkernel = np . ones ( radius ) out = np . convolve ( y , convkernel , mode = 'full' ) / np . convolve ( np . ones_like ( y ) , convkernel , mode = 'full' ) out = out [ : - radius + 1 ] if valid_only : out [ : radius ] = np . nan return out
11535	def available_drivers ( ) : global __modules global __available if type ( __modules ) is not list : __modules = list ( __modules ) if not __available : __available = [ d . ahioDriverInfo . NAME for d in __modules if d . ahioDriverInfo . AVAILABLE ] return __available
13370	def is_redirecting ( path ) : candidate = unipath ( path , '.cpenv' ) return os . path . exists ( candidate ) and os . path . isfile ( candidate )
826	def getScalarNames ( self , parentFieldName = '' ) : names = [ ] if self . encoders is not None : for ( name , encoder , offset ) in self . encoders : subNames = encoder . getScalarNames ( parentFieldName = name ) if parentFieldName != '' : subNames = [ '%s.%s' % ( parentFieldName , name ) for name in subNames ] names . extend ( subNames ) else : if parentFieldName != '' : names . append ( parentFieldName ) else : names . append ( self . name ) return names
11871	def color_from_hex ( value ) : if "#" in value : value = value [ 1 : ] try : unhexed = bytes . fromhex ( value ) except : unhexed = binascii . unhexlify ( value ) # Fallback for 2.7 compatibility return color_from_rgb ( * struct . unpack ( 'BBB' , unhexed ) )
2547	def add_annotation_type ( self , doc , annotation_type ) : if len ( doc . annotations ) != 0 : if not self . annotation_type_set : if annotation_type . endswith ( 'annotationType_other' ) : self . annotation_type_set = True doc . annotations [ - 1 ] . annotation_type = 'OTHER' return True elif annotation_type . endswith ( 'annotationType_review' ) : self . annotation_type_set = True doc . annotations [ - 1 ] . annotation_type = 'REVIEW' return True else : raise SPDXValueError ( 'Annotation::AnnotationType' ) else : raise CardinalityError ( 'Annotation::AnnotationType' ) else : raise OrderError ( 'Annotation::AnnotationType' )
10550	def find_results ( project_id , * * kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'result' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Result ( result ) for result in res ] else : return res except : # pragma: no cover raise
5451	def convert_to_label_chars ( s ) : # We want the results to be user-friendly, not just functional. # So we can't base-64 encode it. # * If upper-case: lower-case it # * If the char is not a standard letter or digit. make it a dash # March 2019 note: underscores are now allowed in labels. # However, removing the conversion of underscores to dashes here would # create inconsistencies between old jobs and new jobs. # With existing code, $USER "jane_doe" has a user-id label of "jane-doe". # If we remove the conversion, the user-id label for new jobs is "jane_doe". # This makes looking up old jobs more complicated. accepted_characters = string . ascii_lowercase + string . digits + '-' def label_char_transform ( char ) : if char in accepted_characters : return char if char in string . ascii_uppercase : return char . lower ( ) return '-' return '' . join ( label_char_transform ( c ) for c in s )
3757	def LFL ( Hc = None , atoms = { } , CASRN = '' , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'LFL' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'LFL' ] ) : methods . append ( NFPA ) if Hc : methods . append ( SUZUKI ) if atoms : methods . append ( CROWLLOUVAR ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'LFL' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'LFL' ] ) elif Method == SUZUKI : return Suzuki_LFL ( Hc = Hc ) elif Method == CROWLLOUVAR : return Crowl_Louvar_LFL ( atoms = atoms ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
13335	def cache_resolver ( resolver , path ) : env = resolver . cache . find ( path ) if env : return env raise ResolveError
9786	def resources ( ctx , gpu ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . build_job . resources ( user , project_name , _build , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
10255	def get_causal_source_nodes ( graph : BELGraph , func : str ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_source ( graph , node ) }
391	def keypoint_random_flip ( image , annos , mask = None , prob = 0.5 , flip_list = ( 0 , 1 , 5 , 6 , 7 , 2 , 3 , 4 , 11 , 12 , 13 , 8 , 9 , 10 , 15 , 14 , 17 , 16 , 18 ) ) : _prob = np . random . uniform ( 0 , 1.0 ) if _prob < prob : return image , annos , mask _ , width , _ = np . shape ( image ) image = cv2 . flip ( image , 1 ) mask = cv2 . flip ( mask , 1 ) new_joints = [ ] for people in annos : # TODO : speed up with affine transform new_keypoints = [ ] for k in flip_list : point = people [ k ] if point [ 0 ] < 0 or point [ 1 ] < 0 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if point [ 0 ] > image . shape [ 1 ] - 1 or point [ 1 ] > image . shape [ 0 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if ( width - point [ 0 ] ) > image . shape [ 1 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue new_keypoints . append ( ( width - point [ 0 ] , point [ 1 ] ) ) new_joints . append ( new_keypoints ) annos = new_joints return image , annos , mask
12474	def join_path_to_filelist ( path , filelist ) : return [ op . join ( path , str ( item ) ) for item in filelist ]
13809	def get_version ( relpath ) : from os . path import dirname , join if '__file__' not in globals ( ) : # Allow to use function interactively root = '.' else : root = dirname ( __file__ ) # The code below reads text file with unknown encoding in # in Python2/3 compatible way. Reading this text file # without specifying encoding will fail in Python 3 on some # systems (see http://goo.gl/5XmOH). Specifying encoding as # open() parameter is incompatible with Python 2 # cp437 is the encoding without missing points, safe against: # UnicodeDecodeError: 'charmap' codec can't decode byte... for line in open ( join ( root , relpath ) , 'rb' ) : line = line . decode ( 'cp437' ) if '__version__' in line : if '"' in line : # __version__ = "0.9" return line . split ( '"' ) [ 1 ] elif "'" in line : return line . split ( "'" ) [ 1 ]
1062	def formatdate ( timeval = None ) : if timeval is None : timeval = time . time ( ) timeval = time . gmtime ( timeval ) return "%s, %02d %s %04d %02d:%02d:%02d GMT" % ( ( "Mon" , "Tue" , "Wed" , "Thu" , "Fri" , "Sat" , "Sun" ) [ timeval [ 6 ] ] , timeval [ 2 ] , ( "Jan" , "Feb" , "Mar" , "Apr" , "May" , "Jun" , "Jul" , "Aug" , "Sep" , "Oct" , "Nov" , "Dec" ) [ timeval [ 1 ] - 1 ] , timeval [ 0 ] , timeval [ 3 ] , timeval [ 4 ] , timeval [ 5 ] )
9013	def knitting_pattern_set ( self , values ) : self . _start ( ) pattern_collection = self . _new_pattern_collection ( ) self . _fill_pattern_collection ( pattern_collection , values ) self . _create_pattern_set ( pattern_collection , values ) return self . _pattern_set
9737	def get_3d_markers_no_label ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionNoLabel , component_info , data , component_position )
11312	def update_oai_info ( self ) : for field in record_get_field_instances ( self . record , '909' , ind1 = "C" , ind2 = "O" ) : new_subs = [ ] for tag , value in field [ 0 ] : if tag == "o" : new_subs . append ( ( "a" , value ) ) else : new_subs . append ( ( tag , value ) ) if value in [ "CERN" , "CDS" , "ForCDS" ] : self . tag_as_cern = True record_add_field ( self . record , '024' , ind1 = "8" , subfields = new_subs ) record_delete_fields ( self . record , '909' )
8888	def fit ( self , X ) : X = iter2array ( X , dtype = ReactionContainer ) self . _train_signatures = { self . __get_signature ( x ) for x in X } return self
1446	def poll ( self ) : try : # non-blocking ret = self . _buffer . get ( block = False ) if self . _producer_callback is not None : self . _producer_callback ( ) return ret except Queue . Empty : Log . debug ( "%s: Empty in poll()" % str ( self ) ) raise Queue . Empty
6751	def unregister ( self ) : for k in list ( env . keys ( ) ) : if k . startswith ( self . env_prefix ) : del env [ k ] try : del all_satchels [ self . name . upper ( ) ] except KeyError : pass try : del manifest_recorder [ self . name ] except KeyError : pass try : del manifest_deployers [ self . name . upper ( ) ] except KeyError : pass try : del manifest_deployers_befores [ self . name . upper ( ) ] except KeyError : pass try : del required_system_packages [ self . name . upper ( ) ] except KeyError : pass
12034	def kernel_gaussian ( self , sizeMS , sigmaMS = None , forwardOnly = False ) : sigmaMS = sizeMS / 10 if sigmaMS is None else sigmaMS size , sigma = sizeMS * self . pointsPerMs , sigmaMS * self . pointsPerMs self . kernel = swhlab . common . kernel_gaussian ( size , sigma , forwardOnly ) return self . kernel
7226	def paint ( self ) : # TODO Figure out why i cant use some of these props snippet = { 'line-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'line-color' : VectorStyle . get_style_value ( self . color ) , #'line-cap': self.cap, #'line-join': self.join, 'line-width' : VectorStyle . get_style_value ( self . width ) , #'line-gap-width': self.gap_width, #'line-blur': self.blur, } if self . translate : snippet [ 'line-translate' ] = self . translate if self . dasharray : snippet [ 'line-dasharray' ] = VectorStyle . get_style_value ( self . dasharray ) return snippet
1126	def Expect ( inner_rule , loc = None ) : @ llrule ( loc , inner_rule . expected ) def rule ( parser ) : result = inner_rule ( parser ) if result is unmatched : expected = reduce ( list . __add__ , [ rule . expected ( parser ) for rule in parser . _errrules ] ) expected = list ( sorted ( set ( expected ) ) ) if len ( expected ) > 1 : expected = " or " . join ( [ ", " . join ( expected [ 0 : - 1 ] ) , expected [ - 1 ] ] ) elif len ( expected ) == 1 : expected = expected [ 0 ] else : expected = "(impossible)" error_tok = parser . _tokens [ parser . _errindex ] error = diagnostic . Diagnostic ( "fatal" , "unexpected {actual}: expected {expected}" , { "actual" : error_tok . kind , "expected" : expected } , error_tok . loc ) parser . diagnostic_engine . process ( error ) return result return rule
4009	def _start_http_server ( ) : logging . info ( 'Starting HTTP server at {}:{}' . format ( constants . DAEMON_HTTP_BIND_IP , constants . DAEMON_HTTP_BIND_PORT ) ) thread = threading . Thread ( target = http_server . app . run , args = ( constants . DAEMON_HTTP_BIND_IP , constants . DAEMON_HTTP_BIND_PORT ) ) thread . daemon = True thread . start ( )
9958	def restore_ipython ( self ) : if not self . is_ipysetup : return shell_class = type ( self . shell ) shell_class . showtraceback = shell_class . default_showtraceback del shell_class . default_showtraceback self . is_ipysetup = False
2557	def clean_attribute ( attribute ) : # Shorthand attribute = { 'cls' : 'class' , 'className' : 'class' , 'class_name' : 'class' , 'fr' : 'for' , 'html_for' : 'for' , 'htmlFor' : 'for' , } . get ( attribute , attribute ) # Workaround for Python's reserved words if attribute [ 0 ] == '_' : attribute = attribute [ 1 : ] # Workaround for dash if attribute in set ( [ 'http_equiv' ] ) or attribute . startswith ( 'data_' ) : attribute = attribute . replace ( '_' , '-' ) . lower ( ) # Workaround for colon if attribute . split ( '_' ) [ 0 ] in ( 'xlink' , 'xml' , 'xmlns' ) : attribute = attribute . replace ( '_' , ':' , 1 ) . lower ( ) return attribute
6651	def build ( self , builddir , component , args , release_build = False , build_args = None , targets = None , release_no_debug_info_build = False ) : if build_args is None : build_args = [ ] if targets is None : targets = [ ] # in the future this may be specified in the target description, but # for now we only support cmake, so everything is simple: if release_no_debug_info_build : build_type = 'Release' elif release_build : build_type = 'RelWithDebInfo' else : build_type = 'Debug' cmd = [ 'cmake' , '-D' , 'CMAKE_BUILD_TYPE=%s' % build_type , '-G' , args . cmake_generator , '.' ] res = self . exec_helper ( cmd , builddir ) if res is not None : return res # work-around various yotta-specific issues with the generated # Ninja/project files: from yotta . lib import cmake_fixups cmake_fixups . applyFixupsForFenerator ( args . cmake_generator , builddir , component ) build_command = self . overrideBuildCommand ( args . cmake_generator , targets = targets ) if build_command : cmd = build_command + build_args else : cmd = [ 'cmake' , '--build' , builddir ] if len ( targets ) : # !!! FIXME: support multiple targets with the default CMake # build command cmd += [ '--target' , targets [ 0 ] ] cmd += build_args res = self . exec_helper ( cmd , builddir ) if res is not None : return res hint = self . hintForCMakeGenerator ( args . cmake_generator , component ) if hint : logger . info ( hint )
11206	def gettz ( name ) : warnings . warn ( "zoneinfo.gettz() will be removed in future versions, " "to use the dateutil-provided zoneinfo files, instantiate a " "ZoneInfoFile object and use ZoneInfoFile.zones.get() " "instead. See the documentation for details." , DeprecationWarning ) if len ( _CLASS_ZONE_INSTANCE ) == 0 : _CLASS_ZONE_INSTANCE . append ( ZoneInfoFile ( getzoneinfofile_stream ( ) ) ) return _CLASS_ZONE_INSTANCE [ 0 ] . zones . get ( name )
12970	def _doCascadeFetch ( obj ) : obj . validateModel ( ) if not obj . foreignFields : return # NOTE: Currently this fetches using one transaction per object. Implementation for actual resolution is in # IndexedRedisModel.__getattribute__ for foreignField in obj . foreignFields : subObjsData = object . __getattribute__ ( obj , foreignField ) if not subObjsData : setattr ( obj , str ( foreignField ) , irNull ) continue subObjs = subObjsData . getObjs ( ) for subObj in subObjs : if isIndexedRedisModel ( subObj ) : IndexedRedisQuery . _doCascadeFetch ( subObj )
7658	def append_records ( self , records ) : for obs in records : if isinstance ( obs , Observation ) : self . append ( * * obs . _asdict ( ) ) else : self . append ( * * obs )
13188	async def process_lander_page ( session , github_api_token , ltd_product_data , mongo_collection = None ) : logger = logging . getLogger ( __name__ ) # Try to download metadata.jsonld from the Landing page site. published_url = ltd_product_data [ 'published_url' ] jsonld_url = urljoin ( published_url , '/metadata.jsonld' ) try : async with session . get ( jsonld_url ) as response : logger . debug ( '%s response status %r' , jsonld_url , response . status ) response . raise_for_status ( ) json_data = await response . text ( ) except aiohttp . ClientResponseError as err : logger . debug ( 'Tried to download %s, got status %d' , jsonld_url , err . code ) raise NotLanderPageError ( ) # Use our own json parser to get datetimes metadata = decode_jsonld ( json_data ) if mongo_collection is not None : await _upload_to_mongodb ( mongo_collection , metadata ) return metadata
4675	def removeAccount ( self , account ) : accounts = self . getAccounts ( ) for a in accounts : if a [ "name" ] == account : self . store . delete ( a [ "pubkey" ] )
8454	def _cookiecutter_configs_have_changed ( template , old_version , new_version ) : temple . check . is_git_ssh_path ( template ) repo_path = temple . utils . get_repo_path ( template ) github_client = temple . utils . GithubClient ( ) api = '/repos/{}/contents/cookiecutter.json' . format ( repo_path ) old_config_resp = github_client . get ( api , params = { 'ref' : old_version } ) old_config_resp . raise_for_status ( ) new_config_resp = github_client . get ( api , params = { 'ref' : new_version } ) new_config_resp . raise_for_status ( ) return old_config_resp . json ( ) [ 'content' ] != new_config_resp . json ( ) [ 'content' ]
12974	def compat_convertHashedIndexes ( self , fetchAll = True ) : saver = IndexedRedisSave ( self . mdl ) if fetchAll is True : objs = self . all ( ) saver . compat_convertHashedIndexes ( objs ) else : didWarnOnce = False pks = self . getPrimaryKeys ( ) for pk in pks : obj = self . get ( pk ) if not obj : if didWarnOnce is False : sys . stderr . write ( 'WARNING(once)! An object (type=%s , pk=%d) disappered while ' 'running compat_convertHashedIndexes! This probably means an application ' 'is using the model while converting indexes. This is a very BAD IDEA (tm).' ) didWarnOnce = True continue saver . compat_convertHashedIndexes ( [ obj ] )
1132	def _replace ( _self , * * kwds ) : result = _self . _make ( map ( kwds . pop , ( 'scheme' , 'netloc' , 'path' , 'query' , 'fragment' ) , _self ) ) if kwds : raise ValueError ( 'Got unexpected field names: %r' % kwds . keys ( ) ) return result
10998	def moment ( p , v , order = 1 ) : if order == 1 : return ( v * p ) . sum ( ) elif order == 2 : return np . sqrt ( ( ( v ** 2 ) * p ) . sum ( ) - ( v * p ) . sum ( ) ** 2 )
8300	def handle ( self , data , source = None ) : decoded = decodeOSC ( data ) self . dispatch ( decoded , source )
13133	def parse_domain_users ( domain_users_file , domain_groups_file ) : with open ( domain_users_file ) as f : users = json . loads ( f . read ( ) ) domain_groups = { } if domain_groups_file : with open ( domain_groups_file ) as f : groups = json . loads ( f . read ( ) ) for group in groups : sid = get_field ( group , 'objectSid' ) domain_groups [ int ( sid . split ( '-' ) [ - 1 ] ) ] = get_field ( group , 'cn' ) user_search = UserSearch ( ) count = 0 total = len ( users ) print_notification ( "Importing {} users" . format ( total ) ) for entry in users : result = parse_user ( entry , domain_groups ) user = user_search . id_to_object ( result [ 'username' ] ) user . name = result [ 'name' ] user . domain . append ( result [ 'domain' ] ) user . description = result [ 'description' ] user . groups . extend ( result [ 'groups' ] ) user . flags . extend ( result [ 'flags' ] ) user . sid = result [ 'sid' ] user . add_tag ( "domaindump" ) user . save ( ) count += 1 sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}]" . format ( count , total ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
9660	def merge_from_store_and_in_mems ( from_store , in_mem_shas , dont_update_shas_of ) : if not from_store : for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas for key in from_store [ 'files' ] : if key not in in_mem_shas [ 'files' ] and key not in dont_update_shas_of : in_mem_shas [ 'files' ] [ key ] = from_store [ 'files' ] [ key ] for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas
7647	def scaper_to_tag ( annotation ) : annotation . namespace = 'tag_open' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = obs . value [ 'label' ] ) return annotation
3212	def _update_cache_stats ( self , key , result ) : if result is None : self . _CACHE_STATS [ 'access_stats' ] . setdefault ( key , { 'hit' : 0 , 'miss' : 0 , 'expired' : 0 } ) else : self . _CACHE_STATS [ 'access_stats' ] [ key ] [ result ] += 1
9943	def delete_file ( self , path , prefixed_path , source_storage ) : if self . storage . exists ( prefixed_path ) : try : # When was the target file modified last time? target_last_modified = self . storage . modified_time ( prefixed_path ) except ( OSError , NotImplementedError , AttributeError ) : # The storage doesn't support ``modified_time`` or failed pass else : try : # When was the source file modified last time? source_last_modified = source_storage . modified_time ( path ) except ( OSError , NotImplementedError , AttributeError ) : pass else : # The full path of the target file if self . local : full_path = self . storage . path ( prefixed_path ) else : full_path = None # Skip the file if the source file is younger # Avoid sub-second precision (see #14665, #19540) if ( target_last_modified . replace ( microsecond = 0 ) >= source_last_modified . replace ( microsecond = 0 ) ) : if not ( ( self . symlink and full_path and not os . path . islink ( full_path ) ) or ( not self . symlink and full_path and os . path . islink ( full_path ) ) ) : if prefixed_path not in self . unmodified_files : self . unmodified_files . append ( prefixed_path ) self . log ( "Skipping '%s' (not modified)" % path ) return False # Then delete the existing file if really needed if self . dry_run : self . log ( "Pretending to delete '%s'" % path ) else : self . log ( "Deleting '%s'" % path ) self . storage . delete ( prefixed_path ) return True
11939	def broadcast_message ( level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : from django . contrib . auth import get_user_model users = get_user_model ( ) . objects . all ( ) add_message_for ( users , level , message_text , extra_tags = extra_tags , date = date , url = url , fail_silently = fail_silently )
6135	def _fix_docs ( this_abc , child_class ) : # After python 3.5, this is basically handled automatically if sys . version_info >= ( 3 , 5 ) : return child_class if not issubclass ( child_class , this_abc ) : raise KappaError ( 'Cannot fix docs of class that is not decendent.' ) # This method is modified from solution given in # https://stackoverflow.com/a/8101598/8863865 for name , child_func in vars ( child_class ) . items ( ) : if callable ( child_func ) and not child_func . __doc__ : if name in this_abc . __abstractmethods__ : parent_func = getattr ( this_abc , name ) child_func . __doc__ = parent_func . __doc__ return child_class
8259	def _sorted_copy ( self , comparison , reversed = False ) : sorted = self . copy ( ) _list . sort ( sorted , comparison ) if reversed : _list . reverse ( sorted ) return sorted
6612	def put_multiple ( self , task_args_kwargs_list ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return packages = [ ] for t in task_args_kwargs_list : try : task = t [ 'task' ] args = t . get ( 'args' , ( ) ) kwargs = t . get ( 'kwargs' , { } ) package = TaskPackage ( task = task , args = args , kwargs = kwargs ) except TypeError : package = TaskPackage ( task = t , args = ( ) , kwargs = { } ) packages . append ( package ) return self . dropbox . put_multiple ( packages )
9563	def _as_dict ( self , r ) : d = dict ( ) for i , f in enumerate ( self . _field_names ) : d [ f ] = r [ i ] if i < len ( r ) else None return d
8467	def path ( self , value ) : if not value . endswith ( '/' ) : self . _path = '{v}/' . format ( v = value ) else : self . _path = value
7079	def tic_xmatch ( ra , decl , radius_arcsec = 5.0 , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 90.0 , refresh = 5.0 , maxtimeout = 180.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : service = 'Mast.Tic.Crossmatch' xmatch_input = { 'fields' : [ { 'name' : 'ra' , 'type' : 'float' } , { 'name' : 'dec' , 'type' : 'float' } ] } xmatch_input [ 'data' ] = [ { 'ra' : x , 'dec' : y } for ( x , y ) in zip ( ra , decl ) ] params = { 'raColumn' : 'ra' , 'decColumn' : 'dec' , 'radius' : radius_arcsec / 3600.0 } return mast_query ( service , params , data = xmatch_input , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
12839	def init_async ( self , loop ) : super ( PooledAIODatabase , self ) . init_async ( loop ) self . _waiters = collections . deque ( )
7047	def _parallel_bls_worker ( task ) : try : return _bls_runner ( * task ) except Exception as e : LOGEXCEPTION ( 'BLS failed for task %s' % repr ( task [ 2 : ] ) ) return { 'power' : nparray ( [ npnan for x in range ( task [ 2 ] ) ] ) , 'bestperiod' : npnan , 'bestpower' : npnan , 'transdepth' : npnan , 'transduration' : npnan , 'transingressbin' : npnan , 'transegressbin' : npnan }
13582	def format_to_csv ( filename , skiprows = 0 , delimiter = "" ) : if not delimiter : delimiter = "\t" input_file = open ( filename , "r" ) if skiprows : [ input_file . readline ( ) for _ in range ( skiprows ) ] new_filename = os . path . splitext ( filename ) [ 0 ] + ".csv" output_file = open ( new_filename , "w" ) header = input_file . readline ( ) . split ( ) reader = csv . DictReader ( input_file , fieldnames = header , delimiter = delimiter ) writer = csv . DictWriter ( output_file , fieldnames = header , delimiter = "," ) # Write header writer . writerow ( dict ( ( x , x ) for x in header ) ) # Write rows for line in reader : if None in line : del line [ None ] writer . writerow ( line ) input_file . close ( ) output_file . close ( ) print "Saved %s." % new_filename
4872	def to_representation ( self , data ) : return [ self . child . to_representation ( item ) if 'detail' in item else item for item in data ]
4030	def load ( self ) : con = sqlite3 . connect ( self . tmp_cookie_file ) cur = con . cursor ( ) try : # chrome <=55 cur . execute ( 'SELECT host_key, path, secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) except sqlite3 . OperationalError : # chrome >=56 cur . execute ( 'SELECT host_key, path, is_secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) cj = http . cookiejar . CookieJar ( ) for item in cur . fetchall ( ) : host , path , secure , expires , name = item [ : 5 ] value = self . _decrypt ( item [ 5 ] , item [ 6 ] ) c = create_cookie ( host , path , secure , expires , name , value ) cj . set_cookie ( c ) con . close ( ) return cj
11726	def format_seconds ( self , n_seconds ) : func = self . ok if n_seconds >= 60 : n_minutes , n_seconds = divmod ( n_seconds , 60 ) return "%s minutes %s seconds" % ( func ( "%d" % n_minutes ) , func ( "%.3f" % n_seconds ) ) else : return "%s seconds" % ( func ( "%.3f" % n_seconds ) )
6	def cg ( f_Ax , b , cg_iters = 10 , callback = None , verbose = False , residual_tol = 1e-10 ) : p = b . copy ( ) r = b . copy ( ) x = np . zeros_like ( b ) rdotr = r . dot ( r ) fmtstr = "%10i %10.3g %10.3g" titlestr = "%10s %10s %10s" if verbose : print ( titlestr % ( "iter" , "residual norm" , "soln norm" ) ) for i in range ( cg_iters ) : if callback is not None : callback ( x ) if verbose : print ( fmtstr % ( i , rdotr , np . linalg . norm ( x ) ) ) z = f_Ax ( p ) v = rdotr / p . dot ( z ) x += v * p r -= v * z newrdotr = r . dot ( r ) mu = newrdotr / rdotr p = r + mu * p rdotr = newrdotr if rdotr < residual_tol : break if callback is not None : callback ( x ) if verbose : print ( fmtstr % ( i + 1 , rdotr , np . linalg . norm ( x ) ) ) # pylint: disable=W0631 return x
1639	def CheckParenthesisSpacing ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] # No spaces after an if, while, switch, or for match = Search ( r' (if\(|for\(|while\(|switch\()' , line ) if match : error ( filename , linenum , 'whitespace/parens' , 5 , 'Missing space before ( in %s' % match . group ( 1 ) ) # For if/for/while/switch, the left and right parens should be # consistent about how many spaces are inside the parens, and # there should either be zero or one spaces inside the parens. # We don't want: "if ( foo)" or "if ( foo )". # Exception: "for ( ; foo; bar)" and "for (foo; bar; )" are allowed. match = Search ( r'\b(if|for|while|switch)\s*' r'\(([ ]*)(.).*[^ ]+([ ]*)\)\s*{\s*$' , line ) if match : if len ( match . group ( 2 ) ) != len ( match . group ( 4 ) ) : if not ( match . group ( 3 ) == ';' and len ( match . group ( 2 ) ) == 1 + len ( match . group ( 4 ) ) or not match . group ( 2 ) and Search ( r'\bfor\s*\(.*; \)' , line ) ) : error ( filename , linenum , 'whitespace/parens' , 5 , 'Mismatching spaces inside () in %s' % match . group ( 1 ) ) if len ( match . group ( 2 ) ) not in [ 0 , 1 ] : error ( filename , linenum , 'whitespace/parens' , 5 , 'Should have zero or one spaces inside ( and ) in %s' % match . group ( 1 ) )
10842	def move_to_top ( self ) : url = PATHS [ 'MOVE_TO_TOP' ] % self . id response = self . api . post ( url = url ) return Update ( api = self . api , raw_response = response )
13796	def handle_rereduce ( self , reduce_function_names , values ) : # This gets a large list of reduction functions, given their names. reduce_functions = [ ] for reduce_function_name in reduce_function_names : try : reduce_function = get_function ( reduce_function_name ) if getattr ( reduce_function , 'view_decorated' , None ) : reduce_function = reduce_function ( self . log ) reduce_functions . append ( reduce_function ) except Exception , exc : self . log ( repr ( exc ) ) reduce_functions . append ( lambda * args , * * kwargs : None ) # This gets the list of results from those functions. results = [ ] for reduce_function in reduce_functions : try : results . append ( reduce_function ( None , values , rereduce = True ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
10384	def remove_inconsistent_edges ( graph : BELGraph ) -> None : for u , v in get_inconsistent_edges ( graph ) : edges = [ ( u , v , k ) for k in graph [ u ] [ v ] ] graph . remove_edges_from ( edges )
4990	def get ( self , request , * args , * * kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( * * kwargs ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_customer_uuid ) if course_key : try : course_run_id = RouterView . get_course_run_id ( request . user , enterprise_customer , course_key ) except Http404 : context_data = get_global_context ( request , enterprise_customer ) error_code = 'ENTRV000' log_message = ( 'Could not find course run with id {course_run_id} ' 'for course key {course_key} and program_uuid {program_uuid} ' 'for enterprise_customer_uuid {enterprise_customer_uuid} ' 'Returned error code {error_code} to user {userid}' . format ( course_key = course_key , course_run_id = course_run_id , enterprise_customer_uuid = enterprise_customer_uuid , error_code = error_code , userid = request . user . id , program_uuid = program_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) kwargs [ 'course_id' ] = course_run_id # Ensure that the link is saved to the database prior to making some call in a downstream view # which may need to know that the user belongs to an enterprise customer. with transaction . atomic ( ) : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_id = request . user . id ) enterprise_customer_user . update_session ( request ) # Directly enroll in audit mode if the request in question has full direct audit enrollment eligibility. resource_id = course_run_id or program_uuid if self . eligible_for_direct_audit_enrollment ( request , enterprise_customer , resource_id , course_key ) : try : enterprise_customer_user . enroll ( resource_id , 'audit' , cohort = request . GET . get ( 'cohort' , None ) ) track_enrollment ( 'direct-audit-enrollment' , request . user . id , resource_id , request . get_full_path ( ) ) except ( CourseEnrollmentDowngradeError , CourseEnrollmentPermissionError ) : pass # The courseware view logic will check for DSC requirements, and route to the DSC page if necessary. return redirect ( LMS_COURSEWARE_URL . format ( course_id = resource_id ) ) return self . redirect ( request , * args , * * kwargs )
13769	def get_minifier ( self ) : if self . minifier is None : if not self . has_bundles ( ) : raise Exception ( "Unable to get default minifier, no bundles in build group" ) minifier = self . get_first_bundle ( ) . get_default_minifier ( ) else : minifier = self . minifier if minifier : minifier . init_asset ( self ) return minifier
5217	def check_hours ( tickers , tz_exch , tz_loc = DEFAULT_TZ ) -> pd . DataFrame : cols = [ 'Trading_Day_Start_Time_EOD' , 'Trading_Day_End_Time_EOD' ] con , _ = create_connection ( ) hours = con . ref ( tickers = tickers , flds = cols ) cur_dt = pd . Timestamp ( 'today' ) . strftime ( '%Y-%m-%d ' ) hours . loc [ : , 'local' ] = hours . value . astype ( str ) . str [ : - 3 ] hours . loc [ : , 'exch' ] = pd . DatetimeIndex ( cur_dt + hours . value . astype ( str ) ) . tz_localize ( tz_loc ) . tz_convert ( tz_exch ) . strftime ( '%H:%M' ) hours = pd . concat ( [ hours . set_index ( [ 'ticker' , 'field' ] ) . exch . unstack ( ) . loc [ : , cols ] , hours . set_index ( [ 'ticker' , 'field' ] ) . local . unstack ( ) . loc [ : , cols ] , ] , axis = 1 ) hours . columns = [ 'Exch_Start' , 'Exch_End' , 'Local_Start' , 'Local_End' ] return hours
11537	def set_pin_direction ( self , pin , direction ) : if type ( pin ) is list : for p in pin : self . set_pin_direction ( p , direction ) return pin_id = self . _pin_mapping . get ( pin , None ) if pin_id and type ( direction ) is ahio . Direction : self . _set_pin_direction ( pin_id , direction ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
400	def normalized_mean_square_error ( output , target , name = "normalized_mean_squared_error_loss" ) : # with tf.name_scope("normalized_mean_squared_error_loss"): if output . get_shape ( ) . ndims == 2 : # [batch_size, n_feature] nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = 1 ) ) nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = 1 ) ) elif output . get_shape ( ) . ndims == 3 : # [batch_size, w, h] nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = [ 1 , 2 ] ) ) nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = [ 1 , 2 ] ) ) elif output . get_shape ( ) . ndims == 4 : # [batch_size, w, h, c] nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = [ 1 , 2 , 3 ] ) ) nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = [ 1 , 2 , 3 ] ) ) nmse = tf . reduce_mean ( nmse_a / nmse_b , name = name ) return nmse
12948	def reload ( self , cascadeObjects = True ) : _id = self . _id if not _id : raise KeyError ( 'Object has never been saved! Cannot reload.' ) currentData = self . asDict ( False , forStorage = False ) # Get the object, and compare the unconverted "asDict" repr. # If any changes, we will apply the already-convered value from # the object, but we compare the unconverted values (what's in the DB). newDataObj = self . objects . get ( _id ) if not newDataObj : raise KeyError ( 'Object with id=%d is not in database. Cannot reload.' % ( _id , ) ) newData = newDataObj . asDict ( False , forStorage = False ) if currentData == newData and not self . foreignFields : return [ ] updatedFields = { } for thisField , newValue in newData . items ( ) : defaultValue = thisField . getDefaultValue ( ) currentValue = currentData . get ( thisField , defaultValue ) fieldIsUpdated = False if currentValue != newValue : fieldIsUpdated = True elif cascadeObjects is True and issubclass ( thisField . __class__ , IRForeignLinkFieldBase ) : # If we are cascading objects, and at this point the pk is the same if currentValue . isFetched ( ) : # If we have fetched the current set, we might need to update (pks already match) oldObjs = currentValue . getObjs ( ) newObjs = newValue . getObjs ( ) if oldObjs != newObjs : # This will check using __eq__, so one-level including pk fieldIsUpdated = True else : # Use hasSameValues with cascadeObjects=True to scan past one level for i in range ( len ( oldObjs ) ) : if not oldObjs [ i ] . hasSameValues ( newObjs [ i ] , cascadeObjects = True ) : fieldIsUpdated = True break if fieldIsUpdated is True : # Use "converted" values in the updatedFields dict, and apply on the object. updatedFields [ thisField ] = ( currentValue , newValue ) setattr ( self , thisField , newValue ) self . _origData [ thisField ] = newDataObj . _origData [ thisField ] return updatedFields
9318	def wait_until_final ( self , poll_interval = 1 , timeout = 60 ) : start_time = time . time ( ) elapsed = 0 while ( self . status != "complete" and ( timeout <= 0 or elapsed < timeout ) ) : time . sleep ( poll_interval ) self . refresh ( ) elapsed = time . time ( ) - start_time
483	def getSwarmModelParams ( modelID ) : # TODO: the use of nupic.frameworks.opf.helpers.loadExperimentDescriptionScriptFromDir when # retrieving module params results in a leakage of pf_base_descriptionNN and # pf_descriptionNN module imports for every call to getSwarmModelParams, so # the leakage is unlimited when getSwarmModelParams is called by a # long-running process. An alternate solution is to execute the guts of # this function's logic in a seprate process (via multiprocessing module). cjDAO = ClientJobsDAO . get ( ) ( jobID , description ) = cjDAO . modelsGetFields ( modelID , [ "jobId" , "genDescription" ] ) ( baseDescription , ) = cjDAO . jobGetFields ( jobID , [ "genBaseDescription" ] ) # Construct a directory with base.py and description.py for loading model # params, and use nupic.frameworks.opf.helpers to extract model params from # those files descriptionDirectory = tempfile . mkdtemp ( ) try : baseDescriptionFilePath = os . path . join ( descriptionDirectory , "base.py" ) with open ( baseDescriptionFilePath , mode = "wb" ) as f : f . write ( baseDescription ) descriptionFilePath = os . path . join ( descriptionDirectory , "description.py" ) with open ( descriptionFilePath , mode = "wb" ) as f : f . write ( description ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( helpers . loadExperimentDescriptionScriptFromDir ( descriptionDirectory ) ) return json . dumps ( dict ( modelConfig = expIface . getModelDescription ( ) , inferenceArgs = expIface . getModelControl ( ) . get ( "inferenceArgs" , None ) ) ) finally : shutil . rmtree ( descriptionDirectory , ignore_errors = True )
13159	def update ( cls , cur , table : str , values : dict , where_keys : list ) -> tuple : keys = cls . _COMMA . join ( values . keys ( ) ) value_place_holder = cls . _PLACEHOLDER * len ( values ) where_clause , where_values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _update_string . format ( table , keys , value_place_holder [ : - 1 ] , where_clause ) yield from cur . execute ( query , ( tuple ( values . values ( ) ) + where_values ) ) return ( yield from cur . fetchall ( ) )
12522	def die ( msg , code = - 1 ) : sys . stderr . write ( msg + "\n" ) sys . exit ( code )
11267	def walk ( prev , inital_path , * args , * * kw ) : for dir_path , dir_names , filenames in os . walk ( inital_path ) : for filename in filenames : yield os . path . join ( dir_path , filename )
1726	def except_token ( source , start , token , throw = True ) : start = pass_white ( source , start ) if start < len ( source ) and source [ start ] == token : return start + 1 if throw : raise SyntaxError ( 'Missing token. Expected %s' % token ) return None
2694	def fix_hypenation ( foo ) : i = 0 bar = [ ] while i < len ( foo ) : text , lemma , pos , tag = foo [ i ] if ( tag == "HYPH" ) and ( i > 0 ) and ( i < len ( foo ) - 1 ) : prev_tok = bar [ - 1 ] next_tok = foo [ i + 1 ] prev_tok [ 0 ] += "-" + next_tok [ 0 ] prev_tok [ 1 ] += "-" + next_tok [ 1 ] bar [ - 1 ] = prev_tok i += 2 else : bar . append ( foo [ i ] ) i += 1 return bar
9829	def edges ( self ) : return [ self . delta [ d , d ] * numpy . arange ( self . shape [ d ] + 1 ) + self . origin [ d ] - 0.5 * self . delta [ d , d ] for d in range ( self . rank ) ]
1109	def compare ( self , a , b ) : cruncher = SequenceMatcher ( self . linejunk , a , b ) for tag , alo , ahi , blo , bhi in cruncher . get_opcodes ( ) : if tag == 'replace' : g = self . _fancy_replace ( a , alo , ahi , b , blo , bhi ) elif tag == 'delete' : g = self . _dump ( '-' , a , alo , ahi ) elif tag == 'insert' : g = self . _dump ( '+' , b , blo , bhi ) elif tag == 'equal' : g = self . _dump ( ' ' , a , alo , ahi ) else : raise ValueError , 'unknown tag %r' % ( tag , ) for line in g : yield line
12887	def create_session ( self ) : req_url = '%s/%s' % ( self . __webfsapi , 'CREATE_SESSION' ) sid = yield from self . __session . get ( req_url , params = dict ( pin = self . pin ) , timeout = self . timeout ) text = yield from sid . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . sessionId . text
6065	def density_between_circular_annuli_in_angular_units ( self , inner_annuli_radius , outer_annuli_radius ) : annuli_area = ( np . pi * outer_annuli_radius ** 2.0 ) - ( np . pi * inner_annuli_radius ** 2.0 ) return ( self . mass_within_circle_in_units ( radius = outer_annuli_radius ) - self . mass_within_circle_in_units ( radius = inner_annuli_radius ) ) / annuli_area
6857	def create_user ( name , password , host = 'localhost' , * * kwargs ) : with settings ( hide ( 'running' ) ) : query ( "CREATE USER '%(name)s'@'%(host)s' IDENTIFIED BY '%(password)s';" % { 'name' : name , 'password' : password , 'host' : host } , * * kwargs ) puts ( "Created MySQL user '%s'." % name )
12293	def annotate_metadata_action ( repo ) : package = repo . package print ( "Including history of actions" ) with cd ( repo . rootdir ) : filename = ".dgit/log.json" if os . path . exists ( filename ) : history = open ( filename ) . readlines ( ) actions = [ ] for a in history : try : a = json . loads ( a ) for x in [ 'code' ] : if x not in a or a [ x ] == None : a [ x ] = "..." actions . append ( a ) except : pass package [ 'actions' ] = actions
922	def _filterRecord ( filterList , record ) : for ( fieldIdx , fp , params ) in filterList : x = dict ( ) x [ 'value' ] = record [ fieldIdx ] x [ 'acceptValues' ] = params [ 'acceptValues' ] x [ 'min' ] = params [ 'min' ] x [ 'max' ] = params [ 'max' ] if not fp ( x ) : return False # None of the field filters triggered, accept the record as a good one return True
13407	def sendToLogbook ( self , fileName , logType , location = None ) : import subprocess success = True if logType == "MCC" : fileString = "" if not self . imagePixmap . isNull ( ) : fileString = fileName + "." + self . imageType logcmd = "xml2elog " + fileName + ".xml " + fileString process = subprocess . Popen ( logcmd , shell = True ) process . wait ( ) if process . returncode != 0 : success = False else : from shutil import copy path = "/u1/" + location . lower ( ) + "/physics/logbook/data/" # Prod path # path = "/home/softegr/alverson/log_test/" # Dev path try : if not self . imagePixmap . isNull ( ) : copy ( fileName + ".png" , path ) if self . imageType == "png" : copy ( fileName + ".ps" , path ) else : copy ( fileName + "." + self . imageType , path ) # Copy .xml file last to ensure images will be picked up by cron job # print("Copying file " + fileName + " to path " + path) copy ( fileName + ".xml" , path ) except IOError as error : print ( error ) success = False return success
1966	def syscall ( self ) : index = self . _syscall_abi . syscall_number ( ) try : table = getattr ( linux_syscalls , self . current . machine ) name = table . get ( index , None ) implementation = getattr ( self , name ) except ( AttributeError , KeyError ) : if name is not None : raise SyscallNotImplemented ( index , name ) else : raise Exception ( f"Bad syscall index, {index}" ) return self . _syscall_abi . invoke ( implementation )
13547	def _setVirtualEnv ( ) : try : activate = options . virtualenv . activate_cmd except AttributeError : activate = None if activate is None : virtualenv = path ( os . environ . get ( 'VIRTUAL_ENV' , '' ) ) if not virtualenv : virtualenv = options . paved . cwd else : virtualenv = path ( virtualenv ) activate = virtualenv / 'bin' / 'activate' if activate . exists ( ) : info ( 'Using default virtualenv at %s' % activate ) options . setdotted ( 'virtualenv.activate_cmd' , 'source %s' % activate )
11588	def _rc_brpoplpush ( self , src , dst , timeout = 0 ) : rpop = self . brpop ( src , timeout ) if rpop is not None : self . lpush ( dst , rpop [ 1 ] ) return rpop [ 1 ] return None
2072	def get_obj_cols ( df ) : obj_cols = [ ] for idx , dt in enumerate ( df . dtypes ) : if dt == 'object' or is_category ( dt ) : obj_cols . append ( df . columns . values [ idx ] ) return obj_cols
11498	def get_community_by_name ( self , name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.get' , parameters ) return response
6826	def clone ( self , remote_url , path = None , use_sudo = False , user = None ) : cmd = 'git clone --quiet %s' % remote_url if path is not None : cmd = cmd + ' %s' % path if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
13179	def get_or_default ( func = None , default = None ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : try : return func ( * args , * * kwargs ) except ObjectDoesNotExist : if callable ( default ) : return default ( ) else : return default return wrapper if func is None : return decorator else : return decorator ( func )
2039	def CALLCODE ( self , gas , _ignored_ , value , in_offset , in_size , out_offset , out_size ) : self . world . start_transaction ( 'CALLCODE' , address = self . address , data = self . read_buffer ( in_offset , in_size ) , caller = self . address , value = value , gas = gas ) raise StartTx ( )
5346	def compose_mailing_lists ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'mailing_lists' ] ) > 0 ] : if 'mailing_lists' not in projects [ p ] : projects [ p ] [ 'mailing_lists' ] = [ ] urls = [ url [ 'url' ] . replace ( 'mailto:' , '' ) for url in data [ p ] [ 'mailing_lists' ] if url [ 'url' ] not in projects [ p ] [ 'mailing_lists' ] ] projects [ p ] [ 'mailing_lists' ] += urls for p in [ project for project in data if len ( data [ project ] [ 'dev_list' ] ) > 0 ] : if 'mailing_lists' not in projects [ p ] : projects [ p ] [ 'mailing_lists' ] = [ ] mailing_list = data [ p ] [ 'dev_list' ] [ 'url' ] . replace ( 'mailto:' , '' ) projects [ p ] [ 'mailing_lists' ] . append ( mailing_list ) return projects
10164	def get_personalities ( self , line ) : return [ split ( '\W+' , i ) [ 1 ] for i in line . split ( ':' ) [ 1 ] . split ( ' ' ) if i . startswith ( '[' ) ]
8001	def fix_in_stanza ( self , stanza ) : StreamBase . fix_in_stanza ( self , stanza ) if not self . initiator : if stanza . from_jid != self . peer : stanza . set_from ( self . peer )
8068	def refresh ( self ) : self . reset ( ) self . parse ( self . source ) return self . output ( )
3831	async def search_entities ( self , search_entities_request ) : response = hangouts_pb2 . SearchEntitiesResponse ( ) await self . _pb_request ( 'contacts/searchentities' , search_entities_request , response ) return response
7191	def _load_info ( self ) : url = '%s/prefix?duration=36000' % self . base_url r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( )
8393	def parse_pylint_output ( pylint_output ) : for line in pylint_output : if not line . strip ( ) : continue if line [ 0 : 5 ] in ( "-" * 5 , "*" * 5 ) : continue parsed = PYLINT_PARSEABLE_REGEX . search ( line ) if parsed is None : LOG . warning ( u"Unable to parse %r. If this is a lint failure, please re-run pylint with the " u"--output-format=parseable option, otherwise, you can ignore this message." , line ) continue parsed_dict = parsed . groupdict ( ) parsed_dict [ 'linenum' ] = int ( parsed_dict [ 'linenum' ] ) yield PylintError ( * * parsed_dict )
8792	def validate ( self , value ) : try : vlan_id_int = int ( value ) assert vlan_id_int >= self . MIN_VLAN_ID assert vlan_id_int <= self . MAX_VLAN_ID except Exception : msg = ( "Invalid vlan_id. Got '%(vlan_id)s'. " "vlan_id should be an integer between %(min)d and %(max)d " "inclusive." % { 'vlan_id' : value , 'min' : self . MIN_VLAN_ID , 'max' : self . MAX_VLAN_ID } ) raise TagValidationError ( value , msg ) return True
7132	def prepare_docset ( source , dest , name , index_page , enable_js , online_redirect_url ) : resources = os . path . join ( dest , "Contents" , "Resources" ) docs = os . path . join ( resources , "Documents" ) os . makedirs ( resources ) db_conn = sqlite3 . connect ( os . path . join ( resources , "docSet.dsidx" ) ) db_conn . row_factory = sqlite3 . Row db_conn . execute ( "CREATE TABLE searchIndex(id INTEGER PRIMARY KEY, name TEXT, " "type TEXT, path TEXT)" ) db_conn . commit ( ) plist_path = os . path . join ( dest , "Contents" , "Info.plist" ) plist_cfg = { "CFBundleIdentifier" : name , "CFBundleName" : name , "DocSetPlatformFamily" : name . lower ( ) , "DashDocSetFamily" : "python" , "isDashDocset" : True , "isJavaScriptEnabled" : enable_js , } if index_page is not None : plist_cfg [ "dashIndexFilePath" ] = index_page if online_redirect_url is not None : plist_cfg [ "DashDocSetFallbackURL" ] = online_redirect_url write_plist ( plist_cfg , plist_path ) shutil . copytree ( source , docs ) return DocSet ( path = dest , docs = docs , plist = plist_path , db_conn = db_conn )
13545	def get_user ( self , user_id ) : url = "/2/users/%s" % user_id return self . user_from_json ( self . _get_resource ( url ) [ "user" ] )
7112	def serve ( self , port = 62000 ) : from http . server import HTTPServer , CGIHTTPRequestHandler os . chdir ( self . log_folder ) httpd = HTTPServer ( ( '' , port ) , CGIHTTPRequestHandler ) print ( "Starting LanguageBoard on port: " + str ( httpd . server_port ) ) webbrowser . open ( 'http://0.0.0.0:{}' . format ( port ) ) httpd . serve_forever ( )
6207	def merge_da ( self ) : print ( ' - Merging D and A timestamps' , flush = True ) ts_d , ts_par_d = self . S . get_timestamps_part ( self . name_timestamps_d ) ts_a , ts_par_a = self . S . get_timestamps_part ( self . name_timestamps_a ) ts , a_ch , part = merge_da ( ts_d , ts_par_d , ts_a , ts_par_a ) assert a_ch . sum ( ) == ts_a . shape [ 0 ] assert ( ~ a_ch ) . sum ( ) == ts_d . shape [ 0 ] assert a_ch . size == ts_a . shape [ 0 ] + ts_d . shape [ 0 ] self . ts , self . a_ch , self . part = ts , a_ch , part self . clk_p = ts_d . attrs [ 'clk_p' ]
11019	def show_response_messages ( response_json ) : message_type_kwargs = { 'warning' : { 'fg' : 'yellow' } , 'error' : { 'fg' : 'red' } , } for message in response_json . get ( 'messages' , [ ] ) : click . secho ( message [ 'text' ] , * * message_type_kwargs . get ( message [ 'type' ] , { } ) )
2840	def write_gpio ( self , gpio = None ) : if gpio is not None : self . gpio = gpio self . _device . writeList ( self . GPIO , self . gpio )
5010	def _call_post_with_user_override ( self , sap_user_id , url , payload ) : SAPSuccessFactorsEnterpriseCustomerConfiguration = apps . get_model ( # pylint: disable=invalid-name 'sap_success_factors' , 'SAPSuccessFactorsEnterpriseCustomerConfiguration' ) oauth_access_token , _ = SAPSuccessFactorsAPIClient . get_oauth_access_token ( self . enterprise_configuration . sapsf_base_url , self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . sapsf_company_id , sap_user_id , SAPSuccessFactorsEnterpriseCustomerConfiguration . USER_TYPE_USER ) response = requests . post ( url , data = payload , headers = { 'Authorization' : 'Bearer {}' . format ( oauth_access_token ) , 'content-type' : 'application/json' } ) return response . status_code , response . text
7571	def revcomp ( sequence ) : sequence = sequence [ : : - 1 ] . strip ( ) . replace ( "A" , "t" ) . replace ( "T" , "a" ) . replace ( "C" , "g" ) . replace ( "G" , "c" ) . upper ( ) return sequence
7594	def get_player ( self , * tags : crtag , * * params : keys ) : url = self . api . PLAYER + '/' + ',' . join ( tags ) return self . _get_model ( url , FullPlayer , * * params )
10481	def _generateChildren ( self ) : try : children = self . AXChildren except _a11y . Error : return if children : for child in children : yield child
11389	def can_run_from_cli ( self ) : ret = False ast_tree = ast . parse ( self . body , self . path ) calls = self . _find_calls ( ast_tree , __name__ , "exit" ) for call in calls : if re . search ( "{}\(" . format ( re . escape ( call ) ) , self . body ) : ret = True break return ret
5110	def simulate ( self , n = 1 , t = None , nA = None , nD = None ) : if t is None and nD is None and nA is None : for dummy in range ( n ) : self . next_event ( ) elif t is not None : then = self . _current_t + t while self . _current_t < then and self . _time < infty : self . next_event ( ) elif nD is not None : num_departures = self . num_departures + nD while self . num_departures < num_departures and self . _time < infty : self . next_event ( ) elif nA is not None : num_arrivals = self . _oArrivals + nA while self . _oArrivals < num_arrivals and self . _time < infty : self . next_event ( )
1739	def in_op ( self , other ) : if not is_object ( other ) : raise MakeError ( 'TypeError' , "You can\'t use 'in' operator to search in non-objects" ) return other . has_property ( to_string ( self ) )
6154	def position_CD ( Ka , out_type = 'fb_exact' ) : rs = 10 / ( 2 * np . pi ) # Load b and a ndarrays with the coefficients if out_type . lower ( ) == 'open_loop' : b = np . array ( [ Ka * 4000 * rs ] ) a = np . array ( [ 1 , 1275 , 31250 , 0 ] ) elif out_type . lower ( ) == 'fb_approx' : b = np . array ( [ 3.2 * Ka * rs ] ) a = np . array ( [ 1 , 25 , 3.2 * Ka * rs ] ) elif out_type . lower ( ) == 'fb_exact' : b = np . array ( [ 4000 * Ka * rs ] ) a = np . array ( [ 1 , 1250 + 25 , 25 * 1250 , 4000 * Ka * rs ] ) else : raise ValueError ( 'out_type must be: open_loop, fb_approx, or fc_exact' ) return b , a
498	def _addRecordToKNN ( self , record ) : knn = self . _knnclassifier . _knn prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) # If record is already in the classifier, overwrite its labeling if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return # Learn this pattern in the knn pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )
2225	def _update_hasher ( hasher , data , types = True ) : # Determine if the data should be hashed directly or iterated through if isinstance ( data , ( tuple , list , zip ) ) : needs_iteration = True else : needs_iteration = any ( check ( data ) for check in _HASHABLE_EXTENSIONS . iterable_checks ) if needs_iteration : # Denote that we are hashing over an iterable # Multiple structure bytes makes it harder accidently make conflicts SEP = b'_,_' ITER_PREFIX = b'_[_' ITER_SUFFIX = b'_]_' iter_ = iter ( data ) hasher . update ( ITER_PREFIX ) # first, try to nest quickly without recursive calls # (this works if all data in the sequence is a non-iterable) try : for item in iter_ : prefix , hashable = _convert_to_hashable ( item , types ) binary_data = prefix + hashable + SEP hasher . update ( binary_data ) except TypeError : # need to use recursive calls # Update based on current item _update_hasher ( hasher , item , types ) for item in iter_ : # Ensure the items have a spacer between them _update_hasher ( hasher , item , types ) hasher . update ( SEP ) hasher . update ( ITER_SUFFIX ) else : prefix , hashable = _convert_to_hashable ( data , types ) binary_data = prefix + hashable hasher . update ( binary_data )
1649	def GetLineWidth ( line ) : if isinstance ( line , unicode ) : width = 0 for uc in unicodedata . normalize ( 'NFC' , line ) : if unicodedata . east_asian_width ( uc ) in ( 'W' , 'F' ) : width += 2 elif not unicodedata . combining ( uc ) : width += 1 return width else : return len ( line )
10173	def get_bookmark ( self ) : if not Index ( self . aggregation_alias , using = self . client ) . exists ( ) : if not Index ( self . event_index , using = self . client ) . exists ( ) : return datetime . date . today ( ) return self . _get_oldest_event_timestamp ( ) # retrieve the oldest bookmark query_bookmark = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) [ 0 : 1 ] . sort ( { 'date' : { 'order' : 'desc' } } ) bookmarks = query_bookmark . execute ( ) # if no bookmark is found but the index exist, the bookmark was somehow # lost or never written, so restart from the beginning if len ( bookmarks ) == 0 : return self . _get_oldest_event_timestamp ( ) # change it to doc_id_suffix bookmark = datetime . datetime . strptime ( bookmarks [ 0 ] . date , self . doc_id_suffix ) return bookmark
749	def _removeUnlikelyPredictions ( cls , likelihoodsDict , minLikelihoodThreshold , maxPredictionsPerStep ) : maxVal = ( None , None ) for ( k , v ) in likelihoodsDict . items ( ) : if len ( likelihoodsDict ) <= 1 : break if maxVal [ 0 ] is None or v >= maxVal [ 1 ] : if maxVal [ 0 ] is not None and maxVal [ 1 ] < minLikelihoodThreshold : del likelihoodsDict [ maxVal [ 0 ] ] maxVal = ( k , v ) elif v < minLikelihoodThreshold : del likelihoodsDict [ k ] # Limit the number of predictions to include. likelihoodsDict = dict ( sorted ( likelihoodsDict . iteritems ( ) , key = itemgetter ( 1 ) , reverse = True ) [ : maxPredictionsPerStep ] ) return likelihoodsDict
10245	def create_timeline ( year_counter : typing . Counter [ int ] ) -> List [ Tuple [ int , int ] ] : if not year_counter : return [ ] from_year = min ( year_counter ) - 1 until_year = datetime . now ( ) . year + 1 return [ ( year , year_counter . get ( year , 0 ) ) for year in range ( from_year , until_year ) ]
13841	def _ConsumeSingleByteString ( self ) : text = self . token if len ( text ) < 1 or text [ 0 ] not in _QUOTES : raise self . _ParseError ( 'Expected string but found: %r' % ( text , ) ) if len ( text ) < 2 or text [ - 1 ] != text [ 0 ] : raise self . _ParseError ( 'String missing ending quote: %r' % ( text , ) ) try : result = text_encoding . CUnescape ( text [ 1 : - 1 ] ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
9880	def _distances ( value_domain , distance_metric , n_v ) : return np . array ( [ [ distance_metric ( v1 , v2 , i1 = i1 , i2 = i2 , n_v = n_v ) for i2 , v2 in enumerate ( value_domain ) ] for i1 , v1 in enumerate ( value_domain ) ] )
2460	def set_pkg_desc ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_desc_set : self . package_desc_set = True if validations . validate_pkg_desc ( text ) : doc . package . description = str_from_text ( text ) else : raise SPDXValueError ( 'Package::Description' ) else : raise CardinalityError ( 'Package::Description' )
1600	def str_cmd ( cmd , cwd , env ) : process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) stdout_builder , stderr_builder = proc . async_stdout_stderr_builder ( process ) process . wait ( ) stdout , stderr = stdout_builder . result ( ) , stderr_builder . result ( ) return { 'command' : ' ' . join ( cmd ) , 'stderr' : stderr , 'stdout' : stdout }
3985	def _dusty_hosts_config ( hosts_specs ) : rules = '' . join ( [ '{} {}\n' . format ( spec [ 'forwarded_ip' ] , spec [ 'host_address' ] ) for spec in hosts_specs ] ) return config_file . create_config_section ( rules )
9581	def eof ( fd ) : b = fd . read ( 1 ) end = len ( b ) == 0 if not end : curpos = fd . tell ( ) fd . seek ( curpos - 1 ) return end
8805	def build_full_day_ips ( query , period_start , period_end ) : # Filter out only IPv4 that have not been deallocated ip_list = query . filter ( models . IPAddress . version == 4L ) . filter ( models . IPAddress . network_id == PUBLIC_NETWORK_ID ) . filter ( models . IPAddress . used_by_tenant_id is not None ) . filter ( models . IPAddress . allocated_at != null ( ) ) . filter ( models . IPAddress . allocated_at < period_start ) . filter ( or_ ( models . IPAddress . _deallocated is False , models . IPAddress . deallocated_at == null ( ) , models . IPAddress . deallocated_at >= period_end ) ) . all ( ) return ip_list
4799	def is_file ( self ) : self . exists ( ) if not os . path . isfile ( self . val ) : self . _err ( 'Expected <%s> to be a file, but was not.' % self . val ) return self
5757	def get_homogeneous ( package_descriptors , targets , repos_data ) : homogeneous = { } for package_descriptor in package_descriptors . values ( ) : pkg_name = package_descriptor . pkg_name debian_pkg_name = package_descriptor . debian_pkg_name versions = [ ] for repo_data in repos_data : versions . append ( set ( [ ] ) ) for target in targets : version = _strip_version_suffix ( repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) ) versions [ - 1 ] . add ( version ) homogeneous [ pkg_name ] = max ( [ len ( v ) for v in versions ] ) == 1 return homogeneous
1304	def SendMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> int : return ctypes . windll . user32 . SendMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam )
2722	def _perform_action ( self , params , return_dict = True ) : action = self . get_data ( "droplets/%s/actions/" % self . id , type = POST , params = params ) if return_dict : return action else : action = action [ u'action' ] return_action = Action ( token = self . token ) # Loading attributes for attr in action . keys ( ) : setattr ( return_action , attr , action [ attr ] ) return return_action
13478	def _sentence_to_interstitial_spacing ( self ) : not_sentence_end_chars = [ ' ' ] abbreviations = [ 'i.e.' , 'e.g.' , ' v.' , ' w.' , ' wh.' ] titles = [ 'Prof.' , 'Mr.' , 'Mrs.' , 'Messrs.' , 'Mmes.' , 'Msgr.' , 'Ms.' , 'Fr.' , 'Rev.' , 'St.' , 'Dr.' , 'Lieut.' , 'Lt.' , 'Capt.' , 'Cptn.' , 'Sgt.' , 'Sjt.' , 'Gen.' , 'Hon.' , 'Cpl.' , 'L-Cpl.' , 'Pvt.' , 'Dvr.' , 'Gnr.' , 'Spr.' , 'Col.' , 'Lt-Col' , 'Lt-Gen.' , 'Mx.' ] for abbrev in abbreviations : for x in not_sentence_end_chars : self . _str_replacement ( abbrev + x , abbrev + '\ ' ) for title in titles : for x in not_sentence_end_chars : self . _str_replacement ( title + x , title + '~' )
10268	def main ( output ) : from hbp_knowledge import get_graph graph = get_graph ( ) text = to_html ( graph ) print ( text , file = output )
13059	def get_inventory ( self ) : if self . _inventory is not None : return self . _inventory self . _inventory = self . resolver . getMetadata ( ) return self . _inventory
13743	def get_schema ( self ) : if not self . schema : raise NotImplementedError ( 'You must provide a schema value or override the get_schema method' ) return self . conn . create_schema ( * * self . schema )
11565	def stepper_config ( self , steps_per_revolution , stepper_pins ) : data = [ self . STEPPER_CONFIGURE , steps_per_revolution & 0x7f , ( steps_per_revolution >> 7 ) & 0x7f ] for pin in range ( len ( stepper_pins ) ) : data . append ( stepper_pins [ pin ] ) self . _command_handler . send_sysex ( self . _command_handler . STEPPER_DATA , data )
3989	def _nginx_location_spec ( port_spec , bridge_ip ) : location_string_spec = "\t \t location / { \n" for location_setting in [ 'proxy_http_version 1.1;' , 'proxy_set_header Upgrade $http_upgrade;' , 'proxy_set_header Connection "upgrade";' , 'proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;' , 'proxy_set_header Host $http_host;' , _nginx_proxy_string ( port_spec , bridge_ip ) ] : location_string_spec += "\t \t \t {} \n" . format ( location_setting ) location_string_spec += "\t \t } \n" return location_string_spec
6183	def git_path_valid ( git_path = None ) : if git_path is None and GIT_PATH is None : return False if git_path is None : git_path = GIT_PATH try : call ( [ git_path , '--version' ] ) return True except OSError : return False
2829	def convert_upsample_bilinear ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting upsample...' ) if names == 'short' : tf_name = 'UPSL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) output_size = params [ 'output_size' ] align_corners = params [ 'align_corners' ] > 0 def target_layer ( x , size = output_size , align_corners = align_corners ) : import tensorflow as tf x = tf . transpose ( x , [ 0 , 2 , 3 , 1 ] ) x = tf . image . resize_images ( x , size , align_corners = align_corners ) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 ] ) return x lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
657	def averageOnTime ( vectors , numSamples = None ) : # Special case given a 1 dimensional vector: it represents a single column if vectors . ndim == 1 : vectors . shape = ( - 1 , 1 ) numTimeSteps = len ( vectors ) numElements = len ( vectors [ 0 ] ) # How many samples will we look at? if numSamples is None : numSamples = numElements countOn = range ( numElements ) else : countOn = numpy . random . randint ( 0 , numElements , numSamples ) # Compute the on-times and accumulate the frequency counts of each on-time # encountered sumOfLengths = 0.0 onTimeFreqCounts = None n = 0 for i in countOn : ( onTime , segments , durations ) = _listOfOnTimesInVec ( vectors [ : , i ] ) if onTime != 0.0 : sumOfLengths += onTime n += segments onTimeFreqCounts = _accumulateFrequencyCounts ( durations , onTimeFreqCounts ) # Return the average on time of each element that was on. if n > 0 : return ( sumOfLengths / n , onTimeFreqCounts ) else : return ( 0.0 , onTimeFreqCounts )
7997	def set_peer_authenticated ( self , peer , restart_stream = False ) : with self . lock : self . peer_authenticated = True self . peer = peer if restart_stream : self . _restart_stream ( ) self . event ( AuthenticatedEvent ( self . peer ) )
1283	def autolink ( self , link , is_email = False ) : text = link = escape ( link ) if is_email : link = 'mailto:%s' % link return '<a href="%s">%s</a>' % ( link , text )
8542	def _get_username ( self , username = None , use_config = True , config_filename = None ) : if not username and use_config : if self . _config is None : self . _read_config ( config_filename ) username = self . _config . get ( "credentials" , "username" , fallback = None ) if not username : username = input ( "Please enter your username: " ) . strip ( ) while not username : username = input ( "No username specified. Please enter your username: " ) . strip ( ) if 'credendials' not in self . _config : self . _config . add_section ( 'credentials' ) self . _config . set ( "credentials" , "username" , username ) self . _save_config ( ) return username
4785	def starts_with ( self , prefix ) : if prefix is None : raise TypeError ( 'given prefix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( prefix , str_types ) : raise TypeError ( 'given prefix arg must be a string' ) if len ( prefix ) == 0 : raise ValueError ( 'given prefix arg must not be empty' ) if not self . val . startswith ( prefix ) : self . _err ( 'Expected <%s> to start with <%s>, but did not.' % ( self . val , prefix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) first = next ( iter ( self . val ) ) if first != prefix : self . _err ( 'Expected %s to start with <%s>, but did not.' % ( self . val , prefix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
12247	def create_bucket ( self , * args , * * kwargs ) : bucket = super ( S3Connection , self ) . create_bucket ( * args , * * kwargs ) if bucket : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket
8759	def get_subnets ( context , limit = None , page_reverse = False , sorts = [ 'id' ] , marker = None , filters = None , fields = None ) : LOG . info ( "get_subnets for tenant %s with filters %s fields %s" % ( context . tenant_id , filters , fields ) ) filters = filters or { } subnets = db_api . subnet_find ( context , limit = limit , page_reverse = page_reverse , sorts = sorts , marker_obj = marker , join_dns = True , join_routes = True , join_pool = True , * * filters ) for subnet in subnets : cache = subnet . get ( "_allocation_pool_cache" ) if not cache : db_api . subnet_update_set_alloc_pool_cache ( context , subnet , subnet . allocation_pools ) return v . _make_subnets_list ( subnets , fields = fields )
1711	def _set_name ( self , name ) : if self . own . get ( 'name' ) : self . func_name = name self . own [ 'name' ] [ 'value' ] = Js ( name )
5024	def get_enterprise_customer ( uuid ) : if uuid is None : return None try : return EnterpriseCustomer . active_customers . get ( uuid = uuid ) except EnterpriseCustomer . DoesNotExist : raise CommandError ( _ ( 'Enterprise customer {uuid} not found, or not active' ) . format ( uuid = uuid ) )
10582	def set_parent_path ( self , value ) : self . _parent_path = value self . path = value + r'/' + self . name self . _update_childrens_parent_path ( )
8641	def highlight_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'highlight' } # POST /api/projects/0.1/bids/{bid_id}/?action=revoke endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotHighlightedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11	def logs ( self , prefix = 'worker' ) : logs = [ ] logs += [ ( 'success_rate' , np . mean ( self . success_history ) ) ] if self . compute_Q : logs += [ ( 'mean_Q' , np . mean ( self . Q_history ) ) ] logs += [ ( 'episode' , self . n_episodes ) ] if prefix != '' and not prefix . endswith ( '/' ) : return [ ( prefix + '/' + key , val ) for key , val in logs ] else : return logs
13751	def handle_data ( self , data ) : if data . strip ( ) : data = djeffify_string ( data ) self . djhtml += data
10477	def _leftMouseDragged ( self , stopCoord , strCoord , speed ) : # To direct output to the correct application need the PSN: appPid = self . _getPid ( ) # Get current position as start point if strCoord not given if strCoord == ( 0 , 0 ) : loc = AppKit . NSEvent . mouseLocation ( ) strCoord = ( loc . x , Quartz . CGDisplayPixelsHigh ( 0 ) - loc . y ) # To direct output to the correct application need the PSN: appPid = self . _getPid ( ) # Press left button down pressLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseDown , strCoord , Quartz . kCGMouseButtonLeft ) # Queue the events Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , pressLeftButton ) # Wait for reponse of system, a fuzzy icon appears time . sleep ( 5 ) # Simulate mouse moving speed, k is slope speed = round ( 1 / float ( speed ) , 2 ) xmoved = stopCoord [ 0 ] - strCoord [ 0 ] ymoved = stopCoord [ 1 ] - strCoord [ 1 ] if ymoved == 0 : raise ValueError ( 'Not support horizontal moving' ) else : k = abs ( ymoved / xmoved ) if xmoved != 0 : for xpos in range ( int ( abs ( xmoved ) ) ) : if xmoved > 0 and ymoved > 0 : currcoord = ( strCoord [ 0 ] + xpos , strCoord [ 1 ] + xpos * k ) elif xmoved > 0 and ymoved < 0 : currcoord = ( strCoord [ 0 ] + xpos , strCoord [ 1 ] - xpos * k ) elif xmoved < 0 and ymoved < 0 : currcoord = ( strCoord [ 0 ] - xpos , strCoord [ 1 ] - xpos * k ) elif xmoved < 0 and ymoved > 0 : currcoord = ( strCoord [ 0 ] - xpos , strCoord [ 1 ] + xpos * k ) # Drag with left button dragLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseDragged , currcoord , Quartz . kCGMouseButtonLeft ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , dragLeftButton ) # Wait for reponse of system time . sleep ( speed ) else : raise ValueError ( 'Not support vertical moving' ) upLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseUp , stopCoord , Quartz . kCGMouseButtonLeft ) # Wait for reponse of system, a plus icon appears time . sleep ( 5 ) # Up left button up Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , upLeftButton )
2476	def set_lic_comment ( self , doc , comment ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_comment_set : self . extr_lic_comment_set = True if validations . validate_is_free_form_text ( comment ) : self . extr_lic ( doc ) . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ExtractedLicense::comment' ) else : raise CardinalityError ( 'ExtractedLicense::comment' ) else : raise OrderError ( 'ExtractedLicense::comment' )
7357	def _check_peptide_lengths ( self , peptide_lengths = None ) : if not peptide_lengths : peptide_lengths = self . default_peptide_lengths if not peptide_lengths : raise ValueError ( ( "Must either provide 'peptide_lengths' argument " "or set 'default_peptide_lengths" ) ) if isinstance ( peptide_lengths , int ) : peptide_lengths = [ peptide_lengths ] require_iterable_of ( peptide_lengths , int ) for peptide_length in peptide_lengths : if ( self . min_peptide_length is not None and peptide_length < self . min_peptide_length ) : raise ValueError ( "Invalid peptide length %d, shorter than min %d" % ( peptide_length , self . min_peptide_length ) ) elif ( self . max_peptide_length is not None and peptide_length > self . max_peptide_length ) : raise ValueError ( "Invalid peptide length %d, longer than max %d" % ( peptide_length , self . max_peptide_length ) ) return peptide_lengths
8585	def get_attached_cdroms ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/cdroms?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
1057	def add_extension ( module , name , code ) : code = int ( code ) if not 1 <= code <= 0x7fffffff : raise ValueError , "code out of range" key = ( module , name ) if ( _extension_registry . get ( key ) == code and _inverted_registry . get ( code ) == key ) : return # Redundant registrations are benign if key in _extension_registry : raise ValueError ( "key %s is already registered with code %s" % ( key , _extension_registry [ key ] ) ) if code in _inverted_registry : raise ValueError ( "code %s is already in use for key %s" % ( code , _inverted_registry [ code ] ) ) _extension_registry [ key ] = code _inverted_registry [ code ] = key
4933	def get_content_id ( self , content_metadata_item ) : content_id = content_metadata_item . get ( 'key' , '' ) if content_metadata_item [ 'content_type' ] == 'program' : content_id = content_metadata_item . get ( 'uuid' , '' ) return content_id
2885	def n_subscribers ( self ) : hard = self . hard_subscribers and len ( self . hard_subscribers ) or 0 weak = self . weak_subscribers and len ( self . weak_subscribers ) or 0 return hard + weak
5868	def _activate_organization_course_relationship ( relationship ) : # pylint: disable=invalid-name # If the relationship doesn't exist or the organization isn't active we'll want to raise an error relationship = internal . OrganizationCourse . objects . get ( id = relationship . id , active = False , organization__active = True ) _activate_record ( relationship )
476	def data_to_token_ids ( data_path , target_path , vocabulary_path , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br"\d" ) ) : if not gfile . Exists ( target_path ) : tl . logging . info ( "Tokenizing data in %s" % data_path ) vocab , _ = initialize_vocabulary ( vocabulary_path ) with gfile . GFile ( data_path , mode = "rb" ) as data_file : with gfile . GFile ( target_path , mode = "w" ) as tokens_file : counter = 0 for line in data_file : counter += 1 if counter % 100000 == 0 : tl . logging . info ( " tokenizing line %d" % counter ) token_ids = sentence_to_token_ids ( line , vocab , tokenizer , normalize_digits , UNK_ID = UNK_ID , _DIGIT_RE = _DIGIT_RE ) tokens_file . write ( " " . join ( [ str ( tok ) for tok in token_ids ] ) + "\n" ) else : tl . logging . info ( "Target path %s exists" % target_path )
6935	def add_cmds_cpdir ( cpdir , cmdpkl , cpfileglob = 'checkplot*.pkl*' , require_cmd_magcolor = True , save_cmd_pngs = False ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return add_cmds_cplist ( cplist , cmdpkl , require_cmd_magcolor = require_cmd_magcolor , save_cmd_pngs = save_cmd_pngs )
10517	def setmax ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) object_handle . AXValue = 1 return 1
3494	def total_yield ( input_fluxes , input_elements , output_flux , output_elements ) : carbon_input_flux = sum ( total_components_flux ( flux , components , consumption = True ) for flux , components in zip ( input_fluxes , input_elements ) ) carbon_output_flux = total_components_flux ( output_flux , output_elements , consumption = False ) try : return carbon_output_flux / carbon_input_flux except ZeroDivisionError : return nan
10464	def verifymenucheck ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) try : if menu_handle . AXMenuItemMarkChar : # Checked return 1 except atomac . _a11y . Error : pass except LdtpServerException : pass return 0
12244	def styblinski_tang ( theta ) : x , y = theta obj = 0.5 * ( x ** 4 - 16 * x ** 2 + 5 * x + y ** 4 - 16 * y ** 2 + 5 * y ) grad = np . array ( [ 2 * x ** 3 - 16 * x + 2.5 , 2 * y ** 3 - 16 * y + 2.5 , ] ) return obj , grad
6263	def check_glfw_version ( self ) : print ( "glfw version: {} (python wrapper version {})" . format ( glfw . get_version ( ) , glfw . __version__ ) ) if glfw . get_version ( ) < self . min_glfw_version : raise ValueError ( "Please update glfw binaries to version {} or later" . format ( self . min_glfw_version ) )
11681	def send ( self , command , timeout = 5 ) : logger . info ( u'Sending %s' % command ) _ , writable , __ = select . select ( [ ] , [ self . sock ] , [ ] , timeout ) if not writable : raise SendTimeoutError ( ) writable [ 0 ] . sendall ( command + '\n' )
6075	def einstein_radius_in_units ( self , unit_length = 'arcsec' , kpc_per_arcsec = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . einstein_radius_in_units ( unit_length = unit_length , kpc_per_arcsec = kpc_per_arcsec ) , self . mass_profiles ) ) else : return None
4153	def rst2md ( text ) : top_heading = re . compile ( r'^=+$\s^([\w\s-]+)^=+$' , flags = re . M ) text = re . sub ( top_heading , r'# \1' , text ) math_eq = re . compile ( r'^\.\. math::((?:.+)?(?:\n+^ .+)*)' , flags = re . M ) text = re . sub ( math_eq , lambda match : r'$${0}$$' . format ( match . group ( 1 ) . strip ( ) ) , text ) inline_math = re . compile ( r':math:`(.+)`' ) text = re . sub ( inline_math , r'$\1$' , text ) return text
7456	def splitfiles ( data , raws , ipyclient ) : ## create a tmpdir for chunked_files and a chunk optimizer tmpdir = os . path . join ( data . paramsdict [ "project_dir" ] , "tmp-chunks-" + data . name ) if os . path . exists ( tmpdir ) : shutil . rmtree ( tmpdir ) os . makedirs ( tmpdir ) ## chunk into 8M reads totalreads = estimate_optim ( data , raws [ 0 ] [ 0 ] , ipyclient ) optim = int ( 8e6 ) njobs = int ( totalreads / ( optim / 4. ) ) * len ( raws ) ## if more files than cpus: no chunking nosplit = 0 if ( len ( raws ) > len ( ipyclient ) ) or ( totalreads < optim ) : nosplit = 1 ## send slices N at a time. The dict chunkfiles stores a tuple of rawpairs ## dictionary to store asyncresults for sorting jobs start = time . time ( ) chunkfiles = { } for fidx , tups in enumerate ( raws ) : handle = os . path . splitext ( os . path . basename ( tups [ 0 ] ) ) [ 0 ] ## if number of lines is > 20M then just submit it if nosplit : chunkfiles [ handle ] = [ tups ] else : ## chunk the file using zcat_make_temps chunklist = zcat_make_temps ( data , tups , fidx , tmpdir , optim , njobs , start ) chunkfiles [ handle ] = chunklist if not nosplit : print ( "" ) return chunkfiles
9884	def _read_all_z_variable_info ( self ) : self . z_variable_info = { } self . z_variable_names_by_num = { } # call Fortran that grabs all of the basic stats on all of the # zVariables in one go. info = fortran_cdf . z_var_all_inquire ( self . fname , self . _num_z_vars , len ( self . fname ) ) status = info [ 0 ] data_types = info [ 1 ] num_elems = info [ 2 ] rec_varys = info [ 3 ] dim_varys = info [ 4 ] num_dims = info [ 5 ] dim_sizes = info [ 6 ] rec_nums = info [ 7 ] var_nums = info [ 8 ] var_names = info [ 9 ] if status == 0 : for i in np . arange ( len ( data_types ) ) : out = { } out [ 'data_type' ] = data_types [ i ] out [ 'num_elems' ] = num_elems [ i ] out [ 'rec_vary' ] = rec_varys [ i ] out [ 'dim_varys' ] = dim_varys [ i ] out [ 'num_dims' ] = num_dims [ i ] # only looking at first possible extra dimension out [ 'dim_sizes' ] = dim_sizes [ i , : 1 ] if out [ 'dim_sizes' ] [ 0 ] == 0 : out [ 'dim_sizes' ] [ 0 ] += 1 out [ 'rec_num' ] = rec_nums [ i ] out [ 'var_num' ] = var_nums [ i ] var_name = '' . join ( var_names [ i ] . astype ( 'U' ) ) out [ 'var_name' ] = var_name . rstrip ( ) self . z_variable_info [ out [ 'var_name' ] ] = out self . z_variable_names_by_num [ out [ 'var_num' ] ] = var_name else : raise IOError ( fortran_cdf . statusreporter ( status ) )
1912	def SInt ( value , width ) : return Operators . ITEBV ( width , Bit ( value , width - 1 ) == 1 , GetNBits ( value , width ) - 2 ** width , GetNBits ( value , width ) )
2274	def _win32_rmtree ( path , verbose = 0 ) : # --- old version using the shell --- # def _rmjunctions(root): # subdirs = [] # for type_or_size, name, pointed in _win32_dir(root): # if type_or_size == '<DIR>': # subdirs.append(name) # elif type_or_size == '<JUNCTION>': # # remove any junctions as we encounter them # # os.unlink(join(root, name)) # os.rmdir(join(root, name)) # # recurse in all real directories # for name in subdirs: # _rmjunctions(join(root, name)) def _rmjunctions ( root ) : subdirs = [ ] for name in os . listdir ( root ) : current = join ( root , name ) if os . path . isdir ( current ) : if _win32_is_junction ( current ) : # remove any junctions as we encounter them os . rmdir ( current ) elif not os . path . islink ( current ) : subdirs . append ( current ) # recurse in all real directories for subdir in subdirs : _rmjunctions ( subdir ) if _win32_is_junction ( path ) : if verbose : print ( 'Deleting <JUNCTION> directory="{}"' . format ( path ) ) os . rmdir ( path ) else : if verbose : print ( 'Deleting directory="{}"' . format ( path ) ) # first remove all junctions _rmjunctions ( path ) # now we can rmtree as normal import shutil shutil . rmtree ( path )
11761	def refresh ( self ) : # `values_list('name', 'value')` doesn't work because `value` is not a # setting (base class) field, it's a setting value (subclass) field. So # we have to get real instances. args = [ ( obj . name , obj . value ) for obj in self . queryset . all ( ) ] super ( SettingDict , self ) . update ( args ) self . empty_cache = False
4973	def clean_channel_worker_username ( self ) : channel_worker_username = self . cleaned_data [ 'channel_worker_username' ] . strip ( ) try : User . objects . get ( username = channel_worker_username ) except User . DoesNotExist : raise ValidationError ( ValidationMessages . INVALID_CHANNEL_WORKER . format ( channel_worker_username = channel_worker_username ) ) return channel_worker_username
4384	def allow ( self , roles , methods , with_children = True ) : def decorator ( view_func ) : _methods = [ m . upper ( ) for m in methods ] for r , m , v in itertools . product ( roles , _methods , [ view_func . __name__ ] ) : self . before_acl [ 'allow' ] . append ( ( r , m , v , with_children ) ) return view_func return decorator
6412	def agmean ( nums ) : m_a = amean ( nums ) m_g = gmean ( nums ) if math . isnan ( m_a ) or math . isnan ( m_g ) : return float ( 'nan' ) while round ( m_a , 12 ) != round ( m_g , 12 ) : m_a , m_g = ( m_a + m_g ) / 2 , ( m_a * m_g ) ** ( 1 / 2 ) return m_a
8543	def _get_password ( self , password , use_config = True , config_filename = None , use_keyring = HAS_KEYRING ) : if not password and use_config : if self . _config is None : self . _read_config ( config_filename ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if not password and use_keyring : logger = logging . getLogger ( __name__ ) question = ( "Please enter your password for {} on {}: " . format ( self . username , self . host_base ) ) if HAS_KEYRING : password = keyring . get_password ( self . keyring_identificator , self . username ) if password is None : password = getpass . getpass ( question ) try : keyring . set_password ( self . keyring_identificator , self . username , password ) except keyring . errors . PasswordSetError as error : logger . warning ( "Storing password in keyring '%s' failed: %s" , self . keyring_identificator , error ) else : logger . warning ( "Install the 'keyring' Python module to store your password " "securely in your keyring!" ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if password is None : password = getpass . getpass ( question ) store_plaintext_passwords = self . _config . get ( "preferences" , "store-plaintext-passwords" , fallback = None ) if store_plaintext_passwords != "no" : question = ( "Do you want to store your password in plain text in " + self . _config_filename ( ) ) answer = ask ( question , [ "yes" , "no" , "never" ] , "no" ) if answer == "yes" : self . _config . set ( "credentials" , "password" , password ) self . _save_config ( ) elif answer == "never" : if "preferences" not in self . _config : self . _config . add_section ( "preferences" ) self . _config . set ( "preferences" , "store-plaintext-passwords" , "no" ) self . _save_config ( ) return password
10564	def get_supported_filepaths ( filepaths , supported_extensions , max_depth = float ( 'inf' ) ) : supported_filepaths = [ ] for path in filepaths : if os . name == 'nt' and CYGPATH_RE . match ( path ) : path = convert_cygwin_path ( path ) if os . path . isdir ( path ) : for root , __ , files in walk_depth ( path , max_depth ) : for f in files : if f . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( os . path . join ( root , f ) ) elif os . path . isfile ( path ) and path . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( path ) return supported_filepaths
9161	def delete_acl_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted = request . json permissions = [ ( x [ 'uid' ] , x [ 'permission' ] , ) for x in posted ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_acl ( cursor , uuid_ , permissions ) resp = request . response resp . status_int = 200 return resp
2673	def invoke ( src , event_file = 'event.json' , config_file = 'config.yaml' , profile_name = None , verbose = False , ) : # Load and parse the config file. path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) # Set AWS_PROFILE environment variable based on `--profile` option. if profile_name : os . environ [ 'AWS_PROFILE' ] = profile_name # Load environment variables from the config file into the actual # environment. env_vars = cfg . get ( 'environment_variables' ) if env_vars : for key , value in env_vars . items ( ) : os . environ [ key ] = get_environment_variable_value ( value ) # Load and parse event file. path_to_event_file = os . path . join ( src , event_file ) event = read ( path_to_event_file , loader = json . loads ) # Tweak to allow module to import local modules try : sys . path . index ( src ) except ValueError : sys . path . append ( src ) handler = cfg . get ( 'handler' ) # Inspect the handler string (<module>.<function name>) and translate it # into a function we can execute. fn = get_callable_handler_function ( src , handler ) timeout = cfg . get ( 'timeout' ) if timeout : context = LambdaContext ( cfg . get ( 'function_name' ) , timeout ) else : context = LambdaContext ( cfg . get ( 'function_name' ) ) start = time . time ( ) results = fn ( event , context ) end = time . time ( ) print ( '{0}' . format ( results ) ) if verbose : print ( '\nexecution time: {:.8f}s\nfunction execution ' 'timeout: {:2}s' . format ( end - start , cfg . get ( 'timeout' , 15 ) ) )
11900	def _get_thumbnail_image_from_file ( dir_path , image_file ) : # Get image img = _get_image_from_file ( dir_path , image_file ) # If it's not supported, exit now if img is None : return None if img . format . lower ( ) == 'gif' : return None # Get image dimensions img_width , img_height = img . size # We need to perform a resize - first, work out the scale ratio to take the # image width to THUMBNAIL_WIDTH (THUMBNAIL_WIDTH:img_width ratio) scale_ratio = THUMBNAIL_WIDTH / float ( img_width ) # Work out target image height based on the scale ratio target_height = int ( scale_ratio * img_height ) # Perform the resize try : img . thumbnail ( ( THUMBNAIL_WIDTH , target_height ) , resample = RESAMPLE ) except IOError as exptn : print ( 'WARNING: IOError when thumbnailing %s/%s: %s' % ( dir_path , image_file , exptn ) ) return None # Return the resized image return img
12389	def parse ( specifiers ) : specifiers = "" . join ( specifiers . split ( ) ) for specifier in specifiers . split ( ',' ) : if len ( specifier ) == 0 : raise ValueError ( "Range: Invalid syntax; missing specifier." ) count = specifier . count ( '-' ) if ( count and specifier [ 0 ] == '-' ) or not count : # Single specifier; return as a tuple to itself. yield int ( specifier ) , int ( specifier ) continue specifier = list ( map ( int , specifier . split ( '-' ) ) ) if len ( specifier ) == 2 : # Range specifier; return as a tuple. if specifier [ 0 ] < 0 or specifier [ 1 ] < 0 : # Negative indexing is not supported in range specifiers # as stated in the HTTP/1.1 Range header specification. raise ValueError ( "Range: Invalid syntax; negative indexing " "not supported in a range specifier." ) if specifier [ 1 ] < specifier [ 0 ] : # Range must be for at least one item. raise ValueError ( "Range: Invalid syntax; stop is less than start." ) # Return them as a immutable tuple. yield tuple ( specifier ) continue # Something weird happened. raise ValueError ( "Range: Invalid syntax." )
7154	def prepare_options ( options ) : options_ , verbose_options = [ ] , [ ] for option in options : if is_string ( option ) : options_ . append ( option ) verbose_options . append ( option ) else : options_ . append ( option [ 0 ] ) verbose_options . append ( option [ 1 ] ) return options_ , verbose_options
13667	def execute ( self , command , timeout = None ) : try : self . channel = self . ssh . get_transport ( ) . open_session ( ) except paramiko . SSHException as e : self . unknown ( "Create channel error: %s" % e ) try : self . channel . settimeout ( self . args . timeout if not timeout else timeout ) except socket . timeout as e : self . unknown ( "Settimeout for channel error: %s" % e ) try : self . logger . debug ( "command: {}" . format ( command ) ) self . channel . exec_command ( command ) except paramiko . SSHException as e : self . unknown ( "Execute command error: %s" % e ) try : self . stdin = self . channel . makefile ( 'wb' , - 1 ) self . stderr = map ( string . strip , self . channel . makefile_stderr ( 'rb' , - 1 ) . readlines ( ) ) self . stdout = map ( string . strip , self . channel . makefile ( 'rb' , - 1 ) . readlines ( ) ) except Exception as e : self . unknown ( "Get result error: %s" % e ) try : self . status = self . channel . recv_exit_status ( ) except paramiko . SSHException as e : self . unknown ( "Get return code error: %s" % e ) else : if self . status != 0 : self . unknown ( "Return code: %d , stderr: %s" % ( self . status , self . errors ) ) else : return self . stdout finally : self . logger . debug ( "Execute command finish." )
6294	def transform ( self , program : moderngl . Program , buffer : moderngl . Buffer , mode = None , vertices = - 1 , first = 0 , instances = 1 ) : vao = self . instance ( program ) if mode is None : mode = self . mode vao . transform ( buffer , mode = mode , vertices = vertices , first = first , instances = instances )
9998	def cellsiter_to_dataframe ( cellsiter , args , drop_allna = True ) : from modelx . core . cells import shareable_parameters if len ( args ) : indexes = shareable_parameters ( cellsiter ) else : indexes = get_all_params ( cellsiter . values ( ) ) result = None for cells in cellsiter . values ( ) : df = cells_to_dataframe ( cells , args ) if drop_allna and df . isnull ( ) . all ( ) . all ( ) : continue # Ignore all NA or empty if df . index . names != [ None ] : if isinstance ( df . index , pd . MultiIndex ) : if _pd_ver < ( 0 , 20 ) : df = _reset_naindex ( df ) df = df . reset_index ( ) missing_params = set ( indexes ) - set ( df ) for params in missing_params : df [ params ] = np . nan if result is None : result = df else : try : result = pd . merge ( result , df , how = "outer" ) except MergeError : # When no common column exists, i.e. all cells are scalars. result = pd . concat ( [ result , df ] , axis = 1 ) except ValueError : # When common columns are not coercible (numeric vs object), # Make the numeric column object type cols = set ( result . columns ) & set ( df . columns ) for col in cols : # When only either of them has object dtype if ( len ( [ str ( frame [ col ] . dtype ) for frame in ( result , df ) if str ( frame [ col ] . dtype ) == "object" ] ) == 1 ) : if str ( result [ col ] . dtype ) == "object" : frame = df else : frame = result frame [ [ col ] ] = frame [ col ] . astype ( "object" ) # Try again result = pd . merge ( result , df , how = "outer" ) if result is None : return pd . DataFrame ( ) else : return result . set_index ( indexes ) if indexes else result
3192	def create ( self , list_id , data ) : self . list_id = list_id if 'status' not in data : raise KeyError ( 'The list member must have a status' ) if data [ 'status' ] not in [ 'subscribed' , 'unsubscribed' , 'cleaned' , 'pending' , 'transactional' ] : raise ValueError ( 'The list member status must be one of "subscribed", "unsubscribed", "cleaned", ' '"pending", or "transactional"' ) if 'email_address' not in data : raise KeyError ( 'The list member must have an email_address' ) check_email ( data [ 'email_address' ] ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' ) , data = data ) if response is not None : self . subscriber_hash = response [ 'id' ] else : self . subscriber_hash = None return response
13846	def __get_numbered_paths ( filepath ) : format = '%s (%%d)%s' % splitext_files_only ( filepath ) return map ( lambda n : format % n , itertools . count ( 1 ) )
5629	def _get_digest ( self ) : return hmac . new ( self . _secret , request . data , hashlib . sha1 ) . hexdigest ( ) if self . _secret else None
7864	def main ( ) : parser = argparse . ArgumentParser ( description = 'XMPP version checker' , parents = [ XMPPSettings . get_arg_parser ( ) ] ) parser . add_argument ( 'source' , metavar = 'SOURCE' , help = 'Source JID' ) parser . add_argument ( 'target' , metavar = 'TARGET' , nargs = '?' , help = 'Target JID (default: domain of SOURCE)' ) parser . add_argument ( '--debug' , action = 'store_const' , dest = 'log_level' , const = logging . DEBUG , default = logging . INFO , help = 'Print debug messages' ) parser . add_argument ( '--quiet' , const = logging . ERROR , action = 'store_const' , dest = 'log_level' , help = 'Print only error messages' ) args = parser . parse_args ( ) settings = XMPPSettings ( ) settings . load_arguments ( args ) if settings . get ( "password" ) is None : password = getpass ( "{0!r} password: " . format ( args . source ) ) if sys . version_info . major < 3 : password = password . decode ( "utf-8" ) settings [ "password" ] = password if sys . version_info . major < 3 : args . source = args . source . decode ( "utf-8" ) source = JID ( args . source ) if args . target : if sys . version_info . major < 3 : args . target = args . target . decode ( "utf-8" ) target = JID ( args . target ) else : target = JID ( source . domain ) logging . basicConfig ( level = args . log_level ) checker = VersionChecker ( source , target , settings ) try : checker . run ( ) except KeyboardInterrupt : checker . disconnect ( )
4969	def get_catalog_options ( self ) : # TODO: We will remove the discovery service catalog implementation # once we have fully migrated customer's to EnterpriseCustomerCatalogs. # For now, this code will prevent an admin from creating a new # EnterpriseCustomer with a discovery service catalog. They will have to first # save the EnterpriseCustomer admin form and then edit the EnterpriseCustomer # to add a discovery service catalog. if hasattr ( self . instance , 'site' ) : catalog_api = CourseCatalogApiClient ( self . user , self . instance . site ) else : catalog_api = CourseCatalogApiClient ( self . user ) catalogs = catalog_api . get_all_catalogs ( ) # order catalogs by name. catalogs = sorted ( catalogs , key = lambda catalog : catalog . get ( 'name' , '' ) . lower ( ) ) return BLANK_CHOICE_DASH + [ ( catalog [ 'id' ] , catalog [ 'name' ] , ) for catalog in catalogs ]
8565	def update_loadbalancer ( self , datacenter_id , loadbalancer_id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
4600	def detach ( self , overlay ) : # See #868 for i , a in enumerate ( self . animations ) : a . layout = a . layout . clone ( ) if overlay and i : a . preclear = False
13065	def expose_ancestors_or_children ( self , member , collection , lang = None ) : x = { "id" : member . id , "label" : str ( member . get_label ( lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size , "semantic" : self . semantic ( member , parent = collection ) } if isinstance ( member , ResourceCollection ) : x [ "lang" ] = str ( member . lang ) return x
10690	def render ( self , format = ReportFormat . printout ) : table = self . _generate_table_ ( ) if format == ReportFormat . printout : print ( tabulate ( table , headers = "firstrow" , tablefmt = "simple" ) ) elif format == ReportFormat . latex : self . _render_latex_ ( table ) elif format == ReportFormat . txt : self . _render_txt_ ( table ) elif format == ReportFormat . csv : self . _render_csv_ ( table ) elif format == ReportFormat . string : return str ( tabulate ( table , headers = "firstrow" , tablefmt = "simple" ) ) elif format == ReportFormat . matplotlib : self . _render_matplotlib_ ( ) elif format == ReportFormat . png : if self . output_path is None : self . _render_matplotlib_ ( ) else : self . _render_matplotlib_ ( True )
13806	def validate_params ( required , optional , params ) : missing_fields = [ x for x in required if x not in params ] if missing_fields : field_strings = ", " . join ( missing_fields ) raise Exception ( "Missing fields: %s" % field_strings ) disallowed_fields = [ x for x in params if x not in optional and x not in required ] if disallowed_fields : field_strings = ", " . join ( disallowed_fields ) raise Exception ( "Disallowed fields: %s" % field_strings )
1582	def read ( self , dispatcher ) : try : if not self . is_header_read : # try reading header to_read = HeronProtocol . HEADER_SIZE - len ( self . header ) self . header += dispatcher . recv ( to_read ) if len ( self . header ) == HeronProtocol . HEADER_SIZE : self . is_header_read = True else : Log . debug ( "Header read incomplete; read %d bytes of header" % len ( self . header ) ) return if self . is_header_read and not self . is_complete : # try reading data to_read = self . get_datasize ( ) - len ( self . data ) self . data += dispatcher . recv ( to_read ) if len ( self . data ) == self . get_datasize ( ) : self . is_complete = True except socket . error as e : if e . errno == socket . errno . EAGAIN or e . errno == socket . errno . EWOULDBLOCK : # Try again later -> call continue_read later Log . debug ( "Try again error" ) else : # Fatal error Log . debug ( "Fatal error when reading IncomingPacket" ) raise RuntimeError ( "Fatal error occured in IncomingPacket.read()" )
7062	def sqs_put_item ( queue_url , item , delay_seconds = 0 , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : json_msg = json . dumps ( item ) resp = client . send_message ( QueueUrl = queue_url , MessageBody = json_msg , DelaySeconds = delay_seconds , ) if not resp : LOGERROR ( 'could not send item to queue: %s' % queue_url ) return None else : return resp except Exception as e : LOGEXCEPTION ( 'could not send item to queue: %s' % queue_url ) if raiseonfail : raise return None
13054	def nmap_smb_vulnscan ( ) : service_search = ServiceSearch ( ) services = service_search . get_services ( ports = [ '445' ] , tags = [ '!smb_vulnscan' ] , up = True ) services = [ service for service in services ] service_dict = { } for service in services : service . add_tag ( 'smb_vulnscan' ) service_dict [ str ( service . address ) ] = service nmap_args = "-Pn -n --disable-arp-ping --script smb-security-mode.nse,smb-vuln-ms17-010.nse -p 445" . split ( " " ) if services : result = nmap ( nmap_args , [ str ( s . address ) for s in services ] ) parser = NmapParser ( ) report = parser . parse_fromstring ( result ) smb_signing = 0 ms17 = 0 for nmap_host in report . hosts : for script_result in nmap_host . scripts_results : script_result = script_result . get ( 'elements' , { } ) service = service_dict [ str ( nmap_host . address ) ] if script_result . get ( 'message_signing' , '' ) == 'disabled' : print_success ( "({}) SMB Signing disabled" . format ( nmap_host . address ) ) service . add_tag ( 'smb_signing_disabled' ) smb_signing += 1 if script_result . get ( 'CVE-2017-0143' , { } ) . get ( 'state' , '' ) == 'VULNERABLE' : print_success ( "({}) Vulnerable for MS17-010" . format ( nmap_host . address ) ) service . add_tag ( 'MS17-010' ) ms17 += 1 service . update ( tags = service . tags ) print_notification ( "Completed, 'smb_signing_disabled' tag added to systems with smb signing disabled, 'MS17-010' tag added to systems that did not apply MS17-010." ) stats = { 'smb_signing' : smb_signing , 'MS17_010' : ms17 , 'scanned_services' : len ( services ) } Logger ( ) . log ( 'smb_vulnscan' , 'Scanned {} smb services for vulnerabilities' . format ( len ( services ) ) , stats ) else : print_notification ( "No services found to scan." )
1602	def parse_topo_loc ( cl_args ) : try : topo_loc = cl_args [ 'cluster/[role]/[env]' ] . split ( '/' ) topo_name = cl_args [ 'topology-name' ] topo_loc . append ( topo_name ) if len ( topo_loc ) != 4 : raise return topo_loc except Exception : Log . error ( 'Invalid topology location' ) raise
2939	def deserialize_logical ( self , node ) : term1_attrib = node . getAttribute ( 'left-field' ) term1_value = node . getAttribute ( 'left-value' ) op = node . nodeName . lower ( ) term2_attrib = node . getAttribute ( 'right-field' ) term2_value = node . getAttribute ( 'right-value' ) if op not in _op_map : _exc ( 'Invalid operator' ) if term1_attrib != '' and term1_value != '' : _exc ( 'Both, left-field and left-value attributes found' ) elif term1_attrib == '' and term1_value == '' : _exc ( 'left-field or left-value attribute required' ) elif term1_value != '' : left = term1_value else : left = operators . Attrib ( term1_attrib ) if term2_attrib != '' and term2_value != '' : _exc ( 'Both, right-field and right-value attributes found' ) elif term2_attrib == '' and term2_value == '' : _exc ( 'right-field or right-value attribute required' ) elif term2_value != '' : right = term2_value else : right = operators . Attrib ( term2_attrib ) return _op_map [ op ] ( left , right )
6754	def all_other_enabled_satchels ( self ) : return dict ( ( name , satchel ) for name , satchel in self . all_satchels . items ( ) if name != self . name . upper ( ) and name . lower ( ) in map ( str . lower , self . genv . services ) )
12322	def to_holvi_dict ( self ) : self . _jsondata [ "items" ] = [ ] for item in self . items : self . _jsondata [ "items" ] . append ( item . to_holvi_dict ( ) ) self . _jsondata [ "issue_date" ] = self . issue_date . isoformat ( ) self . _jsondata [ "due_date" ] = self . due_date . isoformat ( ) self . _jsondata [ "receiver" ] = self . receiver . to_holvi_dict ( ) return { k : v for ( k , v ) in self . _jsondata . items ( ) if k in self . _valid_keys }
9728	def get_analog ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . device_count ) : component_position , device = QRTPacket . _get_exact ( RTAnalogDevice , data , component_position ) if device . sample_count > 0 : component_position , sample_number = QRTPacket . _get_exact ( RTSampleNumber , data , component_position ) RTAnalogChannel . format = struct . Struct ( RTAnalogChannel . format_str % device . sample_count ) for _ in range ( device . channel_count ) : component_position , channel = QRTPacket . _get_tuple ( RTAnalogChannel , data , component_position ) append_components ( ( device , sample_number , channel ) ) return components
9514	def is_complete_orf ( self ) : if len ( self ) % 3 != 0 or len ( self ) < 6 : return False orfs = self . orfs ( ) complete_orf = intervals . Interval ( 0 , len ( self ) - 1 ) for orf in orfs : if orf == complete_orf : return True return False
11833	def make_undirected ( self ) : for a in self . dict . keys ( ) : for ( b , distance ) in self . dict [ a ] . items ( ) : self . connect1 ( b , a , distance )
5402	def _get_prepare_env ( self , script , job_descriptor , inputs , outputs , mounts ) : # Add the _SCRIPT_REPR with the repr(script) contents # Add the _META_YAML_REPR with the repr(meta) contents # Add variables for directories that need to be created, for example: # DIR_COUNT: 2 # DIR_0: /mnt/data/input/gs/bucket/path1/ # DIR_1: /mnt/data/output/gs/bucket/path2 # List the directories in sorted order so that they are created in that # order. This is primarily to ensure that permissions are set as we create # each directory. # For example: # mkdir -m 777 -p /root/first/second # mkdir -m 777 -p /root/first # *may* not actually set 777 on /root/first docker_paths = sorted ( [ var . docker_path if var . recursive else os . path . dirname ( var . docker_path ) for var in inputs | outputs | mounts if var . value ] ) env = { _SCRIPT_VARNAME : repr ( script . value ) , _META_YAML_VARNAME : repr ( job_descriptor . to_yaml ( ) ) , 'DIR_COUNT' : str ( len ( docker_paths ) ) } for idx , path in enumerate ( docker_paths ) : env [ 'DIR_{}' . format ( idx ) ] = os . path . join ( providers_util . DATA_MOUNT_POINT , path ) return env
7229	def paint ( self ) : snippet = { 'heatmap-radius' : VectorStyle . get_style_value ( self . radius ) , 'heatmap-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'heatmap-color' : VectorStyle . get_style_value ( self . color ) , 'heatmap-intensity' : VectorStyle . get_style_value ( self . intensity ) , 'heatmap-weight' : VectorStyle . get_style_value ( self . weight ) } return snippet
9232	def fetch_date_of_tag ( self , tag ) : if self . options . verbose > 1 : print ( "\tFetching date for tag {}" . format ( tag [ "name" ] ) ) gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . git . commits [ tag [ "commit" ] [ "sha" ] ] . get ( ) if rc == 200 : return data [ "committer" ] [ "date" ] self . raise_GitHubError ( rc , data , gh . getheaders ( ) )
13331	def add ( name , path , branch , type ) : if not name and not path : ctx = click . get_current_context ( ) click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples:\n' ' cpenv module add my_module ./path/to/my_module\n' ' cpenv module add my_module git@github.com:user/my_module.git' ' cpenv module add my_module git@github.com:user/my_module.git --branch=master --type=shared' ) click . echo ( examples ) return if not name : click . echo ( 'Missing required argument: name' ) return if not path : click . echo ( 'Missing required argument: path' ) env = cpenv . get_active_env ( ) if type == 'local' : if not env : click . echo ( '\nActivate an environment to add a local module.\n' ) return if click . confirm ( '\nAdd {} to active env {}?' . format ( name , env . name ) ) : click . echo ( 'Adding module...' , nl = False ) try : env . add_module ( name , path , branch ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) ) return module_paths = cpenv . get_module_paths ( ) click . echo ( '\nAvailable module paths:\n' ) for i , mod_path in enumerate ( module_paths ) : click . echo ( ' {}. {}' . format ( i , mod_path ) ) choice = click . prompt ( 'Where do you want to add your module?' , type = int , default = 0 ) module_root = module_paths [ choice ] module_path = utils . unipath ( module_root , name ) click . echo ( 'Creating module {}...' . format ( module_path ) , nl = False ) try : cpenv . create_module ( module_path , path , branch ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) )
12982	def file ( file_object , start_on = None , ignore = ( ) , use_short = True , * * queries ) : return string ( file_object . read ( ) , start_on = start_on , ignore = ignore , use_short = use_short , * * queries )
2984	def set_cors_headers ( resp , options ) : # If CORS has already been evaluated via the decorator, skip if hasattr ( resp , FLASK_CORS_EVALUATED ) : LOG . debug ( 'CORS have been already evaluated, skipping' ) return resp # Some libraries, like OAuthlib, set resp.headers to non Multidict # objects (Werkzeug Headers work as well). This is a problem because # headers allow repeated values. if ( not isinstance ( resp . headers , Headers ) and not isinstance ( resp . headers , MultiDict ) ) : resp . headers = MultiDict ( resp . headers ) headers_to_set = get_cors_headers ( options , request . headers , request . method ) LOG . debug ( 'Settings CORS headers: %s' , str ( headers_to_set ) ) for k , v in headers_to_set . items ( ) : resp . headers . add ( k , v ) return resp
6206	def _calc_hash_da ( self , rs ) : self . hash_d = hash_ ( rs . get_state ( ) ) [ : 6 ] self . hash_a = self . hash_d
7376	async def prepare_request ( self , method , url , headers = None , skip_params = False , proxy = None , * * kwargs ) : if method . lower ( ) == "post" : key = 'data' else : key = 'params' if key in kwargs and not skip_params : request_params = { key : kwargs . pop ( key ) } else : request_params = { } request_params . update ( dict ( method = method . upper ( ) , url = url ) ) coro = self . sign ( * * request_params , skip_params = skip_params , headers = headers ) request_params [ 'headers' ] = await utils . execute ( coro ) request_params [ 'proxy' ] = proxy kwargs . update ( request_params ) return kwargs
4066	def item_fields ( self ) : # Check for a valid cached version if self . templates . get ( "item_fields" ) and not self . _updated ( "/itemFields" , self . templates [ "item_fields" ] , "item_fields" ) : return self . templates [ "item_fields" ] [ "tmplt" ] query_string = "/itemFields" # otherwise perform a normal request and cache the response retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , "item_fields" )
6596	def receive_one ( self ) : if self . nruns == 0 : return None ret = self . communicationChannel . receive_one ( ) if ret is not None : self . nruns -= 1 return ret
9114	def fs_dirty_attachments ( self ) : if exists ( self . fs_attachment_container ) : return [ join ( self . fs_attachment_container , attachment ) for attachment in listdir ( self . fs_attachment_container ) ] else : return [ ]
8753	def partition_vifs ( xapi_client , interfaces , security_group_states ) : added = [ ] updated = [ ] removed = [ ] for vif in interfaces : # Quark should not action on isonet vifs in regions that use FLIP if ( 'floating_ip' in CONF . QUARK . environment_capabilities and is_isonet_vif ( vif ) ) : continue vif_has_groups = vif in security_group_states if vif . tagged and vif_has_groups and security_group_states [ vif ] [ sg_cli . SECURITY_GROUP_ACK ] : # Already ack'd these groups and VIF is tagged, reapply. # If it's not tagged, fall through and have it self-heal continue if vif . tagged : if vif_has_groups : updated . append ( vif ) else : removed . append ( vif ) else : if vif_has_groups : added . append ( vif ) # if not tagged and no groups, skip return added , updated , removed
5563	def output ( self ) : output_params = dict ( self . _raw [ "output" ] , grid = self . output_pyramid . grid , pixelbuffer = self . output_pyramid . pixelbuffer , metatiling = self . output_pyramid . metatiling ) if "path" in output_params : output_params . update ( path = absolute_path ( path = output_params [ "path" ] , base_dir = self . config_dir ) ) if "format" not in output_params : raise MapcheteConfigError ( "output format not specified" ) if output_params [ "format" ] not in available_output_formats ( ) : raise MapcheteConfigError ( "format %s not available in %s" % ( output_params [ "format" ] , str ( available_output_formats ( ) ) ) ) writer = load_output_writer ( output_params ) try : writer . is_valid_with_config ( output_params ) except Exception as e : logger . exception ( e ) raise MapcheteConfigError ( "driver %s not compatible with configuration: %s" % ( writer . METADATA [ "driver_name" ] , e ) ) return writer
5076	def is_course_run_upgradeable ( course_run ) : now = datetime . datetime . now ( pytz . UTC ) for seat in course_run . get ( 'seats' , [ ] ) : if seat . get ( 'type' ) == 'verified' : upgrade_deadline = parse_datetime_handle_invalid ( seat . get ( 'upgrade_deadline' ) ) return not upgrade_deadline or upgrade_deadline > now return False
3046	def _do_refresh_request ( self , http ) : body = self . _generate_refresh_request_body ( ) headers = self . _generate_refresh_request_headers ( ) logger . info ( 'Refreshing access_token' ) resp , content = transport . request ( http , self . token_uri , method = 'POST' , body = body , headers = headers ) content = _helpers . _from_bytes ( content ) if resp . status == http_client . OK : d = json . loads ( content ) self . token_response = d self . access_token = d [ 'access_token' ] self . refresh_token = d . get ( 'refresh_token' , self . refresh_token ) if 'expires_in' in d : delta = datetime . timedelta ( seconds = int ( d [ 'expires_in' ] ) ) self . token_expiry = delta + _UTCNOW ( ) else : self . token_expiry = None if 'id_token' in d : self . id_token = _extract_id_token ( d [ 'id_token' ] ) self . id_token_jwt = d [ 'id_token' ] else : self . id_token = None self . id_token_jwt = None # On temporary refresh errors, the user does not actually have to # re-authorize, so we unflag here. self . invalid = False if self . store : self . store . locked_put ( self ) else : # An {'error':...} response body means the token is expired or # revoked, so we flag the credentials as such. logger . info ( 'Failed to retrieve access token: %s' , content ) error_msg = 'Invalid response {0}.' . format ( resp . status ) try : d = json . loads ( content ) if 'error' in d : error_msg = d [ 'error' ] if 'error_description' in d : error_msg += ': ' + d [ 'error_description' ] self . invalid = True if self . store is not None : self . store . locked_put ( self ) except ( TypeError , ValueError ) : pass raise HttpAccessTokenRefreshError ( error_msg , status = resp . status )
5914	def _process_command ( self , command , name = None ) : self . _command_counter += 1 if name is None : name = "CMD{0:03d}" . format ( self . _command_counter ) # Need to build it with two make_ndx calls because I cannot reliably # name the new group without knowing its number. try : fd , tmp_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'tmp_' + name + '__' ) cmd = [ command , '' , 'q' ] # empty command '' necessary to get list # This sometimes fails with 'OSError: Broken Pipe' --- hard to debug rc , out , err = self . make_ndx ( o = tmp_ndx , input = cmd ) self . check_output ( out , "No atoms found for selection {command!r}." . format ( * * vars ( ) ) , err = err ) # For debugging, look at out and err or set stdout=True, stderr=True # TODO: check ' 0 r_300_&_ALA_&_O : 1 atoms' has at least 1 atom ##print "DEBUG: _process_command()" ##print out groups = parse_ndxlist ( out ) last = groups [ - 1 ] # reduce and name this group fd , ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = name + '__' ) name_cmd = [ "keep {0:d}" . format ( last [ 'nr' ] ) , "name 0 {0!s}" . format ( name ) , 'q' ] rc , out , err = self . make_ndx ( n = tmp_ndx , o = ndx , input = name_cmd ) finally : utilities . unlink_gmx ( tmp_ndx ) return name , ndx
12403	def requirements_for_changes ( self , changes ) : requirements = [ ] reqs_set = set ( ) if isinstance ( changes , str ) : changes = changes . split ( '\n' ) if not changes or changes [ 0 ] . startswith ( '-' ) : return requirements for line in changes : line = line . strip ( ' -+*' ) if not line : continue match = IS_REQUIREMENTS_RE2 . search ( line ) # or IS_REQUIREMENTS_RE.match(line) if match : for match in REQUIREMENTS_RE . findall ( match . group ( 1 ) ) : if match [ 1 ] : version = '==' + match [ 2 ] if match [ 1 ] . startswith ( ' to ' ) else match [ 1 ] req_str = match [ 0 ] + version else : req_str = match [ 0 ] if req_str not in reqs_set : reqs_set . add ( req_str ) try : requirements . append ( pkg_resources . Requirement . parse ( req_str ) ) except Exception as e : log . warn ( 'Could not parse requirement "%s" from changes: %s' , req_str , e ) return requirements
7146	def to_atomic ( amount ) : if not isinstance ( amount , ( Decimal , float ) + _integer_types ) : raise ValueError ( "Amount '{}' doesn't have numeric type. Only Decimal, int, long and " "float (not recommended) are accepted as amounts." ) return int ( amount * 10 ** 12 )
6398	def dist_monge_elkan ( src , tar , sim_func = sim_levenshtein , symmetric = False ) : return MongeElkan ( ) . dist ( src , tar , sim_func , symmetric )
3923	def _set_title ( self ) : self . title = get_conv_name ( self . _conversation , show_unread = True , truncate = True ) self . _set_title_cb ( self , self . title )
8606	def list_group_users ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/users?depth=%s' % ( group_id , str ( depth ) ) ) return response
13514	def froude_number ( speed , length ) : g = 9.80665 # conventional standard value m/s^2 Fr = speed / np . sqrt ( g * length ) return Fr
9530	def encrypt ( base_field , key = None , ttl = None ) : if not isinstance ( base_field , models . Field ) : assert key is None assert ttl is None return get_encrypted_field ( base_field ) name , path , args , kwargs = base_field . deconstruct ( ) kwargs . update ( { 'key' : key , 'ttl' : ttl } ) return get_encrypted_field ( base_field . __class__ ) ( * args , * * kwargs )
12065	def gain ( abf ) : Ys = np . nan_to_num ( swhlab . ap . getAvgBySweep ( abf , 'freq' ) ) Xs = abf . clampValues ( abf . dataX [ int ( abf . protoSeqX [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = "gain function" , xlabel = "command current (pA)" , ylabel = "average inst. freq. (Hz)" ) pylab . plot ( Xs , Ys , '.-' , ms = 20 , alpha = .5 , color = 'b' ) pylab . axhline ( 0 , alpha = .5 , lw = 2 , color = 'r' , ls = "--" ) pylab . margins ( .1 , .1 )
4096	def KIC ( N , rho , k ) : from numpy import log , array res = log ( rho ) + 3. * ( k + 1. ) / float ( N ) return res
2607	def update_memo ( self , task_id , task , r ) : if not self . memoize or not task [ 'memoize' ] : return if task [ 'hashsum' ] in self . memo_lookup_table : logger . info ( 'Updating appCache entry with latest %s:%s call' % ( task [ 'func_name' ] , task_id ) ) self . memo_lookup_table [ task [ 'hashsum' ] ] = r else : self . memo_lookup_table [ task [ 'hashsum' ] ] = r
11227	def _invalidates_cache ( f ) : def inner_func ( self , * args , * * kwargs ) : rv = f ( self , * args , * * kwargs ) self . _invalidate_cache ( ) return rv return inner_func
9687	def read_gsc_sfr ( self ) : config = [ ] data = { } # Send the command byte and sleep for 10 ms self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) # Read the config variables by sending 256 empty bytes for i in range ( 8 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) data [ "GSC" ] = self . _calculate_float ( config [ 0 : 4 ] ) data [ "SFR" ] = self . _calculate_float ( config [ 4 : ] ) return data
9838	def __gridpositions ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'counts' ) : shape = [ ] try : while True : # raises exception if not an int self . __peek ( ) . value ( 'INTEGER' ) tok = self . __consume ( ) shape . append ( tok . value ( 'INTEGER' ) ) except ( DXParserNoTokens , ValueError ) : pass if len ( shape ) == 0 : raise DXParseError ( 'gridpositions: no shape parameters' ) self . currentobject [ 'shape' ] = shape elif tok . equals ( 'origin' ) : origin = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) origin . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( origin ) == 0 : raise DXParseError ( 'gridpositions: no origin parameters' ) self . currentobject [ 'origin' ] = origin elif tok . equals ( 'delta' ) : d = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) d . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( d ) == 0 : raise DXParseError ( 'gridpositions: missing delta parameters' ) try : self . currentobject [ 'delta' ] . append ( d ) except KeyError : self . currentobject [ 'delta' ] = [ d ] else : raise DXParseError ( 'gridpositions: ' + str ( tok ) + ' not recognized.' )
5095	def get_map_image ( url , dest_path = None ) : image = requests . get ( url , stream = True , timeout = 10 ) if dest_path : image_url = url . rsplit ( '/' , 2 ) [ 1 ] + '-' + url . rsplit ( '/' , 1 ) [ 1 ] image_filename = image_url . split ( '?' ) [ 0 ] dest = os . path . join ( dest_path , image_filename ) image . raise_for_status ( ) with open ( dest , 'wb' ) as data : image . raw . decode_content = True shutil . copyfileobj ( image . raw , data ) return image . raw
10575	def get_local_playlist_songs ( playlist , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None ) : logger . info ( "Loading local playlist songs..." ) if os . name == 'nt' and CYGPATH_RE . match ( playlist ) : playlist = convert_cygwin_path ( playlist ) filepaths = [ ] base_filepath = os . path . dirname ( os . path . abspath ( playlist ) ) with open ( playlist ) as local_playlist : for line in local_playlist . readlines ( ) : line = line . strip ( ) if line . lower ( ) . endswith ( SUPPORTED_SONG_FORMATS ) : path = line if not os . path . isabs ( path ) : path = os . path . join ( base_filepath , path ) if os . path . isfile ( path ) : filepaths . append ( path ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local playlist songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local playlist songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local playlist songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
7650	def load ( path_or_file , validate = True , strict = True , fmt = 'auto' ) : with _open ( path_or_file , mode = 'r' , fmt = fmt ) as fdesc : jam = JAMS ( * * json . load ( fdesc ) ) if validate : jam . validate ( strict = strict ) return jam
6846	def is_present ( self , host = None ) : r = self . local_renderer r . env . host = host or self . genv . host_string ret = r . _local ( "getent hosts {host} | awk '{{ print $1 }}'" , capture = True ) or '' if self . verbose : print ( 'ret:' , ret ) ret = ret . strip ( ) if self . verbose : print ( 'Host %s %s present.' % ( r . env . host , 'IS' if bool ( ret ) else 'IS NOT' ) ) ip = ret ret = bool ( ret ) if not ret : return False r . env . ip = ip with settings ( warn_only = True ) : ret = r . _local ( 'ping -c 1 {ip}' , capture = True ) or '' packet_loss = re . findall ( r'([0-9]+)% packet loss' , ret ) # print('packet_loss:',packet_loss) ip_accessible = packet_loss and int ( packet_loss [ 0 ] ) < 100 if self . verbose : print ( 'IP %s accessible: %s' % ( ip , ip_accessible ) ) return bool ( ip_accessible )
2882	def get_timer_event_definition ( self , timerEventDefinition ) : timeDate = first ( self . xpath ( './/bpmn:timeDate' ) ) return TimerEventDefinition ( self . node . get ( 'name' , timeDate . text ) , self . parser . parse_condition ( timeDate . text , None , None , None , None , self ) )
11747	def routes_simple ( self ) : routes = [ ] for bundle in self . _registered_bundles : bundle_path = bundle [ 'path' ] for blueprint in bundle [ 'blueprints' ] : bp_path = blueprint [ 'path' ] for child in blueprint [ 'routes' ] : routes . append ( ( child [ 'endpoint' ] , bundle_path + bp_path + child [ 'path' ] , child [ 'methods' ] ) ) return routes
6614	def receive_one ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . receive_one ( )
12230	def unpatch_locals ( depth = 3 ) : for name , locals_dict in traverse_local_prefs ( depth ) : if isinstance ( locals_dict [ name ] , PatchedLocal ) : locals_dict [ name ] = locals_dict [ name ] . val del get_frame_locals ( depth ) [ __PATCHED_LOCALS_SENTINEL ]
3483	def write_sbml_model ( cobra_model , filename , f_replace = F_REPLACE , * * kwargs ) : doc = _model_to_sbml ( cobra_model , f_replace = f_replace , * * kwargs ) if isinstance ( filename , string_types ) : # write to path libsbml . writeSBMLToFile ( doc , filename ) elif hasattr ( filename , "write" ) : # write to file handle sbml_str = libsbml . writeSBMLToString ( doc ) filename . write ( sbml_str )
13732	def validate_is_boolean_true ( config_val , evar ) : if config_val is None : raise ValueError ( "Value for environment variable '{evar_name}' can't " "be empty." . format ( evar_name = evar . name ) ) return config_val
3080	def get_token ( http , service_account = 'default' ) : token_json = get ( http , 'instance/service-accounts/{0}/token' . format ( service_account ) ) token_expiry = client . _UTCNOW ( ) + datetime . timedelta ( seconds = token_json [ 'expires_in' ] ) return token_json [ 'access_token' ] , token_expiry
12435	def parse ( cls , path ) : # Iterate through the available patterns. for resource , pattern in cls . meta . patterns : # Attempt to match the path. match = re . match ( pattern , path ) if match is not None : # Found something. return resource , match . groupdict ( ) , match . string [ match . end ( ) : ] # No patterns at all; return unsuccessful. return None if not cls . meta . patterns else False
2970	def _sm_cleanup ( self , * args , * * kwargs ) : if self . _done_notification_func is not None : self . _done_notification_func ( ) self . _timer . cancel ( )
4928	def transform_image ( self , content_metadata_item ) : image_url = '' if content_metadata_item [ 'content_type' ] in [ 'course' , 'program' ] : image_url = content_metadata_item . get ( 'card_image_url' ) elif content_metadata_item [ 'content_type' ] == 'courserun' : image_url = content_metadata_item . get ( 'image_url' ) return image_url
6009	def load_background_noise_map ( background_noise_map_path , background_noise_map_hdu , pixel_scale , convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map ) : background_noise_map_options = sum ( [ convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map ] ) if background_noise_map_options == 0 and background_noise_map_path is not None : return NoiseMap . from_fits_with_pixel_scale ( file_path = background_noise_map_path , hdu = background_noise_map_hdu , pixel_scale = pixel_scale ) elif convert_background_noise_map_from_weight_map and background_noise_map_path is not None : weight_map = Array . from_fits ( file_path = background_noise_map_path , hdu = background_noise_map_hdu ) return NoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_background_noise_map_from_inverse_noise_map and background_noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = background_noise_map_path , hdu = background_noise_map_hdu ) return NoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) else : return None
8389	def write ( self , text , hashline = b"# {}" ) : if not text . endswith ( b"\n" ) : text += b"\n" actual_hash = hashlib . sha1 ( text ) . hexdigest ( ) with open ( self . filename , "wb" ) as f : f . write ( text ) f . write ( hashline . decode ( "utf8" ) . format ( actual_hash ) . encode ( "utf8" ) ) f . write ( b"\n" )
8409	def _extend_breaks ( self , major ) : trans = self . trans trans = trans if isinstance ( trans , type ) else trans . __class__ # so far we are only certain about this extending stuff # making sense for log transform is_log = trans . __name__ . startswith ( 'log' ) diff = np . diff ( major ) step = diff [ 0 ] if is_log and all ( diff == step ) : major = np . hstack ( [ major [ 0 ] - step , major , major [ - 1 ] + step ] ) return major
43	def make_sample_her_transitions ( replay_strategy , replay_k , reward_fun ) : if replay_strategy == 'future' : future_p = 1 - ( 1. / ( 1 + replay_k ) ) else : # 'replay_strategy' == 'none' future_p = 0 def _sample_her_transitions ( episode_batch , batch_size_in_transitions ) : """episode_batch is {key: array(buffer_size x T x dim_key)} """ T = episode_batch [ 'u' ] . shape [ 1 ] rollout_batch_size = episode_batch [ 'u' ] . shape [ 0 ] batch_size = batch_size_in_transitions # Select which episodes and time steps to use. episode_idxs = np . random . randint ( 0 , rollout_batch_size , batch_size ) t_samples = np . random . randint ( T , size = batch_size ) transitions = { key : episode_batch [ key ] [ episode_idxs , t_samples ] . copy ( ) for key in episode_batch . keys ( ) } # Select future time indexes proportional with probability future_p. These # will be used for HER replay by substituting in future goals. her_indexes = np . where ( np . random . uniform ( size = batch_size ) < future_p ) future_offset = np . random . uniform ( size = batch_size ) * ( T - t_samples ) future_offset = future_offset . astype ( int ) future_t = ( t_samples + 1 + future_offset ) [ her_indexes ] # Replace goal with achieved goal but only for the previously-selected # HER transitions (as defined by her_indexes). For the other transitions, # keep the original goal. future_ag = episode_batch [ 'ag' ] [ episode_idxs [ her_indexes ] , future_t ] transitions [ 'g' ] [ her_indexes ] = future_ag # Reconstruct info dictionary for reward computation. info = { } for key , value in transitions . items ( ) : if key . startswith ( 'info_' ) : info [ key . replace ( 'info_' , '' ) ] = value # Re-compute reward since we may have substituted the goal. reward_params = { k : transitions [ k ] for k in [ 'ag_2' , 'g' ] } reward_params [ 'info' ] = info transitions [ 'r' ] = reward_fun ( * * reward_params ) transitions = { k : transitions [ k ] . reshape ( batch_size , * transitions [ k ] . shape [ 1 : ] ) for k in transitions . keys ( ) } assert ( transitions [ 'u' ] . shape [ 0 ] == batch_size_in_transitions ) return transitions return _sample_her_transitions
12833	def on_enter_stage ( self ) : with self . world . _unlock_temporarily ( ) : self . forum . connect_everyone ( self . world , self . actors ) # 1. Setup the forum. self . forum . on_start_game ( ) # 2. Setup the world. with self . world . _unlock_temporarily ( ) : self . world . on_start_game ( ) # 3. Setup the actors. Because this is done after the forum and the # world have been setup, this signals to the actors that they can # send messages and query the game world as usual. num_players = len ( self . actors ) - 1 for actor in self . actors : actor . on_setup_gui ( self . gui ) for actor in self . actors : actor . on_start_game ( num_players )
6585	def input ( self , input , song ) : try : cmd = getattr ( self , self . CMD_MAP [ input ] [ 1 ] ) except ( IndexError , KeyError ) : return self . screen . print_error ( "Invalid command {!r}!" . format ( input ) ) cmd ( song )
10498	def tripleClickMouse ( self , coord ) : # Note above re: double-clicks applies to triple-clicks modFlags = 0 for i in range ( 2 ) : self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , clickCount = 3 ) self . _postQueuedEvents ( )
8493	def _handle_module ( args ) : module = _get_module_filename ( args . module ) if not module : _error ( "Could not load module or package: %r" , args . module ) elif isinstance ( module , Unparseable ) : _error ( "Could not determine module source: %r" , args . module ) _parse_and_output ( module , args )
259	def compute_exposures ( positions , factor_loadings , stack_positions = True , pos_in_dollars = True ) : if stack_positions : positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . compute_exposures ( positions , factor_loadings )
6043	def regular_to_sparse ( self ) : return mapping_util . regular_to_sparse_from_sparse_mappings ( regular_to_unmasked_sparse = self . regular_to_unmasked_sparse , unmasked_sparse_to_sparse = self . unmasked_sparse_to_sparse ) . astype ( 'int' )
8862	def quick_doc ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] # encoding = 'utf-8' encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto_definitions ( ) except jedi . NotFoundError : return [ ] else : ret_val = [ d . docstring ( ) for d in definitions ] return ret_val
9693	def cut ( self , by , from_start = True ) : s , e = copy ( self . start ) , copy ( self . end ) if from_start : e = s + by else : s = e - by return Range ( s , e )
1928	def load_overrides ( path = None ) : if path is not None : names = [ path ] else : possible_names = [ 'mcore.yml' , 'manticore.yml' ] names = [ os . path . join ( '.' , '' . join ( x ) ) for x in product ( [ '' , '.' ] , possible_names ) ] for name in names : try : with open ( name , 'r' ) as yml_f : logger . info ( f'Reading configuration from {name}' ) parse_config ( yml_f ) break except FileNotFoundError : pass else : if path is not None : raise FileNotFoundError ( f"'{path}' not found for config overrides" )
5851	def get_dataset_files ( self , dataset_id , glob = "." , is_dir = False , version_number = None ) : if version_number is None : latest = True else : latest = False data = { "download_request" : { "glob" : glob , "isDir" : is_dir , "latest" : latest } } failure_message = "Failed to get matched files in dataset {}" . format ( dataset_id ) versions = self . _get_success_json ( self . _post_json ( routes . matched_files ( dataset_id ) , data , failure_message = failure_message ) ) [ 'versions' ] # if you don't provide a version number, only the latest # will be included in the response body if version_number is None : version = versions [ 0 ] else : try : version = list ( filter ( lambda v : v [ 'number' ] == version_number , versions ) ) [ 0 ] except IndexError : raise ResourceNotFoundException ( ) return list ( map ( lambda f : DatasetFile ( path = f [ 'filename' ] , url = f [ 'url' ] ) , version [ 'files' ] ) )
10073	def build_deposit_schema ( self , record ) : schema_path = current_jsonschemas . url_to_path ( record [ '$schema' ] ) schema_prefix = current_app . config [ 'DEPOSIT_JSONSCHEMAS_PREFIX' ] if schema_path : return current_jsonschemas . path_to_url ( schema_prefix + schema_path )
9984	def has_lambda ( src ) : module_node = ast . parse ( dedent ( src ) ) lambdaexp = [ node for node in ast . walk ( module_node ) if isinstance ( node , ast . Lambda ) ] return bool ( lambdaexp )
5715	def _slugify_foreign_key ( schema ) : for foreign_key in schema . get ( 'foreignKeys' , [ ] ) : foreign_key [ 'reference' ] [ 'resource' ] = _slugify_resource_name ( foreign_key [ 'reference' ] . get ( 'resource' , '' ) ) return schema
8772	def _remove_default_tz_bindings ( self , context , network_id ) : default_tz = CONF . NVP . default_tz if not default_tz : LOG . warn ( "additional_default_tz_types specified, " "but no default_tz. Skipping " "_remove_default_tz_bindings()." ) return if not network_id : LOG . warn ( "neutron network_id not specified, skipping " "_remove_default_tz_bindings()" ) return for net_type in CONF . NVP . additional_default_tz_types : if net_type in TZ_BINDINGS : binding = TZ_BINDINGS [ net_type ] binding . remove ( context , default_tz , network_id ) else : LOG . warn ( "Unknown default tz type %s" % ( net_type ) )
13644	def append_arguments ( klass , sub_parsers , default_epilog , general_arguments ) : entry_name = hump_to_underscore ( klass . __name__ ) . replace ( '_component' , '' ) # set sub command document epilog = default_epilog if default_epilog else 'This tool generate by `cliez` ' 'https://www.github.com/wangwenpei/cliez' sub_parser = sub_parsers . add_parser ( entry_name , help = klass . __doc__ , epilog = epilog ) sub_parser . description = klass . add_arguments . __doc__ # add slot arguments if hasattr ( klass , 'add_slot_args' ) : slot_args = klass . add_slot_args ( ) or [ ] for v in slot_args : sub_parser . add_argument ( * v [ 0 ] , * * v [ 1 ] ) sub_parser . description = klass . add_slot_args . __doc__ pass user_arguments = klass . add_arguments ( ) or [ ] for v in user_arguments : sub_parser . add_argument ( * v [ 0 ] , * * v [ 1 ] ) if not klass . exclude_global_option : for v in general_arguments : sub_parser . add_argument ( * v [ 0 ] , * * v [ 1 ] ) return sub_parser
2592	def get_all_checkpoints ( rundir = "runinfo" ) : if ( not os . path . isdir ( rundir ) ) : return [ ] dirs = sorted ( os . listdir ( rundir ) ) checkpoints = [ ] for runid in dirs : checkpoint = os . path . abspath ( '{}/{}/checkpoint' . format ( rundir , runid ) ) if os . path . isdir ( checkpoint ) : checkpoints . append ( checkpoint ) return checkpoints
9279	def from_decimal ( number , width = 1 ) : text = [ ] if not isinstance ( number , int_type ) : raise TypeError ( "Expected number to be int, got %s" , type ( number ) ) elif not isinstance ( width , int_type ) : raise TypeError ( "Expected width to be int, got %s" , type ( number ) ) elif number < 0 : raise ValueError ( "Expected number to be positive integer" ) elif number > 0 : max_n = ceil ( log ( number ) / log ( 91 ) ) for n in _range ( int ( max_n ) , - 1 , - 1 ) : quotient , number = divmod ( number , 91 ** n ) text . append ( chr ( 33 + quotient ) ) return "" . join ( text ) . lstrip ( '!' ) . rjust ( max ( 1 , width ) , '!' )
7398	def parse ( string ) : # bibliography bib = [ ] # make sure we are dealing with unicode strings if not isinstance ( string , six . text_type ) : string = string . decode ( 'utf-8' ) # replace special characters for key , value in special_chars : string = string . replace ( key , value ) string = re . sub ( r'\\[cuHvs]{?([a-zA-Z])}?' , r'\1' , string ) # split into BibTex entries entries = re . findall ( r'(?u)@(\w+)[ \t]?{[ \t]*([^,\s]*)[ \t]*,?\s*((?:[^=,\s]+\s*\=\s*(?:"[^"]*"|{(?:[^{}]*|{[^{}]*})*}|[^,}]*),?\s*?)+)\s*}' , string ) for entry in entries : # parse entry pairs = re . findall ( r'(?u)([^=,\s]+)\s*\=\s*("[^"]*"|{(?:[^{}]*|{[^{}]*})*}|[^,]*)' , entry [ 2 ] ) # add to bibliography bib . append ( { 'type' : entry [ 0 ] . lower ( ) , 'key' : entry [ 1 ] } ) for key , value in pairs : # post-process key and value key = key . lower ( ) if value and value [ 0 ] == '"' and value [ - 1 ] == '"' : value = value [ 1 : - 1 ] if value and value [ 0 ] == '{' and value [ - 1 ] == '}' : value = value [ 1 : - 1 ] if key not in [ 'booktitle' , 'title' ] : value = value . replace ( '}' , '' ) . replace ( '{' , '' ) else : if value . startswith ( '{' ) and value . endswith ( '}' ) : value = value [ 1 : ] value = value [ : - 1 ] value = value . strip ( ) value = re . sub ( r'\s+' , ' ' , value ) # store pair in bibliography bib [ - 1 ] [ key ] = value return bib
9129	def store_populate_failed ( cls , resource : str , session : Optional [ Session ] = None ) -> 'Action' : action = cls . make_populate_failed ( resource ) _store_helper ( action , session = session ) return action
10080	def _publish_edited ( self ) : record_pid , record = self . fetch_published ( ) if record . revision_id == self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] : data = dict ( self . dumps ( ) ) else : data = self . merge_with_published ( ) data [ '$schema' ] = self . record_schema data [ '_deposit' ] = self [ '_deposit' ] record = record . __class__ ( data , model = record . model ) return record
5829	def check_for_rate_limiting ( response , response_lambda , timeout = 1 , attempts = 0 ) : if attempts >= 3 : raise RateLimitingException ( ) if response . status_code == 429 : sleep ( timeout ) new_timeout = timeout + 1 new_attempts = attempts + 1 return check_for_rate_limiting ( response_lambda ( timeout , attempts ) , response_lambda , timeout = new_timeout , attempts = new_attempts ) return response
9050	def bernoulli_sample ( offset , G , heritability = 0.5 , causal_variants = None , causal_variance = 0 , random_state = None , ) : link = LogitLink ( ) mean , cov = _mean_cov ( offset , G , heritability , causal_variants , causal_variance , random_state ) lik = BernoulliProdLik ( link ) sampler = GGPSampler ( lik , mean , cov ) return sampler . sample ( random_state )
13404	def prettify ( self , elem ) : from xml . etree import ElementTree from re import sub rawString = ElementTree . tostring ( elem , 'utf-8' ) parsedString = sub ( r'(?=<[^/].*>)' , '\n' , rawString ) # Adds newline after each closing tag return parsedString [ 1 : ]
8998	def string ( self , string ) : object_ = json . loads ( string ) return self . object ( object_ )
3730	def third_property ( CASRN = None , T = False , P = False , V = False ) : Third = None if V : Tc_methods = Tc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] Pc_methods = Pc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] if Tc_methods and Pc_methods : _Tc = Tc ( CASRN = CASRN , Method = Tc_methods [ 0 ] ) _Pc = Pc ( CASRN = CASRN , Method = Pc_methods [ 0 ] ) Third = critical_surface ( Tc = _Tc , Pc = _Pc , Vc = None ) elif P : Tc_methods = Tc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] Vc_methods = Vc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] if Tc_methods and Vc_methods : _Tc = Tc ( CASRN = CASRN , Method = Tc_methods [ 0 ] ) _Vc = Vc ( CASRN = CASRN , Method = Vc_methods [ 0 ] ) Third = critical_surface ( Tc = _Tc , Vc = _Vc , Pc = None ) elif T : Pc_methods = Pc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] Vc_methods = Vc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] if Pc_methods and Vc_methods : _Pc = Pc ( CASRN = CASRN , Method = Pc_methods [ 0 ] ) _Vc = Vc ( CASRN = CASRN , Method = Vc_methods [ 0 ] ) Third = critical_surface ( Pc = _Pc , Vc = _Vc , Tc = None ) else : raise Exception ( 'Error in function' ) if not Third : return None return Third
10415	def function_namespace_inclusion_builder ( func : str , namespace : Strings ) -> NodePredicate : if isinstance ( namespace , str ) : def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : """Pass only for nodes that have the enclosed function and enclosed namespace.""" if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] == namespace elif isinstance ( namespace , Iterable ) : namespaces = set ( namespace ) def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : """Pass only for nodes that have the enclosed function and namespace in the enclose set.""" if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] in namespaces else : raise ValueError ( 'Invalid type for argument: {}' . format ( namespace ) ) return function_namespaces_filter
8376	def parse ( svg , cached = False , _copy = True ) : if not cached : dom = parser . parseString ( svg ) paths = parse_node ( dom , [ ] ) else : id = _cache . id ( svg ) if not _cache . has_key ( id ) : dom = parser . parseString ( svg ) _cache . save ( id , parse_node ( dom , [ ] ) ) paths = _cache . load ( id , _copy ) return paths
630	def binSearch ( arr , val ) : i = bisect_left ( arr , val ) if i != len ( arr ) and arr [ i ] == val : return i return - 1
11855	def predictor ( self , ( i , j , A , alpha , Bb ) ) : B = Bb [ 0 ] if B in self . grammar . rules : for rhs in self . grammar . rewrites_for ( B ) : self . add_edge ( [ j , j , B , [ ] , rhs ] )
8961	def build ( ctx , docs = False ) : cfg = config . load ( ) ctx . run ( "python setup.py build" ) if docs : for doc_path in ( 'docs' , 'doc' ) : if os . path . exists ( cfg . rootjoin ( doc_path , 'conf.py' ) ) : break else : doc_path = None if doc_path : ctx . run ( "invoke docs" ) else : notify . warning ( "Cannot find either a 'docs' or 'doc' Sphinx directory!" )
9629	def split_docstring ( value ) : docstring = textwrap . dedent ( getattr ( value , '__doc__' , '' ) ) if not docstring : return None pieces = docstring . strip ( ) . split ( '\n\n' , 1 ) try : body = pieces [ 1 ] except IndexError : body = None return Docstring ( pieces [ 0 ] , body )
716	def __loadHyperSearchJobID ( cls , permWorkDir , outputLabel ) : filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , outputLabel = outputLabel ) jobID = None with open ( filePath , "r" ) as jobIdPickleFile : jobInfo = pickle . load ( jobIdPickleFile ) jobID = jobInfo [ "hyperSearchJobID" ] return jobID
11943	def _get ( self , * args , * * kwargs ) : messages , all_retrieved = super ( StorageMixin , self ) . _get ( * args , * * kwargs ) if self . user . is_authenticated ( ) : inbox_messages = self . backend . inbox_list ( self . user ) else : inbox_messages = [ ] return messages + inbox_messages , all_retrieved
891	def _adaptSegment ( cls , connections , segment , prevActiveCells , permanenceIncrement , permanenceDecrement ) : # Destroying a synapse modifies the set that we're iterating through. synapsesToDestroy = [ ] for synapse in connections . synapsesForSegment ( segment ) : permanence = synapse . permanence if binSearch ( prevActiveCells , synapse . presynapticCell ) != - 1 : permanence += permanenceIncrement else : permanence -= permanenceDecrement # Keep permanence within min/max bounds permanence = max ( 0.0 , min ( 1.0 , permanence ) ) if permanence < EPSILON : synapsesToDestroy . append ( synapse ) else : connections . updateSynapsePermanence ( synapse , permanence ) for synapse in synapsesToDestroy : connections . destroySynapse ( synapse ) if connections . numSynapses ( segment ) == 0 : connections . destroySegment ( segment )
6459	def _has_vowel ( self , term ) : for letter in term : if letter in self . _vowels : return True return False
2693	def filter_quotes ( text , is_email = True ) : global DEBUG global PAT_FORWARD , PAT_REPLIED , PAT_UNSUBSC if is_email : text = filter ( lambda x : x in string . printable , text ) if DEBUG : print ( "text:" , text ) # strip off quoted text in a forward m = PAT_FORWARD . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] # strip off quoted text in a reply m = PAT_REPLIED . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] # strip off any trailing unsubscription notice m = PAT_UNSUBSC . split ( text , re . M ) if m : text = m [ 0 ] # replace any remaining quoted text with blank lines lines = [ ] for line in text . split ( "\n" ) : if line . startswith ( ">" ) : lines . append ( "" ) else : lines . append ( line ) return list ( split_grafs ( lines ) )
13031	def poll_once ( self , timeout = 0.0 ) : if self . _map : self . _poll_func ( timeout , self . _map )
321	def get_max_drawdown ( returns ) : returns = returns . copy ( ) df_cum = cum_returns ( returns , 1.0 ) running_max = np . maximum . accumulate ( df_cum ) underwater = df_cum / running_max - 1 return get_max_drawdown_underwater ( underwater )
8827	def update_ports_for_sg ( self , context , portid , jobid ) : port = db_api . port_find ( context , id = portid , scope = db_api . ONE ) if not port : LOG . warning ( "Port not found" ) return net_driver = port_api . _get_net_driver ( port . network , port = port ) base_net_driver = port_api . _get_net_driver ( port . network ) sg_list = [ sg for sg in port . security_groups ] success = False error = None retries = 3 retry_delay = 2 for retry in xrange ( retries ) : try : net_driver . update_port ( context , port_id = port [ "backend_key" ] , mac_address = port [ "mac_address" ] , device_id = port [ "device_id" ] , base_net_driver = base_net_driver , security_groups = sg_list ) success = True error = None break except Exception as error : LOG . warning ( "Could not connect to redis, but retrying soon" ) time . sleep ( retry_delay ) status_str = "" if not success : status_str = "Port %s update failed after %d tries. Error: %s" % ( portid , retries , error ) update_body = dict ( completed = True , status = status_str ) update_body = dict ( job = update_body ) job_api . update_job ( context . elevated ( ) , jobid , update_body )
3296	def ref_url_to_path ( self , ref_url ) : return "/" + compat . unquote ( util . lstripstr ( ref_url , self . share_path ) ) . lstrip ( "/" )
8257	def _average ( self ) : r , g , b , a = 0 , 0 , 0 , 0 for clr in self : r += clr . r g += clr . g b += clr . b a += clr . alpha r /= len ( self ) g /= len ( self ) b /= len ( self ) a /= len ( self ) return color ( r , g , b , a , mode = "rgb" )
12494	def check_X_y ( X , y , accept_sparse = None , dtype = None , order = None , copy = False , force_all_finite = True , ensure_2d = True , allow_nd = False , multi_output = False ) : X = check_array ( X , accept_sparse , dtype , order , copy , force_all_finite , ensure_2d , allow_nd ) if multi_output : y = check_array ( y , 'csr' , force_all_finite = True , ensure_2d = False ) else : y = column_or_1d ( y , warn = True ) _assert_all_finite ( y ) check_consistent_length ( X , y ) return X , y
11252	def get_datetime_string ( datetime_obj ) : if isinstance ( datetime_obj , datetime ) : dft = DTFormat ( ) return datetime_obj . strftime ( dft . datetime_format ) return None
7627	def add_namespace ( filename ) : with open ( filename , mode = 'r' ) as fileobj : __NAMESPACE__ . update ( json . load ( fileobj ) )
9844	def __refill_tokenbuffer ( self ) : if len ( self . tokens ) == 0 : self . __tokenize ( self . dxfile . readline ( ) )
1078	def isoformat ( self ) : # return "%04d-%02d-%02d" % (self._year, self._month, self._day) return "%s-%s-%s" % ( str ( self . _year ) . zfill ( 4 ) , str ( self . _month ) . zfill ( 2 ) , str ( self . _day ) . zfill ( 2 ) )
9877	def _ratio_metric ( v1 , v2 , * * _kwargs ) : return ( ( ( v1 - v2 ) / ( v1 + v2 ) ) ** 2 ) if v1 + v2 != 0 else 0
91	def derive_random_states ( random_state , n = 1 ) : seed_ = random_state . randint ( SEED_MIN_VALUE , SEED_MAX_VALUE , 1 ) [ 0 ] return [ new_random_state ( seed_ + i ) for i in sm . xrange ( n ) ]
11280	def get_item_creator ( item_type ) : if item_type not in Pipe . pipe_item_types : for registered_type in Pipe . pipe_item_types : if issubclass ( item_type , registered_type ) : return Pipe . pipe_item_types [ registered_type ] return None else : return Pipe . pipe_item_types [ item_type ]
12216	def traverse_local_prefs ( stepback = 0 ) : locals_dict = get_frame_locals ( stepback + 1 ) for k in locals_dict : if not k . startswith ( '_' ) and k . upper ( ) == k : yield k , locals_dict
1335	def best_other_class ( logits , exclude ) : other_logits = logits - onehot_like ( logits , exclude , value = np . inf ) return np . argmax ( other_logits )
824	def mostLikely ( self , pred ) : if len ( pred ) == 1 : return pred . keys ( ) [ 0 ] mostLikelyOutcome = None maxProbability = 0 for prediction , probability in pred . items ( ) : if probability > maxProbability : mostLikelyOutcome = prediction maxProbability = probability return mostLikelyOutcome
3957	def update_running_containers_from_spec ( compose_config , recreate_containers = True ) : write_composefile ( compose_config , constants . COMPOSEFILE_PATH ) compose_up ( constants . COMPOSEFILE_PATH , 'dusty' , recreate_containers = recreate_containers )
5373	def file_exists ( file_path , credentials = None ) : if file_path . startswith ( 'gs://' ) : return _file_exists_in_gcs ( file_path , credentials ) else : return os . path . isfile ( file_path )
2926	def create_package ( self ) : # Check that all files exist (and calculate the longest shared path # prefix): self . input_path_prefix = None for filename in self . input_files : if not os . path . isfile ( filename ) : raise ValueError ( '%s does not exist or is not a file' % filename ) if self . input_path_prefix : full = os . path . abspath ( os . path . dirname ( filename ) ) while not ( full . startswith ( self . input_path_prefix ) and self . input_path_prefix ) : self . input_path_prefix = self . input_path_prefix [ : - 1 ] else : self . input_path_prefix = os . path . abspath ( os . path . dirname ( filename ) ) # Parse all of the XML: self . bpmn = { } for filename in self . input_files : bpmn = ET . parse ( filename ) self . bpmn [ os . path . abspath ( filename ) ] = bpmn # Now run through pre-parsing and validation: for filename , bpmn in list ( self . bpmn . items ( ) ) : bpmn = self . pre_parse_and_validate ( bpmn , filename ) self . bpmn [ os . path . abspath ( filename ) ] = bpmn # Now check that we can parse it fine: for filename , bpmn in list ( self . bpmn . items ( ) ) : self . parser . add_bpmn_xml ( bpmn , filename = filename ) self . wf_spec = self . parser . get_spec ( self . entry_point_process ) # Now package everything: self . package_zip = zipfile . ZipFile ( self . package_file , "w" , compression = zipfile . ZIP_DEFLATED ) done_files = set ( ) for spec in self . wf_spec . get_specs_depth_first ( ) : filename = spec . file if filename not in done_files : done_files . add ( filename ) bpmn = self . bpmn [ os . path . abspath ( filename ) ] self . write_to_package_zip ( "%s.bpmn" % spec . name , ET . tostring ( bpmn . getroot ( ) ) ) self . write_file_to_package_zip ( "src/" + self . _get_zip_path ( filename ) , filename ) self . _call_editor_hook ( 'package_for_editor' , spec , filename ) self . write_meta_data ( ) self . write_manifest ( ) self . package_zip . close ( )
9561	def _apply_check_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'check' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except RecordError as e : code = e . code if e . code is not None else RECORD_CHECK_FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD_CHECK_FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
8718	def file_remove ( self , path ) : log . info ( 'Remove ' + path ) cmd = 'file.remove("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
7540	def basecaller ( arrayed , mindepth_majrule , mindepth_statistical , estH , estE ) : ## an array to fill with consensus site calls cons = np . zeros ( arrayed . shape [ 1 ] , dtype = np . uint8 ) cons . fill ( 78 ) arr = arrayed . view ( np . uint8 ) ## iterate over columns for col in xrange ( arr . shape [ 1 ] ) : ## the site of focus carr = arr [ : , col ] ## make mask of N and - sites mask = carr == 45 mask += carr == 78 marr = carr [ ~ mask ] ## skip if only empties (e.g., N-) if not marr . shape [ 0 ] : cons [ col ] = 78 ## skip if not variable elif np . all ( marr == marr [ 0 ] ) : cons [ col ] = marr [ 0 ] ## estimate variable site call else : ## get allele freqs (first-most, second, third = p, q, r) counts = np . bincount ( marr ) pbase = np . argmax ( counts ) nump = counts [ pbase ] counts [ pbase ] = 0 qbase = np . argmax ( counts ) numq = counts [ qbase ] counts [ qbase ] = 0 rbase = np . argmax ( counts ) numr = counts [ rbase ] ## based on biallelic depth bidepth = nump + numq if bidepth < mindepth_majrule : cons [ col ] = 78 else : ## if depth is too high, reduce to sampled int if bidepth > 500 : base1 = int ( 500 * ( nump / float ( bidepth ) ) ) base2 = int ( 500 * ( numq / float ( bidepth ) ) ) else : base1 = nump base2 = numq ## make statistical base call if bidepth >= mindepth_statistical : ishet , prob = get_binom ( base1 , base2 , estE , estH ) #LOGGER.info("ishet, prob, b1, b2: %s %s %s %s", ishet, prob, base1, base2) if prob < 0.95 : cons [ col ] = 78 else : if ishet : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase ## make majrule base call else : #if bidepth >= mindepth_majrule: if nump == numq : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase return cons . view ( "S1" )
3771	def mixing_logarithmic ( fracs , props ) : if not none_and_length_check ( [ fracs , props ] ) : return None return exp ( sum ( frac * log ( prop ) for frac , prop in zip ( fracs , props ) ) )
382	def drop ( x , keep = 0.5 ) : if len ( x . shape ) == 3 : if x . shape [ - 1 ] == 3 : # color img_size = x . shape mask = np . random . binomial ( n = 1 , p = keep , size = x . shape [ : - 1 ] ) for i in range ( 3 ) : x [ : , : , i ] = np . multiply ( x [ : , : , i ] , mask ) elif x . shape [ - 1 ] == 1 : # greyscale image img_size = x . shape x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) else : raise Exception ( "Unsupported shape {}" . format ( x . shape ) ) elif len ( x . shape ) == 2 or 1 : # greyscale matrix (image) or vector img_size = x . shape x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) else : raise Exception ( "Unsupported shape {}" . format ( x . shape ) ) return x
4568	def dumps ( data , use_yaml = None , safe = True , * * kwds ) : if use_yaml is None : use_yaml = ALWAYS_DUMP_YAML if use_yaml : dumps = yaml . safe_dump if safe else yaml . dump else : dumps = json . dumps kwds . update ( indent = 4 , sort_keys = True ) if not safe : kwds . update ( default = repr ) return dumps ( data , * * kwds )
345	def _load_mnist_dataset ( shape , path , name = 'mnist' , url = 'http://yann.lecun.com/exdb/mnist/' ) : path = os . path . join ( path , name ) # Define functions for loading mnist-like data's images and labels. # For convenience, they also download the requested files if needed. def load_mnist_images ( path , filename ) : filepath = maybe_download_and_extract ( filename , path , url ) logging . info ( filepath ) # Read the inputs in Yann LeCun's binary format. with gzip . open ( filepath , 'rb' ) as f : data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 16 ) # The inputs are vectors now, we reshape them to monochrome 2D images, # following the shape convention: (examples, channels, rows, columns) data = data . reshape ( shape ) # The inputs come as bytes, we convert them to float32 in range [0,1]. # (Actually to range [0, 255/256], for compatibility to the version # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.) return data / np . float32 ( 256 ) def load_mnist_labels ( path , filename ) : filepath = maybe_download_and_extract ( filename , path , url ) # Read the labels in Yann LeCun's binary format. with gzip . open ( filepath , 'rb' ) as f : data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 8 ) # The labels are vectors of integers now, that's exactly what we want. return data # Download and read the training and test set images and labels. logging . info ( "Load or Download {0} > {1}" . format ( name . upper ( ) , path ) ) X_train = load_mnist_images ( path , 'train-images-idx3-ubyte.gz' ) y_train = load_mnist_labels ( path , 'train-labels-idx1-ubyte.gz' ) X_test = load_mnist_images ( path , 't10k-images-idx3-ubyte.gz' ) y_test = load_mnist_labels ( path , 't10k-labels-idx1-ubyte.gz' ) # We reserve the last 10000 training examples for validation. X_train , X_val = X_train [ : - 10000 ] , X_train [ - 10000 : ] y_train , y_val = y_train [ : - 10000 ] , y_train [ - 10000 : ] # We just return all the arrays in order, as expected in main(). # (It doesn't matter how we do this as long as we can read them again.) X_train = np . asarray ( X_train , dtype = np . float32 ) y_train = np . asarray ( y_train , dtype = np . int32 ) X_val = np . asarray ( X_val , dtype = np . float32 ) y_val = np . asarray ( y_val , dtype = np . int32 ) X_test = np . asarray ( X_test , dtype = np . float32 ) y_test = np . asarray ( y_test , dtype = np . int32 ) return X_train , y_train , X_val , y_val , X_test , y_test
1076	def _ymd2ord ( year , month , day ) : assert 1 <= month <= 12 , 'month must be in 1..12' dim = _days_in_month ( year , month ) assert 1 <= day <= dim , ( 'day must be in 1..%d' % dim ) return ( _days_before_year ( year ) + _days_before_month ( year , month ) + day )
9942	def clear_dir ( self , path ) : dirs , files = self . storage . listdir ( path ) for f in files : fpath = os . path . join ( path , f ) if self . dry_run : self . log ( "Pretending to delete '%s'" % smart_text ( fpath ) , level = 1 ) else : self . log ( "Deleting '%s'" % smart_text ( fpath ) , level = 1 ) self . storage . delete ( fpath ) for d in dirs : self . clear_dir ( os . path . join ( path , d ) )
1908	def forward_events_to ( self , sink , include_source = False ) : assert isinstance ( sink , Eventful ) , f'{sink.__class__.__name__} is not Eventful' self . _forwards [ sink ] = include_source
2969	def _sm_stop_from_pain ( self , * args , * * kwargs ) : _logger . info ( "Stopping chaos for blockade %s" % self . _blockade_name ) self . _do_reset_all ( )
3021	def get_access_token ( self , http = None , additional_claims = None ) : if additional_claims is None : if self . access_token is None or self . access_token_expired : self . refresh ( None ) return client . AccessTokenInfo ( access_token = self . access_token , expires_in = self . _expires_in ( ) ) else : # Create a 1 time token token , unused_expiry = self . _create_token ( additional_claims ) return client . AccessTokenInfo ( access_token = token , expires_in = self . _MAX_TOKEN_LIFETIME_SECS )
12391	def indexesOptional ( f ) : stack = inspect . stack ( ) _NO_INDEX_CHECK_NEEDED . add ( '%s.%s.%s' % ( f . __module__ , stack [ 1 ] [ 3 ] , f . __name__ ) ) del stack return f
13142	def recover_triples_from_mapping ( indexes , ents : bidict , rels : bidict ) : triples = [ ] for t in indexes : triples . append ( kgedata . Triple ( ents . inverse [ t . head ] , rels . inverse [ t . relation ] , ents . inverse [ t . tail ] ) ) return triples
12214	def get_pref_model_class ( app , prefs , get_prefs_func ) : module = '%s.%s' % ( app , PREFS_MODULE_NAME ) model_dict = { '_prefs_app' : app , '_get_prefs' : staticmethod ( get_prefs_func ) , '__module__' : module , 'Meta' : type ( 'Meta' , ( models . options . Options , ) , { 'verbose_name' : _ ( 'Preference' ) , 'verbose_name_plural' : _ ( 'Preferences' ) , 'app_label' : app , 'managed' : False , } ) } for field_name , val_proxy in prefs . items ( ) : model_dict [ field_name ] = val_proxy . field model = type ( 'Preferences' , ( models . Model , ) , model_dict ) def fake_save_base ( self , * args , * * kwargs ) : updated_prefs = { f . name : getattr ( self , f . name ) for f in self . _meta . fields if not isinstance ( f , models . fields . AutoField ) } app_prefs = self . _get_prefs ( self . _prefs_app ) for pref in app_prefs . keys ( ) : if pref in updated_prefs : app_prefs [ pref ] . db_value = updated_prefs [ pref ] self . pk = self . _prefs_app # Make Django 1.7 happy. prefs_save . send ( sender = self , app = self . _prefs_app , updated_prefs = updated_prefs ) return True model . save_base = fake_save_base return model
11375	def _crawl_elsevier_and_find_issue_xml ( self ) : self . _found_issues = [ ] if not self . path and not self . package_name : for issue in self . conn . _get_issues ( ) : dirname = issue . rstrip ( '/issue.xml' ) try : self . _normalize_issue_dir_with_dtd ( dirname ) self . _found_issues . append ( dirname ) except Exception as err : register_exception ( ) print ( "ERROR: can't normalize %s: %s" % ( dirname , err ) ) else : def visit ( dummy , dirname , names ) : if "issue.xml" in names : try : self . _normalize_issue_dir_with_dtd ( dirname ) self . _found_issues . append ( dirname ) except Exception as err : register_exception ( ) print ( "ERROR: can't normalize %s: %s" % ( dirname , err ) ) walk ( self . path , visit , None )
10406	def canonical_averages_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'number_of_runs' , 'uint32' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability_mean' , 'float64' ) , ( 'percolation_probability_m2' , 'float64' ) , ] ) fields . extend ( [ ( 'max_cluster_size_mean' , 'float64' ) , ( 'max_cluster_size_m2' , 'float64' ) , ( 'moments_mean' , '(5,)float64' ) , ( 'moments_m2' , '(5,)float64' ) , ] ) return _ndarray_dtype ( fields )
2962	def expand_partitions ( containers , partitions ) : # filter out holy containers that don't belong # to any partition at all all_names = frozenset ( c . name for c in containers if not c . holy ) holy_names = frozenset ( c . name for c in containers if c . holy ) neutral_names = frozenset ( c . name for c in containers if c . neutral ) partitions = [ frozenset ( p ) for p in partitions ] unknown = set ( ) holy = set ( ) union = set ( ) for partition in partitions : unknown . update ( partition - all_names - holy_names ) holy . update ( partition - all_names ) union . update ( partition ) if unknown : raise BlockadeError ( 'Partitions contain unknown containers: %s' % list ( unknown ) ) if holy : raise BlockadeError ( 'Partitions contain holy containers: %s' % list ( holy ) ) # put any leftover containers in an implicit partition leftover = all_names . difference ( union ) if leftover : partitions . append ( leftover ) # we create an 'implicit' partition for the neutral containers # in case they are not part of the leftover anyways if not neutral_names . issubset ( leftover ) : partitions . append ( neutral_names ) return partitions
11914	def initialize ( self , templates_path , global_data ) : self . env = Environment ( loader = FileSystemLoader ( templates_path ) ) self . env . trim_blocks = True self . global_data = global_data
7889	def update_presence ( self , presence ) : self . presence = MucPresence ( presence ) t = presence . get_type ( ) if t == "unavailable" : self . role = "none" self . affiliation = "none" self . room_jid = self . presence . get_from ( ) self . nick = self . room_jid . resource mc = self . presence . get_muc_child ( ) if isinstance ( mc , MucUserX ) : items = mc . get_items ( ) for item in items : if not isinstance ( item , MucItem ) : continue if item . role : self . role = item . role if item . affiliation : self . affiliation = item . affiliation if item . jid : self . real_jid = item . jid if item . nick : self . new_nick = item . nick break
10496	def leftMouseDragged ( self , stopCoord , strCoord = ( 0 , 0 ) , speed = 1 ) : self . _leftMouseDragged ( stopCoord , strCoord , speed )
9853	def centers ( self ) : for idx in numpy . ndindex ( self . grid . shape ) : yield self . delta * numpy . array ( idx ) + self . origin
7682	def display ( annotation , meta = True , * * kwargs ) : for namespace , func in six . iteritems ( VIZ_MAPPING ) : try : ann = coerce_annotation ( annotation , namespace ) axes = func ( ann , * * kwargs ) # Title should correspond to original namespace, not the coerced version axes . set_title ( annotation . namespace ) if meta : description = pprint_jobject ( annotation . annotation_metadata , indent = 2 ) anchored_box = AnchoredText ( description . strip ( '\n' ) , loc = 2 , frameon = True , bbox_to_anchor = ( 1.02 , 1.0 ) , bbox_transform = axes . transAxes , borderpad = 0.0 ) axes . add_artist ( anchored_box ) axes . figure . subplots_adjust ( right = 0.8 ) return axes except NamespaceError : pass raise NamespaceError ( 'Unable to visualize annotation of namespace="{:s}"' . format ( annotation . namespace ) )
9781	def get ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : response = PolyaxonClient ( ) . build_job . get_build ( user , project_name , _build ) cache . cache ( config_manager = BuildJobManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_build_details ( response )
8757	def _validate_subnet_cidr ( context , network_id , new_subnet_cidr ) : if neutron_cfg . cfg . CONF . allow_overlapping_ips : return try : new_subnet_ipset = netaddr . IPSet ( [ new_subnet_cidr ] ) except TypeError : LOG . exception ( "Invalid or missing cidr: %s" % new_subnet_cidr ) raise n_exc . BadRequest ( resource = "subnet" , msg = "Invalid or missing cidr" ) filters = { 'network_id' : network_id , 'shared' : [ False ] } # Using admin context here, in case we actually share networks later subnet_list = db_api . subnet_find ( context = context . elevated ( ) , * * filters ) for subnet in subnet_list : if ( netaddr . IPSet ( [ subnet . cidr ] ) & new_subnet_ipset ) : # don't give out details of the overlapping subnet err_msg = ( _ ( "Requested subnet with cidr: %(cidr)s for " "network: %(network_id)s overlaps with another " "subnet" ) % { 'cidr' : new_subnet_cidr , 'network_id' : network_id } ) LOG . error ( _ ( "Validation for CIDR: %(new_cidr)s failed - " "overlaps with subnet %(subnet_id)s " "(CIDR: %(cidr)s)" ) , { 'new_cidr' : new_subnet_cidr , 'subnet_id' : subnet . id , 'cidr' : subnet . cidr } ) raise n_exc . InvalidInput ( error_message = err_msg )
2702	def enumerate_chunks ( phrase , spacy_nlp ) : if ( len ( phrase ) > 1 ) : found = False text = " " . join ( [ rl . text for rl in phrase ] ) doc = spacy_nlp ( text . strip ( ) , parse = True ) for np in doc . noun_chunks : if np . text != text : found = True yield np . text , find_chunk ( phrase , np . text . split ( " " ) ) if not found and all ( [ rl . pos [ 0 ] != "v" for rl in phrase ] ) : yield text , phrase
66	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 1 , copy = True , raise_if_out_of_image = False , thickness = None ) : if thickness is not None : ia . warn_deprecated ( "Usage of argument 'thickness' in BoundingBox.draw_on_image() " "is deprecated. The argument was renamed to 'size'." ) size = thickness if raise_if_out_of_image and self . is_out_of_image ( image ) : raise Exception ( "Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s." % ( self . x1 , self . y1 , self . x2 , self . y2 , image . shape ) ) result = np . copy ( image ) if copy else image if isinstance ( color , ( tuple , list ) ) : color = np . uint8 ( color ) for i in range ( size ) : y1 , y2 , x1 , x2 = self . y1_int , self . y2_int , self . x1_int , self . x2_int # When y values get into the range (H-0.5, H), the *_int functions round them to H. # That is technically sensible, but in the case of drawing means that the border lies # just barely outside of the image, making the border disappear, even though the BB # is fully inside the image. Here we correct for that because of beauty reasons. # Same is the case for x coordinates. if self . is_fully_within_image ( image ) : y1 = np . clip ( y1 , 0 , image . shape [ 0 ] - 1 ) y2 = np . clip ( y2 , 0 , image . shape [ 0 ] - 1 ) x1 = np . clip ( x1 , 0 , image . shape [ 1 ] - 1 ) x2 = np . clip ( x2 , 0 , image . shape [ 1 ] - 1 ) y = [ y1 - i , y1 - i , y2 + i , y2 + i ] x = [ x1 - i , x2 + i , x2 + i , x1 - i ] rr , cc = skimage . draw . polygon_perimeter ( y , x , shape = result . shape ) if alpha >= 0.99 : result [ rr , cc , : ] = color else : if ia . is_float_array ( result ) : result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) else : input_dtype = result . dtype result = result . astype ( np . float32 ) result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) . astype ( input_dtype ) return result
3956	def update_nginx_from_config ( nginx_config ) : logging . info ( 'Updating nginx with new Dusty config' ) temp_dir = tempfile . mkdtemp ( ) os . mkdir ( os . path . join ( temp_dir , 'html' ) ) _write_nginx_config ( constants . NGINX_BASE_CONFIG , os . path . join ( temp_dir , constants . NGINX_PRIMARY_CONFIG_NAME ) ) _write_nginx_config ( nginx_config [ 'http' ] , os . path . join ( temp_dir , constants . NGINX_HTTP_CONFIG_NAME ) ) _write_nginx_config ( nginx_config [ 'stream' ] , os . path . join ( temp_dir , constants . NGINX_STREAM_CONFIG_NAME ) ) _write_nginx_config ( constants . NGINX_502_PAGE_HTML , os . path . join ( temp_dir , 'html' , constants . NGINX_502_PAGE_NAME ) ) sync_local_path_to_vm ( temp_dir , constants . NGINX_CONFIG_DIR_IN_VM )
11329	def main ( ) : argparser = ArgumentParser ( ) subparsers = argparser . add_subparsers ( dest = 'selected_subparser' ) all_parser = subparsers . add_parser ( 'all' ) elsevier_parser = subparsers . add_parser ( 'elsevier' ) oxford_parser = subparsers . add_parser ( 'oxford' ) springer_parser = subparsers . add_parser ( 'springer' ) all_parser . add_argument ( '--update-credentials' , action = 'store_true' ) elsevier_parser . add_argument ( '--run-locally' , action = 'store_true' ) elsevier_parser . add_argument ( '--package-name' ) elsevier_parser . add_argument ( '--path' ) elsevier_parser . add_argument ( '--CONSYN' , action = 'store_true' ) elsevier_parser . add_argument ( '--update-credentials' , action = 'store_true' ) elsevier_parser . add_argument ( '--extract-nations' , action = 'store_true' ) oxford_parser . add_argument ( '--dont-empty-ftp' , action = 'store_true' ) oxford_parser . add_argument ( '--package-name' ) oxford_parser . add_argument ( '--path' ) oxford_parser . add_argument ( '--update-credentials' , action = 'store_true' ) oxford_parser . add_argument ( '--extract-nations' , action = 'store_true' ) springer_parser . add_argument ( '--package-name' ) springer_parser . add_argument ( '--path' ) springer_parser . add_argument ( '--update-credentials' , action = 'store_true' ) springer_parser . add_argument ( '--extract-nations' , action = 'store_true' ) settings = Bunch ( vars ( argparser . parse_args ( ) ) ) call_package ( settings )
1152	def warn ( message , category = None , stacklevel = 1 ) : # Check if message is already a Warning object if isinstance ( message , Warning ) : category = message . __class__ # Check category argument if category is None : category = UserWarning assert issubclass ( category , Warning ) # Get context information try : caller = sys . _getframe ( stacklevel ) except ValueError : globals = sys . __dict__ lineno = 1 else : globals = caller . f_globals lineno = caller . f_lineno if '__name__' in globals : module = globals [ '__name__' ] else : module = "<string>" filename = globals . get ( '__file__' ) if filename : fnl = filename . lower ( ) if fnl . endswith ( ( ".pyc" , ".pyo" ) ) : filename = filename [ : - 1 ] else : if module == "__main__" : try : filename = sys . argv [ 0 ] except AttributeError : # embedded interpreters don't have sys.argv, see bug #839151 filename = '__main__' if not filename : filename = module registry = globals . setdefault ( "__warningregistry__" , { } ) warn_explicit ( message , category , filename , lineno , module , registry , globals )
7151	def get_checksum ( cls , phrase ) : phrase_split = phrase . split ( " " ) if len ( phrase_split ) < 12 : raise ValueError ( "Invalid mnemonic phrase" ) if len ( phrase_split ) > 13 : # Standard format phrase = phrase_split [ : 24 ] else : # MyMonero format phrase = phrase_split [ : 12 ] wstr = "" . join ( word [ : cls . unique_prefix_length ] for word in phrase ) wstr = bytearray ( wstr . encode ( 'utf-8' ) ) z = ( ( crc32 ( wstr ) & 0xffffffff ) ^ 0xffffffff ) >> 0 z2 = ( ( z ^ 0xffffffff ) >> 0 ) % len ( phrase ) return phrase_split [ z2 ]
2004	def function_selector ( method_name_and_signature ) : s = sha3 . keccak_256 ( ) s . update ( method_name_and_signature . encode ( ) ) return bytes ( s . digest ( ) [ : 4 ] )
4068	def update_items ( self , payload ) : to_send = [ self . check_items ( [ p ] ) [ 0 ] for p in payload ] headers = { } headers . update ( self . default_headers ( ) ) # the API only accepts 50 items at a time, so we have to split # anything longer for chunk in chunks ( to_send , 50 ) : req = requests . post ( url = self . endpoint + "/{t}/{u}/items/" . format ( t = self . library_type , u = self . library_id ) , headers = headers , data = json . dumps ( chunk ) , ) self . request = req try : req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( req ) return True
7635	def import_lab ( namespace , filename , infer_duration = True , * * parse_options ) : # Create a new annotation object annotation = core . Annotation ( namespace ) parse_options . setdefault ( 'sep' , r'\s+' ) parse_options . setdefault ( 'engine' , 'python' ) parse_options . setdefault ( 'header' , None ) parse_options . setdefault ( 'index_col' , False ) # This is a hack to handle potentially ragged .lab data parse_options . setdefault ( 'names' , range ( 20 ) ) data = pd . read_csv ( filename , * * parse_options ) # Drop all-nan columns data = data . dropna ( how = 'all' , axis = 1 ) # Do we need to add a duration column? # This only applies to event annotations if len ( data . columns ) == 2 : # Insert a column of zeros after the timing data . insert ( 1 , 'duration' , 0 ) if infer_duration : data [ 'duration' ] [ : - 1 ] = data . loc [ : , 0 ] . diff ( ) [ 1 : ] . values else : # Convert from time to duration if infer_duration : data . loc [ : , 1 ] -= data [ 0 ] for row in data . itertuples ( ) : time , duration = row [ 1 : 3 ] value = [ x for x in row [ 3 : ] if x is not None ] [ - 1 ] annotation . append ( time = time , duration = duration , confidence = 1.0 , value = value ) return annotation
11677	def transform ( self , X ) : X = as_features ( X ) return np . vstack ( [ np . mean ( bag , axis = 0 ) for bag in X ] )
13486	def minify ( self , css ) : css = css . replace ( "\r\n" , "\n" ) # get rid of Windows line endings, if they exist for rule in _REPLACERS [ self . level ] : css = re . compile ( rule [ 0 ] , re . MULTILINE | re . UNICODE | re . DOTALL ) . sub ( rule [ 1 ] , css ) return css
8794	def set_all ( self , model , * * tags ) : for name , tag in self . tags . items ( ) : if name in tags : value = tags . pop ( name ) if value : try : tag . set ( model , value ) except TagValidationError as e : raise n_exc . BadRequest ( resource = "tags" , msg = "%s" % ( e . message ) )
2051	def MRC ( cpu , coprocessor , opcode1 , dest , coprocessor_reg_n , coprocessor_reg_m , opcode2 ) : assert coprocessor . type == 'coprocessor' assert opcode1 . type == 'immediate' assert opcode2 . type == 'immediate' assert dest . type == 'register' imm_coprocessor = coprocessor . read ( ) imm_opcode1 = opcode1 . read ( ) imm_opcode2 = opcode2 . read ( ) coprocessor_n_name = coprocessor_reg_n . read ( ) coprocessor_m_name = coprocessor_reg_m . read ( ) if 15 == imm_coprocessor : # MMU if 0 == imm_opcode1 : if 13 == coprocessor_n_name : if 3 == imm_opcode2 : dest . write ( cpu . regfile . read ( 'P15_C13' ) ) return raise NotImplementedError ( "MRC: unimplemented combination of coprocessor, opcode, and coprocessor register" )
8530	def of_structs ( cls , a , b ) : t_diff = ThriftDiff ( a , b ) t_diff . _do_diff ( ) return t_diff
10507	def server_bind ( self , * args , * * kwargs ) : self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) # Can't use super() here since SimpleXMLRPCServer is an old-style class SimpleXMLRPCServer . server_bind ( self , * args , * * kwargs )
12533	def from_set ( self , fileset , check_if_dicoms = True ) : if check_if_dicoms : self . items = [ ] for f in fileset : if is_dicom_file ( f ) : self . items . append ( f ) else : self . items = fileset
12945	def hasSameValues ( self , other , cascadeObject = True ) : if self . FIELDS != other . FIELDS : return False oga = object . __getattribute__ for field in self . FIELDS : thisVal = oga ( self , field ) otherVal = oga ( other , field ) if thisVal != otherVal : return False if cascadeObject is True and issubclass ( field . __class__ , IRForeignLinkFieldBase ) : if thisVal and thisVal . isFetched ( ) : if otherVal and otherVal . isFetched ( ) : theseForeign = thisVal . getObjs ( ) othersForeign = otherVal . getObjs ( ) for i in range ( len ( theseForeign ) ) : if not theseForeign [ i ] . hasSameValues ( othersForeign [ i ] ) : return False else : theseForeign = thisVal . getObjs ( ) for i in range ( len ( theseForeign ) ) : if theseForeign [ i ] . hasUnsavedChanges ( cascadeObjects = True ) : return False else : if otherVal and otherVal . isFetched ( ) : othersForeign = otherVal . getObjs ( ) for i in range ( len ( othersForeign ) ) : if othersForeign [ i ] . hasUnsavedChanges ( cascadeObjects = True ) : return False return True
12842	def receive_id_from_server ( self ) : for message in self . pipe . receive ( ) : if isinstance ( message , IdFactory ) : self . actor_id_factory = message return True return False
5621	def tile_to_zoom_level ( tile , dst_pyramid = None , matching_method = "gdal" , precision = 8 ) : def width_height ( bounds ) : try : l , b , r , t = reproject_geometry ( box ( * bounds ) , src_crs = tile . crs , dst_crs = dst_pyramid . crs ) . bounds except ValueError : raise TopologicalError ( "bounds cannot be translated into target CRS" ) return r - l , t - b if tile . tp . crs == dst_pyramid . crs : return tile . zoom else : if matching_method == "gdal" : # use rasterio/GDAL method to calculate default warp target properties transform , width , height = calculate_default_transform ( tile . tp . crs , dst_pyramid . crs , tile . width , tile . height , * tile . bounds ) # this is the resolution the tile would have in destination TilePyramid CRS tile_resolution = round ( transform [ 0 ] , precision ) elif matching_method == "min" : # calculate the minimum pixel size from the four tile corner pixels l , b , r , t = tile . bounds x = tile . pixel_x_size y = tile . pixel_y_size res = [ ] for bounds in [ ( l , t - y , l + x , t ) , # left top ( l , b , l + x , b + y ) , # left bottom ( r - x , b , r , b + y ) , # right bottom ( r - x , t - y , r , t ) # right top ] : try : w , h = width_height ( bounds ) res . extend ( [ w , h ] ) except TopologicalError : logger . debug ( "pixel outside of destination pyramid" ) if res : tile_resolution = round ( min ( res ) , precision ) else : raise TopologicalError ( "tile outside of destination pyramid" ) else : raise ValueError ( "invalid method given: %s" , matching_method ) logger . debug ( "we are looking for a zoom level interpolating to %s resolution" , tile_resolution ) zoom = 0 while True : td_resolution = round ( dst_pyramid . pixel_x_size ( zoom ) , precision ) if td_resolution <= tile_resolution : break zoom += 1 logger . debug ( "target zoom for %s: %s (%s)" , tile_resolution , zoom , td_resolution ) return zoom
4897	def get_course_duration ( self , obj ) : duration = obj . end - obj . start if obj . start and obj . end else None if duration : return strfdelta ( duration , '{W} weeks {D} days.' ) return ''
2159	def _format_yaml ( self , payload ) : return parser . ordered_dump ( payload , Dumper = yaml . SafeDumper , default_flow_style = False )
12202	def _from_jsonlines ( cls , lines , selector_handler = None , strict = False , debug = False ) : return cls ( json . loads ( "\n" . join ( [ l for l in lines if not cls . REGEX_COMMENT_LINE . match ( l ) ] ) ) , selector_handler = selector_handler , strict = strict , debug = debug )
