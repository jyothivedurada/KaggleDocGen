7109	def get_from_cache ( url : str , cache_dir : Path = None ) -> Path : cache_dir . mkdir ( parents = True , exist_ok = True ) filename = re . sub ( r'.+/' , '' , url ) # get cache path to put the file cache_path = cache_dir / filename if cache_path . exists ( ) : return cache_path # make HEAD request to check ETag response = requests . head ( url ) if response . status_code != 200 : if "www.dropbox.com" in url : # dropbox return code 301, so we ignore this error pass else : raise IOError ( "HEAD request failed for url {}" . format ( url ) ) # add ETag to filename if it exists # etag = response.headers.get("ETag") if not cache_path . exists ( ) : # Download to temporary file, then copy to cache dir once finished. # Otherwise you get corrupt cache entries if the download gets interrupted. fd , temp_filename = tempfile . mkstemp ( ) logger . info ( "%s not found in cache, downloading to %s" , url , temp_filename ) # GET file object req = requests . get ( url , stream = True ) content_length = req . headers . get ( 'Content-Length' ) total = int ( content_length ) if content_length is not None else None progress = Tqdm . tqdm ( unit = "B" , total = total ) with open ( temp_filename , 'wb' ) as temp_file : for chunk in req . iter_content ( chunk_size = 1024 ) : if chunk : # filter out keep-alive new chunks progress . update ( len ( chunk ) ) temp_file . write ( chunk ) progress . close ( ) logger . info ( "copying %s to cache at %s" , temp_filename , cache_path ) shutil . copyfile ( temp_filename , str ( cache_path ) ) logger . info ( "removing temp file %s" , temp_filename ) os . close ( fd ) os . remove ( temp_filename ) return cache_path
7258	def search_address ( self , address , filters = None , startDate = None , endDate = None , types = None ) : lat , lng = self . get_address_coords ( address ) return self . search_point ( lat , lng , filters = filters , startDate = startDate , endDate = endDate , types = types )
7583	def run ( self , ipyclient = None , quiet = False , force = False , block = False , ) : ## stop before trying in raxml if force : for key , oldfile in self . trees : if os . path . exists ( oldfile ) : os . remove ( oldfile ) if os . path . exists ( self . trees . info ) : print ( "Error: set a new name for this job or use Force flag.\nFile exists: {}" . format ( self . trees . info ) ) return ## TODO: add a progress bar tracker here. It could even read it from ## the info file that is being written. ## submit it if not ipyclient : proc = _call_raxml ( self . _command_list ) self . stdout = proc [ 0 ] self . stderr = proc [ 1 ] else : ## find all hosts and submit job to the host with most available engines lbview = ipyclient . load_balanced_view ( ) self . async = lbview . apply ( _call_raxml , self . _command_list ) ## initiate random seed if not quiet : if not ipyclient : ## look for errors if "Overall execution time" not in self . stdout : print ( "Error in raxml run\n" + self . stdout ) else : print ( "job {} finished successfully" . format ( self . params . n ) ) else : print ( "job {} submitted to cluster" . format ( self . params . n ) )
4798	def exists ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a path' ) if not os . path . exists ( self . val ) : self . _err ( 'Expected <%s> to exist, but was not found.' % self . val ) return self
6596	def receive_one ( self ) : if self . nruns == 0 : return None ret = self . communicationChannel . receive_one ( ) if ret is not None : self . nruns -= 1 return ret
11308	def get_image ( self , obj ) : if self . _meta . image_field : return getattr ( obj , self . _meta . image_field )
3572	def peripheral_didDiscoverServices_ ( self , peripheral , services ) : logger . debug ( 'peripheral_didDiscoverServices called' ) # Make sure the discovered services are added to the list of known # services, and kick off characteristic discovery for each one. # NOTE: For some reason the services parameter is never set to a good # value, instead you must query peripheral.services() to enumerate the # discovered services. for service in peripheral . services ( ) : if service_list ( ) . get ( service ) is None : service_list ( ) . add ( service , CoreBluetoothGattService ( service ) ) # Kick off characteristic discovery for this service. Just discover # all characteristics for now. peripheral . discoverCharacteristics_forService_ ( None , service )
11837	def actions ( self , state ) : if state [ - 1 ] is not None : return [ ] # All columns filled; no successors else : col = state . index ( None ) return [ row for row in range ( self . N ) if not self . conflicted ( state , row , col ) ]
11740	def first ( self , symbols ) : ret = set ( ) if EPSILON in symbols : return set ( [ EPSILON ] ) for symbol in symbols : ret |= self . _first [ symbol ] - set ( [ EPSILON ] ) if EPSILON not in self . _first [ symbol ] : break else : ret . add ( EPSILON ) return ret
13820	def _ConvertListValueMessage ( value , message ) : if not isinstance ( value , list ) : raise ParseError ( 'ListValue must be in [] which is {0}.' . format ( value ) ) message . ClearField ( 'values' ) for item in value : _ConvertValueMessage ( item , message . values . add ( ) )
11863	def show_approx ( self , numfmt = '%.3g' ) : return ', ' . join ( [ ( '%s: ' + numfmt ) % ( v , p ) for ( v , p ) in sorted ( self . prob . items ( ) ) ] )
4630	def _derive_y_from_x ( self , x , is_even ) : curve = ecdsa . SECP256k1 . curve # The curve equation over F_p is: # y^2 = x^3 + ax + b a , b , p = curve . a ( ) , curve . b ( ) , curve . p ( ) alpha = ( pow ( x , 3 , p ) + a * x + b ) % p beta = ecdsa . numbertheory . square_root_mod_prime ( alpha , p ) if ( beta % 2 ) == is_even : beta = p - beta return beta
4047	def _totals ( self , query ) : self . add_parameters ( limit = 1 ) query = self . _build_query ( query ) self . _retrieve_data ( query ) self . url_params = None # extract the 'total items' figure return int ( self . request . headers [ "Total-Results" ] )
13091	def get_interface_name ( ) : interface_name = '' interfaces = psutil . net_if_addrs ( ) for name , details in interfaces . items ( ) : for detail in details : if detail . family == socket . AF_INET : ip_address = ipaddress . ip_address ( detail . address ) if not ( ip_address . is_link_local or ip_address . is_loopback ) : interface_name = name break return interface_name
10864	def _update_type ( self , params ) : dozscale = False particles = [ ] for p in listify ( params ) : typ , ind = self . _p2i ( p ) particles . append ( ind ) dozscale = dozscale or typ == 'zscale' particles = set ( particles ) return dozscale , particles
828	def getFieldDescription ( self , fieldName ) : # Find which field it's in description = self . getDescription ( ) + [ ( "end" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if ( name == fieldName ) : break if i >= len ( description ) - 1 : raise RuntimeError ( "Field name %s not found in this encoder" % fieldName ) # Return the offset and width return ( offset , description [ i + 1 ] [ 1 ] - offset )
2225	def _update_hasher ( hasher , data , types = True ) : # Determine if the data should be hashed directly or iterated through if isinstance ( data , ( tuple , list , zip ) ) : needs_iteration = True else : needs_iteration = any ( check ( data ) for check in _HASHABLE_EXTENSIONS . iterable_checks ) if needs_iteration : # Denote that we are hashing over an iterable # Multiple structure bytes makes it harder accidently make conflicts SEP = b'_,_' ITER_PREFIX = b'_[_' ITER_SUFFIX = b'_]_' iter_ = iter ( data ) hasher . update ( ITER_PREFIX ) # first, try to nest quickly without recursive calls # (this works if all data in the sequence is a non-iterable) try : for item in iter_ : prefix , hashable = _convert_to_hashable ( item , types ) binary_data = prefix + hashable + SEP hasher . update ( binary_data ) except TypeError : # need to use recursive calls # Update based on current item _update_hasher ( hasher , item , types ) for item in iter_ : # Ensure the items have a spacer between them _update_hasher ( hasher , item , types ) hasher . update ( SEP ) hasher . update ( ITER_SUFFIX ) else : prefix , hashable = _convert_to_hashable ( data , types ) binary_data = prefix + hashable hasher . update ( binary_data )
2343	def predict_proba ( self , a , b , nb_runs = 6 , nb_jobs = None , gpu = None , idx = 0 , verbose = None , ttest_threshold = 0.01 , nb_max_runs = 16 , train_epochs = 1000 , test_epochs = 1000 ) : Nb_jobs , verbose , gpu = SETTINGS . get_default ( ( 'nb_jobs' , nb_jobs ) , ( 'verbose' , verbose ) , ( 'gpu' , gpu ) ) x = np . stack ( [ a . ravel ( ) , b . ravel ( ) ] , 1 ) ttest_criterion = TTestCriterion ( max_iter = nb_max_runs , runs_per_iter = nb_runs , threshold = ttest_threshold ) AB = [ ] BA = [ ] while ttest_criterion . loop ( AB , BA ) : if nb_jobs != 1 : result_pair = Parallel ( n_jobs = nb_jobs ) ( delayed ( GNN_instance ) ( x , idx = idx , device = 'cuda:{}' . format ( run % gpu ) if gpu else 'cpu' , verbose = verbose , train_epochs = train_epochs , test_epochs = test_epochs ) for run in range ( ttest_criterion . iter , ttest_criterion . iter + nb_runs ) ) else : result_pair = [ GNN_instance ( x , idx = idx , device = 'cuda:0' if gpu else 'cpu' , verbose = verbose , train_epochs = train_epochs , test_epochs = test_epochs ) for run in range ( ttest_criterion . iter , ttest_criterion . iter + nb_runs ) ] AB . extend ( [ runpair [ 0 ] for runpair in result_pair ] ) BA . extend ( [ runpair [ 1 ] for runpair in result_pair ] ) if verbose : print ( "P-value after {} runs : {}" . format ( ttest_criterion . iter , ttest_criterion . p_value ) ) score_AB = np . mean ( AB ) score_BA = np . mean ( BA ) return ( score_BA - score_AB ) / ( score_BA + score_AB )
3290	def get_preferred_path ( self ) : if self . path in ( "" , "/" ) : return "/" # Append '/' for collections if self . is_collection and not self . path . endswith ( "/" ) : return self . path + "/" # TODO: handle case-sensitivity, depending on OS # (FileSystemProvider could do this with os.path: # (?) on unix we can assume that the path already matches exactly the case of filepath # on windows we could use path.lower() or get the real case from the # file system return self . path
12765	def reposition ( self , frame_no ) : for label , j in self . channels . items ( ) : body = self . bodies [ label ] body . position = self . positions [ frame_no , j ] body . linear_velocity = self . velocities [ frame_no , j ]
85	def ContrastNormalization ( alpha = 1.0 , per_channel = False , name = None , deterministic = False , random_state = None ) : # placed here to avoid cyclic dependency from . import contrast as contrast_lib return contrast_lib . LinearContrast ( alpha = alpha , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
5923	def get_configuration ( filename = CONFIGNAME ) : global cfg , configuration # very iffy --- most of the whole config mod should a class #: :data:`cfg` is the instance of :class:`GMXConfigParser` that makes all #: global configuration data accessible cfg = GMXConfigParser ( filename = filename ) # update module-level cfg globals ( ) . update ( cfg . configuration ) # update configdir, templatesdir ... configuration = cfg . configuration # update module-level configuration return cfg
386	def obj_box_horizontal_flip ( im , coords = None , is_rescale = False , is_center = False , is_random = False ) : if coords is None : coords = [ ] def _flip ( im , coords ) : im = flip_axis ( im , axis = 1 , is_random = False ) coords_new = list ( ) for coord in coords : if len ( coord ) != 4 : raise AssertionError ( "coordinate should be 4 values : [x, y, w, h]" ) if is_rescale : if is_center : # x_center' = 1 - x x = 1. - coord [ 0 ] else : # x_center' = 1 - x - w x = 1. - coord [ 0 ] - coord [ 2 ] else : if is_center : # x' = im.width - x x = im . shape [ 1 ] - coord [ 0 ] else : # x' = im.width - x - w x = im . shape [ 1 ] - coord [ 0 ] - coord [ 2 ] coords_new . append ( [ x , coord [ 1 ] , coord [ 2 ] , coord [ 3 ] ] ) return im , coords_new if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : return _flip ( im , coords ) else : return im , coords else : return _flip ( im , coords )
9141	def filter_labels_by_language ( labels , language , broader = False ) : if language == 'any' : return labels if broader : language = tags . tag ( language ) . language . format return [ l for l in labels if tags . tag ( l . language ) . language . format == language ] else : language = tags . tag ( language ) . format return [ l for l in labels if tags . tag ( l . language ) . format == language ]
13324	def info ( ) : env = cpenv . get_active_env ( ) modules = [ ] if env : modules = env . get_modules ( ) active_modules = cpenv . get_active_modules ( ) if not env and not modules and not active_modules : click . echo ( '\nNo active modules...' ) return click . echo ( bold ( '\nActive modules' ) ) if env : click . echo ( format_objects ( [ env ] + active_modules ) ) available_modules = set ( modules ) - set ( active_modules ) if available_modules : click . echo ( bold ( '\nInactive modules in {}\n' ) . format ( cyan ( env . name ) ) ) click . echo ( format_objects ( available_modules , header = False ) ) else : click . echo ( format_objects ( active_modules ) ) available_shared_modules = set ( cpenv . get_modules ( ) ) - set ( active_modules ) if not available_shared_modules : return click . echo ( bold ( '\nInactive shared modules \n' ) ) click . echo ( format_objects ( available_shared_modules , header = False ) )
665	def numpyStr ( array , format = '%f' , includeIndices = False , includeZeros = True ) : shape = array . shape assert ( len ( shape ) <= 2 ) items = [ '[' ] if len ( shape ) == 1 : if includeIndices : format = '%d:' + format if includeZeros : rowItems = [ format % ( c , x ) for ( c , x ) in enumerate ( array ) ] else : rowItems = [ format % ( c , x ) for ( c , x ) in enumerate ( array ) if x != 0 ] else : rowItems = [ format % ( x ) for x in array ] items . extend ( rowItems ) else : ( rows , cols ) = shape if includeIndices : format = '%d,%d:' + format for r in xrange ( rows ) : if includeIndices : rowItems = [ format % ( r , c , x ) for c , x in enumerate ( array [ r ] ) ] else : rowItems = [ format % ( x ) for x in array [ r ] ] if r > 0 : items . append ( '' ) items . append ( '[' ) items . extend ( rowItems ) if r < rows - 1 : items . append ( ']\n' ) else : items . append ( ']' ) items . append ( ']' ) return ' ' . join ( items )
12683	def notice_settings ( request ) : notice_types = NoticeType . objects . all ( ) settings_table = [ ] for notice_type in notice_types : settings_row = [ ] for medium_id , medium_display in NOTICE_MEDIA : form_label = "%s_%s" % ( notice_type . label , medium_id ) setting = NoticeSetting . for_user ( request . user , notice_type , medium_id ) if request . method == "POST" : if request . POST . get ( form_label ) == "on" : if not setting . send : setting . send = True setting . save ( ) else : if setting . send : setting . send = False setting . save ( ) settings_row . append ( ( form_label , setting . send ) ) settings_table . append ( { "notice_type" : notice_type , "cells" : settings_row } ) if request . method == "POST" : next_page = request . POST . get ( "next_page" , "." ) return HttpResponseRedirect ( next_page ) settings = { "column_headers" : [ medium_display for medium_id , medium_display in NOTICE_MEDIA ] , "rows" : settings_table , } return render_to_response ( "notification/notice_settings.html" , { "notice_types" : notice_types , "notice_settings" : settings , } , context_instance = RequestContext ( request ) )
12855	def subtree ( events ) : stack = 0 for obj in events : if obj [ 'type' ] == ENTER : stack += 1 elif obj [ 'type' ] == EXIT : if stack == 0 : break stack -= 1 yield obj
4540	def advance_permutation ( a , increasing = True , forward = True ) : if not forward : a . reverse ( ) cmp = operator . lt if increasing else operator . gt try : i = next ( i for i in reversed ( range ( len ( a ) - 1 ) ) if cmp ( a [ i ] , a [ i + 1 ] ) ) j = next ( j for j in reversed ( range ( i + 1 , len ( a ) ) ) if cmp ( a [ i ] , a [ j ] ) ) except StopIteration : # This is the lexicographically last permutation. if forward : a . reverse ( ) return False a [ i ] , a [ j ] = a [ j ] , a [ i ] a [ i + 1 : ] = reversed ( a [ i + 1 : ] ) if not forward : a . reverse ( ) return True
7553	def nworker ( data , chunk ) : ## set the thread limit on the remote engine oldlimit = set_mkl_thread_limit ( 1 ) ## open seqarray view, the modified arr is in bootstarr with h5py . File ( data . database . input , 'r' ) as io5 : seqview = io5 [ "bootsarr" ] [ : ] maparr = io5 [ "bootsmap" ] [ : , 0 ] smps = io5 [ "quartets" ] [ chunk : chunk + data . _chunksize ] ## create an N-mask array of all seq cols nall_mask = seqview [ : ] == 78 ## init arrays to fill with results rquartets = np . zeros ( ( smps . shape [ 0 ] , 4 ) , dtype = np . uint16 ) rinvariants = np . zeros ( ( smps . shape [ 0 ] , 16 , 16 ) , dtype = np . uint16 ) ## fill arrays with results as we compute them. This iterates ## over all of the quartet sets in this sample chunk. It would ## be nice to have this all numbified. for idx in xrange ( smps . shape [ 0 ] ) : sidx = smps [ idx ] seqs = seqview [ sidx ] ## these axis calls cannot be numbafied, but I can't ## find a faster way that is JIT compiled, and I've ## really, really, really tried. Tried again now that ## numba supports axis args for np.sum. Still can't ## get speed improvements by numbifying this loop. nmask = np . any ( nall_mask [ sidx ] , axis = 0 ) nmask += np . all ( seqs == seqs [ 0 ] , axis = 0 ) ## here are the jitted funcs bidx , invar = calculate ( seqs , maparr , nmask , TESTS ) ## store results rquartets [ idx ] = smps [ idx ] [ bidx ] rinvariants [ idx ] = invar ## reset thread limit set_mkl_thread_limit ( oldlimit ) ## return results... return rquartets , rinvariants
8350	def _toStringSubclass ( self , text , subclass ) : self . endData ( ) self . handle_data ( text ) self . endData ( subclass )
6529	def get_extenders ( ) : # pylint: disable=protected-access if not hasattr ( get_extenders , '_CACHE' ) : get_extenders . _CACHE = dict ( ) for entry in pkg_resources . iter_entry_points ( 'tidypy.extenders' ) : try : get_extenders . _CACHE [ entry . name ] = entry . load ( ) except ImportError as exc : # pragma: no cover output_error ( 'Could not load extender "%s" defined by "%s": %s' % ( entry , entry . dist , exc , ) , ) return get_extenders . _CACHE
5477	def parse_rfc3339_utc_string ( rfc3339_utc_string ) : # The timestamp from the Google Operations are all in RFC3339 format, but # they are sometimes formatted to millisconds, microseconds, sometimes # nanoseconds, and sometimes only seconds: # * 2016-11-14T23:05:56Z # * 2016-11-14T23:05:56.010Z # * 2016-11-14T23:05:56.010429Z # * 2016-11-14T23:05:56.010429380Z m = re . match ( r'(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):(\d{2}).?(\d*)Z' , rfc3339_utc_string ) # It would be unexpected to get a different date format back from Google. # If we raise an exception here, we can break people completely. # Instead, let's just return None and people can report that some dates # are not showing up. # We might reconsider this approach in the future; it was originally # established when dates were only used for display. if not m : return None groups = m . groups ( ) if len ( groups [ 6 ] ) not in ( 0 , 3 , 6 , 9 ) : return None # Create a UTC datestamp from parsed components # 1- Turn components 0-5 from strings to integers # 2- If the last component does not exist, set it to 0. # If it does exist, make sure to interpret it as milliseconds. g = [ int ( val ) for val in groups [ : 6 ] ] fraction = groups [ 6 ] if not fraction : micros = 0 elif len ( fraction ) == 3 : micros = int ( fraction ) * 1000 elif len ( fraction ) == 6 : micros = int ( fraction ) elif len ( fraction ) == 9 : # When nanoseconds are provided, we round micros = int ( round ( int ( fraction ) / 1000 ) ) else : assert False , 'Fraction length not 0, 6, or 9: {}' . len ( fraction ) try : return datetime ( g [ 0 ] , g [ 1 ] , g [ 2 ] , g [ 3 ] , g [ 4 ] , g [ 5 ] , micros , tzinfo = pytz . utc ) except ValueError as e : assert False , 'Could not parse RFC3339 datestring: {} exception: {}' . format ( rfc3339_utc_string , e )
4226	def _data_root_Linux ( ) : fallback = os . path . expanduser ( '~/.local/share' ) root = os . environ . get ( 'XDG_DATA_HOME' , None ) or fallback return os . path . join ( root , 'python_keyring' )
9656	def get_the_node_dict ( G , name ) : for node in G . nodes ( data = True ) : if node [ 0 ] == name : return node [ 1 ]
13065	def expose_ancestors_or_children ( self , member , collection , lang = None ) : x = { "id" : member . id , "label" : str ( member . get_label ( lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size , "semantic" : self . semantic ( member , parent = collection ) } if isinstance ( member , ResourceCollection ) : x [ "lang" ] = str ( member . lang ) return x
13407	def sendToLogbook ( self , fileName , logType , location = None ) : import subprocess success = True if logType == "MCC" : fileString = "" if not self . imagePixmap . isNull ( ) : fileString = fileName + "." + self . imageType logcmd = "xml2elog " + fileName + ".xml " + fileString process = subprocess . Popen ( logcmd , shell = True ) process . wait ( ) if process . returncode != 0 : success = False else : from shutil import copy path = "/u1/" + location . lower ( ) + "/physics/logbook/data/" # Prod path # path = "/home/softegr/alverson/log_test/" # Dev path try : if not self . imagePixmap . isNull ( ) : copy ( fileName + ".png" , path ) if self . imageType == "png" : copy ( fileName + ".ps" , path ) else : copy ( fileName + "." + self . imageType , path ) # Copy .xml file last to ensure images will be picked up by cron job # print("Copying file " + fileName + " to path " + path) copy ( fileName + ".xml" , path ) except IOError as error : print ( error ) success = False return success
1231	def tf_import_experience ( self , states , internals , actions , terminal , reward ) : return self . memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
4973	def clean_channel_worker_username ( self ) : channel_worker_username = self . cleaned_data [ 'channel_worker_username' ] . strip ( ) try : User . objects . get ( username = channel_worker_username ) except User . DoesNotExist : raise ValidationError ( ValidationMessages . INVALID_CHANNEL_WORKER . format ( channel_worker_username = channel_worker_username ) ) return channel_worker_username
3021	def get_access_token ( self , http = None , additional_claims = None ) : if additional_claims is None : if self . access_token is None or self . access_token_expired : self . refresh ( None ) return client . AccessTokenInfo ( access_token = self . access_token , expires_in = self . _expires_in ( ) ) else : # Create a 1 time token token , unused_expiry = self . _create_token ( additional_claims ) return client . AccessTokenInfo ( access_token = token , expires_in = self . _MAX_TOKEN_LIFETIME_SECS )
8523	def add_float ( self , name , min , max , warp = None ) : min , max = map ( float , ( min , max ) ) if not min < max : raise ValueError ( 'variable %s: min >= max error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or "log",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = FloatVariable ( name , min , max , warp )
1968	def wait ( self , readfds , writefds , timeout ) : logger . debug ( "WAIT:" ) logger . debug ( f"\tProcess {self._current} is going to wait for [ {readfds!r} {writefds!r} {timeout!r} ]" ) logger . debug ( f"\tProcess: {self.procs!r}" ) logger . debug ( f"\tRunning: {self.running!r}" ) logger . debug ( f"\tRWait: {self.rwait!r}" ) logger . debug ( f"\tTWait: {self.twait!r}" ) logger . debug ( f"\tTimers: {self.timers!r}" ) for fd in readfds : self . rwait [ fd ] . add ( self . _current ) for fd in writefds : self . twait [ fd ] . add ( self . _current ) if timeout is not None : self . timers [ self . _current ] = self . clocks + timeout procid = self . _current # self.sched() next_index = ( self . running . index ( procid ) + 1 ) % len ( self . running ) self . _current = self . running [ next_index ] logger . debug ( f"\tTransfer control from process {procid} to {self._current}" ) logger . debug ( f"\tREMOVING {procid!r} from {self.running!r}. Current: {self._current!r}" ) self . running . remove ( procid ) if self . _current not in self . running : logger . debug ( "\tCurrent not running. Checking for timers..." ) self . _current = None self . check_timers ( )
8377	def get_attribute ( element , attribute , default = 0 ) : a = element . getAttribute ( attribute ) if a == "" : return default return a
11193	def metadata ( proto_dataset_uri , relpath_in_dataset , key , value ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri , config_path = CONFIG_PATH ) proto_dataset . add_item_metadata ( handle = relpath_in_dataset , key = key , value = value )
13224	def main ( ) : parser = argparse . ArgumentParser ( description = 'Discover and ingest metadata from document sources, ' 'including lsstdoc-based LaTeX documents and ' 'reStructuredText-based technotes. Metadata can be ' 'upserted into the LSST Projectmeta MongoDB.' ) parser . add_argument ( '--ltd-product' , dest = 'ltd_product_url' , help = 'URL of an LSST the Docs product ' '(https://keeper.lsst.codes/products/<slug>). If provided, ' 'only this document will be ingested.' ) parser . add_argument ( '--github-token' , help = 'GitHub personal access token.' ) parser . add_argument ( '--mongodb-uri' , help = 'MongoDB connection URI. If provided, metadata will be loaded ' 'into the Projectmeta database. Omit this argument to just ' 'test the ingest pipeline.' ) parser . add_argument ( '--mongodb-db' , default = 'lsstprojectmeta' , help = 'Name of MongoDB database' ) parser . add_argument ( '--mongodb-collection' , default = 'resources' , help = 'Name of the MongoDB collection for projectmeta resources' ) args = parser . parse_args ( ) # Configure the root logger stream_handler = logging . StreamHandler ( ) stream_formatter = logging . Formatter ( '%(asctime)s %(levelname)8s %(name)s | %(message)s' ) stream_handler . setFormatter ( stream_formatter ) root_logger = logging . getLogger ( ) root_logger . addHandler ( stream_handler ) root_logger . setLevel ( logging . WARNING ) # Configure app logger app_logger = logging . getLogger ( 'lsstprojectmeta' ) app_logger . setLevel ( logging . DEBUG ) if args . mongodb_uri is not None : mongo_client = AsyncIOMotorClient ( args . mongodb_uri , ssl = True ) collection = mongo_client [ args . mongodb_db ] [ args . mongodb_collection ] else : collection = None loop = asyncio . get_event_loop ( ) if args . ltd_product_url is not None : # Run single technote loop . run_until_complete ( run_single_ltd_doc ( args . ltd_product_url , args . github_token , collection ) ) else : # Run bulk technote processing loop . run_until_complete ( run_bulk_etl ( args . github_token , collection ) )
6198	def print_sizes ( self ) : float_size = 4 MB = 1024 * 1024 size_ = self . n_samples * float_size em_size = size_ * self . num_particles / MB pos_size = 3 * size_ * self . num_particles / MB print ( " Number of particles:" , self . num_particles ) print ( " Number of time steps:" , self . n_samples ) print ( " Emission array - 1 particle (float32): %.1f MB" % ( size_ / MB ) ) print ( " Emission array (float32): %.1f MB" % em_size ) print ( " Position array (float32): %.1f MB " % pos_size )
11190	def show ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) click . secho ( readme_content )
12718	def angle_rates ( self ) : return [ self . ode_obj . getAngleRate ( i ) for i in range ( self . ADOF ) ]
71	def clip_out_of_image ( self ) : bbs_cut = [ bb . clip_out_of_image ( self . shape ) for bb in self . bounding_boxes if bb . is_partly_within_image ( self . shape ) ] return BoundingBoxesOnImage ( bbs_cut , shape = self . shape )
394	def cross_entropy_reward_loss ( logits , actions , rewards , name = None ) : cross_entropy = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = actions , logits = logits , name = name ) return tf . reduce_sum ( tf . multiply ( cross_entropy , rewards ) )
8532	def can_diff ( msg_a , msg_b ) : if msg_a . method != msg_b . method : return False , 'method name of messages do not match' if len ( msg_a . args ) != len ( msg_b . args ) or not msg_a . args . is_isomorphic_to ( msg_b . args ) : return False , 'argument signature of methods do not match' return True , None
8630	def get_projects ( session , query ) : # GET /api/projects/0.1/projects response = make_get_request ( session , 'projects' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
13002	def _filter_cluster_data ( self ) : min_temp = self . temperature_range_slider . value [ 0 ] max_temp = self . temperature_range_slider . value [ 1 ] temp_mask = np . logical_and ( self . cluster . catalog [ 'temperature' ] >= min_temp , self . cluster . catalog [ 'temperature' ] <= max_temp ) min_lum = self . luminosity_range_slider . value [ 0 ] max_lum = self . luminosity_range_slider . value [ 1 ] lum_mask = np . logical_and ( self . cluster . catalog [ 'luminosity' ] >= min_lum , self . cluster . catalog [ 'luminosity' ] <= max_lum ) selected_mask = np . isin ( self . cluster . catalog [ 'id' ] , self . selection_ids ) filter_mask = temp_mask & lum_mask & selected_mask self . filtered_data = self . cluster . catalog [ filter_mask ] . data self . source . data = { 'id' : list ( self . filtered_data [ 'id' ] ) , 'temperature' : list ( self . filtered_data [ 'temperature' ] ) , 'luminosity' : list ( self . filtered_data [ 'luminosity' ] ) , 'color' : list ( self . filtered_data [ 'color' ] ) } logging . debug ( "Selected data is now: %s" , self . filtered_data )
10950	def sample ( field , inds = None , slicer = None , flat = True ) : if inds is not None : out = field . ravel ( ) [ inds ] elif slicer is not None : out = field [ slicer ] . ravel ( ) else : out = field if flat : return out . ravel ( ) return out
1256	def setup_hooks ( self ) : hooks = list ( ) # Checkpoint saver hook if self . saver_spec is not None and ( self . execution_type == 'single' or self . distributed_spec [ 'task_index' ] == 0 ) : self . saver_directory = self . saver_spec [ 'directory' ] hooks . append ( tf . train . CheckpointSaverHook ( checkpoint_dir = self . saver_directory , save_secs = self . saver_spec . get ( 'seconds' , None if 'steps' in self . saver_spec else 600 ) , save_steps = self . saver_spec . get ( 'steps' ) , # Either one or the other has to be set. saver = None , # None since given via 'scaffold' argument. checkpoint_basename = self . saver_spec . get ( 'basename' , 'model.ckpt' ) , scaffold = self . scaffold , listeners = None ) ) else : self . saver_directory = None # Stop at step hook # hooks.append(tf.train.StopAtStepHook( # num_steps=???, # This makes more sense, if load and continue training. # last_step=None # Either one or the other has to be set. # )) # # Step counter hook # hooks.append(tf.train.StepCounterHook( # every_n_steps=counter_config.get('steps', 100), # Either one or the other has to be set. # every_n_secs=counter_config.get('secs'), # Either one or the other has to be set. # output_dir=None, # None since given via 'summary_writer' argument. # summary_writer=summary_writer # )) # Other available hooks: # tf.train.FinalOpsHook(final_ops, final_ops_feed_dict=None) # tf.train.GlobalStepWaiterHook(wait_until_step) # tf.train.LoggingTensorHook(tensors, every_n_iter=None, every_n_secs=None) # tf.train.NanTensorHook(loss_tensor, fail_on_nan_loss=True) # tf.train.ProfilerHook(save_steps=None, save_secs=None, output_dir='', show_dataflow=True, show_memory=False) return hooks
962	def matchPatterns ( patterns , keys ) : results = [ ] if patterns : for pattern in patterns : prog = re . compile ( pattern ) for key in keys : if prog . match ( key ) : results . append ( key ) else : return None return results
4042	def _extract_links ( self ) : extracted = dict ( ) try : for key , value in self . request . links . items ( ) : parsed = urlparse ( value [ "url" ] ) fragment = "{path}?{query}" . format ( path = parsed [ 2 ] , query = parsed [ 4 ] ) extracted [ key ] = fragment # add a 'self' link parsed = list ( urlparse ( self . self_link ) ) # strip 'format' query parameter stripped = "&" . join ( [ "%s=%s" % ( p [ 0 ] , p [ 1 ] ) for p in parse_qsl ( parsed [ 4 ] ) if p [ 0 ] != "format" ] ) # rebuild url fragment # this is a death march extracted [ "self" ] = urlunparse ( [ parsed [ 0 ] , parsed [ 1 ] , parsed [ 2 ] , parsed [ 3 ] , stripped , parsed [ 5 ] ] ) return extracted except KeyError : # No links present, because it's a single item return None
13136	def request ( key , features , query , timeout = 5 ) : data = { } data [ 'key' ] = key data [ 'features' ] = '/' . join ( [ f for f in features if f in FEATURES ] ) data [ 'query' ] = quote ( query ) data [ 'format' ] = 'json' r = requests . get ( API_URL . format ( * * data ) , timeout = timeout ) results = json . loads ( _unicode ( r . content ) ) return results
1124	def Loc ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : result = parser . _accept ( kind ) if result is unmatched : return result return result . loc return rule
2572	def dbm_starter ( priority_msgs , resource_msgs , * args , * * kwargs ) : dbm = DatabaseManager ( * args , * * kwargs ) dbm . start ( priority_msgs , resource_msgs )
6437	def dist_abs ( self , src , tar , weights = 'exponential' , max_length = 8 , normalized = False ) : # Calculate the eudex hashes and XOR them xored = eudex ( src , max_length = max_length ) ^ eudex ( tar , max_length = max_length ) # Simple hamming distance (all bits are equal) if not weights : binary = bin ( xored ) distance = binary . count ( '1' ) if normalized : return distance / ( len ( binary ) - 2 ) return distance # If weights is a function, it should create a generator, # which we now use to populate a list if callable ( weights ) : weights = weights ( ) elif weights == 'exponential' : weights = Eudex . gen_exponential ( ) elif weights == 'fibonacci' : weights = Eudex . gen_fibonacci ( ) if isinstance ( weights , GeneratorType ) : weights = [ next ( weights ) for _ in range ( max_length ) ] [ : : - 1 ] # Sum the weighted hamming distance distance = 0 max_distance = 0 while ( xored or normalized ) and weights : max_distance += 8 * weights [ - 1 ] distance += bin ( xored & 0xFF ) . count ( '1' ) * weights . pop ( ) xored >>= 8 if normalized : distance /= max_distance return distance
8706	def verify_file ( self , path , destination , verify = 'none' ) : content = from_file ( path ) log . info ( 'Verifying using %s...' % verify ) if verify == 'raw' : data = self . download_file ( destination ) if content != data : log . error ( 'Raw verification failed.' ) raise VerificationError ( 'Verification failed.' ) else : log . info ( 'Verification successful. Contents are identical.' ) elif verify == 'sha1' : #Calculate SHA1 on remote file. Extract just hash from result data = self . __exchange ( 'shafile("' + destination + '")' ) . splitlines ( ) [ 1 ] log . info ( 'Remote SHA1: %s' , data ) #Calculate hash of local data filehashhex = hashlib . sha1 ( content . encode ( ENCODING ) ) . hexdigest ( ) log . info ( 'Local SHA1: %s' , filehashhex ) if data != filehashhex : log . error ( 'SHA1 verification failed.' ) raise VerificationError ( 'SHA1 Verification failed.' ) else : log . info ( 'Verification successful. Checksums match' ) elif verify != 'none' : raise Exception ( verify + ' is not a valid verification method.' )
2087	def _convert_pagenum ( self , kwargs ) : for key in ( 'next' , 'previous' ) : if not kwargs . get ( key ) : continue match = re . search ( r'page=(?P<num>[\d]+)' , kwargs [ key ] ) if match is None and key == 'previous' : kwargs [ key ] = 1 continue kwargs [ key ] = int ( match . groupdict ( ) [ 'num' ] )
4657	def clear ( self ) : self . ops = [ ] self . wifs = set ( ) self . signing_accounts = [ ] # This makes sure that _is_constructed will return False afterwards self [ "expiration" ] = None dict . __init__ ( self , { } )
3393	def undelete_model_genes ( cobra_model ) : if cobra_model . _trimmed_genes is not None : for x in cobra_model . _trimmed_genes : x . functional = True if cobra_model . _trimmed_reactions is not None : for the_reaction , ( lower_bound , upper_bound ) in cobra_model . _trimmed_reactions . items ( ) : the_reaction . lower_bound = lower_bound the_reaction . upper_bound = upper_bound cobra_model . _trimmed_genes = [ ] cobra_model . _trimmed_reactions = { } cobra_model . _trimmed = False
4579	def update ( desc , other = None , * * kwds ) : other = other and _as_dict ( other ) or { } for i in other , kwds : for k , v in i . items ( ) : if isinstance ( v , dict ) : # Only for dicts, merge instead of overwriting old_v = desc [ k ] for k2 , v2 in v . items ( ) : if v2 is None : old_v . pop ( k2 , None ) else : old_v [ k2 ] = v2 else : set_one ( desc , k , v )
10178	def list_bookmarks ( self , start_date = None , end_date = None , limit = None ) : query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) . sort ( { 'date' : { 'order' : 'desc' } } ) range_args = { } if start_date : range_args [ 'gte' ] = self . _format_range_dt ( start_date . replace ( microsecond = 0 ) ) if end_date : range_args [ 'lte' ] = self . _format_range_dt ( end_date . replace ( microsecond = 0 ) ) if range_args : query = query . filter ( 'range' , date = range_args ) return query [ 0 : limit ] . execute ( ) if limit else query . scan ( )
13455	def _parse_args ( args ) : # parser uses custom usage string, with 'usage: ' removed, as it is # added automatically via argparser. parser = argparse . ArgumentParser ( description = "Remove and/or rearrange " + "sections from each line of a file(s)." , usage = _usage ( ) [ len ( 'usage: ' ) : ] ) parser . add_argument ( '-b' , "--bytes" , action = 'store' , type = lst , default = [ ] , help = "Bytes to select" ) parser . add_argument ( '-c' , "--chars" , action = 'store' , type = lst , default = [ ] , help = "Character to select" ) parser . add_argument ( '-f' , "--fields" , action = 'store' , type = lst , default = [ ] , help = "Fields to select" ) parser . add_argument ( '-d' , "--delimiter" , action = 'store' , default = "\t" , help = "Sets field delimiter(default is TAB)" ) parser . add_argument ( '-e' , "--regex" , action = 'store_true' , help = 'Enable regular expressions to be used as input ' + 'delimiter' ) parser . add_argument ( '-s' , '--skip' , action = 'store_true' , help = "Skip lines that do not contain input delimiter." ) parser . add_argument ( '-S' , "--separator" , action = 'store' , default = "\t" , help = "Sets field separator for output." ) parser . add_argument ( 'file' , nargs = '*' , default = "-" , help = "File(s) to cut" ) return parser . parse_args ( args )
1970	def signal_receive ( self , fd ) : connections = self . connections if connections ( fd ) and self . twait [ connections ( fd ) ] : procid = random . sample ( self . twait [ connections ( fd ) ] , 1 ) [ 0 ] self . awake ( procid )
5892	def get_urls ( self ) : urls = patterns ( '' , url ( r'^upload/$' , self . admin_site . admin_view ( self . handle_upload ) , name = 'quill-file-upload' ) , ) return urls + super ( QuillAdmin , self ) . get_urls ( )
2535	def set_doc_comment ( self , doc , comment ) : if not self . doc_comment_set : self . doc_comment_set = True doc . comment = comment else : raise CardinalityError ( 'Document::Comment' )
605	def getVersion ( ) : with open ( os . path . join ( REPO_DIR , "VERSION" ) , "r" ) as versionFile : return versionFile . read ( ) . strip ( )
4410	async def disconnect ( self ) : if not self . is_connected : return await self . stop ( ) ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , None )
7922	def __prepare_resource ( data ) : if not data : return None data = unicode ( data ) try : resource = RESOURCEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( resource . encode ( "utf-8" ) ) > 1023 : raise JIDError ( "Resource name too long" ) return resource
8264	def swarm ( self , x , y , r = 100 ) : sc = _ctx . stroke ( 0 , 0 , 0 , 0 ) sw = _ctx . strokewidth ( 0 ) _ctx . push ( ) _ctx . transform ( _ctx . CORNER ) _ctx . translate ( x , y ) for i in _range ( r * 3 ) : clr = choice ( self ) . copy ( ) clr . alpha -= 0.5 * random ( ) _ctx . fill ( clr ) clr = choice ( self ) _ctx . stroke ( clr ) _ctx . strokewidth ( 10 * random ( ) ) _ctx . rotate ( 360 * random ( ) ) r2 = r * 0.5 * random ( ) _ctx . oval ( r * random ( ) , 0 , r2 , r2 ) _ctx . pop ( ) _ctx . strokewidth ( sw ) if sc is None : _ctx . nostroke ( ) else : _ctx . stroke ( sc )
6391	def encode ( self , word , max_length = 8 ) : # Lowercase input & filter unknown characters word = '' . join ( char for char in word . lower ( ) if char in self . _initial_phones ) if not word : word = 'รท' # Perform initial eudex coding of each character values = [ self . _initial_phones [ word [ 0 ] ] ] values += [ self . _trailing_phones [ char ] for char in word [ 1 : ] ] # Right-shift by one to determine if second instance should be skipped shifted_values = [ _ >> 1 for _ in values ] condensed_values = [ values [ 0 ] ] for n in range ( 1 , len ( shifted_values ) ) : if shifted_values [ n ] != shifted_values [ n - 1 ] : condensed_values . append ( values [ n ] ) # Add padding after first character & trim beyond max_length values = ( [ condensed_values [ 0 ] ] + [ 0 ] * max ( 0 , max_length - len ( condensed_values ) ) + condensed_values [ 1 : max_length ] ) # Combine individual character values into eudex hash hash_value = 0 for val in values : hash_value = ( hash_value << 8 ) | val return hash_value
7377	def _user_headers ( self , headers = None ) : h = self . copy ( ) if headers is not None : keys = set ( headers . keys ( ) ) if h . get ( 'Authorization' , False ) : keys -= { 'Authorization' } for key in keys : h [ key ] = headers [ key ] return h
10460	def _ldtpize_accessible ( self , acc ) : actual_role = self . _get_role ( acc ) label = self . _get_title ( acc ) if re . match ( "AXWindow" , actual_role , re . M | re . U | re . L ) : # Strip space and new line from window title strip = r"( |\n)" else : # Strip space, colon, dot, underscore and new line from # all other object types strip = r"( |:|\.|_|\n)" if label : # Return the role type (if, not in the know list of roles, # return ukn - unknown), strip the above characters from name # also return labely_by string label = re . sub ( strip , u"" , label ) role = abbreviated_roles . get ( actual_role , "ukn" ) if self . _ldtp_debug and role == "ukn" : print ( actual_role , acc ) return role , label
3725	def dipole_moment ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in _dipole_CCDB . index and not np . isnan ( _dipole_CCDB . at [ CASRN , 'Dipole' ] ) : methods . append ( CCCBDB ) if CASRN in _dipole_Muller . index and not np . isnan ( _dipole_Muller . at [ CASRN , 'Dipole' ] ) : methods . append ( MULLER ) if CASRN in _dipole_Poling . index and not np . isnan ( _dipole_Poling . at [ CASRN , 'Dipole' ] ) : methods . append ( POLING ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CCCBDB : _dipole = float ( _dipole_CCDB . at [ CASRN , 'Dipole' ] ) elif Method == MULLER : _dipole = float ( _dipole_Muller . at [ CASRN , 'Dipole' ] ) elif Method == POLING : _dipole = float ( _dipole_Poling . at [ CASRN , 'Dipole' ] ) elif Method == NONE : _dipole = None else : raise Exception ( 'Failure in in function' ) return _dipole
8076	def rectmode ( self , mode = None ) : if mode in ( self . CORNER , self . CENTER , self . CORNERS ) : self . rectmode = mode return self . rectmode elif mode is None : return self . rectmode else : raise ShoebotError ( _ ( "rectmode: invalid input" ) )
4309	def _validate_num_channels ( input_filepath_list , combine_type ) : channels = [ file_info . channels ( f ) for f in input_filepath_list ] if not core . all_equal ( channels ) : raise IOError ( "Input files do not have the same number of channels. The " "{} combine type requires that all files have the same " "number of channels" . format ( combine_type ) )
5342	def __get_dash_menu ( self , kibiter_major ) : # omenu = OrderedDict() omenu = [ ] # Start with Overview omenu . append ( self . menu_panels_common [ 'Overview' ] ) # Now the data _getsources ds_menu = self . __get_menu_entries ( kibiter_major ) # Remove the kafka and community menus, they will be included at the end kafka_menu = None community_menu = None found_kafka = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == KAFKA_NAME ] if found_kafka : kafka_menu = ds_menu . pop ( found_kafka [ 0 ] ) found_community = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == COMMUNITY_NAME ] if found_community : community_menu = ds_menu . pop ( found_community [ 0 ] ) ds_menu . sort ( key = operator . itemgetter ( 'name' ) ) omenu += ds_menu # If kafka and community are present add them before the Data Status and About if kafka_menu : omenu . append ( kafka_menu ) if community_menu : omenu . append ( community_menu ) # At the end Data Status, About omenu . append ( self . menu_panels_common [ 'Data Status' ] ) omenu . append ( self . menu_panels_common [ 'About' ] ) logger . debug ( "Menu for panels: %s" , json . dumps ( ds_menu , indent = 4 ) ) return omenu
13175	def prev ( self , name = None ) : if self . parent is None or self . index is None : return None for idx in xrange ( self . index - 1 , - 1 , - 1 ) : if name is None or self . parent [ idx ] . tagname == name : return self . parent [ idx ]
916	def debug ( self , msg , * args , * * kwargs ) : self . _baseLogger . debug ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )
312	def downside_risk ( returns , required_return = 0 , period = DAILY ) : return ep . downside_risk ( returns , required_return = required_return , period = period )
1840	def JNG ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . OR ( cpu . ZF , cpu . SF != cpu . OF ) , target . read ( ) , cpu . PC )
13610	def load_gitconfig ( self ) : gitconfig_path = os . path . expanduser ( '~/.gitconfig' ) if os . path . exists ( gitconfig_path ) : parser = Parser ( ) parser . read ( gitconfig_path ) parser . sections ( ) return parser pass
6794	def shell ( self ) : r = self . local_renderer if '@' in self . genv . host_string : r . env . shell_host_string = self . genv . host_string else : r . env . shell_host_string = '{user}@{host_string}' r . env . shell_default_dir = self . genv . shell_default_dir_template r . env . shell_interactive_djshell_str = self . genv . interactive_shell_template r . run_or_local ( 'ssh -t -i {key_filename} {shell_host_string} "{shell_interactive_djshell_str}"' )
10102	def _make_file_dict ( self , f ) : if isinstance ( f , dict ) : file_obj = f [ 'file' ] if 'filename' in f : file_name = f [ 'filename' ] else : file_name = file_obj . name else : file_obj = f file_name = f . name b64_data = base64 . b64encode ( file_obj . read ( ) ) return { 'id' : file_name , 'data' : b64_data . decode ( ) if six . PY3 else b64_data , }
12183	def method_exists ( cls , method ) : methods = cls . API_METHODS for key in method . split ( '.' ) : methods = methods . get ( key ) if methods is None : break if isinstance ( methods , str ) : logger . debug ( '%r: %r' , method , methods ) return True return False
6708	def get_file_hash ( fin , block_size = 2 ** 20 ) : if isinstance ( fin , six . string_types ) : fin = open ( fin ) h = hashlib . sha512 ( ) while True : data = fin . read ( block_size ) if not data : break try : h . update ( data ) except TypeError : # Fixes Python3 error "TypeError: Unicode-objects must be encoded before hashing". h . update ( data . encode ( 'utf-8' ) ) return h . hexdigest ( )
6315	def _add_resource_descriptions_to_pools ( self , meta_list ) : if not meta_list : return for meta in meta_list : getattr ( resources , meta . resource_type ) . add ( meta )
11501	def list_communities ( self , token = None ) : parameters = dict ( ) if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.list' , parameters ) return response
4497	def project ( self , project_id ) : type_ = self . guid ( project_id ) url = self . _build_url ( type_ , project_id ) if type_ in Project . _types : return Project ( self . _json ( self . _get ( url ) , 200 ) , self . session ) raise OSFException ( '{} is unrecognized type {}. Clone supports projects and registrations' . format ( project_id , type_ ) )
4521	def get ( self , ring , angle ) : pixel = self . angleToPixel ( angle , ring ) return self . _get_base ( pixel )
8114	def angle ( x0 , y0 , x1 , y1 ) : return degrees ( atan2 ( y1 - y0 , x1 - x0 ) )
2668	def sixteen_oscillator_two_stimulated_ensembles_grid ( ) : parameters = legion_parameters ( ) parameters . teta_x = - 1.1 template_dynamic_legion ( 16 , 2000 , 1500 , conn_type = conn_type . GRID_FOUR , params = parameters , stimulus = [ 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 ] )
5784	def select_write ( self , timeout = None ) : _ , write_ready , _ = select . select ( [ ] , [ self . _socket ] , [ ] , timeout ) return len ( write_ready ) > 0
9970	def _get_col_index ( name ) : index = string . ascii_uppercase . index col = 0 for c in name . upper ( ) : col = col * 26 + index ( c ) + 1 return col
1395	def addNewTopology ( self , state_manager , topologyName ) : topology = Topology ( topologyName , state_manager . name ) Log . info ( "Adding new topology: %s, state_manager: %s" , topologyName , state_manager . name ) self . topologies . append ( topology ) # Register a watch on topology and change # the topologyInfo on any new change. topology . register_watch ( self . setTopologyInfo ) def on_topology_pplan ( data ) : """watch physical plan""" Log . info ( "Watch triggered for topology pplan: " + topologyName ) topology . set_physical_plan ( data ) if not data : Log . debug ( "No data to be set" ) def on_topology_packing_plan ( data ) : """watch packing plan""" Log . info ( "Watch triggered for topology packing plan: " + topologyName ) topology . set_packing_plan ( data ) if not data : Log . debug ( "No data to be set" ) def on_topology_execution_state ( data ) : """watch execution state""" Log . info ( "Watch triggered for topology execution state: " + topologyName ) topology . set_execution_state ( data ) if not data : Log . debug ( "No data to be set" ) def on_topology_tmaster ( data ) : """set tmaster""" Log . info ( "Watch triggered for topology tmaster: " + topologyName ) topology . set_tmaster ( data ) if not data : Log . debug ( "No data to be set" ) def on_topology_scheduler_location ( data ) : """set scheduler location""" Log . info ( "Watch triggered for topology scheduler location: " + topologyName ) topology . set_scheduler_location ( data ) if not data : Log . debug ( "No data to be set" ) # Set watches on the pplan, execution_state, tmaster and scheduler_location. state_manager . get_pplan ( topologyName , on_topology_pplan ) state_manager . get_packing_plan ( topologyName , on_topology_packing_plan ) state_manager . get_execution_state ( topologyName , on_topology_execution_state ) state_manager . get_tmaster ( topologyName , on_topology_tmaster ) state_manager . get_scheduler_location ( topologyName , on_topology_scheduler_location )
12482	def find_in_sections ( var_name , app_name ) : sections = get_sections ( app_name ) if not sections : raise ValueError ( 'No sections found in {} rcfiles.' . format ( app_name ) ) for s in sections : try : var_value = get_rcfile_variable_value ( var_name , section_name = s , app_name = app_name ) except : pass else : return s , var_value raise KeyError ( 'No variable {} has been found in {} ' 'rcfiles.' . format ( var_name , app_name ) )
3742	def StielPolar ( Tc = None , Pc = None , omega = None , CASRN = '' , Method = None , AvailableMethods = False ) : def list_methods ( ) : methods = [ ] if Tc and Pc and omega : methods . append ( 'DEFINITION' ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'DEFINITION' : P = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tc * 0.6 ) if not P : factor = None else : Pr = P / Pc factor = log10 ( Pr ) + 1.70 * omega + 1.552 elif Method == 'NONE' : factor = None else : raise Exception ( 'Failure in in function' ) return factor
12965	def allOnlyFields ( self , fields , cascadeFetch = False ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultipleOnlyFields ( matchedKeys , fields , cascadeFetch = cascadeFetch ) return IRQueryableList ( [ ] , mdl = self . mdl )
9029	def _step ( self , row , position , passed ) : if row in passed or not self . _row_should_be_placed ( row , position ) : return self . _place_row ( row , position ) passed = [ row ] + passed # print("{}{} at\t{} {}".format(" " * len(passed), row, position, # passed)) for i , produced_mesh in enumerate ( row . produced_meshes ) : self . _expand_produced_mesh ( produced_mesh , i , position , passed ) for i , consumed_mesh in enumerate ( row . consumed_meshes ) : self . _expand_consumed_mesh ( consumed_mesh , i , position , passed )
7446	def _step5func ( self , samples , force , ipyclient ) : ## print header if self . _headers : print ( "\n Step 5: Consensus base calling " ) ## Get sample objects from list of strings samples = _get_samples ( self , samples ) ## Check if all/none in the right state if not self . _samples_precheck ( samples , 5 , force ) : raise IPyradError ( FIRST_RUN_4 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 5 for i in samples ] ) : print ( CONSENS_EXIST . format ( len ( samples ) ) ) return ## pass samples to rawedit assemble . consens_se . run ( self , samples , force , ipyclient )
2426	def set_doc_spdx_id ( self , doc , doc_spdx_id_line ) : if not self . doc_spdx_id_set : if doc_spdx_id_line == 'SPDXRef-DOCUMENT' : doc . spdx_id = doc_spdx_id_line self . doc_spdx_id_set = True return True else : raise SPDXValueError ( 'Document::SPDXID' ) else : raise CardinalityError ( 'Document::SPDXID' )
9772	def restart ( ctx , copy , file , u ) : # pylint:disable=redefined-builtin config = None update_code = None if file : config = rhea . read ( file ) # Check if we need to upload if u : ctx . invoke ( upload , sync = False ) update_code = True user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : if copy : response = PolyaxonClient ( ) . job . copy ( user , project_name , _job , config = config , update_code = update_code ) else : response = PolyaxonClient ( ) . job . restart ( user , project_name , _job , config = config , update_code = update_code ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not restart job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_job_details ( response )
9229	def fetch_closed_pull_requests ( self ) : pull_requests = [ ] verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project if verbose : print ( "Fetching closed pull requests..." ) page = 1 while page > 0 : if verbose > 2 : print ( "." , end = "" ) if self . options . release_branch : rc , data = gh . repos [ user ] [ repo ] . pulls . get ( page = page , per_page = PER_PAGE_NUMBER , state = 'closed' , base = self . options . release_branch ) else : rc , data = gh . repos [ user ] [ repo ] . pulls . get ( page = page , per_page = PER_PAGE_NUMBER , state = 'closed' , ) if rc == 200 : pull_requests . extend ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) if verbose > 2 : print ( "." ) if verbose > 1 : print ( "\tfetched {} closed pull requests." . format ( len ( pull_requests ) ) ) return pull_requests
6592	def poll ( self ) : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret = self . _collect_all_finished_pkgidx_result_pairs ( ) return ret
1950	def update_segment ( self , selector , base , size , perms ) : logger . info ( "Updating selector %s to 0x%02x (%s bytes) (%s)" , selector , base , size , perms ) if selector == 99 : self . set_fs ( base ) else : logger . error ( "No way to write segment: %d" , selector )
7240	def window_cover ( self , window_shape , pad = True ) : size_y , size_x = window_shape [ 0 ] , window_shape [ 1 ] _ndepth , _nheight , _nwidth = self . shape nheight , _m = divmod ( _nheight , size_y ) nwidth , _n = divmod ( _nwidth , size_x ) img = self if pad is True : new_height , new_width = _nheight , _nwidth if _m != 0 : new_height = ( nheight + 1 ) * size_y if _n != 0 : new_width = ( nwidth + 1 ) * size_x if ( new_height , new_width ) != ( _nheight , _nwidth ) : bounds = box ( 0 , 0 , new_width , new_height ) geom = ops . transform ( self . __geo_transform__ . fwd , bounds ) img = self [ geom ] row_lims = range ( 0 , img . shape [ 1 ] , size_y ) col_lims = range ( 0 , img . shape [ 2 ] , size_x ) for maxy , maxx in product ( row_lims , col_lims ) : reg = img [ : , maxy : ( maxy + size_y ) , maxx : ( maxx + size_x ) ] if pad is False : if reg . shape [ 1 : ] == window_shape : yield reg else : yield reg
9300	def list ( self , filters , cursor , count ) : assert isinstance ( filters , dict ) , "expected filters type 'dict'" assert isinstance ( cursor , dict ) , "expected cursor type 'dict'" # start with our base query query = self . get_query ( ) assert isinstance ( query , peewee . Query ) # XXX: convert and apply user specified filters #filters = {field.name: cursor[field.name] for field in fields} #query.where( paginator = self . get_paginator ( ) assert isinstance ( paginator , Pagination ) # always include an extra row for next cursor position count += 1 # apply pagination to query pquery = paginator . filter_query ( query , cursor , count ) items = [ item for item in pquery ] # determine next cursor position next_item = items . pop ( 1 ) next_cursor = next_item . to_cursor_ref ( ) ''' # is this field allowed for sort? if field not in self.sort_fields: raise ValueError("Cannot sort on field '{}'".format(field)) ''' return items , next_cursor
5994	def set_colorbar ( cb_ticksize , cb_fraction , cb_pad , cb_tick_values , cb_tick_labels ) : if cb_tick_values is None and cb_tick_labels is None : cb = plt . colorbar ( fraction = cb_fraction , pad = cb_pad ) elif cb_tick_values is not None and cb_tick_labels is not None : cb = plt . colorbar ( fraction = cb_fraction , pad = cb_pad , ticks = cb_tick_values ) cb . ax . set_yticklabels ( cb_tick_labels ) else : raise exc . PlottingException ( 'Only 1 entry of cb_tick_values or cb_tick_labels was input. You must either supply' 'both the values and labels, or neither.' ) cb . ax . tick_params ( labelsize = cb_ticksize )
2526	def get_annotation_type ( self , r_term ) : for _ , _ , typ in self . graph . triples ( ( r_term , self . spdx_namespace [ 'annotationType' ] , None ) ) : if typ is not None : return typ else : self . error = True msg = 'Annotation must have exactly one annotation type.' self . logger . log ( msg ) return
3843	def from_timestamp ( microsecond_timestamp ) : # Create datetime without losing precision from floating point (yes, this # is actually needed): return datetime . datetime . fromtimestamp ( microsecond_timestamp // 1000000 , datetime . timezone . utc ) . replace ( microsecond = ( microsecond_timestamp % 1000000 ) )
11179	def authorize_url ( self ) : auth_url = OAUTH_ROOT + '/authorize' params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , } return "{}?{}" . format ( auth_url , urlencode ( params ) )
4627	def encrypt ( self , wif ) : if not self . unlocked ( ) : raise WalletLocked return format ( bip38 . encrypt ( str ( wif ) , self . masterkey ) , "encwif" )
13867	def truncate ( when , unit , week_start = mon ) : if is_datetime ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( round ( when . microsecond / 1000.0 ) ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) elif unit == hour : return when . replace ( minute = 0 , second = 0 , microsecond = 0 ) elif unit == day : return when . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == week : weekday = prevweekday ( when , week_start ) return when . replace ( year = weekday . year , month = weekday . month , day = weekday . day , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == month : return when . replace ( day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == year : return when . replace ( month = 1 , day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif is_date ( when ) : if unit == week : return prevweekday ( when , week_start ) elif unit == month : return when . replace ( day = 1 ) elif unit == year : return when . replace ( month = 1 , day = 1 ) elif is_time ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( when . microsecond / 1000.0 ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) return when
277	def plotting_context ( context = 'notebook' , font_scale = 1.5 , rc = None ) : if rc is None : rc = { } rc_default = { 'lines.linewidth' : 1.5 } # Add defaults if they do not exist for name , val in rc_default . items ( ) : rc . setdefault ( name , val ) return sns . plotting_context ( context = context , font_scale = font_scale , rc = rc )
13109	def r_annotations ( self ) : target = request . args . get ( "target" , None ) wildcard = request . args . get ( "wildcard" , "." , type = str ) include = request . args . get ( "include" ) exclude = request . args . get ( "exclude" ) limit = request . args . get ( "limit" , None , type = int ) start = request . args . get ( "start" , 1 , type = int ) expand = request . args . get ( "expand" , False , type = bool ) if target : try : urn = MyCapytain . common . reference . URN ( target ) except ValueError : return "invalid urn" , 400 count , annotations = self . __queryinterface__ . getAnnotations ( urn , wildcard = wildcard , include = include , exclude = exclude , limit = limit , start = start , expand = expand ) else : # Note that this implementation is not done for too much annotations # because we do not implement pagination here count , annotations = self . __queryinterface__ . getAnnotations ( None , limit = limit , start = start , expand = expand ) mapped = [ ] response = { "@context" : type ( self ) . JSONLD_CONTEXT , "id" : url_for ( ".r_annotations" , start = start , limit = limit ) , "type" : "AnnotationCollection" , "startIndex" : start , "items" : [ ] , "total" : count } for a in annotations : mapped . append ( { "id" : url_for ( ".r_annotation" , sha = a . sha ) , "body" : url_for ( ".r_annotation_body" , sha = a . sha ) , "type" : "Annotation" , "target" : a . target . to_json ( ) , "dc:type" : a . type_uri , "owl:sameAs" : [ a . uri ] , "nemo:slug" : a . slug } ) response [ "items" ] = mapped response = jsonify ( response ) return response
2065	def inverse_transform ( self , X_in ) : X = X_in . copy ( deep = True ) # first check the type X = util . convert_input ( X ) if self . _dim is None : raise ValueError ( 'Must train encoder before it can be used to inverse_transform data' ) # then make sure that it is the right size if X . shape [ 1 ] != self . _dim : if self . drop_invariant : raise ValueError ( "Unexpected input dimension %d, the attribute drop_invariant should " "set as False when transform data" % ( X . shape [ 1 ] , ) ) else : raise ValueError ( 'Unexpected input dimension %d, expected %d' % ( X . shape [ 1 ] , self . _dim , ) ) if not self . cols : return X if self . return_df else X . values if self . handle_unknown == 'value' : for col in self . cols : if any ( X [ col ] == - 1 ) : warnings . warn ( "inverse_transform is not supported because transform impute " "the unknown category -1 when encode %s" % ( col , ) ) if self . handle_unknown == 'return_nan' and self . handle_missing == 'return_nan' : for col in self . cols : if X [ col ] . isnull ( ) . any ( ) : warnings . warn ( "inverse_transform is not supported because transform impute " "the unknown category nan when encode %s" % ( col , ) ) for switch in self . mapping : column_mapping = switch . get ( 'mapping' ) inverse = pd . Series ( data = column_mapping . index , index = column_mapping . get_values ( ) ) X [ switch . get ( 'col' ) ] = X [ switch . get ( 'col' ) ] . map ( inverse ) . astype ( switch . get ( 'data_type' ) ) return X if self . return_df else X . values
4204	def rlevinson ( a , efinal ) : a = numpy . array ( a ) realdata = numpy . isrealobj ( a ) assert a [ 0 ] == 1 , 'First coefficient of the prediction polynomial must be unity' p = len ( a ) if p < 2 : raise ValueError ( 'Polynomial should have at least two coefficients' ) if realdata == True : U = numpy . zeros ( ( p , p ) ) # This matrix will have the prediction # polynomials of orders 1:p else : U = numpy . zeros ( ( p , p ) , dtype = complex ) U [ : , p - 1 ] = numpy . conj ( a [ - 1 : : - 1 ] ) # Prediction coefficients of order p p = p - 1 e = numpy . zeros ( p ) # First we find the prediction coefficients of smaller orders and form the # Matrix U # Initialize the step down e [ - 1 ] = efinal # Prediction error of order p # Step down for k in range ( p - 1 , 0 , - 1 ) : [ a , e [ k - 1 ] ] = levdown ( a , e [ k ] ) U [ : , k ] = numpy . concatenate ( ( numpy . conj ( a [ - 1 : : - 1 ] . transpose ( ) ) , [ 0 ] * ( p - k ) ) ) e0 = e [ 0 ] / ( 1. - abs ( a [ 1 ] ** 2 ) ) #% Because a[1]=1 (true polynomial) U [ 0 , 0 ] = 1 #% Prediction coefficient of zeroth order kr = numpy . conj ( U [ 0 , 1 : ] ) #% The reflection coefficients kr = kr . transpose ( ) #% To make it into a column vector # % Once we have the matrix U and the prediction error at various orders, we can # % use this information to find the autocorrelation coefficients. R = numpy . zeros ( 1 , dtype = complex ) #% Initialize recursion k = 1 R0 = e0 # To take care of the zero indexing problem R [ 0 ] = - numpy . conj ( U [ 0 , 1 ] ) * R0 # R[1]=-a1[1]*R[0] # Actual recursion for k in range ( 1 , p ) : r = - sum ( numpy . conj ( U [ k - 1 : : - 1 , k ] ) * R [ - 1 : : - 1 ] ) - kr [ k ] * e [ k - 1 ] R = numpy . insert ( R , len ( R ) , r ) # Include R(0) and make it a column vector. Note the dot transpose #R = [R0 R].'; R = numpy . insert ( R , 0 , e0 ) return R , U , kr , e
12655	def remove_dcm2nii_underprocessed ( filepaths ) : cln_flist = [ ] # sort them by size len_sorted = sorted ( filepaths , key = len ) for idx , fpath in enumerate ( len_sorted ) : remove = False # get the basename and the rest of the files fname = op . basename ( fpath ) rest = len_sorted [ idx + 1 : ] # check if the basename is in the basename of the rest of the files for rest_fpath in rest : rest_file = op . basename ( rest_fpath ) if rest_file . endswith ( fname ) : remove = True break if not remove : cln_flist . append ( fpath ) return cln_flist
5374	def _prefix_exists_in_gcs ( gcs_prefix , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , prefix = gcs_prefix [ len ( 'gs://' ) : ] . split ( '/' , 1 ) # documentation in # https://cloud.google.com/storage/docs/json_api/v1/objects/list request = gcs_service . objects ( ) . list ( bucket = bucket_name , prefix = prefix , maxResults = 1 ) response = request . execute ( ) return response . get ( 'items' , None )
1884	def concretize ( self , symbolic , policy , maxcount = 7 ) : assert self . constraints == self . platform . constraints symbolic = self . migrate_expression ( symbolic ) vals = [ ] if policy == 'MINMAX' : vals = self . _solver . minmax ( self . _constraints , symbolic ) elif policy == 'MAX' : vals = self . _solver . max ( self . _constraints , symbolic ) elif policy == 'MIN' : vals = self . _solver . min ( self . _constraints , symbolic ) elif policy == 'SAMPLED' : m , M = self . _solver . minmax ( self . _constraints , symbolic ) vals += [ m , M ] if M - m > 3 : if self . _solver . can_be_true ( self . _constraints , symbolic == ( m + M ) // 2 ) : vals . append ( ( m + M ) // 2 ) if M - m > 100 : for i in ( 0 , 1 , 2 , 5 , 32 , 64 , 128 , 320 ) : if self . _solver . can_be_true ( self . _constraints , symbolic == m + i ) : vals . append ( m + i ) if maxcount <= len ( vals ) : break if M - m > 1000 and maxcount > len ( vals ) : vals += self . _solver . get_all_values ( self . _constraints , symbolic , maxcnt = maxcount - len ( vals ) , silent = True ) elif policy == 'ONE' : vals = [ self . _solver . get_value ( self . _constraints , symbolic ) ] else : assert policy == 'ALL' vals = solver . get_all_values ( self . _constraints , symbolic , maxcnt = maxcount , silent = True ) return tuple ( set ( vals ) )
4259	def read_markdown ( filename ) : global MD # Use utf-8-sig codec to remove BOM if it is present. This is only possible # this way prior to feeding the text to the markdown parser (which would # also default to pure utf-8) with open ( filename , 'r' , encoding = 'utf-8-sig' ) as f : text = f . read ( ) if MD is None : MD = Markdown ( extensions = [ 'markdown.extensions.meta' , 'markdown.extensions.tables' ] , output_format = 'html5' ) else : MD . reset ( ) # When https://github.com/Python-Markdown/markdown/pull/672 # will be available, this can be removed. MD . Meta = { } # Mark HTML with Markup to prevent jinja2 autoescaping output = { 'description' : Markup ( MD . convert ( text ) ) } try : meta = MD . Meta . copy ( ) except AttributeError : pass else : output [ 'meta' ] = meta try : output [ 'title' ] = MD . Meta [ 'title' ] [ 0 ] except KeyError : pass return output
12501	def _smooth_data_array ( arr , affine , fwhm , copy = True ) : if arr . dtype . kind == 'i' : if arr . dtype == np . int64 : arr = arr . astype ( np . float64 ) else : arr = arr . astype ( np . float32 ) if copy : arr = arr . copy ( ) # Zeroe possible NaNs and Inf in the image. arr [ np . logical_not ( np . isfinite ( arr ) ) ] = 0 try : # Keep the 3D part of the affine. affine = affine [ : 3 , : 3 ] # Convert from FWHM in mm to a sigma. fwhm_sigma_ratio = np . sqrt ( 8 * np . log ( 2 ) ) vox_size = np . sqrt ( np . sum ( affine ** 2 , axis = 0 ) ) sigma = fwhm / ( fwhm_sigma_ratio * vox_size ) for n , s in enumerate ( sigma ) : ndimage . gaussian_filter1d ( arr , s , output = arr , axis = n ) except : raise ValueError ( 'Error smoothing the array.' ) else : return arr
2490	def create_file_node ( self , doc_file ) : file_node = URIRef ( 'http://www.spdx.org/files#{id}' . format ( id = str ( doc_file . spdx_id ) ) ) type_triple = ( file_node , RDF . type , self . spdx_namespace . File ) self . graph . add ( type_triple ) name_triple = ( file_node , self . spdx_namespace . fileName , Literal ( doc_file . name ) ) self . graph . add ( name_triple ) if doc_file . has_optional_field ( 'comment' ) : comment_triple = ( file_node , RDFS . comment , Literal ( doc_file . comment ) ) self . graph . add ( comment_triple ) if doc_file . has_optional_field ( 'type' ) : ftype = self . spdx_namespace [ self . FILE_TYPES [ doc_file . type ] ] ftype_triple = ( file_node , self . spdx_namespace . fileType , ftype ) self . graph . add ( ftype_triple ) self . graph . add ( ( file_node , self . spdx_namespace . checksum , self . create_checksum_node ( doc_file . chk_sum ) ) ) conc_lic_node = self . license_or_special ( doc_file . conc_lics ) conc_lic_triple = ( file_node , self . spdx_namespace . licenseConcluded , conc_lic_node ) self . graph . add ( conc_lic_triple ) license_info_nodes = map ( self . license_or_special , doc_file . licenses_in_file ) for lic in license_info_nodes : triple = ( file_node , self . spdx_namespace . licenseInfoInFile , lic ) self . graph . add ( triple ) if doc_file . has_optional_field ( 'license_comment' ) : comment_triple = ( file_node , self . spdx_namespace . licenseComments , Literal ( doc_file . license_comment ) ) self . graph . add ( comment_triple ) cr_text_node = self . to_special_value ( doc_file . copyright ) cr_text_triple = ( file_node , self . spdx_namespace . copyrightText , cr_text_node ) self . graph . add ( cr_text_triple ) if doc_file . has_optional_field ( 'notice' ) : notice_triple = ( file_node , self . spdx_namespace . noticeText , doc_file . notice ) self . graph . add ( notice_triple ) contrib_nodes = map ( lambda c : Literal ( c ) , doc_file . contributors ) contrib_triples = [ ( file_node , self . spdx_namespace . fileContributor , node ) for node in contrib_nodes ] for triple in contrib_triples : self . graph . add ( triple ) return file_node
7634	def __load_jams_schema ( ) : schema_file = os . path . join ( SCHEMA_DIR , 'jams_schema.json' ) jams_schema = None with open ( resource_filename ( __name__ , schema_file ) , mode = 'r' ) as fdesc : jams_schema = json . load ( fdesc ) if jams_schema is None : raise JamsError ( 'Unable to load JAMS schema' ) return jams_schema
3042	def get_access_token ( self , http = None ) : if not self . access_token or self . access_token_expired : if not http : http = transport . get_http_object ( ) self . refresh ( http ) return AccessTokenInfo ( access_token = self . access_token , expires_in = self . _expires_in ( ) )
1812	def SETNB ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF == False , 1 , 0 ) )
13057	def get_locale ( self ) : best_match = request . accept_languages . best_match ( [ 'de' , 'fr' , 'en' , 'la' ] ) if best_match is None : if len ( request . accept_languages ) > 0 : best_match = request . accept_languages [ 0 ] [ 0 ] [ : 2 ] else : return self . __default_lang__ lang = self . __default_lang__ if best_match == "de" : lang = "ger" elif best_match == "fr" : lang = "fre" elif best_match == "en" : lang = "eng" elif best_match == "la" : lang = "lat" return lang
9765	def check ( file , # pylint:disable=redefined-builtin version , definition ) : file = file or 'polyaxonfile.yaml' specification = check_polyaxonfile ( file ) . specification if version : Printer . decorate_format_value ( 'The version is: {}' , specification . version , 'yellow' ) if definition : job_condition = ( specification . is_job or specification . is_build or specification . is_notebook or specification . is_tensorboard ) if specification . is_experiment : Printer . decorate_format_value ( 'This polyaxon specification has {}' , 'One experiment' , 'yellow' ) if job_condition : Printer . decorate_format_value ( 'This {} polyaxon specification is valid' , specification . kind , 'yellow' ) if specification . is_group : experiments_def = specification . experiments_def click . echo ( 'This polyaxon specification has experiment group with the following definition:' ) get_group_experiments_info ( * * experiments_def ) return specification
11832	def mate ( self , other ) : c = random . randrange ( len ( self . genes ) ) return self . __class__ ( self . genes [ : c ] + other . genes [ c : ] )
12708	def body_to_world ( self , position ) : return np . array ( self . ode_body . getRelPointPos ( tuple ( position ) ) )
657	def averageOnTime ( vectors , numSamples = None ) : # Special case given a 1 dimensional vector: it represents a single column if vectors . ndim == 1 : vectors . shape = ( - 1 , 1 ) numTimeSteps = len ( vectors ) numElements = len ( vectors [ 0 ] ) # How many samples will we look at? if numSamples is None : numSamples = numElements countOn = range ( numElements ) else : countOn = numpy . random . randint ( 0 , numElements , numSamples ) # Compute the on-times and accumulate the frequency counts of each on-time # encountered sumOfLengths = 0.0 onTimeFreqCounts = None n = 0 for i in countOn : ( onTime , segments , durations ) = _listOfOnTimesInVec ( vectors [ : , i ] ) if onTime != 0.0 : sumOfLengths += onTime n += segments onTimeFreqCounts = _accumulateFrequencyCounts ( durations , onTimeFreqCounts ) # Return the average on time of each element that was on. if n > 0 : return ( sumOfLengths / n , onTimeFreqCounts ) else : return ( 0.0 , onTimeFreqCounts )
3227	def rewrite_kwargs ( conn_type , kwargs , module_name = None ) : if conn_type != 'cloud' and module_name != 'compute' : if 'project' in kwargs : kwargs [ 'name' ] = 'projects/%s' % kwargs . pop ( 'project' ) if conn_type == 'cloud' and module_name == 'storage' : if 'project' in kwargs : del kwargs [ 'project' ] return kwargs
7033	def get_new_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) # url for getting an API key url = '%s/api/key' % lcc_server # get the API key resp = urlopen ( url ) if resp . code == 200 : respdict = json . loads ( resp . read ( ) ) else : LOGERROR ( 'could not fetch the API key from LCC-Server at: %s' % lcc_server ) LOGERROR ( 'the HTTP status code was: %s' % resp . status_code ) return None # # now that we have an API key dict, get the API key out of it and write it # to the APIKEYFILE # apikey = respdict [ 'result' ] [ 'apikey' ] expires = respdict [ 'result' ] [ 'expires' ] # write this to the apikey file if not os . path . exists ( os . path . dirname ( APIKEYFILE ) ) : os . makedirs ( os . path . dirname ( APIKEYFILE ) ) with open ( APIKEYFILE , 'w' ) as outfd : outfd . write ( '%s %s\n' % ( apikey , expires ) ) # chmod it to the correct value os . chmod ( APIKEYFILE , 0o100600 ) LOGINFO ( 'key fetched successfully from: %s. expires on: %s' % ( lcc_server , expires ) ) LOGINFO ( 'written to: %s' % APIKEYFILE ) return apikey , expires
4966	def clean ( self ) : cleaned_data = super ( ManageLearnersForm , self ) . clean ( ) # Here we take values from `data` (and not `cleaned_data`) as we need raw values - field clean methods # might "invalidate" the value and set it to None, while all we care here is if it was provided at all or not email_or_username = self . data . get ( self . Fields . EMAIL_OR_USERNAME , None ) bulk_upload_csv = self . files . get ( self . Fields . BULK_UPLOAD , None ) if not email_or_username and not bulk_upload_csv : raise ValidationError ( ValidationMessages . NO_FIELDS_SPECIFIED ) if email_or_username and bulk_upload_csv : raise ValidationError ( ValidationMessages . BOTH_FIELDS_SPECIFIED ) if email_or_username : mode = self . Modes . MODE_SINGULAR else : mode = self . Modes . MODE_BULK cleaned_data [ self . Fields . MODE ] = mode cleaned_data [ self . Fields . NOTIFY ] = self . clean_notify ( ) self . _validate_course ( ) self . _validate_program ( ) if self . data . get ( self . Fields . PROGRAM , None ) and self . data . get ( self . Fields . COURSE , None ) : raise ValidationError ( ValidationMessages . COURSE_AND_PROGRAM_ERROR ) return cleaned_data
12347	def field_metadata ( self , well_row = 0 , well_column = 0 , field_row = 0 , field_column = 0 ) : def condition ( path ) : attrs = attributes ( path ) return ( attrs . u == well_column and attrs . v == well_row and attrs . x == field_column and attrs . y == field_row ) field = [ f for f in self . fields if condition ( f ) ] if field : field = field [ 0 ] filename = _pattern ( field , 'metadata' , _image , extension = '*.ome.xml' ) filename = glob ( filename ) [ 0 ] # resolve, assume found return objectify . parse ( filename ) . getroot ( )
7348	async def get_access_token ( consumer_key , consumer_secret , oauth_token , oauth_token_secret , oauth_verifier , * * kwargs ) : client = BasePeonyClient ( consumer_key = consumer_key , consumer_secret = consumer_secret , access_token = oauth_token , access_token_secret = oauth_token_secret , api_version = "" , suffix = "" ) response = await client . api . oauth . access_token . get ( _suffix = "" , oauth_verifier = oauth_verifier ) return parse_token ( response )
8454	def _cookiecutter_configs_have_changed ( template , old_version , new_version ) : temple . check . is_git_ssh_path ( template ) repo_path = temple . utils . get_repo_path ( template ) github_client = temple . utils . GithubClient ( ) api = '/repos/{}/contents/cookiecutter.json' . format ( repo_path ) old_config_resp = github_client . get ( api , params = { 'ref' : old_version } ) old_config_resp . raise_for_status ( ) new_config_resp = github_client . get ( api , params = { 'ref' : new_version } ) new_config_resp . raise_for_status ( ) return old_config_resp . json ( ) [ 'content' ] != new_config_resp . json ( ) [ 'content' ]
11605	def convert_ranges ( cls , ranges , length ) : result = [ ] for start , end in ranges : if end is None : result . append ( ( start , length - 1 ) ) elif start is None : s = length - end result . append ( ( 0 if s < 0 else s , length - 1 ) ) else : result . append ( ( start , end if end < length else length - 1 ) ) return result
13743	def get_schema ( self ) : if not self . schema : raise NotImplementedError ( 'You must provide a schema value or override the get_schema method' ) return self . conn . create_schema ( * * self . schema )
8546	def delete_datacenter ( self , datacenter_id ) : response = self . _perform_request ( url = '/datacenters/%s' % ( datacenter_id ) , method = 'DELETE' ) return response
8215	def show_variables_window ( self ) : if self . var_window is None and self . bot . _vars : self . var_window = VarWindow ( self , self . bot , '%s variables' % ( self . title or 'Shoebot' ) ) self . var_window . window . connect ( "destroy" , self . var_window_closed )
3592	def encryptPassword ( self , login , passwd ) : # structure of the binary key: # # *-------------------------------------------------------* # | modulus_length | modulus | exponent_length | exponent | # *-------------------------------------------------------* # # modulus_length and exponent_length are uint32 binaryKey = b64decode ( config . GOOGLE_PUBKEY ) # modulus i = utils . readInt ( binaryKey , 0 ) modulus = utils . toBigInt ( binaryKey [ 4 : ] [ 0 : i ] ) # exponent j = utils . readInt ( binaryKey , i + 4 ) exponent = utils . toBigInt ( binaryKey [ i + 8 : ] [ 0 : j ] ) # calculate SHA1 of the pub key digest = hashes . Hash ( hashes . SHA1 ( ) , backend = default_backend ( ) ) digest . update ( binaryKey ) h = b'\x00' + digest . finalize ( ) [ 0 : 4 ] # generate a public key der_data = encode_dss_signature ( modulus , exponent ) publicKey = load_der_public_key ( der_data , backend = default_backend ( ) ) # encrypt email and password using pubkey to_be_encrypted = login . encode ( ) + b'\x00' + passwd . encode ( ) ciphertext = publicKey . encrypt ( to_be_encrypted , padding . OAEP ( mgf = padding . MGF1 ( algorithm = hashes . SHA1 ( ) ) , algorithm = hashes . SHA1 ( ) , label = None ) ) return urlsafe_b64encode ( h + ciphertext )
2519	def p_file_comments_on_lics ( self , f_term , predicate ) : try : for _ , _ , comment in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_license_comment ( self . doc , six . text_type ( comment ) ) except CardinalityError : self . more_than_one_error ( 'file comments on license' )
8172	def limit ( self , max = 30 ) : if abs ( self . vx ) > max : self . vx = self . vx / abs ( self . vx ) * max if abs ( self . vy ) > max : self . vy = self . vy / abs ( self . vy ) * max if abs ( self . vz ) > max : self . vz = self . vz / abs ( self . vz ) * max
12553	def copy_mhd_and_raw ( src , dst ) : # check if src exists if not op . exists ( src ) : raise IOError ( 'Could not find file {}.' . format ( src ) ) # check its extension ext = get_extension ( src ) if ext != '.mhd' : msg = 'The src file path must be a .mhd file. Given: {}.' . format ( src ) raise ValueError ( msg ) # get the raw file for this src mhd file meta_src = _read_meta_header ( src ) # get the source raw file src_raw = meta_src [ 'ElementDataFile' ] if not op . isabs ( src_raw ) : src_raw = op . join ( op . dirname ( src ) , src_raw ) # check if dst is dir if op . isdir ( dst ) : # copy the mhd and raw file to its destiny shutil . copyfile ( src , dst ) shutil . copyfile ( src_raw , dst ) return dst # build raw file dst file name dst_raw = op . join ( op . dirname ( dst ) , remove_ext ( op . basename ( dst ) ) ) + '.raw' # add extension to the dst path if get_extension ( dst ) != '.mhd' : dst += '.mhd' # copy the mhd and raw file to its destiny log . debug ( 'cp: {} -> {}' . format ( src , dst ) ) log . debug ( 'cp: {} -> {}' . format ( src_raw , dst_raw ) ) shutil . copyfile ( src , dst ) shutil . copyfile ( src_raw , dst_raw ) # check if src file name is different than dst file name # if not the same file name, change the content of the ElementDataFile field if op . basename ( dst ) != op . basename ( src ) : log . debug ( 'modify {}: ElementDataFile: {} -> {}' . format ( dst , src_raw , op . basename ( dst_raw ) ) ) meta_dst = _read_meta_header ( dst ) meta_dst [ 'ElementDataFile' ] = op . basename ( dst_raw ) write_meta_header ( dst , meta_dst ) return dst
8491	def _parse_hosts ( self , hosts ) : # Default host if hosts is None : return # If it's a string, we allow comma separated strings if isinstance ( hosts , six . string_types ) : # Split comma-separated list hosts = [ host . strip ( ) for host in hosts . split ( ',' ) ] # Split host and port hosts = [ host . split ( ':' ) for host in hosts ] # Coerce ports to int hosts = [ ( host [ 0 ] , int ( host [ 1 ] ) ) for host in hosts ] # The python-etcd client explicitly checks for a tuple type return tuple ( hosts )
2074	def convert_input_vector ( y , index ) : if y is None : return None if isinstance ( y , pd . Series ) : return y elif isinstance ( y , np . ndarray ) : if len ( np . shape ( y ) ) == 1 : # vector return pd . Series ( y , name = 'target' , index = index ) elif len ( np . shape ( y ) ) == 2 and np . shape ( y ) [ 0 ] == 1 : # single row in a matrix return pd . Series ( y [ 0 , : ] , name = 'target' , index = index ) elif len ( np . shape ( y ) ) == 2 and np . shape ( y ) [ 1 ] == 1 : # single column in a matrix return pd . Series ( y [ : , 0 ] , name = 'target' , index = index ) else : raise ValueError ( 'Unexpected input shape: %s' % ( str ( np . shape ( y ) ) ) ) elif np . isscalar ( y ) : return pd . Series ( [ y ] , name = 'target' , index = index ) elif isinstance ( y , list ) : if len ( y ) == 0 or ( len ( y ) > 0 and not isinstance ( y [ 0 ] , list ) ) : # empty list or a vector return pd . Series ( y , name = 'target' , index = index ) elif len ( y ) > 0 and isinstance ( y [ 0 ] , list ) and len ( y [ 0 ] ) == 1 : # single row in a matrix flatten = lambda y : [ item for sublist in y for item in sublist ] return pd . Series ( flatten ( y ) , name = 'target' , index = index ) elif len ( y ) == 1 and isinstance ( y [ 0 ] , list ) : # single column in a matrix return pd . Series ( y [ 0 ] , name = 'target' , index = index ) else : raise ValueError ( 'Unexpected input shape' ) elif isinstance ( y , pd . DataFrame ) : if len ( list ( y ) ) == 0 : # empty DataFrame return pd . Series ( y , name = 'target' ) if len ( list ( y ) ) == 1 : # a single column return y . iloc [ : , 0 ] else : raise ValueError ( 'Unexpected input shape: %s' % ( str ( y . shape ) ) ) else : return pd . Series ( y , name = 'target' , index = index )
2554	def setdocument ( self , doc ) : # assume that a document is correct in the subtree if self . document != doc : self . document = doc for i in self . children : if not isinstance ( i , dom_tag ) : return i . setdocument ( doc )
6161	def scatter ( x , Ns , start ) : xI = np . real ( x [ start : : Ns ] ) xQ = np . imag ( x [ start : : Ns ] ) return xI , xQ
2446	def create_package ( self , doc , name ) : if not self . package_set : self . package_set = True doc . package = package . Package ( name = name ) return True else : raise CardinalityError ( 'Package::Name' )
12205	def url_builder ( self , endpoint , * , root = None , params = None , url_params = None ) : if root is None : root = self . ROOT scheme , netloc , path , _ , _ = urlsplit ( root ) return urlunsplit ( ( scheme , netloc , urljoin ( path , endpoint ) , urlencode ( url_params or { } ) , '' , ) ) . format ( * * params or { } )
7515	def enter_singles ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : ## grab all seqs between edges seq = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] ## snps was created using only the selected samples, and is edge masked. ## The mask is for counting snps quickly, but trimming is still needed here ## to make the snps line up with the seqs in the snp string. snp = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] ## remove rows with all Ns, seq has only selected samples nalln = np . all ( seq == "N" , axis = 1 ) ## make mask of removed rows and excluded samples. Use the inverse ## of this to save the coverage for samples nsidx = nalln + smask samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) locuscov [ idx ] += 1 ## select the remaining names in order seq = seq [ ~ nsidx , ] names = pnames [ ~ nsidx ] ## save string for printing, excluding names not in samples outstr = "\n" . join ( [ name + s . tostring ( ) for name , s in zip ( names , seq ) ] ) ## get snp string and add to store snpstring = [ "-" if snp [ i , 0 ] else "*" if snp [ i , 1 ] else " " for i in range ( len ( snp ) ) ] outstr += "\n" + snppad + "" . join ( snpstring ) + "|{}|" . format ( iloc + start ) #LOGGER.info("outstr %s", outstr) return outstr , samplecov , locuscov
13164	def format_value ( value ) : value_id = id ( value ) if value_id in recursion_breaker . processed : return u'<recursion>' recursion_breaker . processed . add ( value_id ) try : if isinstance ( value , six . binary_type ) : # suppose, all byte strings are in unicode # don't know if everybody in the world uses anything else? return u"'{0}'" . format ( value . decode ( 'utf-8' ) ) elif isinstance ( value , six . text_type ) : return u"u'{0}'" . format ( value ) elif isinstance ( value , ( list , tuple ) ) : # long lists or lists with multiline items # will be shown vertically values = list ( map ( format_value , value ) ) result = serialize_list ( u'[' , values , delimiter = u',' ) + u']' return force_unicode ( result ) elif isinstance ( value , dict ) : items = six . iteritems ( value ) # format each key/value pair as a text, # calling format_value recursively items = ( tuple ( map ( format_value , item ) ) for item in items ) items = list ( items ) # sort by keys for readability items . sort ( ) # for each item value items = [ serialize_text ( u'{0}: ' . format ( key ) , item_value ) for key , item_value in items ] # and serialize these pieces as a list, enclosing # them into a curve brackets result = serialize_list ( u'{' , items , delimiter = u',' ) + u'}' return force_unicode ( result ) return force_unicode ( repr ( value ) ) finally : recursion_breaker . processed . remove ( value_id )
1908	def forward_events_to ( self , sink , include_source = False ) : assert isinstance ( sink , Eventful ) , f'{sink.__class__.__name__} is not Eventful' self . _forwards [ sink ] = include_source
2329	def create_graph_from_data ( self , data ) : # Building setup w/ arguments. self . arguments [ '{SCORE}' ] = self . score self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{BETA}' ] = str ( self . beta ) self . arguments [ '{OPTIM}' ] = str ( self . optim ) . upper ( ) self . arguments [ '{ALPHA}' ] = str ( self . alpha ) results = self . _run_bnlearn ( data , verbose = self . verbose ) graph = nx . DiGraph ( ) graph . add_edges_from ( results ) return graph
7451	def get_quart_iter ( tups ) : if tups [ 0 ] . endswith ( ".gz" ) : ofunc = gzip . open else : ofunc = open ## create iterators ofile1 = ofunc ( tups [ 0 ] , 'r' ) fr1 = iter ( ofile1 ) quart1 = itertools . izip ( fr1 , fr1 , fr1 , fr1 ) if tups [ 1 ] : ofile2 = ofunc ( tups [ 1 ] , 'r' ) fr2 = iter ( ofile2 ) quart2 = itertools . izip ( fr2 , fr2 , fr2 , fr2 ) quarts = itertools . izip ( quart1 , quart2 ) else : ofile2 = 0 quarts = itertools . izip ( quart1 , iter ( int , 1 ) ) ## make a generator def feedme ( quarts ) : for quart in quarts : yield quart genquarts = feedme ( quarts ) ## return generator and handles return genquarts , ofile1 , ofile2
10172	def _get_oldest_event_timestamp ( self ) : # Retrieve the oldest event in order to start aggregation # from there query_events = Search ( using = self . client , index = self . event_index ) [ 0 : 1 ] . sort ( { 'timestamp' : { 'order' : 'asc' } } ) result = query_events . execute ( ) # There might not be any events yet if the first event have been # indexed but the indices have not been refreshed yet. if len ( result ) == 0 : return None return parser . parse ( result [ 0 ] [ 'timestamp' ] )
2830	def convert_upsample ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting upsample...' ) if params [ 'mode' ] != 'nearest' : raise AssertionError ( 'Cannot convert non-nearest upsampling' ) if names == 'short' : tf_name = 'UPSL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'height_scale' in params : scale = ( params [ 'height_scale' ] , params [ 'width_scale' ] ) elif len ( inputs ) == 2 : scale = layers [ inputs [ - 1 ] + '_np' ] [ - 2 : ] upsampling = keras . layers . UpSampling2D ( size = scale , name = tf_name ) layers [ scope_name ] = upsampling ( layers [ inputs [ 0 ] ] )
3972	def _composed_service_dict ( service_spec ) : compose_dict = service_spec . plain_dict ( ) _apply_env_overrides ( env_overrides_for_app_or_service ( service_spec . name ) , compose_dict ) compose_dict . setdefault ( 'volumes' , [ ] ) . append ( _get_cp_volume_mount ( service_spec . name ) ) compose_dict [ 'container_name' ] = "dusty_{}_1" . format ( service_spec . name ) return compose_dict
5472	def prepare_output ( self , row ) : date_fields = [ 'last-update' , 'create-time' , 'start-time' , 'end-time' ] int_fields = [ 'task-attempt' ] for col in date_fields : if col in row : row [ col ] = self . default_format_date ( row [ col ] ) for col in int_fields : if col in row and row [ col ] is not None : row [ col ] = int ( row [ col ] ) return row
5267	def snakecase ( string ) : string = re . sub ( r"[\-\.\s]" , '_' , str ( string ) ) if not string : return string return lowercase ( string [ 0 ] ) + re . sub ( r"[A-Z]" , lambda matched : '_' + lowercase ( matched . group ( 0 ) ) , string [ 1 : ] )
7366	def run_multiple_commands_redirect_stdout ( multiple_args_dict , print_commands = True , process_limit = - 1 , polling_freq = 0.5 , * * kwargs ) : assert len ( multiple_args_dict ) > 0 assert all ( len ( args ) > 0 for args in multiple_args_dict . values ( ) ) assert all ( hasattr ( f , 'name' ) for f in multiple_args_dict . keys ( ) ) if process_limit < 0 : logger . debug ( "Using %d processes" % cpu_count ( ) ) process_limit = cpu_count ( ) start_time = time . time ( ) processes = Queue ( maxsize = process_limit ) def add_to_queue ( process ) : process . start ( ) if print_commands : handler = logging . FileHandler ( process . redirect_stdout_file . name ) handler . setLevel ( logging . DEBUG ) logger . addHandler ( handler ) logger . debug ( " " . join ( process . args ) ) logger . removeHandler ( handler ) processes . put ( process ) for f , args in multiple_args_dict . items ( ) : p = AsyncProcess ( args , redirect_stdout_file = f , * * kwargs ) if not processes . full ( ) : add_to_queue ( p ) else : while processes . full ( ) : # Are there any done processes? to_remove = [ ] for possibly_done in processes . queue : if possibly_done . poll ( ) is not None : possibly_done . wait ( ) to_remove . append ( possibly_done ) # Remove them from the queue and stop checking if to_remove : for process_to_remove in to_remove : processes . queue . remove ( process_to_remove ) break # Check again in a second if there weren't time . sleep ( polling_freq ) add_to_queue ( p ) # Wait for all the rest of the processes while not processes . empty ( ) : processes . get ( ) . wait ( ) elapsed_time = time . time ( ) - start_time logger . info ( "Ran %d commands in %0.4f seconds" , len ( multiple_args_dict ) , elapsed_time )
12225	def convertGribToTiff ( listeFile , listParam , listLevel , liststep , grid , startDate , endDate , outFolder ) : dicoValues = { } for l in listeFile : grbs = pygrib . open ( l ) grbs . seek ( 0 ) index = 1 for j in range ( len ( listLevel ) , 0 , - 1 ) : for i in range ( len ( listParam ) - 1 , - 1 , - 1 ) : grb = grbs [ index ] p = grb . name . replace ( ' ' , '_' ) if grb . level != 0 : l = str ( grb . level ) + '_' + grb . typeOfLevel else : l = grb . typeOfLevel if p + '_' + l not in dicoValues . keys ( ) : dicoValues [ p + '_' + l ] = [ ] dicoValues [ p + '_' + l ] . append ( grb . values ) shape = grb . values . shape lat , lon = grb . latlons ( ) geoparam = ( lon . min ( ) , lat . max ( ) , grid , grid ) index += 1 nbJour = ( endDate - startDate ) . days + 1 #on joute des arrayNan si il manque des fichiers for s in range ( 0 , ( len ( liststep ) * nbJour - len ( listeFile ) ) ) : for k in dicoValues . keys ( ) : dicoValues [ k ] . append ( np . full ( shape , np . nan ) ) #On รฉcrit pour chacune des variables dans un fichier for i in range ( len ( dicoValues . keys ( ) ) - 1 , - 1 , - 1 ) : dictParam = dict ( ( k , dicoValues [ dicoValues . keys ( ) [ i ] ] [ k ] ) for k in range ( 0 , len ( dicoValues [ dicoValues . keys ( ) [ i ] ] ) ) ) sorted ( dictParam . items ( ) , key = lambda x : x [ 0 ] ) outputImg = outFolder + '/' + dicoValues . keys ( ) [ i ] + '_' + startDate . strftime ( '%Y%M%d' ) + '_' + endDate . strftime ( '%Y%M%d' ) + '.tif' writeTiffFromDicoArray ( dictParam , outputImg , shape , geoparam ) for f in listeFile : os . remove ( f )
6035	def map_function ( self , func , * arg_lists ) : return GridStack ( * [ func ( * args ) for args in zip ( self , * arg_lists ) ] )
5168	def __intermediate_htmode ( self , radio ) : protocol = radio . pop ( 'protocol' ) channel_width = radio . pop ( 'channel_width' ) # allow overriding htmode if 'htmode' in radio : return radio [ 'htmode' ] if protocol == '802.11n' : return 'HT{0}' . format ( channel_width ) elif protocol == '802.11ac' : return 'VHT{0}' . format ( channel_width ) # disables n return 'NONE'
6210	def print_children ( data_file , group = '/' ) : base = data_file . get_node ( group ) print ( 'Groups in:\n %s\n' % base ) for node in base . _f_walk_groups ( ) : if node is not base : print ( ' %s' % node ) print ( '\nLeaf-nodes in %s:' % group ) for node in base . _v_leaves . itervalues ( ) : info = node . shape if len ( info ) == 0 : info = node . read ( ) print ( '\t%s, %s' % ( node . name , info ) ) if len ( node . title ) > 0 : print ( '\t %s' % node . title )
5777	def _advapi32_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = 0 if rsa_oaep_padding : flags = Advapi32Const . CRYPT_OAEP out_len = new ( advapi32 , 'DWORD *' , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , null ( ) , out_len , 0 ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) write_to_buffer ( buffer , data ) pointer_set ( out_len , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , buffer , out_len , buffer_len ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) ) [ : : - 1 ]
12889	def handle_set ( self , item , value ) : doc = yield from self . call ( 'SET/{}' . format ( item ) , dict ( value = value ) ) if doc is None : return None return doc . status == 'FS_OK'
1531	def get_pplan ( self , topologyName , callback = None ) : if callback : self . pplan_watchers [ topologyName ] . append ( callback ) else : pplan_path = self . get_pplan_path ( topologyName ) with open ( pplan_path ) as f : data = f . read ( ) pplan = PhysicalPlan ( ) pplan . ParseFromString ( data ) return pplan
3280	def resolve_provider ( self , path ) : # Find DAV provider that matches the share share = None lower_path = path . lower ( ) for r in self . sorted_share_list : # @@: Case sensitivity should be an option of some sort here; # os.path.normpath might give the preferred case for a filename. if r == "/" : share = r break elif lower_path == r or lower_path . startswith ( r + "/" ) : share = r break if share is None : return None , None return share , self . provider_map . get ( share )
12571	def get ( self , key ) : node = self . get_node ( key ) if node is None : raise KeyError ( 'No object named %s in the file' % key ) if hasattr ( node , 'attrs' ) : if 'pandas_type' in node . attrs : return self . _read_group ( node ) return self . _read_array ( node )
8752	def is_isonet_vif ( vif ) : nicira_iface_id = vif . record . get ( 'other_config' ) . get ( 'nicira-iface-id' ) if nicira_iface_id : return True return False
2261	def dict_hist ( item_list , weight_list = None , ordered = False , labels = None ) : if labels is None : hist_ = defaultdict ( lambda : 0 ) else : hist_ = { k : 0 for k in labels } if weight_list is None : weight_list = it . repeat ( 1 ) # Accumulate frequency for item , weight in zip ( item_list , weight_list ) : hist_ [ item ] += weight if ordered : # Order by value getval = op . itemgetter ( 1 ) hist = OrderedDict ( [ ( key , value ) for ( key , value ) in sorted ( hist_ . items ( ) , key = getval ) ] ) else : # Cast to a normal dictionary hist = dict ( hist_ ) return hist
4914	def course_enrollments ( self , request , pk ) : enterprise_customer = self . get_object ( ) serializer = serializers . EnterpriseCustomerCourseEnrollmentsSerializer ( data = request . data , many = True , context = { 'enterprise_customer' : enterprise_customer , 'request_user' : request . user , } ) if serializer . is_valid ( ) : serializer . save ( ) return Response ( serializer . data , status = HTTP_200_OK ) return Response ( serializer . errors , status = HTTP_400_BAD_REQUEST )
368	def crop ( x , wrg , hrg , is_random = False , row_index = 0 , col_index = 1 ) : h , w = x . shape [ row_index ] , x . shape [ col_index ] if ( h < hrg ) or ( w < wrg ) : raise AssertionError ( "The size of cropping should smaller than or equal to the original image" ) if is_random : h_offset = int ( np . random . uniform ( 0 , h - hrg ) ) w_offset = int ( np . random . uniform ( 0 , w - wrg ) ) # tl.logging.info(h_offset, w_offset, x[h_offset: hrg+h_offset ,w_offset: wrg+w_offset].shape) return x [ h_offset : hrg + h_offset , w_offset : wrg + w_offset ] else : # central crop h_offset = int ( np . floor ( ( h - hrg ) / 2. ) ) w_offset = int ( np . floor ( ( w - wrg ) / 2. ) ) h_end = h_offset + hrg w_end = w_offset + wrg return x [ h_offset : h_end , w_offset : w_end ]
12801	def get_rooms ( self , sort = True ) : rooms = self . _connection . get ( "rooms" ) if sort : rooms . sort ( key = operator . itemgetter ( "name" ) ) return rooms
1826	def PUSH ( cpu , src ) : # http://stackoverflow.com/questions/11291151/how-push-imm-encodes size = src . size v = src . read ( ) if size != 64 and size != cpu . address_bit_size // 2 : v = Operators . SEXTEND ( v , size , cpu . address_bit_size ) size = cpu . address_bit_size cpu . push ( v , size )
12486	def get_possible_paths ( base_path , path_regex ) : if not path_regex : return [ ] if len ( path_regex ) < 1 : return [ ] if path_regex [ 0 ] == os . sep : path_regex = path_regex [ 1 : ] rest_files = '' if os . sep in path_regex : #split by os.sep node_names = path_regex . partition ( os . sep ) first_node = node_names [ 0 ] rest_nodes = node_names [ 2 ] folder_names = filter_list ( os . listdir ( base_path ) , first_node ) for nom in folder_names : new_base = op . join ( base_path , nom ) if op . isdir ( new_base ) : rest_files = get_possible_paths ( new_base , rest_nodes ) else : rest_files = filter_list ( os . listdir ( base_path ) , path_regex ) files = [ ] if rest_files : files = [ op . join ( base_path , f ) for f in rest_files ] return files
7170	def train ( self , debug = True , force = False , single_thread = False , timeout = 20 ) : if not self . must_train and not force : return self . padaos . compile ( ) self . train_thread = Thread ( target = self . _train , kwargs = dict ( debug = debug , single_thread = single_thread , timeout = timeout ) , daemon = True ) self . train_thread . start ( ) self . train_thread . join ( timeout ) self . must_train = False return not self . train_thread . is_alive ( )
7436	def _tuplecheck ( newvalue , dtype = str ) : if isinstance ( newvalue , list ) : newvalue = tuple ( newvalue ) if isinstance ( newvalue , str ) : newvalue = newvalue . rstrip ( ")" ) . strip ( "(" ) try : newvalue = tuple ( [ dtype ( i . strip ( ) ) for i in newvalue . split ( "," ) ] ) ## Type error is thrown by tuple if it's applied to a non-iterable. except TypeError : newvalue = tuple ( dtype ( newvalue ) ) ## If dtype fails to cast any element of newvalue except ValueError : LOGGER . info ( "Assembly.tuplecheck() failed to cast to {} - {}" . format ( dtype , newvalue ) ) raise except Exception as inst : LOGGER . info ( inst ) raise SystemExit ( "\nError: Param`{}` is not formatted correctly.\n({})\n" . format ( newvalue , inst ) ) return newvalue
1738	def unify_string_literals ( js_string ) : n = 0 res = '' limit = len ( js_string ) while n < limit : char = js_string [ n ] if char == '\\' : new , n = do_escape ( js_string , n ) res += new else : res += char n += 1 return res
5124	def show_type ( self , edge_type , * * kwargs ) : for v in self . g . nodes ( ) : e = ( v , v ) if self . g . is_edge ( e ) and self . g . ep ( e , 'edge_type' ) == edge_type : ei = self . g . edge_index [ e ] self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_highlight' ] ) self . g . set_vp ( v , 'vertex_color' , self . edge2queue [ ei ] . colors [ 'vertex_color' ] ) else : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_inactive' ] ) self . g . set_vp ( v , 'vertex_color' , [ 0 , 0 , 0 , 0.9 ] ) for e in self . g . edges ( ) : if self . g . ep ( e , 'edge_type' ) == edge_type : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_active' ] ) else : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_inactive' ] ) self . draw ( update_colors = False , * * kwargs ) self . _update_all_colors ( )
12243	def dixon_price ( theta ) : x , y = theta obj = ( x - 1 ) ** 2 + 2 * ( 2 * y ** 2 - x ) ** 2 grad = np . array ( [ 2 * x - 2 - 4 * ( 2 * y ** 2 - x ) , 16 * ( 2 * y ** 2 - x ) * y , ] ) return obj , grad
1745	def _set_perms ( self , perms ) : assert isinstance ( perms , str ) and len ( perms ) <= 3 and perms . strip ( ) in [ '' , 'r' , 'w' , 'x' , 'rw' , 'r x' , 'rx' , 'rwx' , 'wx' , ] self . _perms = perms
12753	def indices_for_body ( self , name , step = 3 ) : for j , body in enumerate ( self . bodies ) : if body . name == name : return list ( range ( j * step , ( j + 1 ) * step ) ) return [ ]
12988	def remote_jupyter_proxy_url ( port ) : base_url = os . environ [ 'EXTERNAL_URL' ] host = urllib . parse . urlparse ( base_url ) . netloc # If port is None we're asking for the URL origin # so return the public hostname. if port is None : return host service_url_path = os . environ [ 'JUPYTERHUB_SERVICE_PREFIX' ] proxy_url_path = 'proxy/%d' % port user_url = urllib . parse . urljoin ( base_url , service_url_path ) full_url = urllib . parse . urljoin ( user_url , proxy_url_path ) return full_url
13171	def iter ( self , name = None ) : for c in self . _children : if name is None or c . tagname == name : yield c for gc in c . find ( name ) : yield gc
7862	def handle_tls_connected_event ( self , event ) : if self . settings [ "tls_verify_peer" ] : valid = self . settings [ "tls_verify_callback" ] ( event . stream , event . peer_certificate ) if not valid : raise SSLError ( "Certificate verification failed" ) event . stream . tls_established = True with event . stream . lock : event . stream . _restart_stream ( )
6668	def populate_fabfile ( ) : stack = inspect . stack ( ) fab_frame = None for frame_obj , script_fn , line , _ , _ , _ in stack : if 'fabfile.py' in script_fn : fab_frame = frame_obj break if not fab_frame : return try : locals_ = fab_frame . f_locals for module_name , module in sub_modules . items ( ) : locals_ [ module_name ] = module for role_name , role_func in role_commands . items ( ) : assert role_name not in sub_modules , ( 'The role %s conflicts with a built-in submodule. ' 'Please choose a different name.' ) % ( role_name ) locals_ [ role_name ] = role_func locals_ [ 'common' ] = common # Put all debug commands into the global namespace. # for _debug_name in debug.debug.get_tasks(): # print('_debug_name:', _debug_name) locals_ [ 'shell' ] = shell #debug.debug.shell # Put all virtual satchels in the global namespace so Fabric can find them. for _module_alias in common . post_import_modules : exec ( "import %s" % _module_alias ) # pylint: disable=exec-used locals_ [ _module_alias ] = locals ( ) [ _module_alias ] finally : del stack
2553	def set_attribute ( self , key , value ) : if isinstance ( key , int ) : self . children [ key ] = value elif isinstance ( key , basestring ) : self . attributes [ key ] = value else : raise TypeError ( 'Only integer and string types are valid for assigning ' 'child tags and attributes, respectively.' )
10782	def _feature_guess ( im , rad , minmass = None , use_tp = False , trim_edge = False ) : if minmass is None : # we use 1% of the feature size mass as a cutoff; # it's easier to remove than to add minmass = rad ** 3 * 4 / 3. * np . pi * 0.01 # 0.03 is a magic number; works well if use_tp : diameter = np . ceil ( 2 * rad ) diameter += 1 - ( diameter % 2 ) df = peri . trackpy . locate ( im , int ( diameter ) , minmass = minmass ) npart = np . array ( df [ 'mass' ] ) . size guess = np . zeros ( [ npart , 3 ] ) guess [ : , 0 ] = df [ 'z' ] guess [ : , 1 ] = df [ 'y' ] guess [ : , 2 ] = df [ 'x' ] mass = df [ 'mass' ] else : guess , mass = initializers . local_max_featuring ( im , radius = rad , minmass = minmass , trim_edge = trim_edge ) npart = guess . shape [ 0 ] # I want to return these sorted by mass: inds = np . argsort ( mass ) [ : : - 1 ] # biggest mass first return guess [ inds ] . copy ( ) , npart
8756	def delete_tenant_quota ( context , tenant_id ) : tenant_quotas = context . session . query ( Quota ) tenant_quotas = tenant_quotas . filter_by ( tenant_id = tenant_id ) tenant_quotas . delete ( )
7195	def ndvi ( self , * * kwargs ) : data = self . _read ( self [ self . _ndvi_bands , ... ] ) . astype ( np . float32 ) return ( data [ 0 , : , : ] - data [ 1 , : , : ] ) / ( data [ 0 , : , : ] + data [ 1 , : , : ] )
1028	def b64encode ( s , altchars = None ) : # Strip off the trailing newline encoded = binascii . b2a_base64 ( s ) [ : - 1 ] if altchars is not None : return encoded . translate ( string . maketrans ( b'+/' , altchars [ : 2 ] ) ) return encoded
6501	def course_discovery_search ( search_term = None , size = 20 , from_ = 0 , field_dictionary = None ) : # We'll ignore the course-enrollemnt informaiton in field and filter # dictionary, and use our own logic upon enrollment dates for these use_search_fields = [ "org" ] ( search_fields , _ , exclude_dictionary ) = SearchFilterGenerator . generate_field_filters ( ) use_field_dictionary = { } use_field_dictionary . update ( { field : search_fields [ field ] for field in search_fields if field in use_search_fields } ) if field_dictionary : use_field_dictionary . update ( field_dictionary ) if not getattr ( settings , "SEARCH_SKIP_ENROLLMENT_START_DATE_FILTERING" , False ) : use_field_dictionary [ "enrollment_start" ] = DateRange ( None , datetime . utcnow ( ) ) searcher = SearchEngine . get_search_engine ( getattr ( settings , "COURSEWARE_INDEX_NAME" , "courseware_index" ) ) if not searcher : raise NoSearchEngineError ( "No search engine specified in settings.SEARCH_ENGINE" ) results = searcher . search ( query_string = search_term , doc_type = "course_info" , size = size , from_ = from_ , # only show when enrollment start IS provided and is before now field_dictionary = use_field_dictionary , # show if no enrollment end is provided and has not yet been reached filter_dictionary = { "enrollment_end" : DateRange ( datetime . utcnow ( ) , None ) } , exclude_dictionary = exclude_dictionary , facet_terms = course_discovery_facets ( ) , ) return results
8983	def get_instruction_id ( self , instruction_or_id ) : if isinstance ( instruction_or_id , tuple ) : return _InstructionId ( instruction_or_id ) return _InstructionId ( instruction_or_id . type , instruction_or_id . hex_color )
12002	def _remove_magic ( self , data ) : if not self . magic : return data magic_size = len ( self . magic ) magic = data [ : magic_size ] if magic != self . magic : raise Exception ( 'Invalid magic' ) data = data [ magic_size : ] return data
10098	def update_template_version ( self , name , subject , template_id , version_id , text = '' , html = None , timeout = None ) : if ( html ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } else : payload = { 'name' : name , 'subject' : subject , 'text' : text } return self . _api_request ( self . TEMPLATES_VERSION_ENDPOINT % ( template_id , version_id ) , self . HTTP_PUT , payload = payload , timeout = timeout )
10469	def launchAppByBundlePath ( bundlePath , arguments = None ) : if arguments is None : arguments = [ ] bundleUrl = NSURL . fileURLWithPath_ ( bundlePath ) workspace = AppKit . NSWorkspace . sharedWorkspace ( ) arguments_strings = list ( map ( lambda a : NSString . stringWithString_ ( str ( a ) ) , arguments ) ) arguments = NSDictionary . dictionaryWithDictionary_ ( { AppKit . NSWorkspaceLaunchConfigurationArguments : NSArray . arrayWithArray_ ( arguments_strings ) } ) return workspace . launchApplicationAtURL_options_configuration_error_ ( bundleUrl , AppKit . NSWorkspaceLaunchAllowingClassicStartup , arguments , None )
3783	def select_valid_methods_P ( self , T , P ) : # Same as select_valid_methods but with _P added to variables if self . forced_P : considered_methods = list ( self . user_methods_P ) else : considered_methods = list ( self . all_methods_P ) if self . user_methods_P : [ considered_methods . remove ( i ) for i in self . user_methods_P ] preferences = sorted ( [ self . ranked_methods_P . index ( i ) for i in considered_methods ] ) sorted_methods = [ self . ranked_methods_P [ i ] for i in preferences ] if self . user_methods_P : [ sorted_methods . insert ( 0 , i ) for i in reversed ( self . user_methods_P ) ] sorted_valid_methods_P = [ ] for method in sorted_methods : if self . test_method_validity_P ( T , P , method ) : sorted_valid_methods_P . append ( method ) return sorted_valid_methods_P
13352	def monitor ( self , sleep = 5 ) : manager = FileModificationObjectManager ( ) timestamps = { } filebodies = { } # register original timestamp and filebody to dict for file in self . f_repository : timestamps [ file ] = self . _get_mtime ( file ) filebodies [ file ] = open ( file ) . read ( ) while True : for file in self . f_repository : mtime = timestamps [ file ] fbody = filebodies [ file ] modified = self . _check_modify ( file , mtime , fbody ) # file not modify -> continue if not modified : continue # file modifies -> create the modification object new_mtime = self . _get_mtime ( file ) new_fbody = open ( file ) . read ( ) obj = FileModificationObject ( file , ( mtime , new_mtime ) , ( fbody , new_fbody ) ) # overwrite new timestamp and filebody timestamps [ file ] = new_mtime filebodies [ file ] = new_fbody # append file modification object to manager manager . add_object ( obj ) # return new modification object yield obj time . sleep ( sleep )
4646	def exists ( self ) : query = ( "SELECT name FROM sqlite_master " + "WHERE type='table' AND name=?" , ( self . __tablename__ , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False
4030	def load ( self ) : con = sqlite3 . connect ( self . tmp_cookie_file ) cur = con . cursor ( ) try : # chrome <=55 cur . execute ( 'SELECT host_key, path, secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) except sqlite3 . OperationalError : # chrome >=56 cur . execute ( 'SELECT host_key, path, is_secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) cj = http . cookiejar . CookieJar ( ) for item in cur . fetchall ( ) : host , path , secure , expires , name = item [ : 5 ] value = self . _decrypt ( item [ 5 ] , item [ 6 ] ) c = create_cookie ( host , path , secure , expires , name , value ) cj . set_cookie ( c ) con . close ( ) return cj
10683	def S ( self , T ) : result = self . Sref for Tmax in sorted ( [ float ( TT ) for TT in self . _Cp_records . keys ( ) ] ) : result += self . _Cp_records [ str ( Tmax ) ] . S ( T ) if T <= Tmax : return result + self . S_mag ( T ) # Extrapolate beyond the upper limit by using a constant heat capacity. Tmax = max ( [ float ( TT ) for TT in self . _Cp_records . keys ( ) ] ) result += self . Cp ( Tmax ) * math . log ( T / Tmax ) return result + self . S_mag ( T )
12119	def headerHTML ( self , fname = None ) : if fname is None : fname = self . fname . replace ( ".abf" , "_header.html" ) html = "<html><body><code>" html += "<h2>abfinfo() for %s.abf</h2>" % self . ID html += self . abfinfo ( ) . replace ( "<" , "&lt;" ) . replace ( ">" , "&gt;" ) . replace ( "\n" , "<br>" ) html += "<h2>Header for %s.abf</h2>" % self . ID html += pprint . pformat ( self . header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "WRITING HEADER TO:" ) print ( fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( )
2033	def MSTORE ( self , address , value ) : if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . _allocate ( address , 32 ) self . _store ( address , value , 32 )
11737	def route ( bp , * args , * * kwargs ) : kwargs [ 'strict_slashes' ] = kwargs . pop ( 'strict_slashes' , False ) body = _validate_schema ( kwargs . pop ( '_body' , None ) ) query = _validate_schema ( kwargs . pop ( '_query' , None ) ) output = _validate_schema ( kwargs . pop ( 'marshal_with' , None ) ) validate = kwargs . pop ( 'validate' , True ) def decorator ( f ) : @ bp . route ( * args , * * kwargs ) @ wraps ( f ) def wrapper ( * inner_args , * * inner_kwargs ) : """If a schema (_body and/or _query) was supplied to the route decorator, the deserialized :class`marshmallow.Schema` object is injected into the decorated function's kwargs.""" try : if query is not None : query . strict = validate url = furl ( request . url ) inner_kwargs [ '_query' ] = query . load ( data = url . args ) if body is not None : body . strict = validate json_data = request . get_json ( ) if json_data is None : # Set json_data to empty dict if body is empty, so it gets picked up by the validator json_data = { } inner_kwargs [ '_body' ] = body . load ( data = json_data ) except ValidationError as err : return jsonify ( err . messages ) , 422 if output : data = output . dump ( f ( * inner_args , * * inner_kwargs ) ) return jsonify ( data [ 0 ] ) return f ( * inner_args , * * inner_kwargs ) return f return decorator
1148	def _keep_alive ( x , memo ) : try : memo [ id ( memo ) ] . append ( x ) except KeyError : # aha, this is the first one :-) memo [ id ( memo ) ] = [ x ]
4654	def constructTx ( self ) : ops = list ( ) for op in self . ops : if isinstance ( op , ProposalBuilder ) : # This operation is a proposal an needs to be deal with # differently proposal = op . get_raw ( ) if proposal : ops . append ( proposal ) elif isinstance ( op , self . operation_class ) : ops . extend ( [ op ] ) else : # otherwise, we simply wrap ops into Operations ops . extend ( [ self . operation_class ( op ) ] ) # We now wrap everything into an actual transaction ops = self . add_required_fees ( ops , asset_id = self . fee_asset_id ) expiration = formatTimeFromNow ( self . expiration or self . blockchain . expiration or 30 # defaults to 30 seconds ) ref_block_num , ref_block_prefix = self . get_block_params ( ) self . tx = self . signed_transaction_class ( ref_block_num = ref_block_num , ref_block_prefix = ref_block_prefix , expiration = expiration , operations = ops , ) dict . update ( self , self . tx . json ( ) ) self . _unset_require_reconstruction ( )
8053	def listener ( self , sock , * args ) : conn , addr = sock . accept ( ) f = conn . makefile ( conn ) self . shell = ShoebotCmd ( self . bot , stdin = f , stdout = f , intro = INTRO ) print ( _ ( "Connected" ) ) GObject . io_add_watch ( conn , GObject . IO_IN , self . handler ) if self . shell . intro : self . shell . stdout . write ( str ( self . shell . intro ) + "\n" ) self . shell . stdout . flush ( ) return True
3920	def set_focus ( self , position ) : self . _focus_position = position self . _modified ( ) # If we set focus to anywhere but the last position, the user if # scrolling up: try : self . next_position ( position ) except IndexError : self . _is_scrolling = False else : self . _is_scrolling = True
3620	def unregister ( self , model ) : if not self . is_registered ( model ) : raise RegistrationError ( '{} is not registered with Algolia engine' . format ( model ) ) # Perform the unregistration. del self . __registered_models [ model ] # Disconnect from the signalling framework. post_save . disconnect ( self . __post_save_receiver , model ) pre_delete . disconnect ( self . __pre_delete_receiver , model ) logger . info ( 'UNREGISTER %s' , model )
7644	def can_convert ( annotation , target_namespace ) : # If we're already in the target namespace, do nothing if annotation . namespace == target_namespace : return True if target_namespace in __CONVERSION__ : # Look for a way to map this namespace to the target for source in __CONVERSION__ [ target_namespace ] : if annotation . search ( namespace = source ) : return True return False
12591	def get_reliabledictionary_schema ( client , application_name , service_name , dictionary_name , output_file = None ) : cluster = Cluster . from_sfclient ( client ) dictionary = cluster . get_application ( application_name ) . get_service ( service_name ) . get_dictionary ( dictionary_name ) result = json . dumps ( dictionary . get_information ( ) , indent = 4 ) if ( output_file == None ) : output_file = "{}-{}-{}-schema-output.json" . format ( application_name , service_name , dictionary_name ) with open ( output_file , "w" ) as output : output . write ( result ) print ( 'Printed schema information to: ' + output_file ) print ( result )
148	def remove_out_of_image ( self , fully = True , partly = False ) : polys_clean = [ poly for poly in self . polygons if not poly . is_out_of_image ( self . shape , fully = fully , partly = partly ) ] # TODO use deepcopy() here return PolygonsOnImage ( polys_clean , shape = self . shape )
10331	def group_nodes_by_annotation ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , Set [ BaseEntity ] ] : result = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : if not edge_has_annotation ( d , annotation ) : continue result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( u ) result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( v ) return dict ( result )
12010	def getTableOfContents ( self ) : self . directory_size = self . getDirectorySize ( ) if self . directory_size > 65536 : self . directory_size += 2 self . requestContentDirectory ( ) # and find the offset from start of file where it can be found directory_start = unpack ( "i" , self . raw_bytes [ self . directory_end + 16 : self . directory_end + 20 ] ) [ 0 ] # find the data in the raw_bytes self . raw_bytes = self . raw_bytes current_start = directory_start - self . start filestart = 0 compressedsize = 0 tableOfContents = [ ] try : while True : # get file name size (n), extra len (m) and comm len (k) zip_n = unpack ( "H" , self . raw_bytes [ current_start + 28 : current_start + 28 + 2 ] ) [ 0 ] zip_m = unpack ( "H" , self . raw_bytes [ current_start + 30 : current_start + 30 + 2 ] ) [ 0 ] zip_k = unpack ( "H" , self . raw_bytes [ current_start + 32 : current_start + 32 + 2 ] ) [ 0 ] filename = self . raw_bytes [ current_start + 46 : current_start + 46 + zip_n ] # check if this is the index file filestart = unpack ( "I" , self . raw_bytes [ current_start + 42 : current_start + 42 + 4 ] ) [ 0 ] compressedsize = unpack ( "I" , self . raw_bytes [ current_start + 20 : current_start + 20 + 4 ] ) [ 0 ] uncompressedsize = unpack ( "I" , self . raw_bytes [ current_start + 24 : current_start + 24 + 4 ] ) [ 0 ] tableItem = { 'filename' : filename , 'compressedsize' : compressedsize , 'uncompressedsize' : uncompressedsize , 'filestart' : filestart } tableOfContents . append ( tableItem ) # not this file, move along current_start = current_start + 46 + zip_n + zip_m + zip_k except : pass self . tableOfContents = tableOfContents return tableOfContents
6741	def get_os_version ( ) : # TODO: remove once fabric stops using contextlib.nested. # https://github.com/fabric/fabric/issues/1364 import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_os_version = get_rc ( 'common_os_version' ) if common_os_version : return common_os_version with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run_or_local ( 'cat /etc/lsb-release' ) if ret . succeeded : return OS ( type = LINUX , distro = UBUNTU , release = re . findall ( r'DISTRIB_RELEASE=([0-9\.]+)' , ret ) [ 0 ] ) ret = _run_or_local ( 'cat /etc/debian_version' ) if ret . succeeded : return OS ( type = LINUX , distro = DEBIAN , release = re . findall ( r'([0-9\.]+)' , ret ) [ 0 ] ) ret = _run_or_local ( 'cat /etc/fedora-release' ) if ret . succeeded : return OS ( type = LINUX , distro = FEDORA , release = re . findall ( r'release ([0-9]+)' , ret ) [ 0 ] ) raise Exception ( 'Unable to determine OS version.' )
12685	def pods ( self ) : # Return empty list if xml_tree is not defined (error Result object) if not self . xml_tree : return [ ] # Create a Pod object for every pod group in xml return [ Pod ( elem ) for elem in self . xml_tree . findall ( 'pod' ) ]
3875	async def _on_state_update ( self , state_update ) : # The state update will include some type of notification: notification_type = state_update . WhichOneof ( 'state_update' ) # If conversation fields have been updated, the state update will have # a conversation containing changed fields. Handle updating the # conversation from this delta: if state_update . HasField ( 'conversation' ) : try : await self . _handle_conversation_delta ( state_update . conversation ) except exceptions . NetworkError : logger . warning ( 'Discarding %s for %s: Failed to fetch conversation' , notification_type . replace ( '_' , ' ' ) , state_update . conversation . conversation_id . id ) return if notification_type == 'typing_notification' : await self . _handle_set_typing_notification ( state_update . typing_notification ) elif notification_type == 'watermark_notification' : await self . _handle_watermark_notification ( state_update . watermark_notification ) elif notification_type == 'event_notification' : await self . _on_event ( state_update . event_notification . event )
4822	def get_course_details ( self , course_id ) : try : return self . client . course ( course_id ) . get ( ) except ( SlumberBaseException , ConnectionError , Timeout ) as exc : LOGGER . exception ( 'Failed to retrieve course enrollment details for course [%s] due to: [%s]' , course_id , str ( exc ) ) return { }
105	def pool ( arr , block_size , func , cval = 0 , preserve_dtype = True ) : # TODO find better way to avoid circular import from . import dtypes as iadt iadt . gate_dtypes ( arr , allowed = [ "bool" , "uint8" , "uint16" , "uint32" , "int8" , "int16" , "int32" , "float16" , "float32" , "float64" , "float128" ] , disallowed = [ "uint64" , "uint128" , "uint256" , "int64" , "int128" , "int256" , "float256" ] , augmenter = None ) do_assert ( arr . ndim in [ 2 , 3 ] ) is_valid_int = is_single_integer ( block_size ) and block_size >= 1 is_valid_tuple = is_iterable ( block_size ) and len ( block_size ) in [ 2 , 3 ] and [ is_single_integer ( val ) and val >= 1 for val in block_size ] do_assert ( is_valid_int or is_valid_tuple ) if is_single_integer ( block_size ) : block_size = [ block_size , block_size ] if len ( block_size ) < arr . ndim : block_size = list ( block_size ) + [ 1 ] input_dtype = arr . dtype arr_reduced = skimage . measure . block_reduce ( arr , tuple ( block_size ) , func , cval = cval ) if preserve_dtype and arr_reduced . dtype . type != input_dtype : arr_reduced = arr_reduced . astype ( input_dtype ) return arr_reduced
8765	def opt_args_decorator ( func ) : @ wraps ( func ) def wrapped_dec ( * args , * * kwargs ) : if len ( args ) == 1 and len ( kwargs ) == 0 and callable ( args [ 0 ] ) : # actual decorated function return func ( args [ 0 ] ) else : # decorator arguments return lambda realf : func ( realf , * args , * * kwargs ) return wrapped_dec
5215	def active_futures ( ticker : str , dt ) -> str : t_info = ticker . split ( ) prefix , asset = ' ' . join ( t_info [ : - 1 ] ) , t_info [ - 1 ] info = const . market_info ( f'{prefix[:-1]}1 {asset}' ) f1 , f2 = f'{prefix[:-1]}1 {asset}' , f'{prefix[:-1]}2 {asset}' fut_2 = fut_ticker ( gen_ticker = f2 , dt = dt , freq = info [ 'freq' ] ) fut_1 = fut_ticker ( gen_ticker = f1 , dt = dt , freq = info [ 'freq' ] ) fut_tk = bdp ( tickers = [ fut_1 , fut_2 ] , flds = 'Last_Tradeable_Dt' , cache = True ) if pd . Timestamp ( dt ) . month < pd . Timestamp ( fut_tk . last_tradeable_dt [ 0 ] ) . month : return fut_1 d1 = bdib ( ticker = f1 , dt = dt ) d2 = bdib ( ticker = f2 , dt = dt ) return fut_1 if d1 [ f1 ] . volume . sum ( ) > d2 [ f2 ] . volume . sum ( ) else fut_2
7779	def __from_rfc2426 ( self , data ) : data = from_utf8 ( data ) lines = data . split ( "\n" ) started = 0 current = None for l in lines : if not l : continue if l [ - 1 ] == "\r" : l = l [ : - 1 ] if not l : continue if l [ 0 ] in " \t" : if current is None : continue current += l [ 1 : ] continue if not started and current and current . upper ( ) . strip ( ) == "BEGIN:VCARD" : started = 1 elif started and current . upper ( ) . strip ( ) == "END:VCARD" : current = None break elif current and started : self . _process_rfc2425_record ( current ) current = l if started and current : self . _process_rfc2425_record ( current )
11944	def add ( self , level , message , extra_tags = '' ) : if not message : return # Check that the message level is not less than the recording level. level = int ( level ) if level < self . level : return # Check if the message doesn't have a level that needs to be persisted if level not in stored_messages_settings . STORE_LEVELS or self . user . is_anonymous ( ) : return super ( StorageMixin , self ) . add ( level , message , extra_tags ) self . added_new = True m = self . backend . create_message ( level , message , extra_tags ) self . backend . archive_store ( [ self . user ] , m ) self . _queued_messages . append ( m )
3517	def spring_metrics ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return SpringMetricsNode ( )
5443	def _validate_paths_or_fail ( uri , recursive ) : path , filename = os . path . split ( uri ) # dsub could support character ranges ([0-9]) with some more work, but for # now we assume that basic asterisk wildcards are sufficient. Reject any URI # that includes square brackets or question marks, since we know that # if they actually worked, it would be accidental. if '[' in uri or ']' in uri : raise ValueError ( 'Square bracket (character ranges) are not supported: %s' % uri ) if '?' in uri : raise ValueError ( 'Question mark wildcards are not supported: %s' % uri ) # Only support file URIs and *filename* wildcards # Wildcards at the directory level or "**" syntax would require better # support from the Pipelines API *or* doing expansion here and # (potentially) producing a series of FileParams, instead of one. if '*' in path : raise ValueError ( 'Path wildcard (*) are only supported for files: %s' % uri ) if '**' in filename : raise ValueError ( 'Recursive wildcards ("**") not supported: %s' % uri ) if filename in ( '..' , '.' ) : raise ValueError ( 'Path characters ".." and "." not supported ' 'for file names: %s' % uri ) # Do not allow non-recursive IO to reference directories. if not recursive and not filename : raise ValueError ( 'Input or output values that are not recursive must ' 'reference a filename or wildcard: %s' % uri )
6328	def _add_to_ngcorpus ( self , corpus , words , count ) : if words [ 0 ] not in corpus : corpus [ words [ 0 ] ] = Counter ( ) if len ( words ) == 1 : corpus [ words [ 0 ] ] [ None ] += count else : self . _add_to_ngcorpus ( corpus [ words [ 0 ] ] , words [ 1 : ] , count )
404	def swish ( x , name = 'swish' ) : with tf . name_scope ( name ) : x = tf . nn . sigmoid ( x ) * x return x
9739	def get_2d_markers ( self , component_info = None , data = None , component_position = None , index = None ) : return self . _get_2d_markers ( data , component_info , component_position , index = index )
10071	def pid ( self ) : pid = self . deposit_fetcher ( self . id , self ) return PersistentIdentifier . get ( pid . pid_type , pid . pid_value )
9887	def _read_all_attribute_info ( self ) : num = copy . deepcopy ( self . _num_attrs ) fname = copy . deepcopy ( self . fname ) out = fortran_cdf . inquire_all_attr ( fname , num , len ( fname ) ) status = out [ 0 ] names = out [ 1 ] . astype ( 'U' ) scopes = out [ 2 ] max_gentries = out [ 3 ] max_rentries = out [ 4 ] max_zentries = out [ 5 ] attr_nums = out [ 6 ] global_attrs_info = { } var_attrs_info = { } if status == 0 : for name , scope , gentry , rentry , zentry , num in zip ( names , scopes , max_gentries , max_rentries , max_zentries , attr_nums ) : name = '' . join ( name ) name = name . rstrip ( ) nug = { } nug [ 'scope' ] = scope nug [ 'max_gentry' ] = gentry nug [ 'max_rentry' ] = rentry nug [ 'max_zentry' ] = zentry nug [ 'attr_num' ] = num flag = ( gentry == 0 ) & ( rentry == 0 ) & ( zentry == 0 ) if not flag : if scope == 1 : global_attrs_info [ name ] = nug elif scope == 2 : var_attrs_info [ name ] = nug self . global_attrs_info = global_attrs_info self . var_attrs_info = var_attrs_info else : raise IOError ( fortran_cdf . statusreporter ( status ) )
9994	def set_attr ( self , name , value ) : if not is_valid_name ( name ) : raise ValueError ( "Invalid name '%s'" % name ) if name in self . namespace : if name in self . refs : if name in self . self_refs : self . new_ref ( name , value ) else : raise KeyError ( "Ref '%s' cannot be changed" % name ) elif name in self . cells : if self . cells [ name ] . is_scalar ( ) : self . cells [ name ] . set_value ( ( ) , value ) else : raise AttributeError ( "Cells '%s' is not a scalar." % name ) else : raise ValueError else : self . new_ref ( name , value )
1192	def translate ( pat ) : i , n = 0 , len ( pat ) res = '' while i < n : c = pat [ i ] i = i + 1 if c == '*' : res = res + '.*' elif c == '?' : res = res + '.' elif c == '[' : j = i if j < n and pat [ j ] == '!' : j = j + 1 if j < n and pat [ j ] == ']' : j = j + 1 while j < n and pat [ j ] != ']' : j = j + 1 if j >= n : res = res + '\\[' else : stuff = pat [ i : j ] . replace ( '\\' , '\\\\' ) i = j + 1 if stuff [ 0 ] == '!' : stuff = '^' + stuff [ 1 : ] elif stuff [ 0 ] == '^' : stuff = '\\' + stuff res = '%s[%s]' % ( res , stuff ) else : res = res + re . escape ( c ) return res + '\Z(?ms)'
7973	def start ( self , daemon = False ) : self . daemon = daemon self . io_threads = [ ] self . event_thread = EventDispatcherThread ( self . event_dispatcher , daemon = daemon , exc_queue = self . exc_queue ) self . event_thread . start ( ) for handler in self . io_handlers : self . _run_io_threads ( handler ) for handler in self . timeout_handlers : self . _run_timeout_threads ( handler )
8133	def delete ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ]
7681	def piano_roll ( annotation , * * kwargs ) : times , midi = annotation . to_interval_values ( ) return mir_eval . display . piano_roll ( times , midi = midi , * * kwargs )
8503	def as_call ( self ) : default = self . _default ( ) default = ', ' + default if default else '' return "pyconfig.%s(%r%s)" % ( self . method , self . get_key ( ) , default )
12401	def satisfied_by_checked ( self , req ) : req_man = RequirementsManager ( [ req ] ) return any ( req_man . check ( * checked ) for checked in self . checked )
5239	def market_open ( self , session , mins ) -> Session : if session not in self . exch : return SessNA start_time = self . exch [ session ] [ 0 ] return Session ( start_time , shift_time ( start_time , int ( mins ) ) )
4773	def contains_only ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : extra = [ ] for i in self . val : if i not in items : extra . append ( i ) if extra : self . _err ( 'Expected <%s> to contain only %s, but did contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( extra ) ) ) missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to contain only %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
12596	def _openpyxl_read_xl ( xl_path : str ) : try : wb = load_workbook ( filename = xl_path , read_only = True ) except : raise else : return wb
3314	def _stream_data ( self , environ , content_length , block_size ) : if content_length == 0 : # TODO: review this # XP and Vista MiniRedir submit PUT with Content-Length 0, # before LOCK and the real PUT. So we have to accept this. _logger . info ( "PUT: Content-Length == 0. Creating empty file..." ) # elif content_length < 0: # # TODO: review this # # If CONTENT_LENGTH is invalid, we may try to workaround this # # by reading until the end of the stream. This may block however! # # The iterator produced small chunks of varying size, but not # # sure, if we always get everything before it times out. # _logger.warning("PUT with invalid Content-Length (%s). " # "Trying to read all (this may timeout)..." # .format(environ.get("CONTENT_LENGTH"))) # nb = 0 # try: # for s in environ["wsgi.input"]: # environ["wsgidav.some_input_read"] = 1 # _logger.debug("PUT: read from wsgi.input.__iter__, len=%s" % len(s)) # yield s # nb += len (s) # except socket.timeout: # _logger.warning("PUT: input timed out after writing %s bytes" % nb) # hasErrors = True else : assert content_length > 0 contentremain = content_length while contentremain > 0 : n = min ( contentremain , block_size ) readbuffer = environ [ "wsgi.input" ] . read ( n ) # This happens with litmus expect-100 test: if not len ( readbuffer ) > 0 : _logger . error ( "input.read({}) returned 0 bytes" . format ( n ) ) break environ [ "wsgidav.some_input_read" ] = 1 yield readbuffer contentremain -= len ( readbuffer ) if contentremain == 0 : environ [ "wsgidav.all_input_read" ] = 1
3462	def double_reaction_deletion ( model , reaction_list1 = None , reaction_list2 = None , method = "fba" , solution = None , processes = None , * * kwargs ) : reaction_list1 , reaction_list2 = _element_lists ( model . reactions , reaction_list1 , reaction_list2 ) return _multi_deletion ( model , 'reaction' , element_lists = [ reaction_list1 , reaction_list2 ] , method = method , solution = solution , processes = processes , * * kwargs )
2864	def readS8 ( self , register ) : result = self . readU8 ( register ) if result > 127 : result -= 256 return result
6783	def lock ( self ) : self . init ( ) r = self . local_renderer if self . file_exists ( r . env . lockfile_path ) : raise exceptions . AbortDeployment ( 'Lock file %s exists. Perhaps another deployment is currently underway?' % r . env . lockfile_path ) else : self . vprint ( 'Locking %s.' % r . env . lockfile_path ) r . env . hostname = socket . gethostname ( ) r . run_or_local ( 'echo "{hostname}" > {lockfile_path}' )
5023	def get_integrated_channels ( self , options ) : channel_classes = self . get_channel_classes ( options . get ( 'channel' ) ) filter_kwargs = { 'active' : True , 'enterprise_customer__active' : True , } enterprise_customer = self . get_enterprise_customer ( options . get ( 'enterprise_customer' ) ) if enterprise_customer : filter_kwargs [ 'enterprise_customer' ] = enterprise_customer for channel_class in channel_classes : for integrated_channel in channel_class . objects . filter ( * * filter_kwargs ) : yield integrated_channel
13208	def _parse_abstract ( self ) : command = LatexCommand ( 'setDocAbstract' , { 'name' : 'abstract' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no abstract' ) self . _abstract = None return try : content = parsed [ 'abstract' ] except KeyError : self . _logger . warning ( 'lsstdoc has no abstract' ) self . _abstract = None return content = content . strip ( ) self . _abstract = content
6251	def create_transformation ( self , rotation = None , translation = None ) : mat = None if rotation is not None : mat = Matrix44 . from_eulers ( Vector3 ( rotation ) ) if translation is not None : trans = matrix44 . create_from_translation ( Vector3 ( translation ) ) if mat is None : mat = trans else : mat = matrix44 . multiply ( mat , trans ) return mat
8379	def copy ( self , graph ) : e = events ( graph , self . _ctx ) e . clicked = self . clicked return e
4322	def channels ( self , n_channels ) : if not isinstance ( n_channels , int ) or n_channels <= 0 : raise ValueError ( 'n_channels must be a positive integer.' ) effect_args = [ 'channels' , '{}' . format ( n_channels ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'channels' ) return self
7213	def get_proj ( prj_code ) : if prj_code in CUSTOM_PRJ : proj = pyproj . Proj ( CUSTOM_PRJ [ prj_code ] ) else : proj = pyproj . Proj ( init = prj_code ) return proj
13463	def add_event ( request ) : form = AddEventForm ( request . POST or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . sites = settings . SITE_ID instance . submitted_by = request . user instance . approved = True instance . slug = slugify ( instance . name ) instance . save ( ) messages . success ( request , 'Your event has been added.' ) return HttpResponseRedirect ( reverse ( 'events_index' ) ) return render ( request , 'happenings/event_form.html' , { 'form' : form , 'form_title' : 'Add an event' } )
10567	def _check_filters ( song , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : include = True if include_filters : if all_includes : if not all ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in include_filters ) : include = False else : if not any ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in include_filters ) : include = False if exclude_filters : if all_excludes : if all ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in exclude_filters ) : include = False else : if any ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in exclude_filters ) : include = False return include
11099	def select_by_mtime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . mtime <= max_time return self . select_file ( filters , recursive )
272	def clip_returns_to_benchmark ( rets , benchmark_rets ) : if ( rets . index [ 0 ] < benchmark_rets . index [ 0 ] ) or ( rets . index [ - 1 ] > benchmark_rets . index [ - 1 ] ) : clipped_rets = rets [ benchmark_rets . index ] else : clipped_rets = rets return clipped_rets
12952	def _get_connection ( self ) : if self . _connection is None : self . _connection = self . _get_new_connection ( ) return self . _connection
5726	def get_gdb_response ( self , timeout_sec = DEFAULT_GDB_TIMEOUT_SEC , raise_error_on_timeout = True ) : self . verify_valid_gdb_subprocess ( ) if timeout_sec < 0 : self . logger . warning ( "timeout_sec was negative, replacing with 0" ) timeout_sec = 0 if USING_WINDOWS : retval = self . _get_responses_windows ( timeout_sec ) else : retval = self . _get_responses_unix ( timeout_sec ) if not retval and raise_error_on_timeout : raise GdbTimeoutError ( "Did not get response from gdb after %s seconds" % timeout_sec ) else : return retval
7898	def process_configuration_form_success ( self , stanza ) : if stanza . get_query_ns ( ) != MUC_OWNER_NS : raise ValueError ( "Bad result namespace" ) # TODO: ProtocolError query = stanza . get_query ( ) form = None for el in xml_element_ns_iter ( query . children , DATAFORM_NS ) : form = Form ( el ) break if not form : raise ValueError ( "No form received" ) # TODO: ProtocolError self . configuration_form = form self . handler . configuration_form_received ( form )
2126	def data_endpoint ( cls , in_data , ignore = [ ] ) : obj , obj_type , res , res_type = cls . obj_res ( in_data , fail_on = [ ] ) data = { } if 'obj' in ignore : obj = None if 'res' in ignore : res = None # Input fields are not actually present on role model, and all have # to be managed as individual special-cases if obj and obj_type == 'user' : data [ 'members__in' ] = obj if obj and obj_type == 'team' : endpoint = '%s/%s/roles/' % ( grammar . pluralize ( obj_type ) , obj ) if res is not None : # For teams, this is the best lookup we can do # without making the additional request for its member_role data [ 'object_id' ] = res elif res : endpoint = '%s/%s/object_roles/' % ( grammar . pluralize ( res_type ) , res ) else : endpoint = '/roles/' if in_data . get ( 'type' , False ) : data [ 'role_field' ] = '%s_role' % in_data [ 'type' ] . lower ( ) # Add back fields unrelated to role lookup, such as all_pages for key , value in in_data . items ( ) : if key not in RESOURCE_FIELDS and key not in [ 'type' , 'user' , 'team' ] : data [ key ] = value return data , endpoint
6756	def param_changed_to ( self , key , to_value , from_value = None ) : last_value = getattr ( self . last_manifest , key ) current_value = self . current_manifest . get ( key ) if from_value is not None : return last_value == from_value and current_value == to_value return last_value != to_value and current_value == to_value
8004	def submit_form ( self , form ) : result = Register ( ) if self . form : result . form = form . make_submit ( ) return result if "FORM_TYPE" not in form or "jabber:iq:register" not in form [ "FORM_TYPE" ] . values : raise ValueError ( "FORM_TYPE is not jabber:iq:register" ) for field in legacy_fields : self . __logger . debug ( u"submitted field %r" % ( field , ) ) value = getattr ( self , field ) try : form_value = form [ field ] . value except KeyError : if value : raise ValueError ( "Required field with no value!" ) continue setattr ( result , field , form_value ) return result
4706	def read ( self , path ) : with open ( path , "rb" ) as fout : memmove ( self . m_buf , fout . read ( self . m_size ) , self . m_size )
13869	def _GetNativeEolStyle ( platform = sys . platform ) : _NATIVE_EOL_STYLE_MAP = { 'win32' : EOL_STYLE_WINDOWS , 'linux2' : EOL_STYLE_UNIX , 'linux' : EOL_STYLE_UNIX , 'darwin' : EOL_STYLE_MAC , } result = _NATIVE_EOL_STYLE_MAP . get ( platform ) if result is None : from . _exceptions import UnknownPlatformError raise UnknownPlatformError ( platform ) return result
12239	def beale ( theta ) : x , y = theta A = 1.5 - x + x * y B = 2.25 - x + x * y ** 2 C = 2.625 - x + x * y ** 3 obj = A ** 2 + B ** 2 + C ** 2 grad = np . array ( [ 2 * A * ( y - 1 ) + 2 * B * ( y ** 2 - 1 ) + 2 * C * ( y ** 3 - 1 ) , 2 * A * x + 4 * B * x * y + 6 * C * x * y ** 2 ] ) return obj , grad
12097	def delete ( self , force = False , * * kwargs ) : if force : return super ( BaseActivatableModel , self ) . delete ( * * kwargs ) else : setattr ( self , self . ACTIVATABLE_FIELD_NAME , False ) return self . save ( update_fields = [ self . ACTIVATABLE_FIELD_NAME ] )
8479	def potential ( self , value ) : if value : self . _potential = True else : self . _potential = False
4192	def compute_response ( self , * * kargs ) : from numpy . fft import fft , fftshift norm = kargs . get ( 'norm' , self . norm ) # do some padding. Default is max(2048, data.len*2) NFFT = kargs . get ( 'NFFT' , 2048 ) if NFFT < len ( self . data ) : NFFT = self . data . size * 2 # compute the fft modulus A = fft ( self . data , NFFT ) mag = abs ( fftshift ( A ) ) # do we want to normalise the data if norm is True : mag = mag / max ( mag ) response = 20. * stools . log10 ( mag ) # factor 20 we are looking at the response # not the powe #response = clip(response,mindB,100) self . __response = response
12543	def merge_images ( images , axis = 't' ) : # check if images is not empty if not images : return None # the given axis name to axis idx axis_dim = { 'x' : 0 , 'y' : 1 , 'z' : 2 , 't' : 3 , } # check if the given axis name is valid if axis not in axis_dim : raise ValueError ( 'Expected `axis` to be one of ({}), got {}.' . format ( set ( axis_dim . keys ( ) ) , axis ) ) # check if all images are compatible with each other img1 = images [ 0 ] for img in images : check_img_compatibility ( img1 , img ) # read the data of all the given images # TODO: optimize memory consumption by merging one by one. image_data = [ ] for img in images : image_data . append ( check_img ( img ) . get_data ( ) ) # if the work_axis is bigger than the number of axis of the images, # create a new axis for the images work_axis = axis_dim [ axis ] ndim = image_data [ 0 ] . ndim if ndim - 1 < work_axis : image_data = [ np . expand_dims ( img , axis = work_axis ) for img in image_data ] # concatenate and return return np . concatenate ( image_data , axis = work_axis )
254	def add_closing_transactions ( positions , transactions ) : closed_txns = transactions [ [ 'symbol' , 'amount' , 'price' ] ] pos_at_end = positions . drop ( 'cash' , axis = 1 ) . iloc [ - 1 ] open_pos = pos_at_end . replace ( 0 , np . nan ) . dropna ( ) # Add closing round_trips one second after the close to be sure # they don't conflict with other round_trips executed at that time. end_dt = open_pos . name + pd . Timedelta ( seconds = 1 ) for sym , ending_val in open_pos . iteritems ( ) : txn_sym = transactions [ transactions . symbol == sym ] ending_amount = txn_sym . amount . sum ( ) ending_price = ending_val / ending_amount closing_txn = { 'symbol' : sym , 'amount' : - ending_amount , 'price' : ending_price } closing_txn = pd . DataFrame ( closing_txn , index = [ end_dt ] ) closed_txns = closed_txns . append ( closing_txn ) closed_txns = closed_txns [ closed_txns . amount != 0 ] return closed_txns
710	def _backupFile ( filePath ) : assert os . path . exists ( filePath ) stampNum = 0 ( prefix , suffix ) = os . path . splitext ( filePath ) while True : backupPath = "%s.%d%s" % ( prefix , stampNum , suffix ) stampNum += 1 if not os . path . exists ( backupPath ) : break shutil . copyfile ( filePath , backupPath ) return backupPath
6046	def array_2d_from_array_1d ( self , padded_array_1d ) : padded_array_2d = self . map_to_2d_keep_padded ( padded_array_1d ) pad_size_0 = self . mask . shape [ 0 ] - self . image_shape [ 0 ] pad_size_1 = self . mask . shape [ 1 ] - self . image_shape [ 1 ] return ( padded_array_2d [ pad_size_0 // 2 : self . mask . shape [ 0 ] - pad_size_0 // 2 , pad_size_1 // 2 : self . mask . shape [ 1 ] - pad_size_1 // 2 ] )
3962	def prep_for_start_local_env ( pull_repos ) : if pull_repos : update_managed_repos ( force = True ) assembled_spec = spec_assembler . get_assembled_specs ( ) if not assembled_spec [ constants . CONFIG_BUNDLES_KEY ] : raise RuntimeError ( 'No bundles are activated. Use `dusty bundles` to activate bundles before running `dusty up`.' ) virtualbox . initialize_docker_vm ( )
11083	def help ( self , msg , args ) : output = [ ] if len ( args ) == 0 : commands = sorted ( self . _bot . dispatcher . commands . items ( ) , key = itemgetter ( 0 ) ) commands = filter ( lambda x : x [ 1 ] . is_subcmd is False , commands ) # Filter commands if auth is enabled, hide_admin_commands is enabled, and user is not admin if self . _should_filter_help_commands ( msg . user ) : commands = filter ( lambda x : x [ 1 ] . admin_only is False , commands ) for name , cmd in commands : output . append ( self . _get_short_help_for_command ( name ) ) else : name = '!' + args [ 0 ] output = [ self . _get_help_for_command ( name ) ] return '\n' . join ( output )
8069	def randomChildElement ( self , node ) : choices = [ e for e in node . childNodes if e . nodeType == e . ELEMENT_NODE ] chosen = random . choice ( choices ) if _debug : sys . stderr . write ( '%s available choices: %s\n' % ( len ( choices ) , [ e . toxml ( ) for e in choices ] ) ) sys . stderr . write ( 'Chosen: %s\n' % chosen . toxml ( ) ) return chosen
1128	def SeqN ( n , * inner_rules , * * kwargs ) : @ action ( Seq ( * inner_rules ) , loc = kwargs . get ( "loc" , None ) ) def rule ( parser , * values ) : return values [ n ] return rule
7226	def paint ( self ) : # TODO Figure out why i cant use some of these props snippet = { 'line-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'line-color' : VectorStyle . get_style_value ( self . color ) , #'line-cap': self.cap, #'line-join': self.join, 'line-width' : VectorStyle . get_style_value ( self . width ) , #'line-gap-width': self.gap_width, #'line-blur': self.blur, } if self . translate : snippet [ 'line-translate' ] = self . translate if self . dasharray : snippet [ 'line-dasharray' ] = VectorStyle . get_style_value ( self . dasharray ) return snippet
5404	def _get_delocalization_env ( self , outputs , user_project ) : # Add variables for paths that need to be delocalized, for example: # OUTPUT_COUNT: 1 # OUTPUT_0: MY_OUTPUT_FILE # OUTPUT_RECURSIVE_0: 0 # OUTPUT_SRC_0: gs://mybucket/mypath/myfile # OUTPUT_DST_0: /mnt/data/outputs/mybucket/mypath/myfile non_empty_outputs = [ var for var in outputs if var . value ] env = { 'OUTPUT_COUNT' : str ( len ( non_empty_outputs ) ) } for idx , var in enumerate ( non_empty_outputs ) : env [ 'OUTPUT_{}' . format ( idx ) ] = var . name env [ 'OUTPUT_RECURSIVE_{}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'OUTPUT_SRC_{}' . format ( idx ) ] = os . path . join ( providers_util . DATA_MOUNT_POINT , var . docker_path ) # For wildcard paths, the destination must be a directory if '*' in var . uri . basename : dst = var . uri . path else : dst = var . uri env [ 'OUTPUT_DST_{}' . format ( idx ) ] = dst env [ 'USER_PROJECT' ] = user_project return env
7524	def _collapse_outgroup ( tree , taxdicts ) : ## check that all tests have the same outgroup outg = taxdicts [ 0 ] [ "p4" ] if not all ( [ i [ "p4" ] == outg for i in taxdicts ] ) : raise Exception ( "no good" ) ## prune tree, keep only one sample from outgroup tre = ete . Tree ( tree . write ( format = 1 ) ) #tree.copy(method="deepcopy") alltax = [ i for i in tre . get_leaf_names ( ) if i not in outg ] alltax += [ outg [ 0 ] ] tre . prune ( alltax ) tre . search_nodes ( name = outg [ 0 ] ) [ 0 ] . name = "outgroup" tre . ladderize ( ) ## remove other ougroups from taxdicts taxd = copy . deepcopy ( taxdicts ) newtaxdicts = [ ] for test in taxd : #test["p4"] = [outg[0]] test [ "p4" ] = [ "outgroup" ] newtaxdicts . append ( test ) return tre , newtaxdicts
12142	def _info ( self , source , key , filetype , ignore ) : specs , mdata = [ ] , { } mdata_clashes = set ( ) for spec in source . specs : if key not in spec : raise Exception ( "Key %r not available in 'source'." % key ) mdata = dict ( ( k , v ) for ( k , v ) in filetype . metadata ( spec [ key ] ) . items ( ) if k not in ignore ) mdata_spec = { } mdata_spec . update ( spec ) mdata_spec . update ( mdata ) specs . append ( mdata_spec ) mdata_clashes = mdata_clashes | ( set ( spec . keys ( ) ) & set ( mdata . keys ( ) ) ) # Metadata clashes can be avoided by using the ignore list. if mdata_clashes : self . warning ( "Loaded metadata keys overriding source keys." ) return specs
7846	def add_item ( self , jid , node = None , name = None , action = None ) : return DiscoItem ( self , jid , node , name , action )
10361	def rewire_targets ( graph , rewiring_probability ) : if not all_edges_consistent ( graph ) : raise ValueError ( '{} is not consistent' . format ( graph ) ) result = graph . copy ( ) nodes = result . nodes ( ) for u , v in result . edges ( ) : if random . random ( ) < rewiring_probability : continue w = random . choice ( nodes ) while w == u or result . has_edge ( u , w ) : w = random . choice ( nodes ) result . add_edge ( w , v ) result . remove_edge ( u , v ) return result
88	def is_float_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . floating )
733	def inferSingleStep ( self , patternNZ , weightMatrix ) : outputActivation = weightMatrix [ patternNZ ] . sum ( axis = 0 ) # softmax normalization outputActivation = outputActivation - numpy . max ( outputActivation ) expOutputActivation = numpy . exp ( outputActivation ) predictDist = expOutputActivation / numpy . sum ( expOutputActivation ) return predictDist
10854	def sphere_triangle_cdf ( dr , a , alpha ) : p0 = ( dr + alpha ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 > dr ) * ( dr > - alpha ) p1 = 1 * ( dr > 0 ) - ( alpha - dr ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 < dr ) * ( dr < alpha ) return ( 1 - np . clip ( p0 + p1 , 0 , 1 ) )
12484	def get_subdict ( adict , path , sep = os . sep ) : return reduce ( adict . __class__ . get , [ p for p in op . split ( sep ) if p ] , adict )
1701	def join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . INNER , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
10105	def get_group_tabs ( self ) : if self . tab_group is None : raise ImproperlyConfigured ( "%s requires a definition of 'tab_group'" % self . __class__ . __name__ ) group_members = [ t for t in self . _registry if t . tab_group == self . tab_group ] return [ t ( ) for t in group_members ]
766	def clean ( s ) : lines = [ l . rstrip ( ) for l in s . split ( '\n' ) ] return '\n' . join ( lines )
8577	def get_server ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
9694	def replace ( self , * * k ) : if self . date != 'infinity' : return Date ( self . date . replace ( * * k ) ) else : return Date ( 'infinity' )
8457	def _needs_new_cc_config_for_update ( old_template , old_version , new_template , new_version ) : if old_template != new_template : return True else : return _cookiecutter_configs_have_changed ( new_template , old_version , new_version )
5878	def get_video ( self , node ) : video = Video ( ) video . _embed_code = self . get_embed_code ( node ) video . _embed_type = self . get_embed_type ( node ) video . _width = self . get_width ( node ) video . _height = self . get_height ( node ) video . _src = self . get_src ( node ) video . _provider = self . get_provider ( video . src ) return video
7152	def one ( prompt , * args , * * kwargs ) : indicator = 'โฃ' if sys . version_info < ( 3 , 0 ) : indicator = '>' def go_back ( picker ) : return None , - 1 options , verbose_options = prepare_options ( args ) idx = kwargs . get ( 'idx' , 0 ) picker = Picker ( verbose_options , title = prompt , indicator = indicator , default_index = idx ) picker . register_custom_handler ( ord ( 'h' ) , go_back ) picker . register_custom_handler ( curses . KEY_LEFT , go_back ) with stdout_redirected ( sys . stderr ) : option , index = picker . start ( ) if index == - 1 : raise QuestionnaireGoBack if kwargs . get ( 'return_index' , False ) : # `one` was called by a special client, e.g. `many` return index return options [ index ]
9414	def _make_user_class ( session , name ) : attrs = session . eval ( 'fieldnames(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) methods = session . eval ( 'methods(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) ref = weakref . ref ( session ) doc = _DocDescriptor ( ref , name ) values = dict ( __doc__ = doc , _name = name , _ref = ref , _attrs = attrs , __module__ = 'oct2py.dynamic' ) for method in methods : doc = _MethodDocDescriptor ( ref , name , method ) cls_name = '%s_%s' % ( name , method ) method_values = dict ( __doc__ = doc ) method_cls = type ( str ( cls_name ) , ( OctaveUserClassMethod , ) , method_values ) values [ method ] = method_cls ( ref , method , name ) for attr in attrs : values [ attr ] = OctaveUserClassAttr ( ref , attr , attr ) return type ( str ( name ) , ( OctaveUserClass , ) , values )
13592	def main ( target , label ) : check_environment ( target , label ) click . secho ( 'Fetching tags from the upstream ...' ) handler = TagHandler ( git . list_tags ( ) ) print_information ( handler , label ) tag = handler . yield_tag ( target , label ) confirm ( tag )
5674	def get_main_database_path ( self ) : cur = self . conn . cursor ( ) cur . execute ( "PRAGMA database_list" ) rows = cur . fetchall ( ) for row in rows : if row [ 1 ] == str ( "main" ) : return row [ 2 ]
667	def logProbability ( self , distn ) : x = numpy . asarray ( distn ) n = x . sum ( ) return ( logFactorial ( n ) - numpy . sum ( [ logFactorial ( k ) for k in x ] ) + numpy . sum ( x * numpy . log ( self . dist . pmf ) ) )
9155	def scaling ( self , x , y ) : self . drawer . append ( pgmagick . DrawableScaling ( float ( x ) , float ( y ) ) )
6451	def flake8_color ( score ) : # These are the score cutoffs for each color above. # I.e. score==0 -> brightgreen, down to 100 < score <= 200 -> orange score_cutoffs = ( 0 , 20 , 50 , 100 , 200 ) for i in range ( len ( score_cutoffs ) ) : if score <= score_cutoffs [ i ] : return BADGE_COLORS [ i ] # and score > 200 -> red return BADGE_COLORS [ - 1 ]
7747	def _process_handler_result ( self , response ) : if response is None or response is False : return False if isinstance ( response , Stanza ) : self . send ( response ) return True try : response = iter ( response ) except TypeError : return bool ( response ) for stanza in response : if isinstance ( stanza , Stanza ) : self . send ( stanza ) else : logger . warning ( u"Unexpected object in stanza handler result:" u" {0!r}" . format ( stanza ) ) return True
13132	def parse_user ( entry , domain_groups ) : result = { } distinguished_name = get_field ( entry , 'distinguishedName' ) result [ 'domain' ] = "." . join ( distinguished_name . split ( ',DC=' ) [ 1 : ] ) result [ 'name' ] = get_field ( entry , 'name' ) result [ 'username' ] = get_field ( entry , 'sAMAccountName' ) result [ 'description' ] = get_field ( entry , 'description' ) result [ 'sid' ] = get_field ( entry , 'objectSid' ) . split ( '-' ) [ - 1 ] primary_group = get_field ( entry , 'primaryGroupID' ) member_of = entry [ 'attributes' ] . get ( 'memberOf' , [ ] ) groups = [ ] for member in member_of : for e in member . split ( ',' ) : if e . startswith ( 'CN=' ) : groups . append ( e [ 3 : ] ) groups . append ( domain_groups . get ( primary_group , '' ) ) result [ 'groups' ] = groups flags = [ ] try : uac = int ( get_field ( entry , 'userAccountControl' ) ) for flag , value in uac_flags . items ( ) : if uac & value : flags . append ( flag ) except ValueError : pass result [ 'flags' ] = flags return result
338	def _GetNextLogCountPerToken ( token ) : global _log_counter_per_token # pylint: disable=global-variable-not-assigned _log_counter_per_token [ token ] = 1 + _log_counter_per_token . get ( token , - 1 ) return _log_counter_per_token [ token ]
296	def plot_sector_allocations ( returns , sector_alloc , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) sector_alloc . plot ( title = 'Sector allocation over time' , alpha = 0.5 , ax = ax , * * kwargs ) box = ax . get_position ( ) ax . set_position ( [ box . x0 , box . y0 + box . height * 0.1 , box . width , box . height * 0.9 ] ) # Put a legend below current axis ax . legend ( loc = 'upper center' , frameon = True , framealpha = 0.5 , bbox_to_anchor = ( 0.5 , - 0.14 ) , ncol = 5 ) ax . set_xlim ( ( sector_alloc . index [ 0 ] , sector_alloc . index [ - 1 ] ) ) ax . set_ylabel ( 'Exposure by sector' ) ax . set_xlabel ( '' ) return ax
4799	def is_file ( self ) : self . exists ( ) if not os . path . isfile ( self . val ) : self . _err ( 'Expected <%s> to be a file, but was not.' % self . val ) return self
7133	def add_icon ( icon_data , dest ) : with open ( os . path . join ( dest , "icon.png" ) , "wb" ) as f : f . write ( icon_data )
11822	def is_compatible ( cls , value ) : if not hasattr ( cls , 'value_type' ) : raise NotImplementedError ( 'You must define a `value_type` attribute or override the ' '`is_compatible()` method on `SettingValueModel` subclasses.' ) return isinstance ( value , cls . value_type )
1252	def add_random_tile ( self ) : x_pos , y_pos = np . where ( self . _state == 0 ) assert len ( x_pos ) != 0 empty_index = np . random . choice ( len ( x_pos ) ) value = np . random . choice ( [ 1 , 2 ] , p = [ 0.9 , 0.1 ] ) self . _state [ x_pos [ empty_index ] , y_pos [ empty_index ] ] = value
6899	def parallel_periodicfeatures ( pfpkl_list , lcbasedir , outdir , starfeaturesdir = None , fourierorder = 5 , # these are depth, duration, ingress duration transitparams = ( - 0.01 , 0.1 , 0.1 ) , # these are depth, duration, depth ratio, secphase ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None , nworkers = NCPUS ) : # make sure to make the output directory if it doesn't exist if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : pfpkl_list = pfpkl_list [ : maxobjects ] LOGINFO ( '%s periodfinding pickles to process' % len ( pfpkl_list ) ) # if the starfeaturedir is provided, try to find a starfeatures pickle for # each periodfinding pickle in pfpkl_list if starfeaturesdir and os . path . exists ( starfeaturesdir ) : starfeatures_list = [ ] LOGINFO ( 'collecting starfeatures pickles...' ) for pfpkl in pfpkl_list : sfpkl1 = os . path . basename ( pfpkl ) . replace ( 'periodfinding' , 'starfeatures' ) sfpkl2 = sfpkl1 . replace ( '.gz' , '' ) sfpath1 = os . path . join ( starfeaturesdir , sfpkl1 ) sfpath2 = os . path . join ( starfeaturesdir , sfpkl2 ) if os . path . exists ( sfpath1 ) : starfeatures_list . append ( sfpkl1 ) elif os . path . exists ( sfpath2 ) : starfeatures_list . append ( sfpkl2 ) else : starfeatures_list . append ( None ) else : starfeatures_list = [ None for x in pfpkl_list ] # generate the task list kwargs = { 'fourierorder' : fourierorder , 'transitparams' : transitparams , 'ebparams' : ebparams , 'pdiff_threshold' : pdiff_threshold , 'sidereal_threshold' : sidereal_threshold , 'sampling_peak_multiplier' : sampling_peak_multiplier , 'sampling_startp' : sampling_startp , 'sampling_endp' : sampling_endp , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'lcformat' : lcformat , 'lcformatdir' : lcformat , 'sigclip' : sigclip , 'verbose' : verbose } tasks = [ ( x , lcbasedir , outdir , y , kwargs ) for ( x , y ) in zip ( pfpkl_list , starfeatures_list ) ] LOGINFO ( 'processing periodfinding pickles...' ) with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( _periodicfeatures_worker , tasks ) results = [ x for x in resultfutures ] resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( pfpkl_list , results ) } return resdict
2414	def write_creation_info ( creation_info , out ) : out . write ( '# Creation Info\n\n' ) # Write sorted creators for creator in sorted ( creation_info . creators ) : write_value ( 'Creator' , creator , out ) # write created write_value ( 'Created' , creation_info . created_iso_format , out ) # possible comment if creation_info . has_comment : write_text_value ( 'CreatorComment' , creation_info . comment , out )
11187	def create ( quiet , name , base_uri , symlink_path ) : _validate_name ( name ) admin_metadata = dtoolcore . generate_admin_metadata ( name ) parsed_base_uri = dtoolcore . utils . generous_parse_uri ( base_uri ) if parsed_base_uri . scheme == "symlink" : if symlink_path is None : raise click . UsageError ( "Need to specify symlink path using the -s/--symlink-path option" ) # NOQA if symlink_path : base_uri = dtoolcore . utils . sanitise_uri ( "symlink:" + parsed_base_uri . path ) parsed_base_uri = dtoolcore . utils . generous_parse_uri ( base_uri ) # Create the dataset. proto_dataset = dtoolcore . generate_proto_dataset ( admin_metadata = admin_metadata , base_uri = dtoolcore . utils . urlunparse ( parsed_base_uri ) , config_path = CONFIG_PATH ) # If we are creating a symlink dataset we need to set the symlink_path # attribute on the storage broker. if symlink_path : symlink_abspath = os . path . abspath ( symlink_path ) proto_dataset . _storage_broker . symlink_path = symlink_abspath try : proto_dataset . create ( ) except dtoolcore . storagebroker . StorageBrokerOSError as err : raise click . UsageError ( str ( err ) ) proto_dataset . put_readme ( "" ) if quiet : click . secho ( proto_dataset . uri ) else : # Give the user some feedback and hints on what to do next. click . secho ( "Created proto dataset " , nl = False , fg = "green" ) click . secho ( proto_dataset . uri ) click . secho ( "Next steps: " ) step = 1 if parsed_base_uri . scheme != "symlink" : click . secho ( "{}. Add raw data, eg:" . format ( step ) ) click . secho ( " dtool add item my_file.txt {}" . format ( proto_dataset . uri ) , fg = "cyan" ) if parsed_base_uri . scheme == "file" : # Find the abspath of the data directory for user feedback. data_path = proto_dataset . _storage_broker . _data_abspath click . secho ( " Or use your system commands, e.g: " ) click . secho ( " mv my_data_directory {}/" . format ( data_path ) , fg = "cyan" ) step = step + 1 click . secho ( "{}. Add descriptive metadata, e.g: " . format ( step ) ) click . secho ( " dtool readme interactive {}" . format ( proto_dataset . uri ) , fg = "cyan" ) step = step + 1 click . secho ( "{}. Convert the proto dataset into a dataset: " . format ( step ) ) click . secho ( " dtool freeze {}" . format ( proto_dataset . uri ) , fg = "cyan" )
6904	def hms_to_decimal ( hours , minutes , seconds , returndeg = True ) : if hours > 24 : return None else : dec_hours = fabs ( hours ) + fabs ( minutes ) / 60.0 + fabs ( seconds ) / 3600.0 if returndeg : dec_deg = dec_hours * 15.0 if dec_deg < 0 : dec_deg = dec_deg + 360.0 dec_deg = dec_deg % 360.0 return dec_deg else : return dec_hours
2953	def container_id ( self , name ) : container = self . _containers . get ( name , None ) if not container is None : return container . get ( 'id' , None ) return None
5213	def intraday ( ticker , dt , session = '' , * * kwargs ) -> pd . DataFrame : from xbbg . core import intervals cur_data = bdib ( ticker = ticker , dt = dt , typ = kwargs . get ( 'typ' , 'TRADE' ) ) if cur_data . empty : return pd . DataFrame ( ) fmt = '%H:%M:%S' ss = intervals . SessNA ref = kwargs . get ( 'ref' , None ) exch = pd . Series ( ) if ref is None else const . exch_info ( ticker = ref ) if session : ss = intervals . get_interval ( ticker = kwargs . get ( 'ref' , ticker ) , session = session ) start_time = kwargs . get ( 'start_time' , None ) end_time = kwargs . get ( 'end_time' , None ) if ss != intervals . SessNA : start_time = pd . Timestamp ( ss . start_time ) . strftime ( fmt ) end_time = pd . Timestamp ( ss . end_time ) . strftime ( fmt ) if start_time and end_time : kw = dict ( start_time = start_time , end_time = end_time ) if not exch . empty : cur_tz = cur_data . index . tz res = cur_data . tz_convert ( exch . tz ) . between_time ( * * kw ) if kwargs . get ( 'keep_tz' , False ) : res = res . tz_convert ( cur_tz ) return pd . DataFrame ( res ) return pd . DataFrame ( cur_data . between_time ( * * kw ) ) return cur_data
10404	def canonical_statistics_dtype ( spanning_cluster = True ) : fields = list ( ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability' , 'float64' ) , ] ) fields . extend ( [ ( 'max_cluster_size' , 'float64' ) , ( 'moments' , '(5,)float64' ) , ] ) return _ndarray_dtype ( fields )
5773	def dsa_sign ( private_key , data , hash_algorithm ) : if private_key . algorithm != 'dsa' : raise ValueError ( 'The key specified is not a DSA private key' ) return _sign ( private_key , data , hash_algorithm )
13069	def r_collection ( self , objectId , lang = None ) : collection = self . resolver . getMetadata ( objectId ) return { "template" : "main::collection.html" , "collections" : { "current" : { "label" : str ( collection . get_label ( lang ) ) , "id" : collection . id , "model" : str ( collection . model ) , "type" : str ( collection . type ) , } , "members" : self . make_members ( collection , lang = lang ) , "parents" : self . make_parents ( collection , lang = lang ) } , }
13667	def execute ( self , command , timeout = None ) : try : self . channel = self . ssh . get_transport ( ) . open_session ( ) except paramiko . SSHException as e : self . unknown ( "Create channel error: %s" % e ) try : self . channel . settimeout ( self . args . timeout if not timeout else timeout ) except socket . timeout as e : self . unknown ( "Settimeout for channel error: %s" % e ) try : self . logger . debug ( "command: {}" . format ( command ) ) self . channel . exec_command ( command ) except paramiko . SSHException as e : self . unknown ( "Execute command error: %s" % e ) try : self . stdin = self . channel . makefile ( 'wb' , - 1 ) self . stderr = map ( string . strip , self . channel . makefile_stderr ( 'rb' , - 1 ) . readlines ( ) ) self . stdout = map ( string . strip , self . channel . makefile ( 'rb' , - 1 ) . readlines ( ) ) except Exception as e : self . unknown ( "Get result error: %s" % e ) try : self . status = self . channel . recv_exit_status ( ) except paramiko . SSHException as e : self . unknown ( "Get return code error: %s" % e ) else : if self . status != 0 : self . unknown ( "Return code: %d , stderr: %s" % ( self . status , self . errors ) ) else : return self . stdout finally : self . logger . debug ( "Execute command finish." )
13514	def froude_number ( speed , length ) : g = 9.80665 # conventional standard value m/s^2 Fr = speed / np . sqrt ( g * length ) return Fr
12680	def get_formatted_messages ( self , formats , label , context ) : format_templates = { } for fmt in formats : # conditionally turn off autoescaping for .txt extensions in format if fmt . endswith ( ".txt" ) : context . autoescape = False format_templates [ fmt ] = render_to_string ( ( "notification/%s/%s" % ( label , fmt ) , "notification/%s" % fmt ) , context_instance = context ) return format_templates
3799	def Bahadori_liquid ( T , M ) : A = [ - 6.48326E-2 , 2.715015E-3 , - 1.08580E-5 , 9.853917E-9 ] B = [ 1.565612E-2 , - 1.55833E-4 , 5.051114E-7 , - 4.68030E-10 ] C = [ - 1.80304E-4 , 1.758693E-6 , - 5.55224E-9 , 5.201365E-12 ] D = [ 5.880443E-7 , - 5.65898E-9 , 1.764384E-11 , - 1.65944E-14 ] X , Y = M , T a = A [ 0 ] + B [ 0 ] * X + C [ 0 ] * X ** 2 + D [ 0 ] * X ** 3 b = A [ 1 ] + B [ 1 ] * X + C [ 1 ] * X ** 2 + D [ 1 ] * X ** 3 c = A [ 2 ] + B [ 2 ] * X + C [ 2 ] * X ** 2 + D [ 2 ] * X ** 3 d = A [ 3 ] + B [ 3 ] * X + C [ 3 ] * X ** 2 + D [ 3 ] * X ** 3 return a + b * Y + c * Y ** 2 + d * Y ** 3
4803	def raises ( self , ex ) : if not callable ( self . val ) : raise TypeError ( 'val must be callable' ) if not issubclass ( ex , BaseException ) : raise TypeError ( 'given arg must be exception' ) return AssertionBuilder ( self . val , self . description , self . kind , ex )
3992	def get_nginx_configuration_spec ( port_spec_dict , docker_bridge_ip ) : nginx_http_config , nginx_stream_config = "" , "" for port_spec in port_spec_dict [ 'nginx' ] : if port_spec [ 'type' ] == 'http' : nginx_http_config += _nginx_http_spec ( port_spec , docker_bridge_ip ) elif port_spec [ 'type' ] == 'stream' : nginx_stream_config += _nginx_stream_spec ( port_spec , docker_bridge_ip ) return { 'http' : nginx_http_config , 'stream' : nginx_stream_config }
1773	def pop ( cpu , size ) : assert size in ( 16 , cpu . address_bit_size ) base , _ , _ = cpu . get_descriptor ( cpu . SS ) address = cpu . STACK + base value = cpu . read_int ( address , size ) cpu . STACK = cpu . STACK + size // 8 return value
5559	def _unflatten_tree ( flat ) : tree = { } for key , value in flat . items ( ) : path = key . split ( "/" ) # we are at the end of a branch if len ( path ) == 1 : tree [ key ] = value # there are more branches else : # create new dict if not path [ 0 ] in tree : tree [ path [ 0 ] ] = _unflatten_tree ( { "/" . join ( path [ 1 : ] ) : value } ) # add keys to existing dict else : branch = _unflatten_tree ( { "/" . join ( path [ 1 : ] ) : value } ) if not path [ 1 ] in tree [ path [ 0 ] ] : tree [ path [ 0 ] ] [ path [ 1 ] ] = branch [ path [ 1 ] ] else : tree [ path [ 0 ] ] [ path [ 1 ] ] . update ( branch [ path [ 1 ] ] ) return tree
11346	def handle_endtag ( self , tag ) : if tag in self . mathml_elements : self . fed . append ( "</{0}>" . format ( tag ) )
4331	def loudness ( self , gain_db = - 10.0 , reference_level = 65.0 ) : if not is_number ( gain_db ) : raise ValueError ( 'gain_db must be a number.' ) if not is_number ( reference_level ) : raise ValueError ( 'reference_level must be a number' ) if reference_level > 75 or reference_level < 50 : raise ValueError ( 'reference_level must be between 50 and 75' ) effect_args = [ 'loudness' , '{:f}' . format ( gain_db ) , '{:f}' . format ( reference_level ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'loudness' ) return self
8035	def summarize ( text , char_limit , sentence_filter = None , debug = False ) : debug_info = { } sents = list ( tools . sent_splitter_ja ( text ) ) words_list = [ # pulp variables should be utf-8 encoded w . encode ( 'utf-8' ) for s in sents for w in tools . word_segmenter_ja ( s ) ] tf = collections . Counter ( ) for words in words_list : for w in words : tf [ w ] += 1.0 if sentence_filter is not None : valid_indices = [ i for i , s in enumerate ( sents ) if sentence_filter ( s ) ] sents = [ sents [ i ] for i in valid_indices ] words_list = [ words_list [ i ] for i in valid_indices ] sent_ids = [ str ( i ) for i in range ( len ( sents ) ) ] # sentence id sent_id2len = dict ( ( id_ , len ( s ) ) for id_ , s in zip ( sent_ids , sents ) ) # c word_contain = dict ( ) # a for id_ , words in zip ( sent_ids , words_list ) : word_contain [ id_ ] = collections . defaultdict ( lambda : 0 ) for w in words : word_contain [ id_ ] [ w ] = 1 prob = pulp . LpProblem ( 'summarize' , pulp . LpMaximize ) # x sent_vars = pulp . LpVariable . dicts ( 'sents' , sent_ids , 0 , 1 , pulp . LpBinary ) # z word_vars = pulp . LpVariable . dicts ( 'words' , tf . keys ( ) , 0 , 1 , pulp . LpBinary ) # first, set objective function: sum(w*z) prob += pulp . lpSum ( [ tf [ w ] * word_vars [ w ] for w in tf ] ) # next, add constraints # limit summary length: sum(c*x) <= K prob += pulp . lpSum ( [ sent_id2len [ id_ ] * sent_vars [ id_ ] for id_ in sent_ids ] ) <= char_limit , 'lengthRequirement' # for each term, sum(a*x) <= z for w in tf : prob += pulp . lpSum ( [ word_contain [ id_ ] [ w ] * sent_vars [ id_ ] for id_ in sent_ids ] ) >= word_vars [ w ] , 'z:{}' . format ( w ) prob . solve ( ) # print("Status:", pulp.LpStatus[prob.status]) sent_indices = [ ] for v in prob . variables ( ) : # print v.name, "=", v.varValue if v . name . startswith ( 'sents' ) and v . varValue == 1 : sent_indices . append ( int ( v . name . split ( '_' ) [ - 1 ] ) ) return [ sents [ i ] for i in sent_indices ] , debug_info
3646	def sendToWatchlist ( self , trade_id ) : method = 'PUT' url = 'watchlist' data = { 'auctionInfo' : [ { 'id' : trade_id } ] } return self . __request__ ( method , url , data = json . dumps ( data ) )
317	def perf_stats_bootstrap ( returns , factor_returns = None , return_stats = True , * * kwargs ) : bootstrap_values = OrderedDict ( ) for stat_func in SIMPLE_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns ) if factor_returns is not None : for stat_func in FACTOR_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns , factor_returns = factor_returns ) bootstrap_values = pd . DataFrame ( bootstrap_values ) if return_stats : stats = bootstrap_values . apply ( calc_distribution_stats ) return stats . T [ [ 'mean' , 'median' , '5%' , '95%' ] ] else : return bootstrap_values
9307	def get_canonical_request ( self , req , cano_headers , signed_headers ) : url = urlparse ( req . url ) path = self . amz_cano_path ( url . path ) # AWS handles "extreme" querystrings differently to urlparse # (see post-vanilla-query-nonunreserved test in aws_testsuite) split = req . url . split ( '?' , 1 ) qs = split [ 1 ] if len ( split ) == 2 else '' qs = self . amz_cano_querystring ( qs ) payload_hash = req . headers [ 'x-amz-content-sha256' ] req_parts = [ req . method . upper ( ) , path , qs , cano_headers , signed_headers , payload_hash ] cano_req = '\n' . join ( req_parts ) return cano_req
8450	def has_env_vars ( * env_vars ) : for env_var in env_vars : if not os . environ . get ( env_var ) : msg = ( 'Must set {} environment variable. View docs for setting up environment at {}' ) . format ( env_var , temple . constants . TEMPLE_DOCS_URL ) raise temple . exceptions . InvalidEnvironmentError ( msg )
396	def choice_action_by_probs ( probs = ( 0.5 , 0.5 ) , action_list = None ) : if action_list is None : n_action = len ( probs ) action_list = np . arange ( n_action ) else : if len ( action_list ) != len ( probs ) : raise Exception ( "number of actions should equal to number of probabilities." ) return np . random . choice ( action_list , p = probs )
476	def data_to_token_ids ( data_path , target_path , vocabulary_path , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br"\d" ) ) : if not gfile . Exists ( target_path ) : tl . logging . info ( "Tokenizing data in %s" % data_path ) vocab , _ = initialize_vocabulary ( vocabulary_path ) with gfile . GFile ( data_path , mode = "rb" ) as data_file : with gfile . GFile ( target_path , mode = "w" ) as tokens_file : counter = 0 for line in data_file : counter += 1 if counter % 100000 == 0 : tl . logging . info ( " tokenizing line %d" % counter ) token_ids = sentence_to_token_ids ( line , vocab , tokenizer , normalize_digits , UNK_ID = UNK_ID , _DIGIT_RE = _DIGIT_RE ) tokens_file . write ( " " . join ( [ str ( tok ) for tok in token_ids ] ) + "\n" ) else : tl . logging . info ( "Target path %s exists" % target_path )
5305	def detect_color_support ( env ) : # noqa if env . get ( 'COLORFUL_DISABLE' , '0' ) == '1' : return NO_COLORS if env . get ( 'COLORFUL_FORCE_8_COLORS' , '0' ) == '1' : return ANSI_8_COLORS if env . get ( 'COLORFUL_FORCE_16_COLORS' , '0' ) == '1' : return ANSI_16_COLORS if env . get ( 'COLORFUL_FORCE_256_COLORS' , '0' ) == '1' : return ANSI_256_COLORS if env . get ( 'COLORFUL_FORCE_TRUE_COLORS' , '0' ) == '1' : return TRUE_COLORS # if we are not a tty if not sys . stdout . isatty ( ) : return NO_COLORS colorterm_env = env . get ( 'COLORTERM' ) if colorterm_env : if colorterm_env in { 'truecolor' , '24bit' } : return TRUE_COLORS if colorterm_env in { '8bit' } : return ANSI_256_COLORS termprog_env = env . get ( 'TERM_PROGRAM' ) if termprog_env : if termprog_env in { 'iTerm.app' , 'Hyper' } : return TRUE_COLORS if termprog_env in { 'Apple_Terminal' } : return ANSI_256_COLORS term_env = env . get ( 'TERM' ) if term_env : if term_env in { 'screen-256' , 'screen-256color' , 'xterm-256' , 'xterm-256color' } : return ANSI_256_COLORS if term_env in { 'screen' , 'xterm' , 'vt100' , 'color' , 'ansi' , 'cygwin' , 'linux' } : return ANSI_16_COLORS if colorterm_env : # if there was no match with $TERM either but we # had one with $COLORTERM, we use it! return ANSI_16_COLORS return ANSI_8_COLORS
29	def get_session ( config = None ) : sess = tf . get_default_session ( ) if sess is None : sess = make_session ( config = config , make_default = True ) return sess
12624	def recursive_dir_match ( folder_path , regex = '' ) : outlist = [ ] for root , dirs , files in os . walk ( folder_path ) : outlist . extend ( [ op . join ( root , f ) for f in dirs if re . match ( regex , f ) ] ) return outlist
256	def gen_round_trip_stats ( round_trips ) : stats = { } stats [ 'pnl' ] = agg_all_long_short ( round_trips , 'pnl' , PNL_STATS ) stats [ 'summary' ] = agg_all_long_short ( round_trips , 'pnl' , SUMMARY_STATS ) stats [ 'duration' ] = agg_all_long_short ( round_trips , 'duration' , DURATION_STATS ) stats [ 'returns' ] = agg_all_long_short ( round_trips , 'returns' , RETURN_STATS ) stats [ 'symbols' ] = round_trips . groupby ( 'symbol' ) [ 'returns' ] . agg ( RETURN_STATS ) . T return stats
7465	def _parse_01 ( ofiles , individual = False ) : ## parse results from outfiles cols = [ ] dats = [ ] for ofile in ofiles : ## parse file with open ( ofile ) as infile : dat = infile . read ( ) lastbits = dat . split ( ".mcmc.txt\n\n" ) [ 1 : ] results = lastbits [ 0 ] . split ( "\n\n" ) [ 0 ] . split ( ) ## get shape from ... shape = ( ( ( len ( results ) - 3 ) / 4 ) , 4 ) dat = np . array ( results [ 3 : ] ) . reshape ( shape ) cols . append ( dat [ : , 3 ] . astype ( float ) ) if not individual : ## get mean results across reps cols = np . array ( cols ) cols = cols . sum ( axis = 0 ) / len ( ofiles ) #10. dat [ : , 3 ] = cols . astype ( str ) ## format as a DF df = pd . DataFrame ( dat [ : , 1 : ] ) df . columns = [ "delim" , "prior" , "posterior" ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) df [ "nspecies" ] = nspecies return df else : ## get mean results across reps #return cols res = [ ] for i in xrange ( len ( cols ) ) : x = dat x [ : , 3 ] = cols [ i ] . astype ( str ) x = pd . DataFrame ( x [ : , 1 : ] ) x . columns = [ 'delim' , 'prior' , 'posterior' ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) x [ "nspecies" ] = nspecies res . append ( x ) return res
10883	def patch_docs ( subclass , superclass ) : funcs0 = inspect . getmembers ( subclass , predicate = inspect . ismethod ) funcs1 = inspect . getmembers ( superclass , predicate = inspect . ismethod ) funcs1 = [ f [ 0 ] for f in funcs1 ] for name , func in funcs0 : if name . startswith ( '_' ) : continue if name not in funcs1 : continue if func . __doc__ is None : func = getattr ( subclass , name ) func . __func__ . __doc__ = getattr ( superclass , name ) . __func__ . __doc__
6374	def pr_lmean ( self ) : precision = self . precision ( ) recall = self . recall ( ) if not precision or not recall : return 0.0 elif precision == recall : return precision return ( precision - recall ) / ( math . log ( precision ) - math . log ( recall ) )
13434	def _setup_index ( index ) : index = int ( index ) if index > 0 : index -= 1 elif index == 0 : # Zero indicies should not be allowed by default. raise ValueError return index
3190	def update ( self , list_id , segment_id , data ) : self . list_id = list_id self . segment_id = segment_id if 'name' not in data : raise KeyError ( 'The list segment must have a name' ) return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'segments' , segment_id ) , data = data )
12757	def add_torques ( self , torques ) : j = 0 for joint in self . joints : joint . add_torques ( list ( torques [ j : j + joint . ADOF ] ) + [ 0 ] * ( 3 - joint . ADOF ) ) j += joint . ADOF
9632	def render_subject ( self , context ) : rendered = self . subject_template . render ( unescape ( context ) ) return rendered . strip ( )
4660	def finalizeOp ( self , ops , account , permission , * * kwargs ) : if "append_to" in kwargs and kwargs [ "append_to" ] : if self . proposer : log . warning ( "You may not use append_to and self.proposer at " "the same time. Append new_proposal(..) instead" ) # Append to the append_to and return append_to = kwargs [ "append_to" ] parent = append_to . get_parent ( ) assert isinstance ( append_to , ( self . transactionbuilder_class , self . proposalbuilder_class ) ) append_to . appendOps ( ops ) # Add the signer to the buffer so we sign the tx properly if isinstance ( append_to , self . proposalbuilder_class ) : parent . appendSigner ( append_to . proposer , permission ) else : parent . appendSigner ( account , permission ) # This returns as we used append_to, it does NOT broadcast, or sign return append_to . get_parent ( ) elif self . proposer : # Legacy proposer mode! proposal = self . proposal ( ) proposal . set_proposer ( self . proposer ) proposal . set_expiration ( self . proposal_expiration ) proposal . set_review ( self . proposal_review ) proposal . appendOps ( ops ) # Go forward to see what the other options do ... else : # Append tot he default buffer self . txbuffer . appendOps ( ops ) # The API that obtains the fee only allows to specify one particular # fee asset for all operations in that transaction even though the # blockchain itself could allow to pay multiple operations with # different fee assets. if "fee_asset" in kwargs and kwargs [ "fee_asset" ] : self . txbuffer . set_fee_asset ( kwargs [ "fee_asset" ] ) # Add signing information, signer, sign and optionally broadcast if self . unsigned : # In case we don't want to sign anything self . txbuffer . addSigningInformation ( account , permission ) return self . txbuffer elif self . bundle : # In case we want to add more ops to the tx (bundle) self . txbuffer . appendSigner ( account , permission ) return self . txbuffer . json ( ) else : # default behavior: sign + broadcast self . txbuffer . appendSigner ( account , permission ) self . txbuffer . sign ( ) return self . txbuffer . broadcast ( )
2029	def CALLDATACOPY ( self , mem_offset , data_offset , size ) : if issymbolic ( size ) : if solver . can_be_true ( self . _constraints , size <= len ( self . data ) + 32 ) : self . constraints . add ( size <= len ( self . data ) + 32 ) raise ConcretizeArgument ( 3 , policy = 'SAMPLED' ) if issymbolic ( data_offset ) : if solver . can_be_true ( self . _constraints , data_offset == self . _used_calldata_size ) : self . constraints . add ( data_offset == self . _used_calldata_size ) raise ConcretizeArgument ( 2 , policy = 'SAMPLED' ) #account for calldata usage self . _use_calldata ( data_offset , size ) self . _allocate ( mem_offset , size ) for i in range ( size ) : try : c = Operators . ITEBV ( 8 , data_offset + i < len ( self . data ) , Operators . ORD ( self . data [ data_offset + i ] ) , 0 ) except IndexError : # data_offset + i is concrete and outside data c = 0 self . _store ( mem_offset + i , c )
13568	def selected_course ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : course = Course . get_selected ( ) return func ( course , * args , * * kwargs ) return inner
9246	def compound_changelog ( self ) : self . fetch_and_filter_tags ( ) tags_sorted = self . sort_tags_by_date ( self . filtered_tags ) self . filtered_tags = tags_sorted self . fetch_and_filter_issues_and_pr ( ) log = str ( self . options . frontmatter ) if self . options . frontmatter else u"" log += u"{0}\n\n" . format ( self . options . header ) if self . options . unreleased_only : log += self . generate_unreleased_section ( ) else : log += self . generate_log_for_all_tags ( ) try : with open ( self . options . base ) as fh : log += fh . read ( ) except ( TypeError , IOError ) : pass return log
6311	def from_single ( cls , meta : ProgramDescription , source : str ) : instance = cls ( meta ) instance . vertex_source = ShaderSource ( VERTEX_SHADER , meta . path or meta . vertex_shader , source ) if GEOMETRY_SHADER in source : instance . geometry_source = ShaderSource ( GEOMETRY_SHADER , meta . path or meta . geometry_shader , source , ) if FRAGMENT_SHADER in source : instance . fragment_source = ShaderSource ( FRAGMENT_SHADER , meta . path or meta . fragment_shader , source , ) if TESS_CONTROL_SHADER in source : instance . tess_control_source = ShaderSource ( TESS_CONTROL_SHADER , meta . path or meta . tess_control_shader , source , ) if TESS_EVALUATION_SHADER in source : instance . tess_evaluation_source = ShaderSource ( TESS_EVALUATION_SHADER , meta . path or meta . tess_evaluation_shader , source , ) return instance
12230	def unpatch_locals ( depth = 3 ) : for name , locals_dict in traverse_local_prefs ( depth ) : if isinstance ( locals_dict [ name ] , PatchedLocal ) : locals_dict [ name ] = locals_dict [ name ] . val del get_frame_locals ( depth ) [ __PATCHED_LOCALS_SENTINEL ]
8928	def prep ( ctx , commit = True ) : cfg = config . load ( ) scm = scm_provider ( cfg . project_root , commit = commit , ctx = ctx ) # Check for uncommitted changes if not scm . workdir_is_clean ( ) : notify . failure ( "You have uncommitted changes, please commit or stash them!" ) # TODO Check that changelog entry carries the current date # Rewrite 'setup.cfg' setup_cfg = cfg . rootjoin ( 'setup.cfg' ) if os . path . exists ( setup_cfg ) : with io . open ( setup_cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if any ( line . startswith ( i ) for i in ( 'tag_build' , 'tag_date' ) ) : data [ i ] = '#' + data [ i ] changed = True if changed and commit : notify . info ( "Rewriting 'setup.cfg'..." ) with io . open ( setup_cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) scm . add_file ( 'setup.cfg' ) elif changed : notify . warning ( "WOULD rewrite 'setup.cfg', but --no-commit was passed" ) else : notify . warning ( "Cannot rewrite 'setup.cfg', none found!" ) # Update metadata and command stubs ctx . run ( 'python setup.py -q develop -U' ) # Build a clean dist and check version number version = capture ( 'python setup.py --version' ) ctx . run ( 'invoke clean --all build --docs release.dist' ) for distfile in os . listdir ( 'dist' ) : trailer = distfile . split ( '-' + version ) [ 1 ] trailer , _ = os . path . splitext ( trailer ) if trailer and trailer [ 0 ] not in '.-' : notify . failure ( "The version found in 'dist' seems to be" " a pre-release one! [{}{}]" . format ( version , trailer ) ) # Commit changes and tag the release scm . commit ( ctx . rituals . release . commit . message . format ( version = version ) ) scm . tag ( ctx . rituals . release . tag . name . format ( version = version ) , ctx . rituals . release . tag . message . format ( version = version ) )
8616	def _b ( s , encoding = 'utf-8' ) : if six . PY2 : # This is Python2 if isinstance ( s , str ) : return s elif isinstance ( s , unicode ) : # noqa, pylint: disable=undefined-variable return s . encode ( encoding ) else : # And this is Python3 if isinstance ( s , bytes ) : return s elif isinstance ( s , str ) : return s . encode ( encoding ) raise TypeError ( "Invalid argument %r for _b()" % ( s , ) )
146	def on ( self , image ) : shape = normalize_shape ( image ) if shape [ 0 : 2 ] == self . shape [ 0 : 2 ] : return self . deepcopy ( ) polygons = [ poly . project ( self . shape , shape ) for poly in self . polygons ] # TODO use deepcopy() here return PolygonsOnImage ( polygons , shape )
8856	def on_current_tab_changed ( self ) : self . menuEdit . clear ( ) self . menuModes . clear ( ) self . menuPanels . clear ( ) editor = self . tabWidget . current_widget ( ) self . menuEdit . setEnabled ( editor is not None ) self . menuModes . setEnabled ( editor is not None ) self . menuPanels . setEnabled ( editor is not None ) self . actionSave . setEnabled ( editor is not None ) self . actionSave_as . setEnabled ( editor is not None ) self . actionConfigure_run . setEnabled ( editor is not None ) self . actionRun . setEnabled ( editor is not None ) if editor is not None : self . setup_mnu_edit ( editor ) self . setup_mnu_modes ( editor ) self . setup_mnu_panels ( editor ) self . widgetOutline . set_editor ( editor ) self . _update_status_bar ( editor )
11128	def ensure_str ( value ) : if isinstance ( value , six . string_types ) : return value else : return six . text_type ( value )
3733	def charge ( self ) : try : return self . _charge except AttributeError : self . _charge = charge_from_formula ( self . formula ) return self . _charge
7073	def magbin_varind_gridsearch_worker ( task ) : simbasedir , gridpoint , magbinmedian = task try : res = get_recovered_variables_for_magbin ( simbasedir , magbinmedian , stetson_stdev_min = gridpoint [ 0 ] , inveta_stdev_min = gridpoint [ 1 ] , iqr_stdev_min = gridpoint [ 2 ] , statsonly = True ) return res except Exception as e : LOGEXCEPTION ( 'failed to get info for %s' % gridpoint ) return None
10191	def get_anonymization_salt ( ts ) : salt_key = 'stats:salt:{}' . format ( ts . date ( ) . isoformat ( ) ) salt = current_cache . get ( salt_key ) if not salt : salt_bytes = os . urandom ( 32 ) salt = b64encode ( salt_bytes ) . decode ( 'utf-8' ) current_cache . set ( salt_key , salt , timeout = 60 * 60 * 24 ) return salt
7185	def copy_type_comments_to_annotations ( args ) : for arg in args . args : copy_type_comment_to_annotation ( arg ) if args . vararg : copy_type_comment_to_annotation ( args . vararg ) for arg in args . kwonlyargs : copy_type_comment_to_annotation ( arg ) if args . kwarg : copy_type_comment_to_annotation ( args . kwarg )
8101	def copy ( self , graph ) : g = styleguide ( graph ) g . order = self . order dict . __init__ ( g , [ ( k , v ) for k , v in self . iteritems ( ) ] ) return g
10394	def calculate_average_score_by_annotation ( graph : BELGraph , annotation : str , key : Optional [ str ] = None , runs : Optional [ int ] = None , use_tqdm : bool = False , ) -> Mapping [ str , float ] : candidate_mechanisms = generate_bioprocess_mechanisms ( graph , key = key ) #: {bp tuple: list of scores} scores : Mapping [ BaseEntity , Tuple ] = calculate_average_scores_on_subgraphs ( subgraphs = candidate_mechanisms , key = key , runs = runs , use_tqdm = use_tqdm , ) subgraph_bp : Mapping [ str , List [ BaseEntity ] ] = defaultdict ( list ) subgraphs : Mapping [ str , BELGraph ] = get_subgraphs_by_annotation ( graph , annotation ) for annotation_value , subgraph in subgraphs . items ( ) : subgraph_bp [ annotation_value ] . extend ( get_nodes_by_function ( subgraph , BIOPROCESS ) ) #: Pick the average by slicing with 0. Refer to :func:`calculate_average_score_on_subgraphs` return { annotation_value : np . average ( scores [ bp ] [ 0 ] for bp in bps ) for annotation_value , bps in subgraph_bp . items ( ) }
13459	def create_ical ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) # convert dates to datetimes. # when we change code to datetimes, we won't have to do this. start = event . start_date start = datetime . datetime ( start . year , start . month , start . day ) if event . end_date : end = event . end_date end = datetime . datetime ( end . year , end . month , end . day ) else : end = start cal = card_me . iCalendar ( ) cal . add ( 'method' ) . value = 'PUBLISH' vevent = cal . add ( 'vevent' ) vevent . add ( 'dtstart' ) . value = start vevent . add ( 'dtend' ) . value = end vevent . add ( 'dtstamp' ) . value = datetime . datetime . now ( ) vevent . add ( 'summary' ) . value = event . name response = HttpResponse ( cal . serialize ( ) , content_type = 'text/calendar' ) response [ 'Filename' ] = 'filename.ics' response [ 'Content-Disposition' ] = 'attachment; filename=filename.ics' return response
634	def destroySynapse ( self , synapse ) : self . _numSynapses -= 1 self . _removeSynapseFromPresynapticMap ( synapse ) synapse . segment . _synapses . remove ( synapse )
11757	def pl_true ( exp , model = { } ) : op , args = exp . op , exp . args if exp == TRUE : return True elif exp == FALSE : return False elif is_prop_symbol ( op ) : return model . get ( exp ) elif op == '~' : p = pl_true ( args [ 0 ] , model ) if p is None : return None else : return not p elif op == '|' : result = False for arg in args : p = pl_true ( arg , model ) if p is True : return True if p is None : result = None return result elif op == '&' : result = True for arg in args : p = pl_true ( arg , model ) if p is False : return False if p is None : result = None return result p , q = args if op == '>>' : return pl_true ( ~ p | q , model ) elif op == '<<' : return pl_true ( p | ~ q , model ) pt = pl_true ( p , model ) if pt is None : return None qt = pl_true ( q , model ) if qt is None : return None if op == '<=>' : return pt == qt elif op == '^' : return pt != qt else : raise ValueError , "illegal operator in logic expression" + str ( exp )
12556	def save_varlist ( filename , varnames , varlist ) : variables = { } for i , vn in enumerate ( varnames ) : variables [ vn ] = varlist [ i ] ExportData . save_variables ( filename , variables )
7690	def sonify ( annotation , sr = 22050 , duration = None , * * kwargs ) : length = None if duration is None : duration = annotation . duration if duration is not None : length = int ( duration * sr ) # If the annotation can be directly sonified, try that first if annotation . namespace in SONIFY_MAPPING : ann = coerce_annotation ( annotation , annotation . namespace ) return SONIFY_MAPPING [ annotation . namespace ] ( ann , sr = sr , length = length , * * kwargs ) for namespace , func in six . iteritems ( SONIFY_MAPPING ) : try : ann = coerce_annotation ( annotation , namespace ) return func ( ann , sr = sr , length = length , * * kwargs ) except NamespaceError : pass raise NamespaceError ( 'Unable to sonify annotation of namespace="{:s}"' . format ( annotation . namespace ) )
83	def Pepper ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : replacement01 = iap . ForceSign ( iap . Beta ( 0.5 , 0.5 ) - 0.5 , positive = False , mode = "invert" ) + 0.5 replacement = replacement01 * 255 if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = p , replacement = replacement , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
7625	def pattern ( ref , est , * * kwargs ) : namespace = 'pattern_jku' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_patterns = pattern_to_mireval ( ref ) est_patterns = pattern_to_mireval ( est ) return mir_eval . pattern . evaluate ( ref_patterns , est_patterns , * * kwargs )
9715	async def qtm_version ( self ) : return await asyncio . wait_for ( self . _protocol . send_command ( "qtmversion" ) , timeout = self . _timeout )
6226	def rot_state ( self , x , y ) : if self . last_x is None : self . last_x = x if self . last_y is None : self . last_y = y x_offset = self . last_x - x y_offset = self . last_y - y self . last_x = x self . last_y = y x_offset *= self . mouse_sensitivity y_offset *= self . mouse_sensitivity self . yaw -= x_offset self . pitch += y_offset if self . pitch > 85.0 : self . pitch = 85.0 if self . pitch < - 85.0 : self . pitch = - 85.0 self . _update_yaw_and_pitch ( )
7371	def main ( args_list = None ) : args = parse_args ( args_list ) binding_predictions = run_predictor ( args ) df = binding_predictions . to_dataframe ( ) logger . info ( '\n%s' , df ) if args . output_csv : df . to_csv ( args . output_csv , index = False ) print ( "Wrote: %s" % args . output_csv )
519	def _initPermConnected ( self ) : p = self . _synPermConnected + ( self . _synPermMax - self . _synPermConnected ) * self . _random . getReal64 ( ) # Ensure we don't have too much unnecessary precision. A full 64 bits of # precision causes numerical stability issues across platforms and across # implementations p = int ( p * 100000 ) / 100000.0 return p
12529	def load_command_table ( self , args ) : #pylint: disable=too-many-statements # Need an empty client for the select and upload operations with CommandSuperGroup ( __name__ , self , 'rcctl.custom_cluster#{}' ) as super_group : with super_group . group ( 'cluster' ) as group : group . command ( 'select' , 'select' ) with CommandSuperGroup ( __name__ , self , 'rcctl.custom_reliablecollections#{}' , client_factory = client_create ) as super_group : with super_group . group ( 'dictionary' ) as group : group . command ( 'query' , 'query_reliabledictionary' ) group . command ( 'execute' , 'execute_reliabledictionary' ) group . command ( 'schema' , 'get_reliabledictionary_schema' ) group . command ( 'list' , 'get_reliabledictionary_list' ) group . command ( 'type-schema' , 'get_reliabledictionary_type_schema' ) with ArgumentsContext ( self , 'dictionary' ) as ac : ac . argument ( 'application_name' , options_list = [ '--application-name' , '-a' ] ) ac . argument ( 'service_name' , options_list = [ '--service-name' , '-s' ] ) ac . argument ( 'dictionary_name' , options_list = [ '--dictionary-name' , '-d' ] ) ac . argument ( 'output_file' , options_list = [ '--output-file' , '-out' ] ) ac . argument ( 'input_file' , options_list = [ '--input-file' , '-in' ] ) ac . argument ( 'query_string' , options_list = [ '--query-string' , '-q' ] ) ac . argument ( 'type_name' , options_list = [ '--type-name' , '-t' ] ) return OrderedDict ( self . command_table )
7538	def getassembly ( args , parsedict ) : ## Creating an assembly with a full path in the name will "work" ## but it is potentially dangerous, so here we have assembly_name ## and assembly_file, name is used for creating new in cwd, file is ## used for loading existing. ## ## Be nice if the user includes the extension. #project_dir = ip.core.assembly._expander(parsedict['1']) #assembly_name = parsedict['0'] project_dir = ip . core . assembly . _expander ( parsedict [ 'project_dir' ] ) assembly_name = parsedict [ 'assembly_name' ] assembly_file = os . path . join ( project_dir , assembly_name ) ## Assembly creation will handle error checking on ## the format of the assembly_name ## make sure the working directory exists. if not os . path . exists ( project_dir ) : os . mkdir ( project_dir ) try : ## If 1 and force then go ahead and create a new assembly if ( '1' in args . steps ) and args . force : data = ip . Assembly ( assembly_name , cli = True ) else : data = ip . load_json ( assembly_file , cli = True ) data . _cli = True except IPyradWarningExit as _ : ## if no assembly is found then go ahead and make one if '1' not in args . steps : raise IPyradWarningExit ( " Error: You must first run step 1 on the assembly: {}" . format ( assembly_file ) ) else : ## create a new assembly object data = ip . Assembly ( assembly_name , cli = True ) ## for entering some params... for param in parsedict : ## trap assignment of assembly_name since it is immutable. if param == "assembly_name" : ## Raise error if user tried to change assembly name if parsedict [ param ] != data . name : data . set_params ( param , parsedict [ param ] ) else : ## all other params should be handled by set_params try : data . set_params ( param , parsedict [ param ] ) except IndexError as _ : print ( " Malformed params file: {}" . format ( args . params ) ) print ( " Bad parameter {} - {}" . format ( param , parsedict [ param ] ) ) sys . exit ( - 1 ) return data
5440	def get_variable_name ( self , name ) : if not name : name = '%s%s' % ( self . _auto_prefix , self . _auto_index ) self . _auto_index += 1 return name
3771	def mixing_logarithmic ( fracs , props ) : if not none_and_length_check ( [ fracs , props ] ) : return None return exp ( sum ( frac * log ( prop ) for frac , prop in zip ( fracs , props ) ) )
11406	def records_identical ( rec1 , rec2 , skip_005 = True , ignore_field_order = False , ignore_subfield_order = False , ignore_duplicate_subfields = False , ignore_duplicate_controlfields = False ) : rec1_keys = set ( rec1 . keys ( ) ) rec2_keys = set ( rec2 . keys ( ) ) if skip_005 : rec1_keys . discard ( "005" ) rec2_keys . discard ( "005" ) if rec1_keys != rec2_keys : return False for key in rec1_keys : if ignore_duplicate_controlfields and key . startswith ( '00' ) : if set ( field [ 3 ] for field in rec1 [ key ] ) != set ( field [ 3 ] for field in rec2 [ key ] ) : return False continue rec1_fields = rec1 [ key ] rec2_fields = rec2 [ key ] if len ( rec1_fields ) != len ( rec2_fields ) : # They already differs in length... return False if ignore_field_order : # We sort the fields, first by indicators and then by anything else rec1_fields = sorted ( rec1_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 3 ] , elem [ 0 ] ) ) rec2_fields = sorted ( rec2_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 3 ] , elem [ 0 ] ) ) else : # We sort the fields, first by indicators, then by global position # and then by anything else rec1_fields = sorted ( rec1_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 4 ] , elem [ 3 ] , elem [ 0 ] ) ) rec2_fields = sorted ( rec2_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 4 ] , elem [ 3 ] , elem [ 0 ] ) ) for field1 , field2 in zip ( rec1_fields , rec2_fields ) : if ignore_duplicate_subfields : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] or set ( field1 [ 0 ] ) != set ( field2 [ 0 ] ) : return False elif ignore_subfield_order : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] or sorted ( field1 [ 0 ] ) != sorted ( field2 [ 0 ] ) : return False elif field1 [ : 4 ] != field2 [ : 4 ] : return False return True
4891	def export ( self ) : # Fetch the consenting enrollment data, including the enterprise_customer_user. # Order by the course_id, to avoid fetching course API data more than we have to. enrollment_queryset = EnterpriseCourseEnrollment . objects . select_related ( 'enterprise_customer_user' ) . filter ( enterprise_customer_user__enterprise_customer = self . enterprise_customer , enterprise_customer_user__active = True , ) . order_by ( 'course_id' ) # Fetch course details from the Course API, and cache between calls. course_details = None for enterprise_enrollment in enrollment_queryset : course_id = enterprise_enrollment . course_id # Fetch course details from Courses API # pylint: disable=unsubscriptable-object if course_details is None or course_details [ 'course_id' ] != course_id : if self . course_api is None : self . course_api = CourseApiClient ( ) course_details = self . course_api . get_course_details ( course_id ) if course_details is None : # Course not found, so we have nothing to report. LOGGER . error ( "No course run details found for enrollment [%d]: [%s]" , enterprise_enrollment . pk , course_id ) continue consent = DataSharingConsent . objects . proxied_get ( username = enterprise_enrollment . enterprise_customer_user . username , course_id = enterprise_enrollment . course_id , enterprise_customer = enterprise_enrollment . enterprise_customer_user . enterprise_customer ) if not consent . granted or enterprise_enrollment . audit_reporting_disabled : continue # For instructor-paced courses, let the certificate determine course completion if course_details . get ( 'pacing' ) == 'instructor' : completed_date , grade , is_passing = self . _collect_certificate_data ( enterprise_enrollment ) # For self-paced courses, check the Grades API else : completed_date , grade , is_passing = self . _collect_grades_data ( enterprise_enrollment , course_details ) records = self . get_learner_data_records ( enterprise_enrollment = enterprise_enrollment , completed_date = completed_date , grade = grade , is_passing = is_passing , ) if records : # There are some cases where we won't receive a record from the above # method; right now, that should only happen if we have an Enterprise-linked # user for the integrated channel, and transmission of that user's # data requires an upstream user identifier that we don't have (due to a # failure of SSO or similar). In such a case, `get_learner_data_record` # would return None, and we'd simply skip yielding it here. for record in records : yield record
13774	def format ( self , record ) : record_fields = record . __dict__ . copy ( ) self . _set_exc_info ( record_fields ) event_name = 'default' if record_fields . get ( 'event_name' ) : event_name = record_fields . pop ( 'event_name' ) log_level = 'INFO' if record_fields . get ( 'log_level' ) : log_level = record_fields . pop ( 'log_level' ) [ record_fields . pop ( k ) for k in record_fields . keys ( ) if k not in self . fields ] defaults = self . defaults . copy ( ) fields = self . fields . copy ( ) fields . update ( record_fields ) filtered_fields = { } for k , v in fields . iteritems ( ) : if v is not None : filtered_fields [ k ] = v defaults . update ( { 'event_timestamp' : self . _get_now ( ) , 'event_name' : event_name , 'log_level' : log_level , 'fields' : filtered_fields } ) return json . dumps ( defaults , default = self . json_default )
3445	def add_linear_obj ( model ) : coefs = { } for rxn in find_boundary_types ( model , "exchange" ) : export = len ( rxn . reactants ) == 1 if export : coefs [ rxn . reverse_variable ] = 1 else : coefs [ rxn . forward_variable ] = 1 model . objective . set_linear_coefficients ( coefs ) model . objective . direction = "min"
2846	def close ( self ) : if self . _ctx is not None : ftdi . free ( self . _ctx ) self . _ctx = None
675	def _getMetrics ( self ) : metric = None if self . metrics is not None : metric = self . metrics ( self . _currentRecordIndex + 1 ) elif self . metricValue is not None : metric = self . metricValue else : raise RuntimeError ( 'No metrics or metric value specified for dummy model' ) return { self . _optimizeKeyPattern : metric }
1749	def mmap ( self , addr , size , perms , data_init = None , name = None ) : # If addr is NULL, the system determines where to allocate the region. assert addr is None or isinstance ( addr , int ) , 'Address shall be concrete' self . cpu . _publish ( 'will_map_memory' , addr , size , perms , None , None ) # address is rounded down to the nearest multiple of the allocation granularity if addr is not None : assert addr < self . memory_size , 'Address too big' addr = self . _floor ( addr ) # size value is rounded up to the next page boundary size = self . _ceil ( size ) # If zero search for a spot addr = self . _search ( size , addr ) # It should not be allocated for i in range ( self . _page ( addr ) , self . _page ( addr + size ) ) : assert i not in self . _page2map , 'Map already used' # Create the anonymous map m = AnonMap ( start = addr , size = size , perms = perms , data_init = data_init , name = name ) # Okay, ready to alloc self . _add ( m ) logger . debug ( 'New memory map @%x size:%x' , addr , size ) self . cpu . _publish ( 'did_map_memory' , addr , size , perms , None , None , addr ) return addr
12990	def overview ( ) : range_search = RangeSearch ( ) ranges = range_search . get_ranges ( ) if ranges : formatted_ranges = [ ] tags_lookup = { } for r in ranges : formatted_ranges . append ( { 'mask' : r . range } ) tags_lookup [ r . range ] = r . tags search = Host . search ( ) search = search . filter ( 'term' , status = 'up' ) search . aggs . bucket ( 'hosts' , 'ip_range' , field = 'address' , ranges = formatted_ranges ) response = search . execute ( ) print_line ( "{0:<18} {1:<6} {2}" . format ( "Range" , "Count" , "Tags" ) ) print_line ( "-" * 60 ) for entry in response . aggregations . hosts . buckets : print_line ( "{0:<18} {1:<6} {2}" . format ( entry . key , entry . doc_count , tags_lookup [ entry . key ] ) ) else : print_error ( "No ranges defined." )
6296	def instance ( self , program : moderngl . Program ) -> moderngl . VertexArray : vao = self . vaos . get ( program . glo ) if vao : return vao program_attributes = [ name for name , attr in program . _members . items ( ) if isinstance ( attr , moderngl . Attribute ) ] # Make sure all attributes are covered for attrib_name in program_attributes : # Ignore built in attributes for now if attrib_name . startswith ( 'gl_' ) : continue # Do we have a buffer mapping to this attribute? if not sum ( buffer . has_attribute ( attrib_name ) for buffer in self . buffers ) : raise VAOError ( "VAO {} doesn't have attribute {} for program {}" . format ( self . name , attrib_name , program . name ) ) vao_content = [ ] # Pick out the attributes we can actually map for buffer in self . buffers : content = buffer . content ( program_attributes ) if content : vao_content . append ( content ) # Any attribute left is not accounted for if program_attributes : for attrib_name in program_attributes : if attrib_name . startswith ( 'gl_' ) : continue raise VAOError ( "Did not find a buffer mapping for {}" . format ( [ n for n in program_attributes ] ) ) # Create the vao if self . _index_buffer : vao = context . ctx ( ) . vertex_array ( program , vao_content , self . _index_buffer , self . _index_element_size ) else : vao = context . ctx ( ) . vertex_array ( program , vao_content ) self . vaos [ program . glo ] = vao return vao
12505	def signed_session ( self , session = None ) : if session : session = super ( ClientCertAuthentication , self ) . signed_session ( session ) else : session = super ( ClientCertAuthentication , self ) . signed_session ( ) if self . cert is not None : session . cert = self . cert if self . ca_cert is not None : session . verify = self . ca_cert if self . no_verify : session . verify = False return session
3823	async def get_conversation ( self , get_conversation_request ) : response = hangouts_pb2 . GetConversationResponse ( ) await self . _pb_request ( 'conversations/getconversation' , get_conversation_request , response ) return response
9461	def conference_kick ( self , call_params ) : path = '/' + self . api_version + '/ConferenceKick/' method = 'POST' return self . request ( path , method , call_params )
9391	def aggregate_count_over_time ( self , metric_store , groupby_name , aggregate_timestamp ) : all_qps = metric_store [ 'qps' ] qps = all_qps [ groupby_name ] if aggregate_timestamp in qps : qps [ aggregate_timestamp ] += 1 else : qps [ aggregate_timestamp ] = 1 return None
11118	def get_parent_directory_info ( self , relativePath ) : relativePath = os . path . normpath ( relativePath ) # if root directory if relativePath in ( '' , '.' ) : return self , "relativePath is empty pointing to the repostitory itself." # split path parentDirPath , _ = os . path . split ( relativePath ) # get parent directory info return self . get_directory_info ( parentDirPath )
12613	def is_unique ( self , table_name , sample , unique_fields = None ) : try : eid = find_unique ( self . table ( table_name ) , sample = sample , unique_fields = unique_fields ) except : return False else : return eid is not None
4721	def trun_exit ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun:exit" ) rcode = 0 for hook in reversed ( trun [ "hooks" ] [ "exit" ] ) : # EXIT-hooks rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::exit { rcode: %r }" % rcode , rcode ) return rcode
7265	def run ( self , ctx ) : # Reverse engine assertion if needed if ctx . reverse : self . engine . reverse ( ) if self . engine . empty : raise AssertionError ( 'grappa: no assertions to run' ) try : # Run assertion in series and return error, if present return self . run_assertions ( ctx ) except Exception as _err : # Handle legit grappa internval errors if getattr ( _err , '__legit__' , False ) : raise _err # Otherwise render it return self . render_error ( ctx , _err )
3964	def stop_apps_or_services ( app_or_service_names = None , rm_containers = False ) : if app_or_service_names : log_to_client ( "Stopping the following apps or services: {}" . format ( ', ' . join ( app_or_service_names ) ) ) else : log_to_client ( "Stopping all running containers associated with Dusty" ) compose . stop_running_services ( app_or_service_names ) if rm_containers : compose . rm_containers ( app_or_service_names )
5402	def _get_prepare_env ( self , script , job_descriptor , inputs , outputs , mounts ) : # Add the _SCRIPT_REPR with the repr(script) contents # Add the _META_YAML_REPR with the repr(meta) contents # Add variables for directories that need to be created, for example: # DIR_COUNT: 2 # DIR_0: /mnt/data/input/gs/bucket/path1/ # DIR_1: /mnt/data/output/gs/bucket/path2 # List the directories in sorted order so that they are created in that # order. This is primarily to ensure that permissions are set as we create # each directory. # For example: # mkdir -m 777 -p /root/first/second # mkdir -m 777 -p /root/first # *may* not actually set 777 on /root/first docker_paths = sorted ( [ var . docker_path if var . recursive else os . path . dirname ( var . docker_path ) for var in inputs | outputs | mounts if var . value ] ) env = { _SCRIPT_VARNAME : repr ( script . value ) , _META_YAML_VARNAME : repr ( job_descriptor . to_yaml ( ) ) , 'DIR_COUNT' : str ( len ( docker_paths ) ) } for idx , path in enumerate ( docker_paths ) : env [ 'DIR_{}' . format ( idx ) ] = os . path . join ( providers_util . DATA_MOUNT_POINT , path ) return env
2496	def create_package_node ( self , package ) : package_node = BNode ( ) type_triple = ( package_node , RDF . type , self . spdx_namespace . Package ) self . graph . add ( type_triple ) # Handle optional fields: self . handle_pkg_optional_fields ( package , package_node ) # package name name_triple = ( package_node , self . spdx_namespace . name , Literal ( package . name ) ) self . graph . add ( name_triple ) # Package download location down_loc_node = ( package_node , self . spdx_namespace . downloadLocation , self . to_special_value ( package . download_location ) ) self . graph . add ( down_loc_node ) # Handle package verification verif_node = self . package_verif_node ( package ) verif_triple = ( package_node , self . spdx_namespace . packageVerificationCode , verif_node ) self . graph . add ( verif_triple ) # Handle concluded license conc_lic_node = self . license_or_special ( package . conc_lics ) conc_lic_triple = ( package_node , self . spdx_namespace . licenseConcluded , conc_lic_node ) self . graph . add ( conc_lic_triple ) # Handle declared license decl_lic_node = self . license_or_special ( package . license_declared ) decl_lic_triple = ( package_node , self . spdx_namespace . licenseDeclared , decl_lic_node ) self . graph . add ( decl_lic_triple ) # Package licenses from files licenses_from_files_nodes = map ( lambda el : self . license_or_special ( el ) , package . licenses_from_files ) lic_from_files_predicate = self . spdx_namespace . licenseInfoFromFiles lic_from_files_triples = [ ( package_node , lic_from_files_predicate , node ) for node in licenses_from_files_nodes ] for triple in lic_from_files_triples : self . graph . add ( triple ) # Copyright Text cr_text_node = self . to_special_value ( package . cr_text ) cr_text_triple = ( package_node , self . spdx_namespace . copyrightText , cr_text_node ) self . graph . add ( cr_text_triple ) # Handle files self . handle_package_has_file ( package , package_node ) return package_node
9963	def get_impls ( interfaces ) : if interfaces is None : return None elif isinstance ( interfaces , Mapping ) : return { name : interfaces [ name ] . _impl for name in interfaces } elif isinstance ( interfaces , Sequence ) : return [ interfaces . _impl for interfaces in interfaces ] else : return interfaces . _impl
2965	def _sm_start ( self , * args , * * kwargs ) : millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
7412	def run_tree_inference ( self , nexus , idx ) : ## create a tmpdir for this test tmpdir = tempfile . tempdir tmpfile = os . path . join ( tempfile . NamedTemporaryFile ( delete = False , prefix = str ( idx ) , dir = tmpdir , ) ) ## write nexus to tmpfile tmpfile . write ( nexus ) tmpfile . flush ( ) ## infer the tree rax = raxml ( name = str ( idx ) , data = tmpfile . name , workdir = tmpdir , N = 1 , T = 2 ) rax . run ( force = True , block = True , quiet = True ) ## clean up tmpfile . close ( ) ## return tree order order = get_order ( toytree . tree ( rax . trees . bestTree ) ) return "" . join ( order )
5567	def area_at_zoom ( self , zoom = None ) : if zoom is None : if not self . _cache_full_process_area : logger . debug ( "calculate process area ..." ) self . _cache_full_process_area = cascaded_union ( [ self . _area_at_zoom ( z ) for z in self . init_zoom_levels ] ) . buffer ( 0 ) return self . _cache_full_process_area else : if zoom not in self . init_zoom_levels : raise ValueError ( "zoom level not available with current configuration" ) return self . _area_at_zoom ( zoom )
7241	def aoi ( self , * * kwargs ) : g = self . _parse_geoms ( * * kwargs ) if g is None : return self else : return self [ g ]
10160	def ci ( ctx ) : opts = [ '' ] # 'tox' makes no sense in Travis if os . environ . get ( 'TRAVIS' , '' ) . lower ( ) == 'true' : opts += [ 'test.pytest' ] else : opts += [ 'test.tox' ] ctx . run ( "invoke --echo --pty clean --all build --docs check --reports{}" . format ( ' ' . join ( opts ) ) )
11853	def add_edge ( self , edge ) : start , end , lhs , found , expects = edge if edge not in self . chart [ end ] : self . chart [ end ] . append ( edge ) if self . trace : print '%10s: added %s' % ( caller ( 2 ) , edge ) if not expects : self . extender ( edge ) else : self . predictor ( edge )
2429	def reset_document ( self ) : # FIXME: this state does not make sense self . doc_version_set = False self . doc_comment_set = False self . doc_namespace_set = False self . doc_data_lics_set = False self . doc_name_set = False self . doc_spdx_id_set = False
11497	def create_community ( self , token , name , * * kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'name' ] = name optional_keys = [ 'description' , 'uuid' , 'privacy' , 'can_join' ] for key in optional_keys : if key in kwargs : if key == 'can_join' : parameters [ 'canjoin' ] = kwargs [ key ] continue parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.community.create' , parameters ) return response
10560	def _get_mutagen_metadata ( filepath ) : try : metadata = mutagen . File ( filepath , easy = True ) except mutagen . MutagenError : logger . warning ( "Can't load {} as music file." . format ( filepath ) ) raise return metadata
1921	def _hook_callback ( self , state , pc , instruction ) : # Ignore symbolic pc. # TODO(yan): Should we ask the solver if any of the hooks are possible, # and execute those that are? if issymbolic ( pc ) : return # Invoke all pc-specific hooks for cb in self . _hooks . get ( pc , [ ] ) : cb ( state ) # Invoke all pc-agnostic hooks for cb in self . _hooks . get ( None , [ ] ) : cb ( state )
6881	def _parse_csv_header_lcc_csv_v1 ( headerlines ) : # the first three lines indicate the format name, comment char, separator commentchar = headerlines [ 1 ] separator = headerlines [ 2 ] headerlines = [ x . lstrip ( '%s ' % commentchar ) for x in headerlines [ 3 : ] ] # next, find the indices of the various LC sections metadatastart = headerlines . index ( 'OBJECT METADATA' ) columnstart = headerlines . index ( 'COLUMN DEFINITIONS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) metadata = ' ' . join ( headerlines [ metadatastart + 1 : columnstart - 1 ] ) columns = ' ' . join ( headerlines [ columnstart + 1 : lcstart - 1 ] ) metadata = json . loads ( metadata ) columns = json . loads ( columns ) return metadata , columns , separator
9966	def convert_args ( args , kwargs ) : found = False for arg in args : if isinstance ( arg , Cells ) : found = True break if found : args = tuple ( arg . value if isinstance ( arg , Cells ) else arg for arg in args ) if kwargs is not None : for key , arg in kwargs . items ( ) : if isinstance ( arg , Cells ) : kwargs [ key ] = arg . value return args , kwargs
2840	def write_gpio ( self , gpio = None ) : if gpio is not None : self . gpio = gpio self . _device . writeList ( self . GPIO , self . gpio )
12607	def find_unique ( table , sample , unique_fields = None ) : res = search_unique ( table , sample , unique_fields ) if res is not None : return res . eid else : return res
1588	def get_topology_config ( self ) : if self . pplan . topology . HasField ( "topology_config" ) : return self . _get_dict_from_config ( self . pplan . topology . topology_config ) else : return { }
5491	def create_config ( cls , cfgfile , nick , twtfile , twturl , disclose_identity , add_news ) : cfgfile_dir = os . path . dirname ( cfgfile ) if not os . path . exists ( cfgfile_dir ) : os . makedirs ( cfgfile_dir ) cfg = configparser . ConfigParser ( ) cfg . add_section ( "twtxt" ) cfg . set ( "twtxt" , "nick" , nick ) cfg . set ( "twtxt" , "twtfile" , twtfile ) cfg . set ( "twtxt" , "twturl" , twturl ) cfg . set ( "twtxt" , "disclose_identity" , str ( disclose_identity ) ) cfg . set ( "twtxt" , "character_limit" , "140" ) cfg . set ( "twtxt" , "character_warning" , "140" ) cfg . add_section ( "following" ) if add_news : cfg . set ( "following" , "twtxt" , "https://buckket.org/twtxt_news.txt" ) conf = cls ( cfgfile , cfg ) conf . write_config ( ) return conf
10515	def verifyscrollbarvertical ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXOrientation == "AXVerticalOrientation" : return 1 except : pass return 0
13327	def remove ( name_or_path ) : click . echo ( ) try : r = cpenv . resolve ( name_or_path ) except cpenv . ResolveError as e : click . echo ( e ) return obj = r . resolved [ 0 ] if not isinstance ( obj , cpenv . VirtualEnvironment ) : click . echo ( '{} is a module. Use `cpenv module remove` instead.' ) return click . echo ( format_objects ( [ obj ] ) ) click . echo ( ) user_confirmed = click . confirm ( red ( 'Are you sure you want to remove this environment?' ) ) if user_confirmed : click . echo ( 'Attempting to remove...' , nl = False ) try : obj . remove ( ) except Exception as e : click . echo ( bold_red ( 'FAIL' ) ) click . echo ( e ) else : click . echo ( bold_green ( 'OK!' ) )
5641	def remove_dangling_shapes ( db_conn ) : db_conn . execute ( DELETE_SHAPES_NOT_REFERENCED_IN_TRIPS_SQL ) SELECT_MIN_MAX_SHAPE_BREAKS_BY_TRIP_I_SQL = "SELECT trips.trip_I, shape_id, min(shape_break) as min_shape_break, max(shape_break) as max_shape_break FROM trips, stop_times WHERE trips.trip_I=stop_times.trip_I GROUP BY trips.trip_I" trip_min_max_shape_seqs = pandas . read_sql ( SELECT_MIN_MAX_SHAPE_BREAKS_BY_TRIP_I_SQL , db_conn ) rows = [ ] for row in trip_min_max_shape_seqs . itertuples ( ) : shape_id , min_shape_break , max_shape_break = row . shape_id , row . min_shape_break , row . max_shape_break if min_shape_break is None or max_shape_break is None : min_shape_break = float ( '-inf' ) max_shape_break = float ( '-inf' ) rows . append ( ( shape_id , min_shape_break , max_shape_break ) ) DELETE_SQL_BASE = "DELETE FROM shapes WHERE shape_id=? AND (seq<? OR seq>?)" db_conn . executemany ( DELETE_SQL_BASE , rows ) remove_dangling_shapes_references ( db_conn )
1049	def print_exception ( etype , value , tb , limit = None , file = None ) : if file is None : # TODO: Use sys.stderr when that's implemented. file = open ( '/dev/stderr' , 'w' ) #file = sys.stderr if tb : _print ( file , 'Traceback (most recent call last):' ) print_tb ( tb , limit , file ) lines = format_exception_only ( etype , value ) for line in lines : _print ( file , line , '' )
1262	def import_demonstrations ( self , demonstrations ) : if isinstance ( demonstrations , dict ) : if self . unique_state : demonstrations [ 'states' ] = dict ( state = demonstrations [ 'states' ] ) if self . unique_action : demonstrations [ 'actions' ] = dict ( action = demonstrations [ 'actions' ] ) self . model . import_demo_experience ( * * demonstrations ) else : if self . unique_state : states = dict ( state = list ( ) ) else : states = { name : list ( ) for name in demonstrations [ 0 ] [ 'states' ] } internals = { name : list ( ) for name in demonstrations [ 0 ] [ 'internals' ] } if self . unique_action : actions = dict ( action = list ( ) ) else : actions = { name : list ( ) for name in demonstrations [ 0 ] [ 'actions' ] } terminal = list ( ) reward = list ( ) for demonstration in demonstrations : if self . unique_state : states [ 'state' ] . append ( demonstration [ 'states' ] ) else : for name , state in states . items ( ) : state . append ( demonstration [ 'states' ] [ name ] ) for name , internal in internals . items ( ) : internal . append ( demonstration [ 'internals' ] [ name ] ) if self . unique_action : actions [ 'action' ] . append ( demonstration [ 'actions' ] ) else : for name , action in actions . items ( ) : action . append ( demonstration [ 'actions' ] [ name ] ) terminal . append ( demonstration [ 'terminal' ] ) reward . append ( demonstration [ 'reward' ] ) self . model . import_demo_experience ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
13280	def child_end_handler ( self , scache ) : desc = self . desc desc_level = scache . desc_level breadth = desc_level . __len__ ( ) desc [ 'breadth' ] = breadth desc [ 'breadth_path' ] . append ( breadth ) desc_level . append ( desc )
4843	def get_common_course_modes ( self , course_run_ids ) : available_course_modes = None for course_run_id in course_run_ids : course_run = self . get_course_run ( course_run_id ) or { } course_run_modes = { seat . get ( 'type' ) for seat in course_run . get ( 'seats' , [ ] ) } if available_course_modes is None : available_course_modes = course_run_modes else : available_course_modes &= course_run_modes if not available_course_modes : return available_course_modes return available_course_modes
3196	def delete_permanent ( self , list_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' , subscriber_hash , 'actions' , 'delete-permanent' ) )
4867	def to_representation ( self , instance ) : updated_course = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_course [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_enrollment_url ( updated_course [ 'key' ] ) for course_run in updated_course [ 'course_runs' ] : course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( course_run [ 'key' ] ) return updated_course
8134	def up ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = min ( len ( self . canvas . layers ) , i + 1 ) self . canvas . layers . insert ( i , self )
7121	def filter_config ( config , deploy_config ) : if not os . path . isfile ( deploy_config ) : return DotDict ( ) config_module = get_config_module ( deploy_config ) return config_module . filter ( config )
2773	def create ( self , * args , * * kwargs ) : rules_dict = [ rule . __dict__ for rule in self . forwarding_rules ] params = { 'name' : self . name , 'region' : self . region , 'forwarding_rules' : rules_dict , 'redirect_http_to_https' : self . redirect_http_to_https } if self . droplet_ids and self . tag : raise ValueError ( 'droplet_ids and tag are mutually exclusive args' ) elif self . tag : params [ 'tag' ] = self . tag else : params [ 'droplet_ids' ] = self . droplet_ids if self . algorithm : params [ 'algorithm' ] = self . algorithm if self . health_check : params [ 'health_check' ] = self . health_check . __dict__ if self . sticky_sessions : params [ 'sticky_sessions' ] = self . sticky_sessions . __dict__ data = self . get_data ( 'load_balancers/' , type = POST , params = params ) if data : self . id = data [ 'load_balancer' ] [ 'id' ] self . ip = data [ 'load_balancer' ] [ 'ip' ] self . algorithm = data [ 'load_balancer' ] [ 'algorithm' ] self . health_check = HealthCheck ( * * data [ 'load_balancer' ] [ 'health_check' ] ) self . sticky_sessions = StickySesions ( * * data [ 'load_balancer' ] [ 'sticky_sessions' ] ) self . droplet_ids = data [ 'load_balancer' ] [ 'droplet_ids' ] self . status = data [ 'load_balancer' ] [ 'status' ] self . created_at = data [ 'load_balancer' ] [ 'created_at' ] return self
5182	def nodes ( self , unreported = 2 , with_status = False , * * kwargs ) : nodes = self . _query ( 'nodes' , * * kwargs ) now = datetime . datetime . utcnow ( ) # If we happen to only get one node back it # won't be inside a list so iterating over it # goes boom. Therefor we wrap a list around it. if type ( nodes ) == dict : nodes = [ nodes , ] if with_status : latest_events = self . event_counts ( query = EqualsOperator ( "latest_report?" , True ) , summarize_by = 'certname' ) for node in nodes : node [ 'status_report' ] = None node [ 'events' ] = None if with_status : status = [ s for s in latest_events if s [ 'subject' ] [ 'title' ] == node [ 'certname' ] ] try : node [ 'status_report' ] = node [ 'latest_report_status' ] if status : node [ 'events' ] = status [ 0 ] except KeyError : if status : node [ 'events' ] = status = status [ 0 ] if status [ 'successes' ] > 0 : node [ 'status_report' ] = 'changed' if status [ 'noops' ] > 0 : node [ 'status_report' ] = 'noop' if status [ 'failures' ] > 0 : node [ 'status_report' ] = 'failed' else : node [ 'status_report' ] = 'unchanged' # node report age if node [ 'report_timestamp' ] is not None : try : last_report = json_to_datetime ( node [ 'report_timestamp' ] ) last_report = last_report . replace ( tzinfo = None ) unreported_border = now - timedelta ( hours = unreported ) if last_report < unreported_border : delta = ( now - last_report ) node [ 'unreported' ] = True node [ 'unreported_time' ] = '{0}d {1}h {2}m' . format ( delta . days , int ( delta . seconds / 3600 ) , int ( ( delta . seconds % 3600 ) / 60 ) ) except AttributeError : node [ 'unreported' ] = True if not node [ 'report_timestamp' ] : node [ 'unreported' ] = True yield Node ( self , name = node [ 'certname' ] , deactivated = node [ 'deactivated' ] , expired = node [ 'expired' ] , report_timestamp = node [ 'report_timestamp' ] , catalog_timestamp = node [ 'catalog_timestamp' ] , facts_timestamp = node [ 'facts_timestamp' ] , status_report = node [ 'status_report' ] , noop = node . get ( 'latest_report_noop' ) , noop_pending = node . get ( 'latest_report_noop_pending' ) , events = node [ 'events' ] , unreported = node . get ( 'unreported' ) , unreported_time = node . get ( 'unreported_time' ) , report_environment = node [ 'report_environment' ] , catalog_environment = node [ 'catalog_environment' ] , facts_environment = node [ 'facts_environment' ] , latest_report_hash = node . get ( 'latest_report_hash' ) , cached_catalog_status = node . get ( 'cached_catalog_status' ) )
5473	def trim_display_field ( self , value , max_length ) : if not value : return '' if len ( value ) > max_length : return value [ : max_length - 3 ] + '...' return value
11418	def record_modify_subfield ( rec , tag , subfield_code , value , subfield_position , field_position_global = None , field_position_local = None ) : subfields = record_get_subfields ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) try : subfields [ subfield_position ] = ( subfield_code , value ) except IndexError : raise InvenioBibRecordFieldError ( "There is no subfield with position '%d'." % subfield_position )
12249	def get_key ( self , * args , * * kwargs ) : if kwargs . pop ( 'force' , None ) : headers = kwargs . get ( 'headers' , { } ) headers [ 'force' ] = True kwargs [ 'headers' ] = headers return super ( Bucket , self ) . get_key ( * args , * * kwargs )
3273	def as_DAVError ( e ) : if isinstance ( e , DAVError ) : return e elif isinstance ( e , Exception ) : # traceback.print_exc() return DAVError ( HTTP_INTERNAL_ERROR , src_exception = e ) else : return DAVError ( HTTP_INTERNAL_ERROR , "{}" . format ( e ) )
9658	def get_sinks ( G ) : sinks = [ ] for node in G : if not len ( list ( G . successors ( node ) ) ) : sinks . append ( node ) return sinks
7207	def execute ( self ) : # if not self.tasks: # raise WorkflowError('Workflow contains no tasks, and cannot be executed.') # for task in self.tasks: # self.definition['tasks'].append( task.generate_task_workflow_json() ) self . generate_workflow_description ( ) # hit batch workflow endpoint if batch values if self . batch_values : self . id = self . workflow . launch_batch_workflow ( self . definition ) # use regular workflow endpoint if no batch values else : self . id = self . workflow . launch ( self . definition ) return self . id
2117	def convert ( self , value , param , ctx ) : choice = super ( MappedChoice , self ) . convert ( value , param , ctx ) ix = self . choices . index ( choice ) return self . actual_choices [ ix ]
10304	def min_tanimoto_set_similarity ( x : Iterable [ X ] , y : Iterable [ X ] ) -> float : a , b = set ( x ) , set ( y ) if not a or not b : return 0.0 return len ( a & b ) / min ( len ( a ) , len ( b ) )
7964	def event ( self , event ) : logger . debug ( u"TCP transport event: {0}" . format ( event ) ) if self . _stream : event . stream = self . _stream self . _event_queue . put ( event )
2223	def _hashable_sequence ( data , types = True ) : hasher = _HashTracer ( ) _update_hasher ( hasher , data , types = types ) return hasher . sequence
12618	def get_shape ( img ) : if hasattr ( img , 'shape' ) : shape = img . shape else : shape = img . get_data ( ) . shape return shape
1509	def stop_cluster ( cl_args ) : Log . info ( "Terminating cluster..." ) roles = read_and_parse_roles ( cl_args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] dist_nodes = masters . union ( slaves ) # stop all jobs if masters : try : single_master = list ( masters ) [ 0 ] jobs = get_jobs ( cl_args , single_master ) for job in jobs : job_id = job [ "ID" ] Log . info ( "Terminating job %s" % job_id ) delete_job ( cl_args , job_id , single_master ) except : Log . debug ( "Error stopping jobs" ) Log . debug ( sys . exc_info ( ) [ 0 ] ) for node in dist_nodes : Log . info ( "Terminating processes on %s" % node ) if not is_self ( node ) : cmd = "ps aux | grep heron-nomad | awk '{print \$2}' " "| xargs kill" cmd = ssh_remote_execute ( cmd , node , cl_args ) else : cmd = "ps aux | grep heron-nomad | awk '{print $2}' " "| xargs kill" Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) Log . info ( "Cleaning up directories on %s" % node ) cmd = "rm -rf /tmp/slave ; rm -rf /tmp/master" if not is_self ( node ) : cmd = ssh_remote_execute ( cmd , node , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) )
11612	def report_read_counts ( self , filename , grp_wise = False , reorder = 'as-is' , notes = None ) : expected_read_counts = self . probability . sum ( axis = APM . Axis . READ ) if grp_wise : lname = self . probability . gname expected_read_counts = expected_read_counts * self . grp_conv_mat else : lname = self . probability . lname total_read_counts = expected_read_counts . sum ( axis = 0 ) if reorder == 'decreasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) report_order = report_order [ : : - 1 ] elif reorder == 'increasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) elif reorder == 'as-is' : report_order = np . arange ( len ( lname ) ) # report in the original locus order cntdata = np . vstack ( ( expected_read_counts , total_read_counts ) ) fhout = open ( filename , 'w' ) fhout . write ( "locus\t" + "\t" . join ( self . probability . hname ) + "\ttotal" ) if notes is not None : fhout . write ( "\tnotes" ) fhout . write ( "\n" ) for locus_id in report_order : lname_cur = lname [ locus_id ] fhout . write ( "\t" . join ( [ lname_cur ] + map ( str , cntdata [ : , locus_id ] . ravel ( ) ) ) ) if notes is not None : fhout . write ( "\t%s" % notes [ lname_cur ] ) fhout . write ( "\n" ) fhout . close ( )
13664	def set_item ( filename , item ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : # load the JSON data into memory products_data = json . load ( products_file ) # check if UUID already exists uuid_list = [ i for i in filter ( lambda z : z [ "uuid" ] == str ( item [ "uuid" ] ) , products_data ) ] if len ( uuid_list ) == 0 : # add the new item to the JSON file products_data . append ( item ) # save the new JSON to the temp file json . dump ( products_data , temp_file ) return True return None
5910	def delete_frames ( self ) : for frame in glob . glob ( self . frameglob ) : os . unlink ( frame )
3691	def solve_T ( self , P , V , quick = True ) : if self . S2 == 0 : self . m = self . S1 return SRK . solve_T ( self , P , V , quick = quick ) else : # Previously coded method is 63 microseconds vs 47 here # return super(SRK, self).solve_T(P, V, quick=quick) Tc , a , b , S1 , S2 = self . Tc , self . a , self . b , self . S1 , self . S2 if quick : x2 = R / ( V - b ) x3 = ( V * ( V + b ) ) def to_solve ( T ) : x0 = ( T / Tc ) ** 0.5 x1 = x0 - 1. return ( x2 * T - a * ( S1 * x1 + S2 * x1 / x0 - 1. ) ** 2 / x3 ) - P else : def to_solve ( T ) : P_calc = R * T / ( V - b ) - a * ( S1 * ( - sqrt ( T / Tc ) + 1 ) + S2 * ( - sqrt ( T / Tc ) + 1 ) / sqrt ( T / Tc ) + 1 ) ** 2 / ( V * ( V + b ) ) return P_calc - P return newton ( to_solve , Tc * 0.5 )
9313	def sign_sha256 ( key , msg ) : if isinstance ( msg , text_type ) : msg = msg . encode ( 'utf-8' ) return hmac . new ( key , msg , hashlib . sha256 ) . digest ( )
9163	def lookup_api_key_info ( ) : info = { } with db_connect ( ) as conn : with conn . cursor ( ) as cursor : cursor . execute ( ALL_KEY_INFO_SQL_STMT ) for row in cursor . fetchall ( ) : id , key , name , groups = row user_id = "api_key:{}" . format ( id ) info [ key ] = dict ( id = id , user_id = user_id , name = name , groups = groups ) return info
8288	def get_child_by_name ( parent , name ) : # http://stackoverflow.com/questions/2072976/access-to-widget-in-gtk def iterate_children ( widget , name ) : if widget . get_name ( ) == name : return widget try : for w in widget . get_children ( ) : result = iterate_children ( w , name ) if result is not None : return result else : continue except AttributeError : pass return iterate_children ( parent , name )
4048	def key_info ( self , * * kwargs ) : query_string = "/keys/{k}" . format ( k = self . api_key ) return self . _build_query ( query_string )
9684	def sn ( self ) : string = [ ] # Send the command byte and sleep for 9 ms self . cnxn . xfer ( [ 0x10 ] ) sleep ( 9e-3 ) # Read the info string by sending 60 empty bytes for i in range ( 60 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] string . append ( chr ( resp ) ) sleep ( 0.1 ) return '' . join ( string )
7615	def get_datetime ( self , timestamp : str , unix = True ) : time = datetime . strptime ( timestamp , '%Y%m%dT%H%M%S.%fZ' ) if unix : return int ( time . timestamp ( ) ) else : return time
622	def coordinatesFromIndex ( index , dimensions ) : coordinates = [ 0 ] * len ( dimensions ) shifted = index for i in xrange ( len ( dimensions ) - 1 , 0 , - 1 ) : coordinates [ i ] = shifted % dimensions [ i ] shifted = shifted / dimensions [ i ] coordinates [ 0 ] = shifted return coordinates
1095	def escape ( pattern ) : s = list ( pattern ) alphanum = _alphanum for i , c in enumerate ( pattern ) : if c not in alphanum : if c == "\000" : s [ i ] = "\\000" else : s [ i ] = "\\" + c return pattern [ : 0 ] . join ( s )
12099	def get_root_directory ( self , timestamp = None ) : if timestamp is None : timestamp = self . timestamp if self . timestamp_format is not None : root_name = ( time . strftime ( self . timestamp_format , timestamp ) + '-' + self . batch_name ) else : root_name = self . batch_name path = os . path . join ( self . output_directory , * ( self . subdir + [ root_name ] ) ) return os . path . abspath ( path )
13558	def get_all_images_count ( self ) : self_imgs = self . image_set . count ( ) update_ids = self . update_set . values_list ( 'id' , flat = True ) u_images = UpdateImage . objects . filter ( update__id__in = update_ids ) . count ( ) count = self_imgs + u_images return count
13166	def parse_query ( query ) : parts = query . split ( '/' ) norm = [ ] for p in parts : p = p . strip ( ) if p : norm . append ( p ) elif '' not in norm : norm . append ( '' ) return norm
9721	async def start ( self , rtfromfile = False ) : cmd = "start" + ( " rtfromfile" if rtfromfile else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
12009	def _update_dict ( data , default_data , replace_data = False ) : if not data : data = default_data . copy ( ) return data if not isinstance ( data , dict ) : raise TypeError ( 'Value not dict type' ) if len ( data ) > 255 : raise ValueError ( 'More than 255 values defined' ) for i in data . keys ( ) : if not isinstance ( i , int ) : raise TypeError ( 'Index not int type' ) if i < 0 or i > 255 : raise ValueError ( 'Index value out of range' ) if not replace_data : data . update ( default_data ) return data
13419	def schema ( args ) : try : import south cmd = args and 'schemamigration %s' % ' ' . join ( options . args ) or 'schemamigration' call_manage ( cmd ) except ImportError : error ( 'Could not import south.' )
2352	def root ( self ) : if self . _root is None and self . _root_locator is not None : return self . page . find_element ( * self . _root_locator ) return self . _root
11399	def update_keywords ( self ) : for field in record_get_field_instances ( self . record , '653' , ind1 = '1' ) : subs = field_get_subfields ( field ) new_subs = [ ] if 'a' in subs : for val in subs [ 'a' ] : new_subs . extend ( [ ( '9' , 'author' ) , ( 'a' , val ) ] ) new_field = create_field ( subfields = new_subs , ind1 = '1' ) record_replace_field ( self . record , '653' , new_field , field_position_global = field [ 4 ] )
13414	def removeLayout ( self , layout ) : for cnt in reversed ( range ( layout . count ( ) ) ) : item = layout . takeAt ( cnt ) widget = item . widget ( ) if widget is not None : widget . deleteLater ( ) else : '''If sublayout encountered, iterate recursively.''' self . removeLayout ( item . layout ( ) )
8996	def example ( self , relative_path ) : example_path = os . path . join ( "examples" , relative_path ) return self . relative_file ( __file__ , example_path )
11594	def _rc_rename ( self , src , dst ) : if src == dst : return self . rename ( src + "{" + src + "}" , src ) if not self . exists ( src ) : return self . rename ( src + "{" + src + "}" , src ) self . delete ( dst ) ktype = self . type ( src ) kttl = self . ttl ( src ) if ktype == b ( 'none' ) : return False if ktype == b ( 'string' ) : self . set ( dst , self . get ( src ) ) elif ktype == b ( 'hash' ) : self . hmset ( dst , self . hgetall ( src ) ) elif ktype == b ( 'list' ) : for k in self . lrange ( src , 0 , - 1 ) : self . rpush ( dst , k ) elif ktype == b ( 'set' ) : for k in self . smembers ( src ) : self . sadd ( dst , k ) elif ktype == b ( 'zset' ) : for k , v in self . zrange ( src , 0 , - 1 , withscores = True ) : self . zadd ( dst , v , k ) # Handle keys with an expire time set kttl = - 1 if kttl is None or kttl < 0 else int ( kttl ) if kttl != - 1 : self . expire ( dst , kttl ) return self . delete ( src )
3056	def _create_file_if_needed ( filename ) : if os . path . exists ( filename ) : return False else : # Equivalent to "touch". open ( filename , 'a+b' ) . close ( ) logger . info ( 'Credential file {0} created' . format ( filename ) ) return True
12369	def rename ( self , id , name ) : return super ( DomainRecords , self ) . update ( id , name = name ) [ self . singular ]
8227	def _makeColorableInstance ( self , clazz , args , kwargs ) : kwargs = dict ( kwargs ) fill = kwargs . get ( 'fill' , self . _canvas . fillcolor ) if not isinstance ( fill , Color ) : fill = Color ( fill , mode = 'rgb' , color_range = 1 ) kwargs [ 'fill' ] = fill stroke = kwargs . get ( 'stroke' , self . _canvas . strokecolor ) if not isinstance ( stroke , Color ) : stroke = Color ( stroke , mode = 'rgb' , color_range = 1 ) kwargs [ 'stroke' ] = stroke kwargs [ 'strokewidth' ] = kwargs . get ( 'strokewidth' , self . _canvas . strokewidth ) inst = clazz ( self , * args , * * kwargs ) return inst
1933	def function_signature_for_name_and_inputs ( name : str , inputs : Sequence [ Mapping [ str , Any ] ] ) -> str : return name + SolidityMetadata . tuple_signature_for_components ( inputs )
9197	def get ( self , key , default = _sentinel ) : tup = self . _data . get ( key . lower ( ) ) if tup is not None : return tup [ 1 ] elif default is not _sentinel : return default else : return None
3773	def set_user_methods ( self , user_methods , forced = False ) : # Accept either a string or a list of methods, and whether # or not to only consider the false methods if isinstance ( user_methods , str ) : user_methods = [ user_methods ] # The user's order matters and is retained for use by select_valid_methods self . user_methods = user_methods self . forced = forced # Validate that the user's specified methods are actual methods if set ( self . user_methods ) . difference ( self . all_methods ) : raise Exception ( "One of the given methods is not available for this chemical" ) if not self . user_methods and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) # Remove previously selected methods self . method = None self . sorted_valid_methods = [ ] self . T_cached = None
13092	def load_targets ( self ) : ldap_services = [ ] if self . ldap : ldap_services = self . search . get_services ( ports = [ 389 ] , up = True ) self . ldap_strings = [ "ldap://{}" . format ( service . address ) for service in ldap_services ] self . services = self . search . get_services ( tags = [ 'smb_signing_disabled' ] ) self . ips = [ str ( service . address ) for service in self . services ]
11091	def select_file ( self , filters = all_true , recursive = True ) : for p in self . select ( filters , recursive ) : if p . is_file ( ) : yield p
12390	def paginate ( request , response , items ) : # TODO: support dynamic rangewords and page lengths # TODO: support multi-part range requests # Get the header header = request . headers . get ( 'Range' ) if not header : # No range header; move along. return items # do some validation prefix = RANGE_SPECIFIER + '=' if not header . find ( prefix ) == 0 : # This is not using a range specifier that we understand raise exceptions . RequestedRangeNotSatisfiable ( ) else : # Chop the prefix off the header and parse it ranges = parse ( header [ len ( prefix ) : ] ) ranges = list ( ranges ) if len ( ranges ) > 1 : raise exceptions . RequestedRangeNotSatisfiable ( 'Multiple ranges in a single request is not yet supported.' ) start , end = ranges [ 0 ] # Make sure the length is not higher than the total number allowed. max_length = request . resource . count ( items ) end = min ( end , max_length ) response . status = client . PARTIAL_CONTENT response . headers [ 'Content-Range' ] = '%d-%d/%d' % ( start , end , max_length ) response . headers [ 'Accept-Ranges' ] = RANGE_SPECIFIER # Splice and return the items. items = items [ start : end + 1 ] return items
4826	def enroll_user_in_course ( self , username , course_id , mode , cohort = None ) : return self . client . enrollment . post ( { 'user' : username , 'course_details' : { 'course_id' : course_id } , 'mode' : mode , 'cohort' : cohort , } )
276	def customize ( func ) : @ wraps ( func ) def call_w_context ( * args , * * kwargs ) : set_context = kwargs . pop ( 'set_context' , True ) if set_context : with plotting_context ( ) , axes_style ( ) : return func ( * args , * * kwargs ) else : return func ( * args , * * kwargs ) return call_w_context
12562	def get_unique_nonzeros ( arr ) : rois = np . unique ( arr ) rois = rois [ np . nonzero ( rois ) ] rois . sort ( ) return rois
2808	def convert_matmul ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting matmul ...' ) if names == 'short' : tf_name = 'MMUL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if len ( inputs ) == 1 : weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = False , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] ) elif len ( inputs ) == 2 : weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = False , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] ) else : raise AssertionError ( 'Cannot convert matmul layer' )
7262	def get_most_recent_images ( self , results , types = [ ] , sensors = [ ] , N = 1 ) : if not len ( results ) : return None # filter on type if types : results = [ r for r in results if r [ 'type' ] in types ] # filter on sensor if sensors : results = [ r for r in results if r [ 'properties' ] . get ( 'sensorPlatformName' ) in sensors ] # sort by date: #sorted(results, key=results.__getitem__('properties').get('timestamp')) newlist = sorted ( results , key = lambda k : k [ 'properties' ] . get ( 'timestamp' ) , reverse = True ) return newlist [ : N ]
11718	def delete ( self ) : headers = self . _default_headers ( ) return self . _request ( self . name , ok_status = None , data = None , headers = headers , method = "DELETE" )
7179	def lib2to3_unparse ( node , * , hg = False ) : code = str ( node ) if hg : from retype_hgext import apply_job_security code = apply_job_security ( code ) return code
3652	def run ( self ) : while self . _base . is_running : if self . _worker : self . _worker ( ) time . sleep ( self . _sleep_duration )
12678	def unescape ( escaped , escape_char = ESCAPE_CHAR ) : if isinstance ( escaped , bytes ) : # always work on text escaped = escaped . decode ( 'utf8' ) escape_pat = re . compile ( re . escape ( escape_char ) . encode ( 'utf8' ) + b'([a-z0-9]{2})' , re . IGNORECASE ) buf = escape_pat . subn ( _unescape_char , escaped . encode ( 'utf8' ) ) [ 0 ] return buf . decode ( 'utf8' )
12703	def _set_params ( target , param , values , dof ) : if not isinstance ( values , ( list , tuple , np . ndarray ) ) : values = [ values ] * dof assert dof == len ( values ) for s , value in zip ( [ '' , '2' , '3' ] [ : dof ] , values ) : target . setParam ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) , value )
3812	async def set_active ( self ) : is_active = ( self . _active_client_state == hangouts_pb2 . ACTIVE_CLIENT_STATE_IS_ACTIVE ) timed_out = ( time . time ( ) - self . _last_active_secs > SETACTIVECLIENT_LIMIT_SECS ) if not is_active or timed_out : # Update these immediately so if the function is called again # before the API request finishes, we don't start extra requests. self . _active_client_state = ( hangouts_pb2 . ACTIVE_CLIENT_STATE_IS_ACTIVE ) self . _last_active_secs = time . time ( ) # The first time this is called, we need to retrieve the user's # email address. if self . _email is None : try : get_self_info_request = hangouts_pb2 . GetSelfInfoRequest ( request_header = self . get_request_header ( ) , ) get_self_info_response = await self . get_self_info ( get_self_info_request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to find email address: {}' . format ( e ) ) return self . _email = ( get_self_info_response . self_entity . properties . email [ 0 ] ) # If the client_id hasn't been received yet, we can't set the # active client. if self . _client_id is None : logger . info ( 'Cannot set active client until client_id is received' ) return try : set_active_request = hangouts_pb2 . SetActiveClientRequest ( request_header = self . get_request_header ( ) , is_active = True , full_jid = "{}/{}" . format ( self . _email , self . _client_id ) , timeout_secs = ACTIVE_TIMEOUT_SECS , ) await self . set_active_client ( set_active_request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to set active client: {}' . format ( e ) ) else : logger . info ( 'Set active client for {} seconds' . format ( ACTIVE_TIMEOUT_SECS ) )
3153	def all ( self , list_id , * * queryparams ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' ) , * * queryparams )
8985	def instruction_to_svg_dict ( self , instruction_or_id , copy_result = True ) : instruction_id = self . get_instruction_id ( instruction_or_id ) if instruction_id in self . _cache : result = self . _cache [ instruction_id ] else : result = self . _instruction_to_svg_dict ( instruction_id ) self . _cache [ instruction_id ] = result if copy_result : result = deepcopy ( result ) return result
4670	def setKeys ( self , loadkeys ) : log . debug ( "Force setting of private keys. Not using the wallet database!" ) if isinstance ( loadkeys , dict ) : loadkeys = list ( loadkeys . values ( ) ) elif not isinstance ( loadkeys , ( list , set ) ) : loadkeys = [ loadkeys ] for wif in loadkeys : pub = self . publickey_from_wif ( wif ) self . store . add ( str ( wif ) , pub )
3115	def _get_flow_for_token ( csrf_token , request ) : flow_pickle = request . session . get ( _FLOW_KEY . format ( csrf_token ) , None ) return None if flow_pickle is None else jsonpickle . decode ( flow_pickle )
9501	def _disassemble ( self , lineno_width = 3 , mark_as_current = False ) : fields = [ ] # Column: Source code line number if lineno_width : if self . starts_line is not None : lineno_fmt = "%%%dd" % lineno_width fields . append ( lineno_fmt % self . starts_line ) else : fields . append ( ' ' * lineno_width ) # Column: Current instruction indicator if mark_as_current : fields . append ( '-->' ) else : fields . append ( ' ' ) # Column: Jump target marker if self . is_jump_target : fields . append ( '>>' ) else : fields . append ( ' ' ) # Column: Instruction offset from start of code sequence fields . append ( repr ( self . offset ) . rjust ( 4 ) ) # Column: Opcode name fields . append ( self . opname . ljust ( 20 ) ) # Column: Opcode argument if self . arg is not None : fields . append ( repr ( self . arg ) . rjust ( 5 ) ) # Column: Opcode argument details if self . argrepr : fields . append ( '(' + self . argrepr + ')' ) return ' ' . join ( fields ) . rstrip ( )
9605	def fluent ( func ) : @ wraps ( func ) def fluent_interface ( instance , * args , * * kwargs ) : ret = func ( instance , * args , * * kwargs ) if ret is not None : return ret return instance return fluent_interface
11261	def resplit ( prev , pattern , * args , * * kw ) : maxsplit = 0 if 'maxsplit' not in kw else kw . pop ( 'maxsplit' ) pattern_obj = re . compile ( pattern , * args , * * kw ) for s in prev : yield pattern_obj . split ( s , maxsplit = maxsplit )
2791	def get_object ( cls , api_token , cert_id ) : certificate = cls ( token = api_token , id = cert_id ) certificate . load ( ) return certificate
4262	def save_cache ( gallery ) : if hasattr ( gallery , "exifCache" ) : cache = gallery . exifCache else : cache = gallery . exifCache = { } for album in gallery . albums . values ( ) : for image in album . images : cache [ os . path . join ( image . path , image . filename ) ] = image . exif cachePath = os . path . join ( gallery . settings [ "destination" ] , ".exif_cache" ) if len ( cache ) == 0 : if os . path . exists ( cachePath ) : os . remove ( cachePath ) return try : with open ( cachePath , "wb" ) as cacheFile : pickle . dump ( cache , cacheFile ) logger . debug ( "Stored cache with %d entries" , len ( gallery . exifCache ) ) except Exception as e : logger . warn ( "Could not store cache: %s" , e ) os . remove ( cachePath )
235	def compute_cap_exposures ( positions , caps ) : long_exposures = [ ] short_exposures = [ ] gross_exposures = [ ] net_exposures = [ ] positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) tot_gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) tot_long_exposure = positions_wo_cash [ positions_wo_cash > 0 ] . sum ( axis = 'columns' ) tot_short_exposure = positions_wo_cash [ positions_wo_cash < 0 ] . abs ( ) . sum ( axis = 'columns' ) for bucket_name , boundaries in CAP_BUCKETS . items ( ) : in_bucket = positions_wo_cash [ ( caps >= boundaries [ 0 ] ) & ( caps <= boundaries [ 1 ] ) ] gross_bucket = in_bucket . abs ( ) . sum ( axis = 'columns' ) . divide ( tot_gross_exposure ) long_bucket = in_bucket [ in_bucket > 0 ] . sum ( axis = 'columns' ) . divide ( tot_long_exposure ) short_bucket = in_bucket [ in_bucket < 0 ] . sum ( axis = 'columns' ) . divide ( tot_short_exposure ) net_bucket = long_bucket . subtract ( short_bucket ) gross_exposures . append ( gross_bucket ) long_exposures . append ( long_bucket ) short_exposures . append ( short_bucket ) net_exposures . append ( net_bucket ) return long_exposures , short_exposures , gross_exposures , net_exposures
13107	def remove_tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) - set ( [ tag ] ) )
1717	def replacement_template ( rep , source , span , npar ) : n = 0 res = '' while n < len ( rep ) - 1 : char = rep [ n ] if char == '$' : if rep [ n + 1 ] == '$' : res += '$' n += 2 continue elif rep [ n + 1 ] == '`' : # replace with string that is BEFORE match res += source [ : span [ 0 ] ] n += 2 continue elif rep [ n + 1 ] == '\'' : # replace with string that is AFTER match res += source [ span [ 1 ] : ] n += 2 continue elif rep [ n + 1 ] in DIGS : dig = rep [ n + 1 ] if n + 2 < len ( rep ) and rep [ n + 2 ] in DIGS : dig += rep [ n + 2 ] num = int ( dig ) # we will not do any replacements if we dont have this npar or dig is 0 if not num or num > len ( npar ) : res += '$' + dig else : # None - undefined has to be replaced with '' res += npar [ num - 1 ] if npar [ num - 1 ] else '' n += 1 + len ( dig ) continue res += char n += 1 if n < len ( rep ) : res += rep [ - 1 ] return res
10349	def lint_file ( in_file , out_file = None ) : for line in in_file : print ( line . strip ( ) , file = out_file )
7000	def _runpf_worker ( task ) : ( lcfile , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nworkers , minobservations , excludeprocessed ) = task if os . path . exists ( lcfile ) : pfresult = runpf ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nworkers = nworkers , minobservations = minobservations , excludeprocessed = excludeprocessed ) return pfresult else : LOGERROR ( 'LC does not exist for requested file %s' % lcfile ) return None
8870	def create_metafile ( bgen_filepath , metafile_filepath , verbose = True ) : if verbose : verbose = 1 else : verbose = 0 bgen_filepath = make_sure_bytes ( bgen_filepath ) metafile_filepath = make_sure_bytes ( metafile_filepath ) assert_file_exist ( bgen_filepath ) assert_file_readable ( bgen_filepath ) if exists ( metafile_filepath ) : raise ValueError ( f"The file {metafile_filepath} already exists." ) with bgen_file ( bgen_filepath ) as bgen : nparts = _estimate_best_npartitions ( lib . bgen_nvariants ( bgen ) ) metafile = lib . bgen_create_metafile ( bgen , metafile_filepath , nparts , verbose ) if metafile == ffi . NULL : raise RuntimeError ( f"Error while creating metafile: {metafile_filepath}." ) if lib . bgen_close_metafile ( metafile ) != 0 : raise RuntimeError ( f"Error while closing metafile: {metafile_filepath}." )
5531	def get_process_tiles ( self , zoom = None ) : if zoom or zoom == 0 : for tile in self . config . process_pyramid . tiles_from_geom ( self . config . area_at_zoom ( zoom ) , zoom ) : yield tile else : for zoom in reversed ( self . config . zoom_levels ) : for tile in self . config . process_pyramid . tiles_from_geom ( self . config . area_at_zoom ( zoom ) , zoom ) : yield tile
12893	def get_power ( self ) : power = ( yield from self . handle_int ( self . API . get ( 'power' ) ) ) return bool ( power )
6155	def cruise_control ( wn , zeta , T , vcruise , vmax , tf_mode = 'H' ) : tau = T / 2. * vmax / vcruise g = 9.8 g *= 3 * 60 ** 2 / 5280. # m/s to mph conversion Kp = T * ( 2 * zeta * wn - 1 / tau ) / vmax Ki = T * wn ** 2. / vmax K = Kp * vmax / T print ( 'wn = ' , np . sqrt ( K / ( Kp / Ki ) ) ) print ( 'zeta = ' , ( K + 1 / tau ) / ( 2 * wn ) ) a = np . array ( [ 1 , 2 * zeta * wn , wn ** 2 ] ) if tf_mode == 'H' : b = np . array ( [ K , wn ** 2 ] ) elif tf_mode == 'HE' : b = np . array ( [ 1 , 2 * zeta * wn - K , 0. ] ) elif tf_mode == 'HVW' : b = np . array ( [ 1 , wn ** 2 / K + 1 / tau , wn ** 2 / ( K * tau ) ] ) b *= Kp elif tf_mode == 'HED' : b = np . array ( [ g , 0 ] ) else : raise ValueError ( 'tf_mode must be: H, HE, HVU, or HED' ) return b , a
12844	def execute_undo ( self , message ) : info ( "undoing message: {message}" ) # Roll back changes that the original message made to the world. with self . world . _unlock_temporarily ( ) : message . _undo ( self . world ) self . world . _react_to_undo_response ( message ) # Give the actors a chance to react to the error. For example, a # GUI actor might inform the user that there are connectivity # issues and that their last action was countermanded. for actor in self . actors : actor . _react_to_undo_response ( message )
5387	def _convert_suffix_to_docker_chars ( suffix ) : # Docker container names must match: [a-zA-Z0-9][a-zA-Z0-9_.-] accepted_characters = string . ascii_letters + string . digits + '_.-' def label_char_transform ( char ) : if char in accepted_characters : return char return '-' return '' . join ( label_char_transform ( c ) for c in suffix )
537	def run ( self ) : # ----------------------------------------------------------------------- # Load the experiment's description.py module descriptionPyModule = helpers . loadExperimentDescriptionScriptFromDir ( self . _experimentDir ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) expIface . normalizeStreamSources ( ) modelDescription = expIface . getModelDescription ( ) self . _modelControl = expIface . getModelControl ( ) # ----------------------------------------------------------------------- # Create the input data stream for this task streamDef = self . _modelControl [ 'dataset' ] from nupic . data . stream_reader import StreamReader readTimeout = 0 self . _inputSource = StreamReader ( streamDef , isBlocking = False , maxTimeout = readTimeout ) # ----------------------------------------------------------------------- #Get field statistics from the input source fieldStats = self . _getFieldStats ( ) # ----------------------------------------------------------------------- # Construct the model instance self . _model = ModelFactory . create ( modelDescription ) self . _model . setFieldStatistics ( fieldStats ) self . _model . enableLearning ( ) self . _model . enableInference ( self . _modelControl . get ( "inferenceArgs" , None ) ) # ----------------------------------------------------------------------- # Instantiate the metrics self . __metricMgr = MetricsManager ( self . _modelControl . get ( 'metrics' , None ) , self . _model . getFieldInfo ( ) , self . _model . getInferenceType ( ) ) self . __loggedMetricPatterns = self . _modelControl . get ( "loggedMetrics" , [ ] ) self . _optimizedMetricLabel = self . __getOptimizedMetricLabel ( ) self . _reportMetricLabels = matchPatterns ( self . _reportKeyPatterns , self . _getMetricLabels ( ) ) # ----------------------------------------------------------------------- # Initialize periodic activities (e.g., for model result updates) self . _periodic = self . _initPeriodicActivities ( ) # ----------------------------------------------------------------------- # Create our top-level loop-control iterator numIters = self . _modelControl . get ( 'iterationCount' , - 1 ) # Are we asked to turn off learning for a certain # of iterations near the # end? learningOffAt = None iterationCountInferOnly = self . _modelControl . get ( 'iterationCountInferOnly' , 0 ) if iterationCountInferOnly == - 1 : self . _model . disableLearning ( ) elif iterationCountInferOnly > 0 : assert numIters > iterationCountInferOnly , "when iterationCountInferOnly " "is specified, iterationCount must be greater than " "iterationCountInferOnly." learningOffAt = numIters - iterationCountInferOnly self . __runTaskMainLoop ( numIters , learningOffAt = learningOffAt ) # ----------------------------------------------------------------------- # Perform final operations for model self . _finalize ( ) return ( self . _cmpReason , None )
12297	def plugins_show ( what = None , name = None , version = None , details = False ) : global pluginmgr return pluginmgr . show ( what , name , version , details )
1800	def CMOVNO ( cpu , dest , src ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF == False , src . read ( ) , dest . read ( ) ) )
11779	def SyntheticRestaurant ( n = 20 ) : def gen ( ) : example = map ( random . choice , restaurant . values ) example [ restaurant . target ] = Fig [ 18 , 2 ] ( example ) return example return RestaurantDataSet ( [ gen ( ) for i in range ( n ) ] )
11763	def utility ( self , state , player ) : return if_ ( player == 'X' , state . utility , - state . utility )
8944	def pushd ( path ) : saved = os . getcwd ( ) os . chdir ( path ) try : yield saved finally : os . chdir ( saved )
5345	def compose_git ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'source_repo' ] ) > 0 ] : repos = [ ] for url in data [ p ] [ 'source_repo' ] : if len ( url [ 'url' ] . split ( ) ) > 1 : # Error at upstream the project 'tools.corrosion' repo = url [ 'url' ] . split ( ) [ 1 ] . replace ( '/c/' , '/gitroot/' ) else : repo = url [ 'url' ] . replace ( '/c/' , '/gitroot/' ) if repo not in repos : repos . append ( repo ) projects [ p ] [ 'git' ] = repos return projects
2863	def ping ( self ) : self . _idle ( ) self . _transaction_start ( ) self . _i2c_start ( ) self . _i2c_write_bytes ( [ self . _address_byte ( False ) ] ) self . _i2c_stop ( ) response = self . _transaction_end ( ) if len ( response ) != 1 : raise RuntimeError ( 'Expected 1 response byte but received {0} byte(s).' . format ( len ( response ) ) ) return ( ( response [ 0 ] & 0x01 ) == 0x00 )
7796	def _register_client_authenticator ( klass , name ) : # pylint: disable-msg=W0212 CLIENT_MECHANISMS_D [ name ] = klass items = sorted ( CLIENT_MECHANISMS_D . items ( ) , key = _key_func , reverse = True ) CLIENT_MECHANISMS [ : ] = [ k for ( k , v ) in items ] SECURE_CLIENT_MECHANISMS [ : ] = [ k for ( k , v ) in items if v . _pyxmpp_sasl_secure ]
7263	def use ( cls , name , method : [ str , Set , List ] , url = None ) : if not isinstance ( method , ( str , list , set , tuple ) ) : raise BaseException ( 'Invalid type of method: %s' % type ( method ) . __name__ ) if isinstance ( method , str ) : method = { method } # TODO: check methods available cls . _interface [ name ] = [ { 'method' : method , 'url' : url } ]
701	def firstNonFullGeneration ( self , swarmId , minNumParticles ) : if not swarmId in self . _swarmNumParticlesPerGeneration : return None numPsPerGen = self . _swarmNumParticlesPerGeneration [ swarmId ] numPsPerGen = numpy . array ( numPsPerGen ) firstNonFull = numpy . where ( numPsPerGen < minNumParticles ) [ 0 ] if len ( firstNonFull ) == 0 : return len ( numPsPerGen ) else : return firstNonFull [ 0 ]
11935	def reuse ( context , block_list , * * kwargs ) : try : block_context = context . render_context [ BLOCK_CONTEXT_KEY ] except KeyError : block_context = BlockContext ( ) if not isinstance ( block_list , ( list , tuple ) ) : block_list = [ block_list ] for block in block_list : block = block_context . get_block ( block ) if block : break else : return '' with context . push ( kwargs ) : return block . render ( context )
1465	def get_command_handlers ( ) : return { 'activate' : activate , 'config' : hconfig , 'deactivate' : deactivate , 'help' : cli_help , 'kill' : kill , 'restart' : restart , 'submit' : submit , 'update' : update , 'version' : version }
8512	def _create_kernel ( self ) : # Check kernels kernels = self . kernel_params if not isinstance ( kernels , list ) : raise RuntimeError ( 'Must provide enumeration of kernels' ) for kernel in kernels : if sorted ( list ( kernel . keys ( ) ) ) != [ 'name' , 'options' , 'params' ] : raise RuntimeError ( 'strategy/params/kernels must contain keys: "name", "options", "params"' ) # Turn into entry points. # TODO use eval to allow user to specify internal variables for kernels (e.g. V) in config file. kernels = [ ] for kern in self . kernel_params : params = kern [ 'params' ] options = kern [ 'options' ] name = kern [ 'name' ] kernel_ep = load_entry_point ( name , 'strategy/params/kernels' ) if issubclass ( kernel_ep , KERNEL_BASE_CLASS ) : if options [ 'independent' ] : # TODO Catch errors here? Estimator entry points don't catch instantiation errors kernel = np . sum ( [ kernel_ep ( 1 , active_dims = [ i ] , * * params ) for i in range ( self . n_dims ) ] ) else : kernel = kernel_ep ( self . n_dims , * * params ) if not isinstance ( kernel , KERNEL_BASE_CLASS ) : raise RuntimeError ( 'strategy/params/kernel must load a' 'GPy derived Kernel' ) kernels . append ( kernel ) self . kernel = np . sum ( kernels )
9207	def add_prefix ( multicodec , bytes_ ) : prefix = get_prefix ( multicodec ) return b'' . join ( [ prefix , bytes_ ] )
1073	def getdelimited ( self , beginchar , endchars , allowcomments = 1 ) : if self . field [ self . pos ] != beginchar : return '' slist = [ '' ] quote = 0 self . pos += 1 while self . pos < len ( self . field ) : if quote == 1 : slist . append ( self . field [ self . pos ] ) quote = 0 elif self . field [ self . pos ] in endchars : self . pos += 1 break elif allowcomments and self . field [ self . pos ] == '(' : slist . append ( self . getcomment ( ) ) continue # have already advanced pos from getcomment elif self . field [ self . pos ] == '\\' : quote = 1 else : slist . append ( self . field [ self . pos ] ) self . pos += 1 return '' . join ( slist )
9225	def permutations_with_replacement ( iterable , r = None ) : pool = tuple ( iterable ) n = len ( pool ) r = n if r is None else r for indices in itertools . product ( range ( n ) , repeat = r ) : yield list ( pool [ i ] for i in indices )
2625	def submit ( self , command = 'sleep 1' , blocksize = 1 , tasks_per_node = 1 , job_name = "parsl.auto" ) : job_name = "parsl.auto.{0}" . format ( time . time ( ) ) wrapped_cmd = self . launcher ( command , tasks_per_node , self . nodes_per_block ) [ instance , * rest ] = self . spin_up_instance ( command = wrapped_cmd , job_name = job_name ) if not instance : logger . error ( "Failed to submit request to EC2" ) return None logger . debug ( "Started instance_id: {0}" . format ( instance . instance_id ) ) state = translate_table . get ( instance . state [ 'Name' ] , "PENDING" ) self . resources [ instance . instance_id ] = { "job_id" : instance . instance_id , "instance" : instance , "status" : state } return instance . instance_id
1379	def print_build_info ( zipped_pex = False ) : if zipped_pex : release_file = get_zipped_heron_release_file ( ) else : release_file = get_heron_release_file ( ) with open ( release_file ) as release_info : release_map = yaml . load ( release_info ) release_items = sorted ( release_map . items ( ) , key = lambda tup : tup [ 0 ] ) for key , value in release_items : print ( "%s : %s" % ( key , value ) )
6589	def open ( self ) : self . workingArea . open ( ) self . runid_pkgidx_map = { } self . runid_to_return = deque ( )
999	def printColConfidence ( self , aState , maxCols = 20 ) : def formatFPRow ( var ) : s = '' for c in range ( min ( maxCols , self . numberOfCols ) ) : if c > 0 and c % 10 == 0 : s += ' ' s += ' %5.3f' % var [ c ] s += ' ' return s print formatFPRow ( aState )
10722	def _wrapper ( func ) : @ functools . wraps ( func ) def the_func ( expr ) : """ The actual function. :param object expr: the expression to be xformed to dbus-python types """ try : return func ( expr ) except ( TypeError , ValueError ) as err : raise IntoDPValueError ( expr , "expr" , "could not be transformed" ) from err return the_func
12875	def not_followed_by ( parser ) : @ tri def not_followed_by_block ( ) : failed = object ( ) result = optional ( tri ( parser ) , failed ) if result != failed : fail ( [ "not " + _fun_to_str ( parser ) ] ) choice ( not_followed_by_block )
12276	def barcode ( iban , reference , amount , due = None ) : iban = iban . replace ( ' ' , '' ) reference = reference . replace ( ' ' , '' ) if reference . startswith ( 'RF' ) : version = 5 else : version = 4 if version == 5 : reference = reference [ 2 : ] # test RF and add 00 where needed if len ( reference ) < 23 : reference = reference [ : 2 ] + ( "0" * ( 23 - len ( reference ) ) ) + reference [ 2 : ] elif version == 4 : reference = reference . zfill ( 20 ) if not iban . startswith ( 'FI' ) : raise BarcodeException ( 'Barcodes can be printed only for IBANs starting with FI' ) iban = iban [ 2 : ] amount = "%08d" % ( amount . quantize ( Decimal ( '.01' ) ) . shift ( 2 ) . to_integral_value ( ) ) if len ( amount ) != 8 : raise BarcodeException ( "Barcode payment amount must be less than 1000000.00" ) if due : due = due . strftime ( "%y%m%d" ) else : due = "000000" if version == 4 : barcode = "%s%s%s000%s%s" % ( version , iban , amount , reference , due ) elif version == 5 : barcode = "%s%s%s%s%s" % ( version , iban , amount , reference , due ) return barcode
7035	def submit_post_searchquery ( url , data , apikey ) : # first, we need to convert any columns and collections items to broken out # params postdata = { } for key in data : if key == 'columns' : postdata [ 'columns[]' ] = data [ key ] elif key == 'collections' : postdata [ 'collections[]' ] = data [ key ] else : postdata [ key ] = data [ key ] # do the urlencode with doseq=True # we also need to encode to bytes encoded_postdata = urlencode ( postdata , doseq = True ) . encode ( ) # if apikey is not None, add it in as an Authorization: Bearer [apikey] # header if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } LOGINFO ( 'submitting search query to LCC-Server API URL: %s' % url ) try : # hit the server with a POST request req = Request ( url , data = encoded_postdata , headers = headers ) resp = urlopen ( req ) if resp . code == 200 : # we'll iterate over the lines in the response # this works super-well for ND-JSON! for line in resp : data = json . loads ( line ) msg = data [ 'message' ] status = data [ 'status' ] if status != 'failed' : LOGINFO ( 'status: %s, %s' % ( status , msg ) ) else : LOGERROR ( 'status: %s, %s' % ( status , msg ) ) # here, we'll decide what to do about the query # completed query or query sent to background... if status in ( 'ok' , 'background' ) : setid = data [ 'result' ] [ 'setid' ] # save the data pickle to astrobase lccs directory outpickle = os . path . join ( os . path . expanduser ( '~' ) , '.astrobase' , 'lccs' , 'query-%s.pkl' % setid ) if not os . path . exists ( os . path . dirname ( outpickle ) ) : os . makedirs ( os . path . dirname ( outpickle ) ) with open ( outpickle , 'wb' ) as outfd : pickle . dump ( data , outfd , pickle . HIGHEST_PROTOCOL ) LOGINFO ( 'saved query info to %s, use this to ' 'download results later with ' 'retrieve_dataset_files' % outpickle ) # we're done at this point, return return status , data , data [ 'result' ] [ 'setid' ] # the query probably failed... elif status == 'failed' : # we're done at this point, return return status , data , None # if the response was not OK, then we probably failed else : try : data = json . load ( resp ) msg = data [ 'message' ] LOGERROR ( msg ) return 'failed' , None , None except Exception as e : LOGEXCEPTION ( 'failed to submit query to %s' % url ) return 'failed' , None , None except HTTPError as e : LOGERROR ( 'could not submit query to LCC API at: %s' % url ) LOGERROR ( 'HTTP status code was %s, reason: %s' % ( e . code , e . reason ) ) return 'failed' , None , None
1174	def lock ( self , function , argument ) : if self . testandset ( ) : function ( argument ) else : self . queue . append ( ( function , argument ) )
6331	def encode ( self , word , terminator = '\0' ) : if word : if terminator in word : raise ValueError ( 'Specified terminator, {}, already in word.' . format ( terminator if terminator != '\0' else '\\0' ) ) else : word += terminator wordlist = sorted ( word [ i : ] + word [ : i ] for i in range ( len ( word ) ) ) return '' . join ( [ w [ - 1 ] for w in wordlist ] ) else : return terminator
5579	def write_output_metadata ( output_params ) : if "path" in output_params : metadata_path = os . path . join ( output_params [ "path" ] , "metadata.json" ) logger . debug ( "check for output %s" , metadata_path ) try : existing_params = read_output_metadata ( metadata_path ) logger . debug ( "%s exists" , metadata_path ) logger . debug ( "existing output parameters: %s" , pformat ( existing_params ) ) existing_tp = existing_params [ "pyramid" ] current_params = params_to_dump ( output_params ) logger . debug ( "current output parameters: %s" , pformat ( current_params ) ) current_tp = BufferedTilePyramid ( * * current_params [ "pyramid" ] ) if existing_tp != current_tp : raise MapcheteConfigError ( "pyramid definitions between existing and new output do not match: " "%s != %s" % ( existing_tp , current_tp ) ) existing_format = existing_params [ "driver" ] [ "format" ] current_format = current_params [ "driver" ] [ "format" ] if existing_format != current_format : raise MapcheteConfigError ( "existing output format does not match new output format: " "%s != %s" % ( ( existing_format , current_format ) ) ) except FileNotFoundError : logger . debug ( "%s does not exist" , metadata_path ) dump_params = params_to_dump ( output_params ) # dump output metadata write_json ( metadata_path , dump_params ) else : logger . debug ( "no path parameter found" )
7775	def rfc2425encode ( name , value , parameters = None , charset = "utf-8" ) : if not parameters : parameters = { } if type ( value ) is unicode : value = value . replace ( u"\r\n" , u"\\n" ) value = value . replace ( u"\n" , u"\\n" ) value = value . replace ( u"\r" , u"\\n" ) value = value . encode ( charset , "replace" ) elif type ( value ) is not str : raise TypeError ( "Bad type for rfc2425 value" ) elif not valid_string_re . match ( value ) : parameters [ "encoding" ] = "b" value = binascii . b2a_base64 ( value ) ret = str ( name ) . lower ( ) for k , v in parameters . items ( ) : ret += ";%s=%s" % ( str ( k ) , str ( v ) ) ret += ":" while ( len ( value ) > 70 ) : ret += value [ : 70 ] + "\r\n " value = value [ 70 : ] ret += value + "\r\n" return ret
1004	def _inferPhase2 ( self ) : # Init to zeros to start self . infPredictedState [ 't' ] . fill ( 0 ) self . cellConfidence [ 't' ] . fill ( 0 ) self . colConfidence [ 't' ] . fill ( 0 ) # Phase 2 - Compute new predicted state and update cell and column # confidences for c in xrange ( self . numberOfCols ) : # For each cell in the column for i in xrange ( self . cellsPerColumn ) : # For each segment in the cell for s in self . cells [ c ] [ i ] : # See if it has the min number of active synapses numActiveSyns = self . _getSegmentActivityLevel ( s , self . infActiveState [ 't' ] , connectedSynapsesOnly = False ) if numActiveSyns < self . activationThreshold : continue # Incorporate the confidence into the owner cell and column if self . verbosity >= 6 : print "incorporating DC from cell[%d,%d]: " % ( c , i ) , s . debugPrint ( ) dc = s . dutyCycle ( ) self . cellConfidence [ 't' ] [ c , i ] += dc self . colConfidence [ 't' ] [ c ] += dc # If we reach threshold on the connected synapses, predict it # If not active, skip over it if self . _isSegmentActive ( s , self . infActiveState [ 't' ] ) : self . infPredictedState [ 't' ] [ c , i ] = 1 # Normalize column and cell confidences sumConfidences = self . colConfidence [ 't' ] . sum ( ) if sumConfidences > 0 : self . colConfidence [ 't' ] /= sumConfidences self . cellConfidence [ 't' ] /= sumConfidences # Are we predicting the required minimum number of columns? numPredictedCols = self . infPredictedState [ 't' ] . max ( axis = 1 ) . sum ( ) if numPredictedCols >= 0.5 * self . avgInputDensity : return True else : return False
10315	def pair_has_contradiction ( graph : BELGraph , u : BaseEntity , v : BaseEntity ) -> bool : relations = { data [ RELATION ] for data in graph [ u ] [ v ] . values ( ) } return relation_set_has_contradictions ( relations )
5706	def clean ( self ) : cleaned_data = super ( AuthForm , self ) . clean ( ) user = self . get_user ( ) if self . staff_only and ( not user or not user . is_staff ) : raise forms . ValidationError ( 'Sorry, only staff are allowed.' ) if self . superusers_only and ( not user or not user . is_superuser ) : raise forms . ValidationError ( 'Sorry, only superusers are allowed.' ) return cleaned_data
2951	def _on_trigger ( self , my_task ) : for task in my_task . workflow . task_tree . _find_any ( self ) : if task . thread_id != my_task . thread_id : continue self . _do_join ( task )
9302	def regenerate_signing_key ( self , secret_key = None , region = None , service = None , date = None ) : if secret_key is None and ( self . signing_key is None or self . signing_key . secret_key is None ) : raise NoSecretKeyError secret_key = secret_key or self . signing_key . secret_key region = region or self . region service = service or self . service date = date or self . date if self . signing_key is None : store_secret_key = True else : store_secret_key = self . signing_key . store_secret_key self . signing_key = AWS4SigningKey ( secret_key , region , service , date , store_secret_key ) self . region = region self . service = service self . date = self . signing_key . date
6010	def load_poisson_noise_map ( poisson_noise_map_path , poisson_noise_map_hdu , pixel_scale , convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map , poisson_noise_map_from_image , image , exposure_time_map , convert_from_electrons , gain , convert_from_adus ) : poisson_noise_map_options = sum ( [ convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map , poisson_noise_map_from_image ] ) if poisson_noise_map_options == 0 and poisson_noise_map_path is not None : return PoissonNoiseMap . from_fits_with_pixel_scale ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu , pixel_scale = pixel_scale ) elif poisson_noise_map_from_image : if not ( convert_from_electrons or convert_from_adus ) and exposure_time_map is None : raise exc . DataException ( 'Cannot compute the Poisson noise-map from the image if an ' 'exposure-time (or exposure time map) is not supplied to convert to adus' ) if convert_from_adus and gain is None : raise exc . DataException ( 'Cannot compute the Poisson noise-map from the image if a' 'gain is not supplied to convert from adus' ) return PoissonNoiseMap . from_image_and_exposure_time_map ( pixel_scale = pixel_scale , image = image , exposure_time_map = exposure_time_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) elif convert_poisson_noise_map_from_weight_map and poisson_noise_map_path is not None : weight_map = Array . from_fits ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu ) return PoissonNoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_poisson_noise_map_from_inverse_noise_map and poisson_noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu ) return PoissonNoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) else : return None
4926	def transform_title ( self , content_metadata_item ) : title_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : title_with_locales . append ( { 'locale' : locale , 'value' : content_metadata_item . get ( 'title' , '' ) } ) return title_with_locales
10391	def workflow_aggregate ( graph : BELGraph , node : BaseEntity , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , aggregator : Optional [ Callable [ [ Iterable [ float ] ] , float ] ] = None , ) -> Optional [ float ] : runners = workflow ( graph , node , key = key , tag = tag , default_score = default_score , runs = runs ) scores = [ runner . get_final_score ( ) for runner in runners ] if not scores : log . warning ( 'Unable to run the heat diffusion workflow for %s' , node ) return if aggregator is None : return np . average ( scores ) return aggregator ( scores )
12627	def iter_recursive_find ( folder_path , * regex ) : for root , dirs , files in os . walk ( folder_path ) : if len ( files ) > 0 : outlist = [ ] for f in files : for reg in regex : if re . search ( reg , f ) : outlist . append ( op . join ( root , f ) ) if len ( outlist ) == len ( regex ) : yield outlist
9502	def intersection ( l1 , l2 ) : if len ( l1 ) == 0 or len ( l2 ) == 0 : return [ ] out = [ ] l2_pos = 0 for l in l1 : while l2_pos < len ( l2 ) and l2 [ l2_pos ] . end < l . start : l2_pos += 1 if l2_pos == len ( l2 ) : break while l2_pos < len ( l2 ) and l . intersects ( l2 [ l2_pos ] ) : out . append ( l . intersection ( l2 [ l2_pos ] ) ) l2_pos += 1 l2_pos = max ( 0 , l2_pos - 1 ) return out
11532	def setup ( self , address , rack = 0 , slot = 1 , port = 102 ) : rack = int ( rack ) slot = int ( slot ) port = int ( port ) address = str ( address ) self . _client = snap7 . client . Client ( ) self . _client . connect ( address , rack , slot , port )
7558	def resolve_ambigs ( tmpseq ) : ## the order of rows in GETCONS for aidx in xrange ( 6 ) : #np.uint([82, 75, 83, 89, 87, 77]): ambig , res1 , res2 = GETCONS [ aidx ] ## get true wherever tmpseq is ambig idx , idy = np . where ( tmpseq == ambig ) halfmask = np . random . choice ( np . array ( [ True , False ] ) , idx . shape [ 0 ] ) for col in xrange ( idx . shape [ 0 ] ) : if halfmask [ col ] : tmpseq [ idx [ col ] , idy [ col ] ] = res1 else : tmpseq [ idx [ col ] , idy [ col ] ] = res2 return tmpseq
12051	def getParent2 ( abfFname , groups ) : if ".abf" in abfFname : abfFname = os . path . basename ( abfFname ) . replace ( ".abf" , "" ) for parentID in groups . keys ( ) : if abfFname in groups [ parentID ] : return parentID return abfFname
3386	def _random_point ( self ) : idx = np . random . randint ( self . n_warmup , size = min ( 2 , np . ceil ( np . sqrt ( self . n_warmup ) ) ) ) return self . warmup [ idx , : ] . mean ( axis = 0 )
1629	def GetHeaderGuardCPPVariable ( filename ) : # Restores original filename in case that cpplint is invoked from Emacs's # flymake. filename = re . sub ( r'_flymake\.h$' , '.h' , filename ) filename = re . sub ( r'/\.flymake/([^/]*)$' , r'/\1' , filename ) # Replace 'c++' with 'cpp'. filename = filename . replace ( 'C++' , 'cpp' ) . replace ( 'c++' , 'cpp' ) fileinfo = FileInfo ( filename ) file_path_from_root = fileinfo . RepositoryName ( ) if _root : suffix = os . sep # On Windows using directory separator will leave us with # "bogus escape error" unless we properly escape regex. if suffix == '\\' : suffix += '\\' file_path_from_root = re . sub ( '^' + _root + suffix , '' , file_path_from_root ) return re . sub ( r'[^a-zA-Z0-9]' , '_' , file_path_from_root ) . upper ( ) + '_'
4751	def run ( self , shell = True , cmdline = False , echo = True ) : if env ( ) : return 1 cmd = [ "fio" ] + self . __parse_parms ( ) if cmdline : cij . emph ( "cij.fio.run: shell: %r, cmd: %r" % ( shell , cmd ) ) return cij . ssh . command ( cmd , shell , echo )
8121	def union ( self , b ) : mx , my = min ( self . x , b . x ) , min ( self . y , b . y ) return Bounds ( mx , my , max ( self . x + self . width , b . x + b . width ) - mx , max ( self . y + self . height , b . y + b . height ) - my )
42	def wrap_deepmind_retro ( env , scale = True , frame_stack = 4 ) : env = WarpFrame ( env ) env = ClipRewardEnv ( env ) if frame_stack > 1 : env = FrameStack ( env , frame_stack ) if scale : env = ScaledFloatFrame ( env ) return env
2528	def get_annotation_date ( self , r_term ) : annotation_date_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'annotationDate' ] , None ) ) ) if len ( annotation_date_list ) != 1 : self . error = True msg = 'Annotation must have exactly one annotation date.' self . logger . log ( msg ) return return six . text_type ( annotation_date_list [ 0 ] [ 2 ] )
6168	def from_bin ( bin_array ) : width = len ( bin_array ) bin_wgts = 2 ** np . arange ( width - 1 , - 1 , - 1 ) return int ( np . dot ( bin_array , bin_wgts ) )
10462	def doesmenuitemexist ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) return 1 except LdtpServerException : return 0
1032	def b16decode ( s , casefold = False ) : if casefold : s = s . upper ( ) if re . search ( '[^0-9A-F]' , s ) : raise TypeError ( 'Non-base16 digit found' ) return binascii . unhexlify ( s )
8738	def get_ports_count ( context , filters = None ) : LOG . info ( "get_ports_count for tenant %s filters %s" % ( context . tenant_id , filters ) ) return db_api . port_count_all ( context , join_security_groups = True , * * filters )
5921	def strip_fit ( self , * * kwargs ) : kwargs . setdefault ( 'fit' , 'rot+trans' ) kw_fit = { } for k in ( 'xy' , 'fit' , 'fitgroup' , 'input' ) : if k in kwargs : kw_fit [ k ] = kwargs . pop ( k ) kwargs [ 'input' ] = kwargs . pop ( 'strip_input' , [ 'Protein' ] ) kwargs [ 'force' ] = kw_fit [ 'force' ] = kwargs . pop ( 'force' , self . force ) paths = self . strip_water ( * * kwargs ) # updates self.nowater transformer_nowater = self . nowater [ paths [ 'xtc' ] ] # make sure to get the one we just produced return transformer_nowater . fit ( * * kw_fit )
2415	def write_review ( review , out ) : out . write ( '# Review\n\n' ) write_value ( 'Reviewer' , review . reviewer , out ) write_value ( 'ReviewDate' , review . review_date_iso_format , out ) if review . has_comment : write_text_value ( 'ReviewComment' , review . comment , out )
4050	def last_modified_version ( self , * * kwargs ) : self . items ( * * kwargs ) return int ( self . request . headers . get ( "last-modified-version" , 0 ) )
59	def intersection ( self , other , default = None ) : x1_i = max ( self . x1 , other . x1 ) y1_i = max ( self . y1 , other . y1 ) x2_i = min ( self . x2 , other . x2 ) y2_i = min ( self . y2 , other . y2 ) if x1_i > x2_i or y1_i > y2_i : return default else : return BoundingBox ( x1 = x1_i , y1 = y1_i , x2 = x2_i , y2 = y2_i )
1476	def _get_ckptmgr_process ( self ) : ckptmgr_main_class = 'org.apache.heron.ckptmgr.CheckpointManager' ckptmgr_ram_mb = self . checkpoint_manager_ram / ( 1024 * 1024 ) ckptmgr_cmd = [ os . path . join ( self . heron_java_home , "bin/java" ) , '-Xms%dM' % ckptmgr_ram_mb , '-Xmx%dM' % ckptmgr_ram_mb , '-XX:+PrintCommandLineFlags' , '-verbosegc' , '-XX:+PrintGCDetails' , '-XX:+PrintGCTimeStamps' , '-XX:+PrintGCDateStamps' , '-XX:+PrintGCCause' , '-XX:+UseGCLogFileRotation' , '-XX:NumberOfGCLogFiles=5' , '-XX:GCLogFileSize=100M' , '-XX:+PrintPromotionFailure' , '-XX:+PrintTenuringDistribution' , '-XX:+PrintHeapAtGC' , '-XX:+HeapDumpOnOutOfMemoryError' , '-XX:+UseConcMarkSweepGC' , '-XX:+UseConcMarkSweepGC' , '-Xloggc:log-files/gc.ckptmgr.log' , '-Djava.net.preferIPv4Stack=true' , '-cp' , self . checkpoint_manager_classpath , ckptmgr_main_class , '-t' + self . topology_name , '-i' + self . topology_id , '-c' + self . ckptmgr_ids [ self . shard ] , '-p' + self . checkpoint_manager_port , '-f' + self . stateful_config_file , '-o' + self . override_config_file , '-g' + self . heron_internals_config_file ] retval = { } retval [ self . ckptmgr_ids [ self . shard ] ] = Command ( ckptmgr_cmd , self . shell_env ) return retval
1875	def PSUBB ( cpu , dest , src ) : result = [ ] value_a = dest . read ( ) value_b = src . read ( ) for i in reversed ( range ( 0 , dest . size , 8 ) ) : a = Operators . EXTRACT ( value_a , i , 8 ) b = Operators . EXTRACT ( value_b , i , 8 ) result . append ( ( a - b ) & 0xff ) dest . write ( Operators . CONCAT ( 8 * len ( result ) , * result ) )
2102	def configure_model ( self , attrs , field_name ) : self . relationship = field_name self . _set_method_names ( relationship = field_name ) if self . res_name is None : self . res_name = grammar . singularize ( attrs . get ( 'endpoint' , 'unknown' ) . strip ( '/' ) )
2695	def parse_doc ( json_iter ) : global DEBUG for meta in json_iter : base_idx = 0 for graf_text in filter_quotes ( meta [ "text" ] , is_email = False ) : if DEBUG : print ( "graf_text:" , graf_text ) grafs , new_base_idx = parse_graf ( meta [ "id" ] , graf_text , base_idx ) base_idx = new_base_idx for graf in grafs : yield graf
10804	def resolve_admin_type ( admin ) : if admin is current_user or isinstance ( admin , UserMixin ) : return 'User' else : return admin . __class__ . __name__
1647	def CheckCheck ( filename , clean_lines , linenum , error ) : # Decide the set of replacement macros that should be suggested lines = clean_lines . elided ( check_macro , start_pos ) = FindCheckMacro ( lines [ linenum ] ) if not check_macro : return # Find end of the boolean expression by matching parentheses ( last_line , end_line , end_pos ) = CloseExpression ( clean_lines , linenum , start_pos ) if end_pos < 0 : return # If the check macro is followed by something other than a # semicolon, assume users will log their own custom error messages # and don't suggest any replacements. if not Match ( r'\s*;' , last_line [ end_pos : ] ) : return if linenum == end_line : expression = lines [ linenum ] [ start_pos + 1 : end_pos - 1 ] else : expression = lines [ linenum ] [ start_pos + 1 : ] for i in xrange ( linenum + 1 , end_line ) : expression += lines [ i ] expression += last_line [ 0 : end_pos - 1 ] # Parse expression so that we can take parentheses into account. # This avoids false positives for inputs like "CHECK((a < 4) == b)", # which is not replaceable by CHECK_LE. lhs = '' rhs = '' operator = None while expression : matched = Match ( r'^\s*(<<|<<=|>>|>>=|->\*|->|&&|\|\||' r'==|!=|>=|>|<=|<|\()(.*)$' , expression ) if matched : token = matched . group ( 1 ) if token == '(' : # Parenthesized operand expression = matched . group ( 2 ) ( end , _ ) = FindEndOfExpressionInLine ( expression , 0 , [ '(' ] ) if end < 0 : return # Unmatched parenthesis lhs += '(' + expression [ 0 : end ] expression = expression [ end : ] elif token in ( '&&' , '||' ) : # Logical and/or operators. This means the expression # contains more than one term, for example: # CHECK(42 < a && a < b); # # These are not replaceable with CHECK_LE, so bail out early. return elif token in ( '<<' , '<<=' , '>>' , '>>=' , '->*' , '->' ) : # Non-relational operator lhs += token expression = matched . group ( 2 ) else : # Relational operator operator = token rhs = matched . group ( 2 ) break else : # Unparenthesized operand. Instead of appending to lhs one character # at a time, we do another regular expression match to consume several # characters at once if possible. Trivial benchmark shows that this # is more efficient when the operands are longer than a single # character, which is generally the case. matched = Match ( r'^([^-=!<>()&|]+)(.*)$' , expression ) if not matched : matched = Match ( r'^(\s*\S)(.*)$' , expression ) if not matched : break lhs += matched . group ( 1 ) expression = matched . group ( 2 ) # Only apply checks if we got all parts of the boolean expression if not ( lhs and operator and rhs ) : return # Check that rhs do not contain logical operators. We already know # that lhs is fine since the loop above parses out && and ||. if rhs . find ( '&&' ) > - 1 or rhs . find ( '||' ) > - 1 : return # At least one of the operands must be a constant literal. This is # to avoid suggesting replacements for unprintable things like # CHECK(variable != iterator) # # The following pattern matches decimal, hex integers, strings, and # characters (in that order). lhs = lhs . strip ( ) rhs = rhs . strip ( ) match_constant = r'^([-+]?(\d+|0[xX][0-9a-fA-F]+)[lLuU]{0,3}|".*"|\'.*\')$' if Match ( match_constant , lhs ) or Match ( match_constant , rhs ) : # Note: since we know both lhs and rhs, we can provide a more # descriptive error message like: # Consider using CHECK_EQ(x, 42) instead of CHECK(x == 42) # Instead of: # Consider using CHECK_EQ instead of CHECK(a == b) # # We are still keeping the less descriptive message because if lhs # or rhs gets long, the error message might become unreadable. error ( filename , linenum , 'readability/check' , 2 , 'Consider using %s instead of %s(a %s b)' % ( _CHECK_REPLACEMENT [ check_macro ] [ operator ] , check_macro , operator ) )
6976	def keplermag_to_sdssr ( keplermag , kic_sdssg , kic_sdssr ) : kic_sdssgr = kic_sdssg - kic_sdssr if kic_sdssgr < 0.8 : kepsdssr = ( keplermag - 0.2 * kic_sdssg ) / 0.8 else : kepsdssr = ( keplermag - 0.1 * kic_sdssg ) / 0.9 return kepsdssr
8728	def strftime ( fmt , t ) : if isinstance ( t , ( time . struct_time , tuple ) ) : t = datetime . datetime ( * t [ : 6 ] ) assert isinstance ( t , ( datetime . datetime , datetime . time , datetime . date ) ) try : year = t . year if year < 1900 : t = t . replace ( year = 1900 ) except AttributeError : year = 1900 subs = ( ( '%Y' , '%04d' % year ) , ( '%y' , '%02d' % ( year % 100 ) ) , ( '%s' , '%03d' % ( t . microsecond // 1000 ) ) , ( '%u' , '%03d' % ( t . microsecond % 1000 ) ) ) def doSub ( s , sub ) : return s . replace ( * sub ) def doSubs ( s ) : return functools . reduce ( doSub , subs , s ) fmt = '%%' . join ( map ( doSubs , fmt . split ( '%%' ) ) ) return t . strftime ( fmt )
9665	def clean_all ( G , settings ) : quiet = settings [ "quiet" ] recon = settings [ "recon" ] sprint = settings [ "sprint" ] error = settings [ "error" ] all_outputs = [ ] for node in G . nodes ( data = True ) : if "output" in node [ 1 ] : for item in get_all_outputs ( node [ 1 ] ) : all_outputs . append ( item ) all_outputs . append ( ".shastore" ) retcode = 0 for item in sorted ( all_outputs ) : if os . path . isfile ( item ) : if recon : sprint ( "Would remove file: {}" . format ( item ) ) continue sprint ( "Attempting to remove file '{}'" , level = "verbose" ) try : os . remove ( item ) sprint ( "Removed file" , level = "verbose" ) except : errmes = "Error: file '{}' failed to be removed" error ( errmes . format ( item ) ) retcode = 1 if not retcode and not recon : sprint ( "All clean" , color = True ) return retcode
8032	def pruneUI ( dupeList , mainPos = 1 , mainLen = 1 ) : dupeList = sorted ( dupeList ) print for pos , val in enumerate ( dupeList ) : print "%d) %s" % ( pos + 1 , val ) while True : choice = raw_input ( "[%s/%s] Keepers: " % ( mainPos , mainLen ) ) . strip ( ) if not choice : print ( "Please enter a space/comma-separated list of numbers or " "'all'." ) continue elif choice . lower ( ) == 'all' : return [ ] try : out = [ int ( x ) - 1 for x in choice . replace ( ',' , ' ' ) . split ( ) ] return [ val for pos , val in enumerate ( dupeList ) if pos not in out ] except ValueError : print ( "Invalid choice. Please enter a space/comma-separated list" "of numbers or 'all'." )
3463	def double_gene_deletion ( model , gene_list1 = None , gene_list2 = None , method = "fba" , solution = None , processes = None , * * kwargs ) : gene_list1 , gene_list2 = _element_lists ( model . genes , gene_list1 , gene_list2 ) return _multi_deletion ( model , 'gene' , element_lists = [ gene_list1 , gene_list2 ] , method = method , solution = solution , processes = processes , * * kwargs )
13089	def ensure_remote_branch_is_tracked ( branch ) : if branch == MASTER_BRANCH : # We don't need to explicitly track the master branch, so we're done. return # Ensure the specified branch is in the local branch list. output = subprocess . check_output ( [ 'git' , 'branch' , '--list' ] ) for line in output . split ( '\n' ) : if line . strip ( ) == branch : # We are already tracking the remote branch break else : # We are not tracking the remote branch, so track it. try : sys . stdout . write ( subprocess . check_output ( [ 'git' , 'checkout' , '--track' , 'origin/%s' % branch ] ) ) except subprocess . CalledProcessError : # Bail gracefully. raise SystemExit ( 1 )
850	def setParameter ( self , parameterName , index , parameterValue ) : if parameterName == 'topDownMode' : self . topDownMode = parameterValue elif parameterName == 'predictedField' : self . predictedField = parameterValue else : raise Exception ( 'Unknown parameter: ' + parameterName )
4016	def get_app_volume_mounts ( app_name , assembled_specs , test = False ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] volumes = [ get_command_files_volume_mount ( app_name , test = test ) ] volumes . append ( get_asset_volume_mount ( app_name ) ) repo_mount = _get_app_repo_volume_mount ( app_spec ) if repo_mount : volumes . append ( repo_mount ) volumes += _get_app_libs_volume_mounts ( app_name , assembled_specs ) return volumes
887	def _createSegment ( cls , connections , lastUsedIterationForSegment , cell , iteration , maxSegmentsPerCell ) : # Enforce maxSegmentsPerCell. while connections . numSegments ( cell ) >= maxSegmentsPerCell : leastRecentlyUsedSegment = min ( connections . segmentsForCell ( cell ) , key = lambda segment : lastUsedIterationForSegment [ segment . flatIdx ] ) connections . destroySegment ( leastRecentlyUsedSegment ) # Create the segment. segment = connections . createSegment ( cell ) # Do TM-specific bookkeeping for the segment. if segment . flatIdx == len ( lastUsedIterationForSegment ) : lastUsedIterationForSegment . append ( iteration ) elif segment . flatIdx < len ( lastUsedIterationForSegment ) : # A flatIdx was recycled. lastUsedIterationForSegment [ segment . flatIdx ] = iteration else : raise AssertionError ( "All segments should be created with the TM createSegment method." ) return segment
3318	def get ( self , token ) : self . _lock . acquire_read ( ) try : lock = self . _dict . get ( token ) if lock is None : # Lock not found: purge dangling URL2TOKEN entries _logger . debug ( "Lock purged dangling: {}" . format ( token ) ) self . delete ( token ) return None expire = float ( lock [ "expire" ] ) if expire >= 0 and expire < time . time ( ) : _logger . debug ( "Lock timed-out({}): {}" . format ( expire , lock_string ( lock ) ) ) self . delete ( token ) return None return lock finally : self . _lock . release ( )
5657	def main_make_views ( gtfs_fname ) : print ( "creating views" ) conn = GTFS ( fname_or_conn = gtfs_fname ) . conn for L in Loaders : L ( None ) . make_views ( conn ) conn . commit ( )
13725	def register_credentials ( self , credentials = None , user = None , user_file = None , password = None , password_file = None ) : # lets store all kind of credential data into this dict if credentials is not None : self . credentials = credentials else : self . credentials = { } # set the user from CLI or file if user : self . credentials [ "user" ] = user elif user_file : with open ( user_file , "r" ) as of : # what would the file entry look like? pattern = re . compile ( "^user: " ) for l in of : if re . match ( pattern , l ) : # strip away the newline l = l [ 0 : - 1 ] self . credentials [ "user" ] = re . sub ( pattern , "" , l ) # remove any surrounding quotes if self . credentials [ "user" ] [ 0 : 1 ] == '"' and self . credentials [ "user" ] [ - 1 : ] == '"' : self . credentials [ "user" ] = self . credentials [ "user" ] [ 1 : - 1 ] # set the password from CLI or file if password : self . credentials [ "password" ] = password elif password_file : with open ( password_file , "r" ) as of : # what would the file entry look like? pattern = re . compile ( "^password: " ) for l in of : if re . match ( pattern , l ) : # strip away the newline l = l [ 0 : - 1 ] self . credentials [ "password" ] = re . sub ( pattern , "" , l ) # remove any surrounding quotes if self . credentials [ "password" ] [ 0 : 1 ] == '"' and self . credentials [ "password" ] [ - 1 : ] == '"' : self . credentials [ "password" ] = self . credentials [ "password" ] [ 1 : - 1 ] # if both user and password is set, # 1. encode to base 64 for basic auth if "user" in self . credentials and "password" in self . credentials : c = self . credentials [ "user" ] + ":" + self . credentials [ "password" ] self . credentials [ "base64" ] = b64encode ( c . encode ( ) ) . decode ( "ascii" )
3558	def discover ( cls , device , timeout_sec = TIMEOUT_SEC ) : device . discover ( cls . SERVICES , cls . CHARACTERISTICS , timeout_sec )
3182	def create ( self , data ) : if 'id' not in data : raise KeyError ( 'The store must have an id' ) if 'list_id' not in data : raise KeyError ( 'The store must have a list_id' ) if 'name' not in data : raise KeyError ( 'The store must have a name' ) if 'currency_code' not in data : raise KeyError ( 'The store must have a currency_code' ) if not re . match ( r"^[A-Z]{3}$" , data [ 'currency_code' ] ) : raise ValueError ( 'The currency_code must be a valid 3-letter ISO 4217 currency code' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . store_id = response [ 'id' ] else : self . store_id = None return response
3946	def _decode_repeated_field ( message , field , value_list ) : if field . type == FieldDescriptor . TYPE_MESSAGE : for value in value_list : decode ( getattr ( message , field . name ) . add ( ) , value ) else : try : for value in value_list : if field . type == FieldDescriptor . TYPE_BYTES : value = base64 . b64decode ( value ) getattr ( message , field . name ) . append ( value ) except ( ValueError , TypeError ) as e : # ValueError: invalid enum value, negative unsigned int value, or # invalid base64 # TypeError: mismatched type logger . warning ( 'Message %r ignoring repeated field %s: %s' , message . __class__ . __name__ , field . name , e ) # Ignore any values already decoded by clearing list message . ClearField ( field . name )
12479	def rcfile ( appname , section = None , args = { } , strip_dashes = True ) : if strip_dashes : for k in args . keys ( ) : args [ k . lstrip ( '-' ) ] = args . pop ( k ) environ = get_environment ( appname ) if section is None : section = appname config = get_config ( appname , section , args . get ( 'config' , '' ) , args . get ( 'path' , '' ) ) config = merge ( merge ( args , config ) , environ ) if not config : raise IOError ( 'Could not find any rcfile for application ' '{}.' . format ( appname ) ) return config
628	def _orderForCoordinate ( cls , coordinate ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getReal64 ( )
12629	def recursive_glob ( base_directory , regex = '' ) : files = glob ( op . join ( base_directory , regex ) ) for path , dirlist , filelist in os . walk ( base_directory ) : for dir_name in dirlist : files . extend ( glob ( op . join ( path , dir_name , regex ) ) ) return files
8824	def context ( self ) : if not self . _context : self . _context = context . get_admin_context ( ) return self . _context
7146	def to_atomic ( amount ) : if not isinstance ( amount , ( Decimal , float ) + _integer_types ) : raise ValueError ( "Amount '{}' doesn't have numeric type. Only Decimal, int, long and " "float (not recommended) are accepted as amounts." ) return int ( amount * 10 ** 12 )
3942	async def _longpoll_request ( self ) : params = { 'VER' : 8 , # channel protocol version 'gsessionid' : self . _gsessionid_param , 'RID' : 'rpc' , # request identifier 't' : 1 , # trial 'SID' : self . _sid_param , # session ID 'CI' : 0 , # 0 if streaming/chunked requests should be used 'ctype' : 'hangouts' , # client type 'TYPE' : 'xmlhttp' , # type of request } logger . info ( 'Opening new long-polling request' ) try : async with self . _session . fetch_raw ( 'GET' , CHANNEL_URL , params = params ) as res : if res . status != 200 : if res . status == 400 and res . reason == 'Unknown SID' : raise ChannelSessionError ( 'SID became invalid' ) raise exceptions . NetworkError ( 'Request return unexpected status: {}: {}' . format ( res . status , res . reason ) ) while True : async with async_timeout . timeout ( PUSH_TIMEOUT ) : chunk = await res . content . read ( MAX_READ_BYTES ) if not chunk : break await self . _on_push_data ( chunk ) except asyncio . TimeoutError : raise exceptions . NetworkError ( 'Request timed out' ) except aiohttp . ServerDisconnectedError as err : raise exceptions . NetworkError ( 'Server disconnected error: %s' % err ) except aiohttp . ClientPayloadError : raise ChannelSessionError ( 'SID is about to expire' ) except aiohttp . ClientError as err : raise exceptions . NetworkError ( 'Request connection error: %s' % err )
11739	def move_dot ( self ) : return self . __class__ ( self . production , self . pos + 1 , self . lookahead )
10631	def clear ( self ) : self . _compound_mfrs = self . _compound_mfrs * 0.0 self . _P = 1.0 self . _T = 25.0 self . _H = 0.0
2185	def tryload ( self , cfgstr = None , on_error = 'raise' ) : cfgstr = self . _rectify_cfgstr ( cfgstr ) if self . enabled : try : if self . verbose > 1 : self . log ( '[cacher] tryload fname={}' . format ( self . fname ) ) return self . load ( cfgstr ) except IOError : if self . verbose > 0 : self . log ( '[cacher] ... {} cache miss' . format ( self . fname ) ) except Exception : if self . verbose > 0 : self . log ( '[cacher] ... failed to load' ) if on_error == 'raise' : raise elif on_error == 'clear' : self . clear ( cfgstr ) return None else : raise KeyError ( 'Unknown method on_error={}' . format ( on_error ) ) else : if self . verbose > 1 : self . log ( '[cacher] ... cache disabled: fname={}' . format ( self . fname ) ) return None
13494	def write ( args ) : logging . info ( "Writing configure file: %s" % args . config_file ) if args . config_file is None : return #Let's add each attribute of 'args' to the configure file config = cparser . ConfigParser ( ) config . add_section ( "lrcloud" ) for p in [ x for x in dir ( args ) if not x . startswith ( "_" ) ] : if p in IGNORE_ARGS : continue #We ignore some attributes value = getattr ( args , p ) if value is not None : config . set ( 'lrcloud' , p , str ( value ) ) with open ( args . config_file , 'w' ) as f : config . write ( f )
10913	def find_particles_in_tile ( positions , tile ) : bools = tile . contains ( positions ) return np . arange ( bools . size ) [ bools ]
2926	def create_package ( self ) : # Check that all files exist (and calculate the longest shared path # prefix): self . input_path_prefix = None for filename in self . input_files : if not os . path . isfile ( filename ) : raise ValueError ( '%s does not exist or is not a file' % filename ) if self . input_path_prefix : full = os . path . abspath ( os . path . dirname ( filename ) ) while not ( full . startswith ( self . input_path_prefix ) and self . input_path_prefix ) : self . input_path_prefix = self . input_path_prefix [ : - 1 ] else : self . input_path_prefix = os . path . abspath ( os . path . dirname ( filename ) ) # Parse all of the XML: self . bpmn = { } for filename in self . input_files : bpmn = ET . parse ( filename ) self . bpmn [ os . path . abspath ( filename ) ] = bpmn # Now run through pre-parsing and validation: for filename , bpmn in list ( self . bpmn . items ( ) ) : bpmn = self . pre_parse_and_validate ( bpmn , filename ) self . bpmn [ os . path . abspath ( filename ) ] = bpmn # Now check that we can parse it fine: for filename , bpmn in list ( self . bpmn . items ( ) ) : self . parser . add_bpmn_xml ( bpmn , filename = filename ) self . wf_spec = self . parser . get_spec ( self . entry_point_process ) # Now package everything: self . package_zip = zipfile . ZipFile ( self . package_file , "w" , compression = zipfile . ZIP_DEFLATED ) done_files = set ( ) for spec in self . wf_spec . get_specs_depth_first ( ) : filename = spec . file if filename not in done_files : done_files . add ( filename ) bpmn = self . bpmn [ os . path . abspath ( filename ) ] self . write_to_package_zip ( "%s.bpmn" % spec . name , ET . tostring ( bpmn . getroot ( ) ) ) self . write_file_to_package_zip ( "src/" + self . _get_zip_path ( filename ) , filename ) self . _call_editor_hook ( 'package_for_editor' , spec , filename ) self . write_meta_data ( ) self . write_manifest ( ) self . package_zip . close ( )
534	def _getRegions ( self ) : def makeRegion ( name , r ) : """Wrap a engine region with a nupic.engine_internal.Region Also passes the containing nupic.engine_internal.Network network in _network. This function is passed a value wrapper to the CollectionWrapper """ r = Region ( r , self ) #r._network = self return r regions = CollectionWrapper ( engine_internal . Network . getRegions ( self ) , makeRegion ) return regions
4550	def fill_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : fill_rect ( setter , x + r , y , w - 2 * r , h , color , aa ) _fill_circle_helper ( setter , x + w - r - 1 , y + r , r , 1 , h - 2 * r - 1 , color , aa ) _fill_circle_helper ( setter , x + r , y + r , r , 2 , h - 2 * r - 1 , color , aa )
5755	def get_regressions ( package_descriptors , targets , building_repo_data , testing_repo_data , main_repo_data ) : regressions = { } for package_descriptor in package_descriptors . values ( ) : pkg_name = package_descriptor . pkg_name debian_pkg_name = package_descriptor . debian_pkg_name regressions [ pkg_name ] = { } for target in targets : regressions [ pkg_name ] [ target ] = False main_version = main_repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) if main_version is not None : main_ver_loose = LooseVersion ( main_version ) for repo_data in [ building_repo_data , testing_repo_data ] : version = repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) if not version or main_ver_loose > LooseVersion ( version ) : regressions [ pkg_name ] [ target ] = True return regressions
134	def extract_from_image ( self , image ) : ia . do_assert ( image . ndim in [ 2 , 3 ] ) if len ( self . exterior ) <= 2 : raise Exception ( "Polygon must be made up of at least 3 points to extract its area from an image." ) bb = self . to_bounding_box ( ) bb_area = bb . extract_from_image ( image ) if self . is_out_of_image ( image , fully = True , partly = False ) : return bb_area xx = self . xx_int yy = self . yy_int xx_mask = xx - np . min ( xx ) yy_mask = yy - np . min ( yy ) height_mask = np . max ( yy_mask ) width_mask = np . max ( xx_mask ) rr_face , cc_face = skimage . draw . polygon ( yy_mask , xx_mask , shape = ( height_mask , width_mask ) ) mask = np . zeros ( ( height_mask , width_mask ) , dtype = np . bool ) mask [ rr_face , cc_face ] = True if image . ndim == 3 : mask = np . tile ( mask [ : , : , np . newaxis ] , ( 1 , 1 , image . shape [ 2 ] ) ) return bb_area * mask
8537	def push ( self , ip_packet ) : data_len = len ( ip_packet . data . data ) seq_id = ip_packet . data . seq if data_len == 0 : self . _next_seq_id = seq_id return False # have we seen this packet? if self . _next_seq_id != - 1 and seq_id != self . _next_seq_id : return False self . _next_seq_id = seq_id + data_len with self . _lock_packets : # Note: we only account for payload (i.e.: tcp data) self . _length += len ( ip_packet . data . data ) self . _remaining += len ( ip_packet . data . data ) self . _packets . append ( ip_packet ) return True
12163	def _check_limit ( self , event ) : if self . count ( event ) > self . max_listeners : warnings . warn ( 'Too many listeners for event {}' . format ( event ) , ResourceWarning , )
8177	def copy ( self , graph ) : l = self . __class__ ( graph , self . n ) l . i = 0 return l
3315	def _find ( self , url ) : # Query the permanent view to find a url vr = self . db . view ( "properties/by_url" , key = url , include_docs = True ) _logger . debug ( "find(%r) returned %s" % ( url , len ( vr ) ) ) assert len ( vr ) <= 1 , "Found multiple matches for %r" % url for row in vr : assert row . doc return row . doc return None
907	def replaceIterationCycle ( self , phaseSpecs ) : # ----------------------------------------------------------------------- # Replace our phase manager # self . __phaseManager = _PhaseManager ( model = self . __model , phaseSpecs = phaseSpecs ) return
11955	def is_dot ( ip ) : octets = str ( ip ) . split ( '.' ) if len ( octets ) != 4 : return False for i in octets : try : val = int ( i ) except ValueError : return False if val > 255 or val < 0 : return False return True
6635	def ignores ( self , path ) : test_path = PurePath ( '/' , path ) # also check any parent directories of this path against the ignore # patterns: test_paths = tuple ( [ test_path ] + list ( test_path . parents ) ) for exp in self . ignore_patterns : for tp in test_paths : if tp . match ( exp ) : logger . debug ( '"%s" ignored ("%s" matched "%s")' , path , tp , exp ) return True return False
2850	def _mpsse_enable ( self ) : # Reset MPSSE by sending mask = 0 and mode = 0 self . _check ( ftdi . set_bitmode , 0 , 0 ) # Enable MPSSE by sending mask = 0 and mode = 2 self . _check ( ftdi . set_bitmode , 0 , 2 )
12128	def summary ( self ) : print ( "Items: %s" % len ( self ) ) varying_keys = ', ' . join ( '%r' % k for k in self . varying_keys ) print ( "Varying Keys: %s" % varying_keys ) items = ', ' . join ( [ '%s=%r' % ( k , v ) for ( k , v ) in self . constant_items ] ) if self . constant_items : print ( "Constant Items: %s" % items )
8868	def _unique ( self , seq ) : # order preserving checked = [ ] for e in seq : present = False for c in checked : if str ( c ) == str ( e ) : present = True break if not present : checked . append ( e ) return checked
12839	def init_async ( self , loop ) : super ( PooledAIODatabase , self ) . init_async ( loop ) self . _waiters = collections . deque ( )
11717	def create ( self , config ) : assert config [ "name" ] == self . name , "Given config is not for this template" data = self . _json_encode ( config ) headers = self . _default_headers ( ) return self . _request ( "" , ok_status = None , data = data , headers = headers )
717	def __getHyperSearchJobIDFilePath ( cls , permWorkDir , outputLabel ) : # Get the base path and figure out the path of the report file. basePath = permWorkDir # Form the name of the output csv file that will contain all the results filename = "%s_HyperSearchJobID.pkl" % ( outputLabel , ) filepath = os . path . join ( basePath , filename ) return filepath
12451	def deref ( self , data ) : # We have to make a deepcopy here to create a proper JSON # compatible object, otherwise `json.dumps` fails when it # hits jsonref.JsonRef objects. deref = copy . deepcopy ( jsonref . JsonRef . replace_refs ( data ) ) # Write out JSON version because we might want this. self . write_template ( deref , filename = 'swagger.json' ) return deref
13326	def create ( name_or_path , config ) : if not name_or_path : ctx = click . get_current_context ( ) click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples:\n' ' cpenv create my_env\n' ' cpenv create ./relative/path/to/my_env\n' ' cpenv create my_env --config ./relative/path/to/config\n' ' cpenv create my_env --config git@github.com:user/config.git\n' ) click . echo ( examples ) return click . echo ( blue ( 'Creating a new virtual environment ' + name_or_path ) ) try : env = cpenv . create ( name_or_path , config ) except Exception as e : click . echo ( bold_red ( 'FAILED TO CREATE ENVIRONMENT!' ) ) click . echo ( e ) else : click . echo ( bold_green ( 'Successfully created environment!' ) ) click . echo ( blue ( 'Launching subshell' ) ) cpenv . activate ( env ) shell . launch ( env . name )
11789	def sample ( self ) : if self . sampler is None : self . sampler = weighted_sampler ( self . dictionary . keys ( ) , self . dictionary . values ( ) ) return self . sampler ( )
3410	def remove_from_model ( self , model = None , make_dependent_reactions_nonfunctional = True ) : warn ( "Use cobra.manipulation.remove_genes instead" ) if model is not None : if model != self . _model : raise Exception ( "%s is a member of %s, not %s" % ( repr ( self ) , repr ( self . _model ) , repr ( model ) ) ) if self . _model is None : raise Exception ( '%s is not in a model' % repr ( self ) ) if make_dependent_reactions_nonfunctional : gene_state = 'False' else : gene_state = 'True' the_gene_re = re . compile ( '(^|(?<=( |\()))%s(?=( |\)|$))' % re . escape ( self . id ) ) # remove reference to the gene in all groups associated_groups = self . _model . get_associated_groups ( self ) for group in associated_groups : group . remove_members ( self ) self . _model . genes . remove ( self ) self . _model = None for the_reaction in list ( self . _reaction ) : the_reaction . _gene_reaction_rule = the_gene_re . sub ( gene_state , the_reaction . gene_reaction_rule ) the_reaction . _genes . remove ( self ) # Now, deactivate the reaction if its gene association evaluates # to False the_gene_reaction_relation = the_reaction . gene_reaction_rule for other_gene in the_reaction . _genes : other_gene_re = re . compile ( '(^|(?<=( |\()))%s(?=( |\)|$))' % re . escape ( other_gene . id ) ) the_gene_reaction_relation = other_gene_re . sub ( 'True' , the_gene_reaction_relation ) if not eval ( the_gene_reaction_relation ) : the_reaction . lower_bound = 0 the_reaction . upper_bound = 0 self . _reaction . clear ( )
11941	def mark_all_read ( user ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) backend . inbox_purge ( user )
7531	def _plot_dag ( dag , results , snames ) : try : import matplotlib . pyplot as plt from matplotlib . dates import date2num from matplotlib . cm import gist_rainbow ## first figure is dag layout plt . figure ( "dag_layout" , figsize = ( 10 , 10 ) ) nx . draw ( dag , pos = nx . spring_layout ( dag ) , node_color = 'pink' , with_labels = True ) plt . savefig ( "./dag_layout.png" , bbox_inches = 'tight' , dpi = 200 ) ## second figure is times for steps pos = { } colors = { } for node in dag : #jobkey = "{}-{}".format(node, sample) mtd = results [ node ] . metadata start = date2num ( mtd . started ) #runtime = date2num(md.completed)# - start ## sample id to separate samples on x-axis _ , _ , sname = node . split ( "-" , 2 ) sid = snames . index ( sname ) ## 1e6 to separate on y-axis pos [ node ] = ( start + sid , start * 1e6 ) colors [ node ] = mtd . engine_id ## x just spaces out samples; ## y is start time of each job with edge leading to next job ## color is the engine that ran the job ## all jobs were submitted as 3 second wait times plt . figure ( "dag_starttimes" , figsize = ( 10 , 16 ) ) nx . draw ( dag , pos , node_list = colors . keys ( ) , node_color = colors . values ( ) , cmap = gist_rainbow , with_labels = True ) plt . savefig ( "./dag_starttimes.png" , bbox_inches = 'tight' , dpi = 200 ) except Exception as inst : LOGGER . warning ( inst )
169	def find_intersections_with ( self , other ) : import shapely . geometry geom = _convert_var_to_shapely_geometry ( other ) result = [ ] for p_start , p_end in zip ( self . coords [ : - 1 ] , self . coords [ 1 : ] ) : ls = shapely . geometry . LineString ( [ p_start , p_end ] ) intersections = ls . intersection ( geom ) intersections = list ( _flatten_shapely_collection ( intersections ) ) intersections_points = [ ] for inter in intersections : if isinstance ( inter , shapely . geometry . linestring . LineString ) : inter_start = ( inter . coords [ 0 ] [ 0 ] , inter . coords [ 0 ] [ 1 ] ) inter_end = ( inter . coords [ - 1 ] [ 0 ] , inter . coords [ - 1 ] [ 1 ] ) intersections_points . extend ( [ inter_start , inter_end ] ) else : assert isinstance ( inter , shapely . geometry . point . Point ) , ( "Expected to find shapely.geometry.point.Point or " "shapely.geometry.linestring.LineString intersection, " "actually found %s." % ( type ( inter ) , ) ) intersections_points . append ( ( inter . x , inter . y ) ) # sort by distance to start point, this makes it later on easier # to remove duplicate points inter_sorted = sorted ( intersections_points , key = lambda p : np . linalg . norm ( np . float32 ( p ) - p_start ) ) result . append ( inter_sorted ) return result
1140	def fill ( text , width = 70 , * * kwargs ) : w = TextWrapper ( width = width , * * kwargs ) return w . fill ( text )
8467	def path ( self , value ) : if not value . endswith ( '/' ) : self . _path = '{v}/' . format ( v = value ) else : self . _path = value
13438	def _extendrange ( self , start , end ) : range_positions = [ ] for i in range ( start , end ) : if i != 0 : range_positions . append ( str ( i ) ) if i < end : range_positions . append ( self . separator ) return range_positions
7726	def __init ( self , code ) : code = int ( code ) if code < 0 or code > 999 : raise ValueError ( "Bad status code" ) self . code = code
6658	def _core_computation ( X_train , X_test , inbag , pred_centered , n_trees , memory_constrained = False , memory_limit = None , test_mode = False ) : if not memory_constrained : return np . sum ( ( np . dot ( inbag - 1 , pred_centered . T ) / n_trees ) ** 2 , 0 ) if not memory_limit : raise ValueError ( 'If memory_constrained=True, must provide' , 'memory_limit.' ) # Assumes double precision float chunk_size = int ( ( memory_limit * 1e6 ) / ( 8.0 * X_train . shape [ 0 ] ) ) if chunk_size == 0 : min_limit = 8.0 * X_train . shape [ 0 ] / 1e6 raise ValueError ( 'memory_limit provided is too small.' + 'For these dimensions, memory_limit must ' + 'be greater than or equal to %.3e' % min_limit ) chunk_edges = np . arange ( 0 , X_test . shape [ 0 ] + chunk_size , chunk_size ) inds = range ( X_test . shape [ 0 ] ) chunks = [ inds [ chunk_edges [ i ] : chunk_edges [ i + 1 ] ] for i in range ( len ( chunk_edges ) - 1 ) ] if test_mode : print ( 'Number of chunks: %d' % ( len ( chunks ) , ) ) V_IJ = np . concatenate ( [ np . sum ( ( np . dot ( inbag - 1 , pred_centered [ chunk ] . T ) / n_trees ) ** 2 , 0 ) for chunk in chunks ] ) return V_IJ
776	def __getDBNameForVersion ( cls , dbVersion ) : # DB Name prefix for the given version prefix = cls . __getDBNamePrefixForVersion ( dbVersion ) # DB Name suffix suffix = Configuration . get ( 'nupic.cluster.database.nameSuffix' ) # Replace dash and dot with underscore (e.g. 'ec2-user' or ec2.user will break SQL) suffix = suffix . replace ( "-" , "_" ) suffix = suffix . replace ( "." , "_" ) # Create the name of the database for the given DB version dbName = '%s_%s' % ( prefix , suffix ) return dbName
1670	def ProcessFileData ( filename , file_extension , lines , error , extra_check_functions = None ) : lines = ( [ '// marker so line numbers and indices both start at 1' ] + lines + [ '// marker so line numbers end in a known way' ] ) include_state = _IncludeState ( ) function_state = _FunctionState ( ) nesting_state = NestingState ( ) ResetNolintSuppressions ( ) CheckForCopyright ( filename , lines , error ) ProcessGlobalSuppresions ( lines ) RemoveMultiLineComments ( filename , lines , error ) clean_lines = CleansedLines ( lines ) if file_extension in GetHeaderExtensions ( ) : CheckForHeaderGuard ( filename , clean_lines , error ) for line in range ( clean_lines . NumLines ( ) ) : ProcessLine ( filename , file_extension , clean_lines , line , include_state , function_state , nesting_state , error , extra_check_functions ) FlagCxx11Features ( filename , clean_lines , line , error ) nesting_state . CheckCompletedBlocks ( filename , error ) CheckForIncludeWhatYouUse ( filename , clean_lines , include_state , error ) # Check that the .cc file has included its header if it exists. if _IsSourceExtension ( file_extension ) : CheckHeaderFileIncluded ( filename , include_state , error ) # We check here rather than inside ProcessLine so that we see raw # lines rather than "cleaned" lines. CheckForBadCharacters ( filename , lines , error ) CheckForNewlineAtEOF ( filename , lines , error )
2917	def _eval_args ( args , my_task ) : results = [ ] for arg in args : if isinstance ( arg , Attrib ) or isinstance ( arg , PathAttrib ) : results . append ( valueof ( my_task , arg ) ) else : results . append ( arg ) return results
2578	def submit ( self , func , * args , executors = 'all' , fn_hash = None , cache = False , * * kwargs ) : if self . cleanup_called : raise ValueError ( "Cannot submit to a DFK that has been cleaned up" ) task_id = self . task_count self . task_count += 1 if isinstance ( executors , str ) and executors . lower ( ) == 'all' : choices = list ( e for e in self . executors if e != 'data_manager' ) elif isinstance ( executors , list ) : choices = executors executor = random . choice ( choices ) # Transform remote input files to data futures args , kwargs = self . _add_input_deps ( executor , args , kwargs ) task_def = { 'depends' : None , 'executor' : executor , 'func' : func , 'func_name' : func . __name__ , 'args' : args , 'kwargs' : kwargs , 'fn_hash' : fn_hash , 'memoize' : cache , 'callback' : None , 'exec_fu' : None , 'checkpoint' : None , 'fail_count' : 0 , 'fail_history' : [ ] , 'env' : None , 'status' : States . unsched , 'id' : task_id , 'time_submitted' : None , 'time_returned' : None , 'app_fu' : None } if task_id in self . tasks : raise DuplicateTaskError ( "internal consistency error: Task {0} already exists in task list" . format ( task_id ) ) else : self . tasks [ task_id ] = task_def # Get the dep count and a list of dependencies for the task dep_cnt , depends = self . _gather_all_deps ( args , kwargs ) self . tasks [ task_id ] [ 'depends' ] = depends # Extract stdout and stderr to pass to AppFuture: task_stdout = kwargs . get ( 'stdout' ) task_stderr = kwargs . get ( 'stderr' ) logger . info ( "Task {} submitted for App {}, waiting on tasks {}" . format ( task_id , task_def [ 'func_name' ] , [ fu . tid for fu in depends ] ) ) self . tasks [ task_id ] [ 'task_launch_lock' ] = threading . Lock ( ) app_fu = AppFuture ( tid = task_id , stdout = task_stdout , stderr = task_stderr ) self . tasks [ task_id ] [ 'app_fu' ] = app_fu app_fu . add_done_callback ( partial ( self . handle_app_update , task_id ) ) self . tasks [ task_id ] [ 'status' ] = States . pending logger . debug ( "Task {} set to pending state with AppFuture: {}" . format ( task_id , task_def [ 'app_fu' ] ) ) # at this point add callbacks to all dependencies to do a launch_if_ready # call whenever a dependency completes. # we need to be careful about the order of setting the state to pending, # adding the callbacks, and caling launch_if_ready explicitly once always below. # I think as long as we call launch_if_ready once after setting pending, then # we can add the callback dependencies at any point: if the callbacks all fire # before then, they won't cause a launch, but the one below will. if they fire # after we set it pending, then the last one will cause a launch, and the # explicit one won't. for d in depends : def callback_adapter ( dep_fut ) : self . launch_if_ready ( task_id ) try : d . add_done_callback ( callback_adapter ) except Exception as e : logger . error ( "add_done_callback got an exception {} which will be ignored" . format ( e ) ) self . launch_if_ready ( task_id ) return task_def [ 'app_fu' ]
9363	def user_name ( with_num = False ) : result = first_name ( ) if with_num : result += str ( random . randint ( 63 , 94 ) ) return result . lower ( )
12739	def create_joints ( self ) : stack = [ 'root' ] while stack : parent = stack . pop ( ) for child in self . hierarchy . get ( parent , ( ) ) : stack . append ( child ) if parent not in self . bones : continue bone = self . bones [ parent ] body = [ b for b in self . bodies if b . name == parent ] [ 0 ] for child in self . hierarchy . get ( parent , ( ) ) : child_bone = self . bones [ child ] child_body = [ b for b in self . bodies if b . name == child ] [ 0 ] shape = ( '' , 'hinge' , 'universal' , 'ball' ) [ len ( child_bone . dof ) ] self . joints . append ( self . world . join ( shape , body , child_body ) )
3260	def get_layergroup ( self , name , workspace = None ) : layergroups = self . get_layergroups ( names = name , workspaces = workspace ) return self . _return_first_item ( layergroups )
10127	def draw ( self ) : if self . enabled : self . _vertex_list . colors = self . _gl_colors self . _vertex_list . vertices = self . _gl_vertices self . _vertex_list . draw ( pyglet . gl . GL_TRIANGLES )
7100	def on_marker ( self , marker ) : mid , pos = marker self . marker = Marker ( __id__ = mid ) mapview = self . parent ( ) # Save ref mapview . markers [ mid ] = self # Required so the packer can pass the id self . marker . setTag ( mid ) # If we have a child widget we must configure the map to use the # custom adapter for w in self . child_widgets ( ) : mapview . init_info_window_adapter ( ) break d = self . declaration if d . show_info : self . set_show_info ( d . show_info ) #: Can free the options now del self . options
13035	def read_translation ( filename ) : translation = triple_pb . Translation ( ) with open ( filename , "rb" ) as f : translation . ParseFromString ( f . read ( ) ) def unwrap_translation_units ( units ) : for u in units : yield u . element , u . index return ( list ( unwrap_translation_units ( translation . entities ) ) , list ( unwrap_translation_units ( translation . relations ) ) )
6630	def get ( self , path ) : path = _splitPath ( path ) for config in self . configs . values ( ) : cur = config for el in path : if el in cur : cur = cur [ el ] else : cur = None break if cur is not None : return cur return None
3857	def is_quiet ( self ) : level = self . _conversation . self_conversation_state . notification_level return level == hangouts_pb2 . NOTIFICATION_LEVEL_QUIET
6582	def play_station ( self , station ) : for song in iterate_forever ( station . get_playlist ) : try : self . play ( song ) except StopIteration : self . stop ( ) return
7700	def verify_roster_set ( self , fix = False , settings = None ) : # pylint: disable=R0912 try : self . _verify ( ( None , u"remove" ) , fix ) except ValueError , err : raise BadRequestProtocolError ( unicode ( err ) ) if self . ask : if fix : self . ask = None else : raise BadRequestProtocolError ( "'ask' in roster set" ) if self . approved : if fix : self . approved = False else : raise BadRequestProtocolError ( "'approved' in roster set" ) if settings is None : settings = XMPPSettings ( ) name_length_limit = settings [ "roster_name_length_limit" ] if self . name and len ( self . name ) > name_length_limit : raise NotAcceptableProtocolError ( u"Roster item name too long" ) group_length_limit = settings [ "roster_group_name_length_limit" ] for group in self . groups : if not group : raise NotAcceptableProtocolError ( u"Roster group name empty" ) if len ( group ) > group_length_limit : raise NotAcceptableProtocolError ( u"Roster group name too long" ) if self . _duplicate_group : raise BadRequestProtocolError ( u"Item group duplicated" )
6624	def availableVersions ( self ) : r = [ ] for t in self . _getTags ( ) : logger . debug ( "available version tag: %s" , t ) # ignore empty tags: if not len ( t [ 0 ] . strip ( ) ) : continue try : r . append ( GithubComponentVersion ( t [ 0 ] , t [ 0 ] , url = t [ 1 ] , name = self . name , cache_key = None ) ) except ValueError : logger . debug ( 'invalid version tag: %s' , t ) return r
8190	def nodes_by_betweenness ( self , treshold = 0.0 ) : nodes = [ ( n . betweenness , n ) for n in self . nodes if n . betweenness > treshold ] nodes . sort ( ) nodes . reverse ( ) return [ n for w , n in nodes ]
219	def get_directories ( self , directory : str = None , packages : typing . List [ str ] = None ) -> typing . List [ str ] : directories = [ ] if directory is not None : directories . append ( directory ) for package in packages or [ ] : spec = importlib . util . find_spec ( package ) assert spec is not None , f"Package {package!r} could not be found." assert ( spec . origin is not None ) , "Directory 'statics' in package {package!r} could not be found." directory = os . path . normpath ( os . path . join ( spec . origin , ".." , "statics" ) ) assert os . path . isdir ( directory ) , "Directory 'statics' in package {package!r} could not be found." directories . append ( directory ) return directories
11029	def sse_content ( response , handler , * * sse_kwargs ) : # An SSE response must be 200/OK and have content-type 'text/event-stream' raise_for_not_ok_status ( response ) raise_for_header ( response , 'Content-Type' , 'text/event-stream' ) finished , _ = _sse_content_with_protocol ( response , handler , * * sse_kwargs ) return finished
3926	def keypress ( self , size , key ) : key = super ( ) . keypress ( size , key ) num_tabs = len ( self . _widgets ) if key == self . _keys [ 'prev_tab' ] : self . _tab_index = ( self . _tab_index - 1 ) % num_tabs self . _update_tabs ( ) elif key == self . _keys [ 'next_tab' ] : self . _tab_index = ( self . _tab_index + 1 ) % num_tabs self . _update_tabs ( ) elif key == self . _keys [ 'close_tab' ] : # Don't allow closing the Conversations tab if self . _tab_index > 0 : curr_tab = self . _widgets [ self . _tab_index ] self . _widgets . remove ( curr_tab ) del self . _widget_title [ curr_tab ] self . _tab_index -= 1 self . _update_tabs ( ) else : return key
6190	def set_sim_params ( self , nparams , attr_params ) : for name , value in nparams . items ( ) : val = value [ 0 ] if value [ 0 ] is not None else 'none' self . h5file . create_array ( '/parameters' , name , obj = val , title = value [ 1 ] ) for name , value in attr_params . items ( ) : self . h5file . set_node_attr ( '/parameters' , name , value )
9107	def sanitize_filename ( filename ) : # TODO: fix broken splitext (it reveals everything of the filename after the first `.` - doh!) token = generate_drop_id ( ) name , extension = splitext ( filename ) if extension : return '%s%s' % ( token , extension ) else : return token
7514	def enter_pairs ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : ## snps was created using only the selected samples. LOGGER . info ( "edges in enter_pairs %s" , edg ) seq1 = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] snp1 = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] ## the 2nd read edges are +5 for the spacer seq2 = aseqs [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] snp2 = asnps [ iloc , edg [ 2 ] : edg [ 3 ] + 1 , ] ## remove rows with all Ns, seq has only selected samples nalln = np . all ( seq1 == "N" , axis = 1 ) ## make mask of removed rows and excluded samples. Use the inverse ## of this to save the coverage for samples nsidx = nalln + smask LOGGER . info ( "nsidx %s, nalln %s, smask %s" , nsidx , nalln , smask ) samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) LOGGER . info ( "samplecov %s" , samplecov ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) LOGGER . info ( "idx %s" , idx ) locuscov [ idx ] += 1 ## select the remaining names in order seq1 = seq1 [ ~ nsidx , ] seq2 = seq2 [ ~ nsidx , ] names = pnames [ ~ nsidx ] ## save string for printing, excluding names not in samples outstr = "\n" . join ( [ name + s1 . tostring ( ) + "nnnn" + s2 . tostring ( ) for name , s1 , s2 in zip ( names , seq1 , seq2 ) ] ) #LOGGER.info("s1 %s", s1.tostring()) #LOGGER.info("s2 %s", s2.tostring()) ## get snp string and add to store snpstring1 = [ "-" if snp1 [ i , 0 ] else "*" if snp1 [ i , 1 ] else " " for i in range ( len ( snp1 ) ) ] snpstring2 = [ "-" if snp2 [ i , 0 ] else "*" if snp2 [ i , 1 ] else " " for i in range ( len ( snp2 ) ) ] #npis = str(snpstring1+snpstring2).count("*") #nvars = str(snpstring1+snpstring2).count("-") + npis outstr += "\n" + snppad + "" . join ( snpstring1 ) + " " + "" . join ( snpstring2 ) + "|{}|" . format ( iloc + start ) #"|LOCID={},DBID={},NVAR={},NPIS={}|"\ #.format(1+iloc+start, iloc, nvars, npis) return outstr , samplecov , locuscov
4217	def delete_password ( self , service , username ) : if not self . connected ( service ) : # the user pressed "cancel" when prompted to unlock their keyring. raise PasswordDeleteError ( "Cancelled by user" ) if not self . iface . hasEntry ( self . handle , service , username , self . appid ) : raise PasswordDeleteError ( "Password not found" ) self . iface . removeEntry ( self . handle , service , username , self . appid )
3383	def __build_problem ( self ) : # Set up the mathematical problem prob = constraint_matrices ( self . model , zero_tol = self . feasibility_tol ) # check if there any non-zero equality constraints equalities = prob . equalities b = prob . b bounds = np . atleast_2d ( prob . bounds ) . T var_bounds = np . atleast_2d ( prob . variable_bounds ) . T homogeneous = all ( np . abs ( b ) < self . feasibility_tol ) fixed_non_zero = np . abs ( prob . variable_bounds [ : , 1 ] ) > self . feasibility_tol fixed_non_zero &= prob . variable_fixed # check if there are any non-zero fixed variables, add them as # equalities to the stoichiometric matrix if any ( fixed_non_zero ) : n_fixed = fixed_non_zero . sum ( ) rows = np . zeros ( ( n_fixed , prob . equalities . shape [ 1 ] ) ) rows [ range ( n_fixed ) , np . where ( fixed_non_zero ) ] = 1.0 equalities = np . vstack ( [ equalities , rows ] ) var_b = prob . variable_bounds [ : , 1 ] b = np . hstack ( [ b , var_b [ fixed_non_zero ] ] ) homogeneous = False # Set up a projection that can cast point into the nullspace nulls = nullspace ( equalities ) # convert bounds to a matrix and add variable bounds as well return Problem ( equalities = shared_np_array ( equalities . shape , equalities ) , b = shared_np_array ( b . shape , b ) , inequalities = shared_np_array ( prob . inequalities . shape , prob . inequalities ) , bounds = shared_np_array ( bounds . shape , bounds ) , variable_fixed = shared_np_array ( prob . variable_fixed . shape , prob . variable_fixed , integer = True ) , variable_bounds = shared_np_array ( var_bounds . shape , var_bounds ) , nullspace = shared_np_array ( nulls . shape , nulls ) , homogeneous = homogeneous )
3100	def loadfile ( filename , cache = None ) : _SECRET_NAMESPACE = 'oauth2client:secrets#ns' if not cache : return _loadfile ( filename ) obj = cache . get ( filename , namespace = _SECRET_NAMESPACE ) if obj is None : client_type , client_info = _loadfile ( filename ) obj = { client_type : client_info } cache . set ( filename , obj , namespace = _SECRET_NAMESPACE ) return next ( six . iteritems ( obj ) )
3105	def code_verifier ( n_bytes = 64 ) : verifier = base64 . urlsafe_b64encode ( os . urandom ( n_bytes ) ) . rstrip ( b'=' ) # https://tools.ietf.org/html/rfc7636#section-4.1 # minimum length of 43 characters and a maximum length of 128 characters. if len ( verifier ) < 43 : raise ValueError ( "Verifier too short. n_bytes must be > 30." ) elif len ( verifier ) > 128 : raise ValueError ( "Verifier too long. n_bytes must be < 97." ) else : return verifier
13892	def _AssertIsLocal ( path ) : from six . moves . urllib . parse import urlparse if not _UrlIsLocal ( urlparse ( path ) ) : from . _exceptions import NotImplementedForRemotePathError raise NotImplementedForRemotePathError
3660	def _coeff_ind_from_T ( self , T ) : # DO NOT CHANGE if self . n == 1 : return 0 for i in range ( self . n ) : if T <= self . Ts [ i + 1 ] : return i return self . n - 1
495	def _releaseConnection ( self , dbConn , cursor ) : self . _logger . debug ( "Releasing connection" ) # Close the cursor cursor . close ( ) # ... then close the database connection dbConn . close ( ) return
4753	def tcase_parse_descr ( tcase ) : descr_short = "SHORT" descr_long = "LONG" try : comment = tcase_comment ( tcase ) except ( IOError , OSError , ValueError ) as exc : comment = [ ] cij . err ( "tcase_parse_descr: failed: %r, tcase: %r" % ( exc , tcase ) ) comment = [ l for l in comment if l . strip ( ) ] # Remove empty lines for line_number , line in enumerate ( comment ) : if line . startswith ( "#" ) : comment [ line_number ] = line [ 1 : ] if comment : descr_short = comment [ 0 ] if len ( comment ) > 1 : descr_long = "\n" . join ( comment [ 1 : ] ) return descr_short , descr_long
506	def getLabels ( self , start = None , end = None ) : if len ( self . _recordsCache ) == 0 : return { 'isProcessing' : False , 'recordLabels' : [ ] } try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = self . _recordsCache [ - 1 ] . ROWID if end <= start : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for 'getLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'numRecordsStored' : len ( self . _recordsCache ) } ) results = { 'isProcessing' : False , 'recordLabels' : [ ] } ROWIDX = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) validIdx = numpy . where ( ( ROWIDX >= start ) & ( ROWIDX < end ) ) [ 0 ] . tolist ( ) categories = self . _knnclassifier . getCategoryList ( ) for idx in validIdx : row = dict ( ROWID = int ( ROWIDX [ idx ] ) , labels = self . _categoryToLabelList ( categories [ idx ] ) ) results [ 'recordLabels' ] . append ( row ) return results
1136	def isdir ( s ) : try : st = os . stat ( s ) except os . error : return False return stat . S_ISDIR ( st . st_mode )
11268	def join ( prev , sep , * args , * * kw ) : yield sep . join ( prev , * args , * * kw )
451	def flatten_reshape ( variable , name = 'flatten' ) : dim = 1 for d in variable . get_shape ( ) [ 1 : ] . as_list ( ) : dim *= d return tf . reshape ( variable , shape = [ - 1 , dim ] , name = name )
8959	def build ( ctx , dput = '' , opts = '' ) : # Get package metadata with io . open ( 'debian/changelog' , encoding = 'utf-8' ) as changes : metadata = re . match ( r'^([^ ]+) \(([^)]+)\) ([^;]+); urgency=(.+)$' , changes . readline ( ) . rstrip ( ) ) if not metadata : notify . failure ( 'Badly formatted top entry in changelog' ) name , version , _ , _ = metadata . groups ( ) # Build package ctx . run ( 'dpkg-buildpackage {} {}' . format ( ctx . rituals . deb . build . opts , opts ) ) # Move created artifacts into "dist" if not os . path . exists ( 'dist' ) : os . makedirs ( 'dist' ) artifact_pattern = '{}?{}*' . format ( name , re . sub ( r'[^-_.a-zA-Z0-9]' , '?' , version ) ) changes_files = [ ] for debfile in glob . glob ( '../' + artifact_pattern ) : shutil . move ( debfile , 'dist' ) if debfile . endswith ( '.changes' ) : changes_files . append ( os . path . join ( 'dist' , os . path . basename ( debfile ) ) ) ctx . run ( 'ls -l dist/{}' . format ( artifact_pattern ) ) if dput : ctx . run ( 'dput {} {}' . format ( dput , ' ' . join ( changes_files ) ) )
45	def compute_geometric_median ( X , eps = 1e-5 ) : y = np . mean ( X , 0 ) while True : D = scipy . spatial . distance . cdist ( X , [ y ] ) nonzeros = ( D != 0 ) [ : , 0 ] Dinv = 1 / D [ nonzeros ] Dinvs = np . sum ( Dinv ) W = Dinv / Dinvs T = np . sum ( W * X [ nonzeros ] , 0 ) num_zeros = len ( X ) - np . sum ( nonzeros ) if num_zeros == 0 : y1 = T elif num_zeros == len ( X ) : return y else : R = ( T - y ) * Dinvs r = np . linalg . norm ( R ) rinv = 0 if r == 0 else num_zeros / r y1 = max ( 0 , 1 - rinv ) * T + min ( 1 , rinv ) * y if scipy . spatial . distance . euclidean ( y , y1 ) < eps : return y1 y = y1
11296	def process_response ( self , resp , multiple_rates ) : self . _check_for_exceptions ( resp , multiple_rates ) rates = { } for result in resp [ 'results' ] : rate = ZipTaxClient . _cast_tax_rate ( result [ 'taxSales' ] ) rates [ result [ 'geoCity' ] ] = rate if not multiple_rates : return rates [ list ( rates . keys ( ) ) [ 0 ] ] return rates
2807	def convert_gemm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting Linear ...' ) if names == 'short' : tf_name = 'FC' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] has_bias = False if bias_name in weights : bias = weights [ bias_name ] . numpy ( ) keras_weights = [ W , bias ] has_bias = True dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = has_bias , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] )
4519	def set_project ( self , project ) : def visit ( x ) : # Try to set_project, then recurse through any values() set_project = getattr ( x , 'set_project' , None ) if set_project : set_project ( project ) values = getattr ( x , 'values' , lambda : ( ) ) for v in values ( ) : visit ( v ) visit ( self . routing )
12846	def generate ( request ) : models . DataItem . create ( content = '' . join ( random . choice ( string . ascii_uppercase + string . digits ) for _ in range ( 20 ) ) ) return muffin . HTTPFound ( '/' )
6069	def intensity_at_radius ( self , radius ) : return self . intensity * np . exp ( - self . sersic_constant * ( ( ( radius / self . effective_radius ) ** ( 1. / self . sersic_index ) ) - 1 ) )
1550	def _get_spout ( self ) : spout = topology_pb2 . Spout ( ) spout . comp . CopyFrom ( self . _get_base_component ( ) ) # Add output streams self . _add_out_streams ( spout ) return spout
2925	def _on_ready ( self , my_task ) : assert my_task is not None self . test ( ) # Acquire locks, if any. for lock in self . locks : mutex = my_task . workflow . _get_mutex ( lock ) if not mutex . testandset ( ) : return # Assign variables, if so requested. for assignment in self . pre_assign : assignment . assign ( my_task , my_task ) # Run task-specific code. self . _on_ready_before_hook ( my_task ) self . reached_event . emit ( my_task . workflow , my_task ) self . _on_ready_hook ( my_task ) # Run user code, if any. if self . ready_event . emit ( my_task . workflow , my_task ) : # Assign variables, if so requested. for assignment in self . post_assign : assignment . assign ( my_task , my_task ) # Release locks, if any. for lock in self . locks : mutex = my_task . workflow . _get_mutex ( lock ) mutex . unlock ( ) self . finished_event . emit ( my_task . workflow , my_task )
12415	def send ( self , * args , * * kwargs ) : self . write ( * args , * * kwargs ) self . flush ( )
9095	def _add_annotation_to_graph ( self , graph : BELGraph ) -> None : if 'bio2bel' not in graph . annotation_list : graph . annotation_list [ 'bio2bel' ] = set ( ) graph . annotation_list [ 'bio2bel' ] . add ( self . module_name )
13116	def create_connection ( conf ) : host_config = { } host_config [ 'hosts' ] = [ conf . get ( 'jackal' , 'host' ) ] if int ( conf . get ( 'jackal' , 'use_ssl' ) ) : host_config [ 'use_ssl' ] = True if conf . get ( 'jackal' , 'ca_certs' ) : host_config [ 'ca_certs' ] = conf . get ( 'jackal' , 'ca_certs' ) if int ( conf . get ( 'jackal' , 'client_certs' ) ) : host_config [ 'client_cert' ] = conf . get ( 'jackal' , 'client_cert' ) host_config [ 'client_key' ] = conf . get ( 'jackal' , 'client_key' ) # Disable hostname checking for now. host_config [ 'ssl_assert_hostname' ] = False connections . create_connection ( * * host_config )
7874	def get_payload ( self , payload_class , payload_key = None , specialize = False ) : if self . _payload is None : self . decode_payload ( ) if payload_class is None : if self . _payload : payload = self . _payload [ 0 ] if specialize and isinstance ( payload , XMLPayload ) : klass = payload_class_for_element_name ( payload . element . tag ) if klass is not XMLPayload : payload = klass . from_xml ( payload . element ) self . _payload [ 0 ] = payload return payload else : return None # pylint: disable=W0212 elements = payload_class . _pyxmpp_payload_element_name for i , payload in enumerate ( self . _payload ) : if isinstance ( payload , XMLPayload ) : if payload_class is not XMLPayload : if payload . xml_element_name not in elements : continue payload = payload_class . from_xml ( payload . element ) elif not isinstance ( payload , payload_class ) : continue if payload_key is not None and payload_key != payload . handler_key ( ) : continue self . _payload [ i ] = payload return payload return None
6485	def do_search ( request , course_id = None ) : # Setup search environment SearchInitializer . set_search_enviroment ( request = request , course_id = course_id ) results = { "error" : _ ( "Nothing to search" ) } status_code = 500 search_term = request . POST . get ( "search_string" , None ) try : if not search_term : raise ValueError ( _ ( 'No search term provided for search' ) ) size , from_ , page = _process_pagination_values ( request ) # Analytics - log search request track . emit ( 'edx.course.search.initiated' , { "search_term" : search_term , "page_size" : size , "page_number" : page , } ) results = perform_search ( search_term , user = request . user , size = size , from_ = from_ , course_id = course_id ) status_code = 200 # Analytics - log search results before sending to browser track . emit ( 'edx.course.search.results_displayed' , { "search_term" : search_term , "page_size" : size , "page_number" : page , "results_count" : results [ "total" ] , } ) except ValueError as invalid_err : results = { "error" : six . text_type ( invalid_err ) } log . debug ( six . text_type ( invalid_err ) ) except QueryParseError : results = { "error" : _ ( 'Your query seems malformed. Check for unmatched quotes.' ) } # Allow for broad exceptions here - this is an entry point from external reference except Exception as err : # pylint: disable=broad-except results = { "error" : _ ( 'An error occurred when searching for "{search_string}"' ) . format ( search_string = search_term ) } log . exception ( 'Search view exception when searching for %s for user %s: %r' , search_term , request . user . id , err ) return JsonResponse ( results , status = status_code )
2455	def set_pkg_licenses_concluded ( self , doc , licenses ) : self . assert_package_exists ( ) if not self . package_conc_lics_set : self . package_conc_lics_set = True if validations . validate_lics_conc ( licenses ) : doc . package . conc_lics = licenses return True else : raise SPDXValueError ( 'Package::ConcludedLicenses' ) else : raise CardinalityError ( 'Package::ConcludedLicenses' )
6326	def corpus_importer ( self , corpus , n_val = 1 , bos = '_START_' , eos = '_END_' ) : if not corpus or not isinstance ( corpus , Corpus ) : raise TypeError ( 'Corpus argument of the Corpus class required.' ) sentences = corpus . sents ( ) for sent in sentences : ngs = Counter ( sent ) for key in ngs . keys ( ) : self . _add_to_ngcorpus ( self . ngcorpus , [ key ] , ngs [ key ] ) if n_val > 1 : if bos and bos != '' : sent = [ bos ] + sent if eos and eos != '' : sent += [ eos ] for i in range ( 2 , n_val + 1 ) : for j in range ( len ( sent ) - i + 1 ) : self . _add_to_ngcorpus ( self . ngcorpus , sent [ j : j + i ] , 1 )
4412	def fetch ( self , key : object , default = None ) : return self . _user_data . get ( key , default )
5583	def get_path ( self , tile ) : return os . path . join ( * [ self . path , str ( tile . zoom ) , str ( tile . row ) , str ( tile . col ) + self . file_extension ] )
6039	def xticks ( self ) : return np . linspace ( np . min ( self [ : , 1 ] ) , np . max ( self [ : , 1 ] ) , 4 )
12288	def datapackage_exists ( repo ) : datapath = os . path . join ( repo . rootdir , "datapackage.json" ) return os . path . exists ( datapath )
9468	def conference_speak ( self , call_params ) : path = '/' + self . api_version + '/ConferenceSpeak/' method = 'POST' return self . request ( path , method , call_params )
600	def compute ( self , activeColumns , predictedColumns , inputValue = None , timestamp = None ) : # Start by computing the raw anomaly score. anomalyScore = computeRawAnomalyScore ( activeColumns , predictedColumns ) # Compute final anomaly based on selected mode. if self . _mode == Anomaly . MODE_PURE : score = anomalyScore elif self . _mode == Anomaly . MODE_LIKELIHOOD : if inputValue is None : raise ValueError ( "Selected anomaly mode 'Anomaly.MODE_LIKELIHOOD' " "requires 'inputValue' as parameter to compute() method. " ) probability = self . _likelihood . anomalyProbability ( inputValue , anomalyScore , timestamp ) # low likelihood -> hi anomaly score = 1 - probability elif self . _mode == Anomaly . MODE_WEIGHTED : probability = self . _likelihood . anomalyProbability ( inputValue , anomalyScore , timestamp ) score = anomalyScore * ( 1 - probability ) # Last, do moving-average if windowSize was specified. if self . _movingAverage is not None : score = self . _movingAverage . next ( score ) # apply binary discretization if required if self . _binaryThreshold is not None : if score >= self . _binaryThreshold : score = 1.0 else : score = 0.0 return score
4938	def logo_path ( instance , filename ) : extension = os . path . splitext ( filename ) [ 1 ] . lower ( ) instance_id = str ( instance . id ) fullname = os . path . join ( "enterprise/branding/" , instance_id , instance_id + "_logo" + extension ) if default_storage . exists ( fullname ) : default_storage . delete ( fullname ) return fullname
4356	def get_multiple_client_msgs ( self , * * kwargs ) : client_queue = self . client_queue msgs = [ client_queue . get ( * * kwargs ) ] while client_queue . qsize ( ) : msgs . append ( client_queue . get ( ) ) return msgs
4868	def to_representation ( self , instance ) : updated_course_run = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( updated_course_run [ 'key' ] ) return updated_course_run
6173	def single_instance ( func = None , lock_timeout = None , include_args = False ) : if func is None : return partial ( single_instance , lock_timeout = lock_timeout , include_args = include_args ) @ wraps ( func ) def wrapped ( celery_self , * args , * * kwargs ) : """Wrapped Celery task, for single_instance().""" # Select the manager and get timeout. timeout = ( lock_timeout or celery_self . soft_time_limit or celery_self . time_limit or celery_self . app . conf . get ( 'CELERYD_TASK_SOFT_TIME_LIMIT' ) or celery_self . app . conf . get ( 'CELERYD_TASK_TIME_LIMIT' ) or ( 60 * 5 ) ) manager_class = _select_manager ( celery_self . backend . __class__ . __name__ ) lock_manager = manager_class ( celery_self , timeout , include_args , args , kwargs ) # Lock and execute. with lock_manager : ret_value = func ( * args , * * kwargs ) return ret_value return wrapped
21	def boolean_flag ( parser , name , default = False , help = None ) : dest = name . replace ( '-' , '_' ) parser . add_argument ( "--" + name , action = "store_true" , default = default , dest = dest , help = help ) parser . add_argument ( "--no-" + name , action = "store_false" , dest = dest )
6855	def ismounted ( device ) : # Check filesystem with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'mount' ) for line in res . splitlines ( ) : fields = line . split ( ) if fields [ 0 ] == device : return True # Check swap with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'swapon -s' ) for line in res . splitlines ( ) : fields = line . split ( ) if fields [ 0 ] == device : return True return False
8304	def eof ( self ) : return ( not self . is_alive ( ) ) and self . _queue . empty ( ) or self . _fd . closed
5519	def append ( self , name , data , start ) : for throttle in self . throttles . values ( ) : getattr ( throttle , name ) . append ( data , start )
10275	def generate_mechanism ( graph : BELGraph , node : BaseEntity , key : Optional [ str ] = None ) -> BELGraph : subgraph = get_upstream_causal_subgraph ( graph , node ) expand_upstream_causal ( graph , subgraph ) remove_inconsistent_edges ( subgraph ) collapse_consistent_edges ( subgraph ) if key is not None : # FIXME when is it not pruned? prune_mechanism_by_data ( subgraph , key ) return subgraph
7194	def histogram_stretch ( self , use_bands , * * kwargs ) : data = self . _read ( self [ use_bands , ... ] , * * kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) return self . _histogram_stretch ( data , * * kwargs )
6918	def _get_acf_peakheights ( lags , acf , npeaks = 20 , searchinterval = 1 ) : maxinds = argrelmax ( acf , order = searchinterval ) [ 0 ] maxacfs = acf [ maxinds ] maxlags = lags [ maxinds ] mininds = argrelmin ( acf , order = searchinterval ) [ 0 ] minacfs = acf [ mininds ] minlags = lags [ mininds ] relpeakheights = npzeros ( npeaks ) relpeaklags = npzeros ( npeaks , dtype = npint64 ) peakindices = npzeros ( npeaks , dtype = npint64 ) for peakind , mxi in enumerate ( maxinds [ : npeaks ] ) : # check if there are no mins to the left # throw away this peak because it's probably spurious # (FIXME: is this OK?) if npall ( mxi < mininds ) : continue leftminind = mininds [ mininds < mxi ] [ - 1 ] # the last index to the left rightminind = mininds [ mininds > mxi ] [ 0 ] # the first index to the right relpeakheights [ peakind ] = ( acf [ mxi ] - ( acf [ leftminind ] + acf [ rightminind ] ) / 2.0 ) relpeaklags [ peakind ] = lags [ mxi ] peakindices [ peakind ] = peakind # figure out the bestperiod if possible if relpeakheights [ 0 ] > relpeakheights [ 1 ] : bestlag = relpeaklags [ 0 ] bestpeakheight = relpeakheights [ 0 ] bestpeakindex = peakindices [ 0 ] else : bestlag = relpeaklags [ 1 ] bestpeakheight = relpeakheights [ 1 ] bestpeakindex = peakindices [ 1 ] return { 'maxinds' : maxinds , 'maxacfs' : maxacfs , 'maxlags' : maxlags , 'mininds' : mininds , 'minacfs' : minacfs , 'minlags' : minlags , 'relpeakheights' : relpeakheights , 'relpeaklags' : relpeaklags , 'peakindices' : peakindices , 'bestlag' : bestlag , 'bestpeakheight' : bestpeakheight , 'bestpeakindex' : bestpeakindex }
282	def plot_holdings ( returns , positions , legend_loc = 'best' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) positions = positions . copy ( ) . drop ( 'cash' , axis = 'columns' ) df_holdings = positions . replace ( 0 , np . nan ) . count ( axis = 1 ) df_holdings_by_month = df_holdings . resample ( '1M' ) . mean ( ) df_holdings . plot ( color = 'steelblue' , alpha = 0.6 , lw = 0.5 , ax = ax , * * kwargs ) df_holdings_by_month . plot ( color = 'orangered' , lw = 2 , ax = ax , * * kwargs ) ax . axhline ( df_holdings . values . mean ( ) , color = 'steelblue' , ls = '--' , lw = 3 ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) leg = ax . legend ( [ 'Daily holdings' , 'Average daily holdings, by month' , 'Average daily holdings, overall' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . set_title ( 'Total holdings' ) ax . set_ylabel ( 'Holdings' ) ax . set_xlabel ( '' ) return ax
11887	def receive ( self ) : try : buffer = self . _socket . recv ( BUFFER_SIZE ) except socket . timeout as error : # Something is wrong, assume it's offline temporarily _LOGGER . error ( "Error receiving: %s" , error ) # self._socket.close() return "" # Read until a newline or timeout buffering = True response = '' while buffering : if '\n' in buffer . decode ( "utf8" ) : response = buffer . decode ( "utf8" ) . split ( '\n' ) [ 0 ] buffering = False else : try : more = self . _socket . recv ( BUFFER_SIZE ) except socket . timeout : more = None if not more : buffering = False response = buffer . decode ( "utf8" ) else : buffer += more return response
12305	def get_module_class ( class_path ) : mod_name , cls_name = class_path . rsplit ( '.' , 1 ) try : mod = import_module ( mod_name ) except ImportError as ex : raise EvoStreamException ( 'Error importing module %s: ' '"%s"' % ( mod_name , ex ) ) return getattr ( mod , cls_name )
13138	def http_get_provider ( provider , request_url , params , token_secret , token_cookie = None ) : if not validate_provider ( provider ) : raise InvalidUsage ( 'Provider not supported' ) klass = getattr ( socialauth . providers , provider . capitalize ( ) ) provider = klass ( request_url , params , token_secret , token_cookie ) if provider . status == 302 : ret = dict ( status = 302 , redirect = provider . redirect ) tc = getattr ( provider , 'set_token_cookie' , None ) if tc is not None : ret [ 'set_token_cookie' ] = tc return ret if provider . status == 200 and provider . user_id is not None : ret = dict ( status = 200 , provider_user_id = provider . user_id ) if provider . user_name is not None : ret [ 'provider_user_name' ] = provider . user_name return ret raise InvalidUsage ( 'Invalid request' )
6779	def get_deploy_funcs ( components , current_thumbprint , previous_thumbprint , preview = False ) : for component in components : funcs = manifest_deployers . get ( component , [ ] ) for func_name in funcs : #TODO:remove this after burlap.* naming prefix bug fixed if func_name . startswith ( 'burlap.' ) : print ( 'skipping %s' % func_name ) continue takes_diff = manifest_deployers_takes_diff . get ( func_name , False ) func = resolve_deployer ( func_name ) current = current_thumbprint . get ( component ) last = previous_thumbprint . get ( component ) if takes_diff : yield func_name , partial ( func , last = last , current = current ) else : yield func_name , partial ( func )
4202	def aryule ( X , order , norm = 'biased' , allow_singularity = True ) : assert norm in [ 'biased' , 'unbiased' ] r = CORRELATION ( X , maxlags = order , norm = norm ) A , P , k = LEVINSON ( r , allow_singularity = allow_singularity ) return A , P , k
2734	def get_object ( cls , api_token , ip ) : floating_ip = cls ( token = api_token , ip = ip ) floating_ip . load ( ) return floating_ip
10968	def setup_passthroughs ( self ) : self . _nopickle = [ ] for c in self . comps : # take all member functions that start with 'param_' funcs = inspect . getmembers ( c , predicate = inspect . ismethod ) for func in funcs : if func [ 0 ] . startswith ( 'param_' ) : setattr ( self , func [ 0 ] , func [ 1 ] ) self . _nopickle . append ( func [ 0 ] ) # add everything from exports funcs = c . exports ( ) for func in funcs : newname = c . category + '_' + func . __func__ . __name__ setattr ( self , newname , func ) self . _nopickle . append ( newname )
9316	def _to_json ( resp ) : try : return resp . json ( ) except ValueError as e : # Maybe better to report the original request URL? six . raise_from ( InvalidJSONError ( "Invalid JSON was received from " + resp . request . url ) , e )
7342	def set_debug ( ) : logging . basicConfig ( level = logging . WARNING ) peony . logger . setLevel ( logging . DEBUG )
1685	def RepositoryName ( self ) : fullname = self . FullName ( ) if os . path . exists ( fullname ) : project_dir = os . path . dirname ( fullname ) # If the user specified a repository path, it exists, and the file is # contained in it, use the specified repository path if _repository : repo = FileInfo ( _repository ) . FullName ( ) root_dir = project_dir while os . path . exists ( root_dir ) : # allow case insensitive compare on Windows if os . path . normcase ( root_dir ) == os . path . normcase ( repo ) : return os . path . relpath ( fullname , root_dir ) . replace ( '\\' , '/' ) one_up_dir = os . path . dirname ( root_dir ) if one_up_dir == root_dir : break root_dir = one_up_dir if os . path . exists ( os . path . join ( project_dir , ".svn" ) ) : # If there's a .svn file in the current directory, we recursively look # up the directory tree for the top of the SVN checkout root_dir = project_dir one_up_dir = os . path . dirname ( root_dir ) while os . path . exists ( os . path . join ( one_up_dir , ".svn" ) ) : root_dir = os . path . dirname ( root_dir ) one_up_dir = os . path . dirname ( one_up_dir ) prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] # Not SVN <= 1.6? Try to find a git, hg, or svn top level directory by # searching up from the current path. root_dir = current_dir = os . path . dirname ( fullname ) while current_dir != os . path . dirname ( current_dir ) : if ( os . path . exists ( os . path . join ( current_dir , ".git" ) ) or os . path . exists ( os . path . join ( current_dir , ".hg" ) ) or os . path . exists ( os . path . join ( current_dir , ".svn" ) ) ) : root_dir = current_dir current_dir = os . path . dirname ( current_dir ) if ( os . path . exists ( os . path . join ( root_dir , ".git" ) ) or os . path . exists ( os . path . join ( root_dir , ".hg" ) ) or os . path . exists ( os . path . join ( root_dir , ".svn" ) ) ) : prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] # Don't know what to do; header guard warnings may be wrong... return fullname
10101	def create_snippet ( self , name , body , timeout = None ) : payload = { 'name' : name , 'body' : body } return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
13686	def embed_data ( request ) : result = _EmbedDataFixture ( request ) result . delete_data_dir ( ) result . create_data_dir ( ) yield result result . delete_data_dir ( )
3607	def put ( self , url , name , data , params = None , headers = None , connection = None ) : assert name , 'Snapshot name must be specified' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) return make_put_request ( endpoint , data , params , headers , connection = connection )
1996	def cmp_regs ( cpu , should_print = False ) : differing = False gdb_regs = gdb . getCanonicalRegisters ( ) for name in sorted ( gdb_regs ) : vg = gdb_regs [ name ] if name . endswith ( 'psr' ) : name = 'apsr' v = cpu . read_register ( name . upper ( ) ) if should_print : logger . debug ( f'{name} gdb:{vg:x} mcore:{v:x}' ) if vg != v : if should_print : logger . warning ( '^^ unequal' ) differing = True if differing : logger . debug ( qemu . correspond ( None ) ) return differing
5688	def get_transit_events ( self , start_time_ut = None , end_time_ut = None , route_type = None ) : table_name = self . _get_day_trips_table_name ( ) event_query = "SELECT stop_I, seq, trip_I, route_I, routes.route_id AS route_id, routes.type AS route_type, " "shape_id, day_start_ut+dep_time_ds AS dep_time_ut, day_start_ut+arr_time_ds AS arr_time_ut " "FROM " + table_name + " " "JOIN trips USING(trip_I) " "JOIN routes USING(route_I) " "JOIN stop_times USING(trip_I)" where_clauses = [ ] if end_time_ut : where_clauses . append ( table_name + ".start_time_ut< {end_time_ut}" . format ( end_time_ut = end_time_ut ) ) where_clauses . append ( "dep_time_ut <={end_time_ut}" . format ( end_time_ut = end_time_ut ) ) if start_time_ut : where_clauses . append ( table_name + ".end_time_ut > {start_time_ut}" . format ( start_time_ut = start_time_ut ) ) where_clauses . append ( "arr_time_ut >={start_time_ut}" . format ( start_time_ut = start_time_ut ) ) if route_type is not None : assert route_type in ALL_ROUTE_TYPES where_clauses . append ( "routes.type={route_type}" . format ( route_type = route_type ) ) if len ( where_clauses ) > 0 : event_query += " WHERE " for i , where_clause in enumerate ( where_clauses ) : if i is not 0 : event_query += " AND " event_query += where_clause # ordering is required for later stages event_query += " ORDER BY trip_I, day_start_ut+dep_time_ds;" events_result = pd . read_sql_query ( event_query , self . conn ) # 'filter' results so that only real "events" are taken into account from_indices = numpy . nonzero ( ( events_result [ 'trip_I' ] [ : - 1 ] . values == events_result [ 'trip_I' ] [ 1 : ] . values ) * ( events_result [ 'seq' ] [ : - 1 ] . values < events_result [ 'seq' ] [ 1 : ] . values ) ) [ 0 ] to_indices = from_indices + 1 # these should have same trip_ids assert ( events_result [ 'trip_I' ] [ from_indices ] . values == events_result [ 'trip_I' ] [ to_indices ] . values ) . all ( ) trip_Is = events_result [ 'trip_I' ] [ from_indices ] from_stops = events_result [ 'stop_I' ] [ from_indices ] to_stops = events_result [ 'stop_I' ] [ to_indices ] shape_ids = events_result [ 'shape_id' ] [ from_indices ] dep_times = events_result [ 'dep_time_ut' ] [ from_indices ] arr_times = events_result [ 'arr_time_ut' ] [ to_indices ] route_types = events_result [ 'route_type' ] [ from_indices ] route_ids = events_result [ 'route_id' ] [ from_indices ] route_Is = events_result [ 'route_I' ] [ from_indices ] durations = arr_times . values - dep_times . values assert ( durations >= 0 ) . all ( ) from_seqs = events_result [ 'seq' ] [ from_indices ] to_seqs = events_result [ 'seq' ] [ to_indices ] data_tuples = zip ( from_stops , to_stops , dep_times , arr_times , shape_ids , route_types , route_ids , trip_Is , durations , from_seqs , to_seqs , route_Is ) columns = [ "from_stop_I" , "to_stop_I" , "dep_time_ut" , "arr_time_ut" , "shape_id" , "route_type" , "route_id" , "trip_I" , "duration" , "from_seq" , "to_seq" , "route_I" ] df = pd . DataFrame . from_records ( data_tuples , columns = columns ) return df
8809	def delete_mac_address_range ( context , id ) : LOG . info ( "delete_mac_address_range %s for tenant %s" % ( id , context . tenant_id ) ) if not context . is_admin : raise n_exc . NotAuthorized ( ) with context . session . begin ( ) : mar = db_api . mac_address_range_find ( context , id = id , scope = db_api . ONE ) if not mar : raise q_exc . MacAddressRangeNotFound ( mac_address_range_id = id ) _delete_mac_address_range ( context , mar )
8031	def compareChunks ( handles , chunk_size = CHUNK_SIZE ) : chunks = [ ( path , fh , fh . read ( chunk_size ) ) for path , fh , _ in handles ] more , done = [ ] , [ ] # While there are combinations not yet tried... while chunks : # Compare the first chunk to all successive chunks matches , non_matches = [ chunks [ 0 ] ] , [ ] for chunk in chunks [ 1 : ] : if matches [ 0 ] [ 2 ] == chunk [ 2 ] : matches . append ( chunk ) else : non_matches . append ( chunk ) # Check for EOF or obviously unique files if len ( matches ) == 1 or matches [ 0 ] [ 2 ] == "" : for x in matches : x [ 1 ] . close ( ) done . append ( [ x [ 0 ] for x in matches ] ) else : more . append ( matches ) chunks = non_matches return more , done
7609	def get_all_locations ( self , timeout : int = None ) : url = self . api . LOCATIONS return self . _get_model ( url , timeout = timeout )
1147	def deepcopy ( x , memo = None , _nil = [ ] ) : if memo is None : memo = { } d = id ( x ) y = memo . get ( d , _nil ) if y is not _nil : return y cls = type ( x ) copier = _deepcopy_dispatch . get ( cls ) if copier : y = copier ( x , memo ) else : try : issc = issubclass ( cls , type ) except TypeError : # cls is not a class (old Boost; see SF #502085) issc = 0 if issc : y = _deepcopy_atomic ( x , memo ) else : copier = getattr ( x , "__deepcopy__" , None ) if copier : y = copier ( memo ) else : reductor = dispatch_table . get ( cls ) if reductor : rv = reductor ( x ) else : reductor = getattr ( x , "__reduce_ex__" , None ) if reductor : rv = reductor ( 2 ) else : reductor = getattr ( x , "__reduce__" , None ) if reductor : rv = reductor ( ) else : raise Error ( "un(deep)copyable object of type %s" % cls ) y = _reconstruct ( x , rv , 1 , memo ) memo [ d ] = y _keep_alive ( x , memo ) # Make sure x lives at least as long as d return y
9338	def map ( self , func , sequence , reduce = None , star = False , minlength = 0 ) : def realreduce ( r ) : if reduce : if isinstance ( r , tuple ) : return reduce ( * r ) else : return reduce ( r ) return r def realfunc ( i ) : if star : return func ( * i ) else : return func ( i ) if len ( sequence ) <= 0 or self . np == 0 or get_debug ( ) : # Do this in serial self . local = lambda : None self . local . rank = 0 rt = [ realreduce ( realfunc ( i ) ) for i in sequence ] self . local = None return rt # never use more than len(sequence) processes np = min ( [ self . np , len ( sequence ) ] ) Q = self . backend . QueueFactory ( 64 ) R = self . backend . QueueFactory ( 64 ) self . ordered . reset ( ) pg = ProcessGroup ( main = self . _main , np = np , backend = self . backend , args = ( Q , R , sequence , realfunc ) ) pg . start ( ) L = [ ] N = [ ] def feeder ( pg , Q , N ) : # will fail silently if any error occurs. j = 0 try : for i , work in enumerate ( sequence ) : if not hasattr ( sequence , '__getitem__' ) : pg . put ( Q , ( i , work ) ) else : pg . put ( Q , ( i , ) ) j = j + 1 N . append ( j ) for i in range ( np ) : pg . put ( Q , None ) except StopProcessGroup : return finally : pass feeder = threading . Thread ( None , feeder , args = ( pg , Q , N ) ) feeder . start ( ) # we run fetcher on main thread to catch exceptions # raised by reduce count = 0 try : while True : try : capsule = pg . get ( R ) except queue . Empty : continue except StopProcessGroup : raise pg . get_exception ( ) capsule = capsule [ 0 ] , realreduce ( capsule [ 1 ] ) heapq . heappush ( L , capsule ) count = count + 1 if len ( N ) > 0 and count == N [ 0 ] : # if finished feeding see if all # results have been obtained break rt = [ ] # R.close() # R.join_thread() while len ( L ) > 0 : rt . append ( heapq . heappop ( L ) [ 1 ] ) pg . join ( ) feeder . join ( ) assert N [ 0 ] == len ( rt ) return rt except BaseException as e : pg . killall ( ) pg . join ( ) feeder . join ( ) raise
9126	def _make_session ( connection : Optional [ str ] = None ) -> Session : if connection is None : connection = get_global_connection ( ) engine = create_engine ( connection ) create_all ( engine ) session_cls = sessionmaker ( bind = engine ) session = session_cls ( ) return session
9691	def stop ( self ) : if self . receiver != None : self . receiver . join ( ) for s in self . senders . values ( ) : s . join ( )
3067	def clean_headers ( headers ) : clean = { } try : for k , v in six . iteritems ( headers ) : if not isinstance ( k , six . binary_type ) : k = str ( k ) if not isinstance ( v , six . binary_type ) : v = str ( v ) clean [ _helpers . _to_bytes ( k ) ] = _helpers . _to_bytes ( v ) except UnicodeEncodeError : from oauth2client . client import NonAsciiHeaderError raise NonAsciiHeaderError ( k , ': ' , v ) return clean
8298	def readLong ( data ) : high , low = struct . unpack ( ">ll" , data [ 0 : 8 ] ) big = ( long ( high ) << 32 ) + low rest = data [ 8 : ] return ( big , rest )
9054	def posteriori_covariance ( self ) : K = GLMM . covariance ( self ) tau = self . _ep . _posterior . tau return pinv ( pinv ( K ) + diag ( 1 / tau ) )
12388	def set ( self , target , value ) : if not self . _set : return if self . path is None : # There is no path defined on this resource. # We can do no magic to set the value. self . set = lambda * a : None return None if self . _segments [ target . __class__ ] : # Attempt to resolve access to this attribute. self . get ( target ) if self . _segments [ target . __class__ ] : # Attribute is not fully resolved; an interim segment is null. return # Resolve access to the parent object. # For a single-segment path this will effectively be a no-op. parent_getter = compose ( * self . _getters [ target . __class__ ] [ : - 1 ] ) target = parent_getter ( target ) # Make the setter. func = self . _make_setter ( self . path . split ( '.' ) [ - 1 ] , target . __class__ ) # Apply the setter now. func ( target , value ) # Replace this function with the constructed setter. def setter ( target , value ) : func ( parent_getter ( target ) , value ) self . set = setter
4976	def render_page_with_error_code_message ( request , context_data , error_code , log_message ) : LOGGER . error ( log_message ) messages . add_generic_error_message_with_code ( request , error_code ) return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , )
5122	def set_transitions ( self , mat ) : if isinstance ( mat , dict ) : for key , value in mat . items ( ) : probs = list ( value . values ( ) ) if key not in self . g . node : msg = "One of the keys don't correspond to a vertex." raise ValueError ( msg ) elif len ( self . out_edges [ key ] ) > 0 and not np . isclose ( sum ( probs ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( np . array ( probs ) < 0 ) . any ( ) : msg = "Some transition probabilities were negative." raise ValueError ( msg ) for k , e in enumerate ( sorted ( self . g . out_edges ( key ) ) ) : self . _route_probs [ key ] [ k ] = value . get ( e [ 1 ] , 0 ) elif isinstance ( mat , np . ndarray ) : non_terminal = np . array ( [ self . g . out_degree ( v ) > 0 for v in self . g . nodes ( ) ] ) if mat . shape != ( self . nV , self . nV ) : msg = ( "Matrix is the wrong shape, should " "be {0} x {1}." ) . format ( self . nV , self . nV ) raise ValueError ( msg ) elif not np . allclose ( np . sum ( mat [ non_terminal , : ] , axis = 1 ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( mat < 0 ) . any ( ) : raise ValueError ( "Some transition probabilities were negative." ) for k in range ( self . nV ) : for j , e in enumerate ( sorted ( self . g . out_edges ( k ) ) ) : self . _route_probs [ k ] [ j ] = mat [ k , e [ 1 ] ] else : raise TypeError ( "mat must be a numpy array or a dict." )
5782	def select_read ( self , timeout = None ) : # If we have buffered data, we consider a read possible if len ( self . _decrypted_bytes ) > 0 : return True read_ready , _ , _ = select . select ( [ self . _socket ] , [ ] , [ ] , timeout ) return len ( read_ready ) > 0
3395	def remove_genes ( cobra_model , gene_list , remove_reactions = True ) : gene_set = { cobra_model . genes . get_by_id ( str ( i ) ) for i in gene_list } gene_id_set = { i . id for i in gene_set } remover = _GeneRemover ( gene_id_set ) ast_rules = get_compiled_gene_reaction_rules ( cobra_model ) target_reactions = [ ] for reaction , rule in iteritems ( ast_rules ) : if reaction . gene_reaction_rule is None or len ( reaction . gene_reaction_rule ) == 0 : continue # reactions to remove if remove_reactions and not eval_gpr ( rule , gene_id_set ) : target_reactions . append ( reaction ) else : # if the reaction is not removed, remove the gene # from its gpr remover . visit ( rule ) new_rule = ast2str ( rule ) if new_rule != reaction . gene_reaction_rule : reaction . gene_reaction_rule = new_rule for gene in gene_set : cobra_model . genes . remove ( gene ) # remove reference to the gene in all groups associated_groups = cobra_model . get_associated_groups ( gene ) for group in associated_groups : group . remove_members ( gene ) cobra_model . remove_reactions ( target_reactions )
13387	def make_upstream_request ( self ) : url = self . upstream_url ( self . request . uri ) return tornado . httpclient . HTTPRequest ( url , method = self . request . method , headers = self . request . headers , body = self . request . body if self . request . body else None )
1062	def formatdate ( timeval = None ) : if timeval is None : timeval = time . time ( ) timeval = time . gmtime ( timeval ) return "%s, %02d %s %04d %02d:%02d:%02d GMT" % ( ( "Mon" , "Tue" , "Wed" , "Thu" , "Fri" , "Sat" , "Sun" ) [ timeval [ 6 ] ] , timeval [ 2 ] , ( "Jan" , "Feb" , "Mar" , "Apr" , "May" , "Jun" , "Jul" , "Aug" , "Sep" , "Oct" , "Nov" , "Dec" ) [ timeval [ 1 ] - 1 ] , timeval [ 0 ] , timeval [ 3 ] , timeval [ 4 ] , timeval [ 5 ] )
5862	def validate ( self , ml_template ) : data = { "ml_template" : ml_template } failure_message = "ML template validation invoke failed" res = self . _get_success_json ( self . _post_json ( 'ml_templates/validate' , data , failure_message = failure_message ) ) [ 'data' ] if res [ 'valid' ] : return 'OK' return res [ 'reason' ]
11391	def contribute_to_class ( self , cls , name ) : super ( EmbeddedMediaField , self ) . contribute_to_class ( cls , name ) register_field ( cls , self ) # add a virtual field that will create signals on any/all subclasses cls . _meta . add_virtual_field ( EmbeddedSignalCreator ( self ) )
11019	def show_response_messages ( response_json ) : message_type_kwargs = { 'warning' : { 'fg' : 'yellow' } , 'error' : { 'fg' : 'red' } , } for message in response_json . get ( 'messages' , [ ] ) : click . secho ( message [ 'text' ] , * * message_type_kwargs . get ( message [ 'type' ] , { } ) )
11913	def git_tag ( tag ) : print ( 'Tagging "{}"' . format ( tag ) ) msg = '"Released version {}"' . format ( tag ) Popen ( [ 'git' , 'tag' , '-s' , '-m' , msg , tag ] ) . wait ( )
2599	def uncan ( obj , g = None ) : import_needed = False for cls , uncanner in iteritems ( uncan_map ) : if isinstance ( cls , string_types ) : import_needed = True break elif isinstance ( obj , cls ) : return uncanner ( obj , g ) if import_needed : # perform uncan_map imports, then try again # this will usually only happen once _import_mapping ( uncan_map , _original_uncan_map ) return uncan ( obj , g ) return obj
5299	def get_context_data ( self , * * kwargs ) : data = super ( BaseCalendarMonthView , self ) . get_context_data ( * * kwargs ) year = self . get_year ( ) month = self . get_month ( ) date = _date_from_string ( year , self . get_year_format ( ) , month , self . get_month_format ( ) ) cal = Calendar ( self . get_first_of_week ( ) ) month_calendar = [ ] now = datetime . datetime . utcnow ( ) date_lists = defaultdict ( list ) multidate_objs = [ ] for obj in data [ 'object_list' ] : obj_date = self . get_start_date ( obj ) end_date_field = self . get_end_date_field ( ) if end_date_field : end_date = self . get_end_date ( obj ) if end_date and end_date != obj_date : multidate_objs . append ( { 'obj' : obj , 'range' : [ x for x in daterange ( obj_date , end_date ) ] } ) continue # We don't put multi-day events in date_lists date_lists [ obj_date ] . append ( obj ) for week in cal . monthdatescalendar ( date . year , date . month ) : week_range = set ( daterange ( week [ 0 ] , week [ 6 ] ) ) week_events = [ ] for val in multidate_objs : intersect_length = len ( week_range . intersection ( val [ 'range' ] ) ) if intersect_length : # Event happens during this week slot = 1 width = intersect_length # How many days is the event during this week? nowrap_previous = True # Does the event continue from the previous week? nowrap_next = True # Does the event continue to the next week? if val [ 'range' ] [ 0 ] >= week [ 0 ] : slot = 1 + ( val [ 'range' ] [ 0 ] - week [ 0 ] ) . days else : nowrap_previous = False if val [ 'range' ] [ - 1 ] > week [ 6 ] : nowrap_next = False week_events . append ( { 'event' : val [ 'obj' ] , 'slot' : slot , 'width' : width , 'nowrap_previous' : nowrap_previous , 'nowrap_next' : nowrap_next , } ) week_calendar = { 'events' : week_events , 'date_list' : [ ] , } for day in week : week_calendar [ 'date_list' ] . append ( { 'day' : day , 'events' : date_lists [ day ] , 'today' : day == now . date ( ) , 'is_current_month' : day . month == date . month , } ) month_calendar . append ( week_calendar ) data [ 'calendar' ] = month_calendar data [ 'weekdays' ] = [ DAYS [ x ] for x in cal . iterweekdays ( ) ] data [ 'month' ] = date data [ 'next_month' ] = self . get_next_month ( date ) data [ 'previous_month' ] = self . get_previous_month ( date ) return data
4014	def register_consumer ( ) : global _consumers hostname , port = request . form [ 'hostname' ] , request . form [ 'port' ] app_name = _app_name_from_forwarding_info ( hostname , port ) containers = get_dusty_containers ( [ app_name ] , include_exited = True ) if not containers : raise ValueError ( 'No container exists for app {}' . format ( app_name ) ) container = containers [ 0 ] new_id = uuid1 ( ) new_consumer = Consumer ( container [ 'Id' ] , datetime . utcnow ( ) ) _consumers [ str ( new_id ) ] = new_consumer response = jsonify ( { 'app_name' : app_name , 'consumer_id' : new_id } ) response . headers [ 'Access-Control-Allow-Origin' ] = '*' response . headers [ 'Access-Control-Allow-Methods' ] = 'GET, POST' return response
5476	def get_zones ( input_list ) : if not input_list : return [ ] output_list = [ ] for zone in input_list : if zone . endswith ( '*' ) : prefix = zone [ : - 1 ] output_list . extend ( [ z for z in _ZONES if z . startswith ( prefix ) ] ) else : output_list . append ( zone ) return output_list
7410	def count_var ( nex ) : arr = np . array ( [ list ( i . split ( ) [ - 1 ] ) for i in nex ] ) miss = np . any ( arr == "N" , axis = 0 ) nomiss = arr [ : , ~ miss ] nsnps = np . invert ( np . all ( nomiss == nomiss [ 0 , : ] , axis = 0 ) ) . sum ( ) return nomiss . shape [ 1 ] , nsnps
13817	def _ConvertFieldValuePair ( js , message ) : names = [ ] message_descriptor = message . DESCRIPTOR for name in js : try : field = message_descriptor . fields_by_camelcase_name . get ( name , None ) if not field : raise ParseError ( 'Message type "{0}" has no field named "{1}".' . format ( message_descriptor . full_name , name ) ) if name in names : raise ParseError ( 'Message type "{0}" should not have multiple "{1}" fields.' . format ( message . DESCRIPTOR . full_name , name ) ) names . append ( name ) # Check no other oneof field is parsed. if field . containing_oneof is not None : oneof_name = field . containing_oneof . name if oneof_name in names : raise ParseError ( 'Message type "{0}" should not have multiple "{1}" ' 'oneof fields.' . format ( message . DESCRIPTOR . full_name , oneof_name ) ) names . append ( oneof_name ) value = js [ name ] if value is None : message . ClearField ( field . name ) continue # Parse field value. if _IsMapEntry ( field ) : message . ClearField ( field . name ) _ConvertMapFieldValue ( value , message , field ) elif field . label == descriptor . FieldDescriptor . LABEL_REPEATED : message . ClearField ( field . name ) if not isinstance ( value , list ) : raise ParseError ( 'repeated field {0} must be in [] which is ' '{1}.' . format ( name , value ) ) if field . cpp_type == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : # Repeated message field. for item in value : sub_message = getattr ( message , field . name ) . add ( ) # None is a null_value in Value. if ( item is None and sub_message . DESCRIPTOR . full_name != 'google.protobuf.Value' ) : raise ParseError ( 'null is not allowed to be used as an element' ' in a repeated field.' ) _ConvertMessage ( item , sub_message ) else : # Repeated scalar field. for item in value : if item is None : raise ParseError ( 'null is not allowed to be used as an element' ' in a repeated field.' ) getattr ( message , field . name ) . append ( _ConvertScalarFieldValue ( item , field ) ) elif field . cpp_type == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : sub_message = getattr ( message , field . name ) _ConvertMessage ( value , sub_message ) else : setattr ( message , field . name , _ConvertScalarFieldValue ( value , field ) ) except ParseError as e : if field and field . containing_oneof is None : raise ParseError ( 'Failed to parse {0} field: {1}' . format ( name , e ) ) else : raise ParseError ( str ( e ) ) except ValueError as e : raise ParseError ( 'Failed to parse {0} field: {1}.' . format ( name , e ) ) except TypeError as e : raise ParseError ( 'Failed to parse {0} field: {1}.' . format ( name , e ) )
3937	def submit_form ( self , form_selector , input_dict ) : logger . info ( 'Submitting form on page %r' , self . _page . url . split ( '?' ) [ 0 ] ) logger . info ( 'Page contains forms: %s' , [ elem . get ( 'id' ) for elem in self . _page . soup . select ( 'form' ) ] ) try : form = self . _page . soup . select ( form_selector ) [ 0 ] except IndexError : raise GoogleAuthError ( 'Failed to find form {!r} in page' . format ( form_selector ) ) logger . info ( 'Page contains inputs: %s' , [ elem . get ( 'id' ) for elem in form . select ( 'input' ) ] ) for selector , value in input_dict . items ( ) : try : form . select ( selector ) [ 0 ] [ 'value' ] = value except IndexError : raise GoogleAuthError ( 'Failed to find input {!r} in form' . format ( selector ) ) try : self . _page = self . _browser . submit ( form , self . _page . url ) self . _page . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'Failed to submit form: {}' . format ( e ) )
5310	def translate_rgb_to_ansi_code ( red , green , blue , offset , colormode ) : if colormode == terminal . NO_COLORS : # colors are disabled, thus return empty string return '' , '' if colormode == terminal . ANSI_8_COLORS or colormode == terminal . ANSI_16_COLORS : color_code = ansi . rgb_to_ansi16 ( red , green , blue ) start_code = ansi . ANSI_ESCAPE_CODE . format ( code = color_code + offset - ansi . FOREGROUND_COLOR_OFFSET ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code if colormode == terminal . ANSI_256_COLORS : color_code = ansi . rgb_to_ansi256 ( red , green , blue ) start_code = ansi . ANSI_ESCAPE_CODE . format ( code = '{base};5;{code}' . format ( base = 8 + offset , code = color_code ) ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code if colormode == terminal . TRUE_COLORS : start_code = ansi . ANSI_ESCAPE_CODE . format ( code = '{base};2;{red};{green};{blue}' . format ( base = 8 + offset , red = red , green = green , blue = blue ) ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code raise ColorfulError ( 'invalid color mode "{0}"' . format ( colormode ) )
10345	def get_merged_namespace_names ( locations , check_keywords = True ) : resources = { location : get_bel_resource ( location ) for location in locations } if check_keywords : resource_keywords = set ( config [ 'Namespace' ] [ 'Keyword' ] for config in resources . values ( ) ) if 1 != len ( resource_keywords ) : raise ValueError ( 'Tried merging namespaces with different keywords: {}' . format ( resource_keywords ) ) result = { } for resource in resources : result . update ( resource [ 'Values' ] ) return result
10611	def _calculate_H_coal ( self , T ) : m_C = 0 # kg m_H = 0 # kg m_O = 0 # kg m_N = 0 # kg m_S = 0 # kg H = 0.0 # kWh/h for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) if stoich . element_mass_fraction ( compound , 'C' ) == 1.0 : m_C += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'H' ) == 1.0 : m_H += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'O' ) == 1.0 : m_O += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'N' ) == 1.0 : m_N += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'S' ) == 1.0 : m_S += self . _compound_masses [ index ] else : dH = thermo . H ( compound , T , self . _compound_masses [ index ] ) H += dH m_total = y_C + y_H + y_O + y_N + y_S # kg/h y_C = m_C / m_total y_H = m_H / m_total y_O = m_O / m_total y_N = m_N / m_total y_S = m_S / m_total hmodel = coals . DafHTy ( ) H = hmodel . calculate ( T = T + 273.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 # kWh/kg H298 = hmodel . calculate ( T = 298.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 # kWh/kg Hdaf = H - H298 + self . _DH298 # kWh/kg Hdaf *= m_total # kWh H += Hdaf return H
13152	def log_error ( error , result ) : p = { 'error' : error , 'result' : result } _log ( TYPE_CODES . ERROR , p )
6320	def create_entrypoint ( self ) : with open ( os . path . join ( self . template_dir , 'manage.py' ) , 'r' ) as fd : data = fd . read ( ) . format ( project_name = self . project_name ) with open ( 'manage.py' , 'w' ) as fd : fd . write ( data ) os . chmod ( 'manage.py' , 0o777 )
1538	def add_spout ( self , name , spout_cls , par , config = None , optional_outputs = None ) : spout_spec = spout_cls . spec ( name = name , par = par , config = config , optional_outputs = optional_outputs ) self . add_spec ( spout_spec ) return spout_spec
4918	def course_detail ( self , request , pk , course_key ) : # pylint: disable=invalid-name,unused-argument enterprise_customer_catalog = self . get_object ( ) course = enterprise_customer_catalog . get_course ( course_key ) if not course : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . CourseDetailSerializer ( course , context = context ) return Response ( serializer . data )
1725	def eval ( self , expression , use_compilation_plan = False ) : code = 'PyJsEvalResult = eval(%s)' % json . dumps ( expression ) self . execute ( code , use_compilation_plan = use_compilation_plan ) return self [ 'PyJsEvalResult' ]
3373	def add_cons_vars_to_problem ( model , what , * * kwargs ) : context = get_context ( model ) model . solver . add ( what , * * kwargs ) if context : context ( partial ( model . solver . remove , what ) )
13396	def settings_and_attributes ( self ) : attrs = self . setting_values ( ) attrs . update ( self . __dict__ ) skip = [ "_instance_settings" , "aliases" ] for a in skip : del attrs [ a ] return attrs
1707	def run ( command , data = None , timeout = None , kill_timeout = None , env = None , cwd = None ) : command = expand_args ( command ) history = [ ] for c in command : if len ( history ) : # due to broken pipe problems pass only first 10 KiB data = history [ - 1 ] . std_out [ 0 : 10 * 1024 ] cmd = Command ( c ) try : out , err = cmd . run ( data , timeout , kill_timeout , env , cwd ) status_code = cmd . returncode except OSError as e : out , err = '' , u"\n" . join ( [ e . strerror , traceback . format_exc ( ) ] ) status_code = 127 r = Response ( process = cmd ) r . command = c r . std_out = out r . std_err = err r . status_code = status_code history . append ( r ) r = history . pop ( ) r . history = history return r
1106	def get_matching_blocks ( self ) : if self . matching_blocks is not None : return self . matching_blocks la , lb = len ( self . a ) , len ( self . b ) # This is most naturally expressed as a recursive algorithm, but # at least one user bumped into extreme use cases that exceeded # the recursion limit on their box. So, now we maintain a list # ('queue`) of blocks we still need to look at, and append partial # results to `matching_blocks` in a loop; the matches are sorted # at the end. queue = [ ( 0 , la , 0 , lb ) ] matching_blocks = [ ] while queue : alo , ahi , blo , bhi = queue . pop ( ) i , j , k = x = self . find_longest_match ( alo , ahi , blo , bhi ) # a[alo:i] vs b[blo:j] unknown # a[i:i+k] same as b[j:j+k] # a[i+k:ahi] vs b[j+k:bhi] unknown if k : # if k is 0, there was no matching block matching_blocks . append ( x ) if alo < i and blo < j : queue . append ( ( alo , i , blo , j ) ) if i + k < ahi and j + k < bhi : queue . append ( ( i + k , ahi , j + k , bhi ) ) matching_blocks . sort ( ) # It's possible that we have adjacent equal blocks in the # matching_blocks list now. Starting with 2.5, this code was added # to collapse them. i1 = j1 = k1 = 0 non_adjacent = [ ] for i2 , j2 , k2 in matching_blocks : # Is this block adjacent to i1, j1, k1? if i1 + k1 == i2 and j1 + k1 == j2 : # Yes, so collapse them -- this just increases the length of # the first block by the length of the second, and the first # block so lengthened remains the block to compare against. k1 += k2 else : # Not adjacent. Remember the first block (k1==0 means it's # the dummy we started with), and make the second block the # new block to compare against. if k1 : non_adjacent . append ( ( i1 , j1 , k1 ) ) i1 , j1 , k1 = i2 , j2 , k2 if k1 : non_adjacent . append ( ( i1 , j1 , k1 ) ) non_adjacent . append ( ( la , lb , 0 ) ) self . matching_blocks = map ( Match . _make , non_adjacent ) return self . matching_blocks
4796	def contains_entry ( self , * args , * * kwargs ) : self . _check_dict_like ( self . val , check_values = False ) entries = list ( args ) + [ { k : v } for k , v in kwargs . items ( ) ] if len ( entries ) == 0 : raise ValueError ( 'one or more entry args must be given' ) missing = [ ] for e in entries : if type ( e ) is not dict : raise TypeError ( 'given entry arg must be a dict' ) if len ( e ) != 1 : raise ValueError ( 'given entry args must contain exactly one key-value pair' ) k = next ( iter ( e ) ) if k not in self . val : missing . append ( e ) # bad key elif self . val [ k ] != e [ k ] : missing . append ( e ) # bad val if missing : self . _err ( 'Expected <%s> to contain entries %s, but did not contain %s.' % ( self . val , self . _fmt_items ( entries ) , self . _fmt_items ( missing ) ) ) return self
6852	def configure ( self , reboot = 1 ) : r = self . local_renderer for ip , hostname in self . iter_hostnames ( ) : self . vprint ( 'ip/hostname:' , ip , hostname ) r . genv . host_string = ip r . env . hostname = hostname with settings ( warn_only = True ) : r . sudo ( 'echo "{hostname}" > /etc/hostname' ) r . sudo ( 'echo "127.0.0.1 {hostname}" | cat - /etc/hosts > /tmp/out && mv /tmp/out /etc/hosts' ) r . sudo ( r . env . set_hostname_command ) if r . env . auto_reboot and int ( reboot ) : r . reboot ( )
11354	def record_add_field ( rec , tag , ind1 = '' , ind2 = '' , subfields = [ ] , controlfield_value = '' ) : if controlfield_value : doc = etree . Element ( "controlfield" , attrib = { "tag" : tag , } ) doc . text = unicode ( controlfield_value ) else : doc = etree . Element ( "datafield" , attrib = { "tag" : tag , "ind1" : ind1 , "ind2" : ind2 , } ) for code , value in subfields : field = etree . SubElement ( doc , "subfield" , attrib = { "code" : code } ) field . text = value rec . append ( doc ) return rec
5166	def __intermediate_dns_servers ( self , uci , address ) : # allow override if 'dns' in uci : return uci [ 'dns' ] # ignore if using DHCP or if "proto" is none if address [ 'proto' ] in [ 'dhcp' , 'dhcpv6' , 'none' ] : return None dns = self . netjson . get ( 'dns_servers' , None ) if dns : return ' ' . join ( dns )
10746	def get_default_fields ( self ) : field_names = self . _meta . get_all_field_names ( ) if 'id' in field_names : field_names . remove ( 'id' ) return field_names
7770	def base_handlers_factory ( self ) : tls_handler = StreamTLSHandler ( self . settings ) sasl_handler = StreamSASLHandler ( self . settings ) session_handler = SessionHandler ( ) binding_handler = ResourceBindingHandler ( self . settings ) return [ tls_handler , sasl_handler , binding_handler , session_handler ]
3600	def http_connection ( timeout ) : def wrapper ( f ) : def wrapped ( * args , * * kwargs ) : if not ( 'connection' in kwargs ) or not kwargs [ 'connection' ] : connection = requests . Session ( ) kwargs [ 'connection' ] = connection else : connection = kwargs [ 'connection' ] if not getattr ( connection , 'timeout' , False ) : connection . timeout = timeout connection . headers . update ( { 'Content-type' : 'application/json' } ) return f ( * args , * * kwargs ) return wraps ( f ) ( wrapped ) return wrapper
11130	def start ( self ) : with self . _status_lock : if self . _running : raise RuntimeError ( "Already running" ) self . _running = True # Cannot re-use Observer after stopped self . _observer = Observer ( ) self . _observer . schedule ( self . _event_handler , self . _directory_location , recursive = True ) self . _observer . start ( ) # Load all in directory afterwards to ensure no undetected changes between loading all and observing self . _origin_mapped_data = self . _load_all_in_directory ( )
12449	def _add_method ( self , effect , verb , resource , conditions ) : if verb != '*' and not hasattr ( HttpVerb , verb ) : raise NameError ( 'Invalid HTTP verb ' + verb + '. Allowed verbs in HttpVerb class' ) resource_pattern = re . compile ( self . path_regex ) if not resource_pattern . match ( resource ) : raise NameError ( 'Invalid resource path: ' + resource + '. Path should match ' + self . path_regex ) if resource [ : 1 ] == '/' : resource = resource [ 1 : ] resource_arn = ( 'arn:aws:execute-api:' + self . region + ':' + self . aws_account_id + ':' + self . rest_api_id + '/' + self . stage + '/' + verb + '/' + resource ) if effect . lower ( ) == 'allow' : self . allowMethods . append ( { 'resource_arn' : resource_arn , 'conditions' : conditions } ) elif effect . lower ( ) == 'deny' : self . denyMethods . append ( { 'resource_arn' : resource_arn , 'conditions' : conditions } )
8184	def update ( self , iterations = 10 ) : # The graph fades in when initially constructed. self . alpha += 0.05 self . alpha = min ( self . alpha , 1.0 ) # Iterates over the graph's layout. # Each step the graph's bounds are recalculated # and a number of iterations are processed, # more and more as the layout progresses. if self . layout . i == 0 : self . layout . prepare ( ) self . layout . i += 1 elif self . layout . i == 1 : self . layout . iterate ( ) elif self . layout . i < self . layout . n : n = min ( iterations , self . layout . i / 10 + 1 ) for i in range ( n ) : self . layout . iterate ( ) # Calculate the absolute center of the graph. min_ , max = self . layout . bounds self . x = _ctx . WIDTH - max . x * self . d - min_ . x * self . d self . y = _ctx . HEIGHT - max . y * self . d - min_ . y * self . d self . x /= 2 self . y /= 2 return not self . layout . done
7310	def without_tz ( request ) : t = Template ( '{% load tz %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}' ) c = RequestContext ( request ) response = t . render ( c ) return HttpResponse ( response )
11748	def _bundle_exists ( self , path ) : for attached_bundle in self . _attached_bundles : if path == attached_bundle . path : return True return False
3904	def get_conv_widget ( self , conv_id ) : if conv_id not in self . _conv_widgets : set_title_cb = ( lambda widget , title : self . _tabbed_window . set_tab ( widget , title = title ) ) widget = ConversationWidget ( self . _client , self . _coroutine_queue , self . _conv_list . get ( conv_id ) , set_title_cb , self . _keys , self . _datetimefmt ) self . _conv_widgets [ conv_id ] = widget return self . _conv_widgets [ conv_id ]
2421	def checksum_from_sha1 ( value ) : # More constrained regex at lexer level CHECKSUM_RE = re . compile ( 'SHA1:\s*([\S]+)' , re . UNICODE ) match = CHECKSUM_RE . match ( value ) if match : return checksum . Algorithm ( identifier = 'SHA1' , value = match . group ( 1 ) ) else : return None
6518	def is_excluded ( self , path ) : relpath = path . relative_to ( self . base_path ) . as_posix ( ) return matches_masks ( relpath , self . excludes )
7728	def add_item ( self , item ) : if not isinstance ( item , MucItemBase ) : raise TypeError ( "Bad item type for muc#user" ) item . as_xml ( self . xmlnode )
2875	def add_bpmn_files ( self , filenames ) : for filename in filenames : f = open ( filename , 'r' ) try : self . add_bpmn_xml ( ET . parse ( f ) , filename = filename ) finally : f . close ( )
8857	def on_run ( self ) : filename = self . tabWidget . current_widget ( ) . file . path wd = os . path . dirname ( filename ) args = Settings ( ) . get_run_config_for_file ( filename ) self . interactiveConsole . start_process ( Settings ( ) . interpreter , args = [ filename ] + args , cwd = wd ) self . dockWidget . show ( ) self . actionRun . setEnabled ( False ) self . actionConfigure_run . setEnabled ( False )
11974	def convert_nm ( nm , notation = IP_DOT , inotation = IP_UNKNOWN , check = True ) : return _convert ( nm , notation , inotation , _check = check , _isnm = True )
10260	def remove_falsy_values ( counter : Mapping [ Any , int ] ) -> Mapping [ Any , int ] : return { label : count for label , count in counter . items ( ) if count }
136	def change_first_point_by_index ( self , point_idx ) : ia . do_assert ( 0 <= point_idx < len ( self . exterior ) ) if point_idx == 0 : return self . deepcopy ( ) exterior = np . concatenate ( ( self . exterior [ point_idx : , : ] , self . exterior [ : point_idx , : ] ) , axis = 0 ) return self . deepcopy ( exterior = exterior )
7158	def add ( self , * args , * * kwargs ) : if 'question' in kwargs and isinstance ( kwargs [ 'question' ] , Question ) : question = kwargs [ 'question' ] else : question = Question ( * args , * * kwargs ) self . questions . setdefault ( question . key , [ ] ) . append ( question ) return question
7760	def _call_timeout_handlers ( self ) : sources_handled = 0 now = time . time ( ) schedule = None while self . _timeout_handlers : schedule , handler = self . _timeout_handlers [ 0 ] if schedule <= now : # pylint: disable-msg=W0212 logger . debug ( "About to call a timeout handler: {0!r}" . format ( handler ) ) self . _timeout_handlers = self . _timeout_handlers [ 1 : ] result = handler ( ) logger . debug ( " handler result: {0!r}" . format ( result ) ) rec = handler . _pyxmpp_recurring if rec : logger . debug ( " recurring, restarting in {0} s" . format ( handler . _pyxmpp_timeout ) ) self . _timeout_handlers . append ( ( now + handler . _pyxmpp_timeout , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) elif rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) self . _timeout_handlers . append ( ( now + result , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) sources_handled += 1 else : break if self . check_events ( ) : return 0 , sources_handled if self . _timeout_handlers and schedule : timeout = schedule - now else : timeout = None return timeout , sources_handled
2814	def convert_shape ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting shape ...' ) def target_layer ( x ) : import tensorflow as tf return tf . shape ( x ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
11850	def move_to ( self , thing , destination ) : thing . bump = self . some_things_at ( destination , Obstacle ) if not thing . bump : thing . location = destination for o in self . observers : o . thing_moved ( thing )
12027	def abfProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 30 * 1000 ) #it should be in the first 30k of the file f . close ( ) raw = raw . decode ( "utf-8" , "ignore" ) raw = raw . split ( "Clampex" ) [ 1 ] . split ( ".pro" ) [ 0 ] protocol = os . path . basename ( raw ) # the whole protocol filename protocolID = protocol . split ( " " ) [ 0 ] # just the first number return protocolID
2428	def set_doc_namespace ( self , doc , namespace ) : if not self . doc_namespace_set : self . doc_namespace_set = True if validations . validate_doc_namespace ( namespace ) : doc . namespace = namespace return True else : raise SPDXValueError ( 'Document::Namespace' ) else : raise CardinalityError ( 'Document::Comment' )
13155	def nt_cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , * * kwargs ) : with ( yield from cls . get_cursor ( _CursorType . NAMEDTUPLE ) ) as c : return ( yield from func ( cls , c , * args , * * kwargs ) ) return wrapper
6616	def receive ( self ) : pkgidx_result_pairs = self . receive_all ( ) if pkgidx_result_pairs is None : return results = [ r for _ , r in pkgidx_result_pairs ] return results
1245	def import_experience ( self , experiences ) : if isinstance ( experiences , dict ) : if self . unique_state : experiences [ 'states' ] = dict ( state = experiences [ 'states' ] ) if self . unique_action : experiences [ 'actions' ] = dict ( action = experiences [ 'actions' ] ) self . model . import_experience ( * * experiences ) else : if self . unique_state : states = dict ( state = list ( ) ) else : states = { name : list ( ) for name in experiences [ 0 ] [ 'states' ] } internals = [ list ( ) for _ in experiences [ 0 ] [ 'internals' ] ] if self . unique_action : actions = dict ( action = list ( ) ) else : actions = { name : list ( ) for name in experiences [ 0 ] [ 'actions' ] } terminal = list ( ) reward = list ( ) for experience in experiences : if self . unique_state : states [ 'state' ] . append ( experience [ 'states' ] ) else : for name in sorted ( states ) : states [ name ] . append ( experience [ 'states' ] [ name ] ) for n , internal in enumerate ( internals ) : internal . append ( experience [ 'internals' ] [ n ] ) if self . unique_action : actions [ 'action' ] . append ( experience [ 'actions' ] ) else : for name in sorted ( actions ) : actions [ name ] . append ( experience [ 'actions' ] [ name ] ) terminal . append ( experience [ 'terminal' ] ) reward . append ( experience [ 'reward' ] ) self . model . import_experience ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
9282	def set_filter ( self , filter_text ) : self . filter = filter_text self . logger . info ( "Setting filter to: %s" , self . filter ) if self . _connected : self . _sendall ( "#filter %s\r\n" % self . filter )
12512	def niftilist_to_array ( img_filelist , outdtype = None ) : try : first_img = img_filelist [ 0 ] vol = get_img_data ( first_img ) except IndexError as ie : raise Exception ( 'Error getting the first item of img_filelis: {}' . format ( repr_imgs ( img_filelist [ 0 ] ) ) ) from ie if not outdtype : outdtype = vol . dtype outmat = np . zeros ( ( len ( img_filelist ) , np . prod ( vol . shape ) ) , dtype = outdtype ) try : for i , img_file in enumerate ( img_filelist ) : vol = get_img_data ( img_file ) outmat [ i , : ] = vol . flatten ( ) except Exception as exc : raise Exception ( 'Error on reading file {0}.' . format ( img_file ) ) from exc return outmat , vol . shape
11525	def create_big_thumbnail ( self , token , bitstream_id , item_id , width = 575 ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'bitstreamId' ] = bitstream_id parameters [ 'itemId' ] = item_id parameters [ 'width' ] = width response = self . request ( 'midas.thumbnailcreator.create.big.thumbnail' , parameters ) return response
4561	def simpixel ( new = 0 , autoraise = True ) : simpixel_driver . open_browser ( new = new , autoraise = autoraise )
8463	def _call_api ( self , verb , url , * * request_kwargs ) : api = 'https://api.github.com{}' . format ( url ) auth_headers = { 'Authorization' : 'token {}' . format ( self . api_token ) } headers = { * * auth_headers , * * request_kwargs . pop ( 'headers' , { } ) } return getattr ( requests , verb ) ( api , headers = headers , * * request_kwargs )
597	def _compute ( self , inputs , outputs ) : #if self.topDownMode and (not 'topDownIn' in inputs): # raise RuntimeError("The input topDownIn must be linked in if " # "topDownMode is True") if self . _tfdr is None : raise RuntimeError ( "TM has not been initialized" ) # Conditional compute break self . _conditionalBreak ( ) self . _iterations += 1 # Get our inputs as numpy array buInputVector = inputs [ 'bottomUpIn' ] # Handle reset signal resetSignal = False if 'resetIn' in inputs : assert len ( inputs [ 'resetIn' ] ) == 1 if inputs [ 'resetIn' ] [ 0 ] != 0 : self . _tfdr . reset ( ) self . _sequencePos = 0 # Position within the current sequence if self . computePredictedActiveCellIndices : prevPredictedState = self . _tfdr . getPredictedState ( ) . reshape ( - 1 ) . astype ( 'float32' ) if self . anomalyMode : prevPredictedColumns = self . _tfdr . topDownCompute ( ) . copy ( ) . nonzero ( ) [ 0 ] # Perform inference and/or learning tpOutput = self . _tfdr . compute ( buInputVector , self . learningMode , self . inferenceMode ) self . _sequencePos += 1 # OR'ing together the cells in each column? if self . orColumnOutputs : tpOutput = tpOutput . reshape ( self . columnCount , self . cellsPerColumn ) . max ( axis = 1 ) # Direct logging of non-zero TM outputs if self . _fpLogTPOutput : output = tpOutput . reshape ( - 1 ) outputNZ = tpOutput . nonzero ( ) [ 0 ] outStr = " " . join ( [ "%d" % int ( token ) for token in outputNZ ] ) print >> self . _fpLogTPOutput , output . size , outStr # Write the bottom up out to our node outputs outputs [ 'bottomUpOut' ] [ : ] = tpOutput . flat if self . topDownMode : # Top-down compute outputs [ 'topDownOut' ] [ : ] = self . _tfdr . topDownCompute ( ) . copy ( ) # Set output for use with anomaly classification region if in anomalyMode if self . anomalyMode : activeLearnCells = self . _tfdr . getLearnActiveStateT ( ) size = activeLearnCells . shape [ 0 ] * activeLearnCells . shape [ 1 ] outputs [ 'lrnActiveStateT' ] [ : ] = activeLearnCells . reshape ( size ) activeColumns = buInputVector . nonzero ( ) [ 0 ] outputs [ 'anomalyScore' ] [ : ] = anomaly . computeRawAnomalyScore ( activeColumns , prevPredictedColumns ) if self . computePredictedActiveCellIndices : # Reshape so we are dealing with 1D arrays activeState = self . _tfdr . _getActiveState ( ) . reshape ( - 1 ) . astype ( 'float32' ) activeIndices = numpy . where ( activeState != 0 ) [ 0 ] predictedIndices = numpy . where ( prevPredictedState != 0 ) [ 0 ] predictedActiveIndices = numpy . intersect1d ( activeIndices , predictedIndices ) outputs [ "activeCells" ] . fill ( 0 ) outputs [ "activeCells" ] [ activeIndices ] = 1 outputs [ "predictedActiveCells" ] . fill ( 0 ) outputs [ "predictedActiveCells" ] [ predictedActiveIndices ] = 1
6621	def getVector ( self , tree , branchName ) : if ( tree , branchName ) in self . __class__ . addressDict : return self . __class__ . addressDict [ ( tree , branchName ) ] itsVector = self . _getVector ( tree , branchName ) self . __class__ . addressDict [ ( tree , branchName ) ] = itsVector return itsVector
2833	def stop ( self , pin ) : if pin not in self . pwm : raise ValueError ( 'Pin {0} is not configured as a PWM. Make sure to first call start for the pin.' . format ( pin ) ) self . pwm [ pin ] . stop ( ) del self . pwm [ pin ]
10987	def _pick_state_im_name ( state_name , im_name , use_full_path = False ) : initial_dir = os . getcwd ( ) if ( state_name is None ) or ( im_name is None ) : wid = tk . Tk ( ) wid . withdraw ( ) if state_name is None : state_name = tkfd . askopenfilename ( initialdir = initial_dir , title = 'Select pre-featured state' ) os . chdir ( os . path . dirname ( state_name ) ) if im_name is None : im_name = tkfd . askopenfilename ( initialdir = initial_dir , title = 'Select new image' ) if ( not use_full_path ) and ( os . path . dirname ( im_name ) != '' ) : im_path = os . path . dirname ( im_name ) os . chdir ( im_path ) im_name = os . path . basename ( im_name ) else : os . chdir ( initial_dir ) return state_name , im_name
9273	def filter_between_tags ( self , all_tags ) : tag_names = [ t [ "name" ] for t in all_tags ] between_tags = [ ] for tag in self . options . between_tags : try : idx = tag_names . index ( tag ) except ValueError : raise ChangelogGeneratorError ( "ERROR: can't find tag {0}, specified with " "--between-tags option." . format ( tag ) ) between_tags . append ( all_tags [ idx ] ) between_tags = self . sort_tags_by_date ( between_tags ) if len ( between_tags ) == 1 : # if option --between-tags was only 1 tag given, duplicate it # to generate the changelog only for that one tag. between_tags . append ( between_tags [ 0 ] ) older = self . get_time_of_tag ( between_tags [ 1 ] ) newer = self . get_time_of_tag ( between_tags [ 0 ] ) for tag in all_tags : if older < self . get_time_of_tag ( tag ) < newer : between_tags . append ( tag ) if older == newer : between_tags . pop ( 0 ) return between_tags
1891	def _solver_version ( self ) -> Version : self . _reset ( ) if self . _received_version is None : self . _send ( '(get-info :version)' ) self . _received_version = self . _recv ( ) key , version = shlex . split ( self . _received_version [ 1 : - 1 ] ) return Version ( * map ( int , version . split ( '.' ) ) )
3759	def atom_fractions ( self ) : things = dict ( ) for zi , atoms in zip ( self . zs , self . atomss ) : for atom , count in atoms . iteritems ( ) : if atom in things : things [ atom ] += zi * count else : things [ atom ] = zi * count tot = sum ( things . values ( ) ) return { atom : value / tot for atom , value in things . iteritems ( ) }
8892	def get_default ( self ) : if self . has_default ( ) : if callable ( self . default ) : default = self . default ( ) if isinstance ( default , uuid . UUID ) : return default . hex return default if isinstance ( self . default , uuid . UUID ) : return self . default . hex return self . default return None
4679	def getAccountFromPrivateKey ( self , wif ) : pub = self . publickey_from_wif ( wif ) return self . getAccountFromPublicKey ( pub )
13297	def default ( self , obj ) : if isinstance ( obj , datetime . datetime ) : return self . _encode_datetime ( obj ) # Fallback to the default encoding return json . JSONEncoder . default ( self , obj )
11376	def _normalize_issue_dir_with_dtd ( self , path ) : if exists ( join ( path , 'resolved_issue.xml' ) ) : return issue_xml_content = open ( join ( path , 'issue.xml' ) ) . read ( ) sis = [ 'si510.dtd' , 'si520.dtd' , 'si540.dtd' ] tmp_extracted = 0 for si in sis : if si in issue_xml_content : self . _extract_correct_dtd_package ( si . split ( '.' ) [ 0 ] , path ) tmp_extracted = 1 if not tmp_extracted : message = "It looks like the path " + path message += " does not contain an si510, si520 or si540 in issue.xml file" self . logger . error ( message ) raise ValueError ( message ) command = [ "xmllint" , "--format" , "--loaddtd" , join ( path , 'issue.xml' ) , "--output" , join ( path , 'resolved_issue.xml' ) ] dummy , dummy , cmd_err = run_shell_command ( command ) if cmd_err : message = "Error in cleaning %s: %s" % ( join ( path , 'issue.xml' ) , cmd_err ) self . logger . error ( message ) raise ValueError ( message )
2479	def datetime_iso_format ( date ) : return "{0:0>4}-{1:0>2}-{2:0>2}T{3:0>2}:{4:0>2}:{5:0>2}Z" . format ( date . year , date . month , date . day , date . hour , date . minute , date . second )
9050	def bernoulli_sample ( offset , G , heritability = 0.5 , causal_variants = None , causal_variance = 0 , random_state = None , ) : link = LogitLink ( ) mean , cov = _mean_cov ( offset , G , heritability , causal_variants , causal_variance , random_state ) lik = BernoulliProdLik ( link ) sampler = GGPSampler ( lik , mean , cov ) return sampler . sample ( random_state )
13232	def get_newcommand_macros ( tex_source ) : macros = { } command = LatexCommand ( 'newcommand' , { 'name' : 'name' , 'required' : True , 'bracket' : '{' } , { 'name' : 'content' , 'required' : True , 'bracket' : '{' } ) for macro in command . parse ( tex_source ) : macros [ macro [ 'name' ] ] = macro [ 'content' ] return macros
6784	def unlock ( self ) : self . init ( ) r = self . local_renderer if self . file_exists ( r . env . lockfile_path ) : self . vprint ( 'Unlocking %s.' % r . env . lockfile_path ) r . run_or_local ( 'rm -f {lockfile_path}' )
3430	def add_reactions ( self , reaction_list ) : def existing_filter ( rxn ) : if rxn . id in self . reactions : LOGGER . warning ( "Ignoring reaction '%s' since it already exists." , rxn . id ) return False return True # First check whether the reactions exist in the model. pruned = DictList ( filter ( existing_filter , reaction_list ) ) context = get_context ( self ) # Add reactions. Also take care of genes and metabolites in the loop. for reaction in pruned : reaction . _model = self # Build a `list()` because the dict will be modified in the loop. for metabolite in list ( reaction . metabolites ) : # TODO: Should we add a copy of the metabolite instead? if metabolite not in self . metabolites : self . add_metabolites ( metabolite ) # A copy of the metabolite exists in the model, the reaction # needs to point to the metabolite in the model. else : # FIXME: Modifying 'private' attributes is horrible. stoichiometry = reaction . _metabolites . pop ( metabolite ) model_metabolite = self . metabolites . get_by_id ( metabolite . id ) reaction . _metabolites [ model_metabolite ] = stoichiometry model_metabolite . _reaction . add ( reaction ) if context : context ( partial ( model_metabolite . _reaction . remove , reaction ) ) for gene in list ( reaction . _genes ) : # If the gene is not in the model, add it if not self . genes . has_id ( gene . id ) : self . genes += [ gene ] gene . _model = self if context : # Remove the gene later context ( partial ( self . genes . __isub__ , [ gene ] ) ) context ( partial ( setattr , gene , '_model' , None ) ) # Otherwise, make the gene point to the one in the model else : model_gene = self . genes . get_by_id ( gene . id ) if model_gene is not gene : reaction . _dissociate_gene ( gene ) reaction . _associate_gene ( model_gene ) self . reactions += pruned if context : context ( partial ( self . reactions . __isub__ , pruned ) ) # from cameo ... self . _populate_solver ( pruned )
10792	def tile_overlap ( inner , outer , norm = False ) : div = 1.0 / inner . volume if norm else 1.0 return div * ( inner . volume - util . Tile . intersection ( inner , outer ) . volume )
2488	def create_disjunction_node ( self , disjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . DisjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( disjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
11804	def assign ( self , var , val , assignment ) : oldval = assignment . get ( var , None ) if val != oldval : if oldval is not None : # Remove old val if there was one self . record_conflict ( assignment , var , oldval , - 1 ) self . record_conflict ( assignment , var , val , + 1 ) CSP . assign ( self , var , val , assignment )
7572	def clustdealer ( pairdealer , optim ) : ccnt = 0 chunk = [ ] while ccnt < optim : ## try refreshing taker, else quit try : taker = itertools . takewhile ( lambda x : x [ 0 ] != "//\n" , pairdealer ) oneclust = [ "" . join ( taker . next ( ) ) ] except StopIteration : #LOGGER.debug('last chunk %s', chunk) return 1 , chunk ## load one cluster while 1 : try : oneclust . append ( "" . join ( taker . next ( ) ) ) except StopIteration : break chunk . append ( "" . join ( oneclust ) ) ccnt += 1 return 0 , chunk
11564	def servo_config ( self , pin , min_pulse = 544 , max_pulse = 2400 ) : self . set_pin_mode ( pin , self . SERVO , self . OUTPUT ) command = [ pin , min_pulse & 0x7f , ( min_pulse >> 7 ) & 0x7f , max_pulse & 0x7f , ( max_pulse >> 7 ) & 0x7f ] self . _command_handler . send_sysex ( self . _command_handler . SERVO_CONFIG , command )
1818	def SETNZ ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF == False , 1 , 0 ) )
11058	def start ( self ) : self . bot_start_time = datetime . now ( ) self . webserver = Webserver ( self . config [ 'webserver' ] [ 'host' ] , self . config [ 'webserver' ] [ 'port' ] ) self . plugins . load ( ) self . plugins . load_state ( ) self . _find_event_handlers ( ) self . sc = ThreadedSlackClient ( self . config [ 'slack_token' ] ) self . always_send_dm = [ '_unauthorized_' ] if 'always_send_dm' in self . config : self . always_send_dm . extend ( map ( lambda x : '!' + x , self . config [ 'always_send_dm' ] ) ) # Rocket is very noisy at debug logging . getLogger ( 'Rocket.Errors.ThreadPool' ) . setLevel ( logging . INFO ) self . is_setup = True if self . test_mode : self . metrics [ 'startup_time' ] = ( datetime . now ( ) - self . bot_start_time ) . total_seconds ( ) * 1000.0
8877	def allele_expectation ( bgen , variant_idx ) : geno = bgen [ "genotype" ] [ variant_idx ] . compute ( ) if geno [ "phased" ] : raise ValueError ( "Allele expectation is define for unphased genotypes only." ) nalleles = bgen [ "variants" ] . loc [ variant_idx , "nalleles" ] . compute ( ) . item ( ) genotypes = get_genotypes ( geno [ "ploidy" ] , nalleles ) expec = [ ] for i in range ( len ( genotypes ) ) : count = asarray ( genotypes_to_allele_counts ( genotypes [ i ] ) , float ) n = count . shape [ 0 ] expec . append ( ( count . T * geno [ "probs" ] [ i , : n ] ) . sum ( 1 ) ) return stack ( expec , axis = 0 )
5667	def add_walk_distances_to_db_python ( gtfs , osm_path , cutoff_distance_m = 1000 ) : if isinstance ( gtfs , str ) : gtfs = GTFS ( gtfs ) assert ( isinstance ( gtfs , GTFS ) ) print ( "Reading in walk network" ) walk_network = create_walk_network_from_osm ( osm_path ) print ( "Matching stops to the OSM network" ) stop_I_to_nearest_osm_node , stop_I_to_nearest_osm_node_distance = match_stops_to_nodes ( gtfs , walk_network ) transfers = gtfs . get_straight_line_transfer_distances ( ) from_I_to_to_stop_Is = { stop_I : set ( ) for stop_I in stop_I_to_nearest_osm_node } for transfer_tuple in transfers . itertuples ( ) : from_I = transfer_tuple . from_stop_I to_I = transfer_tuple . to_stop_I from_I_to_to_stop_Is [ from_I ] . add ( to_I ) print ( "Computing walking distances" ) for from_I , to_stop_Is in from_I_to_to_stop_Is . items ( ) : from_node = stop_I_to_nearest_osm_node [ from_I ] from_dist = stop_I_to_nearest_osm_node_distance [ from_I ] shortest_paths = networkx . single_source_dijkstra_path_length ( walk_network , from_node , cutoff = cutoff_distance_m - from_dist , weight = "distance" ) for to_I in to_stop_Is : to_distance = stop_I_to_nearest_osm_node_distance [ to_I ] to_node = stop_I_to_nearest_osm_node [ to_I ] osm_distance = shortest_paths . get ( to_node , float ( 'inf' ) ) total_distance = from_dist + osm_distance + to_distance from_stop_I_transfers = transfers [ transfers [ 'from_stop_I' ] == from_I ] straigth_distance = from_stop_I_transfers [ from_stop_I_transfers [ "to_stop_I" ] == to_I ] [ "d" ] . values [ 0 ] assert ( straigth_distance < total_distance + 2 ) # allow for a maximum of 2 meters in calculations if total_distance <= cutoff_distance_m : gtfs . conn . execute ( "UPDATE stop_distances " "SET d_walk = " + str ( int ( total_distance ) ) + " WHERE from_stop_I=" + str ( from_I ) + " AND to_stop_I=" + str ( to_I ) ) gtfs . conn . commit ( )
2216	def _dict_itemstrs ( dict_ , * * kwargs ) : import ubelt as ub explicit = kwargs . get ( 'explicit' , False ) kwargs [ 'explicit' ] = _rectify_countdown_or_bool ( explicit ) precision = kwargs . get ( 'precision' , None ) kvsep = kwargs . get ( 'kvsep' , ': ' ) if explicit : kvsep = '=' def make_item_str ( key , val ) : if explicit or kwargs . get ( 'strkeys' , False ) : key_str = six . text_type ( key ) else : key_str = repr2 ( key , precision = precision , newlines = 0 ) prefix = key_str + kvsep kwargs [ '_return_info' ] = True val_str , _leaf_info = repr2 ( val , * * kwargs ) # If the first line does not end with an open nest char # (e.g. for ndarrays), otherwise we need to worry about # residual indentation. pos = val_str . find ( '\n' ) first_line = val_str if pos == - 1 else val_str [ : pos ] compact_brace = kwargs . get ( 'cbr' , kwargs . get ( 'compact_brace' , False ) ) if compact_brace or not first_line . rstrip ( ) . endswith ( tuple ( '([{<' ) ) : rest = '' if pos == - 1 else val_str [ pos : ] val_str = first_line . lstrip ( ) + rest if '\n' in prefix : # Fix issue with keys that span new lines item_str = prefix + val_str else : item_str = ub . hzcat ( [ prefix , val_str ] ) else : item_str = prefix + val_str return item_str , _leaf_info items = list ( six . iteritems ( dict_ ) ) _tups = [ make_item_str ( key , val ) for ( key , val ) in items ] itemstrs = [ t [ 0 ] for t in _tups ] max_height = max ( [ t [ 1 ] [ 'max_height' ] for t in _tups ] ) if _tups else 0 _leaf_info = { 'max_height' : max_height + 1 , } sort = kwargs . get ( 'sort' , None ) if sort is None : # Force ordering on unordered dicts sort = True if isinstance ( dict_ , collections . OrderedDict ) : # never sort ordered dicts; they are perfect just the way they are! sort = False if sort : itemstrs = _sort_itemstrs ( items , itemstrs ) return itemstrs , _leaf_info
6671	def is_file ( self , path , use_sudo = False ) : if self . is_local and not use_sudo : return os . path . isfile ( path ) else : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -f "%(path)s" ]' % locals ( ) ) . succeeded
11003	def psffunc ( self , * args , * * kwargs ) : if self . polychromatic : func = psfcalc . calculate_polychrome_linescan_psf else : func = psfcalc . calculate_linescan_psf return func ( * args , * * kwargs )
9956	def tracemessage ( self , maxlen = 6 ) : result = "" for i , value in enumerate ( self ) : result += "{0}: {1}\n" . format ( i , get_node_repr ( value ) ) result = result . strip ( "\n" ) lines = result . split ( "\n" ) if maxlen and len ( lines ) > maxlen : i = int ( maxlen / 2 ) lines = lines [ : i ] + [ "..." ] + lines [ - ( maxlen - i ) : ] result = "\n" . join ( lines ) return result
799	def modelsInfo ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( "wrong modelIDs type: %s" ) % ( type ( modelIDs ) , ) assert modelIDs , "modelIDs is empty" rows = self . _getMatchingRowsWithRetries ( self . _models , dict ( model_id = modelIDs ) , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . modelInfoNamedTuple . _fields ] ) results = [ self . _models . modelInfoNamedTuple . _make ( r ) for r in rows ] # NOTE: assetion will also fail if modelIDs contains duplicates assert len ( results ) == len ( modelIDs ) , "modelIDs not found: %s" % ( set ( modelIDs ) - set ( r . modelId for r in results ) ) return results
6639	def runScript ( self , scriptname , additional_environment = None ) : import subprocess import shlex command = self . getScript ( scriptname ) if command is None : logger . debug ( '%s has no script %s' , self , scriptname ) return 0 if not len ( command ) : logger . error ( "script %s of %s is empty" , scriptname , self . getName ( ) ) return 1 # define additional environment variables for scripts: env = os . environ . copy ( ) if additional_environment is not None : env . update ( additional_environment ) errcode = 0 child = None try : logger . debug ( 'running script: %s' , command ) child = subprocess . Popen ( command , cwd = self . path , env = env ) child . wait ( ) if child . returncode : logger . error ( "script %s (from %s) exited with non-zero status %s" , scriptname , self . getName ( ) , child . returncode ) errcode = child . returncode child = None finally : if child is not None : tryTerminate ( child ) return errcode
13827	def get_doc ( doc_id , db_name , server_url = 'http://127.0.0.1:5984/' , rev = None ) : db = get_server ( server_url ) [ db_name ] if rev : headers , response = db . resource . get ( doc_id , rev = rev ) return couchdb . client . Document ( response ) return db [ doc_id ]
4809	def prepare_feature ( best_processed_path , option = 'train' ) : # padding for training and testing set n_pad = 21 n_pad_2 = int ( ( n_pad - 1 ) / 2 ) pad = [ { 'char' : ' ' , 'type' : 'p' , 'target' : True } ] df_pad = pd . DataFrame ( pad * n_pad_2 ) df = [ ] for article_type in article_types : df . append ( pd . read_csv ( os . path . join ( best_processed_path , option , 'df_best_{}_{}.csv' . format ( article_type , option ) ) ) ) df = pd . concat ( df ) df = pd . concat ( ( df_pad , df , df_pad ) ) # pad with empty string feature df [ 'char' ] = df [ 'char' ] . map ( lambda x : CHARS_MAP . get ( x , 80 ) ) df [ 'type' ] = df [ 'type' ] . map ( lambda x : CHAR_TYPES_MAP . get ( x , 4 ) ) df_pad = create_n_gram_df ( df , n_pad = n_pad ) char_row = [ 'char' + str ( i + 1 ) for i in range ( n_pad_2 ) ] + [ 'char-' + str ( i + 1 ) for i in range ( n_pad_2 ) ] + [ 'char' ] type_row = [ 'type' + str ( i + 1 ) for i in range ( n_pad_2 ) ] + [ 'type-' + str ( i + 1 ) for i in range ( n_pad_2 ) ] + [ 'type' ] x_char = df_pad [ char_row ] . as_matrix ( ) x_type = df_pad [ type_row ] . as_matrix ( ) y = df_pad [ 'target' ] . astype ( int ) . as_matrix ( ) return x_char , x_type , y
11282	def append ( self , next ) : next . chained = True if self . next : self . next . append ( next ) else : self . next = next
607	def findRequirements ( ) : requirementsPath = os . path . join ( REPO_DIR , "requirements.txt" ) requirements = parse_file ( requirementsPath ) if nupicBindingsPrereleaseInstalled ( ) : # User has a pre-release version of nupic.bindings installed, which is only # possible if the user installed and built nupic.bindings from source and # it is up to the user to decide when to update nupic.bindings. We'll # quietly remove the entry in requirements.txt so as to not conflate the # two. requirements = [ req for req in requirements if "nupic.bindings" not in req ] return requirements
8461	def get_cookiecutter_config ( template , default_config = None , version = None ) : default_config = default_config or { } config_dict = cc_config . get_user_config ( ) repo_dir , _ = cc_repository . determine_repo_dir ( template = template , abbreviations = config_dict [ 'abbreviations' ] , clone_to_dir = config_dict [ 'cookiecutters_dir' ] , checkout = version , no_input = True ) context_file = os . path . join ( repo_dir , 'cookiecutter.json' ) context = cc_generate . generate_context ( context_file = context_file , default_context = { * * config_dict [ 'default_context' ] , * * default_config } ) return repo_dir , cc_prompt . prompt_for_config ( context )
2609	def _extract_buffers ( obj , threshold = MAX_BYTES ) : buffers = [ ] if isinstance ( obj , CannedObject ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : nbytes = _nbytes ( buf ) if nbytes > threshold : # buffer larger than threshold, prevent pickling obj . buffers [ i ] = None buffers . append ( buf ) # buffer too small for separate send, coerce to bytes # because pickling buffer objects just results in broken pointers elif isinstance ( buf , memoryview ) : obj . buffers [ i ] = buf . tobytes ( ) elif isinstance ( buf , buffer ) : obj . buffers [ i ] = bytes ( buf ) return buffers
3483	def write_sbml_model ( cobra_model , filename , f_replace = F_REPLACE , * * kwargs ) : doc = _model_to_sbml ( cobra_model , f_replace = f_replace , * * kwargs ) if isinstance ( filename , string_types ) : # write to path libsbml . writeSBMLToFile ( doc , filename ) elif hasattr ( filename , "write" ) : # write to file handle sbml_str = libsbml . writeSBMLToString ( doc ) filename . write ( sbml_str )
8881	def fit ( self , X , y = None ) : # Check that X have correct shape X = check_array ( X ) self . inverse_influence_matrix = self . __make_inverse_matrix ( X ) if self . threshold == 'auto' : self . threshold_value = 3 * ( 1 + X . shape [ 1 ] ) / X . shape [ 0 ] elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) ad_model = self . __make_inverse_matrix ( x_train ) AD . append ( self . __find_leverages ( x_test , ad_model ) ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
9291	def db_value ( self , value ) : # ensure we have a valid UUID if not isinstance ( value , UUID ) : value = UUID ( value ) # reconstruct for optimal indexing parts = str ( value ) . split ( "-" ) reordered = '' . join ( [ parts [ 2 ] , parts [ 1 ] , parts [ 0 ] , parts [ 3 ] , parts [ 4 ] ] ) value = binascii . unhexlify ( reordered ) return super ( OrderedUUIDField , self ) . db_value ( value )
7637	def smkdirs ( dpath , mode = 0o777 ) : if not os . path . exists ( dpath ) : os . makedirs ( dpath , mode = mode )
7319	def _create_boundary ( message ) : if not message . is_multipart ( ) or message . get_boundary ( ) is not None : return message # HACK: Python2 lists do not natively have a `copy` method. Unfortunately, # due to a bug in the Backport for the email module, the method # `Message.set_boundary` converts the Message headers into a native list, # so that other methods that rely on "copying" the Message headers fail. # `Message.set_boundary` is called from `Generator.handle_multipart` if the # message does not already have a boundary present. (This method itself is # called from `Message.as_string`.) # Hence, to prevent `Message.set_boundary` from being called, add a # boundary header manually. from future . backports . email . generator import Generator # pylint: disable=protected-access boundary = Generator . _make_boundary ( message . policy . linesep ) message . set_param ( 'boundary' , boundary ) return message
5422	def _resolve_task_logging ( job_metadata , job_resources , task_descriptors ) : if not job_resources . logging : return for task_descriptor in task_descriptors : logging_uri = provider_base . format_logging_uri ( job_resources . logging . uri , job_metadata , task_descriptor . task_metadata ) logging_path = job_model . LoggingParam ( logging_uri , job_resources . logging . file_provider ) if task_descriptor . task_resources : task_descriptor . task_resources = task_descriptor . task_resources . _replace ( logging_path = logging_path ) else : task_descriptor . task_resources = job_model . Resources ( logging_path = logging_path )
6665	def verify_certificate_chain ( self , base = None , crt = None , csr = None , key = None ) : from burlap . common import get_verbose , print_fail , print_success r = self . local_renderer if base : crt = base + '.crt' csr = base + '.csr' key = base + '.key' else : assert crt and csr and key , 'If base not provided, crt and csr and key must be given.' assert os . path . isfile ( crt ) assert os . path . isfile ( csr ) assert os . path . isfile ( key ) csr_md5 = r . local ( 'openssl req -noout -modulus -in %s | openssl md5' % csr , capture = True ) key_md5 = r . local ( 'openssl rsa -noout -modulus -in %s | openssl md5' % key , capture = True ) crt_md5 = r . local ( 'openssl x509 -noout -modulus -in %s | openssl md5' % crt , capture = True ) match = crt_md5 == csr_md5 == key_md5 if self . verbose or not match : print ( 'crt:' , crt_md5 ) print ( 'csr:' , csr_md5 ) print ( 'key:' , key_md5 ) if match : print_success ( 'Files look good!' ) else : print_fail ( 'Files no not match!' ) raise Exception ( 'Files no not match!' )
1238	def put ( self , item , priority = None ) : if not self . _isfull ( ) : self . _memory . append ( None ) position = self . _next_position_then_increment ( ) old_priority = 0 if self . _memory [ position ] is None else ( self . _memory [ position ] . priority or 0 ) row = _SumRow ( item , priority ) self . _memory [ position ] = row self . _update_internal_nodes ( position , ( row . priority or 0 ) - old_priority )
7151	def get_checksum ( cls , phrase ) : phrase_split = phrase . split ( " " ) if len ( phrase_split ) < 12 : raise ValueError ( "Invalid mnemonic phrase" ) if len ( phrase_split ) > 13 : # Standard format phrase = phrase_split [ : 24 ] else : # MyMonero format phrase = phrase_split [ : 12 ] wstr = "" . join ( word [ : cls . unique_prefix_length ] for word in phrase ) wstr = bytearray ( wstr . encode ( 'utf-8' ) ) z = ( ( crc32 ( wstr ) & 0xffffffff ) ^ 0xffffffff ) >> 0 z2 = ( ( z ^ 0xffffffff ) >> 0 ) % len ( phrase ) return phrase_split [ z2 ]
8804	def build_payload ( ipaddress , event_type , event_time = None , start_time = None , end_time = None ) : # This is the common part of all message types payload = { 'event_type' : unicode ( event_type ) , 'tenant_id' : unicode ( ipaddress . used_by_tenant_id ) , 'ip_address' : unicode ( ipaddress . address_readable ) , 'ip_version' : int ( ipaddress . version ) , 'ip_type' : unicode ( ipaddress . address_type ) , 'id' : unicode ( ipaddress . id ) } # Depending on the message type add the appropriate fields if event_type == IP_EXISTS : if start_time is None or end_time is None : raise ValueError ( 'IP_BILL: {} start_time/end_time cannot be empty' . format ( event_type ) ) payload . update ( { 'startTime' : unicode ( convert_timestamp ( start_time ) ) , 'endTime' : unicode ( convert_timestamp ( end_time ) ) } ) elif event_type in [ IP_ADD , IP_DEL , IP_ASSOC , IP_DISASSOC ] : if event_time is None : raise ValueError ( 'IP_BILL: {}: event_time cannot be NULL' . format ( event_type ) ) payload . update ( { 'eventTime' : unicode ( convert_timestamp ( event_time ) ) , 'subnet_id' : unicode ( ipaddress . subnet_id ) , 'network_id' : unicode ( ipaddress . network_id ) , 'public' : True if ipaddress . network_id == PUBLIC_NETWORK_ID else False , } ) else : raise ValueError ( 'IP_BILL: bad event_type: {}' . format ( event_type ) ) return payload
4378	def add_parent ( self , parent ) : parent . children . add ( self ) self . parents . add ( parent )
7835	def __from_xml ( self , xmlnode ) : self . fields = [ ] self . reported_fields = [ ] self . items = [ ] self . title = None self . instructions = None if ( xmlnode . type != "element" or xmlnode . name != "x" or xmlnode . ns ( ) . content != DATAFORM_NS ) : raise ValueError ( "Not a form: " + xmlnode . serialize ( ) ) self . type = xmlnode . prop ( "type" ) if not self . type in self . allowed_types : raise BadRequestProtocolError ( "Bad form type: %r" % ( self . type , ) ) child = xmlnode . children while child : if child . type != "element" or child . ns ( ) . content != DATAFORM_NS : pass elif child . name == "title" : self . title = from_utf8 ( child . getContent ( ) ) elif child . name == "instructions" : self . instructions = from_utf8 ( child . getContent ( ) ) elif child . name == "field" : self . fields . append ( Field . _new_from_xml ( child ) ) elif child . name == "item" : self . items . append ( Item . _new_from_xml ( child ) ) elif child . name == "reported" : self . __get_reported ( child ) child = child . next
6513	def _most_popular_gender ( self , name , counter ) : if name not in self . names : return self . unknown_value max_count , max_tie = ( 0 , 0 ) best = self . names [ name ] . keys ( ) [ 0 ] for gender , country_values in self . names [ name ] . items ( ) : count , tie = counter ( country_values ) if count > max_count or ( count == max_count and tie > max_tie ) : max_count , max_tie , best = count , tie , gender return best if max_count > 0 else self . unknown_value
3049	def _get_implicit_credentials ( cls ) : # Environ checks (in order). environ_checkers = [ cls . _implicit_credentials_from_files , cls . _implicit_credentials_from_gae , cls . _implicit_credentials_from_gce , ] for checker in environ_checkers : credentials = checker ( ) if credentials is not None : return credentials # If no credentials, fail. raise ApplicationDefaultCredentialsError ( ADC_HELP_MSG )
6807	def create_raspbian_vagrant_box ( self ) : r = self . local_renderer r . sudo ( 'adduser --disabled-password --gecos "" vagrant' ) #vagrant user should be able to run sudo commands without a password prompt r . sudo ( 'echo "vagrant ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/vagrant' ) r . sudo ( 'chmod 0440 /etc/sudoers.d/vagrant' ) r . sudo ( 'apt-get update' ) r . sudo ( 'apt-get install -y openssh-server' ) #put ssh key from vagrant user r . sudo ( 'mkdir -p /home/vagrant/.ssh' ) r . sudo ( 'chmod 0700 /home/vagrant/.ssh' ) r . sudo ( 'wget --no-check-certificate https://raw.github.com/mitchellh/vagrant/master/keys/vagrant.pub -O /home/vagrant/.ssh/authorized_keys' ) r . sudo ( 'chmod 0600 /home/vagrant/.ssh/authorized_keys' ) r . sudo ( 'chown -R vagrant /home/vagrant/.ssh' ) #open sudo vi /etc/ssh/sshd_config and change #PubKeyAuthentication yes #PermitEmptyPasswords no r . sudo ( "sed -i '/AuthorizedKeysFile/s/^#//g' /etc/ssh/sshd_config" ) #PasswordAuthentication no r . sudo ( "sed -i '/PasswordAuthentication/s/^#//g' /etc/ssh/sshd_config" ) r . sudo ( "sed -i 's/PasswordAuthentication yes/PasswordAuthentication no/g' /etc/ssh/sshd_config" ) #restart ssh service using #sudo service ssh restart #install additional development packages for the tools to properly compile and install r . sudo ( 'apt-get upgrade' ) r . sudo ( 'apt-get install -y gcc build-essential' ) #TODO:fix? throws dpkg: error: fgets gave an empty string from `/var/lib/dpkg/triggers/File' #r.sudo('apt-get install -y linux-headers-rpi') #do any change that you want and shutdown the VM . now , come to host machine on which guest VM is running and goto #the /var/lib/libvirt/images/ and choose raw image in which you did the change and copy somewhere for example /test r . sudo ( 'mkdir /tmp/test' ) r . sudo ( 'cp {libvirt_images_dir}/{raspbian_image} /tmp/test' ) r . sudo ( 'cp {libvirt_boot_dir}/{raspbian_kernel} /tmp/test' ) #create two file metadata.json and Vagrantfile in /test do entry in metadata.json r . render_to_file ( 'rpi/metadata.json' , '/tmp/test/metadata.json' ) r . render_to_file ( 'rpi/Vagrantfile' , '/tmp/test/Vagrantfile' ) #convert test.img to qcow2 format using r . sudo ( 'qemu-img convert -f raw -O qcow2 {libvirt_images_dir}/{raspbian_image} {libvirt_images_dir}/{raspbian_image}.qcow2' ) #rename ubuntu.qcow2 to box.img r . sudo ( 'mv {libvirt_images_dir}/{raspbian_image}.qcow2 {libvirt_images_dir}/box.img' ) #Note: currently,libvirt-vagrant support only qcow2 format. so , don't change the format just rename to box.img. #because it takes input with name box.img by default. #create box r . sudo ( 'cd /tmp/test; tar cvzf custom_box.box ./metadata.json ./Vagrantfile ./{raspbian_kernel} ./box.img' )
4921	def list ( self , request ) : catalog_api = CourseCatalogApiClient ( request . user ) catalogs = catalog_api . get_paginated_catalogs ( request . GET ) self . ensure_data_exists ( request , catalogs ) serializer = serializers . ResponsePaginationSerializer ( catalogs ) return get_paginated_response ( serializer . data , request )
13777	def AddEnumDescriptor ( self , enum_desc ) : if not isinstance ( enum_desc , descriptor . EnumDescriptor ) : raise TypeError ( 'Expected instance of descriptor.EnumDescriptor.' ) self . _enum_descriptors [ enum_desc . full_name ] = enum_desc self . AddFileDescriptor ( enum_desc . file )
6735	def write_temp_file_or_dryrun ( content , * args , * * kwargs ) : dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) if dryrun : fd , tmp_fn = tempfile . mkstemp ( ) os . remove ( tmp_fn ) cmd_run = 'local' cmd = 'cat <<EOT >> %s\n%s\nEOT' % ( tmp_fn , content ) if BURLAP_COMMAND_PREFIX : print ( '%s %s: %s' % ( render_command_prefix ( ) , cmd_run , cmd ) ) else : print ( cmd ) else : fd , tmp_fn = tempfile . mkstemp ( ) fout = open ( tmp_fn , 'w' ) fout . write ( content ) fout . close ( ) return tmp_fn
1650	def _DropCommonSuffixes ( filename ) : for suffix in itertools . chain ( ( '%s.%s' % ( test_suffix . lstrip ( '_' ) , ext ) for test_suffix , ext in itertools . product ( _test_suffixes , GetNonHeaderExtensions ( ) ) ) , ( '%s.%s' % ( suffix , ext ) for suffix , ext in itertools . product ( [ 'inl' , 'imp' , 'internal' ] , GetHeaderExtensions ( ) ) ) ) : if ( filename . endswith ( suffix ) and len ( filename ) > len ( suffix ) and filename [ - len ( suffix ) - 1 ] in ( '-' , '_' ) ) : return filename [ : - len ( suffix ) - 1 ] return os . path . splitext ( filename ) [ 0 ]
1370	def get_subparser ( parser , command ) : # pylint: disable=protected-access subparsers_actions = [ action for action in parser . _actions if isinstance ( action , argparse . _SubParsersAction ) ] # there will probably only be one subparser_action, # but better save than sorry for subparsers_action in subparsers_actions : # get all subparsers for choice , subparser in subparsers_action . choices . items ( ) : if choice == command : return subparser return None
12908	def load ( cls , fh ) : dat = fh . read ( ) try : ret = cls . from_json ( dat ) except : ret = cls . from_yaml ( dat ) return ret
3881	async def _sync ( self ) : logger . info ( 'Syncing events since {}' . format ( self . _sync_timestamp ) ) try : res = await self . _client . sync_all_new_events ( hangouts_pb2 . SyncAllNewEventsRequest ( request_header = self . _client . get_request_header ( ) , last_sync_timestamp = parsers . to_timestamp ( self . _sync_timestamp ) , max_response_size_bytes = 1048576 , # 1 MB ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to sync events, some events may be lost: {}' . format ( e ) ) else : for conv_state in res . conversation_state : conv_id = conv_state . conversation_id . id conv = self . _conv_dict . get ( conv_id , None ) if conv is not None : conv . update_conversation ( conv_state . conversation ) for event_ in conv_state . event : timestamp = parsers . from_timestamp ( event_ . timestamp ) if timestamp > self . _sync_timestamp : # This updates the sync_timestamp for us, as well # as triggering events. await self . _on_event ( event_ ) else : self . _add_conversation ( conv_state . conversation , conv_state . event , conv_state . event_continuation_token )
12140	def load_table ( self , table ) : items , data_keys = [ ] , None for key , filename in table . items ( ) : data_dict = self . filetype . data ( filename [ 0 ] ) current_keys = tuple ( sorted ( data_dict . keys ( ) ) ) values = [ data_dict [ k ] for k in current_keys ] if data_keys is None : data_keys = current_keys elif data_keys != current_keys : raise Exception ( "Data keys are inconsistent" ) items . append ( ( key , values ) ) return Table ( items , kdims = table . kdims , vdims = data_keys )
13306	def correlation ( a , b ) : diff1 = a - a . mean ( ) diff2 = b - b . mean ( ) return ( diff1 * diff2 ) . mean ( ) / ( np . sqrt ( np . square ( diff1 ) . mean ( ) * np . square ( diff2 ) . mean ( ) ) )
10271	def is_unweighted_source ( graph : BELGraph , node : BaseEntity , key : str ) -> bool : return graph . in_degree ( node ) == 0 and key not in graph . nodes [ node ]
13048	def f_annotation_filter ( annotations , type_uri , number ) : filtered = [ annotation for annotation in annotations if annotation . type_uri == type_uri ] number = min ( [ len ( filtered ) , number ] ) if number == 0 : return None else : return filtered [ number - 1 ]
2822	def convert_relu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting relu ...' ) if names == 'short' : tf_name = 'RELU' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) relu = keras . layers . Activation ( 'relu' , name = tf_name ) layers [ scope_name ] = relu ( layers [ inputs [ 0 ] ] )
1258	def get_savable_components ( self ) : components = self . get_components ( ) components = [ components [ name ] for name in sorted ( components ) ] return set ( filter ( lambda x : isinstance ( x , util . SavableComponent ) , components ) )
12130	def show ( self , exclude = [ ] ) : ordering = self . constant_keys + self . varying_keys spec_lines = [ ', ' . join ( [ '%s=%s' % ( k , s [ k ] ) for k in ordering if ( k in s ) and ( k not in exclude ) ] ) for s in self . specs ] print ( '\n' . join ( [ '%d: %s' % ( i , l ) for ( i , l ) in enumerate ( spec_lines ) ] ) )
8945	def url_as_file ( url , ext = None ) : if ext : ext = '.' + ext . strip ( '.' ) # normalize extension url_hint = 'www-{}-' . format ( urlparse ( url ) . hostname or 'any' ) if url . startswith ( 'file://' ) : url = os . path . abspath ( url [ len ( 'file://' ) : ] ) if os . path . isabs ( url ) : with open ( url , 'rb' ) as handle : content = handle . read ( ) else : content = requests . get ( url ) . content with tempfile . NamedTemporaryFile ( suffix = ext or '' , prefix = url_hint , delete = False ) as handle : handle . write ( content ) try : yield handle . name finally : if os . path . exists ( handle . name ) : os . remove ( handle . name )
1821	def SETPE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . PF , 1 , 0 ) )
3500	def assess_component ( model , reaction , side , flux_coefficient_cutoff = 0.001 , solver = None ) : reaction = model . reactions . get_by_any ( reaction ) [ 0 ] result_key = dict ( reactants = 'produced' , products = 'capacity' ) [ side ] get_components = attrgetter ( side ) with model as m : m . objective = reaction if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True simulation_results = { } # build the demand reactions and add all at once demand_reactions = { } for component in get_components ( reaction ) : coeff = reaction . metabolites [ component ] demand = m . add_boundary ( component , type = 'demand' ) demand . metabolites [ component ] = coeff demand_reactions [ demand ] = ( component , coeff ) # First assess whether all precursors can be produced simultaneously joint_demand = Reaction ( "joint_demand" ) for demand_reaction in demand_reactions : joint_demand += demand_reaction m . add_reactions ( [ joint_demand ] ) m . objective = joint_demand if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True # Otherwise assess the ability of the model to produce each precursor # individually. Now assess the ability of the model to produce each # reactant for a reaction for demand_reaction , ( component , coeff ) in iteritems ( demand_reactions ) : # Calculate the maximum amount of the with m : m . objective = demand_reaction flux = _optimize_or_value ( m , solver = solver ) # metabolite that can be produced. if flux_coefficient_cutoff > flux : # Scale the results to a single unit simulation_results . update ( { component : { 'required' : flux_coefficient_cutoff / abs ( coeff ) , result_key : flux / abs ( coeff ) } } ) if len ( simulation_results ) == 0 : simulation_results = False return simulation_results
12806	def attach ( self , observer ) : if not observer in self . _observers : self . _observers . append ( observer ) return self
1815	def SETNLE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . ZF == False , cpu . SF == cpu . OF ) , 1 , 0 ) )
2916	def get_dump ( self , indent = 0 , recursive = True ) : dbg = ( ' ' * indent * 2 ) dbg += '%s/' % self . id dbg += '%s:' % self . thread_id dbg += ' Task of %s' % self . get_name ( ) if self . task_spec . description : dbg += ' (%s)' % self . get_description ( ) dbg += ' State: %s' % self . get_state_name ( ) dbg += ' Children: %s' % len ( self . children ) if recursive : for child in self . children : dbg += '\n' + child . get_dump ( indent + 1 ) return dbg
5568	def bounds_at_zoom ( self , zoom = None ) : return ( ) if self . area_at_zoom ( zoom ) . is_empty else Bounds ( * self . area_at_zoom ( zoom ) . bounds )
4770	def is_length ( self , length ) : if type ( length ) is not int : raise TypeError ( 'given arg must be an int' ) if length < 0 : raise ValueError ( 'given arg must be a positive int' ) if len ( self . val ) != length : self . _err ( 'Expected <%s> to be of length <%d>, but was <%d>.' % ( self . val , length , len ( self . val ) ) ) return self
10860	def param_particle ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' , 'a' ] ]
6661	def generate_self_signed_certificate ( self , domain = '' , r = None ) : r = self . local_renderer r . env . domain = domain or r . env . domain assert r . env . domain , 'No SSL domain defined.' role = r or self . genv . ROLE or ALL ssl_dst = 'roles/%s/ssl' % ( role , ) if not os . path . isdir ( ssl_dst ) : os . makedirs ( ssl_dst ) r . env . base_dst = '%s/%s' % ( ssl_dst , r . env . domain ) r . local ( 'openssl req -new -newkey rsa:{ssl_length} ' '-days {ssl_days} -nodes -x509 ' '-subj "/C={ssl_country}/ST={ssl_state}/L={ssl_city}/O={ssl_organization}/CN={ssl_domain}" ' '-keyout {ssl_base_dst}.key -out {ssl_base_dst}.crt' )
5539	def read ( self , * * kwargs ) : if self . tile . pixelbuffer > self . config . output . pixelbuffer : output_tiles = list ( self . config . output_pyramid . tiles_from_bounds ( self . tile . bounds , self . tile . zoom ) ) else : output_tiles = self . config . output_pyramid . intersecting ( self . tile ) return self . config . output . extract_subset ( input_data_tiles = [ ( output_tile , self . config . output . read ( output_tile ) ) for output_tile in output_tiles ] , out_tile = self . tile , )
10386	def match_simple_metapath ( graph , node , simple_metapath ) : if 0 == len ( simple_metapath ) : yield node , else : for neighbor in graph . edges [ node ] : if graph . nodes [ neighbor ] [ FUNCTION ] == simple_metapath [ 0 ] : for path in match_simple_metapath ( graph , neighbor , simple_metapath [ 1 : ] ) : if node not in path : yield ( node , ) + path
6449	def pylint_color ( score ) : # These are the score cutoffs for each color above. # I.e. score==10 -> brightgreen, down to 7.5 > score >= 5 -> orange score_cutoffs = ( 10 , 9.5 , 8.5 , 7.5 , 5 ) for i in range ( len ( score_cutoffs ) ) : if score >= score_cutoffs [ i ] : return BADGE_COLORS [ i ] # and score < 5 -> red return BADGE_COLORS [ - 1 ]
7984	def registration_form_received ( self , stanza ) : self . lock . acquire ( ) try : self . __register = Register ( stanza . get_query ( ) ) self . registration_callback ( stanza , self . __register . get_form ( ) ) finally : self . lock . release ( )
13182	def writerow ( self , observation_data ) : if isinstance ( observation_data , ( list , tuple ) ) : row = observation_data else : row = self . dict_to_row ( observation_data ) self . writer . writerow ( row )
6068	def tabulate_integral ( self , grid , tabulate_bins ) : eta_min = 1.0e-4 eta_max = 1.05 * np . max ( self . grid_to_elliptical_radii ( grid ) ) minimum_log_eta = np . log10 ( eta_min ) maximum_log_eta = np . log10 ( eta_max ) bin_size = ( maximum_log_eta - minimum_log_eta ) / ( tabulate_bins - 1 ) return eta_min , eta_max , minimum_log_eta , maximum_log_eta , bin_size
855	def getBookmark ( self ) : if self . _write and self . _recordCount == 0 : return None rowDict = dict ( filepath = os . path . realpath ( self . _filename ) , currentRow = self . _recordCount ) return json . dumps ( rowDict )
4970	def clean ( self ) : cleaned_data = super ( EnterpriseCustomerAdminForm , self ) . clean ( ) if 'catalog' in cleaned_data and not cleaned_data [ 'catalog' ] : cleaned_data [ 'catalog' ] = None return cleaned_data
6270	def swap_buffers ( self ) : if not self . window . context : return self . frames += 1 self . window . flip ( ) self . window . dispatch_events ( )
1069	def gotonext ( self ) : while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS + '\n\r' : self . pos = self . pos + 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) else : break
10549	def get_results ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'result' , params = params ) if type ( res ) . __name__ == 'list' : return [ Result ( result ) for result in res ] else : return res except : # pragma: no cover raise
1802	def LEA ( cpu , dest , src ) : dest . write ( Operators . EXTRACT ( src . address ( ) , 0 , dest . size ) )
1577	def make_shell_logfiles_url ( host , shell_port , _ , instance_id = None ) : if not shell_port : return None if not instance_id : return "http://%s:%d/browse/log-files" % ( host , shell_port ) else : return "http://%s:%d/file/log-files/%s.log.0" % ( host , shell_port , instance_id )
7272	def register_operators ( * operators ) : def validate ( operator ) : if isoperator ( operator ) : return True raise NotImplementedError ( 'invalid operator: {}' . format ( operator ) ) def register ( operator ) : # Register operator by DSL keywords for name in operator . operators : # Check valid operators if name in Engine . operators : raise ValueError ( 'operator name "{}" from {} is already ' 'in use by other operator' . format ( name , operator . __name__ ) ) # Register operator by name Engine . operators [ name ] = operator # Validates and registers operators [ register ( operator ) for operator in operators if validate ( operator ) ]
2821	def convert_dropout ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting dropout ...' ) if names == 'short' : tf_name = 'DO' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) dropout = keras . layers . Dropout ( rate = params [ 'ratio' ] , name = tf_name ) layers [ scope_name ] = dropout ( layers [ inputs [ 0 ] ] )
3991	def _nginx_stream_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_proxy_string ( port_spec , bridge_ip ) ) server_string_spec += "\t }\n" return server_string_spec
1058	def remove_extension ( module , name , code ) : key = ( module , name ) if ( _extension_registry . get ( key ) != code or _inverted_registry . get ( code ) != key ) : raise ValueError ( "key %s is not registered with code %s" % ( key , code ) ) del _extension_registry [ key ] del _inverted_registry [ code ] if code in _extension_cache : del _extension_cache [ code ]
1604	def run_metrics ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) spouts = result [ 'physical_plan' ] [ 'spouts' ] . keys ( ) bolts = result [ 'physical_plan' ] [ 'bolts' ] . keys ( ) components = spouts + bolts cname = cl_args [ 'component' ] if cname : if cname in components : components = [ cname ] else : Log . error ( 'Unknown component: \'%s\'' % cname ) raise except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False cresult = [ ] for comp in components : try : metrics = tracker_access . get_component_metrics ( comp , cluster , env , topology , role ) except : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False stat , header = to_table ( metrics ) cresult . append ( ( comp , stat , header ) ) for i , ( comp , stat , header ) in enumerate ( cresult ) : if i != 0 : print ( '' ) print ( '\'%s\' metrics:' % comp ) print ( tabulate ( stat , headers = header ) ) return True
8554	def reserve_ipblock ( self , ipblock ) : properties = { "name" : ipblock . name } if ipblock . location : properties [ 'location' ] = ipblock . location if ipblock . size : properties [ 'size' ] = str ( ipblock . size ) raw = { "properties" : properties , } response = self . _perform_request ( url = '/ipblocks' , method = 'POST' , data = json . dumps ( raw ) ) return response
970	def _getEphemeralMembers ( self ) : e = BacktrackingTM . _getEphemeralMembers ( self ) if self . makeCells4Ephemeral : e . extend ( [ 'cells4' ] ) return e
3282	def read ( self , size = 0 ) : res = self . unread self . unread = "" # Get next chunk, cumulating requested size as needed while res == "" or size < 0 or ( size > 0 and len ( res ) < size ) : try : # Read pending data, blocking if neccessary # (but handle the case that close() is called while waiting) res += compat . to_native ( self . queue . get ( True , 0.1 ) ) except compat . queue . Empty : # There was no pending data: wait for more, unless close() was called if self . is_closed : break # Deliver `size` bytes from buffer if size > 0 and len ( res ) > size : self . unread = res [ size : ] res = res [ : size ] # print("FileLikeQueue.read({}) => {} bytes".format(size, len(res))) return res
7959	def handle_hup ( self ) : with self . lock : if self . _state == 'connecting' and self . _dst_addrs : self . _hup = False self . _set_state ( "connect" ) return self . _hup = True
2453	def set_pkg_verif_code ( self , doc , code ) : self . assert_package_exists ( ) if not self . package_verif_set : self . package_verif_set = True match = self . VERIF_CODE_REGEX . match ( code ) if match : doc . package . verif_code = match . group ( self . VERIF_CODE_CODE_GRP ) if match . group ( self . VERIF_CODE_EXC_FILES_GRP ) is not None : doc . package . verif_exc_files = match . group ( self . VERIF_CODE_EXC_FILES_GRP ) . split ( ',' ) return True else : raise SPDXValueError ( 'Package::VerificationCode' ) else : raise CardinalityError ( 'Package::VerificationCode' )
7014	def concatenate_textlcs ( lclist , sortby = 'rjd' , normalize = True ) : # read the first light curve lcdict = read_hatpi_textlc ( lclist [ 0 ] ) # track which LC goes where # initial LC lccounter = 0 lcdict [ 'concatenated' ] = { lccounter : os . path . abspath ( lclist [ 0 ] ) } lcdict [ 'lcn' ] = np . full_like ( lcdict [ 'rjd' ] , lccounter ) # normalize if needed if normalize : for col in MAGCOLS : if col in lcdict : thismedval = np . nanmedian ( lcdict [ col ] ) # handle fluxes if col in ( 'ifl1' , 'ifl2' , 'ifl3' ) : lcdict [ col ] = lcdict [ col ] / thismedval # handle mags else : lcdict [ col ] = lcdict [ col ] - thismedval # now read the rest for lcf in lclist [ 1 : ] : thislcd = read_hatpi_textlc ( lcf ) # if the columns don't agree, skip this LC if thislcd [ 'columns' ] != lcdict [ 'columns' ] : LOGERROR ( 'file %s does not have the ' 'same columns as first file %s, skipping...' % ( lcf , lclist [ 0 ] ) ) continue # otherwise, go ahead and start concatenatin' else : LOGINFO ( 'adding %s (ndet: %s) to %s (ndet: %s)' % ( lcf , thislcd [ 'objectinfo' ] [ 'ndet' ] , lclist [ 0 ] , lcdict [ lcdict [ 'columns' ] [ 0 ] ] . size ) ) # update LC tracking lccounter = lccounter + 1 lcdict [ 'concatenated' ] [ lccounter ] = os . path . abspath ( lcf ) lcdict [ 'lcn' ] = np . concatenate ( ( lcdict [ 'lcn' ] , np . full_like ( thislcd [ 'rjd' ] , lccounter ) ) ) # concatenate the columns for col in lcdict [ 'columns' ] : # handle normalization for magnitude columns if normalize and col in MAGCOLS : thismedval = np . nanmedian ( thislcd [ col ] ) # handle fluxes if col in ( 'ifl1' , 'ifl2' , 'ifl3' ) : thislcd [ col ] = thislcd [ col ] / thismedval # handle mags else : thislcd [ col ] = thislcd [ col ] - thismedval # concatenate the values lcdict [ col ] = np . concatenate ( ( lcdict [ col ] , thislcd [ col ] ) ) # # now we're all done concatenatin' # # make sure to add up the ndet lcdict [ 'objectinfo' ] [ 'ndet' ] = lcdict [ lcdict [ 'columns' ] [ 0 ] ] . size # update the stations lcdict [ 'objectinfo' ] [ 'stations' ] = [ 'HP%s' % x for x in np . unique ( lcdict [ 'stf' ] ) . tolist ( ) ] # update the total LC count lcdict [ 'nconcatenated' ] = lccounter + 1 # if we're supposed to sort by a column, do so if sortby and sortby in [ x [ 0 ] for x in COLDEFS ] : LOGINFO ( 'sorting concatenated light curve by %s...' % sortby ) sortind = np . argsort ( lcdict [ sortby ] ) # sort all the measurement columns by this index for col in lcdict [ 'columns' ] : lcdict [ col ] = lcdict [ col ] [ sortind ] # make sure to sort the lcn index as well lcdict [ 'lcn' ] = lcdict [ 'lcn' ] [ sortind ] LOGINFO ( 'done. concatenated light curve has %s detections' % lcdict [ 'objectinfo' ] [ 'ndet' ] ) return lcdict
6333	def dist_abs ( self , src , tar ) : return self . _lev . dist_abs ( src , tar , mode = 'lev' , cost = ( 1 , 1 , 9999 , 9999 ) )
8568	def get_loadbalanced_nic ( self , datacenter_id , loadbalancer_id , nic_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s/balancednics/%s?depth=%s' % ( datacenter_id , loadbalancer_id , nic_id , str ( depth ) ) ) return response
6815	def enable_mods ( self ) : r = self . local_renderer for mod_name in r . env . mods_enabled : with self . settings ( warn_only = True ) : self . enable_mod ( mod_name )
8919	def _get_service ( self ) : if "service" in self . document . attrib : value = self . document . attrib [ "service" ] . lower ( ) if value in allowed_service_types : self . params [ "service" ] = value else : raise OWSInvalidParameterValue ( "Service %s is not supported" % value , value = "service" ) else : raise OWSMissingParameterValue ( 'Parameter "service" is missing' , value = "service" ) return self . params [ "service" ]
11017	def signed_number ( number , precision = 2 ) : prefix = '' if number <= 0 else '+' number_str = '{}{:.{precision}f}' . format ( prefix , number , precision = precision ) return number_str
12605	def _to_string ( data ) : sdata = data . copy ( ) for k , v in data . items ( ) : if isinstance ( v , datetime ) : sdata [ k ] = timestamp_to_date_str ( v ) elif not isinstance ( v , ( string_types , float , int ) ) : sdata [ k ] = str ( v ) return sdata
5372	def _file_exists_in_gcs ( gcs_file_path , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , object_name = gcs_file_path [ len ( 'gs://' ) : ] . split ( '/' , 1 ) request = gcs_service . objects ( ) . get ( bucket = bucket_name , object = object_name , projection = 'noAcl' ) try : request . execute ( ) return True except errors . HttpError : return False
2579	def cleanup ( self ) : logger . info ( "DFK cleanup initiated" ) # this check won't detect two DFK cleanups happening from # different threads extremely close in time because of # non-atomic read/modify of self.cleanup_called if self . cleanup_called : raise Exception ( "attempt to clean up DFK when it has already been cleaned-up" ) self . cleanup_called = True self . log_task_states ( ) # Checkpointing takes priority over the rest of the tasks # checkpoint if any valid checkpoint method is specified if self . checkpoint_mode is not None : self . checkpoint ( ) if self . _checkpoint_timer : logger . info ( "Stopping checkpoint timer" ) self . _checkpoint_timer . close ( ) # Send final stats self . usage_tracker . send_message ( ) self . usage_tracker . close ( ) logger . info ( "Terminating flow_control and strategy threads" ) self . flowcontrol . close ( ) for executor in self . executors . values ( ) : if executor . managed : if executor . scaling_enabled : job_ids = executor . provider . resources . keys ( ) executor . scale_in ( len ( job_ids ) ) executor . shutdown ( ) self . time_completed = datetime . datetime . now ( ) if self . monitoring : self . monitoring . send ( MessageType . WORKFLOW_INFO , { 'tasks_failed_count' : self . tasks_failed_count , 'tasks_completed_count' : self . tasks_completed_count , "time_began" : self . time_began , 'time_completed' : self . time_completed , 'workflow_duration' : ( self . time_completed - self . time_began ) . total_seconds ( ) , 'run_id' : self . run_id , 'rundir' : self . run_dir } ) self . monitoring . close ( ) """ if self.logging_server is not None: self.logging_server.terminate() self.logging_server.join() if self.web_app is not None: self.web_app.terminate() self.web_app.join() """ logger . info ( "DFK cleanup complete" )
8458	def shell ( cmd , check = True , stdin = None , stdout = None , stderr = None ) : return subprocess . run ( cmd , shell = True , check = check , stdin = stdin , stdout = stdout , stderr = stderr )
8273	def color ( self , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s return rng ( clr , d )
11573	def clear_display_buffer ( self ) : for row in range ( 0 , 8 ) : self . firmata . i2c_write ( 0x70 , row * 2 , 0 , 0 ) self . firmata . i2c_write ( 0x70 , ( row * 2 ) + 1 , 0 , 0 ) for column in range ( 0 , 8 ) : self . display_buffer [ row ] [ column ] = 0
5373	def file_exists ( file_path , credentials = None ) : if file_path . startswith ( 'gs://' ) : return _file_exists_in_gcs ( file_path , credentials ) else : return os . path . isfile ( file_path )
2032	def MLOAD ( self , address ) : self . _allocate ( address , 32 ) value = self . _load ( address , 32 ) return value
6757	def reboot_or_dryrun ( self , * args , * * kwargs ) : warnings . warn ( 'Use self.run() instead.' , DeprecationWarning , stacklevel = 2 ) self . reboot ( * args , * * kwargs )
12666	def apply_mask_4d ( image , mask_img ) : # , smooth_mm=None, remove_nans=True): img = check_img ( image ) mask = check_img ( mask_img ) check_img_compatibility ( img , mask , only_check_3d = True ) vol = get_data ( img ) series , mask_data = _apply_mask_to_4d_data ( vol , mask ) return series , mask_data
7214	def preview ( image , * * kwargs ) : try : from IPython . display import Javascript , HTML , display from gbdxtools . rda . interface import RDA from gbdxtools import Interface gbdx = Interface ( ) except : print ( "IPython is required to produce maps." ) return zoom = kwargs . get ( "zoom" , 16 ) bands = kwargs . get ( "bands" ) if bands is None : bands = image . _rgb_bands wgs84_bounds = kwargs . get ( "bounds" , list ( loads ( image . metadata [ "image" ] [ "imageBoundsWGS84" ] ) . bounds ) ) center = kwargs . get ( "center" , list ( shape ( image ) . centroid . bounds [ 0 : 2 ] ) ) if image . proj != 'EPSG:4326' : code = image . proj . split ( ':' ) [ 1 ] conn = gbdx . gbdx_connection proj_info = conn . get ( 'https://ughlicoordinates.geobigdata.io/ughli/v1/projinfo/{}' . format ( code ) ) . json ( ) tfm = partial ( pyproj . transform , pyproj . Proj ( init = 'EPSG:4326' ) , pyproj . Proj ( init = image . proj ) ) bounds = list ( ops . transform ( tfm , box ( * wgs84_bounds ) ) . bounds ) else : proj_info = { } bounds = wgs84_bounds # Applying DRA to a DRA'ed image looks bad, skip if already in graph if not image . options . get ( 'dra' ) : rda = RDA ( ) # Need some simple DRA to get the image in range for display. dra = rda . HistogramDRA ( image ) image = dra . aoi ( bbox = image . bounds ) graph_id = image . rda_id node_id = image . rda . graph ( ) [ 'nodes' ] [ 0 ] [ 'id' ] map_id = "map_{}" . format ( str ( int ( time . time ( ) ) ) ) scales = ',' . join ( [ '1' ] * len ( bands ) ) offsets = ',' . join ( [ '0' ] * len ( bands ) ) display ( HTML ( Template ( ''' <div id="$map_id"/> <link href='https://openlayers.org/en/v4.6.4/css/ol.css' rel='stylesheet' /> <script src="https://cdn.polyfill.io/v2/polyfill.min.js?features=requestAnimationFrame,Element.prototype.classList,URL"></script> <style>body{margin:0;padding:0;}#$map_id{position:relative;top:0;bottom:0;width:100%;height:400px;}</style> <style></style> ''' ) . substitute ( { "map_id" : map_id } ) ) ) js = Template ( """ require.config({ paths: { oljs: 'https://cdnjs.cloudflare.com/ajax/libs/openlayers/4.6.4/ol', proj4: 'https://cdnjs.cloudflare.com/ajax/libs/proj4js/2.4.4/proj4' } }); require(['oljs', 'proj4'], function(oljs, proj4) { oljs.proj.setProj4(proj4) var md = $md; var georef = $georef; var graphId = '$graphId'; var nodeId = '$nodeId'; var extents = $bounds; var x1 = md.minTileX * md.tileXSize; var y1 = ((md.minTileY + md.numYTiles) * md.tileYSize + md.tileYSize); var x2 = ((md.minTileX + md.numXTiles) * md.tileXSize + md.tileXSize); var y2 = md.minTileY * md.tileYSize; var tileLayerResolutions = [georef.scaleX]; var url = '$url' + '/tile/'; url += graphId + '/' + nodeId; url += "/{x}/{y}.png?token=$token&display_bands=$bands&display_scales=$scales&display_offsets=$offsets"; var proj = '$proj'; var projInfo = $projInfo; if ( proj !== 'EPSG:4326' ) { var proj4def = projInfo["proj4"]; proj4.defs(proj, proj4def); var area = projInfo["area_of_use"]; var bbox = [area["area_west_bound_lon"], area["area_south_bound_lat"], area["area_east_bound_lon"], area["area_north_bound_lat"]] var projection = oljs.proj.get(proj); var fromLonLat = oljs.proj.getTransform('EPSG:4326', projection); var extent = oljs.extent.applyTransform( [bbox[0], bbox[1], bbox[2], bbox[3]], fromLonLat); projection.setExtent(extent); } else { var projection = oljs.proj.get(proj); } var rda = new oljs.layer.Tile({ title: 'RDA', opacity: 1, extent: extents, source: new oljs.source.TileImage({ crossOrigin: null, projection: projection, extent: extents, tileGrid: new oljs.tilegrid.TileGrid({ extent: extents, origin: [extents[0], extents[3]], resolutions: tileLayerResolutions, tileSize: [md.tileXSize, md.tileYSize], }), tileUrlFunction: function (coordinate) { if (coordinate === null) return undefined; const x = coordinate[1] + md.minTileX; const y = -(coordinate[2] + 1 - md.minTileY); if (x < md.minTileX || x > md.maxTileX) return undefined; if (y < md.minTileY || y > md.maxTileY) return undefined; return url.replace('{x}', x).replace('{y}', y); } }) }); var map = new oljs.Map({ layers: [ rda ], target: '$map_id', view: new oljs.View({ projection: projection, center: $center, zoom: $zoom }) }); }); """ ) . substitute ( { "map_id" : map_id , "proj" : image . proj , "projInfo" : json . dumps ( proj_info ) , "graphId" : graph_id , "bounds" : bounds , "bands" : "," . join ( map ( str , bands ) ) , "nodeId" : node_id , "md" : json . dumps ( image . metadata [ "image" ] ) , "georef" : json . dumps ( image . metadata [ "georef" ] ) , "center" : center , "zoom" : zoom , "token" : gbdx . gbdx_connection . access_token , "scales" : scales , "offsets" : offsets , "url" : VIRTUAL_RDA_URL } ) display ( Javascript ( js ) )
11962	def _dot_to_dec ( ip , check = True ) : if check and not is_dot ( ip ) : raise ValueError ( '_dot_to_dec: invalid IP: "%s"' % ip ) octets = str ( ip ) . split ( '.' ) dec = 0 dec |= int ( octets [ 0 ] ) << 24 dec |= int ( octets [ 1 ] ) << 16 dec |= int ( octets [ 2 ] ) << 8 dec |= int ( octets [ 3 ] ) return dec
6825	def deploy_services ( self , site = None ) : verbose = self . verbose r = self . local_renderer if not r . env . manage_configs : return # # target_sites = self.genv.available_sites_by_host.get(hostname, None) self . render_paths ( ) supervisor_services = [ ] if r . env . purge_all_confs : r . sudo ( 'rm -Rf /etc/supervisor/conf.d/*' ) #TODO:check available_sites_by_host and remove dead? self . write_configs ( site = site ) for _site , site_data in self . iter_sites ( site = site , renderer = self . render_paths ) : if verbose : print ( 'deploy_services.site:' , _site ) # Only load site configurations that are allowed for this host. # if target_sites is not None: # assert isinstance(target_sites, (tuple, list)) # if site not in target_sites: # continue for cb in self . genv . _supervisor_create_service_callbacks : if self . verbose : print ( 'cb:' , cb ) ret = cb ( site = _site ) if self . verbose : print ( 'ret:' , ret ) if isinstance ( ret , six . string_types ) : supervisor_services . append ( ret ) elif isinstance ( ret , tuple ) : assert len ( ret ) == 2 conf_name , conf_content = ret if self . dryrun : print ( 'supervisor conf filename:' , conf_name ) print ( conf_content ) self . write_to_file ( conf_content ) self . env . services_rendered = '\n' . join ( supervisor_services ) fn = self . render_to_file ( self . env . config_template ) r . put ( local_path = fn , remote_path = self . env . config_path , use_sudo = True ) # We use supervisorctl to configure supervisor, but this will throw a uselessly vague # error message is supervisor isn't running. if not self . is_running ( ) : self . start ( ) # Reload config and then add and remove as necessary (restarts programs) r . sudo ( 'supervisorctl update' )
13877	def CopyDirectory ( source_dir , target_dir , override = False ) : _AssertIsLocal ( source_dir ) _AssertIsLocal ( target_dir ) if override and IsDir ( target_dir ) : DeleteDirectory ( target_dir , skip_on_error = False ) import shutil shutil . copytree ( source_dir , target_dir )
10374	def get_cutoff ( value : float , cutoff : Optional [ float ] = None ) -> int : cutoff = cutoff if cutoff is not None else 0 if value > cutoff : return 1 if value < ( - 1 * cutoff ) : return - 1 return 0
4041	def _retrieve_data ( self , request = None ) : full_url = "%s%s" % ( self . endpoint , request ) # The API doesn't return this any more, so we have to cheat self . self_link = request self . request = requests . get ( url = full_url , headers = self . default_headers ( ) ) self . request . encoding = "utf-8" try : self . request . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( self . request ) return self . request
2667	def write_then_readinto ( self , out_buffer , in_buffer , * , out_start = 0 , out_end = None , in_start = 0 , in_end = None , stop = True ) : if out_end is None : out_end = len ( out_buffer ) if in_end is None : in_end = len ( in_buffer ) if hasattr ( self . i2c , 'writeto_then_readfrom' ) : if self . _debug : print ( "i2c_device.writeto_then_readfrom.out_buffer:" , [ hex ( i ) for i in out_buffer [ out_start : out_end ] ] ) # In linux, at least, this is a special kernel function call self . i2c . writeto_then_readfrom ( self . device_address , out_buffer , in_buffer , out_start = out_start , out_end = out_end , in_start = in_start , in_end = in_end , stop = stop ) if self . _debug : print ( "i2c_device.writeto_then_readfrom.in_buffer:" , [ hex ( i ) for i in in_buffer [ in_start : in_end ] ] ) else : # If we don't have a special implementation, we can fake it with two calls self . write ( out_buffer , start = out_start , end = out_end , stop = stop ) if self . _debug : print ( "i2c_device.write_then_readinto.write.out_buffer:" , [ hex ( i ) for i in out_buffer [ out_start : out_end ] ] ) self . readinto ( in_buffer , start = in_start , end = in_end ) if self . _debug : print ( "i2c_device.write_then_readinto.readinto.in_buffer:" , [ hex ( i ) for i in in_buffer [ in_start : in_end ] ] )
3931	def _auth_with_refresh_token ( session , refresh_token ) : # Make a token request. token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'grant_type' : 'refresh_token' , 'refresh_token' : refresh_token , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ]
12193	def _respond ( self , channel , text ) : result = self . _format_message ( channel , text ) if result is not None : logger . info ( 'Sending message: %r' , truncate ( result , max_len = 50 ) , ) self . socket . send_str ( result )
2766	def get_volume_snapshots ( self ) : data = self . get_data ( "snapshots?resource_type=volume" ) return [ Snapshot ( token = self . token , * * snapshot ) for snapshot in data [ 'snapshots' ] ]
3641	def tradepile ( self ) : method = 'GET' url = 'tradepile' rc = self . __request__ ( method , url ) # pinEvents events = [ self . pin . event ( 'page_view' , 'Hub - Transfers' ) , self . pin . event ( 'page_view' , 'Transfer List - List View' ) ] if rc . get ( 'auctionInfo' ) : events . append ( self . pin . event ( 'page_view' , 'Item - Detail View' ) ) self . pin . send ( events ) return [ itemParse ( i ) for i in rc . get ( 'auctionInfo' , ( ) ) ]
9554	def _apply_record_length_checks ( self , i , r , summarize = False , context = None ) : for code , message , modulus in self . _record_length_checks : if i % modulus == 0 : # support sampling if len ( r ) != len ( self . _field_names ) : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'length' ] = len ( r ) if context is not None : p [ 'context' ] = context yield p
1295	def import_demo_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_demo_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )
7284	def has_edit_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_staff
6151	def fir_remez_hpf ( f_stop , f_pass , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : # Transform HPF critical frequencies to lowpass equivalent f_pass_eq = fs / 2. - f_pass f_stop_eq = fs / 2. - f_stop # Design LPF equivalent n , ff , aa , wts = lowpass_order ( f_pass_eq , f_stop_eq , d_pass , d_stop , fsamp = fs ) # Bump up the order by N_bump to bring down the final d_pass & d_stop N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) # Transform LPF equivalent to HPF n = np . arange ( len ( b ) ) b *= ( - 1 ) ** n print ( 'Remez filter taps = %d.' % N_taps ) return b
4981	def set_final_prices ( self , modes , request ) : result = [ ] for mode in modes : if mode [ 'premium' ] : mode [ 'final_price' ] = EcommerceApiClient ( request . user ) . get_course_final_price ( mode = mode , enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None , ) result . append ( mode ) return result
10844	def sent ( self ) : sent_updates = [ ] url = PATHS [ 'GET_SENT' ] % self . profile_id response = self . api . get ( url = url ) for update in response [ 'updates' ] : sent_updates . append ( Update ( api = self . api , raw_response = update ) ) self . __sent = sent_updates return self . __sent
1569	def invoke_hook_bolt_fail ( self , heron_tuple , fail_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_fail_info = BoltFailInfo ( heron_tuple = heron_tuple , failing_task_id = self . get_task_id ( ) , fail_latency_ms = fail_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_fail ( bolt_fail_info )
11539	def set_pin_type ( self , pin , ptype ) : if type ( pin ) is list : for p in pin : self . set_pin_type ( p , ptype ) return pin_id = self . _pin_mapping . get ( pin , None ) if type ( ptype ) is not ahio . PortType : raise KeyError ( 'ptype must be of type ahio.PortType' ) elif pin_id : self . _set_pin_type ( pin_id , ptype ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
11235	def get ( ) : config = { } try : config = _load_config ( ) except IOError : try : _create_default_config ( ) config = _load_config ( ) except IOError as e : raise ConfigError ( _FILE_CREATION_ERROR . format ( e . args [ 0 ] ) ) except SyntaxError as e : raise ConfigError ( _JSON_SYNTAX_ERROR . format ( e . args [ 0 ] ) ) except Exception : raise ConfigError ( _JSON_SYNTAX_ERROR . format ( 'Yaml syntax error..' ) ) try : _validate ( config ) except KeyError as e : raise ConfigError ( _MANDATORY_KEY_ERROR . format ( e . args [ 0 ] ) ) except SyntaxError as e : raise ConfigError ( _INVALID_KEY_ERROR . format ( e . args [ 0 ] ) ) except ValueError as e : raise ConfigError ( _INVALID_VALUE_ERROR . format ( e . args [ 0 ] ) ) config [ 'projects-path' ] = os . path . expanduser ( config [ 'projects-path' ] ) _complete_config ( config ) return config
6546	def connect ( self , host ) : if not self . app . connect ( host ) : command = "Connect({0})" . format ( host ) . encode ( "ascii" ) self . exec_command ( command ) self . last_host = host
10695	def yiq_to_rgb ( yiq ) : y , i , q = yiq r = y + ( 0.956 * i ) + ( 0.621 * q ) g = y - ( 0.272 * i ) - ( 0.647 * q ) b = y - ( 1.108 * i ) + ( 1.705 * q ) r = 1 if r > 1 else max ( 0 , r ) g = 1 if g > 1 else max ( 0 , g ) b = 1 if b > 1 else max ( 0 , b ) return round ( r * 255 , 3 ) , round ( g * 255 , 3 ) , round ( b * 255 , 3 )
5935	def irecarray_to_py ( a ) : pytypes = [ pyify ( typestr ) for name , typestr in a . dtype . descr ] def convert_record ( r ) : return tuple ( [ converter ( value ) for converter , value in zip ( pytypes , r ) ] ) return ( convert_record ( r ) for r in a )
13443	def path ( self , a_hash , b_hash ) : def _path ( a , b ) : if a is b : return [ a ] else : assert len ( a . children ) == 1 return [ a ] + _path ( a . children [ 0 ] , b ) a = self . nodes [ a_hash ] b = self . nodes [ b_hash ] return _path ( a , b ) [ 1 : ]
1746	def access_ok ( self , access ) : for c in access : if c not in self . perms : return False return True
9822	def list ( page ) : # pylint:disable=redefined-builtin user = AuthConfigManager . get_value ( 'username' ) if not user : Printer . print_error ( 'Please login first. `polyaxon login --help`' ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_projects ( user , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get list of projects.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Projects for current user' ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No projects found for current user' ) objects = list_dicts_to_tabulate ( [ o . to_light_dict ( humanize_values = True , exclude_attrs = [ 'uuid' , 'experiment_groups' , 'experiments' , 'description' , 'num_experiments' , 'num_independent_experiments' , 'num_experiment_groups' , 'num_jobs' , 'num_builds' , 'unique_name' ] ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Projects:" ) dict_tabulate ( objects , is_list_dict = True )
8704	def read_file ( self , filename , destination = '' ) : if not destination : destination = filename log . info ( 'Transferring %s to %s' , filename , destination ) data = self . download_file ( filename ) # Just in case, the filename may contain folder, so create it if needed. log . info ( destination ) if not os . path . exists ( os . path . dirname ( destination ) ) : try : os . makedirs ( os . path . dirname ( destination ) ) except OSError as e : # Guard against race condition if e . errno != errno . EEXIST : raise with open ( destination , 'w' ) as fil : fil . write ( data )
7222	def get ( self , recipe_id ) : self . logger . debug ( 'Retrieving recipe by id: ' + recipe_id ) url = '%(base_url)s/recipe/%(recipe_id)s' % { 'base_url' : self . base_url , 'recipe_id' : recipe_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( )
5754	def bootstrap_paginate ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" " (Page object reference)" % bits [ 0 ] ) page = parser . compile_filter ( bits [ 1 ] ) kwargs = { } bits = bits [ 2 : ] kwarg_re = re . compile ( r'(\w+)=(.+)' ) if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to bootstrap_pagination paginate tag" ) name , value = match . groups ( ) kwargs [ name ] = parser . compile_filter ( value ) return BootstrapPaginationNode ( page , kwargs )
5729	def main ( verbose = True ) : # Build C program find_executable ( MAKE_CMD ) if not find_executable ( MAKE_CMD ) : print ( 'Could not find executable "%s". Ensure it is installed and on your $PATH.' % MAKE_CMD ) exit ( 1 ) subprocess . check_output ( [ MAKE_CMD , "-C" , SAMPLE_C_CODE_DIR , "--quiet" ] ) # Initialize object that manages gdb subprocess gdbmi = GdbController ( verbose = verbose ) # Send gdb commands. Gdb machine interface commands are easier to script around, # hence the name "machine interface". # Responses are automatically printed as they are received if verbose is True. # Responses are returned after writing, by default. # Load the file responses = gdbmi . write ( "-file-exec-and-symbols %s" % SAMPLE_C_BINARY ) # Get list of source files used to compile the binary responses = gdbmi . write ( "-file-list-exec-source-files" ) # Add breakpoint responses = gdbmi . write ( "-break-insert main" ) # Run responses = gdbmi . write ( "-exec-run" ) responses = gdbmi . write ( "-exec-next" ) responses = gdbmi . write ( "-exec-next" ) responses = gdbmi . write ( "-exec-continue" ) # noqa: F841 # gdbmi.gdb_process will be None because the gdb subprocess (and its inferior # program) will be terminated gdbmi . exit ( )
2682	def upload_s3 ( cfg , path_to_zip_file , * use_s3 ) : print ( 'Uploading your new Lambda function' ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 's3' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) byte_stream = b'' with open ( path_to_zip_file , mode = 'rb' ) as fh : byte_stream = fh . read ( ) s3_key_prefix = cfg . get ( 's3_key_prefix' , '/dist' ) checksum = hashlib . new ( 'md5' , byte_stream ) . hexdigest ( ) timestamp = str ( time . time ( ) ) filename = '{prefix}{checksum}-{ts}.zip' . format ( prefix = s3_key_prefix , checksum = checksum , ts = timestamp , ) # Do we prefer development variable over config? buck_name = ( os . environ . get ( 'S3_BUCKET_NAME' ) or cfg . get ( 'bucket_name' ) ) func_name = ( os . environ . get ( 'LAMBDA_FUNCTION_NAME' ) or cfg . get ( 'function_name' ) ) kwargs = { 'Bucket' : '{}' . format ( buck_name ) , 'Key' : '{}' . format ( filename ) , 'Body' : byte_stream , } client . put_object ( * * kwargs ) print ( 'Finished uploading {} to S3 bucket {}' . format ( func_name , buck_name ) ) if use_s3 : return filename
681	def addValuesToField ( self , i , numValues ) : assert ( len ( self . fields ) > i ) values = [ self . addValueToField ( i ) for n in range ( numValues ) ] return values
13522	def _make_url ( self , slug ) : if slug . startswith ( "http" ) : return slug return "{0}{1}" . format ( self . server_url , slug )
13040	def process ( self , nemo ) : self . __nemo__ = nemo for annotation in self . __annotations__ : annotation . target . expanded = frozenset ( self . __getinnerreffs__ ( objectId = annotation . target . objectId , subreference = annotation . target . subreference ) )
8130	def layer ( self , img , x = 0 , y = 0 , name = "" ) : from types import StringType if isinstance ( img , Image . Image ) : img = img . convert ( "RGBA" ) self . layers . append ( Layer ( self , img , x , y , name ) ) return len ( self . layers ) - 1 if isinstance ( img , Layer ) : img . canvas = self self . layers . append ( img ) return len ( self . layers ) - 1 if type ( img ) == StringType : img = Image . open ( img ) img = img . convert ( "RGBA" ) self . layers . append ( Layer ( self , img , x , y , name ) ) return len ( self . layers ) - 1
8558	def create_lan ( self , datacenter_id , lan ) : data = json . dumps ( self . _create_lan_dict ( lan ) ) response = self . _perform_request ( url = '/datacenters/%s/lans' % datacenter_id , method = 'POST' , data = data ) return response
10282	def get_peripheral_predecessor_edges ( graph : BELGraph , subgraph : BELGraph ) -> EdgeIterator : for v in subgraph : for u , _ , k in graph . in_edges ( v , keys = True ) : if u not in subgraph : yield u , v , k
848	def _convertNonNumericData ( self , spatialOutput , temporalOutput , output ) : encoders = self . encoder . getEncoderList ( ) types = self . encoder . getDecoderOutputFieldTypes ( ) for i , ( encoder , type ) in enumerate ( zip ( encoders , types ) ) : spatialData = spatialOutput [ i ] temporalData = temporalOutput [ i ] if type != FieldMetaType . integer and type != FieldMetaType . float : # TODO: Make sure that this doesn't modify any state spatialData = encoder . getScalars ( spatialData ) [ 0 ] temporalData = encoder . getScalars ( temporalData ) [ 0 ] assert isinstance ( spatialData , ( float , int ) ) assert isinstance ( temporalData , ( float , int ) ) output [ 'spatialTopDownOut' ] [ i ] = spatialData output [ 'temporalTopDownOut' ] [ i ] = temporalData
8115	def distance ( x0 , y0 , x1 , y1 ) : return sqrt ( pow ( x1 - x0 , 2 ) + pow ( y1 - y0 , 2 ) )
13207	def _parse_author ( self ) : command = LatexCommand ( 'author' , { 'name' : 'authors' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no author' ) self . _authors = [ ] return try : content = parsed [ 'authors' ] except KeyError : self . _logger . warning ( 'lsstdoc has no author' ) self . _authors = [ ] return # Clean content content = content . replace ( '\n' , ' ' ) content = content . replace ( '~' , ' ' ) content = content . strip ( ) # Split content into list of individual authors authors = [ ] for part in content . split ( ',' ) : part = part . strip ( ) for split_part in part . split ( 'and ' ) : split_part = split_part . strip ( ) if len ( split_part ) > 0 : authors . append ( split_part ) self . _authors = authors
11420	def record_xml_output ( rec , tags = None , order_fn = None ) : if tags is None : tags = [ ] if isinstance ( tags , str ) : tags = [ tags ] if tags and '001' not in tags : # Add the missing controlfield. tags . append ( '001' ) marcxml = [ '<record>' ] # Add the tag 'tag' to each field in rec[tag] fields = [ ] if rec is not None : for tag in rec : if not tags or tag in tags : for field in rec [ tag ] : fields . append ( ( tag , field ) ) if order_fn is None : record_order_fields ( fields ) else : record_order_fields ( fields , order_fn ) for field in fields : marcxml . append ( field_xml_output ( field [ 1 ] , field [ 0 ] ) ) marcxml . append ( '</record>' ) return '\n' . join ( marcxml )
4992	def transmit_content_metadata ( username , channel_code , channel_pk ) : start = time . time ( ) api_user = User . objects . get ( username = username ) integrated_channel = INTEGRATED_CHANNEL_CHOICES [ channel_code ] . objects . get ( pk = channel_pk ) LOGGER . info ( 'Transmitting content metadata to integrated channel using configuration: [%s]' , integrated_channel ) try : integrated_channel . transmit_content_metadata ( api_user ) except Exception : # pylint: disable=broad-except LOGGER . exception ( 'Transmission of content metadata failed for user [%s] and for integrated ' 'channel with code [%s] and id [%s].' , username , channel_code , channel_pk ) duration = time . time ( ) - start LOGGER . info ( 'Content metadata transmission task for integrated channel configuration [%s] took [%s] seconds' , integrated_channel , duration )
6857	def create_user ( name , password , host = 'localhost' , * * kwargs ) : with settings ( hide ( 'running' ) ) : query ( "CREATE USER '%(name)s'@'%(host)s' IDENTIFIED BY '%(password)s';" % { 'name' : name , 'password' : password , 'host' : host } , * * kwargs ) puts ( "Created MySQL user '%s'." % name )
7986	def registration_success ( self , stanza ) : _unused = stanza self . lock . acquire ( ) try : self . state_change ( "registered" , self . registration_form ) if ( 'FORM_TYPE' in self . registration_form and self . registration_form [ 'FORM_TYPE' ] . value == 'jabber:iq:register' ) : if 'username' in self . registration_form : self . my_jid = JID ( self . registration_form [ 'username' ] . value , self . my_jid . domain , self . my_jid . resource ) if 'password' in self . registration_form : self . password = self . registration_form [ 'password' ] . value self . registration_callback = None self . _post_connect ( ) finally : self . lock . release ( )
9599	def elements ( self , using , value ) : return self . _execute ( Command . FIND_ELEMENTS , { 'using' : using , 'value' : value } )
4945	def get_program_data_sharing_consent ( username , program_uuid , enterprise_customer_uuid ) : enterprise_customer = get_enterprise_customer ( enterprise_customer_uuid ) discovery_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) course_ids = discovery_client . get_program_course_keys ( program_uuid ) child_consents = ( get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = individual_course_id ) for individual_course_id in course_ids ) return ProxyDataSharingConsent . from_children ( program_uuid , * child_consents )
6712	def tunnel ( self , local_port , remote_port ) : r = self . local_renderer r . env . tunnel_local_port = local_port r . env . tunnel_remote_port = remote_port r . local ( ' ssh -i {key_filename} -L {tunnel_local_port}:localhost:{tunnel_remote_port} {user}@{host_string} -N' )
8732	def divide_timedelta_float ( td , divisor ) : # td is comprised of days, seconds, microseconds dsm = [ getattr ( td , attr ) for attr in ( 'days' , 'seconds' , 'microseconds' ) ] dsm = map ( lambda elem : elem / divisor , dsm ) return datetime . timedelta ( * dsm )
3615	def get_raw_record ( self , instance , update_fields = None ) : tmp = { 'objectID' : self . objectID ( instance ) } if update_fields : if isinstance ( update_fields , str ) : update_fields = ( update_fields , ) for elt in update_fields : key = self . __translate_fields . get ( elt , None ) if key : tmp [ key ] = self . __named_fields [ key ] ( instance ) else : for key , value in self . __named_fields . items ( ) : tmp [ key ] = value ( instance ) if self . geo_field : loc = self . geo_field ( instance ) if isinstance ( loc , tuple ) : tmp [ '_geoloc' ] = { 'lat' : loc [ 0 ] , 'lng' : loc [ 1 ] } elif isinstance ( loc , dict ) : self . _validate_geolocation ( loc ) tmp [ '_geoloc' ] = loc elif isinstance ( loc , list ) : [ self . _validate_geolocation ( geo ) for geo in loc ] tmp [ '_geoloc' ] = loc if self . tags : if callable ( self . tags ) : tmp [ '_tags' ] = self . tags ( instance ) if not isinstance ( tmp [ '_tags' ] , list ) : tmp [ '_tags' ] = list ( tmp [ '_tags' ] ) logger . debug ( 'BUILD %s FROM %s' , tmp [ 'objectID' ] , self . model ) return tmp
6237	def draw_buffers ( self , near , far ) : self . ctx . disable ( moderngl . DEPTH_TEST ) helper . draw ( self . gbuffer . color_attachments [ 0 ] , pos = ( 0.0 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw ( self . gbuffer . color_attachments [ 1 ] , pos = ( 0.5 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw_depth ( self . gbuffer . depth_attachment , near , far , pos = ( 1.0 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw ( self . lightbuffer . color_attachments [ 0 ] , pos = ( 1.5 , 0.0 ) , scale = ( 0.25 , 0.25 ) )
3515	def chartbeat_bottom ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ChartbeatBottomNode ( )
5806	def parse_alert ( server_handshake_bytes ) : for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x15' : continue if len ( record_data ) != 2 : return None return ( int_from_bytes ( record_data [ 0 : 1 ] ) , int_from_bytes ( record_data [ 1 : 2 ] ) ) return None
10899	def update ( self , value = 0 ) : self . _deltas . append ( time . time ( ) ) self . value = value self . _percent = 100.0 * self . value / self . num if self . bar : self . _bars = self . _bar_symbol * int ( np . round ( self . _percent / 100. * self . _barsize ) ) if ( len ( self . _deltas ) < 2 ) or ( self . _deltas [ - 1 ] - self . _deltas [ - 2 ] ) > 1e-1 : self . _estimate_time ( ) self . _draw ( ) if self . value == self . num : self . end ( )
5380	def build_pipeline_args ( cls , project , script , job_params , task_params , reserved_labels , preemptible , logging_uri , scopes , keep_alive ) : # For the Pipelines API, envs and file inputs are all "inputs". inputs = { } inputs . update ( { SCRIPT_VARNAME : script } ) inputs . update ( { var . name : var . value for var in job_params [ 'envs' ] | task_params [ 'envs' ] if var . value } ) inputs . update ( { var . name : var . uri for var in job_params [ 'inputs' ] | task_params [ 'inputs' ] if not var . recursive and var . value } ) # Remove wildcard references for non-recursive output. When the pipelines # controller generates a delocalize call, it must point to a bare directory # for patterns. The output param OUTFILE=gs://bucket/path/*.bam should # delocalize with a call similar to: # gsutil cp /mnt/data/output/gs/bucket/path/*.bam gs://bucket/path/ outputs = { } for var in job_params [ 'outputs' ] | task_params [ 'outputs' ] : if var . recursive or not var . value : continue if '*' in var . uri . basename : outputs [ var . name ] = var . uri . path else : outputs [ var . name ] = var . uri labels = { } labels . update ( { label . name : label . value if label . value else '' for label in ( reserved_labels | job_params [ 'labels' ] | task_params [ 'labels' ] ) } ) # pyformat: disable args = { 'pipelineArgs' : { 'projectId' : project , 'resources' : { 'preemptible' : preemptible , } , 'inputs' : inputs , 'outputs' : outputs , 'labels' : labels , 'serviceAccount' : { 'email' : 'default' , 'scopes' : scopes , } , # Pass the user-specified GCS destination for pipeline logging. 'logging' : { 'gcsPath' : logging_uri } , } } # pyformat: enable if keep_alive : args [ 'pipelineArgs' ] [ 'keep_vm_alive_on_failure_duration' ] = '%ss' % keep_alive return args
8038	def code_mapping ( level , msg , default = 99 ) : try : return code_mappings_by_level [ level ] [ msg ] except KeyError : pass # Following assumes any variable messages take the format # of 'Fixed text "variable text".' only: # e.g. 'Unknown directive type "req".' # ---> 'Unknown directive type' # e.g. 'Unknown interpreted text role "need".' # ---> 'Unknown interpreted text role' if msg . count ( '"' ) == 2 and ' "' in msg and msg . endswith ( '".' ) : txt = msg [ : msg . index ( ' "' ) ] return code_mappings_by_level [ level ] . get ( txt , default ) return default
1893	def _reset ( self , constraints = None ) : if self . _proc is None : self . _start_proc ( ) else : if self . support_reset : self . _send ( "(reset)" ) for cfg in self . _init : self . _send ( cfg ) else : self . _stop_proc ( ) self . _start_proc ( ) if constraints is not None : self . _send ( constraints )
2498	def handle_package_has_file ( self , package , package_node ) : file_nodes = map ( self . handle_package_has_file_helper , package . files ) triples = [ ( package_node , self . spdx_namespace . hasFile , node ) for node in file_nodes ] for triple in triples : self . graph . add ( triple )
6692	def invalidate ( self , * paths ) : dj = self . get_satchel ( 'dj' ) if not paths : return # http://boto.readthedocs.org/en/latest/cloudfront_tut.html _settings = dj . get_settings ( ) if not _settings . AWS_STATIC_BUCKET_NAME : print ( 'No static media bucket set.' ) return if isinstance ( paths , six . string_types ) : paths = paths . split ( ',' ) all_paths = map ( str . strip , paths ) i = 0 while 1 : paths = all_paths [ i : i + 1000 ] if not paths : break c = boto . connect_cloudfront ( ) rs = c . get_all_distributions ( ) target_dist = None for dist in rs : print ( dist . domain_name , dir ( dist ) , dist . __dict__ ) bucket_name = dist . origin . dns_name . replace ( '.s3.amazonaws.com' , '' ) if bucket_name == _settings . AWS_STATIC_BUCKET_NAME : target_dist = dist break if not target_dist : raise Exception ( ( 'Target distribution %s could not be found in the AWS account.' ) % ( settings . AWS_STATIC_BUCKET_NAME , ) ) print ( 'Using distribution %s associated with origin %s.' % ( target_dist . id , _settings . AWS_STATIC_BUCKET_NAME ) ) inval_req = c . create_invalidation_request ( target_dist . id , paths ) print ( 'Issue invalidation request %s.' % ( inval_req , ) ) i += 1000
4942	def enterprise_customer_uuid ( self ) : try : enterprise_user = EnterpriseCustomerUser . objects . get ( user_id = self . user . id ) except ObjectDoesNotExist : LOGGER . warning ( 'User {} has a {} assignment but is not linked to an enterprise!' . format ( self . __class__ , self . user . id ) ) return None except MultipleObjectsReturned : LOGGER . warning ( 'User {} is linked to multiple enterprises, which is not yet supported!' . format ( self . user . id ) ) return None return str ( enterprise_user . enterprise_customer . uuid )
2566	def udp_messenger ( domain_name , UDP_IP , UDP_PORT , sock_timeout , message ) : try : if message is None : raise ValueError ( "message was none" ) encoded_message = bytes ( message , "utf-8" ) if encoded_message is None : raise ValueError ( "utf-8 encoding of message failed" ) if domain_name : try : UDP_IP = socket . gethostbyname ( domain_name ) except Exception : # (False, "Domain lookup failed, defaulting to {0}".format(UDP_IP)) pass if UDP_IP is None : raise Exception ( "UDP_IP is None" ) if UDP_PORT is None : raise Exception ( "UDP_PORT is None" ) sock = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) # UDP sock . settimeout ( sock_timeout ) sock . sendto ( bytes ( message , "utf-8" ) , ( UDP_IP , UDP_PORT ) ) sock . close ( ) except socket . timeout : logger . debug ( "Failed to send usage tracking data: socket timeout" ) except OSError as e : logger . debug ( "Failed to send usage tracking data: OSError: {}" . format ( e ) ) except Exception as e : logger . debug ( "Failed to send usage tracking data: Exception: {}" . format ( e ) )
5678	def get_stop_count_data ( self , start_ut , end_ut ) : # TODO! this function could perhaps be made a single sql query now with the new tables? trips_df = self . get_tripIs_active_in_range ( start_ut , end_ut ) # stop_I -> count, lat, lon, name stop_counts = Counter ( ) # loop over all trips: for row in trips_df . itertuples ( ) : # get stop_data and store it: stops_seq = self . get_trip_stop_time_data ( row . trip_I , row . day_start_ut ) for stop_time_row in stops_seq . itertuples ( index = False ) : if ( stop_time_row . dep_time_ut >= start_ut ) and ( stop_time_row . dep_time_ut <= end_ut ) : stop_counts [ stop_time_row . stop_I ] += 1 all_stop_data = self . stops ( ) counts = [ stop_counts [ stop_I ] for stop_I in all_stop_data [ "stop_I" ] . values ] all_stop_data . loc [ : , "count" ] = pd . Series ( counts , index = all_stop_data . index ) return all_stop_data
12888	def call ( self , path , extra = None ) : try : if not self . __webfsapi : self . __webfsapi = yield from self . get_fsapi_endpoint ( ) if not self . sid : self . sid = yield from self . create_session ( ) if not isinstance ( extra , dict ) : extra = dict ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( * * extra ) req_url = ( '%s/%s' % ( self . __webfsapi , path ) ) result = yield from self . __session . get ( req_url , params = params , timeout = self . timeout ) if result . status == 200 : text = yield from result . text ( encoding = 'utf-8' ) else : self . sid = yield from self . create_session ( ) params = dict ( pin = self . pin , sid = self . sid ) params . update ( * * extra ) result = yield from self . __session . get ( req_url , params = params , timeout = self . timeout ) text = yield from result . text ( encoding = 'utf-8' ) return objectify . fromstring ( text ) except Exception as e : logging . info ( 'AFSAPI Exception: ' + traceback . format_exc ( ) ) return None
13469	def error ( self , error_code , value , * * kwargs ) : code = self . error_code_map . get ( error_code , error_code ) try : message = Template ( self . error_messages [ code ] ) except KeyError : message = Template ( self . error_messages [ error_code ] ) placeholders = { "value" : self . hidden_value if self . hidden else value } placeholders . update ( kwargs ) placeholders . update ( self . message_values ) self . messages [ code ] = message . safe_substitute ( placeholders )
1206	def from_spec ( spec , kwargs ) : env = tensorforce . util . get_object ( obj = spec , predefined_objects = tensorforce . environments . environments , kwargs = kwargs ) assert isinstance ( env , Environment ) return env
8896	def _default_poll_callback ( self , poll_resp ) : if poll_resp . parsed is None : return False success_list = [ 'UpdatesComplete' , True , 'COMPLETE' ] status = None if self . response_format == 'xml' : status = poll_resp . parsed . find ( './Status' ) . text elif self . response_format == 'json' : status = poll_resp . parsed . get ( 'Status' , poll_resp . parsed . get ( 'status' ) ) if status is None : raise RuntimeError ( 'Unable to get poll response status.' ) return status in success_list
1813	def SETNBE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , 1 , 0 ) )
13335	def cache_resolver ( resolver , path ) : env = resolver . cache . find ( path ) if env : return env raise ResolveError
5859	def __prune_search_template ( self , extract_as_keys , search_template ) : data = { "extract_as_keys" : extract_as_keys , "search_template" : search_template } failure_message = "Failed to prune a search template" return self . _get_success_json ( self . _post_json ( 'v1/search_templates/prune-to-extract-as' , data , failure_message = failure_message ) ) [ 'data' ]
6261	def swap_buffers ( self ) : self . frames += 1 glfw . swap_buffers ( self . window ) self . poll_events ( )
10305	def calculate_single_tanimoto_set_distances ( target : Iterable [ X ] , dict_of_sets : Mapping [ Y , Set [ X ] ] ) -> Mapping [ Y , float ] : target_set = set ( target ) return { k : tanimoto_set_similarity ( target_set , s ) for k , s in dict_of_sets . items ( ) }
4614	def awaitTxConfirmation ( self , transaction , limit = 10 ) : counter = 10 for block in self . blocks ( ) : counter += 1 for tx in block [ "transactions" ] : if sorted ( tx [ "signatures" ] ) == sorted ( transaction [ "signatures" ] ) : return tx if counter > limit : raise Exception ( "The operation has not been added after 10 blocks!" )
1009	def compute ( self , bottomUpInput , enableLearn , enableInference = None ) : # As a speed optimization for now (until we need online learning), skip # computing the inference output while learning if enableInference is None : if enableLearn : enableInference = False else : enableInference = True assert ( enableLearn or enableInference ) # Get the list of columns that have bottom-up activeColumns = bottomUpInput . nonzero ( ) [ 0 ] if enableLearn : self . lrnIterationIdx += 1 self . iterationIdx += 1 if self . verbosity >= 3 : print "\n==== PY Iteration: %d =====" % ( self . iterationIdx ) print "Active cols:" , activeColumns # Update segment duty cycles if we are crossing a "tier" # We determine if it's time to update the segment duty cycles. Since the # duty cycle calculation is a moving average based on a tiered alpha, it is # important that we update all segments on each tier boundary if enableLearn : if self . lrnIterationIdx in Segment . dutyCycleTiers : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : for segment in self . cells [ c ] [ i ] : segment . dutyCycle ( ) # Update the average input density if self . avgInputDensity is None : self . avgInputDensity = len ( activeColumns ) else : self . avgInputDensity = ( 0.99 * self . avgInputDensity + 0.01 * len ( activeColumns ) ) # First, update the inference state # As a speed optimization for now (until we need online learning), skip # computing the inference output while learning if enableInference : self . _updateInferenceState ( activeColumns ) # Next, update the learning state if enableLearn : self . _updateLearningState ( activeColumns ) # Apply global decay, and remove synapses and/or segments. # Synapses are removed if their permanence value is <= 0. # Segments are removed when they don't have synapses anymore. # Removal of synapses can trigger removal of whole segments! # todo: isolate the synapse/segment retraction logic so that # it can be called in adaptSegments, in the case where we # do global decay only episodically. if self . globalDecay > 0.0 and ( ( self . lrnIterationIdx % self . maxAge ) == 0 ) : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : segsToDel = [ ] # collect and remove outside the loop for segment in self . cells [ c ] [ i ] : age = self . lrnIterationIdx - segment . lastActiveIteration if age <= self . maxAge : continue synsToDel = [ ] # collect and remove outside the loop for synapse in segment . syns : synapse [ 2 ] = synapse [ 2 ] - self . globalDecay # decrease permanence if synapse [ 2 ] <= 0 : synsToDel . append ( synapse ) # add to list to delete # 1 for sequenceSegment flag if len ( synsToDel ) == segment . getNumSynapses ( ) : segsToDel . append ( segment ) # will remove the whole segment elif len ( synsToDel ) > 0 : for syn in synsToDel : # remove some synapses on segment segment . syns . remove ( syn ) for seg in segsToDel : # remove some segments of this cell self . _cleanUpdatesList ( c , i , seg ) self . cells [ c ] [ i ] . remove ( seg ) # Update the prediction score stats # Learning always includes inference if self . collectStats : if enableInference : predictedState = self . infPredictedState [ 't-1' ] else : predictedState = self . lrnPredictedState [ 't-1' ] self . _updateStatsInferEnd ( self . _internalStats , activeColumns , predictedState , self . colConfidence [ 't-1' ] ) # Finally return the TM output output = self . _computeOutput ( ) # Print diagnostic information based on the current verbosity level self . printComputeEnd ( output , learn = enableLearn ) self . resetCalled = False return output
3412	def _fix_type ( value ) : # Because numpy floats can not be pickled to json if isinstance ( value , string_types ) : return str ( value ) if isinstance ( value , float_ ) : return float ( value ) if isinstance ( value , bool_ ) : return bool ( value ) if isinstance ( value , set ) : return list ( value ) if isinstance ( value , dict ) : return OrderedDict ( ( key , value [ key ] ) for key in sorted ( value ) ) # handle legacy Formula type if value . __class__ . __name__ == "Formula" : return str ( value ) if value is None : return "" return value
9535	def get_complete_version ( version = None ) : if version is None : from django_cryptography import VERSION as version else : assert len ( version ) == 5 assert version [ 3 ] in ( 'alpha' , 'beta' , 'rc' , 'final' ) return version
7707	def save_roster ( self , dest , pretty = True ) : if self . roster is None : raise ValueError ( "No roster" ) element = self . roster . as_xml ( ) if pretty : if len ( element ) : element . text = u'\n ' p_child = None for child in element : if p_child is not None : p_child . tail = u'\n ' if len ( child ) : child . text = u'\n ' p_grand = None for grand in child : if p_grand is not None : p_grand . tail = u'\n ' p_grand = grand if p_grand is not None : p_grand . tail = u'\n ' p_child = child if p_child is not None : p_child . tail = u"\n" tree = ElementTree . ElementTree ( element ) tree . write ( dest , "utf-8" )
10087	def update ( self , * args , * * kwargs ) : super ( Deposit , self ) . update ( * args , * * kwargs )
1150	def _show_warning ( message , category , filename , lineno , file = None , line = None ) : if file is None : file = sys . stderr if file is None : # sys.stderr is None - warnings get lost return try : file . write ( formatwarning ( message , category , filename , lineno , line ) ) except ( IOError , UnicodeError ) : pass # the file (probably stderr) is invalid - this warning gets lost.
3777	def calculate_integral ( self , T1 , T2 , method ) : return float ( quad ( self . calculate , T1 , T2 , args = ( method ) ) [ 0 ] )
5142	def make_index ( self ) : for prev , block in zip ( self . blocks [ : - 1 ] , self . blocks [ 1 : ] ) : if not block . is_comment : self . index [ block . start_lineno ] = prev
4452	def aggregate ( self , query ) : if isinstance ( query , AggregateRequest ) : has_schema = query . _with_schema has_cursor = bool ( query . _cursor ) cmd = [ self . AGGREGATE_CMD , self . index_name ] + query . build_args ( ) elif isinstance ( query , Cursor ) : has_schema = False has_cursor = True cmd = [ self . CURSOR_CMD , 'READ' , self . index_name ] + query . build_args ( ) else : raise ValueError ( 'Bad query' , query ) raw = self . redis . execute_command ( * cmd ) if has_cursor : if isinstance ( query , Cursor ) : query . cid = raw [ 1 ] cursor = query else : cursor = Cursor ( raw [ 1 ] ) raw = raw [ 0 ] else : cursor = None if query . _with_schema : schema = raw [ 0 ] rows = raw [ 2 : ] else : schema = None rows = raw [ 1 : ] res = AggregateResult ( rows , cursor , schema ) return res
8556	def list_lans ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
5106	def _current_color ( self , which = 0 ) : if which == 1 : color = self . colors [ 'edge_loop_color' ] elif which == 2 : color = self . colors [ 'vertex_color' ] else : div = self . coloring_sensitivity * self . num_servers + 1. tmp = 1. - min ( self . num_system / div , 1 ) if self . edge [ 0 ] == self . edge [ 1 ] : color = [ i * tmp for i in self . colors [ 'vertex_fill_color' ] ] color [ 3 ] = 1.0 else : color = [ i * tmp for i in self . colors [ 'edge_color' ] ] color [ 3 ] = 1 / 2. return color
9212	def get_channel_image ( self , channel , img_size = 300 , skip_cache = False ) : from bs4 import BeautifulSoup from wikipedia . exceptions import PageError import re import wikipedia wikipedia . set_lang ( 'fr' ) if not channel : _LOGGER . error ( 'Channel is not set. Could not retrieve image.' ) return # Check if the image is in cache if channel in self . _cache_channel_img and not skip_cache : img = self . _cache_channel_img [ channel ] _LOGGER . debug ( 'Cache hit: %s -> %s' , channel , img ) return img channel_info = self . get_channel_info ( channel ) query = channel_info [ 'wiki_page' ] if not query : _LOGGER . debug ( 'Wiki page is not set for channel %s' , channel ) return _LOGGER . debug ( 'Query: %s' , query ) # If there is a max image size defined use it. if 'max_img_size' in channel_info : if img_size > channel_info [ 'max_img_size' ] : _LOGGER . info ( 'Requested image size is bigger than the max, ' 'setting it to %s' , channel_info [ 'max_img_size' ] ) img_size = channel_info [ 'max_img_size' ] try : page = wikipedia . page ( query ) _LOGGER . debug ( 'Wikipedia article title: %s' , page . title ) soup = BeautifulSoup ( page . html ( ) , 'html.parser' ) images = soup . find_all ( 'img' ) img_src = None for i in images : if i [ 'alt' ] . startswith ( 'Image illustrative' ) : img_src = re . sub ( r'\d+px' , '{}px' . format ( img_size ) , i [ 'src' ] ) img = 'https:{}' . format ( img_src ) if img_src else None # Cache result self . _cache_channel_img [ channel ] = img return img except PageError : _LOGGER . error ( 'Could not fetch channel image for %s' , channel )
9835	def __general ( self ) : while 1 : # main loop try : tok = self . __peek ( ) # only peek, apply_parser() will consume except DXParserNoTokens : # save previous DXInitObject # (kludge in here as the last level-2 parser usually does not return # via the object parser) if self . currentobject and self . currentobject not in self . objects : self . objects . append ( self . currentobject ) return # stop parsing and finish # decision branches for all level-1 parsers: # (the only way to get out of the lower level parsers!) if tok . iscode ( 'COMMENT' ) : self . set_parser ( 'comment' ) # switch the state elif tok . iscode ( 'WORD' ) and tok . equals ( 'object' ) : self . set_parser ( 'object' ) # switch the state elif self . __parser is self . __general : # Either a level-2 parser screwed up or some level-1 # construct is not implemented. (Note: this elif can # be only reached at the beginning or after comments; # later we never formally switch back to __general # (would create inifinite loop) raise DXParseError ( 'Unknown level-1 construct at ' + str ( tok ) ) self . apply_parser ( )
3858	def _on_watermark_notification ( self , notif ) : # Update the conversation: if self . get_user ( notif . user_id ) . is_self : logger . info ( 'latest_read_timestamp for {} updated to {}' . format ( self . id_ , notif . read_timestamp ) ) self_conversation_state = ( self . _conversation . self_conversation_state ) self_conversation_state . self_read_state . latest_read_timestamp = ( parsers . to_timestamp ( notif . read_timestamp ) ) # Update the participants' watermarks: previous_timestamp = self . _watermarks . get ( notif . user_id , datetime . datetime . min . replace ( tzinfo = datetime . timezone . utc ) ) if notif . read_timestamp > previous_timestamp : logger . info ( ( 'latest_read_timestamp for conv {} participant {}' + ' updated to {}' ) . format ( self . id_ , notif . user_id . chat_id , notif . read_timestamp ) ) self . _watermarks [ notif . user_id ] = notif . read_timestamp
7482	def assembly_cleanup ( data ) : ## build s2 results data frame data . stats_dfs . s2 = data . _build_stat ( "s2" ) data . stats_files . s2 = os . path . join ( data . dirs . edits , 's2_rawedit_stats.txt' ) ## write stats for all samples with io . open ( data . stats_files . s2 , 'w' , encoding = 'utf-8' ) as outfile : data . stats_dfs . s2 . fillna ( value = 0 ) . astype ( np . int ) . to_string ( outfile )
11249	def average ( numbers , numtype = 'float' ) : if type == 'decimal' : return Decimal ( sum ( numbers ) ) / len ( numbers ) else : return float ( sum ( numbers ) ) / len ( numbers )
4776	def does_not_contain_duplicates ( self ) : try : if len ( self . val ) == len ( set ( self . val ) ) : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to not contain duplicates, but did.' % self . val )
13172	def last ( self , name = None ) : for c in self . children ( name , reverse = True ) : return c
6887	def parallel_epd_lclist ( lclist , externalparams , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None , nworkers = NCPUS , maxworkertasks = 1000 ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None # override the default timecols, magcols, and errcols # using the ones provided to the function if timecols is None : timecols = dtimecols if magcols is None : magcols = dmagcols if errcols is None : errcols = derrcols outdict = { } # run by magcol for t , m , e in zip ( timecols , magcols , errcols ) : tasks = [ ( x , t , m , e , externalparams , lcformat , lcformatdir , epdsmooth_sigclip , epdsmooth_windowsize , epdsmooth_func , epdsmooth_extraparams ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel_epd_worker , tasks ) pool . close ( ) pool . join ( ) outdict [ m ] = results return outdict
7928	def stop ( self ) : with self . lock : for dummy in self . threads : self . queue . put ( None )
8946	def run ( self , cmd , * args , * * kwargs ) : runner = self . ctx . run if self . ctx else None return run ( cmd , runner = runner , * args , * * kwargs )
13413	def addMenu ( self ) : self . parent . multiLogLayout . addLayout ( self . logSelectLayout ) self . getPrograms ( logType , programName )
6461	def _ends_in_cvc ( self , term ) : return len ( term ) > 2 and ( term [ - 1 ] not in self . _vowels and term [ - 2 ] in self . _vowels and term [ - 3 ] not in self . _vowels and term [ - 1 ] not in tuple ( 'wxY' ) )
2337	def remove_indirect_links ( g , alg = "aracne" , * * kwargs ) : alg = { "aracne" : aracne , "nd" : network_deconvolution , "clr" : clr } [ alg ] mat = np . array ( nx . adjacency_matrix ( g ) . todense ( ) ) return nx . relabel_nodes ( nx . DiGraph ( alg ( mat , * * kwargs ) ) , { idx : i for idx , i in enumerate ( list ( g . nodes ( ) ) ) } )
3213	def get_access_details ( self , key = None ) : if key in self . _CACHE_STATS : return self . _CACHE_STATS [ 'access_stats' ] [ key ] else : return self . _CACHE_STATS [ 'access_stats' ]
10418	def get_variants_to_controllers ( graph : BELGraph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Mapping [ Protein , Set [ Protein ] ] : rv = defaultdict ( set ) variants = variants_of ( graph , node , modifications ) for controller , variant , data in graph . in_edges ( variants , data = True ) : if data [ RELATION ] in CAUSAL_RELATIONS : rv [ variant ] . add ( controller ) return rv
4979	def post ( self , request ) : enterprise_uuid = request . POST . get ( 'enterprise_customer_uuid' ) success_url = request . POST . get ( 'redirect_url' ) failure_url = request . POST . get ( 'failure_url' ) course_id = request . POST . get ( 'course_id' , '' ) program_uuid = request . POST . get ( 'program_uuid' , '' ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) context_data = get_global_context ( request , enterprise_customer ) if not ( enterprise_uuid and success_url and failure_url ) : error_code = 'ENTGDS005' log_message = ( 'Error: one or more of the following values was falsy: ' 'enterprise_uuid: {enterprise_uuid}, ' 'success_url: {success_url}, ' 'failure_url: {failure_url} for course_id {course_id}. ' 'The following error code was reported to the user {userid}: {error_code}' . format ( userid = request . user . id , enterprise_uuid = enterprise_uuid , success_url = success_url , failure_url = failure_url , error_code = error_code , course_id = course_id , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) if not self . course_or_program_exist ( course_id , program_uuid ) : error_code = 'ENTGDS006' log_message = ( 'Neither the course with course_id: {course_id} ' 'or program with {program_uuid} exist for ' 'enterprise customer {enterprise_uuid}' 'Error code {error_code} presented to user {userid}' . format ( course_id = course_id , program_uuid = program_uuid , error_code = error_code , userid = request . user . id , enterprise_uuid = enterprise_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) consent_record = get_data_sharing_consent ( request . user . username , enterprise_uuid , program_uuid = program_uuid , course_id = course_id ) if consent_record is None : error_code = 'ENTGDS007' log_message = ( 'The was a problem with the consent record of user {userid} with ' 'enterprise_uuid {enterprise_uuid}. consent_record has a value ' 'of {consent_record} and a ' 'value for course_id {course_id}. ' 'Error code {error_code} presented to user' . format ( userid = request . user . id , enterprise_uuid = enterprise_uuid , consent_record = consent_record , error_code = error_code , course_id = course_id , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) defer_creation = request . POST . get ( 'defer_creation' ) consent_provided = bool ( request . POST . get ( 'data_sharing_consent' , False ) ) if defer_creation is None and consent_record . consent_required ( ) : if course_id : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = consent_record . enterprise_customer , user_id = request . user . id ) enterprise_customer_user . update_session ( request ) __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'data-consent-page-enrollment' , request . user . id , course_id , request . path ) consent_record . granted = consent_provided consent_record . save ( ) return redirect ( success_url if consent_provided else failure_url )
3814	def _get_upload_session_status ( res ) : response = json . loads ( res . body . decode ( ) ) if 'sessionStatus' not in response : try : info = ( response [ 'errorMessage' ] [ 'additionalInfo' ] [ 'uploader_service.GoogleRupioAdditionalInfo' ] [ 'completionInfo' ] [ 'customerSpecificInfo' ] ) reason = '{} : {}' . format ( info [ 'status' ] , info [ 'message' ] ) except KeyError : reason = 'unknown reason' raise exceptions . NetworkError ( 'image upload failed: {}' . format ( reason ) ) return response [ 'sessionStatus' ]
7238	def iterwindows ( self , count = 64 , window_shape = ( 256 , 256 ) ) : if count is None : while True : yield self . randwindow ( window_shape ) else : for i in xrange ( count ) : yield self . randwindow ( window_shape )
10014	def create_archive ( directory , filename , config = { } , ignore_predicate = None , ignored_files = [ '.git' , '.svn' ] ) : with zipfile . ZipFile ( filename , 'w' , compression = zipfile . ZIP_DEFLATED ) as zip_file : root_len = len ( os . path . abspath ( directory ) ) # create it out ( "Creating archive: " + str ( filename ) ) for root , dirs , files in os . walk ( directory , followlinks = True ) : archive_root = os . path . abspath ( root ) [ root_len + 1 : ] for f in files : fullpath = os . path . join ( root , f ) archive_name = os . path . join ( archive_root , f ) # ignore the file we're creating if filename in fullpath : continue # ignored files if ignored_files is not None : for name in ignored_files : if fullpath . endswith ( name ) : out ( "Skipping: " + str ( name ) ) continue # do predicate if ignore_predicate is not None : if not ignore_predicate ( archive_name ) : out ( "Skipping: " + str ( archive_name ) ) continue out ( "Adding: " + str ( archive_name ) ) zip_file . write ( fullpath , archive_name , zipfile . ZIP_DEFLATED ) return filename
12793	def get ( self , url = None , parse_data = True , key = None , parameters = None ) : return self . _fetch ( "GET" , url , post_data = None , parse_data = parse_data , key = key , parameters = parameters )
5423	def _wait_after ( provider , job_ids , poll_interval , stop_on_failure ) : # Each time through the loop, the job_set is re-set to the jobs remaining to # check. Jobs are removed from the list when they complete. # # We exit the loop when: # * No jobs remain are running, OR # * stop_on_failure is TRUE AND at least one job returned an error # remove NO_JOB job_ids_to_check = { j for j in job_ids if j != dsub_util . NO_JOB } error_messages = [ ] while job_ids_to_check and ( not error_messages or not stop_on_failure ) : print ( 'Waiting for: %s.' % ( ', ' . join ( job_ids_to_check ) ) ) # Poll until any remaining jobs have completed jobs_left = _wait_for_any_job ( provider , job_ids_to_check , poll_interval ) # Calculate which jobs just completed jobs_completed = job_ids_to_check . difference ( jobs_left ) # Get all tasks for the newly completed jobs tasks_completed = provider . lookup_job_tasks ( { '*' } , job_ids = jobs_completed ) # We don't want to overwhelm the user with output when there are many # tasks per job. So we get a single "dominant" task for each of the # completed jobs (one that is representative of the job's fate). dominant_job_tasks = _dominant_task_for_jobs ( tasks_completed ) if len ( dominant_job_tasks ) != len ( jobs_completed ) : # print info about the jobs we couldn't find # (should only occur for "--after" where the job ID is a typo). jobs_found = dsub_util . tasks_to_job_ids ( dominant_job_tasks ) jobs_not_found = jobs_completed . difference ( jobs_found ) for j in jobs_not_found : error = '%s: not found' % j print_error ( ' %s' % error ) error_messages += [ error ] # Print the dominant task for the completed jobs for t in dominant_job_tasks : job_id = t . get_field ( 'job-id' ) status = t . get_field ( 'task-status' ) print ( ' %s: %s' % ( str ( job_id ) , str ( status ) ) ) if status in [ 'FAILURE' , 'CANCELED' ] : error_messages += [ provider . get_tasks_completion_messages ( [ t ] ) ] job_ids_to_check = jobs_left return error_messages
2139	def associate ( self , group , parent , * * kwargs ) : parent_id = self . lookup_with_inventory ( parent , kwargs . get ( 'inventory' , None ) ) [ 'id' ] group_id = self . lookup_with_inventory ( group , kwargs . get ( 'inventory' , None ) ) [ 'id' ] return self . _assoc ( 'children' , parent_id , group_id )
12494	def check_X_y ( X , y , accept_sparse = None , dtype = None , order = None , copy = False , force_all_finite = True , ensure_2d = True , allow_nd = False , multi_output = False ) : X = check_array ( X , accept_sparse , dtype , order , copy , force_all_finite , ensure_2d , allow_nd ) if multi_output : y = check_array ( y , 'csr' , force_all_finite = True , ensure_2d = False ) else : y = column_or_1d ( y , warn = True ) _assert_all_finite ( y ) check_consistent_length ( X , y ) return X , y
8332	def findNextSiblings ( self , name = None , attrs = { } , text = None , limit = None , * * kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextSiblingGenerator , * * kwargs )
3474	def check_mass_balance ( self ) : reaction_element_dict = defaultdict ( int ) for metabolite , coefficient in iteritems ( self . _metabolites ) : if metabolite . charge is not None : reaction_element_dict [ "charge" ] += coefficient * metabolite . charge if metabolite . elements is None : raise ValueError ( "No elements found in metabolite %s" % metabolite . id ) for element , amount in iteritems ( metabolite . elements ) : reaction_element_dict [ element ] += coefficient * amount # filter out 0 values return { k : v for k , v in iteritems ( reaction_element_dict ) if v != 0 }
1446	def poll ( self ) : try : # non-blocking ret = self . _buffer . get ( block = False ) if self . _producer_callback is not None : self . _producer_callback ( ) return ret except Queue . Empty : Log . debug ( "%s: Empty in poll()" % str ( self ) ) raise Queue . Empty
4739	def err ( txt ) : print ( "%s# %s%s%s" % ( PR_ERR_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
4611	def block_time ( self , block_num ) : return self . block_class ( block_num , blockchain_instance = self . blockchain ) . time ( )
12619	def check_img_compatibility ( one_img , another_img , only_check_3d = False ) : nd_to_check = None if only_check_3d : nd_to_check = 3 if hasattr ( one_img , 'shape' ) and hasattr ( another_img , 'shape' ) : if not have_same_shape ( one_img , another_img , nd_to_check = nd_to_check ) : msg = 'Shape of the first image: \n{}\n is different from second one: \n{}' . format ( one_img . shape , another_img . shape ) raise NiftiFilesNotCompatible ( repr_imgs ( one_img ) , repr_imgs ( another_img ) , message = msg ) if hasattr ( one_img , 'get_affine' ) and hasattr ( another_img , 'get_affine' ) : if not have_same_affine ( one_img , another_img , only_check_3d = only_check_3d ) : msg = 'Affine matrix of the first image: \n{}\n is different ' 'from second one:\n{}' . format ( one_img . get_affine ( ) , another_img . get_affine ( ) ) raise NiftiFilesNotCompatible ( repr_imgs ( one_img ) , repr_imgs ( another_img ) , message = msg )
13266	def xml_to_json ( root ) : j = { } if len ( root ) == 0 : # Tag with no children, return str/int return _maybe_intify ( root . text ) if len ( root ) == 1 and root [ 0 ] . tag . startswith ( '{' + NS_GML ) : # GML return gml_to_geojson ( root [ 0 ] ) if root . tag == 'open511' : j [ 'meta' ] = { 'version' : root . get ( 'version' ) } for elem in root : name = elem . tag if name == 'link' and elem . get ( 'rel' ) : name = elem . get ( 'rel' ) + '_url' if name == 'self_url' : name = 'url' if root . tag == 'open511' : j [ 'meta' ] [ name ] = elem . get ( 'href' ) continue elif name . startswith ( '{' + NS_PROTECTED ) : name = '!' + name [ name . index ( '}' ) + 1 : ] elif name [ 0 ] == '{' : # Namespace! name = '+' + name [ name . index ( '}' ) + 1 : ] if name in j : continue # duplicate elif elem . tag == 'link' and not elem . text : j [ name ] = elem . get ( 'href' ) elif len ( elem ) : if name == 'grouped_events' : # An array of URLs j [ name ] = [ xml_link_to_json ( child , to_dict = False ) for child in elem ] elif name in ( 'attachments' , 'media_files' ) : # An array of JSON objects j [ name ] = [ xml_link_to_json ( child , to_dict = True ) for child in elem ] elif all ( ( name == pluralize ( child . tag ) for child in elem ) ) : # <something><somethings> serializes to a JSON array j [ name ] = [ xml_to_json ( child ) for child in elem ] else : j [ name ] = xml_to_json ( elem ) else : if root . tag == 'open511' and name . endswith ( 's' ) and not elem . text : # Special case: an empty e.g. <events /> container at the root level # should be serialized to [], not null j [ name ] = [ ] else : j [ name ] = _maybe_intify ( elem . text ) return j
5488	def json_rpc_format ( self ) : error = { 'name' : text_type ( self . __class__ . __name__ ) , 'code' : self . code , 'message' : '{0}' . format ( text_type ( self . message ) ) , 'data' : self . data } if current_app . config [ 'DEBUG' ] : import sys , traceback error [ 'stack' ] = traceback . format_exc ( ) error [ 'executable' ] = sys . executable return error
1185	def dispatch ( self , opcode , context ) : if id ( context ) in self . executing_contexts : generator = self . executing_contexts [ id ( context ) ] del self . executing_contexts [ id ( context ) ] has_finished = generator . next ( ) else : method = self . DISPATCH_TABLE . get ( opcode , _OpcodeDispatcher . unknown ) has_finished = method ( self , context ) if hasattr ( has_finished , "next" ) : # avoid using the types module generator = has_finished has_finished = generator . next ( ) if not has_finished : self . executing_contexts [ id ( context ) ] = generator return has_finished
8860	def goto_assignments ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] # encoding = request_data['encoding'] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto_assignments ( ) except jedi . NotFoundError : pass else : ret_val = [ ( d . module_path , d . line - 1 if d . line else None , d . column , d . full_name ) for d in definitions ] return ret_val
2187	def ensure ( self , func , * args , * * kwargs ) : data = self . tryload ( ) if data is None : data = func ( * args , * * kwargs ) self . save ( data ) return data
7987	def request_software_version ( stanza_processor , target_jid , callback , error_callback = None ) : stanza = Iq ( to_jid = target_jid , stanza_type = "get" ) payload = VersionPayload ( ) stanza . set_payload ( payload ) def wrapper ( stanza ) : """Wrapper for the user-provided `callback` that extracts the payload from stanza received.""" payload = stanza . get_payload ( VersionPayload ) if payload is None : if error_callback : error_callback ( stanza ) else : logger . warning ( "Invalid version query response." ) else : callback ( payload ) stanza_processor . set_response_handlers ( stanza , wrapper , error_callback ) stanza_processor . send ( stanza )
2503	def handle_lics ( self , lics ) : # Handle extracted licensing info type. if ( lics , RDF . type , self . spdx_namespace [ 'ExtractedLicensingInfo' ] ) in self . graph : return self . parse_only_extr_license ( lics ) # Assume resource, hence the path separator ident_start = lics . rfind ( '/' ) + 1 if ident_start == 0 : # special values such as spdx:noassertion special = self . to_special_value ( lics ) if special == lics : if self . LICS_REF_REGEX . match ( lics ) : # Is a license ref i.e LicenseRef-1 return document . License . from_identifier ( lics ) else : # Not a known license form raise SPDXValueError ( 'License' ) else : # is a special value return special else : # license url return document . License . from_identifier ( lics [ ident_start : ] )
12014	def calc_centroids ( self ) : self . cm = np . zeros ( ( len ( self . postcard ) , 2 ) ) for i in range ( len ( self . postcard ) ) : target = self . postcard [ i ] target [ self . targets != 1 ] = 0.0 self . cm [ i ] = center_of_mass ( target )
4212	def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] cli = CommandLineTool ( ) return cli . run ( argv )
5919	def center_fit ( self , * * kwargs ) : kwargs . setdefault ( 's' , self . tpr ) kwargs . setdefault ( 'n' , self . ndx ) kwargs [ 'f' ] = self . xtc kwargs . setdefault ( 'o' , self . outfile ( self . infix_filename ( None , self . xtc , '_centfit' , 'xtc' ) ) ) force = kwargs . pop ( 'force' , self . force ) logger . info ( "Centering and fitting trajectory {f!r}..." . format ( * * kwargs ) ) with utilities . in_dir ( self . dirname ) : if not self . check_file_exists ( kwargs [ 'o' ] , resolve = "indicate" , force = force ) : trj_fitandcenter ( * * kwargs ) logger . info ( "Centered and fit trajectory: {o!r}." . format ( * * kwargs ) ) return { 'tpr' : self . rp ( kwargs [ 's' ] ) , 'xtc' : self . rp ( kwargs [ 'o' ] ) }
9015	def _row ( self , values ) : row_id = self . _to_id ( values [ ID ] ) row = self . _spec . new_row ( row_id , values , self ) if SAME_AS in values : self . _delay_inheritance ( row , self . _to_id ( values [ SAME_AS ] ) ) self . _delay_instructions ( row ) self . _id_cache [ row_id ] = row return row
12668	def matrix_to_4dvolume ( arr , mask , order = 'C' ) : if mask . dtype != np . bool : raise ValueError ( "mask must be a boolean array" ) if arr . ndim != 2 : raise ValueError ( "X must be a 2-dimensional array" ) if mask . sum ( ) != arr . shape [ 0 ] : # raise an error if the shape of arr is not what expected raise ValueError ( 'Expected arr of shape ({}, samples). Got {}.' . format ( mask . sum ( ) , arr . shape ) ) data = np . zeros ( mask . shape + ( arr . shape [ 1 ] , ) , dtype = arr . dtype , order = order ) data [ mask , : ] = arr return data
6894	def parallel_starfeatures_lcdir ( lcdir , outdir , lc_catalog_pickle , neighbor_radius_arcsec , fileglob = None , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None , nworkers = NCPUS , recursive = True ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not fileglob : fileglob = dfileglob # now find the files LOGINFO ( 'searching for %s light curves in %s ...' % ( lcformat , lcdir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcdir , fileglob ) ) else : # use recursive glob for Python 3.5+ if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcdir , '**' , fileglob ) , recursive = True ) # otherwise, use os.walk and glob else : # use os.walk to go through the directories walker = os . walk ( lcdir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) # now that we have all the files, process them if matching and len ( matching ) > 0 : LOGINFO ( 'found %s light curves, getting starfeatures...' % len ( matching ) ) return parallel_starfeatures ( matching , outdir , lc_catalog_pickle , neighbor_radius_arcsec , deredden = deredden , custom_bandpasses = custom_bandpasses , maxobjects = maxobjects , lcformat = lcformat , lcformatdir = lcformatdir , nworkers = nworkers ) else : LOGERROR ( 'no light curve files in %s format found in %s' % ( lcformat , lcdir ) ) return None
2130	def list ( self , * * kwargs ) : data , self . endpoint = self . data_endpoint ( kwargs ) r = super ( Resource , self ) . list ( * * data ) # Change display settings and data format for human consumption self . configure_display ( r ) return r
8002	def __from_xml ( self , xmlnode ) : self . __logger . debug ( "Converting jabber:iq:register element from XML" ) if xmlnode . type != "element" : raise ValueError ( "XML node is not a jabber:iq:register element (not an element)" ) ns = get_node_ns_uri ( xmlnode ) if ns and ns != REGISTER_NS or xmlnode . name != "query" : raise ValueError ( "XML node is not a jabber:iq:register element" ) for element in xml_element_iter ( xmlnode . children ) : ns = get_node_ns_uri ( element ) if ns == DATAFORM_NS and element . name == "x" and not self . form : self . form = Form ( element ) elif ns != REGISTER_NS : continue name = element . name if name == "instructions" and not self . instructions : self . instructions = from_utf8 ( element . getContent ( ) ) elif name == "registered" : self . registered = True elif name == "remove" : self . remove = True elif name in legacy_fields and not getattr ( self , name ) : value = from_utf8 ( element . getContent ( ) ) if value is None : value = u"" self . __logger . debug ( u"Setting legacy field %r to %r" % ( name , value ) ) setattr ( self , name , value )
12405	def reverse ( self ) : if self . _original_target_content : with open ( self . target , 'w' ) as fp : fp . write ( self . _original_target_content )
8624	def add_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_post_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotAddedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
7289	def has_digit ( string_or_list , sep = "_" ) : if isinstance ( string_or_list , ( tuple , list ) ) : list_length = len ( string_or_list ) if list_length : return six . text_type ( string_or_list [ - 1 ] ) . isdigit ( ) else : return False else : return has_digit ( string_or_list . split ( sep ) )
4012	def configure_nfs_server ( ) : repos_for_export = get_all_repos ( active_only = True , include_specs_repo = False ) current_exports = _get_current_exports ( ) needed_exports = _get_exports_for_repos ( repos_for_export ) _ensure_managed_repos_dir_exists ( ) if not needed_exports . difference ( current_exports ) : if not _server_is_running ( ) : _restart_server ( ) return _write_exports_config ( needed_exports ) _restart_server ( )
5552	def clip_bounds ( bounds = None , clip = None ) : bounds = Bounds ( * bounds ) clip = Bounds ( * clip ) return Bounds ( max ( bounds . left , clip . left ) , max ( bounds . bottom , clip . bottom ) , min ( bounds . right , clip . right ) , min ( bounds . top , clip . top ) )
13015	def match ( self , uri ) : absolute_uri = self . __absolute__ ( uri ) return absolute_uri . startswith ( self . __path__ ) and op . exists ( absolute_uri )
5378	def _build_pipeline_docker_command ( cls , script_name , inputs , outputs , envs ) : # We upload the user script as an environment argument # and write it to SCRIPT_DIR (preserving its local file name). # # The docker_command: # * writes the script body to a file # * installs gcloud if there are recursive copies to do # * sets environment variables for inputs with wildcards # * sets environment variables for recursive input directories # * recursively copies input directories # * creates output directories # * sets environment variables for recursive output directories # * sets the DATA_ROOT environment variable to /mnt/data # * sets the working directory to ${DATA_ROOT} # * executes the user script # * recursively copies output directories recursive_input_dirs = [ var for var in inputs if var . recursive and var . value ] recursive_output_dirs = [ var for var in outputs if var . recursive and var . value ] install_cloud_sdk = '' if recursive_input_dirs or recursive_output_dirs : install_cloud_sdk = INSTALL_CLOUD_SDK export_input_dirs = '' copy_input_dirs = '' if recursive_input_dirs : export_input_dirs = providers_util . build_recursive_localize_env ( providers_util . DATA_MOUNT_POINT , inputs ) copy_input_dirs = providers_util . build_recursive_localize_command ( providers_util . DATA_MOUNT_POINT , inputs , job_model . P_GCS ) export_output_dirs = '' copy_output_dirs = '' if recursive_output_dirs : export_output_dirs = providers_util . build_recursive_gcs_delocalize_env ( providers_util . DATA_MOUNT_POINT , outputs ) copy_output_dirs = providers_util . build_recursive_delocalize_command ( providers_util . DATA_MOUNT_POINT , outputs , job_model . P_GCS ) docker_paths = [ var . docker_path if var . recursive else os . path . dirname ( var . docker_path ) for var in outputs if var . value ] mkdirs = '\n' . join ( [ 'mkdir -p {0}/{1}' . format ( providers_util . DATA_MOUNT_POINT , path ) for path in docker_paths ] ) inputs_with_wildcards = [ var for var in inputs if not var . recursive and var . docker_path and '*' in os . path . basename ( var . docker_path ) ] export_inputs_with_wildcards = '\n' . join ( [ 'export {0}="{1}/{2}"' . format ( var . name , providers_util . DATA_MOUNT_POINT , var . docker_path ) for var in inputs_with_wildcards ] ) export_empty_envs = '\n' . join ( [ 'export {0}=""' . format ( var . name ) for var in envs | inputs | outputs if not var . value ] ) return DOCKER_COMMAND . format ( mk_runtime_dirs = MK_RUNTIME_DIRS_COMMAND , script_path = '%s/%s' % ( providers_util . SCRIPT_DIR , script_name ) , install_cloud_sdk = install_cloud_sdk , export_inputs_with_wildcards = export_inputs_with_wildcards , export_input_dirs = export_input_dirs , copy_input_dirs = copy_input_dirs , mk_output_dirs = mkdirs , export_output_dirs = export_output_dirs , export_empty_envs = export_empty_envs , tmpdir = providers_util . TMP_DIR , working_dir = providers_util . WORKING_DIR , copy_output_dirs = copy_output_dirs )
453	def get_variables_with_name ( name = None , train_only = True , verbose = False ) : if name is None : raise Exception ( "please input a name" ) logging . info ( " [*] geting variables with %s" % name ) # tvar = tf.trainable_variables() if train_only else tf.all_variables() if train_only : t_vars = tf . trainable_variables ( ) else : t_vars = tf . global_variables ( ) d_vars = [ var for var in t_vars if name in var . name ] if verbose : for idx , v in enumerate ( d_vars ) : logging . info ( " got {:3}: {:15} {}" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) return d_vars
8740	def _create_flip ( context , flip , port_fixed_ips ) : if port_fixed_ips : context . session . begin ( ) try : ports = [ val [ 'port' ] for val in port_fixed_ips . values ( ) ] flip = db_api . port_associate_ip ( context , ports , flip , port_fixed_ips . keys ( ) ) for port_id in port_fixed_ips : fixed_ip = port_fixed_ips [ port_id ] [ 'fixed_ip' ] flip = db_api . floating_ip_associate_fixed_ip ( context , flip , fixed_ip ) flip_driver = registry . DRIVER_REGISTRY . get_driver ( ) flip_driver . register_floating_ip ( flip , port_fixed_ips ) context . session . commit ( ) except Exception : context . session . rollback ( ) raise # alexm: Notify from this method for consistency with _delete_flip billing . notify ( context , billing . IP_ASSOC , flip )
12802	def get_room_by_name ( self , name ) : rooms = self . get_rooms ( ) for room in rooms or [ ] : if room [ "name" ] == name : return self . get_room ( room [ "id" ] ) raise RoomNotFoundException ( "Room %s not found" % name )
13814	def _MessageToJsonObject ( message , including_default_value_fields ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : return _WrapperMessageToJsonObject ( message ) if full_name in _WKTJSONMETHODS : return _WKTJSONMETHODS [ full_name ] [ 0 ] ( message , including_default_value_fields ) js = { } return _RegularMessageToJsonObject ( message , js , including_default_value_fields )
12281	def run ( self , cmd , * args ) : if self . manager is None : raise Exception ( "Fatal internal error: Missing repository manager" ) if cmd not in dir ( self . manager ) : raise Exception ( "Fatal internal error: Invalid command {} being run" . format ( cmd ) ) func = getattr ( self . manager , cmd ) repo = self return func ( repo , * args )
2982	def cmd_add ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . add_container ( opts . containers )
7579	def result_files ( self ) : reps = OPJ ( self . workdir , self . name + "-K-*-rep-*_f" ) repfiles = glob . glob ( reps ) return repfiles
1948	def write_back_memory ( self , where , expr , size ) : if self . write_backs_disabled : return if type ( expr ) is bytes : self . _emu . mem_write ( where , expr ) else : if issymbolic ( expr ) : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] concrete_data = [ ] for c in data : if issymbolic ( c ) : c = chr ( solver . get_value ( self . _cpu . memory . constraints , c ) ) concrete_data . append ( c ) data = concrete_data else : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] logger . debug ( f"Writing back {hr_size(size // 8)} to {hex(where)}: {data}" ) # TODO - the extra encoding is to handle null bytes output as strings when we concretize. That's probably a bug. self . _emu . mem_write ( where , b'' . join ( b . encode ( 'utf-8' ) if type ( b ) is str else b for b in data ) )
8847	def mouseMoveEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mouseMoveEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) assert isinstance ( cursor , QtGui . QTextCursor ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if QtWidgets . QApplication . overrideCursor ( ) is None : QtWidgets . QApplication . setOverrideCursor ( QtGui . QCursor ( QtCore . Qt . PointingHandCursor ) ) else : if QtWidgets . QApplication . overrideCursor ( ) is not None : QtWidgets . QApplication . restoreOverrideCursor ( )
2816	def convert_maxpool3 ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width , depth = params [ 'kernel_shape' ] else : height , width , depth = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width , stride_depth = params [ 'strides' ] else : stride_height , stride_width , stride_depth = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , padding_d , _ , _ = params [ 'pads' ] else : padding_h , padding_w , padding_d = params [ 'padding' ] input_name = inputs [ 0 ] if padding_h > 0 and padding_w > 0 and padding_d > 0 : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding3D ( padding = ( padding_h , padding_w , padding_d ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name # Pooling type pooling = keras . layers . MaxPooling3D ( pool_size = ( height , width , depth ) , strides = ( stride_height , stride_width , stride_depth ) , padding = 'valid' , name = tf_name ) layers [ scope_name ] = pooling ( layers [ input_name ] )
3208	def _reformat_policy ( policy ) : policy_name = policy [ 'PolicyName' ] ret = { } ret [ 'type' ] = policy [ 'PolicyTypeName' ] attrs = policy [ 'PolicyAttributeDescriptions' ] if ret [ 'type' ] != 'SSLNegotiationPolicyType' : return policy_name , ret attributes = dict ( ) for attr in attrs : attributes [ attr [ 'AttributeName' ] ] = attr [ 'AttributeValue' ] ret [ 'protocols' ] = dict ( ) ret [ 'protocols' ] [ 'sslv2' ] = bool ( attributes . get ( 'Protocol-SSLv2' ) ) ret [ 'protocols' ] [ 'sslv3' ] = bool ( attributes . get ( 'Protocol-SSLv3' ) ) ret [ 'protocols' ] [ 'tlsv1' ] = bool ( attributes . get ( 'Protocol-TLSv1' ) ) ret [ 'protocols' ] [ 'tlsv1_1' ] = bool ( attributes . get ( 'Protocol-TLSv1.1' ) ) ret [ 'protocols' ] [ 'tlsv1_2' ] = bool ( attributes . get ( 'Protocol-TLSv1.2' ) ) ret [ 'server_defined_cipher_order' ] = bool ( attributes . get ( 'Server-Defined-Cipher-Order' ) ) ret [ 'reference_security_policy' ] = attributes . get ( 'Reference-Security-Policy' , None ) non_ciphers = [ 'Server-Defined-Cipher-Order' , 'Protocol-SSLv2' , 'Protocol-SSLv3' , 'Protocol-TLSv1' , 'Protocol-TLSv1.1' , 'Protocol-TLSv1.2' , 'Reference-Security-Policy' ] ciphers = [ ] for cipher in attributes : if attributes [ cipher ] == 'true' and cipher not in non_ciphers : ciphers . append ( cipher ) ciphers . sort ( ) ret [ 'supported_ciphers' ] = ciphers return policy_name , ret
8580	def create_server ( self , datacenter_id , server ) : data = json . dumps ( self . _create_server_dict ( server ) ) response = self . _perform_request ( url = '/datacenters/%s/servers' % ( datacenter_id ) , method = 'POST' , data = data ) return response
11155	def print_big_dir ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size_table [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size ) , p . abspath ) )
8368	def _parse ( self ) : p1 = "\[.*?\](.*?)\[\/.*?\]" p2 = "\[(.*?)\]" self . links = [ ] for p in ( p1 , p2 ) : for link in re . findall ( p , self . description ) : self . links . append ( link ) self . description = re . sub ( p , "\\1" , self . description ) self . description = self . description . strip ( )
8790	def pop ( self , model ) : tags = self . _pop ( model ) if tags : for tag in tags : value = self . deserialize ( tag ) try : self . validate ( value ) return value except TagValidationError : continue
13570	def false_exit ( func ) : @ wraps ( func ) def inner ( * args , * * kwargs ) : ret = func ( * args , * * kwargs ) if ret is False : if "TMC_TESTING" in os . environ : raise TMCExit ( ) else : sys . exit ( - 1 ) return ret return inner
5896	def render_toolbar ( context , config ) : quill_config = getattr ( quill_app , config ) t = template . loader . get_template ( quill_config [ 'toolbar_template' ] ) return t . render ( context )
11062	def send_im ( self , user , text ) : if isinstance ( user , SlackUser ) : user = user . id channelid = self . _find_im_channel ( user ) else : channelid = user . id self . send_message ( channelid , text )
10574	def get_local_playlists ( filepaths , exclude_patterns = None , max_depth = float ( 'inf' ) ) : logger . info ( "Loading local playlists..." ) included_playlists = [ ] excluded_playlists = [ ] supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_PLAYLIST_FORMATS , max_depth = max_depth ) included_playlists , excluded_playlists = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) logger . info ( "Excluded {0} local playlists" . format ( len ( excluded_playlists ) ) ) logger . info ( "Loaded {0} local playlists" . format ( len ( included_playlists ) ) ) return included_playlists , excluded_playlists
5216	def fut_ticker ( gen_ticker : str , dt , freq : str , log = logs . LOG_LEVEL ) -> str : logger = logs . get_logger ( fut_ticker , level = log ) dt = pd . Timestamp ( dt ) t_info = gen_ticker . split ( ) asset = t_info [ - 1 ] if asset in [ 'Index' , 'Curncy' , 'Comdty' ] : ticker = ' ' . join ( t_info [ : - 1 ] ) prefix , idx , postfix = ticker [ : - 1 ] , int ( ticker [ - 1 ] ) - 1 , asset elif asset == 'Equity' : ticker = t_info [ 0 ] prefix , idx , postfix = ticker [ : - 1 ] , int ( ticker [ - 1 ] ) - 1 , ' ' . join ( t_info [ 1 : ] ) else : logger . error ( f'unkonwn asset type for ticker: {gen_ticker}' ) return '' month_ext = 4 if asset == 'Comdty' else 2 months = pd . date_range ( start = dt , periods = max ( idx + month_ext , 3 ) , freq = freq ) logger . debug ( f'pulling expiry dates for months: {months}' ) def to_fut ( month ) : return prefix + const . Futures [ month . strftime ( '%b' ) ] + month . strftime ( '%y' ) [ - 1 ] + ' ' + postfix fut = [ to_fut ( m ) for m in months ] logger . debug ( f'trying futures: {fut}' ) # noinspection PyBroadException try : fut_matu = bdp ( tickers = fut , flds = 'last_tradeable_dt' , cache = True ) except Exception as e1 : logger . error ( f'error downloading futures contracts (1st trial) {e1}:\n{fut}' ) # noinspection PyBroadException try : fut = fut [ : - 1 ] logger . debug ( f'trying futures (2nd trial): {fut}' ) fut_matu = bdp ( tickers = fut , flds = 'last_tradeable_dt' , cache = True ) except Exception as e2 : logger . error ( f'error downloading futures contracts (2nd trial) {e2}:\n{fut}' ) return '' sub_fut = fut_matu [ pd . DatetimeIndex ( fut_matu . last_tradeable_dt ) > dt ] logger . debug ( f'futures full chain:\n{fut_matu.to_string()}' ) logger . debug ( f'getting index {idx} from:\n{sub_fut.to_string()}' ) return sub_fut . index . values [ idx ]
6275	def resolve_loader ( self , meta : ResourceDescription ) : meta . loader_cls = self . get_loader ( meta , raise_on_error = True )
8083	def transform ( self , mode = None ) : if mode : self . _canvas . mode = mode return self . _canvas . mode
8079	def star ( self , startx , starty , points = 20 , outer = 100 , inner = 50 , draw = True , * * kwargs ) : # Taken from Nodebox. self . beginpath ( * * kwargs ) self . moveto ( startx , starty + outer ) for i in range ( 1 , int ( 2 * points ) ) : angle = i * pi / points x = sin ( angle ) y = cos ( angle ) if i % 2 : radius = inner else : radius = outer x = startx + radius * x y = starty + radius * y self . lineto ( x , y ) return self . endpath ( draw )
9998	def cellsiter_to_dataframe ( cellsiter , args , drop_allna = True ) : from modelx . core . cells import shareable_parameters if len ( args ) : indexes = shareable_parameters ( cellsiter ) else : indexes = get_all_params ( cellsiter . values ( ) ) result = None for cells in cellsiter . values ( ) : df = cells_to_dataframe ( cells , args ) if drop_allna and df . isnull ( ) . all ( ) . all ( ) : continue # Ignore all NA or empty if df . index . names != [ None ] : if isinstance ( df . index , pd . MultiIndex ) : if _pd_ver < ( 0 , 20 ) : df = _reset_naindex ( df ) df = df . reset_index ( ) missing_params = set ( indexes ) - set ( df ) for params in missing_params : df [ params ] = np . nan if result is None : result = df else : try : result = pd . merge ( result , df , how = "outer" ) except MergeError : # When no common column exists, i.e. all cells are scalars. result = pd . concat ( [ result , df ] , axis = 1 ) except ValueError : # When common columns are not coercible (numeric vs object), # Make the numeric column object type cols = set ( result . columns ) & set ( df . columns ) for col in cols : # When only either of them has object dtype if ( len ( [ str ( frame [ col ] . dtype ) for frame in ( result , df ) if str ( frame [ col ] . dtype ) == "object" ] ) == 1 ) : if str ( result [ col ] . dtype ) == "object" : frame = df else : frame = result frame [ [ col ] ] = frame [ col ] . astype ( "object" ) # Try again result = pd . merge ( result , df , how = "outer" ) if result is None : return pd . DataFrame ( ) else : return result . set_index ( indexes ) if indexes else result
6017	def absolute_signal_to_noise_map ( self ) : return np . divide ( np . abs ( self . image ) , self . noise_map )
13399	def resourcePath ( self , relative_path ) : from os import path import sys try : # PyInstaller creates a temp folder and stores path in _MEIPASS base_path = sys . _MEIPASS except Exception : base_path = path . dirname ( path . abspath ( __file__ ) ) return path . join ( base_path , relative_path )
4936	def strfdelta ( tdelta , fmt = '{D:02}d {H:02}h {M:02}m {S:02}s' , input_type = 'timedelta' ) : # Convert tdelta to integer seconds. if input_type == 'timedelta' : remainder = int ( tdelta . total_seconds ( ) ) elif input_type in [ 's' , 'seconds' ] : remainder = int ( tdelta ) elif input_type in [ 'm' , 'minutes' ] : remainder = int ( tdelta ) * 60 elif input_type in [ 'h' , 'hours' ] : remainder = int ( tdelta ) * 3600 elif input_type in [ 'd' , 'days' ] : remainder = int ( tdelta ) * 86400 elif input_type in [ 'w' , 'weeks' ] : remainder = int ( tdelta ) * 604800 else : raise ValueError ( 'input_type is not valid. Valid input_type strings are: "timedelta", "s", "m", "h", "d", "w"' ) f = Formatter ( ) desired_fields = [ field_tuple [ 1 ] for field_tuple in f . parse ( fmt ) ] possible_fields = ( 'W' , 'D' , 'H' , 'M' , 'S' ) constants = { 'W' : 604800 , 'D' : 86400 , 'H' : 3600 , 'M' : 60 , 'S' : 1 } values = { } for field in possible_fields : if field in desired_fields and field in constants : values [ field ] , remainder = divmod ( remainder , constants [ field ] ) return f . format ( fmt , * * values )
3702	def Tm_depression_eutectic ( Tm , Hm , x = None , M = None , MW = None ) : if x : dTm = R * Tm ** 2 * x / Hm elif M and MW : MW = MW / 1000. #g/mol to kg/mol dTm = R * Tm ** 2 * MW * M / Hm else : raise Exception ( 'Either molality or mole fraction of the solute must be specified; MW of the solvent is required also if molality is provided' ) return dTm
625	def _neighbors ( coordinate , radius ) : ranges = ( xrange ( n - radius , n + radius + 1 ) for n in coordinate . tolist ( ) ) return numpy . array ( list ( itertools . product ( * ranges ) ) )
4687	def decrypt ( self , message ) : if not message : return None # We first try to decode assuming we received the memo try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( message [ "to" ] ) pubkey = message [ "from" ] except KeyNotFound : try : # if that failed, we assume that we have sent the memo memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( message [ "from" ] ) pubkey = message [ "to" ] except KeyNotFound : # if all fails, raise exception raise MissingKeyError ( "None of the required memo keys are installed!" "Need any of {}" . format ( [ message [ "to" ] , message [ "from" ] ] ) ) if not hasattr ( self , "chain_prefix" ) : self . chain_prefix = self . blockchain . prefix return memo . decode_memo ( self . privatekey_class ( memo_wif ) , self . publickey_class ( pubkey , prefix = self . chain_prefix ) , message . get ( "nonce" ) , message . get ( "message" ) , )
9146	def sheet ( connection , skip , file : TextIO ) : from tabulate import tabulate header = [ '' , 'Name' , 'Description' , 'Terms' , 'Relations' ] rows = [ ] for i , ( idx , name , manager ) in enumerate ( _iterate_managers ( connection , skip ) , start = 1 ) : try : if not manager . is_populated ( ) : continue except AttributeError : click . secho ( f'{name} does not implement is_populated' , fg = 'red' ) continue terms , relations = None , None if isinstance ( manager , BELNamespaceManagerMixin ) : terms = manager . _count_model ( manager . namespace_model ) if isinstance ( manager , BELManagerMixin ) : try : relations = manager . count_relations ( ) except TypeError as e : relations = str ( e ) rows . append ( ( i , name , manager . __doc__ . split ( '\n' ) [ 0 ] . strip ( ) . strip ( '.' ) , terms , relations ) ) print ( tabulate ( rows , headers = header , # tablefmt="fancy_grid", ) )
392	def keypoint_random_resize ( image , annos , mask = None , zoom_range = ( 0.8 , 1.2 ) ) : height = image . shape [ 0 ] width = image . shape [ 1 ] _min , _max = zoom_range scalew = np . random . uniform ( _min , _max ) scaleh = np . random . uniform ( _min , _max ) neww = int ( width * scalew ) newh = int ( height * scaleh ) dst = cv2 . resize ( image , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) if mask is not None : mask = cv2 . resize ( mask , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) # adjust meta data adjust_joint_list = [ ] for joint in annos : # TODO : speed up with affine transform adjust_joint = [ ] for point in joint : if point [ 0 ] < - 100 or point [ 1 ] < - 100 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue adjust_joint . append ( ( int ( point [ 0 ] * scalew + 0.5 ) , int ( point [ 1 ] * scaleh + 0.5 ) ) ) adjust_joint_list . append ( adjust_joint ) if mask is not None : return dst , adjust_joint_list , mask else : return dst , adjust_joint_list , None
9775	def logs ( ctx , past , follow , hide_time ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if past : try : response = PolyaxonClient ( ) . job . logs ( user , project_name , _job , stream = False ) get_logs_handler ( handle_job_info = False , show_timestamp = not hide_time , stream = False ) ( response . content . decode ( ) . split ( '\n' ) ) print ( ) if not follow : return except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : if not follow : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) try : PolyaxonClient ( ) . job . logs ( user , project_name , _job , message_handler = get_logs_handler ( handle_job_info = False , show_timestamp = not hide_time ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
7120	def _convert_item ( self , obj ) : if isinstance ( obj , dict ) and not isinstance ( obj , DotDict ) : obj = DotDict ( obj ) elif isinstance ( obj , list ) : # must mutate and not just reassign, otherwise it will # just use original object mutable/immutable for i , item in enumerate ( obj ) : if isinstance ( item , dict ) and not isinstance ( item , DotDict ) : obj [ i ] = DotDict ( item ) return obj
2547	def add_annotation_type ( self , doc , annotation_type ) : if len ( doc . annotations ) != 0 : if not self . annotation_type_set : if annotation_type . endswith ( 'annotationType_other' ) : self . annotation_type_set = True doc . annotations [ - 1 ] . annotation_type = 'OTHER' return True elif annotation_type . endswith ( 'annotationType_review' ) : self . annotation_type_set = True doc . annotations [ - 1 ] . annotation_type = 'REVIEW' return True else : raise SPDXValueError ( 'Annotation::AnnotationType' ) else : raise CardinalityError ( 'Annotation::AnnotationType' ) else : raise OrderError ( 'Annotation::AnnotationType' )
3099	def _validate_clientsecrets ( clientsecrets_dict ) : _INVALID_FILE_FORMAT_MSG = ( 'Invalid file format. See ' 'https://developers.google.com/api-client-library/' 'python/guide/aaa_client_secrets' ) if clientsecrets_dict is None : raise InvalidClientSecretsError ( _INVALID_FILE_FORMAT_MSG ) try : ( client_type , client_info ) , = clientsecrets_dict . items ( ) except ( ValueError , AttributeError ) : raise InvalidClientSecretsError ( _INVALID_FILE_FORMAT_MSG + ' ' 'Expected a JSON object with a single property for a "web" or ' '"installed" application' ) if client_type not in VALID_CLIENT : raise InvalidClientSecretsError ( 'Unknown client type: {0}.' . format ( client_type ) ) for prop_name in VALID_CLIENT [ client_type ] [ 'required' ] : if prop_name not in client_info : raise InvalidClientSecretsError ( 'Missing property "{0}" in a client type of "{1}".' . format ( prop_name , client_type ) ) for prop_name in VALID_CLIENT [ client_type ] [ 'string' ] : if client_info [ prop_name ] . startswith ( '[[' ) : raise InvalidClientSecretsError ( 'Property "{0}" is not configured.' . format ( prop_name ) ) return client_type , client_info
2637	def parent_callback ( self , parent_fu ) : if parent_fu . done ( ) is True : e = parent_fu . _exception if e : super ( ) . set_exception ( e ) else : super ( ) . set_result ( self . file_obj ) return
3886	def _add_user_from_conv_part ( self , conv_part ) : user_ = User . from_conv_part_data ( conv_part , self . _self_user . id_ ) existing = self . _user_dict . get ( user_ . id_ ) if existing is None : logger . warning ( 'Adding fallback User with %s name "%s"' , user_ . name_type . name . lower ( ) , user_ . full_name ) self . _user_dict [ user_ . id_ ] = user_ return user_ else : existing . upgrade_name ( user_ ) return existing
9420	def is_rarfile ( filename ) : mode = constants . RAR_OM_LIST_INCSPLIT archive = unrarlib . RAROpenArchiveDataEx ( filename , mode = mode ) try : handle = unrarlib . RAROpenArchiveEx ( ctypes . byref ( archive ) ) except unrarlib . UnrarException : return False unrarlib . RARCloseArchive ( handle ) return ( archive . OpenResult == constants . SUCCESS )
2224	def _convert_to_hashable ( data , types = True ) : # HANDLE MOST COMMON TYPES FIRST if data is None : hashable = b'NONE' prefix = b'NULL' elif isinstance ( data , six . binary_type ) : hashable = data prefix = b'TXT' elif isinstance ( data , six . text_type ) : # convert unicode into bytes hashable = data . encode ( 'utf-8' ) prefix = b'TXT' elif isinstance ( data , _intlike ) : # warnings.warn('Hashing ints is slow, numpy is prefered') hashable = _int_to_bytes ( data ) # hashable = data.to_bytes(8, byteorder='big') prefix = b'INT' elif isinstance ( data , float ) : a , b = float ( data ) . as_integer_ratio ( ) hashable = _int_to_bytes ( a ) + b'/' + _int_to_bytes ( b ) prefix = b'FLT' else : # Then dynamically look up any other type hash_func = _HASHABLE_EXTENSIONS . lookup ( data ) prefix , hashable = hash_func ( data ) if types : return prefix , hashable else : return b'' , hashable
11458	def keep_only_fields ( self ) : for tag in self . record . keys ( ) : if tag not in self . fields_list : record_delete_fields ( self . record , tag )
11441	def _correct_record ( record ) : errors = [ ] for tag in record . keys ( ) : upper_bound = '999' n = len ( tag ) if n > 3 : i = n - 3 while i > 0 : upper_bound = '%s%s' % ( '0' , upper_bound ) i -= 1 # Missing tag. Replace it with dummy tag '000'. if tag == '!' : errors . append ( ( 1 , '(field number(s): ' + str ( [ f [ 4 ] for f in record [ tag ] ] ) + ')' ) ) record [ '000' ] = record . pop ( tag ) tag = '000' elif not ( '001' <= tag <= upper_bound or tag in ( 'FMT' , 'FFT' , 'BDR' , 'BDM' ) ) : errors . append ( 2 ) record [ '000' ] = record . pop ( tag ) tag = '000' fields = [ ] for field in record [ tag ] : # Datafield without any subfield. if field [ 0 ] == [ ] and field [ 3 ] == '' : errors . append ( ( 8 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) subfields = [ ] for subfield in field [ 0 ] : if subfield [ 0 ] == '!' : errors . append ( ( 3 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) newsub = ( '' , subfield [ 1 ] ) else : newsub = subfield subfields . append ( newsub ) if field [ 1 ] == '!' : errors . append ( ( 4 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) ind1 = " " else : ind1 = field [ 1 ] if field [ 2 ] == '!' : errors . append ( ( 5 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) ind2 = " " else : ind2 = field [ 2 ] fields . append ( ( subfields , ind1 , ind2 , field [ 3 ] , field [ 4 ] ) ) record [ tag ] = fields return errors
8917	def _get_param ( self , param , allowed_values = None , optional = False ) : request_params = self . _request_params ( ) if param in request_params : value = request_params [ param ] . lower ( ) if allowed_values is not None : if value in allowed_values : self . params [ param ] = value else : raise OWSInvalidParameterValue ( "%s %s is not supported" % ( param , value ) , value = param ) elif optional : self . params [ param ] = None else : raise OWSMissingParameterValue ( 'Parameter "%s" is missing' % param , value = param ) return self . params [ param ]
4948	def export ( self ) : content_metadata_export = { } content_metadata_items = self . enterprise_api . get_content_metadata ( self . enterprise_customer ) LOGGER . info ( 'Retrieved content metadata for enterprise [%s]' , self . enterprise_customer . name ) for item in content_metadata_items : transformed = self . _transform_item ( item ) LOGGER . info ( 'Exporting content metadata item with plugin configuration [%s]: [%s]' , self . enterprise_configuration , json . dumps ( transformed , indent = 4 ) , ) content_metadata_item_export = ContentMetadataItemExport ( item , transformed ) content_metadata_export [ content_metadata_item_export . content_id ] = content_metadata_item_export return OrderedDict ( sorted ( content_metadata_export . items ( ) ) )
13431	def admin_link_move_up ( obj , link_text = 'up' ) : if obj . rank == 1 : return '' content_type = ContentType . objects . get_for_model ( obj ) link = reverse ( 'awl-rankedmodel-move' , args = ( content_type . id , obj . id , obj . rank - 1 ) ) return '<a href="%s">%s</a>' % ( link , link_text )
6772	def install_required ( self , type = None , service = None , list_only = 0 , * * kwargs ) : # pylint: disable=redefined-builtin r = self . local_renderer list_only = int ( list_only ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE_TYPES , 'Unknown package type: %s' % ( type , ) lst = [ ] if type : types = [ type ] else : types = PACKAGE_TYPES for _type in types : if _type == SYSTEM : content = '\n' . join ( self . list_required ( type = _type , service = service ) ) if list_only : lst . extend ( _ for _ in content . split ( '\n' ) if _ . strip ( ) ) if self . verbose : print ( 'content:' , content ) break fd , fn = tempfile . mkstemp ( ) fout = open ( fn , 'w' ) fout . write ( content ) fout . close ( ) self . install_custom ( fn = fn ) else : raise NotImplementedError return lst
12349	def create ( self , name , region , size , image , ssh_keys = None , backups = None , ipv6 = None , private_networking = None , wait = True ) : if ssh_keys and not isinstance ( ssh_keys , ( list , tuple ) ) : raise TypeError ( "ssh_keys must be a list" ) resp = self . post ( name = name , region = region , size = size , image = image , ssh_keys = ssh_keys , private_networking = private_networking , backups = backups , ipv6 = ipv6 ) droplet = self . get ( resp [ self . singular ] [ 'id' ] ) if wait : droplet . wait ( ) # HACK sometimes the IP address doesn't return correctly droplet = self . get ( resp [ self . singular ] [ 'id' ] ) return droplet
10802	def _c2x ( self , c ) : return 0.5 * ( self . window [ 0 ] + self . window [ 1 ] + c * ( self . window [ 1 ] - self . window [ 0 ] ) )
7522	def concat_vcf ( data , names , full ) : ## open handle and write headers if not full : writer = open ( data . outfiles . vcf , 'w' ) else : writer = gzip . open ( data . outfiles . VCF , 'w' ) vcfheader ( data , names , writer ) writer . close ( ) ## get vcf chunks vcfchunks = glob . glob ( data . outfiles . vcf + ".*" ) vcfchunks . sort ( key = lambda x : int ( x . rsplit ( "." ) [ - 1 ] ) ) ## concatenate if not full : writer = open ( data . outfiles . vcf , 'a' ) else : writer = gzip . open ( data . outfiles . VCF , 'a' ) ## what order do users want? The order in the original ref file? ## Sorted by the size of chroms? that is the order in faidx. ## If reference mapping then it's nice to sort the vcf data by ## CHROM and POS. This is doing a very naive sort right now, so the ## CHROM will be ordered, but not the pos within each chrom. if data . paramsdict [ "assembly_method" ] in [ "reference" , "denovo+reference" ] : ## Some unix sorting magic to get POS sorted within CHROM ## First you sort by POS (-k 2,2), then you do a `stable` sort ## by CHROM. You end up with POS ordered and grouped correctly by CHROM ## but relatively unordered CHROMs (locus105 will be before locus11). cmd = [ "cat" ] + vcfchunks + [ " | sort -k 2,2 -n | sort -k 1,1 -s" ] cmd = " " . join ( cmd ) proc = sps . Popen ( cmd , shell = True , stderr = sps . STDOUT , stdout = writer , close_fds = True ) else : proc = sps . Popen ( [ "cat" ] + vcfchunks , stderr = sps . STDOUT , stdout = writer , close_fds = True ) err = proc . communicate ( ) [ 0 ] if proc . returncode : raise IPyradWarningExit ( "err in concat_vcf: %s" , err ) writer . close ( ) for chunk in vcfchunks : os . remove ( chunk )
2973	def from_dict ( values ) : try : containers = values [ 'containers' ] parsed_containers = { } for name , container_dict in containers . items ( ) : try : # one config entry might result in many container # instances (indicated by the 'count' config value) for cnt in BlockadeContainerConfig . from_dict ( name , container_dict ) : # check for duplicate 'container_name' definitions if cnt . container_name : cname = cnt . container_name existing = [ c for c in parsed_containers . values ( ) if c . container_name == cname ] if existing : raise BlockadeConfigError ( "Duplicate 'container_name' definition: %s" % ( cname ) ) parsed_containers [ cnt . name ] = cnt except Exception as err : raise BlockadeConfigError ( "Container '%s' config problem: %s" % ( name , err ) ) network = values . get ( 'network' ) if network : defaults = _DEFAULT_NETWORK_CONFIG . copy ( ) defaults . update ( network ) network = defaults else : network = _DEFAULT_NETWORK_CONFIG . copy ( ) return BlockadeConfig ( parsed_containers , network = network ) except KeyError as err : raise BlockadeConfigError ( "Config missing value: " + str ( err ) ) except Exception as err : # TODO log this to some debug stream? raise BlockadeConfigError ( "Failed to load config: " + str ( err ) )
11701	def breed ( self , egg_donor , sperm_donor ) : offspring = [ ] try : num_children = npchoice ( [ 1 , 2 ] , 1 , p = [ 0.8 , 0.2 ] ) [ 0 ] # 20% chance of twins for _ in range ( num_children ) : child = God ( egg_donor , sperm_donor ) offspring . append ( child ) send_birth_announcement ( egg_donor , sperm_donor , child ) except ValueError : print ( "Breeding error occurred. Likely the generator ran out of names." ) return offspring
9899	def _data ( self ) : if self . is_caching : return self . cache with open ( self . path , "r" ) as f : return json . load ( f )
3899	def main ( ) : parser = argparse . ArgumentParser ( ) parser . add_argument ( 'protofilepath' ) args = parser . parse_args ( ) out_file = compile_protofile ( args . protofilepath ) with open ( out_file , 'rb' ) as proto_file : # pylint: disable=no-member file_descriptor_set = descriptor_pb2 . FileDescriptorSet . FromString ( proto_file . read ( ) ) # pylint: enable=no-member for file_descriptor in file_descriptor_set . file : # Build dict of location tuples locations = { } for location in file_descriptor . source_code_info . location : locations [ tuple ( location . path ) ] = location # Add comment to top print ( make_comment ( 'This file was automatically generated from {} and ' 'should not be edited directly.' . format ( args . protofilepath ) ) ) # Generate documentation for index , message_desc in enumerate ( file_descriptor . message_type ) : generate_message_doc ( message_desc , locations , ( 4 , index ) ) for index , enum_desc in enumerate ( file_descriptor . enum_type ) : generate_enum_doc ( enum_desc , locations , ( 5 , index ) )
13826	def FromJsonString ( self , value ) : self . Clear ( ) for path in value . split ( ',' ) : self . paths . append ( path )
9181	def _validate_subjects ( cursor , model ) : subject_vocab = [ term [ 0 ] for term in acquire_subject_vocabulary ( cursor ) ] subjects = model . metadata . get ( 'subjects' , [ ] ) invalid_subjects = [ s for s in subjects if s not in subject_vocab ] if invalid_subjects : raise exceptions . InvalidMetadata ( 'subjects' , invalid_subjects )
10500	def waitForCreation ( self , timeout = 10 , notification = 'AXCreated' ) : callback = AXCallbacks . returnElemCallback retelem = None args = ( retelem , ) return self . waitFor ( timeout , notification , callback = callback , args = args )
1283	def autolink ( self , link , is_email = False ) : text = link = escape ( link ) if is_email : link = 'mailto:%s' % link return '<a href="%s">%s</a>' % ( link , text )
9450	def cancel_scheduled_hangup ( self , call_params ) : path = '/' + self . api_version + '/CancelScheduledHangup/' method = 'POST' return self . request ( path , method , call_params )
3866	async def rename ( self , name ) : await self . _client . rename_conversation ( hangouts_pb2 . RenameConversationRequest ( request_header = self . _client . get_request_header ( ) , new_name = name , event_request_header = self . _get_event_request_header ( ) , ) )
2571	def send_message ( self ) : start = time . time ( ) message = None if not self . initialized : message = self . construct_start_message ( ) self . initialized = True else : message = self . construct_end_message ( ) self . send_UDP_message ( message ) end = time . time ( ) return end - start
11347	def html_to_text ( cls , html ) : s = cls ( ) s . feed ( html ) unescaped_data = s . unescape ( s . get_data ( ) ) return escape_for_xml ( unescaped_data , tags_to_keep = s . mathml_elements )
11826	def print_boggle ( board ) : n2 = len ( board ) n = exact_sqrt ( n2 ) for i in range ( n2 ) : if i % n == 0 and i > 0 : print if board [ i ] == 'Q' : print 'Qu' , else : print str ( board [ i ] ) + ' ' , print
6927	def newcursor ( self , dictcursor = False ) : handle = hashlib . sha256 ( os . urandom ( 12 ) ) . hexdigest ( ) if dictcursor : self . cursors [ handle ] = self . connection . cursor ( cursor_factory = psycopg2 . extras . DictCursor ) else : self . cursors [ handle ] = self . connection . cursor ( ) return ( self . cursors [ handle ] , handle )
9321	def _validate_api_root ( self ) : if not self . _title : msg = "No 'title' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . _versions : msg = "No 'versions' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _max_content_length is None : msg = "No 'max_content_length' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) )
6787	def preview ( self , components = None , ask = 0 ) : ask = int ( ask ) self . init ( ) component_order , plan_funcs = self . get_component_funcs ( components = components ) print ( '\n%i changes found for host %s.\n' % ( len ( component_order ) , self . genv . host_string ) ) if component_order and plan_funcs : if self . verbose : print ( 'These components have changed:\n' ) for component in sorted ( component_order ) : print ( ( ' ' * 4 ) + component ) print ( 'Deployment plan for host %s:\n' % self . genv . host_string ) for func_name , _ in plan_funcs : print ( success_str ( ( ' ' * 4 ) + func_name ) ) if component_order : print ( ) if ask and self . genv . host_string == self . genv . hosts [ - 1 ] : if component_order : if not raw_input ( 'Begin deployment? [yn] ' ) . strip ( ) . lower ( ) . startswith ( 'y' ) : sys . exit ( 0 ) else : sys . exit ( 0 )
8011	def from_request ( cls , request , webhook_id = PAYPAL_WEBHOOK_ID ) : headers = fix_django_headers ( request . META ) assert headers try : body = request . body . decode ( request . encoding or "utf-8" ) except Exception : body = "(error decoding body)" ip = request . META [ "REMOTE_ADDR" ] obj = cls . objects . create ( headers = headers , body = body , remote_ip = ip ) try : obj . valid = obj . verify ( PAYPAL_WEBHOOK_ID ) if obj . valid : # Process the item (do not save it, it'll get saved below) obj . process ( save = False ) except Exception as e : max_length = WebhookEventTrigger . _meta . get_field ( "exception" ) . max_length obj . exception = str ( e ) [ : max_length ] obj . traceback = format_exc ( ) finally : obj . save ( ) return obj
7566	def ambigcutters ( seq ) : resos = [ ] if any ( [ i in list ( "RKSYWM" ) for i in seq ] ) : for base in list ( "RKSYWM" ) : if base in seq : resos . append ( seq . replace ( base , AMBIGS [ base ] [ 0 ] ) ) resos . append ( seq . replace ( base , AMBIGS [ base ] [ 1 ] ) ) return resos else : return [ seq , "" ]
4680	def getAccountsFromPublicKey ( self , pub ) : names = self . rpc . get_key_references ( [ str ( pub ) ] ) [ 0 ] for name in names : yield name
5772	def _bcrypt_verify ( certificate_or_public_key , signature , data , hash_algorithm , rsa_pss_padding = False ) : if hash_algorithm == 'raw' : digest = data else : hash_constant = { 'md5' : BcryptConst . BCRYPT_MD5_ALGORITHM , 'sha1' : BcryptConst . BCRYPT_SHA1_ALGORITHM , 'sha256' : BcryptConst . BCRYPT_SHA256_ALGORITHM , 'sha384' : BcryptConst . BCRYPT_SHA384_ALGORITHM , 'sha512' : BcryptConst . BCRYPT_SHA512_ALGORITHM } [ hash_algorithm ] digest = getattr ( hashlib , hash_algorithm ) ( data ) . digest ( ) padding_info = null ( ) flags = 0 if certificate_or_public_key . algorithm == 'rsa' : if rsa_pss_padding : flags = BcryptConst . BCRYPT_PAD_PSS padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PSS_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) # This has to be assigned to a variable to prevent cffi from gc'ing it hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . cbSalt = len ( digest ) else : flags = BcryptConst . BCRYPT_PAD_PKCS1 padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PKCS1_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) # This has to be assigned to a variable to prevent cffi from gc'ing it if hash_algorithm == 'raw' : padding_info_struct . pszAlgId = null ( ) else : hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) else : # Windows doesn't use the ASN.1 Sequence for DSA/ECDSA signatures, # so we have to convert it here for the verification to work try : signature = algos . DSASignature . load ( signature ) . to_p1363 ( ) except ( ValueError , OverflowError , TypeError ) : raise SignatureError ( 'Signature is invalid' ) res = bcrypt . BCryptVerifySignature ( certificate_or_public_key . key_handle , padding_info , digest , len ( digest ) , signature , len ( signature ) , flags ) failure = res == BcryptConst . STATUS_INVALID_SIGNATURE failure = failure or res == BcryptConst . STATUS_INVALID_PARAMETER if failure : raise SignatureError ( 'Signature is invalid' ) handle_error ( res )
3984	def get_same_container_repos ( app_or_library_name ) : specs = get_expanded_libs_specs ( ) spec = specs . get_app_or_lib ( app_or_library_name ) return get_same_container_repos_from_spec ( spec )
7785	def error ( self , error_data ) : if not self . active : return if not self . _try_backup_item ( ) : self . _error_handler ( self . address , error_data ) self . cache . invalidate_object ( self . address ) self . _deactivate ( )
7626	def transcription ( ref , est , * * kwargs ) : namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_intervals , ref_p = ref . to_interval_values ( ) est_intervals , est_p = est . to_interval_values ( ) ref_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . transcription . evaluate ( ref_intervals , ref_pitches , est_intervals , est_pitches , * * kwargs )
10602	def alpha ( self , * * state ) : return self . k ( * * state ) / self . rho ( * * state ) / self . Cp ( * * state )
10461	def _glob_match ( self , pattern , string ) : # regex flags Multi-line, Unicode, Locale return bool ( re . match ( fnmatch . translate ( pattern ) , string , re . M | re . U | re . L ) )
2793	def create ( self ) : params = { "name" : self . name , "type" : self . type , "dns_names" : self . dns_names , "private_key" : self . private_key , "leaf_certificate" : self . leaf_certificate , "certificate_chain" : self . certificate_chain } data = self . get_data ( "certificates/" , type = POST , params = params ) if data : self . id = data [ 'certificate' ] [ 'id' ] self . not_after = data [ 'certificate' ] [ 'not_after' ] self . sha1_fingerprint = data [ 'certificate' ] [ 'sha1_fingerprint' ] self . created_at = data [ 'certificate' ] [ 'created_at' ] self . type = data [ 'certificate' ] [ 'type' ] self . dns_names = data [ 'certificate' ] [ 'dns_names' ] self . state = data [ 'certificate' ] [ 'state' ] return self
5045	def get_failed_enrollment_message ( cls , users , enrolled_in ) : failed_emails = [ user . email for user in users ] return ( 'error' , _ ( 'The following learners could not be enrolled in {enrolled_in}: {user_list}' ) . format ( enrolled_in = enrolled_in , user_list = ', ' . join ( failed_emails ) , ) )
6740	def get_packager ( ) : # TODO: remove once fabric stops using contextlib.nested. # https://github.com/fabric/fabric/issues/1364 import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_packager = get_rc ( 'common_packager' ) if common_packager : return common_packager #TODO:cache result by current env.host_string so we can handle multiple hosts with different OSes with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run ( 'cat /etc/fedora-release' ) if ret . succeeded : common_packager = YUM else : ret = _run ( 'cat /etc/lsb-release' ) if ret . succeeded : common_packager = APT else : for pn in PACKAGERS : ret = _run ( 'which %s' % pn ) if ret . succeeded : common_packager = pn break if not common_packager : raise Exception ( 'Unable to determine packager.' ) set_rc ( 'common_packager' , common_packager ) return common_packager
4549	def draw_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : _draw_fast_hline ( setter , x + r , y , w - 2 * r , color , aa ) # Top _draw_fast_hline ( setter , x + r , y + h - 1 , w - 2 * r , color , aa ) # Bottom _draw_fast_vline ( setter , x , y + r , h - 2 * r , color , aa ) # Left _draw_fast_vline ( setter , x + w - 1 , y + r , h - 2 * r , color , aa ) # Right # draw four corners _draw_circle_helper ( setter , x + r , y + r , r , 1 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + r , r , 2 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + h - r - 1 , r , 4 , color , aa ) _draw_circle_helper ( setter , x + r , y + h - r - 1 , r , 8 , color , aa )
8373	def widget_changed ( self , widget , v ) : # set the appropriate bot var if v . type is NUMBER : self . bot . _namespace [ v . name ] = widget . get_value ( ) self . bot . _vars [ v . name ] . value = widget . get_value ( ) ## Not sure if this is how to do this - stu publish_event ( VARIABLE_UPDATED_EVENT , v ) # pretty dumb for now elif v . type is BOOLEAN : self . bot . _namespace [ v . name ] = widget . get_active ( ) self . bot . _vars [ v . name ] . value = widget . get_active ( ) ## Not sure if this is how to do this - stu publish_event ( VARIABLE_UPDATED_EVENT , v ) # pretty dumb for now elif v . type is TEXT : self . bot . _namespace [ v . name ] = widget . get_text ( ) self . bot . _vars [ v . name ] . value = widget . get_text ( ) ## Not sure if this is how to do this - stu publish_event ( VARIABLE_UPDATED_EVENT , v )
9785	def bookmark ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : PolyaxonClient ( ) . build_job . bookmark ( user , project_name , _build ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not bookmark build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Build job bookmarked." )
1980	def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to read from a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to read to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to read a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 2 ) if issymbolic ( rx_bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 3 ) return super ( ) . sys_receive ( cpu , fd , buf , count , rx_bytes )
8995	def relative_file ( self , module , file ) : path = self . _relative_to_absolute ( module , file ) return self . path ( path )
6301	def get_effect_resources ( self ) -> List [ Any ] : resources = [ ] for package in self . packages : resources . extend ( package . resources ) return resources
8516	def _assert_all_finite ( X ) : X = np . asanyarray ( X ) # First try an O(n) time, O(1) space solution for the common case that # everything is finite; fall back to O(n) space np.isfinite to prevent # false positives from overflow in sum method if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : raise ValueError ( "Input contains NaN, infinity" " or a value too large for %r." % X . dtype )
5544	def clip_array_with_vector ( array , array_affine , geometries , inverted = False , clip_buffer = 0 ) : # buffer input geometries and clean up buffered_geometries = [ ] for feature in geometries : feature_geom = to_shape ( feature [ "geometry" ] ) if feature_geom . is_empty : continue if feature_geom . geom_type == "GeometryCollection" : # for GeometryCollections apply buffer to every subgeometry # and make union buffered_geom = unary_union ( [ g . buffer ( clip_buffer ) for g in feature_geom ] ) else : buffered_geom = feature_geom . buffer ( clip_buffer ) if not buffered_geom . is_empty : buffered_geometries . append ( buffered_geom ) # mask raster by buffered geometries if buffered_geometries : if array . ndim == 2 : return ma . masked_array ( array , geometry_mask ( buffered_geometries , array . shape , array_affine , invert = inverted ) ) elif array . ndim == 3 : mask = geometry_mask ( buffered_geometries , ( array . shape [ 1 ] , array . shape [ 2 ] ) , array_affine , invert = inverted ) return ma . masked_array ( array , mask = np . stack ( ( mask for band in array ) ) ) # if no geometries, return unmasked array else : fill = False if inverted else True return ma . masked_array ( array , mask = np . full ( array . shape , fill , dtype = bool ) )
7052	def parallel_tfa_lcdir ( lcdir , templateinfo , lcfileglob = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , interp = 'nearest' , sigclip = 5.0 , mintemplatedist_arcmin = 10.0 , nworkers = NCPUS , maxworkertasks = 1000 ) : # open the templateinfo first if isinstance ( templateinfo , str ) and os . path . exists ( templateinfo ) : with open ( templateinfo , 'rb' ) as infd : templateinfo = pickle . load ( infd ) try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None # find all the files matching the lcglob in lcdir if lcfileglob is None : lcfileglob = dfileglob lclist = sorted ( glob . glob ( os . path . join ( lcdir , lcfileglob ) ) ) return parallel_tfa_lclist ( lclist , templateinfo , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = None , interp = interp , sigclip = sigclip , mintemplatedist_arcmin = mintemplatedist_arcmin , nworkers = nworkers , maxworkertasks = maxworkertasks )
3523	def intercom_user_hash ( data ) : if getattr ( settings , 'INTERCOM_HMAC_SECRET_KEY' , None ) : return hmac . new ( key = _hashable_bytes ( settings . INTERCOM_HMAC_SECRET_KEY ) , msg = _hashable_bytes ( data ) , digestmod = hashlib . sha256 , ) . hexdigest ( ) else : return None
153	def min_item ( self ) : if self . is_empty ( ) : raise ValueError ( "Tree is empty" ) node = self . _root while node . left is not None : node = node . left return node . key , node . value
5391	def _delocalize_logging_command ( self , logging_path , user_project ) : # Get the logging prefix (everything up to ".log") logging_prefix = os . path . splitext ( logging_path . uri ) [ 0 ] # Set the provider-specific mkdir and file copy commands if logging_path . file_provider == job_model . P_LOCAL : mkdir_cmd = 'mkdir -p "%s"\n' % os . path . dirname ( logging_prefix ) cp_cmd = 'cp' elif logging_path . file_provider == job_model . P_GCS : mkdir_cmd = '' if user_project : cp_cmd = 'gsutil -u {} -mq cp' . format ( user_project ) else : cp_cmd = 'gsutil -mq cp' else : assert False # Construct the copy command copy_logs_cmd = textwrap . dedent ( """\ local cp_cmd="{cp_cmd}" local prefix="{prefix}" """ ) . format ( cp_cmd = cp_cmd , prefix = logging_prefix ) # Build up the command body = textwrap . dedent ( """\ {mkdir_cmd} {copy_logs_cmd} """ ) . format ( mkdir_cmd = mkdir_cmd , copy_logs_cmd = copy_logs_cmd ) return body
7954	def starttls ( self , * * kwargs ) : with self . lock : self . event ( TLSConnectingEvent ( ) ) self . _write_queue . append ( StartTLS ( * * kwargs ) ) self . _write_queue_cond . notify ( )
3074	def authorize_view ( self ) : args = request . args . to_dict ( ) # Scopes will be passed as mutliple args, and to_dict() will only # return one. So, we use getlist() to get all of the scopes. args [ 'scopes' ] = request . args . getlist ( 'scopes' ) return_url = args . pop ( 'return_url' , None ) if return_url is None : return_url = request . referrer or '/' flow = self . _make_flow ( return_url = return_url , * * args ) auth_url = flow . step1_get_authorize_url ( ) return redirect ( auth_url )
5419	def _google_v2_parse_arguments ( args ) : if ( args . zones and args . regions ) or ( not args . zones and not args . regions ) : raise ValueError ( 'Exactly one of --regions and --zones must be specified' ) if args . machine_type and ( args . min_cores or args . min_ram ) : raise ValueError ( '--machine-type not supported together with --min-cores or --min-ram.' )
9200	def _sort_lows_and_highs ( func ) : @ functools . wraps ( func ) def wrapper ( * args , * * kwargs ) : for low , high , mult in func ( * args , * * kwargs ) : if low < high : yield low , high , mult else : yield high , low , mult return wrapper
9685	def read_firmware ( self ) : # Send the command byte and sleep for 9 ms self . cnxn . xfer ( [ 0x12 ] ) sleep ( 10e-3 ) self . firmware [ 'major' ] = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] self . firmware [ 'minor' ] = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] # Build the firmware version self . firmware [ 'version' ] = float ( '{}.{}' . format ( self . firmware [ 'major' ] , self . firmware [ 'minor' ] ) ) sleep ( 0.1 ) return self . firmware
11646	def transform ( self , X ) : n = self . flip_ . shape [ 0 ] if X . ndim != 2 or X . shape [ 1 ] != n : msg = "X should have {} columns, the number of samples at fit time" raise TypeError ( msg . format ( self . flip_ . shape [ 0 ] ) ) return np . dot ( X , self . flip_ )
702	def getResultsPerChoice ( self , swarmId , maxGenIdx , varName ) : results = dict ( ) # Get all the completed particles in this swarm ( allParticles , _ , resultErrs , _ , _ ) = self . getParticleInfos ( swarmId , genIdx = None , matured = True ) for particleState , resultErr in itertools . izip ( allParticles , resultErrs ) : # Consider this generation? if maxGenIdx is not None : if particleState [ 'genIdx' ] > maxGenIdx : continue # Ignore unless this model completed successfully if resultErr == numpy . inf : continue position = Particle . getPositionFromState ( particleState ) varPosition = position [ varName ] varPositionStr = str ( varPosition ) if varPositionStr in results : results [ varPositionStr ] [ 1 ] . append ( resultErr ) else : results [ varPositionStr ] = ( varPosition , [ resultErr ] ) return results
5240	def market_close ( self , session , mins ) -> Session : if session not in self . exch : return SessNA end_time = self . exch [ session ] [ - 1 ] return Session ( shift_time ( end_time , - int ( mins ) + 1 ) , end_time )
8045	def parse_docstring ( self ) : self . log . debug ( "parsing docstring, token is %r (%s)" , self . current . kind , self . current . value ) while self . current . kind in ( tk . COMMENT , tk . NEWLINE , tk . NL ) : self . stream . move ( ) self . log . debug ( "parsing docstring, token is %r (%s)" , self . current . kind , self . current . value , ) if self . current . kind == tk . STRING : docstring = self . current . value self . stream . move ( ) return docstring return None
13578	def update ( course = False ) : if course : with Spinner . context ( msg = "Updated course metadata." , waitmsg = "Updating course metadata." ) : for course in api . get_courses ( ) : old = None try : old = Course . get ( Course . tid == course [ "id" ] ) except peewee . DoesNotExist : old = None if old : old . details_url = course [ "details_url" ] old . save ( ) continue Course . create ( tid = course [ "id" ] , name = course [ "name" ] , details_url = course [ "details_url" ] ) else : selected = Course . get_selected ( ) # with Spinner.context(msg="Updated exercise metadata.", # waitmsg="Updating exercise metadata."): print ( "Updating exercise data." ) for exercise in api . get_exercises ( selected ) : old = None try : old = Exercise . byid ( exercise [ "id" ] ) except peewee . DoesNotExist : old = None if old is not None : old . name = exercise [ "name" ] old . course = selected . id old . is_attempted = exercise [ "attempted" ] old . is_completed = exercise [ "completed" ] old . deadline = exercise . get ( "deadline" ) old . is_downloaded = os . path . isdir ( old . path ( ) ) old . return_url = exercise [ "return_url" ] old . zip_url = exercise [ "zip_url" ] old . submissions_url = exercise [ "exercise_submissions_url" ] old . save ( ) download_exercise ( old , update = True ) else : ex = Exercise . create ( tid = exercise [ "id" ] , name = exercise [ "name" ] , course = selected . id , is_attempted = exercise [ "attempted" ] , is_completed = exercise [ "completed" ] , deadline = exercise . get ( "deadline" ) , return_url = exercise [ "return_url" ] , zip_url = exercise [ "zip_url" ] , submissions_url = exercise [ ( "exercise_" "submissions_" "url" ) ] ) ex . is_downloaded = os . path . isdir ( ex . path ( ) ) ex . save ( )
122	def _augment_images_worker ( self , augseq , queue_source , queue_result , seedval ) : np . random . seed ( seedval ) random . seed ( seedval ) augseq . reseed ( seedval ) ia . seed ( seedval ) loader_finished = False while not loader_finished : # wait for a new batch in the source queue and load it try : batch_str = queue_source . get ( timeout = 0.1 ) batch = pickle . loads ( batch_str ) if batch is None : loader_finished = True # put it back in so that other workers know that the loading queue is finished queue_source . put ( pickle . dumps ( None , protocol = - 1 ) ) else : batch_aug = augseq . augment_batch ( batch ) # send augmented batch to output queue batch_str = pickle . dumps ( batch_aug , protocol = - 1 ) queue_result . put ( batch_str ) except QueueEmpty : time . sleep ( 0.01 ) queue_result . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 )
919	def error ( self , msg , * args , * * kwargs ) : self . _baseLogger . error ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )
10200	def run ( self ) : return elasticsearch . helpers . bulk ( self . client , self . actionsiter ( ) , stats_only = True , chunk_size = 50 )
10040	def deposit_fetcher ( record_uuid , data ) : return FetchedPID ( provider = DepositProvider , pid_type = DepositProvider . pid_type , pid_value = str ( data [ '_deposit' ] [ 'id' ] ) , )
10721	def get_command ( namespace ) : cmd = [ "pylint" , namespace . package ] + arg_map [ namespace . package ] if namespace . ignore : cmd . append ( "--ignore=%s" % namespace . ignore ) return cmd
8924	def verify ( self ) : value = self . get ( 'verify' , 'true' ) if isinstance ( value , bool ) : verify = value elif value . lower ( ) == 'true' : verify = True elif value . lower ( ) == 'false' : verify = False else : verify = value return verify
9903	def is_configured ( self , project , * * kwargs ) : params = self . get_option return bool ( params ( 'server_host' , project ) and params ( 'server_port' , project ) )
5003	def handle ( self , * args , * * options ) : LOGGER . info ( 'Starting assigning enterprise roles to users!' ) role = options [ 'role' ] if role == ENTERPRISE_ADMIN_ROLE : # Assign admin role to non-staff users with enterprise data api access. self . _assign_enterprise_role_to_users ( self . _get_enterprise_admin_users_batch , options ) elif role == ENTERPRISE_OPERATOR_ROLE : # Assign operator role to staff users with enterprise data api access. self . _assign_enterprise_role_to_users ( self . _get_enterprise_operator_users_batch , options ) elif role == ENTERPRISE_LEARNER_ROLE : # Assign enterprise learner role to enterprise customer users. self . _assign_enterprise_role_to_users ( self . _get_enterprise_customer_users_batch , options ) elif role == ENTERPRISE_ENROLLMENT_API_ADMIN_ROLE : # Assign enterprise enrollment api admin to non-staff users with enterprise data api access. self . _assign_enterprise_role_to_users ( self . _get_enterprise_enrollment_api_admin_users_batch , options , True ) elif role == ENTERPRISE_CATALOG_ADMIN_ROLE : # Assign enterprise catalog admin role to users with having credentials in catalog. self . _assign_enterprise_role_to_users ( self . _get_enterprise_catalog_admin_users_batch , options , True ) else : raise CommandError ( 'Please provide a valid role name. Supported roles are {admin} and {learner}' . format ( admin = ENTERPRISE_ADMIN_ROLE , learner = ENTERPRISE_LEARNER_ROLE ) ) LOGGER . info ( 'Successfully finished assigning enterprise roles to users!' )
12455	def error_handler ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : """ Run actual function and if exception catched and error handler enabled put traceback to log file """ try : return func ( * args , * * kwargs ) except BaseException as err : # Do not catch exceptions on testing if BOOTSTRAPPER_TEST_KEY in os . environ : raise # Fail silently if error handling disabled if ERROR_HANDLER_DISABLED : return True # Otherwise save traceback to log return save_traceback ( err ) return wrapper
40	def add ( self , * args , * * kwargs ) : idx = self . _next_idx super ( ) . add ( * args , * * kwargs ) self . _it_sum [ idx ] = self . _max_priority ** self . _alpha self . _it_min [ idx ] = self . _max_priority ** self . _alpha
5969	def em_schedule ( * * kwargs ) : mdrunner = kwargs . pop ( 'mdrunner' , None ) integrators = kwargs . pop ( 'integrators' , [ 'l-bfgs' , 'steep' ] ) kwargs . pop ( 'integrator' , None ) # clean input; we set intgerator from integrators nsteps = kwargs . pop ( 'nsteps' , [ 100 , 1000 ] ) outputs = [ 'em{0:03d}_{1!s}.pdb' . format ( i , integrator ) for i , integrator in enumerate ( integrators ) ] outputs [ - 1 ] = kwargs . pop ( 'output' , 'em.pdb' ) files = { 'struct' : kwargs . pop ( 'struct' , None ) } # fake output from energy_minimize() for i , integrator in enumerate ( integrators ) : struct = files [ 'struct' ] logger . info ( "[em %d] energy minimize with %s for maximum %d steps" , i , integrator , nsteps [ i ] ) kwargs . update ( { 'struct' : struct , 'output' : outputs [ i ] , 'integrator' : integrator , 'nsteps' : nsteps [ i ] } ) if not integrator == 'l-bfgs' : kwargs [ 'mdrunner' ] = mdrunner else : kwargs [ 'mdrunner' ] = None logger . warning ( "[em %d] Not using mdrunner for L-BFGS because it cannot " "do parallel runs." , i ) files = energy_minimize ( * * kwargs ) return files
12181	def api_subclass_factory ( name , docstring , remove_methods , base = SlackApi ) : methods = deepcopy ( base . API_METHODS ) for parent , to_remove in remove_methods . items ( ) : if to_remove is ALL : del methods [ parent ] else : for method in to_remove : del methods [ parent ] [ method ] return type ( name , ( base , ) , dict ( API_METHODS = methods , __doc__ = docstring ) )
5745	def fsplit ( file_to_split ) : dirname = file_to_split + '_splitted' if not os . path . exists ( dirname ) : os . mkdir ( dirname ) part_file_size = os . path . getsize ( file_to_split ) / number_of_files + 1 splitted_files = [ ] with open ( file_to_split , "r" ) as f : number = 0 actual = 0 while 1 : prec = actual # Jump of "size" from the current place in the file f . seek ( part_file_size , os . SEEK_CUR ) # find the next separator or EOF s = f . readline ( ) if len ( s ) == 0 : s = f . readline ( ) while len ( s ) != 0 and s != separator : s = f . readline ( ) # Get the current place actual = f . tell ( ) new_file = os . path . join ( dirname , str ( number ) ) # Create the new file with open ( file_to_split , "r" ) as temp : temp . seek ( prec ) # Get the text we want to put in the new file copy = temp . read ( actual - prec ) # Write the new file open ( new_file , 'w' ) . write ( copy ) splitted_files . append ( new_file ) number += 1 # End of file if len ( s ) == 0 : break return splitted_files
362	def maybe_download_and_extract ( filename , working_directory , url_source , extract = False , expected_bytes = None ) : # We first define a download function, supporting both Python 2 and 3. def _download ( filename , working_directory , url_source ) : progress_bar = progressbar . ProgressBar ( ) def _dlProgress ( count , blockSize , totalSize , pbar = progress_bar ) : if ( totalSize != 0 ) : if not pbar . max_value : totalBlocks = math . ceil ( float ( totalSize ) / float ( blockSize ) ) pbar . max_value = int ( totalBlocks ) pbar . update ( count , force = True ) filepath = os . path . join ( working_directory , filename ) logging . info ( 'Downloading %s...\n' % filename ) urlretrieve ( url_source + filename , filepath , reporthook = _dlProgress ) exists_or_mkdir ( working_directory , verbose = False ) filepath = os . path . join ( working_directory , filename ) if not os . path . exists ( filepath ) : _download ( filename , working_directory , url_source ) statinfo = os . stat ( filepath ) logging . info ( 'Succesfully downloaded %s %s bytes.' % ( filename , statinfo . st_size ) ) # , 'bytes.') if ( not ( expected_bytes is None ) and ( expected_bytes != statinfo . st_size ) ) : raise Exception ( 'Failed to verify ' + filename + '. Can you get to it with a browser?' ) if ( extract ) : if tarfile . is_tarfile ( filepath ) : logging . info ( 'Trying to extract tar file' ) tarfile . open ( filepath , 'r' ) . extractall ( working_directory ) logging . info ( '... Success!' ) elif zipfile . is_zipfile ( filepath ) : logging . info ( 'Trying to extract zip file' ) with zipfile . ZipFile ( filepath ) as zf : zf . extractall ( working_directory ) logging . info ( '... Success!' ) else : logging . info ( "Unknown compression_format only .tar.gz/.tar.bz2/.tar and .zip supported" ) return filepath
6019	def from_inverse_noise_map ( cls , pixel_scale , inverse_noise_map ) : noise_map = 1.0 / inverse_noise_map return NoiseMap ( array = noise_map , pixel_scale = pixel_scale )
5933	def besttype ( x ) : x = to_unicode ( x ) # make unicode as soon as possible try : x = x . strip ( ) except AttributeError : pass m = re . match ( r"""['"](?P<value>.*)["']$""" , x ) if m is None : # not a quoted string, try different types for converter in int , float , to_unicode : # try them in increasing order of lenience try : return converter ( x ) except ValueError : pass else : # quoted string x = to_unicode ( m . group ( 'value' ) ) return x
8191	def nodes_by_eigenvalue ( self , treshold = 0.0 ) : nodes = [ ( n . eigenvalue , n ) for n in self . nodes if n . eigenvalue > treshold ] nodes . sort ( ) nodes . reverse ( ) return [ n for w , n in nodes ]
459	def predict ( sess , network , X , x , y_op , batch_size = None ) : if batch_size is None : dp_dict = dict_to_one ( network . all_drop ) # disable noise layers feed_dict = { x : X , } feed_dict . update ( dp_dict ) return sess . run ( y_op , feed_dict = feed_dict ) else : result = None for X_a , _ in tl . iterate . minibatches ( X , X , batch_size , shuffle = False ) : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X_a , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) if result is None : result = result_a else : result = np . concatenate ( ( result , result_a ) ) if result is None : if len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = result_a else : if len ( X ) != len ( result ) and len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = np . concatenate ( ( result , result_a ) ) return result
2986	def get_cors_options ( appInstance , * dicts ) : options = DEFAULT_OPTIONS . copy ( ) options . update ( get_app_kwarg_dict ( appInstance ) ) if dicts : for d in dicts : options . update ( d ) return serialize_options ( options )
8714	def file_format ( self ) : log . info ( 'Formating, can take minutes depending on flash size...' ) res = self . __exchange ( 'file.format()' , timeout = 300 ) if 'format done' not in res : log . error ( res ) else : log . info ( res ) return res
5050	def from_children ( cls , program_uuid , * children ) : if not children or any ( child is None for child in children ) : return None granted = all ( ( child . granted for child in children ) ) exists = any ( ( child . exists for child in children ) ) usernames = set ( [ child . username for child in children ] ) enterprises = set ( [ child . enterprise_customer for child in children ] ) if not len ( usernames ) == len ( enterprises ) == 1 : raise InvalidProxyConsent ( 'Children used to create a bulk proxy consent object must ' 'share a single common username and EnterpriseCustomer.' ) username = children [ 0 ] . username enterprise_customer = children [ 0 ] . enterprise_customer return cls ( enterprise_customer = enterprise_customer , username = username , program_uuid = program_uuid , exists = exists , granted = granted , child_consents = children )
11177	def parsestr ( self , argstr ) : argv = shlex . split ( argstr , comments = True ) if len ( argv ) != 1 : raise BadNumberOfArguments ( 1 , len ( argv ) ) arg = argv [ 0 ] lower = arg . lower ( ) if lower in self . true : return True if lower in self . false : return False raise BadArgument ( arg , "Allowed values are " + self . allowed + '.' )
12521	def to_file ( self , output_file , smooth_fwhm = 0 , outdtype = None ) : outmat , mask_indices , mask_shape = self . to_matrix ( smooth_fwhm , outdtype ) exporter = ExportData ( ) content = { 'data' : outmat , 'labels' : self . labels , 'mask_indices' : mask_indices , 'mask_shape' : mask_shape , } if self . others : content . update ( self . others ) log . debug ( 'Creating content in file {}.' . format ( output_file ) ) try : exporter . save_variables ( output_file , content ) except Exception as exc : raise Exception ( 'Error saving variables to file {}.' . format ( output_file ) ) from exc
2089	def get ( self , pk = None , * * kwargs ) : if kwargs . pop ( 'include_debug_header' , True ) : debug . log ( 'Getting the record.' , header = 'details' ) response = self . read ( pk = pk , fail_on_no_results = True , fail_on_multiple_results = True , * * kwargs ) return response [ 'results' ] [ 0 ]
1614	def Match ( pattern , s ) : # The regexp compilation caching is inlined in both Match and Search for # performance reasons; factoring it out into a separate function turns out # to be noticeably expensive. if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . match ( s )
11293	def oembed_schema ( request ) : current_domain = Site . objects . get_current ( ) . domain url_schemes = [ ] # a list of dictionaries for all the urls we can match endpoint = reverse ( 'oembed_json' ) # the public endpoint for our oembeds providers = oembed . site . get_providers ( ) for provider in providers : # first make sure this provider class is exposed at the public endpoint if not provider . provides : continue match = None if isinstance ( provider , DjangoProvider ) : # django providers define their regex_list by using urlreversing url_pattern = resolver . reverse_dict . get ( provider . _meta . named_view ) # this regex replacement is set to be non-greedy, which results # in things like /news/*/*/*/*/ -- this is more explicit if url_pattern : regex = re . sub ( r'%\(.+?\)s' , '*' , url_pattern [ 0 ] [ 0 ] [ 0 ] ) match = 'http://%s/%s' % ( current_domain , regex ) elif isinstance ( provider , HTTPProvider ) : match = provider . url_scheme else : match = provider . regex if match : url_schemes . append ( { 'type' : provider . resource_type , 'matches' : match , 'endpoint' : endpoint } ) url_schemes . sort ( key = lambda item : item [ 'matches' ] ) response = HttpResponse ( mimetype = 'application/json' ) response . write ( simplejson . dumps ( url_schemes ) ) return response
11800	def infer_assignment ( self ) : self . support_pruning ( ) return dict ( ( v , self . curr_domains [ v ] [ 0 ] ) for v in self . vars if 1 == len ( self . curr_domains [ v ] ) )
335	def compute_consistency_score ( returns_test , preds ) : returns_test_cum = cum_returns ( returns_test , starting_value = 1. ) cum_preds = np . cumprod ( preds + 1 , 1 ) q = [ sp . stats . percentileofscore ( cum_preds [ : , i ] , returns_test_cum . iloc [ i ] , kind = 'weak' ) for i in range ( len ( returns_test_cum ) ) ] # normalize to be from 100 (perfect median line) to 0 (completely outside # of cone) return 100 - np . abs ( 50 - np . mean ( q ) ) / .5
13385	def store_env ( path = None ) : path = path or get_store_env_tmp ( ) env_dict = yaml . safe_dump ( os . environ . data , default_flow_style = False ) with open ( path , 'w' ) as f : f . write ( env_dict ) return path
2642	def submit ( self , func , * args , * * kwargs ) : task_id = uuid . uuid4 ( ) logger . debug ( "Pushing function {} to queue with args {}" . format ( func , args ) ) self . tasks [ task_id ] = Future ( ) fn_buf = pack_apply_message ( func , args , kwargs , buffer_threshold = 1024 * 1024 , item_threshold = 1024 ) msg = { "task_id" : task_id , "buffer" : fn_buf } # Post task to the the outgoing queue self . outgoing_q . put ( msg ) # Return the future return self . tasks [ task_id ]
2475	def set_lic_name ( self , doc , name ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_name_set : self . extr_lic_name_set = True if validations . validate_extr_lic_name ( name ) : self . extr_lic ( doc ) . full_name = name return True else : raise SPDXValueError ( 'ExtractedLicense::Name' ) else : raise CardinalityError ( 'ExtractedLicense::Name' ) else : raise OrderError ( 'ExtractedLicense::Name' )
13732	def validate_is_boolean_true ( config_val , evar ) : if config_val is None : raise ValueError ( "Value for environment variable '{evar_name}' can't " "be empty." . format ( evar_name = evar . name ) ) return config_val
10258	def count_top_centrality ( graph : BELGraph , number : Optional [ int ] = 30 ) -> Mapping [ BaseEntity , int ] : dd = nx . betweenness_centrality ( graph ) dc = Counter ( dd ) return dict ( dc . most_common ( number ) )
9330	def total_memory ( ) : with file ( '/proc/meminfo' , 'r' ) as f : for line in f : words = line . split ( ) if words [ 0 ] . upper ( ) == 'MEMTOTAL:' : return int ( words [ 1 ] ) * 1024 raise IOError ( 'MemTotal unknown' )
11640	def json_get_data ( filename ) : with open ( filename ) as fp : json_data = json . load ( fp ) return json_data return False
12779	def get_stream ( self , error_callback = None , live = True ) : self . join ( ) return Stream ( self , error_callback = error_callback , live = live )
6181	def load_PSFLab_file ( fname ) : if os . path . exists ( fname ) or os . path . exists ( fname + '.mat' ) : return loadmat ( fname ) [ 'data' ] else : raise IOError ( "Can't find PSF file '%s'" % fname )
986	def mmPrettyPrintConnections ( self ) : text = "" text += ( "Segments: (format => " "(#) [(source cell=permanence ...), ...]\n" ) text += "------------------------------------\n" columns = range ( self . numberOfColumns ( ) ) for column in columns : cells = self . cellsForColumn ( column ) for cell in cells : segmentDict = dict ( ) for seg in self . connections . segmentsForCell ( cell ) : synapseList = [ ] for synapse in self . connections . synapsesForSegment ( seg ) : synapseData = self . connections . dataForSynapse ( synapse ) synapseList . append ( ( synapseData . presynapticCell , synapseData . permanence ) ) synapseList . sort ( ) synapseStringList = [ "{0:3}={1:.2f}" . format ( sourceCell , permanence ) for sourceCell , permanence in synapseList ] segmentDict [ seg ] = "({0})" . format ( " " . join ( synapseStringList ) ) text += ( "Column {0:3} / Cell {1:3}:\t({2}) {3}\n" . format ( column , cell , len ( segmentDict . values ( ) ) , "[{0}]" . format ( ", " . join ( segmentDict . values ( ) ) ) ) ) if column < len ( columns ) - 1 : # not last text += "\n" text += "------------------------------------\n" return text
10332	def average_node_annotation ( graph : BELGraph , key : str , annotation : str = 'Subgraph' , aggregator : Optional [ Callable [ [ Iterable [ X ] ] , X ] ] = None , ) -> Mapping [ str , X ] : if aggregator is None : def aggregator ( x ) : """Calculates the average""" return sum ( x ) / len ( x ) result = { } for subgraph , nodes in group_nodes_by_annotation ( graph , annotation ) . items ( ) : values = [ graph . nodes [ node ] [ key ] for node in nodes if key in graph . nodes [ node ] ] result [ subgraph ] = aggregator ( values ) return result
9067	def _lml_optimal_scale ( self ) : assert self . _optimal [ "scale" ] n = len ( self . _y ) lml = - self . _df * log2pi - self . _df - n * log ( self . scale ) lml -= sum ( npsum ( log ( D ) ) for D in self . _D ) return lml / 2
13101	def main ( ) : config = Config ( ) core = HostSearch ( ) hosts = core . get_hosts ( tags = [ '!nessus' ] , up = True ) hosts = [ host for host in hosts ] host_ips = "," . join ( [ str ( host . address ) for host in hosts ] ) url = config . get ( 'nessus' , 'host' ) access = config . get ( 'nessus' , 'access_key' ) secret = config . get ( 'nessus' , 'secret_key' ) template_name = config . get ( 'nessus' , 'template_name' ) nessus = Nessus ( access , secret , url , template_name ) scan_id = nessus . create_scan ( host_ips ) nessus . start_scan ( scan_id ) for host in hosts : host . add_tag ( 'nessus' ) host . save ( ) Logger ( ) . log ( "nessus" , "Nessus scan started on {} hosts" . format ( len ( hosts ) ) , { 'scanned_hosts' : len ( hosts ) } )
6895	def pwd_phasebin ( phases , mags , binsize = 0.002 , minbin = 9 ) : bins = np . arange ( 0.0 , 1.0 , binsize ) binnedphaseinds = npdigitize ( phases , bins ) binnedphases , binnedmags = [ ] , [ ] for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_phases = phases [ thisbin_inds ] thisbin_mags = mags [ thisbin_inds ] if thisbin_inds . size > minbin : binnedphases . append ( npmedian ( thisbin_phases ) ) binnedmags . append ( npmedian ( thisbin_mags ) ) return np . array ( binnedphases ) , np . array ( binnedmags )
13194	def remove_comments ( tex_source ) : # Expression via http://stackoverflow.com/a/13365453 return re . sub ( r'(?<!\\)%.*$' , r'' , tex_source , flags = re . M )
13212	def build_jsonld ( self , url = None , code_url = None , ci_url = None , readme_url = None , license_id = None ) : jsonld = { '@context' : [ "https://raw.githubusercontent.com/codemeta/codemeta/2.0-rc/" "codemeta.jsonld" , "http://schema.org" ] , '@type' : [ 'Report' , 'SoftwareSourceCode' ] , 'language' : 'TeX' , 'reportNumber' : self . handle , 'name' : self . plain_title , 'description' : self . plain_abstract , 'author' : [ { '@type' : 'Person' , 'name' : author_name } for author_name in self . plain_authors ] , # This is a datetime.datetime; not a string. If writing to a file, # Need to convert this to a ISO 8601 string. 'dateModified' : self . revision_datetime } try : jsonld [ 'articleBody' ] = self . plain_content jsonld [ 'fileFormat' ] = 'text/plain' # MIME type of articleBody except RuntimeError : # raised by pypandoc when it can't convert the tex document self . _logger . exception ( 'Could not convert latex body to plain ' 'text for articleBody.' ) self . _logger . warning ( 'Falling back to tex source for articleBody' ) jsonld [ 'articleBody' ] = self . _tex jsonld [ 'fileFormat' ] = 'text/plain' # no mimetype for LaTeX? if url is not None : jsonld [ '@id' ] = url jsonld [ 'url' ] = url else : # Fallback to using the document handle as the ID. This isn't # entirely ideal from a linked data perspective. jsonld [ '@id' ] = self . handle if code_url is not None : jsonld [ 'codeRepository' ] = code_url if ci_url is not None : jsonld [ 'contIntegration' ] = ci_url if readme_url is not None : jsonld [ 'readme' ] = readme_url if license_id is not None : jsonld [ 'license_id' ] = None return jsonld
9204	def path_to_node ( tree , path ) : if path is None : return None node = tree for key in path : node = child_by_key ( node , key ) return node
12737	def parse_amc ( source ) : lines = 0 frames = 1 frame = { } degrees = False for line in source : lines += 1 line = line . split ( '#' ) [ 0 ] . strip ( ) if not line : continue if line . startswith ( ':' ) : if line . lower ( ) . startswith ( ':deg' ) : degrees = True continue if line . isdigit ( ) : if int ( line ) != frames : raise RuntimeError ( 'frame mismatch on line {}: ' 'produced {} but file claims {}' . format ( lines , frames , line ) ) yield frame frames += 1 frame = { } continue fields = line . split ( ) frame [ fields [ 0 ] ] = list ( map ( float , fields [ 1 : ] ) )
13457	def upload_s3 ( file_path , bucket_name , file_key , force = False , acl = 'private' ) : file_path = path ( file_path ) bucket = open_s3 ( bucket_name ) if file_path . isdir ( ) : # Upload the contents of the dir path. paths = file_path . listdir ( ) paths_keys = list ( zip ( paths , [ '%s/%s' % ( file_key , p . name ) for p in paths ] ) ) else : # Upload just the given file path. paths_keys = [ ( file_path , file_key ) ] for p , k in paths_keys : headers = { } s3_key = bucket . get_key ( k ) if not s3_key : from boto . s3 . key import Key s3_key = Key ( bucket , k ) content_type = mimetypes . guess_type ( p ) [ 0 ] if content_type : headers [ 'Content-Type' ] = content_type file_size = p . stat ( ) . st_size file_data = p . bytes ( ) file_md5 , file_md5_64 = s3_key . get_md5_from_hexdigest ( hashlib . md5 ( file_data ) . hexdigest ( ) ) # Check the hash. if s3_key . etag : s3_md5 = s3_key . etag . replace ( '"' , '' ) if s3_md5 == file_md5 : info ( 'Hash is the same. Skipping %s' % file_path ) continue elif not force : # Check if file on S3 is older than local file. s3_datetime = datetime . datetime ( * time . strptime ( s3_key . last_modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local_datetime = datetime . datetime . utcfromtimestamp ( p . stat ( ) . st_mtime ) if local_datetime < s3_datetime : info ( "File %s hasn't been modified since last " "being uploaded" % ( file_key ) ) continue # File is newer, let's process and upload info ( "Uploading %s..." % ( file_key ) ) try : s3_key . set_contents_from_string ( file_data , headers , policy = acl , replace = True , md5 = ( file_md5 , file_md5_64 ) ) except Exception as e : error ( "Failed: %s" % e ) raise
5357	def _add_to_conf ( self , new_conf ) : for section in new_conf : if section not in self . conf : self . conf [ section ] = new_conf [ section ] else : for param in new_conf [ section ] : self . conf [ section ] [ param ] = new_conf [ section ] [ param ]
9682	def set_fan_power ( self , power ) : # Check to make sure the value is a single byte if power > 255 : raise ValueError ( "The fan power should be a single byte (0-255)." ) # Send the command byte and wait 10 ms a = self . cnxn . xfer ( [ 0x42 ] ) [ 0 ] sleep ( 10e-3 ) # Send the next two bytes b = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] c = self . cnxn . xfer ( [ power ] ) [ 0 ] sleep ( 0.1 ) return True if a == 0xF3 and b == 0x42 and c == 0x00 else False
1930	def add ( self , name : str , default = None , description : str = None ) : if name in self . _vars : raise ConfigError ( f"{self.name}.{name} already defined." ) if name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default ) self . _vars [ name ] = v
13475	def start ( self ) : assert not self . has_started ( ) , "called start() on an active GeventLoop" self . _stop_event = Event ( ) # note that we don't use safe_greenlets.spawn because we take care of it in _loop by ourselves self . _greenlet = gevent . spawn ( self . _loop )
12688	def send_now ( users , label , extra_context = None , sender = None ) : sent = False if extra_context is None : extra_context = { } notice_type = NoticeType . objects . get ( label = label ) current_language = get_language ( ) for user in users : # get user language for user from language store defined in # NOTIFICATION_LANGUAGE_MODULE setting try : language = get_notification_language ( user ) except LanguageStoreNotAvailable : language = None if language is not None : # activate the user's language activate ( language ) for backend in NOTIFICATION_BACKENDS . values ( ) : if backend . can_send ( user , notice_type ) : backend . deliver ( user , sender , notice_type , extra_context ) sent = True # reset environment to original language activate ( current_language ) return sent
5470	def _prepare_summary_table ( rows ) : if not rows : return [ ] # We either group on the job-name (if present) or fall back to the job-id key_field = 'job-name' if key_field not in rows [ 0 ] : key_field = 'job-id' # Group each of the rows based on (job-name or job-id, status) grouped = collections . defaultdict ( lambda : collections . defaultdict ( lambda : [ ] ) ) for row in rows : grouped [ row . get ( key_field , '' ) ] [ row . get ( 'status' , '' ) ] += [ row ] # Now that we have the rows grouped, create a summary table. # Use the original table as the driver in order to preserve the order. new_rows = [ ] for job_key in sorted ( grouped . keys ( ) ) : group = grouped . get ( job_key , None ) canonical_status = [ 'RUNNING' , 'SUCCESS' , 'FAILURE' , 'CANCEL' ] # Written this way to ensure that if somehow a new status is introduced, # it shows up in our output. for status in canonical_status + sorted ( group . keys ( ) ) : if status not in group : continue task_count = len ( group [ status ] ) del group [ status ] if task_count : summary_row = collections . OrderedDict ( ) summary_row [ key_field ] = job_key summary_row [ 'status' ] = status summary_row [ 'task-count' ] = task_count new_rows . append ( summary_row ) return new_rows
10139	def encrypt_files ( selected_host , only_link , file_name ) : if ENCRYPTION_DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source_filename = file_name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source_filename ) encrypted_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted_data = encrypted_output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload_files ( encrypted_data , selected_host , only_link , file_name ) + '#' + passphrase
5303	def parse_json_color_file ( path ) : with open ( path , "r" ) as color_file : color_list = json . load ( color_file ) # transform raw color list into color dict color_dict = { c [ "name" ] : c [ "hex" ] for c in color_list } return color_dict
8721	def operation_download ( uploader , sources ) : sources , destinations = destination_from_source ( sources , False ) print ( 'sources' , sources ) print ( 'destinations' , destinations ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : uploader . read_file ( filename , dst ) else : raise Exception ( 'You must specify a destination filename for each file you want to download.' ) log . info ( 'All done!' )
7327	def parse_int_list ( string ) : integers = [ ] for comma_part in string . split ( "," ) : for substring in comma_part . split ( " " ) : if len ( substring ) == 0 : continue if "-" in substring : left , right = substring . split ( "-" ) left_val = int ( left . strip ( ) ) right_val = int ( right . strip ( ) ) integers . extend ( range ( left_val , right_val + 1 ) ) else : integers . append ( int ( substring . strip ( ) ) ) return integers
7979	def _try_auth ( self ) : if self . authenticated : self . __logger . debug ( "try_auth: already authenticated" ) return self . __logger . debug ( "trying auth: %r" % ( self . _auth_methods_left , ) ) if not self . _auth_methods_left : raise LegacyAuthenticationError ( "No allowed authentication methods available" ) method = self . _auth_methods_left [ 0 ] if method . startswith ( "sasl:" ) : return ClientStream . _try_auth ( self ) elif method not in ( "plain" , "digest" ) : self . _auth_methods_left . pop ( 0 ) self . __logger . debug ( "Skipping unknown auth method: %s" % method ) return self . _try_auth ( ) elif self . available_auth_methods is not None : if method in self . available_auth_methods : self . _auth_methods_left . pop ( 0 ) self . auth_method_used = method if method == "digest" : self . _digest_auth_stage2 ( self . auth_stanza ) else : self . _plain_auth_stage2 ( self . auth_stanza ) self . auth_stanza = None return else : self . __logger . debug ( "Skipping unavailable auth method: %s" % method ) else : self . _auth_stage1 ( )
647	def generateSimpleSequences ( nCoinc = 10 , seqLength = [ 5 , 6 , 7 ] , nSeq = 100 ) : coincList = range ( nCoinc ) seqList = [ ] for i in xrange ( nSeq ) : if max ( seqLength ) <= nCoinc : seqList . append ( random . sample ( coincList , random . choice ( seqLength ) ) ) else : len = random . choice ( seqLength ) seq = [ ] for x in xrange ( len ) : seq . append ( random . choice ( coincList ) ) seqList . append ( seq ) return seqList
7595	def get_clan ( self , * tags : crtag , * * params : keys ) : url = self . api . CLAN + '/' + ',' . join ( tags ) return self . _get_model ( url , FullClan , * * params )
11957	def is_oct ( ip ) : try : dec = int ( str ( ip ) , 8 ) except ( TypeError , ValueError ) : return False if dec > 0o37777777777 or dec < 0 : return False return True
8402	def rescale_max ( x , to = ( 0 , 1 ) , _from = None ) : array_like = True try : len ( x ) except TypeError : array_like = False x = [ x ] if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) if _from is None : _from = np . array ( [ np . min ( x ) , np . max ( x ) ] ) out = x / _from [ 1 ] * to [ 1 ] if not array_like : out = out [ 0 ] return out
5459	def find_task_descriptor ( self , task_id ) : # It is not guaranteed that the index will be task_id - 1 when --tasks is # used with a min/max range. for task_descriptor in self . task_descriptors : if task_descriptor . task_metadata . get ( 'task-id' ) == task_id : return task_descriptor return None
10308	def barh ( d , plt , title = None ) : labels = sorted ( d , key = d . get ) index = range ( len ( labels ) ) plt . yticks ( index , labels ) plt . barh ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
3917	def from_conversation_event ( conversation , conv_event , prev_conv_event , datetimefmt , watermark_users = None ) : user = conversation . get_user ( conv_event . user_id ) # Check whether the previous event occurred on the same day as this # event. if prev_conv_event is not None : is_new_day = ( conv_event . timestamp . astimezone ( tz = None ) . date ( ) != prev_conv_event . timestamp . astimezone ( tz = None ) . date ( ) ) else : is_new_day = False if isinstance ( conv_event , hangups . ChatMessageEvent ) : return MessageWidget ( conv_event . timestamp , conv_event . text , datetimefmt , user , show_date = is_new_day , watermark_users = watermark_users ) elif isinstance ( conv_event , hangups . RenameEvent ) : if conv_event . new_name == '' : text = ( '{} cleared the conversation name' . format ( user . first_name ) ) else : text = ( '{} renamed the conversation to {}' . format ( user . first_name , conv_event . new_name ) ) return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users ) elif isinstance ( conv_event , hangups . MembershipChangeEvent ) : event_users = [ conversation . get_user ( user_id ) for user_id in conv_event . participant_ids ] names = ', ' . join ( [ user . full_name for user in event_users ] ) if conv_event . type_ == hangups . MEMBERSHIP_CHANGE_TYPE_JOIN : text = ( '{} added {} to the conversation' . format ( user . first_name , names ) ) else : # LEAVE text = ( '{} left the conversation' . format ( names ) ) return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users ) elif isinstance ( conv_event , hangups . HangoutEvent ) : text = { hangups . HANGOUT_EVENT_TYPE_START : ( 'A Hangout call is starting.' ) , hangups . HANGOUT_EVENT_TYPE_END : ( 'A Hangout call ended.' ) , hangups . HANGOUT_EVENT_TYPE_ONGOING : ( 'A Hangout call is ongoing.' ) , } . get ( conv_event . event_type , 'Unknown Hangout call event.' ) return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users ) elif isinstance ( conv_event , hangups . GroupLinkSharingModificationEvent ) : status_on = hangups . GROUP_LINK_SHARING_STATUS_ON status_text = ( 'on' if conv_event . new_status == status_on else 'off' ) text = '{} turned {} joining by link.' . format ( user . first_name , status_text ) return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users ) else : # conv_event is a generic hangups.ConversationEvent. text = 'Unknown conversation event' return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users )
2845	def enumerate_device_serials ( vid = FT232H_VID , pid = FT232H_PID ) : try : # Create a libftdi context. ctx = None ctx = ftdi . new ( ) # Enumerate FTDI devices. device_list = None count , device_list = ftdi . usb_find_all ( ctx , vid , pid ) if count < 0 : raise RuntimeError ( 'ftdi_usb_find_all returned error {0}: {1}' . format ( count , ftdi . get_error_string ( self . _ctx ) ) ) # Walk through list of devices and assemble list of serial numbers. devices = [ ] while device_list is not None : # Get USB device strings and add serial to list of devices. ret , manufacturer , description , serial = ftdi . usb_get_strings ( ctx , device_list . dev , 256 , 256 , 256 ) if serial is not None : devices . append ( serial ) device_list = device_list . next return devices finally : # Make sure to clean up list and context when done. if device_list is not None : ftdi . list_free ( device_list ) if ctx is not None : ftdi . free ( ctx )
7456	def splitfiles ( data , raws , ipyclient ) : ## create a tmpdir for chunked_files and a chunk optimizer tmpdir = os . path . join ( data . paramsdict [ "project_dir" ] , "tmp-chunks-" + data . name ) if os . path . exists ( tmpdir ) : shutil . rmtree ( tmpdir ) os . makedirs ( tmpdir ) ## chunk into 8M reads totalreads = estimate_optim ( data , raws [ 0 ] [ 0 ] , ipyclient ) optim = int ( 8e6 ) njobs = int ( totalreads / ( optim / 4. ) ) * len ( raws ) ## if more files than cpus: no chunking nosplit = 0 if ( len ( raws ) > len ( ipyclient ) ) or ( totalreads < optim ) : nosplit = 1 ## send slices N at a time. The dict chunkfiles stores a tuple of rawpairs ## dictionary to store asyncresults for sorting jobs start = time . time ( ) chunkfiles = { } for fidx , tups in enumerate ( raws ) : handle = os . path . splitext ( os . path . basename ( tups [ 0 ] ) ) [ 0 ] ## if number of lines is > 20M then just submit it if nosplit : chunkfiles [ handle ] = [ tups ] else : ## chunk the file using zcat_make_temps chunklist = zcat_make_temps ( data , tups , fidx , tmpdir , optim , njobs , start ) chunkfiles [ handle ] = chunklist if not nosplit : print ( "" ) return chunkfiles
13781	def FindExtensionByName ( self , full_name ) : full_name = _NormalizeFullyQualifiedName ( full_name ) message_name , _ , extension_name = full_name . rpartition ( '.' ) try : # Most extensions are nested inside a message. scope = self . FindMessageTypeByName ( message_name ) except KeyError : # Some extensions are defined at file scope. scope = self . FindFileContainingSymbol ( full_name ) return scope . extensions_by_name [ extension_name ]
1236	def get_named_tensor ( self , name ) : if name in self . named_tensors : return True , self . named_tensors [ name ] else : return False , None
4527	def report ( function , * args , * * kwds ) : try : function ( * args , * * kwds ) except Exception : traceback . print_exc ( )
1499	def ack ( self , tup ) : if not isinstance ( tup , HeronTuple ) : Log . error ( "Only HeronTuple type is supported in ack()" ) return if self . acking_enabled : ack_tuple = tuple_pb2 . AckTuple ( ) ack_tuple . ackedtuple = int ( tup . id ) tuple_size_in_bytes = 0 for rt in tup . roots : to_add = ack_tuple . roots . add ( ) to_add . CopyFrom ( rt ) tuple_size_in_bytes += rt . ByteSize ( ) super ( BoltInstance , self ) . admit_control_tuple ( ack_tuple , tuple_size_in_bytes , True ) process_latency_ns = ( time . time ( ) - tup . creation_time ) * system_constants . SEC_TO_NS self . pplan_helper . context . invoke_hook_bolt_ack ( tup , process_latency_ns ) self . bolt_metrics . acked_tuple ( tup . stream , tup . component , process_latency_ns )
6327	def get_count ( self , ngram , corpus = None ) : if not corpus : corpus = self . ngcorpus # if ngram is empty, we're at our leaf node and should return the # value in None if not ngram : return corpus [ None ] # support strings or lists/tuples by splitting strings if isinstance ( ngram , ( text_type , str ) ) : ngram = text_type ( ngram ) . split ( ) # if ngram is not empty, check whether the next element is in the # corpus; if so, recurse--if not, return 0 if ngram [ 0 ] in corpus : return self . get_count ( ngram [ 1 : ] , corpus [ ngram [ 0 ] ] ) return 0
8152	def hex_to_rgb ( hex ) : hex = hex . lstrip ( "#" ) if len ( hex ) < 6 : hex += hex [ - 1 ] * ( 6 - len ( hex ) ) if len ( hex ) == 6 : r , g , b = hex [ 0 : 2 ] , hex [ 2 : 4 ] , hex [ 4 : ] r , g , b = [ int ( n , 16 ) / 255.0 for n in ( r , g , b ) ] a = 1.0 elif len ( hex ) == 8 : r , g , b , a = hex [ 0 : 2 ] , hex [ 2 : 4 ] , hex [ 4 : 6 ] , hex [ 6 : ] r , g , b , a = [ int ( n , 16 ) / 255.0 for n in ( r , g , b , a ) ] return r , g , b , a
13124	def object_to_id ( self , obj ) : search = Service . search ( ) search = search . filter ( "term" , address = obj . address ) search = search . filter ( "term" , protocol = obj . protocol ) search = search . filter ( "term" , port = obj . port ) search = search . filter ( "term" , state = obj . state ) if search . count ( ) : result = search [ 0 ] . execute ( ) [ 0 ] return result . meta . id else : return None
7545	def calculate_depths ( data , samples , lbview ) : ## send jobs to be processed on engines start = time . time ( ) printstr = " calculating depths | {} | s5 |" recaljobs = { } maxlens = [ ] for sample in samples : recaljobs [ sample . name ] = lbview . apply ( recal_hidepth , * ( data , sample ) ) ## block until finished while 1 : ready = [ i . ready ( ) for i in recaljobs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break ## check for failures and collect results modsamples = [ ] for sample in samples : if not recaljobs [ sample . name ] . successful ( ) : LOGGER . error ( " sample %s failed: %s" , sample . name , recaljobs [ sample . name ] . exception ( ) ) else : modsample , _ , maxlen , _ , _ = recaljobs [ sample . name ] . result ( ) modsamples . append ( modsample ) maxlens . append ( maxlen ) ## reset global maxlen if something changed data . _hackersonly [ "max_fragment_length" ] = int ( max ( maxlens ) ) + 4 return samples
8325	def buildTagMap ( default , * args ) : built = { } for portion in args : if hasattr ( portion , 'items' ) : #It's a map. Merge it. for k , v in portion . items ( ) : built [ k ] = v elif isList ( portion ) : #It's a list. Map each item to the default. for k in portion : built [ k ] = default else : #It's a scalar. Map it to the default. built [ portion ] = default return built
8342	def _convertEntities ( self , match ) : x = match . group ( 1 ) if self . convertHTMLEntities and x in name2codepoint : return unichr ( name2codepoint [ x ] ) elif x in self . XML_ENTITIES_TO_SPECIAL_CHARS : if self . convertXMLEntities : return self . XML_ENTITIES_TO_SPECIAL_CHARS [ x ] else : return u'&%s;' % x elif len ( x ) > 0 and x [ 0 ] == '#' : # Handle numeric entities if len ( x ) > 1 and x [ 1 ] == 'x' : return unichr ( int ( x [ 2 : ] , 16 ) ) else : return unichr ( int ( x [ 1 : ] ) ) elif self . escapeUnrecognizedEntities : return u'&amp;%s;' % x else : return u'&%s;' % x
9763	def upload ( sync = True ) : # pylint:disable=assign-to-new-keyword project = ProjectManager . get_config_or_raise ( ) files = IgnoreManager . get_unignored_file_paths ( ) try : with create_tarfile ( files , project . name ) as file_path : with get_files_in_current_directory ( 'repo' , [ file_path ] ) as ( files , files_size ) : try : PolyaxonClient ( ) . project . upload_repo ( project . user , project . name , files , files_size , sync = sync ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not upload code for project `{}`.' . format ( project . name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) Printer . print_error ( 'Check the project exists, ' 'and that you have access rights, ' 'this could happen as well when uploading large files.' 'If you are running a notebook and mounting the code to the notebook, ' 'you should stop it before uploading.' ) sys . exit ( 1 ) Printer . print_success ( 'Files uploaded.' ) except Exception as e : Printer . print_error ( "Could not upload the file." ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
2820	def convert_instancenorm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting instancenorm ...' ) if names == 'short' : tf_name = 'IN' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) assert ( len ( inputs ) == 3 ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) # Use previously taken constants if inputs [ - 2 ] + '_np' in layers : gamma = layers [ inputs [ - 2 ] + '_np' ] else : gamma = weights [ weights_name ] . numpy ( ) if inputs [ - 1 ] + '_np' in layers : beta = layers [ inputs [ - 1 ] + '_np' ] else : beta = weights [ bias_name ] . numpy ( ) def target_layer ( x , epsilon = params [ 'epsilon' ] , gamma = gamma , beta = beta ) : layer = tf . contrib . layers . instance_norm ( x , param_initializers = { 'beta' : tf . constant_initializer ( beta ) , 'gamma' : tf . constant_initializer ( gamma ) } , epsilon = epsilon , data_format = 'NCHW' , trainable = False ) return layer lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
813	def read ( cls , proto ) : tm = super ( TemporalMemoryMonitorMixin , cls ) . read ( proto ) # initialize `TemporalMemoryMonitorMixin` attributes tm . mmName = None tm . _mmTraces = None tm . _mmData = None tm . mmClearHistory ( ) tm . _mmResetActive = True return tm
2293	def eval_entropy ( x ) : hx = 0. sx = sorted ( x ) for i , j in zip ( sx [ : - 1 ] , sx [ 1 : ] ) : delta = j - i if bool ( delta ) : hx += np . log ( np . abs ( delta ) ) hx = hx / ( len ( x ) - 1 ) + psi ( len ( x ) ) - psi ( 1 ) return hx
5930	def _canonicalize ( self , filename ) : path , ext = os . path . splitext ( filename ) if not ext : ext = ".collection" return path + ext
4695	def execute ( cmd = None , shell = True , echo = True ) : if echo : cij . emph ( "cij.util.execute: shell: %r, cmd: %r" % ( shell , cmd ) ) rcode = 1 stdout , stderr = ( "" , "" ) if cmd : if shell : cmd = " " . join ( cmd ) proc = Popen ( cmd , stdout = PIPE , stderr = PIPE , shell = shell , close_fds = True ) stdout , stderr = proc . communicate ( ) rcode = proc . returncode if rcode and echo : cij . warn ( "cij.util.execute: stdout: %s" % stdout ) cij . err ( "cij.util.execute: stderr: %s" % stderr ) cij . err ( "cij.util.execute: rcode: %s" % rcode ) return rcode , stdout , stderr
11807	def encode ( plaintext , code ) : from string import maketrans trans = maketrans ( alphabet + alphabet . upper ( ) , code + code . upper ( ) ) return plaintext . translate ( trans )
7386	def find_node_group_membership ( self , node ) : for group , nodelist in self . nodes . items ( ) : if node in nodelist : return group
8935	def provider ( workdir , commit = True , * * kwargs ) : return SCM_PROVIDER [ auto_detect ( workdir ) ] ( workdir , commit = commit , * * kwargs )
10329	def get_path_effect ( graph , path , relationship_dict ) : causal_effect = [ ] for predecessor , successor in pairwise ( path ) : if pair_has_contradiction ( graph , predecessor , successor ) : return Effect . ambiguous edges = graph . get_edge_data ( predecessor , successor ) edge_key , edge_relation , _ = rank_edges ( edges ) relation = graph [ predecessor ] [ successor ] [ edge_key ] [ RELATION ] # Returns Effect.no_effect if there is a non causal edge in path if relation not in relationship_dict or relationship_dict [ relation ] == 0 : return Effect . no_effect causal_effect . append ( relationship_dict [ relation ] ) final_effect = reduce ( lambda x , y : x * y , causal_effect ) return Effect . activation if final_effect == 1 else Effect . inhibition
1284	def footnote_ref ( self , key , index ) : html = ( '<sup class="footnote-ref" id="fnref-%s">' '<a href="#fn-%s">%d</a></sup>' ) % ( escape ( key ) , escape ( key ) , index ) return html
2359	def t_intnumber ( self , t ) : t . value = int ( t . value ) t . type = 'NUMBER' return t
12005	def _remove_header ( self , data , options ) : version_info = self . _get_version_info ( options [ 'version' ] ) header_size = version_info [ 'header_size' ] if options [ 'flags' ] [ 'timestamp' ] : header_size += version_info [ 'timestamp_size' ] data = data [ header_size : ] return data
11703	def set_gender ( self , gender = None ) : if gender and gender in genders : self . gender = gender else : if not self . chromosomes : self . set_chromosomes ( ) self . gender = npchoice ( genders , 1 , p = p_gender [ self . chromosomes ] ) [ 0 ]
4971	def clean ( self ) : super ( EnterpriseCustomerIdentityProviderAdminForm , self ) . clean ( ) provider_id = self . cleaned_data . get ( 'provider_id' , None ) enterprise_customer = self . cleaned_data . get ( 'enterprise_customer' , None ) if provider_id is None or enterprise_customer is None : # field validation for either provider_id or enterprise_customer has already raised # a validation error. return identity_provider = utils . get_identity_provider ( provider_id ) if not identity_provider : # This should not happen, as identity providers displayed in drop down are fetched dynamically. message = _ ( "The specified Identity Provider does not exist. For more " "information, contact a system administrator." , ) # Log message for debugging logger . exception ( message ) raise ValidationError ( message ) if identity_provider and identity_provider . site != enterprise_customer . site : raise ValidationError ( _ ( "The site for the selected identity provider " "({identity_provider_site}) does not match the site for " "this enterprise customer ({enterprise_customer_site}). " "To correct this problem, select a site that has a domain " "of '{identity_provider_site}', or update the identity " "provider to '{enterprise_customer_site}'." ) . format ( enterprise_customer_site = enterprise_customer . site , identity_provider_site = identity_provider . site , ) , )
9093	def _update_namespace ( self , namespace : Namespace ) -> None : old_entry_identifiers = self . _get_old_entry_identifiers ( namespace ) new_count = 0 skip_count = 0 for model in self . _iterate_namespace_models ( ) : if self . _get_identifier ( model ) in old_entry_identifiers : continue entry = self . _create_namespace_entry_from_model ( model , namespace = namespace ) if entry is None or entry . name is None : skip_count += 1 continue new_count += 1 self . session . add ( entry ) t = time . time ( ) log . info ( 'got %d new entries. skipped %d entries missing names. committing models' , new_count , skip_count ) self . session . commit ( ) log . info ( 'committed models in %.2f seconds' , time . time ( ) - t )
11619	def _setup ( ) : s = str . split if sys . version_info < ( 3 , 0 ) : # noinspection PyUnresolvedReferences s = unicode . split def pop_all ( some_dict , some_list ) : for scheme in some_list : some_dict . pop ( scheme ) global SCHEMES SCHEMES = copy . deepcopy ( sanscript . SCHEMES ) pop_all ( SCHEMES , [ sanscript . ORIYA , sanscript . BENGALI , sanscript . GUJARATI ] ) SCHEMES [ HK ] . update ( { 'vowels' : s ( """a A i I u U R RR lR lRR E ai O au""" ) + s ( """e o""" ) , 'marks' : s ( """A i I u U R RR lR lRR E ai O au""" ) + s ( """e o""" ) , 'consonants' : sanscript . SCHEMES [ HK ] [ 'consonants' ] + s ( """n2 r2 zh""" ) } ) SCHEMES [ ITRANS ] . update ( { 'vowels' : s ( """a A i I u U R RR LLi LLI E ai O au""" ) + s ( """e o""" ) , 'marks' : s ( """A i I u U R RR LLi LLI E ai O au""" ) + s ( """e o""" ) , 'consonants' : sanscript . SCHEMES [ ITRANS ] [ 'consonants' ] + s ( """n2 r2 zh""" ) } ) pop_all ( SCHEMES [ ITRANS ] . synonym_map , s ( """e o""" ) ) SCHEMES [ OPTITRANS ] . update ( { 'vowels' : s ( """a A i I u U R RR LLi LLI E ai O au""" ) + s ( """e o""" ) , 'marks' : s ( """A i I u U R RR LLi LLI E ai O au""" ) + s ( """e o""" ) , 'consonants' : sanscript . SCHEMES [ OPTITRANS ] [ 'consonants' ] + s ( """n2 r2 zh""" ) } ) pop_all ( SCHEMES [ OPTITRANS ] . synonym_map , s ( """e o""" ) )
293	def plot_gross_leverage ( returns , positions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) gl = timeseries . gross_lev ( positions ) gl . plot ( lw = 0.5 , color = 'limegreen' , legend = False , ax = ax , * * kwargs ) ax . axhline ( gl . mean ( ) , color = 'g' , linestyle = '--' , lw = 3 ) ax . set_title ( 'Gross leverage' ) ax . set_ylabel ( 'Gross leverage' ) ax . set_xlabel ( '' ) return ax
2473	def set_lic_id ( self , doc , lic_id ) : # FIXME: this state does not make sense self . reset_extr_lics ( ) if validations . validate_extracted_lic_id ( lic_id ) : doc . add_extr_lic ( document . ExtractedLicense ( lic_id ) ) return True else : raise SPDXValueError ( 'ExtractedLicense::id' )
10843	def pending ( self ) : pending_updates = [ ] url = PATHS [ 'GET_PENDING' ] % self . profile_id response = self . api . get ( url = url ) for update in response [ 'updates' ] : pending_updates . append ( Update ( api = self . api , raw_response = update ) ) self . __pending = pending_updates return self . __pending
7154	def prepare_options ( options ) : options_ , verbose_options = [ ] , [ ] for option in options : if is_string ( option ) : options_ . append ( option ) verbose_options . append ( option ) else : options_ . append ( option [ 0 ] ) verbose_options . append ( option [ 1 ] ) return options_ , verbose_options
5107	def fetch_data ( self , return_header = False ) : qdata = [ ] for d in self . data . values ( ) : qdata . extend ( d ) dat = np . zeros ( ( len ( qdata ) , 6 ) ) if len ( qdata ) > 0 : dat [ : , : 5 ] = np . array ( qdata ) dat [ : , 5 ] = self . edge [ 2 ] dType = [ ( 'a' , float ) , ( 's' , float ) , ( 'd' , float ) , ( 'q' , float ) , ( 'n' , float ) , ( 'id' , float ) ] dat = np . array ( [ tuple ( d ) for d in dat ] , dtype = dType ) dat = np . sort ( dat , order = 'a' ) dat = np . array ( [ tuple ( d ) for d in dat ] ) if return_header : return dat , 'arrival,service,departure,num_queued,num_total,q_id' return dat
4393	def adsSyncWriteReqEx ( port , address , index_group , index_offset , value , plc_data_type ) : # type: (int, AmsAddr, int, int, Any, Type) -> None sync_write_request = _adsDLL . AdsSyncWriteReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) index_group_c = ctypes . c_ulong ( index_group ) index_offset_c = ctypes . c_ulong ( index_offset ) if plc_data_type == PLCTYPE_STRING : data = ctypes . c_char_p ( value . encode ( "utf-8" ) ) data_pointer = data # type: Union[ctypes.c_char_p, ctypes.pointer] data_length = len ( data_pointer . value ) + 1 # type: ignore else : if type ( plc_data_type ) . __name__ == "PyCArrayType" : data = plc_data_type ( * value ) else : data = plc_data_type ( value ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . sizeof ( data ) error_code = sync_write_request ( port , ams_address_pointer , index_group_c , index_offset_c , data_length , data_pointer , ) if error_code : raise ADSError ( error_code )
713	def __startSearch ( self ) : # This search uses a pre-existing permutations script params = _ClientJobUtils . makeSearchJobParamsDict ( options = self . _options , forRunning = True ) if self . _options [ "action" ] == "dryRun" : args = [ sys . argv [ 0 ] , "--params=%s" % ( json . dumps ( params ) ) ] print print "==================================================================" print "RUNNING PERMUTATIONS INLINE as \"DRY RUN\"..." print "==================================================================" jobID = hypersearch_worker . main ( args ) else : cmdLine = _setUpExports ( self . _options [ "exports" ] ) # Begin the new search. The {JOBID} string is replaced by the actual # jobID returned from jobInsert. cmdLine += "$HYPERSEARCH" maxWorkers = self . _options [ "maxWorkers" ] jobID = self . __cjDAO . jobInsert ( client = "GRP" , cmdLine = cmdLine , params = json . dumps ( params ) , minimumWorkers = 1 , maximumWorkers = maxWorkers , jobType = self . __cjDAO . JOB_TYPE_HS ) cmdLine = "python -m nupic.swarming.hypersearch_worker" " --jobID=%d" % ( jobID ) self . _launchWorkers ( cmdLine , maxWorkers ) searchJob = _HyperSearchJob ( jobID ) # Save search ID to file (this is used for report generation) self . __saveHyperSearchJobID ( permWorkDir = self . _options [ "permWorkDir" ] , outputLabel = self . _options [ "outputLabel" ] , hyperSearchJob = searchJob ) if self . _options [ "action" ] == "dryRun" : print "Successfully executed \"dry-run\" hypersearch, jobID=%d" % ( jobID ) else : print "Successfully submitted new HyperSearch job, jobID=%d" % ( jobID ) _emit ( Verbosity . DEBUG , "Each worker executing the command line: %s" % ( cmdLine , ) ) return searchJob
12993	def level_chunker ( text , getreffs , level = 1 ) : references = getreffs ( level = level ) return [ ( ref . split ( ":" ) [ - 1 ] , ref . split ( ":" ) [ - 1 ] ) for ref in references ]
329	def model_returns_t_alpha_beta ( data , bmark , samples = 2000 , progressbar = True ) : data_bmark = pd . concat ( [ data , bmark ] , axis = 1 ) . dropna ( ) with pm . Model ( ) as model : sigma = pm . HalfCauchy ( 'sigma' , beta = 1 ) nu = pm . Exponential ( 'nu_minus_two' , 1. / 10. ) # alpha and beta X = data_bmark . iloc [ : , 1 ] y = data_bmark . iloc [ : , 0 ] alpha_reg = pm . Normal ( 'alpha' , mu = 0 , sd = .1 ) beta_reg = pm . Normal ( 'beta' , mu = 0 , sd = 1 ) mu_reg = alpha_reg + beta_reg * X pm . StudentT ( 'returns' , nu = nu + 2 , mu = mu_reg , sd = sigma , observed = y ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
1957	def _init_arm_kernel_helpers ( self ) : page_data = bytearray ( b'\xf1\xde\xfd\xe7' * 1024 ) # Extracted from a RPi2 preamble = binascii . unhexlify ( 'ff0300ea' + '650400ea' + 'f0ff9fe5' + '430400ea' + '220400ea' + '810400ea' + '000400ea' + '870400ea' ) # XXX(yan): The following implementations of cmpxchg and cmpxchg64 were # handwritten to not use any exclusive instructions (e.g. ldrexd) or # locking. For actual implementations, refer to # arch/arm64/kernel/kuser32.S in the Linux source code. __kuser_cmpxchg64 = binascii . unhexlify ( '30002de9' + # push {r4, r5} '08c09de5' + # ldr ip, [sp, #8] '30009ce8' + # ldm ip, {r4, r5} '010055e1' + # cmp r5, r1 '00005401' + # cmpeq r4, r0 '0100a013' + # movne r0, #1 '0000a003' + # moveq r0, #0 '0c008c08' + # stmeq ip, {r2, r3} '3000bde8' + # pop {r4, r5} '1eff2fe1' # bx lr ) __kuser_dmb = binascii . unhexlify ( '5bf07ff5' + # dmb ish '1eff2fe1' # bx lr ) __kuser_cmpxchg = binascii . unhexlify ( '003092e5' + # ldr r3, [r2] '000053e1' + # cmp r3, r0 '0000a003' + # moveq r0, #0 '00108205' + # streq r1, [r2] '0100a013' + # movne r0, #1 '1eff2fe1' # bx lr ) # Map a TLS segment self . _arm_tls_memory = self . current . memory . mmap ( None , 4 , 'rw ' ) __kuser_get_tls = binascii . unhexlify ( '04009FE5' + # ldr r0, [pc, #4] '010090e8' + # ldm r0, {r0} '1eff2fe1' # bx lr ) + struct . pack ( '<I' , self . _arm_tls_memory ) tls_area = b'\x00' * 12 version = struct . pack ( '<I' , 5 ) def update ( address , code ) : page_data [ address : address + len ( code ) ] = code # Offsets from Documentation/arm/kernel_user_helpers.txt in Linux update ( 0x000 , preamble ) update ( 0xf60 , __kuser_cmpxchg64 ) update ( 0xfa0 , __kuser_dmb ) update ( 0xfc0 , __kuser_cmpxchg ) update ( 0xfe0 , __kuser_get_tls ) update ( 0xff0 , tls_area ) update ( 0xffc , version ) self . current . memory . mmap ( 0xffff0000 , len ( page_data ) , 'r x' , page_data )
3921	def get_menu_widget ( self , close_callback ) : return ConversationMenu ( self . _coroutine_queue , self . _conversation , close_callback , self . _keys )
6732	def add_class_methods_as_module_level_functions_for_fabric ( instance , module_name , method_name , module_alias = None ) : import imp from . decorators import task_or_dryrun # get the module as an object module_obj = sys . modules [ module_name ] module_alias = re . sub ( '[^a-zA-Z0-9]+' , '' , module_alias or '' ) # Iterate over the methods of the class and dynamically create a function # for each method that calls the method and add it to the current module # NOTE: inspect.ismethod actually executes the methods?! #for method in inspect.getmembers(instance, predicate=inspect.ismethod): method_obj = getattr ( instance , method_name ) if not method_name . startswith ( '_' ) : # get the bound method func = getattr ( instance , method_name ) # if module_name == 'buildbot' or module_alias == 'buildbot': # print('-'*80) # print('module_name:', module_name) # print('method_name:', method_name) # print('module_alias:', module_alias) # print('module_obj:', module_obj) # print('func.module:', func.__module__) # Convert executable to a Fabric task, if not done so already. if not hasattr ( func , 'is_task_or_dryrun' ) : func = task_or_dryrun ( func ) if module_name == module_alias or ( module_name . startswith ( 'satchels.' ) and module_name . endswith ( module_alias ) ) : # add the function to the current module setattr ( module_obj , method_name , func ) else : # Dynamically create a module for the virtual satchel. _module_obj = module_obj module_obj = create_module ( module_alias ) setattr ( module_obj , method_name , func ) post_import_modules . add ( module_alias ) fabric_name = '%s.%s' % ( module_alias or module_name , method_name ) func . wrapped . __func__ . fabric_name = fabric_name return func
4469	def _pprint ( params , offset = 0 , printer = repr ) : # Do a multi-line justified repr: options = np . get_printoptions ( ) np . set_printoptions ( precision = 5 , threshold = 64 , edgeitems = 2 ) params_list = list ( ) this_line_length = offset line_sep = ',\n' + ( 1 + offset // 2 ) * ' ' for i , ( k , v ) in enumerate ( sorted ( six . iteritems ( params ) ) ) : if type ( v ) is float : # use str for representing floating point numbers # this way we get consistent representation across # architectures and versions. this_repr = '%s=%s' % ( k , str ( v ) ) else : # use repr of the rest this_repr = '%s=%s' % ( k , printer ( v ) ) if len ( this_repr ) > 500 : this_repr = this_repr [ : 300 ] + '...' + this_repr [ - 100 : ] if i > 0 : if ( this_line_length + len ( this_repr ) >= 75 or '\n' in this_repr ) : params_list . append ( line_sep ) this_line_length = len ( line_sep ) else : params_list . append ( ', ' ) this_line_length += 2 params_list . append ( this_repr ) this_line_length += len ( this_repr ) np . set_printoptions ( * * options ) lines = '' . join ( params_list ) # Strip trailing space to avoid nightmare in doctests lines = '\n' . join ( l . rstrip ( ' ' ) for l in lines . split ( '\n' ) ) return lines
520	def _initPermNonConnected ( self ) : p = self . _synPermConnected * self . _random . getReal64 ( ) # Ensure we don't have too much unnecessary precision. A full 64 bits of # precision causes numerical stability issues across platforms and across # implementations p = int ( p * 100000 ) / 100000.0 return p
12126	def spec_formatter ( cls , spec ) : return type ( spec ) ( ( k , str ( v ) ) for ( k , v ) in spec . items ( ) )
1055	def shuffle ( self , x , random = None ) : if random is None : random = self . random _int = int for i in reversed ( xrange ( 1 , len ( x ) ) ) : # pick an element in x[:i+1] with which to exchange x[i] j = _int ( random ( ) * ( i + 1 ) ) x [ i ] , x [ j ] = x [ j ] , x [ i ]
8773	def get_lswitch_ids_for_network ( self , context , network_id ) : lswitches = self . _lswitches_for_network ( context , network_id ) . results ( ) return [ s [ 'uuid' ] for s in lswitches [ "results" ] ]
1380	def get_version_number ( zipped_pex = False ) : if zipped_pex : release_file = get_zipped_heron_release_file ( ) else : release_file = get_heron_release_file ( ) with open ( release_file ) as release_info : for line in release_info : trunks = line [ : - 1 ] . split ( ' ' ) if trunks [ 0 ] == 'heron.build.version' : return trunks [ - 1 ] . replace ( "'" , "" ) return 'unknown'
11559	def i2c_write ( self , address , * args ) : data = [ address , self . I2C_WRITE ] for item in args : data . append ( item & 0x7f ) data . append ( ( item >> 7 ) & 0x7f ) self . _command_handler . send_sysex ( self . _command_handler . I2C_REQUEST , data )
301	def plot_daily_turnover_hist ( transactions , positions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) turnover = txn . get_turnover ( positions , transactions ) sns . distplot ( turnover , ax = ax , * * kwargs ) ax . set_title ( 'Distribution of daily turnover rates' ) ax . set_xlabel ( 'Turnover rate' ) return ax
5588	def calculate_slope_aspect ( elevation , xres , yres , z = 1.0 , scale = 1.0 ) : z = float ( z ) scale = float ( scale ) height , width = elevation . shape [ 0 ] - 2 , elevation . shape [ 1 ] - 2 window = [ z * elevation [ row : ( row + height ) , col : ( col + width ) ] for ( row , col ) in product ( range ( 3 ) , range ( 3 ) ) ] x = ( ( window [ 0 ] + window [ 3 ] + window [ 3 ] + window [ 6 ] ) - ( window [ 2 ] + window [ 5 ] + window [ 5 ] + window [ 8 ] ) ) / ( 8.0 * xres * scale ) y = ( ( window [ 6 ] + window [ 7 ] + window [ 7 ] + window [ 8 ] ) - ( window [ 0 ] + window [ 1 ] + window [ 1 ] + window [ 2 ] ) ) / ( 8.0 * yres * scale ) # in radians, from 0 to pi/2 slope = math . pi / 2 - np . arctan ( np . sqrt ( x * x + y * y ) ) # in radians counterclockwise, from -pi at north back to pi aspect = np . arctan2 ( x , y ) return slope , aspect
4413	def add ( self , requester : int , track : dict ) : self . queue . append ( AudioTrack ( ) . build ( track , requester ) )
4123	def _twosided_zerolag ( data , zerolag ) : res = twosided ( np . insert ( data , 0 , zerolag ) ) return res
7011	def plot_periodbase_lsp ( lspinfo , outfile = None , plotdpi = 100 ) : # get the lspinfo from a pickle file transparently if isinstance ( lspinfo , str ) and os . path . exists ( lspinfo ) : LOGINFO ( 'loading LSP info from pickle %s' % lspinfo ) with open ( lspinfo , 'rb' ) as infd : lspinfo = pickle . load ( infd ) try : # get the things to plot out of the data periods = lspinfo [ 'periods' ] lspvals = lspinfo [ 'lspvals' ] bestperiod = lspinfo [ 'bestperiod' ] lspmethod = lspinfo [ 'method' ] # make the LSP plot on the first subplot plt . plot ( periods , lspvals ) plt . xscale ( 'log' , basex = 10 ) plt . xlabel ( 'Period [days]' ) plt . ylabel ( PLOTYLABELS [ lspmethod ] ) plottitle = '%s best period: %.6f d' % ( METHODSHORTLABELS [ lspmethod ] , bestperiod ) plt . title ( plottitle ) # show the best five peaks on the plot for bestperiod , bestpeak in zip ( lspinfo [ 'nbestperiods' ] , lspinfo [ 'nbestlspvals' ] ) : plt . annotate ( '%.6f' % bestperiod , xy = ( bestperiod , bestpeak ) , xycoords = 'data' , xytext = ( 0.0 , 25.0 ) , textcoords = 'offset points' , arrowprops = dict ( arrowstyle = "->" ) , fontsize = 'x-small' ) # make a grid plt . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) # make the figure if outfile and isinstance ( outfile , str ) : if outfile . endswith ( '.png' ) : plt . savefig ( outfile , bbox_inches = 'tight' , dpi = plotdpi ) else : plt . savefig ( outfile , bbox_inches = 'tight' ) plt . close ( ) return os . path . abspath ( outfile ) elif dispok : plt . show ( ) plt . close ( ) return else : LOGWARNING ( 'no output file specified and no $DISPLAY set, ' 'saving to lsp-plot.png in current directory' ) outfile = 'lsp-plot.png' plt . savefig ( outfile , bbox_inches = 'tight' , dpi = plotdpi ) plt . close ( ) return os . path . abspath ( outfile ) except Exception as e : LOGEXCEPTION ( 'could not plot this LSP, appears to be empty' ) return
1937	def get_abi ( self , hsh : bytes ) -> Dict [ str , Any ] : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) if sig is not None : return dict ( self . _function_abi_items_by_signature [ sig ] ) item = self . _fallback_function_abi_item if item is not None : return dict ( item ) # An item describing the default fallback function. return { 'payable' : False , 'stateMutability' : 'nonpayable' , 'type' : 'fallback' }
5293	def get_context_data ( self , * * kwargs ) : context = { } inlines_names = self . get_inlines_names ( ) if inlines_names : # We have formset or inlines in context, but never both context . update ( zip ( inlines_names , kwargs . get ( 'inlines' , [ ] ) ) ) if 'formset' in kwargs : context [ inlines_names [ 0 ] ] = kwargs [ 'formset' ] context . update ( kwargs ) return super ( NamedFormsetsMixin , self ) . get_context_data ( * * context )
9531	def value_to_string ( self , obj ) : value = self . value_from_object ( obj ) return b64encode ( self . _dump ( value ) ) . decode ( 'ascii' )
11716	def edit ( self , config , etag ) : data = self . _json_encode ( config ) headers = self . _default_headers ( ) if etag is not None : headers [ "If-Match" ] = etag return self . _request ( self . name , ok_status = None , data = data , headers = headers , method = "PUT" )
4590	def serpentine_y ( x , y , matrix ) : if x % 2 : return x , matrix . rows - 1 - y return x , y
10228	def get_separate_unstable_correlation_triples ( graph : BELGraph ) -> Iterable [ NodeTriple ] : cg = get_correlation_graph ( graph ) for a , b , c in get_correlation_triangles ( cg ) : if POSITIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and NEGATIVE_CORRELATION in cg [ a ] [ c ] : yield b , a , c if POSITIVE_CORRELATION in cg [ a ] [ b ] and NEGATIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield a , b , c if NEGATIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield c , a , b
13689	def add_peer ( self , peer ) : if type ( peer ) == list : for i in peer : check_url ( i ) self . PEERS . extend ( peer ) elif type ( peer ) == str : check_url ( peer ) self . PEERS . append ( peer )
9474	def BFS ( self , root = None ) : if not root : root = self . root ( ) queue = deque ( ) queue . append ( root ) nodes = [ ] while len ( queue ) > 0 : x = queue . popleft ( ) nodes . append ( x ) for child in x . children ( ) : queue . append ( child ) return nodes
2190	def _product_file_hash ( self , product = None ) : if self . hasher is None : return None else : products = self . _rectify_products ( product ) product_file_hash = [ util_hash . hash_file ( p , hasher = self . hasher , base = 'hex' ) for p in products ] return product_file_hash
3080	def get_token ( http , service_account = 'default' ) : token_json = get ( http , 'instance/service-accounts/{0}/token' . format ( service_account ) ) token_expiry = client . _UTCNOW ( ) + datetime . timedelta ( seconds = token_json [ 'expires_in' ] ) return token_json [ 'access_token' ] , token_expiry
12227	def bind_proxy ( values , category = None , field = None , verbose_name = None , help_text = '' , static = True , readonly = False ) : addrs = OrderedDict ( ) depth = 3 for local_name , locals_dict in traverse_local_prefs ( depth ) : addrs [ id ( locals_dict [ local_name ] ) ] = local_name proxies = [ ] locals_dict = get_frame_locals ( depth ) for value in values : # Try to preserve fields order. id_val = id ( value ) if id_val in addrs : local_name = addrs [ id_val ] local_val = locals_dict [ local_name ] if isinstance ( local_val , PatchedLocal ) and not isinstance ( local_val , PrefProxy ) : proxy = PrefProxy ( local_name , value . val , category = category , field = field , verbose_name = verbose_name , help_text = help_text , static = static , readonly = readonly , ) app_name = locals_dict [ '__name__' ] . split ( '.' ) [ - 2 ] # x.y.settings -> y prefs = get_prefs ( ) if app_name not in prefs : prefs [ app_name ] = OrderedDict ( ) prefs [ app_name ] [ local_name . lower ( ) ] = proxy # Replace original pref variable with a proxy. locals_dict [ local_name ] = proxy proxies . append ( proxy ) return proxies
10335	def build_delete_node_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , str ] , None ] : @ in_place_transformation def delete_node_by_hash ( graph : BELGraph , node_hash : str ) -> None : """Remove a node by identifier.""" node = manager . get_dsl_by_hash ( node_hash ) graph . remove_node ( node ) return delete_node_by_hash
4427	async def _seek ( self , ctx , * , time : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_playing : return await ctx . send ( 'Not playing.' ) seconds = time_rx . search ( time ) if not seconds : return await ctx . send ( 'You need to specify the amount of seconds to skip!' ) seconds = int ( seconds . group ( ) ) * 1000 if time . startswith ( '-' ) : seconds *= - 1 track_time = player . position + seconds await player . seek ( track_time ) await ctx . send ( f'Moved track to **{lavalink.Utils.format_time(track_time)}**' )
8720	def operation_upload ( uploader , sources , verify , do_compile , do_file , do_restart ) : sources , destinations = destination_from_source ( sources ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : if do_compile : uploader . file_remove ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) uploader . write_file ( filename , dst , verify ) #init.lua is not allowed to be compiled if do_compile and dst != 'init.lua' : uploader . file_compile ( dst ) uploader . file_remove ( dst ) if do_file : uploader . file_do ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) elif do_file : uploader . file_do ( dst ) else : raise Exception ( 'Error preparing nodemcu for reception' ) else : raise Exception ( 'You must specify a destination filename for each file you want to upload.' ) if do_restart : uploader . node_restart ( ) log . info ( 'All done!' )
6929	def trapezoid_transit_func ( transitparams , times , mags , errs , get_ntransitpoints = False ) : ( transitperiod , transitepoch , transitdepth , transitduration , ingressduration ) = transitparams # generate the phases iphase = ( times - transitepoch ) / transitperiod iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] zerolevel = np . median ( pmags ) modelmags = np . full_like ( phase , zerolevel ) halftransitduration = transitduration / 2.0 bottomlevel = zerolevel - transitdepth slope = transitdepth / ingressduration # the four contact points of the eclipse firstcontact = 1.0 - halftransitduration secondcontact = firstcontact + ingressduration thirdcontact = halftransitduration - ingressduration fourthcontact = halftransitduration ## the phase indices ## # during ingress ingressind = ( phase > firstcontact ) & ( phase < secondcontact ) # at transit bottom bottomind = ( phase > secondcontact ) | ( phase < thirdcontact ) # during egress egressind = ( phase > thirdcontact ) & ( phase < fourthcontact ) # count the number of points in transit in_transit_points = ingressind | bottomind | egressind n_transit_points = np . sum ( in_transit_points ) # set the mags modelmags [ ingressind ] = zerolevel - slope * ( phase [ ingressind ] - firstcontact ) modelmags [ bottomind ] = bottomlevel modelmags [ egressind ] = bottomlevel + slope * ( phase [ egressind ] - thirdcontact ) if get_ntransitpoints : return modelmags , phase , ptimes , pmags , perrs , n_transit_points else : return modelmags , phase , ptimes , pmags , perrs
7734	def nfkc ( data ) : if isinstance ( data , list ) : data = u"" . join ( data ) return unicodedata . normalize ( "NFKC" , data )
11650	def fit ( self , X , y = None ) : if is_integer ( X ) : dim = X else : X = as_features ( X ) dim = X . dim M = self . smoothness # figure out the smooth-enough elements of our basis inds = np . mgrid [ ( slice ( M + 1 ) , ) * dim ] . reshape ( dim , ( M + 1 ) ** dim ) . T self . inds_ = inds [ ( inds ** 2 ) . sum ( axis = 1 ) <= M ** 2 ] return self
533	def setParameter ( self , paramName , value ) : ( setter , getter ) = self . _getParameterMethods ( paramName ) if setter is None : import exceptions raise exceptions . Exception ( "setParameter -- parameter name '%s' does not exist in region %s of type %s" % ( paramName , self . name , self . type ) ) setter ( paramName , value )
13245	async def _download_text ( url , session ) : logger = logging . getLogger ( __name__ ) async with session . get ( url ) as response : # aiohttp decodes the content to a Python string logger . info ( 'Downloading %r' , url ) return await response . text ( )
1705	def extract_common_args ( command , parser , cl_args ) : try : # do not pop like cli because ``topologies`` subcommand still needs it cluster_role_env = cl_args [ 'cluster/[role]/[env]' ] config_path = cl_args [ 'config_path' ] except KeyError : # if some of the arguments are not found, print error and exit subparser = config . get_subparser ( parser , command ) print ( subparser . format_help ( ) ) return dict ( ) cluster = config . get_heron_cluster ( cluster_role_env ) config_path = config . get_heron_cluster_conf_dir ( cluster , config_path ) new_cl_args = dict ( ) try : cluster_tuple = config . parse_cluster_role_env ( cluster_role_env , config_path ) new_cl_args [ 'cluster' ] = cluster_tuple [ 0 ] new_cl_args [ 'role' ] = cluster_tuple [ 1 ] new_cl_args [ 'environ' ] = cluster_tuple [ 2 ] new_cl_args [ 'config_path' ] = config_path except Exception as e : Log . error ( "Unable to get valid topology location: %s" , str ( e ) ) return dict ( ) cl_args . update ( new_cl_args ) return cl_args
8279	def _append_element ( self , render_func , pe ) : self . _render_funcs . append ( render_func ) self . _elements . append ( pe )
4873	def create ( self , validated_data ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) lms_user = validated_data . get ( 'lms_user_id' ) tpa_user = validated_data . get ( 'tpa_user_id' ) user_email = validated_data . get ( 'user_email' ) course_run_id = validated_data . get ( 'course_run_id' ) course_mode = validated_data . get ( 'course_mode' ) cohort = validated_data . get ( 'cohort' ) email_students = validated_data . get ( 'email_students' ) is_active = validated_data . get ( 'is_active' ) enterprise_customer_user = lms_user or tpa_user or user_email if isinstance ( enterprise_customer_user , models . EnterpriseCustomerUser ) : validated_data [ 'enterprise_customer_user' ] = enterprise_customer_user try : if is_active : enterprise_customer_user . enroll ( course_run_id , course_mode , cohort = cohort ) else : enterprise_customer_user . unenroll ( course_run_id ) except ( CourseEnrollmentDowngradeError , CourseEnrollmentPermissionError , HttpClientError ) as exc : validated_data [ 'detail' ] = str ( exc ) return validated_data if is_active : track_enrollment ( 'enterprise-customer-enrollment-api' , enterprise_customer_user . user_id , course_run_id ) else : if is_active : enterprise_customer_user = enterprise_customer . enroll_user_pending_registration ( user_email , course_mode , course_run_id , cohort = cohort ) else : enterprise_customer . clear_pending_registration ( user_email , course_run_id ) if email_students : enterprise_customer . notify_enrolled_learners ( self . context . get ( 'request_user' ) , course_run_id , [ enterprise_customer_user ] ) validated_data [ 'detail' ] = 'success' return validated_data
6456	def sim ( src , tar , method = sim_levenshtein ) : if callable ( method ) : return method ( src , tar ) else : raise AttributeError ( 'Unknown similarity function: ' + str ( method ) )
3081	def xsrf_secret_key ( ) : secret = memcache . get ( XSRF_MEMCACHE_ID , namespace = OAUTH2CLIENT_NAMESPACE ) if not secret : # Load the one and only instance of SiteXsrfSecretKey. model = SiteXsrfSecretKey . get_or_insert ( key_name = 'site' ) if not model . secret : model . secret = _generate_new_xsrf_secret_key ( ) model . put ( ) secret = model . secret memcache . add ( XSRF_MEMCACHE_ID , secret , namespace = OAUTH2CLIENT_NAMESPACE ) return str ( secret )
11841	def TraceAgent ( agent ) : old_program = agent . program def new_program ( percept ) : action = old_program ( percept ) print '%s perceives %s and does %s' % ( agent , percept , action ) return action agent . program = new_program return agent
8092	def node ( s , node , alpha = 1.0 ) : if s . depth : try : colors . shadow ( dx = 5 , dy = 5 , blur = 10 , alpha = 0.5 * alpha ) except : pass s . _ctx . nofill ( ) s . _ctx . nostroke ( ) if s . fill : s . _ctx . fill ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * alpha ) if s . stroke : s . _ctx . strokewidth ( s . strokewidth ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * alpha * 3 ) r = node . r s . _ctx . oval ( node . x - r , node . y - r , r * 2 , r * 2 )
10050	def create_blueprint ( endpoints ) : blueprint = Blueprint ( 'invenio_deposit_rest' , __name__ , url_prefix = '' , ) create_error_handlers ( blueprint ) for endpoint , options in ( endpoints or { } ) . items ( ) : options = deepcopy ( options ) if 'files_serializers' in options : files_serializers = options . get ( 'files_serializers' ) files_serializers = { mime : obj_or_import_string ( func ) for mime , func in files_serializers . items ( ) } del options [ 'files_serializers' ] else : files_serializers = { } if 'record_serializers' in options : serializers = options . get ( 'record_serializers' ) serializers = { mime : obj_or_import_string ( func ) for mime , func in serializers . items ( ) } else : serializers = { } file_list_route = options . pop ( 'file_list_route' , '{0}/files' . format ( options [ 'item_route' ] ) ) file_item_route = options . pop ( 'file_item_route' , '{0}/files/<path:key>' . format ( options [ 'item_route' ] ) ) options . setdefault ( 'search_class' , DepositSearch ) search_class = obj_or_import_string ( options [ 'search_class' ] ) # records rest endpoints will use the deposit class as record class options . setdefault ( 'record_class' , Deposit ) record_class = obj_or_import_string ( options [ 'record_class' ] ) # backward compatibility for indexer class options . setdefault ( 'indexer_class' , None ) for rule in records_rest_url_rules ( endpoint , * * options ) : blueprint . add_url_rule ( * * rule ) search_class_kwargs = { } if options . get ( 'search_index' ) : search_class_kwargs [ 'index' ] = options [ 'search_index' ] if options . get ( 'search_type' ) : search_class_kwargs [ 'doc_type' ] = options [ 'search_type' ] ctx = dict ( read_permission_factory = obj_or_import_string ( options . get ( 'read_permission_factory_imp' ) ) , create_permission_factory = obj_or_import_string ( options . get ( 'create_permission_factory_imp' ) ) , update_permission_factory = obj_or_import_string ( options . get ( 'update_permission_factory_imp' ) ) , delete_permission_factory = obj_or_import_string ( options . get ( 'delete_permission_factory_imp' ) ) , record_class = record_class , search_class = partial ( search_class , * * search_class_kwargs ) , default_media_type = options . get ( 'default_media_type' ) , ) deposit_actions = DepositActionResource . as_view ( DepositActionResource . view_name . format ( endpoint ) , serializers = serializers , pid_type = options [ 'pid_type' ] , ctx = ctx , ) blueprint . add_url_rule ( '{0}/actions/<any({1}):action>' . format ( options [ 'item_route' ] , ',' . join ( extract_actions_from_class ( record_class ) ) , ) , view_func = deposit_actions , methods = [ 'POST' ] , ) deposit_files = DepositFilesResource . as_view ( DepositFilesResource . view_name . format ( endpoint ) , serializers = files_serializers , pid_type = options [ 'pid_type' ] , ctx = ctx , ) blueprint . add_url_rule ( file_list_route , view_func = deposit_files , methods = [ 'GET' , 'POST' , 'PUT' ] , ) deposit_file = DepositFileResource . as_view ( DepositFileResource . view_name . format ( endpoint ) , serializers = files_serializers , pid_type = options [ 'pid_type' ] , ctx = ctx , ) blueprint . add_url_rule ( file_item_route , view_func = deposit_file , methods = [ 'GET' , 'PUT' , 'DELETE' ] , ) return blueprint
4517	def fillScreen ( self , color = None ) : md . fill_rect ( self . set , 0 , 0 , self . width , self . height , color )
13114	def resolve_domains ( domains , disable_zone = False ) : dnsresolver = dns . resolver . Resolver ( ) ips = [ ] for domain in domains : print_notification ( "Resolving {}" . format ( domain ) ) try : result = dnsresolver . query ( domain , 'A' ) for a in result . response . answer [ 0 ] : ips . append ( str ( a ) ) if not disable_zone : ips . extend ( zone_transfer ( str ( a ) , domain ) ) except dns . resolver . NXDOMAIN as e : print_error ( e ) return ips
12348	def stitch_coordinates ( self , well_row = 0 , well_column = 0 ) : well = [ w for w in self . wells if attribute ( w , 'u' ) == well_column and attribute ( w , 'v' ) == well_row ] if len ( well ) == 1 : well = well [ 0 ] tile = os . path . join ( well , 'TileConfiguration.registered.txt' ) with open ( tile ) as f : data = [ x . strip ( ) for l in f . readlines ( ) if l [ 0 : 7 ] == 'image--' for x in l . split ( ';' ) ] # flat list coordinates = ( ast . literal_eval ( x ) for x in data [ 2 : : 3 ] ) # flatten coordinates = sum ( coordinates , ( ) ) attr = tuple ( attributes ( x ) for x in data [ 0 : : 3 ] ) return coordinates [ 0 : : 2 ] , coordinates [ 1 : : 2 ] , attr else : print ( 'leicaexperiment stitch_coordinates' '({}, {}) Well not found' . format ( well_row , well_column ) )
1516	def make_tarfile ( output_filename , source_dir ) : with tarfile . open ( output_filename , "w:gz" ) as tar : tar . add ( source_dir , arcname = os . path . basename ( source_dir ) )
9231	def fetch_events_async ( self , issues , tag_name ) : if not issues : return issues max_simultaneous_requests = self . options . max_simultaneous_requests verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project self . events_cnt = 0 if verbose : print ( "fetching events for {} {}... " . format ( len ( issues ) , tag_name ) ) def worker ( issue ) : page = 1 issue [ 'events' ] = [ ] while page > 0 : rc , data = gh . repos [ user ] [ repo ] . issues [ issue [ 'number' ] ] . events . get ( page = page , per_page = PER_PAGE_NUMBER ) if rc == 200 : issue [ 'events' ] . extend ( data ) self . events_cnt += len ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) threads = [ ] cnt = len ( issues ) for i in range ( 0 , ( cnt // max_simultaneous_requests ) + 1 ) : for j in range ( max_simultaneous_requests ) : idx = i * max_simultaneous_requests + j if idx == cnt : break t = threading . Thread ( target = worker , args = ( issues [ idx ] , ) ) threads . append ( t ) t . start ( ) if verbose > 2 : print ( "." , end = "" ) if not idx % PER_PAGE_NUMBER : print ( "" ) for t in threads : t . join ( ) if verbose > 2 : print ( "." )
1146	def copy ( x ) : cls = type ( x ) copier = _copy_dispatch . get ( cls ) if copier : return copier ( x ) copier = getattr ( cls , "__copy__" , None ) if copier : return copier ( x ) reductor = dispatch_table . get ( cls ) if reductor : rv = reductor ( x ) else : reductor = getattr ( x , "__reduce_ex__" , None ) if reductor : rv = reductor ( 2 ) else : reductor = getattr ( x , "__reduce__" , None ) if reductor : rv = reductor ( ) else : raise Error ( "un(shallow)copyable object of type %s" % cls ) return _reconstruct ( x , rv , 0 )
2855	def setup ( self , pin , mode ) : self . _setup_pin ( pin , mode ) self . mpsse_write_gpio ( )
6553	def fix_variable ( self , v , value ) : variables = self . variables try : idx = variables . index ( v ) except ValueError : raise ValueError ( "given variable {} is not part of the constraint" . format ( v ) ) if value not in self . vartype . value : raise ValueError ( "expected value to be in {}, received {} instead" . format ( self . vartype . value , value ) ) configurations = frozenset ( config [ : idx ] + config [ idx + 1 : ] # exclude the fixed var for config in self . configurations if config [ idx ] == value ) if not configurations : raise UnsatError ( "fixing {} to {} makes this constraint unsatisfiable" . format ( v , value ) ) variables = variables [ : idx ] + variables [ idx + 1 : ] self . configurations = configurations self . variables = variables def func ( * args ) : return args in configurations self . func = func self . name = '{} ({} fixed to {})' . format ( self . name , v , value )
1374	def parse_override_config_and_write_file ( namespace ) : overrides = parse_override_config ( namespace ) try : tmp_dir = tempfile . mkdtemp ( ) override_config_file = os . path . join ( tmp_dir , OVERRIDE_YAML ) with open ( override_config_file , 'w' ) as f : f . write ( yaml . dump ( overrides ) ) return override_config_file except Exception as e : raise Exception ( "Failed to parse override config: %s" % str ( e ) )
11914	def initialize ( self , templates_path , global_data ) : self . env = Environment ( loader = FileSystemLoader ( templates_path ) ) self . env . trim_blocks = True self . global_data = global_data
5601	def serve ( mapchete_file , port = None , internal_cache = None , zoom = None , bounds = None , overwrite = False , readonly = False , memory = False , input_file = None , debug = False , logfile = None ) : app = create_app ( mapchete_files = [ mapchete_file ] , zoom = zoom , bounds = bounds , single_input_file = input_file , mode = _get_mode ( memory , readonly , overwrite ) , debug = debug ) if os . environ . get ( "MAPCHETE_TEST" ) == "TRUE" : logger . debug ( "don't run flask app, MAPCHETE_TEST environment detected" ) else : app . run ( threaded = True , debug = True , port = port , host = '0.0.0.0' , extra_files = [ mapchete_file ] )
366	def projective_transform_by_points ( x , src , dst , map_args = None , output_shape = None , order = 1 , mode = 'constant' , cval = 0.0 , clip = True , preserve_range = False ) : if map_args is None : map_args = { } # if type(src) is list: if isinstance ( src , list ) : # convert to numpy src = np . array ( src ) # if type(dst) is list: if isinstance ( dst , list ) : dst = np . array ( dst ) if np . max ( x ) > 1 : # convert to [0, 1] x = x / 255 m = transform . ProjectiveTransform ( ) m . estimate ( dst , src ) warped = transform . warp ( x , m , map_args = map_args , output_shape = output_shape , order = order , mode = mode , cval = cval , clip = clip , preserve_range = preserve_range ) return warped
12611	def search_by_eid ( self , table_name , eid ) : elem = self . table ( table_name ) . get ( eid = eid ) if elem is None : raise KeyError ( 'Could not find {} with eid {}.' . format ( table_name , eid ) ) return elem
12091	def proto_02_01_MT70 ( abf = exampleABF ) : standard_overlayWithAverage ( abf ) swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = 'check' , resize = False )
5557	def _strip_zoom ( input_string , strip_string ) : try : return int ( input_string . strip ( strip_string ) ) except Exception as e : raise MapcheteConfigError ( "zoom level could not be determined: %s" % e )
3935	def get ( self ) : logger . info ( 'Loading refresh_token from %s' , repr ( self . _filename ) ) try : with open ( self . _filename ) as f : return f . read ( ) except IOError as e : logger . info ( 'Failed to load refresh_token: %s' , e )
9033	def _walk ( self ) : while self . _todo : args = self . _todo . pop ( 0 ) self . _step ( * args )
6587	def iterate_forever ( func , * args , * * kwargs ) : output = func ( * args , * * kwargs ) while True : try : playlist_item = next ( output ) playlist_item . prepare_playback ( ) yield playlist_item except StopIteration : output = func ( * args , * * kwargs )
4688	def get_shared_secret ( priv , pub ) : pub_point = pub . point ( ) priv_point = int ( repr ( priv ) , 16 ) res = pub_point * priv_point res_hex = "%032x" % res . x ( ) # Zero padding res_hex = "0" * ( 64 - len ( res_hex ) ) + res_hex return res_hex
10323	def spanning_2d_grid ( length ) : ret = nx . grid_2d_graph ( length + 2 , length ) for i in range ( length ) : # side 0 ret . node [ ( 0 , i ) ] [ 'span' ] = 0 ret [ ( 0 , i ) ] [ ( 1 , i ) ] [ 'span' ] = 0 # side 1 ret . node [ ( length + 1 , i ) ] [ 'span' ] = 1 ret [ ( length + 1 , i ) ] [ ( length , i ) ] [ 'span' ] = 1 return ret
13192	def geojson_to_gml ( gj , set_srs = True ) : tag = G ( gj [ 'type' ] ) if set_srs : tag . set ( 'srsName' , 'urn:ogc:def:crs:EPSG::4326' ) if gj [ 'type' ] == 'Point' : tag . append ( G . pos ( _reverse_geojson_coords ( gj [ 'coordinates' ] ) ) ) elif gj [ 'type' ] == 'LineString' : tag . append ( G . posList ( ' ' . join ( _reverse_geojson_coords ( ll ) for ll in gj [ 'coordinates' ] ) ) ) elif gj [ 'type' ] == 'Polygon' : rings = [ G . LinearRing ( G . posList ( ' ' . join ( _reverse_geojson_coords ( ll ) for ll in ring ) ) ) for ring in gj [ 'coordinates' ] ] tag . append ( G . exterior ( rings . pop ( 0 ) ) ) for ring in rings : tag . append ( G . interior ( ring ) ) elif gj [ 'type' ] in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' ) : single_type = gj [ 'type' ] [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' for coord in gj [ 'coordinates' ] : tag . append ( G ( member_tag , geojson_to_gml ( { 'type' : single_type , 'coordinates' : coord } , set_srs = False ) ) ) else : raise NotImplementedError return tag
5769	def _advapi32_load_key ( key_object , key_info , container ) : key_type = 'public' if isinstance ( key_info , keys . PublicKeyInfo ) else 'private' algo = key_info . algorithm if algo == 'rsa' : provider = Advapi32Const . MS_ENH_RSA_AES_PROV else : provider = Advapi32Const . MS_ENH_DSS_DH_PROV context_handle = None key_handle = None try : context_handle = open_context_handle ( provider , verify_only = key_type == 'public' ) blob = _advapi32_create_blob ( key_info , key_type , algo ) buffer_ = buffer_from_bytes ( blob ) key_handle_pointer = new ( advapi32 , 'HCRYPTKEY *' ) res = advapi32 . CryptImportKey ( context_handle , buffer_ , len ( blob ) , null ( ) , 0 , key_handle_pointer ) handle_error ( res ) key_handle = unwrap ( key_handle_pointer ) output = container ( key_handle , key_object ) output . context_handle = context_handle if algo == 'rsa' : ex_blob = _advapi32_create_blob ( key_info , key_type , algo , signing = False ) ex_buffer = buffer_from_bytes ( ex_blob ) ex_key_handle_pointer = new ( advapi32 , 'HCRYPTKEY *' ) res = advapi32 . CryptImportKey ( context_handle , ex_buffer , len ( ex_blob ) , null ( ) , 0 , ex_key_handle_pointer ) handle_error ( res ) output . ex_key_handle = unwrap ( ex_key_handle_pointer ) return output except ( Exception ) : if key_handle : advapi32 . CryptDestroyKey ( key_handle ) if context_handle : close_context_handle ( context_handle ) raise
7781	def rfc2426 ( self ) : ret = "begin:VCARD\r\n" ret += "version:3.0\r\n" for _unused , value in self . content . items ( ) : if value is None : continue if type ( value ) is list : for v in value : ret += v . rfc2426 ( ) else : v = value . rfc2426 ( ) ret += v return ret + "end:VCARD\r\n"
12542	def get_attributes ( self , attributes , default = '' ) : if isinstance ( attributes , str ) : attributes = [ attributes ] attrs = [ getattr ( self , attr , default ) for attr in attributes ] if len ( attrs ) == 1 : return attrs [ 0 ] return tuple ( attrs )
2679	def get_account_id ( profile_name , aws_access_key_id , aws_secret_access_key , region = None , ) : client = get_client ( 'sts' , profile_name , aws_access_key_id , aws_secret_access_key , region , ) return client . get_caller_identity ( ) . get ( 'Account' )
7205	def savedata ( self , output , location = None ) : output . persist = True if location : output . persist_location = location
2254	def argunique ( items , key = None ) : # yield from unique(range(len(items)), key=lambda i: items[i]) if key is None : return unique ( range ( len ( items ) ) , key = lambda i : items [ i ] ) else : return unique ( range ( len ( items ) ) , key = lambda i : key ( items [ i ] ) )
8290	def _description ( self ) : meta = self . find ( "meta" , { "name" : "description" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : return meta [ "content" ] else : return u""
4074	def get_cfg_value ( config , section , option ) : try : value = config [ section ] [ option ] except KeyError : if ( section , option ) in MULTI_OPTIONS : return [ ] else : return '' if ( section , option ) in MULTI_OPTIONS : value = split_multiline ( value ) if ( section , option ) in ENVIRON_OPTIONS : value = eval_environ ( value ) return value
12847	def require_active_token ( object ) : require_token ( object ) token = object if not token . has_id : raise ApiUsageError ( """\ token {token} should have an id, but doesn't. This error usually means that a token was added to the world without being assigned an id number. To correct this, make sure that you're using a message (i.e. CreateToken) to create all of your tokens.""" ) if not token . has_world : raise ApiUsageError ( """\ token {token} (id={token.id}) not in world. You can get this error if you try to remove the same token from the world twice. This might happen is you don't get rid of every reference to a token after it's removed the first time, then later on you try to remove the stale reference.""" )
401	def cross_entropy_seq_with_mask ( logits , target_seqs , input_mask , return_details = False , name = None ) : targets = tf . reshape ( target_seqs , [ - 1 ] ) # to one vector weights = tf . to_float ( tf . reshape ( input_mask , [ - 1 ] ) ) # to one vector like targets losses = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits = logits , labels = targets , name = name ) * weights # losses = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets, name=name)) # for TF1.0 and others loss = tf . divide ( tf . reduce_sum ( losses ) , # loss from mask. reduce_sum before element-wise mul with mask !! tf . reduce_sum ( weights ) , name = "seq_loss_with_mask" ) if return_details : return loss , losses , weights , targets else : return loss
2283	def check_R_package ( self , package ) : test_package = not bool ( launch_R_script ( "{}/R_templates/test_import.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , { "{package}" : package } , verbose = True ) ) return test_package
11204	def valuestodict ( key ) : dout = { } size = winreg . QueryInfoKey ( key ) [ 1 ] tz_res = None for i in range ( size ) : key_name , value , dtype = winreg . EnumValue ( key , i ) if dtype == winreg . REG_DWORD or dtype == winreg . REG_DWORD_LITTLE_ENDIAN : # If it's a DWORD (32-bit integer), it's stored as unsigned - convert # that to a proper signed integer if value & ( 1 << 31 ) : value = value - ( 1 << 32 ) elif dtype == winreg . REG_SZ : # If it's a reference to the tzres DLL, load the actual string if value . startswith ( '@tzres' ) : tz_res = tz_res or tzres ( ) value = tz_res . name_from_string ( value ) value = value . rstrip ( '\x00' ) # Remove trailing nulls dout [ key_name ] = value return dout
10233	def _reaction_cartesion_expansion_unqualified_helper ( graph : BELGraph , u : BaseEntity , v : BaseEntity , d : dict , ) -> None : if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( reactant , product , INCREASES ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( product , reactant , d [ RELATION ] , ) elif isinstance ( u , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) for product in u . products : # Skip create increases edges between enzymes if product in enzymes : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips hasReactant, hasProduct edges if v not in u . products and v not in u . reactants : graph . add_unqualified_edge ( product , v , INCREASES ) for reactant in u . reactants : graph . add_unqualified_edge ( reactant , product , INCREASES ) elif isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( v ) for reactant in v . reactants : # Skip create increases edges between enzymes if reactant in enzymes : continue # Only add edge between v and reaction if the node is not part of the reaction # In practice skips hasReactant, hasProduct edges if u not in v . products and u not in v . reactants : graph . add_unqualified_edge ( u , reactant , INCREASES ) for product in v . products : graph . add_unqualified_edge ( reactant , product , INCREASES )
8610	def get_resource ( self , resource_type , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/resources/%s/%s?depth=%s' % ( resource_type , resource_id , str ( depth ) ) ) return response
4121	def twosided_2_centerdc ( data ) : N = len ( data ) # could us int() or // in python 3 newpsd = np . concatenate ( ( cshift ( data [ N // 2 : ] , 1 ) , data [ 0 : N // 2 ] ) ) newpsd [ 0 ] = data [ - 1 ] return newpsd
9469	def conference_list ( self , call_params ) : path = '/' + self . api_version + '/ConferenceList/' method = 'POST' return self . request ( path , method , call_params )
7608	def get_all_cards ( self , timeout : int = None ) : url = self . api . CARDS return self . _get_model ( url , timeout = timeout )
4554	def pointOnCircle ( cx , cy , radius , angle ) : angle = math . radians ( angle ) - ( math . pi / 2 ) x = cx + radius * math . cos ( angle ) if x < cx : x = math . ceil ( x ) else : x = math . floor ( x ) y = cy + radius * math . sin ( angle ) if y < cy : y = math . ceil ( y ) else : y = math . floor ( y ) return ( int ( x ) , int ( y ) )
11218	def encode ( self ) -> str : payload = { } payload . update ( self . registered_claims ) payload . update ( self . payload ) return encode ( self . secret , payload , self . alg , self . header )
5757	def get_homogeneous ( package_descriptors , targets , repos_data ) : homogeneous = { } for package_descriptor in package_descriptors . values ( ) : pkg_name = package_descriptor . pkg_name debian_pkg_name = package_descriptor . debian_pkg_name versions = [ ] for repo_data in repos_data : versions . append ( set ( [ ] ) ) for target in targets : version = _strip_version_suffix ( repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) ) versions [ - 1 ] . add ( version ) homogeneous [ pkg_name ] = max ( [ len ( v ) for v in versions ] ) == 1 return homogeneous
12258	def sdcone ( x , rho ) : U , V = np . linalg . eigh ( x ) return V . dot ( np . diag ( np . maximum ( U , 0 ) ) . dot ( V . T ) )
4200	def _thumbnail_div ( full_dir , fname , snippet , is_backref = False ) : thumb = os . path . join ( full_dir , 'images' , 'thumb' , 'sphx_glr_%s_thumb.png' % fname [ : - 3 ] ) ref_name = os . path . join ( full_dir , fname ) . replace ( os . path . sep , '_' ) template = BACKREF_THUMBNAIL_TEMPLATE if is_backref else THUMBNAIL_TEMPLATE return template . format ( snippet = snippet , thumbnail = thumb , ref_name = ref_name )
9171	def declare_browsable_routes ( config ) : # This makes our routes slashed, which is good browser behavior. config . add_notfound_view ( default_exceptionresponse_view , append_slash = True ) add_route = config . add_route add_route ( 'admin-index' , '/a/' ) add_route ( 'admin-moderation' , '/a/moderation/' ) add_route ( 'admin-api-keys' , '/a/api-keys/' ) add_route ( 'admin-add-site-messages' , '/a/site-messages/' , request_method = 'GET' ) add_route ( 'admin-add-site-messages-POST' , '/a/site-messages/' , request_method = 'POST' ) add_route ( 'admin-delete-site-messages' , '/a/site-messages/' , request_method = 'DELETE' ) add_route ( 'admin-edit-site-message' , '/a/site-messages/{id}/' , request_method = 'GET' ) add_route ( 'admin-edit-site-message-POST' , '/a/site-messages/{id}/' , request_method = 'POST' ) add_route ( 'admin-content-status' , '/a/content-status/' ) add_route ( 'admin-content-status-single' , '/a/content-status/{uuid}' ) add_route ( 'admin-print-style' , '/a/print-style/' ) add_route ( 'admin-print-style-single' , '/a/print-style/{style}' )
11971	def _detect ( ip , _isnm ) : ip = str ( ip ) if len ( ip ) > 1 : if ip [ 0 : 2 ] == '0x' : if _CHECK_FUNCT [ IP_HEX ] [ _isnm ] ( ip ) : return IP_HEX elif ip [ 0 ] == '0' : if _CHECK_FUNCT [ IP_OCT ] [ _isnm ] ( ip ) : return IP_OCT if _CHECK_FUNCT [ IP_DOT ] [ _isnm ] ( ip ) : return IP_DOT elif _isnm and _CHECK_FUNCT [ NM_BITS ] [ _isnm ] ( ip ) : return NM_BITS elif _CHECK_FUNCT [ IP_DEC ] [ _isnm ] ( ip ) : return IP_DEC elif _isnm and _CHECK_FUNCT [ NM_WILDCARD ] [ _isnm ] ( ip ) : return NM_WILDCARD elif _CHECK_FUNCT [ IP_BIN ] [ _isnm ] ( ip ) : return IP_BIN return IP_UNKNOWN
11521	def add_condor_dag ( self , token , batchmaketaskid , dagfilename , dagmanoutfilename ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'batchmaketaskid' ] = batchmaketaskid parameters [ 'dagfilename' ] = dagfilename parameters [ 'outfilename' ] = dagmanoutfilename response = self . request ( 'midas.batchmake.add.condor.dag' , parameters ) return response
920	def critical ( self , msg , * args , * * kwargs ) : self . _baseLogger . critical ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )
9359	def paragraphs ( quantity = 2 , separator = '\n\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 , as_list = False ) : if html : wrap_start = '<p>' wrap_end = '</p>' separator = '\n\n' result = [ ] try : for _ in xrange ( 0 , quantity ) : result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) # Python 3 compatibility except NameError : for _ in range ( 0 , quantity ) : result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) if as_list : return result else : return separator . join ( result )
11976	def _sub ( self , other ) : if isinstance ( other , self . __class__ ) : sub = self . _ip_dec - other . _ip_dec if isinstance ( other , int ) : sub = self . _ip_dec - other else : other = self . __class__ ( other ) sub = self . _ip_dec - other . _ip_dec return sub
12826	def flush_buffer ( self ) : self . code_builder . add_line ( '{0}.extend([{1}])' , self . result_var , ',' . join ( self . buffered ) ) self . buffered = [ ]
6125	def plot_image ( image , plot_origin = True , mask = None , extract_array_from_mask = False , zoom_around_mask = False , should_plot_border = False , positions = None , as_subplot = False , units = 'arcsec' , kpc_per_arcsec = None , figsize = ( 7 , 7 ) , aspect = 'square' , cmap = 'jet' , norm = 'linear' , norm_min = None , norm_max = None , linthresh = 0.05 , linscale = 0.01 , cb_ticksize = 10 , cb_fraction = 0.047 , cb_pad = 0.01 , cb_tick_values = None , cb_tick_labels = None , title = 'Image' , titlesize = 16 , xlabelsize = 16 , ylabelsize = 16 , xyticksize = 16 , mask_pointsize = 10 , position_pointsize = 30 , grid_pointsize = 1 , output_path = None , output_format = 'show' , output_filename = 'image' ) : origin = get_origin ( array = image , plot_origin = plot_origin ) array_plotters . plot_array ( array = image , origin = origin , mask = mask , extract_array_from_mask = extract_array_from_mask , zoom_around_mask = zoom_around_mask , should_plot_border = should_plot_border , positions = positions , as_subplot = as_subplot , units = units , kpc_per_arcsec = kpc_per_arcsec , figsize = figsize , aspect = aspect , cmap = cmap , norm = norm , norm_min = norm_min , norm_max = norm_max , linthresh = linthresh , linscale = linscale , cb_ticksize = cb_ticksize , cb_fraction = cb_fraction , cb_pad = cb_pad , cb_tick_values = cb_tick_values , cb_tick_labels = cb_tick_labels , title = title , titlesize = titlesize , xlabelsize = xlabelsize , ylabelsize = ylabelsize , xyticksize = xyticksize , mask_pointsize = mask_pointsize , position_pointsize = position_pointsize , grid_pointsize = grid_pointsize , output_path = output_path , output_format = output_format , output_filename = output_filename )
7198	def describe_images ( self , idaho_image_results ) : results = idaho_image_results [ 'results' ] # filter only idaho images: results = [ r for r in results if 'IDAHOImage' in r [ 'type' ] ] self . logger . debug ( 'Describing %s IDAHO images.' % len ( results ) ) # figure out which catids are represented in this set of images catids = set ( [ r [ 'properties' ] [ 'catalogID' ] for r in results ] ) description = { } for catid in catids : # images associated with a single catid description [ catid ] = { } description [ catid ] [ 'parts' ] = { } images = [ r for r in results if r [ 'properties' ] [ 'catalogID' ] == catid ] for image in images : description [ catid ] [ 'sensorPlatformName' ] = image [ 'properties' ] [ 'sensorPlatformName' ] part = int ( image [ 'properties' ] [ 'vendorDatasetIdentifier' ] . split ( ':' ) [ 1 ] [ - 3 : ] ) color = image [ 'properties' ] [ 'colorInterpretation' ] bucket = image [ 'properties' ] [ 'tileBucketName' ] identifier = image [ 'identifier' ] boundstr = image [ 'properties' ] [ 'footprintWkt' ] try : description [ catid ] [ 'parts' ] [ part ] except : description [ catid ] [ 'parts' ] [ part ] = { } description [ catid ] [ 'parts' ] [ part ] [ color ] = { } description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'id' ] = identifier description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'bucket' ] = bucket description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'boundstr' ] = boundstr return description
4709	def power_btn ( self , interval = 200 ) : if self . __power_btn_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_BTN" ) return 1 return self . __press ( self . __power_btn_port , interval = interval )
12934	def _parse_frequencies ( self ) : frequencies = OrderedDict ( [ ( 'EXAC' , 'Unknown' ) , ( 'ESP' , 'Unknown' ) , ( 'TGP' , 'Unknown' ) ] ) pref_freq = 'Unknown' for source in frequencies . keys ( ) : freq_key = 'AF_' + source if freq_key in self . info : frequencies [ source ] = self . info [ freq_key ] if pref_freq == 'Unknown' : pref_freq = frequencies [ source ] return pref_freq , frequencies
9041	def as_instruction ( self , specification ) : instruction = self . _instruction_class ( specification ) type_ = instruction . type if type_ in self . _type_to_instruction : instruction . inherit_from ( self . _type_to_instruction [ type_ ] ) return instruction
565	def updateResultsForJob ( self , forceUpdate = True ) : updateInterval = time . time ( ) - self . _lastUpdateAttemptTime if updateInterval < self . _MIN_UPDATE_INTERVAL and not forceUpdate : return self . logger . info ( "Attempting model selection for jobID=%d: time=%f" " lastUpdate=%f" % ( self . _jobID , time . time ( ) , self . _lastUpdateAttemptTime ) ) timestampUpdated = self . _cjDB . jobUpdateSelectionSweep ( self . _jobID , self . _MIN_UPDATE_INTERVAL ) if not timestampUpdated : self . logger . info ( "Unable to update selection sweep timestamp: jobID=%d" " updateTime=%f" % ( self . _jobID , self . _lastUpdateAttemptTime ) ) if not forceUpdate : return self . _lastUpdateAttemptTime = time . time ( ) self . logger . info ( "Succesfully updated selection sweep timestamp jobid=%d updateTime=%f" % ( self . _jobID , self . _lastUpdateAttemptTime ) ) minUpdateRecords = self . _MIN_UPDATE_THRESHOLD jobResults = self . _getJobResults ( ) if forceUpdate or jobResults is None : minUpdateRecords = 0 candidateIDs , bestMetric = self . _cjDB . modelsGetCandidates ( self . _jobID , minUpdateRecords ) self . logger . info ( "Candidate models=%s, metric=%s, jobID=%s" % ( candidateIDs , bestMetric , self . _jobID ) ) if len ( candidateIDs ) == 0 : return self . _jobUpdateCandidate ( candidateIDs [ 0 ] , bestMetric , results = jobResults )
4411	def store ( self , key : object , value : object ) : self . _user_data . update ( { key : value } )
10658	def amount_fractions ( masses ) : n = amounts ( masses ) n_total = sum ( n . values ( ) ) return { compound : n [ compound ] / n_total for compound in n . keys ( ) }
11563	def set_digital_latch ( self , pin , threshold_type , cb = None ) : if 0 <= threshold_type <= 1 : self . _command_handler . set_digital_latch ( pin , threshold_type , cb ) return True else : return False
6751	def unregister ( self ) : for k in list ( env . keys ( ) ) : if k . startswith ( self . env_prefix ) : del env [ k ] try : del all_satchels [ self . name . upper ( ) ] except KeyError : pass try : del manifest_recorder [ self . name ] except KeyError : pass try : del manifest_deployers [ self . name . upper ( ) ] except KeyError : pass try : del manifest_deployers_befores [ self . name . upper ( ) ] except KeyError : pass try : del required_system_packages [ self . name . upper ( ) ] except KeyError : pass
984	def mmGetMetricSequencesPredictedActiveCellsPerColumn ( self ) : self . _mmComputeTransitionTraces ( ) numCellsPerColumn = [ ] for predictedActiveCells in ( self . _mmData [ "predictedActiveCellsForSequence" ] . values ( ) ) : cellsForColumn = self . mapCellsToColumns ( predictedActiveCells ) numCellsPerColumn += [ len ( x ) for x in cellsForColumn . values ( ) ] return Metric ( self , "# predicted => active cells per column for each sequence" , numCellsPerColumn )
6144	def DSP_callback_tic ( self ) : if self . Tcapture > 0 : self . DSP_tic . append ( time . time ( ) - self . start_time )
7744	def _timeout_cb ( self , method ) : self . _anything_done = True logger . debug ( "_timeout_cb() called for: {0!r}" . format ( method ) ) result = method ( ) # pylint: disable=W0212 rec = method . _pyxmpp_recurring if rec : self . _prepare_pending ( ) return True if rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) tag = glib . timeout_add ( int ( result * 1000 ) , self . _timeout_cb , method ) self . _timer_sources [ method ] = tag else : self . _timer_sources . pop ( method , None ) self . _prepare_pending ( ) return False
965	def bitsToString ( arr ) : s = array ( 'c' , '.' * len ( arr ) ) for i in xrange ( len ( arr ) ) : if arr [ i ] == 1 : s [ i ] = '*' return s
574	def clippedObj ( obj , maxElementSize = 64 ) : # Is it a named tuple? if hasattr ( obj , '_asdict' ) : obj = obj . _asdict ( ) # Printing a dict? if isinstance ( obj , dict ) : objOut = dict ( ) for key , val in obj . iteritems ( ) : objOut [ key ] = clippedObj ( val ) # Printing a list? elif hasattr ( obj , '__iter__' ) : objOut = [ ] for val in obj : objOut . append ( clippedObj ( val ) ) # Some other object else : objOut = str ( obj ) if len ( objOut ) > maxElementSize : objOut = objOut [ 0 : maxElementSize ] + '...' return objOut
6120	def elliptical ( cls , shape , pixel_scale , major_axis_radius_arcsec , axis_ratio , phi , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_elliptical_from_shape_pixel_scale_and_radius ( shape , pixel_scale , major_axis_radius_arcsec , axis_ratio , phi , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
2771	def get_object ( cls , api_token , id ) : load_balancer = cls ( token = api_token , id = id ) load_balancer . load ( ) return load_balancer
12943	def diff ( firstObj , otherObj , includeMeta = False ) : if not isIndexedRedisModel ( firstObj ) : raise ValueError ( 'Type < %s > does not extend IndexedRedisModel.' % ( type ( firstObj ) . __name__ , ) ) if not isIndexedRedisModel ( otherObj ) : raise ValueError ( 'Type < %s > does not extend IndexedRedisModel.' % ( type ( otherObj ) . __name__ , ) ) firstObj . validateModel ( ) otherObj . validateModel ( ) # Types may not match, but could be subclass, special copy class (like connectAlt), etc. # So check if FIELDS matches, and if so, we can continue. if getattr ( firstObj , 'FIELDS' ) != getattr ( otherObj , 'FIELDS' ) : # NOTE: Maybe we should iterate here and compare just that field types and names match? # In case a copy changes a default or something, we would still be able to diff.. raise ValueError ( 'Cannot compare < %s > and < %s > . Must be same model OR have equal FIELDS.' % ( firstObj . __class__ , otherObj . __class__ ) ) diffFields = { } for thisField in firstObj . FIELDS : thisFieldStr = str ( thisField ) firstVal = object . __getattribute__ ( firstObj , thisFieldStr ) otherVal = object . __getattribute__ ( otherObj , thisFieldStr ) if firstVal != otherVal : diffFields [ thisFieldStr ] = ( ( firstVal , otherVal ) ) if includeMeta : firstPk = firstObj . getPk ( ) otherPk = otherObj . getPk ( ) if firstPk != otherPk : diffFields [ '_id' ] = ( firstPk , otherPk ) return diffFields
10554	def get_helping_materials ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'helpingmaterial' , params = params ) if type ( res ) . __name__ == 'list' : return [ HelpingMaterial ( helping ) for helping in res ] else : return res except : # pragma: no cover raise
8078	def arrow ( self , x , y , width , type = NORMAL , draw = True , * * kwargs ) : # Taken from Nodebox path = self . BezierPath ( * * kwargs ) if type == self . NORMAL : head = width * .4 tail = width * .2 path . moveto ( x , y ) path . lineto ( x - head , y + head ) path . lineto ( x - head , y + tail ) path . lineto ( x - width , y + tail ) path . lineto ( x - width , y - tail ) path . lineto ( x - head , y - tail ) path . lineto ( x - head , y - head ) path . lineto ( x , y ) elif type == self . FORTYFIVE : head = .3 tail = 1 + head path . moveto ( x , y ) path . lineto ( x , y + width * ( 1 - head ) ) path . lineto ( x - width * head , y + width ) path . lineto ( x - width * head , y + width * tail * .4 ) path . lineto ( x - width * tail * .6 , y + width ) path . lineto ( x - width , y + width * tail * .6 ) path . lineto ( x - width * tail * .4 , y + width * head ) path . lineto ( x - width , y + width * head ) path . lineto ( x - width * ( 1 - head ) , y ) path . lineto ( x , y ) else : raise NameError ( _ ( "arrow: available types for arrow() are NORMAL and FORTYFIVE\n" ) ) if draw : path . draw ( ) return path
13712	def invalidate_cache ( self ) : if self . _use_cache : self . _cache_version += 1 self . _cache . increment ( 'cached_httpbl_{0}_version' . format ( self . _api_key ) )
8015	async def dispatch_downstream ( self , message , steam_name ) : handler = getattr ( self , get_handler_name ( message ) , None ) if handler : await handler ( message , stream_name = steam_name ) else : # if there is not handler then just pass the message further downstream. await self . base_send ( message )
11028	def _sse_content_with_protocol ( response , handler , * * sse_kwargs ) : protocol = SseProtocol ( handler , * * sse_kwargs ) finished = protocol . when_finished ( ) response . deliverBody ( protocol ) return finished , protocol
8159	def edit ( self , id , * args , * * kw ) : if args and kw : return if args and type ( args [ 0 ] ) == dict : fields = [ k for k in args [ 0 ] ] v = [ args [ 0 ] [ k ] for k in args [ 0 ] ] if kw : fields = [ k for k in kw ] v = [ kw [ k ] for k in kw ] sql = "update " + self . _name + " set " + "=?, " . join ( fields ) + "=? where " + self . _key + "=" + unicode ( id ) self . _db . _cur . execute ( sql , v ) self . _db . _i += 1 if self . _db . _i >= self . _db . _commit : self . _db . _i = 0 self . _db . _con . commit ( )
2008	def _deserialize_int ( data , nbytes = 32 , padding = 0 ) : assert isinstance ( data , ( bytearray , Array ) ) value = ABI . _readBE ( data , nbytes , padding = True ) value = Operators . SEXTEND ( value , nbytes * 8 , ( nbytes + padding ) * 8 ) if not issymbolic ( value ) : # sign bit on if value & ( 1 << ( nbytes * 8 - 1 ) ) : value = - ( ( ( ~ value ) + 1 ) & ( ( 1 << ( nbytes * 8 ) ) - 1 ) ) return value
11753	def make_tables ( grammar , precedence ) : ACTION = { } GOTO = { } labels = { } def get_label ( closure ) : if closure not in labels : labels [ closure ] = len ( labels ) return labels [ closure ] def resolve_shift_reduce ( lookahead , s_action , r_action ) : s_assoc , s_level = precedence [ lookahead ] r_assoc , r_level = precedence [ r_action [ 1 ] ] if s_level < r_level : return r_action elif s_level == r_level and r_assoc == LEFT : return r_action else : return s_action initial , closures , goto = grammar . closures ( ) for closure in closures : label = get_label ( closure ) for rule in closure : new_action , lookahead = None , rule . lookahead if not rule . at_end : symbol = rule . rhs [ rule . pos ] is_terminal = symbol in grammar . terminals has_goto = symbol in goto [ closure ] if is_terminal and has_goto : next_state = get_label ( goto [ closure ] [ symbol ] ) new_action , lookahead = ( 'shift' , next_state ) , symbol elif rule . production == grammar . start and rule . at_end : new_action = ( 'accept' , ) elif rule . at_end : new_action = ( 'reduce' , rule . production ) if new_action is None : continue prev_action = ACTION . get ( ( label , lookahead ) ) if prev_action is None or prev_action == new_action : ACTION [ label , lookahead ] = new_action else : types = ( prev_action [ 0 ] , new_action [ 0 ] ) if types == ( 'shift' , 'reduce' ) : chosen = resolve_shift_reduce ( lookahead , prev_action , new_action ) elif types == ( 'reduce' , 'shift' ) : chosen = resolve_shift_reduce ( lookahead , new_action , prev_action ) else : raise TableConflictError ( prev_action , new_action ) ACTION [ label , lookahead ] = chosen for symbol in grammar . nonterminals : if symbol in goto [ closure ] : GOTO [ label , symbol ] = get_label ( goto [ closure ] [ symbol ] ) return get_label ( initial ) , ACTION , GOTO
8107	def dumps ( obj , skipkeys = False , ensure_ascii = True , check_circular = True , allow_nan = True , cls = None , indent = None , separators = None , encoding = 'utf-8' , default = None , * * kw ) : # cached encoder if ( skipkeys is False and ensure_ascii is True and check_circular is True and allow_nan is True and cls is None and indent is None and separators is None and encoding == 'utf-8' and default is None and not kw ) : return _default_encoder . encode ( obj ) if cls is None : cls = JSONEncoder return cls ( skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , encoding = encoding , default = default , * * kw ) . encode ( obj )
7106	def export ( self , model_name , export_folder ) : for transformer in self . transformers : if isinstance ( transformer , MultiLabelBinarizer ) : joblib . dump ( transformer , join ( export_folder , "label.transformer.bin" ) , protocol = 2 ) if isinstance ( transformer , TfidfVectorizer ) : joblib . dump ( transformer , join ( export_folder , "tfidf.transformer.bin" ) , protocol = 2 ) if isinstance ( transformer , CountVectorizer ) : joblib . dump ( transformer , join ( export_folder , "count.transformer.bin" ) , protocol = 2 ) if isinstance ( transformer , NumberRemover ) : joblib . dump ( transformer , join ( export_folder , "number.transformer.bin" ) , protocol = 2 ) model = [ model for model in self . models if model . name == model_name ] [ 0 ] e = Experiment ( self . X , self . y , model . estimator , None ) model_filename = join ( export_folder , "model.bin" ) e . export ( model_filename )
13900	def db_to_specifier ( db_string ) : local_match = PLAIN_RE . match ( db_string ) remote_match = URL_RE . match ( db_string ) # If this looks like a local specifier: if local_match : return 'local:' + local_match . groupdict ( ) [ 'database' ] # If this looks like a remote specifier: elif remote_match : # Just a fancy way of getting 3 variables in 2 lines... hostname , portnum , database = map ( remote_match . groupdict ( ) . get , ( 'hostname' , 'portnum' , 'database' ) ) local_url = settings . _ ( 'COUCHDB_SERVER' , 'http://127.0.0.1:5984/' ) localhost , localport = urlparse . urlparse ( local_url ) [ 1 ] . split ( ':' ) # If it's the local server, then return a local specifier. if ( localhost == hostname ) and ( localport == portnum ) : return 'local:' + database # Otherwise, prepare and return the remote specifier. return 'remote:%s:%s:%s' % ( hostname , portnum , database ) # Throw a wobbly. raise ValueError ( 'Invalid database string: %r' % ( db_string , ) )
708	def runWithJsonFile ( expJsonFilePath , options , outputLabel , permWorkDir ) : if "verbosityCount" in options : verbosity = options [ "verbosityCount" ] del options [ "verbosityCount" ] else : verbosity = 1 _setupInterruptHandling ( ) with open ( expJsonFilePath , "r" ) as jsonFile : expJsonConfig = json . loads ( jsonFile . read ( ) ) outDir = os . path . dirname ( expJsonFilePath ) return runWithConfig ( expJsonConfig , options , outDir = outDir , outputLabel = outputLabel , permWorkDir = permWorkDir , verbosity = verbosity )
2531	def parse_doc_fields ( self , doc_term ) : try : self . builder . set_doc_spdx_id ( self . doc , doc_term ) except SPDXValueError : self . value_error ( 'DOC_SPDX_ID_VALUE' , doc_term ) try : if doc_term . count ( '#' , 0 , len ( doc_term ) ) <= 1 : doc_namespace = doc_term . split ( '#' ) [ 0 ] self . builder . set_doc_namespace ( self . doc , doc_namespace ) else : self . value_error ( 'DOC_NAMESPACE_VALUE' , doc_term ) except SPDXValueError : self . value_error ( 'DOC_NAMESPACE_VALUE' , doc_term ) for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'specVersion' ] , None ) ) : try : self . builder . set_doc_version ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'DOC_VERS_VALUE' , o ) except CardinalityError : self . more_than_one_error ( 'specVersion' ) break for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'dataLicense' ] , None ) ) : try : self . builder . set_doc_data_lic ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'DOC_D_LICS' , o ) except CardinalityError : self . more_than_one_error ( 'dataLicense' ) break for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'name' ] , None ) ) : try : self . builder . set_doc_name ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'name' ) break for _s , _p , o in self . graph . triples ( ( doc_term , RDFS . comment , None ) ) : try : self . builder . set_doc_comment ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'Document comment' ) break
6603	def package_fullpath ( self , package_index ) : ret = os . path . join ( self . path , self . package_relpath ( package_index ) ) # e.g., '{path}/tpd_20161129_122841_HnpcmF/task_00009.p.gz' return ret
9584	def write_var_data ( fd , data ) : # write array data elements (size info) fd . write ( struct . pack ( 'b3xI' , etypes [ 'miMATRIX' ] [ 'n' ] , len ( data ) ) ) # write the data fd . write ( data )
1482	def get_commands_to_run ( self ) : # During shutdown the watch might get triggered with the empty packing plan if len ( self . packing_plan . container_plans ) == 0 : return { } if self . _get_instance_plans ( self . packing_plan , self . shard ) is None and self . shard != 0 : retval = { } retval [ 'heron-shell' ] = Command ( [ '%s' % self . heron_shell_binary , '--port=%s' % self . shell_port , '--log_file_prefix=%s/heron-shell-%s.log' % ( self . log_dir , self . shard ) , '--secret=%s' % self . topology_id ] , self . shell_env ) return retval if self . shard == 0 : commands = self . _get_tmaster_processes ( ) else : self . _untar_if_needed ( ) commands = self . _get_streaming_processes ( ) # Attach daemon processes commands . update ( self . _get_heron_support_processes ( ) ) return commands
2937	def deserialize_data ( self , workflow , start_node ) : name = start_node . getAttribute ( 'name' ) value = start_node . getAttribute ( 'value' ) return name , value
9271	def filter_since_tag ( self , all_tags ) : tag = self . detect_since_tag ( ) if not tag or tag == REPO_CREATED_TAG_NAME : return copy . deepcopy ( all_tags ) filtered_tags = [ ] tag_names = [ t [ "name" ] for t in all_tags ] try : idx = tag_names . index ( tag ) except ValueError : self . warn_if_tag_not_found ( tag , "since-tag" ) return copy . deepcopy ( all_tags ) since_tag = all_tags [ idx ] since_date = self . get_time_of_tag ( since_tag ) for t in all_tags : tag_date = self . get_time_of_tag ( t ) if since_date <= tag_date : filtered_tags . append ( t ) return filtered_tags
1649	def GetLineWidth ( line ) : if isinstance ( line , unicode ) : width = 0 for uc in unicodedata . normalize ( 'NFC' , line ) : if unicodedata . east_asian_width ( uc ) in ( 'W' , 'F' ) : width += 2 elif not unicodedata . combining ( uc ) : width += 1 return width else : return len ( line )
12583	def _safe_cache ( memory , func , * * kwargs ) : cachedir = memory . cachedir if cachedir is None or cachedir in __CACHE_CHECKED : return memory . cache ( func , * * kwargs ) version_file = os . path . join ( cachedir , 'module_versions.json' ) versions = dict ( ) if os . path . exists ( version_file ) : with open ( version_file , 'r' ) as _version_file : versions = json . load ( _version_file ) modules = ( nibabel , ) # Keep only the major + minor version numbers my_versions = dict ( ( m . __name__ , LooseVersion ( m . __version__ ) . version [ : 2 ] ) for m in modules ) commons = set ( versions . keys ( ) ) . intersection ( set ( my_versions . keys ( ) ) ) collisions = [ m for m in commons if versions [ m ] != my_versions [ m ] ] # Flush cache if version collision if len ( collisions ) > 0 : if nilearn . CHECK_CACHE_VERSION : warnings . warn ( "Incompatible cache in %s: " "different version of nibabel. Deleting " "the cache. Put nilearn.CHECK_CACHE_VERSION " "to false to avoid this behavior." % cachedir ) try : tmp_dir = ( os . path . split ( cachedir ) [ : - 1 ] + ( 'old_%i' % os . getpid ( ) , ) ) tmp_dir = os . path . join ( * tmp_dir ) # We use rename + unlink to be more robust to race # conditions os . rename ( cachedir , tmp_dir ) shutil . rmtree ( tmp_dir ) except OSError : # Another process could have removed this dir pass try : os . makedirs ( cachedir ) except OSError : # File exists? pass else : warnings . warn ( "Incompatible cache in %s: " "old version of nibabel." % cachedir ) # Write json files if configuration is different if versions != my_versions : with open ( version_file , 'w' ) as _version_file : json . dump ( my_versions , _version_file ) __CACHE_CHECKED [ cachedir ] = True return memory . cache ( func , * * kwargs )
8071	def not_found ( url , wait = 10 ) : try : connection = open ( url , wait ) except HTTP404NotFound : return True except : return False return False
2361	def t_tabbedheredoc ( self , t ) : t . lexer . is_tabbed = True self . _init_heredoc ( t ) t . lexer . begin ( 'tabbedheredoc' )
7126	def add_download_total ( rows ) : total_row = [ "" ] * len ( rows [ 0 ] ) total_row [ 0 ] = "Total" total_downloads , downloads_column = get_download_total ( rows ) total_row [ downloads_column ] = str ( total_downloads ) rows . append ( total_row ) return rows
7974	def stop ( self , join = False , timeout = None ) : logger . debug ( "Closing the io handlers..." ) for handler in self . io_handlers : handler . close ( ) if self . event_thread and self . event_thread . is_alive ( ) : logger . debug ( "Sending the QUIT signal" ) self . event_queue . put ( QUIT ) logger . debug ( " sent" ) threads = self . io_threads + self . timeout_threads for thread in threads : logger . debug ( "Stopping thread: {0!r}" . format ( thread ) ) thread . stop ( ) if not join : return if self . event_thread : threads . append ( self . event_thread ) if timeout is None : for thread in threads : thread . join ( ) else : timeout1 = ( timeout * 0.01 ) / len ( threads ) threads_left = [ ] for thread in threads : logger . debug ( "Quick-joining thread {0!r}..." . format ( thread ) ) thread . join ( timeout1 ) if thread . is_alive ( ) : logger . debug ( " thread still alive" . format ( thread ) ) threads_left . append ( thread ) if threads_left : timeout2 = ( timeout * 0.99 ) / len ( threads_left ) for thread in threads_left : logger . debug ( "Joining thread {0!r}..." . format ( thread ) ) thread . join ( timeout2 ) self . io_threads = [ ] self . event_thread = None
1537	def add_spec ( self , * specs ) : for spec in specs : if not isinstance ( spec , HeronComponentSpec ) : raise TypeError ( "Argument to add_spec needs to be HeronComponentSpec, given: %s" % str ( spec ) ) if spec . name is None : raise ValueError ( "TopologyBuilder cannot take a spec without name" ) if spec . name == "config" : raise ValueError ( "config is a reserved name" ) if spec . name in self . _specs : raise ValueError ( "Attempting to add duplicate spec name: %r %r" % ( spec . name , spec ) ) self . _specs [ spec . name ] = spec
5537	def get_raw_output ( self , tile , _baselevel_readonly = False ) : if not isinstance ( tile , ( BufferedTile , tuple ) ) : raise TypeError ( "'tile' must be a tuple or BufferedTile" ) if isinstance ( tile , tuple ) : tile = self . config . output_pyramid . tile ( * tile ) if _baselevel_readonly : tile = self . config . baselevels [ "tile_pyramid" ] . tile ( * tile . id ) # Return empty data if zoom level is outside of process zoom levels. if tile . zoom not in self . config . zoom_levels : return self . config . output . empty ( tile ) # TODO implement reprojection if tile . crs != self . config . process_pyramid . crs : raise NotImplementedError ( "reprojection between processes not yet implemented" ) if self . config . mode == "memory" : # Determine affected process Tile and check whether it is already # cached. process_tile = self . config . process_pyramid . intersecting ( tile ) [ 0 ] return self . _extract ( in_tile = process_tile , in_data = self . _execute_using_cache ( process_tile ) , out_tile = tile ) # TODO: cases where tile intersects with multiple process tiles process_tile = self . config . process_pyramid . intersecting ( tile ) [ 0 ] # get output_tiles that intersect with current tile if tile . pixelbuffer > self . config . output . pixelbuffer : output_tiles = list ( self . config . output_pyramid . tiles_from_bounds ( tile . bounds , tile . zoom ) ) else : output_tiles = self . config . output_pyramid . intersecting ( tile ) if self . config . mode == "readonly" or _baselevel_readonly : if self . config . output . tiles_exist ( process_tile ) : return self . _read_existing_output ( tile , output_tiles ) else : return self . config . output . empty ( tile ) elif self . config . mode == "continue" and not _baselevel_readonly : if self . config . output . tiles_exist ( process_tile ) : return self . _read_existing_output ( tile , output_tiles ) else : return self . _process_and_overwrite_output ( tile , process_tile ) elif self . config . mode == "overwrite" and not _baselevel_readonly : return self . _process_and_overwrite_output ( tile , process_tile )
9846	def resample_factor ( self , factor ) : # new number of edges N' = (N-1)*f + 1 newlengths = [ ( N - 1 ) * float ( factor ) + 1 for N in self . _len_edges ( ) ] edges = [ numpy . linspace ( start , stop , num = int ( N ) , endpoint = True ) for ( start , stop , N ) in zip ( self . _min_edges ( ) , self . _max_edges ( ) , newlengths ) ] return self . resample ( edges )
7668	def trim ( self , start_time , end_time , strict = False ) : trimmed_array = AnnotationArray ( ) for ann in self : trimmed_array . append ( ann . trim ( start_time , end_time , strict = strict ) ) return trimmed_array
10156	def merge_dicts ( base , changes ) : for k , v in changes . items ( ) : if isinstance ( v , dict ) : merge_dicts ( base . setdefault ( k , { } ) , v ) else : base . setdefault ( k , v )
7193	def histogram_match ( self , use_bands , blm_source = None , * * kwargs ) : assert has_rio , "To match image histograms please install rio_hist" data = self . _read ( self [ use_bands , ... ] , * * kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) if 0 in data : data = np . ma . masked_values ( data , 0 ) bounds = self . _reproject ( box ( * self . bounds ) , from_proj = self . proj , to_proj = "EPSG:4326" ) . bounds if blm_source == 'browse' : from gbdxtools . images . browse_image import BrowseImage ref = BrowseImage ( self . cat_id , bbox = bounds ) . read ( ) else : from gbdxtools . images . tms_image import TmsImage tms = TmsImage ( zoom = self . _calc_tms_zoom ( self . affine [ 0 ] ) , bbox = bounds , * * kwargs ) ref = np . rollaxis ( tms . read ( ) , 0 , 3 ) out = np . dstack ( [ rio_match ( data [ : , : , idx ] , ref [ : , : , idx ] . astype ( np . double ) / 255.0 ) for idx in range ( data . shape [ - 1 ] ) ] ) if 'stretch' in kwargs or 'gamma' in kwargs : return self . _histogram_stretch ( out , * * kwargs ) else : return out
6491	def _process_exclude_dictionary ( exclude_dictionary ) : # not_properties will hold the generated term queries. not_properties = [ ] for exclude_property in exclude_dictionary : exclude_values = exclude_dictionary [ exclude_property ] if not isinstance ( exclude_values , list ) : exclude_values = [ exclude_values ] not_properties . extend ( [ { "term" : { exclude_property : exclude_value } } for exclude_value in exclude_values ] ) # Returning a query segment with an empty list freaks out ElasticSearch, # so just return an empty segment. if not not_properties : return { } return { "not" : { "filter" : { "or" : not_properties } } }
3345	def parse_if_header_dict ( environ ) : if "wsgidav.conditions.if" in environ : return if "HTTP_IF" not in environ : environ [ "wsgidav.conditions.if" ] = None environ [ "wsgidav.ifLockTokenList" ] = [ ] return iftext = environ [ "HTTP_IF" ] . strip ( ) if not iftext . startswith ( "<" ) : iftext = "<*>" + iftext ifDict = dict ( [ ] ) ifLockList = [ ] resource1 = "*" for ( tmpURLVar , URLVar , _tmpContentVar , contentVar ) in reIfSeparator . findall ( iftext ) : if tmpURLVar != "" : resource1 = URLVar else : listTagContents = [ ] testflag = True for listitem in reIfTagListContents . findall ( contentVar ) : if listitem . upper ( ) != "NOT" : if listitem . startswith ( "[" ) : listTagContents . append ( ( testflag , "entity" , listitem . strip ( '"[]' ) ) ) else : listTagContents . append ( ( testflag , "locktoken" , listitem . strip ( "<>" ) ) ) ifLockList . append ( listitem . strip ( "<>" ) ) testflag = listitem . upper ( ) != "NOT" if resource1 in ifDict : listTag = ifDict [ resource1 ] else : listTag = [ ] ifDict [ resource1 ] = listTag listTag . append ( listTagContents ) environ [ "wsgidav.conditions.if" ] = ifDict environ [ "wsgidav.ifLockTokenList" ] = ifLockList _logger . debug ( "parse_if_header_dict\n{}" . format ( pformat ( ifDict ) ) ) return
2928	def write_to_package_zip ( self , filename , data ) : self . manifest [ filename ] = md5hash ( data ) self . package_zip . writestr ( filename , data )
5998	def plot_grid ( grid_arcsec , array , units , kpc_per_arcsec , pointsize , zoom_offset_arcsec ) : if grid_arcsec is not None : if zoom_offset_arcsec is not None : grid_arcsec -= zoom_offset_arcsec grid_units = convert_grid_units ( grid_arcsec = grid_arcsec , array = array , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = np . asarray ( grid_units [ : , 0 ] ) , x = np . asarray ( grid_units [ : , 1 ] ) , s = pointsize , c = 'k' )
1301	def mouse_event ( dwFlags : int , dx : int , dy : int , dwData : int , dwExtraInfo : int ) -> None : ctypes . windll . user32 . mouse_event ( dwFlags , dx , dy , dwData , dwExtraInfo )
6995	def runcp_producer_loop_savedstate ( use_saved_state = None , lightcurve_list = None , input_queue = None , input_bucket = None , result_queue = None , result_bucket = None , pfresult_list = None , runcp_kwargs = None , process_list_slice = None , download_when_done = True , purge_queues_when_done = True , save_state_when_done = True , delete_queues_when_done = False , s3_client = None , sqs_client = None ) : if use_saved_state is not None and os . path . exists ( use_saved_state ) : with open ( use_saved_state , 'rb' ) as infd : saved_state = pickle . load ( infd ) # run the producer loop using the saved state's todo list return runcp_producer_loop ( saved_state [ 'in_progress' ] , saved_state [ 'args' ] [ 1 ] , saved_state [ 'args' ] [ 2 ] , saved_state [ 'args' ] [ 3 ] , saved_state [ 'args' ] [ 4 ] , * * saved_state [ 'kwargs' ] ) else : return runcp_producer_loop ( lightcurve_list , input_queue , input_bucket , result_queue , result_bucket , pfresult_list = pfresult_list , runcp_kwargs = runcp_kwargs , process_list_slice = process_list_slice , download_when_done = download_when_done , purge_queues_when_done = purge_queues_when_done , save_state_when_done = save_state_when_done , delete_queues_when_done = delete_queues_when_done , s3_client = s3_client , sqs_client = sqs_client )
251	def get_turnover ( positions , transactions , denominator = 'AGB' ) : txn_vol = get_txn_vol ( transactions ) traded_value = txn_vol . txn_volume if denominator == 'AGB' : # Actual gross book is the same thing as the algo's GMV # We want our denom to be avg(AGB previous, AGB current) AGB = positions . drop ( 'cash' , axis = 1 ) . abs ( ) . sum ( axis = 1 ) denom = AGB . rolling ( 2 ) . mean ( ) # Since the first value of pd.rolling returns NaN, we # set our "day 0" AGB to 0. denom . iloc [ 0 ] = AGB . iloc [ 0 ] / 2 elif denominator == 'portfolio_value' : denom = positions . sum ( axis = 1 ) else : raise ValueError ( "Unexpected value for denominator '{}'. The " "denominator parameter must be either 'AGB'" " or 'portfolio_value'." . format ( denominator ) ) denom . index = denom . index . normalize ( ) turnover = traded_value . div ( denom , axis = 'index' ) turnover = turnover . fillna ( 0 ) return turnover
6172	def _select_manager ( backend_name ) : if backend_name == 'RedisBackend' : lock_manager = _LockManagerRedis elif backend_name == 'DatabaseBackend' : lock_manager = _LockManagerDB else : raise NotImplementedError return lock_manager
4813	def _document_frequency ( X ) : if sp . isspmatrix_csr ( X ) : return np . bincount ( X . indices , minlength = X . shape [ 1 ] ) return np . diff ( sp . csc_matrix ( X , copy = False ) . indptr )
2691	def iter_cython ( path ) : for dir_path , dir_names , file_names in os . walk ( path ) : for file_name in file_names : if file_name . startswith ( '.' ) : continue if os . path . splitext ( file_name ) [ 1 ] not in ( '.pyx' , '.pxd' ) : continue yield os . path . join ( dir_path , file_name )
12364	def transfer ( self , region ) : action = self . post ( type = 'transfer' , region = region ) [ 'action' ] return self . parent . get ( action [ 'resource_id' ] )
9928	def authenticate ( self , request , email = None , password = None , username = None ) : email = email or username try : email_instance = models . EmailAddress . objects . get ( is_verified = True , email = email ) except models . EmailAddress . DoesNotExist : return None user = email_instance . user if user . check_password ( password ) : return user return None
7667	def search ( self , * * kwargs ) : results = AnnotationArray ( ) for annotation in self : if annotation . search ( * * kwargs ) : results . append ( annotation ) return results
13624	def Integer ( value , base = 10 , encoding = None ) : try : return int ( Text ( value , encoding ) , base ) except ( TypeError , ValueError ) : return None
7659	def append_columns ( self , columns ) : self . append_records ( [ dict ( time = t , duration = d , value = v , confidence = c ) for ( t , d , v , c ) in six . moves . zip ( columns [ 'time' ] , columns [ 'duration' ] , columns [ 'value' ] , columns [ 'confidence' ] ) ] )
9424	def _open ( self , archive ) : try : handle = unrarlib . RAROpenArchiveEx ( ctypes . byref ( archive ) ) except unrarlib . UnrarException : raise BadRarFile ( "Invalid RAR file." ) return handle
8141	def translate ( self , x , y ) : self . x = x self . y = y
11070	def proxy_factory ( BaseSchema , label , ProxiedClass , get_key ) : def local ( ) : key = get_key ( ) try : return proxies [ BaseSchema ] [ label ] [ key ] except KeyError : proxies [ BaseSchema ] [ label ] [ key ] = ProxiedClass ( ) return proxies [ BaseSchema ] [ label ] [ key ] return LocalProxy ( local )
7683	def display_multi ( annotations , fig_kw = None , meta = True , * * kwargs ) : if fig_kw is None : fig_kw = dict ( ) fig_kw . setdefault ( 'sharex' , True ) fig_kw . setdefault ( 'squeeze' , True ) # Filter down to coercable annotations first display_annotations = [ ] for ann in annotations : for namespace in VIZ_MAPPING : if can_convert ( ann , namespace ) : display_annotations . append ( ann ) break # If there are no displayable annotations, fail here if not len ( display_annotations ) : raise ParameterError ( 'No displayable annotations found' ) fig , axs = plt . subplots ( nrows = len ( display_annotations ) , ncols = 1 , * * fig_kw ) # MPL is stupid when making singleton subplots. # We catch this and make it always iterable. if len ( display_annotations ) == 1 : axs = [ axs ] for ann , ax in zip ( display_annotations , axs ) : kwargs [ 'ax' ] = ax display ( ann , meta = meta , * * kwargs ) return fig , axs
558	def bestModelInSprint ( self , sprintIdx ) : # Get all the swarms in this sprint swarms = self . getAllSwarms ( sprintIdx ) # Get the best model and score from each swarm bestModelId = None bestErrScore = numpy . inf for swarmId in swarms : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) if errScore < bestErrScore : bestModelId = modelId bestErrScore = errScore return ( bestModelId , bestErrScore )
13367	def register_proxy_type ( cls , real_type , proxy_type ) : if distob . engine is None : cls . _initial_proxy_types [ real_type ] = proxy_type elif isinstance ( distob . engine , ObjectHub ) : distob . engine . _runtime_reg_proxy_type ( real_type , proxy_type ) else : # TODO: remove next line after issue #58 in dill is fixed. distob . engine . _singleeng_reg_proxy_type ( real_type , proxy_type ) pass
8573	def delete_nic ( self , datacenter_id , server_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'DELETE' ) return response
7032	def check_existing_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) if os . path . exists ( APIKEYFILE ) : # check if this file is readable/writeable by user only fileperm = oct ( os . stat ( APIKEYFILE ) [ stat . ST_MODE ] ) if fileperm == '0100600' or fileperm == '0o100600' : with open ( APIKEYFILE ) as infd : apikey , expires = infd . read ( ) . strip ( '\n' ) . split ( ) # get today's datetime now = datetime . now ( utc ) if sys . version_info [ : 2 ] < ( 3 , 7 ) : # this hideous incantation is required for lesser Pythons expdt = datetime . strptime ( expires . replace ( 'Z' , '' ) , '%Y-%m-%dT%H:%M:%S.%f' ) . replace ( tzinfo = utc ) else : expdt = datetime . fromisoformat ( expires . replace ( 'Z' , '+00:00' ) ) if now > expdt : LOGERROR ( 'API key has expired. expiry was on: %s' % expires ) return False , apikey , expires else : return True , apikey , expires else : LOGWARNING ( 'The API key file %s has bad permissions ' 'and is insecure, not reading it.\n' '(you need to chmod 600 this file)' % APIKEYFILE ) return False , None , None else : LOGWARNING ( 'No LCC-Server API key ' 'found in: {apikeyfile}' . format ( apikeyfile = APIKEYFILE ) ) return False , None , None
4445	def create_index ( self , fields , no_term_offsets = False , no_field_flags = False , stopwords = None ) : args = [ self . CREATE_CMD , self . index_name ] if no_term_offsets : args . append ( self . NOOFFSETS ) if no_field_flags : args . append ( self . NOFIELDS ) if stopwords is not None and isinstance ( stopwords , ( list , tuple , set ) ) : args += [ self . STOPWORDS , len ( stopwords ) ] if len ( stopwords ) > 0 : args += list ( stopwords ) args . append ( 'SCHEMA' ) args += list ( itertools . chain ( * ( f . redis_args ( ) for f in fields ) ) ) return self . redis . execute_command ( * args )
1897	def _assert ( self , expression : Bool ) : assert isinstance ( expression , Bool ) smtlib = translate_to_smtlib ( expression ) self . _send ( '(assert %s)' % smtlib )
10254	def get_causal_out_edges ( graph : BELGraph , nbunch : Union [ BaseEntity , Iterable [ BaseEntity ] ] , ) -> Set [ Tuple [ BaseEntity , BaseEntity ] ] : return { ( u , v ) for u , v , k , d in graph . out_edges ( nbunch , keys = True , data = True ) if is_causal_relation ( graph , u , v , k , d ) }
11798	def suppose ( self , var , value ) : self . support_pruning ( ) removals = [ ( var , a ) for a in self . curr_domains [ var ] if a != value ] self . curr_domains [ var ] = [ value ] return removals
5614	def segmentize_geometry ( geometry , segmentize_value ) : if geometry . geom_type != "Polygon" : raise TypeError ( "segmentize geometry type must be Polygon" ) return Polygon ( LinearRing ( [ p # pick polygon linestrings for l in map ( lambda x : LineString ( [ x [ 0 ] , x [ 1 ] ] ) , zip ( geometry . exterior . coords [ : - 1 ] , geometry . exterior . coords [ 1 : ] ) ) # interpolate additional points in between and don't forget end point for p in [ l . interpolate ( segmentize_value * i ) . coords [ 0 ] for i in range ( int ( l . length / segmentize_value ) ) ] + [ l . coords [ 1 ] ] ] ) )
13131	def parse_domain_computers ( filename ) : with open ( filename ) as f : data = json . loads ( f . read ( ) ) hs = HostSearch ( ) count = 0 entry_count = 0 print_notification ( "Parsing {} entries" . format ( len ( data ) ) ) for system in data : entry_count += 1 parsed = parse_single_computer ( system ) if parsed . ip : try : host = hs . id_to_object ( parsed . ip ) host . description . append ( parsed . description ) host . hostname . append ( parsed . dns_hostname ) if parsed . os : host . os = parsed . os host . domain_controller = parsed . dc host . add_tag ( 'domaindump' ) host . save ( ) count += 1 except ValueError : pass sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}] {} resolved" . format ( entry_count , len ( data ) , count ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
879	def __getLogger ( cls ) : if cls . __logger is None : cls . __logger = opf_utils . initLogger ( cls ) return cls . __logger
10980	def remove ( group_id , user_id ) : group = Group . query . get_or_404 ( group_id ) user = User . query . get_or_404 ( user_id ) if group . can_edit ( current_user ) : try : group . remove_member ( user ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( urlparse ( request . referrer ) . path ) flash ( _ ( 'User %(user_email)s was removed from %(group_name)s group.' , user_email = user . email , group_name = group . name ) , 'success' ) return redirect ( urlparse ( request . referrer ) . path ) flash ( _ ( 'You cannot delete users of the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
10132	def parse_grid ( grid_data ) : try : # Split the grid up. grid_parts = NEWLINE_RE . split ( grid_data ) if len ( grid_parts ) < 2 : raise ZincParseException ( 'Malformed grid received' , grid_data , 1 , 1 ) # Grid and column metadata are the first two lines. grid_meta_str = grid_parts . pop ( 0 ) col_meta_str = grid_parts . pop ( 0 ) # First element is the grid metadata ver_match = VERSION_RE . match ( grid_meta_str ) if ver_match is None : raise ZincParseException ( 'Could not determine version from %r' % grid_meta_str , grid_data , 1 , 1 ) version = Version ( ver_match . group ( 1 ) ) # Now parse the rest of the grid accordingly try : grid_meta = hs_gridMeta [ version ] . parseString ( grid_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse grid metadata: %s' % pe , grid_data , 1 , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse grid meta: %r' , grid_meta_str ) raise try : col_meta = hs_cols [ version ] . parseString ( col_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse column metadata: %s' % reformat_exception ( pe , 2 ) , grid_data , 2 , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse column meta: %r' , col_meta_str ) raise row_grammar = hs_row [ version ] def _parse_row ( row_num_and_data ) : ( row_num , row ) = row_num_and_data line_num = row_num + 3 try : return dict ( zip ( col_meta . keys ( ) , row_grammar . parseString ( row , parseAll = True ) [ 0 ] . asList ( ) ) ) except pp . ParseException as pe : # Raise a new exception with the appropriate line number. raise ZincParseException ( 'Failed to parse row: %s' % reformat_exception ( pe , line_num ) , grid_data , line_num , pe . col ) except : # pragma: no cover # Report an error to the log if we fail to parse something. LOG . debug ( 'Failed to parse row: %r' , row ) raise g = Grid ( version = grid_meta . pop ( 'ver' ) , metadata = grid_meta , columns = list ( col_meta . items ( ) ) ) g . extend ( map ( _parse_row , filter ( lambda gp : bool ( gp [ 1 ] ) , enumerate ( grid_parts ) ) ) ) return g except : LOG . debug ( 'Failing grid: %r' , grid_data ) raise
6408	def lmean ( nums ) : if len ( nums ) != len ( set ( nums ) ) : raise AttributeError ( 'No two values in the nums list may be equal' ) rolling_sum = 0 for i in range ( len ( nums ) ) : rolling_prod = 1 for j in range ( len ( nums ) ) : if i != j : rolling_prod *= math . log ( nums [ i ] / nums [ j ] ) rolling_sum += nums [ i ] / rolling_prod return math . factorial ( len ( nums ) - 1 ) * rolling_sum
10409	def finalized_canonical_averages_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'number_of_runs' , 'uint32' ) , ( 'p' , 'float64' ) , ( 'alpha' , 'float64' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability_mean' , 'float64' ) , ( 'percolation_probability_std' , 'float64' ) , ( 'percolation_probability_ci' , '(2,)float64' ) , ] ) fields . extend ( [ ( 'percolation_strength_mean' , 'float64' ) , ( 'percolation_strength_std' , 'float64' ) , ( 'percolation_strength_ci' , '(2,)float64' ) , ( 'moments_mean' , '(5,)float64' ) , ( 'moments_std' , '(5,)float64' ) , ( 'moments_ci' , '(5,2)float64' ) , ] ) return _ndarray_dtype ( fields )
360	def load_folder_list ( path = "" ) : return [ os . path . join ( path , o ) for o in os . listdir ( path ) if os . path . isdir ( os . path . join ( path , o ) ) ]
4869	def to_representation ( self , instance ) : updated_program = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_program [ 'enrollment_url' ] = enterprise_customer_catalog . get_program_enrollment_url ( updated_program [ 'uuid' ] ) for course in updated_program [ 'courses' ] : course [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_enrollment_url ( course [ 'key' ] ) for course_run in course [ 'course_runs' ] : course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( course_run [ 'key' ] ) return updated_program
8993	def folder ( self , folder ) : result = [ ] for root , _ , files in os . walk ( folder ) : for file in files : path = os . path . join ( root , file ) if self . _chooses_path ( path ) : result . append ( self . path ( path ) ) return result
13203	def format_authors ( self , format = 'html5' , deparagraph = True , mathjax = False , smart = True , extra_args = None ) : formatted_authors = [ ] for latex_author in self . authors : formatted_author = convert_lsstdoc_tex ( latex_author , format , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) # removes Pandoc's terminal newlines formatted_author = formatted_author . strip ( ) formatted_authors . append ( formatted_author ) return formatted_authors
5483	def setup_service ( api_name , api_version , credentials = None ) : if not credentials : credentials = oauth2client . client . GoogleCredentials . get_application_default ( ) return apiclient . discovery . build ( api_name , api_version , credentials = credentials )
2929	def write_manifest ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'Manifest' ) for f in sorted ( self . manifest . keys ( ) ) : config . set ( 'Manifest' , f . replace ( '\\' , '/' ) . lower ( ) , self . manifest [ f ] ) ini = StringIO ( ) config . write ( ini ) self . manifest_data = ini . getvalue ( ) self . package_zip . writestr ( self . MANIFEST_FILE , self . manifest_data )
8958	def walk ( self , * * kwargs ) : lead = '' if 'with_root' in kwargs and kwargs . pop ( 'with_root' ) : lead = self . root . rstrip ( os . sep ) + os . sep for base , dirs , files in os . walk ( self . root , * * kwargs ) : prefix = base [ len ( self . root ) : ] . lstrip ( os . sep ) bits = prefix . split ( os . sep ) if prefix else [ ] for dirname in dirs [ : ] : path = '/' . join ( bits + [ dirname ] ) inclusive = self . included ( path , is_dir = True ) if inclusive : yield lead + path + '/' elif inclusive is False : dirs . remove ( dirname ) for filename in files : path = '/' . join ( bits + [ filename ] ) if self . included ( path ) : yield lead + path
13308	def gmv ( a , b ) : return np . exp ( np . square ( np . log ( a ) - np . log ( b ) ) . mean ( ) )
9543	def add_record_length_check ( self , code = RECORD_LENGTH_CHECK_FAILED , message = MESSAGES [ RECORD_LENGTH_CHECK_FAILED ] , modulus = 1 ) : t = code , message , modulus self . _record_length_checks . append ( t )
12756	def set_target_angles ( self , angles ) : j = 0 for joint in self . joints : velocities = [ ctrl ( tgt - cur , self . world . dt ) for cur , tgt , ctrl in zip ( joint . angles , angles [ j : j + joint . ADOF ] , joint . controllers ) ] joint . velocities = velocities j += joint . ADOF
10596	def h_x ( self , L , theta , Ts , * * statef ) : Nu_x = self . Nu_x ( L , theta , Ts , * * statef ) k = self . _fluid . k ( T = self . Tr ) return Nu_x * k / L
12447	def render_to_string ( self ) : values = '' for key , value in self . items ( ) : values += '{}={};' . format ( key , value ) return values
12341	def _set_path ( self , path ) : import os . path self . path = os . path . abspath ( path ) self . dirname = os . path . dirname ( path ) self . basename = os . path . basename ( path )
8871	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : column = l . find ( self . literal ) if column != - 1 : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}, col {}' . format ( str ( truePosition + 1 ) , column ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) return truePosition # No Match found self . failed = True raise DirectiveException ( self )
6946	def jhk_to_imag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , IJHK , IJH , IJK , IHK , IJ , IH , IK )
5728	def _get_responses_unix ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : select_timeout = timeout_time_sec - time . time ( ) # I prefer to not pass a negative value to select if select_timeout <= 0 : select_timeout = 0 events , _ , _ = select . select ( self . read_list , [ ] , [ ] , select_timeout ) responses_list = None # to avoid infinite loop if using Python 2 try : for fileno in events : # new data is ready to read if fileno == self . stdout_fileno : self . gdb_process . stdout . flush ( ) raw_output = self . gdb_process . stdout . read ( ) stream = "stdout" elif fileno == self . stderr_fileno : self . gdb_process . stderr . flush ( ) raw_output = self . gdb_process . stderr . read ( ) stream = "stderr" else : raise ValueError ( "Developer error. Got unexpected file number %d" % fileno ) responses_list = self . _get_responses_list ( raw_output , stream ) responses += responses_list except IOError : # only occurs in python 2.7 pass if timeout_sec == 0 : # just exit immediately break elif responses_list and self . _allow_overwrite_timeout_times : # update timeout time to potentially be closer to now to avoid lengthy wait times when nothing is being output by gdb timeout_time_sec = min ( time . time ( ) + self . time_to_check_for_additional_output_sec , timeout_time_sec , ) elif time . time ( ) > timeout_time_sec : break return responses
9904	def post_process ( self , group , event , is_new , is_sample , * * kwargs ) : if not self . is_configured ( group . project ) : return host = self . get_option ( 'server_host' , group . project ) port = int ( self . get_option ( 'server_port' , group . project ) ) prefix = self . get_option ( 'prefix' , group . project ) hostname = self . get_option ( 'hostname' , group . project ) or socket . gethostname ( ) resolve_age = group . project . get_option ( 'sentry:resolve_age' , None ) now = int ( time . time ( ) ) template = '%s.%%s[%s]' % ( prefix , group . project . slug ) level = group . get_level_display ( ) label = template % level groups = group . project . group_set . filter ( status = STATUS_UNRESOLVED ) if resolve_age : oldest = timezone . now ( ) - timedelta ( hours = int ( resolve_age ) ) groups = groups . filter ( last_seen__gt = oldest ) num_errors = groups . filter ( level = group . level ) . count ( ) metric = Metric ( hostname , label , num_errors , now ) log . info ( 'will send %s=%s to zabbix' , label , num_errors ) send_to_zabbix ( [ metric ] , host , port )
9664	def construct_graph ( sakefile , settings ) : verbose = settings [ "verbose" ] sprint = settings [ "sprint" ] G = nx . DiGraph ( ) sprint ( "Going to construct Graph" , level = "verbose" ) for target in sakefile : if target == "all" : # we don't want this node continue if "formula" not in sakefile [ target ] : # that means this is a meta target for atomtarget in sakefile [ target ] : if atomtarget == "help" : continue sprint ( "Adding '{}'" . format ( atomtarget ) , level = "verbose" ) data_dict = sakefile [ target ] [ atomtarget ] data_dict [ "parent" ] = target G . add_node ( atomtarget , * * data_dict ) else : sprint ( "Adding '{}'" . format ( target ) , level = "verbose" ) G . add_node ( target , * * sakefile [ target ] ) sprint ( "Nodes are built\nBuilding connections" , level = "verbose" ) for node in G . nodes ( data = True ) : sprint ( "checking node {} for dependencies" . format ( node [ 0 ] ) , level = "verbose" ) # normalize all paths in output for k , v in node [ 1 ] . items ( ) : if v is None : node [ 1 ] [ k ] = [ ] if "output" in node [ 1 ] : for index , out in enumerate ( node [ 1 ] [ 'output' ] ) : node [ 1 ] [ 'output' ] [ index ] = clean_path ( node [ 1 ] [ 'output' ] [ index ] ) if "dependencies" not in node [ 1 ] : continue sprint ( "it has dependencies" , level = "verbose" ) connects = [ ] # normalize all paths in dependencies for index , dep in enumerate ( node [ 1 ] [ 'dependencies' ] ) : dep = os . path . normpath ( dep ) shrt = "dependencies" node [ 1 ] [ 'dependencies' ] [ index ] = clean_path ( node [ 1 ] [ shrt ] [ index ] ) for node in G . nodes ( data = True ) : connects = [ ] if "dependencies" not in node [ 1 ] : continue for dep in node [ 1 ] [ 'dependencies' ] : matches = check_for_dep_in_outputs ( dep , verbose , G ) if not matches : continue for match in matches : sprint ( "Appending {} to matches" . format ( match ) , level = "verbose" ) connects . append ( match ) if connects : for connect in connects : G . add_edge ( connect , node [ 0 ] ) return G
905	def write ( self , proto ) : proto . iteration = self . _iteration pHistScores = proto . init ( 'historicalScores' , len ( self . _historicalScores ) ) for i , score in enumerate ( list ( self . _historicalScores ) ) : _ , value , anomalyScore = score record = pHistScores [ i ] record . value = float ( value ) record . anomalyScore = float ( anomalyScore ) if self . _distribution : proto . distribution . name = self . _distribution [ "distribution" ] [ "name" ] proto . distribution . mean = float ( self . _distribution [ "distribution" ] [ "mean" ] ) proto . distribution . variance = float ( self . _distribution [ "distribution" ] [ "variance" ] ) proto . distribution . stdev = float ( self . _distribution [ "distribution" ] [ "stdev" ] ) proto . distribution . movingAverage . windowSize = float ( self . _distribution [ "movingAverage" ] [ "windowSize" ] ) historicalValues = self . _distribution [ "movingAverage" ] [ "historicalValues" ] pHistValues = proto . distribution . movingAverage . init ( "historicalValues" , len ( historicalValues ) ) for i , value in enumerate ( historicalValues ) : pHistValues [ i ] = float ( value ) #proto.distribution.movingAverage.historicalValues = self._distribution["movingAverage"]["historicalValues"] proto . distribution . movingAverage . total = float ( self . _distribution [ "movingAverage" ] [ "total" ] ) historicalLikelihoods = self . _distribution [ "historicalLikelihoods" ] pHistLikelihoods = proto . distribution . init ( "historicalLikelihoods" , len ( historicalLikelihoods ) ) for i , likelihood in enumerate ( historicalLikelihoods ) : pHistLikelihoods [ i ] = float ( likelihood ) proto . probationaryPeriod = self . _probationaryPeriod proto . learningPeriod = self . _learningPeriod proto . reestimationPeriod = self . _reestimationPeriod proto . historicWindowSize = self . _historicalScores . maxlen
8583	def get_attached_volume ( self , datacenter_id , server_id , volume_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes/%s' % ( datacenter_id , server_id , volume_id ) ) return response
6411	def heronian_mean ( nums ) : mag = len ( nums ) rolling_sum = 0 for i in range ( mag ) : for j in range ( i , mag ) : if nums [ i ] == nums [ j ] : rolling_sum += nums [ i ] else : rolling_sum += ( nums [ i ] * nums [ j ] ) ** 0.5 return rolling_sum * 2 / ( mag * ( mag + 1 ) )
2815	def convert_avgpool ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width = params [ 'kernel_shape' ] else : height , width = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width = params [ 'strides' ] else : stride_height , stride_width = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , _ , _ = params [ 'pads' ] else : padding_h , padding_w = params [ 'padding' ] input_name = inputs [ 0 ] pad = 'valid' if height % 2 == 1 and width % 2 == 1 and height // 2 == padding_h and width // 2 == padding_w and stride_height == 1 and stride_width == 1 : pad = 'same' else : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding2D ( padding = ( padding_h , padding_w ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name # Pooling type AveragePooling2D pooling = keras . layers . AveragePooling2D ( pool_size = ( height , width ) , strides = ( stride_height , stride_width ) , padding = pad , name = tf_name , data_format = 'channels_first' ) layers [ scope_name ] = pooling ( layers [ input_name ] )
12789	def get ( self , q = None , page = None ) : # Check cache to exit early if needed etag = generate_etag ( current_ext . content_version . encode ( 'utf8' ) ) self . check_etag ( etag , weak = True ) # Build response res = jsonify ( current_ext . styles ) res . set_etag ( etag ) return res
9942	def clear_dir ( self , path ) : dirs , files = self . storage . listdir ( path ) for f in files : fpath = os . path . join ( path , f ) if self . dry_run : self . log ( "Pretending to delete '%s'" % smart_text ( fpath ) , level = 1 ) else : self . log ( "Deleting '%s'" % smart_text ( fpath ) , level = 1 ) self . storage . delete ( fpath ) for d in dirs : self . clear_dir ( os . path . join ( path , d ) )
2563	def pull_tasks ( self , kill_event ) : logger . info ( "[TASK PULL THREAD] starting" ) poller = zmq . Poller ( ) poller . register ( self . task_incoming , zmq . POLLIN ) # Send a registration message msg = self . create_reg_message ( ) logger . debug ( "Sending registration message: {}" . format ( msg ) ) self . task_incoming . send ( msg ) last_beat = time . time ( ) last_interchange_contact = time . time ( ) task_recv_counter = 0 poll_timer = 1 while not kill_event . is_set ( ) : time . sleep ( LOOP_SLOWDOWN ) ready_worker_count = self . ready_worker_queue . qsize ( ) pending_task_count = self . pending_task_queue . qsize ( ) logger . debug ( "[TASK_PULL_THREAD] ready workers:{}, pending tasks:{}" . format ( ready_worker_count , pending_task_count ) ) if time . time ( ) > last_beat + self . heartbeat_period : self . heartbeat ( ) last_beat = time . time ( ) if pending_task_count < self . max_queue_size and ready_worker_count > 0 : logger . debug ( "[TASK_PULL_THREAD] Requesting tasks: {}" . format ( ready_worker_count ) ) msg = ( ( ready_worker_count ) . to_bytes ( 4 , "little" ) ) self . task_incoming . send ( msg ) socks = dict ( poller . poll ( timeout = poll_timer ) ) if self . task_incoming in socks and socks [ self . task_incoming ] == zmq . POLLIN : _ , pkl_msg = self . task_incoming . recv_multipart ( ) tasks = pickle . loads ( pkl_msg ) last_interchange_contact = time . time ( ) if tasks == 'STOP' : logger . critical ( "[TASK_PULL_THREAD] Received stop request" ) kill_event . set ( ) break elif tasks == HEARTBEAT_CODE : logger . debug ( "Got heartbeat from interchange" ) else : # Reset timer on receiving message poll_timer = 1 task_recv_counter += len ( tasks ) logger . debug ( "[TASK_PULL_THREAD] Got tasks: {} of {}" . format ( [ t [ 'task_id' ] for t in tasks ] , task_recv_counter ) ) for task in tasks : self . pending_task_queue . put ( task ) else : logger . debug ( "[TASK_PULL_THREAD] No incoming tasks" ) # Limit poll duration to heartbeat_period # heartbeat_period is in s vs poll_timer in ms poll_timer = min ( self . heartbeat_period * 1000 , poll_timer * 2 ) # Only check if no messages were received. if time . time ( ) > last_interchange_contact + self . heartbeat_threshold : logger . critical ( "[TASK_PULL_THREAD] Missing contact with interchange beyond heartbeat_threshold" ) kill_event . set ( ) logger . critical ( "[TASK_PULL_THREAD] Exiting" ) break
2544	def set_file_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_comment_set : self . file_comment_set = True self . file ( doc ) . comment = text return True else : raise CardinalityError ( 'File::Comment' ) else : raise OrderError ( 'File::Comment' )
11968	def _dec_to_bin ( ip ) : bits = [ ] while ip : bits . append ( _BYTES_TO_BITS [ ip & 255 ] ) ip >>= 8 bits . reverse ( ) return '' . join ( bits ) or 32 * '0'
8933	def run ( cmd , * * kw ) : kw = kw . copy ( ) kw . setdefault ( 'warn' , False ) # make extra sure errors don't get silenced report_error = kw . pop ( 'report_error' , True ) runner = kw . pop ( 'runner' , invoke_run ) try : return runner ( cmd , * * kw ) except exceptions . Failure as exc : sys . stdout . flush ( ) sys . stderr . flush ( ) if report_error : notify . error ( "Command `{}` failed with RC={}!" . format ( cmd , exc . result . return_code , ) ) raise finally : sys . stdout . flush ( ) sys . stderr . flush ( )
7603	def get_player ( self , tag : crtag , timeout = None ) : url = self . api . PLAYER + '/' + tag return self . _get_model ( url , FullPlayer , timeout = timeout )
10811	def query_by_names ( cls , names ) : assert isinstance ( names , list ) return cls . query . filter ( cls . name . in_ ( names ) )
2748	def get_all_droplets ( self , tag_name = None ) : params = dict ( ) if tag_name : params [ "tag_name" ] = tag_name data = self . get_data ( "droplets/" , params = params ) droplets = list ( ) for jsoned in data [ 'droplets' ] : droplet = Droplet ( * * jsoned ) droplet . token = self . token for net in droplet . networks [ 'v4' ] : if net [ 'type' ] == 'private' : droplet . private_ip_address = net [ 'ip_address' ] if net [ 'type' ] == 'public' : droplet . ip_address = net [ 'ip_address' ] if droplet . networks [ 'v6' ] : droplet . ip_v6_address = droplet . networks [ 'v6' ] [ 0 ] [ 'ip_address' ] if "backups" in droplet . features : droplet . backups = True else : droplet . backups = False if "ipv6" in droplet . features : droplet . ipv6 = True else : droplet . ipv6 = False if "private_networking" in droplet . features : droplet . private_networking = True else : droplet . private_networking = False droplets . append ( droplet ) return droplets
6330	def tf ( self , term ) : if ' ' in term : raise ValueError ( 'tf can only calculate the term frequency of individual words' ) tcount = self . get_count ( term ) if tcount == 0 : return 0.0 return 1 + log10 ( tcount )
38	def dict_gather ( comm , d , op = 'mean' , assert_all_have_data = True ) : if comm is None : return d alldicts = comm . allgather ( d ) size = comm . size k2li = defaultdict ( list ) for d in alldicts : for ( k , v ) in d . items ( ) : k2li [ k ] . append ( v ) result = { } for ( k , li ) in k2li . items ( ) : if assert_all_have_data : assert len ( li ) == size , "only %i out of %i MPI workers have sent '%s'" % ( len ( li ) , size , k ) if op == 'mean' : result [ k ] = np . mean ( li , axis = 0 ) elif op == 'sum' : result [ k ] = np . sum ( li , axis = 0 ) else : assert 0 , op return result
5595	def to_dict ( self ) : return dict ( grid = self . grid . to_dict ( ) , metatiling = self . metatiling , tile_size = self . tile_size , pixelbuffer = self . pixelbuffer )
515	def _avgConnectedSpanForColumn1D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 1 ) connected = self . _connectedSynapses [ columnIndex ] . nonzero ( ) [ 0 ] if connected . size == 0 : return 0 else : return max ( connected ) - min ( connected ) + 1
6988	def serial_varfeatures ( lclist , outdir , maxobjects = None , timecols = None , magcols = None , errcols = None , mindet = 1000 , lcformat = 'hat-sql' , lcformatdir = None ) : if maxobjects : lclist = lclist [ : maxobjects ] tasks = [ ( x , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) for x in lclist ] for task in tqdm ( tasks ) : result = _varfeatures_worker ( task ) return result
7144	def transfer ( self , address , amount , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . _backend . transfer ( [ ( address , amount ) ] , priority , payment_id , unlock_time , account = self . index , relay = relay )
2318	def predict ( self , data , alpha = 0.01 , max_iter = 2000 , * * kwargs ) : edge_model = GraphLasso ( alpha = alpha , max_iter = max_iter ) edge_model . fit ( data . values ) return nx . relabel_nodes ( nx . DiGraph ( edge_model . get_precision ( ) ) , { idx : i for idx , i in enumerate ( data . columns ) } )
11998	def _encode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : return data + self . _hmac_generate ( data , algorithm , key ) elif algorithm [ 'type' ] == 'aes' : return self . _aes_encrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . dumps ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . _zlib_compress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
3141	def get ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'emails' , email_id ) )
13621	def one ( func , n = 0 ) : def _one ( result ) : if _isSequenceTypeNotText ( result ) and len ( result ) > n : return func ( result [ n ] ) return None return maybe ( _one )
3309	def _run_wsgiref ( app , config , mode ) : # http://www.python.org/doc/2.5.2/lib/module-wsgiref.html from wsgiref . simple_server import make_server , software_version version = "WsgiDAV/{} {}" . format ( __version__ , software_version ) _logger . info ( "Running {}..." . format ( version ) ) _logger . warning ( "WARNING: This single threaded server (wsgiref) is not meant for production." ) httpd = make_server ( config [ "host" ] , config [ "port" ] , app ) try : httpd . serve_forever ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
8953	def load ( ) : cfg = Bunch ( DEFAULTS ) # TODO: override with contents of [rituals] section in setup.cfg cfg . project_root = get_project_root ( ) if not cfg . project_root : raise RuntimeError ( "No tasks module is imported, cannot determine project root" ) cfg . rootjoin = lambda * names : os . path . join ( cfg . project_root , * names ) cfg . srcjoin = lambda * names : cfg . rootjoin ( cfg . srcdir , * names ) cfg . testjoin = lambda * names : cfg . rootjoin ( cfg . testdir , * names ) cfg . cwd = os . getcwd ( ) os . chdir ( cfg . project_root ) # this assumes an importable setup.py # TODO: maybe call "python setup.py egg_info" for metadata if cfg . project_root not in sys . path : sys . path . append ( cfg . project_root ) try : from setup import project # pylint: disable=no-name-in-module except ImportError : from setup import setup_args as project # pylint: disable=no-name-in-module cfg . project = Bunch ( project ) return cfg
12308	def get_files_to_commit ( autooptions ) : workingdir = autooptions [ 'working-directory' ] includes = autooptions [ 'track' ] [ 'includes' ] excludes = autooptions [ 'track' ] [ 'excludes' ] # transform glob patterns to regular expressions # print("Includes ", includes) includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) excludes = r'|' . join ( [ fnmatch . translate ( x ) for x in excludes ] ) or r'$.' matched_files = [ ] for root , dirs , files in os . walk ( workingdir ) : # print("Looking at ", files) # exclude dirs # dirs[:] = [os.path.join(root, d) for d in dirs] dirs [ : ] = [ d for d in dirs if not re . match ( excludes , d ) ] # exclude/include files files = [ f for f in files if not re . match ( excludes , f ) ] #print("Files after excludes", files) #print(includes) files = [ f for f in files if re . match ( includes , f ) ] #print("Files after includes", files) files = [ os . path . join ( root , f ) for f in files ] matched_files . extend ( files ) return matched_files
9657	def get_direct_ancestors ( G , list_of_nodes ) : parents = [ ] for item in list_of_nodes : anc = G . predecessors ( item ) for one in anc : parents . append ( one ) return parents
9701	def send ( self , msg ) : # Create a sliplib Driver slipDriver = sliplib . Driver ( ) # Package data in slip format slipData = slipDriver . send ( msg ) # Send data over serial port res = self . _serialPort . write ( slipData ) # Return number of bytes transmitted over serial port return res
9218	def _smixins ( self , name ) : return ( self . _mixins [ name ] if name in self . _mixins else False )
11145	def get_repository_state ( self , relaPath = None ) : state = [ ] def _walk_dir ( relaPath , dirList ) : dirDict = { 'type' : 'dir' , 'exists' : os . path . isdir ( os . path . join ( self . __path , relaPath ) ) , 'pyrepdirinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) , } state . append ( { relaPath : dirDict } ) # loop files and dirobjects for fname in sorted ( [ f for f in dirList if isinstance ( f , basestring ) ] ) : relaFilePath = os . path . join ( relaPath , fname ) realFilePath = os . path . join ( self . __path , relaFilePath ) #if os.path.isdir(realFilePath) and df.startswith('.') and df.endswith(self.__objectDir[3:]): # fileDict = {'type':'objectdir', # 'exists':True, # 'pyrepfileinfo':os.path.isfile(os.path.join(self.__path,relaPath,self.__fileInfo%fname)), # } #else: # fileDict = {'type':'file', # 'exists':os.path.isfile(realFilePath), # 'pyrepfileinfo':os.path.isfile(os.path.join(self.__path,relaPath,self.__fileInfo%fname)), # } fileDict = { 'type' : 'file' , 'exists' : os . path . isfile ( realFilePath ) , 'pyrepfileinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __fileInfo % fname ) ) , } state . append ( { relaFilePath : fileDict } ) # loop directories #for ddict in sorted([d for d in dirList if isinstance(d, dict) and len(d)], key=lambda k: list(k)[0]): for ddict in sorted ( [ d for d in dirList if isinstance ( d , dict ) ] , key = lambda k : list ( k ) [ 0 ] ) : dirname = list ( ddict ) [ 0 ] _walk_dir ( relaPath = os . path . join ( relaPath , dirname ) , dirList = ddict [ dirname ] ) # call recursive _walk_dir if relaPath is None : _walk_dir ( relaPath = '' , dirList = self . __repo [ 'walk_repo' ] ) else : assert isinstance ( relaPath , basestring ) , "relaPath must be None or a str" relaPath = self . to_repo_relative_path ( path = relaPath , split = False ) spath = relaPath . split ( os . sep ) dirList = self . __repo [ 'walk_repo' ] while len ( spath ) : dirname = spath . pop ( 0 ) dList = [ d for d in dirList if isinstance ( d , dict ) ] if not len ( dList ) : dirList = None break cDict = [ d for d in dList if dirname in d ] if not len ( cDict ) : dirList = None break dirList = cDict [ 0 ] [ dirname ] if dirList is not None : _walk_dir ( relaPath = relaPath , dirList = dirList ) # return state list return state
9991	def _get_dynamic_base ( self , bases_ ) : bases = tuple ( base . bases [ 0 ] if base . is_dynamic ( ) else base for base in bases_ ) if len ( bases ) == 1 : return bases [ 0 ] elif len ( bases ) > 1 : return self . model . get_dynamic_base ( bases ) else : RuntimeError ( "must not happen" )
683	def getZeroedOutEncoding ( self , n ) : assert all ( field . numRecords > n for field in self . fields ) encoding = np . concatenate ( [ field . encoder . encode ( SENTINEL_VALUE_FOR_MISSING_DATA ) if field . isPredictedField else field . encodings [ n ] for field in self . fields ] ) return encoding
1382	def register_watch ( self , callback ) : RETRY_COUNT = 5 # Retry in case UID is previously # generated, just in case... for _ in range ( RETRY_COUNT ) : # Generate a random UUID. uid = uuid . uuid4 ( ) if uid not in self . watches : Log . info ( "Registering a watch with uid: " + str ( uid ) ) try : callback ( self ) except Exception as e : Log . error ( "Caught exception while triggering callback: " + str ( e ) ) Log . debug ( traceback . format_exc ( ) ) return None self . watches [ uid ] = callback return uid return None
5940	def _build_arg_list ( self , * * kwargs ) : arglist = [ ] for flag , value in kwargs . items ( ) : # XXX: check flag against allowed values flag = str ( flag ) if flag . startswith ( '_' ) : flag = flag [ 1 : ] # python-illegal keywords are '_'-quoted if not flag . startswith ( '-' ) : flag = '-' + flag # now flag is guaranteed to start with '-' if value is True : arglist . append ( flag ) # simple command line flag elif value is False : if flag . startswith ( '-no' ) : # negate a negated flag ('noX=False' --> X=True --> -X ... but who uses that?) arglist . append ( '-' + flag [ 3 : ] ) else : arglist . append ( '-no' + flag [ 1 : ] ) # gromacs switches booleans by prefixing 'no' elif value is None : pass # ignore flag = None else : try : arglist . extend ( [ flag ] + value ) # option with value list except TypeError : arglist . extend ( [ flag , value ] ) # option with single value return list ( map ( str , arglist ) )
6827	def add_remote ( self , path , name , remote_url , use_sudo = False , user = None , fetch = True ) : if path is None : raise ValueError ( "Path to the working copy is needed to add a remote" ) if fetch : cmd = 'git remote add -f %s %s' % ( name , remote_url ) else : cmd = 'git remote add %s %s' % ( name , remote_url ) with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
4178	def window_lanczos ( N ) : if N == 1 : return ones ( 1 ) n = linspace ( - N / 2. , N / 2. , N ) win = sinc ( 2 * n / ( N - 1. ) ) return win
9621	def gamepad ( self ) : state = _xinput_state ( ) _xinput . XInputGetState ( self . ControllerID - 1 , pointer ( state ) ) self . dwPacketNumber = state . dwPacketNumber return state . XINPUT_GAMEPAD
8277	def fseq ( self , client , message ) : client . last_frame = client . current_frame client . current_frame = message [ 3 ]
12869	async def manage ( self ) : cm = _ContextManager ( self . database ) if isinstance ( self . database . obj , AIODatabase ) : cm . connection = await self . database . async_connect ( ) else : cm . connection = self . database . connect ( ) return cm
11995	def get_algorithms ( self ) : return { 'signature' : self . signature_algorithms , 'encryption' : self . encryption_algorithms , 'serialization' : self . serialization_algorithms , 'compression' : self . compression_algorithms , }
7581	def parse ( self , psearch , dsearch ) : stable = "" with open ( self . repfile ) as orep : dat = orep . readlines ( ) for line in dat : ## stat lines if "Estimated Ln Prob of Data" in line : self . est_lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of ln likelihood" in line : self . mean_lnlik = float ( line . split ( ) [ - 1 ] ) if "Variance of ln likelihood" in line : self . var_lnlik = float ( line . split ( ) [ - 1 ] ) if "Mean value of alpha" in line : self . alpha = float ( line . split ( ) [ - 1 ] ) ## matrix lines nonline = psearch . search ( line ) popline = dsearch . search ( line ) #if ") : " in line: if nonline : ## check if sample is supervised... abc = line . strip ( ) . split ( ) outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " : " , " " . join ( abc [ 4 : ] ) ) self . inds += 1 stable += outstr + "\n" elif popline : ## check if sample is supervised... abc = line . strip ( ) . split ( ) prop = [ "0.000" ] * self . kpop pidx = int ( abc [ 3 ] ) - 1 prop [ pidx ] = "1.000" outstr = "{}{}{}" . format ( " " . join ( [ abc [ 0 ] , abc [ 0 ] , abc [ 2 ] , abc [ 0 ] . split ( '.' ) [ 0 ] ] ) , " : " , " " . join ( prop ) ) self . inds += 1 stable += outstr + "\n" stable += "\n" return stable
9876	def _ordinal_metric ( _v1 , _v2 , i1 , i2 , n_v ) : if i1 > i2 : i1 , i2 = i2 , i1 return ( np . sum ( n_v [ i1 : ( i2 + 1 ) ] ) - ( n_v [ i1 ] + n_v [ i2 ] ) / 2 ) ** 2
13819	def _ConvertValueMessage ( value , message ) : if isinstance ( value , dict ) : _ConvertStructMessage ( value , message . struct_value ) elif isinstance ( value , list ) : _ConvertListValueMessage ( value , message . list_value ) elif value is None : message . null_value = 0 elif isinstance ( value , bool ) : message . bool_value = value elif isinstance ( value , six . string_types ) : message . string_value = value elif isinstance ( value , _INT_OR_FLOAT ) : message . number_value = value else : raise ParseError ( 'Unexpected type for Value message.' )
3765	def Joule_Thomson ( T , V , Cp , dV_dT = None , beta = None ) : if dV_dT : return ( T * dV_dT - V ) / Cp elif beta : return V / Cp * ( beta * T - 1. ) else : raise Exception ( 'Either dV_dT or beta is needed' )
9793	def _matches_patterns ( path , patterns ) : for glob in patterns : try : if PurePath ( path ) . match ( glob ) : return True except TypeError : pass return False
5162	def __intermediate_interface ( self , interface , uci_name ) : interface . update ( { '.type' : 'interface' , '.name' : uci_name , 'ifname' : interface . pop ( 'name' ) } ) if 'network' in interface : del interface [ 'network' ] if 'mac' in interface : # mac address of wireless interface must # be set in /etc/config/wireless, therfore # we can skip this in /etc/config/network if interface . get ( 'type' ) != 'wireless' : interface [ 'macaddr' ] = interface [ 'mac' ] del interface [ 'mac' ] if 'autostart' in interface : interface [ 'auto' ] = interface [ 'autostart' ] del interface [ 'autostart' ] if 'disabled' in interface : interface [ 'enabled' ] = not interface [ 'disabled' ] del interface [ 'disabled' ] if 'wireless' in interface : del interface [ 'wireless' ] if 'addresses' in interface : del interface [ 'addresses' ] return interface
4937	def transform_description ( self , content_metadata_item ) : full_description = content_metadata_item . get ( 'full_description' ) or '' if 0 < len ( full_description ) <= self . LONG_STRING_LIMIT : # pylint: disable=len-as-condition return full_description return content_metadata_item . get ( 'short_description' ) or content_metadata_item . get ( 'title' ) or ''
6483	def _process_pagination_values ( request ) : size = 20 page = 0 from_ = 0 if "page_size" in request . POST : size = int ( request . POST [ "page_size" ] ) max_page_size = getattr ( settings , "SEARCH_MAX_PAGE_SIZE" , 100 ) # The parens below are superfluous, but make it much clearer to the reader what is going on if not ( 0 < size <= max_page_size ) : # pylint: disable=superfluous-parens raise ValueError ( _ ( 'Invalid page size of {page_size}' ) . format ( page_size = size ) ) if "page_index" in request . POST : page = int ( request . POST [ "page_index" ] ) from_ = page * size return size , from_ , page
7917	def are_domains_equal ( domain1 , domain2 ) : domain1 = domain1 . encode ( "idna" ) domain2 = domain2 . encode ( "idna" ) return domain1 . lower ( ) == domain2 . lower ( )
4037	def error_handler ( req ) : error_codes = { 400 : ze . UnsupportedParams , 401 : ze . UserNotAuthorised , 403 : ze . UserNotAuthorised , 404 : ze . ResourceNotFound , 409 : ze . Conflict , 412 : ze . PreConditionFailed , 413 : ze . RequestEntityTooLarge , 428 : ze . PreConditionRequired , 429 : ze . TooManyRequests , } def err_msg ( req ) : """ Return a nicely-formatted error message """ return "\nCode: %s\nURL: %s\nMethod: %s\nResponse: %s" % ( req . status_code , # error.msg, req . url , req . request . method , req . text , ) if error_codes . get ( req . status_code ) : # check to see whether its 429 if req . status_code == 429 : # call our back-off function delay = backoff . delay if delay > 32 : # we've waited a total of 62 seconds (2 + 4 โฆ + 32), so give up backoff . reset ( ) raise ze . TooManyRetries ( "Continuing to receive HTTP 429 \ responses after 62 seconds. You are being rate-limited, try again later" ) time . sleep ( delay ) sess = requests . Session ( ) new_req = sess . send ( req . request ) try : new_req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( new_req ) else : raise error_codes . get ( req . status_code ) ( err_msg ( req ) ) else : raise ze . HTTPError ( err_msg ( req ) )
284	def plot_drawdown_periods ( returns , top = 10 , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) df_drawdowns = timeseries . gen_drawdown_table ( returns , top = top ) df_cum_rets . plot ( ax = ax , * * kwargs ) lim = ax . get_ylim ( ) colors = sns . cubehelix_palette ( len ( df_drawdowns ) ) [ : : - 1 ] for i , ( peak , recovery ) in df_drawdowns [ [ 'Peak date' , 'Recovery date' ] ] . iterrows ( ) : if pd . isnull ( recovery ) : recovery = returns . index [ - 1 ] ax . fill_between ( ( peak , recovery ) , lim [ 0 ] , lim [ 1 ] , alpha = .4 , color = colors [ i ] ) ax . set_ylim ( lim ) ax . set_title ( 'Top %i drawdown periods' % top ) ax . set_ylabel ( 'Cumulative returns' ) ax . legend ( [ 'Portfolio' ] , loc = 'upper left' , frameon = True , framealpha = 0.5 ) ax . set_xlabel ( '' ) return ax
2693	def filter_quotes ( text , is_email = True ) : global DEBUG global PAT_FORWARD , PAT_REPLIED , PAT_UNSUBSC if is_email : text = filter ( lambda x : x in string . printable , text ) if DEBUG : print ( "text:" , text ) # strip off quoted text in a forward m = PAT_FORWARD . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] # strip off quoted text in a reply m = PAT_REPLIED . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] # strip off any trailing unsubscription notice m = PAT_UNSUBSC . split ( text , re . M ) if m : text = m [ 0 ] # replace any remaining quoted text with blank lines lines = [ ] for line in text . split ( "\n" ) : if line . startswith ( ">" ) : lines . append ( "" ) else : lines . append ( line ) return list ( split_grafs ( lines ) )
5807	def parse_session_info ( server_handshake_bytes , client_handshake_bytes ) : protocol = None cipher_suite = None compression = False session_id = None session_ticket = None server_session_id = None client_session_id = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : # Ensure we are working with a ServerHello message if message_type != b'\x02' : continue protocol = { b'\x03\x00' : "SSLv3" , b'\x03\x01' : "TLSv1" , b'\x03\x02' : "TLSv1.1" , b'\x03\x03' : "TLSv1.2" , b'\x03\x04' : "TLSv1.3" , } [ message_data [ 0 : 2 ] ] session_id_length = int_from_bytes ( message_data [ 34 : 35 ] ) if session_id_length > 0 : server_session_id = message_data [ 35 : 35 + session_id_length ] cipher_suite_start = 35 + session_id_length cipher_suite_bytes = message_data [ cipher_suite_start : cipher_suite_start + 2 ] cipher_suite = CIPHER_SUITE_MAP [ cipher_suite_bytes ] compression_start = cipher_suite_start + 2 compression = message_data [ compression_start : compression_start + 1 ] != b'\x00' extensions_length_start = compression_start + 1 extensions_data = message_data [ extensions_length_start : ] for extension_type , extension_data in _parse_hello_extensions ( extensions_data ) : if extension_type == 35 : session_ticket = "new" break break for record_type , _ , record_data in parse_tls_records ( client_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : # Ensure we are working with a ClientHello message if message_type != b'\x01' : continue session_id_length = int_from_bytes ( message_data [ 34 : 35 ] ) if session_id_length > 0 : client_session_id = message_data [ 35 : 35 + session_id_length ] cipher_suite_start = 35 + session_id_length cipher_suite_length = int_from_bytes ( message_data [ cipher_suite_start : cipher_suite_start + 2 ] ) compression_start = cipher_suite_start + 2 + cipher_suite_length compression_length = int_from_bytes ( message_data [ compression_start : compression_start + 1 ] ) # On subsequent requests, the session ticket will only be seen # in the ClientHello message if server_session_id is None and session_ticket is None : extensions_length_start = compression_start + 1 + compression_length extensions_data = message_data [ extensions_length_start : ] for extension_type , extension_data in _parse_hello_extensions ( extensions_data ) : if extension_type == 35 : session_ticket = "reused" break break if server_session_id is not None : if client_session_id is None : session_id = "new" else : if client_session_id != server_session_id : session_id = "new" else : session_id = "reused" return { "protocol" : protocol , "cipher_suite" : cipher_suite , "compression" : compression , "session_id" : session_id , "session_ticket" : session_ticket , }
154	def max_item ( self ) : if self . is_empty ( ) : raise ValueError ( "Tree is empty" ) node = self . _root while node . right is not None : node = node . right return node . key , node . value
12599	def get_sheet_list ( xl_path : str ) -> List : wb = read_xl ( xl_path ) if hasattr ( wb , 'sheetnames' ) : return wb . sheetnames else : return wb . sheet_names ( )
12373	def get_first ( ) : client = po . connect ( ) # this depends on the DIGITALOCEAN_API_KEY envvar all_droplets = client . droplets . list ( ) id = all_droplets [ 0 ] [ 'id' ] # I'm cheating because I only have one droplet return client . droplets . get ( id )
6477	def render ( self , stream ) : encoding = self . option . encoding or self . term . encoding or "utf8" if self . option . color : ramp = self . color_ramp ( self . size . y ) [ : : - 1 ] else : ramp = None if self . cycle >= 1 and self . lines : stream . write ( self . term . csi ( 'cuu' , self . lines ) ) zero = int ( self . null / 4 ) # Zero crossing lines = 0 for y in range ( self . screen . size . y ) : if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) for x in range ( self . screen . size . x ) : point = Point ( ( x , y ) ) if point in self . screen : value = self . screen [ point ] if isinstance ( value , int ) : stream . write ( chr ( self . base + value ) . encode ( encoding ) ) else : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( self . term . csi_wrap ( value . encode ( encoding ) , 'bold' ) ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) else : stream . write ( b' ' ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'rmul' ) ) if ramp : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( b'\n' ) lines += 1 stream . flush ( ) self . cycle = self . cycle + 1 self . lines = lines
13277	def update_desc_lcin_path ( desc , pdesc_level ) : parent_breadth = desc [ 'parent_breadth_path' ] [ - 1 ] if ( desc [ 'sib_seq' ] == 0 ) : if ( parent_breadth == 0 ) : pass else : parent_lsib_breadth = parent_breadth - 1 plsib_desc = pdesc_level [ parent_lsib_breadth ] if ( plsib_desc [ 'leaf' ] ) : pass else : lcin_path = copy . deepcopy ( plsib_desc [ 'path' ] ) lcin_path . append ( plsib_desc [ 'sons_count' ] - 1 ) desc [ 'lcin_path' ] = lcin_path else : pass return ( desc )
4716	def tcase_setup ( trun , parent , tcase_fname ) : #pylint: disable=locally-disabled, unused-argument case = copy . deepcopy ( TESTCASE ) case [ "fname" ] = tcase_fname case [ "fpath_orig" ] = os . sep . join ( [ trun [ "conf" ] [ "TESTCASES" ] , case [ "fname" ] ] ) if not os . path . exists ( case [ "fpath_orig" ] ) : cij . err ( 'rnr:tcase_setup: !case["fpath_orig"]: %r' % case [ "fpath_orig" ] ) return None case [ "name" ] = os . path . splitext ( case [ "fname" ] ) [ 0 ] case [ "ident" ] = "/" . join ( [ parent [ "ident" ] , case [ "fname" ] ] ) case [ "res_root" ] = os . sep . join ( [ parent [ "res_root" ] , case [ "fname" ] ] ) case [ "aux_root" ] = os . sep . join ( [ case [ "res_root" ] , "_aux" ] ) case [ "log_fpath" ] = os . sep . join ( [ case [ "res_root" ] , "run.log" ] ) case [ "fpath" ] = os . sep . join ( [ case [ "res_root" ] , case [ "fname" ] ] ) case [ "evars" ] . update ( copy . deepcopy ( parent [ "evars" ] ) ) # Initalize os . makedirs ( case [ "res_root" ] ) # Create DIRS os . makedirs ( case [ "aux_root" ] ) shutil . copyfile ( case [ "fpath_orig" ] , case [ "fpath" ] ) # Copy testcase # Initialize hooks case [ "hooks" ] = hooks_setup ( trun , case , parent . get ( "hooks_pr_tcase" ) ) return case
3305	def _run_gevent ( app , config , mode ) : import gevent import gevent . monkey gevent . monkey . patch_all ( ) from gevent . pywsgi import WSGIServer server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , # TODO: SSL support "keyfile" : None , "certfile" : None , } protocol = "http" # Override or add custom args server_args . update ( config . get ( "server_args" , { } ) ) dav_server = WSGIServer ( server_args [ "bind_addr" ] , app ) _logger . info ( "Running {}" . format ( dav_server ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) try : gevent . spawn ( dav_server . serve_forever ( ) ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
13682	def get_json_tuples ( self , prettyprint = False , translate = True ) : j = self . get_json ( prettyprint , translate ) if len ( j ) > 2 : if prettyprint : j = j [ 1 : - 2 ] + ",\n" else : j = j [ 1 : - 1 ] + "," else : j = "" return j
9068	def _lml_arbitrary_scale ( self ) : s = self . scale D = self . _D n = len ( self . _y ) lml = - self . _df * log2pi - n * log ( s ) lml -= sum ( npsum ( log ( d ) ) for d in D ) d = ( mTQ - yTQ for ( mTQ , yTQ ) in zip ( self . _mTQ , self . _yTQ ) ) lml -= sum ( ( i / j ) @ i for ( i , j ) in zip ( d , D ) ) / s return lml / 2
12881	def next ( self ) : self . index += 1 t = self . peek ( ) if not self . depth : self . _cut ( ) return t
9258	def find_issues_to_add ( all_issues , tag_name ) : filtered = [ ] for issue in all_issues : if issue . get ( "milestone" ) : if issue [ "milestone" ] [ "title" ] == tag_name : iss = copy . deepcopy ( issue ) filtered . append ( iss ) return filtered
4673	def addPrivateKey ( self , wif ) : try : pub = self . publickey_from_wif ( wif ) except Exception : raise InvalidWifError ( "Invalid Key format!" ) if str ( pub ) in self . store : raise KeyAlreadyInStoreException ( "Key already in the store" ) self . store . add ( str ( wif ) , str ( pub ) )
9102	def get_namespace_hash ( self , hash_fn = hashlib . md5 ) -> str : m = hash_fn ( ) if self . has_names : items = self . _get_namespace_name_to_encoding ( desc = 'getting hash' ) . items ( ) else : items = self . _get_namespace_identifier_to_encoding ( desc = 'getting hash' ) . items ( ) for name , encoding in items : m . update ( f'{name}:{encoding}' . encode ( 'utf8' ) ) return m . hexdigest ( )
3950	def _read ( self , mux , gain , data_rate , mode ) : config = ADS1x15_CONFIG_OS_SINGLE # Go out of power-down mode for conversion. # Specify mux value. config |= ( mux & 0x07 ) << ADS1x15_CONFIG_MUX_OFFSET # Validate the passed in gain and then set it in the config. if gain not in ADS1x15_CONFIG_GAIN : raise ValueError ( 'Gain must be one of: 2/3, 1, 2, 4, 8, 16' ) config |= ADS1x15_CONFIG_GAIN [ gain ] # Set the mode (continuous or single shot). config |= mode # Get the default data rate if none is specified (default differs between # ADS1015 and ADS1115). if data_rate is None : data_rate = self . _data_rate_default ( ) # Set the data rate (this is controlled by the subclass as it differs # between ADS1015 and ADS1115). config |= self . _data_rate_config ( data_rate ) config |= ADS1x15_CONFIG_COMP_QUE_DISABLE # Disble comparator mode. # Send the config value to start the ADC conversion. # Explicitly break the 16-bit value down to a big endian pair of bytes. self . _device . writeList ( ADS1x15_POINTER_CONFIG , [ ( config >> 8 ) & 0xFF , config & 0xFF ] ) # Wait for the ADC sample to finish based on the sample rate plus a # small offset to be sure (0.1 millisecond). time . sleep ( 1.0 / data_rate + 0.0001 ) # Retrieve the result. result = self . _device . readList ( ADS1x15_POINTER_CONVERSION , 2 ) return self . _conversion_value ( result [ 1 ] , result [ 0 ] )
12638	def merge_groups ( self , indices ) : try : merged = merge_dict_of_lists ( self . dicom_groups , indices , pop_later = True , copy = True ) self . dicom_groups = merged except IndexError : raise IndexError ( 'Index out of range to merge DICOM groups.' )
1576	def make_shell_endpoint ( topologyInfo , instance_id ) : # Format: container_<id>_<instance_id> pplan = topologyInfo [ "physical_plan" ] stmgrId = pplan [ "instances" ] [ instance_id ] [ "stmgrId" ] host = pplan [ "stmgrs" ] [ stmgrId ] [ "host" ] shell_port = pplan [ "stmgrs" ] [ stmgrId ] [ "shell_port" ] return "http://%s:%d" % ( host , shell_port )
12515	def new_img_like ( ref_niimg , data , affine = None , copy_header = False ) : # Hand-written loading code to avoid too much memory consumption if not ( hasattr ( ref_niimg , 'get_data' ) and hasattr ( ref_niimg , 'get_affine' ) ) : if isinstance ( ref_niimg , _basestring ) : ref_niimg = nib . load ( ref_niimg ) elif operator . isSequenceType ( ref_niimg ) : ref_niimg = nib . load ( ref_niimg [ 0 ] ) else : raise TypeError ( ( 'The reference image should be a niimg, %r ' 'was passed' ) % ref_niimg ) if affine is None : affine = ref_niimg . get_affine ( ) if data . dtype == bool : default_dtype = np . int8 if ( LooseVersion ( nib . __version__ ) >= LooseVersion ( '1.2.0' ) and isinstance ( ref_niimg , nib . freesurfer . mghformat . MGHImage ) ) : default_dtype = np . uint8 data = as_ndarray ( data , dtype = default_dtype ) header = None if copy_header : header = copy . copy ( ref_niimg . get_header ( ) ) header [ 'scl_slope' ] = 0. header [ 'scl_inter' ] = 0. header [ 'glmax' ] = 0. header [ 'cal_max' ] = np . max ( data ) if data . size > 0 else 0. header [ 'cal_max' ] = np . min ( data ) if data . size > 0 else 0. return ref_niimg . __class__ ( data , affine , header = header )
12215	def get_frame_locals ( stepback = 0 ) : with Frame ( stepback = stepback ) as frame : locals_dict = frame . f_locals return locals_dict
23	def pickle_load ( path , compression = False ) : if compression : with zipfile . ZipFile ( path , "r" , compression = zipfile . ZIP_DEFLATED ) as myzip : with myzip . open ( "data" ) as f : return pickle . load ( f ) else : with open ( path , "rb" ) as f : return pickle . load ( f )
566	def createEncoder ( ) : consumption_encoder = ScalarEncoder ( 21 , 0.0 , 100.0 , n = 50 , name = "consumption" , clipInput = True ) time_encoder = DateEncoder ( timeOfDay = ( 21 , 9.5 ) , name = "timestamp_timeOfDay" ) encoder = MultiEncoder ( ) encoder . addEncoder ( "consumption" , consumption_encoder ) encoder . addEncoder ( "timestamp" , time_encoder ) return encoder
11736	def _validate_schema ( obj ) : if obj is not None and not isinstance ( obj , Schema ) : raise IncompatibleSchema ( 'Schema must be of type {0}' . format ( Schema ) ) return obj
7408	def worker ( self ) : ## subsample loci fullseqs = self . sample_loci ( ) ## find all iterations of samples for this quartet liters = itertools . product ( * self . imap . values ( ) ) ## run tree inference for each iteration of sampledict hashval = uuid . uuid4 ( ) . hex weights = [ ] for ridx , lidx in enumerate ( liters ) : ## get subalignment for this iteration and make to nex a , b , c , d = lidx sub = { } for i in lidx : if self . rmap [ i ] == "p1" : sub [ "A" ] = fullseqs [ i ] elif self . rmap [ i ] == "p2" : sub [ "B" ] = fullseqs [ i ] elif self . rmap [ i ] == "p3" : sub [ "C" ] = fullseqs [ i ] else : sub [ "D" ] = fullseqs [ i ] ## write as nexus file nex = [ ] for tax in list ( "ABCD" ) : nex . append ( ">{} {}" . format ( tax , sub [ tax ] ) ) ## check for too much missing or lack of variants nsites , nvar = count_var ( nex ) ## only run test if there's variation present if nvar > self . minsnps : ## format as nexus file nexus = "{} {}\n" . format ( 4 , len ( fullseqs [ a ] ) ) + "\n" . join ( nex ) ## infer ML tree treeorder = self . run_tree_inference ( nexus , "{}.{}" . format ( hashval , ridx ) ) ## add to list weights . append ( treeorder ) ## cleanup - remove all files with the hash val rfiles = glob . glob ( os . path . join ( tempfile . tempdir , "*{}*" . format ( hashval ) ) ) for rfile in rfiles : if os . path . exists ( rfile ) : os . remove ( rfile ) ## return result as weights for the set topologies. trees = [ "ABCD" , "ACBD" , "ADBC" ] wdict = { i : float ( weights . count ( i ) ) / len ( weights ) for i in trees } return wdict
9175	def db_connect ( connection_string = None , * * kwargs ) : if connection_string is None : connection_string = get_current_registry ( ) . settings [ CONNECTION_STRING ] db_conn = psycopg2 . connect ( connection_string , * * kwargs ) try : with db_conn : yield db_conn finally : db_conn . close ( )
11309	def map_to_dictionary ( self , url , obj , * * kwargs ) : maxwidth = kwargs . get ( 'maxwidth' , None ) maxheight = kwargs . get ( 'maxheight' , None ) provider_url , provider_name = self . provider_from_url ( url ) mapping = { 'version' : '1.0' , 'url' : url , 'provider_name' : provider_name , 'provider_url' : provider_url , 'type' : self . resource_type } # a hook self . preprocess ( obj , mapping , * * kwargs ) # resize image if we have a photo, otherwise use the given maximums if self . resource_type == 'photo' and self . get_image ( obj ) : self . resize_photo ( obj , mapping , maxwidth , maxheight ) elif self . resource_type in ( 'video' , 'rich' , 'photo' ) : width , height = size_to_nearest ( maxwidth , maxheight , self . _meta . valid_sizes , self . _meta . force_fit ) mapping . update ( width = width , height = height ) # create a thumbnail if self . get_image ( obj ) : self . thumbnail ( obj , mapping ) # map attributes to the mapping dictionary. if the attribute is # a callable, it must have an argument signature of # (self, obj) for attr in ( 'title' , 'author_name' , 'author_url' , 'html' ) : self . map_attr ( mapping , attr , obj ) # fix any urls if 'url' in mapping : mapping [ 'url' ] = relative_to_full ( mapping [ 'url' ] , url ) if 'thumbnail_url' in mapping : mapping [ 'thumbnail_url' ] = relative_to_full ( mapping [ 'thumbnail_url' ] , url ) if 'html' not in mapping and mapping [ 'type' ] in ( 'video' , 'rich' ) : mapping [ 'html' ] = self . render_html ( obj , context = Context ( mapping ) ) # a hook self . postprocess ( obj , mapping , * * kwargs ) return mapping
12295	def annotate_metadata_dependencies ( repo ) : options = repo . options if 'dependencies' not in options : print ( "No dependencies" ) return [ ] repos = [ ] dependent_repos = options [ 'dependencies' ] for d in dependent_repos : if "/" not in d : print ( "Invalid dependency specification" ) ( username , reponame ) = d . split ( "/" ) try : repos . append ( repo . manager . lookup ( username , reponame ) ) except : print ( "Repository does not exist. Please create one" , d ) package = repo . package package [ 'dependencies' ] = [ ] for r in repos : package [ 'dependencies' ] . append ( { 'username' : r . username , 'reponame' : r . reponame , } )
13718	def _camelcase_to_underscore ( url ) : def upper2underscore ( text ) : for char in text : if char . islower ( ) : yield char else : yield '_' if char . isalpha ( ) : yield char . lower ( ) return '' . join ( upper2underscore ( url ) )
10454	def stateenabled ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXEnabled : return 1 except LdtpServerException : pass return 0
3549	def rssi ( self , timeout_sec = TIMEOUT_SEC ) : # Kick off query to get RSSI, then wait for it to return asyncronously # when the _rssi_changed() function is called. self . _rssi_read . clear ( ) self . _peripheral . readRSSI ( ) if not self . _rssi_read . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for RSSI value!' ) return self . _rssi
6204	def populations_diff_coeff ( particles , populations ) : D_counts = particles . diffusion_coeff_counts if len ( D_counts ) == 1 : pop_sizes = [ pop . stop - pop . start for pop in populations ] assert D_counts [ 0 ] [ 1 ] >= sum ( pop_sizes ) D_counts = [ ( D_counts [ 0 ] [ 0 ] , ps ) for ps in pop_sizes ] D_list = [ ] D_pop_start = 0 # start index of diffusion-based populations for pop , ( D , counts ) in zip ( populations , D_counts ) : D_list . append ( D ) assert pop . start >= D_pop_start assert pop . stop <= D_pop_start + counts D_pop_start += counts return D_list
10532	def create_project ( name , short_name , description ) : try : project = dict ( name = name , short_name = short_name , description = description ) res = _pybossa_req ( 'post' , 'project' , payload = project ) if res . get ( 'id' ) : return Project ( res ) else : return res except : # pragma: no cover raise
2610	def _restore_buffers ( obj , buffers ) : if isinstance ( obj , CannedObject ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : if buf is None : obj . buffers [ i ] = buffers . pop ( 0 )
6463	def usage_function ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available functions:' ) for function in sorted ( FUNCTION ) : doc = FUNCTION [ function ] . __doc__ . strip ( ) . splitlines ( ) [ 0 ] print ( ' %-12s %s' % ( function + ':' , doc ) ) return 0
2197	def log_part ( self ) : self . cap_stdout . seek ( self . _pos ) text = self . cap_stdout . read ( ) self . _pos = self . cap_stdout . tell ( ) self . parts . append ( text ) self . text = text
12823	def fspaths ( draw , allow_pathlike = None ) : has_pathlike = hasattr ( os , 'PathLike' ) if allow_pathlike is None : allow_pathlike = has_pathlike if allow_pathlike and not has_pathlike : raise InvalidArgument ( 'allow_pathlike: os.PathLike not supported, use None instead ' 'to enable it only when available' ) result_type = draw ( sampled_from ( [ bytes , text_type ] ) ) def tp ( s = '' ) : return _str_to_path ( s , result_type ) special_component = sampled_from ( [ tp ( os . curdir ) , tp ( os . pardir ) ] ) normal_component = _filename ( result_type ) path_component = one_of ( normal_component , special_component ) extension = normal_component . map ( lambda f : tp ( os . extsep ) + f ) root = _path_root ( result_type ) def optional ( st ) : return one_of ( st , just ( result_type ( ) ) ) sep = sampled_from ( [ os . sep , os . altsep or os . sep ] ) . map ( tp ) path_part = builds ( lambda s , l : s . join ( l ) , sep , lists ( path_component ) ) main_strategy = builds ( lambda * x : tp ( ) . join ( x ) , optional ( root ) , path_part , optional ( extension ) ) if allow_pathlike and hasattr ( os , 'fspath' ) : pathlike_strategy = main_strategy . map ( lambda p : _PathLike ( p ) ) main_strategy = one_of ( main_strategy , pathlike_strategy ) return draw ( main_strategy )
9774	def resources ( ctx , gpu ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . job . resources ( user , project_name , _job , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
13690	def remove_peer ( self , peer ) : if type ( peer ) == list : for x in peer : check_url ( x ) for i in self . PEERS : if x in i : self . PEERS . remove ( i ) elif type ( peer ) == str : check_url ( peer ) for i in self . PEERS : if peer == i : self . PEERS . remove ( i ) else : raise ValueError ( 'peer paramater did not pass url validation' )
11383	def parser ( self ) : module = self . module subcommands = self . subcommands if subcommands : module_desc = inspect . getdoc ( module ) parser = Parser ( description = module_desc , module = module ) subparsers = parser . add_subparsers ( ) for sc_name , callback in subcommands . items ( ) : sc_name = sc_name . replace ( "_" , "-" ) cb_desc = inspect . getdoc ( callback ) sc_parser = subparsers . add_parser ( sc_name , callback = callback , help = cb_desc ) else : parser = Parser ( callback = self . callbacks [ self . function_name ] , module = module ) return parser
10426	def enrich_internal_unqualified_edges ( graph , subgraph ) : for u , v in itt . combinations ( subgraph , 2 ) : if not graph . has_edge ( u , v ) : continue for k in graph [ u ] [ v ] : if k < 0 : subgraph . add_edge ( u , v , key = k , * * graph [ u ] [ v ] [ k ] )
12648	def filter_objlist ( olist , fieldname , fieldval ) : return [ x for x in olist if getattr ( x , fieldname ) == fieldval ]
7517	def snpcount_numba ( superints , snpsarr ) : ## iterate over all loci for iloc in xrange ( superints . shape [ 0 ] ) : for site in xrange ( superints . shape [ 2 ] ) : ## make new array catg = np . zeros ( 4 , dtype = np . int16 ) ## a list for only catgs ncol = superints [ iloc , : , site ] for idx in range ( ncol . shape [ 0 ] ) : if ncol [ idx ] == 67 : #C catg [ 0 ] += 1 elif ncol [ idx ] == 65 : #A catg [ 1 ] += 1 elif ncol [ idx ] == 84 : #T catg [ 2 ] += 1 elif ncol [ idx ] == 71 : #G catg [ 3 ] += 1 elif ncol [ idx ] == 82 : #R catg [ 1 ] += 1 #A catg [ 3 ] += 1 #G elif ncol [ idx ] == 75 : #K catg [ 2 ] += 1 #T catg [ 3 ] += 1 #G elif ncol [ idx ] == 83 : #S catg [ 0 ] += 1 #C catg [ 3 ] += 1 #G elif ncol [ idx ] == 89 : #Y catg [ 0 ] += 1 #C catg [ 2 ] += 1 #T elif ncol [ idx ] == 87 : #W catg [ 1 ] += 1 #A catg [ 2 ] += 1 #T elif ncol [ idx ] == 77 : #M catg [ 0 ] += 1 #C catg [ 1 ] += 1 #A ## get second most common site catg . sort ( ) ## if invariant e.g., [0, 0, 0, 9], then nothing (" ") if not catg [ 2 ] : pass else : if catg [ 2 ] > 1 : snpsarr [ iloc , site , 1 ] = True else : snpsarr [ iloc , site , 0 ] = True return snpsarr
7403	def above ( self , ref ) : if not self . _valid_ordering_reference ( ref ) : raise ValueError ( "%r can only be moved above instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = ref . order else : o = self . get_ordering_queryset ( ) . filter ( order__lt = ref . order ) . aggregate ( Max ( 'order' ) ) . get ( 'order__max' ) or 0 self . to ( o )
7768	def _stream_authorized ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer presence = self . settings [ u"initial_presence" ] if presence : self . send ( presence )
13871	def CanonicalPath ( path ) : path = os . path . normpath ( path ) path = os . path . abspath ( path ) path = os . path . normcase ( path ) return path
8506	def _default ( self ) : try : # Check if it's iterable iter ( self . default ) except TypeError : return repr ( self . default ) # This is to look for unparsable values, and if we find one, we try to # directly parse the string for v in self . default : if isinstance ( v , Unparseable ) : default = self . _default_value_only ( ) if default : return default # Otherwise just make it a string and go return ', ' . join ( str ( v ) for v in self . default )
954	def getCallerInfo ( depth = 2 ) : f = sys . _getframe ( depth ) method_name = f . f_code . co_name filename = f . f_code . co_filename arg_class = None args = inspect . getargvalues ( f ) if len ( args [ 0 ] ) > 0 : arg_name = args [ 0 ] [ 0 ] # potentially the 'self' arg if its a method arg_class = args [ 3 ] [ arg_name ] . __class__ . __name__ return ( method_name , filename , arg_class )
13237	def next_interval ( self , after = None ) : if after is None : after = timezone . now ( ) after = self . to_timezone ( after ) return next ( self . intervals ( range_start = after ) , None )
11652	def get_version ( self ) : if ( self . name is not None and self . version is not None and self . version . startswith ( ":versiontools:" ) ) : return ( self . __get_live_version ( ) or self . __get_frozen_version ( ) or self . __fail_to_get_any_version ( ) ) else : return self . __base . get_version ( self )
2433	def set_created_date ( self , doc , created ) : if not self . created_date_set : self . created_date_set = True date = utils . datetime_from_iso_format ( created ) if date is not None : doc . creation_info . created = date return True else : raise SPDXValueError ( 'CreationInfo::Date' ) else : raise CardinalityError ( 'CreationInfo::Created' )
4471	def _transform ( self , jam , state ) : if not hasattr ( jam . sandbox , 'muda' ) : raise RuntimeError ( 'No muda state found in jams sandbox.' ) # We'll need a working copy of this object for modification purposes jam_w = copy . deepcopy ( jam ) # Push our reconstructor onto the history stack jam_w . sandbox . muda [ 'history' ] . append ( { 'transformer' : self . __serialize__ , 'state' : state } ) if hasattr ( self , 'audio' ) : self . audio ( jam_w . sandbox . muda , state ) if hasattr ( self , 'metadata' ) : self . metadata ( jam_w . file_metadata , state ) # Walk over the list of deformers for query , function_name in six . iteritems ( self . dispatch ) : function = getattr ( self , function_name ) for matched_annotation in jam_w . search ( namespace = query ) : function ( matched_annotation , state ) return jam_w
7914	def get_int_range_validator ( start , stop ) : def validate_int_range ( value ) : """Integer range validator.""" value = int ( value ) if value >= start and value < stop : return value raise ValueError ( "Not in <{0},{1}) range" . format ( start , stop ) ) return validate_int_range
4089	def with_logger ( cls ) : attr_name = '_logger' cls_name = cls . __qualname__ module = cls . __module__ if module is not None : cls_name = module + '.' + cls_name else : raise AssertionError setattr ( cls , attr_name , logging . getLogger ( cls_name ) ) return cls
2226	def _convert_hexstr_base ( hexstr , base ) : if base is _ALPHABET_16 : # already in hex, no conversion needed return hexstr baselen = len ( base ) x = int ( hexstr , 16 ) # first convert to base 16 if x == 0 : return '0' sign = 1 if x > 0 else - 1 x *= sign digits = [ ] while x : digits . append ( base [ x % baselen ] ) x //= baselen if sign < 0 : digits . append ( '-' ) digits . reverse ( ) newbase_str = '' . join ( digits ) return newbase_str
10702	def get_modes ( _id ) : url = MODES_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
5042	def enroll_users_in_course ( cls , enterprise_customer , course_id , course_mode , emails ) : existing_users , unregistered_emails = cls . get_users_by_email ( emails ) successes = [ ] pending = [ ] failures = [ ] for user in existing_users : succeeded = cls . enroll_user ( enterprise_customer , user , course_mode , course_id ) if succeeded : successes . append ( user ) else : failures . append ( user ) for email in unregistered_emails : pending_user = enterprise_customer . enroll_user_pending_registration ( email , course_mode , course_id ) pending . append ( pending_user ) return successes , pending , failures
7388	def node_radius ( self , node ) : return self . get_idx ( node ) * self . scale + self . internal_radius
1344	def _get_output ( self , a , image ) : sd = np . square ( self . _input_images - image ) mses = np . mean ( sd , axis = tuple ( range ( 1 , sd . ndim ) ) ) index = np . argmin ( mses ) # if we run into numerical problems with this approach, we might # need to add a very tiny threshold here if mses [ index ] > 0 : raise ValueError ( 'No precomputed output image for this image' ) return self . _output_images [ index ]
7548	def cluster_info ( ipyclient , spacer = "" ) : ## get engine data, skips busy engines. hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : hosts . append ( engine . apply ( _socket . gethostname ) ) ## report it hosts = [ i . get ( ) for i in hosts ] result = [ ] for hostname in set ( hosts ) : result . append ( "{}host compute node: [{} cores] on {}" . format ( spacer , hosts . count ( hostname ) , hostname ) ) print "\n" . join ( result )
13426	def update_message ( self , message ) : url = "/2/messages/%s" % message . message_id data = self . _put_resource ( url , message . json_data ( ) ) return self . message_from_json ( data )
6366	def population ( self ) : return self . _tp + self . _tn + self . _fp + self . _fn
5908	def make_ndx_captured ( * * kwargs ) : kwargs [ 'stdout' ] = False # required for proper output as described in doc user_input = kwargs . pop ( 'input' , [ ] ) user_input = [ cmd for cmd in user_input if cmd != 'q' ] # filter any quit kwargs [ 'input' ] = user_input + [ '' , 'q' ] # necessary commands return gromacs . make_ndx ( * * kwargs )
1098	def get_close_matches ( word , possibilities , n = 3 , cutoff = 0.6 ) : if not n > 0 : raise ValueError ( "n must be > 0: %r" % ( n , ) ) if not 0.0 <= cutoff <= 1.0 : raise ValueError ( "cutoff must be in [0.0, 1.0]: %r" % ( cutoff , ) ) result = [ ] s = SequenceMatcher ( ) s . set_seq2 ( word ) for x in possibilities : s . set_seq1 ( x ) if s . real_quick_ratio ( ) >= cutoff and s . quick_ratio ( ) >= cutoff and s . ratio ( ) >= cutoff : result . append ( ( s . ratio ( ) , x ) ) # Move the best scorers to head of list result = heapq . nlargest ( n , result ) # Strip scores for the best n matches return [ x for score , x in result ]
12496	def warn_if_not_float ( X , estimator = 'This algorithm' ) : if not isinstance ( estimator , str ) : estimator = estimator . __class__ . __name__ if X . dtype . kind != 'f' : warnings . warn ( "%s assumes floating point values as input, " "got %s" % ( estimator , X . dtype ) ) return True return False
13762	def _check_next ( self ) : if self . is_initial : return True if self . before : if self . before_cursor : return True else : return False else : if self . after_cursor : return True else : return False
354	def load_and_assign_npz ( sess = None , name = None , network = None ) : if network is None : raise ValueError ( "network is None." ) if sess is None : raise ValueError ( "session is None." ) if not os . path . exists ( name ) : logging . error ( "file {} doesn't exist." . format ( name ) ) return False else : params = load_npz ( name = name ) assign_params ( sess , params , network ) logging . info ( "[*] Load {} SUCCESS!" . format ( name ) ) return network
3098	def validate_token ( key , token , user_id , action_id = "" , current_time = None ) : if not token : return False try : decoded = base64 . urlsafe_b64decode ( token ) token_time = int ( decoded . split ( DELIMITER ) [ - 1 ] ) except ( TypeError , ValueError , binascii . Error ) : return False if current_time is None : current_time = time . time ( ) # If the token is too old it's not valid. if current_time - token_time > DEFAULT_TIMEOUT_SECS : return False # The given token should match the generated one with the same time. expected_token = generate_token ( key , user_id , action_id = action_id , when = token_time ) if len ( token ) != len ( expected_token ) : return False # Perform constant time comparison to avoid timing attacks different = 0 for x , y in zip ( bytearray ( token ) , bytearray ( expected_token ) ) : different |= x ^ y return not different
4511	def crop ( image , top_offset = 0 , left_offset = 0 , bottom_offset = 0 , right_offset = 0 ) : if bottom_offset or top_offset or left_offset or right_offset : width , height = image . size box = ( left_offset , top_offset , width - right_offset , height - bottom_offset ) image = image . crop ( box = box ) return image
4694	def regex_find ( pattern , content ) : find = re . findall ( pattern , content ) if not find : cij . err ( "pattern <%r> is invalid, no matches!" % pattern ) cij . err ( "content: %r" % content ) return '' if len ( find ) >= 2 : cij . err ( "pattern <%r> is too simple, matched more than 2!" % pattern ) cij . err ( "content: %r" % content ) return '' return find [ 0 ]
13128	def tree2commands ( self , adapter , session , lastcmds , xsync ) : # do some preliminary sanity checks... # todo: do i really want to be using assert statements?... assert xsync . tag == constants . NODE_SYNCML assert len ( xsync ) == 2 assert xsync [ 0 ] . tag == constants . CMD_SYNCHDR assert xsync [ 1 ] . tag == constants . NODE_SYNCBODY version = xsync [ 0 ] . findtext ( 'VerProto' ) if version != constants . SYNCML_VERSION_1_2 : raise common . FeatureNotSupported ( 'unsupported SyncML version "%s" (expected "%s")' % ( version , constants . SYNCML_VERSION_1_2 ) ) verdtd = xsync [ 0 ] . findtext ( 'VerDTD' ) if verdtd != constants . SYNCML_DTD_VERSION_1_2 : raise common . FeatureNotSupported ( 'unsupported SyncML DTD version "%s" (expected "%s")' % ( verdtd , constants . SYNCML_DTD_VERSION_1_2 ) ) ret = self . initialize ( adapter , session , xsync ) hdrcmd = ret [ 0 ] if session . isServer : log . debug ( 'received request SyncML message from "%s" (s%s.m%s)' , hdrcmd . target , hdrcmd . sessionID , hdrcmd . msgID ) else : log . debug ( 'received response SyncML message from "%s" (s%s.m%s)' , lastcmds [ 0 ] . target , lastcmds [ 0 ] . sessionID , lastcmds [ 0 ] . msgID ) try : return self . _tree2commands ( adapter , session , lastcmds , xsync , ret ) except Exception , e : if not session . isServer : raise # TODO: make this configurable as to whether or not any error # is sent back to the peer as a SyncML "standardized" error # status... code = '%s.%s' % ( e . __class__ . __module__ , e . __class__ . __name__ ) msg = '' . join ( traceback . format_exception_only ( type ( e ) , e ) ) . strip ( ) log . exception ( 'failed while interpreting command tree: %s' , msg ) # TODO: for some reason, the active exception is not being logged... return [ hdrcmd , state . Command ( name = constants . CMD_STATUS , cmdID = '1' , msgRef = session . pendingMsgID , cmdRef = 0 , sourceRef = xsync [ 0 ] . findtext ( 'Source/LocURI' ) , targetRef = xsync [ 0 ] . findtext ( 'Target/LocURI' ) , statusOf = constants . CMD_SYNCHDR , statusCode = constants . STATUS_COMMAND_FAILED , errorCode = code , errorMsg = msg , errorTrace = '' . join ( traceback . format_exception ( type ( e ) , e , sys . exc_info ( ) [ 2 ] ) ) , ) , state . Command ( name = constants . CMD_FINAL ) ]
7836	def register_disco_cache_fetchers ( cache_suite , stream ) : tmp = stream class DiscoInfoCacheFetcher ( DiscoCacheFetcherBase ) : """Cache fetcher for DiscoInfo.""" stream = tmp disco_class = DiscoInfo class DiscoItemsCacheFetcher ( DiscoCacheFetcherBase ) : """Cache fetcher for DiscoItems.""" stream = tmp disco_class = DiscoItems cache_suite . register_fetcher ( DiscoInfo , DiscoInfoCacheFetcher ) cache_suite . register_fetcher ( DiscoItems , DiscoItemsCacheFetcher )
7094	def init_options ( self ) : self . options = GoogleMapOptions ( ) d = self . declaration self . set_map_type ( d . map_type ) if d . ambient_mode : self . set_ambient_mode ( d . ambient_mode ) if ( d . camera_position or d . camera_zoom or d . camera_tilt or d . camera_bearing ) : self . update_camera ( ) if d . map_bounds : self . set_map_bounds ( d . map_bounds ) if not d . show_compass : self . set_show_compass ( d . show_compass ) if not d . show_zoom_controls : self . set_show_zoom_controls ( d . show_zoom_controls ) if not d . show_toolbar : self . set_show_toolbar ( d . show_toolbar ) if d . lite_mode : self . set_lite_mode ( d . lite_mode ) if not d . rotate_gestures : self . set_rotate_gestures ( d . rotate_gestures ) if not d . scroll_gestures : self . set_scroll_gestures ( d . scroll_gestures ) if not d . tilt_gestures : self . set_tilt_gestures ( d . tilt_gestures ) if not d . zoom_gestures : self . set_zoom_gestures ( d . zoom_gestures ) if d . min_zoom : self . set_min_zoom ( d . min_zoom ) if d . max_zoom : self . set_max_zoom ( d . max_zoom )
10290	def enrich_variants ( graph : BELGraph , func : Union [ None , str , Iterable [ str ] ] = None ) : if func is None : func = { PROTEIN , RNA , MIRNA , GENE } nodes = list ( get_nodes_by_function ( graph , func ) ) for u in nodes : parent = u . get_parent ( ) if parent is None : continue if parent not in graph : graph . add_has_variant ( parent , u )
9358	def paragraph ( separator = '\n\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 ) : return paragraphs ( quantity = 1 , separator = separator , wrap_start = wrap_start , wrap_end = wrap_end , html = html , sentences_quantity = sentences_quantity )
7842	def set_category ( self , category ) : if not category : raise ValueError ( "Category is required in DiscoIdentity" ) category = unicode ( category ) self . xmlnode . setProp ( "category" , category . encode ( "utf-8" ) )
6150	def fir_remez_lpf ( f_pass , f_stop , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = lowpass_order ( f_pass , f_stop , d_pass , d_stop , fsamp = fs ) # Bump up the order by N_bump to bring down the final d_pass & d_stop N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) print ( 'Remez filter taps = %d.' % N_taps ) return b
2796	def load ( self , use_slug = False ) : identifier = None if use_slug or not self . id : identifier = self . slug else : identifier = self . id if not identifier : raise NotFoundError ( "One of self.id or self.slug must be set." ) data = self . get_data ( "images/%s" % identifier ) image_dict = data [ 'image' ] # Setting the attribute values for attr in image_dict . keys ( ) : setattr ( self , attr , image_dict [ attr ] ) return self
5209	def info_qry ( tickers , flds ) -> str : full_list = '\n' . join ( [ f'tickers: {tickers[:8]}' ] + [ f' {tickers[n:(n + 8)]}' for n in range ( 8 , len ( tickers ) , 8 ) ] ) return f'{full_list}\nfields: {flds}'
4043	def _build_query ( self , query_string , no_params = False ) : try : query = quote ( query_string . format ( u = self . library_id , t = self . library_type ) ) except KeyError as err : raise ze . ParamNotPassed ( "There's a request parameter missing: %s" % err ) # Add the URL parameters and the user key, if necessary if no_params is False : if not self . url_params : self . add_parameters ( ) query = "%s?%s" % ( query , self . url_params ) return query
9830	def write ( self , filename ) : # comments (VMD chokes on lines of len > 80, so truncate) maxcol = 80 with open ( filename , 'w' ) as outfile : for line in self . comments : comment = '# ' + str ( line ) outfile . write ( comment [ : maxcol ] + '\n' ) # each individual object for component , object in self . sorted_components ( ) : object . write ( outfile ) # the field object itself DXclass . write ( self , outfile , quote = True ) for component , object in self . sorted_components ( ) : outfile . write ( 'component "%s" value %s\n' % ( component , str ( object . id ) ) )
11809	def index_collection ( self , filenames ) : for filename in filenames : self . index_document ( open ( filename ) . read ( ) , filename )
3357	def _extend_nocheck ( self , iterable ) : current_length = len ( self ) list . extend ( self , iterable ) _dict = self . _dict if current_length is 0 : self . _generate_index ( ) return for i , obj in enumerate ( islice ( self , current_length , None ) , current_length ) : _dict [ obj . id ] = i
8414	def round_any ( x , accuracy , f = np . round ) : if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) return f ( x / accuracy ) * accuracy
7576	def _call_structure ( mname , ename , sname , name , workdir , seed , ntaxa , nsites , kpop , rep ) : ## create call string outname = os . path . join ( workdir , "{}-K-{}-rep-{}" . format ( name , kpop , rep ) ) cmd = [ "structure" , "-m" , mname , "-e" , ename , "-K" , str ( kpop ) , "-D" , str ( seed ) , "-N" , str ( ntaxa ) , "-L" , str ( nsites ) , "-i" , sname , "-o" , outname ] ## call the shell function proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) comm = proc . communicate ( ) ## cleanup oldfiles = [ mname , ename , sname ] for oldfile in oldfiles : if os . path . exists ( oldfile ) : os . remove ( oldfile ) return comm
9129	def store_populate_failed ( cls , resource : str , session : Optional [ Session ] = None ) -> 'Action' : action = cls . make_populate_failed ( resource ) _store_helper ( action , session = session ) return action
739	def cPrint ( self , level , message , * args , * * kw ) : if level > self . consolePrinterVerbosity : return if len ( kw ) > 1 : raise KeyError ( "Invalid keywords for cPrint: %s" % str ( kw . keys ( ) ) ) newline = kw . get ( "newline" , True ) if len ( kw ) == 1 and 'newline' not in kw : raise KeyError ( "Invalid keyword for cPrint: %s" % kw . keys ( ) [ 0 ] ) if len ( args ) == 0 : if newline : print message else : print message , else : if newline : print message % args else : print message % args ,
2377	def list_rules ( self ) : for rule in sorted ( self . all_rules , key = lambda rule : rule . name ) : print ( rule ) if self . args . verbose : for line in rule . doc . split ( "\n" ) : print ( " " , line )
10466	def getFrontmostApp ( cls ) : # Refresh the runningApplications list apps = cls . _getRunningApps ( ) for app in apps : pid = app . processIdentifier ( ) ref = cls . getAppRefByPid ( pid ) try : if ref . AXFrontmost : return ref except ( _a11y . ErrorUnsupported , _a11y . ErrorCannotComplete , _a11y . ErrorAPIDisabled , _a11y . ErrorNotImplemented ) : # Some applications do not have an explicit GUI # and so will not have an AXFrontmost attribute # Trying to read attributes from Google Chrome Helper returns # ErrorAPIDisabled for some reason - opened radar bug 12837995 pass raise ValueError ( 'No GUI application found.' )
4566	def euclidean ( c1 , c2 ) : diffs = ( ( i - j ) for i , j in zip ( c1 , c2 ) ) return sum ( x * x for x in diffs )
10236	def get_graphs_by_ids ( self , network_ids : Iterable [ int ] ) -> List [ BELGraph ] : return [ self . networks [ network_id ] for network_id in network_ids ]
12123	def validate_activatable_models ( ) : for model in get_activatable_models ( ) : # Verify the activatable model has an activatable boolean field activatable_field = next ( ( f for f in model . _meta . fields if f . __class__ == models . BooleanField and f . name == model . ACTIVATABLE_FIELD_NAME ) , None ) if activatable_field is None : raise ValidationError ( ( 'Model {0} is an activatable model. It must define an activatable BooleanField that ' 'has a field name of model.ACTIVATABLE_FIELD_NAME (which defaults to is_active)' . format ( model ) ) ) # Ensure all foreign keys and onetoone fields will not result in cascade deletions if not cascade deletable if not model . ALLOW_CASCADE_DELETE : for field in model . _meta . fields : if field . __class__ in ( models . ForeignKey , models . OneToOneField ) : if field . remote_field . on_delete == models . CASCADE : raise ValidationError ( ( 'Model {0} is an activatable model. All ForeignKey and OneToOneFields ' 'must set on_delete methods to something other than CASCADE (the default). ' 'If you want to explicitely allow cascade deletes, then you must set the ' 'ALLOW_CASCADE_DELETE=True class variable on your model.' ) . format ( model ) )
13000	def calculate_diagram_ranges ( data ) : data = round_arr_teff_luminosity ( data ) temps = data [ 'temp' ] x_range = [ 1.05 * np . amax ( temps ) , .95 * np . amin ( temps ) ] lums = data [ 'lum' ] y_range = [ .50 * np . amin ( lums ) , 2 * np . amax ( lums ) ] return ( x_range , y_range )
3040	def from_json ( cls , json_data ) : data = json . loads ( _helpers . _from_bytes ( json_data ) ) if ( data . get ( 'token_expiry' ) and not isinstance ( data [ 'token_expiry' ] , datetime . datetime ) ) : try : data [ 'token_expiry' ] = datetime . datetime . strptime ( data [ 'token_expiry' ] , EXPIRY_FORMAT ) except ValueError : data [ 'token_expiry' ] = None retval = cls ( data [ 'access_token' ] , data [ 'client_id' ] , data [ 'client_secret' ] , data [ 'refresh_token' ] , data [ 'token_expiry' ] , data [ 'token_uri' ] , data [ 'user_agent' ] , revoke_uri = data . get ( 'revoke_uri' , None ) , id_token = data . get ( 'id_token' , None ) , id_token_jwt = data . get ( 'id_token_jwt' , None ) , token_response = data . get ( 'token_response' , None ) , scopes = data . get ( 'scopes' , None ) , token_info_uri = data . get ( 'token_info_uri' , None ) ) retval . invalid = data [ 'invalid' ] return retval
12054	def inspectABF ( abf = exampleABF , saveToo = False , justPlot = False ) : pylab . close ( 'all' ) print ( " ~~ inspectABF()" ) if type ( abf ) is str : abf = swhlab . ABF ( abf ) swhlab . plot . new ( abf , forceNewFigure = True ) if abf . sweepInterval * abf . sweeps < 60 * 5 : #shorter than 5 minutes pylab . subplot ( 211 ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . sweep ( abf , 'all' ) pylab . subplot ( 212 ) swhlab . plot . sweep ( abf , 'all' , continuous = True ) swhlab . plot . comments ( abf ) else : print ( " -- plotting as long recording" ) swhlab . plot . sweep ( abf , 'all' , continuous = True , minutes = True ) swhlab . plot . comments ( abf , minutes = True ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . annotate ( abf ) if justPlot : return if saveToo : path = os . path . split ( abf . fname ) [ 0 ] basename = os . path . basename ( abf . fname ) pylab . savefig ( os . path . join ( path , "_" + basename . replace ( ".abf" , ".png" ) ) ) pylab . show ( ) return
12642	def get_config_bool ( name ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) return cli_config . getboolean ( 'servicefabric' , name , False )
5974	def isMine ( self , scriptname ) : suffix = os . path . splitext ( scriptname ) [ 1 ] . lower ( ) if suffix . startswith ( '.' ) : suffix = suffix [ 1 : ] return self . suffix == suffix
2335	def clr ( M , * * kwargs ) : R = np . zeros ( M . shape ) Id = [ [ 0 , 0 ] for i in range ( M . shape [ 0 ] ) ] for i in range ( M . shape [ 0 ] ) : mu_i = np . mean ( M [ i , : ] ) sigma_i = np . std ( M [ i , : ] ) Id [ i ] = [ mu_i , sigma_i ] for i in range ( M . shape [ 0 ] ) : for j in range ( i + 1 , M . shape [ 0 ] ) : z_i = np . max ( [ 0 , ( M [ i , j ] - Id [ i ] [ 0 ] ) / Id [ i ] [ 0 ] ] ) z_j = np . max ( [ 0 , ( M [ i , j ] - Id [ j ] [ 0 ] ) / Id [ j ] [ 0 ] ] ) R [ i , j ] = np . sqrt ( z_i ** 2 + z_j ** 2 ) R [ j , i ] = R [ i , j ] # Symmetric return R
6559	def stitch ( csp , min_classical_gap = 2.0 , max_graph_size = 8 ) : # ensure we have penaltymodel factory available try : dwavebinarycsp . assert_penaltymodel_factory_available ( ) except AssertionError as e : raise RuntimeError ( e ) def aux_factory ( ) : for i in count ( ) : yield 'aux{}' . format ( i ) aux = aux_factory ( ) bqm = dimod . BinaryQuadraticModel . empty ( csp . vartype ) # developer note: we could cache them and relabel, for now though let's do the simple thing # penalty_models = {} for const in csp . constraints : configurations = const . configurations if len ( const . variables ) > max_graph_size : msg = ( "The given csp contains a constraint {const} with {num_var} variables. " "This cannot be mapped to a graph with {max_graph_size} nodes. " "Consider checking whether your constraint is irreducible." "" ) . format ( const = const , num_var = len ( const . variables ) , max_graph_size = max_graph_size ) raise ImpossibleBQM ( msg ) pmodel = None if len ( const ) == 0 : # empty constraint continue if min_classical_gap <= 2.0 : if len ( const ) == 1 and max_graph_size >= 1 : bqm . update ( _bqm_from_1sat ( const ) ) continue elif len ( const ) == 2 and max_graph_size >= 2 : bqm . update ( _bqm_from_2sat ( const ) ) continue # developer note: we could cache them and relabel, for now though let's do the simple thing # if configurations in penalty_models: # raise NotImplementedError for G in iter_complete_graphs ( const . variables , max_graph_size + 1 , aux ) : # construct a specification spec = pm . Specification ( graph = G , decision_variables = const . variables , feasible_configurations = configurations , min_classical_gap = min_classical_gap , vartype = csp . vartype ) # try to use the penaltymodel ecosystem try : pmodel = pm . get_penalty_model ( spec ) except pm . ImpossiblePenaltyModel : # hopefully adding more variables will make it possible continue if pmodel . classical_gap >= min_classical_gap : break # developer note: we could cache them and relabel, for now though let's do the simple thing # penalty_models[configurations] = pmodel else : msg = ( "No penalty model can be build for constraint {}" . format ( const ) ) raise ImpossibleBQM ( msg ) bqm . update ( pmodel . model ) return bqm
4065	def fields_types ( self , tname , qstring , itemtype ) : # check for a valid cached version template_name = tname + itemtype query_string = qstring . format ( i = itemtype ) if self . templates . get ( template_name ) and not self . _updated ( query_string , self . templates [ template_name ] , template_name ) : return self . templates [ template_name ] [ "tmplt" ] # otherwise perform a normal request and cache the response retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , template_name )
13347	def cmd ( ) : if platform == 'win' : return [ 'cmd.exe' , '/K' ] elif platform == 'linux' : ppid = os . getppid ( ) ppid_cmdline_file = '/proc/{0}/cmdline' . format ( ppid ) try : with open ( ppid_cmdline_file ) as f : cmd = f . read ( ) if cmd . endswith ( '\x00' ) : cmd = cmd [ : - 1 ] cmd = cmd . split ( '\x00' ) return cmd + [ binpath ( 'subshell.sh' ) ] except : cmd = 'bash' else : cmd = 'bash' return [ cmd , binpath ( 'subshell.sh' ) ]
6615	def receive_all ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . receive ( )
5432	def build_logging_param ( logging_uri , util_class = OutputFileParamUtil ) : if not logging_uri : return job_model . LoggingParam ( None , None ) recursive = not logging_uri . endswith ( '.log' ) oututil = util_class ( '' ) _ , uri , provider = oututil . parse_uri ( logging_uri , recursive ) if '*' in uri . basename : raise ValueError ( 'Wildcards not allowed in logging URI: %s' % uri ) return job_model . LoggingParam ( uri , provider )
7867	def expire ( self ) : with self . _lock : logger . debug ( "expdict.expire. timeouts: {0!r}" . format ( self . _timeouts ) ) next_timeout = None for k in self . _timeouts . keys ( ) : ret = self . _expire_item ( k ) if ret is not None : if next_timeout is None : next_timeout = ret else : next_timeout = min ( next_timeout , ret ) return next_timeout
3086	def _is_ndb ( self ) : # issubclass will fail if one of the arguments is not a class, only # need worry about new-style classes since ndb and db models are # new-style if isinstance ( self . _model , type ) : if _NDB_MODEL is not None and issubclass ( self . _model , _NDB_MODEL ) : return True elif issubclass ( self . _model , db . Model ) : return False raise TypeError ( 'Model class not an NDB or DB model: {0}.' . format ( self . _model ) )
7838	def set_node ( self , node ) : if node is None : if self . xmlnode . hasProp ( "node" ) : self . xmlnode . unsetProp ( "node" ) return node = unicode ( node ) self . xmlnode . setProp ( "node" , node . encode ( "utf-8" ) )
13216	def restore ( self , name , filename ) : if not self . exists ( name ) : self . create ( name ) else : log . warn ( 'overwriting contents of database %s' % name ) log . info ( 'restoring %s from %s' % ( name , filename ) ) self . _run_cmd ( 'pg_restore' , '--verbose' , '--dbname=%s' % name , filename )
5501	def remove_tweets ( self , url ) : try : del self . cache [ url ] self . mark_updated ( ) return True except KeyError : return False
6819	def sync_media ( self , sync_set = None , clean = 0 , iter_local_paths = 0 ) : # Ensure a site is selected. self . genv . SITE = self . genv . SITE or self . genv . default_site r = self . local_renderer clean = int ( clean ) self . vprint ( 'Getting site data for %s...' % self . genv . SITE ) self . set_site_specifics ( self . genv . SITE ) sync_sets = r . env . sync_sets if sync_set : sync_sets = [ sync_set ] ret_paths = [ ] for _sync_set in sync_sets : for paths in r . env . sync_sets [ _sync_set ] : r . env . sync_local_path = os . path . abspath ( paths [ 'local_path' ] % self . genv ) if paths [ 'local_path' ] . endswith ( '/' ) and not r . env . sync_local_path . endswith ( '/' ) : r . env . sync_local_path += '/' if iter_local_paths : ret_paths . append ( r . env . sync_local_path ) continue r . env . sync_remote_path = paths [ 'remote_path' ] % self . genv if clean : r . sudo ( 'rm -Rf {apache_sync_remote_path}' ) print ( 'Syncing %s to %s...' % ( r . env . sync_local_path , r . env . sync_remote_path ) ) r . env . tmp_chmod = paths . get ( 'chmod' , r . env . chmod ) r . sudo ( 'mkdir -p {apache_sync_remote_path}' ) r . sudo ( 'chmod -R {apache_tmp_chmod} {apache_sync_remote_path}' ) r . local ( 'rsync -rvz --progress --recursive --no-p --no-g ' '--rsh "ssh -o StrictHostKeyChecking=no -i {key_filename}" {apache_sync_local_path} {user}@{host_string}:{apache_sync_remote_path}' ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {apache_sync_remote_path}' ) if iter_local_paths : return ret_paths
1850	def LOOPNZ ( cpu , target ) : counter_name = { 16 : 'CX' , 32 : 'ECX' , 64 : 'RCX' } [ cpu . address_bit_size ] counter = cpu . write_register ( counter_name , cpu . read_register ( counter_name ) - 1 ) cpu . PC = Operators . ITEBV ( cpu . address_bit_size , counter != 0 , ( cpu . PC + target . read ( ) ) & ( ( 1 << target . size ) - 1 ) , cpu . PC + cpu . instruction . size )
244	def days_to_liquidate_positions ( positions , market_data , max_bar_consumption = 0.2 , capital_base = 1e6 , mean_volume_window = 5 ) : DV = market_data [ 'volume' ] * market_data [ 'price' ] roll_mean_dv = DV . rolling ( window = mean_volume_window , center = False ) . mean ( ) . shift ( ) roll_mean_dv = roll_mean_dv . replace ( 0 , np . nan ) positions_alloc = pos . get_percent_alloc ( positions ) positions_alloc = positions_alloc . drop ( 'cash' , axis = 1 ) days_to_liquidate = ( positions_alloc * capital_base ) / ( max_bar_consumption * roll_mean_dv ) return days_to_liquidate . iloc [ mean_volume_window : ]
6834	def vagrant ( self , name = '' ) : r = self . local_renderer config = self . ssh_config ( name ) extra_args = self . _settings_dict ( config ) r . genv . update ( extra_args )
4649	def json ( self ) : if not self . _is_constructed ( ) or self . _is_require_reconstruction ( ) : self . constructTx ( ) return dict ( self )
1426	def getInstanceJstack ( self , topology_info , instance_id ) : pid_response = yield getInstancePid ( topology_info , instance_id ) try : http_client = tornado . httpclient . AsyncHTTPClient ( ) pid_json = json . loads ( pid_response ) pid = pid_json [ 'stdout' ] . strip ( ) if pid == '' : raise Exception ( 'Failed to get pid' ) endpoint = utils . make_shell_endpoint ( topology_info , instance_id ) url = "%s/jstack/%s" % ( endpoint , pid ) response = yield http_client . fetch ( url ) Log . debug ( "HTTP call for url: %s" , url ) raise tornado . gen . Return ( response . body ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) )
4451	def search ( self , query ) : args , query = self . _mk_query_args ( query ) st = time . time ( ) res = self . redis . execute_command ( self . SEARCH_CMD , * args ) return Result ( res , not query . _no_content , duration = ( time . time ( ) - st ) * 1000.0 , has_payload = query . _with_payloads )
4836	def get_paginated_catalogs ( self , querystring = None ) : return self . _load_data ( self . CATALOGS_ENDPOINT , default = [ ] , querystring = querystring , traverse_pagination = False , many = False )
7743	def _prepare_io_handler_cb ( self , handler ) : self . _anything_done = True logger . debug ( "_prepar_io_handler_cb called for {0!r}" . format ( handler ) ) self . _configure_io_handler ( handler ) self . _prepare_sources . pop ( handler , None ) return False
897	def generateFromNumbers ( self , numbers ) : sequence = [ ] for number in numbers : if number == None : sequence . append ( number ) else : pattern = self . patternMachine . get ( number ) sequence . append ( pattern ) return sequence
8286	def _get_elements ( self ) : for index , el in enumerate ( self . _elements ) : if isinstance ( el , tuple ) : el = PathElement ( * el ) self . _elements [ index ] = el yield el
11372	def get_temporary_file ( prefix = "tmp_" , suffix = "" , directory = None ) : try : file_fd , filepath = mkstemp ( prefix = prefix , suffix = suffix , dir = directory ) os . close ( file_fd ) except IOError , e : try : os . remove ( filepath ) except Exception : pass raise e return filepath
4165	def _get_link ( self , cobj ) : fname_idx = None full_name = cobj [ 'module_short' ] + '.' + cobj [ 'name' ] if full_name in self . _searchindex [ 'objects' ] : value = self . _searchindex [ 'objects' ] [ full_name ] if isinstance ( value , dict ) : value = value [ next ( iter ( value . keys ( ) ) ) ] fname_idx = value [ 0 ] elif cobj [ 'module_short' ] in self . _searchindex [ 'objects' ] : value = self . _searchindex [ 'objects' ] [ cobj [ 'module_short' ] ] if cobj [ 'name' ] in value . keys ( ) : fname_idx = value [ cobj [ 'name' ] ] [ 0 ] if fname_idx is not None : fname = self . _searchindex [ 'filenames' ] [ fname_idx ] + '.html' if self . _is_windows : fname = fname . replace ( '/' , '\\' ) link = os . path . join ( self . doc_url , fname ) else : link = posixpath . join ( self . doc_url , fname ) if hasattr ( link , 'decode' ) : link = link . decode ( 'utf-8' , 'replace' ) if link in self . _page_cache : html = self . _page_cache [ link ] else : html = get_data ( link , self . gallery_dir ) self . _page_cache [ link ] = html # test if cobj appears in page comb_names = [ cobj [ 'module_short' ] + '.' + cobj [ 'name' ] ] if self . extra_modules_test is not None : for mod in self . extra_modules_test : comb_names . append ( mod + '.' + cobj [ 'name' ] ) url = False if hasattr ( html , 'decode' ) : # Decode bytes under Python 3 html = html . decode ( 'utf-8' , 'replace' ) for comb_name in comb_names : if hasattr ( comb_name , 'decode' ) : # Decode bytes under Python 3 comb_name = comb_name . decode ( 'utf-8' , 'replace' ) if comb_name in html : url = link + u'#' + comb_name link = url else : link = False return link
13663	def get_item ( filename , uuid ) : with open ( os . fsencode ( str ( filename ) ) , "r" ) as f : data = json . load ( f ) results = [ i for i in data if i [ "uuid" ] == str ( uuid ) ] if results : return results return None
2257	def allsame ( iterable , eq = operator . eq ) : iter_ = iter ( iterable ) try : first = next ( iter_ ) except StopIteration : return True return all ( eq ( first , item ) for item in iter_ )
12825	def handle_extends ( self , text ) : match = self . re_extends . match ( text ) if match : extra_text = self . re_extends . sub ( '' , text , count = 1 ) blocks = self . get_blocks ( extra_text ) path = os . path . join ( self . base_dir , match . group ( 'path' ) ) with open ( path , encoding = 'utf-8' ) as fp : return self . replace_blocks_in_extends ( fp . read ( ) , blocks ) else : return None
10356	def random_by_nodes ( graph : BELGraph , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.9 assert 0 < percentage <= 1 nodes = graph . nodes ( ) n = int ( len ( nodes ) * percentage ) subnodes = random . sample ( nodes , n ) result = graph . subgraph ( subnodes ) update_node_helper ( graph , result ) return result
3703	def Rackett ( T , Tc , Pc , Zc ) : return R * Tc / Pc * Zc ** ( 1 + ( 1 - T / Tc ) ** ( 2 / 7. ) )
7664	def to_samples ( self , times , confidence = False ) : times = np . asarray ( times ) if times . ndim != 1 or np . any ( times < 0 ) : raise ParameterError ( 'times must be 1-dimensional and non-negative' ) idx = np . argsort ( times ) samples = times [ idx ] values = [ list ( ) for _ in samples ] confidences = [ list ( ) for _ in samples ] for obs in self . data : start = np . searchsorted ( samples , obs . time ) end = np . searchsorted ( samples , obs . time + obs . duration , side = 'right' ) for i in range ( start , end ) : values [ idx [ i ] ] . append ( obs . value ) confidences [ idx [ i ] ] . append ( obs . confidence ) if confidence : return values , confidences else : return values
11366	def _do_unzip ( zipped_file , output_directory ) : z = zipfile . ZipFile ( zipped_file ) for path in z . namelist ( ) : relative_path = os . path . join ( output_directory , path ) dirname , dummy = os . path . split ( relative_path ) try : if relative_path . endswith ( os . sep ) and not os . path . exists ( dirname ) : os . makedirs ( relative_path ) elif not os . path . exists ( relative_path ) : dirname = os . path . join ( output_directory , os . path . dirname ( path ) ) if os . path . dirname ( path ) and not os . path . exists ( dirname ) : os . makedirs ( dirname ) fd = open ( relative_path , "w" ) fd . write ( z . read ( path ) ) fd . close ( ) except IOError , e : raise e return output_directory
2199	def platform_config_dir ( ) : if LINUX : # nocover dpath_ = os . environ . get ( 'XDG_CONFIG_HOME' , '~/.config' ) elif DARWIN : # nocover dpath_ = '~/Library/Application Support' elif WIN32 : # nocover dpath_ = os . environ . get ( 'APPDATA' , '~/AppData/Roaming' ) else : # nocover raise NotImplementedError ( 'Unknown Platform %r' % ( sys . platform , ) ) dpath = normpath ( expanduser ( dpath_ ) ) return dpath
1939	def get_func_return_types ( self , hsh : bytes ) -> str : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) abi = self . get_abi ( hsh ) outputs = abi . get ( 'outputs' ) return '()' if outputs is None else SolidityMetadata . tuple_signature_for_components ( outputs )
7043	def lightcurve_moments ( ftimes , fmags , ferrs ) : ndet = len ( fmags ) if ndet > 9 : # now calculate the various things we need series_median = npmedian ( fmags ) series_wmean = ( npsum ( fmags * ( 1.0 / ( ferrs * ferrs ) ) ) / npsum ( 1.0 / ( ferrs * ferrs ) ) ) series_mad = npmedian ( npabs ( fmags - series_median ) ) series_stdev = 1.483 * series_mad series_skew = spskew ( fmags ) series_kurtosis = spkurtosis ( fmags ) # get the beyond1std fraction series_above1std = len ( fmags [ fmags > ( series_median + series_stdev ) ] ) series_below1std = len ( fmags [ fmags < ( series_median - series_stdev ) ] ) # this is the fraction beyond 1 stdev series_beyond1std = ( series_above1std + series_below1std ) / float ( ndet ) # get the magnitude percentiles series_mag_percentiles = nppercentile ( fmags , [ 5.0 , 10 , 17.5 , 25 , 32.5 , 40 , 60 , 67.5 , 75 , 82.5 , 90 , 95 ] ) return { 'median' : series_median , 'wmean' : series_wmean , 'mad' : series_mad , 'stdev' : series_stdev , 'skew' : series_skew , 'kurtosis' : series_kurtosis , 'beyond1std' : series_beyond1std , 'mag_percentiles' : series_mag_percentiles , 'mag_iqr' : series_mag_percentiles [ 8 ] - series_mag_percentiles [ 3 ] , } else : LOGERROR ( 'not enough detections in this magseries ' 'to calculate light curve moments' ) return None
436	def draw_weights ( W = None , second = 10 , saveable = True , shape = None , name = 'mnist' , fig_idx = 2396512 ) : if shape is None : shape = [ 28 , 28 ] import matplotlib . pyplot as plt if saveable is False : plt . ion ( ) fig = plt . figure ( fig_idx ) # show all feature images n_units = W . shape [ 1 ] num_r = int ( np . sqrt ( n_units ) ) # ๆฏ่กๆพ็คบ็ไธชๆฐ ่ฅ25ไธชhidden unit -> ๆฏ่กๆพ็คบ5ไธช num_c = int ( np . ceil ( n_units / num_r ) ) count = int ( 1 ) for _row in range ( 1 , num_r + 1 ) : for _col in range ( 1 , num_c + 1 ) : if count > n_units : break fig . add_subplot ( num_r , num_c , count ) # ------------------------------------------------------------ # plt.imshow(np.reshape(W[:,count-1],(28,28)), cmap='gray') # ------------------------------------------------------------ feature = W [ : , count - 1 ] / np . sqrt ( ( W [ : , count - 1 ] ** 2 ) . sum ( ) ) # feature[feature<0.0001] = 0 # value threshold # if count == 1 or count == 2: # print(np.mean(feature)) # if np.std(feature) < 0.03: # condition threshold # feature = np.zeros_like(feature) # if np.mean(feature) < -0.015: # condition threshold # feature = np.zeros_like(feature) plt . imshow ( np . reshape ( feature , ( shape [ 0 ] , shape [ 1 ] ) ) , cmap = 'gray' , interpolation = "nearest" ) # , vmin=np.min(feature), vmax=np.max(feature)) # plt.title(name) # ------------------------------------------------------------ # plt.imshow(np.reshape(W[:,count-1] ,(np.sqrt(size),np.sqrt(size))), cmap='gray', interpolation="nearest") plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) # distable tick plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )
8481	def env ( key , default ) : value = os . environ . get ( key , None ) if value is not None : log . info ( ' %s = %r' , key . lower ( ) . replace ( '_' , '.' ) , value ) return value key = key . lower ( ) . replace ( '_' , '.' ) value = get ( key ) if value is not None : return value return default
11173	def strsettings ( self , indent = 0 , maxindent = 25 , width = 0 ) : out = [ ] makelabel = lambda name : ' ' * indent + name + ': ' settingsindent = _autoindent ( [ makelabel ( s ) for s in self . options ] , indent , maxindent ) for name in self . option_order : option = self . options [ name ] label = makelabel ( name ) settingshelp = "%s(%s): %s" % ( option . formatname , option . strvalue , option . location ) wrapped = self . _wrap_labelled ( label , settingshelp , settingsindent , width ) out . extend ( wrapped ) return '\n' . join ( out )
7613	def get_arena_image ( self , obj : BaseAttrDict ) : badge_id = obj . arena . id for i in self . constants . arenas : if i . id == badge_id : return 'https://royaleapi.github.io/cr-api-assets/arenas/arena{}.png' . format ( i . arena_id )
12930	def get_pos ( vcf_line ) : if not vcf_line : return None vcf_data = vcf_line . strip ( ) . split ( '\t' ) return_data = dict ( ) return_data [ 'chrom' ] = CHROM_INDEX [ vcf_data [ 0 ] ] return_data [ 'pos' ] = int ( vcf_data [ 1 ] ) return return_data
12155	def list_move_to_front ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . insert ( 0 , value ) return l
13071	def r_first_passage ( self , objectId ) : collection , reffs = self . get_reffs ( objectId = objectId , export_collection = True ) first , _ = reffs [ 0 ] return redirect ( url_for ( ".r_passage_semantic" , objectId = objectId , subreference = first , semantic = self . semantic ( collection ) ) )
6643	def getExtraIncludes ( self ) : if 'extraIncludes' in self . description : return [ os . path . normpath ( x ) for x in self . description [ 'extraIncludes' ] ] else : return [ ]
9944	def link_file ( self , path , prefixed_path , source_storage ) : # Skip this file if it was already copied earlier if prefixed_path in self . symlinked_files : return self . log ( "Skipping '%s' (already linked earlier)" % path ) # Delete the target file if needed or break if not self . delete_file ( path , prefixed_path , source_storage ) : return # The full path of the source file source_path = source_storage . path ( path ) # Finally link the file if self . dry_run : self . log ( "Pretending to link '%s'" % source_path , level = 1 ) else : self . log ( "Linking '%s'" % source_path , level = 1 ) full_path = self . storage . path ( prefixed_path ) try : os . makedirs ( os . path . dirname ( full_path ) ) except OSError : pass try : if os . path . lexists ( full_path ) : os . unlink ( full_path ) os . symlink ( source_path , full_path ) except AttributeError : import platform raise CommandError ( "Symlinking is not supported by Python %s." % platform . python_version ( ) ) except NotImplementedError : import platform raise CommandError ( "Symlinking is not supported in this " "platform (%s)." % platform . platform ( ) ) except OSError as e : raise CommandError ( e ) if prefixed_path not in self . symlinked_files : self . symlinked_files . append ( prefixed_path )
9057	def economic_qs_zeros ( n ) : Q0 = empty ( ( n , 0 ) ) Q1 = eye ( n ) S0 = empty ( 0 ) return ( ( Q0 , Q1 ) , S0 )
5989	def grid_stack_from_deflection_stack ( grid_stack , deflection_stack ) : if deflection_stack is not None : def minus ( grid , deflections ) : return grid - deflections return grid_stack . map_function ( minus , deflection_stack )
12478	def get_sys_path ( rcpath , app_name , section_name = None ) : # first check if it is an existing path if op . exists ( rcpath ) : return op . realpath ( op . expanduser ( rcpath ) ) # look for the rcfile try : settings = rcfile ( app_name , section_name ) except : raise # look for the variable within the rcfile configutarions try : sys_path = op . expanduser ( settings [ rcpath ] ) except KeyError : raise IOError ( 'Could not find an existing variable with name {0} in' ' section {1} of {2}rc config setup. Maybe it is a ' ' folder that could not be found.' . format ( rcpath , section_name , app_name ) ) # found the variable, now check if it is an existing path else : if not op . exists ( sys_path ) : raise IOError ( 'Could not find the path {3} indicated by the ' 'variable {0} in section {1} of {2}rc config ' 'setup.' . format ( rcpath , section_name , app_name , sys_path ) ) # expand the path and return return op . realpath ( op . expanduser ( sys_path ) )
2765	def get_droplet_snapshots ( self ) : data = self . get_data ( "snapshots?resource_type=droplet" ) return [ Snapshot ( token = self . token , * * snapshot ) for snapshot in data [ 'snapshots' ] ]
8442	def _code_search ( query , github_user = None ) : github_client = temple . utils . GithubClient ( ) headers = { 'Accept' : 'application/vnd.github.v3.text-match+json' } resp = github_client . get ( '/search/code' , params = { 'q' : query , 'per_page' : 100 } , headers = headers ) if resp . status_code == requests . codes . unprocessable_entity and github_user : raise temple . exceptions . InvalidGithubUserError ( 'Invalid Github user or org - "{}"' . format ( github_user ) ) resp . raise_for_status ( ) resp_data = resp . json ( ) repositories = collections . defaultdict ( dict ) while True : repositories . update ( { 'git@github.com:{}.git' . format ( repo [ 'repository' ] [ 'full_name' ] ) : repo [ 'repository' ] for repo in resp_data [ 'items' ] } ) next_url = _parse_link_header ( resp . headers ) . get ( 'next' ) if next_url : resp = requests . get ( next_url , headers = headers ) resp . raise_for_status ( ) resp_data = resp . json ( ) else : break return repositories
3879	async def _handle_set_typing_notification ( self , set_typing_notification ) : conv_id = set_typing_notification . conversation_id . id res = parsers . parse_typing_status_message ( set_typing_notification ) await self . on_typing . fire ( res ) try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for typing notification: %s' , conv_id ) else : await conv . on_typing . fire ( res )
13846	def __get_numbered_paths ( filepath ) : format = '%s (%%d)%s' % splitext_files_only ( filepath ) return map ( lambda n : format % n , itertools . count ( 1 ) )
3617	def get_settings ( self ) : try : logger . info ( 'GET SETTINGS ON %s' , self . index_name ) return self . __index . get_settings ( ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( 'ERROR DURING GET_SETTINGS ON %s: %s' , self . model , e )
8487	def init ( self , hosts = None , cacert = None , client_cert = None , client_key = None ) : # Try to get the etcd module try : import etcd self . module = etcd except ImportError : pass if not self . module : return self . _parse_jetconfig ( ) # Check env for overriding configuration or pyconfig setting hosts = env ( 'PYCONFIG_ETCD_HOSTS' , hosts ) protocol = env ( 'PYCONFIG_ETCD_PROTOCOL' , None ) cacert = env ( 'PYCONFIG_ETCD_CACERT' , cacert ) client_cert = env ( 'PYCONFIG_ETCD_CERT' , client_cert ) client_key = env ( 'PYCONFIG_ETCD_KEY' , client_key ) # Parse auth string if there is one username = None password = None auth = env ( 'PYCONFIG_ETCD_AUTH' , None ) if auth : auth = auth . split ( ':' ) auth . append ( '' ) username = auth [ 0 ] password = auth [ 1 ] # Create new etcd instance hosts = self . _parse_hosts ( hosts ) if hosts is None : return kw = { } # Need this when passing a list of hosts to python-etcd, which we # always do, even if it's a list of one kw [ 'allow_reconnect' ] = True # Grab optional protocol argument if protocol : kw [ 'protocol' ] = protocol # Add auth to constructor if we got it if username : kw [ 'username' ] = username if password : kw [ 'password' ] = password # Assign the SSL args if we have 'em if cacert : kw [ 'ca_cert' ] = os . path . abspath ( cacert ) if client_cert and client_key : kw [ 'cert' ] = ( ( os . path . abspath ( client_cert ) , os . path . abspath ( client_key ) ) ) elif client_cert : kw [ 'cert' ] = os . path . abspath ( client_cert ) if cacert or client_cert or client_key : kw [ 'protocol' ] = 'https' self . client = self . module . Client ( hosts , * * kw )
3298	def string_to_xml ( text ) : try : return etree . XML ( text ) except Exception : # TODO: # ExpatError: reference to invalid character number: line 1, column 62 # litmus fails, when xml is used instead of lxml # 18. propget............... FAIL (PROPFIND on `/temp/litmus/prop2': # Could not read status line: connection was closed by server) # text = <ns0:high-unicode xmlns:ns0="http://example.com/neon/litmus/">&#55296;&#56320; # </ns0:high-unicode> # t2 = text.encode("utf8") # return etree.XML(t2) _logger . error ( "Error parsing XML string. " "If lxml is not available, and unicode is involved, then " "installing lxml _may_ solve this issue." ) _logger . error ( "XML source: {}" . format ( text ) ) raise
5492	def write_config ( self ) : with open ( self . config_file , "w" ) as config_file : self . cfg . write ( config_file )
12273	def iso_reference_valid_char ( c , raise_error = True ) : if c in ISO_REFERENCE_VALID : return True if raise_error : raise ValueError ( "'%s' is not in '%s'" % ( c , ISO_REFERENCE_VALID ) ) return False
467	def sample ( a = None , temperature = 1.0 ) : if a is None : raise Exception ( "a : list of float" ) b = np . copy ( a ) try : if temperature == 1 : return np . argmax ( np . random . multinomial ( 1 , a , 1 ) ) if temperature is None : return np . argmax ( a ) else : a = np . log ( a ) / temperature a = np . exp ( a ) / np . sum ( np . exp ( a ) ) return np . argmax ( np . random . multinomial ( 1 , a , 1 ) ) except Exception : # np.set_printoptions(threshold=np.nan) # tl.logging.info(a) # tl.logging.info(np.sum(a)) # tl.logging.info(np.max(a)) # tl.logging.info(np.min(a)) # exit() message = "For large vocabulary_size, choice a higher temperature\ to avoid log error. Hint : use ``sample_top``. " warnings . warn ( message , Warning ) # tl.logging.info(a) # tl.logging.info(b) return np . argmax ( np . random . multinomial ( 1 , b , 1 ) )
8324	def isString ( s ) : try : return isinstance ( s , unicode ) or isinstance ( s , basestring ) except NameError : return isinstance ( s , str )
1474	def _get_tmaster_processes ( self ) : retval = { } tmaster_cmd_lst = [ self . tmaster_binary , '--topology_name=%s' % self . topology_name , '--topology_id=%s' % self . topology_id , '--zkhostportlist=%s' % self . state_manager_connection , '--zkroot=%s' % self . state_manager_root , '--myhost=%s' % self . master_host , '--master_port=%s' % str ( self . master_port ) , '--controller_port=%s' % str ( self . tmaster_controller_port ) , '--stats_port=%s' % str ( self . tmaster_stats_port ) , '--config_file=%s' % self . heron_internals_config_file , '--override_config_file=%s' % self . override_config_file , '--metrics_sinks_yaml=%s' % self . metrics_sinks_config_file , '--metricsmgr_port=%s' % str ( self . metrics_manager_port ) , '--ckptmgr_port=%s' % str ( self . checkpoint_manager_port ) ] tmaster_env = self . shell_env . copy ( ) if self . shell_env is not None else { } tmaster_cmd = Command ( tmaster_cmd_lst , tmaster_env ) if os . environ . get ( 'ENABLE_HEAPCHECK' ) is not None : tmaster_cmd . env . update ( { 'LD_PRELOAD' : "/usr/lib/libtcmalloc.so" , 'HEAPCHECK' : "normal" } ) retval [ "heron-tmaster" ] = tmaster_cmd if self . metricscache_manager_mode . lower ( ) != "disabled" : retval [ "heron-metricscache" ] = self . _get_metrics_cache_cmd ( ) if self . health_manager_mode . lower ( ) != "disabled" : retval [ "heron-healthmgr" ] = self . _get_healthmgr_cmd ( ) retval [ self . metricsmgr_ids [ 0 ] ] = self . _get_metricsmgr_cmd ( self . metricsmgr_ids [ 0 ] , self . metrics_sinks_config_file , self . metrics_manager_port ) if self . is_stateful_topology : retval . update ( self . _get_ckptmgr_process ( ) ) return retval
7217	def get_definition ( self , task_name ) : r = self . gbdx_connection . get ( self . _base_url + '/' + task_name ) raise_for_status ( r ) return r . json ( )
2128	def set_display_columns ( self , set_true = [ ] , set_false = [ ] ) : for i in range ( len ( self . fields ) ) : if self . fields [ i ] . name in set_true : self . fields [ i ] . display = True elif self . fields [ i ] . name in set_false : self . fields [ i ] . display = False
10858	def get_update_tile ( self , params , values ) : doglobal , particles = self . _update_type ( params ) if doglobal : return self . shape . copy ( ) # 1) store the current parameters of interest values0 = self . get_values ( params ) # 2) calculate the current tileset tiles0 = [ self . _tile ( n ) for n in particles ] # 3) update to newer parameters and calculate tileset self . set_values ( params , values ) tiles1 = [ self . _tile ( n ) for n in particles ] # 4) revert parameters & return union of all tiles self . set_values ( params , values0 ) return Tile . boundingtile ( tiles0 + tiles1 )
11459	def strip_fields ( self ) : for tag in self . record . keys ( ) : if tag in self . fields_list : record_delete_fields ( self . record , tag )
1607	def spec ( cls , name = None , inputs = None , par = 1 , config = None , optional_outputs = None ) : python_class_path = "%s.%s" % ( cls . __module__ , cls . __name__ ) if hasattr ( cls , 'outputs' ) : # avoid modification to cls.outputs _outputs = copy . copy ( cls . outputs ) else : _outputs = [ ] if optional_outputs is not None : assert isinstance ( optional_outputs , ( list , tuple ) ) for out in optional_outputs : assert isinstance ( out , ( str , Stream ) ) _outputs . append ( out ) return HeronComponentSpec ( name , python_class_path , is_spout = False , par = par , inputs = inputs , outputs = _outputs , config = config )
3738	def Tstar ( T , epsilon_k = None , epsilon = None ) : if epsilon_k : _Tstar = T / ( epsilon_k ) elif epsilon : _Tstar = k * T / epsilon else : raise Exception ( 'Either epsilon/k or epsilon must be provided' ) return _Tstar
6016	def signal_to_noise_map ( self ) : signal_to_noise_map = np . divide ( self . image , self . noise_map ) signal_to_noise_map [ signal_to_noise_map < 0 ] = 0 return signal_to_noise_map
1414	def get_pplan ( self , topologyName , callback = None ) : isWatching = False # Temp dict used to return result # if callback is not provided. ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : """ Custom callback to get the topologies right now. """ ret [ "result" ] = data self . _get_pplan_with_watch ( topologyName , callback , isWatching ) # The topologies are now populated with the data. return ret [ "result" ]
6685	def update ( kernel = False ) : manager = MANAGER cmds = { 'yum -y --color=never' : { False : '--exclude=kernel* update' , True : 'update' } } cmd = cmds [ manager ] [ kernel ] run_as_root ( "%(manager)s %(cmd)s" % locals ( ) )
958	def aggregationToMonthsSeconds ( interval ) : seconds = interval . get ( 'microseconds' , 0 ) * 0.000001 seconds += interval . get ( 'milliseconds' , 0 ) * 0.001 seconds += interval . get ( 'seconds' , 0 ) seconds += interval . get ( 'minutes' , 0 ) * 60 seconds += interval . get ( 'hours' , 0 ) * 60 * 60 seconds += interval . get ( 'days' , 0 ) * 24 * 60 * 60 seconds += interval . get ( 'weeks' , 0 ) * 7 * 24 * 60 * 60 months = interval . get ( 'months' , 0 ) months += 12 * interval . get ( 'years' , 0 ) return { 'months' : months , 'seconds' : seconds }
6845	def check_ok ( self ) : import requests if not self . env . check_ok : return # Find current git branch. branch_name = self . _local ( 'git rev-parse --abbrev-ref HEAD' , capture = True ) . strip ( ) check_ok_paths = self . env . check_ok_paths or { } if branch_name in check_ok_paths : check = check_ok_paths [ branch_name ] if 'username' in check : auth = ( check [ 'username' ] , check [ 'password' ] ) else : auth = None ret = requests . get ( check [ 'url' ] , auth = auth ) passed = check [ 'text' ] in ret . content assert passed , 'Check failed: %s' % check [ 'url' ]
3444	def load_json_model ( filename ) : if isinstance ( filename , string_types ) : with open ( filename , "r" ) as file_handle : return model_from_dict ( json . load ( file_handle ) ) else : return model_from_dict ( json . load ( filename ) )
3906	async def _on_connect ( self ) : self . _user_list , self . _conv_list = ( await hangups . build_user_conversation_list ( self . _client ) ) self . _conv_list . on_event . add_observer ( self . _on_event ) # show the conversation menu conv_picker = ConversationPickerWidget ( self . _conv_list , self . on_select_conversation , self . _keys ) self . _tabbed_window = TabbedWindowWidget ( self . _keys ) self . _tabbed_window . set_tab ( conv_picker , switch = True , title = 'Conversations' ) self . _urwid_loop . widget = self . _tabbed_window
6137	def add_model_file ( self , model_fpath , position = 1 , file_id = None ) : if file_id is None : file_id = self . make_unique_id ( 'file_input' ) ret_data = self . file_create ( File . from_file ( model_fpath , position , file_id ) ) return ret_data
580	def getSpec ( cls ) : spec = { "description" : IdentityRegion . __doc__ , "singleNodeOnly" : True , "inputs" : { "in" : { "description" : "The input vector." , "dataType" : "Real32" , "count" : 0 , "required" : True , "regionLevel" : False , "isDefaultInput" : True , "requireSplitterMap" : False } , } , "outputs" : { "out" : { "description" : "A copy of the input vector." , "dataType" : "Real32" , "count" : 0 , "regionLevel" : True , "isDefaultOutput" : True } , } , "parameters" : { "dataWidth" : { "description" : "Size of inputs" , "accessMode" : "Read" , "dataType" : "UInt32" , "count" : 1 , "constraints" : "" } , } , } return spec
550	def __checkCancelation ( self ) : # Update a hadoop job counter at least once every 600 seconds so it doesn't # think our map task is dead print >> sys . stderr , "reporter:counter:HypersearchWorker,numRecords,50" # See if the job got cancelled jobCancel = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'cancel' ] ) [ 0 ] if jobCancel : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isCanceled = True self . _logger . info ( "Model %s canceled because Job %s was stopped." , self . _modelID , self . _jobID ) else : stopReason = self . _jobsDAO . modelsGetFields ( self . _modelID , [ 'engStop' ] ) [ 0 ] if stopReason is None : pass elif stopReason == ClientJobsDAO . STOP_REASON_KILLED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isKilled = True self . _logger . info ( "Model %s canceled because it was killed by hypersearch" , self . _modelID ) elif stopReason == ClientJobsDAO . STOP_REASON_STOPPED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isCanceled = True self . _logger . info ( "Model %s stopped because hypersearch ended" , self . _modelID ) else : raise RuntimeError ( "Unexpected stop reason encountered: %s" % ( stopReason ) )
3594	def search ( self , query ) : if self . authSubToken is None : raise LoginError ( "You need to login before executing any request" ) path = SEARCH_URL + "?c=3&q={}" . format ( requests . utils . quote ( query ) ) # FIXME: not sure if this toc call should be here self . toc ( ) data = self . executeRequestApi2 ( path ) if utils . hasPrefetch ( data ) : response = data . preFetch [ 0 ] . response else : response = data resIterator = response . payload . listResponse . doc return list ( map ( utils . parseProtobufObj , resIterator ) )
13126	def get_domains ( self ) : search = User . search ( ) search . aggs . bucket ( 'domains' , 'terms' , field = 'domain' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) return [ entry . key for entry in response . aggregations . domains . buckets ]
3983	def get_same_container_repos_from_spec ( app_or_library_spec ) : repos = set ( ) app_or_lib_repo = get_repo_of_app_or_library ( app_or_library_spec . name ) if app_or_lib_repo is not None : repos . add ( app_or_lib_repo ) for dependent_name in app_or_library_spec [ 'depends' ] [ 'libs' ] : repos . add ( get_repo_of_app_or_library ( dependent_name ) ) return repos
10086	def clear ( self , * args , * * kwargs ) : super ( Deposit , self ) . clear ( * args , * * kwargs )
1323	def SetActive ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : handle = self . NativeWindowHandle if IsIconic ( handle ) : ret = ShowWindow ( handle , SW . Restore ) elif not IsWindowVisible ( handle ) : ret = ShowWindow ( handle , SW . Show ) ret = SetForegroundWindow ( handle ) # may fail if foreground windows's process is not python time . sleep ( waitTime ) return ret return False
8893	def calculate_uuid ( self ) : # raise an error if no inputs to the UUID calculation were specified if self . uuid_input_fields is None : raise NotImplementedError ( """You must define either a 'uuid_input_fields' attribute (with a tuple of field names) or override the 'calculate_uuid' method, on models that inherit from UUIDModelMixin. If you want a fully random UUID, you can set 'uuid_input_fields' to the string 'RANDOM'.""" ) # if the UUID has been set to be random, return a random UUID if self . uuid_input_fields == "RANDOM" : return uuid . uuid4 ( ) . hex # if we got this far, uuid_input_fields should be a tuple assert isinstance ( self . uuid_input_fields , tuple ) , "'uuid_input_fields' must either be a tuple or the string 'RANDOM'" # calculate the input to the UUID function hashable_input_vals = [ ] for field in self . uuid_input_fields : new_value = getattr ( self , field ) if new_value : hashable_input_vals . append ( str ( new_value ) ) hashable_input = ":" . join ( hashable_input_vals ) # if all the values were falsey, just return a random UUID, to avoid collisions if not hashable_input : return uuid . uuid4 ( ) . hex # compute the UUID as a function of the input values return sha2_uuid ( hashable_input )
516	def _avgConnectedSpanForColumn2D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 2 ) connected = self . _connectedSynapses [ columnIndex ] ( rows , cols ) = connected . reshape ( self . _inputDimensions ) . nonzero ( ) if rows . size == 0 and cols . size == 0 : return 0 rowSpan = rows . max ( ) - rows . min ( ) + 1 colSpan = cols . max ( ) - cols . min ( ) + 1 return numpy . average ( [ rowSpan , colSpan ] )
13256	def as_dict ( self ) : entry_dict = { } entry_dict [ 'UUID' ] = self . uuid entry_dict [ 'Creation Date' ] = self . time entry_dict [ 'Time Zone' ] = self . tz if self . tags : entry_dict [ 'Tags' ] = self . tags entry_dict [ 'Entry Text' ] = self . text entry_dict [ 'Starred' ] = self . starred entry_dict [ 'Location' ] = self . location return entry_dict
3715	def calculate ( self , T , P , zs , ws , method ) : if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeSolids ] return mixing_simple ( zs , Vms ) else : raise Exception ( 'Method not valid' )
5727	def _get_responses_windows ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : try : self . gdb_process . stdout . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stdout . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stdout . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stdout" ) except IOError : pass try : self . gdb_process . stderr . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stderr . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stderr . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stderr" ) except IOError : pass if time . time ( ) > timeout_time_sec : break return responses
11583	def image_urls ( self ) : all_image_urls = self . finder_image_urls [ : ] for image_url in self . extender_image_urls : if image_url not in all_image_urls : all_image_urls . append ( image_url ) return all_image_urls
8912	def includeme ( config ) : settings = config . registry . settings if asbool ( settings . get ( 'twitcher.rpcinterface' , True ) ) : LOGGER . debug ( 'Twitcher XML-RPC Interface enabled.' ) # include twitcher config config . include ( 'twitcher.config' ) # using basic auth config . include ( 'twitcher.basicauth' ) # pyramid xml-rpc # http://docs.pylonsproject.org/projects/pyramid-rpc/en/latest/xmlrpc.html config . include ( 'pyramid_rpc.xmlrpc' ) config . include ( 'twitcher.db' ) config . add_xmlrpc_endpoint ( 'api' , '/RPC2' ) # register xmlrpc methods config . add_xmlrpc_method ( RPCInterface , attr = 'generate_token' , endpoint = 'api' , method = 'generate_token' ) config . add_xmlrpc_method ( RPCInterface , attr = 'revoke_token' , endpoint = 'api' , method = 'revoke_token' ) config . add_xmlrpc_method ( RPCInterface , attr = 'revoke_all_tokens' , endpoint = 'api' , method = 'revoke_all_tokens' ) config . add_xmlrpc_method ( RPCInterface , attr = 'register_service' , endpoint = 'api' , method = 'register_service' ) config . add_xmlrpc_method ( RPCInterface , attr = 'unregister_service' , endpoint = 'api' , method = 'unregister_service' ) config . add_xmlrpc_method ( RPCInterface , attr = 'get_service_by_name' , endpoint = 'api' , method = 'get_service_by_name' ) config . add_xmlrpc_method ( RPCInterface , attr = 'get_service_by_url' , endpoint = 'api' , method = 'get_service_by_url' ) config . add_xmlrpc_method ( RPCInterface , attr = 'clear_services' , endpoint = 'api' , method = 'clear_services' ) config . add_xmlrpc_method ( RPCInterface , attr = 'list_services' , endpoint = 'api' , method = 'list_services' )
10614	def T ( self , T ) : self . _T = T self . _H = self . _calculate_H ( T )
11194	def freeze ( proto_dataset_uri ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri , config_path = CONFIG_PATH ) num_items = len ( list ( proto_dataset . _identifiers ( ) ) ) max_files_limit = int ( dtoolcore . utils . get_config_value ( "DTOOL_MAX_FILES_LIMIT" , CONFIG_PATH , 10000 ) ) assert isinstance ( max_files_limit , int ) if num_items > max_files_limit : click . secho ( "Too many items ({} > {}) in proto dataset" . format ( num_items , max_files_limit ) , fg = "red" ) click . secho ( "1. Consider splitting the dataset into smaller datasets" ) click . secho ( "2. Consider packaging small files using tar" ) click . secho ( "3. Increase the limit using the DTOOL_MAX_FILES_LIMIT" ) click . secho ( " environment variable" ) sys . exit ( 2 ) handles = [ h for h in proto_dataset . _storage_broker . iter_item_handles ( ) ] for h in handles : if not valid_handle ( h ) : click . secho ( "Invalid item name: {}" . format ( h ) , fg = "red" ) click . secho ( "1. Consider renaming the item" ) click . secho ( "2. Consider removing the item" ) sys . exit ( 3 ) with click . progressbar ( length = len ( list ( proto_dataset . _identifiers ( ) ) ) , label = "Generating manifest" ) as progressbar : try : proto_dataset . freeze ( progressbar = progressbar ) except dtoolcore . storagebroker . DiskStorageBrokerValidationWarning as e : click . secho ( "" ) click . secho ( str ( e ) , fg = "red" , nl = False ) sys . exit ( 4 ) click . secho ( "Dataset frozen " , nl = False , fg = "green" ) click . secho ( proto_dataset_uri )
2391	def regenerate_good_tokens ( string ) : toks = nltk . word_tokenize ( string ) pos_string = nltk . pos_tag ( toks ) pos_seq = [ tag [ 1 ] for tag in pos_string ] pos_ngrams = ngrams ( pos_seq , 2 , 4 ) sel_pos_ngrams = f7 ( pos_ngrams ) return sel_pos_ngrams
7783	def _deactivate ( self ) : self . cache . remove_fetcher ( self ) if self . active : self . _deactivated ( )
4229	def make_formatter ( format_name ) : if "json" in format_name : from json import dumps import datetime def jsonhandler ( obj ) : obj . isoformat ( ) if isinstance ( obj , ( datetime . datetime , datetime . date ) ) else obj if format_name == "prettyjson" : def jsondumps ( data ) : return dumps ( data , default = jsonhandler , indent = 2 , separators = ( ',' , ': ' ) ) else : def jsondumps ( data ) : return dumps ( data , default = jsonhandler ) def jsonify ( data ) : if isinstance ( data , dict ) : print ( jsondumps ( data ) ) elif isinstance ( data , list ) : print ( jsondumps ( [ device . _asdict ( ) for device in data ] ) ) else : print ( dumps ( { 'result' : data } ) ) return jsonify else : def printer ( data ) : if isinstance ( data , dict ) : print ( data ) else : for row in data : print ( row ) return printer
5313	def translate_style ( style , colormode , colorpalette ) : style_parts = iter ( style . split ( '_' ) ) ansi_start_sequence = [ ] ansi_end_sequence = [ ] try : # consume all modifiers part = None for mod_part in style_parts : part = mod_part if part not in ansi . MODIFIERS : break # all modifiers have been consumed mod_start_code , mod_end_code = resolve_modifier_to_ansi_code ( part , colormode ) ansi_start_sequence . append ( mod_start_code ) ansi_end_sequence . append ( mod_end_code ) else : # we've consumed all parts, thus we can exit raise StopIteration ( ) # next part has to be a foreground color or the 'on' keyword # which means we have to consume background colors if part != 'on' : ansi_start_code , ansi_end_code = translate_colorname_to_ansi_code ( part , ansi . FOREGROUND_COLOR_OFFSET , colormode , colorpalette ) ansi_start_sequence . append ( ansi_start_code ) ansi_end_sequence . append ( ansi_end_code ) # consume the required 'on' keyword after the foreground color next ( style_parts ) # next part has to be the background color part = next ( style_parts ) ansi_start_code , ansi_end_code = translate_colorname_to_ansi_code ( part , ansi . BACKGROUND_COLOR_OFFSET , colormode , colorpalette ) ansi_start_sequence . append ( ansi_start_code ) ansi_end_sequence . append ( ansi_end_code ) except StopIteration : # we've consumed all parts of the styling string pass # construct and return ANSI escape code sequence return '' . join ( ansi_start_sequence ) , '' . join ( ansi_end_sequence )
13292	def json_attributes ( self , vfuncs = None ) : vfuncs = vfuncs or [ ] js = { 'global' : { } } for k in self . ncattrs ( ) : js [ 'global' ] [ k ] = self . getncattr ( k ) for varname , var in self . variables . items ( ) : js [ varname ] = { } for k in var . ncattrs ( ) : z = var . getncattr ( k ) try : assert not np . isnan ( z ) . all ( ) js [ varname ] [ k ] = z except AssertionError : js [ varname ] [ k ] = None except TypeError : js [ varname ] [ k ] = z for vf in vfuncs : try : js [ varname ] . update ( vfuncs ( var ) ) except BaseException : logger . exception ( "Could not apply custom variable attribue function" ) return json . loads ( json . dumps ( js , cls = BasicNumpyEncoder ) )
3891	def html ( tag ) : return ( HTML_START . format ( tag = tag ) , HTML_END . format ( tag = tag ) )
11624	def from_devanagari ( self , data ) : from indic_transliteration import sanscript return sanscript . transliterate ( data = data , _from = sanscript . DEVANAGARI , _to = self . name )
2460	def set_pkg_desc ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_desc_set : self . package_desc_set = True if validations . validate_pkg_desc ( text ) : doc . package . description = str_from_text ( text ) else : raise SPDXValueError ( 'Package::Description' ) else : raise CardinalityError ( 'Package::Description' )
6901	def _parse_xmatch_catalog_header ( xc , xk ) : catdef = [ ] # read in this catalog and transparently handle gzipped files if xc . endswith ( '.gz' ) : infd = gzip . open ( xc , 'rb' ) else : infd = open ( xc , 'rb' ) # read in the defs for line in infd : if line . decode ( ) . startswith ( '#' ) : catdef . append ( line . decode ( ) . replace ( '#' , '' ) . strip ( ) . rstrip ( '\n' ) ) if not line . decode ( ) . startswith ( '#' ) : break if not len ( catdef ) > 0 : LOGERROR ( "catalog definition not parseable " "for catalog: %s, skipping..." % xc ) return None catdef = ' ' . join ( catdef ) catdefdict = json . loads ( catdef ) catdefkeys = [ x [ 'key' ] for x in catdefdict [ 'columns' ] ] catdefdtypes = [ x [ 'dtype' ] for x in catdefdict [ 'columns' ] ] catdefnames = [ x [ 'name' ] for x in catdefdict [ 'columns' ] ] catdefunits = [ x [ 'unit' ] for x in catdefdict [ 'columns' ] ] # get the correct column indices and dtypes for the requested columns # from the catdefdict catcolinds = [ ] catcoldtypes = [ ] catcolnames = [ ] catcolunits = [ ] for xkcol in xk : if xkcol in catdefkeys : xkcolind = catdefkeys . index ( xkcol ) catcolinds . append ( xkcolind ) catcoldtypes . append ( catdefdtypes [ xkcolind ] ) catcolnames . append ( catdefnames [ xkcolind ] ) catcolunits . append ( catdefunits [ xkcolind ] ) return ( infd , catdefdict , catcolinds , catcoldtypes , catcolnames , catcolunits )
10428	def get_pmids ( graph : BELGraph , output : TextIO ) : for pmid in get_pubmed_identifiers ( graph ) : click . echo ( pmid , file = output )
6111	def trace_to_next_plane ( self ) : return list ( map ( lambda positions , deflections : np . subtract ( positions , deflections ) , self . positions , self . deflections ) )
3975	def _env_vars_from_file ( filename ) : def split_env ( env ) : if '=' in env : return env . split ( '=' , 1 ) else : return env , None env = { } for line in open ( filename , 'r' ) : line = line . strip ( ) if line and not line . startswith ( '#' ) : k , v = split_env ( line ) env [ k ] = v return env
8598	def list_shares ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/shares?depth=%s' % ( group_id , str ( depth ) ) ) return response
12854	def parse ( filename ) : for event , elt in et . iterparse ( filename , events = ( 'start' , 'end' , 'comment' , 'pi' ) , huge_tree = True ) : if event == 'start' : obj = _elt2obj ( elt ) obj [ 'type' ] = ENTER yield obj if elt . text : yield { 'type' : TEXT , 'text' : elt . text } elif event == 'end' : yield { 'type' : EXIT } if elt . tail : yield { 'type' : TEXT , 'text' : elt . tail } elt . clear ( ) elif event == 'comment' : yield { 'type' : COMMENT , 'text' : elt . text } elif event == 'pi' : yield { 'type' : PI , 'text' : elt . text } else : assert False , ( event , elt )
13628	def parse ( expected , query ) : return dict ( ( key , parser ( query . get ( key , [ ] ) ) ) for key , parser in expected . items ( ) )
11663	def as_integer_type ( ary ) : ary = np . asanyarray ( ary ) if is_integer_type ( ary ) : return ary rounded = np . rint ( ary ) if np . any ( rounded != ary ) : raise ValueError ( "argument array must contain only integers" ) return rounded . astype ( int )
8627	def get_users ( session , query ) : # GET /api/users/0.1/users response = make_get_request ( session , 'users' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise UsersNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
4129	def _autocov ( s , * * kwargs ) : # only remove the mean once, if needed debias = kwargs . pop ( 'debias' , True ) axis = kwargs . get ( 'axis' , - 1 ) if debias : s = _remove_bias ( s , axis ) kwargs [ 'debias' ] = False return _crosscov ( s , s , * * kwargs )
7822	def _make_response ( self , nonce , salt , iteration_count ) : self . _salted_password = self . Hi ( self . Normalize ( self . password ) , salt , iteration_count ) self . password = None # not needed any more if self . channel_binding : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header + self . _cb_data ) else : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header ) # pylint: disable=C0103 client_final_message_without_proof = ( channel_binding + b",r=" + nonce ) client_key = self . HMAC ( self . _salted_password , b"Client Key" ) stored_key = self . H ( client_key ) auth_message = ( self . _client_first_message_bare + b"," + self . _server_first_message + b"," + client_final_message_without_proof ) self . _auth_message = auth_message client_signature = self . HMAC ( stored_key , auth_message ) client_proof = self . XOR ( client_key , client_signature ) proof = b"p=" + standard_b64encode ( client_proof ) client_final_message = ( client_final_message_without_proof + b"," + proof ) return Response ( client_final_message )
2661	def hold_worker ( self , worker_id ) : c = self . command_client . run ( "HOLD_WORKER;{}" . format ( worker_id ) ) logger . debug ( "Sent hold request to worker: {}" . format ( worker_id ) ) return c
12973	def getMultipleOnlyFields ( self , pks , fields , cascadeFetch = False ) : if type ( pks ) == set : pks = list ( pks ) if len ( pks ) == 1 : return IRQueryableList ( [ self . getOnlyFields ( pks [ 0 ] , fields , cascadeFetch = cascadeFetch ) ] , mdl = self . mdl ) conn = self . _get_connection ( ) pipeline = conn . pipeline ( ) for pk in pks : key = self . _get_key_for_id ( pk ) pipeline . hmget ( key , fields ) res = pipeline . execute ( ) ret = IRQueryableList ( mdl = self . mdl ) pksLen = len ( pks ) i = 0 numFields = len ( fields ) while i < pksLen : objDict = { } anyNotNone = False thisRes = res [ i ] if thisRes is None or type ( thisRes ) != list : ret . append ( None ) i += 1 continue j = 0 while j < numFields : objDict [ fields [ j ] ] = thisRes [ j ] if thisRes [ j ] != None : anyNotNone = True j += 1 if anyNotNone is False : ret . append ( None ) i += 1 continue objDict [ '_id' ] = pks [ i ] obj = self . _redisResultToObj ( objDict ) ret . append ( obj ) i += 1 if cascadeFetch is True : for obj in ret : self . _doCascadeFetch ( obj ) return ret
12863	def quoted ( parser = any_token ) : quote_char = quote ( ) value , _ = many_until ( parser , partial ( one_of , quote_char ) ) return build_string ( value )
196	def Clouds ( name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return meta . SomeOf ( ( 1 , 2 ) , children = [ CloudLayer ( intensity_mean = ( 196 , 255 ) , intensity_freq_exponent = ( - 2.5 , - 2.0 ) , intensity_coarse_scale = 10 , alpha_min = 0 , alpha_multiplier = ( 0.25 , 0.75 ) , alpha_size_px_max = ( 2 , 8 ) , alpha_freq_exponent = ( - 2.5 , - 2.0 ) , sparsity = ( 0.8 , 1.0 ) , density_multiplier = ( 0.5 , 1.0 ) ) , CloudLayer ( intensity_mean = ( 196 , 255 ) , intensity_freq_exponent = ( - 2.0 , - 1.0 ) , intensity_coarse_scale = 10 , alpha_min = 0 , alpha_multiplier = ( 0.5 , 1.0 ) , alpha_size_px_max = ( 64 , 128 ) , alpha_freq_exponent = ( - 2.0 , - 1.0 ) , sparsity = ( 1.0 , 1.4 ) , density_multiplier = ( 0.8 , 1.5 ) ) ] , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
10498	def tripleClickMouse ( self , coord ) : # Note above re: double-clicks applies to triple-clicks modFlags = 0 for i in range ( 2 ) : self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , clickCount = 3 ) self . _postQueuedEvents ( )
5262	def camelcase ( string ) : string = re . sub ( r"^[\-_\.]" , '' , str ( string ) ) if not string : return string return lowercase ( string [ 0 ] ) + re . sub ( r"[\-_\.\s]([a-z])" , lambda matched : uppercase ( matched . group ( 1 ) ) , string [ 1 : ] )
7057	def s3_get_file ( bucket , filename , local_file , altexts = None , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : client . download_file ( bucket , filename , local_file ) return local_file except Exception as e : if altexts is not None : for alt_extension in altexts : split_ext = os . path . splitext ( filename ) check_file = split_ext [ 0 ] + alt_extension try : client . download_file ( bucket , check_file , local_file . replace ( split_ext [ - 1 ] , alt_extension ) ) return local_file . replace ( split_ext [ - 1 ] , alt_extension ) except Exception as e : pass else : LOGEXCEPTION ( 'could not download s3://%s/%s' % ( bucket , filename ) ) if raiseonfail : raise return None
1132	def _replace ( _self , * * kwds ) : result = _self . _make ( map ( kwds . pop , ( 'scheme' , 'netloc' , 'path' , 'query' , 'fragment' ) , _self ) ) if kwds : raise ValueError ( 'Got unexpected field names: %r' % kwds . keys ( ) ) return result
9160	def delete_roles_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted_roles = request . json with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_role_requests ( cursor , uuid_ , posted_roles ) resp = request . response resp . status_int = 200 return resp
11951	def _import_config ( config_file ) : # get config file path jocker_lgr . debug ( 'config file is: {0}' . format ( config_file ) ) # append to path for importing try : jocker_lgr . debug ( 'importing config...' ) with open ( config_file , 'r' ) as c : return yaml . safe_load ( c . read ( ) ) except IOError as ex : jocker_lgr . error ( str ( ex ) ) raise RuntimeError ( 'cannot access config file' ) except yaml . parser . ParserError as ex : jocker_lgr . error ( 'invalid yaml file: {0}' . format ( ex ) ) raise RuntimeError ( 'invalid yaml file' )
4311	def _build_input_args ( input_filepath_list , input_format_list ) : if len ( input_format_list ) != len ( input_filepath_list ) : raise ValueError ( "input_format_list & input_filepath_list are not the same size" ) input_args = [ ] zipped = zip ( input_filepath_list , input_format_list ) for input_file , input_fmt in zipped : input_args . extend ( input_fmt ) input_args . append ( input_file ) return input_args
13397	def get_reference_to_class ( cls , class_or_class_name ) : if isinstance ( class_or_class_name , type ) : return class_or_class_name elif isinstance ( class_or_class_name , string_types ) : if ":" in class_or_class_name : mod_name , class_name = class_or_class_name . split ( ":" ) if not mod_name in sys . modules : __import__ ( mod_name ) mod = sys . modules [ mod_name ] return mod . __dict__ [ class_name ] else : return cls . load_class_from_locals ( class_or_class_name ) else : msg = "Unexpected Type '%s'" % type ( class_or_class_name ) raise InternalCashewException ( msg )
7140	def transfer ( self , address , amount , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . accounts [ 0 ] . transfer ( address , amount , priority = priority , payment_id = payment_id , unlock_time = unlock_time , relay = relay )
9922	def create ( self , validated_data ) : email = validated_data . pop ( "email" ) password = validated_data . pop ( "password" ) # We don't save the user instance yet in case the provided email # address already exists. user = get_user_model ( ) ( * * validated_data ) user . set_password ( password ) # We set an ephemeral email property so that it is included in # the data returned by the serializer. user . email = email email_query = models . EmailAddress . objects . filter ( email = email ) if email_query . exists ( ) : existing_email = email_query . get ( ) existing_email . send_duplicate_notification ( ) else : user . save ( ) email_instance = models . EmailAddress . objects . create ( email = email , user = user ) email_instance . send_confirmation ( ) signals . user_registered . send ( sender = self . __class__ , user = user ) return user
6496	def _get_mappings ( self , doc_type ) : # Try loading the mapping from the cache. mapping = ElasticSearchEngine . get_mappings ( self . index_name , doc_type ) # Fall back to Elasticsearch if not mapping : mapping = self . _es . indices . get_mapping ( index = self . index_name , doc_type = doc_type , ) . get ( self . index_name , { } ) . get ( 'mappings' , { } ) . get ( doc_type , { } ) # Cache the mapping, if one was retrieved if mapping : ElasticSearchEngine . set_mappings ( self . index_name , doc_type , mapping ) return mapping
10975	def manage ( group_id ) : group = Group . query . get_or_404 ( group_id ) form = GroupForm ( request . form , obj = group ) if form . validate_on_submit ( ) : if group . can_edit ( current_user ) : try : group . update ( * * form . data ) flash ( _ ( 'Group "%(name)s" was updated' , name = group . name ) , 'success' ) except Exception as e : flash ( str ( e ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , group = group , ) else : flash ( _ ( 'You cannot edit group %(group_name)s' , group_name = group . name ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , group = group , )
5279	def make_pmml_pipeline ( obj , active_fields = None , target_fields = None ) : steps = _filter_steps ( _get_steps ( obj ) ) pipeline = PMMLPipeline ( steps ) if active_fields is not None : pipeline . active_fields = numpy . asarray ( active_fields ) if target_fields is not None : pipeline . target_fields = numpy . asarray ( target_fields ) return pipeline
6912	def generate_flare_lightcurve ( times , mags = None , errs = None , paramdists = { # flare peak amplitude from 0.01 mag to 1.0 mag above median. this # is tuned for redder bands, flares are much stronger in bluer # bands, so tune appropriately for your situation. 'amplitude' : sps . uniform ( loc = 0.01 , scale = 0.99 ) , # up to 5 flares per LC and at least 1 'nflares' : [ 1 , 5 ] , # 10 minutes to 1 hour for rise stdev 'risestdev' : sps . uniform ( loc = 0.007 , scale = 0.04 ) , # 1 hour to 4 hours for decay time constant 'decayconst' : sps . uniform ( loc = 0.04 , scale = 0.163 ) } , magsarefluxes = False , ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) nflares = npr . randint ( paramdists [ 'nflares' ] [ 0 ] , high = paramdists [ 'nflares' ] [ 1 ] ) # generate random flare peak times based on the number of flares flarepeaktimes = ( npr . random ( size = nflares ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) ) # now add the flares to the time-series params = { 'nflares' : nflares } for flareind , peaktime in zip ( range ( nflares ) , flarepeaktimes ) : # choose the amplitude, rise stdev and decay time constant amp = paramdists [ 'amplitude' ] . rvs ( size = 1 ) risestdev = paramdists [ 'risestdev' ] . rvs ( size = 1 ) decayconst = paramdists [ 'decayconst' ] . rvs ( size = 1 ) # fix the transit depth if it needs to be flipped if magsarefluxes and amp < 0.0 : amp = - amp elif not magsarefluxes and amp > 0.0 : amp = - amp # add this flare to the light curve modelmags , ptimes , pmags , perrs = ( flares . flare_model ( [ amp , peaktime , risestdev , decayconst ] , times , mags , errs ) ) # update the mags mags = modelmags # add the flare params to the modeldict params [ flareind ] = { 'peaktime' : peaktime , 'amplitude' : amp , 'risestdev' : risestdev , 'decayconst' : decayconst } # # done with all flares # # return a dict with everything modeldict = { 'vartype' : 'flare' , 'params' : params , 'times' : times , 'mags' : mags , 'errs' : errs , 'varperiod' : None , # FIXME: this is complicated because we can have multiple flares # figure out a good way to handle this upstream 'varamplitude' : [ params [ x ] [ 'amplitude' ] for x in range ( params [ 'nflares' ] ) ] , } return modeldict
10	def save_policy ( self , path ) : with open ( path , 'wb' ) as f : pickle . dump ( self . policy , f )
1769	def concrete_emulate ( self , insn ) : if not self . emu : self . emu = ConcreteUnicornEmulator ( self ) self . emu . _stop_at = self . _break_unicorn_at try : self . emu . emulate ( insn ) except unicorn . UcError as e : if e . errno == unicorn . UC_ERR_INSN_INVALID : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . error ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) raise InstructionEmulationError ( str ( e ) )
9012	def _start ( self ) : self . _instruction_library = self . _spec . new_default_instructions ( ) self . _as_instruction = self . _instruction_library . as_instruction self . _id_cache = { } self . _pattern_set = None self . _inheritance_todos = [ ] self . _instruction_todos = [ ]
10083	def edit ( self , pid = None ) : pid = pid or self . pid with db . session . begin_nested ( ) : before_record_update . send ( current_app . _get_current_object ( ) , record = self ) record_pid , record = self . fetch_published ( ) assert PIDStatus . REGISTERED == record_pid . status assert record [ '_deposit' ] == self [ '_deposit' ] self . model . json = self . _prepare_edit ( record ) flag_modified ( self . model , 'json' ) db . session . merge ( self . model ) after_record_update . send ( current_app . _get_current_object ( ) , record = self ) return self . __class__ ( self . model . json , model = self . model )
7650	def load ( path_or_file , validate = True , strict = True , fmt = 'auto' ) : with _open ( path_or_file , mode = 'r' , fmt = fmt ) as fdesc : jam = JAMS ( * * json . load ( fdesc ) ) if validate : jam . validate ( strict = strict ) return jam
1508	def add_additional_args ( parsers ) : for parser in parsers : cli_args . add_verbose ( parser ) cli_args . add_config ( parser ) parser . add_argument ( '--heron-dir' , default = config . get_heron_dir ( ) , help = 'Path to Heron home directory' )
3367	def linear_reaction_coefficients ( model , reactions = None ) : linear_coefficients = { } reactions = model . reactions if not reactions else reactions try : objective_expression = model . solver . objective . expression coefficients = objective_expression . as_coefficients_dict ( ) except AttributeError : return linear_coefficients for rxn in reactions : forward_coefficient = coefficients . get ( rxn . forward_variable , 0 ) reverse_coefficient = coefficients . get ( rxn . reverse_variable , 0 ) if forward_coefficient != 0 : if forward_coefficient == - reverse_coefficient : linear_coefficients [ rxn ] = float ( forward_coefficient ) return linear_coefficients
461	def get_random_int ( min_v = 0 , max_v = 10 , number = 5 , seed = None ) : rnd = random . Random ( ) if seed : rnd = random . Random ( seed ) # return [random.randint(min,max) for p in range(0, number)] return [ rnd . randint ( min_v , max_v ) for p in range ( 0 , number ) ]
13813	def MessageToJson ( message , including_default_value_fields = False ) : js = _MessageToJsonObject ( message , including_default_value_fields ) return json . dumps ( js , indent = 2 )
1943	def protect_memory_callback ( self , start , size , perms ) : logger . info ( f"Changing permissions on {hex(start)}:{hex(start + size)} to {perms}" ) self . _emu . mem_protect ( start , size , convert_permissions ( perms ) )
12206	def raise_for_status ( response ) : for err_name in web_exceptions . __all__ : err = getattr ( web_exceptions , err_name ) if err . status_code == response . status : payload = dict ( headers = response . headers , reason = response . reason , ) if issubclass ( err , web_exceptions . _HTTPMove ) : # pylint: disable=protected-access raise err ( response . headers [ 'Location' ] , * * payload ) raise err ( * * payload )
3756	def Tautoignition ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'Tautoignition' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'Tautoignition' ] ) : methods . append ( NFPA ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'Tautoignition' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'Tautoignition' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
11507	def item_get ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id response = self . request ( 'midas.item.get' , parameters ) return response
9925	def get_queryset ( self ) : oldest = timezone . now ( ) - app_settings . PASSWORD_RESET_EXPIRATION queryset = super ( ValidPasswordResetTokenManager , self ) . get_queryset ( ) return queryset . filter ( created_at__gt = oldest )
10700	def paginate_link_tag ( item ) : a_tag = Page . default_link_tag ( item ) if item [ 'type' ] == 'current_page' : return make_html_tag ( 'li' , a_tag , * * { 'class' : 'blue white-text' } ) return make_html_tag ( 'li' , a_tag )
4830	def get_course_grade ( self , course_id , username ) : results = self . client . courses ( course_id ) . get ( username = username ) for row in results : if row . get ( 'username' ) == username : return row raise HttpNotFoundError ( 'No grade record found for course={}, username={}' . format ( course_id , username ) )
12302	def validate ( repo , validator_name = None , filename = None , rulesfiles = None , args = [ ] ) : mgr = plugins_get_mgr ( ) # Expand the specification. Now we have full file paths validator_specs = instantiate ( repo , validator_name , filename , rulesfiles ) # Run the validators with rules files... allresults = [ ] for v in validator_specs : keys = mgr . search ( what = 'validator' , name = v ) [ 'validator' ] for k in keys : validator = mgr . get_by_key ( 'validator' , k ) result = validator . evaluate ( repo , validator_specs [ v ] , args ) allresults . extend ( result ) return allresults
446	def batch_with_dynamic_pad ( images_and_captions , batch_size , queue_capacity , add_summaries = True ) : enqueue_list = [ ] for image , caption in images_and_captions : caption_length = tf . shape ( caption ) [ 0 ] input_length = tf . expand_dims ( tf . subtract ( caption_length , 1 ) , 0 ) input_seq = tf . slice ( caption , [ 0 ] , input_length ) target_seq = tf . slice ( caption , [ 1 ] , input_length ) indicator = tf . ones ( input_length , dtype = tf . int32 ) enqueue_list . append ( [ image , input_seq , target_seq , indicator ] ) images , input_seqs , target_seqs , mask = tf . train . batch_join ( enqueue_list , batch_size = batch_size , capacity = queue_capacity , dynamic_pad = True , name = "batch_and_pad" ) if add_summaries : lengths = tf . add ( tf . reduce_sum ( mask , 1 ) , 1 ) tf . summary . scalar ( "caption_length/batch_min" , tf . reduce_min ( lengths ) ) tf . summary . scalar ( "caption_length/batch_max" , tf . reduce_max ( lengths ) ) tf . summary . scalar ( "caption_length/batch_mean" , tf . reduce_mean ( lengths ) ) return images , input_seqs , target_seqs , mask
11002	def pack_args ( self ) : mapper = { 'psf-kfki' : 'kfki' , 'psf-alpha' : 'alpha' , 'psf-n2n1' : 'n2n1' , 'psf-sigkf' : 'sigkf' , 'psf-sph6-ab' : 'sph6_ab' , 'psf-laser-wavelength' : 'laser_wavelength' , 'psf-pinhole-width' : 'pinhole_width' } bads = [ self . zscale , 'psf-zslab' ] d = { } for k , v in iteritems ( mapper ) : if k in self . param_dict : d [ v ] = self . param_dict [ k ] d . update ( { 'polar_angle' : self . polar_angle , 'normalize' : self . normalize , 'include_K3_det' : self . use_J1 } ) if self . polychromatic : d . update ( { 'nkpts' : self . nkpts } ) d . update ( { 'k_dist' : self . k_dist } ) if self . do_pinhole : d . update ( { 'nlpts' : self . num_line_pts } ) d . update ( { 'use_laggauss' : True } ) return d
9099	def write_bel_annotation ( self , file : TextIO ) -> None : if not self . is_populated ( ) : self . populate ( ) values = self . _get_namespace_name_to_encoding ( desc = 'writing names' ) write_annotation ( keyword = self . _get_namespace_keyword ( ) , citation_name = self . _get_namespace_name ( ) , description = '' , values = values , file = file , )
12466	def run_cmd ( cmd , echo = False , fail_silently = False , * * kwargs ) : out , err = None , None if echo : cmd_str = cmd if isinstance ( cmd , string_types ) else ' ' . join ( cmd ) kwargs [ 'stdout' ] , kwargs [ 'stderr' ] = sys . stdout , sys . stderr print_message ( '$ {0}' . format ( cmd_str ) ) else : out , err = get_temp_streams ( ) kwargs [ 'stdout' ] , kwargs [ 'stderr' ] = out , err try : retcode = subprocess . call ( cmd , * * kwargs ) except subprocess . CalledProcessError as err : if fail_silently : return False print_error ( str ( err ) if IS_PY3 else unicode ( err ) ) # noqa finally : if out : out . close ( ) if err : err . close ( ) if retcode and echo and not fail_silently : print_error ( 'Command {0!r} returned non-zero exit status {1}' . format ( cmd_str , retcode ) ) return retcode
619	def unescape ( s ) : assert isinstance ( s , basestring ) s = s . replace ( '\t' , ',' ) s = s . replace ( '\\,' , ',' ) s = s . replace ( '\\n' , '\n' ) s = s . replace ( '\\\\' , '\\' ) return s
5709	def redirect ( self , request ) : url = request . path querystring = request . GET . copy ( ) if self . logout_key and self . logout_key in request . GET : del querystring [ self . logout_key ] if querystring : url = '%s?%s' % ( url , querystring . urlencode ( ) ) return HttpResponseRedirect ( url )
12077	def figure ( self , forceNew = False ) : if plt . _pylab_helpers . Gcf . get_num_fig_managers ( ) > 0 and forceNew is False : self . log . debug ( "figure already seen, not creating one." ) return if self . subplot : self . log . debug ( "subplot mode enabled, not creating new figure" ) else : self . log . debug ( "creating new figure" ) plt . figure ( figsize = ( self . figure_width , self . figure_height ) )
3704	def Yamada_Gunn ( T , Tc , Pc , omega ) : return R * Tc / Pc * ( 0.29056 - 0.08775 * omega ) ** ( 1 + ( 1 - T / Tc ) ** ( 2 / 7. ) )
1407	def emit ( self , tup , tup_id = None , stream = Stream . DEFAULT_STREAM_ID , direct_task = None , need_task_ids = False ) : # first check whether this tuple is sane self . pplan_helper . check_output_schema ( stream , tup ) # get custom grouping target task ids; get empty list if not custom grouping custom_target_task_ids = self . pplan_helper . choose_tasks_for_custom_grouping ( stream , tup ) self . pplan_helper . context . invoke_hook_emit ( tup , stream , None ) data_tuple = tuple_pb2 . HeronDataTuple ( ) data_tuple . key = 0 if direct_task is not None : if not isinstance ( direct_task , int ) : raise TypeError ( "direct_task argument needs to be an integer, given: %s" % str ( type ( direct_task ) ) ) # performing emit-direct data_tuple . dest_task_ids . append ( direct_task ) elif custom_target_task_ids is not None : # for custom grouping for task_id in custom_target_task_ids : data_tuple . dest_task_ids . append ( task_id ) if tup_id is not None : tuple_info = TupleHelper . make_root_tuple_info ( stream , tup_id ) if self . acking_enabled : # this message is rooted root = data_tuple . roots . add ( ) root . taskid = self . pplan_helper . my_task_id root . key = tuple_info . key self . in_flight_tuples [ tuple_info . key ] = tuple_info else : self . immediate_acks . append ( tuple_info ) tuple_size_in_bytes = 0 start_time = time . time ( ) # Serialize for obj in tup : serialized = self . serializer . serialize ( obj ) data_tuple . values . append ( serialized ) tuple_size_in_bytes += len ( serialized ) serialize_latency_ns = ( time . time ( ) - start_time ) * system_constants . SEC_TO_NS self . spout_metrics . serialize_data_tuple ( stream , serialize_latency_ns ) super ( SpoutInstance , self ) . admit_data_tuple ( stream_id = stream , data_tuple = data_tuple , tuple_size_in_bytes = tuple_size_in_bytes ) self . total_tuples_emitted += 1 self . spout_metrics . update_emit_count ( stream ) if need_task_ids : sent_task_ids = custom_target_task_ids or [ ] if direct_task is not None : sent_task_ids . append ( direct_task ) return sent_task_ids
5414	def get_provider ( args , resources ) : provider = getattr ( args , 'provider' , 'google' ) if provider == 'google' : return google . GoogleJobProvider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry_run' , False ) , args . project ) elif provider == 'google-v2' : return google_v2 . GoogleV2JobProvider ( getattr ( args , 'verbose' , False ) , getattr ( args , 'dry_run' , False ) , args . project ) elif provider == 'local' : return local . LocalJobProvider ( resources ) elif provider == 'test-fails' : return test_fails . FailsJobProvider ( ) else : raise ValueError ( 'Unknown provider: ' + provider )
6290	def add_program_dir ( self , directory ) : dirs = list ( self . PROGRAM_DIRS ) dirs . append ( directory ) self . PROGRAM_DIRS = dirs
12038	def matrixToDicts ( data ) : # 1D array if "float" in str ( type ( data [ 0 ] ) ) : d = { } for x in range ( len ( data ) ) : d [ data . dtype . names [ x ] ] = data [ x ] return d # 2D array l = [ ] for y in range ( len ( data ) ) : d = { } for x in range ( len ( data [ y ] ) ) : d [ data . dtype . names [ x ] ] = data [ y ] [ x ] l . append ( d ) return l
1686	def Split ( self ) : googlename = self . RepositoryName ( ) project , rest = os . path . split ( googlename ) return ( project , ) + os . path . splitext ( rest )
9268	def detect_link_tag_time ( self , tag ) : # if tag is nil - set current time newer_tag_time = self . get_time_of_tag ( tag ) if tag else datetime . datetime . now ( ) # if it's future release tag - set this value if tag [ "name" ] == self . options . unreleased_label and self . options . future_release : newer_tag_name = self . options . future_release newer_tag_link = self . options . future_release elif tag [ "name" ] is not self . options . unreleased_label : # put unreleased label if there is no name for the tag newer_tag_name = tag [ "name" ] newer_tag_link = newer_tag_name else : newer_tag_name = self . options . unreleased_label newer_tag_link = "HEAD" return [ newer_tag_link , newer_tag_name , newer_tag_time ]
1336	def name ( self ) : names = ( criterion . name ( ) for criterion in self . _criteria ) return '__' . join ( sorted ( names ) )
9277	def parse ( packet ) : if not isinstance ( packet , string_type_parse ) : raise TypeError ( "Expected packet to be str/unicode/bytes, got %s" , type ( packet ) ) if len ( packet ) == 0 : raise ParseError ( "packet is empty" , packet ) # attempt to detect encoding if isinstance ( packet , bytes ) : packet = _unicode_packet ( packet ) packet = packet . rstrip ( "\r\n" ) logger . debug ( "Parsing: %s" , packet ) # split into head and body try : ( head , body ) = packet . split ( ':' , 1 ) except : raise ParseError ( "packet has no body" , packet ) if len ( body ) == 0 : raise ParseError ( "packet body is empty" , packet ) parsed = { 'raw' : packet , } # parse head try : parsed . update ( parse_header ( head ) ) except ParseError as msg : raise ParseError ( str ( msg ) , packet ) # parse body packet_type = body [ 0 ] body = body [ 1 : ] if len ( body ) == 0 and packet_type != '>' : raise ParseError ( "packet body is empty after packet type character" , packet ) # attempt to parse the body try : _try_toparse_body ( packet_type , body , parsed ) # capture ParseErrors and attach the packet except ( UnknownFormat , ParseError ) as exp : exp . packet = packet raise # if we fail all attempts to parse, try beacon packet if 'format' not in parsed : if not re . match ( r"^(AIR.*|ALL.*|AP.*|BEACON|CQ.*|GPS.*|DF.*|DGPS.*|" "DRILL.*|DX.*|ID.*|JAVA.*|MAIL.*|MICE.*|QST.*|QTH.*|" "RTCM.*|SKY.*|SPACE.*|SPC.*|SYM.*|TEL.*|TEST.*|TLM.*|" "WX.*|ZIP.*|UIDIGI)$" , parsed [ 'to' ] ) : raise UnknownFormat ( "format is not supported" , packet ) parsed . update ( { 'format' : 'beacon' , 'text' : packet_type + body , } ) logger . debug ( "Parsed ok." ) return parsed
11721	def config_loader ( app , * * kwargs_config ) : # This is the only place customize the Flask application right after # it has been created, but before all extensions etc are loaded. local_templates_path = os . path . join ( app . instance_path , 'templates' ) if os . path . exists ( local_templates_path ) : # Let's customize the template loader to look into packages # and application templates folders. app . jinja_loader = ChoiceLoader ( [ FileSystemLoader ( local_templates_path ) , app . jinja_loader , ] ) app . jinja_options = dict ( app . jinja_options , cache_size = 1000 , bytecode_cache = BytecodeCache ( app ) ) invenio_config_loader ( app , * * kwargs_config )
6675	def umask ( self , use_sudo = False ) : func = use_sudo and run_as_root or self . run return func ( 'umask' )
6841	def supported_locales ( ) : family = distrib_family ( ) if family == 'debian' : return _parse_locales ( '/usr/share/i18n/SUPPORTED' ) elif family == 'arch' : return _parse_locales ( '/etc/locale.gen' ) elif family == 'redhat' : return _supported_locales_redhat ( ) else : raise UnsupportedFamily ( supported = [ 'debian' , 'arch' , 'redhat' ] )
12633	def calculate_file_distances ( dicom_files , field_weights = None , dist_method_cls = None , * * kwargs ) : if dist_method_cls is None : dist_method = LevenshteinDicomFileDistance ( field_weights ) else : try : dist_method = dist_method_cls ( field_weights = field_weights , * * kwargs ) except : log . exception ( 'Could not instantiate {} object with field_weights ' 'and {}' . format ( dist_method_cls , kwargs ) ) dist_dtype = np . float16 n_files = len ( dicom_files ) try : file_dists = np . zeros ( ( n_files , n_files ) , dtype = dist_dtype ) except MemoryError as mee : import scipy . sparse file_dists = scipy . sparse . lil_matrix ( ( n_files , n_files ) , dtype = dist_dtype ) for idxi in range ( n_files ) : dist_method . set_dicom_file1 ( dicom_files [ idxi ] ) for idxj in range ( idxi + 1 , n_files ) : dist_method . set_dicom_file2 ( dicom_files [ idxj ] ) if idxi != idxj : file_dists [ idxi , idxj ] = dist_method . transform ( ) return file_dists
13364	def setup_engines ( client = None ) : if not client : try : client = ipyparallel . Client ( ) except : raise DistobClusterError ( u"""Could not connect to an ipyparallel cluster. Make sure a cluster is started (e.g. to use the CPUs of a single computer, can type 'ipcluster start')""" ) eids = client . ids if not eids : raise DistobClusterError ( u'No ipyparallel compute engines are available' ) nengines = len ( eids ) dv = client [ eids ] dv . use_dill ( ) with dv . sync_imports ( quiet = True ) : import distob # create global ObjectEngine distob.engine on each engine ars = [ ] for i in eids : dv . targets = i ars . append ( dv . apply_async ( _remote_setup_engine , i , nengines ) ) dv . wait ( ars ) for ar in ars : if not ar . successful ( ) : raise ar . r # create global ObjectHub distob.engine on the client host if distob . engine is None : distob . engine = ObjectHub ( - 1 , client )
12560	def large_clusters_mask ( volume , min_cluster_size ) : labels , num_labels = scn . label ( volume ) labels_to_keep = set ( [ i for i in range ( num_labels ) if np . sum ( labels == i ) >= min_cluster_size ] ) clusters_mask = np . zeros_like ( volume , dtype = int ) for l in range ( num_labels ) : if l in labels_to_keep : clusters_mask [ labels == l ] = 1 return clusters_mask
11933	def load_widgets ( context , * * kwargs ) : _soft = kwargs . pop ( '_soft' , False ) try : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] except KeyError : widgets = context . render_context [ WIDGET_CONTEXT_KEY ] = { } for alias , template_name in kwargs . items ( ) : if _soft and alias in widgets : continue with context . render_context . push ( { BLOCK_CONTEXT_KEY : BlockContext ( ) } ) : blocks = resolve_blocks ( template_name , context ) widgets [ alias ] = blocks return ''
1228	def tf_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : # Mean loss per instance loss_per_instance = self . fn_loss_per_instance ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) # Returns no-op. updated = self . memory . update_batch ( loss_per_instance = loss_per_instance ) with tf . control_dependencies ( control_inputs = ( updated , ) ) : loss = tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 ) # Loss without regularization summary. if 'losses' in self . summary_labels : tf . contrib . summary . scalar ( name = 'loss-without-regularization' , tensor = loss ) # Regularization losses. losses = self . fn_regularization_losses ( states = states , internals = internals , update = update ) if len ( losses ) > 0 : loss += tf . add_n ( inputs = [ losses [ name ] for name in sorted ( losses ) ] ) if 'regularization' in self . summary_labels : for name in sorted ( losses ) : tf . contrib . summary . scalar ( name = ( 'regularization/' + name ) , tensor = losses [ name ] ) # Total loss summary. if 'losses' in self . summary_labels or 'total-loss' in self . summary_labels : tf . contrib . summary . scalar ( name = 'total-loss' , tensor = loss ) return loss
3160	def update ( self , campaign_id , data ) : self . campaign_id = campaign_id return self . _mc_client . _put ( url = self . _build_path ( campaign_id , 'content' ) , data = data )
8718	def file_remove ( self , path ) : log . info ( 'Remove ' + path ) cmd = 'file.remove("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
12519	def mask ( self , image ) : if image is None : self . _mask = None try : mask = load_mask ( image ) except Exception as exc : raise Exception ( 'Could not load mask image {}.' . format ( image ) ) from exc else : self . _mask = mask
12532	def _store_dicom_paths ( self , folders ) : if isinstance ( folders , str ) : folders = [ folders ] for folder in folders : if not os . path . exists ( folder ) : raise FolderNotFound ( folder ) self . items . extend ( list ( find_all_dicom_files ( folder ) ) )
3638	def clubConsumables ( self , fast = False ) : method = 'GET' url = 'club/consumables/development' rc = self . __request__ ( method , url ) events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables - List View' ) ] self . pin . send ( events , fast = fast ) return [ itemParse ( i ) for i in rc . get ( 'itemData' , ( ) ) ]
6869	def normalize_magseries ( times , mags , mingap = 4.0 , normto = 'globalmedian' , magsarefluxes = False , debugmode = False ) : ngroups , timegroups = find_lc_timegroups ( times , mingap = mingap ) # find all the non-nan indices finite_ind = np . isfinite ( mags ) if any ( finite_ind ) : # find the global median global_mag_median = np . median ( mags [ finite_ind ] ) # go through the groups and normalize them to the median for # each group for tgind , tg in enumerate ( timegroups ) : finite_ind = np . isfinite ( mags [ tg ] ) # find this timegroup's median mag and normalize the mags in # it to this median group_median = np . median ( ( mags [ tg ] ) [ finite_ind ] ) if magsarefluxes : mags [ tg ] = mags [ tg ] / group_median else : mags [ tg ] = mags [ tg ] - group_median if debugmode : LOGDEBUG ( 'group %s: elems %s, ' 'finite elems %s, median mag %s' % ( tgind , len ( mags [ tg ] ) , len ( finite_ind ) , group_median ) ) # now that everything is normalized to 0.0, add the global median # offset back to all the mags and write the result back to the dict if isinstance ( normto , str ) and normto == 'globalmedian' : if magsarefluxes : mags = mags * global_mag_median else : mags = mags + global_mag_median # if the normto is a float, add everything to that float and return elif isinstance ( normto , float ) : if magsarefluxes : mags = mags * normto else : mags = mags + normto # anything else just returns the normalized mags as usual return times , mags else : LOGERROR ( 'measurements are all nan!' ) return None , None
8472	def setup ( ) : # # Check if dir is writable # if not os.access(AtomShieldsScanner.HOME, os.W_OK): # AtomShieldsScanner.HOME = os.path.expanduser("~/.atomshields") # AtomShieldsScanner.CHECKERS_DIR = os.path.join(AtomShieldsScanner.HOME, "checkers") # AtomShieldsScanner.REPORTS_DIR = os.path.join(AtomShieldsScanner.HOME, "reports") if not os . path . isdir ( AtomShieldsScanner . CHECKERS_DIR ) : os . makedirs ( AtomShieldsScanner . CHECKERS_DIR ) if not os . path . isdir ( AtomShieldsScanner . REPORTS_DIR ) : os . makedirs ( AtomShieldsScanner . REPORTS_DIR ) # Copy all checkers for f in AtomShieldsScanner . _getFiles ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , "checkers" ) , "*.py" ) : AtomShieldsScanner . installChecker ( f ) # Copy all reports for f in AtomShieldsScanner . _getFiles ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , "reports" ) , "*.py" ) : AtomShieldsScanner . installReport ( f ) AtomShieldsScanner . _executeMassiveMethod ( path = AtomShieldsScanner . CHECKERS_DIR , method = "install" , args = { } ) config_dir = os . path . dirname ( AtomShieldsScanner . CONFIG_PATH ) if not os . path . isdir ( config_dir ) : os . makedirs ( config_dir )
13147	def shrink_indexes_in_place ( self , triples ) : _ent_roots = self . UnionFind ( self . _ent_id ) _rel_roots = self . UnionFind ( self . _rel_id ) for t in triples : _ent_roots . add ( t . head ) _ent_roots . add ( t . tail ) _rel_roots . add ( t . relation ) for i , t in enumerate ( triples ) : h = _ent_roots . find ( t . head ) r = _rel_roots . find ( t . relation ) t = _ent_roots . find ( t . tail ) triples [ i ] = kgedata . TripleIndex ( h , r , t ) ents = bidict ( ) available_ent_idx = 0 for previous_idx , ent_exist in enumerate ( _ent_roots . roots ( ) ) : if not ent_exist : self . _ents . inverse . pop ( previous_idx ) else : ents [ self . _ents . inverse [ previous_idx ] ] = available_ent_idx available_ent_idx += 1 rels = bidict ( ) available_rel_idx = 0 for previous_idx , rel_exist in enumerate ( _rel_roots . roots ( ) ) : if not rel_exist : self . _rels . inverse . pop ( previous_idx ) else : rels [ self . _rels . inverse [ previous_idx ] ] = available_rel_idx available_rel_idx += 1 self . _ents = ents self . _rels = rels self . _ent_id = available_ent_idx self . _rel_id = available_rel_idx
13029	def exploit_single ( self , ip , operating_system ) : result = None if "Windows Server 2008" in operating_system or "Windows 7" in operating_system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue_exploit7.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final_combined.bin' ) , "12" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) elif "Windows Server 2012" in operating_system or "Windows 10" in operating_system or "Windows 8.1" in operating_system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue_exploit8.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final_combined.bin' ) , "12" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) else : return [ "System target could not be automatically identified" ] return result . stdout . decode ( 'utf-8' ) . split ( '\n' )
6355	def _language_index_from_code ( self , code , name_mode ) : if code < 1 or code > sum ( _LANG_DICT [ _ ] for _ in BMDATA [ name_mode ] [ 'languages' ] ) : # code out of range return L_ANY if ( code & ( code - 1 ) ) != 0 : # choice was more than one language; use any return L_ANY return code
2681	def create_function ( cfg , path_to_zip_file , use_s3 = False , s3_file = None ) : print ( 'Creating your new Lambda function' ) byte_stream = read ( path_to_zip_file , binary_file = True ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) account_id = get_account_id ( profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' , ) , ) role = get_role_name ( cfg . get ( 'region' ) , account_id , cfg . get ( 'role' , 'lambda_basic_execution' ) , ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) # Do we prefer development variable over config? buck_name = ( os . environ . get ( 'S3_BUCKET_NAME' ) or cfg . get ( 'bucket_name' ) ) func_name = ( os . environ . get ( 'LAMBDA_FUNCTION_NAME' ) or cfg . get ( 'function_name' ) ) print ( 'Creating lambda function with name: {}' . format ( func_name ) ) if use_s3 : kwargs = { 'FunctionName' : func_name , 'Runtime' : cfg . get ( 'runtime' , 'python2.7' ) , 'Role' : role , 'Handler' : cfg . get ( 'handler' ) , 'Code' : { 'S3Bucket' : '{}' . format ( buck_name ) , 'S3Key' : '{}' . format ( s3_file ) , } , 'Description' : cfg . get ( 'description' , '' ) , 'Timeout' : cfg . get ( 'timeout' , 15 ) , 'MemorySize' : cfg . get ( 'memory_size' , 512 ) , 'VpcConfig' : { 'SubnetIds' : cfg . get ( 'subnet_ids' , [ ] ) , 'SecurityGroupIds' : cfg . get ( 'security_group_ids' , [ ] ) , } , 'Publish' : True , } else : kwargs = { 'FunctionName' : func_name , 'Runtime' : cfg . get ( 'runtime' , 'python2.7' ) , 'Role' : role , 'Handler' : cfg . get ( 'handler' ) , 'Code' : { 'ZipFile' : byte_stream } , 'Description' : cfg . get ( 'description' , '' ) , 'Timeout' : cfg . get ( 'timeout' , 15 ) , 'MemorySize' : cfg . get ( 'memory_size' , 512 ) , 'VpcConfig' : { 'SubnetIds' : cfg . get ( 'subnet_ids' , [ ] ) , 'SecurityGroupIds' : cfg . get ( 'security_group_ids' , [ ] ) , } , 'Publish' : True , } if 'tags' in cfg : kwargs . update ( Tags = { key : str ( value ) for key , value in cfg . get ( 'tags' ) . items ( ) } ) if 'environment_variables' in cfg : kwargs . update ( Environment = { 'Variables' : { key : get_environment_variable_value ( value ) for key , value in cfg . get ( 'environment_variables' ) . items ( ) } , } , ) client . create_function ( * * kwargs ) concurrency = get_concurrency ( cfg ) if concurrency > 0 : client . put_function_concurrency ( FunctionName = func_name , ReservedConcurrentExecutions = concurrency )
1599	def pipe ( prev_proc , to_cmd ) : stdin = None if prev_proc is None else prev_proc . stdout process = subprocess . Popen ( to_cmd , stdout = subprocess . PIPE , stdin = stdin ) if prev_proc is not None : prev_proc . stdout . close ( ) # Allow prev_proc to receive a SIGPIPE return process
13654	def Integer ( name , base = 10 , encoding = None ) : def _match ( request , value ) : return name , query . Integer ( value , base = base , encoding = contentEncoding ( request . requestHeaders , encoding ) ) return _match
9559	def _apply_each_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'each' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
5192	def send_select_and_operate_command ( self , command , index , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command , index , callback , config )
9445	def group_call ( self , call_params ) : path = '/' + self . api_version + '/GroupCall/' method = 'POST' return self . request ( path , method , call_params )
4369	def send ( self , message , json = False , callback = None ) : pkt = dict ( type = "message" , data = message , endpoint = self . ns_name ) if json : pkt [ 'type' ] = "json" if callback : # By passing ack=True, we use the old behavior of being returned # an 'ack' packet, automatically triggered by the client-side # with no user-code being run. The emit() version of the # callback is more useful I think :) So migrate your code. pkt [ 'ack' ] = True pkt [ 'id' ] = msgid = self . socket . _get_next_msgid ( ) self . socket . _save_ack_callback ( msgid , callback ) self . socket . send_packet ( pkt )
6254	def root_path ( ) : module_dir = os . path . dirname ( globals ( ) [ '__file__' ] ) return os . path . dirname ( os . path . dirname ( module_dir ) )
8426	def hue_pal ( h = .01 , l = .6 , s = .65 , color_space = 'hls' ) : if not all ( [ 0 <= val <= 1 for val in ( h , l , s ) ] ) : msg = ( "hue_pal expects values to be between 0 and 1. " " I got h={}, l={}, s={}" . format ( h , l , s ) ) raise ValueError ( msg ) if color_space not in ( 'hls' , 'husl' ) : msg = "color_space should be one of ['hls', 'husl']" raise ValueError ( msg ) name = '{}_palette' . format ( color_space ) palette = globals ( ) [ name ] def _hue_pal ( n ) : colors = palette ( n , h = h , l = l , s = s ) return [ mcolors . rgb2hex ( c ) for c in colors ] return _hue_pal
509	def stripUnlearnedColumns ( self , activeArray ) : neverLearned = numpy . where ( self . _activeDutyCycles == 0 ) [ 0 ] activeArray [ neverLearned ] = 0
8447	def _in_git_repo ( ) : ret = temple . utils . shell ( 'git rev-parse' , stderr = subprocess . DEVNULL , check = False ) return ret . returncode == 0
808	def handleLogOutput ( self , output ) : #raise Exception('MULTI-LINE DUMMY\nMULTI-LINE DUMMY') if self . _tapFileOut is not None : for k in range ( len ( output ) ) : print >> self . _tapFileOut , output [ k ] , print >> self . _tapFileOut
3366	def _process_flux_dataframe ( flux_dataframe , fva , threshold , floatfmt ) : abs_flux = flux_dataframe [ 'flux' ] . abs ( ) flux_threshold = threshold * abs_flux . max ( ) # Drop unused boundary fluxes if fva is None : flux_dataframe = flux_dataframe . loc [ abs_flux >= flux_threshold , : ] . copy ( ) else : flux_dataframe = flux_dataframe . loc [ ( abs_flux >= flux_threshold ) | ( flux_dataframe [ 'fmin' ] . abs ( ) >= flux_threshold ) | ( flux_dataframe [ 'fmax' ] . abs ( ) >= flux_threshold ) , : ] . copy ( ) # Why set to zero? If included show true value? # flux_dataframe.loc[ # flux_dataframe['flux'].abs() < flux_threshold, 'flux'] = 0 # Make all fluxes positive if fva is None : flux_dataframe [ 'is_input' ] = ( flux_dataframe [ 'flux' ] >= 0 ) flux_dataframe [ 'flux' ] = flux_dataframe [ 'flux' ] . abs ( ) else : def get_direction ( flux , fmin , fmax ) : """ decide whether or not to reverse a flux to make it positive """ if flux < 0 : return - 1 elif flux > 0 : return 1 elif ( fmax > 0 ) & ( fmin <= 0 ) : return 1 elif ( fmax < 0 ) & ( fmin >= 0 ) : return - 1 elif ( ( fmax + fmin ) / 2 ) < 0 : return - 1 else : return 1 sign = flux_dataframe . apply ( lambda x : get_direction ( x . flux , x . fmin , x . fmax ) , 1 ) flux_dataframe [ 'is_input' ] = sign == 1 flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] = flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] . multiply ( sign , 0 ) . astype ( 'float' ) . round ( 6 ) flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] = flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] . applymap ( lambda x : x if abs ( x ) > 1E-6 else 0 ) if fva is not None : flux_dataframe [ 'fva_fmt' ] = flux_dataframe . apply ( lambda x : ( "[{0.fmin:" + floatfmt + "}, {0.fmax:" + floatfmt + "}]" ) . format ( x ) , 1 ) flux_dataframe = flux_dataframe . sort_values ( by = [ 'flux' , 'fmax' , 'fmin' , 'id' ] , ascending = [ False , False , False , True ] ) else : flux_dataframe = flux_dataframe . sort_values ( by = [ 'flux' , 'id' ] , ascending = [ False , True ] ) return flux_dataframe
2249	def memoize_property ( fget ) : # Unwrap any existing property decorator while hasattr ( fget , 'fget' ) : fget = fget . fget attr_name = '_' + fget . __name__ @ functools . wraps ( fget ) def fget_memoized ( self ) : if not hasattr ( self , attr_name ) : setattr ( self , attr_name , fget ( self ) ) return getattr ( self , attr_name ) return property ( fget_memoized )
4925	def get_missing_params_message ( self , parameter_state ) : params = ', ' . join ( name for name , present in parameter_state if not present ) return self . MISSING_REQUIRED_PARAMS_MSG . format ( params )
6400	def stem ( self , word ) : wlen = len ( word ) - 2 if wlen > 2 and word [ - 1 ] == 's' : word = word [ : - 1 ] wlen -= 1 _endings = { 5 : { 'elser' , 'heten' } , 4 : { 'arne' , 'erna' , 'ande' , 'else' , 'aste' , 'orna' , 'aren' } , 3 : { 'are' , 'ast' , 'het' } , 2 : { 'ar' , 'er' , 'or' , 'en' , 'at' , 'te' , 'et' } , 1 : { 'a' , 'e' , 'n' , 't' } , } for end_len in range ( 5 , 0 , - 1 ) : if wlen > end_len and word [ - end_len : ] in _endings [ end_len ] : return word [ : - end_len ] return word
11448	def login_data_valid ( self ) : login_working = False try : with self . _login ( requests . Session ( ) ) as sess : sess . get ( self . _logout_url ) except self . LoginError : pass else : login_working = True return login_working
2354	def find_element ( self , strategy , locator ) : return self . driver_adapter . find_element ( strategy , locator , root = self . root )
1905	def strcmp ( state , s1 , s2 ) : cpu = state . cpu if issymbolic ( s1 ) : raise ConcretizeArgument ( state . cpu , 1 ) if issymbolic ( s2 ) : raise ConcretizeArgument ( state . cpu , 2 ) s1_zero_idx = _find_zero ( cpu , state . constraints , s1 ) s2_zero_idx = _find_zero ( cpu , state . constraints , s2 ) min_zero_idx = min ( s1_zero_idx , s2_zero_idx ) ret = None for offset in range ( min_zero_idx , - 1 , - 1 ) : s1char = ZEXTEND ( cpu . read_int ( s1 + offset , 8 ) , cpu . address_bit_size ) s2char = ZEXTEND ( cpu . read_int ( s2 + offset , 8 ) , cpu . address_bit_size ) if issymbolic ( s1char ) or issymbolic ( s2char ) : if ret is None or ( not issymbolic ( ret ) and ret == 0 ) : ret = s1char - s2char else : ret = ITEBV ( cpu . address_bit_size , s1char != s2char , s1char - s2char , ret ) else : if s1char != s2char : ret = s1char - s2char elif ret is None : ret = 0 return ret
4874	def validate_lms_user_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) try : # Ensure the given user is associated with the enterprise. return models . EnterpriseCustomerUser . objects . get ( user_id = value , enterprise_customer = enterprise_customer ) except models . EnterpriseCustomerUser . DoesNotExist : pass return None
3061	def positional ( max_positional_args ) : def positional_decorator ( wrapped ) : @ functools . wraps ( wrapped ) def positional_wrapper ( * args , * * kwargs ) : if len ( args ) > max_positional_args : plural_s = '' if max_positional_args != 1 : plural_s = 's' message = ( '{function}() takes at most {args_max} positional ' 'argument{plural} ({args_given} given)' . format ( function = wrapped . __name__ , args_max = max_positional_args , args_given = len ( args ) , plural = plural_s ) ) if positional_parameters_enforcement == POSITIONAL_EXCEPTION : raise TypeError ( message ) elif positional_parameters_enforcement == POSITIONAL_WARNING : logger . warning ( message ) return wrapped ( * args , * * kwargs ) return positional_wrapper if isinstance ( max_positional_args , six . integer_types ) : return positional_decorator else : args , _ , _ , defaults = inspect . getargspec ( max_positional_args ) return positional ( len ( args ) - len ( defaults ) ) ( max_positional_args )
10781	def feature_guess ( st , rad , invert = 'guess' , minmass = None , use_tp = False , trim_edge = False , * * kwargs ) : # FIXME does not use the **kwargs, but needs b/c called with wrong kwargs if invert == 'guess' : invert = guess_invert ( st ) if invert : im = 1 - st . residuals else : im = st . residuals return _feature_guess ( im , rad , minmass = minmass , use_tp = use_tp , trim_edge = trim_edge )
125	def Positive ( other_param , mode = "invert" , reroll_count_max = 2 ) : return ForceSign ( other_param = other_param , positive = True , mode = mode , reroll_count_max = reroll_count_max )
2522	def p_file_chk_sum ( self , f_term , predicate ) : try : for _s , _p , checksum in self . graph . triples ( ( f_term , predicate , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : self . builder . set_file_chksum ( self . doc , six . text_type ( value ) ) except CardinalityError : self . more_than_one_error ( 'File checksum' )
1845	def JP ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . PF , target . read ( ) , cpu . PC )
8560	def get_lan_members ( self , datacenter_id , lan_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans/%s/nics?depth=%s' % ( datacenter_id , lan_id , str ( depth ) ) ) return response
9633	def render_to_message ( self , extra_context = None , * args , * * kwargs ) : message = super ( TemplatedHTMLEmailMessageView , self ) . render_to_message ( extra_context , * args , * * kwargs ) if extra_context is None : extra_context = { } context = self . get_context_data ( * * extra_context ) content = self . render_html_body ( context ) message . attach_alternative ( content , mimetype = 'text/html' ) return message
1876	def MOVQ ( cpu , dest , src ) : # mmx to mmx or mmx to mem if dest . size == src . size and dest . size == 64 : dest . write ( src . read ( ) ) # two xmm regs elif dest . size == src . size and dest . size == 128 : src_lo = Operators . EXTRACT ( src . read ( ) , 0 , 64 ) dest . write ( Operators . ZEXTEND ( src_lo , 128 ) ) # mem to xmm elif dest . size == 128 and src . size == 64 : dest . write ( Operators . ZEXTEND ( src . read ( ) , dest . size ) ) # xmm to mem elif dest . size == 64 and src . size == 128 : dest . write ( Operators . EXTRACT ( src . read ( ) , 0 , dest . size ) ) else : msg = 'Invalid size in MOVQ' logger . error ( msg ) raise Exception ( msg )
13393	def notify_client ( notifier_uri , client_id , status_code , message = None ) : payload = { "client_id" : client_id , "result" : { "response" : { "status_code" : status_code } } } if message is not None : payload [ "result" ] [ "response" ] [ "message" ] = message response = requests . post ( notifier_uri , json = payload ) if response . status_code != 201 : sys . stderr . write ( "failed to notify client: {}\n" . format ( payload ) ) sys . stderr . flush ( )
1019	def getSimplePatterns ( numOnes , numPatterns , patternOverlap = 0 ) : assert ( patternOverlap < numOnes ) # How many new bits are introduced in each successive pattern? numNewBitsInEachPattern = numOnes - patternOverlap numCols = numNewBitsInEachPattern * numPatterns + patternOverlap p = [ ] for i in xrange ( numPatterns ) : x = numpy . zeros ( numCols , dtype = 'float32' ) startBit = i * numNewBitsInEachPattern nextStartBit = startBit + numOnes x [ startBit : nextStartBit ] = 1 p . append ( x ) return p
6217	def prepare_attrib_mapping ( self , primitive ) : buffer_info = [ ] for name , accessor in primitive . attributes . items ( ) : info = VBOInfo ( * accessor . info ( ) ) info . attributes . append ( ( name , info . components ) ) if buffer_info and buffer_info [ - 1 ] . buffer_view == info . buffer_view : if buffer_info [ - 1 ] . interleaves ( info ) : buffer_info [ - 1 ] . merge ( info ) continue buffer_info . append ( info ) return buffer_info
5118	def get_queue_data ( self , queues = None , edge = None , edge_type = None , return_header = False ) : queues = _get_queues ( self . g , queues , edge , edge_type ) data = np . zeros ( ( 0 , 6 ) ) for q in queues : dat = self . edge2queue [ q ] . fetch_data ( ) if len ( dat ) > 0 : data = np . vstack ( ( data , dat ) ) if return_header : return data , 'arrival,service,departure,num_queued,num_total,q_id' return data
1403	def getTopologyInfo ( self , topologyName , cluster , role , environ ) : # Iterate over the values to filter the desired topology. for ( topology_name , _ ) , topologyInfo in self . topologyInfos . items ( ) : executionState = topologyInfo [ "execution_state" ] if ( topologyName == topology_name and cluster == executionState [ "cluster" ] and environ == executionState [ "environ" ] ) : # If role is specified, first try to match "role" field. If "role" field # does not exist, try to match "submission_user" field. if not role or executionState . get ( "role" ) == role : return topologyInfo if role is not None : Log . info ( "Could not find topology info for topology: %s," "cluster: %s, role: %s, and environ: %s" , topologyName , cluster , role , environ ) else : Log . info ( "Could not find topology info for topology: %s," "cluster: %s and environ: %s" , topologyName , cluster , environ ) raise Exception ( "No topology found" )
8448	def _has_branch ( branch ) : ret = temple . utils . shell ( 'git rev-parse --verify {}' . format ( branch ) , stderr = subprocess . DEVNULL , stdout = subprocess . DEVNULL , check = False ) return ret . returncode == 0
8254	def _context ( self ) : tags1 = None for clr in self : overlap = [ ] if clr . is_black : name = "black" elif clr . is_white : name = "white" elif clr . is_grey : name = "grey" else : name = clr . nearest_hue ( primary = True ) if name == "orange" and clr . brightness < 0.6 : name = "brown" tags2 = context [ name ] if tags1 is None : tags1 = tags2 else : for tag in tags2 : if tag in tags1 : if tag not in overlap : overlap . append ( tag ) tags1 = overlap overlap . sort ( ) return overlap
9901	def _updateType ( self ) : data = self . _data ( ) # Change type if needed if isinstance ( data , dict ) and isinstance ( self , ListFile ) : self . __class__ = DictFile elif isinstance ( data , list ) and isinstance ( self , DictFile ) : self . __class__ = ListFile
7809	def from_ssl_socket ( cls , ssl_socket ) : try : data = ssl_socket . getpeercert ( True ) except AttributeError : # PyPy doesn't have .getpeercert data = None if not data : logger . debug ( "No certificate infromation" ) return cls ( ) result = cls . from_der_data ( data ) result . validated = bool ( ssl_socket . getpeercert ( ) ) return result
12398	def gen_methods ( self , * args , * * kwargs ) : token = args [ 0 ] inst = self . inst prefix = self . _method_prefix for method_key in self . gen_method_keys ( * args , * * kwargs ) : method = getattr ( inst , prefix + method_key , None ) if method is not None : yield method # Fall back to built-in types, then types, then collections. typename = type ( token ) . __name__ yield from self . check_basetype ( token , typename , self . builtins . get ( typename ) ) for basetype_name in self . interp_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . types , basetype_name , None ) ) for basetype_name in self . abc_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . collections , basetype_name , None ) ) # Try the generic handler. yield from self . gen_generic ( )
9796	def get ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) try : response = PolyaxonClient ( ) . experiment_group . get_experiment_group ( user , project_name , _group ) cache . cache ( config_manager = GroupManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_group_details ( response )
6307	def load_effects_classes ( self ) : self . effect_classes = [ ] for _ , cls in inspect . getmembers ( self . effect_module ) : if inspect . isclass ( cls ) : if cls == Effect : continue if issubclass ( cls , Effect ) : self . effect_classes . append ( cls ) self . effect_class_map [ cls . __name__ ] = cls cls . _name = "{}.{}" . format ( self . effect_module_name , cls . __name__ )
10556	def update_helping_material ( helpingmaterial ) : try : helpingmaterial_id = helpingmaterial . id helpingmaterial = _forbidden_attributes ( helpingmaterial ) res = _pybossa_req ( 'put' , 'helpingmaterial' , helpingmaterial_id , payload = helpingmaterial . data ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : # pragma: no cover raise
10197	def _handle_request ( self , scheme , netloc , path , headers , body = None , method = "GET" ) : backend_url = "{}://{}{}" . format ( scheme , netloc , path ) try : response = self . http_request . request ( backend_url , method = method , body = body , headers = dict ( headers ) ) self . _return_response ( response ) except Exception as e : body = "Invalid response from backend: '{}' Server might be busy" . format ( e . message ) logging . debug ( body ) self . send_error ( httplib . SERVICE_UNAVAILABLE , body )
11572	def output_entire_buffer ( self ) : green = 0 red = 0 for row in range ( 0 , 8 ) : for col in range ( 0 , 8 ) : if self . display_buffer [ row ] [ col ] == self . LED_GREEN : green |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_RED : red |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_YELLOW : green |= 1 << col red |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_OFF : green &= ~ ( 1 << col ) red &= ~ ( 1 << col ) self . firmata . i2c_write ( 0x70 , row * 2 , 0 , green ) self . firmata . i2c_write ( 0x70 , row * 2 + 1 , 0 , red )
9731	def get_force_single ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . plate_count ) : component_position , plate = QRTPacket . _get_exact ( RTForcePlateSingle , data , component_position ) component_position , force = QRTPacket . _get_exact ( RTForce , data , component_position ) append_components ( ( plate , force ) ) return components
12954	def _rem_id_from_keys ( self , pk , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _get_ids_key ( ) , pk )
3855	async def _sync_all_conversations ( client ) : conv_states = [ ] sync_timestamp = None request = hangouts_pb2 . SyncRecentConversationsRequest ( request_header = client . get_request_header ( ) , max_conversations = CONVERSATIONS_PER_REQUEST , max_events_per_conversation = 1 , sync_filter = [ hangouts_pb2 . SYNC_FILTER_INBOX , hangouts_pb2 . SYNC_FILTER_ARCHIVED , ] ) for _ in range ( MAX_CONVERSATION_PAGES ) : logger . info ( 'Requesting conversations page %s' , request . last_event_timestamp ) response = await client . sync_recent_conversations ( request ) conv_states = list ( response . conversation_state ) + conv_states sync_timestamp = parsers . from_timestamp ( # SyncRecentConversations seems to return a sync_timestamp 4 # minutes before the present. To prevent SyncAllNewEvents later # breaking requesting events older than what we already have, use # current_server_time instead. response . response_header . current_server_time ) if response . continuation_end_timestamp == 0 : logger . info ( 'Reached final conversations page' ) break else : request . last_event_timestamp = response . continuation_end_timestamp else : logger . warning ( 'Exceeded maximum number of conversation pages' ) logger . info ( 'Synced %s total conversations' , len ( conv_states ) ) return conv_states , sync_timestamp
4947	def send_course_completion_statement ( lrs_configuration , user , course_overview , course_grade ) : user_details = LearnerInfoSerializer ( user ) course_details = CourseInfoSerializer ( course_overview ) statement = LearnerCourseCompletionStatement ( user , course_overview , user_details . data , course_details . data , course_grade , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
9608	def format_map ( self , format_string , mapping ) : return self . vformat ( format_string , args = None , kwargs = mapping )
11037	def maybe_key ( pem_path ) : acme_key_file = pem_path . child ( u'client.key' ) if acme_key_file . exists ( ) : key = _load_pem_private_key_bytes ( acme_key_file . getContent ( ) ) else : key = generate_private_key ( u'rsa' ) acme_key_file . setContent ( _dump_pem_private_key_bytes ( key ) ) return succeed ( JWKRSA ( key = key ) )
12615	def count ( self , table_name , sample ) : return len ( list ( search_sample ( table = self . table ( table_name ) , sample = sample ) ) )
9516	def subseq ( self , start , end ) : return Fastq ( self . id , self . seq [ start : end ] , self . qual [ start : end ] )
2699	def render_ranks ( graph , ranks , dot_file = "graph.dot" ) : if dot_file : write_dot ( graph , ranks , path = dot_file )
13719	def create_tree ( endpoints ) : tree = { } for method , url , doc in endpoints : path = [ p for p in url . strip ( '/' ) . split ( '/' ) ] here = tree # First element (API Version). version = path [ 0 ] here . setdefault ( version , { } ) here = here [ version ] # The rest of elements of the URL. for p in path [ 1 : ] : part = _camelcase_to_underscore ( p ) here . setdefault ( part , { } ) here = here [ part ] # Allowed HTTP methods. if not 'METHODS' in here : here [ 'METHODS' ] = [ [ method , doc ] ] else : if not method in here [ 'METHODS' ] : here [ 'METHODS' ] . append ( [ method , doc ] ) return tree
7006	def plot_training_results ( classifier , classlabels , outfile ) : if isinstance ( classifier , str ) and os . path . exists ( classifier ) : with open ( classifier , 'rb' ) as infd : clfdict = pickle . load ( infd ) elif isinstance ( classifier , dict ) : clfdict = classifier else : LOGERROR ( "can't figure out the input classifier arg" ) return None confmatrix = clfdict [ 'best_confmatrix' ] overall_feature_importances = clfdict [ 'best_classifier' ] . feature_importances_ feature_importances_per_tree = np . array ( [ tree . feature_importances_ for tree in clfdict [ 'best_classifier' ] . estimators_ ] ) stdev_feature_importances = np . std ( feature_importances_per_tree , axis = 0 ) feature_names = np . array ( clfdict [ 'feature_names' ] ) plt . figure ( figsize = ( 6.4 * 3.0 , 4.8 ) ) # confusion matrix plt . subplot ( 121 ) classes = np . array ( classlabels ) plt . imshow ( confmatrix , interpolation = 'nearest' , cmap = plt . cm . Blues ) tick_marks = np . arange ( len ( classes ) ) plt . xticks ( tick_marks , classes ) plt . yticks ( tick_marks , classes ) plt . title ( 'evaluation set confusion matrix' ) plt . ylabel ( 'predicted class' ) plt . xlabel ( 'actual class' ) thresh = confmatrix . max ( ) / 2. for i , j in itertools . product ( range ( confmatrix . shape [ 0 ] ) , range ( confmatrix . shape [ 1 ] ) ) : plt . text ( j , i , confmatrix [ i , j ] , horizontalalignment = "center" , color = "white" if confmatrix [ i , j ] > thresh else "black" ) # feature importances plt . subplot ( 122 ) features = np . array ( feature_names ) sorted_ind = np . argsort ( overall_feature_importances ) [ : : - 1 ] features = features [ sorted_ind ] feature_names = feature_names [ sorted_ind ] overall_feature_importances = overall_feature_importances [ sorted_ind ] stdev_feature_importances = stdev_feature_importances [ sorted_ind ] plt . bar ( np . arange ( 0 , features . size ) , overall_feature_importances , yerr = stdev_feature_importances , width = 0.8 , color = 'grey' ) plt . xticks ( np . arange ( 0 , features . size ) , features , rotation = 90 ) plt . yticks ( [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1.0 ] ) plt . xlim ( - 0.75 , features . size - 1.0 + 0.75 ) plt . ylim ( 0.0 , 0.9 ) plt . ylabel ( 'relative importance' ) plt . title ( 'relative importance of features' ) plt . subplots_adjust ( wspace = 0.1 ) plt . savefig ( outfile , bbox_inches = 'tight' , dpi = 100 ) plt . close ( 'all' ) return outfile
9156	def stroke_linecap ( self , linecap ) : linecap = getattr ( pgmagick . LineCap , "%sCap" % linecap . title ( ) ) linecap = pgmagick . DrawableStrokeLineCap ( linecap ) self . drawer . append ( linecap )
7738	def check_unassigned ( self , data ) : for char in data : for lookup in self . unassigned : if lookup ( char ) : raise StringprepError ( "Unassigned character: {0!r}" . format ( char ) ) return data
11879	def scanProcessForCwd ( pid , searchPortion , isExactMatch = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e cwd = getProcessCwd ( pid ) if not cwd : return None isMatch = False if isExactMatch is True : if searchPortion == cwd : isMatch = True else : if searchPortion . endswith ( '/' ) and searchPortion [ : - 1 ] == cwd : isMatch = True else : if searchPortion in cwd : isMatch = True else : if searchPortion . endswith ( '/' ) and searchPortion [ : - 1 ] in cwd : isMatch = True if not isMatch : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'cwd' : cwd , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
11350	def merge_kwargs ( self , kwargs ) : if kwargs : self . parser_kwargs . update ( kwargs ) #self.parser_kwargs['dest'] = self.name self . parser_kwargs . setdefault ( 'dest' , self . name ) # special handling of any passed in values if 'default' in kwargs : # NOTE -- this doesn't use .set_default() because that is meant to # parse from the function definition so it actually has different syntax # than what the .set_default() method does. eg, @arg("--foo", default=[1, 2]) means # that the default value should be an array with 1 and 2 in it, where main(foo=[1, 2]) # means foo should be constrained to choices=[1, 2] self . parser_kwargs [ "default" ] = kwargs [ "default" ] self . parser_kwargs [ "required" ] = False elif 'action' in kwargs : if kwargs [ 'action' ] in set ( [ 'store_false' , 'store_true' ] ) : self . parser_kwargs [ 'required' ] = False elif kwargs [ 'action' ] in set ( [ 'version' ] ) : self . parser_kwargs . pop ( 'required' , False ) else : self . parser_kwargs . setdefault ( "required" , True )
9206	def get_prefix ( multicodec ) : try : prefix = varint . encode ( NAME_TABLE [ multicodec ] ) except KeyError : raise ValueError ( '{} multicodec is not supported.' . format ( multicodec ) ) return prefix
12935	def _parse_allele_data ( self ) : # Get allele frequencies if they exist. pref_freq , frequencies = self . _parse_frequencies ( ) info_clnvar_single_tags = [ 'ALLELEID' , 'CLNSIG' , 'CLNHGVS' ] cln_data = { x . lower ( ) : self . info [ x ] if x in self . info else None for x in info_clnvar_single_tags } cln_data . update ( { 'clndisdb' : [ x . split ( ',' ) for x in self . info [ 'CLNDISDB' ] . split ( '|' ) ] if 'CLNDISDB' in self . info else [ ] } ) cln_data . update ( { 'clndn' : self . info [ 'CLNDN' ] . split ( '|' ) if 'CLNDN' in self . info else [ ] } ) cln_data . update ( { 'clnvi' : self . info [ 'CLNVI' ] . split ( ',' ) if 'CLNVI' in self . info else [ ] } ) try : sequence = self . alt_alleles [ 0 ] except IndexError : sequence = self . ref_allele allele = ClinVarAllele ( frequency = pref_freq , sequence = sequence , * * cln_data ) # A few ClinVar variants are only reported as a combination with # other variants, and no single-variant effect is proposed. Skip these. if not cln_data [ 'clnsig' ] : return [ ] return [ allele ]
1885	def solve_buffer ( self , addr , nbytes , constrain = False ) : buffer = self . cpu . read_bytes ( addr , nbytes ) result = [ ] with self . _constraints as temp_cs : cs_to_use = self . constraints if constrain else temp_cs for c in buffer : result . append ( self . _solver . get_value ( cs_to_use , c ) ) cs_to_use . add ( c == result [ - 1 ] ) return result
11018	def balance ( ctx ) : backend = plugins_registry . get_backends_by_class ( ZebraBackend ) [ 0 ] timesheet_collection = get_timesheet_collection_for_context ( ctx , None ) hours_to_be_pushed = timesheet_collection . get_hours ( pushed = False , ignored = False , unmapped = False ) today = datetime . date . today ( ) user_info = backend . get_user_info ( ) timesheets = backend . get_timesheets ( get_first_dow ( today ) , get_last_dow ( today ) ) total_duration = sum ( [ float ( timesheet [ 'time' ] ) for timesheet in timesheets ] ) vacation = hours_to_days ( user_info [ 'vacation' ] [ 'difference' ] ) vacation_balance = '{} days, {:.2f} hours' . format ( * vacation ) hours_balance = user_info [ 'hours' ] [ 'hours' ] [ 'balance' ] click . echo ( "Hours balance: {}" . format ( signed_number ( hours_balance ) ) ) click . echo ( "Hours balance after push: {}" . format ( signed_number ( hours_balance + hours_to_be_pushed ) ) ) click . echo ( "Hours done this week: {:.2f}" . format ( total_duration ) ) click . echo ( "Vacation left: {}" . format ( vacation_balance ) )
13503	def create ( self , server ) : return server . post ( 'challenge_admin' , self . as_payload ( ) , replacements = { 'slug' : self . slug } )
1887	def emulate ( self , instruction ) : # The emulation might restart if Unicorn needs to bring in a memory map # or bring a value from Manticore state. while True : self . reset ( ) # Establish Manticore state, potentially from past emulation # attempts for base in self . _should_be_mapped : size , perms = self . _should_be_mapped [ base ] self . _emu . mem_map ( base , size , perms ) for address , values in self . _should_be_written . items ( ) : for offset , byte in enumerate ( values , start = address ) : if issymbolic ( byte ) : from . . native . cpu . abstractcpu import ConcretizeMemory raise ConcretizeMemory ( self . _cpu . memory , offset , 8 , "Concretizing for emulation" ) self . _emu . mem_write ( address , b'' . join ( values ) ) # Try emulation self . _should_try_again = False self . _step ( instruction ) if not self . _should_try_again : break
6248	def get_texture ( self , label : str ) -> Union [ moderngl . Texture , moderngl . TextureArray , moderngl . Texture3D , moderngl . TextureCube ] : return self . _project . get_texture ( label )
6223	def look_at ( self , vec = None , pos = None ) : if pos is None : vec = Vector3 ( pos ) if vec is None : raise ValueError ( "vector or pos must be set" ) return self . _gl_look_at ( self . position , vec , self . _up )
743	def writeToFile ( self , f , packed = True ) : # Get capnproto schema from instance schema = self . getSchema ( ) # Construct new message, otherwise refered to as `proto` proto = schema . new_message ( ) # Populate message w/ `write()` instance method self . write ( proto ) # Finally, write to file if packed : proto . write_packed ( f ) else : proto . write ( f )
6954	def make_combined_periodogram ( pflist , outfile , addmethods = False ) : import matplotlib . pyplot as plt for pf in pflist : if pf [ 'method' ] == 'pdm' : plt . plot ( pf [ 'periods' ] , np . max ( pf [ 'lspvals' ] ) / pf [ 'lspvals' ] - 1.0 , label = '%s P=%.5f' % ( pf [ 'method' ] , pf [ 'bestperiod' ] ) , alpha = 0.5 ) else : plt . plot ( pf [ 'periods' ] , pf [ 'lspvals' ] / np . max ( pf [ 'lspvals' ] ) , label = '%s P=%.5f' % ( pf [ 'method' ] , pf [ 'bestperiod' ] ) , alpha = 0.5 ) plt . xlabel ( 'period [days]' ) plt . ylabel ( 'normalized periodogram power' ) plt . xscale ( 'log' ) plt . legend ( ) plt . tight_layout ( ) plt . savefig ( outfile ) plt . close ( 'all' ) return outfile
3760	def mass_fractions ( self ) : things = dict ( ) for zi , atoms in zip ( self . zs , self . atomss ) : for atom , count in atoms . iteritems ( ) : if atom in things : things [ atom ] += zi * count else : things [ atom ] = zi * count return mass_fractions ( things )
12310	def pull_stream ( self , uri , * * kwargs ) : return self . protocol . execute ( 'pullStream' , uri = uri , * * kwargs )
2505	def get_extr_license_text ( self , extr_lic ) : text_tripples = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'extractedText' ] , None ) ) ) if not text_tripples : self . error = True msg = 'Extracted license must have extractedText property' self . logger . log ( msg ) return if len ( text_tripples ) > 1 : self . more_than_one_error ( 'extracted license text' ) return text_tripple = text_tripples [ 0 ] _s , _p , text = text_tripple return text
13436	def _setup_positions ( self , positions ) : updated_positions = [ ] for i , position in enumerate ( positions ) : ranger = re . search ( r'(?P<start>-?\d*):(?P<end>\d*)' , position ) if ranger : if i > 0 : updated_positions . append ( self . separator ) start = group_val ( ranger . group ( 'start' ) ) end = group_val ( ranger . group ( 'end' ) ) if start and end : updated_positions . extend ( self . _extendrange ( start , end + 1 ) ) # Since the number of positions on a line is unknown, # send input to cause exception that can be caught and call # _cut_range helper function elif ranger . group ( 'start' ) : updated_positions . append ( [ start ] ) else : updated_positions . extend ( self . _extendrange ( 1 , end + 1 ) ) else : updated_positions . append ( positions [ i ] ) try : if int ( position ) and int ( positions [ i + 1 ] ) : updated_positions . append ( self . separator ) except ( ValueError , IndexError ) : pass return updated_positions
3498	def find_carbon_sources ( model ) : try : model . slim_optimize ( error_value = None ) except OptimizationError : return [ ] reactions = model . reactions . get_by_any ( list ( model . medium ) ) reactions_fluxes = [ ( rxn , total_components_flux ( rxn . flux , reaction_elements ( rxn ) , consumption = True ) ) for rxn in reactions ] return [ rxn for rxn , c_flux in reactions_fluxes if c_flux > 0 ]
10256	def get_causal_central_nodes ( graph : BELGraph , func : str ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_central ( graph , node ) }
7902	def set_stream ( self , stream ) : self . jid = stream . me self . stream = stream for r in self . rooms . values ( ) : r . set_stream ( stream )
4848	def _partition_items ( self , channel_metadata_item_map ) : items_to_create = { } items_to_update = { } items_to_delete = { } transmission_map = { } export_content_ids = channel_metadata_item_map . keys ( ) # Get the items that were previously transmitted to the integrated channel. # If we are not transmitting something that was previously transmitted, # we need to delete it from the integrated channel. for transmission in self . _get_transmissions ( ) : transmission_map [ transmission . content_id ] = transmission if transmission . content_id not in export_content_ids : items_to_delete [ transmission . content_id ] = transmission . channel_metadata # Compare what is currently being transmitted to what was transmitted # previously, identifying items that need to be created or updated. for item in channel_metadata_item_map . values ( ) : content_id = item . content_id channel_metadata = item . channel_metadata transmitted_item = transmission_map . get ( content_id , None ) if transmitted_item is not None : if diff ( channel_metadata , transmitted_item . channel_metadata ) : items_to_update [ content_id ] = channel_metadata else : items_to_create [ content_id ] = channel_metadata LOGGER . info ( 'Preparing to transmit creation of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_create ) , self . enterprise_configuration , items_to_create . keys ( ) , ) LOGGER . info ( 'Preparing to transmit update of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_update ) , self . enterprise_configuration , items_to_update . keys ( ) , ) LOGGER . info ( 'Preparing to transmit deletion of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_delete ) , self . enterprise_configuration , items_to_delete . keys ( ) , ) return items_to_create , items_to_update , items_to_delete , transmission_map
7308	def set_list_field ( self , document , form_key , current_key , remaining_key , key_array_digit ) : document_field = document . _fields . get ( current_key ) # Figure out what value the list ought to have # None value for ListFields make mongoengine very un-happy list_value = translate_value ( document_field . field , self . form . cleaned_data [ form_key ] ) if list_value is None or ( not list_value and not bool ( list_value ) ) : return None current_list = getattr ( document , current_key , None ) if isinstance ( document_field . field , EmbeddedDocumentField ) : embedded_list_key = u"{0}_{1}" . format ( current_key , key_array_digit ) # Get the embedded document if it exists, else create it. embedded_list_document = self . embedded_list_docs . get ( embedded_list_key , None ) if embedded_list_document is None : embedded_list_document = document_field . field . document_type_obj ( ) new_key , new_remaining_key_array = trim_field_key ( embedded_list_document , remaining_key ) self . process_document ( embedded_list_document , form_key , new_key ) list_value = embedded_list_document self . embedded_list_docs [ embedded_list_key ] = embedded_list_document if isinstance ( current_list , list ) : # Do not add the same document twice if embedded_list_document not in current_list : current_list . append ( embedded_list_document ) else : setattr ( document , current_key , [ embedded_list_document ] ) elif isinstance ( current_list , list ) : current_list . append ( list_value ) else : setattr ( document , current_key , [ list_value ] )
12063	def getAvgBySweep ( abf , feature , T0 = None , T1 = None ) : if T1 is None : T1 = abf . sweepLength if T0 is None : T0 = 0 data = [ np . empty ( ( 0 ) ) ] * abf . sweeps for AP in cm . dictFlat ( cm . matrixToDicts ( abf . APs ) ) : if T0 < AP [ 'sweepT' ] < T1 : val = AP [ feature ] data [ int ( AP [ 'sweep' ] ) ] = np . concatenate ( ( data [ int ( AP [ 'sweep' ] ) ] , [ val ] ) ) for sweep in range ( abf . sweeps ) : if len ( data [ sweep ] ) > 1 and np . any ( data [ sweep ] ) : data [ sweep ] = np . nanmean ( data [ sweep ] ) elif len ( data [ sweep ] ) == 1 : data [ sweep ] = data [ sweep ] [ 0 ] else : data [ sweep ] = np . nan return data
10385	def get_walks_exhaustive ( graph , node , length ) : if 0 == length : return ( node , ) , return tuple ( ( node , key ) + path for neighbor in graph . edge [ node ] for path in get_walks_exhaustive ( graph , neighbor , length - 1 ) if node not in path for key in graph . edge [ node ] [ neighbor ] )
9413	def _setup_log ( ) : try : handler = logging . StreamHandler ( stream = sys . stdout ) except TypeError : # pragma: no cover handler = logging . StreamHandler ( strm = sys . stdout ) log = get_log ( ) log . addHandler ( handler ) log . setLevel ( logging . INFO ) log . propagate = False
5582	def create ( mapchete_file , process_file , out_format , out_path = None , pyramid_type = None , force = False ) : if os . path . isfile ( process_file ) or os . path . isfile ( mapchete_file ) : if not force : raise IOError ( "file(s) already exists" ) out_path = out_path if out_path else os . path . join ( os . getcwd ( ) , "output" ) # copy file template to target directory process_template = pkg_resources . resource_filename ( "mapchete.static" , "process_template.py" ) process_file = os . path . join ( os . getcwd ( ) , process_file ) copyfile ( process_template , process_file ) # modify and copy mapchete file template to target directory mapchete_template = pkg_resources . resource_filename ( "mapchete.static" , "mapchete_template.mapchete" ) output_options = dict ( format = out_format , path = out_path , * * FORMAT_MANDATORY [ out_format ] ) pyramid_options = { 'grid' : pyramid_type } substitute_elements = { 'process_file' : process_file , 'output' : dump ( { 'output' : output_options } , default_flow_style = False ) , 'pyramid' : dump ( { 'pyramid' : pyramid_options } , default_flow_style = False ) } with open ( mapchete_template , 'r' ) as config_template : config = Template ( config_template . read ( ) ) customized_config = config . substitute ( substitute_elements ) with open ( mapchete_file , 'w' ) as target_config : target_config . write ( customized_config )
320	def get_max_drawdown_underwater ( underwater ) : valley = np . argmin ( underwater ) # end of the period # Find first 0 peak = underwater [ : valley ] [ underwater [ : valley ] == 0 ] . index [ - 1 ] # Find last 0 try : recovery = underwater [ valley : ] [ underwater [ valley : ] == 0 ] . index [ 0 ] except IndexError : recovery = np . nan # drawdown not recovered return peak , valley , recovery
2616	def initialize_boto_client ( self ) : self . session = self . create_session ( ) self . client = self . session . client ( 'ec2' ) self . ec2 = self . session . resource ( 'ec2' ) self . instances = [ ] self . instance_states = { } self . vpc_id = 0 self . sg_id = 0 self . sn_ids = [ ]
5840	def check_predict_status ( self , view_id , predict_request_id ) : failure_message = "Get status on predict failed" bare_response = self . _get_success_json ( self . _get ( 'v1/data_views/' + str ( view_id ) + '/predict/' + str ( predict_request_id ) + '/status' , None , failure_message = failure_message ) ) result = bare_response [ "data" ] # result.update({"message": bare_response["message"]}) return result
7510	def _insert_to_array ( self , start , results ) : qrts , wgts , qsts = results #qrts, wgts = results #print(qrts) with h5py . File ( self . database . output , 'r+' ) as out : chunk = self . _chunksize out [ 'quartets' ] [ start : start + chunk ] = qrts ##out['weights'][start:start+chunk] = wgts ## entered as 0-indexed ! if self . checkpoint . boots : key = "qboots/b{}" . format ( self . checkpoint . boots - 1 ) out [ key ] [ start : start + chunk ] = qsts else : out [ "qstats" ] [ start : start + chunk ] = qsts
11676	def fit ( self , X , y = None ) : self . features_ = as_features ( X , stack = True , bare = True ) # TODO: could precompute things like squared norms if kernel == "rbf". # Probably should add support to sklearn instead of hacking it here. return self
1013	def _getBestMatchingCell ( self , c , activeState , minThreshold ) : # Collect all cells in column c that have at least minThreshold in the most # activated segment bestActivityInCol = minThreshold bestSegIdxInCol = - 1 bestCellInCol = - 1 for i in xrange ( self . cellsPerColumn ) : maxSegActivity = 0 maxSegIdx = 0 for j , s in enumerate ( self . cells [ c ] [ i ] ) : activity = self . _getSegmentActivityLevel ( s , activeState ) if activity > maxSegActivity : maxSegActivity = activity maxSegIdx = j if maxSegActivity >= bestActivityInCol : bestActivityInCol = maxSegActivity bestSegIdxInCol = maxSegIdx bestCellInCol = i if bestCellInCol == - 1 : return ( None , None , None ) else : return ( bestCellInCol , self . cells [ c ] [ bestCellInCol ] [ bestSegIdxInCol ] , bestActivityInCol )
5146	def render ( self , files = True ) : self . validate ( ) # convert NetJSON config to intermediate data structure if self . intermediate_data is None : self . to_intermediate ( ) # support multiple renderers renderers = getattr ( self , 'renderers' , None ) or [ self . renderer ] # convert intermediate data structure to native configuration output = '' for renderer_class in renderers : renderer = renderer_class ( self ) output += renderer . render ( ) # remove reference to renderer instance (not needed anymore) del renderer # are we required to include # additional files? if files : # render additional files files_output = self . _render_files ( ) if files_output : # max 2 new lines output += files_output . replace ( '\n\n\n' , '\n\n' ) # return the configuration return output
2630	def scale_in ( self , blocks = 0 , machines = 0 , strategy = None ) : count = 0 instances = self . client . servers . list ( ) for instance in instances [ 0 : machines ] : print ( "Deleting : " , instance ) instance . delete ( ) count += 1 return count
12976	def compat_convertHashedIndexes ( self , objs , conn = None ) : if conn is None : conn = self . _get_connection ( ) # Do one pipeline per object. # XXX: Maybe we should do the whole thing in one pipeline? fields = [ ] # A list of the indexed fields # Iterate now so we do this once instead of per-object. for indexedField in self . indexedFields : origField = self . fields [ indexedField ] # Check if type supports configurable hashIndex, and if not skip it. if 'hashIndex' not in origField . __class__ . __new__ . __code__ . co_varnames : continue if indexedField . hashIndex is True : hashingField = origField regField = origField . copy ( ) regField . hashIndex = False else : regField = origField # Maybe copy should allow a dict of override params? hashingField = origField . copy ( ) hashingField . hashIndex = True fields . append ( ( origField , regField , hashingField ) ) objDicts = [ obj . asDict ( True , forStorage = True ) for obj in objs ] # Iterate over all values. Remove the possibly stringed index, the possibly hashed index, and then put forth the hashed index. for objDict in objDicts : pipeline = conn . pipeline ( ) pk = objDict [ '_id' ] for origField , regField , hashingField in fields : val = objDict [ indexedField ] # Remove the possibly stringed index self . _rem_id_from_index ( regField , pk , val , pipeline ) # Remove the possibly hashed index self . _rem_id_from_index ( hashingField , pk , val , pipeline ) # Add the new (hashed or unhashed) form. self . _add_id_to_index ( origField , pk , val , pipeline ) # Launch all at once pipeline . execute ( )
9034	def instruction_in_grid ( self , instruction ) : row_position = self . _rows_in_grid [ instruction . row ] . xy x = instruction . index_of_first_consumed_mesh_in_row position = Point ( row_position . x + x , row_position . y ) return InstructionInGrid ( instruction , position )
25	def store_args ( method ) : argspec = inspect . getfullargspec ( method ) defaults = { } if argspec . defaults is not None : defaults = dict ( zip ( argspec . args [ - len ( argspec . defaults ) : ] , argspec . defaults ) ) if argspec . kwonlydefaults is not None : defaults . update ( argspec . kwonlydefaults ) arg_names = argspec . args [ 1 : ] @ functools . wraps ( method ) def wrapper ( * positional_args , * * keyword_args ) : self = positional_args [ 0 ] # Get default arg values args = defaults . copy ( ) # Add provided arg values for name , value in zip ( arg_names , positional_args [ 1 : ] ) : args [ name ] = value args . update ( keyword_args ) self . __dict__ . update ( args ) return method ( * positional_args , * * keyword_args ) return wrapper
10855	def sphere_analytical_gaussian_trim ( dr , a , alpha = 0.2765 , cut = 1.6 ) : m = np . abs ( dr ) <= cut # only compute on the relevant scales rr = dr [ m ] t = - rr / ( alpha * np . sqrt ( 2 ) ) q = 0.5 * ( 1 + erf ( t ) ) - np . sqrt ( 0.5 / np . pi ) * ( alpha / ( rr + a + 1e-10 ) ) * np . exp ( - t * t ) # fill in the grid, inside the interpolation and outside where values are constant ans = 0 * dr ans [ m ] = q ans [ dr > cut ] = 0 ans [ dr < - cut ] = 1 return ans
2338	def dagify_min_edge ( g ) : while not nx . is_directed_acyclic_graph ( g ) : cycle = next ( nx . simple_cycles ( g ) ) scores = [ ] edges = [ ] for i , j in zip ( cycle [ : 1 ] , cycle [ : 1 ] ) : edges . append ( ( i , j ) ) scores . append ( g [ i ] [ j ] [ 'weight' ] ) i , j = edges [ scores . index ( min ( scores ) ) ] gc = deepcopy ( g ) gc . remove_edge ( i , j ) gc . add_edge ( j , i ) if len ( list ( nx . simple_cycles ( gc ) ) ) < len ( list ( nx . simple_cycles ( g ) ) ) : g . add_edge ( j , i , weight = min ( scores ) ) g . remove_edge ( i , j ) return g
9368	def legal_inn ( ) : mask = [ 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 10 ) ] weighted = [ v * mask [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 9 ] = sum ( weighted ) % 11 % 10 return "" . join ( map ( str , inn ) )
3848	def _get_authorization_headers ( sapisid_cookie ) : # It doesn't seem to matter what the url and time are as long as they are # consistent. time_msec = int ( time . time ( ) * 1000 ) auth_string = '{} {} {}' . format ( time_msec , sapisid_cookie , ORIGIN_URL ) auth_hash = hashlib . sha1 ( auth_string . encode ( ) ) . hexdigest ( ) sapisidhash = 'SAPISIDHASH {}_{}' . format ( time_msec , auth_hash ) return { 'authorization' : sapisidhash , 'x-origin' : ORIGIN_URL , 'x-goog-authuser' : '0' , }
3968	def get_compose_dict ( assembled_specs , port_specs ) : compose_dict = _compose_dict_for_nginx ( port_specs ) for app_name in assembled_specs [ 'apps' ] . keys ( ) : compose_dict [ app_name ] = _composed_app_dict ( app_name , assembled_specs , port_specs ) for service_spec in assembled_specs [ 'services' ] . values ( ) : compose_dict [ service_spec . name ] = _composed_service_dict ( service_spec ) return compose_dict
4009	def _start_http_server ( ) : logging . info ( 'Starting HTTP server at {}:{}' . format ( constants . DAEMON_HTTP_BIND_IP , constants . DAEMON_HTTP_BIND_PORT ) ) thread = threading . Thread ( target = http_server . app . run , args = ( constants . DAEMON_HTTP_BIND_IP , constants . DAEMON_HTTP_BIND_PORT ) ) thread . daemon = True thread . start ( )
9542	def add_header_check ( self , code = HEADER_CHECK_FAILED , message = MESSAGES [ HEADER_CHECK_FAILED ] ) : t = code , message self . _header_checks . append ( t )
2401	def gen_length_feats ( self , e_set ) : text = e_set . _text lengths = [ len ( e ) for e in text ] word_counts = [ max ( len ( t ) , 1 ) for t in e_set . _tokens ] comma_count = [ e . count ( "," ) for e in text ] ap_count = [ e . count ( "'" ) for e in text ] punc_count = [ e . count ( "." ) + e . count ( "?" ) + e . count ( "!" ) for e in text ] chars_per_word = [ lengths [ m ] / float ( word_counts [ m ] ) for m in xrange ( 0 , len ( text ) ) ] good_pos_tags , bad_pos_positions = self . _get_grammar_errors ( e_set . _pos , e_set . _text , e_set . _tokens ) good_pos_tag_prop = [ good_pos_tags [ m ] / float ( word_counts [ m ] ) for m in xrange ( 0 , len ( text ) ) ] length_arr = numpy . array ( ( lengths , word_counts , comma_count , ap_count , punc_count , chars_per_word , good_pos_tags , good_pos_tag_prop ) ) . transpose ( ) return length_arr . copy ( )
13422	def get_all ( self , key = None ) : key = self . definition . main_key if key is None else key key = self . definition . key_synonyms . get ( key , key ) entries = self . _get_all ( key ) if key in self . definition . scalar_nonunique_keys : return set ( entries ) return entries
1369	def create_tar ( tar_filename , files , config_dir , config_files ) : with contextlib . closing ( tarfile . open ( tar_filename , 'w:gz' , dereference = True ) ) as tar : for filename in files : if os . path . isfile ( filename ) : tar . add ( filename , arcname = os . path . basename ( filename ) ) else : raise Exception ( "%s is not an existing file" % filename ) if os . path . isdir ( config_dir ) : tar . add ( config_dir , arcname = get_heron_sandbox_conf_dir ( ) ) else : raise Exception ( "%s is not an existing directory" % config_dir ) for filename in config_files : if os . path . isfile ( filename ) : arcfile = os . path . join ( get_heron_sandbox_conf_dir ( ) , os . path . basename ( filename ) ) tar . add ( filename , arcname = arcfile ) else : raise Exception ( "%s is not an existing file" % filename )
6197	def numeric_params ( self ) : nparams = dict ( D = ( self . diffusion_coeff . mean ( ) , 'Diffusion coefficient (m^2/s)' ) , np = ( self . num_particles , 'Number of simulated particles' ) , t_step = ( self . t_step , 'Simulation time-step (s)' ) , t_max = ( self . t_max , 'Simulation total time (s)' ) , ID = ( self . ID , 'Simulation ID (int)' ) , EID = ( self . EID , 'IPython Engine ID (int)' ) , pico_mol = ( self . concentration ( ) * 1e12 , 'Particles concentration (pM)' ) ) return nparams
6283	def set_default_viewport ( self ) : # The expected height with the current viewport width expected_height = int ( self . buffer_width / self . aspect_ratio ) # How much positive or negative y padding blank_space = self . buffer_height - expected_height self . fbo . viewport = ( 0 , blank_space // 2 , self . buffer_width , expected_height )
3874	def _add_conversation ( self , conversation , events = [ ] , event_cont_token = None ) : # pylint: disable=dangerous-default-value conv_id = conversation . conversation_id . id logger . debug ( 'Adding new conversation: {}' . format ( conv_id ) ) conv = Conversation ( self . _client , self . _user_list , conversation , events , event_cont_token ) self . _conv_dict [ conv_id ] = conv return conv
10955	def get_update_io_tiles ( self , params , values ) : # get the affected area of the model image otile = self . get_update_tile ( params , values ) if otile is None : return [ None ] * 3 ptile = self . get_padding_size ( otile ) or util . Tile ( 0 , dim = otile . dim ) otile = util . Tile . intersection ( otile , self . oshape ) if ( otile . shape <= 0 ) . any ( ) : raise UpdateError ( "update triggered invalid tile size" ) if ( ptile . shape < 0 ) . any ( ) or ( ptile . shape > self . oshape . shape ) . any ( ) : raise UpdateError ( "update triggered invalid padding tile size" ) # now remove the part of the tile that is outside the image and pad the # interior part with that overhang. reflect the necessary padding back # into the image itself for the outer slice which we will call outer outer = otile . pad ( ( ptile . shape + 1 ) // 2 ) inner , outer = outer . reflect_overhang ( self . oshape ) iotile = inner . translate ( - outer . l ) outer = util . Tile . intersection ( outer , self . oshape ) inner = util . Tile . intersection ( inner , self . oshape ) return outer , inner , iotile
7569	def fullcomp ( seq ) : ## this is surely not the most efficient... seq = seq . replace ( "A" , 'u' ) . replace ( 'T' , 'v' ) . replace ( 'C' , 'p' ) . replace ( 'G' , 'z' ) . replace ( 'u' , 'T' ) . replace ( 'v' , 'A' ) . replace ( 'p' , 'G' ) . replace ( 'z' , 'C' ) ## No complement for S & W b/c complements are S & W, respectively seq = seq . replace ( 'R' , 'u' ) . replace ( 'K' , 'v' ) . replace ( 'Y' , 'b' ) . replace ( 'M' , 'o' ) . replace ( 'u' , 'Y' ) . replace ( 'v' , 'M' ) . replace ( 'b' , 'R' ) . replace ( 'o' , 'K' ) seq = seq . replace ( 'r' , 'u' ) . replace ( 'k' , 'v' ) . replace ( 'y' , 'b' ) . replace ( 'm' , 'o' ) . replace ( 'u' , 'y' ) . replace ( 'v' , 'm' ) . replace ( 'b' , 'r' ) . replace ( 'o' , 'k' ) return seq
979	def _countOverlap ( rep1 , rep2 ) : overlap = 0 for e in rep1 : if e in rep2 : overlap += 1 return overlap
8388	def merge_configs ( main , tweaks ) : for section in tweaks . sections ( ) : for option in tweaks . options ( section ) : value = tweaks . get ( section , option ) if option . endswith ( "+" ) : option = option [ : - 1 ] value = main . get ( section , option ) + value main . set ( section , option , value )
11016	def deploy ( context ) : config = context . obj header ( 'Generating HTML...' ) pelican ( config , '--verbose' , production = True ) header ( 'Removing unnecessary output...' ) unnecessary_paths = [ 'author' , 'category' , 'tag' , 'feeds' , 'tags.html' , 'authors.html' , 'categories.html' , 'archives.html' , ] for path in unnecessary_paths : remove_path ( os . path . join ( config [ 'OUTPUT_DIR' ] , path ) ) if os . environ . get ( 'TRAVIS' ) : # Travis CI header ( 'Setting up Git...' ) run ( 'git config user.name ' + run ( 'git show --format="%cN" -s' , capture = True ) ) run ( 'git config user.email ' + run ( 'git show --format="%cE" -s' , capture = True ) ) github_token = os . environ . get ( 'GITHUB_TOKEN' ) repo_slug = os . environ . get ( 'TRAVIS_REPO_SLUG' ) origin = 'https://{}@github.com/{}.git' . format ( github_token , repo_slug ) run ( 'git remote set-url origin ' + origin ) header ( 'Rewriting gh-pages branch...' ) run ( 'ghp-import -m "{message}" {dir}' . format ( message = 'Deploying {}' . format ( choose_commit_emoji ( ) ) , dir = config [ 'OUTPUT_DIR' ] , ) ) header ( 'Pushing to GitHub...' ) run ( 'git push origin gh-pages:gh-pages --force' )
2477	def add_lic_xref ( self , doc , ref ) : if self . has_extr_lic ( doc ) : self . extr_lic ( doc ) . add_xref ( ref ) return True else : raise OrderError ( 'ExtractedLicense::CrossRef' )
13246	async def _download_lsst_bibtex ( bibtex_names ) : blob_url_template = ( 'https://raw.githubusercontent.com/lsst/lsst-texmf/master/texmf/' 'bibtex/bib/{name}.bib' ) urls = [ blob_url_template . format ( name = name ) for name in bibtex_names ] tasks = [ ] async with ClientSession ( ) as session : for url in urls : task = asyncio . ensure_future ( _download_text ( url , session ) ) tasks . append ( task ) return await asyncio . gather ( * tasks )
12000	def _sign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = get_random_bytes ( algorithm [ 'salt_size' ] ) key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _encode ( data , algorithm , key ) return data + key_salt
11405	def record_drop_duplicate_fields ( record ) : out = { } position = 0 tags = sorted ( record . keys ( ) ) for tag in tags : fields = record [ tag ] out [ tag ] = [ ] current_fields = set ( ) for full_field in fields : field = ( tuple ( full_field [ 0 ] ) , ) + full_field [ 1 : 4 ] if field not in current_fields : current_fields . add ( field ) position += 1 out [ tag ] . append ( full_field [ : 4 ] + ( position , ) ) return out
10593	def get_date ( date ) : if type ( date ) is str : return datetime . strptime ( date , '%Y-%m-%d' ) . date ( ) else : return date
784	def jobCancelAllRunningJobs ( self ) : # Get a database connection and cursor with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET cancel=TRUE WHERE status<>%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) return
13673	def add_file ( self , * args ) : for file_path in args : self . files . append ( FilePath ( file_path , self ) )
6308	def load_resource_module ( self ) : # Attempt to load the dependencies module try : name = '{}.{}' . format ( self . name , 'dependencies' ) self . dependencies_module = importlib . import_module ( name ) except ModuleNotFoundError as err : raise EffectError ( ( "Effect package '{}' has no 'dependencies' module or the module has errors. " "Forwarded error from importlib: {}" ) . format ( self . name , err ) ) # Fetch the resource descriptions try : self . resources = getattr ( self . dependencies_module , 'resources' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has no 'resources' attribute" . format ( name ) ) if not isinstance ( self . resources , list ) : raise EffectError ( "Effect dependencies module '{}': 'resources' is of type {} instead of a list" . format ( name , type ( self . resources ) ) ) # Fetch the effect class list try : self . effect_packages = getattr ( self . dependencies_module , 'effect_packages' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has 'effect_packages' attribute" . format ( name ) ) if not isinstance ( self . effect_packages , list ) : raise EffectError ( "Effect dependencies module '{}': 'effect_packages' is of type {} instead of a list" . format ( name , type ( self . effects ) ) )
5982	def output_figure ( array , as_subplot , output_path , output_filename , output_format ) : if not as_subplot : if output_format is 'show' : plt . show ( ) elif output_format is 'png' : plt . savefig ( output_path + output_filename + '.png' , bbox_inches = 'tight' ) elif output_format is 'fits' : array_util . numpy_array_2d_to_fits ( array_2d = array , file_path = output_path + output_filename + '.fits' , overwrite = True )
13429	def get_sites ( self ) : url = "/2/sites" data = self . _get_resource ( url ) sites = [ ] for entry in data [ 'sites' ] : sites . append ( self . site_from_json ( entry ) ) return sites
1557	def _get_stream_id ( comp_name , stream_id ) : proto_stream_id = topology_pb2 . StreamId ( ) proto_stream_id . id = stream_id proto_stream_id . component_name = comp_name return proto_stream_id
2530	def parse_creation_info ( self , ci_term ) : for _s , _p , o in self . graph . triples ( ( ci_term , self . spdx_namespace [ 'creator' ] , None ) ) : try : ent = self . builder . create_entity ( self . doc , six . text_type ( o ) ) self . builder . add_creator ( self . doc , ent ) except SPDXValueError : self . value_error ( 'CREATOR_VALUE' , o ) for _s , _p , o in self . graph . triples ( ( ci_term , self . spdx_namespace [ 'created' ] , None ) ) : try : self . builder . set_created_date ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'CREATED_VALUE' , o ) except CardinalityError : self . more_than_one_error ( 'created' ) break for _s , _p , o in self . graph . triples ( ( ci_term , RDFS . comment , None ) ) : try : self . builder . set_creation_comment ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'CreationInfo comment' ) break for _s , _p , o in self . graph . triples ( ( ci_term , self . spdx_namespace [ 'licenseListVersion' ] , None ) ) : try : self . builder . set_lics_list_ver ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'licenseListVersion' ) break except SPDXValueError : self . value_error ( 'LL_VALUE' , o )
12003	def _add_header ( self , data , options ) : # pylint: disable=W0142 version_info = self . _get_version_info ( options [ 'version' ] ) flags = options [ 'flags' ] header_flags = dict ( ( i , str ( int ( j ) ) ) for i , j in options [ 'flags' ] . iteritems ( ) ) header_flags = '' . join ( version_info [ 'flags' ] ( * * header_flags ) ) header_flags = int ( header_flags , 2 ) options [ 'flags' ] = header_flags header = version_info [ 'header' ] header = header ( * * options ) header = pack ( version_info [ 'header_format' ] , * header ) if 'timestamp' in flags and flags [ 'timestamp' ] : timestamp = long ( time ( ) ) timestamp = pack ( version_info [ 'timestamp_format' ] , timestamp ) header = header + timestamp return header + data
8329	def findNext ( self , name = None , attrs = { } , text = None , * * kwargs ) : return self . _findOne ( self . findAllNext , name , attrs , text , * * kwargs )
3816	async def _add_channel_services ( self ) : logger . info ( 'Adding channel services...' ) # Based on what Hangouts for Chrome does over 2 requests, this is # trimmed down to 1 request that includes the bare minimum to make # things work. services = [ "babel" , "babel_presence_last_seen" ] map_list = [ dict ( p = json . dumps ( { "3" : { "1" : { "1" : service } } } ) ) for service in services ] await self . _channel . send_maps ( map_list ) logger . info ( 'Channel services added' )
6880	def _parse_csv_header ( header ) : # first, break into lines headerlines = header . split ( '\n' ) headerlines = [ x . lstrip ( '# ' ) for x in headerlines ] # next, find the indices of the metadata sections objectstart = headerlines . index ( 'OBJECT' ) metadatastart = headerlines . index ( 'METADATA' ) camfilterstart = headerlines . index ( 'CAMFILTERS' ) photaperturestart = headerlines . index ( 'PHOTAPERTURES' ) columnstart = headerlines . index ( 'COLUMNS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) # get the lines for the header sections objectinfo = headerlines [ objectstart + 1 : metadatastart - 1 ] metadatainfo = headerlines [ metadatastart + 1 : camfilterstart - 1 ] camfilterinfo = headerlines [ camfilterstart + 1 : photaperturestart - 1 ] photapertureinfo = headerlines [ photaperturestart + 1 : columnstart - 1 ] columninfo = headerlines [ columnstart + 1 : lcstart - 1 ] # parse the header sections and insert the appropriate key-val pairs into # the lcdict metadict = { 'objectinfo' : { } } # first, the objectinfo section objectinfo = [ x . split ( ';' ) for x in objectinfo ] for elem in objectinfo : for kvelem in elem : key , val = kvelem . split ( ' = ' , 1 ) metadict [ 'objectinfo' ] [ key . strip ( ) ] = ( _smartcast ( val , METAKEYS [ key . strip ( ) ] ) ) # the objectid belongs at the top level metadict [ 'objectid' ] = metadict [ 'objectinfo' ] [ 'objectid' ] [ : ] del metadict [ 'objectinfo' ] [ 'objectid' ] # get the lightcurve metadata metadatainfo = [ x . split ( ';' ) for x in metadatainfo ] for elem in metadatainfo : for kvelem in elem : try : key , val = kvelem . split ( ' = ' , 1 ) # get the lcbestaperture into a dict again if key . strip ( ) == 'lcbestaperture' : val = json . loads ( val ) # get the lcversion and datarelease as integers if key . strip ( ) in ( 'datarelease' , 'lcversion' ) : val = int ( val ) # get the lastupdated as a float if key . strip ( ) == 'lastupdated' : val = float ( val ) # put the key-val into the dict metadict [ key . strip ( ) ] = val except Exception as e : LOGWARNING ( 'could not understand header element "%s",' ' skipped.' % kvelem ) # get the camera filters metadict [ 'filters' ] = [ ] for row in camfilterinfo : filterid , filtername , filterdesc = row . split ( ' - ' ) metadict [ 'filters' ] . append ( ( int ( filterid ) , filtername , filterdesc ) ) # get the photometric apertures metadict [ 'lcapertures' ] = { } for row in photapertureinfo : apnum , appix = row . split ( ' - ' ) appix = float ( appix . rstrip ( ' px' ) ) metadict [ 'lcapertures' ] [ apnum . strip ( ) ] = appix # get the columns metadict [ 'columns' ] = [ ] for row in columninfo : colnum , colname , coldesc = row . split ( ' - ' ) metadict [ 'columns' ] . append ( colname ) return metadict
3653	def represent_pixel_location ( self ) : if self . data is None : return None # return self . _data . reshape ( self . height + self . y_padding , int ( self . width * self . _num_components_per_pixel + self . x_padding ) )
5117	def get_agent_data ( self , queues = None , edge = None , edge_type = None , return_header = False ) : queues = _get_queues ( self . g , queues , edge , edge_type ) data = { } for qid in queues : for agent_id , dat in self . edge2queue [ qid ] . data . items ( ) : datum = np . zeros ( ( len ( dat ) , 6 ) ) datum [ : , : 5 ] = np . array ( dat ) datum [ : , 5 ] = qid if agent_id in data : data [ agent_id ] = np . vstack ( ( data [ agent_id ] , datum ) ) else : data [ agent_id ] = datum dType = [ ( 'a' , float ) , ( 's' , float ) , ( 'd' , float ) , ( 'q' , float ) , ( 'n' , float ) , ( 'id' , float ) ] for agent_id , dat in data . items ( ) : datum = np . array ( [ tuple ( d ) for d in dat . tolist ( ) ] , dtype = dType ) datum = np . sort ( datum , order = 'a' ) data [ agent_id ] = np . array ( [ tuple ( d ) for d in datum ] ) if return_header : return data , 'arrival,service,departure,num_queued,num_total,q_id' return data
1040	def line ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return line
4581	def to_color ( c ) : if isinstance ( c , numbers . Number ) : return c , c , c if not c : raise ValueError ( 'Cannot create color from empty "%s"' % c ) if isinstance ( c , str ) : return name_to_color ( c ) if isinstance ( c , list ) : c = tuple ( c ) if isinstance ( c , tuple ) : if len ( c ) > 3 : return c [ : 3 ] while len ( c ) < 3 : c += ( c [ - 1 ] , ) return c raise ValueError ( 'Cannot create color from "%s"' % c )
11323	def check_pkgs_integrity ( filelist , logger , ftp_connector , timeout = 120 , sleep_time = 10 ) : ref_1 = [ ] ref_2 = [ ] i = 1 print >> sys . stdout , "\nChecking packages integrity." for filename in filelist : # ref_1.append(self.get_remote_file_size(filename)) get_remote_file_size ( ftp_connector , filename , ref_1 ) print >> sys . stdout , "\nGoing to sleep for %i sec." % ( sleep_time , ) time . sleep ( sleep_time ) while sleep_time * i < timeout : for filename in filelist : # ref_2.append(self.get_remote_file_size(filename)) get_remote_file_size ( ftp_connector , filename , ref_2 ) if ref_1 == ref_2 : print >> sys . stdout , "\nIntegrity OK:)" logger . info ( "Packages integrity OK." ) break else : print >> sys . stdout , "\nWaiting %d time for itegrity..." % ( i , ) logger . info ( "\nWaiting %d time for itegrity..." % ( i , ) ) i += 1 ref_1 , ref_2 = ref_2 , [ ] time . sleep ( sleep_time ) else : not_finished_files = [ ] for count , val1 in enumerate ( ref_1 ) : if val1 != ref_2 [ count ] : not_finished_files . append ( filelist [ count ] ) print >> sys . stdout , "\nOMG, OMG something wrong with integrity." logger . error ( "Integrity check faild for files %s" % ( not_finished_files , ) )
5256	def disassemble_all ( bytecode , pc = 0 , fork = DEFAULT_FORK ) : if isinstance ( bytecode , bytes ) : bytecode = bytearray ( bytecode ) if isinstance ( bytecode , str ) : bytecode = bytearray ( bytecode . encode ( 'latin-1' ) ) bytecode = iter ( bytecode ) while True : instr = disassemble_one ( bytecode , pc = pc , fork = fork ) if not instr : return pc += instr . size yield instr
9826	def experiments ( ctx , metrics , declarations , independent , group , query , sort , page ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_experiments ( username = user , project_name = project_name , independent = independent , group = group , metrics = metrics , declarations = declarations , query = query , sort = sort , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiments for project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Experiments for project `{}/{}`.' . format ( user , project_name ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No experiments found for project `{}/{}`.' . format ( user , project_name ) ) if metrics : objects = get_experiments_with_metrics ( response ) elif declarations : objects = get_experiments_with_declarations ( response ) else : objects = [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) ) for o in response [ 'results' ] ] objects = list_dicts_to_tabulate ( objects ) if objects : Printer . print_header ( "Experiments:" ) objects . pop ( 'project_name' , None ) dict_tabulate ( objects , is_list_dict = True )
8839	def missing ( data , * args ) : not_found = object ( ) if args and isinstance ( args [ 0 ] , list ) : args = args [ 0 ] ret = [ ] for arg in args : if get_var ( data , arg , not_found ) is not_found : ret . append ( arg ) return ret
9120	def dropbox_fileupload ( dropbox , request ) : attachment = request . POST [ 'attachment' ] attached = dropbox . add_attachment ( attachment ) return dict ( files = [ dict ( name = attached , type = attachment . type , ) ] )
13852	def age ( self ) : # 0 means this composer will never decompose if self . rounds == 1 : self . do_run = False elif self . rounds > 1 : self . rounds -= 1
3973	def _get_ports_list ( app_name , port_specs ) : if app_name not in port_specs [ 'docker_compose' ] : return [ ] return [ "{}:{}" . format ( port_spec [ 'mapped_host_port' ] , port_spec [ 'in_container_port' ] ) for port_spec in port_specs [ 'docker_compose' ] [ app_name ] ]
12808	def fetch ( self ) : try : if not self . _last_message_id : messages = self . _connection . get ( "room/%s/recent" % self . _room_id , key = "messages" , parameters = { "limit" : 1 } ) self . _last_message_id = messages [ - 1 ] [ "id" ] messages = self . _connection . get ( "room/%s/recent" % self . _room_id , key = "messages" , parameters = { "since_message_id" : self . _last_message_id } ) except : messages = [ ] if messages : self . _last_message_id = messages [ - 1 ] [ "id" ] self . received ( messages )
12269	def evaluate ( self , repo , spec , args ) : status = [ ] with cd ( repo . rootdir ) : files = spec . get ( 'files' , [ '*' ] ) resource_files = repo . find_matching_files ( files ) files = glob2 . glob ( "**/*" ) disk_files = [ f for f in files if os . path . isfile ( f ) and f != "datapackage.json" ] allfiles = list ( set ( resource_files + disk_files ) ) allfiles . sort ( ) for f in allfiles : if f in resource_files and f in disk_files : r = repo . get_resource ( f ) coded_sha256 = r [ 'sha256' ] computed_sha256 = compute_sha256 ( f ) if computed_sha256 != coded_sha256 : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "Mismatch in checksum on disk and in datapackage.json" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'OK' , 'message' : "" } ) elif f in resource_files : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In datapackage.json but not in repo" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In repo but not in datapackage.json" } ) return status
5812	def raise_expired_not_yet_valid ( certificate ) : validity = certificate [ 'tbs_certificate' ] [ 'validity' ] not_after = validity [ 'not_after' ] . native not_before = validity [ 'not_before' ] . native now = datetime . now ( timezone . utc ) if not_before > now : formatted_before = not_before . strftime ( '%Y-%m-%d %H:%M:%SZ' ) message = 'Server certificate verification failed - certificate not valid until %s' % formatted_before elif not_after < now : formatted_after = not_after . strftime ( '%Y-%m-%d %H:%M:%SZ' ) message = 'Server certificate verification failed - certificate expired %s' % formatted_after raise TLSVerificationError ( message , certificate )
11023	def gen_key ( self , key ) : b_key = self . _hash_digest ( key ) return self . _hash_val ( b_key , lambda x : x )
10317	def single_run_arrays ( spanning_cluster = True , * * kwargs ) : # initial iteration # we do not need a copy of the result dictionary since we copy the values # anyway kwargs [ 'copy_result' ] = False ret = dict ( ) for n , state in enumerate ( sample_states ( spanning_cluster = spanning_cluster , * * kwargs ) ) : # merge cluster statistics if 'N' in ret : assert ret [ 'N' ] == state [ 'N' ] else : ret [ 'N' ] = state [ 'N' ] if 'M' in ret : assert ret [ 'M' ] == state [ 'M' ] else : ret [ 'M' ] = state [ 'M' ] number_of_states = state [ 'M' ] + 1 max_cluster_size = np . empty ( number_of_states ) if spanning_cluster : has_spanning_cluster = np . empty ( number_of_states , dtype = np . bool ) moments = np . empty ( ( 5 , number_of_states ) ) max_cluster_size [ n ] = state [ 'max_cluster_size' ] for k in range ( 5 ) : moments [ k , n ] = state [ 'moments' ] [ k ] if spanning_cluster : has_spanning_cluster [ n ] = state [ 'has_spanning_cluster' ] ret [ 'max_cluster_size' ] = max_cluster_size ret [ 'moments' ] = moments if spanning_cluster : ret [ 'has_spanning_cluster' ] = has_spanning_cluster return ret
989	def add ( reader , writer , column , start , stop , value ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( value ) ( row [ column ] ) + value writer . appendRecord ( row )
6135	def _fix_docs ( this_abc , child_class ) : # After python 3.5, this is basically handled automatically if sys . version_info >= ( 3 , 5 ) : return child_class if not issubclass ( child_class , this_abc ) : raise KappaError ( 'Cannot fix docs of class that is not decendent.' ) # This method is modified from solution given in # https://stackoverflow.com/a/8101598/8863865 for name , child_func in vars ( child_class ) . items ( ) : if callable ( child_func ) and not child_func . __doc__ : if name in this_abc . __abstractmethods__ : parent_func = getattr ( this_abc , name ) child_func . __doc__ = parent_func . __doc__ return child_class
1672	def ProcessFile ( filename , vlevel , extra_check_functions = None ) : _SetVerboseLevel ( vlevel ) _BackupFilters ( ) if not ProcessConfigOverrides ( filename ) : _RestoreFilters ( ) return lf_lines = [ ] crlf_lines = [ ] try : # Support the UNIX convention of using "-" for stdin. Note that # we are not opening the file with universal newline support # (which codecs doesn't support anyway), so the resulting lines do # contain trailing '\r' characters if we are reading a file that # has CRLF endings. # If after the split a trailing '\r' is present, it is removed # below. if filename == '-' : lines = codecs . StreamReaderWriter ( sys . stdin , codecs . getreader ( 'utf8' ) , codecs . getwriter ( 'utf8' ) , 'replace' ) . read ( ) . split ( '\n' ) else : lines = codecs . open ( filename , 'r' , 'utf8' , 'replace' ) . read ( ) . split ( '\n' ) # Remove trailing '\r'. # The -1 accounts for the extra trailing blank line we get from split() for linenum in range ( len ( lines ) - 1 ) : if lines [ linenum ] . endswith ( '\r' ) : lines [ linenum ] = lines [ linenum ] . rstrip ( '\r' ) crlf_lines . append ( linenum + 1 ) else : lf_lines . append ( linenum + 1 ) except IOError : _cpplint_state . PrintError ( "Skipping input '%s': Can't open for reading\n" % filename ) _RestoreFilters ( ) return # Note, if no dot is found, this will give the entire filename as the ext. file_extension = filename [ filename . rfind ( '.' ) + 1 : ] # When reading from stdin, the extension is unknown, so no cpplint tests # should rely on the extension. if filename != '-' and file_extension not in GetAllExtensions ( ) : # bazel 0.5.1> uses four distinct generated files that gives a warning # we suppress the warning for these files bazel_gen_files = set ( [ "external/local_config_cc/libtool" , "external/local_config_cc/make_hashed_objlist.py" , "external/local_config_cc/wrapped_ar" , "external/local_config_cc/wrapped_clang" , "external/local_config_cc/xcrunwrapper.sh" , ] ) if not filename in bazel_gen_files : _cpplint_state . PrintError ( 'Ignoring %s; not a valid file name ' '(%s)\n' % ( filename , ', ' . join ( GetAllExtensions ( ) ) ) ) else : ProcessFileData ( filename , file_extension , lines , Error , extra_check_functions ) # If end-of-line sequences are a mix of LF and CR-LF, issue # warnings on the lines with CR. # # Don't issue any warnings if all lines are uniformly LF or CR-LF, # since critique can handle these just fine, and the style guide # doesn't dictate a particular end of line sequence. # # We can't depend on os.linesep to determine what the desired # end-of-line sequence should be, since that will return the # server-side end-of-line sequence. if lf_lines and crlf_lines : # Warn on every line with CR. An alternative approach might be to # check whether the file is mostly CRLF or just LF, and warn on the # minority, we bias toward LF here since most tools prefer LF. for linenum in crlf_lines : Error ( filename , linenum , 'whitespace/newline' , 1 , 'Unexpected \\r (^M) found; better to use only \\n' ) _RestoreFilters ( )
1572	def setup ( self , context ) : myindex = context . get_partition_index ( ) self . _files_to_consume = self . _files [ myindex : : context . get_num_partitions ( ) ] self . logger . info ( "TextFileSpout files to consume %s" % self . _files_to_consume ) self . _lines_to_consume = self . _get_next_lines ( ) self . _emit_count = 0
13301	def df_quantile ( df , nb = 100 ) : quantiles = np . linspace ( 0 , 1. , nb ) res = pd . DataFrame ( ) for q in quantiles : res = res . append ( df . quantile ( q ) , ignore_index = True ) return res
1412	def _get_topologies_with_watch ( self , callback , isWatching ) : path = self . get_topologies_path ( ) if isWatching : LOG . info ( "Adding children watch for path: " + path ) # pylint: disable=unused-variable @ self . client . ChildrenWatch ( path ) def watch_topologies ( topologies ) : """ callback to watch topologies """ callback ( topologies ) # Returning False will result in no future watches # being triggered. If isWatching is True, then # the future watches will be triggered. return isWatching
9176	def with_db_cursor ( func ) : @ functools . wraps ( func ) def wrapped ( * args , * * kwargs ) : if 'cursor' in kwargs or func . func_code . co_argcount == len ( args ) : return func ( * args , * * kwargs ) with db_connect ( ) as db_connection : with db_connection . cursor ( ) as cursor : kwargs [ 'cursor' ] = cursor return func ( * args , * * kwargs ) return wrapped
9173	def _formatter_callback_factory ( ) : # pragma: no cover includes = [ ] exercise_url_template = '{baseUrl}/api/exercises?q={field}:"{{itemCode}}"' settings = get_current_registry ( ) . settings exercise_base_url = settings . get ( 'embeddables.exercise.base_url' , None ) exercise_matches = [ match . split ( ',' , 1 ) for match in aslist ( settings . get ( 'embeddables.exercise.match' , '' ) , flatten = False ) ] exercise_token = settings . get ( 'embeddables.exercise.token' , None ) mathml_url = settings . get ( 'mathmlcloud.url' , None ) memcache_servers = settings . get ( 'memcache_servers' ) if memcache_servers : memcache_servers = memcache_servers . split ( ) else : memcache_servers = None if exercise_base_url and exercise_matches : mc_client = None if memcache_servers : mc_client = memcache . Client ( memcache_servers , debug = 0 ) for ( exercise_match , exercise_field ) in exercise_matches : template = exercise_url_template . format ( baseUrl = exercise_base_url , field = exercise_field ) includes . append ( exercise_callback_factory ( exercise_match , template , mc_client , exercise_token , mathml_url ) ) return includes
126	def Negative ( other_param , mode = "invert" , reroll_count_max = 2 ) : return ForceSign ( other_param = other_param , positive = False , mode = mode , reroll_count_max = reroll_count_max )
2866	def write8 ( self , register , value ) : value = value & 0xFF self . _bus . write_byte_data ( self . _address , register , value ) self . _logger . debug ( "Wrote 0x%02X to register 0x%02X" , value , register )
1598	def read_chunk ( filename , offset = - 1 , length = - 1 , escape_data = False ) : try : length = int ( length ) offset = int ( offset ) except ValueError : return { } if not os . path . isfile ( filename ) : return { } try : fstat = os . stat ( filename ) except Exception : return { } if offset == - 1 : offset = fstat . st_size if length == - 1 : length = fstat . st_size - offset with open ( filename , "r" ) as fp : fp . seek ( offset ) try : data = fp . read ( length ) except IOError : return { } if data : data = _escape_data ( data ) if escape_data else data return dict ( offset = offset , length = len ( data ) , data = data ) return dict ( offset = offset , length = 0 )
8452	def _get_current_branch ( ) : result = temple . utils . shell ( 'git rev-parse --abbrev-ref HEAD' , stdout = subprocess . PIPE ) return result . stdout . decode ( 'utf8' ) . strip ( )
8548	def delete_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) , method = 'DELETE' ) return response
4620	def unlock ( self , password ) : self . password = password if self . config_key in self . config and self . config [ self . config_key ] : self . _decrypt_masterpassword ( ) else : self . _new_masterpassword ( password ) self . _save_encrypted_masterpassword ( )
5811	def raise_hostname ( certificate , hostname ) : is_ip = re . match ( '^\\d+\\.\\d+\\.\\d+\\.\\d+$' , hostname ) or hostname . find ( ':' ) != - 1 if is_ip : hostname_type = 'IP address %s' % hostname else : hostname_type = 'domain name %s' % hostname message = 'Server certificate verification failed - %s does not match' % hostname_type valid_ips = ', ' . join ( certificate . valid_ips ) valid_domains = ', ' . join ( certificate . valid_domains ) if valid_domains : message += ' valid domains: %s' % valid_domains if valid_domains and valid_ips : message += ' or' if valid_ips : message += ' valid IP addresses: %s' % valid_ips raise TLSVerificationError ( message , certificate )
1648	def CheckAltTokens ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] # Avoid preprocessor lines if Match ( r'^\s*#' , line ) : return # Last ditch effort to avoid multi-line comments. This will not help # if the comment started before the current line or ended after the # current line, but it catches most of the false positives. At least, # it provides a way to workaround this warning for people who use # multi-line comments in preprocessor macros. # # TODO(unknown): remove this once cpplint has better support for # multi-line comments. if line . find ( '/*' ) >= 0 or line . find ( '*/' ) >= 0 : return for match in _ALT_TOKEN_REPLACEMENT_PATTERN . finditer ( line ) : error ( filename , linenum , 'readability/alt_tokens' , 2 , 'Use operator %s instead of %s' % ( _ALT_TOKEN_REPLACEMENT [ match . group ( 1 ) ] , match . group ( 1 ) ) )
11449	def send ( self , recipient , message ) : if self . _logindata [ 'login_rufnummer' ] is None or self . _logindata [ 'login_passwort' ] is None : err_mess = "YesssSMS: Login data required" raise self . LoginError ( err_mess ) if not recipient : raise self . NoRecipientError ( "YesssSMS: recipient number missing" ) if not isinstance ( recipient , str ) : raise ValueError ( "YesssSMS: str expected as recipient number" ) if not message : raise self . EmptyMessageError ( "YesssSMS: message is empty" ) with self . _login ( requests . Session ( ) ) as sess : sms_data = { 'to_nummer' : recipient , 'nachricht' : message } req = sess . post ( self . _websms_url , data = sms_data ) if not ( req . status_code == 200 or req . status_code == 302 ) : raise self . SMSSendingError ( "YesssSMS: error sending SMS" ) if _UNSUPPORTED_CHARS_STRING in req . text : raise self . UnsupportedCharsError ( "YesssSMS: message contains unsupported character(s)" ) if _SMS_SENDING_SUCCESSFUL_STRING not in req . text : raise self . SMSSendingError ( "YesssSMS: error sending SMS" ) sess . get ( self . _logout_url )
3621	def get_adapter ( self , model ) : if not self . is_registered ( model ) : raise RegistrationError ( '{} is not registered with Algolia engine' . format ( model ) ) return self . __registered_models [ model ]
13708	def check_ip ( self , ip ) : self . _last_result = None if is_valid_ipv4 ( ip ) : key = None if self . _use_cache : key = self . _make_cache_key ( ip ) self . _last_result = self . _cache . get ( key , version = self . _cache_version ) if self . _last_result is None : # request httpBL API error , age , threat , type = self . _request_httpbl ( ip ) if error == 127 or error == 0 : self . _last_result = { 'error' : error , 'age' : age , 'threat' : threat , 'type' : type } if self . _use_cache : self . _cache . set ( key , self . _last_result , timeout = self . _api_timeout , version = self . _cache_version ) if self . _last_result is not None and settings . CACHED_HTTPBL_USE_LOGGING : logger . info ( 'httpBL check ip: {0}; ' 'httpBL result: error: {1}, age: {2}, threat: {3}, type: {4}' . format ( ip , self . _last_result [ 'error' ] , self . _last_result [ 'age' ] , self . _last_result [ 'threat' ] , self . _last_result [ 'type' ] ) ) return self . _last_result
631	def createSegment ( self , cell ) : cellData = self . _cells [ cell ] if len ( self . _freeFlatIdxs ) > 0 : flatIdx = self . _freeFlatIdxs . pop ( ) else : flatIdx = self . _nextFlatIdx self . _segmentForFlatIdx . append ( None ) self . _nextFlatIdx += 1 ordinal = self . _nextSegmentOrdinal self . _nextSegmentOrdinal += 1 segment = Segment ( cell , flatIdx , ordinal ) cellData . _segments . append ( segment ) self . _segmentForFlatIdx [ flatIdx ] = segment return segment
7601	def get_popular_decks ( self , * * params : keys ) : url = self . api . POPULAR + '/decks' return self . _get_model ( url , * * params )
7988	def _setup_stream_element_handlers ( self ) : # pylint: disable-msg=W0212 if self . initiator : mode = "initiator" else : mode = "receiver" self . _element_handlers = { } for handler in self . handlers : if not isinstance ( handler , StreamFeatureHandler ) : continue for _unused , meth in inspect . getmembers ( handler , callable ) : if not hasattr ( meth , "_pyxmpp_stream_element_handled" ) : continue element_handled = meth . _pyxmpp_stream_element_handled if element_handled in self . _element_handlers : # use only the first matching handler continue if meth . _pyxmpp_usage_restriction in ( None , mode ) : self . _element_handlers [ element_handled ] = meth
4366	def process_event ( self , packet ) : args = packet [ 'args' ] name = packet [ 'name' ] if not allowed_event_name_regex . match ( name ) : self . error ( "unallowed_event_name" , "name must only contains alpha numerical characters" ) return method_name = 'on_' + name . replace ( ' ' , '_' ) # This means the args, passed as a list, will be expanded to # the method arg and if you passed a dict, it will be a dict # as the first parameter. return self . call_method_with_acl ( method_name , packet , * args )
4292	def validate_project ( project_name ) : if '-' in project_name : return None if keyword . iskeyword ( project_name ) : return None if project_name in dir ( __builtins__ ) : return None try : __import__ ( project_name ) return None except ImportError : return project_name
8161	def next_event ( block = False , timeout = None ) : try : return channel . listen ( block = block , timeout = timeout ) . next ( ) [ 'data' ] except StopIteration : return None
4195	def plot_time_freq ( self , mindB = - 100 , maxdB = None , norm = True , yaxis_label_position = "right" ) : from pylab import subplot , gca subplot ( 1 , 2 , 1 ) self . plot_window ( ) subplot ( 1 , 2 , 2 ) self . plot_frequencies ( mindB = mindB , maxdB = maxdB , norm = norm ) if yaxis_label_position == "left" : try : tight_layout ( ) except : pass else : ax = gca ( ) ax . yaxis . set_label_position ( "right" )
11918	def get_dataframe ( self ) : assert self . dataframe is not None , ( "'%s' should either include a `dataframe` attribute, " "or override the `get_dataframe()` method." % self . __class__ . __name__ ) dataframe = self . dataframe return dataframe
9097	def drop_bel_namespace ( self ) -> Optional [ Namespace ] : namespace = self . _get_default_namespace ( ) if namespace is not None : for entry in tqdm ( namespace . entries , desc = f'deleting entries in {self._get_namespace_name()}' ) : self . session . delete ( entry ) self . session . delete ( namespace ) log . info ( 'committing deletions' ) self . session . commit ( ) return namespace
2655	def isdir ( self , path ) : result = True try : self . sftp_client . lstat ( path ) except FileNotFoundError : result = False return result
4888	def update_enterprise_courses ( self , enterprise_customer , course_container_key = 'results' , * * kwargs ) : enterprise_context = { 'tpa_hint' : enterprise_customer and enterprise_customer . identity_provider , 'enterprise_id' : enterprise_customer and str ( enterprise_customer . uuid ) , } enterprise_context . update ( * * kwargs ) courses = [ ] for course in self . data [ course_container_key ] : courses . append ( self . update_course ( course , enterprise_customer , enterprise_context ) ) self . data [ course_container_key ] = courses
13316	def create ( name_or_path = None , config = None ) : # Get the real path of the environment if utils . is_system_path ( name_or_path ) : path = unipath ( name_or_path ) else : path = unipath ( get_home_path ( ) , name_or_path ) if os . path . exists ( path ) : raise OSError ( '{} already exists' . format ( path ) ) env = VirtualEnvironment ( path ) utils . ensure_path_exists ( env . path ) if config : if utils . is_git_repo ( config ) : Git ( '' ) . clone ( config , env . path ) else : shutil . copy2 ( config , env . config_path ) else : with open ( env . config_path , 'w' ) as f : f . write ( defaults . environment_config ) utils . ensure_path_exists ( env . hook_path ) utils . ensure_path_exists ( env . modules_path ) env . run_hook ( 'precreate' ) virtualenv . create_environment ( env . path ) if not utils . is_home_environment ( env . path ) : EnvironmentCache . add ( env ) EnvironmentCache . save ( ) try : env . update ( ) except : utils . rmtree ( path ) logger . debug ( 'Failed to update, rolling back...' ) raise else : env . run_hook ( 'postcreate' ) return env
5803	def extract_chain ( server_handshake_bytes ) : output = [ ] chain_bytes = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0b' : chain_bytes = message_data break if chain_bytes : break if chain_bytes : # The first 3 bytes are the cert chain length pointer = 3 while pointer < len ( chain_bytes ) : cert_length = int_from_bytes ( chain_bytes [ pointer : pointer + 3 ] ) cert_start = pointer + 3 cert_end = cert_start + cert_length pointer = cert_end cert_bytes = chain_bytes [ cert_start : cert_end ] output . append ( Certificate . load ( cert_bytes ) ) return output
7803	def display_name ( self ) : if self . subject_name : return u", " . join ( [ u", " . join ( [ u"{0}={1}" . format ( k , v ) for k , v in dn_tuple ] ) for dn_tuple in self . subject_name ] ) for name_type in ( "XmppAddr" , "DNS" , "SRV" ) : names = self . alt_names . get ( name_type ) if names : return names [ 0 ] return u"<unknown>"
11426	def record_strip_empty_volatile_subfields ( rec ) : for tag in rec . keys ( ) : for field in rec [ tag ] : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield [ 1 ] [ : 9 ] != "VOLATILE:" ]
9425	def open ( self , member , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename archive = unrarlib . RAROpenArchiveDataEx ( self . filename , mode = constants . RAR_OM_EXTRACT ) handle = self . _open ( archive ) password = pwd or self . pwd if password is not None : unrarlib . RARSetPassword ( handle , b ( password ) ) # based on BrutuZ (https://github.com/matiasb/python-unrar/pull/4) # and Cubixmeister work data = _ReadIntoMemory ( ) c_callback = unrarlib . UNRARCALLBACK ( data . _callback ) unrarlib . RARSetCallback ( handle , c_callback , 0 ) try : rarinfo = self . _read_header ( handle ) while rarinfo is not None : if rarinfo . filename == member : self . _process_current ( handle , constants . RAR_TEST ) break else : self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle ) if rarinfo is None : data = None except unrarlib . MissingPassword : raise RuntimeError ( "File is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for File" ) except unrarlib . BadDataError : if password is not None : raise RuntimeError ( "File CRC error or incorrect password" ) else : raise RuntimeError ( "File CRC error" ) except unrarlib . UnrarException as e : raise BadRarFile ( "Bad RAR archive data: %s" % str ( e ) ) finally : self . _close ( handle ) if data is None : raise KeyError ( 'There is no item named %r in the archive' % member ) # return file-like object return data . get_bytes ( )
11451	def get_collection ( self , journal ) : conference = '' for tag in self . document . getElementsByTagName ( 'conference' ) : conference = xml_to_text ( tag ) if conference or journal == "International Journal of Modern Physics: Conference Series" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'ConferencePaper' ) ] elif self . _get_article_type ( ) == "review-article" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Review' ) ] else : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Published' ) ]
1612	def ProcessGlobalSuppresions ( lines ) : for line in lines : if _SEARCH_C_FILE . search ( line ) : for category in _DEFAULT_C_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True if _SEARCH_KERNEL_FILE . search ( line ) : for category in _DEFAULT_KERNEL_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True
5114	def clear_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . data = { }
10569	def filter_local_songs ( filepaths , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : matched_songs = [ ] filtered_songs = [ ] for filepath in filepaths : try : song = _get_mutagen_metadata ( filepath ) except mutagen . MutagenError : filtered_songs . append ( filepath ) else : if include_filters or exclude_filters : if _check_filters ( song , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) : matched_songs . append ( filepath ) else : filtered_songs . append ( filepath ) else : matched_songs . append ( filepath ) return matched_songs , filtered_songs
2331	def normal_noise ( points ) : return np . random . rand ( 1 ) * np . random . randn ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
3618	def set_settings ( self ) : if not self . settings : return try : self . __index . set_settings ( self . settings ) logger . info ( 'APPLY SETTINGS ON %s' , self . index_name ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( 'SETTINGS NOT APPLIED ON %s: %s' , self . model , e )
3577	def disconnect_devices ( self , service_uuids ) : # Get list of connected devices with specified services. cbuuids = map ( uuid_to_cbuuid , service_uuids ) for device in self . _central_manager . retrieveConnectedPeripheralsWithServices_ ( cbuuids ) : self . _central_manager . cancelPeripheralConnection_ ( device )
12829	def parse_conll ( self , texts : List [ str ] , retry_count : int = 0 ) -> List [ str ] : post_data = { 'texts' : texts , 'output_type' : 'conll' } try : response = requests . post ( f'http://{self.hostname}:{self.port}' , json = post_data , headers = { 'Connection' : 'close' } ) response . raise_for_status ( ) except ( requests . exceptions . ConnectionError , requests . exceptions . Timeout ) as server_error : raise ServerError ( server_error , self . hostname , self . port ) except requests . exceptions . HTTPError as http_error : raise http_error else : try : return response . json ( ) except json . JSONDecodeError as json_exception : if retry_count == self . retries : self . log_error ( response . text ) raise Exception ( 'Json Decoding error cannot parse this ' f':\n{response.text}' ) return self . parse_conll ( texts , retry_count + 1 )
9666	def write_dot_file ( G , filename ) : with io . open ( filename , "w" ) as fh : fh . write ( "strict digraph DependencyDiagram {\n" ) edge_list = G . edges ( ) node_list = set ( G . nodes ( ) ) if edge_list : for edge in sorted ( edge_list ) : source , targ = edge node_list = node_list - set ( source ) node_list = node_list - set ( targ ) line = '"{}" -> "{}";\n' fh . write ( line . format ( source , targ ) ) # draw nodes with no links if node_list : for node in sorted ( node_list ) : line = '"{}"\n' . format ( node ) fh . write ( line ) fh . write ( "}" )
2648	def make_rundir ( path ) : try : if not os . path . exists ( path ) : os . makedirs ( path ) prev_rundirs = glob ( os . path . join ( path , "[0-9]*" ) ) current_rundir = os . path . join ( path , '000' ) if prev_rundirs : # Since we globbed on files named as 0-9 x = sorted ( [ int ( os . path . basename ( x ) ) for x in prev_rundirs ] ) [ - 1 ] current_rundir = os . path . join ( path , '{0:03}' . format ( x + 1 ) ) os . makedirs ( current_rundir ) logger . debug ( "Parsl run initializing in rundir: {0}" . format ( current_rundir ) ) return os . path . abspath ( current_rundir ) except Exception as e : logger . error ( "Failed to create a run directory" ) logger . error ( "Error: {0}" . format ( e ) ) raise
11788	def smooth_for ( self , o ) : if o not in self . dictionary : self . dictionary [ o ] = self . default self . n_obs += self . default self . sampler = None
449	def batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , data_format , name = None ) : with ops . name_scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : inv = math_ops . rsqrt ( variance + variance_epsilon ) if scale is not None : inv *= scale a = math_ops . cast ( inv , x . dtype ) b = math_ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) # Return a * x + b with customized data_format. # Currently TF doesn't have bias_scale, and tensorRT has bug in converting tf.nn.bias_add # So we reimplemted them to allow make the model work with tensorRT. # See https://github.com/tensorlayer/openpose-plus/issues/75 for more details. df = { 'channels_first' : 'NCHW' , 'channels_last' : 'NHWC' } return _bias_add ( _bias_scale ( x , a , df [ data_format ] ) , b , df [ data_format ] )
3699	def Tliquidus ( Tms = None , ws = None , xs = None , CASRNs = None , AvailableMethods = False , Method = None ) : # pragma: no cover def list_methods ( ) : methods = [ ] if none_and_length_check ( [ Tms ] ) : methods . append ( 'Maximum' ) methods . append ( 'Simple' ) methods . append ( 'None' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section if Method == 'Maximum' : _Tliq = max ( Tms ) elif Method == 'Simple' : _Tliq = mixing_simple ( xs , Tms ) elif Method == 'None' : return None else : raise Exception ( 'Failure in in function' ) return _Tliq
13442	def cmd_init_pull_from_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-pull-from-cloud]: %s => %s" % ( ccat , lcat ) ) if isfile ( lcat ) : args . error ( "[init-pull-from-cloud] The local catalog already exist: %s" % lcat ) if not isfile ( ccat ) : args . error ( "[init-pull-from-cloud] The cloud catalog does not exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-pull-from-cloud] The local meta-data already exist: %s" % lmeta ) if not isfile ( cmeta ) : args . error ( "[init-pull-from-cloud] The cloud meta-data does not exist: %s" % cmeta ) #Let's "lock" the local catalog logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) #Copy base from cloud to local util . copy ( ccat , lcat ) #Apply changesets cloudDAG = ChangesetDAG ( ccat ) path = cloudDAG . path ( cloudDAG . root . hash , cloudDAG . leafs [ 0 ] . hash ) util . apply_changesets ( args , path , lcat ) # Write meta-data both to local and cloud mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'filename' ] mfile [ 'last_push' ] [ 'hash' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'hash' ] mfile [ 'last_push' ] [ 'modification_utc' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'modification_utc' ] mfile . flush ( ) #Let's copy Smart Previews if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = False ) #Finally, let's unlock the catalog files logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-pull-from-cloud]: Success!" )
6048	def relocated_grid_stack_from_grid_stack ( self , grid_stack ) : border_grid = grid_stack . regular [ self ] return GridStack ( regular = self . relocated_grid_from_grid_jit ( grid = grid_stack . regular , border_grid = border_grid ) , sub = self . relocated_grid_from_grid_jit ( grid = grid_stack . sub , border_grid = border_grid ) , blurring = None , pix = self . relocated_grid_from_grid_jit ( grid = grid_stack . pix , border_grid = border_grid ) )
6506	def process_result ( cls , dictionary , match_phrase , user ) : result_processor = _load_class ( getattr ( settings , "SEARCH_RESULT_PROCESSOR" , None ) , cls ) srp = result_processor ( dictionary , match_phrase ) if srp . should_remove ( user ) : return None try : srp . add_properties ( ) # protect around any problems introduced by subclasses within their properties except Exception as ex : # pylint: disable=broad-except log . exception ( "error processing properties for %s - %s: will remove from results" , json . dumps ( dictionary , cls = DjangoJSONEncoder ) , str ( ex ) ) return None return dictionary
8302	def add ( self , callback , name ) : if callback == None : del self . callbacks [ name ] else : self . callbacks [ name ] = callback
698	def getParticleInfo ( self , modelId ) : entry = self . _allResults [ self . _modelIDToIdx [ modelId ] ] return ( entry [ 'modelParams' ] [ 'particleState' ] , modelId , entry [ 'errScore' ] , entry [ 'completed' ] , entry [ 'matured' ] )
11355	def record_xml_output ( rec , pretty = True ) : from . html_utils import MathMLParser ret = etree . tostring ( rec , xml_declaration = False ) # Special MathML handling ret = re . sub ( "(&lt;)(([\/]?{0}))" . format ( "|[\/]?" . join ( MathMLParser . mathml_elements ) ) , '<\g<2>' , ret ) ret = re . sub ( "&gt;" , '>' , ret ) if pretty : # We are doing our own prettyfication as etree pretty_print is too insane. ret = ret . replace ( '</datafield>' , ' </datafield>\n' ) ret = re . sub ( r'<datafield(.*?)>' , r' <datafield\1>\n' , ret ) ret = ret . replace ( '</subfield>' , '</subfield>\n' ) ret = ret . replace ( '<subfield' , ' <subfield' ) ret = ret . replace ( 'record>' , 'record>\n' ) return ret
5740	def main ( path , pid , queue ) : setup_logging ( ) if pid : with open ( os . path . expanduser ( pid ) , "w" ) as f : f . write ( str ( os . getpid ( ) ) ) if not path : path = os . getcwd ( ) sys . path . insert ( 0 , path ) queue = import_queue ( queue ) import psq worker = psq . Worker ( queue = queue ) worker . listen ( )
10872	def get_Kprefactor ( z , cos_theta , zint = 100.0 , n2n1 = 0.95 , get_hdet = False , * * kwargs ) : phase = f_theta ( cos_theta , zint , z , n2n1 = n2n1 , * * kwargs ) to_return = np . exp ( - 1j * phase ) if not get_hdet : to_return *= np . outer ( np . ones_like ( z ) , np . sqrt ( cos_theta ) ) return to_return
2586	def _command_server ( self , kill_event ) : logger . debug ( "[COMMAND] Command Server Starting" ) while not kill_event . is_set ( ) : try : command_req = self . command_channel . recv_pyobj ( ) logger . debug ( "[COMMAND] Received command request: {}" . format ( command_req ) ) if command_req == "OUTSTANDING_C" : outstanding = self . pending_task_queue . qsize ( ) for manager in self . _ready_manager_queue : outstanding += len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) reply = outstanding elif command_req == "WORKERS" : num_workers = 0 for manager in self . _ready_manager_queue : num_workers += self . _ready_manager_queue [ manager ] [ 'worker_count' ] reply = num_workers elif command_req == "MANAGERS" : reply = [ ] for manager in self . _ready_manager_queue : resp = { 'manager' : manager . decode ( 'utf-8' ) , 'block_id' : self . _ready_manager_queue [ manager ] [ 'block_id' ] , 'worker_count' : self . _ready_manager_queue [ manager ] [ 'worker_count' ] , 'tasks' : len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) , 'active' : self . _ready_manager_queue [ manager ] [ 'active' ] } reply . append ( resp ) elif command_req . startswith ( "HOLD_WORKER" ) : cmd , s_manager = command_req . split ( ';' ) manager = s_manager . encode ( 'utf-8' ) logger . info ( "[CMD] Received HOLD_WORKER for {}" . format ( manager ) ) if manager in self . _ready_manager_queue : self . _ready_manager_queue [ manager ] [ 'active' ] = False reply = True else : reply = False elif command_req == "SHUTDOWN" : logger . info ( "[CMD] Received SHUTDOWN command" ) kill_event . set ( ) reply = True else : reply = None logger . debug ( "[COMMAND] Reply: {}" . format ( reply ) ) self . command_channel . send_pyobj ( reply ) except zmq . Again : logger . debug ( "[COMMAND] is alive" ) continue
10779	def _remove_closest_particle ( self , p ) : #1. find closest pos: dp = self . pos - p dist2 = ( dp * dp ) . sum ( axis = 1 ) ind = dist2 . argmin ( ) rp = self . pos [ ind ] . copy ( ) #2. delete self . pos = np . delete ( self . pos , ind , axis = 0 ) return rp
10735	def ld_to_dl ( ld ) : if ld : keys = list ( ld [ 0 ] ) dl = { key : [ d [ key ] for d in ld ] for key in keys } return dl else : return { }
7268	def operator ( name = None , operators = None , aliases = None , kind = None ) : def delegator ( assertion , subject , expected , * args , * * kw ) : return assertion . test ( subject , expected , * args , * * kw ) def decorator ( fn ) : operator = Operator ( fn = fn , aliases = aliases , kind = kind ) _name = name if isinstance ( name , six . string_types ) else fn . __name__ operator . operators = ( _name , ) _operators = operators if isinstance ( _operators , list ) : _operators = tuple ( _operators ) if isinstance ( _operators , tuple ) : operator . operators += _operators # Register operator Engine . register ( operator ) return functools . partial ( delegator , operator ) return decorator ( name ) if inspect . isfunction ( name ) else decorator
9406	def _cleanup ( self ) : self . exit ( ) workspace = osp . join ( os . getcwd ( ) , 'octave-workspace' ) if osp . exists ( workspace ) : os . remove ( workspace )
1348	def write_success_response ( self , result ) : response = self . make_success_response ( result ) now = time . time ( ) spent = now - self . basehandler_starttime response [ constants . RESPONSE_KEY_EXECUTION_TIME ] = spent self . write_json_response ( response )
13789	def marv ( ctx , config , loglevel , logfilter , verbosity ) : if config is None : cwd = os . path . abspath ( os . path . curdir ) while cwd != os . path . sep : config = os . path . join ( cwd , 'marv.conf' ) if os . path . exists ( config ) : break cwd = os . path . dirname ( cwd ) else : config = '/etc/marv/marv.conf' if not os . path . exists ( config ) : config = None ctx . obj = config setup_logging ( loglevel , verbosity , logfilter )
11482	def _upload_folder_recursive ( local_folder , parent_folder_id , leaf_folders_as_items = False , reuse_existing = False ) : if leaf_folders_as_items and _has_only_files ( local_folder ) : print ( 'Creating item from {0}' . format ( local_folder ) ) _upload_folder_as_item ( local_folder , parent_folder_id , reuse_existing ) return else : # do not need to check if folder exists, if it does, an attempt to # create it will just return the existing id print ( 'Creating folder from {0}' . format ( local_folder ) ) new_folder_id = _create_or_reuse_folder ( local_folder , parent_folder_id , reuse_existing ) for entry in sorted ( os . listdir ( local_folder ) ) : full_entry = os . path . join ( local_folder , entry ) if os . path . islink ( full_entry ) : # os.walk skips symlinks by default continue elif os . path . isdir ( full_entry ) : _upload_folder_recursive ( full_entry , new_folder_id , leaf_folders_as_items , reuse_existing ) else : print ( 'Uploading item from {0}' . format ( full_entry ) ) _upload_as_item ( entry , new_folder_id , full_entry , reuse_existing )
1761	def read_string ( self , where , max_length = None , force = False ) : s = io . BytesIO ( ) while True : c = self . read_int ( where , 8 , force ) if issymbolic ( c ) or c == 0 : break if max_length is not None : if max_length == 0 : break max_length = max_length - 1 s . write ( Operators . CHR ( c ) ) where += 1 return s . getvalue ( ) . decode ( )
807	def disableTap ( self ) : if self . _tapFileIn is not None : self . _tapFileIn . close ( ) self . _tapFileIn = None if self . _tapFileOut is not None : self . _tapFileOut . close ( ) self . _tapFileOut = None
6359	def sim ( self , src , tar ) : if src == tar : return 1.0 elif not src or not tar : return 0.0 return len ( self . lcsstr ( src , tar ) ) / max ( len ( src ) , len ( tar ) )
1177	def search ( self , string , pos = 0 , endpos = sys . maxint ) : state = _State ( string , pos , endpos , self . flags ) if state . search ( self . _code ) : return SRE_Match ( self , state ) else : return None
874	def initStateFrom ( self , particleId , particleState , newBest ) : # Get the update best position and result? if newBest : ( bestResult , bestPosition ) = self . _resultsDB . getParticleBest ( particleId ) else : bestResult = bestPosition = None # Replace with the position and velocity of each variable from # saved state varStates = particleState [ 'varStates' ] for varName in varStates . keys ( ) : varState = copy . deepcopy ( varStates [ varName ] ) if newBest : varState [ 'bestResult' ] = bestResult if bestPosition is not None : varState [ 'bestPosition' ] = bestPosition [ varName ] self . permuteVars [ varName ] . setState ( varState )
13468	def normalize_slice ( slice_obj , length ) : if isinstance ( slice_obj , slice ) : start , stop , step = slice_obj . start , slice_obj . stop , slice_obj . step if start is None : start = 0 if stop is None : stop = length if step is None : step = 1 if start < 0 : start += length if stop < 0 : stop += length elif isinstance ( slice_obj , int ) : start = slice_obj if start < 0 : start += length stop = start + 1 step = 1 else : raise TypeError if ( 0 <= start <= length ) and ( 0 <= stop <= length ) : return start , stop , step raise IndexError
13009	def format ( ) : argparser = argparse . ArgumentParser ( description = 'Formats a json object in a certain way. Use with pipes.' ) argparser . add_argument ( 'format' , metavar = 'format' , help = 'How to format the json for example "{address}:{port}".' , nargs = '?' ) arguments = argparser . parse_args ( ) service_style = "{address:15} {port:7} {protocol:5} {service:15} {state:10} {banner} {tags}" host_style = "{address:15} {tags}" ranges_style = "{range:18} {tags}" users_style = "{username}" if arguments . format : format_input ( arguments . format ) else : doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : for obj in doc_mapper . get_pipe ( ) : style = '' if isinstance ( obj , Range ) : style = ranges_style elif isinstance ( obj , Host ) : style = host_style elif isinstance ( obj , Service ) : style = service_style elif isinstance ( obj , User ) : style = users_style print_line ( fmt . format ( style , * * obj . to_dict ( include_meta = True ) ) ) else : print_error ( "Please use this script with pipes" )
1636	def CheckForFunctionLengths ( filename , clean_lines , linenum , function_state , error ) : lines = clean_lines . lines line = lines [ linenum ] joined_line = '' starting_func = False regexp = r'(\w(\w|::|\*|\&|\s)*)\(' # decls * & space::name( ... match_result = Match ( regexp , line ) if match_result : # If the name is all caps and underscores, figure it's a macro and # ignore it, unless it's TEST or TEST_F. function_name = match_result . group ( 1 ) . split ( ) [ - 1 ] if function_name == 'TEST' or function_name == 'TEST_F' or ( not Match ( r'[A-Z_]+$' , function_name ) ) : starting_func = True if starting_func : body_found = False for start_linenum in range ( linenum , clean_lines . NumLines ( ) ) : start_line = lines [ start_linenum ] joined_line += ' ' + start_line . lstrip ( ) if Search ( r'(;|})' , start_line ) : # Declarations and trivial functions body_found = True break # ... ignore elif Search ( r'{' , start_line ) : body_found = True function = Search ( r'((\w|:)*)\(' , line ) . group ( 1 ) if Match ( r'TEST' , function ) : # Handle TEST... macros parameter_regexp = Search ( r'(\(.*\))' , joined_line ) if parameter_regexp : # Ignore bad syntax function += parameter_regexp . group ( 1 ) else : function += '()' function_state . Begin ( function ) break if not body_found : # No body for the function (or evidence of a non-function) was found. error ( filename , linenum , 'readability/fn_size' , 5 , 'Lint failed to find start of function body.' ) elif Match ( r'^\}\s*$' , line ) : # function end function_state . Check ( error , filename , linenum ) function_state . End ( ) elif not Match ( r'^\s*$' , line ) : function_state . Count ( )
11022	def get_node ( self , string_key ) : pos = self . get_node_pos ( string_key ) if pos is None : return None return self . ring [ self . _sorted_keys [ pos ] ]
9651	def check_shastore_version ( from_store , settings ) : sprint = settings [ "sprint" ] error = settings [ "error" ] sprint ( "checking .shastore version for potential incompatibilities" , level = "verbose" ) if not from_store or 'sake version' not in from_store : errmes = [ "Since you've used this project last, a new version of " , "sake was installed that introduced backwards incompatible" , " changes. Run 'sake clean', and rebuild before continuing\n" ] errmes = " " . join ( errmes ) error ( errmes ) sys . exit ( 1 )
5002	def _assign_enterprise_role_to_users ( self , _get_batch_method , options , is_feature_role = False ) : role_name = options [ 'role' ] batch_limit = options [ 'batch_limit' ] batch_sleep = options [ 'batch_sleep' ] batch_offset = options [ 'batch_offset' ] current_batch_index = batch_offset users_batch = _get_batch_method ( batch_offset , batch_offset + batch_limit ) role_class = SystemWideEnterpriseRole role_assignment_class = SystemWideEnterpriseUserRoleAssignment if is_feature_role : role_class = EnterpriseFeatureRole role_assignment_class = EnterpriseFeatureUserRoleAssignment enterprise_role = role_class . objects . get ( name = role_name ) while users_batch . count ( ) > 0 : for index , user in enumerate ( users_batch ) : LOGGER . info ( 'Processing user with index %s and id %s' , current_batch_index + index , user . id ) role_assignment_class . objects . get_or_create ( user = user , role = enterprise_role ) sleep ( batch_sleep ) current_batch_index += len ( users_batch ) users_batch = _get_batch_method ( current_batch_index , current_batch_index + batch_limit )
7787	def _try_backup_item ( self ) : if not self . _backup_state : return False item = self . cache . get_item ( self . address , self . _backup_state ) if item : self . _object_handler ( item . address , item . value , item . state ) return True else : False
5347	def compose_github ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'github_repos' ] ) > 0 ] : if 'github' not in projects [ p ] : projects [ p ] [ 'github' ] = [ ] urls = [ url [ 'url' ] for url in data [ p ] [ 'github_repos' ] if url [ 'url' ] not in projects [ p ] [ 'github' ] ] projects [ p ] [ 'github' ] += urls return projects
355	def save_npz_dict ( save_list = None , name = 'model.npz' , sess = None ) : if sess is None : raise ValueError ( "session is None." ) if save_list is None : save_list = [ ] save_list_names = [ tensor . name for tensor in save_list ] save_list_var = sess . run ( save_list ) save_var_dict = { save_list_names [ idx ] : val for idx , val in enumerate ( save_list_var ) } np . savez ( name , * * save_var_dict ) save_list_var = None save_var_dict = None del save_list_var del save_var_dict logging . info ( "[*] Model saved in npz_dict %s" % name )
819	def updateRow ( self , row , distribution ) : self . grow ( row + 1 , len ( distribution ) ) self . hist_ . axby ( row , 1 , 1 , distribution ) self . rowSums_ [ row ] += distribution . sum ( ) self . colSums_ += distribution self . hack_ = None
2086	def get ( self , pk ) : # The Tower API doesn't provide a mechanism for retrieving a single # setting value at a time, so fetch them all and filter try : return next ( s for s in self . list ( ) [ 'results' ] if s [ 'id' ] == pk ) except StopIteration : raise exc . NotFound ( 'The requested object could not be found.' )
11478	def _streaming_file_md5 ( file_path ) : md5 = hashlib . md5 ( ) with open ( file_path , 'rb' ) as f : # iter needs an empty byte string for the returned iterator to halt at # EOF for chunk in iter ( lambda : f . read ( 128 * md5 . block_size ) , b'' ) : md5 . update ( chunk ) return md5 . hexdigest ( )
10497	def doubleClickMouse ( self , coord ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) # This is a kludge: # If directed towards a Fusion VM the clickCount gets ignored and this # will be seen as a single click, so in sequence this will be a double- # click # Otherwise to a host app only this second one will count as a double- # click self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , clickCount = 2 ) self . _postQueuedEvents ( )
2145	def request ( self , method , url , * args , * * kwargs ) : # If the URL has the api/vX at the front strip it off # This is common to have if you are extracting a URL from an existing object. # For example, any of the 'related' fields of an object will have this import re url = re . sub ( "^/?api/v[0-9]+/" , "" , url ) # Piece together the full URL. use_version = not url . startswith ( '/o/' ) url = '%s%s' % ( self . get_prefix ( use_version ) , url . lstrip ( '/' ) ) # Ansible Tower expects authenticated requests; add the authentication # from settings if it's provided. kwargs . setdefault ( 'auth' , BasicTowerAuth ( settings . username , settings . password , self ) ) # POST and PUT requests will send JSON by default; make this # the content_type by default. This makes it such that we don't have # to constantly write that in our code, which gets repetitive. headers = kwargs . get ( 'headers' , { } ) if method . upper ( ) in ( 'PATCH' , 'POST' , 'PUT' ) : headers . setdefault ( 'Content-Type' , 'application/json' ) kwargs [ 'headers' ] = headers # If debugging is on, print the URL and data being sent. debug . log ( '%s %s' % ( method , url ) , fg = 'blue' , bold = True ) if method in ( 'POST' , 'PUT' , 'PATCH' ) : debug . log ( 'Data: %s' % kwargs . get ( 'data' , { } ) , fg = 'blue' , bold = True ) if method == 'GET' or kwargs . get ( 'params' , None ) : debug . log ( 'Params: %s' % kwargs . get ( 'params' , { } ) , fg = 'blue' , bold = True ) debug . log ( '' ) # If this is a JSON request, encode the data value. if headers . get ( 'Content-Type' , '' ) == 'application/json' : kwargs [ 'data' ] = json . dumps ( kwargs . get ( 'data' , { } ) ) r = self . _make_request ( method , url , args , kwargs ) # Sanity check: Did the server send back some kind of internal error? # If so, bubble this up. if r . status_code >= 500 : raise exc . ServerError ( 'The Tower server sent back a server error. ' 'Please try again later.' ) # Sanity check: Did we fail to authenticate properly? # If so, fail out now; this is always a failure. if r . status_code == 401 : raise exc . AuthError ( 'Invalid Tower authentication credentials (HTTP 401).' ) # Sanity check: Did we get a forbidden response, which means that # the user isn't allowed to do this? Report that. if r . status_code == 403 : raise exc . Forbidden ( "You don't have permission to do that (HTTP 403)." ) # Sanity check: Did we get a 404 response? # Requests with primary keys will return a 404 if there is no response, # and we want to consistently trap these. if r . status_code == 404 : raise exc . NotFound ( 'The requested object could not be found.' ) # Sanity check: Did we get a 405 response? # A 405 means we used a method that isn't allowed. Usually this # is a bad request, but it requires special treatment because the # API sends it as a logic error in a few situations (e.g. trying to # cancel a job that isn't running). if r . status_code == 405 : raise exc . MethodNotAllowed ( "The Tower server says you can't make a request with the " "%s method to that URL (%s)." % ( method , url ) , ) # Sanity check: Did we get some other kind of error? # If so, write an appropriate error message. if r . status_code >= 400 : raise exc . BadRequest ( 'The Tower server claims it was sent a bad request.\n\n' '%s %s\nParams: %s\nData: %s\n\nResponse: %s' % ( method , url , kwargs . get ( 'params' , None ) , kwargs . get ( 'data' , None ) , r . content . decode ( 'utf8' ) ) ) # Django REST Framework intelligently prints API keys in the # order that they are defined in the models and serializer. # # We want to preserve this behavior when it is possible to do so # with minimal effort, because while the order has no explicit meaning, # we make some effort to order keys in a convenient manner. # # To this end, make this response into an APIResponse subclass # (defined below), which has a `json` method that doesn't lose key # order. r . __class__ = APIResponse # Return the response object. return r
1182	def fast_search ( self , pattern_codes ) : # pattern starts with a known prefix # <5=length> <6=skip> <7=prefix data> <overlap data> flags = pattern_codes [ 2 ] prefix_len = pattern_codes [ 5 ] prefix_skip = pattern_codes [ 6 ] # don't really know what this is good for prefix = pattern_codes [ 7 : 7 + prefix_len ] overlap = pattern_codes [ 7 + prefix_len - 1 : pattern_codes [ 1 ] + 1 ] pattern_codes = pattern_codes [ pattern_codes [ 1 ] + 1 : ] i = 0 string_position = self . string_position while string_position < self . end : while True : if ord ( self . string [ string_position ] ) != prefix [ i ] : if i == 0 : break else : i = overlap [ i ] else : i += 1 if i == prefix_len : # found a potential match self . start = string_position + 1 - prefix_len self . string_position = string_position + 1 - prefix_len + prefix_skip if flags & SRE_INFO_LITERAL : return True # matched all of pure literal pattern if self . match ( pattern_codes [ 2 * prefix_skip : ] ) : return True i = overlap [ i ] break string_position += 1 return False
9458	def sound_touch_stop ( self , call_params ) : path = '/' + self . api_version + '/SoundTouchStop/' method = 'POST' return self . request ( path , method , call_params )
1433	def custom ( cls , customgrouper ) : if customgrouper is None : raise TypeError ( "Argument to custom() must be ICustomGrouping instance or classpath" ) if not isinstance ( customgrouper , ICustomGrouping ) and not isinstance ( customgrouper , str ) : raise TypeError ( "Argument to custom() must be ICustomGrouping instance or classpath" ) serialized = default_serializer . serialize ( customgrouper ) return cls . custom_serialized ( serialized , is_java = False )
7084	def _make_magseries_plot ( axes , stimes , smags , serrs , magsarefluxes = False , ms = 2.0 ) : scaledplottime = stimes - npmin ( stimes ) axes . plot ( scaledplottime , smags , marker = 'o' , ms = ms , ls = 'None' , mew = 0 , color = 'green' , rasterized = True ) # flip y axis for mags if not magsarefluxes : plot_ylim = axes . get_ylim ( ) axes . set_ylim ( ( plot_ylim [ 1 ] , plot_ylim [ 0 ] ) ) # set the x axis limit axes . set_xlim ( ( npmin ( scaledplottime ) - 1.0 , npmax ( scaledplottime ) + 1.0 ) ) # make a grid axes . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) # make the x and y axis labels plot_xlabel = 'JD - %.3f' % npmin ( stimes ) if magsarefluxes : plot_ylabel = 'flux' else : plot_ylabel = 'magnitude' axes . set_xlabel ( plot_xlabel ) axes . set_ylabel ( plot_ylabel ) # fix the yaxis ticks (turns off offset and uses the full # value of the yaxis tick) axes . get_yaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) axes . get_xaxis ( ) . get_major_formatter ( ) . set_useOffset ( False )
8711	def __read_chunk ( self , buf ) : log . debug ( 'reading chunk' ) timeout_before = self . _port . timeout if SYSTEM != 'Windows' : # Checking for new data every 100us is fast enough if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout_before while len ( buf ) < 130 and time . time ( ) <= end : buf = buf + self . _port . read ( ) if buf [ 0 ] != BLOCK_START or len ( buf ) < 130 : log . debug ( 'buffer binary: %s ' , hexify ( buf ) ) raise Exception ( 'Bad blocksize or start byte' ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before chunk_size = ord ( buf [ 1 ] ) data = buf [ 2 : chunk_size + 2 ] buf = buf [ 130 : ] return ( data , buf )
4114	def rc2lar ( k ) : assert numpy . isrealobj ( k ) , 'Log area ratios not defined for complex reflection coefficients.' if max ( numpy . abs ( k ) ) >= 1 : raise ValueError ( 'All reflection coefficients should have magnitude less than unity.' ) # Use the relation, atanh(x) = (1/2)*log((1+k)/(1-k)) return - 2 * numpy . arctanh ( - numpy . array ( k ) )
11977	def get_bits ( self ) : return _convert ( self . _ip , notation = NM_BITS , inotation = IP_DOT , _check = False , _isnm = self . _isnm )
370	def flip_axis ( x , axis = 1 , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : x = np . asarray ( x ) . swapaxes ( axis , 0 ) x = x [ : : - 1 , ... ] x = x . swapaxes ( 0 , axis ) return x else : return x else : x = np . asarray ( x ) . swapaxes ( axis , 0 ) x = x [ : : - 1 , ... ] x = x . swapaxes ( 0 , axis ) return x
13191	def json_struct_to_xml ( json_obj , root , custom_namespace = None ) : if isinstance ( root , ( str , unicode ) ) : if root . startswith ( '!' ) : root = etree . Element ( '{%s}%s' % ( NS_PROTECTED , root [ 1 : ] ) ) elif root . startswith ( '+' ) : if not custom_namespace : raise Exception ( "JSON fields starts with +, but no custom namespace provided" ) root = etree . Element ( '{%s}%s' % ( custom_namespace , root [ 1 : ] ) ) else : root = etree . Element ( root ) if root . tag in ( 'attachments' , 'grouped_events' , 'media_files' ) : for link in json_obj : root . append ( json_link_to_xml ( link ) ) elif isinstance ( json_obj , ( str , unicode ) ) : root . text = json_obj elif isinstance ( json_obj , ( int , float ) ) : root . text = unicode ( json_obj ) elif isinstance ( json_obj , dict ) : if frozenset ( json_obj . keys ( ) ) == frozenset ( ( 'type' , 'coordinates' ) ) : root . append ( geojson_to_gml ( json_obj ) ) else : for key , val in json_obj . items ( ) : if key == 'url' or key . endswith ( '_url' ) : el = json_link_to_xml ( val , json_link_key_to_xml_rel ( key ) ) else : el = json_struct_to_xml ( val , key , custom_namespace = custom_namespace ) if el is not None : root . append ( el ) elif isinstance ( json_obj , list ) : tag_name = root . tag if tag_name . endswith ( 'ies' ) : tag_name = tag_name [ : - 3 ] + 'y' elif tag_name . endswith ( 's' ) : tag_name = tag_name [ : - 1 ] for val in json_obj : el = json_struct_to_xml ( val , tag_name , custom_namespace = custom_namespace ) if el is not None : root . append ( el ) elif json_obj is None : return None else : raise NotImplementedError return root
202	def resize ( self , sizes , interpolation = "cubic" ) : arr_resized = ia . imresize_single_image ( self . arr , sizes , interpolation = interpolation ) # cubic interpolation can lead to values outside of [0.0, 1.0], # see https://github.com/opencv/opencv/issues/7195 # TODO area interpolation too? arr_resized = np . clip ( arr_resized , 0.0 , 1.0 ) segmap = SegmentationMapOnImage ( arr_resized , shape = self . shape ) segmap . input_was = self . input_was return segmap
7399	def swap ( self , qs ) : try : replacement = qs [ 0 ] except IndexError : # already first/last return if not self . _valid_ordering_reference ( replacement ) : raise ValueError ( "%r can only be swapped with instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) self . order , replacement . order = replacement . order , self . order self . save ( ) replacement . save ( )
12092	def proto_02_03_IVfast ( abf = exampleABF ) : av1 , sd1 = swhlab . plot . IV ( abf , .6 , .9 , True ) swhlab . plot . save ( abf , tag = 'iv1' ) Xs = abf . clampValues ( .6 ) #generate IV clamp values abf . saveThing ( [ Xs , av1 ] , 'iv' )
6488	def _get_filter_field ( field_name , field_value ) : filter_field = None if isinstance ( field_value , ValueRange ) : range_values = { } if field_value . lower : range_values . update ( { "gte" : field_value . lower_string } ) if field_value . upper : range_values . update ( { "lte" : field_value . upper_string } ) filter_field = { "range" : { field_name : range_values } } elif _is_iterable ( field_value ) : filter_field = { "terms" : { field_name : field_value } } else : filter_field = { "term" : { field_name : field_value } } return filter_field
5891	def smart_str ( string , encoding = 'utf-8' , strings_only = False , errors = 'strict' ) : if strings_only and isinstance ( string , ( type ( None ) , int ) ) : return string # if isinstance(s, Promise): # return unicode(s).encode(encoding, errors) if isinstance ( string , str ) : try : return string . encode ( encoding , errors ) except UnicodeEncodeError : return string . encode ( 'utf-8' , errors ) elif not isinstance ( string , bytes ) : try : return str ( string ) . encode ( encoding , errors ) except UnicodeEncodeError : if isinstance ( string , Exception ) : # An Exception subclass containing non-ASCII data that doesn't # know how to print itself properly. We shouldn't raise a # further exception. return ' ' . join ( [ smart_str ( arg , encoding , strings_only , errors ) for arg in string ] ) return str ( string ) . encode ( encoding , errors ) else : return string
7165	def add_entity ( self , name , lines , reload_cache = False ) : Entity . verify_name ( name ) self . entities . add ( Entity . wrap_name ( name ) , lines , reload_cache ) self . padaos . add_entity ( name , lines ) self . must_train = True
10108	def normalize_name ( s ) : s = s . replace ( '-' , '_' ) . replace ( '.' , '_' ) . replace ( ' ' , '_' ) if s in keyword . kwlist : return s + '_' s = '_' . join ( slug ( ss , lowercase = False ) for ss in s . split ( '_' ) ) if not s : s = '_' if s [ 0 ] not in string . ascii_letters + '_' : s = '_' + s return s
9361	def characters ( quantity = 10 ) : line = map ( _to_lower_alpha_only , '' . join ( random . sample ( get_dictionary ( 'lorem_ipsum' ) , quantity ) ) ) return '' . join ( line ) [ : quantity ]
9932	def get_refkey ( self , obj , referent ) : if isinstance ( obj , dict ) : for k , v in obj . items ( ) : if v is referent : return " (via its %r key)" % k for k in dir ( obj ) + [ '__dict__' ] : if getattr ( obj , k , None ) is referent : return " (via its %r attribute)" % k return ""
9960	def get_object ( self , name ) : parts = name . split ( "." ) model_name = parts . pop ( 0 ) return self . models [ model_name ] . get_object ( "." . join ( parts ) )
9726	async def send_xml ( self , xml ) : return await asyncio . wait_for ( self . _protocol . send_command ( xml , command_type = QRTPacketType . PacketXML ) , timeout = self . _timeout , )
5514	def async_enterable ( f ) : @ functools . wraps ( f ) def wrapper ( * args , * * kwargs ) : class AsyncEnterableInstance : async def __aenter__ ( self ) : self . context = await f ( * args , * * kwargs ) return await self . context . __aenter__ ( ) async def __aexit__ ( self , * args , * * kwargs ) : await self . context . __aexit__ ( * args , * * kwargs ) def __await__ ( self ) : return f ( * args , * * kwargs ) . __await__ ( ) return AsyncEnterableInstance ( ) return wrapper
11947	def jocker ( test_options = None ) : version = ver_check ( ) options = test_options or docopt ( __doc__ , version = version ) _set_global_verbosity_level ( options . get ( '--verbose' ) ) jocker_lgr . debug ( options ) jocker_run ( options )
2221	def _rectify_hasher ( hasher ) : if xxhash is not None : # pragma: nobranch if hasher in { 'xxh32' , 'xx32' , 'xxhash' } : return xxhash . xxh32 if hasher in { 'xxh64' , 'xx64' } : return xxhash . xxh64 if hasher is NoParam or hasher == 'default' : hasher = DEFAULT_HASHER elif isinstance ( hasher , six . string_types ) : if hasher not in hashlib . algorithms_available : raise KeyError ( 'unknown hasher: {}' . format ( hasher ) ) else : hasher = getattr ( hashlib , hasher ) elif isinstance ( hasher , HASH ) : # by default the result of this function is a class we will make an # instance of, if we already have an instance, wrap it in a callable # so the external syntax does not need to change. return lambda : hasher return hasher
4163	def parse_sphinx_searchindex ( searchindex ) : # Make sure searchindex uses UTF-8 encoding if hasattr ( searchindex , 'decode' ) : searchindex = searchindex . decode ( 'UTF-8' ) # parse objects query = 'objects:' pos = searchindex . find ( query ) if pos < 0 : raise ValueError ( '"objects:" not found in search index' ) sel = _select_block ( searchindex [ pos : ] , '{' , '}' ) objects = _parse_dict_recursive ( sel ) # parse filenames query = 'filenames:' pos = searchindex . find ( query ) if pos < 0 : raise ValueError ( '"filenames:" not found in search index' ) filenames = searchindex [ pos + len ( query ) + 1 : ] filenames = filenames [ : filenames . find ( ']' ) ] filenames = [ f . strip ( '"' ) for f in filenames . split ( ',' ) ] return filenames , objects
10444	def getobjectproperty ( self , window_name , object_name , prop ) : try : obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) except atomac . _a11y . ErrorInvalidUIElement : # During the test, when the window closed and reopened # ErrorInvalidUIElement exception will be thrown self . _windows = { } # Call the method again, after updating apps obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) if obj_info and prop != "obj" and prop in obj_info : if prop == "class" : # ldtp_class_type are compatible with Linux and Windows class name # If defined class name exist return that, # else return as it is return ldtp_class_type . get ( obj_info [ prop ] , obj_info [ prop ] ) else : return obj_info [ prop ] raise LdtpServerException ( 'Unknown property "%s" in %s' % ( prop , object_name ) )
10348	def export_namespace ( graph , namespace , directory = None , cacheable = False ) : directory = os . getcwd ( ) if directory is None else directory path = os . path . join ( directory , '{}.belns' . format ( namespace ) ) with open ( path , 'w' ) as file : log . info ( 'Outputting to %s' , path ) right_names = get_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d correct names in %s' , len ( right_names ) , namespace ) wrong_names = get_incorrect_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d incorrect names in %s' , len ( right_names ) , namespace ) undefined_ns_names = get_undefined_namespace_names ( graph , namespace ) log . info ( 'Graph has %d names in missing namespace %s' , len ( right_names ) , namespace ) names = ( right_names | wrong_names | undefined_ns_names ) if 0 == len ( names ) : log . warning ( '%s is empty' , namespace ) write_namespace ( namespace_name = namespace , namespace_keyword = namespace , namespace_domain = 'Other' , author_name = graph . authors , author_contact = graph . contact , citation_name = graph . name , values = names , cacheable = cacheable , file = file )
9296	def get_database ( self , model ) : for router in self . routers : r = router . get_database ( model ) if r is not None : return r return self . get ( 'default' )
11916	def render_to ( self , path , template , * * data ) : html = self . render ( template , * * data ) with open ( path , 'w' ) as f : f . write ( html . encode ( charset ) )
10272	def get_unweighted_sources ( graph : BELGraph , key : Optional [ str ] = None ) -> Iterable [ BaseEntity ] : if key is None : key = WEIGHT for node in graph : if is_unweighted_source ( graph , node , key ) : yield node
6715	def install ( packages , upgrade = False , use_sudo = False , python_cmd = 'python' ) : argv = [ ] if upgrade : argv . append ( "-U" ) if isinstance ( packages , six . string_types ) : argv . append ( packages ) else : argv . extend ( packages ) _easy_install ( argv , python_cmd , use_sudo )
9136	def get_modules ( ) -> Mapping : modules = { } for entry_point in iter_entry_points ( group = 'bio2bel' , name = None ) : entry = entry_point . name try : modules [ entry ] = entry_point . load ( ) except VersionConflict as exc : log . warning ( 'Version conflict in %s: %s' , entry , exc ) continue except UnknownExtra as exc : log . warning ( 'Unknown extra in %s: %s' , entry , exc ) continue except ImportError as exc : log . exception ( 'Issue with importing module %s: %s' , entry , exc ) continue return modules
10866	def update ( self , params , values ) : # radparams = self.param_radii() params = listify ( params ) values = listify ( values ) for i , p in enumerate ( params ) : # if (p in radparams) & (values[i] < 0): if ( p [ - 2 : ] == '-a' ) and ( values [ i ] < 0 ) : values [ i ] = 0.0 super ( PlatonicSpheresCollection , self ) . update ( params , values )
12298	def discover_all_plugins ( self ) : for v in pkg_resources . iter_entry_points ( 'dgit.plugins' ) : m = v . load ( ) m . setup ( self )
10066	def file_serializer ( obj ) : return { "id" : str ( obj . file_id ) , "filename" : obj . key , "filesize" : obj . file . size , "checksum" : obj . file . checksum , }
9011	def index_of_first_consumed_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_consumed_meshes else : self . _raise_not_found_error ( ) return index
5613	def reproject_geometry ( geometry , src_crs = None , dst_crs = None , error_on_clip = False , validity_check = True , antimeridian_cutting = False ) : src_crs = _validated_crs ( src_crs ) dst_crs = _validated_crs ( dst_crs ) def _reproject_geom ( geometry , src_crs , dst_crs ) : if geometry . is_empty : return geometry else : out_geom = to_shape ( transform_geom ( src_crs . to_dict ( ) , dst_crs . to_dict ( ) , mapping ( geometry ) , antimeridian_cutting = antimeridian_cutting ) ) return _repair ( out_geom ) if validity_check else out_geom # return repaired geometry if no reprojection needed if src_crs == dst_crs or geometry . is_empty : return _repair ( geometry ) # geometry needs to be clipped to its CRS bounds elif ( dst_crs . is_epsg_code and # just in case for an CRS with EPSG code dst_crs . get ( "init" ) in CRS_BOUNDS and # if CRS has defined bounds dst_crs . get ( "init" ) != "epsg:4326" # and is not WGS84 (does not need clipping) ) : wgs84_crs = CRS ( ) . from_epsg ( 4326 ) # get dst_crs boundaries crs_bbox = box ( * CRS_BOUNDS [ dst_crs . get ( "init" ) ] ) # reproject geometry to WGS84 geometry_4326 = _reproject_geom ( geometry , src_crs , wgs84_crs ) # raise error if geometry has to be clipped if error_on_clip and not geometry_4326 . within ( crs_bbox ) : raise RuntimeError ( "geometry outside target CRS bounds" ) # clip geometry dst_crs boundaries and return return _reproject_geom ( crs_bbox . intersection ( geometry_4326 ) , wgs84_crs , dst_crs ) # return without clipping if destination CRS does not have defined bounds else : return _reproject_geom ( geometry , src_crs , dst_crs )
7740	def hold_exception ( method ) : @ functools . wraps ( method ) def wrapper ( self , * args , * * kwargs ) : """Wrapper for methods decorated with `hold_exception`.""" # pylint: disable=W0703,W0212 try : return method ( self , * args , * * kwargs ) except Exception : if self . exc_info : raise if not self . _stack : logger . debug ( '@hold_exception wrapped method {0!r} called' ' from outside of the main loop' . format ( method ) ) raise self . exc_info = sys . exc_info ( ) logger . debug ( u"exception in glib main loop callback:" , exc_info = self . exc_info ) # pylint: disable=W0212 main_loop = self . _stack [ - 1 ] if main_loop is not None : main_loop . quit ( ) return False return wrapper
5854	def create_dataset ( self , name = None , description = None , public = False ) : data = { "public" : _convert_bool_to_public_value ( public ) } if name : data [ "name" ] = name if description : data [ "description" ] = description dataset = { "dataset" : data } failure_message = "Unable to create dataset" result = self . _get_success_json ( self . _post_json ( routes . create_dataset ( ) , dataset , failure_message = failure_message ) ) return _dataset_from_response_dict ( result )
3016	def from_json_keyfile_name ( cls , filename , scopes = '' , token_uri = None , revoke_uri = None ) : with open ( filename , 'r' ) as file_obj : client_credentials = json . load ( file_obj ) return cls . _from_parsed_json_keyfile ( client_credentials , scopes , token_uri = token_uri , revoke_uri = revoke_uri )
7215	def list ( self ) : r = self . gbdx_connection . get ( self . _base_url ) raise_for_status ( r ) return r . json ( ) [ 'tasks' ]
8211	def insert_point ( self , x , y ) : try : bezier = _ctx . ximport ( "bezier" ) except : from nodebox . graphics import bezier # Do a number of checks distributed along the path. # Keep the one closest to the actual mouse location. # Ten checks works fast but leads to imprecision in sharp corners # and curves closely located next to each other. # I prefer the slower but more stable approach. n = 100 closest = None dx0 = float ( "inf" ) dy0 = float ( "inf" ) for i in range ( n ) : t = float ( i ) / n pt = self . path . point ( t ) dx = abs ( pt . x - x ) dy = abs ( pt . y - y ) if dx + dy <= dx0 + dy0 : dx0 = dx dy0 = dy closest = t # Next, scan the area around the approximation. # If the closest point is located at 0.2 on the path, # we need to scan between 0.1 and 0.3 for a better # approximation. If 1.5 was the best guess, scan # 1.40, 1.41 ... 1.59 and so on. # Each decimal precision takes 20 iterations. decimals = [ 3 , 4 ] for d in decimals : d = 1.0 / pow ( 10 , d ) for i in range ( 20 ) : t = closest - d + float ( i ) * d * 0.1 if t < 0.0 : t = 1.0 + t if t > 1.0 : t = t - 1.0 pt = self . path . point ( t ) dx = abs ( pt . x - x ) dy = abs ( pt . y - y ) if dx <= dx0 and dy <= dy0 : dx0 = dx dy0 = dy closest_precise = t closest = closest_precise # Update the points list with the inserted point. p = bezier . insert_point ( self . path , closest_precise ) i , t , pt = bezier . _locate ( self . path , closest_precise ) i += 1 pt = PathElement ( ) pt . cmd = p [ i ] . cmd pt . x = p [ i ] . x pt . y = p [ i ] . y pt . ctrl1 = Point ( p [ i ] . ctrl1 . x , p [ i ] . ctrl1 . y ) pt . ctrl2 = Point ( p [ i ] . ctrl2 . x , p [ i ] . ctrl2 . y ) pt . freehand = False self . _points . insert ( i , pt ) self . _points [ i - 1 ] . ctrl1 = Point ( p [ i - 1 ] . ctrl1 . x , p [ i - 1 ] . ctrl1 . y ) self . _points [ i + 1 ] . ctrl1 = Point ( p [ i + 1 ] . ctrl1 . x , p [ i + 1 ] . ctrl1 . y ) self . _points [ i + 1 ] . ctrl2 = Point ( p [ i + 1 ] . ctrl2 . x , p [ i + 1 ] . ctrl2 . y )
705	def recordModelProgress ( self , modelID , modelParams , modelParamsHash , results , completed , completionReason , matured , numRecords ) : if results is None : metricResult = None else : metricResult = results [ 1 ] . values ( ) [ 0 ] # Update our database. errScore = self . _resultsDB . update ( modelID = modelID , modelParams = modelParams , modelParamsHash = modelParamsHash , metricResult = metricResult , completed = completed , completionReason = completionReason , matured = matured , numRecords = numRecords ) # Log message. self . logger . debug ( 'Received progress on model %d: completed: %s, ' 'cmpReason: %s, numRecords: %d, errScore: %s' , modelID , completed , completionReason , numRecords , errScore ) # Log best so far. ( bestModelID , bestResult ) = self . _resultsDB . bestModelIdAndErrScore ( ) self . logger . debug ( 'Best err score seen so far: %s on model %s' % ( bestResult , bestModelID ) )
12275	def iso_reference_isvalid ( ref ) : ref = str ( ref ) cs_source = ref [ 4 : ] + ref [ : 4 ] return ( iso_reference_str2int ( cs_source ) % 97 ) == 1
12171	def count ( self , event ) : return len ( self . _listeners [ event ] ) + len ( self . _once [ event ] )
5241	def market_normal ( self , session , after_open , before_close ) -> Session : logger = logs . get_logger ( self . market_normal ) if session not in self . exch : return SessNA ss = self . exch [ session ] s_time = shift_time ( ss [ 0 ] , int ( after_open ) + 1 ) e_time = shift_time ( ss [ - 1 ] , - int ( before_close ) ) request_cross = pd . Timestamp ( s_time ) >= pd . Timestamp ( e_time ) session_cross = pd . Timestamp ( ss [ 0 ] ) >= pd . Timestamp ( ss [ 1 ] ) if request_cross and ( not session_cross ) : logger . warning ( f'end time {e_time} is earlier than {s_time} ...' ) return SessNA return Session ( s_time , e_time )
10354	def get_subgraph_by_node_search ( graph : BELGraph , query : Strings ) -> BELGraph : nodes = search_node_names ( graph , query ) return get_subgraph_by_induction ( graph , nodes )
8543	def _get_password ( self , password , use_config = True , config_filename = None , use_keyring = HAS_KEYRING ) : if not password and use_config : if self . _config is None : self . _read_config ( config_filename ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if not password and use_keyring : logger = logging . getLogger ( __name__ ) question = ( "Please enter your password for {} on {}: " . format ( self . username , self . host_base ) ) if HAS_KEYRING : password = keyring . get_password ( self . keyring_identificator , self . username ) if password is None : password = getpass . getpass ( question ) try : keyring . set_password ( self . keyring_identificator , self . username , password ) except keyring . errors . PasswordSetError as error : logger . warning ( "Storing password in keyring '%s' failed: %s" , self . keyring_identificator , error ) else : logger . warning ( "Install the 'keyring' Python module to store your password " "securely in your keyring!" ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if password is None : password = getpass . getpass ( question ) store_plaintext_passwords = self . _config . get ( "preferences" , "store-plaintext-passwords" , fallback = None ) if store_plaintext_passwords != "no" : question = ( "Do you want to store your password in plain text in " + self . _config_filename ( ) ) answer = ask ( question , [ "yes" , "no" , "never" ] , "no" ) if answer == "yes" : self . _config . set ( "credentials" , "password" , password ) self . _save_config ( ) elif answer == "never" : if "preferences" not in self . _config : self . _config . add_section ( "preferences" ) self . _config . set ( "preferences" , "store-plaintext-passwords" , "no" ) self . _save_config ( ) return password
4179	def window_bartlett_hann ( N ) : if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) a0 = 0.62 a1 = 0.48 a2 = 0.38 win = a0 - a1 * abs ( n / ( N - 1. ) - 0.5 ) - a2 * cos ( 2 * pi * n / ( N - 1. ) ) return win
4524	def save ( self , project_file = '' ) : self . _request_project_file ( project_file ) data_file . dump ( self . desc . as_dict ( ) , self . project_file )
3678	def legal_status ( self ) : if self . __legal_status : return self . __legal_status else : self . __legal_status = legal_status ( self . CAS , Method = 'COMBINED' ) return self . __legal_status
9399	def restart ( self ) : if self . _engine : self . _engine . repl . terminate ( ) executable = self . _executable if executable : os . environ [ 'OCTAVE_EXECUTABLE' ] = executable if 'OCTAVE_EXECUTABLE' not in os . environ and 'OCTAVE' in os . environ : os . environ [ 'OCTAVE_EXECUTABLE' ] = os . environ [ 'OCTAVE' ] self . _engine = OctaveEngine ( stdin_handler = self . _handle_stdin , logger = self . logger ) # Add local Octave scripts. self . _engine . eval ( 'addpath("%s");' % HERE . replace ( osp . sep , '/' ) )
12324	def save ( self ) : if self . code : raise HolviError ( "Orders cannot be updated" ) send_json = self . to_holvi_dict ( ) send_json . update ( { 'pool' : self . api . connection . pool } ) url = six . u ( self . api . base_url + "order/" ) stat = self . api . connection . make_post ( url , send_json ) code = stat [ "details_uri" ] . split ( "/" ) [ - 2 ] # Maybe slightly ugly but I don't want to basically reimplement all but uri formation of the api method return ( stat [ "checkout_uri" ] , self . api . get_order ( code ) )
522	def _updateBoostFactorsGlobal ( self ) : # When global inhibition is enabled, the target activation level is # the sparsity of the spatial pooler if ( self . _localAreaDensity > 0 ) : targetDensity = self . _localAreaDensity else : inhibitionArea = ( ( 2 * self . _inhibitionRadius + 1 ) ** self . _columnDimensions . size ) inhibitionArea = min ( self . _numColumns , inhibitionArea ) targetDensity = float ( self . _numActiveColumnsPerInhArea ) / inhibitionArea targetDensity = min ( targetDensity , 0.5 ) self . _boostFactors = numpy . exp ( ( targetDensity - self . _activeDutyCycles ) * self . _boostStrength )
10786	def add_subtract ( st , max_iter = 7 , max_npart = 'calc' , max_mem = 2e8 , always_check_remove = False , * * kwargs ) : if max_npart == 'calc' : max_npart = 0.05 * st . obj_get_positions ( ) . shape [ 0 ] total_changed = 0 _change_since_opt = 0 removed_poses = [ ] added_poses0 = [ ] added_poses = [ ] nr = 1 # Check removal on the first loop for _ in range ( max_iter ) : if ( nr != 0 ) or ( always_check_remove ) : nr , rposes = remove_bad_particles ( st , * * kwargs ) na , aposes = add_missing_particles ( st , * * kwargs ) current_changed = na + nr removed_poses . extend ( rposes ) added_poses0 . extend ( aposes ) total_changed += current_changed _change_since_opt += current_changed if current_changed == 0 : break elif _change_since_opt > max_npart : _change_since_opt *= 0 CLOG . info ( 'Start add_subtract optimization.' ) opt . do_levmarq ( st , opt . name_globals ( st , remove_params = st . get ( 'psf' ) . params ) , max_iter = 1 , run_length = 4 , num_eig_dirs = 3 , max_mem = max_mem , eig_update_frequency = 2 , rz_order = 0 , use_accel = True ) CLOG . info ( 'After optimization:\t{:.6}' . format ( st . error ) ) # Optimize the added particles' radii: for p in added_poses0 : i = st . obj_closest_particle ( p ) opt . do_levmarq_particles ( st , np . array ( [ i ] ) , max_iter = 2 , damping = 0.3 ) added_poses . append ( st . obj_get_positions ( ) [ i ] ) return total_changed , np . array ( removed_poses ) , np . array ( added_poses )
11476	def _create_or_reuse_item ( local_file , parent_folder_id , reuse_existing = False ) : local_item_name = os . path . basename ( local_file ) item_id = None if reuse_existing : # check by name to see if the item already exists in the folder children = session . communicator . folder_children ( session . token , parent_folder_id ) items = children [ 'items' ] for item in items : if item [ 'name' ] == local_item_name : item_id = item [ 'item_id' ] break if item_id is None : # create the item for the subdir new_item = session . communicator . create_item ( session . token , local_item_name , parent_folder_id ) item_id = new_item [ 'item_id' ] return item_id
9610	def execute ( self , command , data = { } ) : method , uri = command try : path = self . _formatter . format_map ( uri , data ) body = self . _formatter . get_unused_kwargs ( ) url = "{0}{1}" . format ( self . _url , path ) return self . _request ( method , url , body ) except KeyError as err : LOGGER . debug ( 'Endpoint {0} is missing argument {1}' . format ( uri , err ) ) raise
6840	def distrib_family ( ) : distrib = ( distrib_id ( ) or '' ) . lower ( ) if distrib in [ 'debian' , 'ubuntu' , 'linuxmint' , 'elementary os' ] : return DEBIAN elif distrib in [ 'redhat' , 'rhel' , 'centos' , 'sles' , 'fedora' ] : return REDHAT elif distrib in [ 'sunos' ] : return SUN elif distrib in [ 'gentoo' ] : return GENTOO elif distrib in [ 'arch' , 'manjarolinux' ] : return ARCH return 'other'
952	def trainTM ( sequence , timeSteps , noiseLevel ) : currentColumns = np . zeros ( tm . numberOfColumns ( ) , dtype = "uint32" ) predictedColumns = np . zeros ( tm . numberOfColumns ( ) , dtype = "uint32" ) ts = 0 for t in range ( timeSteps ) : tm . reset ( ) for k in range ( 4 ) : v = corruptVector ( sequence [ k ] [ : ] , noiseLevel , sparseCols ) tm . compute ( set ( v [ : ] . nonzero ( ) [ 0 ] . tolist ( ) ) , learn = True ) activeColumnsIndices = [ tm . columnForCell ( i ) for i in tm . getActiveCells ( ) ] predictedColumnIndices = [ tm . columnForCell ( i ) for i in tm . getPredictiveCells ( ) ] currentColumns = [ 1 if i in activeColumnsIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] acc = accuracy ( currentColumns , predictedColumns ) x . append ( ts ) y . append ( acc ) ts += 1 predictedColumns = [ 1 if i in predictedColumnIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ]
13273	def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , np . generic ) : return np . asscalar ( obj ) # Let the base class default method raise the TypeError return json . JSONEncoder ( self , obj )
2647	def bash_app ( function = None , data_flow_kernel = None , walltime = 60 , cache = False , executors = 'all' ) : from parsl . app . bash import BashApp def decorator ( func ) : def wrapper ( f ) : return BashApp ( f , data_flow_kernel = data_flow_kernel , walltime = walltime , cache = cache , executors = executors ) return wrapper ( func ) if function is not None : return decorator ( function ) return decorator
11611	def run ( self , model , tol = 0.001 , max_iters = 999 , verbose = True ) : orig_err_states = np . seterr ( all = 'raise' ) np . seterr ( under = 'ignore' ) if verbose : print print "Iter No Time (hh:mm:ss) Total change (TPM) " print "------- --------------- ----------------------" num_iters = 0 err_sum = 1000000.0 time0 = time . time ( ) target_err = 1000000.0 * tol while err_sum > target_err and num_iters < max_iters : prev_isoform_expression = self . get_allelic_expression ( ) . sum ( axis = 0 ) prev_isoform_expression *= ( 1000000.0 / prev_isoform_expression . sum ( ) ) self . update_allelic_expression ( model = model ) curr_isoform_expression = self . get_allelic_expression ( ) . sum ( axis = 0 ) curr_isoform_expression *= ( 1000000.0 / curr_isoform_expression . sum ( ) ) err = np . abs ( curr_isoform_expression - prev_isoform_expression ) err_sum = err . sum ( ) num_iters += 1 if verbose : time1 = time . time ( ) delmin , s = divmod ( int ( time1 - time0 ) , 60 ) h , m = divmod ( delmin , 60 ) print " %5d %4d:%02d:%02d %9.1f / 1000000" % ( num_iters , h , m , s , err_sum )
11829	def expand ( self , problem ) : return [ self . child_node ( problem , action ) for action in problem . actions ( self . state ) ]
5420	def _get_job_resources ( args ) : logging = param_util . build_logging_param ( args . logging ) if args . logging else None timeout = param_util . timeout_in_seconds ( args . timeout ) log_interval = param_util . log_interval_in_seconds ( args . log_interval ) return job_model . Resources ( min_cores = args . min_cores , min_ram = args . min_ram , machine_type = args . machine_type , disk_size = args . disk_size , disk_type = args . disk_type , boot_disk_size = args . boot_disk_size , preemptible = args . preemptible , image = args . image , regions = args . regions , zones = args . zones , logging = logging , logging_path = None , service_account = args . service_account , scopes = args . scopes , keep_alive = args . keep_alive , cpu_platform = args . cpu_platform , network = args . network , subnetwork = args . subnetwork , use_private_address = args . use_private_address , accelerator_type = args . accelerator_type , accelerator_count = args . accelerator_count , nvidia_driver_version = args . nvidia_driver_version , timeout = timeout , log_interval = log_interval , ssh = args . ssh )
3191	def update ( self , folder_id , data ) : if 'name' not in data : raise KeyError ( 'The template folder must have a name' ) self . folder_id = folder_id return self . _mc_client . _patch ( url = self . _build_path ( folder_id ) , data = data )
11582	def retrieve_url ( self , url ) : try : r = requests . get ( url ) except requests . ConnectionError : raise exceptions . RetrieveError ( 'Connection fail' ) if r . status_code >= 400 : raise exceptions . RetrieveError ( 'Connected, but status code is %s' % ( r . status_code ) ) real_url = r . url content = r . content try : content_type = r . headers [ 'Content-Type' ] except KeyError : content_type , encoding = mimetypes . guess_type ( real_url , strict = False ) self . response = r return content_type . lower ( ) , content
10490	def popUpItem ( self , * args ) : self . Press ( ) time . sleep ( .5 ) return self . _menuItem ( self , * args )
13168	def children ( self , name = None , reverse = False ) : elems = self . _children if reverse : elems = reversed ( elems ) for elem in elems : if name is None or elem . tagname == name : yield elem
141	def to_line_string ( self , closed = True ) : from imgaug . augmentables . lines import LineString if not closed or len ( self . exterior ) <= 1 : return LineString ( self . exterior , label = self . label ) return LineString ( np . concatenate ( [ self . exterior , self . exterior [ 0 : 1 , : ] ] , axis = 0 ) , label = self . label )
761	def modifyBits ( inputVal , maxChanges ) : changes = np . random . random_integers ( 0 , maxChanges , 1 ) [ 0 ] if changes == 0 : return inputVal inputWidth = len ( inputVal ) whatToChange = np . random . random_integers ( 0 , 41 , changes ) runningIndex = - 1 numModsDone = 0 for i in xrange ( inputWidth ) : if numModsDone >= changes : break if inputVal [ i ] == 1 : runningIndex += 1 if runningIndex in whatToChange : if i != 0 and inputVal [ i - 1 ] == 0 : inputVal [ i - 1 ] = 1 inputVal [ i ] = 0 return inputVal
1928	def load_overrides ( path = None ) : if path is not None : names = [ path ] else : possible_names = [ 'mcore.yml' , 'manticore.yml' ] names = [ os . path . join ( '.' , '' . join ( x ) ) for x in product ( [ '' , '.' ] , possible_names ) ] for name in names : try : with open ( name , 'r' ) as yml_f : logger . info ( f'Reading configuration from {name}' ) parse_config ( yml_f ) break except FileNotFoundError : pass else : if path is not None : raise FileNotFoundError ( f"'{path}' not found for config overrides" )
8451	def is_temple_project ( ) : if not os . path . exists ( temple . constants . TEMPLE_CONFIG_FILE ) : msg = 'No {} file found in repository.' . format ( temple . constants . TEMPLE_CONFIG_FILE ) raise temple . exceptions . InvalidTempleProjectError ( msg )
13700	def init_app ( self , app ) : app . config . setdefault ( "TRACY_REQUIRE_CLIENT" , False ) if not hasattr ( app , 'extensions' ) : app . extensions = { } app . extensions [ 'restpoints' ] = self app . before_request ( self . _before ) app . after_request ( self . _after )
10090	def rst2node ( doc_name , data ) : if not data : return parser = docutils . parsers . rst . Parser ( ) document = docutils . utils . new_document ( '<%s>' % doc_name ) document . settings = docutils . frontend . OptionParser ( ) . get_default_values ( ) document . settings . tab_width = 4 document . settings . pep_references = False document . settings . rfc_references = False document . settings . env = Env ( ) parser . parse ( data , document ) if len ( document . children ) == 1 : return document . children [ 0 ] else : par = docutils . nodes . paragraph ( ) for child in document . children : par += child return par
437	def data_to_tfrecord ( images , labels , filename ) : if os . path . isfile ( filename ) : print ( "%s exists" % filename ) return print ( "Converting data into %s ..." % filename ) # cwd = os.getcwd() writer = tf . python_io . TFRecordWriter ( filename ) for index , img in enumerate ( images ) : img_raw = img . tobytes ( ) # Visualize a image # tl.visualize.frame(np.asarray(img, dtype=np.uint8), second=1, saveable=False, name='frame', fig_idx=1236) label = int ( labels [ index ] ) # print(label) # Convert the bytes back to image as follow: # image = Image.frombytes('RGB', (32, 32), img_raw) # image = np.fromstring(img_raw, np.float32) # image = image.reshape([32, 32, 3]) # tl.visualize.frame(np.asarray(image, dtype=np.uint8), second=1, saveable=False, name='frame', fig_idx=1236) example = tf . train . Example ( features = tf . train . Features ( feature = { "label" : tf . train . Feature ( int64_list = tf . train . Int64List ( value = [ label ] ) ) , 'img_raw' : tf . train . Feature ( bytes_list = tf . train . BytesList ( value = [ img_raw ] ) ) , } ) ) writer . write ( example . SerializeToString ( ) ) # Serialize To String writer . close ( )
4743	def exists ( ) : if env ( ) : cij . err ( "cij.nvm.exists: Invalid NVMe ENV." ) return 1 nvm = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ '[[ -b "%s" ]]' % nvm [ "DEV_PATH" ] ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True , echo = False ) return rcode
8698	def __expect ( self , exp = '> ' , timeout = None ) : timeout_before = self . _port . timeout timeout = timeout or self . _timeout #do NOT set timeout on Windows if SYSTEM != 'Windows' : # Checking for new data every 100us is fast enough if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout # Finish as soon as either exp matches or we run out of time (work like dump, but faster on success) data = '' while not data . endswith ( exp ) and time . time ( ) <= end : data += self . _port . read ( ) log . debug ( 'expect returned: `{0}`' . format ( data ) ) if time . time ( ) > end : raise CommunicationTimeout ( 'Timeout waiting for data' , data ) if not data . endswith ( exp ) and len ( exp ) > 0 : raise BadResponseException ( 'Bad response.' , exp , data ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before return data
9170	def declare_api_routes ( config ) : add_route = config . add_route add_route ( 'get-content' , '/contents/{ident_hash}' ) add_route ( 'get-resource' , '/resources/{hash}' ) # User actions API add_route ( 'license-request' , '/contents/{uuid}/licensors' ) add_route ( 'roles-request' , '/contents/{uuid}/roles' ) add_route ( 'acl-request' , '/contents/{uuid}/permissions' ) # Publishing API add_route ( 'publications' , '/publications' ) add_route ( 'get-publication' , '/publications/{id}' ) add_route ( 'publication-license-acceptance' , '/publications/{id}/license-acceptances/{uid}' ) add_route ( 'publication-role-acceptance' , '/publications/{id}/role-acceptances/{uid}' ) # TODO (8-May-12017) Remove because the term collate is being phased out. add_route ( 'collate-content' , '/contents/{ident_hash}/collate-content' ) add_route ( 'bake-content' , '/contents/{ident_hash}/baked' ) # Moderation routes add_route ( 'moderation' , '/moderations' ) add_route ( 'moderate' , '/moderations/{id}' ) add_route ( 'moderation-rss' , '/feeds/moderations.rss' ) # API Key routes add_route ( 'api-keys' , '/api-keys' ) add_route ( 'api-key' , '/api-keys/{id}' )
5389	def _datetime_in_range ( self , dt , dt_min = None , dt_max = None ) : # The pipelines API stores operation create-time with second granularity. # We mimic this behavior in the local provider by truncating to seconds. dt = dt . replace ( microsecond = 0 ) if dt_min : dt_min = dt_min . replace ( microsecond = 0 ) else : dt_min = dsub_util . replace_timezone ( datetime . datetime . min , pytz . utc ) if dt_max : dt_max = dt_max . replace ( microsecond = 0 ) else : dt_max = dsub_util . replace_timezone ( datetime . datetime . max , pytz . utc ) return dt_min <= dt <= dt_max
12736	def are_connected ( self , body_a , body_b ) : return bool ( ode . areConnected ( self . get_body ( body_a ) . ode_body , self . get_body ( body_b ) . ode_body ) )
4209	def pascal ( n ) : errors . is_positive_integer ( n ) result = numpy . zeros ( ( n , n ) ) #fill the first row and column for i in range ( 0 , n ) : result [ i , 0 ] = 1 result [ 0 , i ] = 1 if n > 1 : for i in range ( 1 , n ) : for j in range ( 1 , n ) : result [ i , j ] = result [ i - 1 , j ] + result [ i , j - 1 ] return result
4360	def _receiver_loop ( self ) : while True : rawdata = self . get_server_msg ( ) if not rawdata : continue # or close the connection ? try : pkt = packet . decode ( rawdata , self . json_loads ) except ( ValueError , KeyError , Exception ) as e : self . error ( 'invalid_packet' , "There was a decoding error when dealing with packet " "with event: %s... (%s)" % ( rawdata [ : 20 ] , e ) ) continue if pkt [ 'type' ] == 'heartbeat' : # This is already dealth with in put_server_msg() when # any incoming raw data arrives. continue if pkt [ 'type' ] == 'disconnect' and pkt [ 'endpoint' ] == '' : # On global namespace, we kill everything. self . kill ( detach = True ) continue endpoint = pkt [ 'endpoint' ] if endpoint not in self . namespaces : self . error ( "no_such_namespace" , "The endpoint you tried to connect to " "doesn't exist: %s" % endpoint , endpoint = endpoint ) continue elif endpoint in self . active_ns : pkt_ns = self . active_ns [ endpoint ] else : new_ns_class = self . namespaces [ endpoint ] pkt_ns = new_ns_class ( self . environ , endpoint , request = self . request ) # This calls initialize() on all the classes and mixins, etc.. # in the order of the MRO for cls in type ( pkt_ns ) . __mro__ : if hasattr ( cls , 'initialize' ) : cls . initialize ( pkt_ns ) # use this instead of __init__, # for less confusion self . active_ns [ endpoint ] = pkt_ns retval = pkt_ns . process_packet ( pkt ) # Has the client requested an 'ack' with the reply parameters ? if pkt . get ( 'ack' ) == "data" and pkt . get ( 'id' ) : if type ( retval ) is tuple : args = list ( retval ) else : args = [ retval ] returning_ack = dict ( type = 'ack' , ackId = pkt [ 'id' ] , args = args , endpoint = pkt . get ( 'endpoint' , '' ) ) self . send_packet ( returning_ack ) # Now, are we still connected ? if not self . connected : self . kill ( detach = True ) # ?? what,s the best clean-up # when its not a # user-initiated disconnect return
8930	def workdir_is_clean ( self , quiet = False ) : # Update the index self . run ( 'git update-index -q --ignore-submodules --refresh' , * * RUN_KWARGS ) unchanged = True # Disallow unstaged changes in the working tree try : self . run ( 'git diff-files --quiet --ignore-submodules --' , report_error = False , * * RUN_KWARGS ) except exceptions . Failure : unchanged = False if not quiet : notify . warning ( 'You have unstaged changes!' ) self . run ( 'git diff-files --name-status -r --ignore-submodules -- >&2' , * * RUN_KWARGS ) # Disallow uncommitted changes in the index try : self . run ( 'git diff-index --cached --quiet HEAD --ignore-submodules --' , report_error = False , * * RUN_KWARGS ) except exceptions . Failure : unchanged = False if not quiet : notify . warning ( 'Your index contains uncommitted changes!' ) self . run ( 'git diff-index --cached --name-status -r --ignore-submodules HEAD -- >&2' , * * RUN_KWARGS ) return unchanged
8671	def delete_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) for key in key_name : try : click . echo ( 'Deleting key {0}...' . format ( key ) ) stash . delete ( key_name = key ) except GhostError as ex : sys . exit ( ex ) click . echo ( 'Keys deleted successfully' )
12168	def _dispatch_function ( self , event , listener , * args , * * kwargs ) : try : return listener ( * args , * * kwargs ) except Exception as exc : if event == self . LISTENER_ERROR_EVENT : raise return self . emit ( self . LISTENER_ERROR_EVENT , event , listener , exc )
12750	def set_pid_params ( self , * args , * * kwargs ) : for joint in self . joints : joint . target_angles = [ None ] * joint . ADOF joint . controllers = [ pid ( * args , * * kwargs ) for i in range ( joint . ADOF ) ]
3384	def generate_fva_warmup ( self ) : self . n_warmup = 0 reactions = self . model . reactions self . warmup = np . zeros ( ( 2 * len ( reactions ) , len ( self . model . variables ) ) ) self . model . objective = Zero for sense in ( "min" , "max" ) : self . model . objective_direction = sense for i , r in enumerate ( reactions ) : variables = ( self . model . variables [ self . fwd_idx [ i ] ] , self . model . variables [ self . rev_idx [ i ] ] ) # Omit fixed reactions if they are non-homogeneous if r . upper_bound - r . lower_bound < self . bounds_tol : LOGGER . info ( "skipping fixed reaction %s" % r . id ) continue self . model . objective . set_linear_coefficients ( { variables [ 0 ] : 1 , variables [ 1 ] : - 1 } ) self . model . slim_optimize ( ) if not self . model . solver . status == OPTIMAL : LOGGER . info ( "can not maximize reaction %s, skipping it" % r . id ) continue primals = self . model . solver . primal_values sol = [ primals [ v . name ] for v in self . model . variables ] self . warmup [ self . n_warmup , ] = sol self . n_warmup += 1 # Reset objective self . model . objective . set_linear_coefficients ( { variables [ 0 ] : 0 , variables [ 1 ] : 0 } ) # Shrink to measure self . warmup = self . warmup [ 0 : self . n_warmup , : ] # Remove redundant search directions keep = np . logical_not ( self . _is_redundant ( self . warmup ) ) self . warmup = self . warmup [ keep , : ] self . n_warmup = self . warmup . shape [ 0 ] # Catch some special cases if len ( self . warmup . shape ) == 1 or self . warmup . shape [ 0 ] == 1 : raise ValueError ( "Your flux cone consists only of a single point!" ) elif self . n_warmup == 2 : if not self . problem . homogeneous : raise ValueError ( "Can not sample from an inhomogenous problem" " with only 2 search directions :(" ) LOGGER . info ( "All search directions on a line, adding another one." ) newdir = self . warmup . T . dot ( [ 0.25 , 0.25 ] ) self . warmup = np . vstack ( [ self . warmup , newdir ] ) self . n_warmup += 1 # Shrink warmup points to measure self . warmup = shared_np_array ( ( self . n_warmup , len ( self . model . variables ) ) , self . warmup )
118	def imap_batches_unordered ( self , batches , chunksize = 1 ) : assert ia . is_generator ( batches ) , ( "Expected to get a generator as 'batches', got type %s. " + "Call map_batches() if you use lists." ) % ( type ( batches ) , ) # TODO change this to 'yield from' once switched to 3.3+ gen = self . pool . imap_unordered ( _Pool_starworker , self . _handle_batch_ids_gen ( batches ) , chunksize = chunksize ) for batch in gen : yield batch
12828	def log_error ( self , text : str ) -> None : if self . log_errors : with self . _log_fp . open ( 'a+' ) as log_file : log_file . write ( f'{text}\n' )
11114	def remove_repository ( self , path = None , relatedFiles = False , relatedFolders = False , verbose = True ) : if path is not None : realPath = os . path . realpath ( os . path . expanduser ( path ) ) else : realPath = self . __path if realPath is None : if verbose : warnings . warn ( 'path is None and current Repository is not initialized!' ) return if not self . is_repository ( realPath ) : if verbose : warnings . warn ( "No repository found in '%s'!" % realPath ) return # check for security if realPath == os . path . realpath ( '/..' ) : if verbose : warnings . warn ( 'You are about to wipe out your system !!! action aboarded' ) return # get repo if path is not None : repo = Repository ( ) repo . load_repository ( realPath ) else : repo = self # delete files if relatedFiles : for relativePath in repo . walk_files_relative_path ( ) : realPath = os . path . join ( repo . path , relativePath ) if not os . path . isfile ( realPath ) : continue if not os . path . exists ( realPath ) : continue os . remove ( realPath ) # delete directories if relatedFolders : for relativePath in reversed ( list ( repo . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( repo . path , relativePath ) # protect from wiping out the system if not os . path . isdir ( realPath ) : continue if not os . path . exists ( realPath ) : continue if not len ( os . listdir ( realPath ) ) : os . rmdir ( realPath ) # delete repository os . remove ( os . path . join ( repo . path , ".pyrepinfo" ) ) for fname in ( ".pyrepstate" , ".pyreplock" ) : p = os . path . join ( repo . path , fname ) if os . path . exists ( p ) : os . remove ( p ) # remove main directory if empty if os . path . isdir ( repo . path ) : if not len ( os . listdir ( repo . path ) ) : os . rmdir ( repo . path ) # reset repository repo . __reset_repository ( )
2137	def create ( self , fail_on_found = False , force_on_exists = False , * * kwargs ) : if kwargs . get ( 'parent' , None ) : parent_data = self . set_child_endpoint ( parent = kwargs [ 'parent' ] , inventory = kwargs . get ( 'inventory' , None ) ) kwargs [ 'inventory' ] = parent_data [ 'inventory' ] elif 'inventory' not in kwargs : raise exc . UsageError ( 'To create a group, you must provide a parent inventory or parent group.' ) return super ( Resource , self ) . create ( fail_on_found = fail_on_found , force_on_exists = force_on_exists , * * kwargs )
3439	def _escape_str_id ( id_str ) : for c in ( "'" , '"' ) : if id_str . startswith ( c ) and id_str . endswith ( c ) and id_str . count ( c ) == 2 : id_str = id_str . strip ( c ) for char , escaped_char in _renames : id_str = id_str . replace ( char , escaped_char ) return id_str
13019	def _assemble_select ( self , sql_str , columns , * args , * * kwargs ) : warnings . warn ( "_assemble_select has been depreciated for _assemble_with_columns. It will be removed in a future version." , DeprecationWarning ) return self . _assemble_with_columns ( sql_str , columns , * args , * * kwargs )
3516	def woopra ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return WoopraNode ( )
12483	def filter_list ( lst , pattern ) : if is_fnmatch_regex ( pattern ) and not is_regex ( pattern ) : #use fnmatch log . info ( 'Using fnmatch for {0}' . format ( pattern ) ) filst = fnmatch . filter ( lst , pattern ) else : #use re log . info ( 'Using regex match for {0}' . format ( pattern ) ) filst = match_list ( lst , pattern ) if filst : filst . sort ( ) return filst
3449	def _init_worker ( model , loopless , sense ) : global _model global _loopless _model = model _model . solver . objective . direction = sense _loopless = loopless
5157	def _add_install ( self , context ) : contents = self . _render_template ( 'install.sh' , context ) self . config . setdefault ( 'files' , [ ] ) # file list might be empty # add install.sh to list of included files self . _add_unique_file ( { "path" : "/install.sh" , "contents" : contents , "mode" : "755" } )
6590	def put ( self , package ) : pkgidx = self . workingArea . put_package ( package ) logger = logging . getLogger ( __name__ ) logger . info ( 'submitting {}' . format ( self . workingArea . package_relpath ( pkgidx ) ) ) runid = self . dispatcher . run ( self . workingArea , pkgidx ) self . runid_pkgidx_map [ runid ] = pkgidx return pkgidx
9087	async def update ( self ) -> None : _LOGGER . debug ( "Requesting state update from server (S00, S14)" ) await asyncio . gather ( # List unsealed Zones self . send_command ( 'S00' ) , # Arming status update self . send_command ( 'S14' ) , )
7666	def _key ( cls , obs ) : if not isinstance ( obs , Observation ) : raise JamsError ( '{} must be of type jams.Observation' . format ( obs ) ) return obs . time
3677	def rdkitmol_Hs ( self ) : if self . __rdkitmol_Hs : return self . __rdkitmol_Hs else : try : self . __rdkitmol_Hs = Chem . AddHs ( self . rdkitmol ) return self . __rdkitmol_Hs except : return None
7829	def add_option ( self , value , label ) : if type ( value ) is list : warnings . warn ( ".add_option() accepts single value now." , DeprecationWarning , stacklevel = 1 ) value = value [ 0 ] if self . type not in ( "list-multi" , "list-single" ) : raise ValueError ( "Options are allowed only for list types." ) option = Option ( value , label ) self . options . append ( option ) return option
442	def get_all_params ( self , session = None ) : _params = [ ] for p in self . all_params : if session is None : _params . append ( p . eval ( ) ) else : _params . append ( session . run ( p ) ) return _params
5000	def require_at_least_one_query_parameter ( * query_parameter_names ) : def outer_wrapper ( view ) : """ Allow the passing of parameters to require_at_least_one_query_parameter. """ @ wraps ( view ) def wrapper ( request , * args , * * kwargs ) : """ Checks for the existence of the specified query parameters, raises a ValidationError if none of them were included in the request. """ requirement_satisfied = False for query_parameter_name in query_parameter_names : query_parameter_values = request . query_params . getlist ( query_parameter_name ) kwargs [ query_parameter_name ] = query_parameter_values if query_parameter_values : requirement_satisfied = True if not requirement_satisfied : raise ValidationError ( detail = 'You must provide at least one of the following query parameters: {params}.' . format ( params = ', ' . join ( query_parameter_names ) ) ) return view ( request , * args , * * kwargs ) return wrapper return outer_wrapper
9549	def validate ( self , data , expect_header_row = True , ignore_lines = 0 , summarize = False , limit = 0 , context = None , report_unexpected_exceptions = True ) : problems = list ( ) problem_generator = self . ivalidate ( data , expect_header_row , ignore_lines , summarize , context , report_unexpected_exceptions ) for i , p in enumerate ( problem_generator ) : if not limit or i < limit : problems . append ( p ) return problems
11554	def disable_digital_reporting ( self , pin ) : port = pin // 8 command = [ self . _command_handler . REPORT_DIGITAL + port , self . REPORTING_DISABLE ] self . _command_handler . send_command ( command )
1163	def wait ( self , timeout = None ) : with self . __cond : if not self . __flag : self . __cond . wait ( timeout ) return self . __flag
996	def _updateStatsInferEnd ( self , stats , bottomUpNZ , predictedState , colConfidence ) : # Return if not collecting stats if not self . collectStats : return stats [ 'nInfersSinceReset' ] += 1 # Compute the prediction score, how well the prediction from the last # time step predicted the current bottom-up input ( numExtra2 , numMissing2 , confidences2 ) = self . _checkPrediction ( patternNZs = [ bottomUpNZ ] , output = predictedState , colConfidence = colConfidence ) predictionScore , positivePredictionScore , negativePredictionScore = ( confidences2 [ 0 ] ) # Store the stats that don't depend on burn-in stats [ 'curPredictionScore2' ] = float ( predictionScore ) stats [ 'curFalseNegativeScore' ] = 1.0 - float ( positivePredictionScore ) stats [ 'curFalsePositiveScore' ] = float ( negativePredictionScore ) stats [ 'curMissing' ] = numMissing2 stats [ 'curExtra' ] = numExtra2 # If we are passed the burn-in period, update the accumulated stats # Here's what various burn-in values mean: # 0: try to predict the first element of each sequence and all subsequent # 1: try to predict the second element of each sequence and all subsequent # etc. if stats [ 'nInfersSinceReset' ] <= self . burnIn : return # Burn-in related stats stats [ 'nPredictions' ] += 1 numExpected = max ( 1.0 , float ( len ( bottomUpNZ ) ) ) stats [ 'totalMissing' ] += numMissing2 stats [ 'totalExtra' ] += numExtra2 stats [ 'pctExtraTotal' ] += 100.0 * numExtra2 / numExpected stats [ 'pctMissingTotal' ] += 100.0 * numMissing2 / numExpected stats [ 'predictionScoreTotal2' ] += float ( predictionScore ) stats [ 'falseNegativeScoreTotal' ] += 1.0 - float ( positivePredictionScore ) stats [ 'falsePositiveScoreTotal' ] += float ( negativePredictionScore ) if self . collectSequenceStats : # Collect cell confidences for every cell that correctly predicted current # bottom up input. Normalize confidence across each column cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] sconf = cc . sum ( axis = 1 ) for c in range ( self . numberOfCols ) : if sconf [ c ] > 0 : cc [ c , : ] /= sconf [ c ] # Update cell confidence histogram: add column-normalized confidence # scores to the histogram self . _internalStats [ 'confHistogram' ] += cc
4005	def streaming_to_client ( ) : for handler in client_logger . handlers : if hasattr ( handler , 'append_newlines' ) : break else : handler = None old_propagate = client_logger . propagate client_logger . propagate = False if handler is not None : old_append = handler . append_newlines handler . append_newlines = False yield client_logger . propagate = old_propagate if handler is not None : handler . append_newlines = old_append
5154	def get_copy ( dict_ , key , default = None ) : value = dict_ . get ( key , default ) if value : return deepcopy ( value ) return value
6970	def _old_epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , epdsmooth_windowsize = 21 , epdsmooth_sigclip = 3.0 , epdsmooth_func = smooth_magseries_signal_medfilt , epdsmooth_extraparams = None ) : # find all the finite values of the magsnitude finiteind = np . isfinite ( mags ) # calculate median and stdev mags_median = np . median ( mags [ finiteind ] ) mags_stdev = np . nanstd ( mags ) # if we're supposed to sigma clip, do so if epdsmooth_sigclip : excludeind = abs ( mags - mags_median ) < epdsmooth_sigclip * mags_stdev finalind = finiteind & excludeind else : finalind = finiteind final_mags = mags [ finalind ] final_len = len ( final_mags ) # smooth the signal if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( final_mags , epdsmooth_windowsize , * * epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( final_mags , epdsmooth_windowsize ) # make the linear equation matrix epdmatrix = np . c_ [ fsv [ finalind ] ** 2.0 , fsv [ finalind ] , fdv [ finalind ] ** 2.0 , fdv [ finalind ] , fkv [ finalind ] ** 2.0 , fkv [ finalind ] , np . ones ( final_len ) , fsv [ finalind ] * fdv [ finalind ] , fsv [ finalind ] * fkv [ finalind ] , fdv [ finalind ] * fkv [ finalind ] , np . sin ( 2 * np . pi * xcc [ finalind ] ) , np . cos ( 2 * np . pi * xcc [ finalind ] ) , np . sin ( 2 * np . pi * ycc [ finalind ] ) , np . cos ( 2 * np . pi * ycc [ finalind ] ) , np . sin ( 4 * np . pi * xcc [ finalind ] ) , np . cos ( 4 * np . pi * xcc [ finalind ] ) , np . sin ( 4 * np . pi * ycc [ finalind ] ) , np . cos ( 4 * np . pi * ycc [ finalind ] ) , bgv [ finalind ] , bge [ finalind ] ] # solve the matrix equation [epdmatrix] . [x] = [smoothedmags] # return the EPD differential magss if the solution succeeds try : coeffs , residuals , rank , singulars = lstsq ( epdmatrix , smoothedmags , rcond = None ) if DEBUG : print ( 'coeffs = %s, residuals = %s' % ( coeffs , residuals ) ) retdict = { 'times' : times , 'mags' : ( mags_median + _old_epd_diffmags ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , mags ) ) , 'errs' : errs , 'fitcoeffs' : coeffs , 'residuals' : residuals } return retdict # if the solution fails, return nothing except Exception as e : LOGEXCEPTION ( 'EPD solution did not converge' ) retdict = { 'times' : times , 'mags' : np . full_like ( mags , np . nan ) , 'errs' : errs , 'fitcoeffs' : coeffs , 'residuals' : residuals } return retdict
3569	def centralManager_didDiscoverPeripheral_advertisementData_RSSI_ ( self , manager , peripheral , data , rssi ) : logger . debug ( 'centralManager_didDiscoverPeripheral_advertisementData_RSSI called' ) # Log name of device found while scanning. #logger.debug('Saw device advertised with name: {0}'.format(peripheral.name())) # Make sure the device is added to the list of devices and then update # its advertisement state. device = device_list ( ) . get ( peripheral ) if device is None : device = device_list ( ) . add ( peripheral , CoreBluetoothDevice ( peripheral ) ) device . _update_advertised ( data )
13209	def _prep_snippet_for_pandoc ( self , latex_text ) : replace_cite = CitationLinker ( self . bib_db ) latex_text = replace_cite ( latex_text ) return latex_text
9702	def checkTUN ( self ) : packet = self . _TUN . _tun . read ( self . _TUN . _tun . mtu ) return ( packet )
3940	async def listen ( self ) : retries = 0 # Number of retries attempted so far need_new_sid = True # whether a new SID is needed while retries <= self . _max_retries : # After the first failed retry, back off exponentially longer after # each attempt. if retries > 0 : backoff_seconds = self . _retry_backoff_base ** retries logger . info ( 'Backing off for %s seconds' , backoff_seconds ) await asyncio . sleep ( backoff_seconds ) # Request a new SID if we don't have one yet, or the previous one # became invalid. if need_new_sid : await self . _fetch_channel_sid ( ) need_new_sid = False # Clear any previous push data, since if there was an error it # could contain garbage. self . _chunk_parser = ChunkParser ( ) try : await self . _longpoll_request ( ) except ChannelSessionError as err : logger . warning ( 'Long-polling interrupted: %s' , err ) need_new_sid = True except exceptions . NetworkError as err : logger . warning ( 'Long-polling request failed: %s' , err ) else : # The connection closed successfully, so reset the number of # retries. retries = 0 continue retries += 1 logger . info ( 'retry attempt count is now %s' , retries ) if self . _is_connected : self . _is_connected = False await self . on_disconnect . fire ( ) # If the request ended with an error, the client must account for # messages being dropped during this time. logger . error ( 'Ran out of retries for long-polling request' )
4539	def multi ( method ) : @ functools . wraps ( method ) def multi ( self , address = '' ) : values = flask . request . values address = urllib . parse . unquote_plus ( address ) if address and values and not address . endswith ( '.' ) : address += '.' result = { } for a in values or '' : try : if not self . project : raise ValueError ( 'No Project is currently loaded' ) ed = editor . Editor ( address + a , self . project ) result [ address + a ] = { 'value' : method ( self , ed , a ) } except : if self . project : traceback . print_exc ( ) result [ address + a ] = { 'error' : 'Could not multi addr %s' % a } return flask . jsonify ( result ) return multi
13836	def _MergeMessageField ( self , tokenizer , message , field ) : is_map_entry = _IsMapEntry ( field ) if tokenizer . TryConsume ( '<' ) : end_token = '>' else : tokenizer . Consume ( '{' ) end_token = '}' if field . label == descriptor . FieldDescriptor . LABEL_REPEATED : if field . is_extension : sub_message = message . Extensions [ field ] . add ( ) elif is_map_entry : # pylint: disable=protected-access sub_message = field . message_type . _concrete_class ( ) else : sub_message = getattr ( message , field . name ) . add ( ) else : if field . is_extension : sub_message = message . Extensions [ field ] else : sub_message = getattr ( message , field . name ) sub_message . SetInParent ( ) while not tokenizer . TryConsume ( end_token ) : if tokenizer . AtEnd ( ) : raise tokenizer . ParseErrorPreviousToken ( 'Expected "%s".' % ( end_token , ) ) self . _MergeField ( tokenizer , sub_message ) if is_map_entry : value_cpptype = field . message_type . fields_by_name [ 'value' ] . cpp_type if value_cpptype == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : value = getattr ( message , field . name ) [ sub_message . key ] value . MergeFrom ( sub_message . value ) else : getattr ( message , field . name ) [ sub_message . key ] = sub_message . value
6279	def clear ( self ) : self . ctx . fbo . clear ( red = self . clear_color [ 0 ] , green = self . clear_color [ 1 ] , blue = self . clear_color [ 2 ] , alpha = self . clear_color [ 3 ] , depth = self . clear_depth , )
292	def plot_rolling_sharpe ( returns , factor_returns = None , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , legend_loc = 'best' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) rolling_sharpe_ts = timeseries . rolling_sharpe ( returns , rolling_window ) rolling_sharpe_ts . plot ( alpha = .7 , lw = 3 , color = 'orangered' , ax = ax , * * kwargs ) if factor_returns is not None : rolling_sharpe_ts_factor = timeseries . rolling_sharpe ( factor_returns , rolling_window ) rolling_sharpe_ts_factor . plot ( alpha = .7 , lw = 3 , color = 'grey' , ax = ax , * * kwargs ) ax . set_title ( 'Rolling Sharpe ratio (6-month)' ) ax . axhline ( rolling_sharpe_ts . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Sharpe ratio' ) ax . set_xlabel ( '' ) if factor_returns is None : ax . legend ( [ 'Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Sharpe' , 'Benchmark Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) return ax
4885	def allow_request ( self , request , view ) : service_users = get_service_usernames ( ) # User service user throttling rates for service user. if request . user . username in service_users : self . update_throttle_scope ( ) return super ( ServiceUserThrottle , self ) . allow_request ( request , view )
13575	def select ( course = False , tid = None , auto = False ) : if course : update ( course = True ) course = None try : course = Course . get_selected ( ) except NoCourseSelected : pass ret = { } if not tid : ret = Menu . launch ( "Select a course" , Course . select ( ) . execute ( ) , course ) else : ret [ "item" ] = Course . get ( Course . tid == tid ) if "item" in ret : ret [ "item" ] . set_select ( ) update ( ) if ret [ "item" ] . path == "" : select_a_path ( auto = auto ) # Selects the first exercise in this course skip ( ) return else : print ( "You can select the course with `tmc select --course`" ) return else : selected = None try : selected = Exercise . get_selected ( ) except NoExerciseSelected : pass ret = { } if not tid : ret = Menu . launch ( "Select an exercise" , Course . get_selected ( ) . exercises , selected ) else : ret [ "item" ] = Exercise . byid ( tid ) if "item" in ret : ret [ "item" ] . set_select ( ) print ( "Selected {}" . format ( ret [ "item" ] ) )
10707	def create_vacation ( body ) : arequest = requests . post ( VACATIONS_URL , headers = HEADERS , data = json . dumps ( body ) ) status_code = str ( arequest . status_code ) if status_code != '200' : _LOGGER . error ( "Failed to create vacation. " + status_code ) _LOGGER . error ( arequest . json ( ) ) return False return arequest . json ( )
5545	def pyramid ( input_raster , output_dir , pyramid_type = None , output_format = None , resampling_method = None , scale_method = None , zoom = None , bounds = None , overwrite = False , debug = False ) : bounds = bounds if bounds else None options = dict ( pyramid_type = pyramid_type , scale_method = scale_method , output_format = output_format , resampling = resampling_method , zoom = zoom , bounds = bounds , overwrite = overwrite ) raster2pyramid ( input_raster , output_dir , options )
2870	def setup ( self , pin , mode ) : self . mraa_gpio . Gpio . dir ( self . mraa_gpio . Gpio ( pin ) , self . _dir_mapping [ mode ] )
5078	def get_current_course_run ( course , users_active_course_runs ) : current_course_run = None filtered_course_runs = [ ] all_course_runs = course [ 'course_runs' ] if users_active_course_runs : current_course_run = get_closest_course_run ( users_active_course_runs ) else : for course_run in all_course_runs : if is_course_run_enrollable ( course_run ) and is_course_run_upgradeable ( course_run ) : filtered_course_runs . append ( course_run ) if not filtered_course_runs : # Consider all runs if there were not any enrollable/upgradeable ones. filtered_course_runs = all_course_runs if filtered_course_runs : current_course_run = get_closest_course_run ( filtered_course_runs ) return current_course_run
7761	def xml_elements_equal ( element1 , element2 , ignore_level1_cdata = False ) : # pylint: disable-msg=R0911 if None in ( element1 , element2 ) or element1 . tag != element2 . tag : return False attrs1 = element1 . items ( ) attrs1 . sort ( ) attrs2 = element2 . items ( ) attrs2 . sort ( ) if not ignore_level1_cdata : if element1 . text != element2 . text : return False if attrs1 != attrs2 : return False if len ( element1 ) != len ( element2 ) : return False for child1 , child2 in zip ( element1 , element2 ) : if child1 . tag != child2 . tag : return False if not ignore_level1_cdata : if element1 . text != element2 . text : return False if not xml_elements_equal ( child1 , child2 ) : return False return True
8389	def write ( self , text , hashline = b"# {}" ) : if not text . endswith ( b"\n" ) : text += b"\n" actual_hash = hashlib . sha1 ( text ) . hexdigest ( ) with open ( self . filename , "wb" ) as f : f . write ( text ) f . write ( hashline . decode ( "utf8" ) . format ( actual_hash ) . encode ( "utf8" ) ) f . write ( b"\n" )
11257	def values ( prev , * keys , * * kw ) : d = next ( prev ) if isinstance ( d , dict ) : yield [ d [ k ] for k in keys if k in d ] for d in prev : yield [ d [ k ] for k in keys if k in d ] else : yield [ d [ i ] for i in keys if 0 <= i < len ( d ) ] for d in prev : yield [ d [ i ] for i in keys if 0 <= i < len ( d ) ]
2186	def load ( self , cfgstr = None ) : from six . moves import cPickle as pickle cfgstr = self . _rectify_cfgstr ( cfgstr ) dpath = self . dpath fname = self . fname verbose = self . verbose if not self . enabled : if verbose > 1 : self . log ( '[cacher] ... cache disabled: fname={}' . format ( self . fname ) ) raise IOError ( 3 , 'Cache Loading Is Disabled' ) fpath = self . get_fpath ( cfgstr = cfgstr ) if not exists ( fpath ) : if verbose > 2 : self . log ( '[cacher] ... cache does not exist: ' 'dpath={} fname={} cfgstr={}' . format ( basename ( dpath ) , fname , cfgstr ) ) raise IOError ( 2 , 'No such file or directory: %r' % ( fpath , ) ) else : if verbose > 3 : self . log ( '[cacher] ... cache exists: ' 'dpath={} fname={} cfgstr={}' . format ( basename ( dpath ) , fname , cfgstr ) ) try : with open ( fpath , 'rb' ) as file_ : data = pickle . load ( file_ ) except Exception as ex : if verbose > 0 : self . log ( 'CORRUPTED? fpath = %s' % ( fpath , ) ) if verbose > 1 : self . log ( '[cacher] ... CORRUPTED? dpath={} cfgstr={}' . format ( basename ( dpath ) , cfgstr ) ) if isinstance ( ex , ( EOFError , IOError , ImportError ) ) : raise IOError ( str ( ex ) ) else : if verbose > 1 : self . log ( '[cacher] ... unknown reason for exception' ) raise else : if self . verbose > 2 : self . log ( '[cacher] ... {} cache hit' . format ( self . fname ) ) elif verbose > 1 : self . log ( '[cacher] ... cache hit' ) return data
842	def getPartitionId ( self , i ) : if ( i < 0 ) or ( i >= self . _numPatterns ) : raise RuntimeError ( "index out of bounds" ) partitionId = self . _partitionIdList [ i ] if partitionId == numpy . inf : return None else : return partitionId
1402	def setTopologyInfo ( self , topology ) : # Execution state is the most basic info. # If there is no execution state, just return # as the rest of the things don't matter. if not topology . execution_state : Log . info ( "No execution state found for: " + topology . name ) return Log . info ( "Setting topology info for topology: " + topology . name ) has_physical_plan = True if not topology . physical_plan : has_physical_plan = False Log . info ( "Setting topology info for topology: " + topology . name ) has_packing_plan = True if not topology . packing_plan : has_packing_plan = False has_tmaster_location = True if not topology . tmaster : has_tmaster_location = False has_scheduler_location = True if not topology . scheduler_location : has_scheduler_location = False topologyInfo = { "name" : topology . name , "id" : topology . id , "logical_plan" : None , "physical_plan" : None , "packing_plan" : None , "execution_state" : None , "tmaster_location" : None , "scheduler_location" : None , } executionState = self . extract_execution_state ( topology ) executionState [ "has_physical_plan" ] = has_physical_plan executionState [ "has_packing_plan" ] = has_packing_plan executionState [ "has_tmaster_location" ] = has_tmaster_location executionState [ "has_scheduler_location" ] = has_scheduler_location executionState [ "status" ] = topology . get_status ( ) topologyInfo [ "metadata" ] = self . extract_metadata ( topology ) topologyInfo [ "runtime_state" ] = self . extract_runtime_state ( topology ) topologyInfo [ "execution_state" ] = executionState topologyInfo [ "logical_plan" ] = self . extract_logical_plan ( topology ) topologyInfo [ "physical_plan" ] = self . extract_physical_plan ( topology ) topologyInfo [ "packing_plan" ] = self . extract_packing_plan ( topology ) topologyInfo [ "tmaster_location" ] = self . extract_tmaster ( topology ) topologyInfo [ "scheduler_location" ] = self . extract_scheduler_location ( topology ) self . topologyInfos [ ( topology . name , topology . state_manager_name ) ] = topologyInfo
33	def reset ( self , * * kwargs ) : if self . was_real_done : obs = self . env . reset ( * * kwargs ) else : # no-op step to advance from terminal/lost life state obs , _ , _ , _ = self . env . step ( 0 ) self . lives = self . env . unwrapped . ale . lives ( ) return obs
3381	def shared_np_array ( shape , data = None , integer = False ) : size = np . prod ( shape ) if integer : array = Array ( ctypes . c_int64 , int ( size ) ) np_array = np . frombuffer ( array . get_obj ( ) , dtype = "int64" ) else : array = Array ( ctypes . c_double , int ( size ) ) np_array = np . frombuffer ( array . get_obj ( ) ) np_array = np_array . reshape ( shape ) if data is not None : if len ( shape ) != len ( data . shape ) : raise ValueError ( "`data` must have the same dimensions" "as the created array." ) same = all ( x == y for x , y in zip ( shape , data . shape ) ) if not same : raise ValueError ( "`data` must have the same shape" "as the created array." ) np_array [ : ] = data return np_array
8077	def ellipsemode ( self , mode = None ) : if mode in ( self . CORNER , self . CENTER , self . CORNERS ) : self . ellipsemode = mode return self . ellipsemode elif mode is None : return self . ellipsemode else : raise ShoebotError ( _ ( "ellipsemode: invalid input" ) )
4186	def window_taylor ( N , nbar = 4 , sll = - 30 ) : B = 10 ** ( - sll / 20 ) A = log ( B + sqrt ( B ** 2 - 1 ) ) / pi s2 = nbar ** 2 / ( A ** 2 + ( nbar - 0.5 ) ** 2 ) ma = arange ( 1 , nbar ) def calc_Fm ( m ) : numer = ( - 1 ) ** ( m + 1 ) * prod ( 1 - m ** 2 / s2 / ( A ** 2 + ( ma - 0.5 ) ** 2 ) ) denom = 2 * prod ( [ 1 - m ** 2 / j ** 2 for j in ma if j != m ] ) return numer / denom Fm = array ( [ calc_Fm ( m ) for m in ma ] ) def W ( n ) : return 2 * np . sum ( Fm * cos ( 2 * pi * ma * ( n - N / 2 + 1 / 2 ) / N ) ) + 1 w = array ( [ W ( n ) for n in range ( N ) ] ) # normalize (Note that this is not described in the original text) scale = W ( ( N - 1 ) / 2 ) w /= scale return w
6417	def stem ( self , word ) : word = normalize ( 'NFKD' , text_type ( word . lower ( ) ) ) word = '' . join ( c for c in word if c in { 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'q' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'x' , 'y' , 'z' , } ) # Rule 2 word = word . replace ( 'j' , 'i' ) . replace ( 'v' , 'u' ) # Rule 3 if word [ - 3 : ] == 'que' : # This diverges from the paper by also returning 'que' itself # unstemmed if word [ : - 3 ] in self . _keep_que or word == 'que' : return { 'n' : word , 'v' : word } else : word = word [ : - 3 ] # Base case will mean returning the words as is noun = word verb = word # Rule 4 for endlen in range ( 4 , 0 , - 1 ) : if word [ - endlen : ] in self . _n_endings [ endlen ] : if len ( word ) - 2 >= endlen : noun = word [ : - endlen ] else : noun = word break for endlen in range ( 6 , 0 , - 1 ) : if word [ - endlen : ] in self . _v_endings_strip [ endlen ] : if len ( word ) - 2 >= endlen : verb = word [ : - endlen ] else : verb = word break if word [ - endlen : ] in self . _v_endings_alter [ endlen ] : if word [ - endlen : ] in { 'iuntur' , 'erunt' , 'untur' , 'iunt' , 'unt' , } : new_word = word [ : - endlen ] + 'i' addlen = 1 elif word [ - endlen : ] in { 'beris' , 'bor' , 'bo' } : new_word = word [ : - endlen ] + 'bi' addlen = 2 else : new_word = word [ : - endlen ] + 'eri' addlen = 3 # Technically this diverges from the paper by considering the # length of the stem without the new suffix if len ( new_word ) >= 2 + addlen : verb = new_word else : verb = word break return { 'n' : noun , 'v' : verb }
12604	def duplicated ( values : Sequence ) : vals = pd . Series ( values ) return vals [ vals . duplicated ( ) ]
9473	def DFS_prefix ( self , root = None ) : if not root : root = self . _root return self . _DFS_prefix ( root )
1457	def valid_java_classpath ( classpath ) : paths = classpath . split ( ':' ) for path_entry in paths : if not valid_path ( path_entry . strip ( ) ) : return False return True
7435	def _zbufcountlines ( filename , gzipped ) : if gzipped : cmd1 = [ "gunzip" , "-c" , filename ] else : cmd1 = [ "cat" , filename ] cmd2 = [ "wc" ] proc1 = sps . Popen ( cmd1 , stdout = sps . PIPE , stderr = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stdin = proc1 . stdout , stdout = sps . PIPE , stderr = sps . PIPE ) res = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "error zbufcountlines {}:" . format ( res ) ) LOGGER . info ( res ) nlines = int ( res . split ( ) [ 0 ] ) return nlines
6091	def cache ( func ) : def wrapper ( instance : GeometryProfile , grid : np . ndarray , * args , * * kwargs ) : if not hasattr ( instance , "cache" ) : instance . cache = { } key = ( func . __name__ , grid . tobytes ( ) ) if key not in instance . cache : instance . cache [ key ] = func ( instance , grid ) return instance . cache [ key ] return wrapper
7384	def plot_nodes ( self , nodelist , theta , group ) : for i , node in enumerate ( nodelist ) : r = self . internal_radius + i * self . scale x , y = get_cartesian ( r , theta ) circle = plt . Circle ( xy = ( x , y ) , radius = self . dot_radius , color = self . node_colormap [ group ] , linewidth = 0 ) self . ax . add_patch ( circle )
6964	def get ( self ) : # generate the project's list of checkplots project_checkplots = self . currentproject [ 'checkplots' ] project_checkplotbasenames = [ os . path . basename ( x ) for x in project_checkplots ] project_checkplotindices = range ( len ( project_checkplots ) ) # get the sortkey and order project_cpsortkey = self . currentproject [ 'sortkey' ] if self . currentproject [ 'sortorder' ] == 'asc' : project_cpsortorder = 'ascending' elif self . currentproject [ 'sortorder' ] == 'desc' : project_cpsortorder = 'descending' # get the filterkey and condition project_cpfilterstatements = self . currentproject [ 'filterstatements' ] self . render ( 'cpindex.html' , project_checkplots = project_checkplots , project_cpsortorder = project_cpsortorder , project_cpsortkey = project_cpsortkey , project_cpfilterstatements = project_cpfilterstatements , project_checkplotbasenames = project_checkplotbasenames , project_checkplotindices = project_checkplotindices , project_checkplotfile = self . cplistfile , readonly = self . readonly , baseurl = self . baseurl )
6505	def add_properties ( self ) : for property_name in [ p [ 0 ] for p in inspect . getmembers ( self . __class__ ) if isinstance ( p [ 1 ] , property ) ] : self . _results_fields [ property_name ] = getattr ( self , property_name , None )
3506	def loopless_fva_iter ( model , reaction , solution = False , zero_cutoff = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) current = model . objective . value sol = get_solution ( model ) objective_dir = model . objective . direction # boundary reactions can not be part of cycles if reaction . boundary : if solution : return sol else : return current with model : _add_cycle_free ( model , sol . fluxes ) model . slim_optimize ( ) # If the previous optimum is maintained in the loopless solution it was # loopless and we are done if abs ( reaction . flux - current ) < zero_cutoff : if solution : return sol return current # If previous optimum was not in the loopless solution create a new # almost loopless solution containing only loops including the current # reaction. Than remove all of those loops. ll_sol = get_solution ( model ) . fluxes reaction . bounds = ( current , current ) model . slim_optimize ( ) almost_ll_sol = get_solution ( model ) . fluxes with model : # find the reactions with loops using the current reaction and remove # the loops for rxn in model . reactions : rid = rxn . id if ( ( abs ( ll_sol [ rid ] ) < zero_cutoff ) and ( abs ( almost_ll_sol [ rid ] ) > zero_cutoff ) ) : rxn . bounds = max ( 0 , rxn . lower_bound ) , min ( 0 , rxn . upper_bound ) if solution : best = model . optimize ( ) else : model . slim_optimize ( ) best = reaction . flux model . objective . direction = objective_dir return best
6063	def mass_within_ellipse_in_units ( self , major_axis , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : self . check_units_of_radius_and_critical_surface_density ( radius = major_axis , critical_surface_density = critical_surface_density ) profile = self . new_profile_with_units_converted ( unit_length = major_axis . unit_length , unit_mass = 'angular' , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) mass_angular = dim . Mass ( value = quad ( profile . mass_integral , a = 0.0 , b = major_axis , args = ( self . axis_ratio , ) ) [ 0 ] , unit_mass = 'angular' ) return mass_angular . convert ( unit_mass = unit_mass , critical_surface_density = critical_surface_density )
10977	def members ( group_id ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) s = request . args . get ( 's' , '' ) group = Group . query . get_or_404 ( group_id ) if group . can_see_members ( current_user ) : members = Membership . query_by_group ( group_id , with_invitations = True ) if q : members = Membership . search ( members , q ) if s : members = Membership . order ( members , Membership . state , s ) members = members . paginate ( page , per_page = per_page ) return render_template ( "invenio_groups/members.html" , group = group , members = members , page = page , per_page = per_page , q = q , s = s , ) flash ( _ ( 'You are not allowed to see members of this group %(group_name)s.' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
2848	def _check ( self , command , * args ) : ret = command ( self . _ctx , * args ) logger . debug ( 'Called ftdi_{0} and got response {1}.' . format ( command . __name__ , ret ) ) if ret != 0 : raise RuntimeError ( 'ftdi_{0} failed with error {1}: {2}' . format ( command . __name__ , ret , ftdi . get_error_string ( self . _ctx ) ) )
1532	def get_execution_state ( self , topologyName , callback = None ) : if callback : self . execution_state_watchers [ topologyName ] . append ( callback ) else : execution_state_path = self . get_execution_state_path ( topologyName ) with open ( execution_state_path ) as f : data = f . read ( ) executionState = ExecutionState ( ) executionState . ParseFromString ( data ) return executionState
1422	def loads ( string ) : f = StringIO . StringIO ( string ) marshaller = JavaObjectUnmarshaller ( f ) marshaller . add_transformer ( DefaultObjectTransformer ( ) ) return marshaller . readObject ( )
4370	def emit ( self , event , * args , * * kwargs ) : callback = kwargs . pop ( 'callback' , None ) if kwargs : raise ValueError ( "emit() only supports positional argument, to stay " "compatible with the Socket.IO protocol. You can " "however pass in a dictionary as the first argument" ) pkt = dict ( type = "event" , name = event , args = args , endpoint = self . ns_name ) if callback : # By passing 'data', we indicate that we *want* an explicit ack # by the client code, not an automatic as with send(). pkt [ 'ack' ] = 'data' pkt [ 'id' ] = msgid = self . socket . _get_next_msgid ( ) self . socket . _save_ack_callback ( msgid , callback ) self . socket . send_packet ( pkt )
1134	def updatecache ( filename , module_globals = None ) : if filename in cache : del cache [ filename ] if not filename or ( filename . startswith ( '<' ) and filename . endswith ( '>' ) ) : return [ ] fullname = filename try : stat = os . stat ( fullname ) except OSError : basename = filename # Try for a __loader__, if available if module_globals and '__loader__' in module_globals : name = module_globals . get ( '__name__' ) loader = module_globals [ '__loader__' ] get_source = getattr ( loader , 'get_source' , None ) if name and get_source : try : data = get_source ( name ) except ( ImportError , IOError ) : pass else : if data is None : # No luck, the PEP302 loader cannot find the source # for this module. return [ ] cache [ filename ] = ( len ( data ) , None , [ line + '\n' for line in data . splitlines ( ) ] , fullname ) return cache [ filename ] [ 2 ] # Try looking through the module search path, which is only useful # when handling a relative filename. if os . path . isabs ( filename ) : return [ ] for dirname in sys . path : # When using imputil, sys.path may contain things other than # strings; ignore them when it happens. try : fullname = os . path . join ( dirname , basename ) except ( TypeError , AttributeError ) : # Not sufficiently string-like to do anything useful with. continue try : stat = os . stat ( fullname ) break except os . error : pass else : return [ ] try : with open ( fullname , 'rU' ) as fp : lines = fp . readlines ( ) except IOError : return [ ] if lines and not lines [ - 1 ] . endswith ( '\n' ) : lines [ - 1 ] += '\n' size , mtime = stat . st_size , stat . st_mtime cache [ filename ] = size , mtime , lines , fullname return lines
7488	def make ( data , samples ) : invcffile = os . path . join ( data . dirs . consens , data . name + ".vcf" ) outlocifile = os . path . join ( data . dirs . outfiles , data . name + ".loci" ) importvcf ( invcffile , outlocifile )
12936	def add ( self , * names ) : def decorator ( blok ) : for name in names or ( blok . __name__ , ) : self [ name ] = blok return blok return decorator
58	def extend ( self , all_sides = 0 , top = 0 , right = 0 , bottom = 0 , left = 0 ) : return BoundingBox ( x1 = self . x1 - all_sides - left , x2 = self . x2 + all_sides + right , y1 = self . y1 - all_sides - top , y2 = self . y2 + all_sides + bottom )
12534	def update ( self , dicomset ) : if not isinstance ( dicomset , DicomFileSet ) : raise ValueError ( 'Given dicomset is not a DicomFileSet.' ) self . items = list ( set ( self . items ) . update ( dicomset ) )
2019	def SMOD ( self , a , b ) : s0 , s1 = to_signed ( a ) , to_signed ( b ) sign = Operators . ITEBV ( 256 , s0 < 0 , - 1 , 1 ) try : result = ( Operators . ABS ( s0 ) % Operators . ABS ( s1 ) ) * sign except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , s1 == 0 , 0 , result )
7355	def seq_to_str ( obj , sep = "," ) : if isinstance ( obj , string_classes ) : return obj elif isinstance ( obj , ( list , tuple ) ) : return sep . join ( [ str ( x ) for x in obj ] ) else : return str ( obj )
12162	async def _try_catch_coro ( emitter , event , listener , coro ) : try : await coro except Exception as exc : if event == emitter . LISTENER_ERROR_EVENT : raise emitter . emit ( emitter . LISTENER_ERROR_EVENT , event , listener , exc )
9274	def filter_excluded_tags ( self , all_tags ) : filtered_tags = copy . deepcopy ( all_tags ) if self . options . exclude_tags : filtered_tags = self . apply_exclude_tags ( filtered_tags ) if self . options . exclude_tags_regex : filtered_tags = self . apply_exclude_tags_regex ( filtered_tags ) return filtered_tags
6924	def open ( self , database , user , password , host ) : try : self . connection = pg . connect ( user = user , password = password , database = database , host = host ) LOGINFO ( 'postgres connection successfully ' 'created, using DB %s, user %s' % ( database , user ) ) self . database = database self . user = user except Exception as e : LOGEXCEPTION ( 'postgres connection failed, ' 'using DB %s, user %s' % ( database , user ) ) self . database = None self . user = None
8433	def cubehelix_pal ( start = 0 , rot = .4 , gamma = 1.0 , hue = 0.8 , light = .85 , dark = .15 , reverse = False ) : cdict = mpl . _cm . cubehelix ( gamma , start , rot , hue ) cubehelix_cmap = mpl . colors . LinearSegmentedColormap ( 'cubehelix' , cdict ) def cubehelix_palette ( n ) : values = np . linspace ( light , dark , n ) return [ mcolors . rgb2hex ( cubehelix_cmap ( x ) ) for x in values ] return cubehelix_palette
12174	def genIndex ( folder , forceIDs = [ ] ) : if not os . path . exists ( folder + "/swhlab4/" ) : print ( " !! cannot index if no /swhlab4/" ) return timestart = cm . timethis ( ) files = glob . glob ( folder + "/*.*" ) #ABF folder files . extend ( glob . glob ( folder + "/swhlab4/*.*" ) ) print ( " -- indexing glob took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) files . extend ( genPNGs ( folder , files ) ) files = sorted ( files ) timestart = cm . timethis ( ) d = cm . getIDfileDict ( files ) #TODO: this is really slow print ( " -- filedict length:" , len ( d ) ) print ( " -- generating ID dict took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) groups = cm . getABFgroups ( files ) print ( " -- groups length:" , len ( groups ) ) for ID in sorted ( list ( groups . keys ( ) ) ) : overwrite = False for abfID in groups [ ID ] : if abfID in forceIDs : overwrite = True try : htmlABF ( ID , groups [ ID ] , d , folder , overwrite ) except : print ( "~~ HTML GENERATION FAILED!!!" ) menu = expMenu ( groups , folder ) makeSplash ( menu , folder ) makeMenu ( menu , folder ) htmlFrames ( d , folder ) makeMenu ( menu , folder ) makeSplash ( menu , folder )
6706	def expire_password ( self , username ) : r = self . local_renderer r . env . username = username r . sudo ( 'chage -d 0 {username}' )
12301	def instantiate ( repo , validator_name = None , filename = None , rulesfiles = None ) : default_validators = repo . options . get ( 'validator' , { } ) validators = { } if validator_name is not None : # Handle the case validator is specified.. if validator_name in default_validators : validators = { validator_name : default_validators [ validator_name ] } else : validators = { validator_name : { 'files' : [ ] , 'rules' : { } , 'rules-files' : [ ] } } else : validators = default_validators #========================================= # Insert the file names #========================================= if filename is not None : matching_files = repo . find_matching_files ( [ filename ] ) if len ( matching_files ) == 0 : print ( "Filename could not be found" , filename ) raise Exception ( "Invalid filename pattern" ) for v in validators : validators [ v ] [ 'files' ] = matching_files else : # Instantiate the files from the patterns specified for v in validators : if 'files' not in validators [ v ] : validators [ v ] [ 'files' ] = [ ] elif len ( validators [ v ] [ 'files' ] ) > 0 : matching_files = repo . find_matching_files ( validators [ v ] [ 'files' ] ) validators [ v ] [ 'files' ] = matching_files #========================================= # Insert the rules files.. #========================================= if rulesfiles is not None : # Command lines... matching_files = repo . find_matching_files ( [ rulesfiles ] ) if len ( matching_files ) == 0 : print ( "Could not find matching rules files ({}) for {}" . format ( rulesfiles , v ) ) raise Exception ( "Invalid rules" ) for v in validators : validators [ v ] [ 'rules-files' ] = matching_files else : # Instantiate the files from the patterns specified for v in validators : if 'rules-files' not in validators [ v ] : validators [ v ] [ 'rules-files' ] = [ ] else : rulesfiles = validators [ v ] [ 'rules-files' ] matching_files = repo . find_matching_files ( rulesfiles ) validators [ v ] [ 'rules-files' ] = matching_files return validators
6381	def dist_jaro_winkler ( src , tar , qval = 1 , mode = 'winkler' , long_strings = False , boost_threshold = 0.7 , scaling_factor = 0.1 , ) : return JaroWinkler ( ) . dist ( src , tar , qval , mode , long_strings , boost_threshold , scaling_factor )
10487	def _findAll ( self , * * kwargs ) : result = [ ] for item in self . _generateFind ( * * kwargs ) : result . append ( item ) return result
9762	def unbookmark ( ctx ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) try : PolyaxonClient ( ) . experiment . unbookmark ( user , project_name , _experiment ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not unbookmark experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment is unbookmarked." )
6482	def _load_class ( class_path , default ) : if class_path is None : return default component = class_path . rsplit ( '.' , 1 ) result_processor = getattr ( importlib . import_module ( component [ 0 ] ) , component [ 1 ] , default ) if len ( component ) > 1 else default return result_processor
11465	def cd ( self , folder ) : if folder . startswith ( '/' ) : self . _ftp . cwd ( folder ) else : for subfolder in folder . split ( '/' ) : if subfolder : self . _ftp . cwd ( subfolder )
3299	def xml_to_bytes ( element , pretty_print = False ) : if use_lxml : xml = etree . tostring ( element , encoding = "UTF-8" , xml_declaration = True , pretty_print = pretty_print ) else : xml = etree . tostring ( element , encoding = "UTF-8" ) if not xml . startswith ( b"<?xml " ) : xml = b'<?xml version="1.0" encoding="utf-8" ?>\n' + xml assert xml . startswith ( b"<?xml " ) # ET should prepend an encoding header return xml
3589	def set_color ( self , r , g , b ) : # See more details on the bulb's protocol from this guide: # https://learn.adafruit.com/reverse-engineering-a-bluetooth-low-energy-light-bulb/overview command = '\x58\x01\x03\x01\xFF\x00{0}{1}{2}' . format ( chr ( r & 0xFF ) , chr ( g & 0xFF ) , chr ( b & 0xFF ) ) self . _color . write_value ( command )
6793	def load_django_settings ( self ) : r = self . local_renderer # Save environment variables so we can restore them later. _env = { } save_vars = [ 'ALLOW_CELERY' , 'DJANGO_SETTINGS_MODULE' ] for var_name in save_vars : _env [ var_name ] = os . environ . get ( var_name ) try : # Allow us to import local app modules. if r . env . local_project_dir : sys . path . insert ( 0 , r . env . local_project_dir ) #TODO:remove this once bug in django-celery has been fixed os . environ [ 'ALLOW_CELERY' ] = '0' # print('settings_module:', r.format(r.env.settings_module)) os . environ [ 'DJANGO_SETTINGS_MODULE' ] = r . format ( r . env . settings_module ) # os.environ['CELERY_LOADER'] = 'django' # os.environ['SITE'] = r.genv.SITE or r.genv.default_site # os.environ['ROLE'] = r.genv.ROLE or r.genv.default_role # In Django >= 1.7, fixes the error AppRegistryNotReady: Apps aren't loaded yet # Disabling, in Django >= 1.10, throws exception: # RuntimeError: Model class django.contrib.contenttypes.models.ContentType # doesn't declare an explicit app_label and isn't in an application in INSTALLED_APPS. # try: # from django.core.wsgi import get_wsgi_application # application = get_wsgi_application() # except (ImportError, RuntimeError): # raise # print('Unable to get wsgi application.') # traceback.print_exc() # In Django >= 1.7, fixes the error AppRegistryNotReady: Apps aren't loaded yet try : import django django . setup ( ) except AttributeError : # This doesn't exist in Django < 1.7, so ignore it. pass # Load Django settings. settings = self . get_settings ( ) try : from django . contrib import staticfiles from django . conf import settings as _settings # get_settings() doesn't raise ImportError but returns None instead if settings is not None : for k , v in settings . __dict__ . items ( ) : setattr ( _settings , k , v ) else : raise ImportError except ( ImportError , RuntimeError ) : print ( 'Unable to load settings.' ) traceback . print_exc ( ) finally : # Restore environment variables. for var_name , var_value in _env . items ( ) : if var_value is None : del os . environ [ var_name ] else : os . environ [ var_name ] = var_value return settings
4421	async def set_pause ( self , pause : bool ) : await self . _lavalink . ws . send ( op = 'pause' , guildId = self . guild_id , pause = pause ) self . paused = pause
1496	def get_sub_parts ( self , query ) : parts = [ ] num_open_braces = 0 delimiter = ',' last_starting_index = 0 for i in range ( len ( query ) ) : if query [ i ] == '(' : num_open_braces += 1 elif query [ i ] == ')' : num_open_braces -= 1 elif query [ i ] == delimiter and num_open_braces == 0 : parts . append ( query [ last_starting_index : i ] . strip ( ) ) last_starting_index = i + 1 parts . append ( query [ last_starting_index : ] . strip ( ) ) return parts
3113	def _validate ( self , value ) : _LOGGER . info ( 'validate: Got type %s' , type ( value ) ) if value is not None and not isinstance ( value , client . Flow ) : raise TypeError ( 'Property {0} must be convertible to a flow ' 'instance; received: {1}.' . format ( self . _name , value ) )
6371	def fallout ( self ) : if self . _fp + self . _tn == 0 : return float ( 'NaN' ) return self . _fp / ( self . _fp + self . _tn )
2317	def _run_pc ( self , data , fixedEdges = None , fixedGaps = None , verbose = True ) : # Checking coherence of arguments # print(self.arguments) if ( self . arguments [ '{CITEST}' ] == self . dir_CI_test [ 'hsic' ] and self . arguments [ '{METHOD_INDEP}' ] == self . dir_method_indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the hsic test,' ' setting the hsic.gamma method.' ) self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ 'hsic_gamma' ] elif ( self . arguments [ '{CITEST}' ] == self . dir_CI_test [ 'gaussian' ] and self . arguments [ '{METHOD_INDEP}' ] != self . dir_method_indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the selected test,' ' setting the classic correlation-based method.' ) self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ 'corr' ] # Run PC id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt_pc' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt_pc' + id + '/' def retrieve_result ( ) : return read_csv ( '/tmp/cdt_pc' + id + '/result.csv' , delimiter = ',' ) . values try : data . to_csv ( '/tmp/cdt_pc' + id + '/data.csv' , header = False , index = False ) if fixedGaps is not None and fixedEdges is not None : fixedGaps . to_csv ( '/tmp/cdt_pc' + id + '/fixedgaps.csv' , index = False , header = False ) fixedEdges . to_csv ( '/tmp/cdt_pc' + id + '/fixededges.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' pc_result = launch_R_script ( "{}/R_templates/pc.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , self . arguments , output_function = retrieve_result , verbose = verbose ) # Cleanup except Exception as e : rmtree ( '/tmp/cdt_pc' + id + '' ) raise e except KeyboardInterrupt : rmtree ( '/tmp/cdt_pc' + id + '/' ) raise KeyboardInterrupt rmtree ( '/tmp/cdt_pc' + id + '' ) return pc_result
6758	def set_site_specifics ( self , site ) : r = self . local_renderer site_data = self . genv . sites [ site ] . copy ( ) r . env . site = site if self . verbose : print ( 'set_site_specifics.data:' ) pprint ( site_data , indent = 4 ) # Remove local namespace settings from the global namespace # by converting <satchel_name>_<variable_name> to <variable_name>. local_ns = { } for k , v in list ( site_data . items ( ) ) : if k . startswith ( self . name + '_' ) : _k = k [ len ( self . name + '_' ) : ] local_ns [ _k ] = v del site_data [ k ] r . env . update ( local_ns ) r . env . update ( site_data )
13299	def install ( self , package ) : logger . debug ( 'Installing ' + package ) shell . run ( self . pip_path , 'install' , package )
8372	def save_as ( self ) : chooser = ShoebotFileChooserDialog ( _ ( 'Save File' ) , None , Gtk . FileChooserAction . SAVE , ( Gtk . STOCK_SAVE , Gtk . ResponseType . ACCEPT , Gtk . STOCK_CANCEL , Gtk . ResponseType . CANCEL ) ) chooser . set_do_overwrite_confirmation ( True ) chooser . set_transient_for ( self ) saved = chooser . run ( ) == Gtk . ResponseType . ACCEPT if saved : old_filename = self . filename self . source_buffer . filename = chooser . get_filename ( ) if not self . save ( ) : self . filename = old_filename chooser . destroy ( ) return saved
564	def toDict ( self ) : def items2dict ( items ) : """Convert a dict of node spec items to a plain dict Each node spec item object will be converted to a dict of its attributes. The entire items dict will become a dict of dicts (same keys). """ d = { } for k , v in items . items ( ) : d [ k ] = v . __dict__ return d self . invariant ( ) return dict ( description = self . description , singleNodeOnly = self . singleNodeOnly , inputs = items2dict ( self . inputs ) , outputs = items2dict ( self . outputs ) , parameters = items2dict ( self . parameters ) , commands = items2dict ( self . commands ) )
3977	def _expand_libs_in_libs ( specs ) : for lib_name , lib_spec in specs [ 'libs' ] . iteritems ( ) : if 'depends' in lib_spec and 'libs' in lib_spec [ 'depends' ] : lib_spec [ 'depends' ] [ 'libs' ] = _get_dependent ( 'libs' , lib_name , specs , 'libs' )
13873	def CopyFile ( source_filename , target_filename , override = True , md5_check = False , copy_symlink = True ) : from . _exceptions import FileNotFoundError # Check override if not override and Exists ( target_filename ) : from . _exceptions import FileAlreadyExistsError raise FileAlreadyExistsError ( target_filename ) # Don't do md5 check for md5 files themselves. md5_check = md5_check and not target_filename . endswith ( '.md5' ) # If we enabled md5 checks, ignore copy of files that haven't changed their md5 contents. if md5_check : source_md5_filename = source_filename + '.md5' target_md5_filename = target_filename + '.md5' try : source_md5_contents = GetFileContents ( source_md5_filename ) except FileNotFoundError : source_md5_contents = None try : target_md5_contents = GetFileContents ( target_md5_filename ) except FileNotFoundError : target_md5_contents = None if source_md5_contents is not None and source_md5_contents == target_md5_contents and Exists ( target_filename ) : return MD5_SKIP # Copy source file _DoCopyFile ( source_filename , target_filename , copy_symlink = copy_symlink ) # If we have a source_md5, but no target_md5, create the target_md5 file if md5_check and source_md5_contents is not None and source_md5_contents != target_md5_contents : CreateFile ( target_md5_filename , source_md5_contents )
9601	def wait_for_element ( self , using , value , timeout = 10000 , interval = 1000 , asserter = is_displayed ) : if not callable ( asserter ) : raise TypeError ( 'Asserter must be callable.' ) @ retry ( retry_on_exception = lambda ex : isinstance ( ex , WebDriverException ) , stop_max_delay = timeout , wait_fixed = interval ) def _wait_for_element ( ctx , using , value ) : el = ctx . element ( using , value ) asserter ( el ) return el return _wait_for_element ( self , using , value )
6660	def random_forest_error ( forest , X_train , X_test , inbag = None , calibrate = True , memory_constrained = False , memory_limit = None ) : if inbag is None : inbag = calc_inbag ( X_train . shape [ 0 ] , forest ) pred = np . array ( [ tree . predict ( X_test ) for tree in forest ] ) . T pred_mean = np . mean ( pred , 0 ) pred_centered = pred - pred_mean n_trees = forest . n_estimators V_IJ = _core_computation ( X_train , X_test , inbag , pred_centered , n_trees , memory_constrained , memory_limit ) V_IJ_unbiased = _bias_correction ( V_IJ , inbag , pred_centered , n_trees ) # Correct for cases where resampling is done without replacement: if np . max ( inbag ) == 1 : variance_inflation = 1 / ( 1 - np . mean ( inbag ) ) ** 2 V_IJ_unbiased *= variance_inflation if not calibrate : return V_IJ_unbiased if V_IJ_unbiased . shape [ 0 ] <= 20 : print ( "No calibration with n_samples <= 20" ) return V_IJ_unbiased if calibrate : calibration_ratio = 2 n_sample = np . ceil ( n_trees / calibration_ratio ) new_forest = copy . deepcopy ( forest ) new_forest . estimators_ = np . random . permutation ( new_forest . estimators_ ) [ : int ( n_sample ) ] new_forest . n_estimators = int ( n_sample ) results_ss = random_forest_error ( new_forest , X_train , X_test , calibrate = False , memory_constrained = memory_constrained , memory_limit = memory_limit ) # Use this second set of variance estimates # to estimate scale of Monte Carlo noise sigma2_ss = np . mean ( ( results_ss - V_IJ_unbiased ) ** 2 ) delta = n_sample / n_trees sigma2 = ( delta ** 2 + ( 1 - delta ) ** 2 ) / ( 2 * ( 1 - delta ) ** 2 ) * sigma2_ss # Use Monte Carlo noise scale estimate for empirical Bayes calibration V_IJ_calibrated = calibrateEB ( V_IJ_unbiased , sigma2 ) return V_IJ_calibrated
9838	def __gridpositions ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'counts' ) : shape = [ ] try : while True : # raises exception if not an int self . __peek ( ) . value ( 'INTEGER' ) tok = self . __consume ( ) shape . append ( tok . value ( 'INTEGER' ) ) except ( DXParserNoTokens , ValueError ) : pass if len ( shape ) == 0 : raise DXParseError ( 'gridpositions: no shape parameters' ) self . currentobject [ 'shape' ] = shape elif tok . equals ( 'origin' ) : origin = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) origin . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( origin ) == 0 : raise DXParseError ( 'gridpositions: no origin parameters' ) self . currentobject [ 'origin' ] = origin elif tok . equals ( 'delta' ) : d = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) d . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( d ) == 0 : raise DXParseError ( 'gridpositions: missing delta parameters' ) try : self . currentobject [ 'delta' ] . append ( d ) except KeyError : self . currentobject [ 'delta' ] = [ d ] else : raise DXParseError ( 'gridpositions: ' + str ( tok ) + ' not recognized.' )
8920	def _get_request_type ( self ) : value = self . document . tag . lower ( ) if value in allowed_request_types [ self . params [ 'service' ] ] : self . params [ "request" ] = value else : raise OWSInvalidParameterValue ( "Request type %s is not supported" % value , value = "request" ) return self . params [ "request" ]
6089	def for_data_and_tracer ( cls , lens_data , tracer , padded_tracer = None ) : if tracer . has_light_profile and not tracer . has_pixelization : return LensProfileFit ( lens_data = lens_data , tracer = tracer , padded_tracer = padded_tracer ) elif not tracer . has_light_profile and tracer . has_pixelization : return LensInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) elif tracer . has_light_profile and tracer . has_pixelization : return LensProfileInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) else : raise exc . FittingException ( 'The fit routine did not call a Fit class - check the ' 'properties of the tracer' )
12339	def compress ( images , delete_tif = False , folder = None ) : if type ( images ) == str : # only one image return [ compress_blocking ( images , delete_tif , folder ) ] filenames = copy ( images ) # as images property will change when looping return Parallel ( n_jobs = _pools ) ( delayed ( compress_blocking ) ( image = image , delete_tif = delete_tif , folder = folder ) for image in filenames )
13320	def get_modules ( ) : modules = set ( ) cwd = os . getcwd ( ) for d in os . listdir ( cwd ) : if d == 'module.yml' : modules . add ( Module ( cwd ) ) path = unipath ( cwd , d ) if utils . is_module ( path ) : modules . add ( Module ( cwd ) ) module_paths = get_module_paths ( ) for module_path in module_paths : for d in os . listdir ( module_path ) : path = unipath ( module_path , d ) if utils . is_module ( path ) : modules . add ( Module ( path ) ) return sorted ( list ( modules ) , key = lambda x : x . name )
5675	def get_shape_distance_between_stops ( self , trip_I , from_stop_seq , to_stop_seq ) : query_template = "SELECT shape_break FROM stop_times WHERE trip_I={trip_I} AND seq={seq} " stop_seqs = [ from_stop_seq , to_stop_seq ] shape_breaks = [ ] for seq in stop_seqs : q = query_template . format ( seq = seq , trip_I = trip_I ) shape_breaks . append ( self . conn . execute ( q ) . fetchone ( ) ) query_template = "SELECT max(d) - min(d) " "FROM shapes JOIN trips ON(trips.shape_id=shapes.shape_id) " "WHERE trip_I={trip_I} AND shapes.seq>={from_stop_seq} AND shapes.seq<={to_stop_seq};" distance_query = query_template . format ( trip_I = trip_I , from_stop_seq = from_stop_seq , to_stop_seq = to_stop_seq ) return self . conn . execute ( distance_query ) . fetchone ( ) [ 0 ]
3854	def add_color_to_scheme ( scheme , name , foreground , background , palette_colors ) : if foreground is None and background is None : return scheme new_scheme = [ ] for item in scheme : if item [ 0 ] == name : if foreground is None : foreground = item [ 1 ] if background is None : background = item [ 2 ] if palette_colors > 16 : new_scheme . append ( ( name , '' , '' , '' , foreground , background ) ) else : new_scheme . append ( ( name , foreground , background ) ) else : new_scheme . append ( item ) return new_scheme
11899	def _get_src_from_image ( img , fallback_image_file ) : # If the image is None, then we can't process, so we should return the # path to the file itself if img is None : return fallback_image_file # Target format should be the same as the original image format, unless it's # a TIF/TIFF, which can't be displayed by most browsers; we convert these # to jpeg target_format = img . format if target_format . lower ( ) in [ 'tif' , 'tiff' ] : target_format = 'JPEG' # If we have an actual Image, great - put together the base64 image string try : bytesio = io . BytesIO ( ) img . save ( bytesio , target_format ) byte_value = bytesio . getvalue ( ) b64 = base64 . b64encode ( byte_value ) return 'data:image/%s;base64,%s' % ( target_format . lower ( ) , b64 ) except IOError as exptn : print ( 'IOError while saving image bytes: %s' % exptn ) return fallback_image_file
11516	def search_item_by_name_and_folder_name ( self , name , folder_name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name parameters [ 'folderName' ] = folder_name if token : parameters [ 'token' ] = token response = self . request ( 'midas.item.searchbynameandfoldername' , parameters ) return response [ 'items' ]
2762	def get_all_certificates ( self ) : data = self . get_data ( "certificates" ) certificates = list ( ) for jsoned in data [ 'certificates' ] : cert = Certificate ( * * jsoned ) cert . token = self . token certificates . append ( cert ) return certificates
8648	def reject_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'reject' , } # POST /api/projects/0.1/milestone_requests/{milestone_request_id}/?action= # reject endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotRejectedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
3097	def generate_token ( key , user_id , action_id = '' , when = None ) : digester = hmac . new ( _helpers . _to_bytes ( key , encoding = 'utf-8' ) ) digester . update ( _helpers . _to_bytes ( str ( user_id ) , encoding = 'utf-8' ) ) digester . update ( DELIMITER ) digester . update ( _helpers . _to_bytes ( action_id , encoding = 'utf-8' ) ) digester . update ( DELIMITER ) when = _helpers . _to_bytes ( str ( when or int ( time . time ( ) ) ) , encoding = 'utf-8' ) digester . update ( when ) digest = digester . digest ( ) token = base64 . urlsafe_b64encode ( digest + DELIMITER + when ) return token
12791	def delete ( self , url = None , post_data = { } , parse_data = False , key = None , parameters = None ) : return self . _fetch ( "DELETE" , url , post_data = post_data , parse_data = parse_data , key = key , parameters = parameters , full_return = True )
11417	def record_modify_controlfield ( rec , tag , controlfield_value , field_position_global = None , field_position_local = None ) : field = record_get_field ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) new_field = ( field [ 0 ] , field [ 1 ] , field [ 2 ] , controlfield_value , field [ 4 ] ) record_replace_field ( rec , tag , new_field , field_position_global = field_position_global , field_position_local = field_position_local )
234	def plot_sector_exposures_net ( net_exposures , sector_dict = None , ax = None ) : if ax is None : ax = plt . gca ( ) if sector_dict is None : sector_names = SECTORS . values ( ) else : sector_names = sector_dict . values ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 11 ) ) for i in range ( len ( net_exposures ) ) : ax . plot ( net_exposures [ i ] , color = color_list [ i ] , alpha = 0.8 , label = sector_names [ i ] ) ax . set ( title = 'Net exposures to sectors' , ylabel = 'Proportion of net exposure \n in sectors' ) return ax
12775	def inverse_dynamics ( self , angles , start = 0 , end = 1e100 , states = None , max_force = 100 ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , frame in enumerate ( angles ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) states = self . skeleton . get_body_states ( ) self . skeleton . set_body_states ( states ) # joseph's stability fix: step to compute torques, then reset the # skeleton to the start of the step, and then step using computed # torques. thus any numerical errors between the body states after # stepping using angle constraints will be removed, because we # will be stepping the model using the computed torques. self . skeleton . enable_motors ( max_force ) self . skeleton . set_target_angles ( angles [ frame_no ] ) self . ode_world . step ( self . dt ) torques = self . skeleton . joint_torques self . skeleton . disable_motors ( ) self . skeleton . set_body_states ( states ) self . skeleton . add_torques ( torques ) yield torques self . ode_world . step ( self . dt ) self . ode_contactgroup . empty ( )
4580	def toggle ( s ) : is_numeric = ',' in s or s . startswith ( '0x' ) or s . startswith ( '#' ) c = name_to_color ( s ) return color_to_name ( c ) if is_numeric else str ( c )
11053	def _issue_cert ( self , domain ) : def errback ( failure ) : # Don't fail on some of the errors we could get from the ACME # server, rather just log an error so that we can continue with # other domains. failure . trap ( txacme_ServerError ) acme_error = failure . value . message if acme_error . code in [ 'rateLimited' , 'serverInternal' , 'connection' , 'unknownHost' ] : # TODO: Fire off an error to Sentry or something? self . log . error ( 'Error ({code}) issuing certificate for "{domain}": ' '{detail}' , code = acme_error . code , domain = domain , detail = acme_error . detail ) else : # There are more error codes but if they happen then something # serious has gone wrong-- carry on error-ing. return failure d = self . txacme_service . issue_cert ( domain ) return d . addErrback ( errback )
10210	def _is_root ( ) : import os import ctypes try : return os . geteuid ( ) == 0 except AttributeError : return ctypes . windll . shell32 . IsUserAnAdmin ( ) != 0 return False
13003	def modify_data ( data ) : with tempfile . NamedTemporaryFile ( 'w' ) as f : for entry in data : f . write ( json . dumps ( entry . to_dict ( include_meta = True ) , default = datetime_handler ) ) f . write ( '\n' ) f . flush ( ) print_success ( "Starting editor" ) subprocess . call ( [ 'nano' , '-' , f . name ] ) with open ( f . name , 'r' ) as f : return f . readlines ( )
3717	def economic_status ( CASRN , Method = None , AvailableMethods = False ) : # pragma: no cover load_economic_data ( ) CASi = CAS2int ( CASRN ) def list_methods ( ) : methods = [ ] methods . append ( 'Combined' ) if CASRN in _EPACDRDict : methods . append ( EPACDR ) if CASRN in _ECHATonnageDict : methods . append ( ECHA ) if CASi in HPV_data . index : methods . append ( OECD ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] # This is the calculate, given the method section if Method == EPACDR : status = 'US public: ' + str ( _EPACDRDict [ CASRN ] ) elif Method == ECHA : status = _ECHATonnageDict [ CASRN ] elif Method == OECD : status = 'OECD HPV Chemicals' elif Method == 'Combined' : status = [ ] if CASRN in _EPACDRDict : status += [ 'US public: ' + str ( _EPACDRDict [ CASRN ] ) ] if CASRN in _ECHATonnageDict : status += _ECHATonnageDict [ CASRN ] if CASi in HPV_data . index : status += [ 'OECD HPV Chemicals' ] elif Method == NONE : status = None else : raise Exception ( 'Failure in in function' ) return status
12743	def get_urls ( self ) : urls = self . get_subfields ( "856" , "u" , i1 = "4" , i2 = "2" ) return map ( lambda x : x . replace ( "&amp;" , "&" ) , urls )
11298	def get_all_text ( node ) : if node . nodeType == node . TEXT_NODE : return node . data else : text_string = "" for child_node in node . childNodes : text_string += get_all_text ( child_node ) return text_string
1495	def find_closing_braces ( self , query ) : if query [ 0 ] != '(' : raise Exception ( "Trying to find closing braces for no opening braces" ) num_open_braces = 0 for i in range ( len ( query ) ) : c = query [ i ] if c == '(' : num_open_braces += 1 elif c == ')' : num_open_braces -= 1 if num_open_braces == 0 : return i raise Exception ( "No closing braces found" )
8097	def path ( s , graph , path ) : def end ( n ) : r = n . r * 0.35 s . _ctx . oval ( n . x - r , n . y - r , r * 2 , r * 2 ) if path and len ( path ) > 1 and s . stroke : s . _ctx . nofill ( ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a ) if s . name != DEFAULT : s . _ctx . strokewidth ( s . strokewidth ) else : s . _ctx . strokewidth ( s . strokewidth * 2 ) first = True for id in path : n = graph [ id ] if first : first = False s . _ctx . beginpath ( n . x , n . y ) end ( n ) else : s . _ctx . lineto ( n . x , n . y ) s . _ctx . endpath ( ) end ( n )
9891	def _uptime_amiga ( ) : global __boottime try : __boottime = os . stat ( 'RAM:' ) . st_ctime return time . time ( ) - __boottime except ( NameError , OSError ) : return None
4373	def create ( ) : name = request . form . get ( "name" ) if name : room , created = get_or_create ( ChatRoom , name = name ) return redirect ( url_for ( 'room' , slug = room . slug ) ) return redirect ( url_for ( 'rooms' ) )
1266	def make_game ( ) : return ascii_art . ascii_art_to_game ( GAME_ART , what_lies_beneath = ' ' , sprites = dict ( [ ( 'P' , PlayerSprite ) ] + [ ( c , UpwardLaserBoltSprite ) for c in UPWARD_BOLT_CHARS ] + [ ( c , DownwardLaserBoltSprite ) for c in DOWNWARD_BOLT_CHARS ] ) , drapes = dict ( X = MarauderDrape , B = BunkerDrape ) , update_schedule = [ 'P' , 'B' , 'X' ] + list ( _ALL_BOLT_CHARS ) )
405	def pixel_wise_softmax ( x , name = 'pixel_wise_softmax' ) : with tf . name_scope ( name ) : return tf . nn . softmax ( x )
1926	def save ( f ) : global _groups c = { } for group_name , group in _groups . items ( ) : section = { var . name : var . value for var in group . updated_vars ( ) } if not section : continue c [ group_name ] = section yaml . safe_dump ( c , f , line_break = True )
12380	def put ( self , request , response ) : if self . slug is None : # Mass-PUT is not implemented. raise http . exceptions . NotImplemented ( ) # Check if the resource exists. target = self . read ( ) # Deserialize and clean the incoming object. data = self . _clean ( target , self . request . read ( deserialize = True ) ) if target is not None : # Ensure we're allowed to update the resource. self . assert_operations ( 'update' ) try : # Delegate to `update` to create the item. self . update ( target , data ) except AttributeError : # No read method defined. raise http . exceptions . NotImplemented ( ) # Build the response object. self . make_response ( target ) else : # Ensure we're allowed to create the resource. self . assert_operations ( 'create' ) # Delegate to `create` to create the item. target = self . create ( data ) # Build the response object. self . response . status = http . client . CREATED self . make_response ( target )
11728	def _assert_contains ( haystack , needle , invert , escape = False ) : myneedle = re . escape ( needle ) if escape else needle matched = re . search ( myneedle , haystack , re . M ) if ( invert and matched ) or ( not invert and not matched ) : raise AssertionError ( "'%s' %sfound in '%s'" % ( needle , "" if invert else "not " , haystack ) )
8897	def _fsic_queuing_calc ( fsic1 , fsic2 ) : return { instance : fsic2 . get ( instance , 0 ) for instance , counter in six . iteritems ( fsic1 ) if fsic2 . get ( instance , 0 ) < counter }
13379	def _join_seq ( d , k , v ) : if k not in d : d [ k ] = list ( v ) elif isinstance ( d [ k ] , list ) : for item in v : if item not in d [ k ] : d [ k ] . insert ( 0 , item ) elif isinstance ( d [ k ] , string_types ) : v . append ( d [ k ] ) d [ k ] = v
868	def resetCustomConfig ( cls ) : _getLogger ( ) . info ( "Resetting all custom configuration properties; " "caller=%r" , traceback . format_stack ( ) ) # Clear the in-memory settings cache, forcing reload upon subsequent "get" # request. super ( Configuration , cls ) . clear ( ) # Delete the persistent custom configuration store and reset in-memory # custom configuration info _CustomConfigurationFileWrapper . clear ( persistent = True )
10367	def complex_increases_activity ( graph : BELGraph , u : BaseEntity , v : BaseEntity , key : str ) -> bool : return ( isinstance ( u , ( ComplexAbundance , NamedComplexAbundance ) ) and complex_has_member ( graph , u , v ) and part_has_modifier ( graph [ u ] [ v ] [ key ] , OBJECT , ACTIVITY ) )
3887	def add_observer ( self , callback ) : if callback in self . _observers : raise ValueError ( '{} is already an observer of {}' . format ( callback , self ) ) self . _observers . append ( callback )
11158	def mirror_to ( self , dst ) : # pragma: no cover self . assert_is_dir_and_exists ( ) src = self . abspath dst = os . path . abspath ( dst ) if os . path . exists ( dst ) : # pragma: no cover raise Exception ( "distination already exist!" ) folder_to_create = list ( ) file_to_create = list ( ) for current_folder , _ , file_list in os . walk ( self . abspath ) : current_folder = current_folder . replace ( src , dst ) try : os . mkdir ( current_folder ) except : # pragma: no cover pass for basename in file_list : abspath = os . path . join ( current_folder , basename ) with open ( abspath , "wb" ) as _ : pass
13196	def ensure_format ( doc , format ) : assert format in ( 'xml' , 'json' ) if getattr ( doc , 'tag' , None ) == 'open511' : if format == 'json' : return xml_to_json ( doc ) elif isinstance ( doc , dict ) and 'meta' in doc : if format == 'xml' : return json_doc_to_xml ( doc ) else : raise ValueError ( "Unrecognized input document" ) return doc
824	def mostLikely ( self , pred ) : if len ( pred ) == 1 : return pred . keys ( ) [ 0 ] mostLikelyOutcome = None maxProbability = 0 for prediction , probability in pred . items ( ) : if probability > maxProbability : mostLikelyOutcome = prediction maxProbability = probability return mostLikelyOutcome
3123	def _check_audience ( payload_dict , audience ) : if audience is None : return audience_in_payload = payload_dict . get ( 'aud' ) if audience_in_payload is None : raise AppIdentityError ( 'No aud field in token: {0}' . format ( payload_dict ) ) if audience_in_payload != audience : raise AppIdentityError ( 'Wrong recipient, {0} != {1}: {2}' . format ( audience_in_payload , audience , payload_dict ) )
13125	def id_to_object ( self , line ) : user = User . get ( line , ignore = 404 ) if not user : user = User ( username = line ) user . save ( ) return user
8748	def update_scalingip ( context , id , content ) : LOG . info ( 'update_scalingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) requested_ports = content . get ( 'ports' , [ ] ) flip = _update_flip ( context , id , ip_types . SCALING , requested_ports ) return v . _make_scaling_ip_dict ( flip )
6629	def read ( self , filenames ) : for fn in filenames : try : self . configs [ fn ] = ordered_json . load ( fn ) except IOError : self . configs [ fn ] = OrderedDict ( ) except Exception as e : self . configs [ fn ] = OrderedDict ( ) logging . warning ( "Failed to read settings file %s, it will be ignored. The error was: %s" , fn , e )
5317	def use_style ( self , style_name ) : try : style = getattr ( styles , style_name . upper ( ) ) except AttributeError : raise ColorfulError ( 'the style "{0}" is undefined' . format ( style_name ) ) else : self . colorpalette = style
5599	def open ( self , tile , process , * * kwargs ) : return InputTile ( tile , process , kwargs . get ( "resampling" , None ) )
1758	def _raw_read ( self , where : int , size = 1 ) -> bytes : map = self . memory . map_containing ( where ) start = map . _get_offset ( where ) mapType = type ( map ) if mapType is FileMap : end = map . _get_offset ( where + size ) if end > map . _mapped_size : logger . warning ( f"Missing {end - map._mapped_size} bytes at the end of {map._filename}" ) raw_data = map . _data [ map . _get_offset ( where ) : min ( end , map . _mapped_size ) ] if len ( raw_data ) < end : raw_data += b'\x00' * ( end - len ( raw_data ) ) data = b'' for offset in sorted ( map . _overlay . keys ( ) ) : data += raw_data [ len ( data ) : offset ] data += map . _overlay [ offset ] data += raw_data [ len ( data ) : ] elif mapType is AnonMap : data = bytes ( map . _data [ start : start + size ] ) else : data = b'' . join ( self . memory [ where : where + size ] ) assert len ( data ) == size , 'Raw read resulted in wrong data read which should never happen' return data
371	def flip_axis_multi ( x , axis , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : # x = np.asarray(x).swapaxes(axis, 0) # x = x[::-1, ...] # x = x.swapaxes(0, axis) # return x results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results ) else : return np . asarray ( x ) else : # x = np.asarray(x).swapaxes(axis, 0) # x = x[::-1, ...] # x = x.swapaxes(0, axis) # return x results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results )
13008	def path ( self ) : path = super ( WindowsPath2 , self ) . path if path . startswith ( "\\\\?\\" ) : return path [ 4 : ] return path
8509	def fit ( self , X , y = None ) : from pylearn2 . config import yaml_parse from pylearn2 . train import Train # build trainer params = self . get_params ( ) yaml_string = Template ( self . yaml_string ) . substitute ( params ) self . trainer = yaml_parse . load ( yaml_string ) assert isinstance ( self . trainer , Train ) if self . trainer . dataset is not None : raise ValueError ( 'Train YAML database must evaluate to None.' ) self . trainer . dataset = self . _get_dataset ( X , y ) # update monitoring dataset(s) if ( hasattr ( self . trainer . algorithm , 'monitoring_dataset' ) and self . trainer . algorithm . monitoring_dataset is not None ) : monitoring_dataset = self . trainer . algorithm . monitoring_dataset if len ( monitoring_dataset ) == 1 and '' in monitoring_dataset : monitoring_dataset [ '' ] = self . trainer . dataset else : monitoring_dataset [ 'train' ] = self . trainer . dataset self . trainer . algorithm . _set_monitoring_dataset ( monitoring_dataset ) else : self . trainer . algorithm . _set_monitoring_dataset ( self . trainer . dataset ) # run main loop self . trainer . main_loop ( )
12082	def clampfit_rename ( path , char ) : assert len ( char ) == 1 and type ( char ) == str , "replacement character must be a single character" assert os . path . exists ( path ) , "path doesn't exist" files = sorted ( os . listdir ( path ) ) files = [ x for x in files if len ( x ) > 18 and x [ 4 ] + x [ 7 ] + x [ 10 ] == '___' ] for fname in files : fname2 = list ( fname ) fname2 [ 11 ] = char fname2 = "" . join ( fname2 ) if fname == fname2 : print ( fname , "==" , fname2 ) else : print ( fname , "->" , fname2 ) # fname=os.path.join(path,fname) # fname2=os.path.join(path,fname2) # if not os.path.exists(fname2): # os.rename(fname,fname2) return
300	def plot_slippage_sensitivity ( returns , positions , transactions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) avg_returns_given_slippage = pd . Series ( ) for bps in range ( 1 , 100 ) : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) avg_returns = ep . annual_return ( adj_returns ) avg_returns_given_slippage . loc [ bps ] = avg_returns avg_returns_given_slippage . plot ( alpha = 1.0 , lw = 2 , ax = ax ) ax . set_title ( 'Average annual returns given additional per-dollar slippage' ) ax . set_xticks ( np . arange ( 0 , 100 , 10 ) ) ax . set_ylabel ( 'Average annual return' ) ax . set_xlabel ( 'Per-dollar slippage (bps)' ) return ax
8757	def _validate_subnet_cidr ( context , network_id , new_subnet_cidr ) : if neutron_cfg . cfg . CONF . allow_overlapping_ips : return try : new_subnet_ipset = netaddr . IPSet ( [ new_subnet_cidr ] ) except TypeError : LOG . exception ( "Invalid or missing cidr: %s" % new_subnet_cidr ) raise n_exc . BadRequest ( resource = "subnet" , msg = "Invalid or missing cidr" ) filters = { 'network_id' : network_id , 'shared' : [ False ] } # Using admin context here, in case we actually share networks later subnet_list = db_api . subnet_find ( context = context . elevated ( ) , * * filters ) for subnet in subnet_list : if ( netaddr . IPSet ( [ subnet . cidr ] ) & new_subnet_ipset ) : # don't give out details of the overlapping subnet err_msg = ( _ ( "Requested subnet with cidr: %(cidr)s for " "network: %(network_id)s overlaps with another " "subnet" ) % { 'cidr' : new_subnet_cidr , 'network_id' : network_id } ) LOG . error ( _ ( "Validation for CIDR: %(new_cidr)s failed - " "overlaps with subnet %(subnet_id)s " "(CIDR: %(cidr)s)" ) , { 'new_cidr' : new_subnet_cidr , 'subnet_id' : subnet . id , 'cidr' : subnet . cidr } ) raise n_exc . InvalidInput ( error_message = err_msg )
10069	def index ( method = None , delete = False ) : if method is None : return partial ( index , delete = delete ) @ wraps ( method ) def wrapper ( self_or_cls , * args , * * kwargs ) : """Send record for indexing.""" result = method ( self_or_cls , * args , * * kwargs ) try : if delete : self_or_cls . indexer . delete ( result ) else : self_or_cls . indexer . index ( result ) except RequestError : current_app . logger . exception ( 'Could not index {0}.' . format ( result ) ) return result return wrapper
8700	def __exchange ( self , output , timeout = None ) : self . __writeln ( output ) self . _port . flush ( ) return self . __expect ( timeout = timeout or self . _timeout )
9649	def determine_paths ( self , package_name = None , create_package_dir = False , dry_run = False ) : # Give preference to the environment variable here as it will not # derefrence sym links self . project_dir = Path ( os . getenv ( 'PWD' ) or os . getcwd ( ) ) # Try and work out the project name distribution = self . get_distribution ( ) if distribution : # Get name from setup.py self . project_name = distribution . get_name ( ) else : # ...failing that, use the current directory name self . project_name = self . project_dir . name # Descend into the 'src' directory to find the package # if necessary if os . path . isdir ( self . project_dir / "src" ) : package_search_dir = self . project_dir / "src" else : package_search_dir = self . project_dir created_package_dir = False if not package_name : # Lets try and work out the package_name from the project_name package_name = self . project_name . replace ( "-" , "_" ) # Now do some fuzzy matching def get_matches ( name ) : possibles = [ n for n in os . listdir ( package_search_dir ) if os . path . isdir ( package_search_dir / n ) ] return difflib . get_close_matches ( name , possibles , n = 1 , cutoff = 0.8 ) close = get_matches ( package_name ) # If no matches, try removing the first part of the package name # (e.g. django-guardian becomes guardian) if not close and "_" in package_name : short_package_name = "_" . join ( package_name . split ( "_" ) [ 1 : ] ) close = get_matches ( short_package_name ) if not close : if create_package_dir : package_dir = package_search_dir / package_name # Gets set to true even during dry run created_package_dir = True if not dry_run : print ( "Creating package directory at %s" % package_dir ) os . mkdir ( package_dir ) else : print ( "Would have created package directory at %s" % package_dir ) else : raise CommandError ( "Could not guess the package name. Specify it using --name." ) else : package_name = close [ 0 ] self . package_name = package_name self . package_dir = package_search_dir / package_name if not os . path . exists ( self . package_dir ) and not created_package_dir : raise CommandError ( "Package directory did not exist at %s. Perhaps specify it using --name" % self . package_dir )
7804	def verify_server ( self , server_name , srv_type = 'xmpp-client' ) : server_jid = JID ( server_name ) if "XmppAddr" not in self . alt_names and "DNS" not in self . alt_names and "SRV" not in self . alt_names : return self . verify_jid_against_common_name ( server_jid ) names = [ name for name in self . alt_names . get ( "DNS" , [ ] ) if not name . startswith ( u"*." ) ] names += self . alt_names . get ( "XmppAddr" , [ ] ) for name in names : logger . debug ( "checking {0!r} against {1!r}" . format ( server_jid , name ) ) try : jid = JID ( name ) except ValueError : logger . debug ( "Not a valid JID: {0!r}" . format ( name ) ) continue if jid == server_jid : logger . debug ( "Match!" ) return True if srv_type and self . verify_jid_against_srv_name ( server_jid , srv_type ) : return True wildcards = [ name [ 2 : ] for name in self . alt_names . get ( "DNS" , [ ] ) if name . startswith ( "*." ) ] if not wildcards or not "." in server_jid . domain : return False logger . debug ( "checking {0!r} against wildcard domains: {1!r}" . format ( server_jid , wildcards ) ) server_domain = JID ( domain = server_jid . domain . split ( "." , 1 ) [ 1 ] ) for domain in wildcards : logger . debug ( "checking {0!r} against {1!r}" . format ( server_domain , domain ) ) try : jid = JID ( domain ) except ValueError : logger . debug ( "Not a valid JID: {0!r}" . format ( name ) ) continue if jid == server_domain : logger . debug ( "Match!" ) return True return False
3536	def gauges ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return GaugesNode ( )
4667	def refresh ( self ) : dict . __init__ ( self , self . blockchain . rpc . get_object ( self . identifier ) , blockchain_instance = self . blockchain , )
7641	def parse_arguments ( args ) : parser = argparse . ArgumentParser ( description = 'Convert JAMS to .lab files' ) parser . add_argument ( '-c' , '--comma-separated' , dest = 'csv' , action = 'store_true' , default = False , help = 'Output in .csv instead of .lab' ) parser . add_argument ( '--comment' , dest = 'comment_char' , type = str , default = '#' , help = 'Comment character' ) parser . add_argument ( '-n' , '--namespace' , dest = 'namespaces' , nargs = '+' , default = [ '.*' ] , help = 'One or more namespaces to output. Default is all.' ) parser . add_argument ( 'jams_file' , help = 'Path to the input jams file' ) parser . add_argument ( 'output_prefix' , help = 'Prefix for output files' ) return vars ( parser . parse_args ( args ) )
2355	def find_elements ( self , strategy , locator ) : return self . driver_adapter . find_elements ( strategy , locator , root = self . root )
10590	def report ( self , format = ReportFormat . printout , output_path = None ) : rpt = GlsRpt ( self , output_path ) return rpt . render ( format )
7711	def _get_success ( self , stanza ) : payload = stanza . get_payload ( RosterPayload ) if payload is None : if "versioning" in self . server_features and self . roster : logger . debug ( "Server will send roster delta in pushes" ) else : logger . warning ( "Bad roster response (no payload)" ) self . _event_queue . put ( RosterNotReceivedEvent ( self , stanza ) ) return else : items = list ( payload ) for item in items : item . verify_roster_result ( True ) self . roster = Roster ( items , payload . version ) self . _event_queue . put ( RosterReceivedEvent ( self , self . roster ) )
8540	def _read_config ( self , filename = None ) : if filename : self . _config_filename = filename else : try : import appdirs except ImportError : raise Exception ( "Missing dependency for determining config path. Please install " "the 'appdirs' Python module." ) self . _config_filename = appdirs . user_config_dir ( _LIBRARY_NAME , "ProfitBricks" ) + ".ini" if not self . _config : self . _config = configparser . ConfigParser ( ) self . _config . optionxform = str self . _config . read ( self . _config_filename )
7985	def submit_registration_form ( self , form ) : self . lock . acquire ( ) try : if form and form . type != "cancel" : self . registration_form = form iq = Iq ( stanza_type = "set" ) iq . set_content ( self . __register . submit_form ( form ) ) self . set_response_handlers ( iq , self . registration_success , self . registration_error ) self . send ( iq ) else : self . __register = None finally : self . lock . release ( )
8295	def cliques ( graph , threshold = 3 ) : cliques = [ ] for n in graph . nodes : c = clique ( graph , n . id ) if len ( c ) >= threshold : c . sort ( ) if c not in cliques : cliques . append ( c ) return cliques
10129	def _map_timezones ( ) : tz_map = { } todo = HAYSTACK_TIMEZONES_SET . copy ( ) for full_tz in pytz . all_timezones : # Finished case: if not bool ( todo ) : # pragma: no cover # This is nearly impossible for us to cover, and an unlikely case. break # Case 1: exact match if full_tz in todo : tz_map [ full_tz ] = full_tz # Exact match todo . discard ( full_tz ) continue # Case 2: suffix match after '/' if '/' not in full_tz : continue ( prefix , suffix ) = full_tz . split ( '/' , 1 ) # Case 2 exception: full timezone contains more than one '/' -> ignore if '/' in suffix : continue if suffix in todo : tz_map [ suffix ] = full_tz todo . discard ( suffix ) continue return tz_map
13111	def verbose ( cls , key = False , default = '' ) : if key is False : items = cls . _item_dict . values ( ) return [ ( x . key , x . value ) for x in sorted ( items , key = lambda x : x . sort or x . key ) ] item = cls . _item_dict . get ( key ) return item . value if item else default
5630	def _postreceive ( self ) : digest = self . _get_digest ( ) if digest is not None : sig_parts = _get_header ( 'X-Hub-Signature' ) . split ( '=' , 1 ) if not isinstance ( digest , six . text_type ) : digest = six . text_type ( digest ) if ( len ( sig_parts ) < 2 or sig_parts [ 0 ] != 'sha1' or not hmac . compare_digest ( sig_parts [ 1 ] , digest ) ) : abort ( 400 , 'Invalid signature' ) event_type = _get_header ( 'X-Github-Event' ) data = request . get_json ( ) if data is None : abort ( 400 , 'Request body must contain json' ) self . _logger . info ( '%s (%s)' , _format_event ( event_type , data ) , _get_header ( 'X-Github-Delivery' ) ) for hook in self . _hooks . get ( event_type , [ ] ) : hook ( data ) return '' , 204
3220	def get_client ( service , service_type = 'client' , * * conn_args ) : client_details = choose_client ( service ) user_agent = get_user_agent ( * * conn_args ) if client_details : if client_details [ 'client_type' ] == 'cloud' : client = get_gcp_client ( mod_name = client_details [ 'module_name' ] , pkg_name = conn_args . get ( 'pkg_name' , 'google.cloud' ) , key_file = conn_args . get ( 'key_file' , None ) , project = conn_args [ 'project' ] , user_agent = user_agent ) else : client = get_google_client ( mod_name = client_details [ 'module_name' ] , key_file = conn_args . get ( 'key_file' , None ) , user_agent = user_agent , api_version = conn_args . get ( 'api_version' , 'v1' ) ) else : # There is no client known for this service. We can try the standard API. try : client = get_google_client ( mod_name = service , key_file = conn_args . get ( 'key_file' , None ) , user_agent = user_agent , api_version = conn_args . get ( 'api_version' , 'v1' ) ) except Exception as e : raise e return client_details , client
12180	def get_author_and_version ( package ) : init_py = open ( os . path . join ( package , '__init__.py' ) ) . read ( ) author = re . search ( "__author__ = ['\"]([^'\"]+)['\"]" , init_py ) . group ( 1 ) version = re . search ( "__version__ = ['\"]([^'\"]+)['\"]" , init_py ) . group ( 1 ) return author , version
4215	def name ( cls ) : parent , sep , mod_name = cls . __module__ . rpartition ( '.' ) mod_name = mod_name . replace ( '_' , ' ' ) return ' ' . join ( [ mod_name , cls . __name__ ] )
10533	def update_project ( project ) : try : project_id = project . id project = _forbidden_attributes ( project ) res = _pybossa_req ( 'put' , 'project' , project_id , payload = project . data ) if res . get ( 'id' ) : return Project ( res ) else : return res except : # pragma: no cover raise
307	def show_profit_attribution ( round_trips ) : total_pnl = round_trips [ 'pnl' ] . sum ( ) pnl_attribution = round_trips . groupby ( 'symbol' ) [ 'pnl' ] . sum ( ) / total_pnl pnl_attribution . name = '' pnl_attribution . index = pnl_attribution . index . map ( utils . format_asset ) utils . print_table ( pnl_attribution . sort_values ( inplace = False , ascending = False , ) , name = 'Profitability (PnL / PnL total) per name' , float_format = '{:.2%}' . format , )
5217	def check_hours ( tickers , tz_exch , tz_loc = DEFAULT_TZ ) -> pd . DataFrame : cols = [ 'Trading_Day_Start_Time_EOD' , 'Trading_Day_End_Time_EOD' ] con , _ = create_connection ( ) hours = con . ref ( tickers = tickers , flds = cols ) cur_dt = pd . Timestamp ( 'today' ) . strftime ( '%Y-%m-%d ' ) hours . loc [ : , 'local' ] = hours . value . astype ( str ) . str [ : - 3 ] hours . loc [ : , 'exch' ] = pd . DatetimeIndex ( cur_dt + hours . value . astype ( str ) ) . tz_localize ( tz_loc ) . tz_convert ( tz_exch ) . strftime ( '%H:%M' ) hours = pd . concat ( [ hours . set_index ( [ 'ticker' , 'field' ] ) . exch . unstack ( ) . loc [ : , cols ] , hours . set_index ( [ 'ticker' , 'field' ] ) . local . unstack ( ) . loc [ : , cols ] , ] , axis = 1 ) hours . columns = [ 'Exch_Start' , 'Exch_End' , 'Local_Start' , 'Local_End' ] return hours
2035	def SLOAD ( self , offset ) : storage_address = self . address self . _publish ( 'will_evm_read_storage' , storage_address , offset ) value = self . world . get_storage_data ( storage_address , offset ) self . _publish ( 'did_evm_read_storage' , storage_address , offset , value ) return value
3748	def calculate ( self , T , method ) : if method == GHARAGHEIZI : mu = Gharagheizi_gas_viscosity ( T , self . Tc , self . Pc , self . MW ) elif method == COOLPROP : mu = CoolProp_T_dependent_property ( T , self . CASRN , 'V' , 'g' ) elif method == DIPPR_PERRY_8E : mu = EQ102 ( T , * self . Perrys2_312_coeffs ) elif method == VDI_PPDS : mu = horner ( self . VDI_PPDS_coeffs , T ) elif method == YOON_THODOS : mu = Yoon_Thodos ( T , self . Tc , self . Pc , self . MW ) elif method == STIEL_THODOS : mu = Stiel_Thodos ( T , self . Tc , self . Pc , self . MW ) elif method == LUCAS_GAS : mu = lucas_gas ( T , self . Tc , self . Pc , self . Zc , self . MW , self . dipole , CASRN = self . CASRN ) elif method in self . tabular_data : mu = self . interpolate ( T , method ) return mu
7174	def _train_and_save ( obj , cache , data , print_updates ) : obj . train ( data ) if print_updates : print ( 'Regenerated ' + obj . name + '.' ) obj . save ( cache )
2006	def _serialize_int ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError if not isinstance ( value , ( int , BitVec ) ) : raise ValueError if issymbolic ( value ) : buf = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) value = Operators . SEXTEND ( value , value . size , size * 8 ) buf = ArrayProxy ( buf . write_BE ( padding , value , size ) ) else : value = int ( value ) buf = bytearray ( ) for _ in range ( padding ) : buf . append ( 0 ) for position in reversed ( range ( size ) ) : buf . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) return buf
10474	def _isSingleCharacter ( keychr ) : if not keychr : return False # Regular character case. if len ( keychr ) == 1 : return True # Tagged character case. return keychr . count ( '<' ) == 1 and keychr . count ( '>' ) == 1 and keychr [ 0 ] == '<' and keychr [ - 1 ] == '>'
9191	def _insert_metadata ( cursor , model , publisher , message ) : params = model . metadata . copy ( ) params [ 'publisher' ] = publisher params [ 'publication_message' ] = message params [ '_portal_type' ] = _model_to_portaltype ( model ) params [ 'summary' ] = str ( cnxepub . DocumentSummaryFormatter ( model ) ) # Transform person structs to id lists for database array entry. for person_field in ATTRIBUTED_ROLE_KEYS : params [ person_field ] = [ parse_user_uri ( x [ 'id' ] ) for x in params . get ( person_field , [ ] ) ] params [ 'parent_ident_hash' ] = parse_parent_ident_hash ( model ) # Assign the id and version if one is known. if model . ident_hash is not None : uuid , version = split_ident_hash ( model . ident_hash , split_version = True ) params [ '_uuid' ] = uuid params [ '_major_version' ] , params [ '_minor_version' ] = version # Lookup legacy ``moduleid``. cursor . execute ( "SELECT moduleid FROM latest_modules WHERE uuid = %s" , ( uuid , ) ) # There is the chance that a uuid and version have been set, # but a previous publication does not exist. Therefore the # moduleid will not be found. This happens on a pre-publication. try : moduleid = cursor . fetchone ( ) [ 0 ] except TypeError : # NoneType moduleid = None params [ '_moduleid' ] = moduleid # Verify that uuid is reserved in document_contols. If not, add it. cursor . execute ( "SELECT * from document_controls where uuid = %s" , ( uuid , ) ) try : cursor . fetchone ( ) [ 0 ] except TypeError : # NoneType cursor . execute ( "INSERT INTO document_controls (uuid) VALUES (%s)" , ( uuid , ) ) created = model . metadata . get ( 'created' , None ) # Format the statement to accept the identifiers. stmt = MODULE_INSERTION_TEMPLATE . format ( * * { '__uuid__' : "%(_uuid)s::uuid" , '__major_version__' : "%(_major_version)s" , '__minor_version__' : "%(_minor_version)s" , '__moduleid__' : moduleid is None and "DEFAULT" or "%(_moduleid)s" , '__created__' : created is None and "DEFAULT" or "%(created)s" , } ) else : created = model . metadata . get ( 'created' , None ) # Format the statement for defaults. stmt = MODULE_INSERTION_TEMPLATE . format ( * * { '__uuid__' : "DEFAULT" , '__major_version__' : "DEFAULT" , '__minor_version__' : "DEFAULT" , '__moduleid__' : "DEFAULT" , '__created__' : created is None and "DEFAULT" or "%(created)s" , } ) # Insert the metadata cursor . execute ( stmt , params ) module_ident , ident_hash = cursor . fetchone ( ) # Insert optional roles _insert_optional_roles ( cursor , model , module_ident ) return module_ident , ident_hash
2493	def create_annotation_node ( self , annotation ) : annotation_node = URIRef ( str ( annotation . spdx_id ) ) type_triple = ( annotation_node , RDF . type , self . spdx_namespace . Annotation ) self . graph . add ( type_triple ) annotator_node = Literal ( annotation . annotator . to_value ( ) ) self . graph . add ( ( annotation_node , self . spdx_namespace . annotator , annotator_node ) ) annotation_date_node = Literal ( annotation . annotation_date_iso_format ) annotation_triple = ( annotation_node , self . spdx_namespace . annotationDate , annotation_date_node ) self . graph . add ( annotation_triple ) if annotation . has_comment : comment_node = Literal ( annotation . comment ) comment_triple = ( annotation_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) annotation_type_node = Literal ( annotation . annotation_type ) annotation_type_triple = ( annotation_node , self . spdx_namespace . annotationType , annotation_type_node ) self . graph . add ( annotation_type_triple ) return annotation_node
13487	def get_or_create_index ( self , index_ratio , index_width ) : if not self . index_path . exists ( ) or not self . filepath . stat ( ) . st_mtime == self . index_path . stat ( ) . st_mtime : create_index ( self . filepath , self . index_path , index_ratio = index_ratio , index_width = index_width ) return IndexFile ( str ( self . index_path ) )
10545	def delete_task ( task_id ) : #: :arg task: A task try : res = _pybossa_req ( 'delete' , 'task' , task_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : # pragma: no cover raise
6353	def _normalize_lang_attrs ( self , text , strip ) : uninitialized = - 1 # all 1's attrib = uninitialized while '[' in text : bracket_start = text . find ( '[' ) bracket_end = text . find ( ']' , bracket_start ) if bracket_end == - 1 : raise ValueError ( 'No closing square bracket: text=(' + text + ') strip=(' + text_type ( strip ) + ')' ) attrib &= int ( text [ bracket_start + 1 : bracket_end ] ) text = text [ : bracket_start ] + text [ bracket_end + 1 : ] if attrib == uninitialized or strip : return text elif attrib == 0 : # means that the attributes were incompatible and there is no # alternative here return '[0]' return text + '[' + str ( attrib ) + ']'
4091	def addSources ( self , * sources ) : self . _sources . extend ( sources ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB source(s): %s' % ', ' . join ( [ str ( x ) for x in self . _sources ] ) ) return self
11469	def rmdir ( self , foldername ) : current_folder = self . _ftp . pwd ( ) try : self . cd ( foldername ) except error_perm : print ( '550 Delete operation failed folder %s ' 'does not exist!' % ( foldername , ) ) else : self . cd ( current_folder ) try : self . _ftp . rmd ( foldername ) except error_perm : # folder not empty self . cd ( foldername ) contents = self . ls ( ) #delete the files map ( self . _ftp . delete , contents [ 0 ] ) #delete the subfolders map ( self . rmdir , contents [ 1 ] ) self . cd ( current_folder ) self . _ftp . rmd ( foldername )
12752	def indices_for_joint ( self , name ) : j = 0 for joint in self . joints : if joint . name == name : return list ( range ( j , j + joint . ADOF ) ) j += joint . ADOF return [ ]
13629	def put ( self , metrics ) : if type ( metrics ) == list : for metric in metrics : self . c . put_metric_data ( * * metric ) else : self . c . put_metric_data ( * * metrics )
10110	def write ( self , _force = False , _exists_ok = False , * * items ) : if self . fname and self . fname . exists ( ) : raise ValueError ( 'db file already exists, use force=True to overwrite' ) with self . connection ( ) as db : for table in self . tables : db . execute ( table . sql ( translate = self . translate ) ) db . execute ( 'PRAGMA foreign_keys = ON;' ) db . commit ( ) refs = defaultdict ( list ) # collects rows in association tables. for t in self . tables : if t . name not in items : continue rows , keys = [ ] , [ ] cols = { c . name : c for c in t . columns } for i , row in enumerate ( items [ t . name ] ) : pk = row [ t . primary_key [ 0 ] ] if t . primary_key and len ( t . primary_key ) == 1 else None values = [ ] for k , v in row . items ( ) : if k in t . many_to_many : assert pk at = t . many_to_many [ k ] atkey = tuple ( [ at . name ] + [ c . name for c in at . columns ] ) for vv in v : fkey , context = self . association_table_context ( t , k , vv ) refs [ atkey ] . append ( ( pk , fkey , context ) ) else : col = cols [ k ] if isinstance ( v , list ) : # Note: This assumes list-valued columns are of datatype string! v = ( col . separator or ';' ) . join ( col . convert ( vv ) for vv in v ) else : v = col . convert ( v ) if v is not None else None if i == 0 : keys . append ( col . name ) values . append ( v ) rows . append ( tuple ( values ) ) insert ( db , self . translate , t . name , keys , * rows ) for atkey , rows in refs . items ( ) : insert ( db , self . translate , atkey [ 0 ] , atkey [ 1 : ] , * rows ) db . commit ( )
6116	def unmasked_for_shape_and_pixel_scale ( cls , shape , pixel_scale , invert = False ) : mask = np . full ( tuple ( map ( lambda d : int ( d ) , shape ) ) , False ) if invert : mask = np . invert ( mask ) return cls ( array = mask , pixel_scale = pixel_scale )
12665	def apply_mask ( image , mask_img ) : img = check_img ( image ) mask = check_img ( mask_img ) check_img_compatibility ( img , mask ) vol = img . get_data ( ) mask_data , _ = load_mask_data ( mask ) return vol [ mask_data ] , mask_data
7130	def create_log_config ( verbose , quiet ) : if verbose and quiet : raise ValueError ( "Supplying both --quiet and --verbose makes no sense." ) elif verbose : level = logging . DEBUG elif quiet : level = logging . ERROR else : level = logging . INFO logger_cfg = { "handlers" : [ "click_handler" ] , "level" : level } return { "version" : 1 , "formatters" : { "click_formatter" : { "format" : "%(message)s" } } , "handlers" : { "click_handler" : { "level" : level , "class" : "doc2dash.__main__.ClickEchoHandler" , "formatter" : "click_formatter" , } } , "loggers" : { "doc2dash" : logger_cfg , "__main__" : logger_cfg } , }
5070	def format_price ( price , currency = '$' ) : if int ( price ) == price : return '{}{}' . format ( currency , int ( price ) ) return '{}{:0.2f}' . format ( currency , price )
12569	def save ( self , ds_name , data , dtype = None ) : return self . create_dataset ( ds_name , data , dtype )
463	def open_tensorboard ( log_dir = '/tmp/tensorflow' , port = 6006 ) : text = "[TL] Open tensorboard, go to localhost:" + str ( port ) + " to access" text2 = " not yet supported by this function (tl.ops.open_tb)" if not tl . files . exists_or_mkdir ( log_dir , verbose = False ) : tl . logging . info ( "[TL] Log reportory was created at %s" % log_dir ) if _platform == "linux" or _platform == "linux2" : raise NotImplementedError ( ) elif _platform == "darwin" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( sys . prefix + " | python -m tensorflow.tensorboard --logdir=" + log_dir + " --port=" + str ( port ) , shell = True ) # open tensorboard in localhost:6006/ or whatever port you chose elif _platform == "win32" : raise NotImplementedError ( "this function is not supported on the Windows platform" ) else : tl . logging . info ( _platform + text2 )
11782	def check_me ( self ) : assert len ( self . attrnames ) == len ( self . attrs ) assert self . target in self . attrs assert self . target not in self . inputs assert set ( self . inputs ) . issubset ( set ( self . attrs ) ) map ( self . check_example , self . examples )
13605	def url_correct ( self , point , auth = None , export = None ) : newUrl = self . __url + point + '.json' if auth or export : newUrl += "?" if auth : newUrl += ( "auth=" + auth ) if export : if not newUrl . endswith ( '?' ) : newUrl += "&" newUrl += "format=export" return newUrl
1030	def b32encode ( s ) : parts = [ ] quanta , leftover = divmod ( len ( s ) , 5 ) # Pad the last quantum with zero bits if necessary if leftover : s += ( '\0' * ( 5 - leftover ) ) quanta += 1 for i in range ( quanta ) : # c1 and c2 are 16 bits wide, c3 is 8 bits wide. The intent of this # code is to process the 40 bits in units of 5 bits. So we take the 1 # leftover bit of c1 and tack it onto c2. Then we take the 2 leftover # bits of c2 and tack them onto c3. The shifts and masks are intended # to give us values of exactly 5 bits in width. c1 , c2 , c3 = struct . unpack ( '!HHB' , s [ i * 5 : ( i + 1 ) * 5 ] ) c2 += ( c1 & 1 ) << 16 # 17 bits wide c3 += ( c2 & 3 ) << 8 # 10 bits wide parts . extend ( [ _b32tab [ c1 >> 11 ] , # bits 1 - 5 _b32tab [ ( c1 >> 6 ) & 0x1f ] , # bits 6 - 10 _b32tab [ ( c1 >> 1 ) & 0x1f ] , # bits 11 - 15 _b32tab [ c2 >> 12 ] , # bits 16 - 20 (1 - 5) _b32tab [ ( c2 >> 7 ) & 0x1f ] , # bits 21 - 25 (6 - 10) _b32tab [ ( c2 >> 2 ) & 0x1f ] , # bits 26 - 30 (11 - 15) _b32tab [ c3 >> 5 ] , # bits 31 - 35 (1 - 5) _b32tab [ c3 & 0x1f ] , # bits 36 - 40 (1 - 5) ] ) encoded = EMPTYSTRING . join ( parts ) # Adjust for any leftover partial quanta if leftover == 1 : return encoded [ : - 6 ] + '======' elif leftover == 2 : return encoded [ : - 4 ] + '====' elif leftover == 3 : return encoded [ : - 3 ] + '===' elif leftover == 4 : return encoded [ : - 1 ] + '=' return encoded
13148	def freeze ( self ) : data = super ( IndexBuilder , self ) . freeze ( ) try : # Sphinx >= 1.5 format # Due to changes from github.com/sphinx-doc/sphinx/pull/2454 base_file_names = data [ 'docnames' ] except KeyError : # Sphinx < 1.5 format base_file_names = data [ 'filenames' ] store = { } c = itertools . count ( ) for prefix , items in iteritems ( data [ 'objects' ] ) : for name , ( index , typeindex , _ , shortanchor ) in iteritems ( items ) : objtype = data [ 'objtypes' ] [ typeindex ] if objtype . startswith ( 'cpp:' ) : split = name . rsplit ( '::' , 1 ) if len ( split ) != 2 : warnings . warn ( "What's up with %s?" % str ( ( prefix , name , objtype ) ) ) continue prefix , name = split last_prefix = prefix . split ( '::' ) [ - 1 ] else : last_prefix = prefix . split ( '.' ) [ - 1 ] store [ next ( c ) ] = { 'filename' : base_file_names [ index ] , 'objtype' : objtype , 'prefix' : prefix , 'last_prefix' : last_prefix , 'name' : name , 'shortanchor' : shortanchor , } data . update ( { 'store' : store } ) return data
7344	def get_data ( self , response ) : if self . _response_list : return response elif self . _response_key is None : if hasattr ( response , "items" ) : for key , data in response . items ( ) : if ( hasattr ( data , "__getitem__" ) and not hasattr ( data , "items" ) and len ( data ) > 0 and 'id' in data [ 0 ] ) : self . _response_key = key return data else : self . _response_list = True return response else : return response [ self . _response_key ] raise NoDataFound ( response = response , url = self . request . get_url ( ) )
2083	def launch ( self , workflow_job_template = None , monitor = False , wait = False , timeout = None , extra_vars = None , * * kwargs ) : if extra_vars is not None and len ( extra_vars ) > 0 : kwargs [ 'extra_vars' ] = parser . process_extra_vars ( extra_vars ) debug . log ( 'Launching the workflow job.' , header = 'details' ) self . _pop_none ( kwargs ) post_response = client . post ( 'workflow_job_templates/{0}/launch/' . format ( workflow_job_template ) , data = kwargs ) . json ( ) workflow_job_id = post_response [ 'id' ] post_response [ 'changed' ] = True if monitor : return self . monitor ( workflow_job_id , timeout = timeout ) elif wait : return self . wait ( workflow_job_id , timeout = timeout ) return post_response
11665	def _build_indices ( X , flann_args ) : # TODO: should probably multithread this logger . info ( "Building indices..." ) indices = [ None ] * len ( X ) for i , bag in enumerate ( plog ( X , name = "index building" ) ) : indices [ i ] = idx = FLANNIndex ( * * flann_args ) idx . build_index ( bag ) return indices
3387	def _is_redundant ( self , matrix , cutoff = None ) : cutoff = 1.0 - self . feasibility_tol # Avoid zero variances extra_col = matrix [ : , 0 ] + 1 # Avoid zero rows being correlated with constant rows extra_col [ matrix . sum ( axis = 1 ) == 0 ] = 2 corr = np . corrcoef ( np . c_ [ matrix , extra_col ] ) corr = np . tril ( corr , - 1 ) return ( np . abs ( corr ) > cutoff ) . any ( axis = 1 )
11480	def _upload_as_item ( local_file , parent_folder_id , file_path , reuse_existing = False ) : current_item_id = _create_or_reuse_item ( local_file , parent_folder_id , reuse_existing ) _create_bitstream ( file_path , local_file , current_item_id ) for callback in session . item_upload_callbacks : callback ( session . communicator , session . token , current_item_id )
9003	def _compute_scale ( self , instruction_id , svg_dict ) : bbox = list ( map ( float , svg_dict [ "svg" ] [ "@viewBox" ] . split ( ) ) ) scale = self . _zoom / ( bbox [ 3 ] - bbox [ 1 ] ) self . _symbol_id_to_scale [ instruction_id ] = scale
13432	def admin_link_move_down ( obj , link_text = 'down' ) : if obj . rank == obj . grouped_filter ( ) . count ( ) : return '' content_type = ContentType . objects . get_for_model ( obj ) link = reverse ( 'awl-rankedmodel-move' , args = ( content_type . id , obj . id , obj . rank + 1 ) ) return '<a href="%s">%s</a>' % ( link , link_text )
287	def show_perf_stats ( returns , factor_returns = None , positions = None , transactions = None , turnover_denom = 'AGB' , live_start_date = None , bootstrap = False , header_rows = None ) : if bootstrap : perf_func = timeseries . perf_stats_bootstrap else : perf_func = timeseries . perf_stats perf_stats_all = perf_func ( returns , factor_returns = factor_returns , positions = positions , transactions = transactions , turnover_denom = turnover_denom ) date_rows = OrderedDict ( ) if len ( returns . index ) > 0 : date_rows [ 'Start date' ] = returns . index [ 0 ] . strftime ( '%Y-%m-%d' ) date_rows [ 'End date' ] = returns . index [ - 1 ] . strftime ( '%Y-%m-%d' ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) returns_is = returns [ returns . index < live_start_date ] returns_oos = returns [ returns . index >= live_start_date ] positions_is = None positions_oos = None transactions_is = None transactions_oos = None if positions is not None : positions_is = positions [ positions . index < live_start_date ] positions_oos = positions [ positions . index >= live_start_date ] if transactions is not None : transactions_is = transactions [ ( transactions . index < live_start_date ) ] transactions_oos = transactions [ ( transactions . index > live_start_date ) ] perf_stats_is = perf_func ( returns_is , factor_returns = factor_returns , positions = positions_is , transactions = transactions_is , turnover_denom = turnover_denom ) perf_stats_oos = perf_func ( returns_oos , factor_returns = factor_returns , positions = positions_oos , transactions = transactions_oos , turnover_denom = turnover_denom ) if len ( returns . index ) > 0 : date_rows [ 'In-sample months' ] = int ( len ( returns_is ) / APPROX_BDAYS_PER_MONTH ) date_rows [ 'Out-of-sample months' ] = int ( len ( returns_oos ) / APPROX_BDAYS_PER_MONTH ) perf_stats = pd . concat ( OrderedDict ( [ ( 'In-sample' , perf_stats_is ) , ( 'Out-of-sample' , perf_stats_oos ) , ( 'All' , perf_stats_all ) , ] ) , axis = 1 ) else : if len ( returns . index ) > 0 : date_rows [ 'Total months' ] = int ( len ( returns ) / APPROX_BDAYS_PER_MONTH ) perf_stats = pd . DataFrame ( perf_stats_all , columns = [ 'Backtest' ] ) for column in perf_stats . columns : for stat , value in perf_stats [ column ] . iteritems ( ) : if stat in STAT_FUNCS_PCT : perf_stats . loc [ stat , column ] = str ( np . round ( value * 100 , 1 ) ) + '%' if header_rows is None : header_rows = date_rows else : header_rows = OrderedDict ( header_rows ) header_rows . update ( date_rows ) utils . print_table ( perf_stats , float_format = '{0:.2f}' . format , header_rows = header_rows , )
10201	def register_events ( ) : return [ dict ( event_type = 'file-download' , templates = 'invenio_stats.contrib.file_download' , processor_class = EventsIndexer , processor_config = dict ( preprocessors = [ flag_robots , anonymize_user , build_file_unique_id ] ) ) , dict ( event_type = 'record-view' , templates = 'invenio_stats.contrib.record_view' , processor_class = EventsIndexer , processor_config = dict ( preprocessors = [ flag_robots , anonymize_user , build_record_unique_id ] ) ) ]
10253	def remove_highlight_edges ( graph : BELGraph , edges = None ) : for u , v , k , _ in graph . edges ( keys = True , data = True ) if edges is None else edges : if is_edge_highlighted ( graph , u , v , k ) : del graph [ u ] [ v ] [ k ] [ EDGE_HIGHLIGHT ]
10112	def rewrite ( fname , visitor , * * kw ) : if not isinstance ( fname , pathlib . Path ) : assert isinstance ( fname , string_types ) fname = pathlib . Path ( fname ) assert fname . is_file ( ) with tempfile . NamedTemporaryFile ( delete = False ) as fp : tmp = pathlib . Path ( fp . name ) with UnicodeReader ( fname , * * kw ) as reader_ : with UnicodeWriter ( tmp , * * kw ) as writer : for i , row in enumerate ( reader_ ) : row = visitor ( i , row ) if row is not None : writer . writerow ( row ) shutil . move ( str ( tmp ) , str ( fname ) )
12220	def _make_wrapper ( self , func ) : #TODO: consider using a class to make attribute forwarding easier. #TODO: consider using simply another DispatchGroup, with self.callees # assigned by reference to the original callees. @ wraps ( func ) def executor ( * args , * * kwargs ) : return self . execute ( args , kwargs ) executor . dispatch = self . dispatch executor . dispatch_first = self . dispatch_first executor . func = func executor . lookup = self . lookup return executor
5893	def handle_upload ( self , request ) : if request . method != 'POST' : raise Http404 if request . is_ajax ( ) : try : filename = request . GET [ 'quillUploadFile' ] data = request is_raw = True except KeyError : return HttpResponseBadRequest ( "Invalid file upload." ) else : if len ( request . FILES ) != 1 : return HttpResponseBadRequest ( "Can only upload 1 file at a time." ) try : data = request . FILES [ 'quillUploadFile' ] filename = data . name is_raw = False except KeyError : return HttpResponseBadRequest ( 'Missing image `quillUploadFile`.' ) url = save_file ( data , filename , is_raw , default_storage ) response_data = { } response_data [ 'url' ] = url # Response content type needs to be text/html here or else # IE will try to download the file. return HttpResponse ( json . dumps ( response_data ) , content_type = "text/html; charset=utf-8" )
5178	def resource ( self , type_ , title , * * kwargs ) : resources = self . __api . resources ( type_ = type_ , title = title , query = EqualsOperator ( "certname" , self . name ) , * * kwargs ) return next ( resource for resource in resources )
6118	def circular_annular ( cls , shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
8904	def _request ( self , endpoint , method = "GET" , lookup = None , data = { } , params = { } , userargs = None , password = None ) : # convert user arguments into query str for passing to auth layer if isinstance ( userargs , dict ) : userargs = "&" . join ( [ "{}={}" . format ( key , val ) for ( key , val ) in iteritems ( userargs ) ] ) # build up url and send request if lookup : lookup = lookup + '/' url = urljoin ( urljoin ( self . base_url , endpoint ) , lookup ) auth = ( userargs , password ) if userargs else None resp = requests . request ( method , url , json = data , params = params , auth = auth ) resp . raise_for_status ( ) return resp
1490	def tail ( filename , n ) : size = os . path . getsize ( filename ) with open ( filename , "rb" ) as f : fm = mmap . mmap ( f . fileno ( ) , 0 , mmap . MAP_SHARED , mmap . PROT_READ ) try : for i in xrange ( size - 1 , - 1 , - 1 ) : if fm [ i ] == '\n' : n -= 1 if n == - 1 : break return fm [ i + 1 if i else 0 : ] . splitlines ( ) finally : fm . close ( )
4894	def _collect_grades_data ( self , enterprise_enrollment , course_details ) : if self . grades_api is None : self . grades_api = GradesApiClient ( self . user ) course_id = enterprise_enrollment . course_id username = enterprise_enrollment . enterprise_customer_user . user . username try : grades_data = self . grades_api . get_course_grade ( course_id , username ) except HttpNotFoundError as error : # Grade not found, so we have nothing to report. if hasattr ( error , 'content' ) : response_content = json . loads ( error . content ) if response_content . get ( 'error_code' , '' ) == 'user_not_enrolled' : # This means the user has an enterprise enrollment record but is not enrolled in the course yet LOGGER . info ( "User [%s] not enrolled in course [%s], enterprise enrollment [%d]" , username , course_id , enterprise_enrollment . pk ) return None , None , None LOGGER . error ( "No grades data found for [%d]: [%s], [%s]" , enterprise_enrollment . pk , course_id , username ) return None , None , None # Prepare to process the course end date and pass/fail grade course_end_date = course_details . get ( 'end' ) if course_end_date is not None : course_end_date = parse_datetime ( course_end_date ) now = timezone . now ( ) is_passing = grades_data . get ( 'passed' ) # We can consider a course complete if: # * the course's end date has passed if course_end_date is not None and course_end_date < now : completed_date = course_end_date grade = self . grade_passing if is_passing else self . grade_failing # * Or, the learner has a passing grade (as of now) elif is_passing : completed_date = now grade = self . grade_passing # Otherwise, the course is still in progress else : completed_date = None grade = self . grade_incomplete return completed_date , grade , is_passing
9341	def MetaOrdered ( parallel , done , turnstile ) : class Ordered : def __init__ ( self , iterref ) : if parallel . master : done [ ... ] = 0 self . iterref = iterref parallel . barrier ( ) @ classmethod def abort ( self ) : turnstile . release ( ) def __enter__ ( self ) : while self . iterref != done : pass turnstile . acquire ( ) return self def __exit__ ( self , * args ) : done [ ... ] += 1 turnstile . release ( ) return Ordered
7353	def NetMHC ( alleles , default_peptide_lengths = [ 9 ] , program_name = "netMHC" ) : # run NetMHC's help command and parse discriminating substrings out of # the resulting str output with open ( os . devnull , 'w' ) as devnull : help_output = check_output ( [ program_name , "-h" ] , stderr = devnull ) help_output_str = help_output . decode ( "ascii" , "ignore" ) substring_to_netmhc_class = { "-listMHC" : NetMHC4 , "--Alleles" : NetMHC3 , } successes = [ ] for substring , netmhc_class in substring_to_netmhc_class . items ( ) : if substring in help_output_str : successes . append ( netmhc_class ) if len ( successes ) > 1 : raise SystemError ( "Command %s is valid for multiple NetMHC versions. " "This is likely an mhctools bug." % program_name ) if len ( successes ) == 0 : raise SystemError ( "Command %s is not a valid way of calling any NetMHC software." % program_name ) netmhc_class = successes [ 0 ] return netmhc_class ( alleles = alleles , default_peptide_lengths = default_peptide_lengths , program_name = program_name )
6753	def local_renderer ( self ) : if not self . _local_renderer : r = self . create_local_renderer ( ) self . _local_renderer = r return self . _local_renderer
10633	def get_compound_afrs ( self ) : result = self . _compound_mfrs * 1.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) result [ index ] = stoich . amount ( compound , result [ index ] ) return result
826	def getScalarNames ( self , parentFieldName = '' ) : names = [ ] if self . encoders is not None : for ( name , encoder , offset ) in self . encoders : subNames = encoder . getScalarNames ( parentFieldName = name ) if parentFieldName != '' : subNames = [ '%s.%s' % ( parentFieldName , name ) for name in subNames ] names . extend ( subNames ) else : if parentFieldName != '' : names . append ( parentFieldName ) else : names . append ( self . name ) return names
13616	def scaffold ( ) : click . echo ( "A whole new site? Awesome." ) title = click . prompt ( "What's the title?" ) url = click . prompt ( "Great. What's url? http://" ) # Make sure that title doesn't exist. click . echo ( "Got it. Creating %s..." % url )
8547	def get_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) ) return response
9401	def _parse_error ( self , err ) : self . logger . debug ( err ) stack = err . get ( 'stack' , [ ] ) if not err [ 'message' ] . startswith ( 'parse error:' ) : err [ 'message' ] = 'error: ' + err [ 'message' ] errmsg = 'Octave evaluation error:\n%s' % err [ 'message' ] if not isinstance ( stack , StructArray ) : return errmsg errmsg += '\nerror: called from:' for item in stack [ : - 1 ] : errmsg += '\n %(name)s at line %(line)d' % item try : errmsg += ', column %(column)d' % item except Exception : pass return errmsg
10158	def get_viewset_transition_action_mixin ( model , * * kwargs ) : instance = model ( ) class Mixin ( object ) : save_after_transition = True transitions = instance . get_all_status_transitions ( ) transition_names = set ( x . name for x in transitions ) for transition_name in transition_names : setattr ( Mixin , transition_name , get_transition_viewset_method ( transition_name , * * kwargs ) ) return Mixin
6905	def great_circle_dist ( ra1 , dec1 , ra2 , dec2 ) : # wrap RA if negative or larger than 360.0 deg in_ra1 = ra1 % 360.0 in_ra1 = in_ra1 + 360.0 * ( in_ra1 < 0.0 ) in_ra2 = ra2 % 360.0 in_ra2 = in_ra2 + 360.0 * ( in_ra1 < 0.0 ) # convert to radians ra1_rad , dec1_rad = np . deg2rad ( in_ra1 ) , np . deg2rad ( dec1 ) ra2_rad , dec2_rad = np . deg2rad ( in_ra2 ) , np . deg2rad ( dec2 ) del_dec2 = ( dec2_rad - dec1_rad ) / 2.0 del_ra2 = ( ra2_rad - ra1_rad ) / 2.0 sin_dist = np . sqrt ( np . sin ( del_dec2 ) * np . sin ( del_dec2 ) + np . cos ( dec1_rad ) * np . cos ( dec2_rad ) * np . sin ( del_ra2 ) * np . sin ( del_ra2 ) ) dist_rad = 2.0 * np . arcsin ( sin_dist ) # return the distance in arcseconds return np . rad2deg ( dist_rad ) * 3600.0
10369	def summarize_edge_filter ( graph : BELGraph , edge_predicates : EdgePredicates ) -> None : passed = count_passed_edge_filter ( graph , edge_predicates ) print ( '{}/{} edges passed {}' . format ( passed , graph . number_of_edges ( ) , ( ', ' . join ( edge_filter . __name__ for edge_filter in edge_predicates ) if isinstance ( edge_predicates , Iterable ) else edge_predicates . __name__ ) ) )
6836	def base_boxes ( self ) : return sorted ( list ( set ( [ name for name , provider in self . _box_list ( ) ] ) ) )
609	def _generateMetricSpecString ( inferenceElement , metric , params = None , field = None , returnLabel = False ) : metricSpecArgs = dict ( metric = metric , field = field , params = params , inferenceElement = inferenceElement ) metricSpecAsString = "MetricSpec(%s)" % ', ' . join ( [ '%s=%r' % ( item [ 0 ] , item [ 1 ] ) for item in metricSpecArgs . iteritems ( ) ] ) if not returnLabel : return metricSpecAsString spec = MetricSpec ( * * metricSpecArgs ) metricLabel = spec . getLabel ( ) return metricSpecAsString , metricLabel
3476	def _associate_gene ( self , cobra_gene ) : self . _genes . add ( cobra_gene ) cobra_gene . _reaction . add ( self ) cobra_gene . _model = self . _model
3829	async def remove_user ( self , remove_user_request ) : response = hangouts_pb2 . RemoveUserResponse ( ) await self . _pb_request ( 'conversations/removeuser' , remove_user_request , response ) return response
3066	def _apply_user_agent ( headers , user_agent ) : if user_agent is not None : if 'user-agent' in headers : headers [ 'user-agent' ] = ( user_agent + ' ' + headers [ 'user-agent' ] ) else : headers [ 'user-agent' ] = user_agent return headers
8255	def copy ( self ) : return ColorList ( [ color ( clr . r , clr . g , clr . b , clr . a , mode = "rgb" ) for clr in self ] , name = self . name , tags = self . tags )
910	def handleInputRecord ( self , inputRecord ) : results = self . __model . run ( inputRecord ) shouldContinue = self . __currentPhase . advance ( ) if not shouldContinue : self . __advancePhase ( ) return results
6890	def read_csv_lightcurve ( lcfile ) : # read in the file first if '.gz' in os . path . basename ( lcfile ) : LOGINFO ( 'reading gzipped K2 LC: %s' % lcfile ) infd = gzip . open ( lcfile , 'rb' ) else : LOGINFO ( 'reading K2 LC: %s' % lcfile ) infd = open ( lcfile , 'rb' ) lctext = infd . read ( ) . decode ( ) infd . close ( ) # figure out the header and get the LC columns lcstart = lctext . index ( '# LIGHTCURVE\n' ) lcheader = lctext [ : lcstart + 12 ] lccolumns = lctext [ lcstart + 13 : ] . split ( '\n' ) lccolumns = [ x . split ( ',' ) for x in lccolumns if len ( x ) > 0 ] # initialize the lcdict and parse the CSV header lcdict = _parse_csv_header ( lcheader ) # tranpose the LC rows into columns lccolumns = list ( zip ( * lccolumns ) ) # write the columns to the dict for colind , col in enumerate ( lcdict [ 'columns' ] ) : # this picks out the caster to use when reading each column using the # definitions in the lcutils.COLUMNDEFS dictionary lcdict [ col . lower ( ) ] = np . array ( [ COLUMNDEFS [ col ] [ 2 ] ( x ) for x in lccolumns [ colind ] ] ) lcdict [ 'columns' ] = [ x . lower ( ) for x in lcdict [ 'columns' ] ] return lcdict
9602	def wait_for_elements ( self , using , value , timeout = 10000 , interval = 1000 , asserter = is_displayed ) : if not callable ( asserter ) : raise TypeError ( 'Asserter must be callable.' ) @ retry ( retry_on_exception = lambda ex : isinstance ( ex , WebDriverException ) , stop_max_delay = timeout , wait_fixed = interval ) def _wait_for_elements ( ctx , using , value ) : els = ctx . elements ( using , value ) if not len ( els ) : raise WebDriverException ( 'no such element' ) else : el = els [ 0 ] asserter ( el ) return els return _wait_for_elements ( self , using , value )
8312	def draw_list ( markup , x , y , w , padding = 5 , callback = None ) : try : from web import _ctx except : pass i = 1 for chunk in markup . split ( "\n" ) : if callback != None : callback ( chunk , i ) m = re . search ( "^([0-9]{1,3}\. )" , chunk . lstrip ( ) ) if m : indent = re . search ( "[0-9]" , chunk ) . start ( ) * padding * 2 bullet = m . group ( 1 ) dx = textwidth ( "000." ) chunk = chunk . lstrip ( m . group ( 1 ) + "\t" ) if chunk . lstrip ( ) . startswith ( "*" ) : indent = chunk . find ( "*" ) * padding * 2 bullet = u"โข" dx = textwidth ( "*" ) chunk = chunk . lstrip ( "* \t" ) _ctx . text ( bullet , x + indent , y ) dx += padding + indent _ctx . text ( chunk , x + dx , y , width = w - dx ) y += _ctx . textheight ( chunk , width = w - dx ) y += _ctx . textheight ( " " ) * 0.25 i += 1
10784	def should_particle_exist ( absent_err , present_err , absent_d , present_d , im_change_frac = 0.2 , min_derr = 0.1 ) : delta_im = np . ravel ( present_d - absent_d ) im_change = np . dot ( delta_im , delta_im ) err_cutoff = max ( [ im_change_frac * im_change , min_derr ] ) return ( absent_err - present_err ) >= err_cutoff
11766	def update ( x , * * entries ) : if isinstance ( x , dict ) : x . update ( entries ) else : x . __dict__ . update ( entries ) return x
5221	def exch_info ( ticker : str ) -> pd . Series : logger = logs . get_logger ( exch_info , level = 'debug' ) if ' ' not in ticker . strip ( ) : ticker = f'XYZ {ticker.strip()} Equity' info = param . load_info ( cat = 'exch' ) . get ( market_info ( ticker = ticker ) . get ( 'exch' , '' ) , dict ( ) ) if ( 'allday' in info ) and ( 'day' not in info ) : info [ 'day' ] = info [ 'allday' ] if any ( req not in info for req in [ 'tz' , 'allday' , 'day' ] ) : logger . error ( f'required exchange info cannot be found in {ticker} ...' ) return pd . Series ( ) for ss in ValidSessions : if ss not in info : continue info [ ss ] = [ param . to_hour ( num = s ) for s in info [ ss ] ] return pd . Series ( info )
6345	def stem ( self , word ) : terminate = False intact = True while not terminate : for n in range ( 6 , 0 , - 1 ) : if word [ - n : ] in self . _rule_table [ n ] : accept = False if len ( self . _rule_table [ n ] [ word [ - n : ] ] ) < 4 : for rule in self . _rule_table [ n ] [ word [ - n : ] ] : ( word , accept , intact , terminate , ) = self . _apply_rule ( word , rule , intact , terminate ) if accept : break else : rule = self . _rule_table [ n ] [ word [ - n : ] ] ( word , accept , intact , terminate ) = self . _apply_rule ( word , rule , intact , terminate ) if accept : break else : break return word
589	def setAutoDetectWaitRecords ( self , waitRecords ) : if not isinstance ( waitRecords , int ) : raise HTMPredictionModelInvalidArgument ( "Invalid argument type \'%s\'. WaitRecord " "must be a number." % ( type ( waitRecords ) ) ) if len ( self . saved_states ) > 0 and waitRecords < self . saved_states [ 0 ] . ROWID : raise HTMPredictionModelInvalidArgument ( "Invalid value. autoDetectWaitRecord value " "must be valid record within output stream. Current minimum ROWID in " "output stream is %d." % ( self . saved_states [ 0 ] . ROWID ) ) self . _autoDetectWaitRecords = waitRecords # Update all the states in the classifier's cache for state in self . saved_states : self . _updateState ( state )
10505	def stopEventLoop ( ) : stopper = PyObjCAppHelperRunLoopStopper_wrap . currentRunLoopStopper ( ) if stopper is None : if NSApp ( ) is not None : NSApp ( ) . terminate_ ( None ) return True return False NSTimer . scheduledTimerWithTimeInterval_target_selector_userInfo_repeats_ ( 0.0 , stopper , 'performStop:' , None , False ) return True
4967	def _validate_course ( self ) : # Verify that the selected mode is valid for the given course . course_details = self . cleaned_data . get ( self . Fields . COURSE ) if course_details : course_mode = self . cleaned_data . get ( self . Fields . COURSE_MODE ) if not course_mode : raise ValidationError ( ValidationMessages . COURSE_WITHOUT_COURSE_MODE ) valid_course_modes = course_details [ "course_modes" ] if all ( course_mode != mode [ "slug" ] for mode in valid_course_modes ) : error = ValidationError ( ValidationMessages . COURSE_MODE_INVALID_FOR_COURSE . format ( course_mode = course_mode , course_id = course_details [ "course_id" ] , ) ) raise ValidationError ( { self . Fields . COURSE_MODE : error } )
9493	def _simulate_stack ( code : list ) -> int : max_stack = 0 curr_stack = 0 def _check_stack ( ins ) : if curr_stack < 0 : raise CompileError ( "Stack turned negative on instruction: {}" . format ( ins ) ) if curr_stack > max_stack : return curr_stack # Iterate over the bytecode. for instruction in code : assert isinstance ( instruction , dis . Instruction ) if instruction . arg is not None : try : effect = dis . stack_effect ( instruction . opcode , instruction . arg ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e else : try : effect = dis . stack_effect ( instruction . opcode ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e curr_stack += effect # Re-check the stack. _should_new_stack = _check_stack ( instruction ) if _should_new_stack : max_stack = _should_new_stack return max_stack
2546	def add_annotation_comment ( self , doc , comment ) : if len ( doc . annotations ) != 0 : if not self . annotation_comment_set : self . annotation_comment_set = True doc . annotations [ - 1 ] . comment = comment return True else : raise CardinalityError ( 'AnnotationComment' ) else : raise OrderError ( 'AnnotationComment' )
7054	def _check_extmodule ( module , formatkey ) : try : if os . path . exists ( module ) : sys . path . append ( os . path . dirname ( module ) ) importedok = importlib . import_module ( os . path . basename ( module . replace ( '.py' , '' ) ) ) else : importedok = importlib . import_module ( module ) except Exception as e : LOGEXCEPTION ( 'could not import the module: %s for LC format: %s. ' 'check the file path or fully qualified module name?' % ( module , formatkey ) ) importedok = False return importedok
6318	def _find_last_of ( self , path , finders ) : found_path = None for finder in finders : result = finder . find ( path ) if result : found_path = result return found_path
13200	def format_title ( self , format = 'html5' , deparagraph = True , mathjax = False , smart = True , extra_args = None ) : if self . title is None : return None output_text = convert_lsstdoc_tex ( self . title , format , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
10355	def get_largest_component ( graph : BELGraph ) -> BELGraph : biggest_component_nodes = max ( nx . weakly_connected_components ( graph ) , key = len ) return subgraph ( graph , biggest_component_nodes )
484	def enableConcurrencyChecks ( maxConcurrency , raiseException = True ) : global g_max_concurrency , g_max_concurrency_raise_exception assert maxConcurrency >= 0 g_max_concurrency = maxConcurrency g_max_concurrency_raise_exception = raiseException return
24	def update ( self , new_val ) : if self . _value is None : self . _value = new_val else : self . _value = self . _gamma * self . _value + ( 1.0 - self . _gamma ) * new_val
6367	def precision ( self ) : if self . _tp + self . _fp == 0 : return float ( 'NaN' ) return self . _tp / ( self . _tp + self . _fp )
1105	def set_seq2 ( self , b ) : if b is self . b : return self . b = b self . matching_blocks = self . opcodes = None self . fullbcount = None self . __chain_b ( )
6396	def sim_minkowski ( src , tar , qval = 2 , pval = 1 , alphabet = None ) : return Minkowski ( ) . sim ( src , tar , qval , pval , alphabet )
172	def draw_points_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_points_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
3283	def read ( self , size = None ) : while size is None or len ( self . buffer ) < size : try : self . buffer += next ( self . data_stream ) except StopIteration : break sized_chunk = self . buffer [ : size ] if size is None : self . buffer = "" else : self . buffer = self . buffer [ size : ] return sized_chunk
10820	def _filter ( cls , query , state = MembershipState . ACTIVE , eager = None ) : query = query . filter_by ( state = state ) eager = eager or [ ] for field in eager : query = query . options ( joinedload ( field ) ) return query
4960	def get_earliest_start_date_from_program ( program ) : start_dates = [ ] for course in program . get ( 'courses' , [ ] ) : for run in course . get ( 'course_runs' , [ ] ) : if run . get ( 'start' ) : start_dates . append ( parse_lms_api_datetime ( run [ 'start' ] ) ) if not start_dates : return None return min ( start_dates )
2910	def _find_ancestor ( self , task_spec ) : if self . parent is None : return self if self . parent . task_spec == task_spec : return self . parent return self . parent . _find_ancestor ( task_spec )
12247	def create_bucket ( self , * args , * * kwargs ) : bucket = super ( S3Connection , self ) . create_bucket ( * args , * * kwargs ) if bucket : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket
777	def connect ( self , deleteOldVersions = False , recreate = False ) : # Initialize tables, if needed with ConnectionFactory . get ( ) as conn : # Initialize tables self . _initTables ( cursor = conn . cursor , deleteOldVersions = deleteOldVersions , recreate = recreate ) # Save our connection id conn . cursor . execute ( 'SELECT CONNECTION_ID()' ) self . _connectionID = conn . cursor . fetchall ( ) [ 0 ] [ 0 ] self . _logger . info ( "clientJobsConnectionID=%r" , self . _connectionID ) return
8065	def drawdaisy ( x , y , color = '#fefefe' ) : # save location, size etc _ctx . push ( ) # save fill and stroke _fill = _ctx . fill ( ) _stroke = _ctx . stroke ( ) sc = ( 1.0 / _ctx . HEIGHT ) * float ( y * 0.5 ) * 4.0 # draw stalk _ctx . strokewidth ( sc * 2.0 ) _ctx . stroke ( '#3B240B' ) _ctx . line ( x + ( sin ( x * 0.1 ) * 10.0 ) , y + 80 , x + sin ( _ctx . FRAME * 0.1 ) , y ) # draw flower _ctx . translate ( - 20 , 0 ) _ctx . scale ( sc ) # draw petals _ctx . fill ( color ) _ctx . nostroke ( ) for angle in xrange ( 0 , 360 , 45 ) : _ctx . rotate ( degrees = 45 ) _ctx . rect ( x , y , 40 , 8 , 1 ) # draw centre _ctx . fill ( '#F7FE2E' ) _ctx . ellipse ( x + 15 , y , 10 , 10 ) # restore fill and stroke _ctx . fill ( _fill ) _ctx . stroke ( _stroke ) # restore location, size etc _ctx . pop ( )
5762	def write_groovy_script_and_configs ( filename , content , job_configs , view_configs = None ) : with open ( filename , 'w' ) as h : h . write ( content ) if view_configs : view_config_dir = os . path . join ( os . path . dirname ( filename ) , 'view_configs' ) if not os . path . isdir ( view_config_dir ) : os . makedirs ( view_config_dir ) for config_name , config_body in view_configs . items ( ) : config_filename = os . path . join ( view_config_dir , config_name ) with open ( config_filename , 'w' ) as config_fh : config_fh . write ( config_body ) job_config_dir = os . path . join ( os . path . dirname ( filename ) , 'job_configs' ) if not os . path . isdir ( job_config_dir ) : os . makedirs ( job_config_dir ) # prefix each config file with a serial number to maintain order format_str = '%0' + str ( len ( str ( len ( job_configs ) ) ) ) + 'd' i = 0 for config_name , config_body in job_configs . items ( ) : i += 1 config_filename = os . path . join ( job_config_dir , format_str % i + ' ' + config_name ) with open ( config_filename , 'w' ) as config_fh : config_fh . write ( config_body )
12734	def set_body_states ( self , states ) : for state in states : self . get_body ( state . name ) . state = state
13215	def dump ( self , name , filename ) : if not self . exists ( name ) : raise DatabaseError ( 'database %s does not exist!' ) log . info ( 'dumping %s to %s' % ( name , filename ) ) self . _run_cmd ( 'pg_dump' , '--verbose' , '--blobs' , '--format=custom' , '--file=%s' % filename , name )
5088	def ecommerce_coupon_url ( self , instance ) : if not instance . entitlement_id : return "N/A" return format_html ( '<a href="{base_url}/coupons/{id}" target="_blank">View coupon "{id}" details</a>' , base_url = settings . ECOMMERCE_PUBLIC_URL_ROOT , id = instance . entitlement_id )
7024	def _read_checkplot_picklefile ( checkplotpickle ) : if checkplotpickle . endswith ( '.gz' ) : try : with gzip . open ( checkplotpickle , 'rb' ) as infd : cpdict = pickle . load ( infd ) except UnicodeDecodeError : with gzip . open ( checkplotpickle , 'rb' ) as infd : cpdict = pickle . load ( infd , encoding = 'latin1' ) else : try : with open ( checkplotpickle , 'rb' ) as infd : cpdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( checkplotpickle , 'rb' ) as infd : cpdict = pickle . load ( infd , encoding = 'latin1' ) return cpdict
10641	def Re ( L : float , v : float , nu : float ) -> float : return v * L / nu
8702	def prepare ( self ) : log . info ( 'Preparing esp for transfer.' ) for func in LUA_FUNCTIONS : detected = self . __exchange ( 'print({0})' . format ( func ) ) if detected . find ( 'function:' ) == - 1 : break else : log . info ( 'Preparation already done. Not adding functions again.' ) return True functions = RECV_LUA + '\n' + SEND_LUA data = functions . format ( baud = self . _port . baudrate ) ##change any \r\n to just \n and split on that lines = data . replace ( '\r' , '' ) . split ( '\n' ) #remove some unneccesary spaces to conserve some bytes for line in lines : line = line . strip ( ) . replace ( ', ' , ',' ) . replace ( ' = ' , '=' ) if len ( line ) == 0 : continue resp = self . __exchange ( line ) #do some basic test of the result if ( 'unexpected' in resp ) or ( 'stdin' in resp ) or len ( resp ) > len ( functions ) + 10 : log . error ( 'error when preparing "%s"' , resp ) return False return True
9179	def _validate_license ( model ) : license_mapping = obtain_licenses ( ) try : license_url = model . metadata [ 'license_url' ] except KeyError : raise exceptions . MissingRequiredMetadata ( 'license_url' ) try : license = license_mapping [ license_url ] except KeyError : raise exceptions . InvalidLicense ( license_url ) if not license [ 'is_valid_for_publication' ] : raise exceptions . InvalidLicense ( license_url )
9215	def t_istringapostrophe_css_string ( self , t ) : t . lexer . lineno += t . value . count ( '\n' ) return t
7826	def payload_element_name ( element_name ) : def decorator ( klass ) : """The payload_element_name decorator.""" # pylint: disable-msg=W0212,W0404 from . stanzapayload import STANZA_PAYLOAD_CLASSES from . stanzapayload import STANZA_PAYLOAD_ELEMENTS if hasattr ( klass , "_pyxmpp_payload_element_name" ) : klass . _pyxmpp_payload_element_name . append ( element_name ) else : klass . _pyxmpp_payload_element_name = [ element_name ] if element_name in STANZA_PAYLOAD_CLASSES : logger = logging . getLogger ( 'pyxmpp.payload_element_name' ) logger . warning ( "Overriding payload class for {0!r}" . format ( element_name ) ) STANZA_PAYLOAD_CLASSES [ element_name ] = klass STANZA_PAYLOAD_ELEMENTS [ klass ] . append ( element_name ) return klass return decorator
8746	def get_floatingips_count ( context , filters = None ) : LOG . info ( 'get_floatingips_count for tenant %s filters %s' % ( context . tenant_id , filters ) ) if filters is None : filters = { } filters [ '_deallocated' ] = False filters [ 'address_type' ] = ip_types . FLOATING count = db_api . ip_address_count_all ( context , filters ) LOG . info ( 'Found %s floating ips for tenant %s' % ( count , context . tenant_id ) ) return count
4334	def norm ( self , db_level = - 3.0 ) : if not is_number ( db_level ) : raise ValueError ( 'db_level must be a number.' ) effect_args = [ 'norm' , '{:f}' . format ( db_level ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'norm' ) return self
92	def _quokka_normalize_extract ( extract ) : # TODO get rid of this deferred import from imgaug . augmentables . bbs import BoundingBox , BoundingBoxesOnImage if extract == "square" : bb = BoundingBox ( x1 = 0 , y1 = 0 , x2 = 643 , y2 = 643 ) elif isinstance ( extract , tuple ) and len ( extract ) == 4 : bb = BoundingBox ( x1 = extract [ 0 ] , y1 = extract [ 1 ] , x2 = extract [ 2 ] , y2 = extract [ 3 ] ) elif isinstance ( extract , BoundingBox ) : bb = extract elif isinstance ( extract , BoundingBoxesOnImage ) : do_assert ( len ( extract . bounding_boxes ) == 1 ) do_assert ( extract . shape [ 0 : 2 ] == ( 643 , 960 ) ) bb = extract . bounding_boxes [ 0 ] else : raise Exception ( "Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage " + "for parameter 'extract', got %s." % ( type ( extract ) , ) ) return bb
7698	def as_xml ( self , parent = None ) : if parent is not None : element = ElementTree . SubElement ( parent , ITEM_TAG ) else : element = ElementTree . Element ( ITEM_TAG ) element . set ( "jid" , unicode ( self . jid ) ) if self . name is not None : element . set ( "name" , self . name ) if self . subscription is not None : element . set ( "subscription" , self . subscription ) if self . ask : element . set ( "ask" , self . ask ) if self . approved : element . set ( "approved" , "true" ) for group in self . groups : ElementTree . SubElement ( element , GROUP_TAG ) . text = group return element
2064	def migrate ( self , expression , name_migration_map = None ) : if name_migration_map is None : name_migration_map = { } # name_migration_map -> object_migration_map # Based on the name mapping in name_migration_map build an object to # object mapping to be used in the replacing of variables # inv: object_migration_map's keys should ALWAYS be external/foreign # expressions, and its values should ALWAYS be internal/local expressions object_migration_map = { } #List of foreign vars used in expression foreign_vars = itertools . filterfalse ( self . is_declared , get_variables ( expression ) ) for foreign_var in foreign_vars : # If a variable with the same name was previously migrated if foreign_var . name in name_migration_map : migrated_name = name_migration_map [ foreign_var . name ] native_var = self . get_variable ( migrated_name ) assert native_var is not None , "name_migration_map contains a variable that does not exist in this ConstraintSet" object_migration_map [ foreign_var ] = native_var else : # foreign_var was not found in the local declared variables nor # any variable with the same name was previously migrated # let's make a new unique internal name for it migrated_name = foreign_var . name if migrated_name in self . _declarations : migrated_name = self . _make_unique_name ( f'{foreign_var.name}_migrated' ) # Create and declare a new variable of given type if isinstance ( foreign_var , Bool ) : new_var = self . new_bool ( name = migrated_name ) elif isinstance ( foreign_var , BitVec ) : new_var = self . new_bitvec ( foreign_var . size , name = migrated_name ) elif isinstance ( foreign_var , Array ) : # Note that we are discarding the ArrayProxy encapsulation new_var = self . new_array ( index_max = foreign_var . index_max , index_bits = foreign_var . index_bits , value_bits = foreign_var . value_bits , name = migrated_name ) . array else : raise NotImplemented ( f"Unknown expression type {type(var)} encountered during expression migration" ) # Update the var to var mapping object_migration_map [ foreign_var ] = new_var # Update the name to name mapping name_migration_map [ foreign_var . name ] = new_var . name # Actually replace each appearance of migrated variables by the new ones migrated_expression = replace ( expression , object_migration_map ) return migrated_expression
3399	def add_switches_and_objective ( self ) : constraints = list ( ) big_m = max ( max ( abs ( b ) for b in r . bounds ) for r in self . model . reactions ) prob = self . model . problem for rxn in self . model . reactions : if not hasattr ( rxn , 'gapfilling_type' ) : continue indicator = prob . Variable ( name = 'indicator_{}' . format ( rxn . id ) , lb = 0 , ub = 1 , type = 'binary' ) if rxn . id in self . penalties : indicator . cost = self . penalties [ rxn . id ] else : indicator . cost = self . penalties [ rxn . gapfilling_type ] indicator . rxn_id = rxn . id self . indicators . append ( indicator ) # if z = 1 v_i is allowed non-zero # v_i - Mz <= 0 and v_i + Mz >= 0 constraint_lb = prob . Constraint ( rxn . flux_expression - big_m * indicator , ub = 0 , name = 'constraint_lb_{}' . format ( rxn . id ) , sloppy = True ) constraint_ub = prob . Constraint ( rxn . flux_expression + big_m * indicator , lb = 0 , name = 'constraint_ub_{}' . format ( rxn . id ) , sloppy = True ) constraints . extend ( [ constraint_lb , constraint_ub ] ) self . model . add_cons_vars ( self . indicators ) self . model . add_cons_vars ( constraints , sloppy = True ) self . model . objective = prob . Objective ( Zero , direction = 'min' , sloppy = True ) self . model . objective . set_linear_coefficients ( { i : 1 for i in self . indicators } ) self . update_costs ( )
3174	def create_or_update ( self , store_id , product_id , variant_id , data ) : self . store_id = store_id self . product_id = product_id self . variant_id = variant_id if 'id' not in data : raise KeyError ( 'The product variant must have an id' ) if 'title' not in data : raise KeyError ( 'The product variant must have a title' ) return self . _mc_client . _put ( url = self . _build_path ( store_id , 'products' , product_id , 'variants' , variant_id ) , data = data )
1375	def parse_override_config ( namespace ) : overrides = dict ( ) for config in namespace : kv = config . split ( "=" ) if len ( kv ) != 2 : raise Exception ( "Invalid config property format (%s) expected key=value" % config ) if kv [ 1 ] in [ 'true' , 'True' , 'TRUE' ] : overrides [ kv [ 0 ] ] = True elif kv [ 1 ] in [ 'false' , 'False' , 'FALSE' ] : overrides [ kv [ 0 ] ] = False else : overrides [ kv [ 0 ] ] = kv [ 1 ] return overrides
5995	def plot_mask ( mask , units , kpc_per_arcsec , pointsize , zoom_offset_pixels ) : if mask is not None : plt . gca ( ) edge_pixels = mask . masked_grid_index_to_pixel [ mask . edge_pixels ] + 0.5 if zoom_offset_pixels is not None : edge_pixels -= zoom_offset_pixels edge_arcsec = mask . grid_pixels_to_grid_arcsec ( grid_pixels = edge_pixels ) edge_units = convert_grid_units ( array = mask , grid_arcsec = edge_arcsec , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = edge_units [ : , 0 ] , x = edge_units [ : , 1 ] , s = pointsize , c = 'k' )
773	def generateStats ( filename , maxSamples = None , ) : # Mapping from field type to stats collector object statsCollectorMapping = { 'float' : FloatStatsCollector , 'int' : IntStatsCollector , 'string' : StringStatsCollector , 'datetime' : DateTimeStatsCollector , 'bool' : BoolStatsCollector , } filename = resource_filename ( "nupic.datafiles" , filename ) print "*" * 40 print "Collecting statistics for file:'%s'" % ( filename , ) dataFile = FileRecordStream ( filename ) # Initialize collector objects # statsCollectors list holds statsCollector objects for each field statsCollectors = [ ] for fieldName , fieldType , fieldSpecial in dataFile . getFields ( ) : # Find the corresponding stats collector for each field based on field type # and intialize an instance statsCollector = statsCollectorMapping [ fieldType ] ( fieldName , fieldType , fieldSpecial ) statsCollectors . append ( statsCollector ) # Now collect the stats if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : record = dataFile . getNextRecord ( ) if record is None : break for i , value in enumerate ( record ) : statsCollectors [ i ] . addValue ( value ) # stats dict holds the statistics for each field stats = { } for statsCollector in statsCollectors : statsCollector . getStats ( stats ) # We don't want to include reset field in permutations # TODO: handle reset field in a clean way if dataFile . getResetFieldIdx ( ) is not None : resetFieldName , _ , _ = dataFile . getFields ( ) [ dataFile . reset ] stats . pop ( resetFieldName ) if VERBOSITY > 0 : pprint . pprint ( stats ) return stats
4310	def _build_input_format_list ( input_filepath_list , input_volumes = None , input_format = None ) : n_inputs = len ( input_filepath_list ) input_format_list = [ ] for _ in range ( n_inputs ) : input_format_list . append ( [ ] ) # Adjust length of input_volumes list if input_volumes is None : vols = [ 1 ] * n_inputs else : n_volumes = len ( input_volumes ) if n_volumes < n_inputs : logger . warning ( 'Volumes were only specified for %s out of %s files.' 'The last %s files will remain at their original volumes.' , n_volumes , n_inputs , n_inputs - n_volumes ) vols = input_volumes + [ 1 ] * ( n_inputs - n_volumes ) elif n_volumes > n_inputs : logger . warning ( '%s volumes were specified but only %s input files exist.' 'The last %s volumes will be ignored.' , n_volumes , n_inputs , n_volumes - n_inputs ) vols = input_volumes [ : n_inputs ] else : vols = [ v for v in input_volumes ] # Adjust length of input_format list if input_format is None : fmts = [ [ ] for _ in range ( n_inputs ) ] else : n_fmts = len ( input_format ) if n_fmts < n_inputs : logger . warning ( 'Input formats were only specified for %s out of %s files.' 'The last %s files will remain unformatted.' , n_fmts , n_inputs , n_inputs - n_fmts ) fmts = [ f for f in input_format ] fmts . extend ( [ [ ] for _ in range ( n_inputs - n_fmts ) ] ) elif n_fmts > n_inputs : logger . warning ( '%s Input formats were specified but only %s input files exist' '. The last %s formats will be ignored.' , n_fmts , n_inputs , n_fmts - n_inputs ) fmts = input_format [ : n_inputs ] else : fmts = [ f for f in input_format ] for i , ( vol , fmt ) in enumerate ( zip ( vols , fmts ) ) : input_format_list [ i ] . extend ( [ '-v' , '{}' . format ( vol ) ] ) input_format_list [ i ] . extend ( fmt ) return input_format_list
10230	def flatten_list_abundance ( node : ListAbundance ) -> ListAbundance : return node . __class__ ( list ( chain . from_iterable ( ( flatten_list_abundance ( member ) . members if isinstance ( member , ListAbundance ) else [ member ] ) for member in node . members ) ) )
837	def getDistances ( self , inputPattern ) : dist = self . _getDistances ( inputPattern ) return ( dist , self . _categoryList )
6538	def compile_masks ( masks ) : if not masks : masks = [ ] elif not isinstance ( masks , ( list , tuple ) ) : masks = [ masks ] return [ re . compile ( mask ) for mask in masks ]
2438	def add_review_comment ( self , doc , comment ) : if len ( doc . reviews ) != 0 : if not self . review_comment_set : self . review_comment_set = True if validations . validate_review_comment ( comment ) : doc . reviews [ - 1 ] . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ReviewComment::Comment' ) else : raise CardinalityError ( 'ReviewComment' ) else : raise OrderError ( 'ReviewComment' )
8604	def update_user ( self , user_id , * * kwargs ) : properties = { } for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/users/%s' % user_id , method = 'PUT' , data = json . dumps ( data ) ) return response
13672	def init_build ( self , asset , builder ) : if not self . abs_path : rel_path = utils . prepare_path ( self . rel_bundle_path ) self . abs_bundle_path = utils . prepare_path ( [ builder . config . input_dir , rel_path ] ) self . abs_path = True self . input_dir = builder . config . input_dir
11907	def four_blocks ( topleft , topright , bottomleft , bottomright ) : return vstack ( hstack ( topleft , topright ) , hstack ( bottomleft , bottomright ) )
7940	def _got_srv ( self , addrs ) : with self . lock : if not addrs : self . _dst_service = None if self . _dst_port : self . _dst_nameports = [ ( self . _dst_name , self . _dst_port ) ] else : self . _dst_nameports = [ ] self . _set_state ( "aborted" ) raise DNSError ( "Could not resolve SRV for service {0!r}" " on host {1!r} and fallback port number not given" . format ( self . _dst_service , self . _dst_name ) ) elif addrs == [ ( "." , 0 ) ] : self . _dst_nameports = [ ] self . _set_state ( "aborted" ) raise DNSError ( "Service {0!r} not available on host {1!r}" . format ( self . _dst_service , self . _dst_name ) ) else : self . _dst_nameports = addrs self . _set_state ( "resolve-hostname" )
3145	def delete ( self , file_id ) : self . file_id = file_id return self . _mc_client . _delete ( url = self . _build_path ( file_id ) )
11470	def get_filesize ( self , filename ) : result = [ ] def dir_callback ( val ) : result . append ( val . split ( ) [ 4 ] ) self . _ftp . dir ( filename , dir_callback ) return result [ 0 ]
9864	def get_home ( self , home_id ) : if home_id not in self . _all_home_ids : _LOGGER . error ( "Could not find any Tibber home with id: %s" , home_id ) return None if home_id not in self . _homes . keys ( ) : self . _homes [ home_id ] = TibberHome ( home_id , self ) return self . _homes [ home_id ]
3948	def _timezone_format ( value ) : return timezone . make_aware ( value , timezone . get_current_timezone ( ) ) if getattr ( settings , 'USE_TZ' , False ) else value
6975	def stellingwerf_pdm_theta ( times , mags , errs , frequency , binsize = 0.05 , minbin = 9 ) : period = 1.0 / frequency fold_time = times [ 0 ] phased = phase_magseries ( times , mags , period , fold_time , wrap = False , sort = True ) phases = phased [ 'phase' ] pmags = phased [ 'mags' ] bins = nparange ( 0.0 , 1.0 , binsize ) binnedphaseinds = npdigitize ( phases , bins ) binvariances = [ ] binndets = [ ] goodbins = 0 for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_mags = pmags [ thisbin_inds ] if thisbin_mags . size > minbin : thisbin_variance = npvar ( thisbin_mags , ddof = 1 ) binvariances . append ( thisbin_variance ) binndets . append ( thisbin_mags . size ) goodbins = goodbins + 1 # now calculate theta binvariances = nparray ( binvariances ) binndets = nparray ( binndets ) theta_top = npsum ( binvariances * ( binndets - 1 ) ) / ( npsum ( binndets ) - goodbins ) theta_bot = npvar ( pmags , ddof = 1 ) theta = theta_top / theta_bot return theta
9265	def fetch_and_filter_tags ( self ) : self . all_tags = self . fetcher . get_all_tags ( ) self . filtered_tags = self . get_filtered_tags ( self . all_tags ) self . fetch_tags_dates ( )
6608	def wait ( self ) : sleep = 5 while True : if self . clusterprocids_outstanding : self . poll ( ) if not self . clusterprocids_outstanding : break time . sleep ( sleep ) return self . clusterprocids_finished
12938	def setDefaultRedisConnectionParams ( connectionParams ) : global _defaultRedisConnectionParams _defaultRedisConnectionParams . clear ( ) for key , value in connectionParams . items ( ) : _defaultRedisConnectionParams [ key ] = value clearRedisPools ( )
9681	def config2 ( self ) : config = [ ] data = { } # Send the command byte and sleep for 10 ms self . cnxn . xfer ( [ 0x3D ] ) sleep ( 10e-3 ) # Read the config variables by sending 256 empty bytes for i in range ( 9 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) data [ "AMSamplingInterval" ] = self . _16bit_unsigned ( config [ 0 ] , config [ 1 ] ) data [ "AMIdleIntervalCount" ] = self . _16bit_unsigned ( config [ 2 ] , config [ 3 ] ) data [ 'AMFanOnIdle' ] = config [ 4 ] data [ 'AMLaserOnIdle' ] = config [ 5 ] data [ 'AMMaxDataArraysInFile' ] = self . _16bit_unsigned ( config [ 6 ] , config [ 7 ] ) data [ 'AMOnlySavePMData' ] = config [ 8 ] sleep ( 0.1 ) return data
10232	def list_abundance_cartesian_expansion ( graph : BELGraph ) -> None : for u , v , k , d in list ( graph . edges ( keys = True , data = True ) ) : if CITATION not in d : continue if isinstance ( u , ListAbundance ) and isinstance ( v , ListAbundance ) : for u_member , v_member in itt . product ( u . members , v . members ) : graph . add_qualified_edge ( u_member , v_member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , ListAbundance ) : for member in u . members : graph . add_qualified_edge ( member , v , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , ListAbundance ) : for member in v . members : graph . add_qualified_edge ( u , member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_list_abundance_nodes ( graph )
4975	def get_global_context ( request , enterprise_customer ) : platform_name = get_configuration_value ( "PLATFORM_NAME" , settings . PLATFORM_NAME ) # pylint: disable=no-member return { 'enterprise_customer' : enterprise_customer , 'LMS_SEGMENT_KEY' : settings . LMS_SEGMENT_KEY , 'LANGUAGE_CODE' : get_language_from_request ( request ) , 'tagline' : get_configuration_value ( "ENTERPRISE_TAGLINE" , settings . ENTERPRISE_TAGLINE ) , 'platform_description' : get_configuration_value ( "PLATFORM_DESCRIPTION" , settings . PLATFORM_DESCRIPTION , ) , 'LMS_ROOT_URL' : settings . LMS_ROOT_URL , 'platform_name' : platform_name , 'header_logo_alt_text' : _ ( '{platform_name} home page' ) . format ( platform_name = platform_name ) , 'welcome_text' : constants . WELCOME_TEXT . format ( platform_name = platform_name ) , 'enterprise_welcome_text' : constants . ENTERPRISE_WELCOME_TEXT . format ( enterprise_customer_name = enterprise_customer . name , platform_name = platform_name , strong_start = '<strong>' , strong_end = '</strong>' , line_break = '<br/>' , privacy_policy_link_start = "<a href='{pp_url}' target='_blank'>" . format ( pp_url = get_configuration_value ( 'PRIVACY' , 'https://www.edx.org/edx-privacy-policy' , type = 'url' ) , ) , privacy_policy_link_end = "</a>" , ) , }
745	def anomalyRemoveLabels ( self , start , end , labelFilter ) : self . _getAnomalyClassifier ( ) . getSelf ( ) . removeLabels ( start , end , labelFilter )
10163	def load ( self ) : ret = { } # Read the mdstat file with open ( self . get_path ( ) , 'r' ) as f : # lines is a list of line (with \n) lines = f . readlines ( ) # First line: get the personalities # The "Personalities" line tells you what RAID level the kernel currently supports. # This can be changed by either changing the raid modules or recompiling the kernel. # Possible personalities include: [raid0] [raid1] [raid4] [raid5] [raid6] [linear] [multipath] [faulty] ret [ 'personalities' ] = self . get_personalities ( lines [ 0 ] ) # Second to last before line: Array definition ret [ 'arrays' ] = self . get_arrays ( lines [ 1 : - 1 ] , ret [ 'personalities' ] ) # Save the file content as it for the __str__ method self . content = reduce ( lambda x , y : x + y , lines ) return ret
980	def _overlapOK ( self , i , j , overlap = None ) : if overlap is None : overlap = self . _countOverlapIndices ( i , j ) if abs ( i - j ) < self . w : if overlap == ( self . w - abs ( i - j ) ) : return True else : return False else : if overlap <= self . _maxOverlap : return True else : return False
5943	def autoconvert ( s ) : if type ( s ) is not str : return s for converter in int , float , str : # try them in increasing order of lenience try : s = [ converter ( i ) for i in s . split ( ) ] if len ( s ) == 1 : return s [ 0 ] else : return numpy . array ( s ) except ( ValueError , AttributeError ) : pass raise ValueError ( "Failed to autoconvert {0!r}" . format ( s ) )
4988	def eligible_for_direct_audit_enrollment ( self , request , enterprise_customer , resource_id , course_key = None ) : course_identifier = course_key if course_key else resource_id # Return it in one big statement to utilize short-circuiting behavior. Avoid the API call if possible. return request . GET . get ( 'audit' ) and request . path == self . COURSE_ENROLLMENT_VIEW_URL . format ( enterprise_customer . uuid , course_identifier ) and enterprise_customer . catalog_contains_course ( resource_id ) and EnrollmentApiClient ( ) . has_course_mode ( resource_id , 'audit' )
8777	def _chunks ( self , iterable , chunk_size ) : iterator = iter ( iterable ) chunk = list ( itertools . islice ( iterator , 0 , chunk_size ) ) while chunk : yield chunk chunk = list ( itertools . islice ( iterator , 0 , chunk_size ) )
4296	def parse_config_file ( parser , stdin_args ) : config_args = [ ] # Temporary switch required args and save them to restore. required_args = [ ] for action in parser . _actions : if action . required : required_args . append ( action ) action . required = False parsed_args = parser . parse_args ( stdin_args ) # Restore required args. for action in required_args : action . required = True if not parsed_args . config_file : return config_args config = ConfigParser ( ) if not config . read ( parsed_args . config_file ) : sys . stderr . write ( 'Config file "{0}" doesn\'t exists\n' . format ( parsed_args . config_file ) ) sys . exit ( 7 ) # It isn't used anywhere. config_args = _convert_config_to_stdin ( config , parser ) return config_args
8268	def color ( self , clr = None , d = 0.035 ) : # Revert to grayscale for black, white and grey hues. if clr != None and not isinstance ( clr , Color ) : clr = color ( clr ) if clr != None and not self . grayscale : if clr . is_black : return self . black . color ( clr , d ) if clr . is_white : return self . white . color ( clr , d ) if clr . is_grey : return choice ( ( self . black . color ( clr , d ) , self . white . color ( clr , d ) ) ) h , s , b , a = self . h , self . s , self . b , self . a if clr != None : h , a = clr . h + d * ( random ( ) * 2 - 1 ) , clr . a hsba = [ ] for v in [ h , s , b , a ] : if isinstance ( v , _list ) : min , max = choice ( v ) elif isinstance ( v , tuple ) : min , max = v else : min , max = v , v hsba . append ( min + ( max - min ) * random ( ) ) h , s , b , a = hsba return color ( h , s , b , a , mode = "hsb" )
505	def _getStateAnomalyVector ( self , state ) : vector = numpy . zeros ( self . _anomalyVectorLength ) vector [ state . anomalyVector ] = 1 return vector
7433	def _read_sample_names ( fname ) : try : with open ( fname , 'r' ) as infile : subsamples = [ x . split ( ) [ 0 ] for x in infile . readlines ( ) if x . strip ( ) ] except Exception as inst : print ( "Failed to read input file with sample names.\n{}" . format ( inst ) ) raise inst return subsamples
2043	def current_human_transaction ( self ) : try : tx , _ , _ , _ , _ = self . _callstack [ 0 ] if tx . result is not None : #That tx finished. No current tx. return None assert tx . depth == 0 return tx except IndexError : return None
2853	def mpsse_read_gpio ( self ) : # Send command to read low byte and high byte. self . _write ( '\x81\x83' ) # Wait for 2 byte response. data = self . _poll_read ( 2 ) # Assemble response into 16 bit value. low_byte = ord ( data [ 0 ] ) high_byte = ord ( data [ 1 ] ) logger . debug ( 'Read MPSSE GPIO low byte = {0:02X} and high byte = {1:02X}' . format ( low_byte , high_byte ) ) return ( high_byte << 8 ) | low_byte
5326	def __create_arthur_json ( self , repo , backend_args ) : backend_args = self . _compose_arthur_params ( self . backend_section , repo ) if self . backend_section == 'git' : backend_args [ 'gitpath' ] = os . path . join ( self . REPOSITORY_DIR , repo ) backend_args [ 'tag' ] = self . backend_tag ( repo ) ajson = { "tasks" : [ { } ] } # This is the perceval tag ajson [ "tasks" ] [ 0 ] [ 'task_id' ] = self . backend_tag ( repo ) ajson [ "tasks" ] [ 0 ] [ 'backend' ] = self . backend_section . split ( ":" ) [ 0 ] ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] = backend_args ajson [ "tasks" ] [ 0 ] [ 'category' ] = backend_args [ 'category' ] ajson [ "tasks" ] [ 0 ] [ 'archive' ] = { } ajson [ "tasks" ] [ 0 ] [ 'scheduler' ] = { "delay" : self . ARTHUR_TASK_DELAY } # from-date or offset param must be added es_col_url = self . _get_collection_url ( ) es_index = self . conf [ self . backend_section ] [ 'raw_index' ] # Get the last activity for the data source es = ElasticSearch ( es_col_url , es_index ) connector = get_connector_from_name ( self . backend_section ) klass = connector [ 0 ] # Backend for the connector signature = inspect . signature ( klass . fetch ) last_activity = None filter_ = { "name" : "tag" , "value" : backend_args [ 'tag' ] } if 'from_date' in signature . parameters : last_activity = es . get_last_item_field ( 'metadata__updated_on' , [ filter_ ] ) if last_activity : ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] [ 'from_date' ] = last_activity . isoformat ( ) elif 'offset' in signature . parameters : last_activity = es . get_last_item_field ( 'offset' , [ filter_ ] ) if last_activity : ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] [ 'offset' ] = last_activity if last_activity : logging . info ( "Getting raw item with arthur since %s" , last_activity ) return ( ajson )
5835	def __convert_response_to_configuration ( self , result_blob , dataset_ids ) : builder = DataViewBuilder ( ) builder . dataset_ids ( dataset_ids ) for i , ( k , v ) in enumerate ( result_blob [ 'descriptors' ] . items ( ) ) : try : descriptor = self . __snake_case ( v [ 0 ] ) print ( json . dumps ( descriptor ) ) descriptor [ 'descriptor_key' ] = k builder . add_raw_descriptor ( descriptor ) except IndexError : pass for i , ( k , v ) in enumerate ( result_blob [ 'types' ] . items ( ) ) : builder . set_role ( k , v . lower ( ) ) return builder . build ( )
8014	async def send_upstream ( self , message , stream_name = None ) : if stream_name is None : for steam_queue in self . application_streams . values ( ) : await steam_queue . put ( message ) return steam_queue = self . application_streams . get ( stream_name ) if steam_queue is None : raise ValueError ( "Invalid multiplexed frame received (stream not mapped)" ) await steam_queue . put ( message )
11157	def print_big_dir_and_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table1 = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p1 , size1 in size_table1 [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size1 ) , p1 . abspath ) ) size_table2 = sorted ( [ ( p , p . size ) for p in p1 . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p2 , size2 in size_table2 [ : top_n ] : print ( " {:<9} {:<9}" . format ( repr_data_size ( size2 ) , p2 . abspath ) )
1060	def cmp_to_key ( mycmp ) : class K ( object ) : __slots__ = [ 'obj' ] def __init__ ( self , obj , * args ) : self . obj = obj def __lt__ ( self , other ) : return mycmp ( self . obj , other . obj ) < 0 def __gt__ ( self , other ) : return mycmp ( self . obj , other . obj ) > 0 def __eq__ ( self , other ) : return mycmp ( self . obj , other . obj ) == 0 def __le__ ( self , other ) : return mycmp ( self . obj , other . obj ) <= 0 def __ge__ ( self , other ) : return mycmp ( self . obj , other . obj ) >= 0 def __ne__ ( self , other ) : return mycmp ( self . obj , other . obj ) != 0 def __hash__ ( self ) : raise TypeError ( 'hash not implemented' ) return K
10679	def S ( self , T ) : result = 0.0 if T < self . Tmax : lT = T else : lT = self . Tmax Tref = self . Tmin for c , e in zip ( self . _coefficients , self . _exponents ) : # Create a modified exponent to analytically integrate Cp(T)/T # instead of Cp(T). e_modified = e - 1.0 if e_modified == - 1.0 : result += c * math . log ( lT / Tref ) else : e_mod = e_modified + 1.0 result += c * ( lT ** e_mod - Tref ** e_mod ) / e_mod return result
5129	def size ( self , s ) : leader = self . find ( s ) return self . _size [ leader ]
9530	def encrypt ( base_field , key = None , ttl = None ) : if not isinstance ( base_field , models . Field ) : assert key is None assert ttl is None return get_encrypted_field ( base_field ) name , path , args , kwargs = base_field . deconstruct ( ) kwargs . update ( { 'key' : key , 'ttl' : ttl } ) return get_encrypted_field ( base_field . __class__ ) ( * args , * * kwargs )
12848	def add_safety_checks ( meta , members ) : for member_name , member_value in members . items ( ) : members [ member_name ] = meta . add_safety_check ( member_name , member_value )
7421	def fetch_cluster_se ( data , samfile , chrom , rstart , rend ) : ## If SE then we enforce the minimum overlap distance to avoid the ## staircase syndrome of multiple reads overlapping just a little. overlap_buffer = data . _hackersonly [ "min_SE_refmap_overlap" ] ## the *_buff variables here are because we have to play patty ## cake here with the rstart/rend vals because we want pysam to ## enforce the buffer for SE, but we want the reference sequence ## start and end positions to print correctly for downstream. rstart_buff = rstart + overlap_buffer rend_buff = rend - overlap_buffer ## Reads that map to only very short segements of the reference ## sequence will return buffer end values that are before the ## start values causing pysam to complain. Very short mappings. if rstart_buff > rend_buff : tmp = rstart_buff rstart_buff = rend_buff rend_buff = tmp ## Buffering can't make start and end equal or pysam returns nothing. if rstart_buff == rend_buff : rend_buff += 1 ## store pairs rdict = { } clust = [ ] iterreg = [ ] iterreg = samfile . fetch ( chrom , rstart_buff , rend_buff ) ## use dict to match up read pairs for read in iterreg : if read . qname not in rdict : rdict [ read . qname ] = read ## sort dict keys so highest derep is first ('seed') sfunc = lambda x : int ( x . split ( ";size=" ) [ 1 ] . split ( ";" ) [ 0 ] ) rkeys = sorted ( rdict . keys ( ) , key = sfunc , reverse = True ) ## get blocks from the seed for filtering, bail out if seed is not paired try : read1 = rdict [ rkeys [ 0 ] ] except ValueError : LOGGER . error ( "Found bad cluster, skipping - key:{} rdict:{}" . format ( rkeys [ 0 ] , rdict ) ) return "" ## the starting blocks for the seed poss = read1 . get_reference_positions ( full_length = True ) seed_r1start = min ( poss ) seed_r1end = max ( poss ) ## store the seed ------------------------------------------- if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq ## store, could write orient but just + for now. size = sfunc ( rkeys [ 0 ] ) clust . append ( ">{}:{}:{};size={};*\n{}" . format ( chrom , seed_r1start , seed_r1end , size , seq ) ) ## If there's only one hit in this region then rkeys will only have ## one element and the call to `rkeys[1:]` will raise. Test for this. if len ( rkeys ) > 1 : ## store the hits to the seed ------------------------------- for key in rkeys [ 1 : ] : skip = False try : read1 = rdict [ key ] except ValueError : ## enter values that will make this read get skipped read1 = rdict [ key ] [ 0 ] skip = True ## orient reads only if not skipping if not skip : poss = read1 . get_reference_positions ( full_length = True ) minpos = min ( poss ) maxpos = max ( poss ) ## store the seq if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq ## store, could write orient but just + for now. size = sfunc ( key ) clust . append ( ">{}:{}:{};size={};+\n{}" . format ( chrom , minpos , maxpos , size , seq ) ) else : ## seq is excluded, though, we could save it and return ## it as a separate cluster that will be aligned separately. pass return clust
9298	def paginate_query ( self , query , count , offset = None , sort = None ) : assert isinstance ( query , peewee . Query ) assert isinstance ( count , int ) assert isinstance ( offset , ( str , int , type ( None ) ) ) assert isinstance ( sort , ( list , set , tuple , type ( None ) ) ) # ensure our model has a primary key fields = query . model . _meta . get_primary_keys ( ) if len ( fields ) == 0 : raise peewee . ProgrammingError ( 'Cannot apply pagination on model without primary key' ) # ensure our model doesn't use a compound primary key if len ( fields ) > 1 : raise peewee . ProgrammingError ( 'Cannot apply pagination on model with compound primary key' ) # apply offset if offset is not None : query = query . where ( fields [ 0 ] >= offset ) # do we need to apply sorting? order_bys = [ ] if sort : for field , direction in sort : # does this field have a valid sort direction? if not isinstance ( direction , str ) : raise ValueError ( "Invalid sort direction on field '{}'" . format ( field ) ) direction = direction . lower ( ) . strip ( ) if direction not in [ 'asc' , 'desc' ] : raise ValueError ( "Invalid sort direction on field '{}'" . format ( field ) ) # apply sorting order_by = peewee . SQL ( field ) order_by = getattr ( order_by , direction ) ( ) order_bys += [ order_by ] # add primary key ordering after user sorting order_bys += [ fields [ 0 ] . asc ( ) ] # apply ordering and limits query = query . order_by ( * order_bys ) query = query . limit ( count ) return query
5714	def _validate_zip ( the_zip ) : datapackage_jsons = [ f for f in the_zip . namelist ( ) if f . endswith ( 'datapackage.json' ) ] if len ( datapackage_jsons ) != 1 : msg = 'DataPackage must have only one "datapackage.json" (had {n})' raise exceptions . DataPackageException ( msg . format ( n = len ( datapackage_jsons ) ) )
7730	def clear_muc_child ( self ) : if self . muc_child : self . muc_child . free_borrowed ( ) self . muc_child = None if not self . xmlnode . children : return n = self . xmlnode . children while n : if n . name not in ( "x" , "query" ) : n = n . next continue ns = n . ns ( ) if not ns : n = n . next continue ns_uri = ns . getContent ( ) if ns_uri in ( MUC_NS , MUC_USER_NS , MUC_ADMIN_NS , MUC_OWNER_NS ) : n . unlinkNode ( ) n . freeNode ( ) n = n . next
4115	def lar2rc ( g ) : assert numpy . isrealobj ( g ) , 'Log area ratios not defined for complex reflection coefficients.' # Use the relation, tanh(x) = (1-exp(2x))/(1+exp(2x)) return - numpy . tanh ( - numpy . array ( g ) / 2 )
11940	def mark_read ( user , message ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) backend . inbox_delete ( user , message )
4056	def _json_processor ( self , retrieved ) : json_kwargs = { } if self . preserve_json_order : json_kwargs [ "object_pairs_hook" ] = OrderedDict # send entries to _tags_data if there's no JSON try : items = [ json . loads ( e [ "content" ] [ 0 ] [ "value" ] , * * json_kwargs ) for e in retrieved . entries ] except KeyError : return self . _tags_data ( retrieved ) return items
1486	def _modules_to_main ( modList ) : if not modList : return main = sys . modules [ '__main__' ] for modname in modList : if isinstance ( modname , str ) : try : mod = __import__ ( modname ) except Exception : sys . stderr . write ( 'warning: could not import %s\n. ' 'Your function may unexpectedly error due to this import failing;' 'A version mismatch is likely. Specific error was:\n' % modname ) print_exec ( sys . stderr ) else : setattr ( main , mod . __name__ , mod )
5235	def filter_by_dates ( files_or_folders : list , date_fmt = DATE_FMT ) -> list : r = re . compile ( f'.*{date_fmt}.*' ) return list ( filter ( lambda vv : r . match ( vv . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] ) is not None , files_or_folders , ) )
2168	def list_misc_commands ( self ) : answer = set ( [ ] ) for cmd_name in misc . __all__ : answer . add ( cmd_name ) return sorted ( answer )
3310	def _run_ext_wsgiutils ( app , config , mode ) : from wsgidav . server import ext_wsgiutils_server _logger . info ( "Running WsgiDAV {} on wsgidav.ext_wsgiutils_server..." . format ( __version__ ) ) _logger . warning ( "WARNING: This single threaded server (ext-wsgiutils) is not meant for production." ) try : ext_wsgiutils_server . serve ( config , app ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
8820	def delete_network ( context , id ) : LOG . info ( "delete_network %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : net = db_api . network_find ( context = context , limit = None , sorts = [ 'id' ] , marker = None , page_reverse = False , id = id , scope = db_api . ONE ) if not net : raise n_exc . NetworkNotFound ( net_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( net . id ) : raise n_exc . NotAuthorized ( net_id = id ) if net . ports : raise n_exc . NetworkInUse ( net_id = id ) net_driver = registry . DRIVER_REGISTRY . get_driver ( net [ "network_plugin" ] ) net_driver . delete_network ( context , id ) for subnet in net [ "subnets" ] : subnets . _delete_subnet ( context , subnet ) db_api . network_delete ( context , net )
7020	def generate_hatpi_binnedlc_pkl ( binnedpklf , textlcf , timebinsec , outfile = None ) : binlcdict = read_hatpi_binnedlc ( binnedpklf , textlcf , timebinsec ) if binlcdict : if outfile is None : outfile = os . path . join ( os . path . dirname ( binnedpklf ) , '%s-hplc.pkl' % ( os . path . basename ( binnedpklf ) . replace ( 'sec-lc.pkl.gz' , '' ) ) ) return lcdict_to_pickle ( binlcdict , outfile = outfile ) else : LOGERROR ( 'could not read binned HATPI LC: %s' % binnedpklf ) return None
12190	def _format_message ( self , channel , text ) : payload = { 'type' : 'message' , 'id' : next ( self . _msg_ids ) } payload . update ( channel = channel , text = text ) return json . dumps ( payload )
8991	def rows_before ( self ) : rows_before = [ ] for mesh in self . consumed_meshes : if mesh . is_produced ( ) : row = mesh . producing_row if rows_before not in rows_before : rows_before . append ( row ) return rows_before
10837	def zjitter ( jitter = 0.0 , radius = 5 ) : psfsize = np . array ( [ 2.0 , 1.0 , 3.0 ] ) # create a base image of one particle s0 = init . create_single_particle_state ( imsize = 4 * radius , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) sl = np . s_ [ s0 . pad : - s0 . pad , s0 . pad : - s0 . pad , s0 . pad : - s0 . pad ] # add up a bunch of trajectories finalimage = 0 * s0 . get_model_image ( ) [ sl ] position = 0 * s0 . obj . pos [ 0 ] for i in xrange ( finalimage . shape [ 0 ] ) : offset = jitter * np . random . randn ( 3 ) * np . array ( [ 1 , 0 , 0 ] ) s0 . obj . pos [ 0 ] = np . array ( s0 . image . shape ) / 2 + offset s0 . reset ( ) finalimage [ i ] = s0 . get_model_image ( ) [ sl ] [ i ] position += s0 . obj . pos [ 0 ] position /= float ( finalimage . shape [ 0 ] ) # place that into a new image at the expected parameters s = init . create_single_particle_state ( imsize = 4 * radius , sigma = 0.05 , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) s . reset ( ) # measure the true inferred parameters return s , finalimage , position
4839	def get_course_details ( self , course_id ) : return self . _load_data ( self . COURSES_ENDPOINT , resource_id = course_id , many = False )
7017	def parallel_concat_worker ( task ) : lcbasedir , objectid , kwargs = task try : return concat_write_pklc ( lcbasedir , objectid , * * kwargs ) except Exception as e : LOGEXCEPTION ( 'failed LC concatenation for %s in %s' % ( objectid , lcbasedir ) ) return None
12214	def get_pref_model_class ( app , prefs , get_prefs_func ) : module = '%s.%s' % ( app , PREFS_MODULE_NAME ) model_dict = { '_prefs_app' : app , '_get_prefs' : staticmethod ( get_prefs_func ) , '__module__' : module , 'Meta' : type ( 'Meta' , ( models . options . Options , ) , { 'verbose_name' : _ ( 'Preference' ) , 'verbose_name_plural' : _ ( 'Preferences' ) , 'app_label' : app , 'managed' : False , } ) } for field_name , val_proxy in prefs . items ( ) : model_dict [ field_name ] = val_proxy . field model = type ( 'Preferences' , ( models . Model , ) , model_dict ) def fake_save_base ( self , * args , * * kwargs ) : updated_prefs = { f . name : getattr ( self , f . name ) for f in self . _meta . fields if not isinstance ( f , models . fields . AutoField ) } app_prefs = self . _get_prefs ( self . _prefs_app ) for pref in app_prefs . keys ( ) : if pref in updated_prefs : app_prefs [ pref ] . db_value = updated_prefs [ pref ] self . pk = self . _prefs_app # Make Django 1.7 happy. prefs_save . send ( sender = self , app = self . _prefs_app , updated_prefs = updated_prefs ) return True model . save_base = fake_save_base return model
10170	def set_scheduled ( self ) : with self . _idle_lock : if self . _idle : self . _idle = False return True return False
6426	def sim_sift4 ( src , tar , max_offset = 5 , max_distance = 0 ) : return Sift4 ( ) . sim ( src , tar , max_offset , max_distance )
3193	def update ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) , data = data )
1825	def LEAVE ( cpu ) : cpu . STACK = cpu . FRAME cpu . FRAME = cpu . pop ( cpu . address_bit_size )
1316	def GetAllPixelColors ( self ) -> ctypes . Array : return self . GetPixelColorsOfRect ( 0 , 0 , self . Width , self . Height )
3317	def digest_auth_user ( self , realm , user_name , environ ) : user = self . _get_realm_entry ( realm , user_name ) if user is None : return False password = user . get ( "password" ) environ [ "wsgidav.auth.roles" ] = user . get ( "roles" , [ ] ) return self . _compute_http_digest_a1 ( realm , user_name , password )
2927	def write_file_to_package_zip ( self , filename , src_filename ) : f = open ( src_filename ) with f : data = f . read ( ) self . manifest [ filename ] = md5hash ( data ) self . package_zip . write ( src_filename , filename )
8230	def size ( self , w = None , h = None ) : if not w : w = self . _canvas . width if not h : h = self . _canvas . height if not w and not h : return ( self . _canvas . width , self . _canvas . height ) # FIXME: Updating in all these places seems a bit hacky w , h = self . _canvas . set_size ( ( w , h ) ) self . _namespace [ 'WIDTH' ] = w self . _namespace [ 'HEIGHT' ] = h self . WIDTH = w # Added to make evolution example work self . HEIGHT = h
1194	def put ( self , item , block = True , timeout = None ) : self . not_full . acquire ( ) try : if self . maxsize > 0 : if not block : if self . _qsize ( ) == self . maxsize : raise Full elif timeout is None : while self . _qsize ( ) == self . maxsize : self . not_full . wait ( ) elif timeout < 0 : raise ValueError ( "'timeout' must be a non-negative number" ) else : endtime = _time ( ) + timeout while self . _qsize ( ) == self . maxsize : remaining = endtime - _time ( ) if remaining <= 0.0 : raise Full self . not_full . wait ( remaining ) self . _put ( item ) self . unfinished_tasks += 1 self . not_empty . notify ( ) finally : self . not_full . release ( )
1684	def Begin ( self , function_name ) : self . in_a_function = True self . lines_in_function = 0 self . current_function = function_name
11247	def triangle_area ( point1 , point2 , point3 ) : """Lengths of the three sides of the triangle""" a = point_distance ( point1 , point2 ) b = point_distance ( point1 , point3 ) c = point_distance ( point2 , point3 ) """Where s is the semiperimeter""" s = ( a + b + c ) / 2.0 """Return the area of the triangle (using Heron's formula)""" return math . sqrt ( s * ( s - a ) * ( s - b ) * ( s - c ) )
9190	def admin_content_status_single ( request ) : uuid = request . matchdict [ 'uuid' ] try : UUID ( uuid ) except ValueError : raise httpexceptions . HTTPBadRequest ( '{} is not a valid uuid' . format ( uuid ) ) statement , sql_args = get_baking_statuses_sql ( { 'uuid' : uuid } ) with db_connect ( cursor_factory = DictCursor ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( statement , sql_args ) modules = cursor . fetchall ( ) if len ( modules ) == 0 : raise httpexceptions . HTTPBadRequest ( '{} is not a book' . format ( uuid ) ) states = [ ] collection_info = modules [ 0 ] for row in modules : message = '' state = row [ 'state' ] or 'PENDING' if state == 'FAILURE' : # pragma: no cover if row [ 'traceback' ] is not None : message = row [ 'traceback' ] latest_recipe = row [ 'latest_recipe_id' ] current_recipe = row [ 'recipe_id' ] if ( latest_recipe is not None and current_recipe != latest_recipe ) : state += ' stale_recipe' states . append ( { 'version' : row [ 'current_version' ] , 'recipe' : row [ 'recipe' ] , 'created' : str ( row [ 'created' ] ) , 'state' : state , 'state_message' : message , } ) return { 'uuid' : str ( collection_info [ 'uuid' ] ) , 'title' : collection_info [ 'name' ] . decode ( 'utf-8' ) , 'authors' : format_authors ( collection_info [ 'authors' ] ) , 'print_style' : collection_info [ 'print_style' ] , 'current_recipe' : collection_info [ 'recipe_id' ] , 'current_ident' : collection_info [ 'module_ident' ] , 'current_state' : states [ 0 ] [ 'state' ] , 'states' : states }
3900	def dir_maker ( path ) : directory = os . path . dirname ( path ) if directory != '' and not os . path . isdir ( directory ) : try : os . makedirs ( directory ) except OSError as e : sys . exit ( 'Failed to create directory: {}' . format ( e ) )
7104	def transform ( self , transformer ) : self . transformers . append ( transformer ) from languageflow . transformer . tagged import TaggedTransformer if isinstance ( transformer , TaggedTransformer ) : self . X , self . y = transformer . transform ( self . sentences ) if isinstance ( transformer , TfidfVectorizer ) : self . X = transformer . fit_transform ( self . X ) if isinstance ( transformer , CountVectorizer ) : self . X = transformer . fit_transform ( self . X ) if isinstance ( transformer , NumberRemover ) : self . X = transformer . transform ( self . X ) if isinstance ( transformer , MultiLabelBinarizer ) : self . y = transformer . fit_transform ( self . y )
2136	def disassociate_notification_template ( self , workflow , notification_template , status ) : return self . _disassoc ( 'notification_templates_%s' % status , workflow , notification_template )
6284	def start ( self ) : self . music . start ( ) if not self . start_paused : self . rocket . start ( )
3438	def merge ( self , right , prefix_existing = None , inplace = True , objective = 'left' ) : if inplace : new_model = self else : new_model = self . copy ( ) new_model . id = '{}_{}' . format ( self . id , right . id ) new_reactions = deepcopy ( right . reactions ) if prefix_existing is not None : existing = new_reactions . query ( lambda rxn : rxn . id in self . reactions ) for reaction in existing : reaction . id = '{}{}' . format ( prefix_existing , reaction . id ) new_model . add_reactions ( new_reactions ) interface = new_model . problem new_vars = [ interface . Variable . clone ( v ) for v in right . variables if v . name not in new_model . variables ] new_model . add_cons_vars ( new_vars ) new_cons = [ interface . Constraint . clone ( c , model = new_model . solver ) for c in right . constraints if c . name not in new_model . constraints ] new_model . add_cons_vars ( new_cons , sloppy = True ) new_model . objective = dict ( left = self . objective , right = right . objective , sum = self . objective . expression + right . objective . expression ) [ objective ] return new_model
0	def save_act ( self , path = None ) : if path is None : path = os . path . join ( logger . get_dir ( ) , "model.pkl" ) with tempfile . TemporaryDirectory ( ) as td : save_variables ( os . path . join ( td , "model" ) ) arc_name = os . path . join ( td , "packed.zip" ) with zipfile . ZipFile ( arc_name , 'w' ) as zipf : for root , dirs , files in os . walk ( td ) : for fname in files : file_path = os . path . join ( root , fname ) if file_path != arc_name : zipf . write ( file_path , os . path . relpath ( file_path , td ) ) with open ( arc_name , "rb" ) as f : model_data = f . read ( ) with open ( path , "wb" ) as f : cloudpickle . dump ( ( model_data , self . _act_params ) , f )
10599	def create_template ( material , path , show = False ) : file_name = 'dataset-%s.csv' % material . lower ( ) file_path = os . path . join ( path , file_name ) with open ( file_path , 'w' , newline = '' ) as csvfile : writer = csv . writer ( csvfile , delimiter = ',' , quotechar = '"' , quoting = csv . QUOTE_MINIMAL ) writer . writerow ( [ 'Name' , material ] ) writer . writerow ( [ 'Description' , '<Add a data set description ' 'here.>' ] ) writer . writerow ( [ 'Reference' , '<Add a reference to the source of ' 'the data set here.>' ] ) writer . writerow ( [ 'Temperature' , '<parameter 1 name>' , '<parameter 2 name>' , '<parameter 3 name>' ] ) writer . writerow ( [ 'T' , '<parameter 1 display symbol>' , '<parameter 2 display symbol>' , '<parameter 3 display symbol>' ] ) writer . writerow ( [ 'K' , '<parameter 1 units>' , '<parameter 2 units>' , '<parameter 3 units>' ] ) writer . writerow ( [ 'T' , '<parameter 1 symbol>' , '<parameter 2 symbol>' , '<parameter 3 symbol>' ] ) for i in range ( 10 ) : writer . writerow ( [ 100.0 + i * 50 , float ( i ) , 10.0 + i , 100.0 + i ] ) if show is True : webbrowser . open_new ( file_path )
6942	def invgauss_eclipses_func ( ebparams , times , mags , errs ) : ( period , epoch , pdepth , pduration , depthratio , secondaryphase ) = ebparams # generate the phases iphase = ( times - epoch ) / period iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] zerolevel = np . median ( pmags ) modelmags = np . full_like ( phase , zerolevel ) primaryecl_amp = - pdepth secondaryecl_amp = - pdepth * depthratio primaryecl_std = pduration / 5.0 # we use 5-sigma as full-width -> duration secondaryecl_std = pduration / 5.0 # secondary eclipse has the same duration halfduration = pduration / 2.0 # phase indices primary_eclipse_ingress = ( ( phase >= ( 1.0 - halfduration ) ) & ( phase <= 1.0 ) ) primary_eclipse_egress = ( ( phase >= 0.0 ) & ( phase <= halfduration ) ) secondary_eclipse_phase = ( ( phase >= ( secondaryphase - halfduration ) ) & ( phase <= ( secondaryphase + halfduration ) ) ) # put in the eclipses modelmags [ primary_eclipse_ingress ] = ( zerolevel + _gaussian ( phase [ primary_eclipse_ingress ] , primaryecl_amp , 1.0 , primaryecl_std ) ) modelmags [ primary_eclipse_egress ] = ( zerolevel + _gaussian ( phase [ primary_eclipse_egress ] , primaryecl_amp , 0.0 , primaryecl_std ) ) modelmags [ secondary_eclipse_phase ] = ( zerolevel + _gaussian ( phase [ secondary_eclipse_phase ] , secondaryecl_amp , secondaryphase , secondaryecl_std ) ) return modelmags , phase , ptimes , pmags , perrs
5464	def get_action_by_id ( op , action_id ) : actions = get_actions ( op ) if actions and 1 <= action_id < len ( actions ) : return actions [ action_id - 1 ]
419	def save_training_log ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) kwargs . update ( { 'time' : datetime . utcnow ( ) } ) _result = self . db . TrainLog . insert_one ( kwargs ) _log = self . _print_dict ( kwargs ) logging . info ( "[Database] train log: " + _log )
3578	def initialize ( self ) : # Ensure GLib's threading is initialized to support python threads, and # make a default mainloop that all DBus objects will inherit. These # commands MUST execute before any other DBus commands! GObject . threads_init ( ) dbus . mainloop . glib . threads_init ( ) # Set the default main loop, this also MUST happen before other DBus calls. self . _mainloop = dbus . mainloop . glib . DBusGMainLoop ( set_as_default = True ) # Get the main DBus system bus and root bluez object. self . _bus = dbus . SystemBus ( ) self . _bluez = dbus . Interface ( self . _bus . get_object ( 'org.bluez' , '/' ) , 'org.freedesktop.DBus.ObjectManager' )
1121	def format ( self , o , context , maxlevels , level ) : return _safe_repr ( o , context , maxlevels , level )
9638	def emit ( self , record ) : try : self . redis_client . publish ( self . channel , self . format ( record ) ) except redis . RedisError : pass
4791	def is_upper ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . upper ( ) : self . _err ( 'Expected <%s> to contain only uppercase chars, but did not.' % self . val ) return self
1817	def SETNS ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . SF == False , 1 , 0 ) )
3798	def setup_a_alpha_and_derivatives ( self , i , T = None ) : self . a , self . Tc , self . S1 , self . S2 = self . ais [ i ] , self . Tcs [ i ] , self . S1s [ i ] , self . S2s [ i ]
5670	def temporal_network ( gtfs , start_time_ut = None , end_time_ut = None , route_type = None ) : events_df = gtfs . get_transit_events ( start_time_ut = start_time_ut , end_time_ut = end_time_ut , route_type = route_type ) events_df . drop ( 'to_seq' , 1 , inplace = True ) events_df . drop ( 'shape_id' , 1 , inplace = True ) events_df . drop ( 'duration' , 1 , inplace = True ) events_df . drop ( 'route_id' , 1 , inplace = True ) events_df . rename ( columns = { 'from_seq' : "seq" } , inplace = True ) return events_df
7560	def get_total ( tots , node ) : if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : ## Get counts on down edges. ## How to treat polytomies here? if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = sum ( 1 for i in down_r . iter_leaves ( ) ) lendl = sum ( 1 for i in down_l . iter_leaves ( ) ) ## get count on up edge sister up_r = node . get_sisters ( ) [ 0 ] lenur = sum ( 1 for i in up_r . iter_leaves ( ) ) ## everyone else lenul = tots - ( lendr + lendl + lenur ) ## return product return lendr * lendl * lenur * lenul
6114	def resized_scaled_array_from_array ( self , new_shape , new_centre_pixels = None , new_centre_arcsec = None ) : if new_centre_pixels is None and new_centre_arcsec is None : new_centre = ( - 1 , - 1 ) # In Numba, the input origin must be the same image type as the origin, thus we cannot # pass 'None' and instead use the tuple (-1, -1). elif new_centre_pixels is not None and new_centre_arcsec is None : new_centre = new_centre_pixels elif new_centre_pixels is None and new_centre_arcsec is not None : new_centre = self . arc_second_coordinates_to_pixel_coordinates ( arc_second_coordinates = new_centre_arcsec ) else : raise exc . DataException ( 'You have supplied two centres (pixels and arc-seconds) to the resize scaled' 'array function' ) return self . new_with_array ( array = array_util . resized_array_2d_from_array_2d_and_resized_shape ( array_2d = self , resized_shape = new_shape , origin = new_centre ) )
2518	def p_file_cr_text ( self , f_term , predicate ) : try : for _ , _ , cr_text in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_copyright ( self . doc , six . text_type ( cr_text ) ) except CardinalityError : self . more_than_one_error ( 'file copyright text' )
7136	def redirect_stdout ( new_stdout ) : old_stdout , sys . stdout = sys . stdout , new_stdout try : yield None finally : sys . stdout = old_stdout
12922	def render ( self , * args , * * kwargs ) : render_to = StringIO ( ) self . output ( render_to , * args , * * kwargs ) return render_to . getvalue ( )
10437	def getapplist ( self ) : app_list = [ ] # Update apps list, before parsing the list self . _update_apps ( ) for gui in self . _running_apps : name = gui . localizedName ( ) # default type was objc.pyobjc_unicode # convert to Unicode, else exception is thrown # TypeError: "cannot marshal <type 'objc.pyobjc_unicode'> objects" try : name = unicode ( name ) except NameError : name = str ( name ) except UnicodeEncodeError : pass app_list . append ( name ) # Return unique application list return list ( set ( app_list ) )
4073	def eval_environ ( value ) : def eval_environ_str ( value ) : parts = value . split ( ';' ) if len ( parts ) < 2 : return value expr = parts [ 1 ] . lstrip ( ) if not re . match ( "^((\\w+(\\.\\w+)?|'.*?'|\".*?\")\\s+" '(in|==|!=|not in)\\s+' "(\\w+(\\.\\w+)?|'.*?'|\".*?\")" '(\\s+(or|and)\\s+)?)+$' , expr ) : raise ValueError ( 'bad environment marker: %r' % expr ) expr = re . sub ( r"(platform\.\w+)" , r"\1()" , expr ) return parts [ 0 ] if eval ( expr ) else '' if isinstance ( value , list ) : new_value = [ ] for element in value : element = eval_environ_str ( element ) if element : new_value . append ( element ) elif isinstance ( value , str ) : new_value = eval_environ_str ( value ) else : new_value = value return new_value
4064	def add_tags ( self , item , * tags ) : # Make sure there's a tags field, or add one try : assert item [ "data" ] [ "tags" ] except AssertionError : item [ "data" ] [ "tags" ] = list ( ) for tag in tags : item [ "data" ] [ "tags" ] . append ( { "tag" : "%s" % tag } ) # make sure everything's OK assert self . check_items ( [ item ] ) return self . update_item ( item )
10940	def calc_J ( self ) : del self . J self . J = np . zeros ( [ self . param_vals . size , self . data . size ] ) dp = np . zeros_like ( self . param_vals ) f0 = self . model . copy ( ) for a in range ( self . param_vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param_vals + dp , * self . func_args , * * self . func_kwargs ) grad_func = ( f1 - f0 ) / dp [ a ] #J = grad(residuals) = -grad(model) self . J [ a ] = - grad_func
7027	def objectlist_radeclbox ( radeclbox , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g_mean_mag' , 'l' , 'b' , 'parallax, parallax_error' , 'pmra' , 'pmra_error' , 'pmdec' , 'pmdec_error' ) , extra_filter = None , returnformat = 'csv' , forcefetch = False , cachedir = '~/.astrobase/gaia-cache' , verbose = True , timeout = 15.0 , refresh = 2.0 , maxtimeout = 300.0 , maxtries = 3 , complete_query_later = True ) : # this was generated using the awesome query generator at: # https://gea.esac.esa.int/archive/ # NOTE: here we don't resolve the table name right away. this is because # some of the GAIA mirrors use different table names, so we leave the table # name to be resolved by the lower level tap_query function. this is done by # the {{table}} construct. query = ( "select {columns} from {{table}} where " "CONTAINS(POINT('ICRS',{{table}}.ra, {{table}}.dec)," "BOX('ICRS',{ra_center:.5f},{decl_center:.5f}," "{ra_width:.5f},{decl_height:.5f}))=1" "{extra_filter_str}" ) ra_min , ra_max , decl_min , decl_max = radeclbox ra_center = ( ra_max + ra_min ) / 2.0 decl_center = ( decl_max + decl_min ) / 2.0 ra_width = ra_max - ra_min decl_height = decl_max - decl_min if extra_filter is not None : extra_filter_str = ' and %s ' % extra_filter else : extra_filter_str = '' formatted_query = query . format ( columns = ', ' . join ( columns ) , extra_filter_str = extra_filter_str , ra_center = ra_center , decl_center = decl_center , ra_width = ra_width , decl_height = decl_height ) return tap_query ( formatted_query , gaia_mirror = gaia_mirror , returnformat = returnformat , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , complete_query_later = complete_query_later )
4394	def adsSyncReadReqEx2 ( port , address , index_group , index_offset , data_type , return_ctypes = False ) : # type: (int, AmsAddr, int, int, Type, bool) -> Any sync_read_request = _adsDLL . AdsSyncReadReqEx2 ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) index_group_c = ctypes . c_ulong ( index_group ) index_offset_c = ctypes . c_ulong ( index_offset ) if data_type == PLCTYPE_STRING : data = ( STRING_BUFFER * PLCTYPE_STRING ) ( ) else : data = data_type ( ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . c_ulong ( ctypes . sizeof ( data ) ) bytes_read = ctypes . c_ulong ( ) bytes_read_pointer = ctypes . pointer ( bytes_read ) error_code = sync_read_request ( port , ams_address_pointer , index_group_c , index_offset_c , data_length , data_pointer , bytes_read_pointer , ) if error_code : raise ADSError ( error_code ) # If we're reading a value of predetermined size (anything but a string), # validate that the correct number of bytes were read if data_type != PLCTYPE_STRING and bytes_read . value != data_length . value : raise RuntimeError ( "Insufficient data (expected {0} bytes, {1} were read)." . format ( data_length . value , bytes_read . value ) ) if return_ctypes : return data if data_type == PLCTYPE_STRING : return data . value . decode ( "utf-8" ) if type ( data_type ) . __name__ == "PyCArrayType" : return [ i for i in data ] if hasattr ( data , "value" ) : return data . value return data
12622	def have_same_shape ( array1 , array2 , nd_to_check = None ) : shape1 = array1 . shape shape2 = array2 . shape if nd_to_check is not None : if len ( shape1 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the first image: \n{}\n.' . format ( shape1 ) raise ValueError ( msg ) elif len ( shape2 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the second image: \n{}\n.' . format ( shape2 ) raise ValueError ( msg ) shape1 = shape1 [ : nd_to_check ] shape2 = shape2 [ : nd_to_check ] return shape1 == shape2
9755	def delete ( ctx ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if not click . confirm ( "Are sure you want to delete experiment `{}`" . format ( _experiment ) ) : click . echo ( 'Existing without deleting experiment.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . experiment . delete_experiment ( user , project_name , _experiment ) # Purge caching ExperimentManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment `{}` was delete successfully" . format ( _experiment ) )
7971	def _run_timeout_threads ( self , handler ) : # pylint: disable-msg=W0212 for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , "_pyxmpp_timeout" ) : continue thread = TimeoutThread ( method , daemon = self . daemon , exc_queue = self . exc_queue ) self . timeout_threads . append ( thread ) thread . start ( )
9172	def includeme ( config ) : config . include ( 'pyramid_jinja2' ) config . add_jinja2_renderer ( '.html' ) config . add_jinja2_renderer ( '.rss' ) config . add_static_view ( name = '/a/static' , path = "cnxpublishing:static/" ) # Commit the configuration otherwise the jija2_env won't have # a `globals` assignment. config . commit ( ) # Place a few globals in the template environment. from cnxdb . ident_hash import join_ident_hash for ext in ( '.html' , '.rss' , ) : jinja2_env = config . get_jinja2_environment ( ext ) jinja2_env . globals . update ( join_ident_hash = join_ident_hash , ) declare_api_routes ( config ) declare_browsable_routes ( config )
4040	def _cleanup ( self , to_clean , allow = ( ) ) : # this item's been retrieved from the API, we only need the 'data' # entry if to_clean . keys ( ) == [ "links" , "library" , "version" , "meta" , "key" , "data" ] : to_clean = to_clean [ "data" ] return dict ( [ [ k , v ] for k , v in list ( to_clean . items ( ) ) if ( k in allow or k not in self . temp_keys ) ] )
5928	def getpath ( self , section , option ) : return os . path . expanduser ( os . path . expandvars ( self . get ( section , option ) ) )
5607	def resample_from_array ( in_raster = None , in_affine = None , out_tile = None , in_crs = None , resampling = "nearest" , nodataval = 0 ) : # TODO rename function if isinstance ( in_raster , ma . MaskedArray ) : pass if isinstance ( in_raster , np . ndarray ) : in_raster = ma . MaskedArray ( in_raster , mask = in_raster == nodataval ) elif isinstance ( in_raster , ReferencedRaster ) : in_affine = in_raster . affine in_crs = in_raster . crs in_raster = in_raster . data elif isinstance ( in_raster , tuple ) : in_raster = ma . MaskedArray ( data = np . stack ( in_raster ) , mask = np . stack ( [ band . mask if isinstance ( band , ma . masked_array ) else np . where ( band == nodataval , True , False ) for band in in_raster ] ) , fill_value = nodataval ) else : raise TypeError ( "wrong input data type: %s" % type ( in_raster ) ) if in_raster . ndim == 2 : in_raster = ma . expand_dims ( in_raster , axis = 0 ) elif in_raster . ndim == 3 : pass else : raise TypeError ( "input array must have 2 or 3 dimensions" ) if in_raster . fill_value != nodataval : ma . set_fill_value ( in_raster , nodataval ) out_shape = ( in_raster . shape [ 0 ] , ) + out_tile . shape dst_data = np . empty ( out_shape , in_raster . dtype ) in_raster = ma . masked_array ( data = in_raster . filled ( ) , mask = in_raster . mask , fill_value = nodataval ) reproject ( in_raster , dst_data , src_transform = in_affine , src_crs = in_crs if in_crs else out_tile . crs , dst_transform = out_tile . affine , dst_crs = out_tile . crs , resampling = Resampling [ resampling ] ) return ma . MaskedArray ( dst_data , mask = dst_data == nodataval )
7616	def typecasted ( func ) : signature = inspect . signature ( func ) . parameters . items ( ) @ wraps ( func ) def wrapper ( * args , * * kwargs ) : args = list ( args ) new_args = [ ] new_kwargs = { } for _ , param in signature : converter = param . annotation if converter is inspect . _empty : converter = lambda a : a # do nothing if param . kind is param . POSITIONAL_OR_KEYWORD : if args : to_conv = args . pop ( 0 ) new_args . append ( converter ( to_conv ) ) elif param . kind is param . VAR_POSITIONAL : for a in args : new_args . append ( converter ( a ) ) else : for k , v in kwargs . items ( ) : nk , nv = converter ( k , v ) new_kwargs [ nk ] = nv return func ( * new_args , * * new_kwargs ) return wrapper
859	def isTemporal ( inferenceElement ) : if InferenceElement . __temporalInferenceElements is None : InferenceElement . __temporalInferenceElements = set ( [ InferenceElement . prediction ] ) return inferenceElement in InferenceElement . __temporalInferenceElements
5648	def _write_stop_to_stop_network_edges ( net , file_name , data = True , fmt = None ) : if fmt is None : fmt = "edg" if fmt == "edg" : if data : networkx . write_edgelist ( net , file_name , data = True ) else : networkx . write_edgelist ( net , file_name ) elif fmt == "csv" : with open ( file_name , 'w' ) as f : # writing out the header edge_iter = net . edges_iter ( data = True ) _ , _ , edg_data = next ( edge_iter ) edg_data_keys = list ( sorted ( edg_data . keys ( ) ) ) header = ";" . join ( [ "from_stop_I" , "to_stop_I" ] + edg_data_keys ) f . write ( header ) for from_node_I , to_node_I , data in net . edges_iter ( data = True ) : f . write ( "\n" ) values = [ str ( from_node_I ) , str ( to_node_I ) ] data_values = [ ] for key in edg_data_keys : if key == "route_I_counts" : route_I_counts_string = str ( data [ key ] ) . replace ( " " , "" ) [ 1 : - 1 ] data_values . append ( route_I_counts_string ) else : data_values . append ( str ( data [ key ] ) ) all_values = values + data_values f . write ( ";" . join ( all_values ) )
9185	def _reassemble_binder ( id , tree , metadata ) : binder = cnxepub . Binder ( id , metadata = metadata ) for item in tree [ 'contents' ] : node = _node_to_model ( item , parent = binder ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set_title_for_node ( node , item [ 'title' ] ) return binder
13498	def with_revision ( self , label , number ) : t = self . clone ( ) t . revision = Revision ( label , number ) return t
1249	def _is_action_available_left ( self , state ) : # True if any field is 0 (empty) on the left of a tile or two tiles can # be merged. for row in range ( 4 ) : has_empty = False for col in range ( 4 ) : has_empty |= state [ row , col ] == 0 if state [ row , col ] != 0 and has_empty : return True if ( state [ row , col ] != 0 and col > 0 and state [ row , col ] == state [ row , col - 1 ] ) : return True return False
11221	def get ( self , request , hash , filename ) : if _ws_download is True : return HttpResponseForbidden ( ) upload = Upload . objects . uploaded ( ) . get ( hash = hash , name = filename ) return FileResponse ( upload . file , content_type = upload . type )
8297	def hexDump ( bytes ) : for i in range ( len ( bytes ) ) : sys . stdout . write ( "%2x " % ( ord ( bytes [ i ] ) ) ) if ( i + 1 ) % 8 == 0 : print repr ( bytes [ i - 7 : i + 1 ] ) if ( len ( bytes ) % 8 != 0 ) : print string . rjust ( "" , 11 ) , repr ( bytes [ i - len ( bytes ) % 8 : i + 1 ] )
1392	def synch_topologies ( self ) : self . state_managers = statemanagerfactory . get_all_state_managers ( self . config . statemgr_config ) try : for state_manager in self . state_managers : state_manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print_exc ( ) sys . exit ( 1 ) # pylint: disable=deprecated-lambda def on_topologies_watch ( state_manager , topologies ) : """watch topologies""" Log . info ( "State watch triggered for topologies." ) Log . debug ( "Topologies: " + str ( topologies ) ) existingTopologies = self . getTopologiesForStateLocation ( state_manager . name ) existingTopNames = map ( lambda t : t . name , existingTopologies ) Log . debug ( "Existing topologies: " + str ( existingTopNames ) ) for name in existingTopNames : if name not in topologies : Log . info ( "Removing topology: %s in rootpath: %s" , name , state_manager . rootpath ) self . removeTopology ( name , state_manager . name ) for name in topologies : if name not in existingTopNames : self . addNewTopology ( state_manager , name ) for state_manager in self . state_managers : # The callback function with the bound # state_manager as first variable. onTopologiesWatch = partial ( on_topologies_watch , state_manager ) state_manager . get_topologies ( onTopologiesWatch )
857	def _updateSequenceInfo ( self , r ) : # Get current sequence id (if any) newSequence = False sequenceId = ( r [ self . _sequenceIdIdx ] if self . _sequenceIdIdx is not None else None ) if sequenceId != self . _currSequence : # verify that the new sequence didn't show up before if sequenceId in self . _sequences : raise Exception ( 'Broken sequence: %s, record: %s' % ( sequenceId , r ) ) # add the finished sequence to the set of sequence self . _sequences . add ( self . _currSequence ) self . _currSequence = sequenceId # Verify that the reset is consistent (if there is one) if self . _resetIdx : assert r [ self . _resetIdx ] == 1 newSequence = True else : # Check the reset reset = False if self . _resetIdx : reset = r [ self . _resetIdx ] if reset == 1 : newSequence = True # If it's still the same old sequence make sure the time flows forward if not newSequence : if self . _timeStampIdx and self . _currTime is not None : t = r [ self . _timeStampIdx ] if t < self . _currTime : raise Exception ( 'No time travel. Early timestamp for record: %s' % r ) if self . _timeStampIdx : self . _currTime = r [ self . _timeStampIdx ]
568	def _matchReportKeys ( reportKeyREs = [ ] , allReportKeys = [ ] ) : matchingReportKeys = [ ] # Extract the report items of interest for keyRE in reportKeyREs : # Find all keys that match this regular expression matchObj = re . compile ( keyRE ) found = False for keyName in allReportKeys : match = matchObj . match ( keyName ) if match and match . end ( ) == len ( keyName ) : matchingReportKeys . append ( keyName ) found = True if not found : raise _BadKeyError ( keyRE ) return matchingReportKeys
8403	def squish_infinite ( x , range = ( 0 , 1 ) ) : xtype = type ( x ) if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) x [ x == - np . inf ] = range [ 0 ] x [ x == np . inf ] = range [ 1 ] if not isinstance ( x , xtype ) : x = xtype ( x ) return x
6804	def init_raspbian_disk ( self , yes = 0 ) : self . assume_localhost ( ) yes = int ( yes ) device_question = 'SD card present at %s? ' % self . env . sd_device if not yes and not raw_input ( device_question ) . lower ( ) . startswith ( 'y' ) : return r = self . local_renderer r . local_if_missing ( fn = '{raspbian_image_zip}' , cmd = 'wget {raspbian_download_url} -O raspbian_lite_latest.zip' ) r . lenv . img_fn = r . local ( "unzip -l {raspbian_image_zip} | sed -n 4p | awk '{{print $4}}'" , capture = True ) or '$IMG_FN' r . local ( 'echo {img_fn}' ) r . local ( '[ ! -f {img_fn} ] && unzip {raspbian_image_zip} {img_fn} || true' ) r . lenv . img_fn = r . local ( 'readlink -f {img_fn}' , capture = True ) r . local ( 'echo {img_fn}' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir}" ] && umount {sd_media_mount_dir} || true' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir2}" ] && umount {sd_media_mount_dir2} || true' ) r . pc ( 'Writing the image onto the card.' ) r . sudo ( 'time dd bs=4M if={img_fn} of={sd_device}' ) # Flush all writes to disk. r . run ( 'sync' )
5017	def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : sys_msg = 'Not available' LOGGER . error ( ( 'Failed to send completion status call for enterprise enrollment %s' 'with payload %s' '\nError message: %s' '\nSystem message: %s' ) , learner_data . enterprise_course_enrollment_id , learner_data , str ( request_exception ) , sys_msg )
13733	def value_to_python_log_level ( config_val , evar ) : if not config_val : config_val = evar . default_val config_val = config_val . upper ( ) # noinspection PyProtectedMember return logging . _checkLevel ( config_val )
5885	def get_canonical_link ( self ) : if self . article . final_url : kwargs = { 'tag' : 'link' , 'attr' : 'rel' , 'value' : 'canonical' } meta = self . parser . getElementsByTag ( self . article . doc , * * kwargs ) if meta is not None and len ( meta ) > 0 : href = self . parser . getAttribute ( meta [ 0 ] , 'href' ) if href : href = href . strip ( ) o = urlparse ( href ) if not o . hostname : tmp = urlparse ( self . article . final_url ) domain = '%s://%s' % ( tmp . scheme , tmp . hostname ) href = urljoin ( domain , href ) return href return self . article . final_url
9647	def is_valid_in_template ( var , attr ) : # Remove private variables or methods if attr . startswith ( '_' ) : return False # Remove any attributes that raise an acception when read try : value = getattr ( var , attr ) except : return False if isroutine ( value ) : # Remove any routines that are flagged with 'alters_data' if getattr ( value , 'alters_data' , False ) : return False else : # Remove any routines that require arguments try : argspec = getargspec ( value ) num_args = len ( argspec . args ) if argspec . args else 0 num_defaults = len ( argspec . defaults ) if argspec . defaults else 0 if num_args - num_defaults > 1 : return False except TypeError : # C extension callables are routines, but getargspec fails with # a TypeError when these are passed. pass return True
5791	def extract_from_system ( cert_callback = None , callback_only_on_failure = False ) : certs_pointer_pointer = new ( CoreFoundation , 'CFArrayRef *' ) res = Security . SecTrustCopyAnchorCertificates ( certs_pointer_pointer ) handle_sec_error ( res ) certs_pointer = unwrap ( certs_pointer_pointer ) certificates = { } trust_info = { } all_purposes = '2.5.29.37.0' default_trust = ( set ( ) , set ( ) ) length = CoreFoundation . CFArrayGetCount ( certs_pointer ) for index in range ( 0 , length ) : cert_pointer = CoreFoundation . CFArrayGetValueAtIndex ( certs_pointer , index ) der_cert , cert_hash = _cert_details ( cert_pointer ) certificates [ cert_hash ] = der_cert CoreFoundation . CFRelease ( certs_pointer ) for domain in [ SecurityConst . kSecTrustSettingsDomainUser , SecurityConst . kSecTrustSettingsDomainAdmin ] : cert_trust_settings_pointer_pointer = new ( CoreFoundation , 'CFArrayRef *' ) res = Security . SecTrustSettingsCopyCertificates ( domain , cert_trust_settings_pointer_pointer ) if res == SecurityConst . errSecNoTrustSettings : continue handle_sec_error ( res ) cert_trust_settings_pointer = unwrap ( cert_trust_settings_pointer_pointer ) length = CoreFoundation . CFArrayGetCount ( cert_trust_settings_pointer ) for index in range ( 0 , length ) : cert_pointer = CoreFoundation . CFArrayGetValueAtIndex ( cert_trust_settings_pointer , index ) trust_settings_pointer_pointer = new ( CoreFoundation , 'CFArrayRef *' ) res = Security . SecTrustSettingsCopyTrustSettings ( cert_pointer , domain , trust_settings_pointer_pointer ) # In OS X 10.11, this value started being seen. From the comments in # the Security Framework Reference, the lack of any settings should # indicate "always trust this certificate" if res == SecurityConst . errSecItemNotFound : continue # If the trust settings for a certificate are invalid, we need to # assume the certificate should not be trusted if res == SecurityConst . errSecInvalidTrustSettings : der_cert , cert_hash = _cert_details ( cert_pointer ) if cert_hash in certificates : _cert_callback ( cert_callback , certificates [ cert_hash ] , 'invalid trust settings' ) del certificates [ cert_hash ] continue handle_sec_error ( res ) trust_settings_pointer = unwrap ( trust_settings_pointer_pointer ) trust_oids = set ( ) reject_oids = set ( ) settings_length = CoreFoundation . CFArrayGetCount ( trust_settings_pointer ) for settings_index in range ( 0 , settings_length ) : settings_dict_entry = CoreFoundation . CFArrayGetValueAtIndex ( trust_settings_pointer , settings_index ) settings_dict = CFHelpers . cf_dictionary_to_dict ( settings_dict_entry ) # No policy OID means the trust result is for all purposes policy_oid = settings_dict . get ( 'kSecTrustSettingsPolicy' , { } ) . get ( 'SecPolicyOid' , all_purposes ) # 0 = kSecTrustSettingsResultInvalid # 1 = kSecTrustSettingsResultTrustRoot # 2 = kSecTrustSettingsResultTrustAsRoot # 3 = kSecTrustSettingsResultDeny # 4 = kSecTrustSettingsResultUnspecified trust_result = settings_dict . get ( 'kSecTrustSettingsResult' , 1 ) should_trust = trust_result != 0 and trust_result != 3 if should_trust : trust_oids . add ( policy_oid ) else : reject_oids . add ( policy_oid ) der_cert , cert_hash = _cert_details ( cert_pointer ) # If rejected for all purposes, we don't export the certificate if all_purposes in reject_oids : if cert_hash in certificates : _cert_callback ( cert_callback , certificates [ cert_hash ] , 'explicitly distrusted' ) del certificates [ cert_hash ] else : if all_purposes in trust_oids : trust_oids = set ( [ all_purposes ] ) trust_info [ cert_hash ] = ( trust_oids , reject_oids ) CoreFoundation . CFRelease ( trust_settings_pointer ) CoreFoundation . CFRelease ( cert_trust_settings_pointer ) output = [ ] for cert_hash in certificates : if not callback_only_on_failure : _cert_callback ( cert_callback , certificates [ cert_hash ] , None ) cert_trust_info = trust_info . get ( cert_hash , default_trust ) output . append ( ( certificates [ cert_hash ] , cert_trust_info [ 0 ] , cert_trust_info [ 1 ] ) ) return output
3400	def fill ( self , iterations = 1 ) : used_reactions = list ( ) for i in range ( iterations ) : self . model . slim_optimize ( error_value = None , message = 'gapfilling optimization failed' ) solution = [ self . model . reactions . get_by_id ( ind . rxn_id ) for ind in self . indicators if ind . _get_primal ( ) > self . integer_threshold ] if not self . validate ( solution ) : raise RuntimeError ( 'failed to validate gapfilled model, ' 'try lowering the integer_threshold' ) used_reactions . append ( solution ) self . update_costs ( ) return used_reactions
9556	def _apply_record_checks ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for check , modulus in self . _record_checks : if i % modulus == 0 : # support sampling rdict = self . _as_dict ( r ) try : check ( rdict ) except RecordError as e : code = e . code if e . code is not None else RECORD_CHECK_FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD_CHECK_FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . __name__ , check . __doc__ ) if context is not None : p [ 'context' ] = context yield p
3911	def _on_event ( self , _ ) : # TODO: handle adding new conversations self . sort ( key = lambda conv_button : conv_button . last_modified , reverse = True )
11144	def to_repo_relative_path ( self , path , split = False ) : path = os . path . normpath ( path ) if path == '.' : path = '' path = path . split ( self . __path ) [ - 1 ] . strip ( os . sep ) if split : return path . split ( os . sep ) else : return path
11160	def trail_space ( self , filters = lambda p : p . ext == ".py" ) : # pragma: no cover self . assert_is_dir_and_exists ( ) for p in self . select_file ( filters ) : try : with open ( p . abspath , "rb" ) as f : lines = list ( ) for line in f : lines . append ( line . decode ( "utf-8" ) . rstrip ( ) ) with open ( p . abspath , "wb" ) as f : f . write ( "\n" . join ( lines ) . encode ( "utf-8" ) ) except Exception as e : # pragma: no cover raise e
5400	def _map ( self , event ) : description = event . get ( 'description' , '' ) start_time = google_base . parse_rfc3339_utc_string ( event . get ( 'timestamp' , '' ) ) for name , regex in _EVENT_REGEX_MAP . items ( ) : match = regex . match ( description ) if match : return { 'name' : name , 'start-time' : start_time } , match return { 'name' : description , 'start-time' : start_time } , None
13039	def overview ( ) : search = Credential . search ( ) search . aggs . bucket ( 'password_count' , 'terms' , field = 'secret' , order = { '_count' : 'desc' } , size = 20 ) . metric ( 'username_count' , 'cardinality' , field = 'username' ) . metric ( 'host_count' , 'cardinality' , field = 'host_ip' ) . metric ( 'top_hits' , 'top_hits' , docvalue_fields = [ 'username' ] , size = 100 ) response = search . execute ( ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( "Secret" , "Count" , "Hosts" , "Users" , "Usernames" ) ) print_line ( "-" * 100 ) for entry in response . aggregations . password_count . buckets : usernames = [ ] for creds in entry . top_hits : usernames . append ( creds . username [ 0 ] ) usernames = list ( set ( usernames ) ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( entry . key , entry . doc_count , entry . host_count . value , entry . username_count . value , usernames ) )
4817	def _dictfetchall ( self , cursor ) : columns = [ col [ 0 ] for col in cursor . description ] return [ dict ( zip ( columns , row ) ) for row in cursor . fetchall ( ) ]
7511	def select_samples ( dbsamples , samples , pidx = None ) : ## get index from dbsamples samples = [ i . name for i in samples ] if pidx : sidx = [ list ( dbsamples [ pidx ] ) . index ( i ) for i in samples ] else : sidx = [ list ( dbsamples ) . index ( i ) for i in samples ] sidx . sort ( ) return sidx
2073	def convert_input ( X ) : if not isinstance ( X , pd . DataFrame ) : if isinstance ( X , list ) : X = pd . DataFrame ( X ) elif isinstance ( X , ( np . generic , np . ndarray ) ) : X = pd . DataFrame ( X ) elif isinstance ( X , csr_matrix ) : X = pd . DataFrame ( X . todense ( ) ) elif isinstance ( X , pd . Series ) : X = pd . DataFrame ( X ) else : raise ValueError ( 'Unexpected input type: %s' % ( str ( type ( X ) ) ) ) X = X . apply ( lambda x : pd . to_numeric ( x , errors = 'ignore' ) ) return X
4072	def split_elements ( value ) : items = [ v . strip ( ) for v in value . split ( ',' ) ] if len ( items ) == 1 : items = value . split ( ) return items
6450	def pydocstyle_color ( score ) : # These are the score cutoffs for each color above. # I.e. score==0 -> brightgreen, down to 100 < score <= 200 -> orange score_cutoffs = ( 0 , 10 , 25 , 50 , 100 ) for i in range ( len ( score_cutoffs ) ) : if score <= score_cutoffs [ i ] : return BADGE_COLORS [ i ] # and score > 200 -> red return BADGE_COLORS [ - 1 ]
918	def warning ( self , msg , * args , * * kwargs ) : self . _baseLogger . warning ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )
3787	def TP_dependent_property_derivative_P ( self , T , P , order = 1 ) : sorted_valid_methods_P = self . select_valid_methods_P ( T , P ) for method in sorted_valid_methods_P : try : return self . calculate_derivative_P ( P , T , method , order ) except : pass return None
9241	def fetch_tags_dates ( self ) : if self . options . verbose : print ( "Fetching dates for {} tags..." . format ( len ( self . filtered_tags ) ) ) def worker ( tag ) : self . get_time_of_tag ( tag ) # Async fetching tags: threads = [ ] max_threads = 50 cnt = len ( self . filtered_tags ) for i in range ( 0 , ( cnt // max_threads ) + 1 ) : for j in range ( max_threads ) : idx = i * 50 + j if idx == cnt : break t = threading . Thread ( target = worker , args = ( self . filtered_tags [ idx ] , ) ) threads . append ( t ) t . start ( ) if self . options . verbose > 2 : print ( "." , end = "" ) for t in threads : t . join ( ) if self . options . verbose > 2 : print ( "." ) if self . options . verbose > 1 : print ( "Fetched dates for {} tags." . format ( len ( self . tag_times_dict ) ) )
12366	def update ( self , id , name ) : return super ( Keys , self ) . update ( id , name = name )
8136	def duplicate ( self ) : i = self . canvas . layer ( self . img . copy ( ) , self . x , self . y , self . name ) clone = self . canvas . layers [ i ] clone . alpha = self . alpha clone . blend = self . blend
3482	def _get_doc_from_filename ( filename ) : if isinstance ( filename , string_types ) : if ( "win" in platform ) and ( len ( filename ) < 260 ) and os . path . exists ( filename ) : # path (win) doc = libsbml . readSBMLFromFile ( filename ) # noqa: E501 type: libsbml.SBMLDocument elif ( "win" not in platform ) and os . path . exists ( filename ) : # path other doc = libsbml . readSBMLFromFile ( filename ) # noqa: E501 type: libsbml.SBMLDocument else : # string representation if "<sbml" not in filename : raise IOError ( "The file with 'filename' does not exist, " "or is not an SBML string. Provide the path to " "an existing SBML file or a valid SBML string " "representation: \n%s" , filename ) doc = libsbml . readSBMLFromString ( filename ) # noqa: E501 type: libsbml.SBMLDocument elif hasattr ( filename , "read" ) : # file handle doc = libsbml . readSBMLFromString ( filename . read ( ) ) # noqa: E501 type: libsbml.SBMLDocument else : raise CobraSBMLError ( "Input type '%s' for 'filename' is not supported." " Provide a path, SBML str, " "or file handle." , type ( filename ) ) return doc
8037	def get_summarizer ( self , name ) : if name in self . summarizers : pass elif name == 'lexrank' : from . import lexrank self . summarizers [ name ] = lexrank . summarize elif name == 'mcp' : from . import mcp_summ self . summarizers [ name ] = mcp_summ . summarize return self . summarizers [ name ]
10572	def walk_depth ( path , max_depth = float ( 'inf' ) ) : start_level = os . path . abspath ( path ) . count ( os . path . sep ) for dir_entry in os . walk ( path ) : root , dirs , _ = dir_entry level = root . count ( os . path . sep ) - start_level yield dir_entry if level >= max_depth : dirs [ : ] = [ ]
4363	def _spawn_heartbeat ( self ) : self . spawn ( self . _heartbeat ) self . spawn ( self . _heartbeat_timeout )
12927	def _parse_allele_data ( self ) : return [ Allele ( sequence = x ) for x in [ self . ref_allele ] + self . alt_alleles ]
640	def set ( cls , prop , value ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) cls . _properties [ prop ] = str ( value )
9604	def raise_for_status ( self ) : if not self . status : return error = find_exception_by_code ( self . status ) message = None screen = None stacktrace = None if isinstance ( self . value , str ) : message = self . value elif isinstance ( self . value , dict ) : message = self . value . get ( 'message' , None ) screen = self . value . get ( 'screen' , None ) stacktrace = self . value . get ( 'stacktrace' , None ) raise WebDriverException ( error , message , screen , stacktrace )
11996	def _set_options ( self , options ) : if not options : return self . options . copy ( ) options = options . copy ( ) if 'magic' in options : self . set_magic ( options [ 'magic' ] ) del ( options [ 'magic' ] ) if 'flags' in options : flags = options [ 'flags' ] del ( options [ 'flags' ] ) for key , value in flags . iteritems ( ) : if not isinstance ( value , bool ) : raise TypeError ( 'Invalid flag type for: %s' % key ) else : flags = self . options [ 'flags' ] if 'info' in options : del ( options [ 'info' ] ) for key , value in options . iteritems ( ) : if not isinstance ( value , int ) : raise TypeError ( 'Invalid option type for: %s' % key ) if value < 0 or value > 255 : raise ValueError ( 'Option value out of range for: %s' % key ) new_options = self . options . copy ( ) new_options . update ( options ) new_options [ 'flags' ] . update ( flags ) return new_options
12958	def _compat_rem_str_id_from_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _compat_get_str_key_for_index ( indexedField , val ) , pk )
512	def _updateMinDutyCyclesLocal ( self ) : for column in xrange ( self . _numColumns ) : neighborhood = self . _getColumnNeighborhood ( column ) maxActiveDuty = self . _activeDutyCycles [ neighborhood ] . max ( ) maxOverlapDuty = self . _overlapDutyCycles [ neighborhood ] . max ( ) self . _minOverlapDutyCycles [ column ] = ( maxOverlapDuty * self . _minPctOverlapDutyCycles )
103	def compute_paddings_for_aspect_ratio ( arr , aspect_ratio ) : do_assert ( arr . ndim in [ 2 , 3 ] ) do_assert ( aspect_ratio > 0 ) height , width = arr . shape [ 0 : 2 ] do_assert ( height > 0 ) aspect_ratio_current = width / height pad_top = 0 pad_right = 0 pad_bottom = 0 pad_left = 0 if aspect_ratio_current < aspect_ratio : # vertical image, height > width diff = ( aspect_ratio * height ) - width pad_right = int ( np . ceil ( diff / 2 ) ) pad_left = int ( np . floor ( diff / 2 ) ) elif aspect_ratio_current > aspect_ratio : # horizontal image, width > height diff = ( ( 1 / aspect_ratio ) * width ) - height pad_top = int ( np . floor ( diff / 2 ) ) pad_bottom = int ( np . ceil ( diff / 2 ) ) return pad_top , pad_right , pad_bottom , pad_left
6473	def human ( self , size , base = 1000 , units = ' kMGTZ' ) : sign = '+' if size >= 0 else '-' size = abs ( size ) if size < 1000 : return '%s%d' % ( sign , size ) for i , suffix in enumerate ( units ) : unit = 1000 ** ( i + 1 ) if size < unit : return ( '%s%.01f%s' % ( sign , size / float ( unit ) * base , suffix , ) ) . strip ( ) raise OverflowError
1619	def CleanseRawStrings ( raw_lines ) : delimiter = None lines_without_raw_strings = [ ] for line in raw_lines : if delimiter : # Inside a raw string, look for the end end = line . find ( delimiter ) if end >= 0 : # Found the end of the string, match leading space for this # line and resume copying the original lines, and also insert # a "" on the last line. leading_space = Match ( r'^(\s*)\S' , line ) line = leading_space . group ( 1 ) + '""' + line [ end + len ( delimiter ) : ] delimiter = None else : # Haven't found the end yet, append a blank line. line = '""' # Look for beginning of a raw string, and replace them with # empty strings. This is done in a loop to handle multiple raw # strings on the same line. while delimiter is None : # Look for beginning of a raw string. # See 2.14.15 [lex.string] for syntax. # # Once we have matched a raw string, we check the prefix of the # line to make sure that the line is not part of a single line # comment. It's done this way because we remove raw strings # before removing comments as opposed to removing comments # before removing raw strings. This is because there are some # cpplint checks that requires the comments to be preserved, but # we don't want to check comments that are inside raw strings. matched = Match ( r'^(.*?)\b(?:R|u8R|uR|UR|LR)"([^\s\\()]*)\((.*)$' , line ) if ( matched and not Match ( r'^([^\'"]|\'(\\.|[^\'])*\'|"(\\.|[^"])*")*//' , matched . group ( 1 ) ) ) : delimiter = ')' + matched . group ( 2 ) + '"' end = matched . group ( 3 ) . find ( delimiter ) if end >= 0 : # Raw string ended on same line line = ( matched . group ( 1 ) + '""' + matched . group ( 3 ) [ end + len ( delimiter ) : ] ) delimiter = None else : # Start of a multi-line raw string line = matched . group ( 1 ) + '""' else : break lines_without_raw_strings . append ( line ) # TODO(unknown): if delimiter is not None here, we might want to # emit a warning for unterminated string. return lines_without_raw_strings
13163	def serialize_text ( out , text ) : padding = len ( out ) # we need to add padding to all lines # except the first one add_padding = padding_adder ( padding ) text = add_padding ( text , ignore_first_line = True ) return out + text
6045	def padded_blurred_image_2d_from_padded_image_1d_and_psf ( self , padded_image_1d , psf ) : padded_model_image_1d = self . convolve_array_1d_with_psf ( padded_array_1d = padded_image_1d , psf = psf ) return self . scaled_array_2d_from_array_1d ( array_1d = padded_model_image_1d )
1778	def OR ( cpu , dest , src ) : res = dest . write ( dest . read ( ) | src . read ( ) ) # Defined Flags: szp cpu . _calculate_logic_flags ( dest . size , res )
6233	def points_random_3d ( count , range_x = ( - 10.0 , 10.0 ) , range_y = ( - 10.0 , 10.0 ) , range_z = ( - 10.0 , 10.0 ) , seed = None ) -> VAO : random . seed ( seed ) def gen ( ) : for _ in range ( count ) : yield random . uniform ( * range_x ) yield random . uniform ( * range_y ) yield random . uniform ( * range_z ) data = numpy . fromiter ( gen ( ) , count = count * 3 , dtype = numpy . float32 ) vao = VAO ( "geometry:points_random_3d" , mode = moderngl . POINTS ) vao . buffer ( data , '3f' , [ 'in_position' ] ) return vao
2516	def p_file_notice ( self , f_term , predicate ) : try : for _ , _ , notice in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_notice ( self . doc , six . text_type ( notice ) ) except CardinalityError : self . more_than_one_error ( 'file notice' )
11227	def _invalidates_cache ( f ) : def inner_func ( self , * args , * * kwargs ) : rv = f ( self , * args , * * kwargs ) self . _invalidate_cache ( ) return rv return inner_func
8986	def _instructions_changed ( self , change ) : if change . adds ( ) : for index , instruction in change . items ( ) : if isinstance ( instruction , dict ) : in_row = self . _parser . instruction_in_row ( self , instruction ) self . instructions [ index ] = in_row else : instruction . transfer_to_row ( self )
9582	def write_elements ( fd , mtp , data , is_name = False ) : fmt = etypes [ mtp ] [ 'fmt' ] if isinstance ( data , Sequence ) : if fmt == 's' or is_name : if isinstance ( data , bytes ) : if is_name and len ( data ) > 31 : raise ValueError ( 'Name "{}" is too long (max. 31 ' 'characters allowed)' . format ( data ) ) fmt = '{}s' . format ( len ( data ) ) data = ( data , ) else : fmt = '' . join ( '{}s' . format ( len ( s ) ) for s in data ) else : l = len ( data ) if l == 0 : # empty array fmt = '' if l > 1 : # more than one element to be written fmt = '{}{}' . format ( l , fmt ) else : data = ( data , ) num_bytes = struct . calcsize ( fmt ) if num_bytes <= 4 : # write SDE if num_bytes < 4 : # add pad bytes fmt += '{}x' . format ( 4 - num_bytes ) fd . write ( struct . pack ( 'hh' + fmt , etypes [ mtp ] [ 'n' ] , * chain ( [ num_bytes ] , data ) ) ) return # write tag: element type and number of bytes fd . write ( struct . pack ( 'b3xI' , etypes [ mtp ] [ 'n' ] , num_bytes ) ) # add pad bytes to fmt, if needed mod8 = num_bytes % 8 if mod8 : fmt += '{}x' . format ( 8 - mod8 ) # write data fd . write ( struct . pack ( fmt , * data ) )
1583	def generate ( ) : data_bytes = bytearray ( random . getrandbits ( 8 ) for i in range ( REQID . REQID_SIZE ) ) return REQID ( data_bytes )
11269	def substitute ( prev , * args , * * kw ) : template_obj = string . Template ( * args , * * kw ) for data in prev : yield template_obj . substitute ( data )
7290	def make_key ( * args , * * kwargs ) : sep = kwargs . get ( 'sep' , u"_" ) exclude_last_string = kwargs . get ( 'exclude_last_string' , False ) string_array = [ ] for arg in args : if isinstance ( arg , list ) : string_array . append ( six . text_type ( sep . join ( arg ) ) ) else : if exclude_last_string : new_key_array = arg . split ( sep ) [ : - 1 ] if len ( new_key_array ) > 0 : string_array . append ( make_key ( new_key_array ) ) else : string_array . append ( six . text_type ( arg ) ) return sep . join ( string_array )
3559	def find_service ( self , uuid ) : for service in self . list_services ( ) : if service . uuid == uuid : return service return None
4675	def removeAccount ( self , account ) : accounts = self . getAccounts ( ) for a in accounts : if a [ "name" ] == account : self . store . delete ( a [ "pubkey" ] )
4450	def info ( self ) : res = self . redis . execute_command ( 'FT.INFO' , self . index_name ) it = six . moves . map ( to_string , res ) return dict ( six . moves . zip ( it , it ) )
2236	def timestamp ( method = 'iso8601' ) : if method == 'iso8601' : # ISO 8601 # datetime.datetime.utcnow().isoformat() # datetime.datetime.now().isoformat() # utcnow tz_hour = time . timezone // 3600 utc_offset = str ( tz_hour ) if tz_hour < 0 else '+' + str ( tz_hour ) stamp = time . strftime ( '%Y-%m-%dT%H%M%S' ) + utc_offset return stamp else : raise ValueError ( 'only iso8601 is accepted for now' )
13289	def get_content_commit_date ( extensions , acceptance_callback = None , root_dir = '.' ) : logger = logging . getLogger ( __name__ ) def _null_callback ( _ ) : return True if acceptance_callback is None : acceptance_callback = _null_callback # Cache the repo object for each query root_dir = os . path . abspath ( root_dir ) repo = git . repo . base . Repo ( path = root_dir , search_parent_directories = True ) # Iterate over all files with all file extensions, looking for the # newest commit datetime. newest_datetime = None iters = [ _iter_filepaths_with_extension ( ext , root_dir = root_dir ) for ext in extensions ] for content_path in itertools . chain ( * iters ) : content_path = os . path . abspath ( os . path . join ( root_dir , content_path ) ) if acceptance_callback ( content_path ) : logger . debug ( 'Found content path %r' , content_path ) try : commit_datetime = read_git_commit_timestamp_for_file ( content_path , repo = repo ) logger . debug ( 'Commit timestamp of %r is %s' , content_path , commit_datetime ) except IOError : logger . warning ( 'Count not get commit for %r, skipping' , content_path ) continue if not newest_datetime or commit_datetime > newest_datetime : # Seed initial newest_datetime # or set a newer newest_datetime newest_datetime = commit_datetime logger . debug ( 'Newest commit timestamp is %s' , newest_datetime ) logger . debug ( 'Final commit timestamp is %s' , newest_datetime ) if newest_datetime is None : raise RuntimeError ( 'No content files found in {}' . format ( root_dir ) ) return newest_datetime
12900	def get_equalisers ( self ) : if not self . __equalisers : self . __equalisers = yield from self . handle_list ( self . API . get ( 'equalisers' ) ) return self . __equalisers
2124	def associate_always_node ( self , parent , child = None , * * kwargs ) : return self . _assoc_or_create ( 'always' , parent , child , * * kwargs )
9788	def bookmark ( ctx , username ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'username' ] = username
669	def createDataOutLink ( network , sensorRegionName , regionName ) : network . link ( sensorRegionName , regionName , "UniformLink" , "" , srcOutput = "dataOut" , destInput = "bottomUpIn" )
7002	def parallel_pf_lcdir ( lcdir , outdir , fileglob = None , recursive = True , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , pfmethods = ( 'gls' , 'pdm' , 'mav' , 'win' ) , pfkwargs = ( { } , { } , { } , { } ) , sigclip = 10.0 , getblssnr = False , nperiodworkers = NCPUS , ncontrolworkers = 1 , liststartindex = None , listmaxobjects = None , minobservations = 500 , excludeprocessed = True ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not fileglob : fileglob = dfileglob # now find the files LOGINFO ( 'searching for %s light curves in %s ...' % ( lcformat , lcdir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcdir , fileglob ) ) else : # use recursive glob for Python 3.5+ if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcdir , '**' , fileglob ) , recursive = True ) # otherwise, use os.walk and glob else : # use os.walk to go through the directories walker = os . walk ( lcdir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) # now that we have all the files, process them if matching and len ( matching ) > 0 : # this helps us process things in deterministic order when we distribute # processing over several machines matching = sorted ( matching ) LOGINFO ( 'found %s light curves, running pf...' % len ( matching ) ) return parallel_pf ( matching , outdir , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nperiodworkers = nperiodworkers , ncontrolworkers = ncontrolworkers , liststartindex = liststartindex , listmaxobjects = listmaxobjects , minobservations = minobservations , excludeprocessed = excludeprocessed ) else : LOGERROR ( 'no light curve files in %s format found in %s' % ( lcformat , lcdir ) ) return None
2981	def cmd_daemon ( opts ) : if opts . data_dir is None : raise BlockadeError ( "You must supply a data directory for the daemon" ) rest . start ( data_dir = opts . data_dir , port = opts . port , debug = opts . debug , host_exec = get_host_exec ( ) )
12586	def get_nifti1hdr_from_h5attrs ( h5attrs ) : hdr = nib . Nifti1Header ( ) for k in list ( h5attrs . keys ( ) ) : hdr [ str ( k ) ] = np . array ( h5attrs [ k ] ) return hdr
8061	def do_windowed ( self , line ) : self . bot . canvas . sink . trigger_fullscreen_action ( False ) print ( self . response_prompt , file = self . stdout )
12563	def get_rois_centers_of_mass ( vol ) : from scipy . ndimage . measurements import center_of_mass roisvals = np . unique ( vol ) roisvals = roisvals [ roisvals != 0 ] rois_centers = OrderedDict ( ) for r in roisvals : rois_centers [ r ] = center_of_mass ( vol , vol , r ) return rois_centers
12443	def require_http_allowed_method ( cls , request ) : allowed = cls . meta . http_allowed_methods if request . method not in allowed : # The specified method is not allowed for the resource # identified by the request URI. # RFC 2616 ยง 10.4.6 โ 405 Method Not Allowed raise http . exceptions . MethodNotAllowed ( allowed )
11672	def fit ( self , X , y = None , get_rhos = False ) : self . features_ = X = as_features ( X , stack = True , bare = True ) # if we're using a function that needs to pick its K vals itself, # then we need to set max_K here. when we transform(), might have to # re-do this :| Ks = self . _get_Ks ( ) _ , _ , _ , max_K , save_all_Ks , _ = _choose_funcs ( self . div_funcs , Ks , X . dim , X . n_pts , None , self . version ) if max_K >= X . n_pts . min ( ) : msg = "asked for K = {}, but there's a bag with only {} points" raise ValueError ( msg . format ( max_K , X . n_pts . min ( ) ) ) memory = self . memory if isinstance ( memory , string_types ) : memory = Memory ( cachedir = memory , verbose = 0 ) self . indices_ = id = memory . cache ( _build_indices ) ( X , self . _flann_args ( ) ) if get_rhos : self . rhos_ = _get_rhos ( X , id , Ks , max_K , save_all_Ks , self . min_dist ) elif hasattr ( self , 'rhos_' ) : del self . rhos_ return self
11097	def select_by_pattern_in_abspath ( self , pattern , recursive = True , case_sensitive = False ) : if case_sensitive : def filters ( p ) : return pattern in p . abspath else : pattern = pattern . lower ( ) def filters ( p ) : return pattern in p . abspath . lower ( ) return self . select_file ( filters , recursive )
7801	def handle_authorized ( self , event ) : stream = event . stream if not stream : return if not stream . initiator : return if stream . features is None : return element = stream . features . find ( SESSION_TAG ) if element is None : return logger . debug ( "Establishing IM session" ) stanza = Iq ( stanza_type = "set" ) payload = XMLPayload ( ElementTree . Element ( SESSION_TAG ) ) stanza . set_payload ( payload ) self . stanza_processor . set_response_handlers ( stanza , self . _session_success , self . _session_error ) stream . send ( stanza )
8742	def update_floatingip ( context , id , content ) : LOG . info ( 'update_floatingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) if 'port_id' not in content : raise n_exc . BadRequest ( resource = 'floating_ip' , msg = 'port_id is required.' ) requested_ports = [ ] if content . get ( 'port_id' ) : requested_ports = [ { 'port_id' : content . get ( 'port_id' ) } ] flip = _update_flip ( context , id , ip_types . FLOATING , requested_ports ) return v . _make_floating_ip_dict ( flip )
3246	def get_managed_policies ( group , * * conn ) : managed_policies = list_attached_group_managed_policies ( group [ 'GroupName' ] , * * conn ) managed_policy_names = [ ] for policy in managed_policies : managed_policy_names . append ( policy [ 'PolicyName' ] ) return managed_policy_names
13346	def run ( * args , * * kwargs ) : kwargs . setdefault ( 'env' , os . environ ) kwargs . setdefault ( 'shell' , True ) try : subprocess . check_call ( ' ' . join ( args ) , * * kwargs ) return True except subprocess . CalledProcessError : logger . debug ( 'Error running: {}' . format ( args ) ) return False
12630	def compose_err_msg ( msg , * * kwargs ) : updated_msg = msg for k , v in sorted ( kwargs . items ( ) ) : if isinstance ( v , _basestring ) : # print only str-like arguments updated_msg += "\n" + k + ": " + v return updated_msg
11484	def _upload_folder_as_item ( local_folder , parent_folder_id , reuse_existing = False ) : item_id = _create_or_reuse_item ( local_folder , parent_folder_id , reuse_existing ) subdir_contents = sorted ( os . listdir ( local_folder ) ) # for each file in the subdir, add it to the item filecount = len ( subdir_contents ) for ( ind , current_file ) in enumerate ( subdir_contents ) : file_path = os . path . join ( local_folder , current_file ) log_ind = '({0} of {1})' . format ( ind + 1 , filecount ) _create_bitstream ( file_path , current_file , item_id , log_ind ) for callback in session . item_upload_callbacks : callback ( session . communicator , session . token , item_id )
6429	def stem ( self , word ) : lowered = word . lower ( ) if lowered [ - 3 : ] == 'ies' and lowered [ - 4 : - 3 ] not in { 'e' , 'a' } : return word [ : - 3 ] + ( 'Y' if word [ - 1 : ] . isupper ( ) else 'y' ) if lowered [ - 2 : ] == 'es' and lowered [ - 3 : - 2 ] not in { 'a' , 'e' , 'o' } : return word [ : - 1 ] if lowered [ - 1 : ] == 's' and lowered [ - 2 : - 1 ] not in { 'u' , 's' } : return word [ : - 1 ] return word
164	def compute_pointwise_distances ( self , other , default = None ) : import shapely . geometry from . kps import Keypoint if isinstance ( other , Keypoint ) : other = shapely . geometry . Point ( ( other . x , other . y ) ) elif isinstance ( other , LineString ) : if len ( other . coords ) == 0 : return default elif len ( other . coords ) == 1 : other = shapely . geometry . Point ( other . coords [ 0 , : ] ) else : other = shapely . geometry . LineString ( other . coords ) elif isinstance ( other , tuple ) : assert len ( other ) == 2 other = shapely . geometry . Point ( other ) else : raise ValueError ( ( "Expected Keypoint or LineString or tuple (x,y), " + "got type %s." ) % ( type ( other ) , ) ) return [ shapely . geometry . Point ( point ) . distance ( other ) for point in self . coords ]
3670	def Wilson ( xs , params ) : gammas = [ ] cmps = range ( len ( xs ) ) for i in cmps : tot1 = log ( sum ( [ params [ i ] [ j ] * xs [ j ] for j in cmps ] ) ) tot2 = 0. for j in cmps : tot2 += params [ j ] [ i ] * xs [ j ] / sum ( [ params [ j ] [ k ] * xs [ k ] for k in cmps ] ) gamma = exp ( 1. - tot1 - tot2 ) gammas . append ( gamma ) return gammas
8170	def separation ( self , r = 10 ) : vx = vy = vz = 0 for b in self . boids : if b != self : if abs ( self . x - b . x ) < r : vx += ( self . x - b . x ) if abs ( self . y - b . y ) < r : vy += ( self . y - b . y ) if abs ( self . z - b . z ) < r : vz += ( self . z - b . z ) return vx , vy , vz
12382	def link ( self , request , response ) : from armet . resources . managed . request import read if self . slug is None : # Mass-LINK is not implemented. raise http . exceptions . NotImplemented ( ) # Get the current target. target = self . read ( ) # Collect all the passed link headers. links = self . _parse_link_headers ( request [ 'Link' ] ) # Pull targets for each represented link. for link in links : # Delegate to a connector. self . relate ( target , read ( self , link [ 'uri' ] ) ) # Build the response object. self . response . status = http . client . NO_CONTENT self . make_response ( )
5720	def _restore_path ( table ) : name = None splited = table . split ( '___' ) path = splited [ 0 ] if len ( splited ) == 2 : name = splited [ 1 ] path = path . replace ( '__' , os . path . sep ) path += '.csv' return path , name
7965	def start ( self , tag , attrs ) : if self . _level == 0 : self . _root = ElementTree . Element ( tag , attrs ) self . _handler . stream_start ( self . _root ) if self . _level < 2 : self . _builder = ElementTree . TreeBuilder ( ) self . _level += 1 return self . _builder . start ( tag , attrs )
10933	def check_completion ( self ) : terminate = False term_dict = self . get_termination_stats ( get_cos = self . costol is not None ) terminate |= np . all ( np . abs ( term_dict [ 'delta_vals' ] ) < self . paramtol ) terminate |= ( term_dict [ 'delta_err' ] < self . errtol ) terminate |= ( term_dict [ 'exp_err' ] < self . exptol ) terminate |= ( term_dict [ 'frac_err' ] < self . fractol ) if self . costol is not None : terminate |= ( curcos < term_dict [ 'model_cosine' ] ) return terminate
11385	def body ( self ) : if not hasattr ( self , '_body' ) : self . _body = inspect . getsource ( self . module ) return self . _body
13806	def validate_params ( required , optional , params ) : missing_fields = [ x for x in required if x not in params ] if missing_fields : field_strings = ", " . join ( missing_fields ) raise Exception ( "Missing fields: %s" % field_strings ) disallowed_fields = [ x for x in params if x not in optional and x not in required ] if disallowed_fields : field_strings = ", " . join ( disallowed_fields ) raise Exception ( "Disallowed fields: %s" % field_strings )
3421	def model_to_pymatbridge ( model , variable_name = "model" , matlab = None ) : if scipy_sparse is None : raise ImportError ( "`model_to_pymatbridge` requires scipy!" ) if matlab is None : # assumed to be running an IPython magic from IPython import get_ipython matlab = get_ipython ( ) . magics_manager . registry [ "MatlabMagics" ] . Matlab model_info = create_mat_dict ( model ) S = model_info [ "S" ] . todok ( ) model_info [ "S" ] = 0 temp_S_name = "cobra_pymatbridge_temp_" + uuid4 ( ) . hex _check ( matlab . set_variable ( variable_name , model_info ) ) _check ( matlab . set_variable ( temp_S_name , S ) ) _check ( matlab . run_code ( "%s.S = %s;" % ( variable_name , temp_S_name ) ) ) # all vectors need to be transposed for i in model_info . keys ( ) : if i == "S" : continue _check ( matlab . run_code ( "{0}.{1} = {0}.{1}';" . format ( variable_name , i ) ) ) _check ( matlab . run_code ( "clear %s;" % temp_S_name ) )
11222	def is_compressed_json_file ( abspath ) : abspath = abspath . lower ( ) fname , ext = os . path . splitext ( abspath ) if ext in [ ".json" , ".js" ] : is_compressed = False elif ext == ".gz" : is_compressed = True else : raise ValueError ( "'%s' is not a valid json file. " "extension has to be '.json' or '.js' for uncompressed, '.gz' " "for compressed." % abspath ) return is_compressed
4661	def broadcast ( self , tx = None ) : if tx : # If tx is provided, we broadcast the tx return self . transactionbuilder_class ( tx , blockchain_instance = self ) . broadcast ( ) else : return self . txbuffer . broadcast ( )
3075	def callback_view ( self ) : if 'error' in request . args : reason = request . args . get ( 'error_description' , request . args . get ( 'error' , '' ) ) reason = markupsafe . escape ( reason ) return ( 'Authorization failed: {0}' . format ( reason ) , httplib . BAD_REQUEST ) try : encoded_state = request . args [ 'state' ] server_csrf = session [ _CSRF_KEY ] code = request . args [ 'code' ] except KeyError : return 'Invalid request' , httplib . BAD_REQUEST try : state = json . loads ( encoded_state ) client_csrf = state [ 'csrf_token' ] return_url = state [ 'return_url' ] except ( ValueError , KeyError ) : return 'Invalid request state' , httplib . BAD_REQUEST if client_csrf != server_csrf : return 'Invalid request state' , httplib . BAD_REQUEST flow = _get_flow_for_token ( server_csrf ) if flow is None : return 'Invalid request state' , httplib . BAD_REQUEST # Exchange the auth code for credentials. try : credentials = flow . step2_exchange ( code ) except client . FlowExchangeError as exchange_error : current_app . logger . exception ( exchange_error ) content = 'An error occurred: {0}' . format ( exchange_error ) return content , httplib . BAD_REQUEST # Save the credentials to the storage. self . storage . put ( credentials ) if self . authorize_callback : self . authorize_callback ( credentials ) return redirect ( return_url )
10218	def plot_summary_axes ( graph : BELGraph , lax , rax , logx = True ) : ntc = count_functions ( graph ) etc = count_relations ( graph ) df = pd . DataFrame . from_dict ( dict ( ntc ) , orient = 'index' ) df_ec = pd . DataFrame . from_dict ( dict ( etc ) , orient = 'index' ) df . sort_values ( 0 , ascending = True ) . plot ( kind = 'barh' , logx = logx , ax = lax ) lax . set_title ( 'Number of nodes: {}' . format ( graph . number_of_nodes ( ) ) ) df_ec . sort_values ( 0 , ascending = True ) . plot ( kind = 'barh' , logx = logx , ax = rax ) rax . set_title ( 'Number of edges: {}' . format ( graph . number_of_edges ( ) ) )
12773	def _step_to_marker_frame ( self , frame_no , dt = None ) : # update the positions and velocities of the markers. self . markers . detach ( ) self . markers . reposition ( frame_no ) self . markers . attach ( frame_no ) # detect collisions. self . ode_space . collide ( None , self . on_collision ) # record the state of each skeleton body. states = self . skeleton . get_body_states ( ) self . skeleton . set_body_states ( states ) # yield the current simulation state to our caller. yield states # update the ode world. self . ode_world . step ( dt or self . dt ) # clear out contact joints to prepare for the next frame. self . ode_contactgroup . empty ( )
1960	def sys_rename ( self , oldnamep , newnamep ) : oldname = self . current . read_string ( oldnamep ) newname = self . current . read_string ( newnamep ) ret = 0 try : os . rename ( oldname , newname ) except OSError as e : ret = - e . errno return ret
6597	def receive ( self ) : ret = self . communicationChannel . receive_all ( ) self . nruns -= len ( ret ) if self . nruns > 0 : import logging logger = logging . getLogger ( __name__ ) logger . warning ( 'too few results received: {} results received, {} more expected' . format ( len ( ret ) , self . nruns ) ) elif self . nruns < 0 : import logging logger = logging . getLogger ( __name__ ) logger . warning ( 'too many results received: {} results received, {} too many' . format ( len ( ret ) , - self . nruns ) ) return ret
138	def to_shapely_line_string ( self , closed = False , interpolate = 0 ) : return _convert_points_to_shapely_line_string ( self . exterior , closed = closed , interpolate = interpolate )
12151	def html_single_basic ( self , abfID , launch = False , overwrite = False ) : if type ( abfID ) is str : abfID = [ abfID ] for thisABFid in cm . abfSort ( abfID ) : parentID = cm . parent ( self . groups , thisABFid ) saveAs = os . path . abspath ( "%s/%s_basic.html" % ( self . folder2 , parentID ) ) if overwrite is False and os . path . basename ( saveAs ) in self . files2 : continue filesByType = cm . filesByType ( self . groupFiles [ parentID ] ) html = "" html += '<div style="background-color: #DDDDDD;">' html += '<span class="title">summary of data from: %s</span></br>' % parentID html += '<code>%s</code>' % os . path . abspath ( self . folder1 + "/" + parentID + ".abf" ) html += '</div>' catOrder = [ "experiment" , "plot" , "tif" , "other" ] categories = cm . list_order_by ( filesByType . keys ( ) , catOrder ) for category in [ x for x in categories if len ( filesByType [ x ] ) ] : if category == 'experiment' : html += "<h3>Experimental Data:</h3>" elif category == 'plot' : html += "<h3>Intrinsic Properties:</h3>" elif category == 'tif' : html += "<h3>Micrographs:</h3>" elif category == 'other' : html += "<h3>Additional Files:</h3>" else : html += "<h3>????:</h3>" #html+="<hr>" #html+='<br>'*3 for fname in filesByType [ category ] : html += self . htmlFor ( fname ) html += '<br>' * 3 print ( "creating" , saveAs , '...' ) style . save ( html , saveAs , launch = launch )
13763	def _wrap_color ( self , code , text , format = None , style = None ) : color = None if code [ : 3 ] == self . bg . PREFIX : color = self . bg . COLORS . get ( code , None ) if not color : color = self . fg . COLORS . get ( code , None ) if not color : raise Exception ( 'Color code not found' ) if format and format not in self . formats : raise Exception ( 'Color format not found' ) fmt = "0;" if format == 'bold' : fmt = "1;" elif format == 'underline' : fmt = "4;" # Manage the format parts = color . split ( '[' ) color = '{0}[{1}{2}' . format ( parts [ 0 ] , fmt , parts [ 1 ] ) if self . has_colors and self . colors_enabled : # Set brightness st = '' if style : st = self . st . COLORS . get ( style , '' ) return "{0}{1}{2}{3}" . format ( st , color , text , self . st . COLORS [ 'reset_all' ] ) else : return text
4219	def get_preferred_collection ( self ) : bus = secretstorage . dbus_init ( ) try : if hasattr ( self , 'preferred_collection' ) : collection = secretstorage . Collection ( bus , self . preferred_collection ) else : collection = secretstorage . get_default_collection ( bus ) except exceptions . SecretStorageException as e : raise InitError ( "Failed to create the collection: %s." % e ) if collection . is_locked ( ) : collection . unlock ( ) if collection . is_locked ( ) : # User dismissed the prompt raise KeyringLocked ( "Failed to unlock the collection!" ) return collection
4882	def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . get_or_create ( name = 'SAP_USE_ENTERPRISE_ENROLLMENT_PAGE' , defaults = { 'active' : False } )
3833	async def modify_otr_status ( self , modify_otr_status_request ) : response = hangouts_pb2 . ModifyOTRStatusResponse ( ) await self . _pb_request ( 'conversations/modifyotrstatus' , modify_otr_status_request , response ) return response
7069	def get_varfeatures ( simbasedir , mindet = 1000 , nworkers = None ) : # get the info from the simbasedir with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) lcfpaths = siminfo [ 'lcfpath' ] varfeaturedir = os . path . join ( simbasedir , 'varfeatures' ) # get the column defs for the fakelcs timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] # get the column defs for the fakelcs timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] # register the fakelc pklc as a custom lcproc format # now we should be able to use all lcproc functions correctly fakelc_formatkey = 'fake-%s' % siminfo [ 'lcformat' ] lcproc . register_lcformat ( fakelc_formatkey , '*-fakelc.pkl' , timecols , magcols , errcols , 'astrobase.lcproc' , '_read_pklc' , magsarefluxes = siminfo [ 'magsarefluxes' ] ) # now we can use lcproc.parallel_varfeatures directly varinfo = lcvfeatures . parallel_varfeatures ( lcfpaths , varfeaturedir , lcformat = fakelc_formatkey , mindet = mindet , nworkers = nworkers ) with open ( os . path . join ( simbasedir , 'fakelc-varfeatures.pkl' ) , 'wb' ) as outfd : pickle . dump ( varinfo , outfd , pickle . HIGHEST_PROTOCOL ) return os . path . join ( simbasedir , 'fakelc-varfeatures.pkl' )
9689	def read_bin_particle_density ( self ) : config = [ ] # Send the command byte and sleep for 10 ms self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) # Read the config variables by sending 256 empty bytes for i in range ( 4 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) bpd = self . _calculate_float ( config ) return bpd
8968	def decryptMessage ( self , ciphertext , header , ad = None ) : if ad == None : ad = self . __ad # Try to decrypt the message using a previously saved message key plaintext = self . __decryptSavedMessage ( ciphertext , header , ad ) if plaintext : return plaintext # Check, whether the public key will trigger a dh ratchet step if self . triggersStep ( header . dh_pub ) : # Save missed message keys for the current receiving chain self . __saveMessageKeys ( header . pn ) # Perform the step self . step ( header . dh_pub ) # Save missed message keys for the current receiving chain self . __saveMessageKeys ( header . n ) # Finally decrypt the message and return the plaintext return self . __decrypt ( ciphertext , self . __skr . nextDecryptionKey ( ) , header , ad )
1434	def custom_serialized ( cls , serialized , is_java = True ) : if not isinstance ( serialized , bytes ) : raise TypeError ( "Argument to custom_serialized() must be " "a serialized Python class as bytes, given: %s" % str ( serialized ) ) if not is_java : return cls . CUSTOM ( gtype = topology_pb2 . Grouping . Value ( "CUSTOM" ) , python_serialized = serialized ) else : raise NotImplementedError ( "Custom grouping implemented in Java for Python topology" "is not yet supported." )
5800	def system_path ( ) : ca_path = None # Common CA cert paths paths = [ '/usr/lib/ssl/certs/ca-certificates.crt' , '/etc/ssl/certs/ca-certificates.crt' , '/etc/ssl/certs/ca-bundle.crt' , '/etc/pki/tls/certs/ca-bundle.crt' , '/etc/ssl/ca-bundle.pem' , '/usr/local/share/certs/ca-root-nss.crt' , '/etc/ssl/cert.pem' ] # First try SSL_CERT_FILE if 'SSL_CERT_FILE' in os . environ : paths . insert ( 0 , os . environ [ 'SSL_CERT_FILE' ] ) for path in paths : if os . path . exists ( path ) and os . path . getsize ( path ) > 0 : ca_path = path break if not ca_path : raise OSError ( pretty_message ( ''' Unable to find a CA certs bundle in common locations - try setting the SSL_CERT_FILE environmental variable ''' ) ) return ca_path
1237	def from_spec ( spec , kwargs = None ) : network = util . get_object ( obj = spec , default_object = LayeredNetwork , kwargs = kwargs ) assert isinstance ( network , Network ) return network
11617	def _brahmic ( data , scheme_map , * * kw ) : if scheme_map . from_scheme . name == northern . GURMUKHI : data = northern . GurmukhiScheme . replace_tippi ( text = data ) marks = scheme_map . marks virama = scheme_map . virama consonants = scheme_map . consonants non_marks_viraama = scheme_map . non_marks_viraama to_roman = scheme_map . to_scheme . is_roman max_key_length_from_scheme = scheme_map . max_key_length_from_scheme buf = [ ] i = 0 to_roman_had_consonant = found = False append = buf . append # logging.debug(pprint.pformat(scheme_map.consonants)) # We dont just translate each brAhmic character one after another in order to prefer concise transliterations when possible - for example เคเฅเค -> jn in optitrans rather than j~n. while i <= len ( data ) : # The longest token in the source scheme has length `max_key_length_from_scheme`. Iterate # over `data` while taking `max_key_length_from_scheme` characters at a time. If we don`t # find the character group in our scheme map, lop off a character and # try again. # # If we've finished reading through `data`, then `token` will be empty # and the loop below will be skipped. token = data [ i : i + max_key_length_from_scheme ] while token : if len ( token ) == 1 : if token in marks : append ( marks [ token ] ) found = True elif token in virama : append ( virama [ token ] ) found = True else : if to_roman_had_consonant : append ( 'a' ) append ( non_marks_viraama . get ( token , token ) ) found = True else : if token in non_marks_viraama : if to_roman_had_consonant : append ( 'a' ) append ( non_marks_viraama . get ( token ) ) found = True if found : to_roman_had_consonant = to_roman and token in consonants i += len ( token ) break else : token = token [ : - 1 ] # Continuing the outer while loop. # We've exhausted the token; this must be some other character. Due to # the implicit 'a', we must explicitly end any lingering consonants # before we can handle the current token. if not found : if to_roman_had_consonant : append ( next ( iter ( virama . values ( ) ) ) ) if i < len ( data ) : append ( data [ i ] ) to_roman_had_consonant = False i += 1 found = False if to_roman_had_consonant : append ( 'a' ) return '' . join ( buf )
5571	def execute ( mp ) : # Reading and writing data works like this: with mp . open ( "file1" , resampling = "bilinear" ) as raster_file : if raster_file . is_empty ( ) : return "empty" # This assures a transparent tile instead of a pink error tile # is returned when using mapchete serve. dem = raster_file . read ( ) return dem
10237	def _generate_citation_dict ( graph : BELGraph ) -> Mapping [ str , Mapping [ Tuple [ BaseEntity , BaseEntity ] , str ] ] : results = defaultdict ( lambda : defaultdict ( set ) ) for u , v , data in graph . edges ( data = True ) : if CITATION not in data : continue results [ data [ CITATION ] [ CITATION_TYPE ] ] [ u , v ] . add ( data [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) return dict ( results )
3504	def _add_cycle_free ( model , fluxes ) : model . objective = model . solver . interface . Objective ( Zero , direction = "min" , sloppy = True ) objective_vars = [ ] for rxn in model . reactions : flux = fluxes [ rxn . id ] if rxn . boundary : rxn . bounds = ( flux , flux ) continue if flux >= 0 : rxn . bounds = max ( 0 , rxn . lower_bound ) , max ( flux , rxn . upper_bound ) objective_vars . append ( rxn . forward_variable ) else : rxn . bounds = min ( flux , rxn . lower_bound ) , min ( 0 , rxn . upper_bound ) objective_vars . append ( rxn . reverse_variable ) model . objective . set_linear_coefficients ( { v : 1.0 for v in objective_vars } )
7135	def filter_dict ( d , exclude ) : ret = { } for key , value in d . items ( ) : if key not in exclude : ret . update ( { key : value } ) return ret
7584	def _get_binary ( self ) : ## check for binary backup_binaries = [ "raxmlHPC-PTHREADS" , "raxmlHPC-PTHREADS-SSE3" ] ## check user binary first, then backups for binary in [ self . params . binary ] + backup_binaries : proc = subprocess . Popen ( [ "which" , self . params . binary ] , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) . communicate ( ) ## update the binary if proc : self . params . binary = binary ## if none then raise error if not proc [ 0 ] : raise Exception ( BINARY_ERROR . format ( self . params . binary ) )
6595	def poll ( self ) : ret = self . communicationChannel . receive_finished ( ) self . nruns -= len ( ret ) return ret
7604	def get_player_chests ( self , tag : crtag , timeout : int = None ) : url = self . api . PLAYER + '/' + tag + '/upcomingchests' return self . _get_model ( url , timeout = timeout )
9198	def pop ( self , key , default = _sentinel ) : if default is not _sentinel : tup = self . _data . pop ( key . lower ( ) , default ) else : tup = self . _data . pop ( key . lower ( ) ) if tup is not default : return tup [ 1 ] else : return default
5866	def organization_data_is_valid ( organization_data ) : if organization_data is None : return False if 'id' in organization_data and not organization_data . get ( 'id' ) : return False if 'name' in organization_data and not organization_data . get ( 'name' ) : return False return True
1673	def PrintCategories ( ) : sys . stderr . write ( '' . join ( ' %s\n' % cat for cat in _ERROR_CATEGORIES ) ) sys . exit ( 0 )
4806	def _fmt_args_kwargs ( self , * some_args , * * some_kwargs ) : if some_args : out_args = str ( some_args ) . lstrip ( '(' ) . rstrip ( ',)' ) if some_kwargs : out_kwargs = ', ' . join ( [ str ( i ) . lstrip ( '(' ) . rstrip ( ')' ) . replace ( ', ' , ': ' ) for i in [ ( k , some_kwargs [ k ] ) for k in sorted ( some_kwargs . keys ( ) ) ] ] ) if some_args and some_kwargs : return out_args + ', ' + out_kwargs elif some_args : return out_args elif some_kwargs : return out_kwargs else : return ''
10622	def get_element_mass ( self , element ) : result = numpy . zeros ( 1 ) for compound in self . material . compounds : result += self . get_compound_mass ( compound ) * numpy . array ( stoich . element_mass_fractions ( compound , [ element ] ) ) return result [ 0 ]
11720	def get_directory ( self , path_to_directory , timeout = 30 , backoff = 0.4 , max_wait = 4 ) : response = None started_at = None time_elapsed = 0 i = 0 while time_elapsed < timeout : response = self . _get ( '{0}.zip' . format ( path_to_directory ) ) if response : break else : if started_at is None : started_at = time . time ( ) time . sleep ( min ( backoff * ( 2 ** i ) , max_wait ) ) i += 1 time_elapsed = time . time ( ) - started_at return response
6156	def stereo_FM ( x , fs = 2.4e6 , file_name = 'test.wav' ) : N1 = 10 b = signal . firwin ( 64 , 2 * 200e3 / float ( fs ) ) # Filter and decimate (should be polyphase) y = signal . lfilter ( b , 1 , x ) z = ss . downsample ( y , N1 ) # Apply complex baseband discriminator z_bb = discrim ( z ) # Work with the (3) stereo multiplex signals: # Begin by designing a lowpass filter for L+R and DSP demoded (L-R) # (fc = 12 KHz) b12 = signal . firwin ( 128 , 2 * 12e3 / ( float ( fs ) / N1 ) ) # The L + R term is at baseband, we just lowpass filter to remove # other terms above 12 kHz. y_lpr = signal . lfilter ( b12 , 1 , z_bb ) b19 = signal . firwin ( 128 , 2 * 1e3 * np . array ( [ 19 - 5 , 19 + 5 ] ) / ( float ( fs ) / N1 ) , pass_zero = False ) z_bb19 = signal . lfilter ( b19 , 1 , z_bb ) # Lock PLL to 19 kHz pilot # A type 2 loop with bandwidth Bn = 10 Hz and damping zeta = 0.707 # The VCO quiescent frequency is set to 19000 Hz. theta , phi_error = pilot_PLL ( z_bb19 , 19000 , fs / N1 , 2 , 10 , 0.707 ) # Coherently demodulate the L - R subcarrier at 38 kHz. # theta is the PLL output phase at 19 kHz, so to double multiply # by 2 and wrap with cos() or sin(). # First bandpass filter b38 = signal . firwin ( 128 , 2 * 1e3 * np . array ( [ 38 - 5 , 38 + 5 ] ) / ( float ( fs ) / N1 ) , pass_zero = False ) x_lmr = signal . lfilter ( b38 , 1 , z_bb ) # Coherently demodulate using the PLL output phase x_lmr = 2 * np . sqrt ( 2 ) * np . cos ( 2 * theta ) * x_lmr # Lowpass at 12 kHz to recover the desired DSB demod term y_lmr = signal . lfilter ( b12 , 1 , x_lmr ) # Matrix the y_lmr and y_lpr for form right and left channels: y_left = y_lpr + y_lmr y_right = y_lpr - y_lmr # Decimate by N2 (nominally 5) N2 = 5 fs2 = float ( fs ) / ( N1 * N2 ) # (nominally 48 ksps) y_left_DN2 = ss . downsample ( y_left , N2 ) y_right_DN2 = ss . downsample ( y_right , N2 ) # Deemphasize with 75 us time constant to 'undo' the preemphasis # applied at the transmitter in broadcast FM. # A 1-pole digital lowpass works well here. a_de = np . exp ( - 2.1 * 1e3 * 2 * np . pi / fs2 ) z_left = signal . lfilter ( [ 1 - a_de ] , [ 1 , - a_de ] , y_left_DN2 ) z_right = signal . lfilter ( [ 1 - a_de ] , [ 1 , - a_de ] , y_right_DN2 ) # Place left and righ channels as side-by-side columns in a 2D array z_out = np . hstack ( ( np . array ( [ z_left ] ) . T , ( np . array ( [ z_right ] ) . T ) ) ) ss . to_wav ( file_name , 48000 , z_out / 2 ) print ( 'Done!' ) #return z_bb, z_out return z_bb , theta , y_lpr , y_lmr , z_out
13571	def configure ( server = None , username = None , password = None , tid = None , auto = False ) : if not server and not username and not password and not tid : if Config . has ( ) : if not yn_prompt ( "Override old configuration" , False ) : return False reset_db ( ) if not server : while True : server = input ( "Server url [https://tmc.mooc.fi/mooc/]: " ) . strip ( ) if len ( server ) == 0 : server = "https://tmc.mooc.fi/mooc/" if not server . endswith ( '/' ) : server += '/' if not ( server . startswith ( "http://" ) or server . startswith ( "https://" ) ) : ret = custom_prompt ( "Server should start with http:// or https://\n" + "R: Retry, H: Assume http://, S: Assume https://" , [ "r" , "h" , "s" ] , "r" ) if ret == "r" : continue # Strip previous schema if "://" in server : server = server . split ( "://" ) [ 1 ] if ret == "h" : server = "http://" + server elif ret == "s" : server = "https://" + server break print ( "Using URL: '{0}'" . format ( server ) ) while True : if not username : username = input ( "Username: " ) if not password : password = getpass ( "Password: " ) # wow, such security token = b64encode ( bytes ( "{0}:{1}" . format ( username , password ) , encoding = 'utf-8' ) ) . decode ( "utf-8" ) try : api . configure ( url = server , token = token , test = True ) except APIError as e : print ( e ) if auto is False and yn_prompt ( "Retry authentication" ) : username = password = None continue return False break if tid : select ( course = True , tid = tid , auto = auto ) else : select ( course = True )
7445	def _step4func ( self , samples , force , ipyclient ) : if self . _headers : print ( "\n Step 4: Joint estimation of error rate and heterozygosity" ) ## Get sample objects from list of strings samples = _get_samples ( self , samples ) ## Check if all/none in the right state if not self . _samples_precheck ( samples , 4 , force ) : raise IPyradError ( FIRST_RUN_3 ) elif not force : ## skip if all are finished if all ( [ i . stats . state >= 4 for i in samples ] ) : print ( JOINTS_EXIST . format ( len ( samples ) ) ) return ## send to function assemble . jointestimate . run ( self , samples , force , ipyclient )
9417	def to_pointer ( cls , instance ) : return OctavePtr ( instance . _ref , instance . _name , instance . _address )
8665	def _prettify_list ( items ) : assert isinstance ( items , list ) keys_list = 'Available Keys:' for item in items : keys_list += '\n - {0}' . format ( item ) return keys_list
7334	async def _chunked_upload ( self , media , media_size , path = None , media_type = None , media_category = None , chunk_size = 2 ** 20 , * * params ) : if isinstance ( media , bytes ) : media = io . BytesIO ( media ) chunk = media . read ( chunk_size ) is_coro = asyncio . iscoroutine ( chunk ) if is_coro : chunk = await chunk if media_type is None : media_metadata = await utils . get_media_metadata ( chunk , path ) media_type , media_category = media_metadata elif media_category is None : media_category = utils . get_category ( media_type ) response = await self . upload . media . upload . post ( command = "INIT" , total_bytes = media_size , media_type = media_type , media_category = media_category , * * params ) media_id = response [ 'media_id' ] i = 0 while chunk : if is_coro : req = self . upload . media . upload . post ( command = "APPEND" , media_id = media_id , media = chunk , segment_index = i ) chunk , _ = await asyncio . gather ( media . read ( chunk_size ) , req ) else : await self . upload . media . upload . post ( command = "APPEND" , media_id = media_id , media = chunk , segment_index = i ) chunk = media . read ( chunk_size ) i += 1 status = await self . upload . media . upload . post ( command = "FINALIZE" , media_id = media_id ) if 'processing_info' in status : while status [ 'processing_info' ] . get ( 'state' ) != "succeeded" : processing_info = status [ 'processing_info' ] if processing_info . get ( 'state' ) == "failed" : error = processing_info . get ( 'error' , { } ) message = error . get ( 'message' , str ( status ) ) raise exceptions . MediaProcessingError ( data = status , message = message , * * params ) delay = processing_info [ 'check_after_secs' ] await asyncio . sleep ( delay ) status = await self . upload . media . upload . get ( command = "STATUS" , media_id = media_id , * * params ) return response
4829	def _get_results ( self , identity_provider , param_name , param_value , result_field_name ) : try : kwargs = { param_name : param_value } returned = self . client . providers ( identity_provider ) . users . get ( * * kwargs ) results = returned . get ( 'results' , [ ] ) except HttpNotFoundError : LOGGER . error ( 'username not found for third party provider={provider}, {querystring_param}={id}' . format ( provider = identity_provider , querystring_param = param_name , id = param_value ) ) results = [ ] for row in results : if row . get ( param_name ) == param_value : return row . get ( result_field_name ) return None
2284	def predict ( self , a , b , * * kwargs ) : binning_alg = kwargs . get ( 'bins' , 'fd' ) return metrics . adjusted_mutual_info_score ( bin_variable ( a , bins = binning_alg ) , bin_variable ( b , bins = binning_alg ) )
4180	def _coeff4 ( N , a0 , a1 , a2 , a3 ) : if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) N1 = N - 1. w = a0 - a1 * cos ( 2. * pi * n / N1 ) + a2 * cos ( 4. * pi * n / N1 ) - a3 * cos ( 6. * pi * n / N1 ) return w
5741	def result ( self , timeout = None ) : start = time . time ( ) while True : task = self . get_task ( ) if not task or task . status not in ( FINISHED , FAILED ) : if not timeout : continue elif time . time ( ) - start < timeout : continue else : raise TimeoutError ( ) if task . status == FAILED : raise task . result return task . result
7860	def _request_tls ( self ) : self . requested = True element = ElementTree . Element ( STARTTLS_TAG ) self . stream . write_element ( element )
13030	def make_server ( host , port , app = None , server_class = AsyncWsgiServer , handler_class = AsyncWsgiHandler , ws_handler_class = None , ws_path = '/ws' ) : handler_class . ws_handler_class = ws_handler_class handler_class . ws_path = ws_path httpd = server_class ( ( host , port ) , RequestHandlerClass = handler_class ) httpd . set_app ( app ) return httpd
8109	def search_images ( q , start = 0 , size = "" , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_IMAGES return GoogleSearch ( q , start , service , size , wait , asynchronous , cached )
9182	def validate_model ( cursor , model ) : # Check the license is one valid for publication. _validate_license ( model ) _validate_roles ( model ) # Other required metadata includes: title, summary required_metadata = ( 'title' , 'summary' , ) for metadata_key in required_metadata : if model . metadata . get ( metadata_key ) in [ None , '' , [ ] ] : raise exceptions . MissingRequiredMetadata ( metadata_key ) # Ensure that derived-from values are either None # or point at a live record in the archive. _validate_derived_from ( cursor , model ) # FIXME Valid language code? # Are the given 'subjects' _validate_subjects ( cursor , model )
10508	def log ( self , message , level = logging . DEBUG ) : if _ldtp_debug : print ( message ) self . logger . log ( level , str ( message ) ) return 1
10868	def j2 ( x ) : to_return = 2. / ( x + 1e-15 ) * j1 ( x ) - j0 ( x ) to_return [ x == 0 ] = 0 return to_return
6253	def available_templates ( value ) : templates = list_templates ( ) if value not in templates : raise ArgumentTypeError ( "Effect template '{}' does not exist.\n Available templates: {} " . format ( value , ", " . join ( templates ) ) ) return value
1289	def baseline_optimizer_arguments ( self , states , internals , reward ) : arguments = dict ( time = self . global_timestep , variables = self . baseline . get_variables ( ) , arguments = dict ( states = states , internals = internals , reward = reward , update = tf . constant ( value = True ) , ) , fn_reference = self . baseline . reference , fn_loss = self . fn_baseline_loss , # source_variables=self.network.get_variables() ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . baseline . get_variables ( ) return arguments
4475	def sample_clip_indices ( filename , n_samples , sr ) : with psf . SoundFile ( str ( filename ) , mode = 'r' ) as soundf : # Measure required length of fragment n_target = int ( np . ceil ( n_samples * soundf . samplerate / float ( sr ) ) ) # Raise exception if source is too short if len ( soundf ) < n_target : raise RuntimeError ( 'Source {} (length={})' . format ( filename , len ( soundf ) ) + ' must be at least the length of the input ({})' . format ( n_target ) ) # Draw a starting point at random in the background waveform start = np . random . randint ( 0 , 1 + len ( soundf ) - n_target ) stop = start + n_target return start , stop
5825	def _patch ( self , route , data , headers = None , failure_message = None ) : headers = self . _get_headers ( headers ) response_lambda = ( lambda : requests . patch ( self . _get_qualified_route ( route ) , headers = headers , data = data , verify = False , proxies = self . proxies ) ) response = check_for_rate_limiting ( response_lambda ( ) , response_lambda ) return self . _handle_response ( response , failure_message )
1806	def SETB ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
11112	def load_repository ( self , path ) : # try to open if path . strip ( ) in ( '' , '.' ) : path = os . getcwd ( ) repoPath = os . path . realpath ( os . path . expanduser ( path ) ) if not self . is_repository ( repoPath ) : raise Exception ( "no repository found in '%s'" % str ( repoPath ) ) # get pyrepinfo path repoInfoPath = os . path . join ( repoPath , ".pyrepinfo" ) try : fd = open ( repoInfoPath , 'rb' ) except Exception as e : raise Exception ( "unable to open repository file(%s)" % e ) # before doing anything try to lock repository # can't decorate with @acquire_lock because this will point to old repository # path or to current working directory which might not be the path anyways L = Locker ( filePath = None , lockPass = str ( uuid . uuid1 ( ) ) , lockPath = os . path . join ( repoPath , ".pyreplock" ) ) acquired , code = L . acquire_lock ( ) # check if acquired. if not acquired : warnings . warn ( "code %s. Unable to aquire the lock when calling 'load_repository'. You may try again!" % ( code , ) ) return try : # unpickle file try : repo = pickle . load ( fd ) except Exception as e : fd . close ( ) raise Exception ( "unable to pickle load repository (%s)" % e ) finally : fd . close ( ) # check if it's a PyrepInfo instance if not isinstance ( repo , Repository ) : raise Exception ( ".pyrepinfo in '%s' is not a repository instance." % s ) else : # update info path self . __reset_repository ( ) self . __update_repository ( repo ) self . __path = repoPath # set timestamp self . __state = self . _get_or_create_state ( ) except Exception as e : L . release_lock ( ) raise Exception ( e ) finally : L . release_lock ( ) # set loaded repo locker path to L because repository have been moved to another directory self . __locker = L # return return self
13726	def set_connection ( host = None , database = None , user = None , password = None ) : c . CONNECTION [ 'HOST' ] = host c . CONNECTION [ 'DATABASE' ] = database c . CONNECTION [ 'USER' ] = user c . CONNECTION [ 'PASSWORD' ] = password
20	def pretty_eta ( seconds_left ) : minutes_left = seconds_left // 60 seconds_left %= 60 hours_left = minutes_left // 60 minutes_left %= 60 days_left = hours_left // 24 hours_left %= 24 def helper ( cnt , name ) : return "{} {}{}" . format ( str ( cnt ) , name , ( 's' if cnt > 1 else '' ) ) if days_left > 0 : msg = helper ( days_left , 'day' ) if hours_left > 0 : msg += ' and ' + helper ( hours_left , 'hour' ) return msg if hours_left > 0 : msg = helper ( hours_left , 'hour' ) if minutes_left > 0 : msg += ' and ' + helper ( minutes_left , 'minute' ) return msg if minutes_left > 0 : return helper ( minutes_left , 'minute' ) return 'less than a minute'
13314	def remove ( self ) : self . run_hook ( 'preremove' ) utils . rmtree ( self . path ) self . run_hook ( 'postremove' )
6000	def pix_to_regular ( self ) : pix_to_regular = [ [ ] for _ in range ( self . pixels ) ] for regular_pixel , pix_pixel in enumerate ( self . regular_to_pix ) : pix_to_regular [ pix_pixel ] . append ( regular_pixel ) return pix_to_regular
9127	def create_all ( engine , checkfirst = True ) : Base . metadata . create_all ( bind = engine , checkfirst = checkfirst )
4339	def pitch ( self , n_semitones , quick = False ) : if not is_number ( n_semitones ) : raise ValueError ( "n_semitones must be a positive number" ) if n_semitones < - 12 or n_semitones > 12 : logger . warning ( "Using an extreme pitch shift. " "Quality of results will be poor" ) if not isinstance ( quick , bool ) : raise ValueError ( "quick must be a boolean." ) effect_args = [ 'pitch' ] if quick : effect_args . append ( '-q' ) effect_args . append ( '{:f}' . format ( n_semitones * 100. ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'pitch' ) return self
4034	def ib64_patched ( self , attrsD , contentparams ) : if attrsD . get ( "mode" , "" ) == "base64" : return 0 if self . contentparams [ "type" ] . startswith ( "text/" ) : return 0 if self . contentparams [ "type" ] . endswith ( "+xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/json" ) : return 0 return 0
5496	def from_file ( cls , file , * args , * * kwargs ) : try : cache = shelve . open ( file ) return cls ( file , cache , * args , * * kwargs ) except OSError as e : logger . debug ( "Loading {0} failed" . format ( file ) ) raise e
3155	def update ( self , list_id , segment_id , data ) : return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'segments' , segment_id ) , data = data )
13251	async def process_sphinx_technote ( session , github_api_token , ltd_product_data , mongo_collection = None ) : logger = logging . getLogger ( __name__ ) github_url = ltd_product_data [ 'doc_repo' ] github_url = normalize_repo_root_url ( github_url ) repo_slug = parse_repo_slug_from_url ( github_url ) try : metadata_yaml = await download_metadata_yaml ( session , github_url ) except aiohttp . ClientResponseError as err : # metadata.yaml not found; probably not a Sphinx technote logger . debug ( 'Tried to download %s\'s metadata.yaml, got status %d' , ltd_product_data [ 'slug' ] , err . code ) raise NotSphinxTechnoteError ( ) # Extract data from the GitHub API github_query = GitHubQuery . load ( 'technote_repo' ) github_variables = { "orgName" : repo_slug . owner , "repoName" : repo_slug . repo } github_data = await github_request ( session , github_api_token , query = github_query , variables = github_variables ) try : jsonld = reduce_technote_metadata ( github_url , metadata_yaml , github_data , ltd_product_data ) except Exception as exception : message = "Issue building JSON-LD for technote %s" logger . exception ( message , github_url , exception ) raise if mongo_collection is not None : await _upload_to_mongodb ( mongo_collection , jsonld ) logger . info ( 'Ingested technote %s into MongoDB' , github_url ) return jsonld
10911	def get_num_px_jtj ( s , nparams , decimate = 1 , max_mem = 1e9 , min_redundant = 20 ) : #1. Max for a given max_mem: px_mem = int ( max_mem // 8 // nparams ) #1 float = 8 bytes #2. num_pix for a given redundancy px_red = min_redundant * nparams #3. And # desired for decimation px_dec = s . residuals . size // decimate if px_red > px_mem : raise RuntimeError ( 'Insufficient max_mem for desired redundancy.' ) num_px = np . clip ( px_dec , px_red , px_mem ) . astype ( 'int' ) return num_px
6195	def _get_group_randomstate ( rs , seed , group ) : if rs is None : rs = np . random . RandomState ( seed = seed ) # Try to set the random state from the last session to preserve # a single random stream when simulating timestamps multiple times if 'last_random_state' in group . _v_attrs : rs . set_state ( group . _v_attrs [ 'last_random_state' ] ) print ( "INFO: Random state set to last saved state in '%s'." % group . _v_name ) else : print ( "INFO: Random state initialized from seed (%d)." % seed ) return rs
5867	def _inactivate_organization ( organization ) : [ _inactivate_organization_course_relationship ( record ) for record in internal . OrganizationCourse . objects . filter ( organization_id = organization . id , active = True ) ] [ _inactivate_record ( record ) for record in internal . Organization . objects . filter ( id = organization . id , active = True ) ]
2229	def hash_file ( fpath , blocksize = 65536 , stride = 1 , hasher = NoParam , hashlen = NoParam , base = NoParam ) : base = _rectify_base ( base ) hashlen = _rectify_hashlen ( hashlen ) hasher = _rectify_hasher ( hasher ) ( ) with open ( fpath , 'rb' ) as file : buf = file . read ( blocksize ) if stride > 1 : # skip blocks when stride is greater than 1 while len ( buf ) > 0 : hasher . update ( buf ) file . seek ( blocksize * ( stride - 1 ) , 1 ) buf = file . read ( blocksize ) else : # otherwise hash the entire file while len ( buf ) > 0 : hasher . update ( buf ) buf = file . read ( blocksize ) # Get the hashed representation text = _digest_hasher ( hasher , hashlen , base ) return text
4704	def memcopy ( self , stream , offset = 0 , length = float ( "inf" ) ) : data = [ ord ( i ) for i in list ( stream ) ] size = min ( length , len ( data ) , self . m_size ) buff = cast ( self . m_buf , POINTER ( c_uint8 ) ) for i in range ( size ) : buff [ offset + i ] = data [ i ]
10829	def accept ( self ) : with db . session . begin_nested ( ) : self . state = MembershipState . ACTIVE db . session . merge ( self )
1679	def CheckNextIncludeOrder ( self , header_type ) : error_message = ( 'Found %s after %s' % ( self . _TYPE_NAMES [ header_type ] , self . _SECTION_NAMES [ self . _section ] ) ) last_section = self . _section if header_type == _C_SYS_HEADER : if self . _section <= self . _C_SECTION : self . _section = self . _C_SECTION else : self . _last_header = '' return error_message elif header_type == _CPP_SYS_HEADER : if self . _section <= self . _CPP_SECTION : self . _section = self . _CPP_SECTION else : self . _last_header = '' return error_message elif header_type == _LIKELY_MY_HEADER : if self . _section <= self . _MY_H_SECTION : self . _section = self . _MY_H_SECTION else : self . _section = self . _OTHER_H_SECTION elif header_type == _POSSIBLE_MY_HEADER : if self . _section <= self . _MY_H_SECTION : self . _section = self . _MY_H_SECTION else : # This will always be the fallback because we're not sure # enough that the header is associated with this file. self . _section = self . _OTHER_H_SECTION else : assert header_type == _OTHER_HEADER self . _section = self . _OTHER_H_SECTION if last_section != self . _section : self . _last_header = '' return ''
1468	def process_tick ( self , tup ) : curtime = int ( time . time ( ) ) window_info = WindowContext ( curtime - self . window_duration , curtime ) tuple_batch = [ ] for ( tup , tm ) in self . current_tuples : tuple_batch . append ( tup ) self . processWindow ( window_info , tuple_batch ) self . _expire ( curtime )
8384	def draw ( self ) : if len ( self . q ) > 0 : self . update ( ) if self . delay == 0 : # Rounded rectangle in the given background color. p , h = self . textpath ( self . i ) f = self . fontsize self . _ctx . fill ( self . background ) self . _ctx . rect ( self . node . x + f * 1.0 , self . node . y + f * 0.5 , self . _w + f , h + f * 1.5 , roundness = 0.2 ) # Fade in/out the current text. alpha = 1.0 if self . fi < 5 : alpha = 0.2 * self . fi if self . fn - self . fi < 5 : alpha = 0.2 * ( self . fn - self . fi ) self . _ctx . fill ( self . text . r , self . text . g , self . text . b , self . text . a * alpha ) self . _ctx . translate ( self . node . x + f * 2.0 , self . node . y + f * 2.5 ) self . _ctx . drawpath ( p )
7270	def use ( plugin ) : log . debug ( 'register new plugin: {}' . format ( plugin ) ) if inspect . isfunction ( plugin ) : return plugin ( Engine ) if plugin and hasattr ( plugin , 'register' ) : return plugin . register ( Engine ) raise ValueError ( 'invalid plugin: must be a function or ' 'implement register() method' )
9349	def date ( past = False , min_delta = 0 , max_delta = 20 ) : timedelta = dt . timedelta ( days = _delta ( past , min_delta , max_delta ) ) return dt . date . today ( ) + timedelta
11447	def _login ( self , session , get_request = False ) : req = session . post ( self . _login_url , data = self . _logindata ) if _LOGIN_ERROR_STRING in req . text or req . status_code == 403 or req . url == _LOGIN_URL : err_mess = "YesssSMS: login failed, username or password wrong" if _LOGIN_LOCKED_MESS in req . text : err_mess += ", page says: " + _LOGIN_LOCKED_MESS_ENG self . _suspended = True raise self . AccountSuspendedError ( err_mess ) raise self . LoginError ( err_mess ) self . _suspended = False # login worked return ( session , req ) if get_request else session
10186	def _aggregations_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_aggs ) : for cfg in ep . load ( ) ( ) : if cfg [ 'aggregation_name' ] not in self . enabled_aggregations : continue elif cfg [ 'aggregation_name' ] in result : raise DuplicateAggregationError ( 'Duplicate aggregation {0} in entry point ' '{1}' . format ( cfg [ 'event_type' ] , ep . name ) ) # Update the default configuration with env/overlay config. cfg . update ( self . enabled_aggregations [ cfg [ 'aggregation_name' ] ] or { } ) result [ cfg [ 'aggregation_name' ] ] = cfg return result
5735	def _get_or_create_subscription ( self ) : topic_path = self . _get_topic_path ( ) subscription_name = '{}-{}-{}-worker' . format ( queue . PUBSUB_OBJECT_PREFIX , self . name , uuid4 ( ) . hex ) subscription_path = self . subscriber_client . subscription_path ( self . project , subscription_name ) try : self . subscriber_client . get_subscription ( subscription_path ) except google . cloud . exceptions . NotFound : logger . info ( "Creating worker subscription {}" . format ( subscription_name ) ) self . subscriber_client . create_subscription ( subscription_path , topic_path ) return subscription_path
783	def jobGetDemand ( self , ) : rows = self . _getMatchingRowsWithRetries ( self . _jobs , dict ( status = self . STATUS_RUNNING ) , [ self . _jobs . pubToDBNameDict [ f ] for f in self . _jobs . jobDemandNamedTuple . _fields ] ) return [ self . _jobs . jobDemandNamedTuple . _make ( r ) for r in rows ]
11456	def load_config ( from_key , to_key ) : from . mappings import mappings kbs = { } for key , values in mappings [ 'config' ] . iteritems ( ) : parse_dict = { } for mapping in values : # {'inspire': 'Norwegian', 'cds': 'nno'} # -> {"Norwegian": "nno"} parse_dict [ mapping [ from_key ] ] = mapping [ to_key ] kbs [ key ] = parse_dict return kbs
7117	def smush_config ( sources , initial = None ) : if initial is None : initial = { } config = DotDict ( initial ) for fn in sources : log . debug ( 'Merging %s' , fn ) mod = get_config_module ( fn ) config = mod . update ( config ) log . debug ( 'Current config:\n%s' , json . dumps ( config , indent = 4 , cls = LenientJSONEncoder ) ) return config
318	def calc_bootstrap ( func , returns , * args , * * kwargs ) : n_samples = kwargs . pop ( 'n_samples' , 1000 ) out = np . empty ( n_samples ) factor_returns = kwargs . pop ( 'factor_returns' , None ) for i in range ( n_samples ) : idx = np . random . randint ( len ( returns ) , size = len ( returns ) ) returns_i = returns . iloc [ idx ] . reset_index ( drop = True ) if factor_returns is not None : factor_returns_i = factor_returns . iloc [ idx ] . reset_index ( drop = True ) out [ i ] = func ( returns_i , factor_returns_i , * args , * * kwargs ) else : out [ i ] = func ( returns_i , * args , * * kwargs ) return out
5508	def worker ( f ) : @ functools . wraps ( f ) async def wrapper ( cls , connection , rest ) : try : await f ( cls , connection , rest ) except asyncio . CancelledError : connection . response ( "426" , "transfer aborted" ) connection . response ( "226" , "abort successful" ) return wrapper
9782	def delete ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) if not click . confirm ( "Are sure you want to delete build job `{}`" . format ( _build ) ) : click . echo ( 'Existing without deleting build job.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . build_job . delete_build ( user , project_name , _build ) # Purge caching BuildJobManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Build job `{}` was deleted successfully" . format ( _build ) )
12030	def comments_load ( self ) : self . comment_times , self . comment_sweeps , self . comment_tags = [ ] , [ ] , [ ] self . comments = 0 # will be >0 if comments exist self . comment_text = "" try : # this used to work self . comment_tags = list ( self . ABFblock . segments [ 0 ] . eventarrays [ 0 ] . annotations [ 'comments' ] ) self . comment_times = list ( self . ABFblock . segments [ 0 ] . eventarrays [ 0 ] . times / self . trace . itemsize ) self . comment_sweeps = list ( self . comment_times ) except : # now this notation seems to work for events in self . ABFblock . segments [ 0 ] . events : # this should only happen once actually self . comment_tags = events . annotations [ 'comments' ] . tolist ( ) self . comment_times = np . array ( events . times . magnitude / self . trace . itemsize ) self . comment_sweeps = self . comment_times / self . sweepInterval for i , c in enumerate ( self . comment_tags ) : self . comment_tags [ i ] = c . decode ( "utf-8" )
12762	def create_bodies ( self ) : self . bodies = { } for label in self . channels : body = self . world . create_body ( 'sphere' , name = 'marker:{}' . format ( label ) , radius = 0.02 ) body . is_kinematic = True body . color = 0.9 , 0.1 , 0.1 , 0.5 self . bodies [ label ] = body
8575	def update_nic ( self , datacenter_id , server_id , nic_id , * * kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
475	def sentence_to_token_ids ( sentence , vocabulary , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br"\d" ) ) : if tokenizer : words = tokenizer ( sentence ) else : words = basic_tokenizer ( sentence ) if not normalize_digits : return [ vocabulary . get ( w , UNK_ID ) for w in words ] # Normalize digits by 0 before looking words up in the vocabulary. return [ vocabulary . get ( re . sub ( _DIGIT_RE , b"0" , w ) , UNK_ID ) for w in words ]
12720	def lo_stops ( self , lo_stops ) : _set_params ( self . ode_obj , 'LoStop' , lo_stops , self . ADOF + self . LDOF )
530	def getInputNames ( self ) : inputs = self . getSpec ( ) . inputs return [ inputs . getByIndex ( i ) [ 0 ] for i in xrange ( inputs . getCount ( ) ) ]
8231	def speed ( self , framerate = None ) : if framerate is not None : self . _speed = framerate self . _dynamic = True else : return self . _speed
13027	def detect_os ( self , ip ) : process = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'checker.py' ) , str ( ip ) ] , stdout = subprocess . PIPE ) out = process . stdout . decode ( 'utf-8' ) . split ( '\n' ) system_os = '' for line in out : if line . startswith ( 'Target OS:' ) : system_os = line . replace ( 'Target OS: ' , '' ) break return system_os
10136	def _detect_or_validate ( self , val ) : if isinstance ( val , list ) or isinstance ( val , dict ) or isinstance ( val , SortableDict ) or isinstance ( val , Grid ) : # Project Haystack 3.0 type. self . _assert_version ( VER_3_0 )
596	def _buildArgs ( f , self = None , kwargs = { } ) : # Get the name, description, and default value for each argument argTuples = getArgumentDescriptions ( f ) argTuples = argTuples [ 1 : ] # Remove 'self' # Get the names of the parameters to our own constructor and remove them # Check for _originial_init first, because if LockAttributesMixin is used, # __init__'s signature will be just (self, *args, **kw), but # _original_init is created with the original signature #init = getattr(self, '_original_init', self.__init__) init = TMRegion . __init__ ourArgNames = [ t [ 0 ] for t in getArgumentDescriptions ( init ) ] # Also remove a few other names that aren't in our constructor but are # computed automatically (e.g. numberOfCols for the TM) ourArgNames += [ 'numberOfCols' , # TM ] for argTuple in argTuples [ : ] : if argTuple [ 0 ] in ourArgNames : argTuples . remove ( argTuple ) # Build the dictionary of arguments if self : for argTuple in argTuples : argName = argTuple [ 0 ] if argName in kwargs : # Argument was provided argValue = kwargs . pop ( argName ) else : # Argument was not provided; use the default value if there is one, and # raise an exception otherwise if len ( argTuple ) == 2 : # No default value raise TypeError ( "Must provide '%s'" % argName ) argValue = argTuple [ 2 ] # Set as an instance variable if 'self' was passed in setattr ( self , argName , argValue ) return argTuples
6067	def convergence_from_grid ( self , grid ) : surface_density_grid = np . zeros ( grid . shape [ 0 ] ) grid_eta = self . grid_to_elliptical_radii ( grid ) for i in range ( grid . shape [ 0 ] ) : surface_density_grid [ i ] = self . convergence_func ( grid_eta [ i ] ) return surface_density_grid
9513	def orfs ( self , frame = 0 , revcomp = False ) : assert frame in [ 0 , 1 , 2 ] if revcomp : self . revcomp ( ) aa_seq = self . translate ( frame = frame ) . seq . rstrip ( 'X' ) if revcomp : self . revcomp ( ) orfs = _orfs_from_aa_seq ( aa_seq ) for i in range ( len ( orfs ) ) : if revcomp : start = len ( self ) - ( orfs [ i ] . end * 3 + 3 ) - frame end = len ( self ) - ( orfs [ i ] . start * 3 ) - 1 - frame else : start = orfs [ i ] . start * 3 + frame end = orfs [ i ] . end * 3 + 2 + frame orfs [ i ] = intervals . Interval ( start , end ) return orfs
6368	def precision_gain ( self ) : if self . population ( ) == 0 : return float ( 'NaN' ) random_precision = self . cond_pos_pop ( ) / self . population ( ) return self . precision ( ) / random_precision
845	def _calcDistance ( self , inputPattern , distanceNorm = None ) : if distanceNorm is None : distanceNorm = self . distanceNorm # Sparse memory if self . useSparseMemory : if self . _protoSizes is None : self . _protoSizes = self . _Memory . rowSums ( ) overlapsWithProtos = self . _Memory . rightVecSumAtNZ ( inputPattern ) inputPatternSum = inputPattern . sum ( ) if self . distanceMethod == "rawOverlap" : dist = inputPattern . sum ( ) - overlapsWithProtos elif self . distanceMethod == "pctOverlapOfInput" : dist = inputPatternSum - overlapsWithProtos if inputPatternSum > 0 : dist /= inputPatternSum elif self . distanceMethod == "pctOverlapOfProto" : overlapsWithProtos /= self . _protoSizes dist = 1.0 - overlapsWithProtos elif self . distanceMethod == "pctOverlapOfLarger" : maxVal = numpy . maximum ( self . _protoSizes , inputPatternSum ) if maxVal . all ( ) > 0 : overlapsWithProtos /= maxVal dist = 1.0 - overlapsWithProtos elif self . distanceMethod == "norm" : dist = self . _Memory . vecLpDist ( self . distanceNorm , inputPattern ) distMax = dist . max ( ) if distMax > 0 : dist /= distMax else : raise RuntimeError ( "Unimplemented distance method %s" % self . distanceMethod ) # Dense memory else : if self . distanceMethod == "norm" : dist = numpy . power ( numpy . abs ( self . _M - inputPattern ) , self . distanceNorm ) dist = dist . sum ( 1 ) dist = numpy . power ( dist , 1.0 / self . distanceNorm ) dist /= dist . max ( ) else : raise RuntimeError ( "Not implemented yet for dense storage...." ) return dist
8074	def image ( self , path , x , y , width = None , height = None , alpha = 1.0 , data = None , draw = True , * * kwargs ) : return self . Image ( path , x , y , width , height , alpha , data , * * kwargs )
3073	def _load_config ( self , client_secrets_file , client_id , client_secret ) : if client_id and client_secret : self . client_id , self . client_secret = client_id , client_secret return if client_secrets_file : self . _load_client_secrets ( client_secrets_file ) return if 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE' in self . app . config : self . _load_client_secrets ( self . app . config [ 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE' ] ) return try : self . client_id , self . client_secret = ( self . app . config [ 'GOOGLE_OAUTH2_CLIENT_ID' ] , self . app . config [ 'GOOGLE_OAUTH2_CLIENT_SECRET' ] ) except KeyError : raise ValueError ( 'OAuth2 configuration could not be found. Either specify the ' 'client_secrets_file or client_id and client_secret or set ' 'the app configuration variables ' 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE or ' 'GOOGLE_OAUTH2_CLIENT_ID and GOOGLE_OAUTH2_CLIENT_SECRET.' )
6796	def manage_async ( self , command = '' , name = 'process' , site = ALL , exclude_sites = '' , end_message = '' , recipients = '' ) : exclude_sites = exclude_sites . split ( ':' ) r = self . local_renderer for _site , site_data in self . iter_sites ( site = site , no_secure = True ) : if _site in exclude_sites : continue r . env . SITE = _site r . env . command = command r . env . end_email_command = '' r . env . recipients = recipients or '' r . env . end_email_command = '' if end_message : end_message = end_message + ' for ' + _site end_message = end_message . replace ( ' ' , '_' ) r . env . end_message = end_message r . env . end_email_command = r . format ( '{manage_cmd} send_mail --subject={end_message} --recipients={recipients}' ) r . env . name = name . format ( * * r . genv ) r . run ( 'screen -dmS {name} bash -c "export SITE={SITE}; ' 'export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} {command} --traceback; {end_email_command}"; sleep 3;' )
5199	def process_point_value ( cls , command_type , command , index , op_type ) : _log . debug ( 'Processing received point value for index {}: {}' . format ( index , command ) )
7839	def set_action ( self , action ) : if action is None : if self . xmlnode . hasProp ( "action" ) : self . xmlnode . unsetProp ( "action" ) return if action not in ( "remove" , "update" ) : raise ValueError ( "Action must be 'update' or 'remove'" ) action = unicode ( action ) self . xmlnode . setProp ( "action" , action . encode ( "utf-8" ) )
1594	def choose_tasks ( self , stream_id , values ) : if stream_id not in self . targets : return [ ] ret = [ ] for target in self . targets [ stream_id ] : ret . extend ( target . choose_tasks ( values ) ) return ret
7279	def play ( self ) : if not self . is_playing ( ) : self . play_pause ( ) self . _is_playing = True self . playEvent ( self )
7546	def make_chunks ( data , samples , lbview ) : ## first progress bar start = time . time ( ) printstr = " chunking clusters | {} | s5 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 10 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) ## send off samples to be chunked lasyncs = { } for sample in samples : lasyncs [ sample . name ] = lbview . apply ( chunk_clusters , * ( data , sample ) ) ## block until finished while 1 : ready = [ i . ready ( ) for i in lasyncs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break ## check for failures for sample in samples : if not lasyncs [ sample . name ] . successful ( ) : LOGGER . error ( " sample %s failed: %s" , sample . name , lasyncs [ sample . name ] . exception ( ) ) return lasyncs
2458	def set_pkg_license_comment ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_license_comment_set : self . package_license_comment_set = True if validations . validate_pkg_lics_comment ( text ) : doc . package . license_comment = str_from_text ( text ) return True else : raise SPDXValueError ( 'Package::LicenseComment' ) else : raise CardinalityError ( 'Package::LicenseComment' )
8916	def _retrieve_certificate ( self , access_token , timeout = 3 ) : logger . debug ( "Retrieve certificate with token." ) # Generate a new key pair key_pair = crypto . PKey ( ) key_pair . generate_key ( crypto . TYPE_RSA , 2048 ) private_key = crypto . dump_privatekey ( crypto . FILETYPE_PEM , key_pair ) . decode ( "utf-8" ) # Generate a certificate request using that key-pair cert_request = crypto . X509Req ( ) # Create public key object cert_request . set_pubkey ( key_pair ) # Add the public key to the request cert_request . sign ( key_pair , 'md5' ) der_cert_req = crypto . dump_certificate_request ( crypto . FILETYPE_ASN1 , cert_request ) encoded_cert_req = base64 . b64encode ( der_cert_req ) # Build the OAuth session object token = { 'access_token' : access_token , 'token_type' : 'Bearer' } client = OAuth2Session ( token = token ) response = client . post ( self . certificate_url , data = { 'certificate_request' : encoded_cert_req } , verify = False , timeout = timeout , ) if response . ok : content = "{} {}" . format ( response . text , private_key ) with open ( self . esgf_credentials , 'w' ) as fh : fh . write ( content ) logger . debug ( 'Fetched certificate successfully.' ) else : msg = "Could not get certificate: {} {}" . format ( response . status_code , response . reason ) raise Exception ( msg ) return True
6530	def purge_config_cache ( location = None ) : cache_path = get_cache_path ( location ) if location : os . remove ( cache_path ) else : shutil . rmtree ( cache_path )
10207	def file_download_event_builder ( event , sender_app , obj = None , * * kwargs ) : event . update ( dict ( # When: timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , # What: bucket_id = str ( obj . bucket_id ) , file_id = str ( obj . file_id ) , file_key = obj . key , size = obj . file . size , referrer = request . referrer , # Who: * * get_user ( ) ) ) return event
9523	def scaffolds_to_contigs ( infile , outfile , number_contigs = False ) : seq_reader = sequences . file_reader ( infile ) fout = utils . open_file_write ( outfile ) for seq in seq_reader : contigs = seq . contig_coords ( ) counter = 1 for contig in contigs : if number_contigs : name = seq . id + '.' + str ( counter ) counter += 1 else : name = '.' . join ( [ seq . id , str ( contig . start + 1 ) , str ( contig . end + 1 ) ] ) print ( sequences . Fasta ( name , seq [ contig . start : contig . end + 1 ] ) , file = fout ) utils . close ( fout )
12874	def satisfies ( guard ) : i = peek ( ) if ( i is EndOfFile ) or ( not guard ( i ) ) : fail ( [ "<satisfies predicate " + _fun_to_str ( guard ) + ">" ] ) next ( ) return i
6939	def checkplot_infokey_worker ( task ) : cpf , keys = task cpd = _read_checkplot_picklefile ( cpf ) resultkeys = [ ] for k in keys : try : resultkeys . append ( _dict_get ( cpd , k ) ) except Exception as e : resultkeys . append ( np . nan ) return resultkeys
10656	def count_with_multiplier ( groups , multiplier ) : counts = collections . defaultdict ( float ) for group in groups : for element , count in group . count ( ) . items ( ) : counts [ element ] += count * multiplier return counts
11713	def instance ( self , counter = None ) : if not counter : history = self . history ( ) if not history : return history else : return Response . _from_json ( history [ 'pipelines' ] [ 0 ] ) return self . _get ( '/instance/{counter:d}' . format ( counter = counter ) )
2759	def get_all_load_balancers ( self ) : data = self . get_data ( "load_balancers" ) load_balancers = list ( ) for jsoned in data [ 'load_balancers' ] : load_balancer = LoadBalancer ( * * jsoned ) load_balancer . token = self . token load_balancer . health_check = HealthCheck ( * * jsoned [ 'health_check' ] ) load_balancer . sticky_sessions = StickySesions ( * * jsoned [ 'sticky_sessions' ] ) forwarding_rules = list ( ) for rule in jsoned [ 'forwarding_rules' ] : forwarding_rules . append ( ForwardingRule ( * * rule ) ) load_balancer . forwarding_rules = forwarding_rules load_balancers . append ( load_balancer ) return load_balancers
6759	def get_package_list ( self ) : os_version = self . os_version # OS(type=LINUX, distro=UBUNTU, release='14.04') self . vprint ( 'os_version:' , os_version ) # Lookup legacy package list. # OS: [package1, package2, ...], req_packages1 = self . required_system_packages if req_packages1 : deprecation ( 'The required_system_packages attribute is deprecated, ' 'use the packager_system_packages property instead.' ) # Lookup new package list. # OS: [package1, package2, ...], req_packages2 = self . packager_system_packages patterns = [ ( os_version . type , os_version . distro , os_version . release ) , ( os_version . distro , os_version . release ) , ( os_version . type , os_version . distro ) , ( os_version . distro , ) , os_version . distro , ] self . vprint ( 'req_packages1:' , req_packages1 ) self . vprint ( 'req_packages2:' , req_packages2 ) package_list = None found = False for pattern in patterns : self . vprint ( 'pattern:' , pattern ) for req_packages in ( req_packages1 , req_packages2 ) : if pattern in req_packages : package_list = req_packages [ pattern ] found = True break if not found : print ( 'Warning: No operating system pattern found for %s' % ( os_version , ) ) self . vprint ( 'package_list:' , package_list ) return package_list
11575	def encoder_data ( self , data ) : prev_val = self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) # set value so that it shows positive and negative values if val > 8192 : val -= 16384 pin = data [ 0 ] with self . pymata . data_lock : self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if prev_val != val : callback = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_CALLBACK ] if callback is not None : callback ( [ self . pymata . ENCODER , pin , self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] ] )
2003	def function_call ( type_spec , * args ) : m = re . match ( r"(?P<name>[a-zA-Z_][a-zA-Z_0-9]*)(?P<type>\(.*\))" , type_spec ) if not m : raise EthereumError ( "Function signature expected" ) ABI . _check_and_warn_num_args ( type_spec , * args ) result = ABI . function_selector ( type_spec ) # Funcid result += ABI . serialize ( m . group ( 'type' ) , * args ) return result
13839	def ConsumeFloat ( self ) : try : result = ParseFloat ( self . token ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
4911	def ensure_data_exists ( self , request , data , error_message = None ) : if not data : error_message = ( error_message or "Unable to fetch API response from endpoint '{}'." . format ( request . get_full_path ( ) ) ) LOGGER . error ( error_message ) raise NotFound ( error_message )
6079	def intensities_of_galaxies_from_grid ( grid , galaxies ) : if galaxies : return sum ( map ( lambda g : g . intensities_from_grid ( grid ) , galaxies ) ) else : return np . full ( ( grid . shape [ 0 ] ) , 0.0 )
2735	def load ( self ) : data = self . get_data ( 'floating_ips/%s' % self . ip , type = GET ) floating_ip = data [ 'floating_ip' ] # Setting the attribute values for attr in floating_ip . keys ( ) : setattr ( self , attr , floating_ip [ attr ] ) return self
13018	def configure ( self , argv = None ) : self . _setupOptions ( ) self . _parseOptions ( argv ) self . _setupLogging ( ) self . _setupModel ( ) self . dbsession . commit ( ) return self
12497	def as_ndarray ( arr , copy = False , dtype = None , order = 'K' ) : if order not in ( 'C' , 'F' , 'A' , 'K' , None ) : raise ValueError ( "Invalid value for 'order': {}" . format ( str ( order ) ) ) if isinstance ( arr , np . memmap ) : if dtype is None : if order in ( 'K' , 'A' , None ) : ret = np . array ( np . asarray ( arr ) , copy = True ) else : ret = np . array ( np . asarray ( arr ) , copy = True , order = order ) else : if order in ( 'K' , 'A' , None ) : # always copy (even when dtype does not change) ret = np . asarray ( arr ) . astype ( dtype ) else : # load data from disk without changing order # Changing order while reading through a memmap is incredibly # inefficient. ret = _asarray ( np . array ( arr , copy = True ) , dtype = dtype , order = order ) elif isinstance ( arr , np . ndarray ) : ret = _asarray ( arr , dtype = dtype , order = order ) # In the present cas, np.may_share_memory result is always reliable. if np . may_share_memory ( ret , arr ) and copy : # order-preserving copy ret = ret . T . copy ( ) . T if ret . flags [ 'F_CONTIGUOUS' ] else ret . copy ( ) elif isinstance ( arr , ( list , tuple ) ) : if order in ( "A" , "K" ) : ret = np . asarray ( arr , dtype = dtype ) else : ret = np . asarray ( arr , dtype = dtype , order = order ) else : raise ValueError ( "Type not handled: {}" . format ( arr . __class__ ) ) return ret
4767	def is_not_same_as ( self , other ) : if self . val is other : self . _err ( 'Expected <%s> to be not identical to <%s>, but was.' % ( self . val , other ) ) return self
5356	def set_param ( self , section , param , value ) : if section not in self . conf or param not in self . conf [ section ] : logger . error ( 'Config section %s and param %s not exists' , section , param ) else : self . conf [ section ] [ param ] = value
10472	def _addKeyToQueue ( self , keychr , modFlags = 0 , globally = False ) : # Awkward, but makes modifier-key-only combinations possible # (since sendKeyWithModifiers() calls this) if not keychr : return if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) if keychr in self . keyboard [ 'upperSymbols' ] and not modFlags : self . _sendKeyWithModifiers ( keychr , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr . isupper ( ) and not modFlags : self . _sendKeyWithModifiers ( keychr . lower ( ) , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr not in self . keyboard : self . _clearEventQueue ( ) raise ValueError ( 'Key %s not found in keyboard layout' % keychr ) # Press the key keyDown = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , True ) # Release the key keyUp = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , False ) # Set modflags on keyDown (default None): Quartz . CGEventSetFlags ( keyDown , modFlags ) # Set modflags on keyUp: Quartz . CGEventSetFlags ( keyUp , modFlags ) # Post the event to the given app if not globally : # To direct output to the correct application need the PSN (macOS <=10.10) or PID(macOS > 10.10): macVer , _ , _ = platform . mac_ver ( ) macVer = int ( macVer . split ( '.' ) [ 1 ] ) if macVer > 10 : appPid = self . _getPid ( ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyUp ) ) else : appPsn = self . _getPsnForPid ( self . _getPid ( ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyUp ) ) else : self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyDown ) ) self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyUp ) )
9223	def away_from_zero_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] >= 3 : p = 10 ** ndigits return float ( math . floor ( ( value * p ) + math . copysign ( 0.5 , value ) ) ) / p else : return round ( value , ndigits )
5220	def save_intraday ( data : pd . DataFrame , ticker : str , dt , typ = 'TRADE' ) : cur_dt = pd . Timestamp ( dt ) . strftime ( '%Y-%m-%d' ) logger = logs . get_logger ( save_intraday , level = 'debug' ) info = f'{ticker} / {cur_dt} / {typ}' data_file = hist_file ( ticker = ticker , dt = dt , typ = typ ) if not data_file : return if data . empty : logger . warning ( f'data is empty for {info} ...' ) return exch = const . exch_info ( ticker = ticker ) if exch . empty : return end_time = pd . Timestamp ( const . market_timing ( ticker = ticker , dt = dt , timing = 'FINISHED' ) ) . tz_localize ( exch . tz ) now = pd . Timestamp ( 'now' , tz = exch . tz ) - pd . Timedelta ( '1H' ) if end_time > now : logger . debug ( f'skip saving cause market close ({end_time}) < now - 1H ({now}) ...' ) return logger . info ( f'saving data to {data_file} ...' ) files . create_folder ( data_file , is_file = True ) data . to_parquet ( data_file )
8147	def hue ( self , img1 , img2 ) : import colorsys p1 = list ( img1 . getdata ( ) ) p2 = list ( img2 . getdata ( ) ) for i in range ( len ( p1 ) ) : r1 , g1 , b1 , a1 = p1 [ i ] r1 = r1 / 255.0 g1 = g1 / 255.0 b1 = b1 / 255.0 h1 , s1 , v1 = colorsys . rgb_to_hsv ( r1 , g1 , b1 ) r2 , g2 , b2 , a2 = p2 [ i ] r2 = r2 / 255.0 g2 = g2 / 255.0 b2 = b2 / 255.0 h2 , s2 , v2 = colorsys . rgb_to_hsv ( r2 , g2 , b2 ) r3 , g3 , b3 = colorsys . hsv_to_rgb ( h2 , s1 , v1 ) r3 = int ( r3 * 255 ) g3 = int ( g3 * 255 ) b3 = int ( b3 * 255 ) p1 [ i ] = ( r3 , g3 , b3 , a1 ) img = Image . new ( "RGBA" , img1 . size , 255 ) img . putdata ( p1 ) return img
3382	def step ( sampler , x , delta , fraction = None , tries = 0 ) : prob = sampler . problem valid = ( ( np . abs ( delta ) > sampler . feasibility_tol ) & np . logical_not ( prob . variable_fixed ) ) # permissible alphas for staying in variable bounds valphas = ( ( 1.0 - sampler . bounds_tol ) * prob . variable_bounds - x ) [ : , valid ] valphas = ( valphas / delta [ valid ] ) . flatten ( ) if prob . bounds . shape [ 0 ] > 0 : # permissible alphas for staying in constraint bounds ineqs = prob . inequalities . dot ( delta ) valid = np . abs ( ineqs ) > sampler . feasibility_tol balphas = ( ( 1.0 - sampler . bounds_tol ) * prob . bounds - prob . inequalities . dot ( x ) ) [ : , valid ] balphas = ( balphas / ineqs [ valid ] ) . flatten ( ) # combined alphas alphas = np . hstack ( [ valphas , balphas ] ) else : alphas = valphas pos_alphas = alphas [ alphas > 0.0 ] neg_alphas = alphas [ alphas <= 0.0 ] alpha_range = np . array ( [ neg_alphas . max ( ) if len ( neg_alphas ) > 0 else 0 , pos_alphas . min ( ) if len ( pos_alphas ) > 0 else 0 ] ) if fraction : alpha = alpha_range [ 0 ] + fraction * ( alpha_range [ 1 ] - alpha_range [ 0 ] ) else : alpha = np . random . uniform ( alpha_range [ 0 ] , alpha_range [ 1 ] ) p = x + alpha * delta # Numerical instabilities may cause bounds invalidation # reset sampler and sample from one of the original warmup directions # if that occurs. Also reset if we got stuck. if ( np . any ( sampler . _bounds_dist ( p ) < - sampler . bounds_tol ) or np . abs ( np . abs ( alpha_range ) . max ( ) * delta ) . max ( ) < sampler . bounds_tol ) : if tries > MAX_TRIES : raise RuntimeError ( "Can not escape sampling region, model seems" " numerically unstable :( Reporting the " "model to " "https://github.com/opencobra/cobrapy/issues " "will help us to fix this :)" ) LOGGER . info ( "found bounds infeasibility in sample, " "resetting to center" ) newdir = sampler . warmup [ np . random . randint ( sampler . n_warmup ) ] sampler . retries += 1 return step ( sampler , sampler . center , newdir - sampler . center , None , tries + 1 ) return p
3673	def draw_2d ( self , width = 300 , height = 300 , Hs = False ) : # pragma: no cover try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mol = self . rdkitmol_Hs else : mol = self . rdkitmol return Draw . MolToImage ( mol , size = ( width , height ) ) except : return 'Rdkit is required for this feature.'
4308	def _validate_sample_rates ( input_filepath_list , combine_type ) : sample_rates = [ file_info . sample_rate ( f ) for f in input_filepath_list ] if not core . all_equal ( sample_rates ) : raise IOError ( "Input files do not have the same sample rate. The {} combine " "type requires that all files have the same sample rate" . format ( combine_type ) )
13230	def get_macros ( tex_source ) : macros = { } macros . update ( get_def_macros ( tex_source ) ) macros . update ( get_newcommand_macros ( tex_source ) ) return macros
847	def remapCategories ( self , mapping ) : categoryArray = numpy . array ( self . _categoryList ) newCategoryArray = numpy . zeros ( categoryArray . shape [ 0 ] ) newCategoryArray . fill ( - 1 ) for i in xrange ( len ( mapping ) ) : newCategoryArray [ categoryArray == i ] = mapping [ i ] self . _categoryList = list ( newCategoryArray )
9334	def full ( shape , value , dtype = 'f8' ) : shared = empty ( shape , dtype ) shared [ : ] = value return shared
13418	def start ( info ) : cmd = options . paved . django . runserver if cmd == 'runserver_plus' : try : import django_extensions except ImportError : info ( "Could not import django_extensions. Using default runserver." ) cmd = 'runserver' port = options . paved . django . runserver_port if port : cmd = '%s %s' % ( cmd , port ) call_manage ( cmd )
9385	def convert_to_G ( self , word ) : value = 0.0 if word [ - 1 ] == 'G' or word [ - 1 ] == 'g' : value = float ( word [ : - 1 ] ) elif word [ - 1 ] == 'M' or word [ - 1 ] == 'm' : value = float ( word [ : - 1 ] ) / 1000.0 elif word [ - 1 ] == 'K' or word [ - 1 ] == 'k' : value = float ( word [ : - 1 ] ) / 1000.0 / 1000.0 else : # No unit value = float ( word ) / 1000.0 / 1000.0 / 1000.0 return str ( value )
1527	def is_host_port_reachable ( self ) : for hostport in self . hostportlist : try : socket . create_connection ( hostport , StateManager . TIMEOUT_SECONDS ) return True except : LOG . info ( "StateManager %s Unable to connect to host: %s port %i" % ( self . name , hostport [ 0 ] , hostport [ 1 ] ) ) continue return False
11706	def reproduce_sexually ( self , egg_donor , sperm_donor ) : egg_word = random . choice ( egg_donor . genome ) egg = self . generate_gamete ( egg_word ) sperm_word = random . choice ( sperm_donor . genome ) sperm = self . generate_gamete ( sperm_word ) self . genome = list ( set ( egg + sperm ) ) # Eliminate duplicates self . parents = [ egg_donor . name , sperm_donor . name ] self . generation = max ( egg_donor . generation , sperm_donor . generation ) + 1 sum_ = egg_donor . divinity + sperm_donor . divinity self . divinity = int ( npchoice ( divinities , 1 , p = p_divinity [ sum_ ] ) [ 0 ] )
2273	def _win32_read_junction ( path ) : if not jwfs . is_reparse_point ( path ) : raise ValueError ( 'not a junction' ) # --- Older version based on using shell commands --- # if not exists(path): # if six.PY2: # raise OSError('Cannot find path={}'.format(path)) # else: # raise FileNotFoundError('Cannot find path={}'.format(path)) # target_name = os.path.basename(path) # for type_or_size, name, pointed in _win32_dir(path, '*'): # if type_or_size == '<JUNCTION>' and name == target_name: # return pointed # raise ValueError('not a junction') # new version using the windows api handle = jwfs . api . CreateFile ( path , 0 , 0 , None , jwfs . api . OPEN_EXISTING , jwfs . api . FILE_FLAG_OPEN_REPARSE_POINT | jwfs . api . FILE_FLAG_BACKUP_SEMANTICS , None ) if handle == jwfs . api . INVALID_HANDLE_VALUE : raise WindowsError ( ) res = jwfs . reparse . DeviceIoControl ( handle , jwfs . api . FSCTL_GET_REPARSE_POINT , None , 10240 ) bytes = jwfs . create_string_buffer ( res ) p_rdb = jwfs . cast ( bytes , jwfs . POINTER ( jwfs . api . REPARSE_DATA_BUFFER ) ) rdb = p_rdb . contents if rdb . tag not in [ 2684354563 , jwfs . api . IO_REPARSE_TAG_SYMLINK ] : raise RuntimeError ( "Expected <2684354563 or 2684354572>, but got %d" % rdb . tag ) jwfs . handle_nonzero_success ( jwfs . api . CloseHandle ( handle ) ) subname = rdb . get_substitute_name ( ) # probably has something to do with long paths, not sure if subname . startswith ( '?\\' ) : subname = subname [ 2 : ] return subname
13739	def _keep_alive_thread ( self ) : while True : with self . _lock : if self . connected ( ) : self . _ws . ping ( ) else : self . disconnect ( ) self . _thread = None return sleep ( 30 )
768	def getMetricDetails ( self , metricLabel ) : try : metricIndex = self . __metricLabels . index ( metricLabel ) except IndexError : return None return self . __metrics [ metricIndex ] . getMetric ( )
2888	def disconnect ( self , callback ) : if self . weak_subscribers is not None : with self . lock : index = self . _weakly_connected_index ( callback ) if index is not None : self . weak_subscribers . pop ( index ) [ 0 ] if self . hard_subscribers is not None : try : index = self . _hard_callbacks ( ) . index ( callback ) except ValueError : pass else : self . hard_subscribers . pop ( index )
1304	def SendMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> int : return ctypes . windll . user32 . SendMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam )
7694	def _sasl_authenticate ( self , stream , username , authzid ) : if not stream . initiator : raise SASLAuthenticationFailed ( "Only initiating entity start" " SASL authentication" ) if stream . features is None or not self . peer_sasl_mechanisms : raise SASLNotAvailable ( "Peer doesn't support SASL" ) props = dict ( stream . auth_properties ) if not props . get ( "service-domain" ) and ( stream . peer and stream . peer . domain ) : props [ "service-domain" ] = stream . peer . domain if username is not None : props [ "username" ] = username if authzid is not None : props [ "authzid" ] = authzid if "password" in self . settings : props [ "password" ] = self . settings [ "password" ] props [ "available_mechanisms" ] = self . peer_sasl_mechanisms enabled = sasl . filter_mechanism_list ( self . settings [ 'sasl_mechanisms' ] , props , self . settings [ 'insecure_auth' ] ) if not enabled : raise SASLNotAvailable ( "None of SASL mechanism selected can be used" ) props [ "enabled_mechanisms" ] = enabled mechanism = None for mech in enabled : if mech in self . peer_sasl_mechanisms : mechanism = mech break if not mechanism : raise SASLMechanismNotAvailable ( "Peer doesn't support any of" " our SASL mechanisms" ) logger . debug ( "Our mechanism: {0!r}" . format ( mechanism ) ) stream . auth_method_used = mechanism self . authenticator = sasl . client_authenticator_factory ( mechanism ) initial_response = self . authenticator . start ( props ) if not isinstance ( initial_response , sasl . Response ) : raise SASLAuthenticationFailed ( "SASL initiation failed" ) element = ElementTree . Element ( AUTH_TAG ) element . set ( "mechanism" , mechanism ) if initial_response . data : if initial_response . encode : element . text = initial_response . encode ( ) else : element . text = initial_response . data stream . write_element ( element )
1363	def get_argument_offset ( self ) : try : offset = self . get_argument ( constants . PARAM_OFFSET ) return offset except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
11057	def _remove_by_pk ( self , key , flush = True ) : try : del self . store [ key ] except Exception as error : pass if flush : self . flush ( )
9262	def filter_by_include_labels ( self , issues ) : if not self . options . include_labels : return copy . deepcopy ( issues ) filtered_issues = [ ] include_labels = set ( self . options . include_labels ) for issue in issues : labels = [ label [ "name" ] for label in issue [ "labels" ] ] if include_labels . intersection ( labels ) : filtered_issues . append ( issue ) return filtered_issues
1198	def tf_loss ( self , states , internals , reward , update , reference = None ) : prediction = self . predict ( states = states , internals = internals , update = update ) return tf . nn . l2_loss ( t = ( prediction - reward ) )
6551	def from_configurations ( cls , configurations , variables , vartype , name = None ) : def func ( * args ) : return args in configurations return cls ( func , configurations , variables , vartype , name )
9504	def distance_to_point ( self , p ) : if self . start <= p <= self . end : return 0 else : return min ( abs ( self . start - p ) , abs ( self . end - p ) )
3793	def setup_a_alpha_and_derivatives ( self , i , T = None ) : self . a , self . kappa , self . Tc = self . ais [ i ] , self . kappas [ i ] , self . Tcs [ i ]
11810	def index_document ( self , text , url ) : ## For now, use first line for title title = text [ : text . index ( '\n' ) ] . strip ( ) docwords = words ( text ) docid = len ( self . documents ) self . documents . append ( Document ( title , url , len ( docwords ) ) ) for word in docwords : if word not in self . stopwords : self . index [ word ] [ docid ] += 1
10632	def get_compound_mfr ( self , compound ) : if compound in self . material . compounds : return self . _compound_mfrs [ self . material . get_compound_index ( compound ) ] else : return 0.0
2392	def edit_distance ( s1 , s2 ) : d = { } lenstr1 = len ( s1 ) lenstr2 = len ( s2 ) for i in xrange ( - 1 , lenstr1 + 1 ) : d [ ( i , - 1 ) ] = i + 1 for j in xrange ( - 1 , lenstr2 + 1 ) : d [ ( - 1 , j ) ] = j + 1 for i in xrange ( lenstr1 ) : for j in xrange ( lenstr2 ) : if s1 [ i ] == s2 [ j ] : cost = 0 else : cost = 1 d [ ( i , j ) ] = min ( d [ ( i - 1 , j ) ] + 1 , # deletion d [ ( i , j - 1 ) ] + 1 , # insertion d [ ( i - 1 , j - 1 ) ] + cost , # substitution ) if i and j and s1 [ i ] == s2 [ j - 1 ] and s1 [ i - 1 ] == s2 [ j ] : d [ ( i , j ) ] = min ( d [ ( i , j ) ] , d [ i - 2 , j - 2 ] + cost ) # transposition return d [ lenstr1 - 1 , lenstr2 - 1 ]
13834	def PrintMessage ( self , message ) : fields = message . ListFields ( ) if self . use_index_order : fields . sort ( key = lambda x : x [ 0 ] . index ) for field , value in fields : if _IsMapEntry ( field ) : for key in sorted ( value ) : # This is slow for maps with submessage entires because it copies the # entire tree. Unfortunately this would take significant refactoring # of this file to work around. # # TODO(haberman): refactor and optimize if this becomes an issue. entry_submsg = field . message_type . _concrete_class ( key = key , value = value [ key ] ) self . PrintField ( field , entry_submsg ) elif field . label == descriptor . FieldDescriptor . LABEL_REPEATED : for element in value : self . PrintField ( field , element ) else : self . PrintField ( field , value )
3351	def get_by_any ( self , iterable ) : def get_item ( item ) : if isinstance ( item , int ) : return self [ item ] elif isinstance ( item , string_types ) : return self . get_by_id ( item ) elif item in self : return item else : raise TypeError ( "item in iterable cannot be '%s'" % type ( item ) ) if not isinstance ( iterable , list ) : iterable = [ iterable ] return [ get_item ( item ) for item in iterable ]
3257	def publish_featuretype ( self , name , store , native_crs , srs = None , jdbc_virtual_table = None , native_name = None ) : # @todo native_srs doesn't seem to get detected, even when in the DB # metadata (at least for postgis in geometry_columns) and then there # will be a misconfigured layer if native_crs is None : raise ValueError ( "must specify native_crs" ) srs = srs or native_crs feature_type = FeatureType ( self , store . workspace , store , name ) # because name is the in FeatureType base class, work around that # and hack in these others that don't have xml properties feature_type . dirty [ 'name' ] = name feature_type . dirty [ 'srs' ] = srs feature_type . dirty [ 'nativeCRS' ] = native_crs feature_type . enabled = True feature_type . advertised = True feature_type . title = name if native_name is not None : feature_type . native_name = native_name headers = { "Content-type" : "application/xml" , "Accept" : "application/xml" } resource_url = store . resource_url if jdbc_virtual_table is not None : feature_type . metadata = ( { 'JDBC_VIRTUAL_TABLE' : jdbc_virtual_table } ) params = dict ( ) resource_url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "datastores" , store . name , "featuretypes.xml" ] , params ) resp = self . http_request ( resource_url , method = 'post' , data = feature_type . message ( ) , headers = headers ) if resp . status_code not in ( 200 , 201 , 202 ) : FailedRequestError ( 'Failed to publish feature type {} : {}, {}' . format ( name , resp . status_code , resp . text ) ) self . _cache . clear ( ) feature_type . fetch ( ) return feature_type
116	def map_batches_async ( self , batches , chunksize = None , callback = None , error_callback = None ) : assert isinstance ( batches , list ) , ( "Expected to get a list as 'batches', got type %s. " + "Call imap_batches() if you use generators." ) % ( type ( batches ) , ) return self . pool . map_async ( _Pool_starworker , self . _handle_batch_ids ( batches ) , chunksize = chunksize , callback = callback , error_callback = error_callback )
5503	def config ( ctx , key , value , remove , edit ) : conf = ctx . obj [ "conf" ] if not edit and not key : raise click . BadArgumentUsage ( "You have to specify either a key or use --edit." ) if edit : return click . edit ( filename = conf . config_file ) if remove : try : conf . cfg . remove_option ( key [ 0 ] , key [ 1 ] ) except Exception as e : logger . debug ( e ) else : conf . write_config ( ) return if not value : try : click . echo ( conf . cfg . get ( key [ 0 ] , key [ 1 ] ) ) except Exception as e : logger . debug ( e ) return if not conf . cfg . has_section ( key [ 0 ] ) : conf . cfg . add_section ( key [ 0 ] ) conf . cfg . set ( key [ 0 ] , key [ 1 ] , value ) conf . write_config ( )
4175	def window_blackman ( N , alpha = 0.16 ) : a0 = ( 1. - alpha ) / 2. a1 = 0.5 a2 = alpha / 2. if ( N == 1 ) : win = array ( [ 1. ] ) else : k = arange ( 0 , N ) / float ( N - 1. ) win = a0 - a1 * cos ( 2 * pi * k ) + a2 * cos ( 4 * pi * k ) return win
5333	def config_logging ( debug ) : if debug : logging . basicConfig ( level = logging . DEBUG , format = '%(asctime)s %(message)s' ) logging . debug ( "Debug mode activated" ) else : logging . basicConfig ( level = logging . INFO , format = '%(asctime)s %(message)s' )
6545	def is_connected ( self ) : # need to wrap in try/except b/c of wc3270's socket connection dynamics try : # this is basically a no-op, but it results in the the current status # getting updated self . exec_command ( b"Query(ConnectionState)" ) # connected status is like 'C(192.168.1.1)', disconnected is 'N' return self . status . connection_state . startswith ( b"C(" ) except NotConnectedException : return False
9827	def download ( ctx ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) try : PolyaxonClient ( ) . project . download_repo ( user , project_name ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not download code for project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( 'Files downloaded.' )
2039	def CALLCODE ( self , gas , _ignored_ , value , in_offset , in_size , out_offset , out_size ) : self . world . start_transaction ( 'CALLCODE' , address = self . address , data = self . read_buffer ( in_offset , in_size ) , caller = self . address , value = value , gas = gas ) raise StartTx ( )
5208	def format_intraday ( data : pd . DataFrame , ticker , * * kwargs ) -> pd . DataFrame : if data . empty : return pd . DataFrame ( ) data . columns = pd . MultiIndex . from_product ( [ [ ticker ] , data . rename ( columns = dict ( numEvents = 'num_trds' ) ) . columns ] , names = [ 'ticker' , 'field' ] ) data . index . name = None if kwargs . get ( 'price_only' , False ) : kw_xs = dict ( axis = 1 , level = 1 ) close = data . xs ( 'close' , * * kw_xs ) volume = data . xs ( 'volume' , * * kw_xs ) . iloc [ : , 0 ] return close . loc [ volume > 0 ] if volume . min ( ) > 0 else close else : return data
13333	def path_resolver ( resolver , path ) : path = unipath ( path ) if is_environment ( path ) : return VirtualEnvironment ( path ) raise ResolveError
4784	def contains_ignoring_case ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) if isinstance ( self . val , str_types ) : if len ( items ) == 1 : if not isinstance ( items [ 0 ] , str_types ) : raise TypeError ( 'given arg must be a string' ) if items [ 0 ] . lower ( ) not in self . val . lower ( ) : self . _err ( 'Expected <%s> to case-insensitive contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if not isinstance ( i , str_types ) : raise TypeError ( 'given args must all be strings' ) if i . lower ( ) not in self . val . lower ( ) : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) elif isinstance ( self . val , Iterable ) : missing = [ ] for i in items : if not isinstance ( i , str_types ) : raise TypeError ( 'given args must all be strings' ) found = False for v in self . val : if not isinstance ( v , str_types ) : raise TypeError ( 'val items must all be strings' ) if i . lower ( ) == v . lower ( ) : found = True break if not found : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
744	def requireAnomalyModel ( func ) : @ wraps ( func ) def _decorator ( self , * args , * * kwargs ) : if not self . getInferenceType ( ) == InferenceType . TemporalAnomaly : raise RuntimeError ( "Method required a TemporalAnomaly model." ) if self . _getAnomalyClassifier ( ) is None : raise RuntimeError ( "Model does not support this command. Model must" "be an active anomalyDetector model." ) return func ( self , * args , * * kwargs ) return _decorator
12580	def to_file ( self , outpath ) : if not self . has_mask ( ) and not self . is_smoothed ( ) : save_niigz ( outpath , self . img ) else : save_niigz ( outpath , self . get_data ( masked = True , smoothed = True ) , self . get_header ( ) , self . get_affine ( ) )
11147	def is_repository_file ( self , relativePath ) : relativePath = self . to_repo_relative_path ( path = relativePath , split = False ) if relativePath == '' : return False , False , False , False relaDir , name = os . path . split ( relativePath ) fileOnDisk = os . path . isfile ( os . path . join ( self . __path , relativePath ) ) infoOnDisk = os . path . isfile ( os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileInfo % name ) ) classOnDisk = os . path . isfile ( os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileClass % name ) ) cDir = self . __repo [ 'walk_repo' ] if len ( relaDir ) : for dirname in relaDir . split ( os . sep ) : dList = [ d for d in cDir if isinstance ( d , dict ) ] if not len ( dList ) : cDir = None break cDict = [ d for d in dList if dirname in d ] if not len ( cDict ) : cDir = None break cDir = cDict [ 0 ] [ dirname ] if cDir is None : return False , fileOnDisk , infoOnDisk , classOnDisk #if name not in cDir: if str ( name ) not in [ str ( i ) for i in cDir ] : return False , fileOnDisk , infoOnDisk , classOnDisk # this is a repository registered file. check whether all is on disk return True , fileOnDisk , infoOnDisk , classOnDisk
6053	def unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask , unmasked_sparse_grid_pixel_centres , total_sparse_pixels ) : total_unmasked_sparse_pixels = unmasked_sparse_grid_pixel_centres . shape [ 0 ] unmasked_sparse_to_sparse = np . zeros ( total_unmasked_sparse_pixels ) pixel_index = 0 for unmasked_sparse_pixel_index in range ( total_unmasked_sparse_pixels ) : y = unmasked_sparse_grid_pixel_centres [ unmasked_sparse_pixel_index , 0 ] x = unmasked_sparse_grid_pixel_centres [ unmasked_sparse_pixel_index , 1 ] unmasked_sparse_to_sparse [ unmasked_sparse_pixel_index ] = pixel_index if not mask [ y , x ] : if pixel_index < total_sparse_pixels - 1 : pixel_index += 1 return unmasked_sparse_to_sparse
1910	def run ( self , procs = 1 , timeout = None , should_profile = False ) : assert not self . running , "Manticore is already running." self . _start_run ( ) self . _last_run_stats [ 'time_started' ] = time . time ( ) with self . shutdown_timeout ( timeout ) : self . _start_workers ( procs , profiling = should_profile ) self . _join_workers ( ) self . _finish_run ( profiling = should_profile )
1640	def CheckCommaSpacing ( filename , clean_lines , linenum , error ) : raw = clean_lines . lines_without_raw_strings line = clean_lines . elided [ linenum ] # You should always have a space after a comma (either as fn arg or operator) # # This does not apply when the non-space character following the # comma is another comma, since the only time when that happens is # for empty macro arguments. # # We run this check in two passes: first pass on elided lines to # verify that lines contain missing whitespaces, second pass on raw # lines to confirm that those missing whitespaces are not due to # elided comments. if ( Search ( r',[^,\s]' , ReplaceAll ( r'\boperator\s*,\s*\(' , 'F(' , line ) ) and Search ( r',[^,\s]' , raw [ linenum ] ) ) : error ( filename , linenum , 'whitespace/comma' , 3 , 'Missing space after ,' ) # You should always have a space after a semicolon # except for few corner cases # TODO(unknown): clarify if 'if (1) { return 1;}' is requires one more # space after ; if Search ( r';[^\s};\\)/]' , line ) : error ( filename , linenum , 'whitespace/semicolon' , 3 , 'Missing space after ;' )
10398	def score_leaves ( self ) -> Set [ BaseEntity ] : leaves = set ( self . iter_leaves ( ) ) if not leaves : log . warning ( 'no leaves.' ) return set ( ) for leaf in leaves : self . graph . nodes [ leaf ] [ self . tag ] = self . calculate_score ( leaf ) log . log ( 5 , 'chomping %s' , leaf ) return leaves
5575	def available_input_formats ( ) : input_formats = [ ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : logger . debug ( "driver found: %s" , v ) driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "mode" ] in [ "r" , "rw" ] ) : input_formats . append ( driver_ . METADATA [ "driver_name" ] ) return input_formats
7138	def get_type_info ( obj ) : if isinstance ( obj , primitive_types ) : return ( 'primitive' , type ( obj ) . __name__ ) if isinstance ( obj , sequence_types ) : return ( 'sequence' , type ( obj ) . __name__ ) if isinstance ( obj , array_types ) : return ( 'array' , type ( obj ) . __name__ ) if isinstance ( obj , key_value_types ) : return ( 'key-value' , type ( obj ) . __name__ ) if isinstance ( obj , types . ModuleType ) : return ( 'module' , type ( obj ) . __name__ ) if isinstance ( obj , ( types . FunctionType , types . MethodType ) ) : return ( 'function' , type ( obj ) . __name__ ) if isinstance ( obj , type ) : if hasattr ( obj , '__dict__' ) : return ( 'class' , obj . __name__ ) if isinstance ( type ( obj ) , type ) : if hasattr ( obj , '__dict__' ) : cls_name = type ( obj ) . __name__ if cls_name == 'classobj' : cls_name = obj . __name__ return ( 'class' , '{}' . format ( cls_name ) ) if cls_name == 'instance' : cls_name = obj . __class__ . __name__ return ( 'instance' , '{} instance' . format ( cls_name ) ) return ( 'unknown' , type ( obj ) . __name__ )
876	def getPositionFromState ( pState ) : result = dict ( ) for ( varName , value ) in pState [ 'varStates' ] . iteritems ( ) : result [ varName ] = value [ 'position' ] return result
2015	def _rollback ( self ) : last_pc , last_gas , last_instruction , last_arguments , fee , allocated = self . _checkpoint_data self . _push_arguments ( last_arguments ) self . _gas = last_gas self . _pc = last_pc self . _allocated = allocated self . _checkpoint_data = None
13490	def reconcile ( self , server ) : if not self . challenge . exists ( server ) : raise Exception ( 'Challenge does not exist on server' ) existing = MapRouletteTaskCollection . from_server ( server , self . challenge ) same = [ ] new = [ ] changed = [ ] deleted = [ ] # reconcile the new tasks with the existing tasks: for task in self . tasks : # if the task exists on the server... if task . identifier in [ existing_task . identifier for existing_task in existing . tasks ] : # and they are equal... if task == existing . get_by_identifier ( task . identifier ) : # add to 'same' list same . append ( task ) # if they are not equal, add to 'changed' list else : changed . append ( task ) # if the task does not exist on the server, add to 'new' list else : new . append ( task ) # next, check for tasks on the server that don't exist in the new collection... for task in existing . tasks : if task . identifier not in [ task . identifier for task in self . tasks ] : # ... and add those to the 'deleted' list. deleted . append ( task ) # update the server with new, changed, and deleted tasks if new : newCollection = MapRouletteTaskCollection ( self . challenge , tasks = new ) newCollection . create ( server ) if changed : changedCollection = MapRouletteTaskCollection ( self . challenge , tasks = changed ) changedCollection . update ( server ) if deleted : deletedCollection = MapRouletteTaskCollection ( self . challenge , tasks = deleted ) for task in deletedCollection . tasks : task . status = 'deleted' deletedCollection . update ( server ) # return same, new, changed and deleted tasks return { 'same' : same , 'new' : new , 'changed' : changed , 'deleted' : deleted }
5637	def largest_finite_distance ( self ) : block_start_distances = [ block . distance_start for block in self . _profile_blocks if block . distance_start < float ( 'inf' ) ] block_end_distances = [ block . distance_end for block in self . _profile_blocks if block . distance_end < float ( 'inf' ) ] distances = block_start_distances + block_end_distances if len ( distances ) > 0 : return max ( distances ) else : return None
4658	def as_base ( self , base ) : if base == self [ "base" ] [ "symbol" ] : return self . copy ( ) elif base == self [ "quote" ] [ "symbol" ] : return self . copy ( ) . invert ( ) else : raise InvalidAssetException
8553	def delete_ipblock ( self , ipblock_id ) : response = self . _perform_request ( url = '/ipblocks/' + ipblock_id , method = 'DELETE' ) return response
9627	def url ( self ) : return reverse ( '%s:detail' % URL_NAMESPACE , kwargs = { 'module' : self . module , 'preview' : type ( self ) . __name__ , } )
4824	def get_course_modes ( self , course_id ) : details = self . get_course_details ( course_id ) modes = details . get ( 'course_modes' , [ ] ) return self . _sort_course_modes ( [ mode for mode in modes if mode [ 'slug' ] not in EXCLUDED_COURSE_MODES ] )
2385	def from_spec_resolver ( cls , spec_resolver ) : deref = DerefValidatorDecorator ( spec_resolver ) for key , validator_callable in iteritems ( cls . validators ) : yield key , deref ( validator_callable )
602	def addHistogram ( self , data , position = 111 , xlabel = None , ylabel = None , bins = None ) : ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . hist ( data , bins = bins , color = "green" , alpha = 0.8 ) plt . draw ( )
1002	def _updateAvgLearnedSeqLength ( self , prevSeqLength ) : if self . lrnIterationIdx < 100 : alpha = 0.5 else : alpha = 0.1 self . avgLearnedSeqLength = ( ( 1.0 - alpha ) * self . avgLearnedSeqLength + ( alpha * prevSeqLength ) )
7490	def get_targets ( ipyclient ) : ## fill hosts with async[gethostname] hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : hosts . append ( engine . apply ( socket . gethostname ) ) ## capture results of asyncs hosts = [ i . get ( ) for i in hosts ] hostset = set ( hosts ) hostzip = zip ( hosts , ipyclient . ids ) hostdict = { host : [ i [ 1 ] for i in hostzip if i [ 0 ] == host ] for host in hostset } targets = list ( itertools . chain ( * [ hostdict [ i ] [ : 2 ] for i in hostdict ] ) ) ## return first two engines from each host return targets
699	def getParticleInfos ( self , swarmId = None , genIdx = None , completed = None , matured = None , lastDescendent = False ) : # The indexes of all the models in this swarm. This list excludes hidden # (orphaned) models. if swarmId is not None : entryIdxs = self . _swarmIdToIndexes . get ( swarmId , [ ] ) else : entryIdxs = range ( len ( self . _allResults ) ) if len ( entryIdxs ) == 0 : return ( [ ] , [ ] , [ ] , [ ] , [ ] ) # Get the particles of interest particleStates = [ ] modelIds = [ ] errScores = [ ] completedFlags = [ ] maturedFlags = [ ] for idx in entryIdxs : entry = self . _allResults [ idx ] # If this entry is hidden (i.e. it was an orphaned model), it should # not be in this list if swarmId is not None : assert ( not entry [ 'hidden' ] ) # Get info on this model modelParams = entry [ 'modelParams' ] isCompleted = entry [ 'completed' ] isMatured = entry [ 'matured' ] particleState = modelParams [ 'particleState' ] particleGenIdx = particleState [ 'genIdx' ] particleId = particleState [ 'id' ] if genIdx is not None and particleGenIdx != genIdx : continue if completed is not None and ( completed != isCompleted ) : continue if matured is not None and ( matured != isMatured ) : continue if lastDescendent and ( self . _particleLatestGenIdx [ particleId ] != particleGenIdx ) : continue # Incorporate into return values particleStates . append ( particleState ) modelIds . append ( entry [ 'modelID' ] ) errScores . append ( entry [ 'errScore' ] ) completedFlags . append ( isCompleted ) maturedFlags . append ( isMatured ) return ( particleStates , modelIds , errScores , completedFlags , maturedFlags )
4880	def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . update_or_create ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH , defaults = { 'active' : False } )
9304	def parse_date ( date_str ) : months = [ 'jan' , 'feb' , 'mar' , 'apr' , 'may' , 'jun' , 'jul' , 'aug' , 'sep' , 'oct' , 'nov' , 'dec' ] formats = { # RFC 7231, e.g. 'Mon, 09 Sep 2011 23:36:00 GMT' r'^(?:\w{3}, )?(\d{2}) (\w{3}) (\d{4})\D.*$' : lambda m : '{}-{:02d}-{}' . format ( m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , # RFC 850 (e.g. Sunday, 06-Nov-94 08:49:37 GMT) # assumes current century r'^\w+day, (\d{2})-(\w{3})-(\d{2})\D.*$' : lambda m : '{}{}-{:02d}-{}' . format ( str ( datetime . date . today ( ) . year ) [ : 2 ] , m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , # C time, e.g. 'Wed Dec 4 00:00:00 2002' r'^\w{3} (\w{3}) (\d{1,2}) \d{2}:\d{2}:\d{2} (\d{4})$' : lambda m : '{}-{:02d}-{:02d}' . format ( m . group ( 3 ) , months . index ( m . group ( 1 ) . lower ( ) ) + 1 , int ( m . group ( 2 ) ) ) , # x-amz-date format dates, e.g. 20100325T010101Z r'^(\d{4})(\d{2})(\d{2})T\d{6}Z$' : lambda m : '{}-{}-{}' . format ( * m . groups ( ) ) , # ISO 8601 / RFC 3339, e.g. '2009-03-25T10:11:12.13-01:00' r'^(\d{4}-\d{2}-\d{2})(?:[Tt].*)?$' : lambda m : m . group ( 1 ) , } out_date = None for regex , xform in formats . items ( ) : m = re . search ( regex , date_str ) if m : out_date = xform ( m ) break if out_date is None : raise DateFormatError else : return out_date
7131	def setup_paths ( source , destination , name , add_to_global , force ) : if source [ - 1 ] == "/" : source = source [ : - 1 ] if not name : name = os . path . split ( source ) [ - 1 ] elif name . endswith ( ".docset" ) : name = name . replace ( ".docset" , "" ) if add_to_global : destination = DEFAULT_DOCSET_PATH dest = os . path . join ( destination or "" , name + ".docset" ) dst_exists = os . path . lexists ( dest ) if dst_exists and force : shutil . rmtree ( dest ) elif dst_exists : log . error ( 'Destination path "{}" already exists.' . format ( click . format_filename ( dest ) ) ) raise SystemExit ( errno . EEXIST ) return source , dest , name
2591	def stage_out ( self , file , executor ) : if file . scheme == 'http' or file . scheme == 'https' : raise Exception ( 'HTTP/HTTPS file staging out is not supported' ) elif file . scheme == 'ftp' : raise Exception ( 'FTP file staging out is not supported' ) elif file . scheme == 'globus' : globus_ep = self . _get_globus_endpoint ( executor ) stage_out_app = self . _globus_stage_out_app ( ) return stage_out_app ( globus_ep , inputs = [ file ] ) else : raise Exception ( 'Staging out with unknown file scheme {} is not supported' . format ( file . scheme ) )
7717	def _roster_set ( self , item , callback , error_callback ) : stanza = Iq ( to_jid = self . server , stanza_type = "set" ) payload = RosterPayload ( [ item ] ) stanza . set_payload ( payload ) def success_cb ( result_stanza ) : """Success callback for roster set.""" if callback : callback ( item ) def error_cb ( error_stanza ) : """Error callback for roster set.""" if error_callback : error_callback ( error_stanza ) else : logger . error ( "Roster change of '{0}' failed" . format ( item . jid ) ) processor = self . stanza_processor processor . set_response_handlers ( stanza , success_cb , error_cb ) processor . send ( stanza )
6071	def luminosity_within_circle_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if self . has_light_profile : return sum ( map ( lambda p : p . luminosity_within_circle_in_units ( radius = radius , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) , self . light_profiles ) ) else : return None
6382	def sim_hamming ( src , tar , diff_lens = True ) : return Hamming ( ) . sim ( src , tar , diff_lens )
11562	def set_analog_latch ( self , pin , threshold_type , threshold_value , cb = None ) : if self . ANALOG_LATCH_GT <= threshold_type <= self . ANALOG_LATCH_LTE : if 0 <= threshold_value <= 1023 : self . _command_handler . set_analog_latch ( pin , threshold_type , threshold_value , cb ) return True else : return False
1938	def get_func_argument_types ( self , hsh : bytes ) : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) return '()' if sig is None else sig [ sig . find ( '(' ) : ]
11599	def verify ( xml , stream ) : # Import xmlsec here to delay initializing the C library in # case we don't need it. import xmlsec # Find the <Signature/> node. signature_node = xmlsec . tree . find_node ( xml , xmlsec . Node . SIGNATURE ) if signature_node is None : # No `signature` node found; we cannot verify return False # Create a digital signature context (no key manager is needed). ctx = xmlsec . SignatureContext ( ) # Register <Response/> and <Assertion/> ctx . register_id ( xml ) for assertion in xml . xpath ( "//*[local-name()='Assertion']" ) : ctx . register_id ( assertion ) # Load the public key. key = None for fmt in [ xmlsec . KeyFormat . PEM , xmlsec . KeyFormat . CERT_PEM ] : stream . seek ( 0 ) try : key = xmlsec . Key . from_memory ( stream , fmt ) break except ValueError : # xmlsec now throws when it can't load the key pass # Set the key on the context. ctx . key = key # Verify the signature. try : ctx . verify ( signature_node ) return True except Exception : return False
527	def _getColumnNeighborhood ( self , centerColumn ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerColumn , self . _inhibitionRadius , self . _columnDimensions ) else : return topology . neighborhood ( centerColumn , self . _inhibitionRadius , self . _columnDimensions )
2237	def import_module_from_path ( modpath , index = - 1 ) : import os if not os . path . exists ( modpath ) : import re import zipimport # We allow (if not prefer or force) the colon to be a path.sep in order # to agree with the mod.__name__ attribute that will be produced # zip followed by colon or slash pat = '(.zip[' + re . escape ( os . path . sep ) + '/:])' parts = re . split ( pat , modpath , flags = re . IGNORECASE ) if len ( parts ) > 2 : archivepath = '' . join ( parts [ : - 1 ] ) [ : - 1 ] internal = parts [ - 1 ] modname = os . path . splitext ( internal ) [ 0 ] modname = os . path . normpath ( modname ) if os . path . exists ( archivepath ) : zimp_file = zipimport . zipimporter ( archivepath ) module = zimp_file . load_module ( modname ) return module raise IOError ( 'modpath={} does not exist' . format ( modpath ) ) else : # the importlib version doesnt work in pytest module = _custom_import_modpath ( modpath ) # TODO: use this implementation once pytest fixes importlib # module = _pkgutil_import_modpath(modpath) return module
12291	def annotate_metadata_data ( repo , task , patterns = [ "*" ] , size = 0 ) : mgr = plugins_get_mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get_by_key ( 'representation' , k ) for k in keys ] matching_files = repo . find_matching_files ( patterns ) package = repo . package rootdir = repo . rootdir files = package [ 'resources' ] for f in files : relativepath = f [ 'relativepath' ] if relativepath in matching_files : path = os . path . join ( rootdir , relativepath ) if task == 'preview' : print ( "Adding preview for " , relativepath ) f [ 'content' ] = open ( path ) . read ( ) [ : size ] elif task == 'schema' : for r in representations : if r . can_process ( path ) : print ( "Adding schema for " , path ) f [ 'schema' ] = r . get_schema ( path ) break
8430	def cmap_d_pal ( name = None , lut = None ) : colormap = get_cmap ( name , lut ) if not isinstance ( colormap , mcolors . ListedColormap ) : raise ValueError ( "For a discrete palette, cmap must be of type " "matplotlib.colors.ListedColormap" ) ncolors = len ( colormap . colors ) def _cmap_d_pal ( n ) : if n > ncolors : raise ValueError ( "cmap `{}` has {} colors you requested {} " "colors." . format ( name , ncolors , n ) ) if ncolors < 256 : return [ mcolors . rgb2hex ( c ) for c in colormap . colors [ : n ] ] else : # Assume these are continuous and get colors equally spaced # intervals e.g. viridis is defined with 256 colors idx = np . linspace ( 0 , ncolors - 1 , n ) . round ( ) . astype ( int ) return [ mcolors . rgb2hex ( colormap . colors [ i ] ) for i in idx ] return _cmap_d_pal
3352	def query ( self , search_function , attribute = None ) : def select_attribute ( x ) : if attribute is None : return x else : return getattr ( x , attribute ) try : # if the search_function is a regular expression regex_searcher = re . compile ( search_function ) if attribute is not None : matches = ( i for i in self if regex_searcher . findall ( select_attribute ( i ) ) != [ ] ) else : # Don't regex on objects matches = ( i for i in self if regex_searcher . findall ( getattr ( i , 'id' ) ) != [ ] ) except TypeError : matches = ( i for i in self if search_function ( select_attribute ( i ) ) ) results = self . __class__ ( ) results . _extend_nocheck ( matches ) return results
13671	def strip_codes ( s : Any ) -> str : return codepat . sub ( '' , str ( s ) if ( s or ( s == 0 ) ) else '' )
5585	def output_is_valid ( self , process_data ) : if self . METADATA [ "data_type" ] == "raster" : return ( is_numpy_or_masked_array ( process_data ) or is_numpy_or_masked_array_with_tags ( process_data ) ) elif self . METADATA [ "data_type" ] == "vector" : return is_feature_list ( process_data )
11827	def boggle_neighbors ( n2 , cache = { } ) : if cache . get ( n2 ) : return cache . get ( n2 ) n = exact_sqrt ( n2 ) neighbors = [ None ] * n2 for i in range ( n2 ) : neighbors [ i ] = [ ] on_top = i < n on_bottom = i >= n2 - n on_left = i % n == 0 on_right = ( i + 1 ) % n == 0 if not on_top : neighbors [ i ] . append ( i - n ) if not on_left : neighbors [ i ] . append ( i - n - 1 ) if not on_right : neighbors [ i ] . append ( i - n + 1 ) if not on_bottom : neighbors [ i ] . append ( i + n ) if not on_left : neighbors [ i ] . append ( i + n - 1 ) if not on_right : neighbors [ i ] . append ( i + n + 1 ) if not on_left : neighbors [ i ] . append ( i - 1 ) if not on_right : neighbors [ i ] . append ( i + 1 ) cache [ n2 ] = neighbors return neighbors
2843	def disable_FTDI_driver ( ) : logger . debug ( 'Disabling FTDI driver.' ) if sys . platform == 'darwin' : logger . debug ( 'Detected Mac OSX' ) # Mac OS commands to disable FTDI driver. _check_running_as_root ( ) subprocess . call ( 'kextunload -b com.apple.driver.AppleUSBFTDI' , shell = True ) subprocess . call ( 'kextunload /System/Library/Extensions/FTDIUSBSerialDriver.kext' , shell = True ) elif sys . platform . startswith ( 'linux' ) : logger . debug ( 'Detected Linux' ) # Linux commands to disable FTDI driver. _check_running_as_root ( ) subprocess . call ( 'modprobe -r -q ftdi_sio' , shell = True ) subprocess . call ( 'modprobe -r -q usbserial' , shell = True )
4053	def all_collections ( self , collid = None ) : all_collections = [ ] def subcoll ( clct ) : """ recursively add collections to a flat master list """ all_collections . append ( clct ) if clct [ "meta" ] . get ( "numCollections" , 0 ) > 0 : # add collection to master list & recur with all child # collections [ subcoll ( c ) for c in self . everything ( self . collections_sub ( clct [ "data" ] [ "key" ] ) ) ] # select all top-level collections or a specific collection and # children if collid : toplevel = [ self . collection ( collid ) ] else : toplevel = self . everything ( self . collections_top ( ) ) [ subcoll ( collection ) for collection in toplevel ] return all_collections
1951	def deprecated ( message : str ) : assert isinstance ( message , str ) , "The deprecated decorator requires a message string argument." def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : warnings . warn ( f"`{func.__qualname__}` is deprecated. {message}" , category = ManticoreDeprecationWarning , stacklevel = 2 ) return func ( * args , * * kwargs ) return wrapper return decorator
1591	def _setup_custom_grouping ( self , topology ) : for i in range ( len ( topology . bolts ) ) : for in_stream in topology . bolts [ i ] . inputs : if in_stream . stream . component_name == self . my_component_name and in_stream . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) : # this bolt takes my output in custom grouping manner if in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) : custom_grouping_obj = default_serializer . deserialize ( in_stream . custom_grouping_object ) if isinstance ( custom_grouping_obj , str ) : pex_loader . load_pex ( self . topology_pex_abs_path ) grouping_cls = pex_loader . import_and_get_class ( self . topology_pex_abs_path , custom_grouping_obj ) custom_grouping_obj = grouping_cls ( ) assert isinstance ( custom_grouping_obj , ICustomGrouping ) self . custom_grouper . add ( in_stream . stream . id , self . _get_taskids_for_component ( topology . bolts [ i ] . comp . name ) , custom_grouping_obj , self . my_component_name ) elif in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "JAVA_OBJECT" ) : raise NotImplementedError ( "Java-serialized custom grouping is not yet supported " "for python topology" ) else : raise ValueError ( "Unrecognized custom grouping type found: %s" % str ( in_stream . type ) )
4812	def tokenize ( text , custom_dict = None ) : global TOKENIZER if not TOKENIZER : TOKENIZER = DeepcutTokenizer ( ) return TOKENIZER . tokenize ( text , custom_dict = custom_dict )
4494	def upload ( args ) : osf = _setup_osf ( args ) if osf . username is None or osf . password is None : sys . exit ( 'To upload a file you need to provide a username and' ' password.' ) project = osf . project ( args . project ) storage , remote_path = split_storage ( args . destination ) store = project . storage ( storage ) if args . recursive : if not os . path . isdir ( args . source ) : raise RuntimeError ( "Expected source ({}) to be a directory when " "using recursive mode." . format ( args . source ) ) # local name of the directory that is being uploaded _ , dir_name = os . path . split ( args . source ) for root , _ , files in os . walk ( args . source ) : subdir_path = os . path . relpath ( root , args . source ) for fname in files : local_path = os . path . join ( root , fname ) with open ( local_path , 'rb' ) as fp : # build the remote path + fname name = os . path . join ( remote_path , dir_name , subdir_path , fname ) store . create_file ( name , fp , force = args . force , update = args . update ) else : with open ( args . source , 'rb' ) as fp : store . create_file ( remote_path , fp , force = args . force , update = args . update )
5798	def _find_sections ( md_ast , sections , last , last_class , total_lines = None ) : def child_walker ( node ) : for child , entering in node . walker ( ) : if child == node : continue yield child , entering for child , entering in child_walker ( md_ast ) : if child . t == 'heading' : start_line = child . sourcepos [ 0 ] [ 0 ] if child . level == 2 : if last : sections [ ( last [ 'type_name' ] , last [ 'identifier' ] ) ] = ( last [ 'start_line' ] , start_line - 1 ) last . clear ( ) if child . level in set ( [ 3 , 5 ] ) : heading_elements = [ ] for heading_child , _ in child_walker ( child ) : heading_elements . append ( heading_child ) if len ( heading_elements ) != 2 : continue first = heading_elements [ 0 ] second = heading_elements [ 1 ] if first . t != 'code' : continue if second . t != 'text' : continue type_name = second . literal . strip ( ) identifier = first . literal . strip ( ) . replace ( '()' , '' ) . lstrip ( '.' ) if last : sections [ ( last [ 'type_name' ] , last [ 'identifier' ] ) ] = ( last [ 'start_line' ] , start_line - 1 ) last . clear ( ) if type_name == 'function' : if child . level != 3 : continue if type_name == 'class' : if child . level != 3 : continue last_class . append ( identifier ) if type_name in set ( [ 'method' , 'attribute' ] ) : if child . level != 5 : continue identifier = last_class [ - 1 ] + '.' + identifier last . update ( { 'type_name' : type_name , 'identifier' : identifier , 'start_line' : start_line , } ) elif child . t == 'block_quote' : find_sections ( child , sections , last , last_class ) if last : sections [ ( last [ 'type_name' ] , last [ 'identifier' ] ) ] = ( last [ 'start_line' ] , total_lines )
4705	def write ( self , path ) : with open ( path , "wb" ) as fout : fout . write ( self . m_buf )
10187	def _queries_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_queries ) : for cfg in ep . load ( ) ( ) : if cfg [ 'query_name' ] not in self . enabled_queries : continue elif cfg [ 'query_name' ] in result : raise DuplicateQueryError ( 'Duplicate query {0} in entry point ' '{1}' . format ( cfg [ 'query' ] , ep . name ) ) # Update the default configuration with env/overlay config. cfg . update ( self . enabled_queries [ cfg [ 'query_name' ] ] or { } ) result [ cfg [ 'query_name' ] ] = cfg return result
3820	async def create_conversation ( self , create_conversation_request ) : response = hangouts_pb2 . CreateConversationResponse ( ) await self . _pb_request ( 'conversations/createconversation' , create_conversation_request , response ) return response
1080	def isocalendar ( self ) : year = self . _year week1monday = _isoweek1monday ( year ) today = _ymd2ord ( self . _year , self . _month , self . _day ) # Internally, week and day have origin 0 week , day = divmod ( today - week1monday , 7 ) if week < 0 : year -= 1 week1monday = _isoweek1monday ( year ) week , day = divmod ( today - week1monday , 7 ) elif week >= 52 : if today >= _isoweek1monday ( year + 1 ) : year += 1 week = 0 return year , week + 1 , day + 1
8186	def draw ( self , dx = 0 , dy = 0 , weighted = False , directed = False , highlight = [ ] , traffic = None ) : self . update ( ) # Draw the graph background. s = self . styles . default s . graph_background ( s ) # Center the graph on the canvas. _ctx . push ( ) _ctx . translate ( self . x + dx , self . y + dy ) # Indicate betweenness centrality. if traffic : if isinstance ( traffic , bool ) : traffic = 5 for n in self . nodes_by_betweenness ( ) [ : traffic ] : try : s = self . styles [ n . style ] except : s = self . styles . default if s . graph_traffic : s . graph_traffic ( s , n , self . alpha ) # Draw the edges and their labels. s = self . styles . default if s . edges : s . edges ( s , self . edges , self . alpha , weighted , directed ) # Draw each node in the graph. # Apply individual style to each node (or default). for n in self . nodes : try : s = self . styles [ n . style ] except : s = self . styles . default if s . node : s . node ( s , n , self . alpha ) # Highlight the given shortest path. try : s = self . styles . highlight except : s = self . styles . default if s . path : s . path ( s , self , highlight ) # Draw node id's as labels on each node. for n in self . nodes : try : s = self . styles [ n . style ] except : s = self . styles . default if s . node_label : s . node_label ( s , n , self . alpha ) # Events for clicked and dragged nodes. # Nodes will resist being dragged by attraction and repulsion, # put the event listener on top to get more direct feedback. #self.events.update() _ctx . pop ( )
12164	def add_listener ( self , event , listener ) : self . emit ( 'new_listener' , event , listener ) self . _listeners [ event ] . append ( listener ) self . _check_limit ( event ) return self
11142	def remove_repository ( self , path = None , removeEmptyDirs = True ) : assert isinstance ( removeEmptyDirs , bool ) , "removeEmptyDirs must be boolean" if path is not None : if path != self . __path : repo = Repository ( ) repo . load_repository ( path ) else : repo = self else : repo = self assert repo . path is not None , "path is not given and repository is not initialized" # remove repo files and directories for fdict in reversed ( repo . get_repository_state ( ) ) : relaPath = list ( fdict ) [ 0 ] realPath = os . path . join ( repo . path , relaPath ) path , name = os . path . split ( realPath ) if fdict [ relaPath ] [ 'type' ] == 'file' : if os . path . isfile ( realPath ) : os . remove ( realPath ) if os . path . isfile ( os . path . join ( repo . path , path , self . __fileInfo % name ) ) : os . remove ( os . path . join ( repo . path , path , self . __fileInfo % name ) ) if os . path . isfile ( os . path . join ( repo . path , path , self . __fileLock % name ) ) : os . remove ( os . path . join ( repo . path , path , self . __fileLock % name ) ) if os . path . isfile ( os . path . join ( repo . path , path , self . __fileClass % name ) ) : os . remove ( os . path . join ( repo . path , path , self . __fileClass % name ) ) elif fdict [ relaPath ] [ 'type' ] == 'dir' : if os . path . isfile ( os . path . join ( realPath , self . __dirInfo ) ) : os . remove ( os . path . join ( realPath , self . __dirInfo ) ) if os . path . isfile ( os . path . join ( realPath , self . __dirLock ) ) : os . remove ( os . path . join ( realPath , self . __dirLock ) ) if not len ( os . listdir ( realPath ) ) and removeEmptyDirs : shutil . rmtree ( realPath ) # remove repo information file if os . path . isfile ( os . path . join ( repo . path , self . __repoFile ) ) : os . remove ( os . path . join ( repo . path , self . __repoFile ) ) if os . path . isfile ( os . path . join ( repo . path , self . __repoLock ) ) : os . remove ( os . path . join ( repo . path , self . __repoLock ) )
885	def activatePredictedColumn ( self , column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) : return self . _activatePredictedColumn ( self . connections , self . _random , columnActiveSegments , prevActiveCells , prevWinnerCells , self . numActivePotentialSynapsesForSegment , self . maxNewSynapseCount , self . initialPermanence , self . permanenceIncrement , self . permanenceDecrement , self . maxSynapsesPerSegment , learn )
348	def load_nietzsche_dataset ( path = 'data' ) : logging . info ( "Load or Download nietzsche dataset > {}" . format ( path ) ) path = os . path . join ( path , 'nietzsche' ) filename = "nietzsche.txt" url = 'https://s3.amazonaws.com/text-datasets/' filepath = maybe_download_and_extract ( filename , path , url ) with open ( filepath , "r" ) as f : words = f . read ( ) return words
5437	def args_to_job_params ( envs , labels , inputs , inputs_recursive , outputs , outputs_recursive , mounts , input_file_param_util , output_file_param_util , mount_param_util ) : # Parse environmental variables and labels. env_data = parse_pair_args ( envs , job_model . EnvParam ) label_data = parse_pair_args ( labels , job_model . LabelParam ) # For input files, we need to: # * split the input into name=uri pairs (name optional) # * get the environmental variable name, or automatically set if null. # * create the input file param input_data = set ( ) for ( recursive , args ) in ( ( False , inputs ) , ( True , inputs_recursive ) ) : for arg in args : name , value = split_pair ( arg , '=' , nullable_idx = 0 ) name = input_file_param_util . get_variable_name ( name ) input_data . add ( input_file_param_util . make_param ( name , value , recursive ) ) # For output files, we need to: # * split the input into name=uri pairs (name optional) # * get the environmental variable name, or automatically set if null. # * create the output file param output_data = set ( ) for ( recursive , args ) in ( ( False , outputs ) , ( True , outputs_recursive ) ) : for arg in args : name , value = split_pair ( arg , '=' , 0 ) name = output_file_param_util . get_variable_name ( name ) output_data . add ( output_file_param_util . make_param ( name , value , recursive ) ) mount_data = set ( ) for arg in mounts : # Mounts can look like `--mount VAR=PATH` or `--mount VAR=PATH {num}`, # where num is the size of the disk in Gb. We assume a space is the # separator between path and disk size. if ' ' in arg : key_value_pair , disk_size = arg . split ( ' ' ) name , value = split_pair ( key_value_pair , '=' , 1 ) mount_data . add ( mount_param_util . make_param ( name , value , disk_size ) ) else : name , value = split_pair ( arg , '=' , 1 ) mount_data . add ( mount_param_util . make_param ( name , value , disk_size = None ) ) return { 'envs' : env_data , 'inputs' : input_data , 'outputs' : output_data , 'labels' : label_data , 'mounts' : mount_data , }
10982	def locate_spheres ( image , feature_rad , dofilter = False , order = ( 3 , 3 , 3 ) , trim_edge = True , * * kwargs ) : # We just want a smoothed field model of the image so that the residuals # are simply the particles without other complications m = models . SmoothFieldModel ( ) I = ilms . LegendrePoly2P1D ( order = order , constval = image . get_image ( ) . mean ( ) ) s = states . ImageState ( image , [ I ] , pad = 0 , mdl = m ) if dofilter : opt . do_levmarq ( s , s . params ) pos = addsub . feature_guess ( s , feature_rad , trim_edge = trim_edge , * * kwargs ) [ 0 ] return pos
13408	def setupUI ( self ) : labelSizePolicy = QSizePolicy ( QSizePolicy . Fixed , QSizePolicy . Fixed ) labelSizePolicy . setHorizontalStretch ( 0 ) labelSizePolicy . setVerticalStretch ( 0 ) menuSizePolicy = QSizePolicy ( QSizePolicy . Expanding , QSizePolicy . Fixed ) menuSizePolicy . setHorizontalStretch ( 0 ) menuSizePolicy . setVerticalStretch ( 0 ) logTypeLayout = QHBoxLayout ( ) logTypeLayout . setSpacing ( 0 ) typeLabel = QLabel ( "Log Type:" ) typeLabel . setMinimumSize ( QSize ( 65 , 0 ) ) typeLabel . setMaximumSize ( QSize ( 65 , 16777215 ) ) typeLabel . setSizePolicy ( labelSizePolicy ) logTypeLayout . addWidget ( typeLabel ) self . logType = QComboBox ( self ) self . logType . setMinimumSize ( QSize ( 100 , 0 ) ) self . logType . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . logType . sizePolicy ( ) . hasHeightForWidth ( ) ) self . logType . setSizePolicy ( menuSizePolicy ) logTypeLayout . addWidget ( self . logType ) logTypeLayout . setStretch ( 1 , 6 ) programLayout = QHBoxLayout ( ) programLayout . setSpacing ( 0 ) programLabel = QLabel ( "Program:" ) programLabel . setMinimumSize ( QSize ( 60 , 0 ) ) programLabel . setMaximumSize ( QSize ( 60 , 16777215 ) ) programLabel . setSizePolicy ( labelSizePolicy ) programLayout . addWidget ( programLabel ) self . programName = QComboBox ( self ) self . programName . setMinimumSize ( QSize ( 100 , 0 ) ) self . programName . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . programName . sizePolicy ( ) . hasHeightForWidth ( ) ) self . programName . setSizePolicy ( menuSizePolicy ) programLayout . addWidget ( self . programName ) programLayout . setStretch ( 1 , 6 ) # Initial instance allows adding additional menus, all following menus can only remove themselves. if self . initialInstance : self . logButton = QPushButton ( "+" , self ) self . logButton . setToolTip ( "Add logbook" ) else : self . logButton = QPushButton ( "-" ) self . logButton . setToolTip ( "Remove logbook" ) self . logButton . setMinimumSize ( QSize ( 16 , 16 ) ) # 24x24 self . logButton . setMaximumSize ( QSize ( 16 , 16 ) ) # 24x24 self . logButton . setObjectName ( "roundButton" ) # self.logButton.setAutoFillBackground(True) # region = QRegion(QRect(self.logButton.x()+15, self.logButton.y()+14, 20, 20), QRegion.Ellipse) # self.logButton.setMask(region) self . logButton . setStyleSheet ( "QPushButton {border-radius: 8px;}" ) self . _logSelectLayout = QHBoxLayout ( ) self . _logSelectLayout . setSpacing ( 6 ) self . _logSelectLayout . addLayout ( logTypeLayout ) self . _logSelectLayout . addLayout ( programLayout ) self . _logSelectLayout . addWidget ( self . logButton ) self . _logSelectLayout . setStretch ( 0 , 6 ) self . _logSelectLayout . setStretch ( 1 , 6 )
1251	def _do_action_left ( self , state ) : reward = 0 for row in range ( 4 ) : # Always the rightmost tile in the current row that was already moved merge_candidate = - 1 merged = np . zeros ( ( 4 , ) , dtype = np . bool ) for col in range ( 4 ) : if state [ row , col ] == 0 : continue if ( merge_candidate != - 1 and not merged [ merge_candidate ] and state [ row , merge_candidate ] == state [ row , col ] ) : # Merge tile with merge_candidate state [ row , col ] = 0 merged [ merge_candidate ] = True state [ row , merge_candidate ] += 1 reward += 2 ** state [ row , merge_candidate ] else : # Move tile to the left merge_candidate += 1 if col != merge_candidate : state [ row , merge_candidate ] = state [ row , col ] state [ row , col ] = 0 return reward
1983	def save_value ( self , key , value ) : with self . save_stream ( key ) as s : s . write ( value )
10540	def delete_category ( category_id ) : try : res = _pybossa_req ( 'delete' , 'category' , category_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : # pragma: no cover raise
6938	def parallel_update_objectinfo_cpdir ( cpdir , cpglob = 'checkplot-*.pkl*' , liststartindex = None , maxobjects = None , nworkers = NCPUS , fast_mode = False , findercmap = 'gray_r' , finderconvolve = None , deredden_object = True , custom_bandpasses = None , gaia_submit_timeout = 10.0 , gaia_submit_tries = 3 , gaia_max_timeout = 180.0 , gaia_mirror = None , complete_query_later = True , lclistpkl = None , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , plotdpi = 100 , findercachedir = '~/.astrobase/stamp-cache' , verbose = True ) : cplist = sorted ( glob . glob ( os . path . join ( cpdir , cpglob ) ) ) return parallel_update_objectinfo_cplist ( cplist , liststartindex = liststartindex , maxobjects = maxobjects , nworkers = nworkers , fast_mode = fast_mode , findercmap = findercmap , finderconvolve = finderconvolve , deredden_object = deredden_object , custom_bandpasses = custom_bandpasses , gaia_submit_timeout = gaia_submit_timeout , gaia_submit_tries = gaia_submit_tries , gaia_max_timeout = gaia_max_timeout , gaia_mirror = gaia_mirror , complete_query_later = complete_query_later , lclistpkl = lclistpkl , nbrradiusarcsec = nbrradiusarcsec , maxnumneighbors = maxnumneighbors , plotdpi = plotdpi , findercachedir = findercachedir , verbose = verbose )
13723	def log_file ( self , url = None ) : if url is None : url = self . url f = re . sub ( "file://" , "" , url ) try : with open ( f , "a" ) as of : of . write ( str ( self . store . get_json_tuples ( True ) ) ) except IOError as e : print ( e ) print ( "Could not write the content to the file.." )
13233	def load ( directory_name , module_name ) : directory_name = os . path . expanduser ( directory_name ) if os . path . isdir ( directory_name ) and directory_name not in sys . path : sys . path . append ( directory_name ) try : return importlib . import_module ( module_name ) except ImportError : pass
13400	def addLogbook ( self , physDef = "LCLS" , mccDef = "MCC" , initialInstance = False ) : if self . logMenuCount < 5 : self . logMenus . append ( LogSelectMenu ( self . logui . multiLogLayout , initialInstance ) ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 1 ] , self . physics_programs , physDef ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 0 ] , self . mcc_programs , mccDef ) self . logMenus [ - 1 ] . show ( ) self . logMenuCount += 1 if initialInstance : # Initial logbook menu can add additional menus, all others can only remove themselves. QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , self . addLogbook ) else : from functools import partial QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , partial ( self . removeLogbook , self . logMenus [ - 1 ] ) )
3808	def charge_from_formula ( formula ) : negative = '-' in formula positive = '+' in formula if positive and negative : raise ValueError ( 'Both negative and positive signs were found in the formula; only one sign is allowed' ) elif not ( positive or negative ) : return 0 multiplier , sign = ( - 1 , '-' ) if negative else ( 1 , '+' ) hit = False if '(' in formula : hit = bracketed_charge_re . findall ( formula ) if hit : formula = hit [ - 1 ] . replace ( '(' , '' ) . replace ( ')' , '' ) count = formula . count ( sign ) if count == 1 : splits = formula . split ( sign ) if splits [ 1 ] == '' or splits [ 1 ] == ')' : return multiplier else : return multiplier * int ( splits [ 1 ] ) else : return multiplier * count
5475	def string_presenter ( self , dumper , data ) : if '\n' in data : return dumper . represent_scalar ( 'tag:yaml.org,2002:str' , data , style = '|' ) else : return dumper . represent_scalar ( 'tag:yaml.org,2002:str' , data )
10807	def validate ( cls , state ) : return state in [ cls . ACTIVE , cls . PENDING_ADMIN , cls . PENDING_USER ]
1547	def configure ( level = logging . INFO , logfile = None ) : # Remove all the existing StreamHandlers to avoid duplicate for handler in Log . handlers : if isinstance ( handler , logging . StreamHandler ) : Log . handlers . remove ( handler ) Log . setLevel ( level ) # if logfile is specified, FileHandler is used if logfile is not None : log_format = "[%(asctime)s] [%(levelname)s]: %(message)s" formatter = logging . Formatter ( fmt = log_format , datefmt = date_format ) file_handler = logging . FileHandler ( logfile ) file_handler . setFormatter ( formatter ) Log . addHandler ( file_handler ) # otherwise, use StreamHandler to output to stream (stdout, stderr...) else : log_format = "[%(asctime)s] %(log_color)s[%(levelname)s]%(reset)s: %(message)s" # pylint: disable=redefined-variable-type formatter = colorlog . ColoredFormatter ( fmt = log_format , datefmt = date_format ) stream_handler = logging . StreamHandler ( ) stream_handler . setFormatter ( formatter ) Log . addHandler ( stream_handler )
6434	def dist_eudex ( src , tar , weights = 'exponential' , max_length = 8 ) : return Eudex ( ) . dist ( src , tar , weights , max_length )
2022	def SIGNEXTEND ( self , size , value ) : # FIXME maybe use Operators.SEXTEND testbit = Operators . ITEBV ( 256 , size <= 31 , size * 8 + 7 , 257 ) result1 = ( value | ( TT256 - ( 1 << testbit ) ) ) result2 = ( value & ( ( 1 << testbit ) - 1 ) ) result = Operators . ITEBV ( 256 , ( value & ( 1 << testbit ) ) != 0 , result1 , result2 ) return Operators . ITEBV ( 256 , size <= 31 , result , value )
3176	def update ( self , campaign_id , feedback_id , data ) : self . campaign_id = campaign_id self . feedback_id = feedback_id if 'message' not in data : raise KeyError ( 'The campaign feedback must have a message' ) return self . _mc_client . _patch ( url = self . _build_path ( campaign_id , 'feedback' , feedback_id ) , data = data )
7053	def _read_pklc ( lcfile ) : if lcfile . endswith ( '.gz' ) : try : with gzip . open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with gzip . open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) else : try : with open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) return lcdict
13340	def rollaxis ( a , axis , start = 0 ) : if isinstance ( a , np . ndarray ) : return np . rollaxis ( a , axis , start ) if axis not in range ( a . ndim ) : raise ValueError ( 'rollaxis: axis (%d) must be >=0 and < %d' % ( axis , a . ndim ) ) if start not in range ( a . ndim + 1 ) : raise ValueError ( 'rollaxis: start (%d) must be >=0 and < %d' % ( axis , a . ndim + 1 ) ) axes = list ( range ( a . ndim ) ) axes . remove ( axis ) axes . insert ( start , axis ) return transpose ( a , axes )
6719	def what_requires ( self , name ) : r = self . local_renderer r . env . name = name r . local ( 'pipdeptree -p {name} --reverse' )
9766	def clean_outputs ( fn ) : @ wraps ( fn ) def clean_outputs_wrapper ( * args , * * kwargs ) : try : return fn ( * args , * * kwargs ) except SystemExit as e : sys . stdout = StringIO ( ) sys . exit ( e . code ) # make sure we still exit with the proper code except Exception as e : sys . stdout = StringIO ( ) raise e return clean_outputs_wrapper
13190	def json_doc_to_xml ( json_obj , lang = 'en' , custom_namespace = None ) : if 'meta' not in json_obj : raise Exception ( "This function requires a conforming Open511 JSON document with a 'meta' section." ) json_obj = dict ( json_obj ) meta = json_obj . pop ( 'meta' ) elem = get_base_open511_element ( lang = lang , version = meta . pop ( 'version' ) ) pagination = json_obj . pop ( 'pagination' , None ) json_struct_to_xml ( json_obj , elem , custom_namespace = custom_namespace ) if pagination : elem . append ( json_struct_to_xml ( pagination , 'pagination' , custom_namespace = custom_namespace ) ) json_struct_to_xml ( meta , elem ) return elem
10044	def default_view_method ( pid , record , template = None ) : record_viewed . send ( current_app . _get_current_object ( ) , pid = pid , record = record , ) deposit_type = request . values . get ( 'type' ) return render_template ( template , pid = pid , record = record , jsonschema = current_deposit . jsonschemas [ deposit_type ] , schemaform = current_deposit . schemaforms [ deposit_type ] , )
3394	def find_gene_knockout_reactions ( cobra_model , gene_list , compiled_gene_reaction_rules = None ) : potential_reactions = set ( ) for gene in gene_list : if isinstance ( gene , string_types ) : gene = cobra_model . genes . get_by_id ( gene ) potential_reactions . update ( gene . _reaction ) gene_set = { str ( i ) for i in gene_list } if compiled_gene_reaction_rules is None : compiled_gene_reaction_rules = { r : parse_gpr ( r . gene_reaction_rule ) [ 0 ] for r in potential_reactions } return [ r for r in potential_reactions if not eval_gpr ( compiled_gene_reaction_rules [ r ] , gene_set ) ]
2899	def get_tasks_from_spec_name ( self , name ) : return [ task for task in self . get_tasks ( ) if task . task_spec . name == name ]
11105	def sync_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , * * kwargs ) : if not self . _keepSynchronized : r = func ( self , * args , * * kwargs ) else : state = self . _load_state ( ) #print("-----------> ",state, self.state) if state is None : r = func ( self , * args , * * kwargs ) elif state == self . state : r = func ( self , * args , * * kwargs ) else : warnings . warn ( "Repository at '%s' is out of date. Need to load it again to avoid conflict." % self . path ) r = None return r return wrapper
8059	def do_vars ( self , line ) : if self . bot . _vars : max_name_len = max ( [ len ( name ) for name in self . bot . _vars ] ) for i , ( name , v ) in enumerate ( self . bot . _vars . items ( ) ) : keep = i < len ( self . bot . _vars ) - 1 self . print_response ( "%s = %s" % ( name . ljust ( max_name_len ) , v . value ) , keep = keep ) else : self . print_response ( "No vars" )
11584	def background_image_finder ( pipeline_index , soup , finder_image_urls = [ ] , * args , * * kwargs ) : now_finder_image_urls = [ ] for tag in soup . find_all ( style = True ) : style_string = tag [ 'style' ] if 'background-image' in style_string . lower ( ) : style = cssutils . parseStyle ( style_string ) background_image = style . getProperty ( 'background-image' ) if background_image : for property_value in background_image . propertyValue : background_image_url = str ( property_value . value ) if background_image_url : if ( background_image_url not in finder_image_urls ) and ( background_image_url not in now_finder_image_urls ) : now_finder_image_urls . append ( background_image_url ) output = { } output [ 'finder_image_urls' ] = finder_image_urls + now_finder_image_urls return output
8561	def get_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) ) return response
12842	def receive_id_from_server ( self ) : for message in self . pipe . receive ( ) : if isinstance ( message , IdFactory ) : self . actor_id_factory = message return True return False
7037	def xmatch_search ( lcc_server , file_to_upload , xmatch_dist_arcsec = 3.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , limitspec = None , samplespec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : with open ( file_to_upload ) as infd : xmq = infd . read ( ) # check the number of lines in the input xmqlines = len ( xmq . split ( '\n' ) [ : - 1 ] ) if xmqlines > 5000 : LOGERROR ( 'you have more than 5000 lines in the file to upload: %s' % file_to_upload ) return None , None , None # turn the input into a param dict params = { 'xmq' : xmq , 'xmd' : xmatch_dist_arcsec } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done # we won't wait for the LC ZIP to complete if email_when_done = True if email_when_done : download_data = False # check if we have an API key already have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) # if not, get a new one if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) # hit the server api_url = '%s/api/xmatch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) # check the status of the search status = searchresult [ 0 ] # now we'll check if we want to download the data if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
11342	def load_config ( filename = None , section_option_dict = { } ) : config = ConfigParser ( ) config . read ( filename ) working_dict = _prepare_working_dict ( config , section_option_dict ) tmp_dict = { } for section , options in working_dict . iteritems ( ) : tmp_dict [ section ] = { } for option in options : tmp_dict [ section ] [ option ] = config . get ( section , option ) return Bunch ( tmp_dict )
8085	def nostroke ( self ) : c = self . _canvas . strokecolor self . _canvas . strokecolor = None return c
11395	def load_class ( path ) : package , klass = path . rsplit ( '.' , 1 ) module = import_module ( package ) return getattr ( module , klass )
5113	def clear ( self ) : self . _t = 0 self . num_events = 0 self . num_agents = np . zeros ( self . nE , int ) self . _fancy_heap = PriorityQueue ( ) self . _prev_edge = None self . _initialized = False self . reset_colors ( ) for q in self . edge2queue : q . clear ( )
8751	def get_scalingips ( context , filters = None , fields = None , sorts = [ 'id' ] , limit = None , marker = None , page_reverse = False ) : LOG . info ( 'get_scalingips for tenant %s filters %s fields %s' % ( context . tenant_id , filters , fields ) ) scaling_ips = _get_ips_by_type ( context , ip_types . SCALING , filters = filters , fields = fields ) return [ v . _make_scaling_ip_dict ( scip ) for scip in scaling_ips ]
13077	def main_collections ( self , lang = None ) : return sorted ( [ { "id" : member . id , "label" : str ( member . get_label ( lang = lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size } for member in self . resolver . getMetadata ( ) . members ] , key = itemgetter ( "label" ) )
4320	def set_globals ( self , dither = False , guard = False , multithread = False , replay_gain = False , verbosity = 2 ) : if not isinstance ( dither , bool ) : raise ValueError ( 'dither must be a boolean.' ) if not isinstance ( guard , bool ) : raise ValueError ( 'guard must be a boolean.' ) if not isinstance ( multithread , bool ) : raise ValueError ( 'multithread must be a boolean.' ) if not isinstance ( replay_gain , bool ) : raise ValueError ( 'replay_gain must be a boolean.' ) if verbosity not in VERBOSITY_VALS : raise ValueError ( 'Invalid value for VERBOSITY. Must be one {}' . format ( VERBOSITY_VALS ) ) global_args = [ ] if not dither : global_args . append ( '-D' ) if guard : global_args . append ( '-G' ) if multithread : global_args . append ( '--multi-threaded' ) if replay_gain : global_args . append ( '--replay-gain' ) global_args . append ( 'track' ) global_args . append ( '-V{}' . format ( verbosity ) ) self . globals = global_args return self
553	def readStateFromDB ( self ) : self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] # Init if no prior state yet if self . _priorStateJSON is None : swarms = dict ( ) # Fast Swarm, first and only sprint has one swarm for each field # in fixedFields if self . _hsObj . _fixedFields is not None : print self . _hsObj . _fixedFields encoderSet = [ ] for field in self . _hsObj . _fixedFields : if field == '_classifierInput' : continue encoderName = self . getEncoderKeyFromName ( field ) assert encoderName in self . _hsObj . _encoderNames , "The field '%s' " " specified in the fixedFields list is not present in this " " model." % ( field ) encoderSet . append ( encoderName ) encoderSet . sort ( ) swarms [ '.' . join ( encoderSet ) ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } # Temporal prediction search, first sprint has N swarms of 1 field each, # the predicted field may or may not be that one field. elif self . _hsObj . _searchType == HsSearchType . temporal : for encoderName in self . _hsObj . _encoderNames : swarms [ encoderName ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } # Classification prediction search, first sprint has N swarms of 1 field # each where this field can NOT be the predicted field. elif self . _hsObj . _searchType == HsSearchType . classification : for encoderName in self . _hsObj . _encoderNames : if encoderName == self . _hsObj . _predictedFieldEncoder : continue swarms [ encoderName ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } # Legacy temporal. This is either a model that uses reconstruction or # an older multi-step model that doesn't have a separate # 'classifierOnly' encoder for the predicted field. Here, the predicted # field must ALWAYS be present and the first sprint tries the predicted # field only elif self . _hsObj . _searchType == HsSearchType . legacyTemporal : swarms [ self . _hsObj . _predictedFieldEncoder ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } else : raise RuntimeError ( "Unsupported search type: %s" % ( self . _hsObj . _searchType ) ) # Initialize the state. self . _state = dict ( # The last time the state was updated by a worker. lastUpdateTime = time . time ( ) , # Set from within setSwarmState() if we detect that the sprint we just # completed did worse than a prior sprint. This stores the index of # the last good sprint. lastGoodSprint = None , # Set from within setSwarmState() if lastGoodSprint is True and all # sprints have completed. searchOver = False , # This is a summary of the active swarms - this information can also # be obtained from the swarms entry that follows, but is summarized here # for easier reference when viewing the state as presented by # log messages and prints of the hsState data structure (by # permutations_runner). activeSwarms = swarms . keys ( ) , # All the swarms that have been created so far. swarms = swarms , # All the sprints that have completed or are in progress. sprints = [ { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None } ] , # The list of encoders we have "blacklisted" because they # performed so poorly. blackListedEncoders = [ ] , ) # This will do nothing if the value of engWorkerState is not still None. self . _hsObj . _cjDAO . jobSetFieldIfEqual ( self . _hsObj . _jobID , 'engWorkerState' , json . dumps ( self . _state ) , None ) self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] assert ( self . _priorStateJSON is not None ) # Read state from the database self . _state = json . loads ( self . _priorStateJSON ) self . _dirty = False
9750	def find_x ( path1 ) : libs = os . listdir ( path1 ) for lib_dir in libs : if "doublefann" in lib_dir : return True
7533	def concat_multiple_edits ( data , sample ) : ## if more than one tuple in fastq list if len ( sample . files . edits ) > 1 : ## create a cat command to append them all (doesn't matter if they ## are gzipped, cat still works). Grab index 0 of tuples for R1s. cmd1 = [ "cat" ] + [ i [ 0 ] for i in sample . files . edits ] ## write to new concat handle conc1 = os . path . join ( data . dirs . edits , sample . name + "_R1_concatedit.fq.gz" ) with open ( conc1 , 'w' ) as cout1 : proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = cout1 , close_fds = True ) res1 = proc1 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( "error in: %s, %s" , cmd1 , res1 ) ## Only set conc2 if R2 actually exists conc2 = 0 if os . path . exists ( str ( sample . files . edits [ 0 ] [ 1 ] ) ) : cmd2 = [ "cat" ] + [ i [ 1 ] for i in sample . files . edits ] conc2 = os . path . join ( data . dirs . edits , sample . name + "_R2_concatedit.fq.gz" ) with gzip . open ( conc2 , 'w' ) as cout2 : proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = cout2 , close_fds = True ) res2 = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "error in: %s, %s" , cmd2 , res2 ) ## store new file handles sample . files . edits = [ ( conc1 , conc2 ) ] return sample . files . edits
7209	def cancel ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot cancel.' ) if self . batch_values : self . workflow . batch_workflow_cancel ( self . id ) else : self . workflow . cancel ( self . id )
3084	def oauth2decorator_from_clientsecrets ( filename , scope , message = None , cache = None ) : return OAuth2DecoratorFromClientSecrets ( filename , scope , message = message , cache = cache )
3003	def start ( self ) : super ( JupyterTensorboardApp , self ) . start ( ) # The above should have called a subcommand and raised NoStart; if we # get here, it didn't, so we should self.log.info a message. subcmds = ", " . join ( sorted ( self . subcommands ) ) sys . exit ( "Please supply at least one subcommand: %s" % subcmds )
4576	def color_cmp ( a , b ) : if a == b : return 0 a , b = rgb_to_hsv ( a ) , rgb_to_hsv ( b ) return - 1 if a < b else 1
7714	def add_item ( self , jid , name = None , groups = None , callback = None , error_callback = None ) : # pylint: disable=R0913 if jid in self . roster : raise ValueError ( "{0!r} already in the roster" . format ( jid ) ) item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
11848	def things_near ( self , location , radius = None ) : if radius is None : radius = self . perceptible_distance radius2 = radius * radius return [ thing for thing in self . things if distance2 ( location , thing . location ) <= radius2 ]
8460	def write_temple_config ( temple_config , template , version ) : with open ( temple . constants . TEMPLE_CONFIG_FILE , 'w' ) as temple_config_file : versioned_config = { * * temple_config , * * { '_version' : version , '_template' : template } , } yaml . dump ( versioned_config , temple_config_file , Dumper = yaml . SafeDumper )
9931	def get_repr ( self , obj , referent = None ) : objtype = type ( obj ) typename = str ( objtype . __module__ ) + "." + objtype . __name__ prettytype = typename . replace ( "__builtin__." , "" ) name = getattr ( obj , "__name__" , "" ) if name : prettytype = "%s %r" % ( prettytype , name ) key = "" if referent : key = self . get_refkey ( obj , referent ) url = reverse ( 'dowser_trace_object' , args = ( typename , id ( obj ) ) ) return ( '<a class="objectid" href="%s">%s</a> ' '<span class="typename">%s</span>%s<br />' '<span class="repr">%s</span>' % ( url , id ( obj ) , prettytype , key , get_repr ( obj , 100 ) ) )
2372	def variables ( self ) : for table in self . tables : if isinstance ( table , VariableTable ) : # FIXME: settings have statements, variables have rows WTF? :-( for statement in table . rows : if statement [ 0 ] != "" : yield statement
9419	def format_docstring ( * args , * * kwargs ) : def decorator ( func ) : func . __doc__ = getdoc ( func ) . format ( * args , * * kwargs ) return func return decorator
13448	def authed_get ( self , url , response_code = 200 , headers = { } , follow = False ) : if not self . authed : self . authorize ( ) response = self . client . get ( url , follow = follow , * * headers ) self . assertEqual ( response_code , response . status_code ) return response
5396	def _localize_inputs_command ( self , task_dir , inputs , user_project ) : commands = [ ] for i in inputs : if i . recursive or not i . value : continue source_file_path = i . uri local_file_path = task_dir + '/' + _DATA_SUBDIR + '/' + i . docker_path dest_file_path = self . _get_input_target_path ( local_file_path ) commands . append ( 'mkdir -p "%s"' % os . path . dirname ( local_file_path ) ) if i . file_provider in [ job_model . P_LOCAL , job_model . P_GCS ] : # The semantics that we expect here are implemented consistently in # "gsutil cp", and are a bit different than "cp" when it comes to # wildcard handling, so use it for both local and GCS: # # - `cp path/* dest/` will error if "path" has subdirectories. # - `cp "path/*" "dest/"` will fail (it expects wildcard expansion # to come from shell). if user_project : command = 'gsutil -u %s -mq cp "%s" "%s"' % ( user_project , source_file_path , dest_file_path ) else : command = 'gsutil -mq cp "%s" "%s"' % ( source_file_path , dest_file_path ) commands . append ( command ) return '\n' . join ( commands )
1571	def submit_tar ( cl_args , unknown_args , tmp_dir ) : # execute main of the topology to create the topology definition topology_file = cl_args [ 'topology-file-name' ] java_defines = cl_args [ 'topology_main_jvm_property' ] main_class = cl_args [ 'topology-class-name' ] res = execute . heron_tar ( main_class , topology_file , tuple ( unknown_args ) , tmp_dir , java_defines ) result . render ( res ) if not result . is_successful ( res ) : err_context = ( "Failed to create topology definition " "file when executing class '%s' of file '%s'" ) % ( main_class , topology_file ) res . add_context ( err_context ) return res return launch_topologies ( cl_args , topology_file , tmp_dir )
1271	def setup_components_and_tf_funcs ( self , custom_getter = None ) : # Create network before super-call, since non-empty internals_spec attribute (for RNN) is required subsequently. self . network = Network . from_spec ( spec = self . network_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) # Now that we have the network component: We can create the internals placeholders. assert len ( self . internals_spec ) == 0 self . internals_spec = self . network . internals_spec ( ) for name in sorted ( self . internals_spec ) : internal = self . internals_spec [ name ] self . internals_input [ name ] = tf . placeholder ( dtype = util . tf_dtype ( internal [ 'type' ] ) , shape = ( None , ) + tuple ( internal [ 'shape' ] ) , name = ( 'internal-' + name ) ) if internal [ 'initialization' ] == 'zeros' : self . internals_init [ name ] = np . zeros ( shape = internal [ 'shape' ] ) else : raise TensorForceError ( "Invalid internal initialization value." ) # And only then call super. custom_getter = super ( DistributionModel , self ) . setup_components_and_tf_funcs ( custom_getter ) # Distributions self . distributions = self . create_distributions ( ) # KL divergence function self . fn_kl_divergence = tf . make_template ( name_ = 'kl-divergence' , func_ = self . tf_kl_divergence , custom_getter_ = custom_getter ) return custom_getter
2366	def walk ( self , * types ) : requested = types if len ( types ) > 0 else [ SuiteFile , ResourceFile , SuiteFolder , Testcase , Keyword ] for thing in self . robot_files : if thing . __class__ in requested : yield thing if isinstance ( thing , SuiteFolder ) : for child in thing . walk ( ) : if child . __class__ in requested : yield child else : for child in thing . walk ( * types ) : yield child
5643	def get_min_visit_time ( self ) : if not self . visit_events : return float ( 'inf' ) else : return min ( self . visit_events , key = lambda event : event . arr_time_ut ) . arr_time_ut
1862	def SCAS ( cpu , dest , src ) : dest_reg = dest . reg mem_reg = src . mem . base # , src.type, src.read() size = dest . size arg0 = dest . read ( ) arg1 = src . read ( ) res = arg0 - arg1 cpu . _calculate_CMP_flags ( size , res , arg0 , arg1 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( mem_reg , cpu . read_register ( mem_reg ) + increment )
9981	def is_funcdef ( src ) : module_node = ast . parse ( dedent ( src ) ) if len ( module_node . body ) == 1 and isinstance ( module_node . body [ 0 ] , ast . FunctionDef ) : return True else : return False
13449	def authed_post ( self , url , data , response_code = 200 , follow = False , headers = { } ) : if not self . authed : self . authorize ( ) response = self . client . post ( url , data , follow = follow , * * headers ) self . assertEqual ( response_code , response . status_code ) return response
4629	def from_pubkey ( cls , pubkey , compressed = True , version = 56 , prefix = None ) : # Ensure this is a public key pubkey = PublicKey ( pubkey , prefix = prefix or Prefix . prefix ) if compressed : pubkey_plain = pubkey . compressed ( ) else : pubkey_plain = pubkey . uncompressed ( ) sha = hashlib . sha256 ( unhexlify ( pubkey_plain ) ) . hexdigest ( ) rep = hexlify ( ripemd160 ( sha ) ) . decode ( "ascii" ) s = ( "%.2x" % version ) + rep result = s + hexlify ( doublesha256 ( s ) [ : 4 ] ) . decode ( "ascii" ) result = hexlify ( ripemd160 ( result ) ) . decode ( "ascii" ) return cls ( result , prefix = pubkey . prefix )
6984	def timebinlc ( lcfile , binsizesec , outdir = None , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , minbinelems = 7 ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None # override the default timecols, magcols, and errcols # using the ones provided to the function if timecols is None : timecols = dtimecols if magcols is None : magcols = dmagcols if errcols is None : errcols = derrcols # get the LC into a dict lcdict = readerfunc ( lcfile ) # this should handle lists/tuples being returned by readerfunc # we assume that the first element is the actual lcdict # FIXME: figure out how to not need this assumption if ( ( isinstance ( lcdict , ( list , tuple ) ) ) and ( isinstance ( lcdict [ 0 ] , dict ) ) ) : lcdict = lcdict [ 0 ] # skip already binned light curves if 'binned' in lcdict : LOGERROR ( 'this light curve appears to be binned already, skipping...' ) return None lcdict [ 'binned' ] = { } for tcol , mcol , ecol in zip ( timecols , magcols , errcols ) : # dereference the columns and get them from the lcdict if '.' in tcol : tcolget = tcol . split ( '.' ) else : tcolget = [ tcol ] times = _dict_get ( lcdict , tcolget ) if '.' in mcol : mcolget = mcol . split ( '.' ) else : mcolget = [ mcol ] mags = _dict_get ( lcdict , mcolget ) if '.' in ecol : ecolget = ecol . split ( '.' ) else : ecolget = [ ecol ] errs = _dict_get ( lcdict , ecolget ) # normalize here if not using special normalization if normfunc is None : ntimes , nmags = normalize_magseries ( times , mags , magsarefluxes = magsarefluxes ) times , mags , errs = ntimes , nmags , errs # now bin the mag series as requested binned = time_bin_magseries_with_errs ( times , mags , errs , binsize = binsizesec , minbinelems = minbinelems ) # put this into the special binned key of the lcdict lcdict [ 'binned' ] [ mcol ] = { 'times' : binned [ 'binnedtimes' ] , 'mags' : binned [ 'binnedmags' ] , 'errs' : binned [ 'binnederrs' ] , 'nbins' : binned [ 'nbins' ] , 'timebins' : binned [ 'jdbins' ] , 'binsizesec' : binsizesec } # done with binning for all magcols, now generate the output file # this will always be a pickle if outdir is None : outdir = os . path . dirname ( lcfile ) outfile = os . path . join ( outdir , '%s-binned%.1fsec-%s.pkl' % ( squeeze ( lcdict [ 'objectid' ] ) . replace ( ' ' , '-' ) , binsizesec , lcformat ) ) with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) return outfile
496	def _classifyState ( self , state ) : # Record is before wait period do not classifiy if state . ROWID < self . getParameter ( 'trainRecords' ) : if not state . setByUser : state . anomalyLabel = [ ] self . _deleteRecordsFromKNN ( [ state ] ) return label = KNNAnomalyClassifierRegion . AUTO_THRESHOLD_CLASSIFIED_LABEL autoLabel = label + KNNAnomalyClassifierRegion . AUTO_TAG # Update the label based on classifications newCategory = self . _recomputeRecordFromKNN ( state ) labelList = self . _categoryToLabelList ( newCategory ) if state . setByUser : if label in state . anomalyLabel : state . anomalyLabel . remove ( label ) if autoLabel in state . anomalyLabel : state . anomalyLabel . remove ( autoLabel ) labelList . extend ( state . anomalyLabel ) # Add threshold classification label if above threshold, else if # classified to add the auto threshold classification. if state . anomalyScore >= self . getParameter ( 'anomalyThreshold' ) : labelList . append ( label ) elif label in labelList : ind = labelList . index ( label ) labelList [ ind ] = autoLabel # Make all entries unique labelList = list ( set ( labelList ) ) # If both above threshold and auto classified above - remove auto label if label in labelList and autoLabel in labelList : labelList . remove ( autoLabel ) if state . anomalyLabel == labelList : return # Update state's labeling state . anomalyLabel = labelList # Update KNN Classifier with new labeling if state . anomalyLabel == [ ] : self . _deleteRecordsFromKNN ( [ state ] ) else : self . _addRecordToKNN ( state )
74	def EdgeDetect ( alpha = 0 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ 0 , 1 , 0 ] , [ 1 , - 4 , 1 ] , [ 0 , 1 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
3668	def K_value ( P = None , Psat = None , phi_l = None , phi_g = None , gamma = None , Poynting = 1 ) : try : if gamma : if phi_l : return gamma * Psat * phi_l * Poynting / ( phi_g * P ) return gamma * Psat * Poynting / P elif phi_l : return phi_l / phi_g return Psat / P except TypeError : raise Exception ( 'Input must consist of one set from (P, Psat, phi_l, \ phi_g, gamma), (P, Psat, gamma), (phi_l, phi_g), (P, Psat)' )
4980	def get ( self , request , enterprise_uuid , course_id ) : enrollment_course_mode = request . GET . get ( 'course_mode' ) enterprise_catalog_uuid = request . GET . get ( 'catalog' ) # Redirect the learner to LMS dashboard in case no course mode is # provided as query parameter `course_mode` if not enrollment_course_mode : return redirect ( LMS_DASHBOARD_URL ) enrollment_api_client = EnrollmentApiClient ( ) course_modes = enrollment_api_client . get_course_modes ( course_id ) # Verify that the request user belongs to the enterprise against the # provided `enterprise_uuid`. enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) enterprise_customer_user = get_enterprise_customer_user ( request . user . id , enterprise_customer . uuid ) if not course_modes : context_data = get_global_context ( request , enterprise_customer ) error_code = 'ENTHCE000' log_message = ( 'No course_modes for course_id {course_id} for enterprise_catalog_uuid ' '{enterprise_catalog_uuid}.' 'The following error was presented to ' 'user {userid}: {error_code}' . format ( userid = request . user . id , enterprise_catalog_uuid = enterprise_catalog_uuid , course_id = course_id , error_code = error_code ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) selected_course_mode = None for course_mode in course_modes : if course_mode [ 'slug' ] == enrollment_course_mode : selected_course_mode = course_mode break if not selected_course_mode : return redirect ( LMS_DASHBOARD_URL ) # Create the Enterprise backend database records for this course # enrollment __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'course-landing-page-enrollment' , request . user . id , course_id , request . get_full_path ( ) ) DataSharingConsent . objects . update_or_create ( username = enterprise_customer_user . username , course_id = course_id , enterprise_customer = enterprise_customer_user . enterprise_customer , defaults = { 'granted' : True } , ) audit_modes = getattr ( settings , 'ENTERPRISE_COURSE_ENROLLMENT_AUDIT_MODES' , [ 'audit' , 'honor' ] ) if selected_course_mode [ 'slug' ] in audit_modes : # In case of Audit course modes enroll the learner directly through # enrollment API client and redirect the learner to dashboard. enrollment_api_client . enroll_user_in_course ( request . user . username , course_id , selected_course_mode [ 'slug' ] ) return redirect ( LMS_COURSEWARE_URL . format ( course_id = course_id ) ) # redirect the enterprise learner to the ecommerce flow in LMS # Note: LMS start flow automatically detects the paid mode premium_flow = LMS_START_PREMIUM_COURSE_FLOW_URL . format ( course_id = course_id ) if enterprise_catalog_uuid : premium_flow += '?catalog={catalog_uuid}' . format ( catalog_uuid = enterprise_catalog_uuid ) return redirect ( premium_flow )
6558	def add_constraint ( self , constraint , variables = tuple ( ) ) : if isinstance ( constraint , Constraint ) : if variables and ( tuple ( variables ) != constraint . variables ) : raise ValueError ( "mismatched variables and Constraint" ) elif isinstance ( constraint , Callable ) : constraint = Constraint . from_func ( constraint , variables , self . vartype ) elif isinstance ( constraint , Iterable ) : constraint = Constraint . from_configurations ( constraint , variables , self . vartype ) else : raise TypeError ( "Unknown constraint type given" ) self . constraints . append ( constraint ) for v in constraint . variables : self . variables [ v ] . append ( constraint )
872	def _setPath ( cls ) : cls . _path = os . path . join ( os . environ [ 'NTA_DYNAMIC_CONF_DIR' ] , cls . customFileName )
6037	def scaled_array_2d_from_array_1d ( self , array_1d ) : return scaled_array . ScaledSquarePixelArray ( array = self . array_2d_from_array_1d ( array_1d ) , pixel_scale = self . mask . pixel_scale , origin = self . mask . origin )
11138	def __clean_before_after ( self , stateBefore , stateAfter , keepNoneEmptyDirectory = True ) : # prepare after for faster search errors = [ ] afterDict = { } [ afterDict . setdefault ( list ( aitem ) [ 0 ] , [ ] ) . append ( aitem ) for aitem in stateAfter ] # loop before for bitem in reversed ( stateBefore ) : relaPath = list ( bitem ) [ 0 ] basename = os . path . basename ( relaPath ) btype = bitem [ relaPath ] [ 'type' ] alist = afterDict . get ( relaPath , [ ] ) aitem = [ a for a in alist if a [ relaPath ] [ 'type' ] == btype ] if len ( aitem ) > 1 : errors . append ( "Multiple '%s' of type '%s' where found in '%s', this should never had happened. Please report issue" % ( basename , btype , relaPath ) ) continue if not len ( aitem ) : removeDirs = [ ] removeFiles = [ ] if btype == 'dir' : if not len ( relaPath ) : errors . append ( "Removing main repository directory is not allowed" ) continue removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirLock ) ) elif btype == 'file' : removeFiles . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileLock % basename ) ) else : ### MUST VERIFY THAT ONCE pyrepobjectdir IS IMPLEMENTED removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) # remove files for fpath in removeFiles : if os . path . isfile ( fpath ) : try : os . remove ( fpath ) except Exception as err : errors . append ( "Unable to clean file '%s' (%s)" % ( fpath , str ( err ) ) ) # remove directories for dpath in removeDirs : if os . path . isdir ( dpath ) : if keepNoneEmptyDirectory or not len ( os . listdir ( dpath ) ) : try : shutil . rmtree ( dpath ) except Exception as err : errors . append ( "Unable to clean directory '%s' (%s)" % ( fpath , str ( err ) ) ) # return result and errors list return len ( errors ) == 0 , errors
4974	def verify_edx_resources ( ) : required_methods = { 'ProgramDataExtender' : ProgramDataExtender , } for method in required_methods : if required_methods [ method ] is None : raise NotConnectedToOpenEdX ( _ ( "The following method from the Open edX platform is necessary for this view but isn't available." ) + "\nUnavailable: {method}" . format ( method = method ) )
6357	def encode ( self , word ) : def _to_regex ( pattern , left_match = True ) : new_pattern = '' replacements = { '#' : '[AEIOU]+' , ':' : '[BCDFGHJKLMNPQRSTVWXYZ]*' , '^' : '[BCDFGHJKLMNPQRSTVWXYZ]' , '.' : '[BDVGJLMNTWZ]' , '%' : '(ER|E|ES|ED|ING|ELY)' , '+' : '[EIY]' , ' ' : '^' , } for char in pattern : new_pattern += ( replacements [ char ] if char in replacements else char ) if left_match : new_pattern += '$' if '^' not in pattern : new_pattern = '^.*' + new_pattern else : new_pattern = '^' + new_pattern . replace ( '^' , '$' ) if '$' not in new_pattern : new_pattern += '.*$' return new_pattern word = word . upper ( ) pron = '' pos = 0 while pos < len ( word ) : left_orig = word [ : pos ] right_orig = word [ pos : ] first = word [ pos ] if word [ pos ] in self . _rules else ' ' for rule in self . _rules [ first ] : left , match , right , out = rule if right_orig . startswith ( match ) : if left : l_pattern = _to_regex ( left , left_match = True ) if right : r_pattern = _to_regex ( right , left_match = False ) if ( not left or re_match ( l_pattern , left_orig ) ) and ( not right or re_match ( r_pattern , right_orig [ len ( match ) : ] ) ) : pron += out pos += len ( match ) break else : pron += word [ pos ] pos += 1 return pron
9089	def _iterate_namespace_models ( self , * * kwargs ) -> Iterable : return tqdm ( self . _get_query ( self . namespace_model ) , total = self . _count_model ( self . namespace_model ) , * * kwargs )
6950	def jhk_to_sdssi ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSI_JHK , SDSSI_JH , SDSSI_JK , SDSSI_HK , SDSSI_J , SDSSI_H , SDSSI_K )
11910	def bump_version ( version , which = None ) : try : parts = [ int ( n ) for n in version . split ( '.' ) ] except ValueError : fail ( 'Current version is not numeric' ) if len ( parts ) != 3 : fail ( 'Current version is not semantic versioning' ) # Determine where to increment the version number PARTS = { 'major' : 0 , 'minor' : 1 , 'patch' : 2 } index = PARTS [ which ] if which in PARTS else 2 # Increment the version number at that index and set the subsequent parts # to 0. before , middle , after = parts [ : index ] , parts [ index ] , parts [ index + 1 : ] middle += 1 return '.' . join ( str ( n ) for n in before + [ middle ] + after )
12640	def get_unique_field_values_per_group ( self , field_name , field_to_use_as_key = None ) : unique_vals = DefaultOrderedDict ( set ) for dcmg in self . dicom_groups : for f in self . dicom_groups [ dcmg ] : field_val = DicomFile ( f ) . get_attributes ( field_name ) key_val = dcmg if field_to_use_as_key is not None : try : key_val = str ( DicomFile ( dcmg ) . get_attributes ( field_to_use_as_key ) ) except KeyError as ke : raise KeyError ( 'Error getting field {} from ' 'file {}' . format ( field_to_use_as_key , dcmg ) ) from ke unique_vals [ key_val ] . add ( field_val ) return unique_vals
5830	def create ( self , configuration , name , description ) : data = { "configuration" : configuration , "name" : name , "description" : description } failure_message = "Dataview creation failed" result = self . _get_success_json ( self . _post_json ( 'v1/data_views' , data , failure_message = failure_message ) ) data_view_id = result [ 'data' ] [ 'id' ] return data_view_id
4338	def phaser ( self , gain_in = 0.8 , gain_out = 0.74 , delay = 3 , decay = 0.4 , speed = 0.5 , modulation_shape = 'sinusoidal' ) : if not is_number ( gain_in ) or gain_in <= 0 or gain_in > 1 : raise ValueError ( "gain_in must be a number between 0 and 1." ) if not is_number ( gain_out ) or gain_out <= 0 or gain_out > 1 : raise ValueError ( "gain_out must be a number between 0 and 1." ) if not is_number ( delay ) or delay <= 0 or delay > 5 : raise ValueError ( "delay must be a positive number." ) if not is_number ( decay ) or decay < 0.1 or decay > 0.5 : raise ValueError ( "decay must be a number between 0.1 and 0.5." ) if not is_number ( speed ) or speed < 0.1 or speed > 2 : raise ValueError ( "speed must be a positive number." ) if modulation_shape not in [ 'sinusoidal' , 'triangular' ] : raise ValueError ( "modulation_shape must be one of 'sinusoidal', 'triangular'." ) effect_args = [ 'phaser' , '{:f}' . format ( gain_in ) , '{:f}' . format ( gain_out ) , '{:f}' . format ( delay ) , '{:f}' . format ( decay ) , '{:f}' . format ( speed ) ] if modulation_shape == 'sinusoidal' : effect_args . append ( '-s' ) elif modulation_shape == 'triangular' : effect_args . append ( '-t' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'phaser' ) return self
7516	def init_arrays ( data ) : ## get stats from step6 h5 and create new h5 co5 = h5py . File ( data . clust_database , 'r' ) io5 = h5py . File ( data . database , 'w' ) ## get maxlen and chunk len maxlen = data . _hackersonly [ "max_fragment_length" ] + 20 chunks = co5 [ "seqs" ] . attrs [ "chunksize" ] [ 0 ] nloci = co5 [ "seqs" ] . shape [ 0 ] ## make array for snp string, 2 cols, - and * snps = io5 . create_dataset ( "snps" , ( nloci , maxlen , 2 ) , dtype = np . bool , chunks = ( chunks , maxlen , 2 ) , compression = 'gzip' ) snps . attrs [ "chunksize" ] = chunks snps . attrs [ "names" ] = [ "-" , "*" ] ## array for filters that will be applied in step7 filters = io5 . create_dataset ( "filters" , ( nloci , 6 ) , dtype = np . bool ) filters . attrs [ "filters" ] = [ "duplicates" , "max_indels" , "max_snps" , "max_shared_hets" , "min_samps" , "max_alleles" ] ## array for edgetrimming edges = io5 . create_dataset ( "edges" , ( nloci , 5 ) , dtype = np . uint16 , chunks = ( chunks , 5 ) , compression = "gzip" ) edges . attrs [ "chunksize" ] = chunks edges . attrs [ "names" ] = [ "R1_L" , "R1_R" , "R2_L" , "R2_R" , "sep" ] ## xfer data from clustdb to finaldb edges [ : , 4 ] = co5 [ "splits" ] [ : ] filters [ : , 0 ] = co5 [ "duplicates" ] [ : ] ## close h5s io5 . close ( ) co5 . close ( )
5696	def copy ( cls , conn , * * where ) : cur = conn . cursor ( ) if where and cls . copy_where : copy_where = cls . copy_where . format ( * * where ) # print(copy_where) else : copy_where = '' cur . execute ( 'INSERT INTO %s ' 'SELECT * FROM source.%s %s' % ( cls . table , cls . table , copy_where ) )
12717	def angles ( self ) : return [ self . ode_obj . getAngle ( i ) for i in range ( self . ADOF ) ]
7825	def feature_uri ( uri ) : def decorator ( class_ ) : """Returns a decorated class""" if "_pyxmpp_feature_uris" not in class_ . __dict__ : class_ . _pyxmpp_feature_uris = set ( ) class_ . _pyxmpp_feature_uris . add ( uri ) return class_ return decorator
426	def check_unfinished_task ( self , task_name = None , * * kwargs ) : if not isinstance ( task_name , str ) : # is None: raise Exception ( "task_name should be string" ) self . _fill_project_info ( kwargs ) kwargs . update ( { '$or' : [ { 'status' : 'pending' } , { 'status' : 'running' } ] } ) # ## find task # task = self.db.Task.find_one(kwargs) task = self . db . Task . find ( kwargs ) task_id_list = task . distinct ( '_id' ) n_task = len ( task_id_list ) if n_task == 0 : logging . info ( "[Database] No unfinished task - task_name: {}" . format ( task_name ) ) return False else : logging . info ( "[Database] Find {} unfinished task - task_name: {}" . format ( n_task , task_name ) ) return True
6770	def install_yum ( self , fn = None , package_name = None , update = 0 , list_only = 0 ) : assert self . genv [ ROLE ] yum_req_fn = fn or self . find_template ( self . genv . yum_requirments_fn ) if not yum_req_fn : return [ ] assert os . path . isfile ( yum_req_fn ) update = int ( update ) if list_only : return [ _ . strip ( ) for _ in open ( yum_req_fn ) . readlines ( ) if _ . strip ( ) and not _ . strip . startswith ( '#' ) and ( not package_name or _ . strip ( ) == package_name ) ] if update : self . sudo_or_dryrun ( 'yum update --assumeyes' ) if package_name : self . sudo_or_dryrun ( 'yum install --assumeyes %s' % package_name ) else : if self . genv . is_local : self . put_or_dryrun ( local_path = yum_req_fn ) yum_req_fn = self . genv . put_remote_fn self . sudo_or_dryrun ( 'yum install --assumeyes $(cat %(yum_req_fn)s)' % yum_req_fn )
13043	def create_pipe_workers ( configfile , directory ) : type_map = { 'service' : ServiceSearch , 'host' : HostSearch , 'range' : RangeSearch , 'user' : UserSearch } config = configparser . ConfigParser ( ) config . read ( configfile ) if not len ( config . sections ( ) ) : print_error ( "No named pipes configured" ) return print_notification ( "Starting {} pipes in directory {}" . format ( len ( config . sections ( ) ) , directory ) ) workers = [ ] for name in config . sections ( ) : section = config [ name ] query = create_query ( section ) object_type = type_map [ section [ 'type' ] ] args = ( name , os . path . join ( directory , name ) , object_type , query , section [ 'format' ] , bool ( section . get ( 'unique' , 0 ) ) ) workers . append ( multiprocessing . Process ( target = pipe_worker , args = args ) ) return workers
10972	def requests ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) memberships = Membership . query_requests ( current_user , eager = True ) . all ( ) return render_template ( 'invenio_groups/pending.html' , memberships = memberships , requests = True , page = page , per_page = per_page , )
8672	def list_keys ( key_name , max_suggestions , cutoff , jsonify , locked , key_type , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase , quiet = jsonify ) try : keys = stash . list ( key_name = key_name , max_suggestions = max_suggestions , cutoff = cutoff , locked_only = locked , key_type = key_type ) except GhostError as ex : sys . exit ( ex ) if jsonify : click . echo ( json . dumps ( keys , indent = 4 , sort_keys = True ) ) elif not keys : click . echo ( 'The stash is empty. Go on, put some keys in there...' ) else : click . echo ( 'Listing all keys...' ) click . echo ( _prettify_list ( keys ) )
6227	def _translate_string ( self , data , length ) : for index , char in enumerate ( data ) : if index == length : break yield self . _meta . characters - 1 - self . _ct [ char ]
7455	def _cleanup_and_die ( data ) : tmpfiles = glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*_R*.fastq" ) ) tmpfiles += glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*.p" ) ) for tmpf in tmpfiles : os . remove ( tmpf )
7320	def make_message_multipart ( message ) : if not message . is_multipart ( ) : multipart_message = email . mime . multipart . MIMEMultipart ( 'alternative' ) for header_key in set ( message . keys ( ) ) : # Preserve duplicate headers values = message . get_all ( header_key , failobj = [ ] ) for value in values : multipart_message [ header_key ] = value original_text = message . get_payload ( ) multipart_message . attach ( email . mime . text . MIMEText ( original_text ) ) message = multipart_message # HACK: For Python2 (see comments in `_create_boundary`) message = _create_boundary ( message ) return message
9294	def python_value ( self , value ) : value = coerce_to_bytes ( value ) obj = HashValue ( value ) obj . field = self return obj
10924	def fit_comp ( new_comp , old_comp , * * kwargs ) : #resetting the category to ilm: new_cat = new_comp . category new_comp . category = 'ilm' fake_s = states . ImageState ( Image ( old_comp . get ( ) . copy ( ) ) , [ new_comp ] , pad = 0 , mdl = mdl . SmoothFieldModel ( ) ) do_levmarq ( fake_s , new_comp . params , * * kwargs ) new_comp . category = new_cat
5939	def _combineargs ( self , * args , * * kwargs ) : d = { arg : True for arg in args } # switches are kwargs with value True d . update ( kwargs ) return d
1155	def pop ( self ) : it = iter ( self ) try : value = next ( it ) except StopIteration : raise KeyError self . discard ( value ) return value
5832	def get ( self , data_view_id ) : failure_message = "Dataview get failed" return self . _get_success_json ( self . _get ( 'v1/data_views/' + data_view_id , None , failure_message = failure_message ) ) [ 'data' ] [ 'data_view' ]
3372	def choose_solver ( model , solver = None , qp = False ) : if solver is None : solver = model . problem else : model . solver = solver # Check for QP, raise error if no QP solver found if qp and interface_to_str ( solver ) not in qp_solvers : solver = solvers [ get_solver_name ( qp = True ) ] return solver
3434	def _populate_solver ( self , reaction_list , metabolite_list = None ) : constraint_terms = AutoVivification ( ) to_add = [ ] if metabolite_list is not None : for met in metabolite_list : to_add += [ self . problem . Constraint ( Zero , name = met . id , lb = 0 , ub = 0 ) ] self . add_cons_vars ( to_add ) for reaction in reaction_list : if reaction . id not in self . variables : forward_variable = self . problem . Variable ( reaction . id ) reverse_variable = self . problem . Variable ( reaction . reverse_id ) self . add_cons_vars ( [ forward_variable , reverse_variable ] ) else : reaction = self . reactions . get_by_id ( reaction . id ) forward_variable = reaction . forward_variable reverse_variable = reaction . reverse_variable for metabolite , coeff in six . iteritems ( reaction . metabolites ) : if metabolite . id in self . constraints : constraint = self . constraints [ metabolite . id ] else : constraint = self . problem . Constraint ( Zero , name = metabolite . id , lb = 0 , ub = 0 ) self . add_cons_vars ( constraint , sloppy = True ) constraint_terms [ constraint ] [ forward_variable ] = coeff constraint_terms [ constraint ] [ reverse_variable ] = - coeff self . solver . update ( ) for reaction in reaction_list : reaction = self . reactions . get_by_id ( reaction . id ) reaction . update_variable_bounds ( ) for constraint , terms in six . iteritems ( constraint_terms ) : constraint . set_linear_coefficients ( terms )
8424	def husl_palette ( n_colors = 6 , h = .01 , s = .9 , l = .65 ) : hues = np . linspace ( 0 , 1 , n_colors + 1 ) [ : - 1 ] hues += h hues %= 1 hues *= 359 s *= 99 l *= 99 palette = [ husl . husl_to_rgb ( h_i , s , l ) for h_i in hues ] return palette
8466	def run ( self ) : options = { } if bool ( self . config [ 'use_proxy' ] ) : options [ 'proxies' ] = { "http" : self . config [ 'proxy' ] , "https" : self . config [ 'proxy' ] } options [ "url" ] = self . config [ 'url' ] options [ "data" ] = { "issues" : json . dumps ( map ( lambda x : x . __todict__ ( ) , self . issues ) ) } if 'get' == self . config [ 'method' ] . lower ( ) : requests . get ( * * options ) else : requests . post ( * * options )
13809	def get_version ( relpath ) : from os . path import dirname , join if '__file__' not in globals ( ) : # Allow to use function interactively root = '.' else : root = dirname ( __file__ ) # The code below reads text file with unknown encoding in # in Python2/3 compatible way. Reading this text file # without specifying encoding will fail in Python 3 on some # systems (see http://goo.gl/5XmOH). Specifying encoding as # open() parameter is incompatible with Python 2 # cp437 is the encoding without missing points, safe against: # UnicodeDecodeError: 'charmap' codec can't decode byte... for line in open ( join ( root , relpath ) , 'rb' ) : line = line . decode ( 'cp437' ) if '__version__' in line : if '"' in line : # __version__ = "0.9" return line . split ( '"' ) [ 1 ] elif "'" in line : return line . split ( "'" ) [ 1 ]
10124	def flip_x ( self , center = None ) : if center is None : self . poly . flip ( ) else : self . poly . flip ( center [ 0 ] )
4726	def get_chunk_meta_item ( self , chunk_meta , grp , pug , chk ) : num_chk = self . envs [ "NUM_CHK" ] num_pu = self . envs [ "NUM_PU" ] index = grp * num_pu * num_chk + pug * num_chk + chk return chunk_meta [ index ]
1357	def get_argument_topology ( self ) : try : topology = self . get_argument ( constants . PARAM_TOPOLOGY ) return topology except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
1025	def quote ( c ) : i = ord ( c ) return ESCAPE + HEX [ i // 16 ] + HEX [ i % 16 ]
9505	def intersects ( self , i ) : return self . start <= i . end and i . start <= self . end
7749	def process_iq ( self , stanza ) : typ = stanza . stanza_type if typ in ( "result" , "error" ) : return self . _process_iq_response ( stanza ) if typ not in ( "get" , "set" ) : raise BadRequestProtocolError ( "Bad <iq/> type" ) logger . debug ( "Handling <iq type='{0}'> stanza: {1!r}" . format ( stanza , typ ) ) payload = stanza . get_payload ( None ) logger . debug ( " payload: {0!r}" . format ( payload ) ) if not payload : raise BadRequestProtocolError ( "<iq/> stanza with no child element" ) handler = self . _get_iq_handler ( typ , payload ) if not handler : payload = stanza . get_payload ( None , specialize = True ) logger . debug ( " specialized payload: {0!r}" . format ( payload ) ) if not isinstance ( payload , XMLPayload ) : handler = self . _get_iq_handler ( typ , payload ) if handler : response = handler ( stanza ) self . _process_handler_result ( response ) return True else : raise ServiceUnavailableProtocolError ( "Not implemented" )
5453	def _remove_empty_items ( d , required ) : new_dict = { } for k , v in d . items ( ) : if k in required : new_dict [ k ] = v elif isinstance ( v , int ) or v : # "if v" would suppress emitting int(0) new_dict [ k ] = v return new_dict
12659	def import_pyfile ( filepath , mod_name = None ) : import sys if sys . version_info . major == 3 : import importlib . machinery loader = importlib . machinery . SourceFileLoader ( '' , filepath ) mod = loader . load_module ( mod_name ) else : import imp mod = imp . load_source ( mod_name , filepath ) return mod
7288	def get_field_value ( self , field_key ) : def get_value ( document , field_key ) : # Short circuit the function if we do not have a document if document is None : return None current_key , new_key_array = trim_field_key ( document , field_key ) key_array_digit = int ( new_key_array [ - 1 ] ) if new_key_array and has_digit ( new_key_array ) else None new_key = make_key ( new_key_array ) if key_array_digit is not None and len ( new_key_array ) > 0 : # Handleing list fields if len ( new_key_array ) == 1 : return_data = document . _data . get ( current_key , [ ] ) elif isinstance ( document , BaseList ) : return_list = [ ] if len ( document ) > 0 : return_list = [ get_value ( doc , new_key ) for doc in document ] return_data = return_list else : return_data = get_value ( getattr ( document , current_key ) , new_key ) elif len ( new_key_array ) > 0 : return_data = get_value ( document . _data . get ( current_key ) , new_key ) else : # Handeling all other fields and id try : # Added try except otherwise we get "TypeError: getattr(): attribute name must be string" error from mongoengine/base/datastructures.py return_data = ( document . _data . get ( None , None ) if current_key == "id" else document . _data . get ( current_key , None ) ) except : return_data = document . _data . get ( current_key , None ) return return_data if self . is_initialized : return get_value ( self . model_instance , field_key ) else : return None
9439	def heartbeat ( ) : print "We got a call heartbeat notification\n" if request . method == 'POST' : print request . form else : print request . args return "OK"
11833	def make_undirected ( self ) : for a in self . dict . keys ( ) : for ( b , distance ) in self . dict [ a ] . items ( ) : self . connect1 ( b , a , distance )
10031	def execute ( helper , config , args ) : env = parse_env_config ( config , args . environment ) option_settings = env . get ( 'option_settings' , { } ) settings = parse_option_settings ( option_settings ) for setting in settings : out ( str ( setting ) )
3424	def get_solution ( model , reactions = None , metabolites = None , raise_error = False ) : check_solver_status ( model . solver . status , raise_error = raise_error ) if reactions is None : reactions = model . reactions if metabolites is None : metabolites = model . metabolites rxn_index = list ( ) fluxes = empty ( len ( reactions ) ) reduced = empty ( len ( reactions ) ) var_primals = model . solver . primal_values shadow = empty ( len ( metabolites ) ) if model . solver . is_integer : reduced . fill ( nan ) shadow . fill ( nan ) for ( i , rxn ) in enumerate ( reactions ) : rxn_index . append ( rxn . id ) fluxes [ i ] = var_primals [ rxn . id ] - var_primals [ rxn . reverse_id ] met_index = [ met . id for met in metabolites ] else : var_duals = model . solver . reduced_costs for ( i , rxn ) in enumerate ( reactions ) : forward = rxn . id reverse = rxn . reverse_id rxn_index . append ( forward ) fluxes [ i ] = var_primals [ forward ] - var_primals [ reverse ] reduced [ i ] = var_duals [ forward ] - var_duals [ reverse ] met_index = list ( ) constr_duals = model . solver . shadow_prices for ( i , met ) in enumerate ( metabolites ) : met_index . append ( met . id ) shadow [ i ] = constr_duals [ met . id ] return Solution ( model . solver . objective . value , model . solver . status , Series ( index = rxn_index , data = fluxes , name = "fluxes" ) , Series ( index = rxn_index , data = reduced , name = "reduced_costs" ) , Series ( index = met_index , data = shadow , name = "shadow_prices" ) )
11328	def select ( options = None ) : if not options : return None width = len ( str ( len ( options ) ) ) for x , option in enumerate ( options ) : sys . stdout . write ( '{:{width}}) {}\n' . format ( x + 1 , option , width = width ) ) sys . stdout . write ( '{:>{width}} ' . format ( '#?' , width = width + 1 ) ) sys . stdout . flush ( ) if sys . stdin . isatty ( ) : # regular prompt try : response = raw_input ( ) . strip ( ) except ( EOFError , KeyboardInterrupt ) : # handle ctrl-d, ctrl-c response = '' else : # try connecting to current tty, when using pipes sys . stdin = open ( "/dev/tty" ) try : response = '' while True : response += sys . stdin . read ( 1 ) if response . endswith ( '\n' ) : break except ( EOFError , KeyboardInterrupt ) : sys . stdout . flush ( ) pass try : response = int ( response ) - 1 except ValueError : return None if response < 0 or response >= len ( options ) : return None return options [ response ]
7631	def get_dtypes ( ns_key ) : # First, get the schema if ns_key not in __NAMESPACE__ : raise NamespaceError ( 'Unknown namespace: {:s}' . format ( ns_key ) ) value_dtype = __get_dtype ( __NAMESPACE__ [ ns_key ] . get ( 'value' , { } ) ) confidence_dtype = __get_dtype ( __NAMESPACE__ [ ns_key ] . get ( 'confidence' , { } ) ) return value_dtype , confidence_dtype
5619	def execute ( mp , td_resampling = "nearest" , td_matching_method = "gdal" , td_matching_max_zoom = None , td_matching_precision = 8 , td_fallback_to_higher_zoom = False , clip_pixelbuffer = 0 , * * kwargs ) : # read clip geometry if "clip" in mp . params [ "input" ] : clip_geom = mp . open ( "clip" ) . read ( ) if not clip_geom : logger . debug ( "no clip data over tile" ) return "empty" else : clip_geom = [ ] with mp . open ( "raster" , matching_method = td_matching_method , matching_max_zoom = td_matching_max_zoom , matching_precision = td_matching_precision , fallback_to_higher_zoom = td_fallback_to_higher_zoom , resampling = td_resampling ) as raster : raster_data = raster . read ( ) if raster . is_empty ( ) or raster_data [ 0 ] . mask . all ( ) : logger . debug ( "raster empty" ) return "empty" if clip_geom : # apply original nodata mask and clip clipped = mp . clip ( np . where ( raster_data [ 0 ] . mask , mp . params [ "output" ] . nodata , raster_data ) , clip_geom , clip_buffer = clip_pixelbuffer , inverted = True ) return np . where ( clipped . mask , clipped , mp . params [ "output" ] . nodata ) else : return np . where ( raster_data [ 0 ] . mask , mp . params [ "output" ] . nodata , raster_data )
1871	def CWDE ( cpu ) : bit = Operators . EXTRACT ( cpu . AX , 15 , 1 ) cpu . EAX = Operators . SEXTEND ( cpu . AX , 16 , 32 ) cpu . EDX = Operators . SEXTEND ( bit , 1 , 32 )
4003	def registry_from_image ( image_name ) : if '/' not in image_name : # official image return constants . PUBLIC_DOCKER_REGISTRY prefix = image_name . split ( '/' ) [ 0 ] if '.' not in prefix : # user image on official repository, e.g. thieman/clojure return constants . PUBLIC_DOCKER_REGISTRY return prefix
5379	def build_pipeline ( cls , project , zones , min_cores , min_ram , disk_size , boot_disk_size , preemptible , accelerator_type , accelerator_count , image , script_name , envs , inputs , outputs , pipeline_name ) : if min_cores is None : min_cores = job_model . DEFAULT_MIN_CORES if min_ram is None : min_ram = job_model . DEFAULT_MIN_RAM if disk_size is None : disk_size = job_model . DEFAULT_DISK_SIZE if boot_disk_size is None : boot_disk_size = job_model . DEFAULT_BOOT_DISK_SIZE if preemptible is None : preemptible = job_model . DEFAULT_PREEMPTIBLE # Format the docker command docker_command = cls . _build_pipeline_docker_command ( script_name , inputs , outputs , envs ) # Pipelines inputParameters can be both simple name/value pairs which get # set as environment variables, as well as input file paths which the # Pipelines controller will automatically localize to the Pipeline VM. # In the ephemeralPipeline object, the inputParameters are only defined; # the values are passed in the pipelineArgs. # Pipelines outputParameters are only output file paths, which the # Pipelines controller can automatically de-localize after the docker # command completes. # The Pipelines API does not support recursive copy of file parameters, # so it is implemented within the dsub-generated pipeline. # Any inputs or outputs marked as "recursive" are completely omitted here; # their environment variables will be set in the docker command, and # recursive copy code will be generated there as well. # The Pipelines API does not accept empty environment variables. Set them to # empty in DOCKER_COMMAND instead. input_envs = [ { 'name' : SCRIPT_VARNAME } ] + [ { 'name' : env . name } for env in envs if env . value ] input_files = [ cls . _build_pipeline_input_file_param ( var . name , var . docker_path ) for var in inputs if not var . recursive and var . value ] # Outputs are an array of file parameters output_files = [ cls . _build_pipeline_file_param ( var . name , var . docker_path ) for var in outputs if not var . recursive and var . value ] # The ephemeralPipeline provides the template for the pipeline. # pyformat: disable return { 'ephemeralPipeline' : { 'projectId' : project , 'name' : pipeline_name , # Define the resources needed for this pipeline. 'resources' : { 'minimumCpuCores' : min_cores , 'minimumRamGb' : min_ram , 'bootDiskSizeGb' : boot_disk_size , 'preemptible' : preemptible , 'zones' : google_base . get_zones ( zones ) , 'acceleratorType' : accelerator_type , 'acceleratorCount' : accelerator_count , # Create a data disk that is attached to the VM and destroyed # when the pipeline terminates. 'disks' : [ { 'name' : 'datadisk' , 'autoDelete' : True , 'sizeGb' : disk_size , 'mountPoint' : providers_util . DATA_MOUNT_POINT , } ] , } , 'inputParameters' : input_envs + input_files , 'outputParameters' : output_files , 'docker' : { 'imageName' : image , 'cmd' : docker_command , } } }
10762	def _wait_for_connection ( self , port ) : connected = False max_tries = 10 num_tries = 0 wait_time = 0.5 while not connected or num_tries >= max_tries : time . sleep ( wait_time ) try : af = socket . AF_INET addr = ( '127.0.0.1' , port ) sock = socket . socket ( af , socket . SOCK_STREAM ) sock . connect ( addr ) except socket . error : if sock : sock . close ( ) num_tries += 1 continue connected = True if not connected : print ( "Error connecting to sphinx searchd" , file = sys . stderr )
6431	def dist_abs ( self , src , tar ) : if src == tar : return 6 if src == '' or tar == '' : return 0 src = list ( mra ( src ) ) tar = list ( mra ( tar ) ) if abs ( len ( src ) - len ( tar ) ) > 2 : return 0 length_sum = len ( src ) + len ( tar ) if length_sum < 5 : min_rating = 5 elif length_sum < 8 : min_rating = 4 elif length_sum < 12 : min_rating = 3 else : min_rating = 2 for _ in range ( 2 ) : new_src = [ ] new_tar = [ ] minlen = min ( len ( src ) , len ( tar ) ) for i in range ( minlen ) : if src [ i ] != tar [ i ] : new_src . append ( src [ i ] ) new_tar . append ( tar [ i ] ) src = new_src + src [ minlen : ] tar = new_tar + tar [ minlen : ] src . reverse ( ) tar . reverse ( ) similarity = 6 - max ( len ( src ) , len ( tar ) ) if similarity >= min_rating : return similarity return 0
12132	def linspace ( self , start , stop , n ) : if n == 1 : return [ start ] L = [ 0.0 ] * n nm1 = n - 1 nm1inv = 1.0 / nm1 for i in range ( n ) : L [ i ] = nm1inv * ( start * ( nm1 - i ) + stop * i ) return L
2914	def _inherit_data ( self ) : LOG . debug ( "'%s' inheriting data from '%s'" % ( self . get_name ( ) , self . parent . get_name ( ) ) , extra = dict ( data = self . parent . data ) ) self . set_data ( * * self . parent . data )
11741	def _compute_first ( self ) : for terminal in self . terminals : self . _first [ terminal ] . add ( terminal ) self . _first [ END_OF_INPUT ] . add ( END_OF_INPUT ) while True : changed = False for nonterminal , productions in self . nonterminals . items ( ) : for production in productions : new_first = self . first ( production . rhs ) if new_first - self . _first [ nonterminal ] : self . _first [ nonterminal ] |= new_first changed = True if not changed : break
11374	def license_is_oa ( license ) : for oal in OA_LICENSES : if re . search ( oal , license ) : return True return False
7789	def get_item ( self , address , state = 'fresh' ) : self . _lock . acquire ( ) try : item = self . _items . get ( address ) if not item : return None self . update_item ( item ) if _state_values [ state ] >= item . state_value : return item return None finally : self . _lock . release ( )
8741	def create_floatingip ( context , content ) : LOG . info ( 'create_floatingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) network_id = content . get ( 'floating_network_id' ) # TODO(blogan): Since the extension logic will reject any requests without # floating_network_id, is this still needed? if not network_id : raise n_exc . BadRequest ( resource = 'floating_ip' , msg = 'floating_network_id is required.' ) fixed_ip_address = content . get ( 'fixed_ip_address' ) ip_address = content . get ( 'floating_ip_address' ) port_id = content . get ( 'port_id' ) port = None port_fixed_ip = { } network = _get_network ( context , network_id ) if port_id : port = _get_port ( context , port_id ) fixed_ip = _get_fixed_ip ( context , fixed_ip_address , port ) port_fixed_ip = { port . id : { 'port' : port , 'fixed_ip' : fixed_ip } } flip = _allocate_ip ( context , network , port , ip_address , ip_types . FLOATING ) _create_flip ( context , flip , port_fixed_ip ) return v . _make_floating_ip_dict ( flip , port_id )
7938	def _connect ( self , addr , port , service ) : self . _dst_name = addr self . _dst_port = port family = None try : res = socket . getaddrinfo ( addr , port , socket . AF_UNSPEC , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) family = res [ 0 ] [ 0 ] sockaddr = res [ 0 ] [ 4 ] except socket . gaierror : family = None sockaddr = None if family is not None : if not port : raise ValueError ( "No port number given with literal IP address" ) self . _dst_service = None self . _family = family self . _dst_addrs = [ ( family , sockaddr ) ] self . _set_state ( "connect" ) elif service is not None : self . _dst_service = service self . _set_state ( "resolve-srv" ) self . _dst_name = addr elif port : self . _dst_nameports = [ ( self . _dst_name , self . _dst_port ) ] self . _dst_service = None self . _set_state ( "resolve-hostname" ) else : raise ValueError ( "No port number and no SRV service name given" )
9585	def write_compressed_var_array ( fd , array , name ) : bd = BytesIO ( ) write_var_array ( bd , array , name ) data = zlib . compress ( bd . getvalue ( ) ) bd . close ( ) # write array data elements (size info) fd . write ( struct . pack ( 'b3xI' , etypes [ 'miCOMPRESSED' ] [ 'n' ] , len ( data ) ) ) # write the compressed data fd . write ( data )
9061	def beta_covariance ( self ) : from numpy_sugar . linalg import ddot tX = self . _X [ "tX" ] Q = concatenate ( self . _QS [ 0 ] , axis = 1 ) S0 = self . _QS [ 1 ] D = self . v0 * S0 + self . v1 D = D . tolist ( ) + [ self . v1 ] * ( len ( self . _y ) - len ( D ) ) D = asarray ( D ) A = inv ( tX . T @ ( Q @ ddot ( 1 / D , Q . T @ tX ) ) ) VT = self . _X [ "VT" ] H = lstsq ( VT , A , rcond = None ) [ 0 ] return lstsq ( VT , H . T , rcond = None ) [ 0 ]
9249	def generate_log_between_tags ( self , older_tag , newer_tag ) : filtered_issues , filtered_pull_requests = self . filter_issues_for_tags ( newer_tag , older_tag ) older_tag_name = older_tag [ "name" ] if older_tag else self . detect_since_tag ( ) if not filtered_issues and not filtered_pull_requests : # do not generate an unreleased section if it would be empty return "" return self . generate_log_for_tag ( filtered_pull_requests , filtered_issues , newer_tag , older_tag_name )
10128	def update ( self , dt ) : self . translate ( dt * self . velocity ) self . rotate ( dt * self . angular_velocity )
1696	def clone ( self , num_clones ) : retval = [ ] for i in range ( num_clones ) : retval . append ( self . repartition ( self . get_num_partitions ( ) ) ) return retval
3925	def _update_tabs ( self ) : text = [ ] for num , widget in enumerate ( self . _widgets ) : palette = ( 'active_tab' if num == self . _tab_index else 'inactive_tab' ) text += [ ( palette , ' {} ' . format ( self . _widget_title [ widget ] ) ) , ( 'tab_background' , ' ' ) , ] self . _tabs . set_text ( text ) self . _frame . contents [ 'body' ] = ( self . _widgets [ self . _tab_index ] , None )
7806	def verify_jid_against_srv_name ( self , jid , srv_type ) : srv_prefix = u"_" + srv_type + u"." srv_prefix_l = len ( srv_prefix ) for srv in self . alt_names . get ( "SRVName" , [ ] ) : logger . debug ( "checking {0!r} against {1!r}" . format ( jid , srv ) ) if not srv . startswith ( srv_prefix ) : logger . debug ( "{0!r} does not start with {1!r}" . format ( srv , srv_prefix ) ) continue try : srv_jid = JID ( srv [ srv_prefix_l : ] ) except ValueError : continue if srv_jid == jid : logger . debug ( "Match!" ) return True return False
1579	def create_packet ( reqid , message ) : assert message . IsInitialized ( ) packet = '' # calculate the totla size of the packet incl. header typename = message . DESCRIPTOR . full_name datasize = HeronProtocol . get_size_to_pack_string ( typename ) + REQID . REQID_SIZE + HeronProtocol . get_size_to_pack_message ( message ) # first write out how much data is there as the header packet += HeronProtocol . pack_int ( datasize ) # next write the type string packet += HeronProtocol . pack_int ( len ( typename ) ) packet += typename # reqid packet += reqid . pack ( ) # add the proto packet += HeronProtocol . pack_int ( message . ByteSize ( ) ) packet += message . SerializeToString ( ) return OutgoingPacket ( packet )
11513	def move_item ( self , token , item_id , src_folder_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id parameters [ 'srcfolderid' ] = src_folder_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.item.move' , parameters ) return response
1001	def printComputeEnd ( self , output , learn = False ) : if self . verbosity >= 3 : print "----- computeEnd summary: " print "learn:" , learn print "numBurstingCols: %s, " % ( self . infActiveState [ 't' ] . min ( axis = 1 ) . sum ( ) ) , print "curPredScore2: %s, " % ( self . _internalStats [ 'curPredictionScore2' ] ) , print "curFalsePosScore: %s, " % ( self . _internalStats [ 'curFalsePositiveScore' ] ) , print "1-curFalseNegScore: %s, " % ( 1 - self . _internalStats [ 'curFalseNegativeScore' ] ) print "numSegments: " , self . getNumSegments ( ) , print "avgLearnedSeqLength: " , self . avgLearnedSeqLength print "----- infActiveState (%d on) ------" % ( self . infActiveState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . infActiveState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . infActiveState [ 't' ] ) print "----- infPredictedState (%d on)-----" % ( self . infPredictedState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . infPredictedState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . infPredictedState [ 't' ] ) print "----- lrnActiveState (%d on) ------" % ( self . lrnActiveState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . lrnActiveState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . lrnActiveState [ 't' ] ) print "----- lrnPredictedState (%d on)-----" % ( self . lrnPredictedState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . lrnPredictedState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . lrnPredictedState [ 't' ] ) print "----- cellConfidence -----" self . printActiveIndices ( self . cellConfidence [ 't' ] , andValues = True ) if self . verbosity >= 6 : self . printConfidence ( self . cellConfidence [ 't' ] ) print "----- colConfidence -----" self . printActiveIndices ( self . colConfidence [ 't' ] , andValues = True ) print "----- cellConfidence[t-1] for currently active cells -----" cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] self . printActiveIndices ( cc , andValues = True ) if self . verbosity == 4 : print "Cells, predicted segments only:" self . printCells ( predictedOnly = True ) elif self . verbosity >= 5 : print "Cells, all segments:" self . printCells ( predictedOnly = False ) print elif self . verbosity >= 1 : print "TM: learn:" , learn print "TM: active outputs(%d):" % len ( output . nonzero ( ) [ 0 ] ) , self . printActiveIndices ( output . reshape ( self . numberOfCols , self . cellsPerColumn ) )
4198	def get_short_module_name ( module_name , obj_name ) : parts = module_name . split ( '.' ) short_name = module_name for i in range ( len ( parts ) - 1 , 0 , - 1 ) : short_name = '.' . join ( parts [ : i ] ) try : exec ( 'from %s import %s' % ( short_name , obj_name ) ) except ImportError : # get the last working module name short_name = '.' . join ( parts [ : ( i + 1 ) ] ) break return short_name
9429	def extract ( self , member , path = None , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename if path is None : path = os . getcwd ( ) self . _extract_members ( [ member ] , path , pwd ) return os . path . join ( path , member )
3294	def is_locked ( self ) : if self . provider . lock_manager is None : return False return self . provider . lock_manager . is_url_locked ( self . get_ref_url ( ) )
6059	def numpy_array_2d_to_fits ( array_2d , file_path , overwrite = False ) : if overwrite and os . path . exists ( file_path ) : os . remove ( file_path ) new_hdr = fits . Header ( ) hdu = fits . PrimaryHDU ( np . flipud ( array_2d ) , new_hdr ) hdu . writeto ( file_path )
3924	def _on_return ( self , text ) : # Ignore if the user hasn't typed a message. if not text : return elif text . startswith ( '/image' ) and len ( text . split ( ' ' ) ) == 2 : # Temporary UI for testing image uploads filename = text . split ( ' ' ) [ 1 ] image_file = open ( filename , 'rb' ) text = '' else : image_file = None text = replace_emoticons ( text ) segments = hangups . ChatMessageSegment . from_str ( text ) self . _coroutine_queue . put ( self . _handle_send_message ( self . _conversation . send_message ( segments , image_file = image_file ) ) )
9856	def get_data ( self , * * kwargs ) : limit = int ( kwargs . get ( 'limit' , 288 ) ) end_date = kwargs . get ( 'end_date' , False ) if end_date and isinstance ( end_date , datetime . datetime ) : end_date = self . convert_datetime ( end_date ) if self . mac_address is not None : service_address = 'devices/%s' % self . mac_address self . api_instance . log ( 'SERVICE ADDRESS: %s' % service_address ) data = dict ( limit = limit ) # If endDate is left blank (not passed in), the most recent results will be returned. if end_date : data . update ( { 'endDate' : end_date } ) self . api_instance . log ( 'DATA:' ) self . api_instance . log ( data ) return self . api_instance . api_call ( service_address , * * data )
6580	def _ensure_started ( self ) : if self . _process and self . _process . poll ( ) is None : return if not getattr ( self , "_cmd" ) : raise RuntimeError ( "Player command is not configured" ) log . debug ( "Starting playback command: %r" , self . _cmd ) self . _process = SilentPopen ( self . _cmd ) self . _post_start ( )
11286	def flush ( self , line ) : # TODO -- maybe use echo? sys . stdout . write ( line ) sys . stdout . flush ( )
2501	def value_error ( self , key , bad_value ) : msg = ERROR_MESSAGES [ key ] . format ( bad_value ) self . logger . log ( msg ) self . error = True
1979	def wait ( self , readfds , writefds , timeout ) : logger . info ( "WAIT:" ) logger . info ( "\tProcess %d is going to wait for [ %r %r %r ]" , self . _current , readfds , writefds , timeout ) logger . info ( "\tProcess: %r" , self . procs ) logger . info ( "\tRunning: %r" , self . running ) logger . info ( "\tRWait: %r" , self . rwait ) logger . info ( "\tTWait: %r" , self . twait ) logger . info ( "\tTimers: %r" , self . timers ) for fd in readfds : self . rwait [ fd ] . add ( self . _current ) for fd in writefds : self . twait [ fd ] . add ( self . _current ) if timeout is not None : self . timers [ self . _current ] = self . clocks + timeout else : self . timers [ self . _current ] = None procid = self . _current # self.sched() next_index = ( self . running . index ( procid ) + 1 ) % len ( self . running ) self . _current = self . running [ next_index ] logger . info ( "\tTransfer control from process %d to %d" , procid , self . _current ) logger . info ( "\tREMOVING %r from %r. Current: %r" , procid , self . running , self . _current ) self . running . remove ( procid ) if self . _current not in self . running : logger . info ( "\tCurrent not running. Checking for timers..." ) self . _current = None if all ( [ x is None for x in self . timers ] ) : raise Deadlock ( ) self . check_timers ( )
253	def extract_round_trips ( transactions , portfolio_value = None ) : transactions = _groupby_consecutive ( transactions ) roundtrips = [ ] for sym , trans_sym in transactions . groupby ( 'symbol' ) : trans_sym = trans_sym . sort_index ( ) price_stack = deque ( ) dt_stack = deque ( ) trans_sym [ 'signed_price' ] = trans_sym . price * np . sign ( trans_sym . amount ) trans_sym [ 'abs_amount' ] = trans_sym . amount . abs ( ) . astype ( int ) for dt , t in trans_sym . iterrows ( ) : if t . price < 0 : warnings . warn ( 'Negative price detected, ignoring for' 'round-trip.' ) continue indiv_prices = [ t . signed_price ] * t . abs_amount if ( len ( price_stack ) == 0 ) or ( copysign ( 1 , price_stack [ - 1 ] ) == copysign ( 1 , t . amount ) ) : price_stack . extend ( indiv_prices ) dt_stack . extend ( [ dt ] * len ( indiv_prices ) ) else : # Close round-trip pnl = 0 invested = 0 cur_open_dts = [ ] for price in indiv_prices : if len ( price_stack ) != 0 and ( copysign ( 1 , price_stack [ - 1 ] ) != copysign ( 1 , price ) ) : # Retrieve first dt, stock-price pair from # stack prev_price = price_stack . popleft ( ) prev_dt = dt_stack . popleft ( ) pnl += - ( price + prev_price ) cur_open_dts . append ( prev_dt ) invested += abs ( prev_price ) else : # Push additional stock-prices onto stack price_stack . append ( price ) dt_stack . append ( dt ) roundtrips . append ( { 'pnl' : pnl , 'open_dt' : cur_open_dts [ 0 ] , 'close_dt' : dt , 'long' : price < 0 , 'rt_returns' : pnl / invested , 'symbol' : sym , } ) roundtrips = pd . DataFrame ( roundtrips ) roundtrips [ 'duration' ] = roundtrips [ 'close_dt' ] . sub ( roundtrips [ 'open_dt' ] ) if portfolio_value is not None : # Need to normalize so that we can join pv = pd . DataFrame ( portfolio_value , columns = [ 'portfolio_value' ] ) . assign ( date = portfolio_value . index ) roundtrips [ 'date' ] = roundtrips . close_dt . apply ( lambda x : x . replace ( hour = 0 , minute = 0 , second = 0 ) ) tmp = roundtrips . join ( pv , on = 'date' , lsuffix = '_' ) roundtrips [ 'returns' ] = tmp . pnl / tmp . portfolio_value roundtrips = roundtrips . drop ( 'date' , axis = 'columns' ) return roundtrips
9383	def parse ( self ) : file_status = True for infile in self . infile_list : file_status = file_status and naarad . utils . is_valid_file ( infile ) if not file_status : return False status = self . parse_xml_jtl ( self . aggregation_granularity ) gc . collect ( ) return status
10767	def submit_poll ( self , poll , * , request_policy = None ) : if poll . id is not None : raise ExistingPoll ( ) options = poll . options data = { 'title' : poll . title , 'options' : options , 'multi' : poll . multi , 'dupcheck' : poll . dupcheck , 'captcha' : poll . captcha } return self . _http_client . post ( self . _POLLS , data = data , request_policy = request_policy , cls = strawpoll . Poll )
13361	def save ( self ) : env_data = [ dict ( name = env . name , root = env . path ) for env in self ] encode = yaml . safe_dump ( env_data , default_flow_style = False ) with open ( self . path , 'w' ) as f : f . write ( encode )
1268	def _fire ( self , layers , things , the_plot ) : # We don't fire if the player fired another bolt just now. if the_plot . get ( 'last_player_shot' ) == the_plot . frame : return the_plot [ 'last_player_shot' ] = the_plot . frame # We start just above the player. row , col = things [ 'P' ] . position self . _teleport ( ( row - 1 , col ) )
4525	def get ( self , position = 0 ) : n = len ( self ) if n == 1 : return self [ 0 ] pos = position if self . length and self . autoscale : pos *= len ( self ) pos /= self . length pos *= self . scale pos += self . offset if not self . continuous : if not self . serpentine : return self [ int ( pos % n ) ] # We want a color sequence of length 2n-2 # e.g. for n=5: a b c d | e d c b | a b c d ... m = ( 2 * n ) - 2 pos %= m if pos < n : return self [ int ( pos ) ] else : return self [ int ( m - pos ) ] if self . serpentine : pos %= ( 2 * n ) if pos > n : pos = ( 2 * n ) - pos else : pos %= n # p is a number in [0, n): scale it to be in [0, n-1) pos *= n - 1 pos /= n index = int ( pos ) fade = pos - index if not fade : return self [ index ] r1 , g1 , b1 = self [ index ] r2 , g2 , b2 = self [ ( index + 1 ) % len ( self ) ] dr , dg , db = r2 - r1 , g2 - g1 , b2 - b1 return r1 + fade * dr , g1 + fade * dg , b1 + fade * db
6666	def update_merge ( d , u ) : import collections for k , v in u . items ( ) : if isinstance ( v , collections . Mapping ) : r = update_merge ( d . get ( k , dict ( ) ) , v ) d [ k ] = r else : d [ k ] = u [ k ] return d
9958	def restore_ipython ( self ) : if not self . is_ipysetup : return shell_class = type ( self . shell ) shell_class . showtraceback = shell_class . default_showtraceback del shell_class . default_showtraceback self . is_ipysetup = False
7691	def validate ( schema_file = None , jams_files = None ) : schema = load_json ( schema_file ) for jams_file in jams_files : try : jams = load_json ( jams_file ) jsonschema . validate ( jams , schema ) print '{:s} was successfully validated' . format ( jams_file ) except jsonschema . ValidationError as exc : print '{:s} was NOT successfully validated' . format ( jams_file ) print exc
8335	def findPreviousSibling ( self , name = None , attrs = { } , text = None , * * kwargs ) : return self . _findOne ( self . findPreviousSiblings , name , attrs , text , * * kwargs )
2952	def connect ( self , task_spec ) : assert self . default_task_spec is None self . outputs . append ( task_spec ) self . default_task_spec = task_spec . name task_spec . _connect_notify ( self )
1703	def outer_left_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_LEFT , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
3110	def locked_delete ( self ) : query = { self . key_name : self . key_value } self . model_class . objects . filter ( * * query ) . delete ( )
10613	def H ( self , H ) : self . _H = H self . _T = self . _calculate_T ( H )
3959	def ensure_local_repo ( self ) : if os . path . exists ( self . managed_path ) : logging . debug ( 'Repo {} already exists' . format ( self . remote_path ) ) return logging . info ( 'Initiating clone of local repo {}' . format ( self . remote_path ) ) repo_path_parent = parent_dir ( self . managed_path ) if not os . path . exists ( repo_path_parent ) : os . makedirs ( repo_path_parent ) with git_error_handling ( ) : git . Repo . clone_from ( self . assemble_remote_path ( ) , self . managed_path )
1927	def parse_config ( f ) : try : c = yaml . safe_load ( f ) for section_name , section in c . items ( ) : group = get_group ( section_name ) for key , val in section . items ( ) : group . update ( key ) setattr ( group , key , val ) # Any exception here should trigger the warning; from not being able to parse yaml # to reading poorly formatted values except Exception : raise ConfigError ( "Failed reading config file. Do you have a local [.]manticore.yml file?" )
11440	def _get_children_as_string ( node ) : out = [ ] if node : for child in node : if child . nodeType == child . TEXT_NODE : out . append ( child . data ) else : out . append ( _get_children_as_string ( child . childNodes ) ) return '' . join ( out )
2803	def convert_slice ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting slice ...' ) if len ( params [ 'axes' ] ) > 1 : raise AssertionError ( 'Cannot convert slice by multiple dimensions' ) if params [ 'axes' ] [ 0 ] not in [ 0 , 1 , 2 , 3 ] : raise AssertionError ( 'Slice by dimension more than 3 or less than 0 is not supported' ) def target_layer ( x , axis = int ( params [ 'axes' ] [ 0 ] ) , start = int ( params [ 'starts' ] [ 0 ] ) , end = int ( params [ 'ends' ] [ 0 ] ) ) : if axis == 0 : return x [ start : end ] elif axis == 1 : return x [ : , start : end ] elif axis == 2 : return x [ : , : , start : end ] elif axis == 3 : return x [ : , : , : , start : end ] lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
4698	def fmt ( lbaf = 3 ) : if env ( ) : cij . err ( "cij.nvme.exists: Invalid NVMe ENV." ) return 1 nvme = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ "nvme" , "format" , nvme [ "DEV_PATH" ] , "-l" , str ( lbaf ) ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True ) return rcode
3758	def UFL ( Hc = None , atoms = { } , CASRN = '' , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'UFL' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'UFL' ] ) : methods . append ( NFPA ) if Hc : methods . append ( SUZUKI ) if atoms : methods . append ( CROWLLOUVAR ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'UFL' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'UFL' ] ) elif Method == SUZUKI : return Suzuki_UFL ( Hc = Hc ) elif Method == CROWLLOUVAR : return Crowl_Louvar_UFL ( atoms = atoms ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
608	def _indentLines ( str , indentLevels = 1 , indentFirstLine = True ) : indent = _ONE_INDENT * indentLevels lines = str . splitlines ( True ) result = '' if len ( lines ) > 0 and not indentFirstLine : first = 1 result += lines [ 0 ] else : first = 0 for line in lines [ first : ] : result += indent + line return result
8475	def _getClassInstance ( path , args = None ) : if not path . endswith ( ".py" ) : return None if args is None : args = { } classname = AtomShieldsScanner . _getClassName ( path ) basename = os . path . basename ( path ) . replace ( ".py" , "" ) sys . path . append ( os . path . dirname ( path ) ) try : mod = __import__ ( basename , globals ( ) , locals ( ) , [ classname ] , - 1 ) class_ = getattr ( mod , classname ) instance = class_ ( * * args ) except Exception as e : AtomShieldsScanner . _debug ( "[!] %s" % e ) return None finally : sys . path . remove ( os . path . dirname ( path ) ) return instance
13001	def hr_diagram_from_data ( data , x_range , y_range ) : _ , color_mapper = hr_diagram_color_helper ( [ ] ) data_dict = { 'x' : list ( data [ 'temperature' ] ) , 'y' : list ( data [ 'luminosity' ] ) , 'color' : list ( data [ 'color' ] ) } source = ColumnDataSource ( data = data_dict ) pf = figure ( y_axis_type = 'log' , x_range = x_range , y_range = y_range ) _diagram ( source = source , plot_figure = pf , color = { 'field' : 'color' , 'transform' : color_mapper } , xaxis_label = 'Temperature (Kelvin)' , yaxis_label = 'Luminosity (solar units)' ) show_with_bokeh_server ( pf )
2517	def p_file_comment ( self , f_term , predicate ) : try : for _ , _ , comment in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_comment ( self . doc , six . text_type ( comment ) ) except CardinalityError : self . more_than_one_error ( 'file comment' )
7875	def element_to_unicode ( element ) : if hasattr ( ElementTree , 'tounicode' ) : # pylint: disable=E1103 return ElementTree . tounicode ( "element" ) elif sys . version_info . major < 3 : return unicode ( ElementTree . tostring ( element ) ) else : return ElementTree . tostring ( element , encoding = "unicode" )
6833	def version ( self ) : r = self . local_renderer with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : res = r . local ( 'vagrant --version' , capture = True ) if res . failed : return None line = res . splitlines ( ) [ - 1 ] version = re . match ( r'Vagrant (?:v(?:ersion )?)?(.*)' , line ) . group ( 1 ) return tuple ( _to_int ( part ) for part in version . split ( '.' ) )
933	def _getModelCheckpointFilePath ( checkpointDir ) : path = os . path . join ( checkpointDir , "model.data" ) path = os . path . abspath ( path ) return path
2367	def _load ( self , path ) : self . tables = [ ] current_table = DefaultTable ( self ) with Utf8Reader ( path ) as f : # N.B. the caller should be catching errors self . raw_text = f . read ( ) f . _file . seek ( 0 ) # bleh; wish this wasn't a private property matcher = Matcher ( re . IGNORECASE ) for linenumber , raw_text in enumerate ( f . readlines ( ) ) : linenumber += 1 # start counting at 1 rather than zero # this mimics what the robot TSV reader does -- # it replaces non-breaking spaces with regular spaces, # and then strips trailing whitespace raw_text = raw_text . replace ( u'\xA0' , ' ' ) raw_text = raw_text . rstrip ( ) # FIXME: I'm keeping line numbers but throwing away # where each cell starts. I should be preserving that # (though to be fair, robot is throwing that away so # I'll have to write my own splitter if I want to save # the character position) cells = TxtReader . split_row ( raw_text ) _heading_regex = r'^\s*\*+\s*(.*?)[ *]*$' if matcher ( _heading_regex , cells [ 0 ] ) : # we've found the start of a new table table_name = matcher . group ( 1 ) current_table = tableFactory ( self , linenumber , table_name , raw_text ) self . tables . append ( current_table ) else : current_table . append ( Row ( linenumber , raw_text , cells ) )
11526	def create_small_thumbnail ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemId' ] = item_id response = self . request ( 'midas.thumbnailcreator.create.small.thumbnail' , parameters ) return response
10103	def send ( self , email_id , recipient , email_data = None , sender = None , cc = None , bcc = None , tags = [ ] , headers = { } , esp_account = None , locale = None , email_version_name = None , inline = None , files = [ ] , timeout = None ) : if not email_data : email_data = { } # for backwards compatibility, will be removed if isinstance ( recipient , string_types ) : warnings . warn ( "Passing email directly for recipient is deprecated" , DeprecationWarning ) recipient = { 'address' : recipient } payload = { 'email_id' : email_id , 'recipient' : recipient , 'email_data' : email_data } if sender : payload [ 'sender' ] = sender if cc : if not type ( cc ) == list : logger . error ( 'kwarg cc must be type(list), got %s' % type ( cc ) ) payload [ 'cc' ] = cc if bcc : if not type ( bcc ) == list : logger . error ( 'kwarg bcc must be type(list), got %s' % type ( bcc ) ) payload [ 'bcc' ] = bcc if tags : if not type ( tags ) == list : logger . error ( 'kwarg tags must be type(list), got %s' % ( type ( tags ) ) ) payload [ 'tags' ] = tags if headers : if not type ( headers ) == dict : logger . error ( 'kwarg headers must be type(dict), got %s' % ( type ( headers ) ) ) payload [ 'headers' ] = headers if esp_account : if not isinstance ( esp_account , string_types ) : logger . error ( 'kwarg esp_account must be a string, got %s' % ( type ( esp_account ) ) ) payload [ 'esp_account' ] = esp_account if locale : if not isinstance ( locale , string_types ) : logger . error ( 'kwarg locale must be a string, got %s' % ( type ( locale ) ) ) payload [ 'locale' ] = locale if email_version_name : if not isinstance ( email_version_name , string_types ) : logger . error ( 'kwarg email_version_name must be a string, got %s' % ( type ( email_version_name ) ) ) payload [ 'version_name' ] = email_version_name if inline : payload [ 'inline' ] = self . _make_file_dict ( inline ) if files : payload [ 'files' ] = [ self . _make_file_dict ( f ) for f in files ] return self . _api_request ( self . SEND_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
4388	def adsPortCloseEx ( port ) : # type: (int) -> None port_close_ex = _adsDLL . AdsPortCloseEx port_close_ex . restype = ctypes . c_long error_code = port_close_ex ( port ) if error_code : raise ADSError ( error_code )
1918	def run ( self ) : # policy_order=self.policy_order # policy=self.policy current_state = None current_state_id = None with WithKeyboardInterruptAs ( self . shutdown ) : # notify siblings we are about to start a run self . _notify_start_run ( ) logger . debug ( "Starting Manticore Symbolic Emulator Worker (pid %d)." , os . getpid ( ) ) solver = Z3Solver ( ) while not self . is_shutdown ( ) : try : # handle fatal errors: exceptions in Manticore try : # handle external (e.g. solver) errors, and executor control exceptions # select a suitable state to analyze if current_state is None : with self . _lock : # notify siblings we are about to stop this run self . _notify_stop_run ( ) try : # Select a single state_id current_state_id = self . get ( ) # load selected state from secondary storage if current_state_id is not None : self . _publish ( 'will_load_state' , current_state_id ) current_state = self . _workspace . load_state ( current_state_id ) self . forward_events_from ( current_state , True ) self . _publish ( 'did_load_state' , current_state , current_state_id ) logger . info ( "load state %r" , current_state_id ) # notify siblings we have a state to play with finally : self . _notify_start_run ( ) # If current_state is still None. We are done. if current_state is None : logger . debug ( "No more states in the queue, byte bye!" ) break assert current_state is not None assert current_state . constraints is current_state . platform . constraints # Allows to terminate manticore worker on user request while not self . is_shutdown ( ) : if not current_state . execute ( ) : break else : # Notify this worker is done self . _publish ( 'will_terminate_state' , current_state , current_state_id , TerminateState ( 'Shutdown' ) ) current_state = None # Handling Forking and terminating exceptions except Concretize as e : # expression # policy # setstate() logger . debug ( "Generic state fork on condition" ) current_state = self . fork ( current_state , e . expression , e . policy , e . setstate ) except TerminateState as e : # Notify this worker is done self . _publish ( 'will_terminate_state' , current_state , current_state_id , e ) logger . debug ( "Generic terminate state" ) if e . testcase : self . _publish ( 'internal_generate_testcase' , current_state , message = str ( e ) ) current_state = None except SolverError as e : # raise import traceback trace = traceback . format_exc ( ) logger . error ( "Exception: %s\n%s" , str ( e ) , trace ) # Notify this state is done self . _publish ( 'will_terminate_state' , current_state , current_state_id , e ) if solver . check ( current_state . constraints ) : self . _publish ( 'internal_generate_testcase' , current_state , message = "Solver failed" + str ( e ) ) current_state = None except ( Exception , AssertionError ) as e : # raise import traceback trace = traceback . format_exc ( ) logger . error ( "Exception: %s\n%s" , str ( e ) , trace ) # Notify this worker is done self . _publish ( 'will_terminate_state' , current_state , current_state_id , e ) current_state = None assert current_state is None or self . is_shutdown ( ) # notify siblings we are about to stop this run self . _notify_stop_run ( )
5511	def register_memory ( ) : # XXX How to get a reliable representation of memory being used is # not clear. (rss - shared) seems kind of ok but we might also use # the private working set via get_memory_maps().private*. def get_mem ( proc ) : if os . name == 'posix' : mem = proc . memory_info_ex ( ) counter = mem . rss if 'shared' in mem . _fields : counter -= mem . shared return counter else : # TODO figure out what to do on Windows return proc . get_memory_info ( ) . rss if SERVER_PROC is not None : mem = get_mem ( SERVER_PROC ) for child in SERVER_PROC . children ( ) : mem += get_mem ( child ) server_memory . append ( bytes2human ( mem ) )
1653	def CheckGlobalStatic ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] # Match two lines at a time to support multiline declarations if linenum + 1 < clean_lines . NumLines ( ) and not Search ( r'[;({]' , line ) : line += clean_lines . elided [ linenum + 1 ] . strip ( ) # Check for people declaring static/global STL strings at the top level. # This is dangerous because the C++ language does not guarantee that # globals with constructors are initialized before the first access, and # also because globals can be destroyed when some threads are still running. # TODO(unknown): Generalize this to also find static unique_ptr instances. # TODO(unknown): File bugs for clang-tidy to find these. match = Match ( r'((?:|static +)(?:|const +))(?::*std::)?string( +const)? +' r'([a-zA-Z0-9_:]+)\b(.*)' , line ) # Remove false positives: # - String pointers (as opposed to values). # string *pointer # const string *pointer # string const *pointer # string *const pointer # # - Functions and template specializations. # string Function<Type>(... # string Class<Type>::Method(... # # - Operators. These are matched separately because operator names # cross non-word boundaries, and trying to match both operators # and functions at the same time would decrease accuracy of # matching identifiers. # string Class::operator*() if ( match and not Search ( r'\bstring\b(\s+const)?\s*[\*\&]\s*(const\s+)?\w' , line ) and not Search ( r'\boperator\W' , line ) and not Match ( r'\s*(<.*>)?(::[a-zA-Z0-9_]+)*\s*\(([^"]|$)' , match . group ( 4 ) ) ) : if Search ( r'\bconst\b' , line ) : error ( filename , linenum , 'runtime/string' , 4 , 'For a static/global string constant, use a C style string ' 'instead: "%schar%s %s[]".' % ( match . group ( 1 ) , match . group ( 2 ) or '' , match . group ( 3 ) ) ) else : error ( filename , linenum , 'runtime/string' , 4 , 'Static/global string variables are not permitted.' ) if ( Search ( r'\b([A-Za-z0-9_]*_)\(\1\)' , line ) or Search ( r'\b([A-Za-z0-9_]*_)\(CHECK_NOTNULL\(\1\)\)' , line ) ) : error ( filename , linenum , 'runtime/init' , 4 , 'You seem to be initializing a member variable with itself.' )
13752	def _reference_table ( cls , ref_table ) : # create pairs of (Foreign key column, primary key column) cols = [ ( sa . Column ( ) , refcol ) for refcol in ref_table . primary_key ] # set "tablename_colname = Foreign key Column" on the local class for col , refcol in cols : setattr ( cls , "%s_%s" % ( ref_table . name , refcol . name ) , col ) # add a ForeignKeyConstraint([local columns], [remote columns]) cls . __table__ . append_constraint ( sa . ForeignKeyConstraint ( * zip ( * cols ) ) )
9194	def publish ( request ) : if 'epub' not in request . POST : raise httpexceptions . HTTPBadRequest ( "Missing EPUB in POST body." ) is_pre_publication = asbool ( request . POST . get ( 'pre-publication' ) ) epub_upload = request . POST [ 'epub' ] . file try : epub = cnxepub . EPUB . from_file ( epub_upload ) except : # noqa: E722 raise httpexceptions . HTTPBadRequest ( 'Format not recognized.' ) # Make a publication entry in the database for status checking # the publication. This also creates publication entries for all # of the content in the EPUB. with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : epub_upload . seek ( 0 ) publication_id , publications = add_publication ( cursor , epub , epub_upload , is_pre_publication ) # Poke at the publication & lookup its state. state , messages = poke_publication_state ( publication_id ) response_data = { 'publication' : publication_id , 'mapping' : publications , 'state' : state , 'messages' : messages , } return response_data
8123	def textpath ( self , txt , x , y , width = None , height = 1000000 , enableRendering = False , * * kwargs ) : txt = self . Text ( txt , x , y , width , height , * * kwargs ) path = txt . path if draw : path . draw ( ) return path
2241	def modpath_to_modname ( modpath , hide_init = True , hide_main = False , check = True , relativeto = None ) : if check and relativeto is None : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) modpath_ = abspath ( expanduser ( modpath ) ) modpath_ = normalize_modpath ( modpath_ , hide_init = hide_init , hide_main = hide_main ) if relativeto : dpath = dirname ( abspath ( expanduser ( relativeto ) ) ) rel_modpath = relpath ( modpath_ , dpath ) else : dpath , rel_modpath = split_modpath ( modpath_ , check = check ) modname = splitext ( rel_modpath ) [ 0 ] if '.' in modname : modname , abi_tag = modname . split ( '.' ) modname = modname . replace ( '/' , '.' ) modname = modname . replace ( '\\' , '.' ) return modname
7302	def get_mongoadmins ( self ) : apps = [ ] for app_name in settings . INSTALLED_APPS : mongoadmin = "{0}.mongoadmin" . format ( app_name ) try : module = import_module ( mongoadmin ) except ImportError as e : if str ( e ) . startswith ( "No module named" ) : continue raise e app_store = AppStore ( module ) apps . append ( dict ( app_name = app_name , obj = app_store ) ) return apps
11307	def map_attr ( self , mapping , attr , obj ) : if attr not in mapping and hasattr ( self , attr ) : if not callable ( getattr ( self , attr ) ) : mapping [ attr ] = getattr ( self , attr ) else : mapping [ attr ] = getattr ( self , attr ) ( obj )
13580	def dmap ( fn , record ) : values = ( fn ( v ) for k , v in record . items ( ) ) return dict ( itertools . izip ( record , values ) )
12656	def dictify ( a_named_tuple ) : return dict ( ( s , getattr ( a_named_tuple , s ) ) for s in a_named_tuple . _fields )
3391	def prune_unused_metabolites ( cobra_model ) : output_model = cobra_model . copy ( ) inactive_metabolites = [ m for m in output_model . metabolites if len ( m . reactions ) == 0 ] output_model . remove_metabolites ( inactive_metabolites ) return output_model , inactive_metabolites
5980	def bin_up_mask_2d ( mask_2d , bin_up_factor ) : padded_array_2d = array_util . pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = mask_2d , bin_up_factor = bin_up_factor , pad_value = True ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = True for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 if padded_array_2d [ padded_y , padded_x ] == False : value = False binned_array_2d [ y , x ] = value return binned_array_2d
13378	def preprocess_dict ( d ) : out_env = { } for k , v in d . items ( ) : if not type ( v ) in PREPROCESSORS : raise KeyError ( 'Invalid type in dict: {}' . format ( type ( v ) ) ) out_env [ k ] = PREPROCESSORS [ type ( v ) ] ( v ) return out_env
6478	def _normalised_numpy ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) points = np . array ( self . points ) - self . minimum points = points * 4.0 / self . extents * self . size . y for x , y in enumerate ( points ) : yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
1559	def component_id ( self ) : if isinstance ( self . _component_id , HeronComponentSpec ) : if self . _component_id . name is None : # HeronComponentSpec instance's name attribute might not be available until # TopologyType metaclass finally sets it. This statement is to support __eq__(), # __hash__() and __str__() methods with safety, as raising Exception is not # appropriate this case. return "<No name available for HeronComponentSpec yet, uuid: %s>" % self . _component_id . uuid return self . _component_id . name elif isinstance ( self . _component_id , str ) : return self . _component_id else : raise ValueError ( "Component Id for this GlobalStreamId is not properly set: <%s:%s>" % ( str ( type ( self . _component_id ) ) , str ( self . _component_id ) ) )
9722	async def load ( self , filename ) : cmd = "load %s" % filename return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
2653	def push_file ( self , local_source , remote_dir ) : remote_dest = remote_dir + '/' + os . path . basename ( local_source ) try : self . makedirs ( remote_dir , exist_ok = True ) except IOError as e : logger . exception ( "Pushing {0} to {1} failed" . format ( local_source , remote_dir ) ) if e . errno == 2 : raise BadScriptPath ( e , self . hostname ) elif e . errno == 13 : raise BadPermsScriptPath ( e , self . hostname ) else : logger . exception ( "File push failed due to SFTP client failure" ) raise FileCopyException ( e , self . hostname ) try : self . sftp_client . put ( local_source , remote_dest , confirm = True ) # Set perm because some systems require the script to be executable self . sftp_client . chmod ( remote_dest , 0o777 ) except Exception as e : logger . exception ( "File push from local source {} to remote destination {} failed" . format ( local_source , remote_dest ) ) raise FileCopyException ( e , self . hostname ) return remote_dest
11103	def backup ( self , dst = None , ignore = None , ignore_ext = None , ignore_pattern = None , ignore_size_smaller_than = None , ignore_size_larger_than = None , case_sensitive = False ) : # pragma: no cover def preprocess_arg ( arg ) : # pragma: no cover if arg is None : return [ ] if isinstance ( arg , ( tuple , list ) ) : return list ( arg ) else : return [ arg , ] self . assert_is_dir_and_exists ( ) ignore = preprocess_arg ( ignore ) for i in ignore : if i . startswith ( "/" ) or i . startswith ( "\\" ) : raise ValueError ignore_ext = preprocess_arg ( ignore_ext ) for ext in ignore_ext : if not ext . startswith ( "." ) : raise ValueError ignore_pattern = preprocess_arg ( ignore_pattern ) if case_sensitive : pass else : ignore = [ i . lower ( ) for i in ignore ] ignore_ext = [ i . lower ( ) for i in ignore_ext ] ignore_pattern = [ i . lower ( ) for i in ignore_pattern ] def filters ( p ) : relpath = p . relative_to ( self ) . abspath if not case_sensitive : relpath = relpath . lower ( ) # ignore for i in ignore : if relpath . startswith ( i ) : return False # ignore_ext if case_sensitive : ext = p . ext else : ext = p . ext . lower ( ) if ext in ignore_ext : return False # ignore_pattern for pattern in ignore_pattern : if pattern in relpath : return False # ignore_size_smaller_than if ignore_size_smaller_than : if p . size < ignore_size_smaller_than : return False # ignore_size_larger_than if ignore_size_larger_than : if p . size > ignore_size_larger_than : return False return True self . make_zip_archive ( dst = dst , filters = filters , compress = True , overwrite = False , verbose = True , )
839	def getClosest ( self , inputPattern , topKCategories = 3 ) : inferenceResult = numpy . zeros ( max ( self . _categoryList ) + 1 ) dist = self . _getDistances ( inputPattern ) sorted = dist . argsort ( ) validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) for j in sorted [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ j ] ] += 1.0 winner = inferenceResult . argmax ( ) topNCats = [ ] for i in range ( topKCategories ) : topNCats . append ( ( self . _categoryList [ sorted [ i ] ] , dist [ sorted [ i ] ] ) ) return winner , dist , topNCats
3024	def _in_gce_environment ( ) : if SETTINGS . env_name is not None : return SETTINGS . env_name == 'GCE_PRODUCTION' if NO_GCE_CHECK != 'True' and _detect_gce_environment ( ) : SETTINGS . env_name = 'GCE_PRODUCTION' return True return False
1435	def register_metrics ( self , metrics_collector , interval ) : for field , metrics in self . metrics . items ( ) : metrics_collector . register_metric ( field , metrics , interval )
2675	def build ( src , requirements = None , local_package = None , config_file = 'config.yaml' , profile_name = None , ) : # Load and parse the config file. path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) # Get the absolute path to the output directory and create it if it doesn't # already exist. dist_directory = cfg . get ( 'dist_directory' , 'dist' ) path_to_dist = os . path . join ( src , dist_directory ) mkdir ( path_to_dist ) # Combine the name of the Lambda function with the current timestamp to use # for the output filename. function_name = cfg . get ( 'function_name' ) output_filename = '{0}-{1}.zip' . format ( timestamp ( ) , function_name ) path_to_temp = mkdtemp ( prefix = 'aws-lambda' ) pip_install_to_target ( path_to_temp , requirements = requirements , local_package = local_package , ) # Hack for Zope. if 'zope' in os . listdir ( path_to_temp ) : print ( 'Zope packages detected; fixing Zope package paths to ' 'make them importable.' , ) # Touch. with open ( os . path . join ( path_to_temp , 'zope/__init__.py' ) , 'wb' ) : pass # Gracefully handle whether ".zip" was included in the filename or not. output_filename = ( '{0}.zip' . format ( output_filename ) if not output_filename . endswith ( '.zip' ) else output_filename ) # Allow definition of source code directories we want to build into our # zipped package. build_config = defaultdict ( * * cfg . get ( 'build' , { } ) ) build_source_directories = build_config . get ( 'source_directories' , '' ) build_source_directories = ( build_source_directories if build_source_directories is not None else '' ) source_directories = [ d . strip ( ) for d in build_source_directories . split ( ',' ) ] files = [ ] for filename in os . listdir ( src ) : if os . path . isfile ( filename ) : if filename == '.DS_Store' : continue if filename == config_file : continue print ( 'Bundling: %r' % filename ) files . append ( os . path . join ( src , filename ) ) elif os . path . isdir ( filename ) and filename in source_directories : print ( 'Bundling directory: %r' % filename ) files . append ( os . path . join ( src , filename ) ) # "cd" into `temp_path` directory. os . chdir ( path_to_temp ) for f in files : if os . path . isfile ( f ) : _ , filename = os . path . split ( f ) # Copy handler file into root of the packages folder. copyfile ( f , os . path . join ( path_to_temp , filename ) ) copystat ( f , os . path . join ( path_to_temp , filename ) ) elif os . path . isdir ( f ) : destination_folder = os . path . join ( path_to_temp , f [ len ( src ) + 1 : ] ) copytree ( f , destination_folder ) # Zip them together into a single file. # TODO: Delete temp directory created once the archive has been compiled. path_to_zip_file = archive ( './' , path_to_dist , output_filename ) return path_to_zip_file
7893	def leave ( self ) : if self . joined : p = MucPresence ( to_jid = self . room_jid , stanza_type = "unavailable" ) self . manager . stream . send ( p )
3243	def get_rules ( security_group , * * kwargs ) : rules = security_group . pop ( 'security_group_rules' , [ ] ) for rule in rules : rule [ 'ip_protocol' ] = rule . pop ( 'protocol' ) rule [ 'from_port' ] = rule . pop ( 'port_range_max' ) rule [ 'to_port' ] = rule . pop ( 'port_range_min' ) rule [ 'cidr_ip' ] = rule . pop ( 'remote_ip_prefix' ) rule [ 'rule_type' ] = rule . pop ( 'direction' ) security_group [ 'rules' ] = sorted ( rules ) return security_group
4288	def read_settings ( filename = None ) : logger = logging . getLogger ( __name__ ) logger . info ( "Reading settings ..." ) settings = _DEFAULT_CONFIG . copy ( ) if filename : logger . debug ( "Settings file: %s" , filename ) settings_path = os . path . dirname ( filename ) tempdict = { } with open ( filename ) as f : code = compile ( f . read ( ) , filename , 'exec' ) exec ( code , tempdict ) settings . update ( ( k , v ) for k , v in tempdict . items ( ) if k not in [ '__builtins__' ] ) # Make the paths relative to the settings file paths = [ 'source' , 'destination' , 'watermark' ] if os . path . isdir ( join ( settings_path , settings [ 'theme' ] ) ) and os . path . isdir ( join ( settings_path , settings [ 'theme' ] , 'templates' ) ) : paths . append ( 'theme' ) for p in paths : path = settings [ p ] if path and not isabs ( path ) : settings [ p ] = abspath ( normpath ( join ( settings_path , path ) ) ) logger . debug ( "Rewrite %s : %s -> %s" , p , path , settings [ p ] ) for key in ( 'img_size' , 'thumb_size' , 'video_size' ) : w , h = settings [ key ] if h > w : settings [ key ] = ( h , w ) logger . warning ( "The %s setting should be specified with the " "largest value first." , key ) if not settings [ 'img_processor' ] : logger . info ( 'No Processor, images will not be resized' ) logger . debug ( 'Settings:\n%s' , pformat ( settings , width = 120 ) ) return settings
6403	def ipa_to_features ( ipa ) : features = [ ] pos = 0 ipa = normalize ( 'NFD' , text_type ( ipa . lower ( ) ) ) maxsymlen = max ( len ( _ ) for _ in _PHONETIC_FEATURES ) while pos < len ( ipa ) : found_match = False for i in range ( maxsymlen , 0 , - 1 ) : if ( pos + i - 1 <= len ( ipa ) and ipa [ pos : pos + i ] in _PHONETIC_FEATURES ) : features . append ( _PHONETIC_FEATURES [ ipa [ pos : pos + i ] ] ) pos += i found_match = True if not found_match : features . append ( - 1 ) pos += 1 return features
13217	def connection_dsn ( self , name = None ) : return ' ' . join ( "%s=%s" % ( param , value ) for param , value in self . _connect_options ( name ) )
11906	def to_permutation_matrix ( matches ) : n = len ( matches ) P = np . zeros ( ( n , n ) ) # This is a cleverer way of doing # # for (u, v) in matches.items(): # P[u, v] = 1 # P [ list ( zip ( * ( matches . items ( ) ) ) ) ] = 1 return P
2275	def _win32_is_hardlinked ( fpath1 , fpath2 ) : # NOTE: jwf.samefile(fpath1, fpath2) seems to behave differently def get_read_handle ( fpath ) : if os . path . isdir ( fpath ) : dwFlagsAndAttributes = jwfs . api . FILE_FLAG_BACKUP_SEMANTICS else : dwFlagsAndAttributes = 0 hFile = jwfs . api . CreateFile ( fpath , jwfs . api . GENERIC_READ , jwfs . api . FILE_SHARE_READ , None , jwfs . api . OPEN_EXISTING , dwFlagsAndAttributes , None ) return hFile def get_unique_id ( hFile ) : info = jwfs . api . BY_HANDLE_FILE_INFORMATION ( ) res = jwfs . api . GetFileInformationByHandle ( hFile , info ) jwfs . handle_nonzero_success ( res ) unique_id = ( info . volume_serial_number , info . file_index_high , info . file_index_low ) return unique_id hFile1 = get_read_handle ( fpath1 ) hFile2 = get_read_handle ( fpath2 ) try : are_equal = ( get_unique_id ( hFile1 ) == get_unique_id ( hFile2 ) ) except Exception : raise finally : jwfs . api . CloseHandle ( hFile1 ) jwfs . api . CloseHandle ( hFile2 ) return are_equal
7448	def _samples_precheck ( self , samples , mystep , force ) : subsample = [ ] ## filter by state for sample in samples : if sample . stats . state < mystep - 1 : LOGGER . debug ( "Sample {} not in proper state." . format ( sample . name ) ) else : subsample . append ( sample ) return subsample
4164	def embed_code_links ( app , exception ) : if exception is not None : return # No need to waste time embedding hyperlinks when not running the examples # XXX: also at the time of writing this fixes make html-noplot # for some reason I don't fully understand if not app . builder . config . plot_gallery : return # XXX: Whitelist of builders for which it makes sense to embed # hyperlinks inside the example html. Note that the link embedding # require searchindex.js to exist for the links to the local doc # and there does not seem to be a good way of knowing which # builders creates a searchindex.js. if app . builder . name not in [ 'html' , 'readthedocs' ] : return print ( 'Embedding documentation hyperlinks in examples..' ) gallery_conf = app . config . sphinx_gallery_conf gallery_dirs = gallery_conf [ 'gallery_dirs' ] if not isinstance ( gallery_dirs , list ) : gallery_dirs = [ gallery_dirs ] for gallery_dir in gallery_dirs : _embed_code_links ( app , gallery_conf , gallery_dir )
4782	def is_close_to ( self , other , tolerance ) : self . _validate_close_to_args ( self . val , other , tolerance ) if self . val < ( other - tolerance ) or self . val > ( other + tolerance ) : if type ( self . val ) is datetime . datetime : tolerance_seconds = tolerance . days * 86400 + tolerance . seconds + tolerance . microseconds / 1000000 h , rem = divmod ( tolerance_seconds , 3600 ) m , s = divmod ( rem , 60 ) self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%d:%02d:%02d>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) , h , m , s ) ) else : self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%s>, but was not.' % ( self . val , other , tolerance ) ) return self
8028	def groupify ( function ) : @ wraps ( function ) def wrapper ( paths , * args , * * kwargs ) : # pylint: disable=missing-docstring groups = { } for path in paths : key = function ( path , * args , * * kwargs ) if key is not None : groups . setdefault ( key , set ( ) ) . add ( path ) return groups return wrapper
3248	def _get_base ( group , * * conn ) : group [ '_version' ] = 1 # Get the initial group details (only needed if we didn't grab the users): group . update ( get_group_api ( group [ 'GroupName' ] , users = False , * * conn ) [ 'Group' ] ) # Cast CreateDate from a datetime to something JSON serializable. group [ 'CreateDate' ] = get_iso_string ( group [ 'CreateDate' ] ) return group
3832	async def send_chat_message ( self , send_chat_message_request ) : response = hangouts_pb2 . SendChatMessageResponse ( ) await self . _pb_request ( 'conversations/sendchatmessage' , send_chat_message_request , response ) return response
12566	def get_dataset ( self , ds_name , mode = 'r' ) : if ds_name in self . _datasets : return self . _datasets [ ds_name ] else : return self . create_empty_dataset ( ds_name )
6952	def aov_theta ( times , mags , errs , frequency , binsize = 0.05 , minbin = 9 ) : period = 1.0 / frequency fold_time = times [ 0 ] phased = phase_magseries ( times , mags , period , fold_time , wrap = False , sort = True ) phases = phased [ 'phase' ] pmags = phased [ 'mags' ] bins = nparange ( 0.0 , 1.0 , binsize ) ndets = phases . size binnedphaseinds = npdigitize ( phases , bins ) bin_s1_tops = [ ] bin_s2_tops = [ ] binndets = [ ] goodbins = 0 all_xbar = npmedian ( pmags ) for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_mags = pmags [ thisbin_inds ] if thisbin_mags . size > minbin : thisbin_ndet = thisbin_mags . size thisbin_xbar = npmedian ( thisbin_mags ) # get s1 thisbin_s1_top = ( thisbin_ndet * ( thisbin_xbar - all_xbar ) * ( thisbin_xbar - all_xbar ) ) # get s2 thisbin_s2_top = npsum ( ( thisbin_mags - all_xbar ) * ( thisbin_mags - all_xbar ) ) bin_s1_tops . append ( thisbin_s1_top ) bin_s2_tops . append ( thisbin_s2_top ) binndets . append ( thisbin_ndet ) goodbins = goodbins + 1 # turn the quantities into arrays bin_s1_tops = nparray ( bin_s1_tops ) bin_s2_tops = nparray ( bin_s2_tops ) binndets = nparray ( binndets ) # calculate s1 first s1 = npsum ( bin_s1_tops ) / ( goodbins - 1.0 ) # then calculate s2 s2 = npsum ( bin_s2_tops ) / ( ndets - goodbins ) theta_aov = s1 / s2 return theta_aov
13541	def create ( self , server ) : if len ( self . geometries ) == 0 : raise Exception ( 'no geometries' ) return server . post ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
382	def drop ( x , keep = 0.5 ) : if len ( x . shape ) == 3 : if x . shape [ - 1 ] == 3 : # color img_size = x . shape mask = np . random . binomial ( n = 1 , p = keep , size = x . shape [ : - 1 ] ) for i in range ( 3 ) : x [ : , : , i ] = np . multiply ( x [ : , : , i ] , mask ) elif x . shape [ - 1 ] == 1 : # greyscale image img_size = x . shape x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) else : raise Exception ( "Unsupported shape {}" . format ( x . shape ) ) elif len ( x . shape ) == 2 or 1 : # greyscale matrix (image) or vector img_size = x . shape x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) else : raise Exception ( "Unsupported shape {}" . format ( x . shape ) ) return x
4596	def pid_context ( pid_filename = None ) : pid_filename = pid_filename or DEFAULT_PID_FILENAME if os . path . exists ( pid_filename ) : contents = open ( pid_filename ) . read ( 16 ) log . warning ( 'pid_filename %s already exists with contents %s' , pid_filename , contents ) with open ( pid_filename , 'w' ) as fp : fp . write ( str ( os . getpid ( ) ) ) fp . write ( '\n' ) try : yield finally : try : os . remove ( pid_filename ) except Exception as e : log . error ( 'Got an exception %s deleting the pid_filename %s' , e , pid_filename )
2716	def __extract_resources_from_droplets ( self , data ) : resources = [ ] if not isinstance ( data , list ) : return data for a_droplet in data : res = { } try : if isinstance ( a_droplet , unicode ) : res = { "resource_id" : a_droplet , "resource_type" : "droplet" } except NameError : pass if isinstance ( a_droplet , str ) or isinstance ( a_droplet , int ) : res = { "resource_id" : str ( a_droplet ) , "resource_type" : "droplet" } elif isinstance ( a_droplet , Droplet ) : res = { "resource_id" : str ( a_droplet . id ) , "resource_type" : "droplet" } if len ( res ) > 0 : resources . append ( res ) return resources
11183	def default_decoder ( self , obj ) : typename , marshalled_state = self . unwrap_callback ( obj ) if typename is None : return obj try : cls , unmarshaller = self . serializer . unmarshallers [ typename ] except KeyError : raise LookupError ( 'no unmarshaller found for type "{}"' . format ( typename ) ) from None if cls is not None : instance = cls . __new__ ( cls ) unmarshaller ( instance , marshalled_state ) return instance else : return unmarshaller ( marshalled_state )
8140	def invert ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "RGB" ) self . img = ImageOps . invert ( self . img ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
9228	def get_all_tags ( self ) : verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project if verbose : print ( "Fetching tags..." ) tags = [ ] page = 1 while page > 0 : if verbose > 2 : print ( "." , end = "" ) rc , data = gh . repos [ user ] [ repo ] . tags . get ( page = page , per_page = PER_PAGE_NUMBER ) if rc == 200 : tags . extend ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) if verbose > 2 : print ( "." ) if len ( tags ) == 0 : if not self . options . quiet : print ( "Warning: Can't find any tags in repo. Make sure, that " "you push tags to remote repo via 'git push --tags'" ) exit ( ) if verbose > 1 : print ( "Found {} tag(s)" . format ( len ( tags ) ) ) return tags
10687	def H ( self , phase , T ) : try : return self . _phases [ phase ] . H ( T ) except KeyError : raise Exception ( "The phase '{}' was not found in compound '{}'." . format ( phase , self . formula ) )
5786	def _raw_write ( self ) : data_available = libssl . BIO_ctrl_pending ( self . _wbio ) if data_available == 0 : return b'' to_read = min ( self . _buffer_size , data_available ) read = libssl . BIO_read ( self . _wbio , self . _bio_write_buffer , to_read ) to_write = bytes_from_buffer ( self . _bio_write_buffer , read ) output = to_write while len ( to_write ) : raise_disconnect = False try : sent = self . _socket . send ( to_write ) except ( socket_ . error ) as e : # Handle ECONNRESET and EPIPE if e . errno == 104 or e . errno == 32 : raise_disconnect = True else : raise if raise_disconnect : raise_disconnection ( ) to_write = to_write [ sent : ] if len ( to_write ) : self . select_write ( ) return output
1720	def trans ( ele , standard = False ) : try : node = globals ( ) . get ( ele [ 'type' ] ) if not node : raise NotImplementedError ( '%s is not supported!' % ele [ 'type' ] ) if standard : node = node . __dict__ [ 'standard' ] if 'standard' in node . __dict__ else node return node ( * * ele ) except : #print ele raise
6022	def from_fits_with_scale ( cls , file_path , hdu , pixel_scale ) : return cls ( array = array_util . numpy_array_2d_from_fits ( file_path , hdu ) , pixel_scale = pixel_scale )
10507	def server_bind ( self , * args , * * kwargs ) : self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) # Can't use super() here since SimpleXMLRPCServer is an old-style class SimpleXMLRPCServer . server_bind ( self , * args , * * kwargs )
1297	def from_config ( config , kwargs = None ) : return util . get_object ( obj = config , predefined = tensorforce . core . optimizers . solvers . solvers , kwargs = kwargs )
7452	def writetofastq ( data , dsort , read ) : if read == 1 : rrr = "R1" else : rrr = "R2" for sname in dsort : ## skip writing if empty. Write to tmpname handle = os . path . join ( data . dirs . fastqs , "{}_{}_.fastq" . format ( sname , rrr ) ) with open ( handle , 'a' ) as out : out . write ( "" . join ( dsort [ sname ] ) )
12338	def stitch_macro ( path , output_folder = None ) : output_folder = output_folder or path debug ( 'stitching ' + path + ' to ' + output_folder ) fields = glob ( _pattern ( path , _field ) ) # assume we have rectangle of fields xs = [ attribute ( field , 'X' ) for field in fields ] ys = [ attribute ( field , 'Y' ) for field in fields ] x_min , x_max = min ( xs ) , max ( xs ) y_min , y_max = min ( ys ) , max ( ys ) fields_column = len ( set ( xs ) ) fields_row = len ( set ( ys ) ) # assume all fields are the same # and get properties from images in first field images = glob ( _pattern ( fields [ 0 ] , _image ) ) # assume attributes are the same on all images attr = attributes ( images [ 0 ] ) # find all channels and z-stacks channels = [ ] z_stacks = [ ] for image in images : channel = attribute_as_str ( image , 'C' ) if channel not in channels : channels . append ( channel ) z = attribute_as_str ( image , 'Z' ) if z not in z_stacks : z_stacks . append ( z ) debug ( 'channels ' + str ( channels ) ) debug ( 'z-stacks ' + str ( z_stacks ) ) # create macro _ , extension = os . path . splitext ( images [ - 1 ] ) if extension == '.tif' : # assume .ome.tif extension = '.ome.tif' macros = [ ] output_files = [ ] for Z in z_stacks : for C in channels : filenames = os . path . join ( _field + '--X{xx}--Y{yy}' , _image + '--L' + attr . L + '--S' + attr . S + '--U' + attr . U + '--V' + attr . V + '--J' + attr . J + '--E' + attr . E + '--O' + attr . O + '--X{xx}--Y{yy}' + '--T' + attr . T + '--Z' + Z + '--C' + C + extension ) debug ( 'filenames ' + filenames ) cur_attr = attributes ( filenames ) . _asdict ( ) f = 'stitched--U{U}--V{V}--C{C}--Z{Z}.png' . format ( * * cur_attr ) output = os . path . join ( output_folder , f ) debug ( 'output ' + output ) output_files . append ( output ) if os . path . isfile ( output ) : # file already exists print ( 'leicaexperiment stitched file already' ' exists {}' . format ( output ) ) continue macros . append ( fijibin . macro . stitch ( path , filenames , fields_column , fields_row , output_filename = output , x_start = x_min , y_start = y_min ) ) return ( output_files , macros )
1510	def start_cluster ( cl_args ) : roles = read_and_parse_roles ( cl_args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] zookeepers = roles [ Role . ZOOKEEPERS ] Log . info ( "Roles:" ) Log . info ( " - Master Servers: %s" % list ( masters ) ) Log . info ( " - Slave Servers: %s" % list ( slaves ) ) Log . info ( " - Zookeeper Servers: %s" % list ( zookeepers ) ) if not masters : Log . error ( "No master servers specified!" ) sys . exit ( - 1 ) if not slaves : Log . error ( "No slave servers specified!" ) sys . exit ( - 1 ) if not zookeepers : Log . error ( "No zookeeper servers specified!" ) sys . exit ( - 1 ) # make sure configs are templated update_config_files ( cl_args ) dist_nodes = list ( masters . union ( slaves ) ) # if just local deployment if not ( len ( dist_nodes ) == 1 and is_self ( dist_nodes [ 0 ] ) ) : distribute_package ( roles , cl_args ) start_master_nodes ( masters , cl_args ) start_slave_nodes ( slaves , cl_args ) start_api_server ( masters , cl_args ) start_heron_tools ( masters , cl_args ) Log . info ( "Heron standalone cluster complete!" )
12738	def create_bodies ( self , translate = ( 0 , 1 , 0 ) , size = 0.1 ) : stack = [ ( 'root' , 0 , self . root [ 'position' ] + translate ) ] while stack : name , depth , end = stack . pop ( ) for child in self . hierarchy . get ( name , ( ) ) : stack . append ( ( child , depth + 1 , end + self . bones [ child ] . end ) ) if name not in self . bones : continue bone = self . bones [ name ] body = self . world . create_body ( 'box' , name = bone . name , density = self . density , lengths = ( size , size , bone . length ) ) body . color = self . color # move the center of the body to the halfway point between # the parent (joint) and child (joint). x , y , z = end - bone . direction * bone . length / 2 # swizzle y and z -- asf uses y as up, but we use z as up. body . position = x , z , y # compute an orthonormal (rotation) matrix using the ground and # the body. this is mind-bending but seems to work. u = bone . direction v = np . cross ( u , [ 0 , 1 , 0 ] ) l = np . linalg . norm ( v ) if l > 0 : v /= l rot = np . vstack ( [ np . cross ( u , v ) , v , u ] ) . T swizzle = [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] , [ 0 , - 1 , 0 ] ] body . rotation = np . dot ( swizzle , rot ) self . bodies . append ( body )
10107	def get_context_data ( self , * * kwargs ) : context = super ( TabView , self ) . get_context_data ( * * kwargs ) # Update the context with kwargs, TemplateView doesn't do this. context . update ( kwargs ) # Add tabs and "current" references to context process_tabs_kwargs = { 'tabs' : self . get_group_tabs ( ) , 'current_tab' : self , 'group_current_tab' : self , } context [ 'tabs' ] = self . _process_tabs ( * * process_tabs_kwargs ) context [ 'current_tab_id' ] = self . tab_id # Handle parent tabs if self . tab_parent is not None : # Verify that tab parent is valid if self . tab_parent not in self . _registry : msg = '%s has no attribute _is_tab' % self . tab_parent . __class__ . __name__ raise ImproperlyConfigured ( msg ) # Get parent tab instance parent = self . tab_parent ( ) # Add parent tabs to context process_parents_kwargs = { 'tabs' : parent . get_group_tabs ( ) , 'current_tab' : self , 'group_current_tab' : parent , } context [ 'parent_tabs' ] = self . _process_tabs ( * * process_parents_kwargs ) context [ 'parent_tab_id' ] = parent . tab_id # Handle child tabs if self . tab_id in self . _children : process_children_kwargs = { 'tabs' : [ t ( ) for t in self . _children [ self . tab_id ] ] , 'current_tab' : self , 'group_current_tab' : None , } context [ 'child_tabs' ] = self . _process_tabs ( * * process_children_kwargs ) return context
8366	def rendering_finished ( self , size , frame , cairo_ctx ) : surface = cairo_ctx . get_target ( ) if self . format == 'png' : surface . write_to_png ( self . _output_file ( frame ) ) surface . finish ( ) surface . flush ( )
9375	def get_run_time_period ( run_steps ) : init_ts_start = get_standardized_timestamp ( 'now' , None ) ts_start = init_ts_start ts_end = '0' for run_step in run_steps : if run_step . ts_start and run_step . ts_end : if run_step . ts_start < ts_start : ts_start = run_step . ts_start if run_step . ts_end > ts_end : ts_end = run_step . ts_end if ts_end == '0' : ts_end = None if ts_start == init_ts_start : ts_start = None logger . info ( 'get_run_time_period range returned ' + str ( ts_start ) + ' to ' + str ( ts_end ) ) return ts_start , ts_end
13668	def slinky ( filename , seconds_available , bucket_name , aws_key , aws_secret ) : if not os . environ . get ( 'AWS_ACCESS_KEY_ID' ) and os . environ . get ( 'AWS_SECRET_ACCESS_KEY' ) : print 'Need to set environment variables for AWS access and create a slinky bucket.' exit ( ) print create_temp_s3_link ( filename , seconds_available , bucket_name )
2204	def find_exe ( name , multi = False , path = None ) : candidates = find_path ( name , path = path , exact = True ) mode = os . X_OK | os . F_OK results = ( fpath for fpath in candidates if os . access ( fpath , mode ) and not isdir ( fpath ) ) if not multi : for fpath in results : return fpath else : return list ( results )
7937	def connect ( self , addr , port = None , service = None ) : with self . lock : self . _connect ( addr , port , service )
1895	def _recv ( self ) -> str : buf , left , right = self . __readline_and_count ( ) bufl = [ buf ] while left != right : buf , l , r = self . __readline_and_count ( ) bufl . append ( buf ) left += l right += r buf = '' . join ( bufl ) . strip ( ) logger . debug ( '<%s' , buf ) if '(error' in bufl [ 0 ] : raise Exception ( f"Error in smtlib: {bufl[0]}" ) return buf
4906	def delete_course_completion ( self , user_id , payload ) : # pylint: disable=unused-argument return self . _delete ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . completion_status_api_path ) , payload , self . COMPLETION_PROVIDER_SCOPE )
1846	def JS ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . SF , target . read ( ) , cpu . PC )
8385	def write_main ( argv ) : if len ( argv ) != 1 : print ( "Please provide the name of a file to write." ) return 1 filename = argv [ 0 ] resource_name = "files/" + filename tweaks_name = amend_filename ( filename , "_tweaks" ) if not pkg_resources . resource_exists ( "edx_lint" , resource_name ) : print ( u"Don't have file %r to write." % filename ) return 2 if os . path . exists ( filename ) : print ( u"Checking existing copy of %s" % filename ) tef = TamperEvidentFile ( filename ) if not tef . validate ( ) : bak_name = amend_filename ( filename , "_backup" ) print ( u"Your copy of %s seems to have been edited, renaming it to %s" % ( filename , bak_name ) ) if os . path . exists ( bak_name ) : print ( u"A previous %s exists, deleting it" % bak_name ) os . remove ( bak_name ) os . rename ( filename , bak_name ) print ( u"Reading edx_lint/files/%s" % filename ) cfg = configparser . RawConfigParser ( ) resource_string = pkg_resources . resource_string ( "edx_lint" , resource_name ) . decode ( "utf8" ) # pkg_resources always reads binary data (in both python2 and python3). # ConfigParser.read_string only exists in python3, so we have to wrap the string # from pkg_resources in a cStringIO so that we can pass it into ConfigParser.readfp. if six . PY2 : cfg . readfp ( cStringIO ( resource_string ) , resource_name ) else : cfg . read_string ( resource_string , resource_name ) # pylint: disable=no-member if os . path . exists ( tweaks_name ) : print ( u"Applying local tweaks from %s" % tweaks_name ) cfg_tweaks = configparser . RawConfigParser ( ) cfg_tweaks . read ( [ tweaks_name ] ) merge_configs ( cfg , cfg_tweaks ) print ( u"Writing %s" % filename ) output_text = cStringIO ( ) output_text . write ( WARNING_HEADER . format ( filename = filename , tweaks_name = tweaks_name ) ) cfg . write ( output_text ) out_tef = TamperEvidentFile ( filename ) if six . PY2 : output_bytes = output_text . getvalue ( ) else : output_bytes = output_text . getvalue ( ) . encode ( "utf8" ) out_tef . write ( output_bytes ) return 0
10592	def get_path_relative_to_module ( module_file_path , relative_target_path ) : module_path = os . path . dirname ( module_file_path ) path = os . path . join ( module_path , relative_target_path ) path = os . path . abspath ( path ) return path
11903	def static ( * * kwargs ) : def wrap ( fn ) : fn . func_globals [ 'static' ] = fn fn . __dict__ . update ( kwargs ) return fn return wrap
381	def zca_whitening ( x , principal_components ) : flatx = np . reshape ( x , ( x . size ) ) # tl.logging.info(principal_components.shape, x.shape) # ((28160, 28160), (160, 176, 1)) # flatx = np.reshape(x, (x.shape)) # flatx = np.reshape(x, (x.shape[0], )) # tl.logging.info(flatx.shape) # (160, 176, 1) whitex = np . dot ( flatx , principal_components ) x = np . reshape ( whitex , ( x . shape [ 0 ] , x . shape [ 1 ] , x . shape [ 2 ] ) ) return x
2025	def SGT ( self , a , b ) : # http://gavwood.com/paper.pdf s0 , s1 = to_signed ( a ) , to_signed ( b ) return Operators . ITEBV ( 256 , s0 > s1 , 1 , 0 )
6029	def set_xy_labels ( units , kpc_per_arcsec , xlabelsize , ylabelsize , xyticksize ) : if units in 'arcsec' or kpc_per_arcsec is None : plt . xlabel ( 'x (arcsec)' , fontsize = xlabelsize ) plt . ylabel ( 'y (arcsec)' , fontsize = ylabelsize ) elif units in 'kpc' : plt . xlabel ( 'x (kpc)' , fontsize = xlabelsize ) plt . ylabel ( 'y (kpc)' , fontsize = ylabelsize ) else : raise exc . PlottingException ( 'The units supplied to the plotted are not a valid string (must be pixels | ' 'arcsec | kpc)' ) plt . tick_params ( labelsize = xyticksize )
13540	def chisq_red ( self ) : if self . _chisq_red is None : self . _chisq_red = chisquare ( self . y_unweighted . transpose ( ) , _np . dot ( self . X_unweighted , self . beta ) , self . y_error , ddof = 3 , verbose = False ) return self . _chisq_red
1742	def make_grid ( tensor , nrow = 8 , padding = 2 , pad_value = 0 ) : if not ( isinstance ( tensor , np . ndarray ) or ( isinstance ( tensor , list ) and all ( isinstance ( t , np . ndarray ) for t in tensor ) ) ) : raise TypeError ( 'tensor or list of tensors expected, got {}' . format ( type ( tensor ) ) ) # if list of tensors, convert to a 4D mini-batch Tensor if isinstance ( tensor , list ) : tensor = np . stack ( tensor , 0 ) if tensor . ndim == 2 : # single image H x W tensor = tensor . reshape ( ( 1 , tensor . shape [ 0 ] , tensor . shape [ 1 ] ) ) if tensor . ndim == 3 : if tensor . shape [ 0 ] == 1 : # if single-channel, single image, convert to 3-channel tensor = np . concatenate ( ( tensor , tensor , tensor ) , 0 ) tensor = tensor . reshape ( ( 1 , tensor . shape [ 0 ] , tensor . shape [ 1 ] , tensor . shape [ 2 ] ) ) if tensor . ndim == 4 and tensor . shape [ 1 ] == 1 : # single-channel images tensor = np . concatenate ( ( tensor , tensor , tensor ) , 1 ) if tensor . shape [ 0 ] == 1 : return np . squeeze ( tensor ) # make the mini-batch of images into a grid nmaps = tensor . shape [ 0 ] xmaps = min ( nrow , nmaps ) ymaps = int ( math . ceil ( float ( nmaps ) / xmaps ) ) height , width = int ( tensor . shape [ 2 ] + padding ) , int ( tensor . shape [ 3 ] + padding ) grid = np . ones ( ( 3 , height * ymaps + padding , width * xmaps + padding ) ) * pad_value k = 0 for y in range ( ymaps ) : for x in range ( xmaps ) : if k >= nmaps : break grid [ : , y * height + padding : ( y + 1 ) * height , x * width + padding : ( x + 1 ) * width ] = tensor [ k ] k = k + 1 return grid
6556	def projection ( self , variables ) : # resolve iterables or mutability problems by casting the variables to a set variables = set ( variables ) if not variables . issubset ( self . variables ) : raise ValueError ( "Cannot project to variables not in the constraint." ) idxs = [ i for i , v in enumerate ( self . variables ) if v in variables ] configurations = frozenset ( tuple ( config [ i ] for i in idxs ) for config in self . configurations ) variables = tuple ( self . variables [ i ] for i in idxs ) return self . from_configurations ( configurations , variables , self . vartype )
2094	def stdout ( self , pk , start_line = None , end_line = None , outfile = sys . stdout , * * kwargs ) : # resource is Unified Job Template if self . unified_job_type != self . endpoint : unified_job = self . last_job_data ( pk , * * kwargs ) pk = unified_job [ 'id' ] # resource is Unified Job, but pk not given elif not pk : unified_job = self . get ( * * kwargs ) pk = unified_job [ 'id' ] content = self . lookup_stdout ( pk , start_line , end_line ) opened = False if isinstance ( outfile , six . string_types ) : outfile = open ( outfile , 'w' ) opened = True if len ( content ) > 0 : click . echo ( content , nl = 1 , file = outfile ) if opened : outfile . close ( ) return { "changed" : False }
5753	def get_page_url ( page_num , current_app , url_view_name , url_extra_args , url_extra_kwargs , url_param_name , url_get_params , url_anchor ) : if url_view_name is not None : # Add page param to the kwargs list. Overrides any previously set parameter of the same name. url_extra_kwargs [ url_param_name ] = page_num try : url = reverse ( url_view_name , args = url_extra_args , kwargs = url_extra_kwargs , current_app = current_app ) except NoReverseMatch as e : # Attempt to load view from application root, allowing the use of non-namespaced view names if your view is defined in the root application if settings . SETTINGS_MODULE : if django . VERSION < ( 1 , 9 , 0 ) : separator = '.' else : separator = ':' # Namespace separator changed to colon after 1.8 project_name = settings . SETTINGS_MODULE . split ( '.' ) [ 0 ] try : url = reverse ( project_name + separator + url_view_name , args = url_extra_args , kwargs = url_extra_kwargs , current_app = current_app ) except NoReverseMatch : raise e # Raise the original exception so the error message doesn't confusingly include something the Developer didn't add to the view name themselves else : raise e # We can't determine the project name so just re-throw the exception else : url = '' url_get_params = url_get_params or QueryDict ( url ) url_get_params = url_get_params . copy ( ) url_get_params [ url_param_name ] = str ( page_num ) if len ( url_get_params ) > 0 : if not isinstance ( url_get_params , QueryDict ) : tmp = QueryDict ( mutable = True ) tmp . update ( url_get_params ) url_get_params = tmp url += '?' + url_get_params . urlencode ( ) if ( url_anchor is not None ) : url += '#' + url_anchor return url
12250	def _get_key_internal ( self , * args , * * kwargs ) : if args [ 1 ] is not None and 'force' in args [ 1 ] : key , res = super ( Bucket , self ) . _get_key_internal ( * args , * * kwargs ) if key : mimicdb . backend . sadd ( tpl . bucket % self . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( self . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) ) return key , res key = None if mimicdb . backend . sismember ( tpl . bucket % self . name , args [ 0 ] ) : key = Key ( self ) key . name = args [ 0 ] return key , None
8380	def drag ( self , node ) : dx = self . mouse . x - self . graph . x dy = self . mouse . y - self . graph . y # A dashed line indicates the drag vector. s = self . graph . styles . default self . _ctx . nofill ( ) self . _ctx . nostroke ( ) if s . stroke : self . _ctx . strokewidth ( s . strokewidth ) self . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . g , 0.75 ) p = self . _ctx . line ( node . x , node . y , dx , dy , draw = False ) try : p . _nsBezierPath . setLineDash_count_phase_ ( [ 2 , 4 ] , 2 , 50 ) except : pass self . _ctx . drawpath ( p ) r = node . __class__ ( None ) . r * 0.75 self . _ctx . oval ( dx - r / 2 , dy - r / 2 , r , r ) node . vx = dx / self . graph . d node . vy = dy / self . graph . d
1168	def b2a_qp ( data , quotetabs = False , istext = True , header = False ) : MAXLINESIZE = 76 # See if this string is using CRLF line ends lf = data . find ( '\n' ) crlf = lf > 0 and data [ lf - 1 ] == '\r' inp = 0 linelen = 0 odata = [ ] while inp < len ( data ) : c = data [ inp ] if ( c > '~' or c == '=' or ( header and c == '_' ) or ( c == '.' and linelen == 0 and ( inp + 1 == len ( data ) or data [ inp + 1 ] == '\n' or data [ inp + 1 ] == '\r' ) ) or ( not istext and ( c == '\r' or c == '\n' ) ) or ( ( c == '\t' or c == ' ' ) and ( inp + 1 == len ( data ) ) ) or ( c <= ' ' and c != '\r' and c != '\n' and ( quotetabs or ( not quotetabs and ( c != '\t' and c != ' ' ) ) ) ) ) : linelen += 3 if linelen >= MAXLINESIZE : odata . append ( '=' ) if crlf : odata . append ( '\r' ) odata . append ( '\n' ) linelen = 3 odata . append ( '=' + two_hex_digits ( ord ( c ) ) ) inp += 1 else : if ( istext and ( c == '\n' or ( inp + 1 < len ( data ) and c == '\r' and data [ inp + 1 ] == '\n' ) ) ) : linelen = 0 # Protect against whitespace on end of line if ( len ( odata ) > 0 and ( odata [ - 1 ] == ' ' or odata [ - 1 ] == '\t' ) ) : ch = ord ( odata [ - 1 ] ) odata [ - 1 ] = '=' odata . append ( two_hex_digits ( ch ) ) if crlf : odata . append ( '\r' ) odata . append ( '\n' ) if c == '\r' : inp += 2 else : inp += 1 else : if ( inp + 1 < len ( data ) and data [ inp + 1 ] != '\n' and ( linelen + 1 ) >= MAXLINESIZE ) : odata . append ( '=' ) if crlf : odata . append ( '\r' ) odata . append ( '\n' ) linelen = 0 linelen += 1 if header and c == ' ' : c = '_' odata . append ( c ) inp += 1 return '' . join ( odata )
11438	def _create_record_lxml ( marcxml , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : parser = etree . XMLParser ( dtd_validation = correct , recover = ( verbose <= 3 ) ) if correct : marcxml = '<?xml version="1.0" encoding="UTF-8"?>\n' '<collection>\n%s\n</collection>' % ( marcxml , ) try : tree = etree . parse ( StringIO ( marcxml ) , parser ) # parser errors are located in parser.error_log # if 1 <= verbose <=3 then show them to the user? # if verbose == 0 then continue # if verbose >3 then an exception will be thrown except Exception as e : raise InvenioBibRecordParserError ( str ( e ) ) record = { } field_position_global = 0 controlfield_iterator = tree . iter ( tag = '{*}controlfield' ) for controlfield in controlfield_iterator : tag = controlfield . attrib . get ( 'tag' , '!' ) . encode ( "UTF-8" ) ind1 = ' ' ind2 = ' ' text = controlfield . text if text is None : text = '' else : text = text . encode ( "UTF-8" ) subfields = [ ] if text or keep_singletons : field_position_global += 1 record . setdefault ( tag , [ ] ) . append ( ( subfields , ind1 , ind2 , text , field_position_global ) ) datafield_iterator = tree . iter ( tag = '{*}datafield' ) for datafield in datafield_iterator : tag = datafield . attrib . get ( 'tag' , '!' ) . encode ( "UTF-8" ) ind1 = datafield . attrib . get ( 'ind1' , '!' ) . encode ( "UTF-8" ) ind2 = datafield . attrib . get ( 'ind2' , '!' ) . encode ( "UTF-8" ) if ind1 in ( '' , '_' ) : ind1 = ' ' if ind2 in ( '' , '_' ) : ind2 = ' ' subfields = [ ] subfield_iterator = datafield . iter ( tag = '{*}subfield' ) for subfield in subfield_iterator : code = subfield . attrib . get ( 'code' , '!' ) . encode ( "UTF-8" ) text = subfield . text if text is None : text = '' else : text = text . encode ( "UTF-8" ) if text or keep_singletons : subfields . append ( ( code , text ) ) if subfields or keep_singletons : text = '' field_position_global += 1 record . setdefault ( tag , [ ] ) . append ( ( subfields , ind1 , ind2 , text , field_position_global ) ) return record
2133	def _workflow_node_structure ( node_results ) : # Build list address translation, and create backlink lists node_list_pos = { } for i , node_result in enumerate ( node_results ) : for rel in [ 'success' , 'failure' , 'always' ] : node_result [ '{0}_backlinks' . format ( rel ) ] = [ ] node_list_pos [ node_result [ 'id' ] ] = i # Populate backlink lists for node_result in node_results : for rel in [ 'success' , 'failure' , 'always' ] : for sub_node_id in node_result [ '{0}_nodes' . format ( rel ) ] : j = node_list_pos [ sub_node_id ] node_results [ j ] [ '{0}_backlinks' . format ( rel ) ] . append ( node_result [ 'id' ] ) # Find the root nodes root_nodes = [ ] for node_result in node_results : is_root = True for rel in [ 'success' , 'failure' , 'always' ] : if node_result [ '{0}_backlinks' . format ( rel ) ] != [ ] : is_root = False break if is_root : root_nodes . append ( node_result [ 'id' ] ) # Create network dictionary recursively from root nodes def branch_schema ( node_id ) : i = node_list_pos [ node_id ] node_dict = node_results [ i ] ret_dict = { "id" : node_id } for fd in NODE_STANDARD_FIELDS : val = node_dict . get ( fd , None ) if val is not None : if fd == 'unified_job_template' : job_type = node_dict [ 'summary_fields' ] [ 'unified_job_template' ] [ 'unified_job_type' ] ujt_key = JOB_TYPES [ job_type ] ret_dict [ ujt_key ] = val else : ret_dict [ fd ] = val for rel in [ 'success' , 'failure' , 'always' ] : sub_node_id_list = node_dict [ '{0}_nodes' . format ( rel ) ] if len ( sub_node_id_list ) == 0 : continue relationship_name = '{0}_nodes' . format ( rel ) ret_dict [ relationship_name ] = [ ] for sub_node_id in sub_node_id_list : ret_dict [ relationship_name ] . append ( branch_schema ( sub_node_id ) ) return ret_dict schema_dict = [ ] for root_node_id in root_nodes : schema_dict . append ( branch_schema ( root_node_id ) ) return schema_dict
8663	def generate_passphrase ( size = 12 ) : chars = string . ascii_lowercase + string . ascii_uppercase + string . digits return str ( '' . join ( random . choice ( chars ) for _ in range ( size ) ) )
2179	def fetch_request_token ( self , url , realm = None , * * request_kwargs ) : self . _client . client . realm = " " . join ( realm ) if realm else None token = self . _fetch_token ( url , * * request_kwargs ) log . debug ( "Resetting callback_uri and realm (not needed in next phase)." ) self . _client . client . callback_uri = None self . _client . client . realm = None return token
11861	def consistent_with ( event , evidence ) : return every ( lambda ( k , v ) : evidence . get ( k , v ) == v , event . items ( ) )
1682	def IncrementErrorCount ( self , category ) : self . error_count += 1 if self . counting in ( 'toplevel' , 'detailed' ) : if self . counting != 'detailed' : category = category . split ( '/' ) [ 0 ] if category not in self . errors_by_category : self . errors_by_category [ category ] = 0 self . errors_by_category [ category ] += 1
8651	def get_jobs ( session , job_ids , seo_details , lang ) : get_jobs_data = { 'jobs[]' : job_ids , 'seo_details' : seo_details , 'lang' : lang , } # GET /api/projects/0.1/jobs/ response = make_get_request ( session , 'jobs' , params_data = get_jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise JobsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
3513	def clicky ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ClickyNode ( )
2944	def accept_message ( self , message ) : assert not self . read_only self . refresh_waiting_tasks ( ) self . do_engine_steps ( ) for my_task in Task . Iterator ( self . task_tree , Task . WAITING ) : my_task . task_spec . accept_message ( my_task , message )
6755	def lenv ( self ) : _env = type ( env ) ( ) for _k , _v in six . iteritems ( env ) : if _k . startswith ( self . name + '_' ) : _env [ _k [ len ( self . name ) + 1 : ] ] = _v return _env
60	def union ( self , other ) : return BoundingBox ( x1 = min ( self . x1 , other . x1 ) , y1 = min ( self . y1 , other . y1 ) , x2 = max ( self . x2 , other . x2 ) , y2 = max ( self . y2 , other . y2 ) , )
77	def project_coords ( coords , from_shape , to_shape ) : from_shape = normalize_shape ( from_shape ) to_shape = normalize_shape ( to_shape ) if from_shape [ 0 : 2 ] == to_shape [ 0 : 2 ] : return coords from_height , from_width = from_shape [ 0 : 2 ] to_height , to_width = to_shape [ 0 : 2 ] assert all ( [ v > 0 for v in [ from_height , from_width , to_height , to_width ] ] ) # make sure to not just call np.float32(coords) here as the following lines # perform in-place changes and np.float32(.) only copies if the input # was *not* a float32 array coords_proj = np . array ( coords ) . astype ( np . float32 ) coords_proj [ : , 0 ] = ( coords_proj [ : , 0 ] / from_width ) * to_width coords_proj [ : , 1 ] = ( coords_proj [ : , 1 ] / from_height ) * to_height return coords_proj
4247	def id_by_addr ( self , addr ) : if self . _databaseType in ( const . PROXY_EDITION , const . NETSPEED_EDITION_REV1 , const . NETSPEED_EDITION_REV1_V6 ) : raise GeoIPError ( 'Invalid database type; this database is not supported' ) ipv = 6 if addr . find ( ':' ) >= 0 else 4 if ipv == 4 and self . _databaseType not in ( const . COUNTRY_EDITION , const . NETSPEED_EDITION ) : raise GeoIPError ( 'Invalid database type; this database supports IPv6 addresses, not IPv4' ) if ipv == 6 and self . _databaseType != const . COUNTRY_EDITION_V6 : raise GeoIPError ( 'Invalid database type; this database supports IPv4 addresses, not IPv6' ) ipnum = util . ip2long ( addr ) return self . _seek_country ( ipnum ) - const . COUNTRY_BEGIN
8913	def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if name in self . name_index : name = namesgenerator . get_random_name ( retry = True ) # check if service is already registered if name in self . name_index : if overwrite : self . _delete ( name = name ) else : raise Exception ( "service name already registered." ) self . _insert ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
4788	def is_alpha ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isalpha ( ) : self . _err ( 'Expected <%s> to contain only alphabetic chars, but did not.' % self . val ) return self
5301	def parse_colors ( path ) : if path . endswith ( ".txt" ) : return parse_rgb_txt_file ( path ) elif path . endswith ( ".json" ) : return parse_json_color_file ( path ) raise TypeError ( "colorful only supports .txt and .json files for colors" )
13447	def authorize ( self ) : response = self . client . login ( username = self . USERNAME , password = self . PASSWORD ) self . assertTrue ( response ) self . authed = True
10583	def create_account ( self , name , number = None , description = None ) : new_account = GeneralLedgerAccount ( name , description , number , self . account_type ) new_account . set_parent_path ( self . path ) self . accounts . append ( new_account ) return new_account
6062	def mass_within_circle_in_units ( self , radius : dim . Length , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : self . check_units_of_radius_and_critical_surface_density ( radius = radius , critical_surface_density = critical_surface_density ) profile = self . new_profile_with_units_converted ( unit_length = radius . unit_length , unit_mass = 'angular' , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) mass_angular = dim . Mass ( value = quad ( profile . mass_integral , a = 0.0 , b = radius , args = ( 1.0 , ) ) [ 0 ] , unit_mass = 'angular' ) return mass_angular . convert ( unit_mass = unit_mass , critical_surface_density = critical_surface_density )
9579	def read_cell_array ( fd , endian , header ) : array = [ list ( ) for i in range ( header [ 'dims' ] [ 0 ] ) ] for row in range ( header [ 'dims' ] [ 0 ] ) : for col in range ( header [ 'dims' ] [ 1 ] ) : # read the matrix header and array vheader , next_pos , fd_var = read_var_header ( fd , endian ) varray = read_var_array ( fd_var , endian , vheader ) array [ row ] . append ( varray ) # move on to next field fd . seek ( next_pos ) # pack and return the array if header [ 'dims' ] [ 0 ] == 1 : return squeeze ( array [ 0 ] ) return squeeze ( array )
6084	def blurred_image_of_planes_from_1d_images_and_convolver ( total_planes , image_plane_image_1d_of_planes , image_plane_blurring_image_1d_of_planes , convolver , map_to_scaled_array ) : blurred_image_of_planes = [ ] for plane_index in range ( total_planes ) : # If all entries are zero, there was no light profile / pixeization if np . count_nonzero ( image_plane_image_1d_of_planes [ plane_index ] ) > 0 : blurred_image_1d_of_plane = blurred_image_1d_from_1d_unblurred_and_blurring_images ( unblurred_image_1d = image_plane_image_1d_of_planes [ plane_index ] , blurring_image_1d = image_plane_blurring_image_1d_of_planes [ plane_index ] , convolver = convolver ) blurred_image_of_plane = map_to_scaled_array ( array_1d = blurred_image_1d_of_plane ) blurred_image_of_planes . append ( blurred_image_of_plane ) else : blurred_image_of_planes . append ( None ) return blurred_image_of_planes
3017	def from_json_keyfile_dict ( cls , keyfile_dict , scopes = '' , token_uri = None , revoke_uri = None ) : return cls . _from_parsed_json_keyfile ( keyfile_dict , scopes , token_uri = token_uri , revoke_uri = revoke_uri )
5603	def read_raster_window ( input_files , tile , indexes = None , resampling = "nearest" , src_nodata = None , dst_nodata = None , gdal_opts = None ) : with rasterio . Env ( * * get_gdal_options ( gdal_opts , is_remote = path_is_remote ( input_files [ 0 ] if isinstance ( input_files , list ) else input_files , s3 = True ) ) ) as env : logger . debug ( "reading %s with GDAL options %s" , input_files , env . options ) return _read_raster_window ( input_files , tile , indexes = indexes , resampling = resampling , src_nodata = src_nodata , dst_nodata = dst_nodata )
4438	async def _previous ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) try : await player . play_previous ( ) except lavalink . NoPreviousTrack : await ctx . send ( 'There is no previous song to play.' )
11085	def shutdown ( self , msg , args ) : self . log . info ( "Received shutdown from %s" , msg . user . username ) self . _bot . runnable = False return "Shutting down..."
11320	def update_date_year ( self ) : dates = record_get_field_instances ( self . record , '260' ) for field in dates : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'c' : field [ 0 ] [ idx ] = ( 'c' , value [ : 4 ] ) elif key == 't' : del field [ 0 ] [ idx ] if not dates : published_years = record_get_field_values ( self . record , "773" , code = "y" ) if published_years : record_add_field ( self . record , "260" , subfields = [ ( "c" , published_years [ 0 ] [ : 4 ] ) ] ) else : other_years = record_get_field_values ( self . record , "269" , code = "c" ) if other_years : record_add_field ( self . record , "260" , subfields = [ ( "c" , other_years [ 0 ] [ : 4 ] ) ] )
474	def save_vocab ( count = None , name = 'vocab.txt' ) : if count is None : count = [ ] pwd = os . getcwd ( ) vocabulary_size = len ( count ) with open ( os . path . join ( pwd , name ) , "w" ) as f : for i in xrange ( vocabulary_size ) : f . write ( "%s %d\n" % ( tf . compat . as_text ( count [ i ] [ 0 ] ) , count [ i ] [ 1 ] ) ) tl . logging . info ( "%d vocab saved to %s in %s" % ( vocabulary_size , name , pwd ) )
12740	def _parse_corporations ( self , datafield , subfield , roles = [ "any" ] ) : if len ( datafield ) != 3 : raise ValueError ( "datafield parameter have to be exactly 3 chars long!" ) if len ( subfield ) != 1 : raise ValueError ( "Bad subfield specification - subield have to be 3 chars long!" ) parsed_corporations = [ ] for corporation in self . get_subfields ( datafield , subfield ) : other_subfields = corporation . other_subfields # check if corporation have at least one of the roles specified in # 'roles' parameter of function if "4" in other_subfields and roles != [ "any" ] : corp_roles = other_subfields [ "4" ] # list of role parameters relevant = any ( map ( lambda role : role in roles , corp_roles ) ) # skip non-relevant corporations if not relevant : continue name = "" place = "" date = "" name = corporation if "c" in other_subfields : place = "," . join ( other_subfields [ "c" ] ) if "d" in other_subfields : date = "," . join ( other_subfields [ "d" ] ) parsed_corporations . append ( Corporation ( name , place , date ) ) return parsed_corporations
2785	def get_object ( cls , api_token , volume_id ) : volume = cls ( token = api_token , id = volume_id ) volume . load ( ) return volume
2491	def add_file_dependencies_helper ( self , doc_file ) : subj_triples = list ( self . graph . triples ( ( None , self . spdx_namespace . fileName , Literal ( doc_file . name ) ) ) ) if len ( subj_triples ) != 1 : raise InvalidDocumentError ( 'Could not find dependency subject {0}' . format ( doc_file . name ) ) subject_node = subj_triples [ 0 ] [ 0 ] for dependency in doc_file . dependencies : dep_triples = list ( self . graph . triples ( ( None , self . spdx_namespace . fileName , Literal ( dependency ) ) ) ) if len ( dep_triples ) == 1 : dep_node = dep_triples [ 0 ] [ 0 ] dep_triple = ( subject_node , self . spdx_namespace . fileDependency , dep_node ) self . graph . add ( dep_triple ) else : print ( 'Warning could not resolve file dependency {0} -> {1}' . format ( doc_file . name , dependency ) )
12797	def build_twisted_request ( self , method , url , extra_headers = { } , body_producer = None , full_url = False ) : uri = url if full_url else self . _url ( url ) raw_headers = self . get_headers ( ) if extra_headers : raw_headers . update ( extra_headers ) headers = http_headers . Headers ( ) for header in raw_headers : headers . addRawHeader ( header , raw_headers [ header ] ) agent = client . Agent ( reactor ) request = agent . request ( method , uri , headers , body_producer ) return ( reactor , request )
7227	def paint ( self ) : snippet = { 'fill-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'fill-color' : VectorStyle . get_style_value ( self . color ) , 'fill-outline-color' : VectorStyle . get_style_value ( self . outline_color ) } if self . translate : snippet [ 'fill-translate' ] = self . translate return snippet
5868	def _activate_organization_course_relationship ( relationship ) : # pylint: disable=invalid-name # If the relationship doesn't exist or the organization isn't active we'll want to raise an error relationship = internal . OrganizationCourse . objects . get ( id = relationship . id , active = False , organization__active = True ) _activate_record ( relationship )
4582	def construct ( cls , project , * , run = None , name = None , data = None , * * desc ) : from . failed import Failed exception = desc . pop ( '_exception' , None ) if exception : a = Failed ( project . layout , desc , exception ) else : try : a = cls ( project . layout , * * desc ) a . _set_runner ( run or { } ) except Exception as e : if cls . FAIL_ON_EXCEPTION : raise a = Failed ( project . layout , desc , e ) a . name = name a . data = data return a
4733	def generate_steady_rt_pic ( process_data , para_meter , scale , steady_time ) : pic_path_steady = para_meter [ 'filename' ] + '_steady.png' plt . figure ( figsize = ( 4 * scale , 2.5 * scale ) ) for key in process_data . keys ( ) : if len ( process_data [ key ] ) < steady_time : steady_time = len ( process_data [ key ] ) plt . scatter ( process_data [ key ] [ - 1 * steady_time : , 0 ] , process_data [ key ] [ - 1 * steady_time : , 1 ] , label = str ( key ) , s = 10 ) steady_value = np . mean ( process_data [ key ] [ - 1 * steady_time : , 1 ] ) steady_value_5 = steady_value * ( 1 + 0.05 ) steady_value_10 = steady_value * ( 1 + 0.1 ) steady_value_ng_5 = steady_value * ( 1 - 0.05 ) steady_value_ng_10 = steady_value * ( 1 - 0.1 ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value ] * steady_time , 'b' ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value_5 ] * steady_time , 'g' ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value_ng_5 ] * steady_time , 'g' ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value_10 ] * steady_time , 'r' ) plt . plot ( process_data [ key ] [ - 1 * steady_time : , 0 ] , [ steady_value_ng_10 ] * steady_time , 'r' ) plt . title ( para_meter [ 'title' ] + '(steady)' ) plt . xlabel ( para_meter [ 'x_axis_name' ] + '(steady)' ) plt . ylabel ( para_meter [ 'y_axis_name' ] + '(steady)' ) plt . legend ( loc = 'upper left' ) plt . savefig ( pic_path_steady ) return pic_path_steady
13822	def update_config ( new_config ) : flask_app . base_config . update ( new_config ) # Check for changed working directory. if new_config . has_key ( 'working_directory' ) : wd = os . path . abspath ( new_config [ 'working_directory' ] ) if nbmanager . notebook_dir != wd : if not os . path . exists ( wd ) : raise IOError ( 'Path not found: %s' % wd ) nbmanager . notebook_dir = wd
3851	async def lookup_entities ( client , args ) : lookup_spec = _get_lookup_spec ( args . entity_identifier ) request = hangups . hangouts_pb2 . GetEntityByIdRequest ( request_header = client . get_request_header ( ) , batch_lookup_spec = [ lookup_spec ] , ) res = await client . get_entity_by_id ( request ) # Print the list of entities in the response. for entity_result in res . entity_result : for entity in entity_result . entity : print ( entity )
11414	def record_get_subfields ( rec , tag , field_position_global = None , field_position_local = None ) : field = record_get_field ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) return field [ 0 ]
10411	def compare ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , Mapping [ str , float ] ] : canonical_mechanisms = get_subgraphs_by_annotation ( graph , annotation ) canonical_nodes = _transform_graph_dict_to_node_dict ( canonical_mechanisms ) candidate_mechanisms = generate_bioprocess_mechanisms ( graph ) candidate_nodes = _transform_graph_dict_to_node_dict ( candidate_mechanisms ) results : Dict [ str , Dict [ str , float ] ] = defaultdict ( dict ) it = itt . product ( canonical_nodes . items ( ) , candidate_nodes . items ( ) ) for ( canonical_name , canonical_graph ) , ( candidate_bp , candidate_graph ) in it : tanimoto = tanimoto_set_similarity ( candidate_nodes , canonical_nodes ) results [ canonical_name ] [ candidate_bp ] = tanimoto return dict ( results )
646	def generateVectors ( numVectors = 100 , length = 500 , activity = 50 ) : vectors = [ ] coinc = numpy . zeros ( length , dtype = 'int32' ) indexList = range ( length ) for i in xrange ( numVectors ) : coinc [ : ] = 0 coinc [ random . sample ( indexList , activity ) ] = 1 vectors . append ( coinc . copy ( ) ) return vectors
4181	def window_nuttall ( N ) : a0 = 0.355768 a1 = 0.487396 a2 = 0.144232 a3 = 0.012604 return _coeff4 ( N , a0 , a1 , a2 , a3 )
2954	def initialize ( self , containers ) : self . _containers = deepcopy ( containers ) self . __write ( containers , initialize = True )
11725	def camel2word ( string ) : def wordize ( match ) : return ' ' + match . group ( 1 ) . lower ( ) return string [ 0 ] + re . sub ( r'([A-Z])' , wordize , string [ 1 : ] )
1606	def run_containers ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] container_id = cl_args [ 'id' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) except : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False containers = result [ 'physical_plan' ] [ 'stmgrs' ] all_bolts , all_spouts = set ( ) , set ( ) for _ , bolts in result [ 'physical_plan' ] [ 'bolts' ] . items ( ) : all_bolts = all_bolts | set ( bolts ) for _ , spouts in result [ 'physical_plan' ] [ 'spouts' ] . items ( ) : all_spouts = all_spouts | set ( spouts ) stmgrs = containers . keys ( ) stmgrs . sort ( ) if container_id is not None : try : normalized_cid = container_id - 1 if normalized_cid < 0 : raise stmgrs = [ stmgrs [ normalized_cid ] ] except : Log . error ( 'Invalid container id: %d' % container_id ) return False table = [ ] for sid , name in enumerate ( stmgrs ) : cid = sid + 1 host = containers [ name ] [ "host" ] port = containers [ name ] [ "port" ] pid = containers [ name ] [ "pid" ] instances = containers [ name ] [ "instance_ids" ] bolt_nums = len ( [ instance for instance in instances if instance in all_bolts ] ) spout_nums = len ( [ instance for instance in instances if instance in all_spouts ] ) table . append ( [ cid , host , port , pid , bolt_nums , spout_nums , len ( instances ) ] ) headers = [ "container" , "host" , "port" , "pid" , "#bolt" , "#spout" , "#instance" ] sys . stdout . flush ( ) print ( tabulate ( table , headers = headers ) ) return True
11135	def copyto ( self , new_abspath = None , new_dirpath = None , new_dirname = None , new_basename = None , new_fname = None , new_ext = None , overwrite = False , makedirs = False ) : self . assert_exists ( ) p = self . change ( new_abspath = new_abspath , new_dirpath = new_dirpath , new_dirname = new_dirname , new_basename = new_basename , new_fname = new_fname , new_ext = new_ext , ) if p . is_not_exist_or_allow_overwrite ( overwrite = overwrite ) : # ๅฆๆไธคไธช่ทฏๅพไธๅ, ๆ่ฟ่กcopy if self . abspath != p . abspath : try : shutil . copy ( self . abspath , p . abspath ) except IOError as e : if makedirs : os . makedirs ( p . parent . abspath ) shutil . copy ( self . abspath , p . abspath ) else : raise e return p
5077	def get_closest_course_run ( course_runs ) : if len ( course_runs ) == 1 : return course_runs [ 0 ] now = datetime . datetime . now ( pytz . UTC ) # course runs with no start date should be considered last. never = now - datetime . timedelta ( days = 3650 ) return min ( course_runs , key = lambda x : abs ( get_course_run_start ( x , never ) - now ) )
7374	async def throw ( response , loads = None , encoding = None , * * kwargs ) : if loads is None : loads = data_processing . loads data = await data_processing . read ( response , loads = loads , encoding = encoding ) error = get_error ( data ) if error is not None : exception = errors [ error [ 'code' ] ] raise exception ( response = response , error = error , data = data , * * kwargs ) if response . status in statuses : exception = statuses [ response . status ] raise exception ( response = response , data = data , * * kwargs ) # raise PeonyException if no specific exception was found raise PeonyException ( response = response , data = data , * * kwargs )
13478	def _sentence_to_interstitial_spacing ( self ) : not_sentence_end_chars = [ ' ' ] abbreviations = [ 'i.e.' , 'e.g.' , ' v.' , ' w.' , ' wh.' ] titles = [ 'Prof.' , 'Mr.' , 'Mrs.' , 'Messrs.' , 'Mmes.' , 'Msgr.' , 'Ms.' , 'Fr.' , 'Rev.' , 'St.' , 'Dr.' , 'Lieut.' , 'Lt.' , 'Capt.' , 'Cptn.' , 'Sgt.' , 'Sjt.' , 'Gen.' , 'Hon.' , 'Cpl.' , 'L-Cpl.' , 'Pvt.' , 'Dvr.' , 'Gnr.' , 'Spr.' , 'Col.' , 'Lt-Col' , 'Lt-Gen.' , 'Mx.' ] for abbrev in abbreviations : for x in not_sentence_end_chars : self . _str_replacement ( abbrev + x , abbrev + '\ ' ) for title in titles : for x in not_sentence_end_chars : self . _str_replacement ( title + x , title + '~' )
6882	def describe_lcc_csv ( lcdict , returndesc = False ) : metadata_lines = [ ] coldef_lines = [ ] if 'lcformat' in lcdict and 'lcc-csv' in lcdict [ 'lcformat' ] . lower ( ) : metadata = lcdict [ 'metadata' ] metakeys = lcdict [ 'objectinfo' ] . keys ( ) coldefs = lcdict [ 'coldefs' ] for mk in metakeys : metadata_lines . append ( '%20s | %s' % ( mk , metadata [ mk ] [ 'desc' ] ) ) for ck in lcdict [ 'columns' ] : coldef_lines . append ( 'column %02d | %8s | numpy dtype: %3s | %s' % ( coldefs [ ck ] [ 'colnum' ] , ck , coldefs [ ck ] [ 'dtype' ] , coldefs [ ck ] [ 'desc' ] ) ) desc = LCC_CSVLC_DESCTEMPLATE . format ( objectid = lcdict [ 'objectid' ] , metadata_desc = '\n' . join ( metadata_lines ) , metadata = pformat ( lcdict [ 'objectinfo' ] ) , columndefs = '\n' . join ( coldef_lines ) ) print ( desc ) if returndesc : return desc else : LOGERROR ( "this lcdict is not from an LCC CSV, can't figure it out..." ) return None
6206	def _calc_hash_da ( self , rs ) : self . hash_d = hash_ ( rs . get_state ( ) ) [ : 6 ] self . hash_a = self . hash_d
13323	def format_objects ( objects , children = False , columns = None , header = True ) : columns = columns or ( 'NAME' , 'TYPE' , 'PATH' ) objects = sorted ( objects , key = _type_and_name ) data = [ ] for obj in objects : if isinstance ( obj , cpenv . VirtualEnvironment ) : data . append ( get_info ( obj ) ) modules = obj . get_modules ( ) if children and modules : for mod in modules : data . append ( get_info ( mod , indent = 2 , root = obj . path ) ) else : data . append ( get_info ( obj ) ) maxes = [ len ( max ( col , key = len ) ) for col in zip ( * data ) ] tmpl = '{:%d} {:%d} {:%d}' % tuple ( maxes ) lines = [ ] if header : lines . append ( '\n' + bold_blue ( tmpl . format ( * columns ) ) ) for obj_data in data : lines . append ( tmpl . format ( * obj_data ) ) return '\n' . join ( lines )
10360	def is_edge_consistent ( graph , u , v ) : if not graph . has_edge ( u , v ) : raise ValueError ( '{} does not contain an edge ({}, {})' . format ( graph , u , v ) ) return 0 == len ( set ( d [ RELATION ] for d in graph . edge [ u ] [ v ] . values ( ) ) )
483	def getSwarmModelParams ( modelID ) : # TODO: the use of nupic.frameworks.opf.helpers.loadExperimentDescriptionScriptFromDir when # retrieving module params results in a leakage of pf_base_descriptionNN and # pf_descriptionNN module imports for every call to getSwarmModelParams, so # the leakage is unlimited when getSwarmModelParams is called by a # long-running process. An alternate solution is to execute the guts of # this function's logic in a seprate process (via multiprocessing module). cjDAO = ClientJobsDAO . get ( ) ( jobID , description ) = cjDAO . modelsGetFields ( modelID , [ "jobId" , "genDescription" ] ) ( baseDescription , ) = cjDAO . jobGetFields ( jobID , [ "genBaseDescription" ] ) # Construct a directory with base.py and description.py for loading model # params, and use nupic.frameworks.opf.helpers to extract model params from # those files descriptionDirectory = tempfile . mkdtemp ( ) try : baseDescriptionFilePath = os . path . join ( descriptionDirectory , "base.py" ) with open ( baseDescriptionFilePath , mode = "wb" ) as f : f . write ( baseDescription ) descriptionFilePath = os . path . join ( descriptionDirectory , "description.py" ) with open ( descriptionFilePath , mode = "wb" ) as f : f . write ( description ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( helpers . loadExperimentDescriptionScriptFromDir ( descriptionDirectory ) ) return json . dumps ( dict ( modelConfig = expIface . getModelDescription ( ) , inferenceArgs = expIface . getModelControl ( ) . get ( "inferenceArgs" , None ) ) ) finally : shutil . rmtree ( descriptionDirectory , ignore_errors = True )
8524	def add_enum ( self , name , choices ) : if not isinstance ( choices , Iterable ) : raise ValueError ( 'variable %s: choices must be iterable' % name ) self . variables [ name ] = EnumVariable ( name , choices )
1894	def _send ( self , cmd : str ) : logger . debug ( '>%s' , cmd ) try : self . _proc . stdout . flush ( ) self . _proc . stdin . write ( f'{cmd}\n' ) except IOError as e : raise SolverError ( str ( e ) )
8283	def _curvepoint ( self , t , x0 , y0 , x1 , y1 , x2 , y2 , x3 , y3 , handles = False ) : # Originally from nodebox-gl mint = 1 - t x01 = x0 * mint + x1 * t y01 = y0 * mint + y1 * t x12 = x1 * mint + x2 * t y12 = y1 * mint + y2 * t x23 = x2 * mint + x3 * t y23 = y2 * mint + y3 * t out_c1x = x01 * mint + x12 * t out_c1y = y01 * mint + y12 * t out_c2x = x12 * mint + x23 * t out_c2y = y12 * mint + y23 * t out_x = out_c1x * mint + out_c2x * t out_y = out_c1y * mint + out_c2y * t if not handles : return ( out_x , out_y , out_c1x , out_c1y , out_c2x , out_c2y ) else : return ( out_x , out_y , out_c1x , out_c1y , out_c2x , out_c2y , x01 , y01 , x23 , y23 )
10067	def json_files_serializer ( objs , status = None ) : files = [ file_serializer ( obj ) for obj in objs ] return make_response ( json . dumps ( files ) , status )
2878	def serialize_value ( self , parent_elem , value ) : if isinstance ( value , ( str , int ) ) or type ( value ) . __name__ == 'str' : parent_elem . text = str ( value ) elif value is None : parent_elem . text = None else : parent_elem . append ( value . serialize ( self ) )
9560	def _apply_assert_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'assert' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except AssertionError as e : code = ASSERT_CHECK_FAILED message = MESSAGES [ ASSERT_CHECK_FAILED ] if len ( e . args ) > 0 : custom = e . args [ 0 ] if isinstance ( custom , ( list , tuple ) ) : if len ( custom ) > 0 : code = custom [ 0 ] if len ( custom ) > 1 : message = custom [ 1 ] else : code = custom p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
827	def _getInputValue ( self , obj , fieldName ) : if isinstance ( obj , dict ) : if not fieldName in obj : knownFields = ", " . join ( key for key in obj . keys ( ) if not key . startswith ( "_" ) ) raise ValueError ( "Unknown field name '%s' in input record. Known fields are '%s'.\n" "This could be because input headers are mislabeled, or because " "input data rows do not contain a value for '%s'." % ( fieldName , knownFields , fieldName ) ) return obj [ fieldName ] else : return getattr ( obj , fieldName )
13264	def get_configuration ( self , key , default = None ) : if key in self . config : return self . config . get ( key ) else : return default
4842	def get_program_type_by_slug ( self , slug ) : return self . _load_data ( self . PROGRAM_TYPES_ENDPOINT , resource_id = slug , default = None , )
1100	def unified_diff ( a , b , fromfile = '' , tofile = '' , fromfiledate = '' , tofiledate = '' , n = 3 , lineterm = '\n' ) : started = False for group in SequenceMatcher ( None , a , b ) . get_grouped_opcodes ( n ) : if not started : started = True # fromdate = '\t{}'.format(fromfiledate) if fromfiledate else '' fromdate = '\t%s' % ( fromfiledate ) if fromfiledate else '' # todate = '\t{}'.format(tofiledate) if tofiledate else '' todate = '\t%s' % ( tofiledate ) if tofiledate else '' # yield '--- {}{}{}'.format(fromfile, fromdate, lineterm) yield '--- %s%s%s' % ( fromfile , fromdate , lineterm ) # yield '+++ {}{}{}'.format(tofile, todate, lineterm) yield '+++ %s%s%s' % ( tofile , todate , lineterm ) first , last = group [ 0 ] , group [ - 1 ] file1_range = _format_range_unified ( first [ 1 ] , last [ 2 ] ) file2_range = _format_range_unified ( first [ 3 ] , last [ 4 ] ) # yield '@@ -{} +{} @@{}'.format(file1_range, file2_range, lineterm) yield '@@ -%s +%s @@%s' % ( file1_range , file2_range , lineterm ) for tag , i1 , i2 , j1 , j2 in group : if tag == 'equal' : for line in a [ i1 : i2 ] : yield ' ' + line continue if tag in ( 'replace' , 'delete' ) : for line in a [ i1 : i2 ] : yield '-' + line if tag in ( 'replace' , 'insert' ) : for line in b [ j1 : j2 ] : yield '+' + line
1255	def setup_scaffold ( self ) : if self . execution_type == "single" : global_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) # global_variables += [self.global_episode, self.global_timestep] init_op = tf . variables_initializer ( var_list = global_variables ) if self . summarizer_init_op is not None : init_op = tf . group ( init_op , self . summarizer_init_op ) if self . graph_summary is None : ready_op = tf . report_uninitialized_variables ( var_list = global_variables ) ready_for_local_init_op = None local_init_op = None else : ready_op = None ready_for_local_init_op = tf . report_uninitialized_variables ( var_list = global_variables ) local_init_op = self . graph_summary else : # Global and local variable initializers. global_variables = self . global_model . get_variables ( include_submodules = True , include_nontrainable = True ) # global_variables += [self.global_episode, self.global_timestep] local_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) init_op = tf . variables_initializer ( var_list = global_variables ) if self . summarizer_init_op is not None : init_op = tf . group ( init_op , self . summarizer_init_op ) ready_op = tf . report_uninitialized_variables ( var_list = ( global_variables + local_variables ) ) ready_for_local_init_op = tf . report_uninitialized_variables ( var_list = global_variables ) if self . graph_summary is None : local_init_op = tf . group ( tf . variables_initializer ( var_list = local_variables ) , # Synchronize values of trainable variables. * ( tf . assign ( ref = local_var , value = global_var ) for local_var , global_var in zip ( self . get_variables ( include_submodules = True ) , self . global_model . get_variables ( include_submodules = True ) ) ) ) else : local_init_op = tf . group ( tf . variables_initializer ( var_list = local_variables ) , self . graph_summary , # Synchronize values of trainable variables. * ( tf . assign ( ref = local_var , value = global_var ) for local_var , global_var in zip ( self . get_variables ( include_submodules = True ) , self . global_model . get_variables ( include_submodules = True ) ) ) ) def init_fn ( scaffold , session ) : if self . saver_spec is not None and self . saver_spec . get ( 'load' , True ) : directory = self . saver_spec [ 'directory' ] file = self . saver_spec . get ( 'file' ) if file is None : file = tf . train . latest_checkpoint ( checkpoint_dir = directory , latest_filename = None # Corresponds to argument of saver.save() in Model.save(). ) elif not os . path . isfile ( file ) : file = os . path . join ( directory , file ) if file is not None : try : scaffold . saver . restore ( sess = session , save_path = file ) session . run ( fetches = self . list_buffer_index_reset_op ) except tf . errors . NotFoundError : raise TensorForceError ( "Error: Existing checkpoint could not be loaded! Set \"load\" to false in saver_spec." ) # TensorFlow scaffold object # TODO explain what it does. self . scaffold = tf . train . Scaffold ( init_op = init_op , init_feed_dict = None , init_fn = init_fn , ready_op = ready_op , ready_for_local_init_op = ready_for_local_init_op , local_init_op = local_init_op , summary_op = None , saver = self . saver , copy_from_scaffold = None )
6892	def serial_starfeatures ( lclist , outdir , lc_catalog_pickle , neighbor_radius_arcsec , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None ) : # make sure to make the output directory if it doesn't exist if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : lclist = lclist [ : maxobjects ] # read in the kdtree pickle with open ( lc_catalog_pickle , 'rb' ) as infd : kdt_dict = pickle . load ( infd ) kdt = kdt_dict [ 'kdtree' ] objlist = kdt_dict [ 'objects' ] [ 'objectid' ] objlcfl = kdt_dict [ 'objects' ] [ 'lcfname' ] tasks = [ ( x , outdir , kdt , objlist , objlcfl , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat , lcformatdir ) for x in lclist ] for task in tqdm ( tasks ) : result = _starfeatures_worker ( task ) return result
7429	def draw ( self , axes ) : ## create a toytree object from the treemix tree result tre = toytree . tree ( newick = self . results . tree ) tre . draw ( axes = axes , use_edge_lengths = True , tree_style = 'c' , tip_labels_align = True , edge_align_style = { "stroke-width" : 1 } ) ## get coords for admix in self . results . admixture : ## parse admix event pidx , pdist , cidx , cdist , weight = admix a = _get_admix_point ( tre , pidx , pdist ) b = _get_admix_point ( tre , cidx , cdist ) ## add line for admixture edge mark = axes . plot ( a = ( a [ 0 ] , b [ 0 ] ) , b = ( a [ 1 ] , b [ 1 ] ) , style = { "stroke-width" : 10 * weight , "stroke-opacity" : 0.95 , "stroke-linecap" : "round" } ) ## add points at admixture sink axes . scatterplot ( a = ( b [ 0 ] ) , b = ( b [ 1 ] ) , size = 8 , title = "weight: {}" . format ( weight ) , ) ## add scale bar for edge lengths axes . y . show = False axes . x . ticks . show = True axes . x . label . text = "Drift parameter" return axes
5636	def mod2md ( module , title , title_api_section , toc = True , maxdepth = 0 ) : docstr = module . __doc__ text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 api_md = [ ] api_sec = [ ] if title_api_section and module . __all__ : sections . append ( ( level + 1 , title_api_section ) ) for name in module . __all__ : api_sec . append ( ( level + 2 , "`" + name + "`" ) ) api_md += [ '' , '' ] entry = module . __dict__ [ name ] if entry . __doc__ : md , sec = doc2md ( entry . __doc__ , "`" + name + "`" , min_level = level + 2 , more_info = True , toc = False ) api_sec += sec api_md += md sections += api_sec # headline head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] # main sections if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] ) # API section md += [ '' , '' , make_heading ( level + 1 , title_api_section ) , ] if toc : md += [ '' ] md += make_toc ( api_sec , 1 ) md += api_md return "\n" . join ( md )
5493	def validate_config_key ( ctx , param , value ) : if not value : return value try : section , item = value . split ( "." , 1 ) except ValueError : raise click . BadArgumentUsage ( "Given key does not contain a section name." ) else : return section , item
10666	def stoichiometry_coefficients ( compound , elements ) : stoichiometry = parse_compound ( compound . strip ( ) ) . count ( ) return [ stoichiometry [ element ] for element in elements ]
5564	def input ( self ) : # the delimiters are used by some input drivers delimiters = dict ( zoom = self . init_zoom_levels , bounds = self . init_bounds , process_bounds = self . bounds , effective_bounds = self . effective_bounds ) # get input items only of initialized zoom levels raw_inputs = { # convert input definition to hash get_hash ( v ) : v for zoom in self . init_zoom_levels if "input" in self . _params_at_zoom [ zoom ] # to preserve file groups, "flatten" the input tree and use # the tree paths as keys for key , v in _flatten_tree ( self . _params_at_zoom [ zoom ] [ "input" ] ) if v is not None } initalized_inputs = { } for k , v in raw_inputs . items ( ) : # for files and tile directories if isinstance ( v , str ) : logger . debug ( "load input reader for simple input %s" , v ) try : reader = load_input_reader ( dict ( path = absolute_path ( path = v , base_dir = self . config_dir ) , pyramid = self . process_pyramid , pixelbuffer = self . process_pyramid . pixelbuffer , delimiters = delimiters ) , readonly = self . mode == "readonly" ) except Exception as e : logger . exception ( e ) raise MapcheteDriverError ( "error when loading input %s: %s" % ( v , e ) ) logger . debug ( "input reader for simple input %s is %s" , v , reader ) # for abstract inputs elif isinstance ( v , dict ) : logger . debug ( "load input reader for abstract input %s" , v ) try : reader = load_input_reader ( dict ( abstract = deepcopy ( v ) , pyramid = self . process_pyramid , pixelbuffer = self . process_pyramid . pixelbuffer , delimiters = delimiters , conf_dir = self . config_dir ) , readonly = self . mode == "readonly" ) except Exception as e : logger . exception ( e ) raise MapcheteDriverError ( "error when loading input %s: %s" % ( v , e ) ) logger . debug ( "input reader for abstract input %s is %s" , v , reader ) else : raise MapcheteConfigError ( "invalid input type %s" , type ( v ) ) # trigger bbox creation reader . bbox ( out_crs = self . process_pyramid . crs ) initalized_inputs [ k ] = reader return initalized_inputs
5662	def return_segments ( shape , break_points ) : # print 'xxx' # print stops # print shape # print break_points # assert len(stops) == len(break_points) segs = [ ] bp = 0 # not used bp2 = 0 for i in range ( len ( break_points ) - 1 ) : bp = break_points [ i ] if break_points [ i ] is not None else bp2 bp2 = break_points [ i + 1 ] if break_points [ i + 1 ] is not None else bp segs . append ( shape [ bp : bp2 + 1 ] ) segs . append ( [ ] ) return segs
13572	def download ( course , tid = None , dl_all = False , force = False , upgradejava = False , update = False ) : def dl ( id ) : download_exercise ( Exercise . get ( Exercise . tid == id ) , force = force , update_java = upgradejava , update = update ) if dl_all : for exercise in list ( course . exercises ) : dl ( exercise . tid ) elif tid is not None : dl ( int ( tid ) ) else : for exercise in list ( course . exercises ) : if not exercise . is_completed : dl ( exercise . tid ) else : exercise . update_downloaded ( )
3628	def pad_cells ( table ) : col_sizes = [ max ( map ( len , col ) ) for col in zip ( * table ) ] for row in table : for cell_num , cell in enumerate ( row ) : row [ cell_num ] = pad_to ( cell , col_sizes [ cell_num ] ) return table
3415	def model_from_dict ( obj ) : if 'reactions' not in obj : raise ValueError ( 'Object has no reactions attribute. Cannot load.' ) model = Model ( ) model . add_metabolites ( [ metabolite_from_dict ( metabolite ) for metabolite in obj [ 'metabolites' ] ] ) model . genes . extend ( [ gene_from_dict ( gene ) for gene in obj [ 'genes' ] ] ) model . add_reactions ( [ reaction_from_dict ( reaction , model ) for reaction in obj [ 'reactions' ] ] ) objective_reactions = [ rxn for rxn in obj [ 'reactions' ] if rxn . get ( 'objective_coefficient' , 0 ) != 0 ] coefficients = { model . reactions . get_by_id ( rxn [ 'id' ] ) : rxn [ 'objective_coefficient' ] for rxn in objective_reactions } set_objective ( model , coefficients ) for k , v in iteritems ( obj ) : if k in { 'id' , 'name' , 'notes' , 'compartments' , 'annotation' } : setattr ( model , k , v ) return model
1613	def IsErrorSuppressedByNolint ( category , linenum ) : return ( _global_error_suppressions . get ( category , False ) or linenum in _error_suppressions . get ( category , set ( ) ) or linenum in _error_suppressions . get ( None , set ( ) ) )
13840	def ConsumeBool ( self ) : try : result = ParseBool ( self . token ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
16	def value ( self , t ) : for ( l_t , l ) , ( r_t , r ) in zip ( self . _endpoints [ : - 1 ] , self . _endpoints [ 1 : ] ) : if l_t <= t and t < r_t : alpha = float ( t - l_t ) / ( r_t - l_t ) return self . _interpolation ( l , r , alpha ) # t does not belong to any of the pieces, so doom. assert self . _outside_value is not None return self . _outside_value
1841	def JNO ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . OF , target . read ( ) , cpu . PC )
5642	def compute_pseudo_connections ( transit_connections , start_time_dep , end_time_dep , transfer_margin , walk_network , walk_speed ) : # A pseudo-connection should be created after (each) arrival to a transit_connection's arrival stop. pseudo_connection_set = set ( ) # use a set to ignore possible duplicates for c in transit_connections : if start_time_dep <= c . departure_time <= end_time_dep : walk_arr_stop = c . departure_stop walk_arr_time = c . departure_time - transfer_margin for _ , walk_dep_stop , data in walk_network . edges ( nbunch = [ walk_arr_stop ] , data = True ) : walk_dep_time = walk_arr_time - data [ 'd_walk' ] / float ( walk_speed ) if walk_dep_time > end_time_dep or walk_dep_time < start_time_dep : continue pseudo_connection = Connection ( walk_dep_stop , walk_arr_stop , walk_dep_time , walk_arr_time , Connection . WALK_TRIP_ID , Connection . WALK_SEQ , is_walk = True ) pseudo_connection_set . add ( pseudo_connection ) return pseudo_connection_set
3965	def restart_apps_or_services ( app_or_service_names = None ) : if app_or_service_names : log_to_client ( "Restarting the following apps or services: {}" . format ( ', ' . join ( app_or_service_names ) ) ) else : log_to_client ( "Restarting all active containers associated with Dusty" ) if app_or_service_names : specs = spec_assembler . get_assembled_specs ( ) specs_list = [ specs [ 'apps' ] [ app_name ] for app_name in app_or_service_names if app_name in specs [ 'apps' ] ] repos = set ( ) for spec in specs_list : if spec [ 'repo' ] : repos = repos . union ( spec_assembler . get_same_container_repos_from_spec ( spec ) ) nfs . update_nfs_with_repos ( repos ) else : nfs . update_nfs_with_repos ( spec_assembler . get_all_repos ( active_only = True , include_specs_repo = False ) ) compose . restart_running_services ( app_or_service_names )
2267	def invert_dict ( dict_ , unique_vals = True ) : if unique_vals : if isinstance ( dict_ , OrderedDict ) : inverted = OrderedDict ( ( val , key ) for key , val in dict_ . items ( ) ) else : inverted = { val : key for key , val in dict_ . items ( ) } else : # Handle non-unique keys using groups inverted = defaultdict ( set ) for key , value in dict_ . items ( ) : inverted [ value ] . add ( key ) inverted = dict ( inverted ) return inverted
12195	def get_app_locations ( ) : return [ os . path . dirname ( os . path . normpath ( import_module ( app_name ) . __file__ ) ) for app_name in PROJECT_APPS ]
9195	def get_publication ( request ) : publication_id = request . matchdict [ 'id' ] state , messages = check_publication_state ( publication_id ) response_data = { 'publication' : publication_id , 'state' : state , 'messages' : messages , } return response_data
2909	def _find_any ( self , task_spec ) : tasks = [ ] if self . task_spec == task_spec : tasks . append ( self ) for child in self : if child . task_spec != task_spec : continue tasks . append ( child ) return tasks
8781	def create_locks ( context , network_ids , addresses ) : for address in addresses : address_model = None try : address_model = _find_or_create_address ( context , network_ids , address ) lock_holder = None if address_model . lock_id : lock_holder = db_api . lock_holder_find ( context , lock_id = address_model . lock_id , name = LOCK_NAME , scope = db_api . ONE ) if not lock_holder : LOG . info ( "Creating lock holder on IPAddress %s with id %s" , address_model . address_readable , address_model . id ) db_api . lock_holder_create ( context , address_model , name = LOCK_NAME , type = "ip_address" ) except Exception : LOG . exception ( "Failed to create lock holder on IPAddress %s" , address_model ) continue context . session . flush ( )
9881	def _reliability_data_to_value_counts ( reliability_data , value_domain ) : return np . array ( [ [ sum ( 1 for rate in unit if rate == v ) for v in value_domain ] for unit in reliability_data . T ] )
2014	def _top ( self , n = 0 ) : if len ( self . stack ) - n < 0 : raise StackUnderflow ( ) return self . stack [ n - 1 ]
2364	def append ( self , linenumber , raw_text , cells ) : self . rows . append ( Row ( linenumber , raw_text , cells ) )
8280	def _render_closure ( self ) : fillcolor = self . fill strokecolor = self . stroke strokewidth = self . strokewidth def _render ( cairo_ctx ) : ''' At the moment this is based on cairo. TODO: Need to work out how to move the cairo specific bits somewhere else. ''' # Go to initial point (CORNER or CENTER): transform = self . _call_transform_mode ( self . _transform ) if fillcolor is None and strokecolor is None : # Fixes _bug_FillStrokeNofillNostroke.bot return cairo_ctx . set_matrix ( transform ) # Run the path commands on the cairo context: self . _traverse ( cairo_ctx ) # Matrix affects stroke, so we need to reset it: cairo_ctx . set_matrix ( cairo . Matrix ( ) ) if fillcolor is not None and strokecolor is not None : if strokecolor [ 3 ] < 1 : # Draw onto intermediate surface so that stroke # does not overlay fill cairo_ctx . push_group ( ) cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill_preserve ( ) e = cairo_ctx . stroke_extents ( ) cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_operator ( cairo . OPERATOR_SOURCE ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) cairo_ctx . pop_group_to_source ( ) cairo_ctx . paint ( ) else : # Fast path if no alpha in stroke cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill_preserve ( ) cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) elif fillcolor is not None : cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill ( ) elif strokecolor is not None : cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) return _render
5052	def get_course_completions ( self , enterprise_customer , days ) : return PersistentCourseGrade . objects . filter ( passed_timestamp__gt = datetime . datetime . now ( ) - datetime . timedelta ( days = days ) ) . filter ( user_id__in = enterprise_customer . enterprise_customer_users . values_list ( 'user_id' , flat = True ) )
5069	def ungettext_min_max ( singular , plural , range_text , min_val , max_val ) : if min_val is None and max_val is None : return None if min_val == max_val or min_val is None or max_val is None : # pylint: disable=translation-of-non-string return ungettext ( singular , plural , min_val or max_val ) . format ( min_val or max_val ) return range_text . format ( min_val , max_val )
3245	def get_inline_policies ( group , * * conn ) : policy_list = list_group_policies ( group [ 'GroupName' ] ) policy_documents = { } for policy in policy_list : policy_documents [ policy ] = get_group_policy_document ( group [ 'GroupName' ] , policy , * * conn ) return policy_documents
8050	def load_source ( self ) : if self . filename in self . STDIN_NAMES : self . filename = "stdin" if sys . version_info [ 0 ] < 3 : self . source = sys . stdin . read ( ) else : self . source = TextIOWrapper ( sys . stdin . buffer , errors = "ignore" ) . read ( ) else : # Could be a Python 2.7 StringIO with no context manager, sigh. # with tokenize_open(self.filename) as fd: # self.source = fd.read() handle = tokenize_open ( self . filename ) self . source = handle . read ( ) handle . close ( )
6143	def DSP_capture_add_samples_stereo ( self , new_data_left , new_data_right ) : self . capture_sample_count = self . capture_sample_count + len ( new_data_left ) + len ( new_data_right ) if self . Tcapture > 0 : self . data_capture_left = np . hstack ( ( self . data_capture_left , new_data_left ) ) self . data_capture_right = np . hstack ( ( self . data_capture_right , new_data_right ) ) if ( len ( self . data_capture_left ) > self . Ncapture ) : self . data_capture_left = self . data_capture_left [ - self . Ncapture : ] if ( len ( self . data_capture_right ) > self . Ncapture ) : self . data_capture_right = self . data_capture_right [ - self . Ncapture : ]
9440	def request ( self , path , method = None , data = { } ) : if not path : raise ValueError ( 'Invalid path parameter' ) if method and method not in [ 'GET' , 'POST' , 'DELETE' , 'PUT' ] : raise NotImplementedError ( 'HTTP %s method not implemented' % method ) if path [ 0 ] == '/' : uri = self . url + path else : uri = self . url + '/' + path if APPENGINE : return json . loads ( self . _appengine_fetch ( uri , data , method ) ) return json . loads ( self . _urllib2_fetch ( uri , data , method ) )
11214	def compare_signature ( expected : Union [ str , bytes ] , actual : Union [ str , bytes ] ) -> bool : expected = util . to_bytes ( expected ) actual = util . to_bytes ( actual ) return hmac . compare_digest ( expected , actual )
10061	def schemaforms ( self ) : _schemaforms = { k : v [ 'schemaform' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'schemaform' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_SCHEMAFORM' ] , _schemaforms )
101	def draw_text ( img , y , x , text , color = ( 0 , 255 , 0 ) , size = 25 ) : do_assert ( img . dtype in [ np . uint8 , np . float32 ] ) input_dtype = img . dtype if img . dtype == np . float32 : img = img . astype ( np . uint8 ) img = PIL_Image . fromarray ( img ) font = PIL_ImageFont . truetype ( DEFAULT_FONT_FP , size ) context = PIL_ImageDraw . Draw ( img ) context . text ( ( x , y ) , text , fill = tuple ( color ) , font = font ) img_np = np . asarray ( img ) # PIL/asarray returns read only array if not img_np . flags [ "WRITEABLE" ] : try : # this seems to no longer work with np 1.16 (or was pillow updated?) img_np . setflags ( write = True ) except ValueError as ex : if "cannot set WRITEABLE flag to True of this array" in str ( ex ) : img_np = np . copy ( img_np ) if img_np . dtype != input_dtype : img_np = img_np . astype ( input_dtype ) return img_np
9453	def play ( self , call_params ) : path = '/' + self . api_version + '/Play/' method = 'POST' return self . request ( path , method , call_params )
8240	def tetrad ( clr , angle = 90 ) : clr = color ( clr ) colors = colorlist ( clr ) c = clr . rotate_ryb ( angle ) if clr . brightness < 0.5 : c . brightness += 0.2 else : c . brightness -= - 0.2 colors . append ( c ) c = clr . rotate_ryb ( angle * 2 ) if clr . brightness < 0.5 : c . brightness += 0.1 else : c . brightness -= - 0.1 colors . append ( c ) colors . append ( clr . rotate_ryb ( angle * 3 ) . lighten ( 0.1 ) ) return colors
7156	def get_operator ( self , op ) : if op in self . OPERATORS : return self . OPERATORS . get ( op ) try : n_args = len ( inspect . getargspec ( op ) [ 0 ] ) if n_args != 2 : raise TypeError except : eprint ( 'Error: invalid operator function. Operators must accept two args.' ) raise else : return op
13220	def settings ( self ) : stmt = "select {fields} from pg_settings" . format ( fields = ', ' . join ( SETTINGS_FIELDS ) ) settings = [ ] for row in self . _iter_results ( stmt ) : row [ 'setting' ] = self . _vartype_map [ row [ 'vartype' ] ] ( row [ 'setting' ] ) settings . append ( Settings ( * * row ) ) return settings
5412	def build_action ( name = None , image_uri = None , commands = None , entrypoint = None , environment = None , pid_namespace = None , flags = None , port_mappings = None , mounts = None , labels = None ) : return { 'name' : name , 'imageUri' : image_uri , 'commands' : commands , 'entrypoint' : entrypoint , 'environment' : environment , 'pidNamespace' : pid_namespace , 'flags' : flags , 'portMappings' : port_mappings , 'mounts' : mounts , 'labels' : labels , }
4022	def docker_vm_is_running ( ) : running_vms = check_output_demoted ( [ 'VBoxManage' , 'list' , 'runningvms' ] ) for line in running_vms . splitlines ( ) : if '"{}"' . format ( constants . VM_MACHINE_NAME ) in line : return True return False
1795	def SBB ( cpu , dest , src ) : cpu . _SUB ( dest , src , carry = True )
5276	def _terminalSymbolsGenerator ( self ) : py2 = sys . version [ 0 ] < '3' UPPAs = list ( list ( range ( 0xE000 , 0xF8FF + 1 ) ) + list ( range ( 0xF0000 , 0xFFFFD + 1 ) ) + list ( range ( 0x100000 , 0x10FFFD + 1 ) ) ) for i in UPPAs : if py2 : yield ( unichr ( i ) ) else : yield ( chr ( i ) ) raise ValueError ( "To many input strings." )
13727	def set_delegate ( address = None , pubkey = None , secret = None ) : c . DELEGATE [ 'ADDRESS' ] = address c . DELEGATE [ 'PUBKEY' ] = pubkey c . DELEGATE [ 'PASSPHRASE' ] = secret
3195	def delete ( self , list_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) )
3116	def oauth2_callback ( request ) : if 'error' in request . GET : reason = request . GET . get ( 'error_description' , request . GET . get ( 'error' , '' ) ) reason = html . escape ( reason ) return http . HttpResponseBadRequest ( 'Authorization failed {0}' . format ( reason ) ) try : encoded_state = request . GET [ 'state' ] code = request . GET [ 'code' ] except KeyError : return http . HttpResponseBadRequest ( 'Request missing state or authorization code' ) try : server_csrf = request . session [ _CSRF_KEY ] except KeyError : return http . HttpResponseBadRequest ( 'No existing session for this flow.' ) try : state = json . loads ( encoded_state ) client_csrf = state [ 'csrf_token' ] return_url = state [ 'return_url' ] except ( ValueError , KeyError ) : return http . HttpResponseBadRequest ( 'Invalid state parameter.' ) if client_csrf != server_csrf : return http . HttpResponseBadRequest ( 'Invalid CSRF token.' ) flow = _get_flow_for_token ( client_csrf , request ) if not flow : return http . HttpResponseBadRequest ( 'Missing Oauth2 flow.' ) try : credentials = flow . step2_exchange ( code ) except client . FlowExchangeError as exchange_error : return http . HttpResponseBadRequest ( 'An error has occurred: {0}' . format ( exchange_error ) ) get_storage ( request ) . put ( credentials ) signals . oauth2_authorized . send ( sender = signals . oauth2_authorized , request = request , credentials = credentials ) return shortcuts . redirect ( return_url )
12698	def _parse_control_fields ( self , fields , tag_id = "tag" ) : for field in fields : params = field . params # skip tags without parameters if tag_id not in params : continue self . controlfields [ params [ tag_id ] ] = field . getContent ( ) . strip ( )
5719	def _convert_path ( path , name ) : table = os . path . splitext ( path ) [ 0 ] table = table . replace ( os . path . sep , '__' ) if name is not None : table = '___' . join ( [ table , name ] ) table = re . sub ( '[^0-9a-zA-Z_]+' , '_' , table ) table = table . lower ( ) return table
5226	def _to_gen_ ( iterable ) : from collections import Iterable for elm in iterable : if isinstance ( elm , Iterable ) and not isinstance ( elm , ( str , bytes ) ) : yield from flatten ( elm ) else : yield elm
5611	def memory_file ( data = None , profile = None ) : memfile = MemoryFile ( ) profile . update ( width = data . shape [ - 2 ] , height = data . shape [ - 1 ] ) with memfile . open ( * * profile ) as dataset : dataset . write ( data ) return memfile
10057	def delete ( self , pid , record , key ) : try : del record . files [ str ( key ) ] record . commit ( ) db . session . commit ( ) return make_response ( '' , 204 ) except KeyError : abort ( 404 , 'The specified object does not exist or has already ' 'been deleted.' )
5063	def get_course_track_selection_url ( course_run , query_parameters ) : try : course_root = reverse ( 'course_modes_choose' , kwargs = { 'course_id' : course_run [ 'key' ] } ) except KeyError : LOGGER . exception ( "KeyError while parsing course run data.\nCourse Run: \n[%s]" , course_run , ) raise url = '{}{}' . format ( settings . LMS_ROOT_URL , course_root ) course_run_url = update_query_parameters ( url , query_parameters ) return course_run_url
13257	def save ( self , entry , with_location = True , debug = False ) : entry_dict = { } if isinstance ( entry , DayOneEntry ) : # Get a dict of the DayOneEntry entry_dict = entry . as_dict ( ) else : entry_dict = entry # Set the UUID entry_dict [ 'UUID' ] = uuid . uuid4 ( ) . get_hex ( ) if with_location and not entry_dict [ 'Location' ] : entry_dict [ 'Location' ] = self . get_location ( ) # Do we have everything needed? if not all ( ( entry_dict [ 'UUID' ] , entry_dict [ 'Time Zone' ] , entry_dict [ 'Entry Text' ] ) ) : print "You must provide: Time zone, UUID, Creation Date, Entry Text" return False if debug is False : file_path = self . _file_path ( entry_dict [ 'UUID' ] ) plistlib . writePlist ( entry_dict , file_path ) else : plist = plistlib . writePlistToString ( entry_dict ) print plist return True
113	def postprocess ( self , images , augmenter , parents ) : if self . postprocessor is None : return images else : return self . postprocessor ( images , augmenter , parents )
11285	def _list_networks ( ) : output = core . run ( "virsh net-list --all" ) networks = { } # Take the header off and normalize whitespace. net_lines = [ n . strip ( ) for n in output . splitlines ( ) [ 2 : ] ] for line in net_lines : if not line : continue name , state , auto = line . split ( ) networks [ name ] = state == "active" return networks
3525	def uservoice ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return UserVoiceNode ( )
7066	def delete_spot_fleet_cluster ( spot_fleet_reqid , client = None , ) : if not client : client = boto3 . client ( 'ec2' ) resp = client . cancel_spot_fleet_requests ( SpotFleetRequestIds = [ spot_fleet_reqid ] , TerminateInstances = True ) return resp
6902	def load_xmatch_external_catalogs ( xmatchto , xmatchkeys , outfile = None ) : outdict = { } for xc , xk in zip ( xmatchto , xmatchkeys ) : parsed_catdef = _parse_xmatch_catalog_header ( xc , xk ) if not parsed_catdef : continue ( infd , catdefdict , catcolinds , catcoldtypes , catcolnames , catcolunits ) = parsed_catdef # get the specified columns out of the catalog catarr = np . genfromtxt ( infd , usecols = catcolinds , names = xk , dtype = ',' . join ( catcoldtypes ) , comments = '#' , delimiter = '|' , autostrip = True ) infd . close ( ) catshortname = os . path . splitext ( os . path . basename ( xc ) ) [ 0 ] catshortname = catshortname . replace ( '.csv' , '' ) # # make a kdtree for this catalog # # get the ra and decl columns objra , objdecl = ( catarr [ catdefdict [ 'colra' ] ] , catarr [ catdefdict [ 'coldec' ] ] ) # get the xyz unit vectors from ra,decl cosdecl = np . cos ( np . radians ( objdecl ) ) sindecl = np . sin ( np . radians ( objdecl ) ) cosra = np . cos ( np . radians ( objra ) ) sinra = np . sin ( np . radians ( objra ) ) xyz = np . column_stack ( ( cosra * cosdecl , sinra * cosdecl , sindecl ) ) # generate the kdtree kdt = cKDTree ( xyz , copy_data = True ) # generate the outdict element for this catalog catoutdict = { 'kdtree' : kdt , 'data' : catarr , 'columns' : xk , 'colnames' : catcolnames , 'colunits' : catcolunits , 'name' : catdefdict [ 'name' ] , 'desc' : catdefdict [ 'description' ] } outdict [ catshortname ] = catoutdict if outfile is not None : # if we're on OSX, we apparently need to save the file in chunks smaller # than 2 GB to make it work right. can't load pickles larger than 4 GB # either, but 3 GB < total size < 4 GB appears to be OK when loading. # also see: https://bugs.python.org/issue24658. # fix adopted from: https://stackoverflow.com/a/38003910 if sys . platform == 'darwin' : dumpbytes = pickle . dumps ( outdict , protocol = pickle . HIGHEST_PROTOCOL ) max_bytes = 2 ** 31 - 1 with open ( outfile , 'wb' ) as outfd : for idx in range ( 0 , len ( dumpbytes ) , max_bytes ) : outfd . write ( dumpbytes [ idx : idx + max_bytes ] ) else : with open ( outfile , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outfile else : return outdict
13771	def collect_links ( self , env = None ) : for asset in self . assets . values ( ) : if asset . has_bundles ( ) : asset . collect_files ( ) if env is None : env = self . config . env if env == static_bundle . ENV_PRODUCTION : self . _minify ( emulate = True ) self . _add_url_prefix ( )
10045	def create ( cls , object_type = None , object_uuid = None , * * kwargs ) : assert 'pid_value' in kwargs kwargs . setdefault ( 'status' , cls . default_status ) return super ( DepositProvider , cls ) . create ( object_type = object_type , object_uuid = object_uuid , * * kwargs )
5201	def Operate ( self , command , index , op_type ) : OutstationApplication . process_point_value ( 'Operate' , command , index , op_type ) return opendnp3 . CommandStatus . SUCCESS
13807	def __get_current_datetime ( self ) : self . wql_time = "SELECT LocalDateTime FROM Win32_OperatingSystem" self . current_time = self . query ( self . wql_time ) # [{'LocalDateTime': '20160824161431.977000+480'}]' self . current_time_string = str ( self . current_time [ 0 ] . get ( 'LocalDateTime' ) . split ( '.' ) [ 0 ] ) # '20160824161431' self . current_time_format = datetime . datetime . strptime ( self . current_time_string , '%Y%m%d%H%M%S' ) # param: datetime.datetime(2016, 8, 24, 16, 14, 31) -> type: # datetime.datetime return self . current_time_format
3212	def _update_cache_stats ( self , key , result ) : if result is None : self . _CACHE_STATS [ 'access_stats' ] . setdefault ( key , { 'hit' : 0 , 'miss' : 0 , 'expired' : 0 } ) else : self . _CACHE_STATS [ 'access_stats' ] [ key ] [ result ] += 1
3805	def calculate_P ( self , T , P , method ) : if method == ELI_HANLEY_DENSE : Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm kg = eli_hanley_dense ( T , self . MW , self . Tc , self . Vc , self . Zc , self . omega , Cvgm , Vmg ) elif method == CHUNG_DENSE : Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T , P ) if hasattr ( self . mug , '__call__' ) else self . mug kg = chung_dense ( T , self . MW , self . Tc , self . Vc , self . omega , Cvgm , Vmg , mug , self . dipole ) elif method == STIEL_THODOS_DENSE : kg = self . T_dependent_property ( T ) Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg kg = stiel_thodos_dense ( T , self . MW , self . Tc , self . Pc , self . Vc , self . Zc , Vmg , kg ) elif method == COOLPROP : kg = PropsSI ( 'L' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : kg = self . interpolate_P ( T , P , method ) return kg
11975	def _add ( self , other ) : if isinstance ( other , self . __class__ ) : sum_ = self . _ip_dec + other . _ip_dec elif isinstance ( other , int ) : sum_ = self . _ip_dec + other else : other = self . __class__ ( other ) sum_ = self . _ip_dec + other . _ip_dec return sum_
1312	def HardwareInput ( uMsg : int , param : int = 0 ) -> INPUT : return _CreateInput ( HARDWAREINPUT ( uMsg , param & 0xFFFF , param >> 16 & 0xFFFF ) )
13100	def render ( self , * * kwargs ) : breadcrumbs = [ ] # this is the list of items we want to accumulate in the breadcrumb trail. # item[0] is the key into the kwargs["url"] object and item[1] is the name of the route # setting a route name to None means that it's needed to construct the route of the next item in the list # but shouldn't be included in the list itself (this is currently the case for work -- # at some point we probably should include work in the navigation) breadcrumbs = [ ] if "collections" in kwargs : breadcrumbs = [ { "title" : "Text Collections" , "link" : ".r_collections" , "args" : { } } ] if "parents" in kwargs [ "collections" ] : breadcrumbs += [ { "title" : parent [ "label" ] , "link" : ".r_collection_semantic" , "args" : { "objectId" : parent [ "id" ] , "semantic" : f_slugify ( parent [ "label" ] ) , } , } for parent in kwargs [ "collections" ] [ "parents" ] ] [ : : - 1 ] if "current" in kwargs [ "collections" ] : breadcrumbs . append ( { "title" : kwargs [ "collections" ] [ "current" ] [ "label" ] , "link" : None , "args" : { } } ) # don't link the last item in the trail if len ( breadcrumbs ) > 0 : breadcrumbs [ - 1 ] [ "link" ] = None return { "breadcrumbs" : breadcrumbs }
6549	def fill_field ( self , ypos , xpos , tosend , length ) : if length < len ( tosend ) : raise FieldTruncateError ( 'length limit %d, but got "%s"' % ( length , tosend ) ) if xpos is not None and ypos is not None : self . move_to ( ypos , xpos ) self . delete_field ( ) self . send_string ( tosend )
7336	def _parse_iedb_response ( response ) : if len ( response ) == 0 : raise ValueError ( "Empty response from IEDB!" ) df = pd . read_csv ( io . BytesIO ( response ) , delim_whitespace = True , header = 0 ) # pylint doesn't realize that df is a DataFrame, so tell is assert type ( df ) == pd . DataFrame df = pd . DataFrame ( df ) if len ( df ) == 0 : raise ValueError ( "No binding predictions in response from IEDB: %s" % ( response , ) ) required_columns = [ "allele" , "peptide" , "ic50" , "start" , "end" , ] for column in required_columns : if column not in df . columns : raise ValueError ( "Response from IEDB is missing '%s' column: %s. Full " "response:\n%s" % ( column , df . ix [ 0 ] , response ) ) # since IEDB has allowed multiple column names for percentile rank, # we're defensively normalizing all of them to just 'rank' df = df . rename ( columns = { "percentile_rank" : "rank" , "percentile rank" : "rank" } ) return df
12891	def handle_int ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u8 . text ) or None
1554	def _add_in_streams ( self , bolt ) : if self . inputs is None : return # sanitize inputs and get a map <GlobalStreamId -> Grouping> input_dict = self . _sanitize_inputs ( ) for global_streamid , gtype in input_dict . items ( ) : in_stream = bolt . inputs . add ( ) in_stream . stream . CopyFrom ( self . _get_stream_id ( global_streamid . component_id , global_streamid . stream_id ) ) if isinstance ( gtype , Grouping . FIELDS ) : # it's a field grouping in_stream . gtype = gtype . gtype in_stream . grouping_fields . CopyFrom ( self . _get_stream_schema ( gtype . fields ) ) elif isinstance ( gtype , Grouping . CUSTOM ) : # it's a custom grouping in_stream . gtype = gtype . gtype in_stream . custom_grouping_object = gtype . python_serialized in_stream . type = topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) else : in_stream . gtype = gtype
10929	def do_internal_run ( self , initial_count = 0 , subblock = None , update_derr = True ) : self . _inner_run_counter = initial_count good_step = True n_good_steps = 0 CLOG . debug ( 'Running...' ) _last_residuals = self . calc_residuals ( ) . copy ( ) while ( ( self . _inner_run_counter < self . run_length ) & good_step & ( not self . check_terminate ( ) ) ) : #1. Checking if we update J if self . check_Broyden_J ( ) and self . _inner_run_counter != 0 : self . update_Broyden_J ( ) if self . check_update_eig_J ( ) and self . _inner_run_counter != 0 : self . update_eig_J ( ) #2. Getting parameters, error er0 = 1 * self . error delta_vals = self . find_LM_updates ( self . calc_grad ( ) , do_correct_damping = False , subblock = subblock ) er1 = self . update_function ( self . param_vals + delta_vals ) good_step = er1 < er0 if good_step : n_good_steps += 1 CLOG . debug ( '%f\t%f' % ( er0 , er1 ) ) #Updating: self . update_param_vals ( delta_vals , incremental = True ) self . _last_residuals = _last_residuals . copy ( ) if update_derr : self . _last_error = er0 self . error = er1 _last_residuals = self . calc_residuals ( ) . copy ( ) else : er0_0 = self . update_function ( self . param_vals ) CLOG . debug ( 'Bad step!' ) if np . abs ( er0 - er0_0 ) > 1e-6 : raise RuntimeError ( 'Function updates are not exact.' ) self . _inner_run_counter += 1 return n_good_steps
11622	def _unrecognised ( chr ) : if options [ 'handleUnrecognised' ] == UNRECOGNISED_ECHO : return chr elif options [ 'handleUnrecognised' ] == UNRECOGNISED_SUBSTITUTE : return options [ 'substituteChar' ] else : raise ( KeyError , chr )
6252	def create_normal_matrix ( self , modelview ) : normal_m = Matrix33 . from_matrix44 ( modelview ) normal_m = normal_m . inverse normal_m = normal_m . transpose ( ) return normal_m
10701	def set_state ( _id , body ) : url = DEVICE_URL % _id if "mode" in body : url = MODES_URL % _id arequest = requests . put ( url , headers = HEADERS , data = json . dumps ( body ) ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "State not accepted. " + status_code ) return False
6720	def init ( self ) : r = self . local_renderer # if self.virtualenv_exists(): # print('virtualenv exists') # return print ( 'Creating new virtual environment...' ) with self . settings ( warn_only = True ) : cmd = '[ ! -d {virtualenv_dir} ] && virtualenv --no-site-packages {virtualenv_dir} || true' if self . is_local : r . run_or_local ( cmd ) else : r . sudo ( cmd )
3013	def locked_delete ( self ) : filters = { self . key_name : self . key_value } self . session . query ( self . model_class ) . filter_by ( * * filters ) . delete ( )
5927	def configuration ( self ) : configuration = { 'configfilename' : self . filename , 'logfilename' : self . getpath ( 'Logging' , 'logfilename' ) , 'loglevel_console' : self . getLogLevel ( 'Logging' , 'loglevel_console' ) , 'loglevel_file' : self . getLogLevel ( 'Logging' , 'loglevel_file' ) , 'configdir' : self . getpath ( 'DEFAULT' , 'configdir' ) , 'qscriptdir' : self . getpath ( 'DEFAULT' , 'qscriptdir' ) , 'templatesdir' : self . getpath ( 'DEFAULT' , 'templatesdir' ) , } configuration [ 'path' ] = [ os . path . curdir , configuration [ 'qscriptdir' ] , configuration [ 'templatesdir' ] ] return configuration
6694	def static ( self ) : fn = self . render_to_file ( 'ip/ip_interfaces_static.template' ) r = self . local_renderer r . put ( local_path = fn , remote_path = r . env . interfaces_fn , use_sudo = True )
11255	def attrdict ( prev , attr_names ) : if isinstance ( attr_names , dict ) : for obj in prev : attr_values = dict ( ) for name in attr_names . keys ( ) : if hasattr ( obj , name ) : attr_values [ name ] = getattr ( obj , name ) else : attr_values [ name ] = attr_names [ name ] yield attr_values else : for obj in prev : attr_values = dict ( ) for name in attr_names : if hasattr ( obj , name ) : attr_values [ name ] = getattr ( obj , name ) yield attr_values
2351	def register ( ) : registerDriver ( ISelenium , Selenium , class_implements = [ Firefox , Chrome , Ie , Edge , Opera , Safari , BlackBerry , PhantomJS , Android , Remote , EventFiringWebDriver , ] , )
8274	def colors ( self , n = 10 , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) colors = colorlist ( ) for i in _range ( n ) : r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s colors . append ( rng ( clr , d ) ) return colors
573	def rApply ( d , f ) : remainingDicts = [ ( d , ( ) ) ] while len ( remainingDicts ) > 0 : current , prevKeys = remainingDicts . pop ( ) for k , v in current . iteritems ( ) : keys = prevKeys + ( k , ) if isinstance ( v , dict ) : remainingDicts . insert ( 0 , ( v , keys ) ) else : f ( v , keys )
12418	def capture_stdout ( ) : stdout = sys . stdout try : capture_out = StringIO ( ) sys . stdout = capture_out yield capture_out finally : sys . stdout = stdout
8455	def _apply_template ( template , target , * , checkout , extra_context ) : with tempfile . TemporaryDirectory ( ) as tempdir : repo_dir = cc_main . cookiecutter ( template , checkout = checkout , no_input = True , output_dir = tempdir , extra_context = extra_context ) for item in os . listdir ( repo_dir ) : src = os . path . join ( repo_dir , item ) dst = os . path . join ( target , item ) if os . path . isdir ( src ) : if os . path . exists ( dst ) : shutil . rmtree ( dst ) shutil . copytree ( src , dst ) else : if os . path . exists ( dst ) : os . remove ( dst ) shutil . copy2 ( src , dst )
8653	def create_project_thread ( session , member_ids , project_id , message ) : return create_thread ( session , member_ids , 'project' , project_id , message )
2812	def convert_squeeze ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting squeeze ...' ) if len ( params [ 'axes' ] ) > 1 : raise AssertionError ( 'Cannot convert squeeze by multiple dimensions' ) def target_layer ( x , axis = int ( params [ 'axes' ] [ 0 ] ) ) : import tensorflow as tf return tf . squeeze ( x , axis = axis ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
6684	def check_for_change ( self ) : r = self . local_renderer lm = self . last_manifest last_fingerprint = lm . fingerprint current_fingerprint = self . get_target_geckodriver_version_number ( ) self . vprint ( 'last_fingerprint:' , last_fingerprint ) self . vprint ( 'current_fingerprint:' , current_fingerprint ) if last_fingerprint != current_fingerprint : print ( 'A new release is available. %s' % self . get_most_recent_version ( ) ) return True print ( 'No updates found.' ) return False
11361	def fix_title_capitalization ( title ) : if re . search ( "[A-Z]" , title ) and re . search ( "[a-z]" , title ) : return title word_list = re . split ( ' +' , title ) final = [ word_list [ 0 ] . capitalize ( ) ] for word in word_list [ 1 : ] : if word . upper ( ) in COMMON_ACRONYMS : final . append ( word . upper ( ) ) elif len ( word ) > 3 : final . append ( word . capitalize ( ) ) else : final . append ( word . lower ( ) ) return " " . join ( final )
11830	def child_node ( self , problem , action ) : next = problem . result ( self . state , action ) return Node ( next , self , action , problem . path_cost ( self . path_cost , self . state , action , next ) )
6013	def load_background_sky_map ( background_sky_map_path , background_sky_map_hdu , pixel_scale ) : if background_sky_map_path is not None : return ScaledSquarePixelArray . from_fits_with_pixel_scale ( file_path = background_sky_map_path , hdu = background_sky_map_hdu , pixel_scale = pixel_scale ) else : return None
11714	def schedule ( self , variables = None , secure_variables = None , materials = None , return_new_instance = False , backoff_time = 1.0 ) : scheduling_args = dict ( variables = variables , secure_variables = secure_variables , material_fingerprint = materials , headers = { "Confirm" : True } , ) scheduling_args = dict ( ( k , v ) for k , v in scheduling_args . items ( ) if v is not None ) # TODO: Replace this with whatever is the official way as soon as gocd#990 is fixed. # https://github.com/gocd/gocd/issues/990 if return_new_instance : pipelines = self . history ( ) [ 'pipelines' ] if len ( pipelines ) == 0 : last_run = None else : last_run = pipelines [ 0 ] [ 'counter' ] response = self . _post ( '/schedule' , ok_status = 202 , * * scheduling_args ) if not response : return response max_tries = 10 while max_tries > 0 : current = self . instance ( ) if not last_run and current : return current elif last_run and current [ 'counter' ] > last_run : return current else : time . sleep ( backoff_time ) max_tries -= 1 # I can't come up with a scenario in testing where this would happen, but it seems # better than returning None. return response else : return self . _post ( '/schedule' , ok_status = 202 , * * scheduling_args )
9551	def _init_unique_sets ( self ) : ks = dict ( ) for t in self . _unique_checks : key = t [ 0 ] ks [ key ] = set ( ) # empty set return ks
2053	def STRD ( cpu , src1 , src2 , dest , offset = None ) : assert src1 . type == 'register' assert src2 . type == 'register' assert dest . type == 'memory' val1 = src1 . read ( ) val2 = src2 . read ( ) writeback = cpu . _compute_writeback ( dest , offset ) cpu . write_int ( dest . address ( ) , val1 , 32 ) cpu . write_int ( dest . address ( ) + 4 , val2 , 32 ) cpu . _cs_hack_ldr_str_writeback ( dest , offset , writeback )
3135	def create ( self , store_id , order_id , data ) : self . store_id = store_id self . order_id = order_id if 'id' not in data : raise KeyError ( 'The order line must have an id' ) if 'product_id' not in data : raise KeyError ( 'The order line must have a product_id' ) if 'product_variant_id' not in data : raise KeyError ( 'The order line must have a product_variant_id' ) if 'quantity' not in data : raise KeyError ( 'The order line must have a quantity' ) if 'price' not in data : raise KeyError ( 'The order line must have a price' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'orders' , order_id , 'lines' ) ) if response is not None : self . line_id = response [ 'id' ] else : self . line_id = None return response
8387	def check_main ( argv ) : if len ( argv ) != 1 : print ( "Please provide the name of a file to check." ) return 1 filename = argv [ 0 ] if os . path . exists ( filename ) : print ( u"Checking existing copy of %s" % filename ) tef = TamperEvidentFile ( filename ) if tef . validate ( ) : print ( u"Your copy of %s is good" % filename ) else : print ( u"Your copy of %s seems to have been edited" % filename ) else : print ( u"You don't have a copy of %s" % filename ) return 0
12951	def _get_new_connection ( self ) : pool = getRedisPool ( self . mdl . REDIS_CONNECTION_PARAMS ) return redis . Redis ( connection_pool = pool )
412	def save_model ( self , network = None , model_name = 'model' , * * kwargs ) : kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) # put project_name into kwargs params = network . get_all_params ( ) s = time . time ( ) kwargs . update ( { 'architecture' : network . all_graphs , 'time' : datetime . utcnow ( ) } ) try : params_id = self . model_fs . put ( self . _serialization ( params ) ) kwargs . update ( { 'params_id' : params_id , 'time' : datetime . utcnow ( ) } ) self . db . Model . insert_one ( kwargs ) print ( "[Database] Save model: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save model: FAIL" ) return False
9673	def resolve ( self , context , quiet = True ) : try : obj = context for level in self . levels : if isinstance ( obj , dict ) : obj = obj [ level ] elif isinstance ( obj , list ) or isinstance ( obj , tuple ) : obj = obj [ int ( level ) ] else : if callable ( getattr ( obj , level ) ) : try : obj = getattr ( obj , level ) ( ) except KeyError : obj = getattr ( obj , level ) else : # for model field that has choice set # use get_xxx_display to access display = 'get_%s_display' % level obj = getattr ( obj , display ) ( ) if hasattr ( obj , display ) else getattr ( obj , level ) if not obj : break return obj except Exception as e : if quiet : return '' else : raise e
6564	def and_gate ( variables , vartype = dimod . BINARY , name = 'AND' ) : variables = tuple ( variables ) if vartype is dimod . BINARY : configurations = frozenset ( [ ( 0 , 0 , 0 ) , ( 0 , 1 , 0 ) , ( 1 , 0 , 0 ) , ( 1 , 1 , 1 ) ] ) def func ( in1 , in2 , out ) : return ( in1 and in2 ) == out else : # SPIN, vartype is checked by the decorator configurations = frozenset ( [ ( - 1 , - 1 , - 1 ) , ( - 1 , + 1 , - 1 ) , ( + 1 , - 1 , - 1 ) , ( + 1 , + 1 , + 1 ) ] ) def func ( in1 , in2 , out ) : return ( ( in1 > 0 ) and ( in2 > 0 ) ) == ( out > 0 ) return Constraint ( func , configurations , variables , vartype = vartype , name = name )
4305	def soxi ( filepath , argument ) : if argument not in SOXI_ARGS : raise ValueError ( "Invalid argument '{}' to SoXI" . format ( argument ) ) args = [ 'sox' , '--i' ] args . append ( "-{}" . format ( argument ) ) args . append ( filepath ) try : shell_output = subprocess . check_output ( args , stderr = subprocess . PIPE ) except CalledProcessError as cpe : logger . info ( "SoXI error message: {}" . format ( cpe . output ) ) raise SoxiError ( "SoXI failed with exit code {}" . format ( cpe . returncode ) ) shell_output = shell_output . decode ( "utf-8" ) return str ( shell_output ) . strip ( '\n' )
6041	def unmasked_sparse_to_sparse ( self ) : return mapping_util . unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres , total_sparse_pixels = self . total_sparse_pixels ) . astype ( 'int' )
1123	def Tok ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : return parser . _accept ( kind ) return rule
2497	def handle_package_has_file_helper ( self , pkg_file ) : nodes = list ( self . graph . triples ( ( None , self . spdx_namespace . fileName , Literal ( pkg_file . name ) ) ) ) if len ( nodes ) == 1 : return nodes [ 0 ] [ 0 ] else : raise InvalidDocumentError ( 'handle_package_has_file_helper could not' + ' find file node for file: {0}' . format ( pkg_file . name ) )
408	def _tf_repeat ( self , a , repeats ) : # https://github.com/tensorflow/tensorflow/issues/8521 if len ( a . get_shape ( ) ) != 1 : raise AssertionError ( "This is not a 1D Tensor" ) a = tf . expand_dims ( a , - 1 ) a = tf . tile ( a , [ 1 , repeats ] ) a = self . tf_flatten ( a ) return a
1183	def push_new_context ( self , pattern_offset ) : child_context = _MatchContext ( self . state , self . pattern_codes [ self . code_position + pattern_offset : ] ) self . state . context_stack . append ( child_context ) return child_context
1925	def get_group ( name : str ) -> _Group : global _groups if name in _groups : return _groups [ name ] group = _Group ( name ) _groups [ name ] = group return group
13054	def nmap_smb_vulnscan ( ) : service_search = ServiceSearch ( ) services = service_search . get_services ( ports = [ '445' ] , tags = [ '!smb_vulnscan' ] , up = True ) services = [ service for service in services ] service_dict = { } for service in services : service . add_tag ( 'smb_vulnscan' ) service_dict [ str ( service . address ) ] = service nmap_args = "-Pn -n --disable-arp-ping --script smb-security-mode.nse,smb-vuln-ms17-010.nse -p 445" . split ( " " ) if services : result = nmap ( nmap_args , [ str ( s . address ) for s in services ] ) parser = NmapParser ( ) report = parser . parse_fromstring ( result ) smb_signing = 0 ms17 = 0 for nmap_host in report . hosts : for script_result in nmap_host . scripts_results : script_result = script_result . get ( 'elements' , { } ) service = service_dict [ str ( nmap_host . address ) ] if script_result . get ( 'message_signing' , '' ) == 'disabled' : print_success ( "({}) SMB Signing disabled" . format ( nmap_host . address ) ) service . add_tag ( 'smb_signing_disabled' ) smb_signing += 1 if script_result . get ( 'CVE-2017-0143' , { } ) . get ( 'state' , '' ) == 'VULNERABLE' : print_success ( "({}) Vulnerable for MS17-010" . format ( nmap_host . address ) ) service . add_tag ( 'MS17-010' ) ms17 += 1 service . update ( tags = service . tags ) print_notification ( "Completed, 'smb_signing_disabled' tag added to systems with smb signing disabled, 'MS17-010' tag added to systems that did not apply MS17-010." ) stats = { 'smb_signing' : smb_signing , 'MS17_010' : ms17 , 'scanned_services' : len ( services ) } Logger ( ) . log ( 'smb_vulnscan' , 'Scanned {} smb services for vulnerabilities' . format ( len ( services ) ) , stats ) else : print_notification ( "No services found to scan." )
8687	def get ( self , key_name ) : result = self . db . search ( Query ( ) . name == key_name ) if not result : return { } return result [ 0 ]
2348	def seed_url ( self ) : url = self . base_url if self . URL_TEMPLATE is not None : url = urlparse . urljoin ( self . base_url , self . URL_TEMPLATE . format ( * * self . url_kwargs ) ) if not url : return None url_parts = list ( urlparse . urlparse ( url ) ) query = urlparse . parse_qsl ( url_parts [ 4 ] ) for k , v in self . url_kwargs . items ( ) : if v is None : continue if "{{{}}}" . format ( k ) not in str ( self . URL_TEMPLATE ) : for i in iterable ( v ) : query . append ( ( k , i ) ) url_parts [ 4 ] = urlencode ( query ) return urlparse . urlunparse ( url_parts )
2592	def get_all_checkpoints ( rundir = "runinfo" ) : if ( not os . path . isdir ( rundir ) ) : return [ ] dirs = sorted ( os . listdir ( rundir ) ) checkpoints = [ ] for runid in dirs : checkpoint = os . path . abspath ( '{}/{}/checkpoint' . format ( rundir , runid ) ) if os . path . isdir ( checkpoint ) : checkpoints . append ( checkpoint ) return checkpoints
12578	def apply_smoothing ( self , smooth_fwhm ) : if smooth_fwhm <= 0 : return old_smooth_fwhm = self . _smooth_fwhm self . _smooth_fwhm = smooth_fwhm try : data = self . get_data ( smoothed = True , masked = True , safe_copy = True ) except ValueError as ve : self . _smooth_fwhm = old_smooth_fwhm raise else : self . _smooth_fwhm = smooth_fwhm return data
4604	def history ( self , first = 0 , last = 0 , limit = - 1 , only_ops = [ ] , exclude_ops = [ ] ) : _limit = 100 cnt = 0 if first < 0 : first = 0 while True : # RPC call txs = self . blockchain . rpc . get_account_history ( self [ "id" ] , "1.11.{}" . format ( last ) , _limit , "1.11.{}" . format ( first - 1 ) , api = "history" , ) for i in txs : if ( exclude_ops and self . operations . getOperationNameForId ( i [ "op" ] [ 0 ] ) in exclude_ops ) : continue if ( not only_ops or self . operations . getOperationNameForId ( i [ "op" ] [ 0 ] ) in only_ops ) : cnt += 1 yield i if limit >= 0 and cnt >= limit : # pragma: no cover return if not txs : log . info ( "No more history returned from API node" ) break if len ( txs ) < _limit : log . info ( "Less than {} have been returned." . format ( _limit ) ) break first = int ( txs [ - 1 ] [ "id" ] . split ( "." ) [ 2 ] )
11175	def parse ( self , file ) : if isinstance ( file , basestring ) : file = open ( file ) line_number = 0 label = None block = self . untagged for line in file : line_number += 1 line = line . rstrip ( '\n' ) if self . tabsize > 0 : line = line . replace ( '\t' , ' ' * self . tabsize ) if self . decommenter : line = self . decommenter . decomment ( line ) if line is None : continue tag = line . split ( ':' , 1 ) [ 0 ] . strip ( ) # Still in the same block? if tag not in self . names : if block is None : if line and not line . isspace ( ) : raise ParseError ( file . name , line , "garbage before first block: %r" % line ) continue block . addline ( line ) continue # Open a new block. name = self . names [ tag ] label = line . split ( ':' , 1 ) [ 1 ] . strip ( ) if name in self . labelled_classes : if not label : raise ParseError ( file . name , line , "missing label for %r block" % name ) block = self . blocks [ name ] . setdefault ( label , self . labelled_classes [ name ] ( ) ) else : if label : msg = "label %r present for unlabelled block %r" % ( label , name ) raise ParseError ( file . name , line_number , msg ) block = self . blocks [ name ] block . startblock ( )
6739	def check_settings_for_differences ( old , new , as_bool = False , as_tri = False ) : assert not as_bool or not as_tri old = old or { } new = new or { } changes = set ( k for k in set ( new . iterkeys ( ) ) . intersection ( old . iterkeys ( ) ) if new [ k ] != old [ k ] ) if changes and as_bool : return True added_keys = set ( new . iterkeys ( ) ) . difference ( old . iterkeys ( ) ) if added_keys and as_bool : return True if not as_tri : changes . update ( added_keys ) deled_keys = set ( old . iterkeys ( ) ) . difference ( new . iterkeys ( ) ) if deled_keys and as_bool : return True if as_bool : return False if not as_tri : changes . update ( deled_keys ) if as_tri : return added_keys , changes , deled_keys return changes
12015	def define_spotsignal ( self ) : client = kplr . API ( ) star = client . star ( self . kic ) lcs = star . get_light_curves ( short_cadence = False ) time , flux , ferr , qual = [ ] , [ ] , [ ] , [ ] for lc in lcs : with lc . open ( ) as f : hdu_data = f [ 1 ] . data time . append ( hdu_data [ "time" ] ) flux . append ( hdu_data [ "pdcsap_flux" ] ) ferr . append ( hdu_data [ "pdcsap_flux_err" ] ) qual . append ( hdu_data [ "sap_quality" ] ) tout = np . array ( [ ] ) fout = np . array ( [ ] ) eout = np . array ( [ ] ) for i in range ( len ( flux ) ) : t = time [ i ] [ qual [ i ] == 0 ] f = flux [ i ] [ qual [ i ] == 0 ] e = ferr [ i ] [ qual [ i ] == 0 ] t = t [ np . isfinite ( f ) ] e = e [ np . isfinite ( f ) ] f = f [ np . isfinite ( f ) ] e /= np . median ( f ) f /= np . median ( f ) tout = np . append ( tout , t [ 50 : ] + 54833 ) fout = np . append ( fout , f [ 50 : ] ) eout = np . append ( eout , e [ 50 : ] ) self . spot_signal = np . zeros ( 52 ) for i in range ( len ( self . times ) ) : if self . times [ i ] < 55000 : self . spot_signal [ i ] = 1.0 else : self . spot_signal [ i ] = fout [ np . abs ( self . times [ i ] - tout ) == np . min ( np . abs ( self . times [ i ] - tout ) ) ]
11075	def set ( self , user ) : self . log . info ( "Loading user information for %s/%s" , user . id , user . username ) self . load_user_info ( user ) self . log . info ( "Loading user rights for %s/%s" , user . id , user . username ) self . load_user_rights ( user ) self . log . info ( "Added user: %s/%s" , user . id , user . username ) self . _add_user_to_cache ( user ) return user
5194	def Process ( self , info , values ) : visitor_class_types = { opendnp3 . ICollectionIndexedBinary : VisitorIndexedBinary , opendnp3 . ICollectionIndexedDoubleBitBinary : VisitorIndexedDoubleBitBinary , opendnp3 . ICollectionIndexedCounter : VisitorIndexedCounter , opendnp3 . ICollectionIndexedFrozenCounter : VisitorIndexedFrozenCounter , opendnp3 . ICollectionIndexedAnalog : VisitorIndexedAnalog , opendnp3 . ICollectionIndexedBinaryOutputStatus : VisitorIndexedBinaryOutputStatus , opendnp3 . ICollectionIndexedAnalogOutputStatus : VisitorIndexedAnalogOutputStatus , opendnp3 . ICollectionIndexedTimeAndInterval : VisitorIndexedTimeAndInterval } visitor_class = visitor_class_types [ type ( values ) ] visitor = visitor_class ( ) values . Foreach ( visitor ) for index , value in visitor . index_and_value : log_string = 'SOEHandler.Process {0}\theaderIndex={1}\tdata_type={2}\tindex={3}\tvalue={4}' _log . debug ( log_string . format ( info . gv , info . headerIndex , type ( values ) . __name__ , index , value ) )
4472	def transform ( self , jam ) : for state in self . states ( jam ) : yield self . _transform ( jam , state )
13808	def run ( self ) : config = config_creator ( ) debug = config . debug branch_thread_sleep = config . branch_thread_sleep while 1 : url = self . branch_queue . get ( ) if debug : print ( 'branch thread-{} start' . format ( url ) ) branch_spider = self . branch_spider ( url ) sleep ( random . randrange ( * branch_thread_sleep ) ) branch_spider . request_page ( ) if debug : print ( 'branch thread-{} end' . format ( url ) ) self . branch_queue . task_done ( )
8570	def get_location ( self , location_id , depth = 0 ) : response = self . _perform_request ( '/locations/%s?depth=%s' % ( location_id , depth ) ) return response
1580	def send ( self , dispatcher ) : if self . sent_complete : return sent = dispatcher . send ( self . to_send ) self . to_send = self . to_send [ sent : ]
7989	def event ( self , event ) : # pylint: disable-msg=R0201 event . stream = self logger . debug ( u"Stream event: {0}" . format ( event ) ) self . settings [ "event_queue" ] . put ( event ) return False
8375	def var_deleted ( self , v ) : widget = self . widgets [ v . name ] # widgets are all in a single container .. parent = widget . get_parent ( ) self . container . remove ( parent ) del self . widgets [ v . name ] self . window . set_size_request ( 400 , 35 * len ( self . widgets . keys ( ) ) ) self . window . show_all ( )
3297	def is_collection ( self , path , environ ) : res = self . get_resource_inst ( path , environ ) return res and res . is_collection
8602	def get_user ( self , user_id , depth = 1 ) : response = self . _perform_request ( '/um/users/%s?depth=%s' % ( user_id , str ( depth ) ) ) return response
2794	def get_object ( cls , api_token , image_id_or_slug ) : if cls . _is_string ( image_id_or_slug ) : image = cls ( token = api_token , slug = image_id_or_slug ) image . load ( use_slug = True ) else : image = cls ( token = api_token , id = image_id_or_slug ) image . load ( ) return image
5115	def copy ( self ) : net = QueueNetwork ( None ) net . g = self . g . copy ( ) net . max_agents = copy . deepcopy ( self . max_agents ) net . nV = copy . deepcopy ( self . nV ) net . nE = copy . deepcopy ( self . nE ) net . num_agents = copy . deepcopy ( self . num_agents ) net . num_events = copy . deepcopy ( self . num_events ) net . _t = copy . deepcopy ( self . _t ) net . _initialized = copy . deepcopy ( self . _initialized ) net . _prev_edge = copy . deepcopy ( self . _prev_edge ) net . _blocking = copy . deepcopy ( self . _blocking ) net . colors = copy . deepcopy ( self . colors ) net . out_edges = copy . deepcopy ( self . out_edges ) net . in_edges = copy . deepcopy ( self . in_edges ) net . edge2queue = copy . deepcopy ( self . edge2queue ) net . _route_probs = copy . deepcopy ( self . _route_probs ) if net . _initialized : keys = [ q . _key ( ) for q in net . edge2queue if q . _time < np . infty ] net . _fancy_heap = PriorityQueue ( keys , net . nE ) return net
8315	def parse_links ( self , markup ) : links = [ ] m = re . findall ( self . re [ "link" ] , markup ) for link in m : # We don't like [[{{{1|Universe (disambiguation)}}}]] if link . find ( "{" ) >= 0 : link = re . sub ( "\{{1,3}[0-9]{0,2}\|" , "" , link ) link = link . replace ( "{" , "" ) link = link . replace ( "}" , "" ) link = link . split ( "|" ) link [ 0 ] = link [ 0 ] . split ( "#" ) page = link [ 0 ] [ 0 ] . strip ( ) #anchor = u"" #display = u"" #if len(link[0]) > 1: # anchor = link[0][1].strip() #if len(link) > 1: # display = link[1].strip() if not page in links : links . append ( page ) #links[page] = WikipediaLink(page, anchor, display) links . sort ( ) return links
10492	def doubleClickDragMouseButtonLeft ( self , coord , dest_coord , interval = 0.5 ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord , clickCount = 2 ) self . _postQueuedEvents ( interval = interval )
2023	def LT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . ULT ( a , b ) , 1 , 0 )
3626	def pad_to ( unpadded , target_len ) : under = target_len - len ( unpadded ) if under <= 0 : return unpadded return unpadded + ( ' ' * under )
8113	def age ( self , id ) : path = self . hash ( id ) if os . path . exists ( path ) : modified = datetime . datetime . fromtimestamp ( os . stat ( path ) [ 8 ] ) age = datetime . datetime . today ( ) - modified return age . days else : return 0
10350	def lint_directory ( source , target ) : for path in os . listdir ( source ) : if not path . endswith ( '.bel' ) : continue log . info ( 'linting: %s' , path ) with open ( os . path . join ( source , path ) ) as i , open ( os . path . join ( target , path ) , 'w' ) as o : lint_file ( i , o )
4194	def plot_window ( self ) : from pylab import plot , xlim , grid , title , ylabel , axis x = linspace ( 0 , 1 , self . N ) xlim ( 0 , 1 ) plot ( x , self . data ) grid ( True ) title ( '%s Window (%s points)' % ( self . name . capitalize ( ) , self . N ) ) ylabel ( 'Amplitude' ) axis ( [ 0 , 1 , 0 , 1.1 ] )
3689	def solve_T ( self , P , V ) : return ( P * V ** 2 * ( V - self . b ) + V * self . a - self . a * self . b ) / ( R * V ** 2 )
8851	def open_file ( self , path , line = None ) : editor = None if path : interpreter , pyserver , args = self . _get_backend_parameters ( ) editor = self . tabWidget . open_document ( path , None , interpreter = interpreter , server_script = pyserver , args = args ) if editor : self . setup_editor ( editor ) self . recent_files_manager . open_file ( path ) self . menu_recents . update_actions ( ) if line is not None : TextHelper ( self . tabWidget . current_widget ( ) ) . goto_line ( line ) return editor
1133	def getlines ( filename , module_globals = None ) : if filename in cache : return cache [ filename ] [ 2 ] try : return updatecache ( filename , module_globals ) except MemoryError : clearcache ( ) return [ ]
10806	def validate ( cls , policy ) : return policy in [ cls . PUBLIC , cls . MEMBERS , cls . ADMINS ]
1318	def SetWindowText ( self , text : str ) -> bool : handle = self . NativeWindowHandle if handle : return SetWindowText ( handle , text ) return False
6602	def collect_result ( self , package_index ) : result_fullpath = self . result_fullpath ( package_index ) # e.g., '{path}/tpd_20161129_122841_HnpcmF/results/task_00009/result.p.gz' try : with gzip . open ( result_fullpath , 'rb' ) as f : result = pickle . load ( f ) except Exception as e : logger = logging . getLogger ( __name__ ) logger . warning ( e ) return None return result
13173	def parents ( self , name = None ) : p = self . parent while p is not None : if name is None or p . tagname == name : yield p p = p . parent
11939	def broadcast_message ( level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : from django . contrib . auth import get_user_model users = get_user_model ( ) . objects . all ( ) add_message_for ( users , level , message_text , extra_tags = extra_tags , date = date , url = url , fail_silently = fail_silently )
12610	def _concat_queries ( queries , operators = '__and__' ) : # checks first if not queries : raise ValueError ( 'Expected some `queries`, got {}.' . format ( queries ) ) if len ( queries ) == 1 : return queries [ 0 ] if isinstance ( operators , str ) : operators = [ operators ] * ( len ( queries ) - 1 ) if len ( queries ) - 1 != len ( operators ) : raise ValueError ( 'Expected `operators` to be a string or a list with the same' ' length as `field_names` ({}), got {}.' . format ( len ( queries ) , operators ) ) # recursively build the query first , rest , end = queries [ 0 ] , queries [ 1 : - 1 ] , queries [ - 1 : ] [ 0 ] bigop = getattr ( first , operators [ 0 ] ) for i , q in enumerate ( rest ) : bigop = getattr ( bigop ( q ) , operators [ i ] ) return bigop ( end )
10099	def snippets ( self , timeout = None ) : return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_GET , timeout = timeout )
9308	def get_canonical_headers ( cls , req , include = None ) : if include is None : include = cls . default_include_headers include = [ x . lower ( ) for x in include ] headers = req . headers . copy ( ) # Temporarily include the host header - AWS requires it to be included # in the signed headers, but Requests doesn't include it in a # PreparedRequest if 'host' not in headers : headers [ 'host' ] = urlparse ( req . url ) . netloc . split ( ':' ) [ 0 ] # Aggregate for upper/lowercase header name collisions in header names, # AMZ requires values of colliding headers be concatenated into a # single header with lowercase name. Although this is not possible with # Requests, since it uses a case-insensitive dict to hold headers, this # is here just in case you duck type with a regular dict cano_headers_dict = { } for hdr , val in headers . items ( ) : hdr = hdr . strip ( ) . lower ( ) val = cls . amz_norm_whitespace ( val ) . strip ( ) if ( hdr in include or '*' in include or ( 'x-amz-*' in include and hdr . startswith ( 'x-amz-' ) and not hdr == 'x-amz-client-context' ) ) : vals = cano_headers_dict . setdefault ( hdr , [ ] ) vals . append ( val ) # Flatten cano_headers dict to string and generate signed_headers cano_headers = '' signed_headers_list = [ ] for hdr in sorted ( cano_headers_dict ) : vals = cano_headers_dict [ hdr ] val = ',' . join ( sorted ( vals ) ) cano_headers += '{}:{}\n' . format ( hdr , val ) signed_headers_list . append ( hdr ) signed_headers = ';' . join ( signed_headers_list ) return ( cano_headers , signed_headers )
7578	def _get_evanno_table ( self , kpops , max_var_multiple , quiet ) : ## iterate across k-vals kpops = sorted ( kpops ) replnliks = [ ] for kpop in kpops : ## concat results for k=x reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) ## report if some results were excluded if excluded : if not quiet : sys . stderr . write ( "[K{}] {} reps excluded (not converged) see 'max_var_multiple'.\n" . format ( kpop , excluded ) ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : print "no result files found" ## all we really need is the lnlik replnliks . append ( [ i . est_lnlik for i in reps ] ) ## compare lnlik and var of results if len ( replnliks ) > 1 : lnmean = [ np . mean ( i ) for i in replnliks ] lnstds = [ np . std ( i , ddof = 1 ) for i in replnliks ] else : lnmean = replnliks lnstds = np . nan tab = pd . DataFrame ( index = kpops , data = { "Nreps" : [ len ( i ) for i in replnliks ] , "lnPK" : [ 0 ] * len ( kpops ) , "lnPPK" : [ 0 ] * len ( kpops ) , "deltaK" : [ 0 ] * len ( kpops ) , "estLnProbMean" : lnmean , "estLnProbStdev" : lnstds , } ) ## calculate Evanno's for kpop in kpops [ 1 : ] : tab . loc [ kpop , "lnPK" ] = tab . loc [ kpop , "estLnProbMean" ] - tab . loc [ kpop - 1 , "estLnProbMean" ] for kpop in kpops [ 1 : - 1 ] : tab . loc [ kpop , "lnPPK" ] = abs ( tab . loc [ kpop + 1 , "lnPK" ] - tab . loc [ kpop , "lnPK" ] ) tab . loc [ kpop , "deltaK" ] = ( abs ( tab . loc [ kpop + 1 , "estLnProbMean" ] - 2.0 * tab . loc [ kpop , "estLnProbMean" ] + tab . loc [ kpop - 1 , "estLnProbMean" ] ) / tab . loc [ kpop , "estLnProbStdev" ] ) ## return table return tab
1754	def write_register ( self , register , value ) : self . _publish ( 'will_write_register' , register , value ) value = self . _regfile . write ( register , value ) self . _publish ( 'did_write_register' , register , value ) return value
4655	def verify_authority ( self ) : try : if not self . blockchain . rpc . verify_authority ( self . json ( ) ) : raise InsufficientAuthorityError except Exception as e : raise e
3687	def a_alpha_and_derivatives ( self , T , full = True , quick = True ) : if not full : return self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 else : if quick : Tc , kappa = self . Tc , self . kappa x0 = T ** 0.5 x1 = Tc ** - 0.5 x2 = kappa * ( x0 * x1 - 1. ) - 1. x3 = self . a * kappa a_alpha = self . a * x2 * x2 da_alpha_dT = x1 * x2 * x3 / x0 d2a_alpha_dT2 = x3 * ( - 0.5 * T ** - 1.5 * x1 * x2 + 0.5 / ( T * Tc ) * kappa ) else : a_alpha = self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 da_alpha_dT = - self . a * self . kappa * sqrt ( T / self . Tc ) * ( self . kappa * ( - sqrt ( T / self . Tc ) + 1. ) + 1. ) / T d2a_alpha_dT2 = self . a * self . kappa * ( self . kappa / self . Tc - sqrt ( T / self . Tc ) * ( self . kappa * ( sqrt ( T / self . Tc ) - 1. ) - 1. ) / T ) / ( 2. * T ) return a_alpha , da_alpha_dT , d2a_alpha_dT2
12199	def description ( self ) : if self . _description is None : text = '\n' . join ( self . __doc__ . splitlines ( ) [ 1 : ] ) . strip ( ) lines = [ ] for line in map ( str . strip , text . splitlines ( ) ) : if line and lines : lines [ - 1 ] = ' ' . join ( ( lines [ - 1 ] , line ) ) elif line : lines . append ( line ) else : lines . append ( '' ) self . _description = '\n' . join ( lines ) return self . _description
5635	def doc2md ( docstr , title , min_level = 1 , more_info = False , toc = True , maxdepth = 0 ) : text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 shiftlevel = 0 if level < min_level : shiftlevel = min_level - level level = min_level sections = [ ( lev + shiftlevel , tit ) for lev , tit in sections ] head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] , shiftlevel ) if more_info : return ( md , sections ) else : return "\n" . join ( md )
1205	def target_optimizer_arguments ( self ) : variables = self . target_network . get_variables ( ) + [ variable for name in sorted ( self . target_distributions ) for variable in self . target_distributions [ name ] . get_variables ( ) ] source_variables = self . network . get_variables ( ) + [ variable for name in sorted ( self . distributions ) for variable in self . distributions [ name ] . get_variables ( ) ] arguments = dict ( time = self . global_timestep , variables = variables , source_variables = source_variables ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . target_network . get_variables ( ) + [ variable for name in sorted ( self . global_model . target_distributions ) for variable in self . global_model . target_distributions [ name ] . get_variables ( ) ] return arguments
9340	def flatten_dtype ( dtype , _next = None ) : types = [ ] if _next is None : _next = [ 0 , '' ] primary = True else : primary = False prefix = _next [ 1 ] if dtype . names is None : for i in numpy . ndindex ( dtype . shape ) : if dtype . base == dtype : types . append ( ( '%s%s' % ( prefix , simplerepr ( i ) ) , dtype ) ) _next [ 0 ] += 1 else : _next [ 1 ] = '%s%s' % ( prefix , simplerepr ( i ) ) types . extend ( flatten_dtype ( dtype . base , _next ) ) else : for field in dtype . names : typ_fields = dtype . fields [ field ] if len ( prefix ) > 0 : _next [ 1 ] = prefix + '.' + field else : _next [ 1 ] = '' + field flat_dt = flatten_dtype ( typ_fields [ 0 ] , _next ) types . extend ( flat_dt ) _next [ 1 ] = prefix if primary : return numpy . dtype ( types ) else : return types
8157	def close ( self ) : self . _con . commit ( ) self . _cur . close ( ) self . _con . close ( )
11300	def unregister ( self , provider_class ) : if not issubclass ( provider_class , BaseProvider ) : raise TypeError ( '%s must be a subclass of BaseProvider' % provider_class . __name__ ) if provider_class not in self . _registered_providers : raise NotRegistered ( '%s is not registered' % provider_class . __name__ ) self . _registered_providers . remove ( provider_class ) # flag for repopulation self . invalidate_providers ( )
13491	def yn_prompt ( msg , default = True ) : ret = custom_prompt ( msg , [ "y" , "n" ] , "y" if default else "n" ) if ret == "y" : return True return False
145	def deepcopy ( self , exterior = None , label = None ) : return Polygon ( exterior = np . copy ( self . exterior ) if exterior is None else exterior , label = self . label if label is None else label )
5834	def create_ml_configuration ( self , search_template , extract_as_keys , dataset_ids ) : data = { "search_template" : search_template , "extract_as_keys" : extract_as_keys } failure_message = "ML Configuration creation failed" config_job_id = self . _get_success_json ( self . _post_json ( 'v1/descriptors/builders/simple/default/trigger' , data , failure_message = failure_message ) ) [ 'data' ] [ 'result' ] [ 'uid' ] while True : config_status = self . __get_ml_configuration_status ( config_job_id ) print ( 'Configuration status: ' , config_status ) if config_status [ 'status' ] == 'Finished' : ml_config = self . __convert_response_to_configuration ( config_status [ 'result' ] , dataset_ids ) return ml_config time . sleep ( 5 )
4771	def contains ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] not in self . val : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain key <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : self . _err ( 'Expected <%s> to contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain keys %s, but did not contain key%s %s.' % ( self . val , self . _fmt_items ( items ) , '' if len ( missing ) == 0 else 's' , self . _fmt_items ( missing ) ) ) else : self . _err ( 'Expected <%s> to contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
5665	def evaluate_earliest_arrival_time_at_target ( self , dep_time , transfer_margin ) : minimum = dep_time + self . _walk_to_target_duration dep_time_plus_transfer_margin = dep_time + transfer_margin for label in self . _labels : if label . departure_time >= dep_time_plus_transfer_margin and label . arrival_time_target < minimum : minimum = label . arrival_time_target return float ( minimum )
7671	def save ( self , path_or_file , strict = True , fmt = 'auto' ) : self . validate ( strict = strict ) with _open ( path_or_file , mode = 'w' , fmt = fmt ) as fdesc : json . dump ( self . __json__ , fdesc , indent = 2 )
9587	def isarray ( array , test , dim = 2 ) : if dim > 1 : return all ( isarray ( array [ i ] , test , dim - 1 ) for i in range ( len ( array ) ) ) return all ( test ( i ) for i in array )
1756	def emulate_until ( self , target : int ) : self . _concrete = True self . _break_unicorn_at = target if self . emu : self . emu . _stop_at = target
2111	def empty ( organization = None , user = None , team = None , credential_type = None , credential = None , notification_template = None , inventory_script = None , inventory = None , project = None , job_template = None , workflow = None , all = None , no_color = False ) : # Create an import/export object from tower_cli . cli . transfer . cleaner import Cleaner destroyer = Cleaner ( no_color ) assets_to_export = { } for asset_type in SEND_ORDER : assets_to_export [ asset_type ] = locals ( ) [ asset_type ] destroyer . go_ham ( all = all , asset_input = assets_to_export )
3727	def Vc ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ SURF ] ) : def list_methods ( ) : methods = [ ] if CASRN in _crit_IUPAC . index and not np . isnan ( _crit_IUPAC . at [ CASRN , 'Vc' ] ) : methods . append ( IUPAC ) if CASRN in _crit_Matthews . index and not np . isnan ( _crit_Matthews . at [ CASRN , 'Vc' ] ) : methods . append ( MATTHEWS ) if CASRN in _crit_CRC . index and not np . isnan ( _crit_CRC . at [ CASRN , 'Vc' ] ) : methods . append ( CRC ) if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'Vc' ] ) : methods . append ( PSRK ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'Vc' ] ) : methods . append ( YAWS ) if CASRN : methods . append ( SURF ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IUPAC : _Vc = float ( _crit_IUPAC . at [ CASRN , 'Vc' ] ) elif Method == PSRK : _Vc = float ( _crit_PSRKR4 . at [ CASRN , 'Vc' ] ) elif Method == MATTHEWS : _Vc = float ( _crit_Matthews . at [ CASRN , 'Vc' ] ) elif Method == CRC : _Vc = float ( _crit_CRC . at [ CASRN , 'Vc' ] ) elif Method == YAWS : _Vc = float ( _crit_Yaws . at [ CASRN , 'Vc' ] ) elif Method == SURF : _Vc = third_property ( CASRN = CASRN , V = True ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Vc
13513	def reynolds_number ( length , speed , temperature = 25 ) : kinematic_viscosity = interpolate . interp1d ( [ 0 , 10 , 20 , 25 , 30 , 40 ] , np . array ( [ 18.54 , 13.60 , 10.50 , 9.37 , 8.42 , 6.95 ] ) / 10 ** 7 ) # Data from http://web.mit.edu/seawater/2017_MIT_Seawater_Property_Tables_r2.pdf Re = length * speed / kinematic_viscosity ( temperature ) return Re
5743	def update_running_pids ( old_procs ) : new_procs = [ ] for proc in old_procs : if proc . poll ( ) is None and check_pid ( proc . pid ) : publisher . debug ( str ( proc . pid ) + ' is alive' ) new_procs . append ( proc ) else : try : publisher . debug ( str ( proc . pid ) + ' is gone' ) os . kill ( proc . pid , signal . SIGKILL ) except : # the process is just already gone pass return new_procs
7159	def ask ( self , error = None ) : q = self . next_question if q is None : return try : answer = q . prompter ( self . get_prompt ( q , error ) , * q . prompter_args , * * q . prompter_kwargs ) except QuestionnaireGoBack as e : steps = e . args [ 0 ] if e . args else 1 if steps == 0 : self . ask ( ) # user can redo current question even if `can_go_back` is `False` return self . go_back ( steps ) else : if q . _validate : error = q . _validate ( answer ) if error : self . ask ( error ) return if q . _transform : answer = q . _transform ( answer ) self . answers [ q . key ] = answer return answer
110	def imshow ( image , backend = IMSHOW_BACKEND_DEFAULT ) : do_assert ( backend in [ "matplotlib" , "cv2" ] , "Expected backend 'matplotlib' or 'cv2', got %s." % ( backend , ) ) if backend == "cv2" : image_bgr = image if image . ndim == 3 and image . shape [ 2 ] in [ 3 , 4 ] : image_bgr = image [ ... , 0 : 3 ] [ ... , : : - 1 ] win_name = "imgaug-default-window" cv2 . namedWindow ( win_name , cv2 . WINDOW_NORMAL ) cv2 . imshow ( win_name , image_bgr ) cv2 . waitKey ( 0 ) cv2 . destroyWindow ( win_name ) else : # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225) import matplotlib . pyplot as plt dpi = 96 h , w = image . shape [ 0 ] / dpi , image . shape [ 1 ] / dpi w = max ( w , 6 ) # if the figure is too narrow, the footer may appear and make the fig suddenly wider (ugly) fig , ax = plt . subplots ( figsize = ( w , h ) , dpi = dpi ) fig . canvas . set_window_title ( "imgaug.imshow(%s)" % ( image . shape , ) ) ax . imshow ( image , cmap = "gray" ) # cmap is only activate for grayscale images plt . show ( )
12271	def get_schema ( self , filename ) : table_set = self . read_file ( filename ) # Have I been able to read the filename if table_set is None : return [ ] # Get the first table as rowset row_set = table_set . tables [ 0 ] offset , headers = headers_guess ( row_set . sample ) row_set . register_processor ( headers_processor ( headers ) ) row_set . register_processor ( offset_processor ( offset + 1 ) ) types = type_guess ( row_set . sample , strict = True ) # Get a sample as well.. sample = next ( row_set . sample ) clean = lambda v : str ( v ) if not isinstance ( v , str ) else v schema = [ ] for i , h in enumerate ( headers ) : schema . append ( [ h , str ( types [ i ] ) , clean ( sample [ i ] . value ) ] ) return schema
11008	def get_project_slug ( self , bet ) : if bet . get ( 'form_params' ) : params = json . loads ( bet [ 'form_params' ] ) return params . get ( 'project' ) return None
13311	def site_path ( self ) : if platform == 'win' : return unipath ( self . path , 'Lib' , 'site-packages' ) py_ver = 'python{0}' . format ( sys . version [ : 3 ] ) return unipath ( self . path , 'lib' , py_ver , 'site-packages' )
2705	def collect_phrases ( sent , ranks , spacy_nlp ) : tail = 0 last_idx = sent [ 0 ] . idx - 1 phrase = [ ] while tail < len ( sent ) : w = sent [ tail ] if ( w . word_id > 0 ) and ( w . root in ranks ) and ( ( w . idx - last_idx ) == 1 ) : # keep collecting... rl = RankedLexeme ( text = w . raw . lower ( ) , rank = ranks [ w . root ] , ids = w . word_id , pos = w . pos . lower ( ) , count = 1 ) phrase . append ( rl ) else : # just hit a phrase boundary for text , p in enumerate_chunks ( phrase , spacy_nlp ) : if p : id_list = [ rl . ids for rl in p ] rank_list = [ rl . rank for rl in p ] np_rl = RankedLexeme ( text = text , rank = rank_list , ids = id_list , pos = "np" , count = 1 ) if DEBUG : print ( np_rl ) yield np_rl phrase = [ ] last_idx = w . idx tail += 1
9640	def require_template_debug ( f ) : def _ ( * args , * * kwargs ) : TEMPLATE_DEBUG = getattr ( settings , 'TEMPLATE_DEBUG' , False ) return f ( * args , * * kwargs ) if TEMPLATE_DEBUG else '' return _
1108	def get_grouped_opcodes ( self , n = 3 ) : codes = self . get_opcodes ( ) if not codes : codes = [ ( "equal" , 0 , 1 , 0 , 1 ) ] # Fixup leading and trailing groups if they show no changes. if codes [ 0 ] [ 0 ] == 'equal' : tag , i1 , i2 , j1 , j2 = codes [ 0 ] codes [ 0 ] = tag , max ( i1 , i2 - n ) , i2 , max ( j1 , j2 - n ) , j2 if codes [ - 1 ] [ 0 ] == 'equal' : tag , i1 , i2 , j1 , j2 = codes [ - 1 ] codes [ - 1 ] = tag , i1 , min ( i2 , i1 + n ) , j1 , min ( j2 , j1 + n ) nn = n + n group = [ ] for tag , i1 , i2 , j1 , j2 in codes : # End the current group and start a new one whenever # there is a large range with no changes. if tag == 'equal' and i2 - i1 > nn : group . append ( ( tag , i1 , min ( i2 , i1 + n ) , j1 , min ( j2 , j1 + n ) ) ) yield group group = [ ] i1 , j1 = max ( i1 , i2 - n ) , max ( j1 , j2 - n ) group . append ( ( tag , i1 , i2 , j1 , j2 ) ) if group and not ( len ( group ) == 1 and group [ 0 ] [ 0 ] == 'equal' ) : yield group
4336	def overdrive ( self , gain_db = 20.0 , colour = 20.0 ) : if not is_number ( gain_db ) : raise ValueError ( 'db_level must be a number.' ) if not is_number ( colour ) : raise ValueError ( 'colour must be a number.' ) effect_args = [ 'overdrive' , '{:f}' . format ( gain_db ) , '{:f}' . format ( colour ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'overdrive' ) return self
5410	def build_machine_type ( cls , min_cores , min_ram ) : min_cores = min_cores or job_model . DEFAULT_MIN_CORES min_ram = min_ram or job_model . DEFAULT_MIN_RAM # First, min_ram is given in GB. Convert to MB. min_ram *= GoogleV2CustomMachine . _MB_PER_GB # Only machine types with 1 vCPU or an even number of vCPUs can be created. cores = cls . _validate_cores ( min_cores ) # The total memory of the instance must be a multiple of 256 MB. ram = cls . _validate_ram ( min_ram ) # Memory must be between 0.9 GB per vCPU, up to 6.5 GB per vCPU. memory_to_cpu_ratio = ram / cores if memory_to_cpu_ratio < GoogleV2CustomMachine . _MIN_MEMORY_PER_CPU : # If we're under the ratio, top up the memory. adjusted_ram = GoogleV2CustomMachine . _MIN_MEMORY_PER_CPU * cores ram = cls . _validate_ram ( adjusted_ram ) elif memory_to_cpu_ratio > GoogleV2CustomMachine . _MAX_MEMORY_PER_CPU : # If we're over the ratio, top up the CPU. adjusted_cores = math . ceil ( ram / GoogleV2CustomMachine . _MAX_MEMORY_PER_CPU ) cores = cls . _validate_cores ( adjusted_cores ) else : # Ratio is within the restrictions - no adjustments needed. pass return 'custom-{}-{}' . format ( int ( cores ) , int ( ram ) )
11797	def nconflicts ( self , var , val , assignment ) : # Subclasses may implement this more efficiently def conflict ( var2 ) : return ( var2 in assignment and not self . constraints ( var , val , var2 , assignment [ var2 ] ) ) return count_if ( conflict , self . neighbors [ var ] )
3683	def calculate ( self , T , method ) : if method == WAGNER_MCGARRY : Psat = Wagner_original ( T , self . WAGNER_MCGARRY_Tc , self . WAGNER_MCGARRY_Pc , * self . WAGNER_MCGARRY_coefs ) elif method == WAGNER_POLING : Psat = Wagner ( T , self . WAGNER_POLING_Tc , self . WAGNER_POLING_Pc , * self . WAGNER_POLING_coefs ) elif method == ANTOINE_EXTENDED_POLING : Psat = TRC_Antoine_extended ( T , * self . ANTOINE_EXTENDED_POLING_coefs ) elif method == ANTOINE_POLING : A , B , C = self . ANTOINE_POLING_coefs Psat = Antoine ( T , A , B , C , base = 10.0 ) elif method == DIPPR_PERRY_8E : Psat = EQ101 ( T , * self . Perrys2_8_coeffs ) elif method == VDI_PPDS : Psat = Wagner ( T , self . VDI_PPDS_Tc , self . VDI_PPDS_Pc , * self . VDI_PPDS_coeffs ) elif method == COOLPROP : Psat = PropsSI ( 'P' , 'T' , T , 'Q' , 0 , self . CASRN ) elif method == BOILING_CRITICAL : Psat = boiling_critical_relation ( T , self . Tb , self . Tc , self . Pc ) elif method == LEE_KESLER_PSAT : Psat = Lee_Kesler ( T , self . Tc , self . Pc , self . omega ) elif method == AMBROSE_WALTON : Psat = Ambrose_Walton ( T , self . Tc , self . Pc , self . omega ) elif method == SANJARI : Psat = Sanjari ( T , self . Tc , self . Pc , self . omega ) elif method == EDALAT : Psat = Edalat ( T , self . Tc , self . Pc , self . omega ) elif method == EOS : Psat = self . eos [ 0 ] . Psat ( T ) elif method in self . tabular_data : Psat = self . interpolate ( T , method ) return Psat
3221	def get_gcp_client ( * * kwargs ) : return _gcp_client ( project = kwargs [ 'project' ] , mod_name = kwargs [ 'mod_name' ] , pkg_name = kwargs . get ( 'pkg_name' , 'google.cloud' ) , key_file = kwargs . get ( 'key_file' , None ) , http_auth = kwargs . get ( 'http' , None ) , user_agent = kwargs . get ( 'user_agent' , None ) )
2060	def add ( self , constraint , check = False ) : if isinstance ( constraint , bool ) : constraint = BoolConstant ( constraint ) assert isinstance ( constraint , Bool ) constraint = simplify ( constraint ) # If self._child is not None this constraint set has been forked and a # a derived constraintset may be using this. So we can't add any more # constraints to this one. After the child constraintSet is deleted # we regain the ability to add constraints. if self . _child is not None : raise Exception ( 'ConstraintSet is frozen' ) if isinstance ( constraint , BoolConstant ) : if not constraint . value : logger . info ( "Adding an impossible constant constraint" ) self . _constraints = [ constraint ] else : return self . _constraints . append ( constraint ) if check : from . . . core . smtlib import solver if not solver . check ( self ) : raise ValueError ( "Added an impossible constraint" )
8051	def _darkest ( self ) : rgb , n = ( 1.0 , 1.0 , 1.0 ) , 3.0 for r , g , b in self : if r + g + b < n : rgb , n = ( r , g , b ) , r + g + b return rgb
2992	def mutualFundSymbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( mutualFundSymbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
12069	def save ( abf , fname = None , tag = None , width = 700 , close = True , facecolor = 'w' , resize = True ) : if len ( pylab . gca ( ) . get_lines ( ) ) == 0 : print ( "can't save, no figure!" ) return if resize : pylab . tight_layout ( ) pylab . subplots_adjust ( bottom = .1 ) annotate ( abf ) if tag : fname = abf . outpath + abf . ID + "_" + tag + ".png" inchesX , inchesY = pylab . gcf ( ) . get_size_inches ( ) dpi = width / inchesX if fname : if not os . path . exists ( abf . outpath ) : os . mkdir ( abf . outpath ) print ( " <- saving [%s] at %d DPI (%dx%d)" % ( os . path . basename ( fname ) , dpi , inchesX * dpi , inchesY * dpi ) ) pylab . savefig ( fname , dpi = dpi , facecolor = facecolor ) else : pylab . show ( ) if close : pylab . close ( )
12169	def _dispatch ( self , event , listener , * args , * * kwargs ) : if ( asyncio . iscoroutinefunction ( listener ) or isinstance ( listener , functools . partial ) and asyncio . iscoroutinefunction ( listener . func ) ) : return self . _dispatch_coroutine ( event , listener , * args , * * kwargs ) return self . _dispatch_function ( event , listener , * args , * * kwargs )
11435	def _validate_record_field_positions_global ( record ) : all_fields = [ ] for tag , fields in record . items ( ) : previous_field_position_global = - 1 for field in fields : if field [ 4 ] < previous_field_position_global : return ( "Non ascending global field positions in tag '%s'." % tag ) previous_field_position_global = field [ 4 ] if field [ 4 ] in all_fields : return ( "Duplicate global field position '%d' in tag '%s'" % ( field [ 4 ] , tag ) )
3501	def assess_precursors ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : warn ( 'use assess_component instead' , DeprecationWarning ) return assess_component ( model , reaction , 'reactants' , flux_coefficient_cutoff , solver )
10372	def node_has_namespace ( node : BaseEntity , namespace : str ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns == namespace
4542	def apply ( self , function ) : for cut in self . cuts : value = self . read ( cut ) function ( value ) self . write ( cut , value )
7346	async def get_oauth_token ( consumer_key , consumer_secret , callback_uri = "oob" ) : client = BasePeonyClient ( consumer_key = consumer_key , consumer_secret = consumer_secret , api_version = "" , suffix = "" ) response = await client . api . oauth . request_token . post ( _suffix = "" , oauth_callback = callback_uri ) return parse_token ( response )
5327	def sha_github_file ( cls , config , repo_file , repository_api , repository_branch ) : repo_file_sha = None cfg = config . get_conf ( ) github_token = cfg [ 'sortinghat' ] [ 'identities_api_token' ] headers = { "Authorization" : "token " + github_token } url_dir = repository_api + "/git/trees/" + repository_branch logger . debug ( "Gettting sha data from tree: %s" , url_dir ) raw_repo_file_info = requests . get ( url_dir , headers = headers ) raw_repo_file_info . raise_for_status ( ) for rfile in raw_repo_file_info . json ( ) [ 'tree' ] : if rfile [ 'path' ] == repo_file : logger . debug ( "SHA found: %s, " , rfile [ "sha" ] ) repo_file_sha = rfile [ "sha" ] break return repo_file_sha
8168	def run ( self ) : with LiveExecution . lock : if self . edited_source : success , ex = self . run_tenuous ( ) if success : return self . do_exec ( self . known_good , self . ns )
11993	def set_encryption_passphrases ( self , encryption_passphrases ) : self . encryption_passphrases = self . _update_dict ( encryption_passphrases , { } , replace_data = True )
10251	def remove_highlight_nodes ( graph : BELGraph , nodes : Optional [ Iterable [ BaseEntity ] ] = None ) -> None : for node in graph if nodes is None else nodes : if is_node_highlighted ( graph , node ) : del graph . node [ node ] [ NODE_HIGHLIGHT ]
3046	def _do_refresh_request ( self , http ) : body = self . _generate_refresh_request_body ( ) headers = self . _generate_refresh_request_headers ( ) logger . info ( 'Refreshing access_token' ) resp , content = transport . request ( http , self . token_uri , method = 'POST' , body = body , headers = headers ) content = _helpers . _from_bytes ( content ) if resp . status == http_client . OK : d = json . loads ( content ) self . token_response = d self . access_token = d [ 'access_token' ] self . refresh_token = d . get ( 'refresh_token' , self . refresh_token ) if 'expires_in' in d : delta = datetime . timedelta ( seconds = int ( d [ 'expires_in' ] ) ) self . token_expiry = delta + _UTCNOW ( ) else : self . token_expiry = None if 'id_token' in d : self . id_token = _extract_id_token ( d [ 'id_token' ] ) self . id_token_jwt = d [ 'id_token' ] else : self . id_token = None self . id_token_jwt = None # On temporary refresh errors, the user does not actually have to # re-authorize, so we unflag here. self . invalid = False if self . store : self . store . locked_put ( self ) else : # An {'error':...} response body means the token is expired or # revoked, so we flag the credentials as such. logger . info ( 'Failed to retrieve access token: %s' , content ) error_msg = 'Invalid response {0}.' . format ( resp . status ) try : d = json . loads ( content ) if 'error' in d : error_msg = d [ 'error' ] if 'error_description' in d : error_msg += ': ' + d [ 'error_description' ] self . invalid = True if self . store is not None : self . store . locked_put ( self ) except ( TypeError , ValueError ) : pass raise HttpAccessTokenRefreshError ( error_msg , status = resp . status )
9979	def extract_params ( source ) : funcdef = find_funcdef ( source ) params = [ ] for node in ast . walk ( funcdef . args ) : if isinstance ( node , ast . arg ) : if node . arg not in params : params . append ( node . arg ) return params
930	def _createAggregateRecord ( self ) : record = [ ] for i , ( fieldIdx , aggFP , paramIdx ) in enumerate ( self . _fields ) : if aggFP is None : # this field is not supposed to be aggregated. continue values = self . _slice [ i ] refIndex = None if paramIdx is not None : record . append ( aggFP ( values , self . _slice [ paramIdx ] ) ) else : record . append ( aggFP ( values ) ) return record
2508	def get_extr_lics_comment ( self , extr_lics ) : comment_list = list ( self . graph . triples ( ( extr_lics , RDFS . comment , None ) ) ) if len ( comment_list ) > 1 : self . more_than_one_error ( 'extracted license comment' ) return elif len ( comment_list ) == 1 : return comment_list [ 0 ] [ 2 ] else : return
695	def _loadDescriptionFile ( descriptionPyPath ) : global g_descriptionImportCount if not os . path . isfile ( descriptionPyPath ) : raise RuntimeError ( ( "Experiment description file %s does not exist or " + "is not a file" ) % ( descriptionPyPath , ) ) mod = imp . load_source ( "pf_description%d" % g_descriptionImportCount , descriptionPyPath ) g_descriptionImportCount += 1 if not hasattr ( mod , "descriptionInterface" ) : raise RuntimeError ( "Experiment description file %s does not define %s" % ( descriptionPyPath , "descriptionInterface" ) ) if not isinstance ( mod . descriptionInterface , exp_description_api . DescriptionIface ) : raise RuntimeError ( ( "Experiment description file %s defines %s but it " + "is not DescriptionIface-based" ) % ( descriptionPyPath , name ) ) return mod
7532	def trackjobs ( func , results , spacer ) : ## TODO: try to insert a better way to break on KBD here. LOGGER . info ( "inside trackjobs of %s" , func ) ## get just the jobs from results that are relevant to this func asyncs = [ ( i , results [ i ] ) for i in results if i . split ( "-" , 2 ) [ 0 ] == func ] ## progress bar start = time . time ( ) while 1 : ## how many of this func have finished so far ready = [ i [ 1 ] . ready ( ) for i in asyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) printstr = " {} | {} | s3 |" . format ( PRINTSTR [ func ] , elapsed ) progressbar ( len ( ready ) , sum ( ready ) , printstr , spacer = spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break sfails = [ ] errmsgs = [ ] for job in asyncs : if not job [ 1 ] . successful ( ) : sfails . append ( job [ 0 ] ) errmsgs . append ( job [ 1 ] . result ( ) ) return func , sfails , errmsgs
5125	def simulate ( self , n = 1 , t = None ) : if not self . _initialized : msg = ( "Network has not been initialized. " "Call '.initialize()' first." ) raise QueueingToolError ( msg ) if t is None : for dummy in range ( n ) : self . _simulate_next_event ( slow = False ) else : now = self . _t while self . _t < now + t : self . _simulate_next_event ( slow = False )
1923	def binary_arch ( binary ) : with open ( binary , 'rb' ) as f : elffile = ELFFile ( f ) if elffile [ 'e_machine' ] == 'EM_X86_64' : return True else : return False
13489	def update ( self , server ) : for chunk in self . __cut_to_size ( ) : server . put ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
11242	def indent_css ( f , output ) : line_count = get_line_count ( f ) f = open ( f , 'r+' ) output = open ( output , 'r+' ) for line in range ( line_count ) : string = f . readline ( ) . rstrip ( ) if len ( string ) > 0 : if string [ - 1 ] == ";" : output . write ( " " + string + "\n" ) else : output . write ( string + "\n" ) output . close ( ) f . close ( )
6606	def run_multiple ( self , workingArea , package_indices ) : if not package_indices : return [ ] job_desc = self . _compose_job_desc ( workingArea , package_indices ) clusterprocids = submit_jobs ( job_desc , cwd = workingArea . path ) # TODO: make configurable clusterids = clusterprocids2clusterids ( clusterprocids ) for clusterid in clusterids : change_job_priority ( [ clusterid ] , 10 ) self . clusterprocids_outstanding . extend ( clusterprocids ) return clusterprocids
5269	def _label_generalized ( self , node ) : if node . is_leaf ( ) : x = { self . _get_word_start_index ( node . idx ) } else : x = { n for ns in node . transition_links for n in ns [ 0 ] . generalized_idxs } node . generalized_idxs = x
5173	def get_install_requires ( ) : requirements = [ ] for line in open ( 'requirements.txt' ) . readlines ( ) : # skip to next iteration if comment or empty line if line . startswith ( '#' ) or line == '' or line . startswith ( 'http' ) or line . startswith ( 'git' ) : continue # add line to requirements requirements . append ( line . replace ( '\n' , '' ) ) # add py2-ipaddress if python2 if sys . version_info . major < 3 : requirements . append ( 'py2-ipaddress' ) return requirements
2387	def sub_chars ( string ) : #Define replacement patterns sub_pat = r"[^A-Za-z\.\?!,';:]" char_pat = r"\." com_pat = r"," ques_pat = r"\?" excl_pat = r"!" sem_pat = r";" col_pat = r":" whitespace_pat = r"\s{1,}" #Replace text. Ordering is very important! nstring = re . sub ( sub_pat , " " , string ) nstring = re . sub ( char_pat , " ." , nstring ) nstring = re . sub ( com_pat , " ," , nstring ) nstring = re . sub ( ques_pat , " ?" , nstring ) nstring = re . sub ( excl_pat , " !" , nstring ) nstring = re . sub ( sem_pat , " ;" , nstring ) nstring = re . sub ( col_pat , " :" , nstring ) nstring = re . sub ( whitespace_pat , " " , nstring ) return nstring
11442	def _warning ( code ) : if isinstance ( code , str ) : return code message = '' if isinstance ( code , tuple ) : if isinstance ( code [ 0 ] , str ) : message = code [ 1 ] code = code [ 0 ] return CFG_BIBRECORD_WARNING_MSGS . get ( code , '' ) + message
2092	def last_job_data ( self , pk = None , * * kwargs ) : ujt = self . get ( pk , include_debug_header = True , * * kwargs ) # Determine the appropriate inventory source update. if 'current_update' in ujt [ 'related' ] : debug . log ( 'A current job; retrieving it.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'current_update' ] [ 7 : ] ) . json ( ) elif ujt [ 'related' ] . get ( 'last_update' , None ) : debug . log ( 'No current job or update exists; retrieving the most recent.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'last_update' ] [ 7 : ] ) . json ( ) else : raise exc . NotFound ( 'No related jobs or updates exist.' )
6811	def pre_deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service_pre_deployers . get ( service ) if funcs : print ( 'Running pre-deployments for service %s...' % ( service , ) ) for func in funcs : func ( )
13889	def ListMappedNetworkDrives ( ) : if sys . platform != 'win32' : raise NotImplementedError drives_list = [ ] netuse = _CallWindowsNetCommand ( [ 'use' ] ) for line in netuse . split ( EOL_STYLE_WINDOWS ) : match = re . match ( "(\w*)\s+(\w:)\s+(.+)" , line . rstrip ( ) ) if match : drives_list . append ( ( match . group ( 2 ) , match . group ( 3 ) , match . group ( 1 ) == 'OK' ) ) return drives_list
8411	def scaled_limits ( self ) : _min = self . limits [ 0 ] / self . factor _max = self . limits [ 1 ] / self . factor return _min , _max
10882	def aN ( a , dim = 3 , dtype = 'int' ) : if not hasattr ( a , '__iter__' ) : return np . array ( [ a ] * dim , dtype = dtype ) return np . array ( a ) . astype ( dtype )
12279	def add ( repo , args , targetdir , execute = False , generator = False , includes = [ ] , script = False , source = None ) : # Gather the files... if not execute : files = add_files ( args = args , targetdir = targetdir , source = source , script = script , generator = generator ) else : files = run_executable ( repo , args , includes ) if files is None or len ( files ) == 0 : return repo # Update the repo package but with only those that have changed. filtered_files = [ ] package = repo . package for h in files : found = False for i , r in enumerate ( package [ 'resources' ] ) : if h [ 'relativepath' ] == r [ 'relativepath' ] : found = True if h [ 'sha256' ] == r [ 'sha256' ] : change = False for attr in [ 'source' ] : if h [ attr ] != r [ attr ] : r [ attr ] = h [ attr ] change = True if change : filtered_files . append ( h ) continue else : filtered_files . append ( h ) package [ 'resources' ] [ i ] = h break if not found : filtered_files . append ( h ) package [ 'resources' ] . append ( h ) if len ( filtered_files ) == 0 : return 0 # Copy the files repo . manager . add_files ( repo , filtered_files ) # Write to disk... rootdir = repo . rootdir with cd ( rootdir ) : datapath = "datapackage.json" with open ( datapath , 'w' ) as fd : fd . write ( json . dumps ( package , indent = 4 ) ) return len ( filtered_files )
6914	def generate_rrab_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.45 , scale = 0.35 ) , 'fourierorder' : [ 8 , 11 ] , 'amplitude' : sps . uniform ( loc = 0.4 , scale = 0.5 ) , 'phioffset' : np . pi , } , magsarefluxes = False ) : modeldict = generate_sinusoidal_lightcurve ( times , mags = mags , errs = errs , paramdists = paramdists , magsarefluxes = magsarefluxes ) modeldict [ 'vartype' ] = 'RRab' return modeldict
1075	def _days_in_month ( year , month ) : assert 1 <= month <= 12 , month if month == 2 and _is_leap ( year ) : return 29 return _DAYS_IN_MONTH [ month ]
11596	def _rc_keys ( self , pattern = '*' ) : result = [ ] for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result . extend ( redisent . keys ( pattern ) ) return result
13074	def register_assets ( self ) : self . blueprint . add_url_rule ( # Register another path to ensure assets compatibility "{0}.secondary/<filetype>/<asset>" . format ( self . static_url_path ) , view_func = self . r_assets , endpoint = "secondary_assets" , methods = [ "GET" ] )
1078	def isoformat ( self ) : # return "%04d-%02d-%02d" % (self._year, self._month, self._day) return "%s-%s-%s" % ( str ( self . _year ) . zfill ( 4 ) , str ( self . _month ) . zfill ( 2 ) , str ( self . _day ) . zfill ( 2 ) )
735	def sort ( filename , key , outputFile , fields = None , watermark = 1024 * 1024 * 100 ) : if fields is not None : assert set ( key ) . issubset ( set ( [ f [ 0 ] for f in fields ] ) ) with FileRecordStream ( filename ) as f : # Find the indices of the requested fields if fields : fieldNames = [ ff [ 0 ] for ff in fields ] indices = [ f . getFieldNames ( ) . index ( name ) for name in fieldNames ] assert len ( indices ) == len ( fields ) else : fileds = f . getFields ( ) fieldNames = f . getFieldNames ( ) indices = None # turn key fields to key indices key = [ fieldNames . index ( name ) for name in key ] chunk = 0 records = [ ] for i , r in enumerate ( f ) : # Select requested fields only if indices : temp = [ ] for i in indices : temp . append ( r [ i ] ) r = temp # Store processed record records . append ( r ) # Check memory available_memory = psutil . avail_phymem ( ) # If bellow the watermark create a new chunk, reset and keep going if available_memory < watermark : _sortChunk ( records , key , chunk , fields ) records = [ ] chunk += 1 # Sort and write the remainder if len ( records ) > 0 : _sortChunk ( records , key , chunk , fields ) chunk += 1 # Marge all the files _mergeFiles ( key , chunk , outputFile , fields )
2082	def lookup_stdout ( self , pk = None , start_line = None , end_line = None , full = True ) : uj_res = get_resource ( 'unified_job' ) # Filters # - limit search to jobs spawned as part of this workflow job # - order in the order in which they should add to the list # - only include final job states query_params = ( ( 'unified_job_node__workflow_job' , pk ) , ( 'order_by' , 'finished' ) , ( 'status__in' , 'successful,failed,error' ) ) jobs_list = uj_res . list ( all_pages = True , query = query_params ) if jobs_list [ 'count' ] == 0 : return '' return_content = ResSubcommand ( uj_res ) . _format_human ( jobs_list ) lines = return_content . split ( '\n' ) if not full : lines = lines [ : - 1 ] N = len ( lines ) start_range = start_line if start_line is None : start_range = 0 elif start_line > N : start_range = N end_range = end_line if end_line is None or end_line > N : end_range = N lines = lines [ start_range : end_range ] return_content = '\n' . join ( lines ) if len ( lines ) > 0 : return_content += '\n' return return_content
13547	def _setVirtualEnv ( ) : try : activate = options . virtualenv . activate_cmd except AttributeError : activate = None if activate is None : virtualenv = path ( os . environ . get ( 'VIRTUAL_ENV' , '' ) ) if not virtualenv : virtualenv = options . paved . cwd else : virtualenv = path ( virtualenv ) activate = virtualenv / 'bin' / 'activate' if activate . exists ( ) : info ( 'Using default virtualenv at %s' % activate ) options . setdotted ( 'virtualenv.activate_cmd' , 'source %s' % activate )
10287	def enrich_complexes ( graph : BELGraph ) -> None : nodes = list ( get_nodes_by_function ( graph , COMPLEX ) ) for u in nodes : for v in u . members : graph . add_has_component ( u , v )
8731	def get_period_seconds ( period ) : if isinstance ( period , six . string_types ) : try : name = 'seconds_per_' + period . lower ( ) result = globals ( ) [ name ] except KeyError : msg = "period not in (second, minute, hour, day, month, year)" raise ValueError ( msg ) elif isinstance ( period , numbers . Number ) : result = period elif isinstance ( period , datetime . timedelta ) : result = period . days * get_period_seconds ( 'day' ) + period . seconds else : raise TypeError ( 'period must be a string or integer' ) return result
2116	def convert ( self , value , param , ctx ) : # Protect against corner cases of invalid inputs if not isinstance ( value , str ) : return value if isinstance ( value , six . binary_type ) : value = value . decode ( 'UTF-8' ) # Read from a file under these cases if value . startswith ( '@' ) : filename = os . path . expanduser ( value [ 1 : ] ) file_obj = super ( Variables , self ) . convert ( filename , param , ctx ) if hasattr ( file_obj , 'read' ) : # Sometimes click.File may return a buffer and not a string return file_obj . read ( ) return file_obj # No file, use given string return value
13603	def error_message ( self , message , fh = None , prefix = "[error]:" , suffix = "..." ) : msg = prefix + message + suffix fh = fh or sys . stderr if fh is sys . stderr : termcolor . cprint ( msg , color = "red" ) else : fh . write ( msg ) pass
11226	def dump_nparray ( self , obj , class_name = numpy_ndarray_class_name ) : return { "$" + class_name : self . _json_convert ( obj . tolist ( ) ) }
11500	def get_community_children ( self , community_id , token = None ) : parameters = dict ( ) parameters [ 'id' ] = community_id if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.children' , parameters ) return response
468	def sample_top ( a = None , top_k = 10 ) : if a is None : a = [ ] idx = np . argpartition ( a , - top_k ) [ - top_k : ] probs = a [ idx ] # tl.logging.info("new %f" % probs) probs = probs / np . sum ( probs ) choice = np . random . choice ( idx , p = probs ) return choice
6292	def add_data_dir ( self , directory ) : dirs = list ( self . DATA_DIRS ) dirs . append ( directory ) self . DATA_DIRS = dirs
4062	def show_condition_operators ( self , condition ) : # dict keys of allowed operators for the current condition permitted_operators = self . savedsearch . conditions_operators . get ( condition ) # transform these into values permitted_operators_list = set ( [ self . savedsearch . operators . get ( op ) for op in permitted_operators ] ) return permitted_operators_list
5817	def _write_callback ( connection_id , data_buffer , data_length_pointer ) : try : self = _connection_refs . get ( connection_id ) if not self : socket = _socket_refs . get ( connection_id ) else : socket = self . _socket if not self and not socket : return 0 data_length = deref ( data_length_pointer ) data = bytes_from_buffer ( data_buffer , data_length ) if self and not self . _done_handshake : self . _client_hello += data error = None try : sent = socket . send ( data ) except ( socket_ . error ) as e : error = e . errno if error is not None and error != errno . EAGAIN : if error == errno . ECONNRESET or error == errno . EPIPE : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort if sent != data_length : pointer_set ( data_length_pointer , sent ) return SecurityConst . errSSLWouldBlock return 0 except ( KeyboardInterrupt ) as e : self . _exception = e return SecurityConst . errSSLPeerUserCancelled
2134	def _get_schema ( self , wfjt_id ) : node_res = get_resource ( 'node' ) node_results = node_res . list ( workflow_job_template = wfjt_id , all_pages = True ) [ 'results' ] return self . _workflow_node_structure ( node_results )
9289	def _send_login ( self ) : login_str = "user {0} pass {1} vers aprslib {3}{2}\r\n" login_str = login_str . format ( self . callsign , self . passwd , ( " filter " + self . filter ) if self . filter != "" else "" , __version__ ) self . logger . info ( "Sending login information" ) try : self . _sendall ( login_str ) self . sock . settimeout ( 5 ) test = self . sock . recv ( len ( login_str ) + 100 ) if is_py3 : test = test . decode ( 'latin-1' ) test = test . rstrip ( ) self . logger . debug ( "Server: %s" , test ) _ , _ , callsign , status , _ = test . split ( ' ' , 4 ) if callsign == "" : raise LoginError ( "Server responded with empty callsign???" ) if callsign != self . callsign : raise LoginError ( "Server: %s" % test ) if status != "verified," and self . passwd != "-1" : raise LoginError ( "Password is incorrect" ) if self . passwd == "-1" : self . logger . info ( "Login successful (receive only)" ) else : self . logger . info ( "Login successful" ) except LoginError as e : self . logger . error ( str ( e ) ) self . close ( ) raise except : self . close ( ) self . logger . error ( "Failed to login" ) raise LoginError ( "Failed to login" )
5198	def GetApplicationIIN ( self ) : application_iin = opendnp3 . ApplicationIIN ( ) application_iin . configCorrupt = False application_iin . deviceTrouble = False application_iin . localControl = False application_iin . needTime = False # Just for testing purposes, convert it to an IINField and display the contents of the two bytes. iin_field = application_iin . ToIIN ( ) _log . debug ( 'OutstationApplication.GetApplicationIIN: IINField LSB={}, MSB={}' . format ( iin_field . LSB , iin_field . MSB ) ) return application_iin
9752	def build_swig ( ) : print ( "Looking for FANN libs..." ) find_fann ( ) print ( "running SWIG..." ) swig_bin = find_swig ( ) swig_cmd = [ swig_bin , '-c++' , '-python' , 'fann2/fann2.i' ] subprocess . Popen ( swig_cmd ) . wait ( )
5360	def execute_batch_tasks ( self , tasks_cls , big_delay = 0 , small_delay = 0 , wait_for_threads = True ) : def _split_tasks ( tasks_cls ) : """ we internally distinguish between tasks executed by backend and tasks executed with no specific backend. """ backend_t = [ ] global_t = [ ] for t in tasks_cls : if t . is_backend_task ( t ) : backend_t . append ( t ) else : global_t . append ( t ) return backend_t , global_t backend_tasks , global_tasks = _split_tasks ( tasks_cls ) logger . debug ( 'backend_tasks = %s' % ( backend_tasks ) ) logger . debug ( 'global_tasks = %s' % ( global_tasks ) ) threads = [ ] # stopper won't be set unless wait_for_threads is True stopper = threading . Event ( ) # launching threads for tasks by backend if len ( backend_tasks ) > 0 : repos_backend = self . _get_repos_by_backend ( ) for backend in repos_backend : # Start new Threads and add them to the threads list to complete t = TasksManager ( backend_tasks , backend , stopper , self . config , small_delay ) threads . append ( t ) t . start ( ) # launch thread for global tasks if len ( global_tasks ) > 0 : # FIXME timer is applied to all global_tasks, does it make sense? # All tasks are executed in the same thread sequentially gt = TasksManager ( global_tasks , "Global tasks" , stopper , self . config , big_delay ) threads . append ( gt ) gt . start ( ) if big_delay > 0 : when = datetime . now ( ) + timedelta ( seconds = big_delay ) when_str = when . strftime ( '%a, %d %b %Y %H:%M:%S %Z' ) logger . info ( "%s will be executed on %s" % ( global_tasks , when_str ) ) if wait_for_threads : time . sleep ( 1 ) # Give enough time create and run all threads stopper . set ( ) # All threads must stop in the next iteration # Wait for all threads to complete for t in threads : t . join ( ) # Checking for exceptions in threads to log them self . __check_queue_for_errors ( ) logger . debug ( "[thread:main] All threads (and their tasks) are finished" )
10827	def create ( cls , group , user , state = MembershipState . ACTIVE ) : with db . session . begin_nested ( ) : membership = cls ( user_id = user . get_id ( ) , id_group = group . id , state = state , ) db . session . add ( membership ) return membership
10509	def stoplog ( self ) : if self . _file_logger : self . logger . removeHandler ( _file_logger ) self . _file_logger = None return 1
13014	def remove_namespace ( doc , namespace ) : ns = u'{%s}' % namespace nsl = len ( ns ) for elem in doc . getiterator ( ) : if elem . tag . startswith ( ns ) : elem . tag = elem . tag [ nsl : ] elem . attrib [ 'oxmlns' ] = namespace
13248	def get_bibliography ( lsst_bib_names = None , bibtex = None ) : bibtex_data = get_lsst_bibtex ( bibtex_filenames = lsst_bib_names ) # Parse with pybtex into BibliographyData instances pybtex_data = [ pybtex . database . parse_string ( _bibtex , 'bibtex' ) for _bibtex in bibtex_data . values ( ) ] # Also parse local bibtex content if bibtex is not None : pybtex_data . append ( pybtex . database . parse_string ( bibtex , 'bibtex' ) ) # Merge BibliographyData bib = pybtex_data [ 0 ] if len ( pybtex_data ) > 1 : for other_bib in pybtex_data [ 1 : ] : for key , entry in other_bib . entries . items ( ) : bib . add_entry ( key , entry ) return bib
7224	def save ( self , project ) : # test if this is a create vs. an update if 'id' in project and project [ 'id' ] is not None : # update -> use put op self . logger . debug ( 'Updating existing project: ' + json . dumps ( project ) ) url = '%(base_url)s/%(project_id)s' % { 'base_url' : self . base_url , 'project_id' : project [ 'id' ] } r = self . gbdx_connection . put ( url , json = project ) try : r . raise_for_status ( ) except : print ( r . text ) raise # updates only get the Accepted response -> return the original project id return project [ 'id' ] else : self . logger . debug ( 'Creating new project: ' + json . dumps ( project ) ) # create -> use post op url = self . base_url r = self . gbdx_connection . post ( url , json = project ) try : r . raise_for_status ( ) except : print ( r . text ) raise project_json = r . json ( ) # create returns the saved project -> return the project id that's saved return project_json [ 'id' ]
10787	def add_subtract_misfeatured_tile ( st , tile , rad = 'calc' , max_iter = 3 , invert = 'guess' , max_allowed_remove = 20 , minmass = None , use_tp = False , * * kwargs ) : if rad == 'calc' : rad = guess_add_radii ( st ) if invert == 'guess' : invert = guess_invert ( st ) # 1. Remove all possibly bad particles within the tile. initial_error = np . copy ( st . error ) rinds = np . nonzero ( tile . contains ( st . obj_get_positions ( ) ) ) [ 0 ] if rinds . size >= max_allowed_remove : CLOG . fatal ( 'Misfeatured region too large!' ) raise RuntimeError elif rinds . size >= max_allowed_remove / 2 : CLOG . warn ( 'Large misfeatured regions.' ) elif rinds . size > 0 : rpos , rrad = st . obj_remove_particle ( rinds ) # 2-4. Feature & add particles to the tile, optimize, run until none added n_added = - rinds . size added_poses = [ ] for _ in range ( max_iter ) : if invert : im = 1 - st . residuals [ tile . slicer ] else : im = st . residuals [ tile . slicer ] guess , _ = _feature_guess ( im , rad , minmass = minmass , use_tp = use_tp ) accepts , poses = check_add_particles ( st , guess + tile . l , rad = rad , do_opt = True , * * kwargs ) added_poses . extend ( poses ) n_added += accepts if accepts == 0 : break else : # for-break-else CLOG . warn ( 'Runaway adds or insufficient max_iter' ) # 5. Optimize added pos + rad: ainds = [ ] for p in added_poses : ainds . append ( st . obj_closest_particle ( p ) ) if len ( ainds ) > max_allowed_remove : for i in range ( 0 , len ( ainds ) , max_allowed_remove ) : opt . do_levmarq_particles ( st , np . array ( ainds [ i : i + max_allowed_remove ] ) , include_rad = True , max_iter = 3 ) elif len ( ainds ) > 0 : opt . do_levmarq_particles ( st , ainds , include_rad = True , max_iter = 3 ) # 6. Ensure that current error after add-subtracting is lower than initial did_something = ( rinds . size > 0 ) or ( len ( ainds ) > 0 ) if did_something & ( st . error > initial_error ) : CLOG . info ( 'Failed addsub, Tile {} -> {}' . format ( tile . l . tolist ( ) , tile . r . tolist ( ) ) ) if len ( ainds ) > 0 : _ = st . obj_remove_particle ( ainds ) if rinds . size > 0 : for p , r in zip ( rpos . reshape ( - 1 , 3 ) , rrad . reshape ( - 1 ) ) : _ = st . obj_add_particle ( p , r ) n_added = 0 ainds = [ ] return n_added , ainds
10895	def set_filter ( self , slices , values ) : self . filters = [ [ sl , values [ sl ] ] for sl in slices ]
10283	def count_sources ( edge_iter : EdgeIterator ) -> Counter : return Counter ( u for u , _ , _ in edge_iter )
1633	def CheckForNewlineAtEOF ( filename , lines , error ) : # The array lines() was created by adding two newlines to the # original file (go figure), then splitting on \n. # To verify that the file ends in \n, we just have to make sure the # last-but-two element of lines() exists and is empty. if len ( lines ) < 3 or lines [ - 2 ] : error ( filename , len ( lines ) - 2 , 'whitespace/ending_newline' , 5 , 'Could not find a newline character at the end of the file.' )
3287	def _get_log ( self , limit = None ) : self . ui . pushbuffer ( ) commands . log ( self . ui , self . repo , limit = limit , date = None , rev = None , user = None ) res = self . ui . popbuffer ( ) . strip ( ) logList = [ ] for logentry in res . split ( "\n\n" ) : log = { } logList . append ( log ) for line in logentry . split ( "\n" ) : k , v = line . split ( ":" , 1 ) assert k in ( "changeset" , "tag" , "user" , "date" , "summary" ) log [ k . strip ( ) ] = v . strip ( ) log [ "parsed_date" ] = util . parse_time_string ( log [ "date" ] ) local_id , unid = log [ "changeset" ] . split ( ":" ) log [ "local_id" ] = int ( local_id ) log [ "unid" ] = unid # pprint(logList) return logList
11524	def mfa_otp_login ( self , temp_token , one_time_pass ) : parameters = dict ( ) parameters [ 'mfaTokenId' ] = temp_token parameters [ 'otp' ] = one_time_pass response = self . request ( 'midas.mfa.otp.login' , parameters ) return response [ 'token' ]
2751	def get_images ( self , private = False , type = None ) : params = { } if private : params [ 'private' ] = 'true' if type : params [ 'type' ] = type data = self . get_data ( "images/" , params = params ) images = list ( ) for jsoned in data [ 'images' ] : image = Image ( * * jsoned ) image . token = self . token images . append ( image ) return images
6436	def gen_fibonacci ( ) : num_a , num_b = 1 , 2 while True : yield num_a num_a , num_b = num_b , num_a + num_b
7962	def _close ( self ) : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) if self . _socket is None : return try : self . _socket . shutdown ( socket . SHUT_RDWR ) except socket . error : pass self . _socket . close ( ) self . _socket = None self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
5089	def dropHistoricalTable ( apps , schema_editor ) : table_name = 'sap_success_factors_historicalsapsuccessfactorsenterprisecus80ad' if table_name in connection . introspection . table_names ( ) : migrations . DeleteModel ( name = table_name , )
7570	def fastq_touchup_for_vsearch_merge ( read , outfile , reverse = False ) : counts = 0 with open ( outfile , 'w' ) as out : ## read in paired end read files 4 lines at a time if read . endswith ( ".gz" ) : fr1 = gzip . open ( read , 'rb' ) else : fr1 = open ( read , 'rb' ) quarts = itertools . izip ( * [ iter ( fr1 ) ] * 4 ) ## a list to store until writing writing = [ ] while 1 : try : lines = quarts . next ( ) except StopIteration : break if reverse : seq = lines [ 1 ] . strip ( ) [ : : - 1 ] else : seq = lines [ 1 ] . strip ( ) writing . append ( "" . join ( [ lines [ 0 ] , seq + "\n" , lines [ 2 ] , "B" * len ( seq ) ] ) ) ## write to disk counts += 1 if not counts % 1000 : out . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] if writing : out . write ( "\n" . join ( writing ) ) out . close ( ) fr1 . close ( )
3442	def to_json ( model , sort = False , * * kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ u"version" ] = JSON_SPEC return json . dumps ( obj , allow_nan = False , * * kwargs )
11533	def setup ( self , port ) : port = str ( port ) # timeout is used by all I/O operations self . _serial = serial . Serial ( port , 115200 , timeout = 2 ) time . sleep ( 2 ) # time to Arduino reset if not self . _serial . is_open : raise RuntimeError ( 'Could not connect to Arduino' ) self . _serial . write ( b'\x01' ) if self . _serial . read ( ) != b'\x06' : raise RuntimeError ( 'Could not connect to Arduino' ) ps = [ p for p in self . available_pins ( ) if p [ 'digital' ] [ 'output' ] ] for pin in ps : self . _set_pin_direction ( pin [ 'id' ] , ahio . Direction . Output )
4420	async def stop ( self ) : await self . _lavalink . ws . send ( op = 'stop' , guildId = self . guild_id ) self . current = None
4763	def soft_fail ( msg = '' ) : global _soft_ctx if _soft_ctx : global _soft_err _soft_err . append ( 'Fail: %s!' % msg if msg else 'Fail!' ) return fail ( msg )
1774	def invalidate_cache ( cpu , address , size ) : cache = cpu . instruction_cache for offset in range ( size ) : if address + offset in cache : del cache [ address + offset ]
4092	def addSearchers ( self , * searchers ) : self . _searchers . extend ( searchers ) debug . logger & debug . flagCompiler and debug . logger ( 'current compiled MIBs location(s): %s' % ', ' . join ( [ str ( x ) for x in self . _searchers ] ) ) return self
4664	def detail ( self , * args , * * kwargs ) : prefix = kwargs . pop ( "prefix" , default_prefix ) # remove dublicates kwargs [ "votes" ] = list ( set ( kwargs [ "votes" ] ) ) # # Sort votes # kwargs["votes"] = sorted( # kwargs["votes"], # key=lambda x: float(x.split(":")[1]), # ) return OrderedDict ( [ ( "memo_key" , PublicKey ( kwargs [ "memo_key" ] , prefix = prefix ) ) , ( "voting_account" , ObjectId ( kwargs [ "voting_account" ] , "account" ) ) , ( "num_witness" , Uint16 ( kwargs [ "num_witness" ] ) ) , ( "num_committee" , Uint16 ( kwargs [ "num_committee" ] ) ) , ( "votes" , Array ( [ VoteId ( o ) for o in kwargs [ "votes" ] ] ) ) , ( "extensions" , Set ( [ ] ) ) , ] )
5869	def _inactivate_organization_course_relationship ( relationship ) : # pylint: disable=invalid-name relationship = internal . OrganizationCourse . objects . get ( id = relationship . id , active = True ) _inactivate_record ( relationship )
4985	def get ( self , request , enterprise_uuid , program_uuid ) : verify_edx_resources ( ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) context_data = get_global_context ( request , enterprise_customer ) program_details , error_code = self . get_program_details ( request , program_uuid , enterprise_customer ) if error_code : return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , ) if program_details [ 'certificate_eligible_for_program' ] : # The user is already enrolled in the program, so redirect to the program's dashboard. return redirect ( LMS_PROGRAMS_DASHBOARD_URL . format ( uuid = program_uuid ) ) # Check to see if access to any of the course runs in the program are restricted for this user. course_run_ids = [ ] for course in program_details [ 'courses' ] : for course_run in course [ 'course_runs' ] : course_run_ids . append ( course_run [ 'key' ] ) embargo_url = EmbargoApiClient . redirect_if_blocked ( course_run_ids , request . user , get_ip ( request ) , request . path ) if embargo_url : return redirect ( embargo_url ) return self . get_enterprise_program_enrollment_page ( request , enterprise_customer , program_details )
1992	def _get_id ( self ) : id_ = self . _last_id . value self . _last_id . value += 1 return id_
10311	def prepare_c3 ( data : Union [ List [ Tuple [ str , int ] ] , Mapping [ str , int ] ] , y_axis_label : str = 'y' , x_axis_label : str = 'x' , ) -> str : if not isinstance ( data , list ) : data = sorted ( data . items ( ) , key = itemgetter ( 1 ) , reverse = True ) try : labels , values = zip ( * data ) except ValueError : log . info ( f'no values found for {x_axis_label}, {y_axis_label}' ) labels , values = [ ] , [ ] return json . dumps ( [ [ x_axis_label ] + list ( labels ) , [ y_axis_label ] + list ( values ) , ] )
4820	def refresh_token ( func ) : @ wraps ( func ) def inner ( self , * args , * * kwargs ) : """ Before calling the wrapped function, we check if the JWT token is expired, and if so, re-connect. """ if self . token_expired ( ) : self . connect ( ) return func ( self , * args , * * kwargs ) return inner
6117	def circular ( cls , shape , pixel_scale , radius_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_from_shape_pixel_scale_and_radius ( shape , pixel_scale , radius_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
12955	def _add_id_to_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . sadd ( self . _get_key_for_index ( indexedField , val ) , pk )
6948	def jhk_to_sdssg ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSG_JHK , SDSSG_JH , SDSSG_JK , SDSSG_HK , SDSSG_J , SDSSG_H , SDSSG_K )
1981	def sys_transmit ( self , cpu , fd , buf , count , tx_bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to write to a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to write to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to write a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 2 ) if issymbolic ( tx_bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 3 ) return super ( ) . sys_transmit ( cpu , fd , buf , count , tx_bytes )
10076	def commit ( self , * args , * * kwargs ) : return super ( Deposit , self ) . commit ( * args , * * kwargs )
11171	def optionhelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : def makelabels ( option ) : labels = '%*s--%s' % ( indent , ' ' , option . name ) if option . abbreviation : labels += ', -' + option . abbreviation return labels + ': ' docs = [ ] helpindent = _autoindent ( [ makelabels ( o ) for o in self . options . values ( ) ] , indent , maxindent ) for name in self . option_order : option = self . options [ name ] labels = makelabels ( option ) helpstring = "%s(%s). %s" % ( option . formatname , option . strvalue , option . docs ) wrapped = self . _wrap_labelled ( labels , helpstring , helpindent , width ) docs . extend ( wrapped ) return '\n' . join ( docs )
3571	def centralManager_didDisconnectPeripheral_error_ ( self , manager , peripheral , error ) : logger . debug ( 'centralManager_didDisconnectPeripheral called' ) # Get the device and remove it from the device list, then fire its # disconnected event. device = device_list ( ) . get ( peripheral ) if device is not None : # Fire disconnected event and remove device from device list. device . _set_disconnected ( ) device_list ( ) . remove ( peripheral )
12686	def find ( self , * args ) : curr_node = self . __root return self . __traverse ( curr_node , 0 , * args )
1736	def parse_num ( source , start , charset ) : while start < len ( source ) and source [ start ] in charset : start += 1 return start
11726	def format_seconds ( self , n_seconds ) : func = self . ok if n_seconds >= 60 : n_minutes , n_seconds = divmod ( n_seconds , 60 ) return "%s minutes %s seconds" % ( func ( "%d" % n_minutes ) , func ( "%.3f" % n_seconds ) ) else : return "%s seconds" % ( func ( "%.3f" % n_seconds ) )
7944	def _connected ( self ) : self . _auth_properties [ 'remote-ip' ] = self . _dst_addr [ 0 ] if self . _dst_service : self . _auth_properties [ 'service-domain' ] = self . _dst_name if self . _dst_hostname is not None : self . _auth_properties [ 'service-hostname' ] = self . _dst_hostname else : self . _auth_properties [ 'service-hostname' ] = self . _dst_addr [ 0 ] self . _auth_properties [ 'security-layer' ] = None self . event ( ConnectedEvent ( self . _dst_addr ) ) self . _set_state ( "connected" ) self . _stream . transport_connected ( )
8228	def show ( self , format = 'png' , as_data = False ) : from io import BytesIO b = BytesIO ( ) if format == 'png' : from IPython . display import Image surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , self . WIDTH , self . HEIGHT ) self . snapshot ( surface ) surface . write_to_png ( b ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return Image ( data ) elif format == 'svg' : from IPython . display import SVG surface = cairo . SVGSurface ( b , self . WIDTH , self . HEIGHT ) surface . finish ( ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return SVG ( data )
13865	def fromts ( ts , tzin = None , tzout = None ) : if ts is None : return None when = datetime . utcfromtimestamp ( ts ) . replace ( tzinfo = tzin or utc ) return totz ( when , tzout )
7123	def write_config ( config , app_dir , filename = 'configuration.json' ) : path = os . path . join ( app_dir , filename ) with open ( path , 'w' ) as f : json . dump ( config , f , indent = 4 , cls = DetectMissingEncoder , separators = ( ',' , ': ' ) )
3413	def _update_optional ( cobra_object , new_dict , optional_attribute_dict , ordered_keys ) : for key in ordered_keys : default = optional_attribute_dict [ key ] value = getattr ( cobra_object , key ) if value is None or value == default : continue new_dict [ key ] = _fix_type ( value )
12176	def plot_shaded_data ( X , Y , variances , varianceX ) : plt . plot ( X , Y , color = 'k' , lw = 2 ) nChunks = int ( len ( Y ) / CHUNK_POINTS ) for i in range ( 0 , 100 , PERCENT_STEP ) : varLimitLow = np . percentile ( variances , i ) varLimitHigh = np . percentile ( variances , i + PERCENT_STEP ) varianceIsAboveMin = np . where ( variances >= varLimitLow ) [ 0 ] varianceIsBelowMax = np . where ( variances <= varLimitHigh ) [ 0 ] varianceIsRange = [ chunkNumber for chunkNumber in range ( nChunks ) if chunkNumber in varianceIsAboveMin and chunkNumber in varianceIsBelowMax ] for chunkNumber in varianceIsRange : t1 = chunkNumber * CHUNK_POINTS / POINTS_PER_SEC t2 = t1 + CHUNK_POINTS / POINTS_PER_SEC plt . axvspan ( t1 , t2 , alpha = .3 , color = COLORMAP ( i / 100 ) , lw = 0 )
4234	def _xml_get ( e , name ) : r = e . find ( name ) if r is not None : return r . text return None
1003	def _inferPhase1 ( self , activeColumns , useStartCells ) : # Init to zeros to start self . infActiveState [ 't' ] . fill ( 0 ) # Phase 1 - turn on predicted cells in each column receiving bottom-up # If we are following a reset, activate only the start cell in each # column that has bottom-up numPredictedColumns = 0 if useStartCells : for c in activeColumns : self . infActiveState [ 't' ] [ c , 0 ] = 1 # else, turn on any predicted cells in each column. If there are none, then # turn on all cells (burst the column) else : for c in activeColumns : predictingCells = numpy . where ( self . infPredictedState [ 't-1' ] [ c ] == 1 ) [ 0 ] numPredictingCells = len ( predictingCells ) if numPredictingCells > 0 : self . infActiveState [ 't' ] [ c , predictingCells ] = 1 numPredictedColumns += 1 else : self . infActiveState [ 't' ] [ c , : ] = 1 # whole column bursts # Did we predict this input well enough? if useStartCells or numPredictedColumns >= 0.50 * len ( activeColumns ) : return True else : return False
10570	def get_suggested_filename ( metadata ) : if metadata . get ( 'title' ) and metadata . get ( 'track_number' ) : suggested_filename = '{track_number:0>2} {title}' . format ( * * metadata ) elif metadata . get ( 'title' ) and metadata . get ( 'trackNumber' ) : suggested_filename = '{trackNumber:0>2} {title}' . format ( * * metadata ) elif metadata . get ( 'title' ) and metadata . get ( 'tracknumber' ) : suggested_filename = '{tracknumber:0>2} {title}' . format ( * * metadata ) else : suggested_filename = '00 {}' . format ( metadata . get ( 'title' , '' ) ) return suggested_filename
4302	def create_user ( config_data ) : with chdir ( os . path . abspath ( config_data . project_directory ) ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) subprocess . check_call ( [ sys . executable , 'create_user.py' ] , env = env , stderr = subprocess . STDOUT ) for ext in [ 'py' , 'pyc' ] : try : os . remove ( 'create_user.{0}' . format ( ext ) ) except OSError : pass
767	def getMetrics ( self ) : result = { } for metricObj , label in zip ( self . __metrics , self . __metricLabels ) : value = metricObj . getMetric ( ) result [ label ] = value [ 'value' ] return result
7139	def spend_key ( self ) : key = self . _backend . spend_key ( ) if key == numbers . EMPTY_KEY : return None return key
10362	def self_edge_filter ( _ : BELGraph , source : BaseEntity , target : BaseEntity , __ : str ) -> bool : return source == target
6994	def shutdown_check_handler ( ) : url = 'http://169.254.169.254/latest/meta-data/spot/instance-action' try : resp = requests . get ( url , timeout = 1.0 ) resp . raise_for_status ( ) stopinfo = resp . json ( ) if 'action' in stopinfo and stopinfo [ 'action' ] in ( 'stop' , 'terminate' , 'hibernate' ) : stoptime = stopinfo [ 'time' ] LOGWARNING ( 'instance is going to %s at %s' % ( stopinfo [ 'action' ] , stoptime ) ) resp . close ( ) return True else : resp . close ( ) return False except HTTPError as e : resp . close ( ) return False except Exception as e : resp . close ( ) return False
4735	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.pci.env: invalid SSH environment" ) return 1 pci = cij . env_to_dict ( PREFIX , REQUIRED ) pci [ "BUS_PATH" ] = "/sys/bus/pci" pci [ "DEV_PATH" ] = os . sep . join ( [ pci [ "BUS_PATH" ] , "devices" , pci [ "DEV_NAME" ] ] ) cij . env_export ( PREFIX , EXPORTED , pci ) return 0
1071	def getaddrspec ( self ) : aslist = [ ] self . gotonext ( ) while self . pos < len ( self . field ) : if self . field [ self . pos ] == '.' : aslist . append ( '.' ) self . pos += 1 elif self . field [ self . pos ] == '"' : aslist . append ( '"%s"' % self . getquote ( ) ) elif self . field [ self . pos ] in self . atomends : break else : aslist . append ( self . getatom ( ) ) self . gotonext ( ) if self . pos >= len ( self . field ) or self . field [ self . pos ] != '@' : return '' . join ( aslist ) aslist . append ( '@' ) self . pos += 1 self . gotonext ( ) return '' . join ( aslist ) + self . getdomain ( )
1771	def visualize ( self ) : if os . path . isfile ( self . workspace ) : t = threading . Thread ( target = self . highlight_from_file , args = ( self . workspace , ) ) elif os . path . isdir ( self . workspace ) : t = threading . Thread ( target = self . highlight_from_dir , args = ( self . workspace , ) ) t . start ( )
12416	def end ( self , * args , * * kwargs ) : self . send ( * args , * * kwargs ) self . close ( )
2943	def validate ( self ) : results = [ ] from . . specs import Join def recursive_find_loop ( task , history ) : current = history [ : ] current . append ( task ) if isinstance ( task , Join ) : if task in history : msg = "Found loop with '%s': %s then '%s' again" % ( task . name , '->' . join ( [ p . name for p in history ] ) , task . name ) raise Exception ( msg ) for predecessor in task . inputs : recursive_find_loop ( predecessor , current ) for parent in task . inputs : recursive_find_loop ( parent , current ) for task_id , task in list ( self . task_specs . items ( ) ) : # Check for cyclic waits try : recursive_find_loop ( task , [ ] ) except Exception as exc : results . append ( exc . __str__ ( ) ) # Check for disconnected tasks if not task . inputs and task . name not in [ 'Start' , 'Root' ] : if task . outputs : results . append ( "Task '%s' is disconnected (no inputs)" % task . name ) else : LOG . debug ( "Task '%s' is not being used" % task . name ) return results
6107	def trace_grid_stack_to_next_plane ( self ) : def minus ( grid , deflections ) : return grid - deflections return self . grid_stack . map_function ( minus , self . deflection_stack )
12356	def wait ( self ) : interval_seconds = 5 while True : actions = self . actions ( ) slept = False for a in actions : if a [ 'status' ] == 'in-progress' : # n.b. gevent will monkey patch time . sleep ( interval_seconds ) slept = True break if not slept : break
12831	def validate_xml_text ( text ) : bad_chars = __INVALID_XML_CHARS & set ( text ) if bad_chars : for offset , c in enumerate ( text ) : if c in bad_chars : raise RuntimeError ( 'invalid XML character: ' + repr ( c ) + ' at offset ' + str ( offset ) )
5027	def get_requirements ( requirements_file ) : lines = open ( requirements_file ) . readlines ( ) dependencies = [ ] dependency_links = [ ] for line in lines : package = line . strip ( ) if package . startswith ( '#' ) : # Skip pure comment lines continue if any ( package . startswith ( prefix ) for prefix in VCS_PREFIXES ) : # VCS reference for dev purposes, expect a trailing comment # with the normal requirement package_link , __ , package = package . rpartition ( '#' ) # Remove -e <version_control> string package_link = re . sub ( r'(.*)(?P<dependency_link>https?.*$)' , r'\g<dependency_link>' , package_link ) package = re . sub ( r'(egg=)?(?P<package_name>.*)==.*$' , r'\g<package_name>' , package ) package_version = re . sub ( r'.*[^=]==' , '' , line . strip ( ) ) if package : dependency_links . append ( '{package_link}#egg={package}-{package_version}' . format ( package_link = package_link , package = package , package_version = package_version , ) ) else : # Ignore any trailing comment package , __ , __ = package . partition ( '#' ) # Remove any whitespace and assume non-empty results are dependencies package = package . strip ( ) if package : dependencies . append ( package ) return dependencies , dependency_links
12109	def _review_all ( self , launchers ) : # Run review of launch args if necessary if self . launch_args is not None : proceed = self . review_args ( self . launch_args , show_repr = True , heading = 'Meta Arguments' ) if not proceed : return False reviewers = [ self . review_args , self . review_command , self . review_launcher ] for ( count , launcher ) in enumerate ( launchers ) : # Run reviews for all launchers if desired... if not all ( reviewer ( launcher ) for reviewer in reviewers ) : print ( "\n == Aborting launch ==" ) return False # But allow the user to skip these extra reviews if len ( launchers ) != 1 and count < len ( launchers ) - 1 : skip_remaining = self . input_options ( [ 'Y' , 'n' , 'quit' ] , '\nSkip remaining reviews?' , default = 'y' ) if skip_remaining == 'y' : break elif skip_remaining == 'quit' : return False if self . input_options ( [ 'y' , 'N' ] , 'Execute?' , default = 'n' ) != 'y' : return False else : return self . _launch_all ( launchers )
8850	def setup_editor ( self , editor ) : editor . cursorPositionChanged . connect ( self . on_cursor_pos_changed ) try : m = editor . modes . get ( modes . GoToAssignmentsMode ) except KeyError : pass else : assert isinstance ( m , modes . GoToAssignmentsMode ) m . out_of_doc . connect ( self . on_goto_out_of_doc )
4189	def window_poisson ( N , alpha = 2 ) : n = linspace ( - N / 2. , ( N ) / 2. , N ) w = exp ( - alpha * abs ( n ) / ( N / 2. ) ) return w
2289	def run ( self , data , train_epochs = 1000 , test_epochs = 1000 , verbose = None , idx = 0 , lr = 0.01 , * * kwargs ) : verbose = SETTINGS . get_default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) self . score . zero_ ( ) with trange ( train_epochs + test_epochs , disable = not verbose ) as t : for epoch in t : optim . zero_grad ( ) generated_data = self . forward ( ) mmd = self . criterion ( generated_data , data ) if not epoch % 200 : t . set_postfix ( idx = idx , epoch = epoch , loss = mmd . item ( ) ) mmd . backward ( ) optim . step ( ) if epoch >= test_epochs : self . score . add_ ( mmd . data ) return self . score . cpu ( ) . numpy ( ) / test_epochs
3961	def update_managed_repos ( force = False ) : log_to_client ( 'Pulling latest updates for all active managed repos:' ) update_specs_repo_and_known_hosts ( ) repos_to_update = get_all_repos ( active_only = True , include_specs_repo = False ) with parallel_task_queue ( ) as queue : log_to_client ( 'Updating managed repos' ) for repo in repos_to_update : if not repo . is_overridden : repo . update_local_repo_async ( queue , force = force )
7655	def update ( self , * * kwargs ) : for name , value in six . iteritems ( kwargs ) : setattr ( self , name , value )
5390	def _get_task_from_task_dir ( self , job_id , user_id , task_id , task_attempt ) : # We need to be very careful about how we read and interpret the contents # of the task directory. The directory could be changing because a new # task is being created. The directory could be changing because a task # is ending. # # If the meta.yaml does not exist, the task does not yet exist. # If the meta.yaml exists, it means the task is scheduled. It does not mean # it is yet running. # If the task.pid file exists, it means that the runner.sh was started. task_dir = self . _task_directory ( job_id , task_id , task_attempt ) job_descriptor = self . _read_task_metadata ( task_dir ) if not job_descriptor : return None # If we read up an old task, the user-id will not be in the job_descriptor. if not job_descriptor . job_metadata . get ( 'user-id' ) : job_descriptor . job_metadata [ 'user-id' ] = user_id # Get the pid of the runner pid = - 1 try : with open ( os . path . join ( task_dir , 'task.pid' ) , 'r' ) as f : pid = int ( f . readline ( ) . strip ( ) ) except ( IOError , OSError ) : pass # Get the script contents script = None script_name = job_descriptor . job_metadata . get ( 'script-name' ) if script_name : script = self . _read_script ( task_dir , script_name ) # Read the files written by the runner.sh. # For new tasks, these may not have been written yet. end_time = self . _get_end_time_from_task_dir ( task_dir ) last_update = self . _get_last_update_time_from_task_dir ( task_dir ) events = self . _get_events_from_task_dir ( task_dir ) status = self . _get_status_from_task_dir ( task_dir ) log_detail = self . _get_log_detail_from_task_dir ( task_dir ) # If the status file is not yet written, then mark the task as pending if not status : status = 'RUNNING' log_detail = [ 'Pending' ] return LocalTask ( task_status = status , events = events , log_detail = log_detail , job_descriptor = job_descriptor , end_time = end_time , last_update = last_update , pid = pid , script = script )
1077	def fromordinal ( cls , n ) : y , m , d = _ord2ymd ( n ) return cls ( y , m , d )
10798	def _weight ( self , rsq , sigma = None ) : sigma = sigma or self . filter_size if not self . clip : o = np . exp ( - rsq / ( 2 * sigma ** 2 ) ) else : o = np . zeros ( rsq . shape , dtype = 'float' ) m = ( rsq < self . clipsize ** 2 ) o [ m ] = np . exp ( - rsq [ m ] / ( 2 * sigma ** 2 ) ) return o
3291	def get_href ( self ) : # Nautilus chokes, if href encodes '(' as '%28' # So we don't encode 'extra' and 'safe' characters (see rfc2068 3.2.1) safe = "/" + "!*'()," + "$-_|." return compat . quote ( self . provider . mount_path + self . provider . share_path + self . get_preferred_path ( ) , safe = safe , )
1860	def LODS ( cpu , dest , src ) : src_reg = { 8 : 'SI' , 32 : 'ESI' , 64 : 'RSI' } [ cpu . address_bit_size ] base , _ , ty = cpu . get_descriptor ( cpu . DS ) src_addr = cpu . read_register ( src_reg ) + base size = dest . size arg0 = cpu . read_int ( src_addr , size ) dest . write ( arg0 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment )
2119	def _parent_filter ( self , parent , relationship , * * kwargs ) : if parent is None or relationship is None : return { } parent_filter_kwargs = { } query_params = ( ( self . _reverse_rel_name ( relationship ) , parent ) , ) parent_filter_kwargs [ 'query' ] = query_params if kwargs . get ( 'workflow_job_template' , None ) is None : parent_data = self . read ( pk = parent ) [ 'results' ] [ 0 ] parent_filter_kwargs [ 'workflow_job_template' ] = parent_data [ 'workflow_job_template' ] return parent_filter_kwargs
11656	def fit_transform ( self , X , y = None , * * params ) : X = as_features ( X , stack = True ) X_new = self . transformer . fit_transform ( X . stacked_features , y , * * params ) return self . _gather_outputs ( X , X_new )
2112	def parse_requirements ( filename ) : reqs = [ ] version_spec_in_play = None # Iterate over each line in the requirements file. for line in open ( filename , 'r' ) . read ( ) . strip ( ) . split ( '\n' ) : # Sanity check: Is this an empty line? # If so, do nothing. if not line . strip ( ) : continue # If this is just a plain requirement (not a comment), then # add it to the requirements list. if not line . startswith ( '#' ) : reqs . append ( line ) continue # "Header" comments take the form of "=== Python {op} {version} ===", # and make the requirement only matter for those versions. # If this line is a header comment, parse it. match = re . search ( r'^# === [Pp]ython (?P<op>[<>=]{1,2}) ' r'(?P<major>[\d])\.(?P<minor>[\d]+) ===[\s]*$' , line ) if match : version_spec_in_play = match . groupdict ( ) for key in ( 'major' , 'minor' ) : version_spec_in_play [ key ] = int ( version_spec_in_play [ key ] ) continue # If this is a comment that otherwise looks like a package, then it # should be a package applying only to the current version spec. # # We can identify something that looks like a package by a lack # of any spaces. if ' ' not in line [ 1 : ] . strip ( ) and version_spec_in_play : package = line [ 1 : ] . strip ( ) # Sanity check: Is our version of Python one of the ones currently # in play? op = version_spec_in_play [ 'op' ] vspec = ( version_spec_in_play [ 'major' ] , version_spec_in_play [ 'minor' ] ) if '=' in op and sys . version_info [ 0 : 2 ] == vspec : reqs . append ( package ) elif '>' in op and sys . version_info [ 0 : 2 ] > vspec : reqs . append ( package ) elif '<' in op and sys . version_info [ 0 : 2 ] < vspec : reqs . append ( package ) # Okay, we should have an entire list of requirements now. return reqs
8659	def filter_by ( zips = _zips , * * kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k , v in kwargs . items ( ) ] ) ]
8029	def sizeClassifier ( path , min_size = DEFAULTS [ 'min_size' ] ) : filestat = _stat ( path ) if stat . S_ISLNK ( filestat . st_mode ) : return # Skip symlinks. if filestat . st_size < min_size : return # Skip files below the size limit return filestat . st_size
8105	def callback ( self , * incoming ) : message = incoming [ 0 ] if message : address , command = message [ 0 ] , message [ 2 ] profile = self . get_profile ( address ) if profile is not None : try : getattr ( profile , command ) ( self , message ) except AttributeError : pass
10266	def collapse_consistent_edges ( graph : BELGraph ) : for u , v in graph . edges ( ) : relation = pair_is_consistent ( graph , u , v ) if not relation : continue edges = [ ( u , v , k ) for k in graph [ u ] [ v ] ] graph . remove_edges_from ( edges ) graph . add_edge ( u , v , attr_dict = { RELATION : relation } )
7376	async def prepare_request ( self , method , url , headers = None , skip_params = False , proxy = None , * * kwargs ) : if method . lower ( ) == "post" : key = 'data' else : key = 'params' if key in kwargs and not skip_params : request_params = { key : kwargs . pop ( key ) } else : request_params = { } request_params . update ( dict ( method = method . upper ( ) , url = url ) ) coro = self . sign ( * * request_params , skip_params = skip_params , headers = headers ) request_params [ 'headers' ] = await utils . execute ( coro ) request_params [ 'proxy' ] = proxy kwargs . update ( request_params ) return kwargs
9776	def outputs ( ctx ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : PolyaxonClient ( ) . job . download_outputs ( user , project_name , _job ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not download outputs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( 'Files downloaded.' )
500	def _deleteRangeFromKNN ( self , start = 0 , end = None ) : prototype_idx = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) if end is None : end = prototype_idx . max ( ) + 1 idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , prototype_idx < end ) idsToDelete = prototype_idx [ idsIdxToDelete ] nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete . tolist ( ) ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
6780	def manifest_filename ( self ) : r = self . local_renderer tp_fn = r . format ( r . env . data_dir + '/manifest.yaml' ) return tp_fn
2991	def symbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( symbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
10214	def summarize_subgraph_node_overlap ( graph : BELGraph , node_predicates = None , annotation : str = 'Subgraph' ) : r1 = group_nodes_by_annotation_filtered ( graph , node_predicates = node_predicates , annotation = annotation ) return calculate_tanimoto_set_distances ( r1 )
11324	def fix_name_capitalization ( lastname , givennames ) : lastnames = lastname . split ( ) if len ( lastnames ) == 1 : if '-' in lastname : names = lastname . split ( '-' ) names = map ( lambda a : a [ 0 ] + a [ 1 : ] . lower ( ) , names ) lastname = '-' . join ( names ) else : lastname = lastname [ 0 ] + lastname [ 1 : ] . lower ( ) else : names = [ ] for name in lastnames : if re . search ( r'[A-Z]\.' , name ) : names . append ( name ) else : names . append ( name [ 0 ] + name [ 1 : ] . lower ( ) ) lastname = ' ' . join ( names ) lastname = collapse_initials ( lastname ) names = [ ] for name in givennames : if re . search ( r'[A-Z]\.' , name ) : names . append ( name ) else : names . append ( name [ 0 ] + name [ 1 : ] . lower ( ) ) givennames = ' ' . join ( names ) return lastname , givennames
2013	def _push ( self , value ) : assert isinstance ( value , int ) or isinstance ( value , BitVec ) and value . size == 256 if len ( self . stack ) >= 1024 : raise StackOverflow ( ) if isinstance ( value , int ) : value = value & TT256M1 value = simplify ( value ) if isinstance ( value , Constant ) and not value . taint : value = value . value self . stack . append ( value )
10941	def update_function ( self , param_vals ) : self . model = self . func ( param_vals , * self . func_args , * * self . func_kwargs ) d = self . calc_residuals ( ) return np . dot ( d . flat , d . flat )
11287	def execute ( self , arg_str = '' , * * kwargs ) : cmd = "{} {} {}" . format ( self . cmd_prefix , self . script , arg_str ) expected_ret_code = kwargs . pop ( 'code' , 0 ) # any kwargs with all capital letters should be considered environment # variables environ = self . environ for k in list ( kwargs . keys ( ) ) : if k . isupper ( ) : environ [ k ] = kwargs . pop ( k ) # we will allow overriding of these values kwargs . setdefault ( "stderr" , subprocess . STDOUT ) # we will not allow these to be overridden via kwargs kwargs [ "shell" ] = True kwargs [ "stdout" ] = subprocess . PIPE kwargs [ "cwd" ] = self . cwd kwargs [ "env" ] = environ process = None self . buf = deque ( maxlen = self . bufsize ) try : process = subprocess . Popen ( cmd , * * kwargs ) # another round of links # http://stackoverflow.com/a/17413045/5006 (what I used) # http://stackoverflow.com/questions/2715847/ for line in iter ( process . stdout . readline , b"" ) : line = line . decode ( self . encoding ) self . buf . append ( line . rstrip ( ) ) yield line process . wait ( ) if process . returncode != expected_ret_code : if process . returncode > 0 : raise RuntimeError ( "{} returned {} with output: {}" . format ( cmd , process . returncode , self . output ) ) except subprocess . CalledProcessError as e : if e . returncode != expected_ret_code : raise RuntimeError ( "{} returned {} with output: {}" . format ( cmd , e . returncode , self . output ) ) finally : if process : process . stdout . close ( )
10603	def calculate ( self , * * state ) : T = state [ 'T' ] y_C = state [ 'y_C' ] y_H = state [ 'y_H' ] y_O = state [ 'y_O' ] y_N = state [ 'y_N' ] y_S = state [ 'y_S' ] a = self . _calc_a ( y_C , y_H , y_O , y_N , y_S ) / 1000 # kg/mol result = ( R / a ) * ( 380 * self . _calc_g0 ( 380 / T ) + 3600 * self . _calc_g0 ( 1800 / T ) ) return result
11129	def stats ( cls , traces ) : data = { } stats = { } # Group traces by key and minute for trace in traces : key = trace [ 'key' ] if key not in data : data [ key ] = [ ] stats [ key ] = { } data [ key ] . append ( trace [ 'total_time' ] ) cls . _traces . pop ( trace [ 'id' ] ) for key in data : times = data [ key ] stats [ key ] = dict ( count = len ( times ) , max = max ( times ) , min = min ( times ) , avg = sum ( times ) / len ( times ) ) return stats
7856	def __response ( self , stanza ) : try : d = self . disco_class ( stanza . get_query ( ) ) self . got_it ( d ) except ValueError , e : self . error ( e )
2090	def _disassoc ( self , url_fragment , me , other ) : # Get the endpoint for foreign records within this object. url = self . endpoint + '%d/%s/' % ( me , url_fragment ) # Attempt to determine whether the other record already is absent, for the "changed" moniker. r = client . get ( url , params = { 'id' : other } ) . json ( ) if r [ 'count' ] == 0 : return { 'changed' : False } # Send a request removing the foreign record from this one. r = client . post ( url , data = { 'disassociate' : True , 'id' : other } ) return { 'changed' : True }
8413	def to_numeric ( self , td ) : if self . package == 'pandas' : return td . value / NANOSECONDS [ self . units ] else : return td . total_seconds ( ) / SECONDS [ self . units ]
2636	def update_parent ( self , fut ) : self . parent = fut try : fut . add_done_callback ( self . parent_callback ) except Exception as e : logger . error ( "add_done_callback got an exception {} which will be ignored" . format ( e ) )
1655	def IsDerivedFunction ( clean_lines , linenum ) : # Scan back a few lines for start of current function for i in xrange ( linenum , max ( - 1 , linenum - 10 ) , - 1 ) : match = Match ( r'^([^()]*\w+)\(' , clean_lines . elided [ i ] ) if match : # Look for "override" after the matching closing parenthesis line , _ , closing_paren = CloseExpression ( clean_lines , i , len ( match . group ( 1 ) ) ) return ( closing_paren >= 0 and Search ( r'\boverride\b' , line [ closing_paren : ] ) ) return False
13319	def deactivate ( ) : if 'CPENV_ACTIVE' not in os . environ or 'CPENV_CLEAN_ENV' not in os . environ : raise EnvironmentError ( 'Can not deactivate environment...' ) utils . restore_env_from_file ( os . environ [ 'CPENV_CLEAN_ENV' ] )
7074	def variable_index_gridsearch_magbin ( simbasedir , stetson_stdev_range = ( 1.0 , 20.0 ) , inveta_stdev_range = ( 1.0 , 20.0 ) , iqr_stdev_range = ( 1.0 , 20.0 ) , ngridpoints = 32 , ngridworkers = None ) : # make the output directory where all the pkls from the variability # threshold runs will go outdir = os . path . join ( simbasedir , 'recvar-threshold-pkls' ) if not os . path . exists ( outdir ) : os . mkdir ( outdir ) # get the info from the simbasedir with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) # get the column defs for the fakelcs timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] # get the magbinmedians to use for the recovery processing magbinmedians = siminfo [ 'magrms' ] [ magcols [ 0 ] ] [ 'binned_sdssr_median' ] # generate the grids for stetson and inveta stetson_grid = np . linspace ( stetson_stdev_range [ 0 ] , stetson_stdev_range [ 1 ] , num = ngridpoints ) inveta_grid = np . linspace ( inveta_stdev_range [ 0 ] , inveta_stdev_range [ 1 ] , num = ngridpoints ) iqr_grid = np . linspace ( iqr_stdev_range [ 0 ] , iqr_stdev_range [ 1 ] , num = ngridpoints ) # generate the grid stet_inveta_iqr_grid = [ ] for stet in stetson_grid : for inveta in inveta_grid : for iqr in iqr_grid : grid_point = [ stet , inveta , iqr ] stet_inveta_iqr_grid . append ( grid_point ) # the output dict grid_results = { 'stetson_grid' : stetson_grid , 'inveta_grid' : inveta_grid , 'iqr_grid' : iqr_grid , 'stet_inveta_iqr_grid' : stet_inveta_iqr_grid , 'magbinmedians' : magbinmedians , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'simbasedir' : os . path . abspath ( simbasedir ) , 'recovery' : [ ] } # set up the pool pool = mp . Pool ( ngridworkers ) # run the grid search per magbinmedian for magbinmedian in magbinmedians : LOGINFO ( 'running stetson J-inveta grid-search ' 'for magbinmedian = %.3f...' % magbinmedian ) tasks = [ ( simbasedir , gp , magbinmedian ) for gp in stet_inveta_iqr_grid ] thisbin_results = pool . map ( magbin_varind_gridsearch_worker , tasks ) grid_results [ 'recovery' ] . append ( thisbin_results ) pool . close ( ) pool . join ( ) LOGINFO ( 'done.' ) with open ( os . path . join ( simbasedir , 'fakevar-recovery-per-magbin.pkl' ) , 'wb' ) as outfd : pickle . dump ( grid_results , outfd , pickle . HIGHEST_PROTOCOL ) return grid_results
5018	def add_missing_price_information_message ( request , item ) : messages . warning ( request , _ ( '{strong_start}We could not gather price information for {em_start}{item}{em_end}.{strong_end} ' '{span_start}If you continue to have these issues, please contact ' '{link_start}{platform_name} support{link_end}.{span_end}' ) . format ( item = item , em_start = '<em>' , em_end = '</em>' , link_start = '<a href="{support_link}" target="_blank">' . format ( support_link = get_configuration_value ( 'ENTERPRISE_SUPPORT_URL' , settings . ENTERPRISE_SUPPORT_URL ) , ) , platform_name = get_configuration_value ( 'PLATFORM_NAME' , settings . PLATFORM_NAME ) , link_end = '</a>' , span_start = '<span>' , span_end = '</span>' , strong_start = '<strong>' , strong_end = '</strong>' , ) )
9411	def _encode ( data , convert_to_float ) : ctf = convert_to_float # Handle variable pointer. if isinstance ( data , ( OctaveVariablePtr ) ) : return _encode ( data . value , ctf ) # Handle a user defined object. if isinstance ( data , OctaveUserClass ) : return _encode ( OctaveUserClass . to_value ( data ) , ctf ) # Handle a function pointer. if isinstance ( data , ( OctaveFunctionPtr , MatlabFunction ) ) : raise Oct2PyError ( 'Cannot write Octave functions' ) # Handle matlab objects. if isinstance ( data , MatlabObject ) : view = data . view ( np . ndarray ) out = MatlabObject ( data , data . classname ) for name in out . dtype . names : out [ name ] = _encode ( view [ name ] , ctf ) return out # Handle pandas series and dataframes if isinstance ( data , ( DataFrame , Series ) ) : return _encode ( data . values , ctf ) # Extract and encode values from dict-like objects. if isinstance ( data , dict ) : out = dict ( ) for ( key , value ) in data . items ( ) : out [ key ] = _encode ( value , ctf ) return out # Send None as nan. if data is None : return np . NaN # Sets are treated like lists. if isinstance ( data , set ) : return _encode ( list ( data ) , ctf ) # Lists can be interpreted as numeric arrays or cell arrays. if isinstance ( data , list ) : if _is_simple_numeric ( data ) : return _encode ( np . array ( data ) , ctf ) return _encode ( tuple ( data ) , ctf ) # Tuples are handled as cells. if isinstance ( data , tuple ) : obj = np . empty ( len ( data ) , dtype = object ) for ( i , item ) in enumerate ( data ) : obj [ i ] = _encode ( item , ctf ) return obj # Sparse data must be floating type. if isinstance ( data , spmatrix ) : return data . astype ( np . float64 ) # Return other data types unchanged. if not isinstance ( data , np . ndarray ) : return data # Extract and encode data from object-like arrays. if data . dtype . kind in 'OV' : out = np . empty ( data . size , dtype = data . dtype ) for ( i , item ) in enumerate ( data . ravel ( ) ) : if data . dtype . names : for name in data . dtype . names : out [ i ] [ name ] = _encode ( item [ name ] , ctf ) else : out [ i ] = _encode ( item , ctf ) return out . reshape ( data . shape ) # Complex 128 is the highest supported by savemat. if data . dtype . name == 'complex256' : return data . astype ( np . complex128 ) # Convert to float if applicable. if ctf and data . dtype . kind in 'ui' : return data . astype ( np . float64 ) # Return standard array. return data
10155	def convert ( self , schema_node , definition_handler ) : converted = { 'name' : schema_node . name , 'in' : self . _in , 'required' : schema_node . required } if schema_node . description : converted [ 'description' ] = schema_node . description if schema_node . default : converted [ 'default' ] = schema_node . default schema = definition_handler ( schema_node ) # Parameters shouldn't have a title schema . pop ( 'title' , None ) converted . update ( schema ) if schema . get ( 'type' ) == 'array' : converted [ 'items' ] = { 'type' : schema [ 'items' ] [ 'type' ] } return converted
5781	def _create_buffers ( self , number ) : buffers = new ( secur32 , 'SecBuffer[%d]' % number ) for index in range ( 0 , number ) : buffers [ index ] . cbBuffer = 0 buffers [ index ] . BufferType = Secur32Const . SECBUFFER_EMPTY buffers [ index ] . pvBuffer = null ( ) sec_buffer_desc_pointer = struct ( secur32 , 'SecBufferDesc' ) sec_buffer_desc = unwrap ( sec_buffer_desc_pointer ) sec_buffer_desc . ulVersion = Secur32Const . SECBUFFER_VERSION sec_buffer_desc . cBuffers = number sec_buffer_desc . pBuffers = buffers return ( sec_buffer_desc_pointer , buffers )
5576	def load_output_writer ( output_params , readonly = False ) : if not isinstance ( output_params , dict ) : raise TypeError ( "output_params must be a dictionary" ) driver_name = output_params [ "format" ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : _driver = v . load ( ) if all ( [ hasattr ( _driver , attr ) for attr in [ "OutputData" , "METADATA" ] ] ) and ( _driver . METADATA [ "driver_name" ] == driver_name ) : return _driver . OutputData ( output_params , readonly = readonly ) raise MapcheteDriverError ( "no loader for driver '%s' could be found." % driver_name )
5590	def tile ( self , zoom , row , col ) : tile = self . tile_pyramid . tile ( zoom , row , col ) return BufferedTile ( tile , pixelbuffer = self . pixelbuffer )
13409	def show ( self ) : self . parent . addLayout ( self . _logSelectLayout ) self . menuCount += 1 self . _connectSlots ( )
8709	def write_lines ( self , data ) : lines = data . replace ( '\r' , '' ) . split ( '\n' ) for line in lines : self . __exchange ( line )
4085	def get_common_prefix ( z ) : name_list = z . namelist ( ) if name_list and all ( n . startswith ( name_list [ 0 ] ) for n in name_list [ 1 : ] ) : return name_list [ 0 ] return None
2062	def declarations ( self ) : declarations = GetDeclarations ( ) for a in self . constraints : try : declarations . visit ( a ) except RuntimeError : # TODO: (defunct) move recursion management out of PickleSerializer if sys . getrecursionlimit ( ) >= PickleSerializer . MAX_RECURSION : raise Exception ( f'declarations recursion limit surpassed {PickleSerializer.MAX_RECURSION}, aborting' ) new_limit = sys . getrecursionlimit ( ) + PickleSerializer . DEFAULT_RECURSION if new_limit <= PickleSerializer . DEFAULT_RECURSION : sys . setrecursionlimit ( new_limit ) return self . declarations return declarations . result
5254	def assemble_all ( asmcode , pc = 0 , fork = DEFAULT_FORK ) : asmcode = asmcode . split ( '\n' ) asmcode = iter ( asmcode ) for line in asmcode : if not line . strip ( ) : continue instr = assemble_one ( line , pc = pc , fork = fork ) yield instr pc += instr . size
12652	def generate_config ( output_directory ) : if not op . isdir ( output_directory ) : os . makedirs ( output_directory ) config_file = op . join ( output_directory , "config.ini" ) open_file = open ( config_file , "w" ) open_file . write ( "[BOOL]\nManualNIfTIConv=0\n" ) open_file . close ( ) return config_file
11390	def register_field ( cls , field ) : FieldRegistry . add_field ( cls , field ) signals . post_save . connect ( handle_save_embeds , sender = cls , dispatch_uid = '%s.%s.%s' % ( cls . _meta . app_label , cls . _meta . module_name , field . name ) )
458	def alphas ( shape , alpha_value , name = None ) : with ops . name_scope ( name , "alphas" , [ shape ] ) as name : alpha_tensor = convert_to_tensor ( alpha_value ) alpha_dtype = dtypes . as_dtype ( alpha_tensor . dtype ) . base_dtype if not isinstance ( shape , ops . Tensor ) : try : shape = constant_op . _tensor_shape_tensor_conversion_function ( tensor_shape . TensorShape ( shape ) ) except ( TypeError , ValueError ) : shape = ops . convert_to_tensor ( shape , dtype = dtypes . int32 ) if not shape . _shape_tuple ( ) : shape = reshape ( shape , [ - 1 ] ) # Ensure it's a vector try : output = constant ( alpha_value , shape = shape , dtype = alpha_dtype , name = name ) except ( TypeError , ValueError ) : output = fill ( shape , constant ( alpha_value , dtype = alpha_dtype ) , name = name ) if output . dtype . base_dtype != alpha_dtype : raise AssertionError ( "Dtypes do not corresponds: %s and %s" % ( output . dtype . base_dtype , alpha_dtype ) ) return output
8911	def ows_security_tween_factory ( handler , registry ) : security = owssecurity_factory ( registry ) def ows_security_tween ( request ) : try : security . check_request ( request ) return handler ( request ) except OWSException as err : logger . exception ( "security check failed." ) return err except Exception as err : logger . exception ( "unknown error" ) return OWSNoApplicableCode ( "{}" . format ( err ) ) return ows_security_tween
473	def build_words_dataset ( words = None , vocabulary_size = 50000 , printable = True , unk_key = 'UNK' ) : if words is None : raise Exception ( "words : list of str or byte" ) count = [ [ unk_key , - 1 ] ] count . extend ( collections . Counter ( words ) . most_common ( vocabulary_size - 1 ) ) dictionary = dict ( ) for word , _ in count : dictionary [ word ] = len ( dictionary ) data = list ( ) unk_count = 0 for word in words : if word in dictionary : index = dictionary [ word ] else : index = 0 # dictionary['UNK'] unk_count += 1 data . append ( index ) count [ 0 ] [ 1 ] = unk_count reverse_dictionary = dict ( zip ( dictionary . values ( ) , dictionary . keys ( ) ) ) if printable : tl . logging . info ( 'Real vocabulary size %d' % len ( collections . Counter ( words ) . keys ( ) ) ) tl . logging . info ( 'Limited vocabulary size {}' . format ( vocabulary_size ) ) if len ( collections . Counter ( words ) . keys ( ) ) < vocabulary_size : raise Exception ( "len(collections.Counter(words).keys()) >= vocabulary_size , the limited vocabulary_size must be less than or equal to the read vocabulary_size" ) return data , count , dictionary , reverse_dictionary
13360	def load ( self ) : if not os . path . exists ( self . path ) : return with open ( self . path , 'r' ) as f : env_data = yaml . load ( f . read ( ) ) if env_data : for env in env_data : self . add ( VirtualEnvironment ( env [ 'root' ] ) )
13531	def ancestors ( self ) : ancestors = set ( [ ] ) self . _depth_ascend ( self , ancestors ) try : ancestors . remove ( self ) except KeyError : # we weren't ancestor of ourself, that's ok pass return list ( ancestors )
4271	def get_exif_tags ( data , datetime_format = '%c' ) : logger = logging . getLogger ( __name__ ) simple = { } for tag in ( 'Model' , 'Make' , 'LensModel' ) : if tag in data : if isinstance ( data [ tag ] , tuple ) : simple [ tag ] = data [ tag ] [ 0 ] . strip ( ) else : simple [ tag ] = data [ tag ] . strip ( ) if 'FNumber' in data : fnumber = data [ 'FNumber' ] try : simple [ 'fstop' ] = float ( fnumber [ 0 ] ) / fnumber [ 1 ] except Exception : logger . debug ( 'Skipped invalid FNumber: %r' , fnumber , exc_info = True ) if 'FocalLength' in data : focal = data [ 'FocalLength' ] try : simple [ 'focal' ] = round ( float ( focal [ 0 ] ) / focal [ 1 ] ) except Exception : logger . debug ( 'Skipped invalid FocalLength: %r' , focal , exc_info = True ) if 'ExposureTime' in data : exptime = data [ 'ExposureTime' ] if isinstance ( exptime , tuple ) : try : simple [ 'exposure' ] = str ( fractions . Fraction ( exptime [ 0 ] , exptime [ 1 ] ) ) except ZeroDivisionError : logger . info ( 'Invalid ExposureTime: %r' , exptime ) elif isinstance ( exptime , int ) : simple [ 'exposure' ] = str ( exptime ) else : logger . info ( 'Unknown format for ExposureTime: %r' , exptime ) if data . get ( 'ISOSpeedRatings' ) : simple [ 'iso' ] = data [ 'ISOSpeedRatings' ] if 'DateTimeOriginal' in data : # Remove null bytes at the end if necessary date = data [ 'DateTimeOriginal' ] . rsplit ( '\x00' ) [ 0 ] try : simple [ 'dateobj' ] = datetime . strptime ( date , '%Y:%m:%d %H:%M:%S' ) simple [ 'datetime' ] = simple [ 'dateobj' ] . strftime ( datetime_format ) except ( ValueError , TypeError ) as e : logger . info ( 'Could not parse DateTimeOriginal: %s' , e ) if 'GPSInfo' in data : info = data [ 'GPSInfo' ] lat_info = info . get ( 'GPSLatitude' ) lon_info = info . get ( 'GPSLongitude' ) lat_ref_info = info . get ( 'GPSLatitudeRef' ) lon_ref_info = info . get ( 'GPSLongitudeRef' ) if lat_info and lon_info and lat_ref_info and lon_ref_info : try : lat = dms_to_degrees ( lat_info ) lon = dms_to_degrees ( lon_info ) except ( ZeroDivisionError , ValueError , TypeError ) : logger . info ( 'Failed to read GPS info' ) else : simple [ 'gps' ] = { 'lat' : - lat if lat_ref_info != 'N' else lat , 'lon' : - lon if lon_ref_info != 'E' else lon , } return simple
5377	def _build_pipeline_input_file_param ( cls , var_name , docker_path ) : # If the filename contains a wildcard, then the target Docker path must # be a directory in order to ensure consistency whether the source pattern # contains 1 or multiple files. # # In that case, we set the docker_path to explicitly have a trailing slash # (for the Pipelines API "gsutil cp" handling, and then override the # associated var_name environment variable in the generated Docker command. path , filename = os . path . split ( docker_path ) if '*' in filename : return cls . _build_pipeline_file_param ( var_name , path + '/' ) else : return cls . _build_pipeline_file_param ( var_name , docker_path )
70	def remove_out_of_image ( self , fully = True , partly = False ) : bbs_clean = [ bb for bb in self . bounding_boxes if not bb . is_out_of_image ( self . shape , fully = fully , partly = partly ) ] return BoundingBoxesOnImage ( bbs_clean , shape = self . shape )
13417	def syncdb ( args ) : cmd = args and 'syncdb %s' % ' ' . join ( options . args ) or 'syncdb --noinput' call_manage ( cmd ) for fixture in options . paved . django . syncdb . fixtures : call_manage ( "loaddata %s" % fixture )
13495	def new ( self , mode ) : dw = DigitWord ( wordtype = mode . digit_type ) dw . random ( mode . digits ) self . _key = str ( uuid . uuid4 ( ) ) self . _status = "" self . _ttl = 3600 self . _answer = dw self . _mode = mode self . _guesses_remaining = mode . guesses_allowed self . _guesses_made = 0
10120	def rectangle ( cls , vertices , * * kwargs ) : bottom_left , top_right = vertices top_left = [ bottom_left [ 0 ] , top_right [ 1 ] ] bottom_right = [ top_right [ 0 ] , bottom_left [ 1 ] ] return cls ( [ bottom_left , bottom_right , top_right , top_left ] , * * kwargs )
11765	def k_in_row ( self , board , move , player , ( delta_x , delta_y ) ) : x , y = move n = 0 # n is number of moves in row while board . get ( ( x , y ) ) == player : n += 1 x , y = x + delta_x , y + delta_y x , y = move while board . get ( ( x , y ) ) == player : n += 1 x , y = x - delta_x , y - delta_y n -= 1 # Because we counted move itself twice return n >= self . k
2615	def cancel ( self , job_ids ) : for job in job_ids : logger . debug ( "Terminating job/proc_id: {0}" . format ( job ) ) # Here we are assuming that for local, the job_ids are the process id's if self . resources [ job ] [ 'proc' ] : proc = self . resources [ job ] [ 'proc' ] os . killpg ( os . getpgid ( proc . pid ) , signal . SIGTERM ) self . resources [ job ] [ 'status' ] = 'CANCELLED' elif self . resources [ job ] [ 'remote_pid' ] : cmd = "kill -- -$(ps -o pgid={} | grep -o '[0-9]*')" . format ( self . resources [ job ] [ 'remote_pid' ] ) retcode , stdout , stderr = self . channel . execute_wait ( cmd , self . cmd_timeout ) if retcode != 0 : logger . warning ( "Failed to kill PID: {} and child processes on {}" . format ( self . resources [ job ] [ 'remote_pid' ] , self . label ) ) rets = [ True for i in job_ids ] return rets
6005	def generate_poisson_noise ( image , exposure_time_map , seed = - 1 ) : setup_random_seed ( seed ) image_counts = np . multiply ( image , exposure_time_map ) return image - np . divide ( np . random . poisson ( image_counts , image . shape ) , exposure_time_map )
10587	def get_account_descendants ( self , account ) : result = [ ] for child in account . accounts : self . _get_account_and_descendants_ ( child , result ) return result
1487	def _load_class ( cls , d ) : for k , v in d . items ( ) : if isinstance ( k , tuple ) : typ , k = k if typ == 'property' : v = property ( * v ) elif typ == 'staticmethod' : v = staticmethod ( v ) # pylint: disable=redefined-variable-type elif typ == 'classmethod' : v = classmethod ( v ) setattr ( cls , k , v ) return cls
9975	def _alter_code ( code , * * attrs ) : PyCode_New = ctypes . pythonapi . PyCode_New PyCode_New . argtypes = ( ctypes . c_int , ctypes . c_int , ctypes . c_int , ctypes . c_int , ctypes . c_int , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . c_int , ctypes . py_object ) PyCode_New . restype = ctypes . py_object args = [ [ code . co_argcount , 'co_argcount' ] , [ code . co_kwonlyargcount , 'co_kwonlyargcount' ] , [ code . co_nlocals , 'co_nlocals' ] , [ code . co_stacksize , 'co_stacksize' ] , [ code . co_flags , 'co_flags' ] , [ code . co_code , 'co_code' ] , [ code . co_consts , 'co_consts' ] , [ code . co_names , 'co_names' ] , [ code . co_varnames , 'co_varnames' ] , [ code . co_freevars , 'co_freevars' ] , [ code . co_cellvars , 'co_cellvars' ] , [ code . co_filename , 'co_filename' ] , [ code . co_name , 'co_name' ] , [ code . co_firstlineno , 'co_firstlineno' ] , [ code . co_lnotab , 'co_lnotab' ] ] for arg in args : if arg [ 1 ] in attrs : arg [ 0 ] = attrs [ arg [ 1 ] ] return PyCode_New ( args [ 0 ] [ 0 ] , # code.co_argcount, args [ 1 ] [ 0 ] , # code.co_kwonlyargcount, args [ 2 ] [ 0 ] , # code.co_nlocals, args [ 3 ] [ 0 ] , # code.co_stacksize, args [ 4 ] [ 0 ] , # code.co_flags, args [ 5 ] [ 0 ] , # code.co_code, args [ 6 ] [ 0 ] , # code.co_consts, args [ 7 ] [ 0 ] , # code.co_names, args [ 8 ] [ 0 ] , # code.co_varnames, args [ 9 ] [ 0 ] , # code.co_freevars, args [ 10 ] [ 0 ] , # code.co_cellvars, args [ 11 ] [ 0 ] , # code.co_filename, args [ 12 ] [ 0 ] , # code.co_name, args [ 13 ] [ 0 ] , # code.co_firstlineno, args [ 14 ] [ 0 ] )
802	def modelsGetParams ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( "Wrong modelIDs type: %r" ) % ( type ( modelIDs ) , ) assert len ( modelIDs ) >= 1 , "modelIDs is empty" rows = self . _getMatchingRowsWithRetries ( self . _models , { 'model_id' : modelIDs } , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . getParamsNamedTuple . _fields ] ) # NOTE: assertion will also fail when modelIDs contains duplicates assert len ( rows ) == len ( modelIDs ) , "Didn't find modelIDs: %r" % ( ( set ( modelIDs ) - set ( r [ 0 ] for r in rows ) ) , ) # Return the params and params hashes as a namedtuple return [ self . _models . getParamsNamedTuple . _make ( r ) for r in rows ]
7160	def next_question ( self ) : for key , questions in self . questions . items ( ) : if key in self . answers : continue for question in questions : if self . check_condition ( question . _condition ) : return question return None
12616	def is_img ( obj ) : try : get_data = getattr ( obj , 'get_data' ) get_affine = getattr ( obj , 'get_affine' ) return isinstance ( get_data , collections . Callable ) and isinstance ( get_affine , collections . Callable ) except AttributeError : return False
9833	def initialize ( self ) : return self . DXclasses [ self . type ] ( self . id , * * self . args )
3510	def add_room ( model , solution = None , linear = False , delta = 0.03 , epsilon = 1E-03 ) : if 'room_old_objective' in model . solver . variables : raise ValueError ( 'model is already adjusted for ROOM' ) # optimizes if no reference solution is provided if solution is None : solution = pfba ( model ) prob = model . problem variable = prob . Variable ( "room_old_objective" , ub = solution . objective_value ) constraint = prob . Constraint ( model . solver . objective . expression - variable , ub = 0.0 , lb = 0.0 , name = "room_old_objective_constraint" ) model . objective = prob . Objective ( Zero , direction = "min" , sloppy = True ) vars_and_cons = [ variable , constraint ] obj_vars = [ ] for rxn in model . reactions : flux = solution . fluxes [ rxn . id ] if linear : y = prob . Variable ( "y_" + rxn . id , lb = 0 , ub = 1 ) delta = epsilon = 0.0 else : y = prob . Variable ( "y_" + rxn . id , type = "binary" ) # upper constraint w_u = flux + ( delta * abs ( flux ) ) + epsilon upper_const = prob . Constraint ( rxn . flux_expression - y * ( rxn . upper_bound - w_u ) , ub = w_u , name = "room_constraint_upper_" + rxn . id ) # lower constraint w_l = flux - ( delta * abs ( flux ) ) - epsilon lower_const = prob . Constraint ( rxn . flux_expression - y * ( rxn . lower_bound - w_l ) , lb = w_l , name = "room_constraint_lower_" + rxn . id ) vars_and_cons . extend ( [ y , upper_const , lower_const ] ) obj_vars . append ( y ) model . add_cons_vars ( vars_and_cons ) model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } )
1626	def ReverseCloseExpression ( clean_lines , linenum , pos ) : line = clean_lines . elided [ linenum ] if line [ pos ] not in ')}]>' : return ( line , 0 , - 1 ) # Check last line ( start_pos , stack ) = FindStartOfExpressionInLine ( line , pos , [ ] ) if start_pos > - 1 : return ( line , linenum , start_pos ) # Continue scanning backward while stack and linenum > 0 : linenum -= 1 line = clean_lines . elided [ linenum ] ( start_pos , stack ) = FindStartOfExpressionInLine ( line , len ( line ) - 1 , stack ) if start_pos > - 1 : return ( line , linenum , start_pos ) # Did not find start of expression before beginning of file, give up return ( line , 0 , - 1 )
7278	def play_sync ( self ) : self . play ( ) logger . info ( "Playing synchronously" ) try : time . sleep ( 0.05 ) logger . debug ( "Wait for playing to start" ) while self . is_playing ( ) : time . sleep ( 0.05 ) except DBusException : logger . error ( "Cannot play synchronously any longer as DBus calls timed out." )
7192	def histogram_equalize ( self , use_bands , * * kwargs ) : data = self . _read ( self [ use_bands , ... ] , * * kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) flattened = data . flatten ( ) if 0 in data : masked = np . ma . masked_values ( data , 0 ) . compressed ( ) image_histogram , bin_edges = np . histogram ( masked , 256 ) else : image_histogram , bin_edges = np . histogram ( flattened , 256 ) bins = ( bin_edges [ : - 1 ] + bin_edges [ 1 : ] ) / 2.0 cdf = image_histogram . cumsum ( ) cdf = cdf / float ( cdf [ - 1 ] ) image_equalized = np . interp ( flattened , bins , cdf ) . reshape ( data . shape ) if 'stretch' in kwargs or 'gamma' in kwargs : return self . _histogram_stretch ( image_equalized , * * kwargs ) else : return image_equalized
5025	def get_channel_classes ( channel_code ) : if channel_code : # Channel code is case-insensitive channel_code = channel_code . upper ( ) if channel_code not in INTEGRATED_CHANNEL_CHOICES : raise CommandError ( _ ( 'Invalid integrated channel: {channel}' ) . format ( channel = channel_code ) ) channel_classes = [ INTEGRATED_CHANNEL_CHOICES [ channel_code ] ] else : channel_classes = INTEGRATED_CHANNEL_CHOICES . values ( ) return channel_classes
12469	def smart_str ( value , encoding = 'utf-8' , errors = 'strict' ) : if not IS_PY3 and isinstance ( value , unicode ) : # noqa return value . encode ( encoding , errors ) return str ( value )
750	def _getClassifierRegion ( self ) : if ( self . _netInfo . net is not None and "Classifier" in self . _netInfo . net . regions ) : return self . _netInfo . net . regions [ "Classifier" ] else : return None
8103	def load_profiles ( self ) : _profiles = { } for name , klass in inspect . getmembers ( profiles ) : if inspect . isclass ( klass ) and name . endswith ( 'Profile' ) and name != 'TuioProfile' : # Adding profile to the self.profiles dictionary profile = klass ( ) _profiles [ profile . address ] = profile # setting convenient variable to access objects of profile try : setattr ( self , profile . list_label , profile . objs ) except AttributeError : continue # Mapping callback method to every profile self . manager . add ( self . callback , profile . address ) return _profiles
2670	def deploy ( src , requirements = None , local_package = None , config_file = 'config.yaml' , profile_name = None , preserve_vpc = False ) : # Load and parse the config file. path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) # Copy all the pip dependencies required to run your code into a temporary # folder then add the handler file in the root of this directory. # Zip the contents of this folder into a single file and output to the dist # directory. path_to_zip_file = build ( src , config_file = config_file , requirements = requirements , local_package = local_package , ) existing_config = get_function_config ( cfg ) if existing_config : update_function ( cfg , path_to_zip_file , existing_config , preserve_vpc = preserve_vpc ) else : create_function ( cfg , path_to_zip_file )
5609	def tiles_to_affine_shape ( tiles ) : if not tiles : raise TypeError ( "no tiles provided" ) pixel_size = tiles [ 0 ] . pixel_x_size left , bottom , right , top = ( min ( [ t . left for t in tiles ] ) , min ( [ t . bottom for t in tiles ] ) , max ( [ t . right for t in tiles ] ) , max ( [ t . top for t in tiles ] ) , ) return ( Affine ( pixel_size , 0 , left , 0 , - pixel_size , top ) , Shape ( width = int ( round ( ( right - left ) / pixel_size , 0 ) ) , height = int ( round ( ( top - bottom ) / pixel_size , 0 ) ) , ) )
12774	def inverse_kinematics ( self , start = 0 , end = 1e100 , states = None , max_force = 20 ) : zeros = None if max_force > 0 : self . skeleton . enable_motors ( max_force ) zeros = np . zeros ( self . skeleton . num_dofs ) for _ in self . follow_markers ( start , end , states ) : if zeros is not None : self . skeleton . set_target_angles ( zeros ) yield self . skeleton . joint_angles
2081	def jt_aggregate ( func , is_create = False , has_pk = False ) : def helper ( kwargs , obj ) : """The helper function preceding actual function that aggregates unified jt fields. """ unified_job_template = None for item in UNIFIED_JT : if kwargs . get ( item , None ) is not None : jt_id = kwargs . pop ( item ) if unified_job_template is None : unified_job_template = ( item , jt_id ) else : raise exc . UsageError ( 'More than one unified job template fields provided, ' 'please tighten your criteria.' ) if unified_job_template is not None : kwargs [ 'unified_job_template' ] = unified_job_template [ 1 ] obj . identity = tuple ( list ( obj . identity ) + [ 'unified_job_template' ] ) return '/' . join ( [ UNIFIED_JT [ unified_job_template [ 0 ] ] , str ( unified_job_template [ 1 ] ) , 'schedules/' ] ) elif is_create : raise exc . UsageError ( 'You must provide exactly one unified job' ' template field during creation.' ) def decorator_without_pk ( obj , * args , * * kwargs ) : old_endpoint = obj . endpoint new_endpoint = helper ( kwargs , obj ) if is_create : obj . endpoint = new_endpoint result = func ( obj , * args , * * kwargs ) obj . endpoint = old_endpoint return result def decorator_with_pk ( obj , pk = None , * args , * * kwargs ) : old_endpoint = obj . endpoint new_endpoint = helper ( kwargs , obj ) if is_create : obj . endpoint = new_endpoint result = func ( obj , pk = pk , * args , * * kwargs ) obj . endpoint = old_endpoint return result decorator = decorator_with_pk if has_pk else decorator_without_pk for item in CLICK_ATTRS : setattr ( decorator , item , getattr ( func , item , [ ] ) ) decorator . __doc__ = func . __doc__ return decorator
1808	def SETC ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
2176	def request ( self , method , url , data = None , headers = None , withhold_token = False , client_id = None , client_secret = None , * * kwargs ) : if not is_secure_transport ( url ) : raise InsecureTransportError ( ) if self . token and not withhold_token : log . debug ( "Invoking %d protected resource request hooks." , len ( self . compliance_hook [ "protected_request" ] ) , ) for hook in self . compliance_hook [ "protected_request" ] : log . debug ( "Invoking hook %s." , hook ) url , headers , data = hook ( url , headers , data ) log . debug ( "Adding token %s to request." , self . token ) try : url , headers , data = self . _client . add_token ( url , http_method = method , body = data , headers = headers ) # Attempt to retrieve and save new access token if expired except TokenExpiredError : if self . auto_refresh_url : log . debug ( "Auto refresh is set, attempting to refresh at %s." , self . auto_refresh_url , ) # We mustn't pass auth twice. auth = kwargs . pop ( "auth" , None ) if client_id and client_secret and ( auth is None ) : log . debug ( 'Encoding client_id "%s" with client_secret as Basic auth credentials.' , client_id , ) auth = requests . auth . HTTPBasicAuth ( client_id , client_secret ) token = self . refresh_token ( self . auto_refresh_url , auth = auth , * * kwargs ) if self . token_updater : log . debug ( "Updating token to %s using %s." , token , self . token_updater ) self . token_updater ( token ) url , headers , data = self . _client . add_token ( url , http_method = method , body = data , headers = headers ) else : raise TokenUpdated ( token ) else : raise log . debug ( "Requesting url %s using method %s." , url , method ) log . debug ( "Supplying headers %s and data %s" , headers , data ) log . debug ( "Passing through key word arguments %s." , kwargs ) return super ( OAuth2Session , self ) . request ( method , url , headers = headers , data = data , * * kwargs )
5395	def _get_input_target_path ( self , local_file_path ) : path , filename = os . path . split ( local_file_path ) if '*' in filename : return path + '/' else : return local_file_path
6293	def render ( self , program : moderngl . Program , mode = None , vertices = - 1 , first = 0 , instances = 1 ) : vao = self . instance ( program ) if mode is None : mode = self . mode vao . render ( mode , vertices = vertices , first = first , instances = instances )
7612	def get_clan_image ( self , obj : BaseAttrDict ) : try : badge_id = obj . clan . badge_id except AttributeError : try : badge_id = obj . badge_id except AttributeError : return 'https://i.imgur.com/Y3uXsgj.png' if badge_id is None : return 'https://i.imgur.com/Y3uXsgj.png' for i in self . constants . alliance_badges : if i . id == badge_id : return 'https://royaleapi.github.io/cr-api-assets/badges/' + i . name + '.png'
2860	def _idle ( self ) : # Put the I2C lines into an idle state with SCL and SDA high. self . _ft232h . setup_pins ( { 0 : GPIO . OUT , 1 : GPIO . OUT , 2 : GPIO . IN } , { 0 : GPIO . HIGH , 1 : GPIO . HIGH } )
5556	def _filter_by_zoom ( element = None , conf_string = None , zoom = None ) : for op_str , op_func in [ # order of operators is important: # prematurely return in cases of "<=" or ">=", otherwise # _strip_zoom() cannot parse config strings starting with "<" # or ">" ( "=" , operator . eq ) , ( "<=" , operator . le ) , ( ">=" , operator . ge ) , ( "<" , operator . lt ) , ( ">" , operator . gt ) , ] : if conf_string . startswith ( op_str ) : return element if op_func ( zoom , _strip_zoom ( conf_string , op_str ) ) else None
3250	def get_short_version ( self ) : gs_version = self . get_version ( ) match = re . compile ( r'[^\d.]+' ) return match . sub ( '' , gs_version ) . strip ( '.' )
12780	def get_users ( self , sort = True ) : self . _load ( ) if sort : self . users . sort ( key = operator . itemgetter ( "name" ) ) return self . users
12317	def delete ( self , repo , args = [ ] ) : result = None with cd ( repo . rootdir ) : try : cmd = [ 'rm' ] + list ( args ) result = { 'status' : 'success' , 'message' : self . _run ( cmd ) } except Exception as e : result = { 'status' : 'error' , 'message' : str ( e ) } # print(result) return result
185	def almost_equals ( self , other , max_distance = 1e-4 , points_per_edge = 8 ) : if self . label != other . label : return False return self . coords_almost_equals ( other , max_distance = max_distance , points_per_edge = points_per_edge )
13488	def create ( self , server ) : for chunk in self . __cut_to_size ( ) : server . post ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
5227	def to_str ( data : dict , fmt = '{key}={value}' , sep = ', ' , public_only = True ) -> str : if public_only : keys = list ( filter ( lambda vv : vv [ 0 ] != '_' , data . keys ( ) ) ) else : keys = list ( data . keys ( ) ) return '{' + sep . join ( [ to_str ( data = v , fmt = fmt , sep = sep ) if isinstance ( v , dict ) else fstr ( fmt = fmt , key = k , value = v ) for k , v in data . items ( ) if k in keys ] ) + '}'
3610	def delete ( self , url , name , params = None , headers = None , connection = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) return make_delete_request ( endpoint , params , headers , connection = connection )
3736	def Stockmayer ( Tm = None , Tb = None , Tc = None , Zc = None , omega = None , CASRN = '' , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in MagalhaesLJ_data . index : methods . append ( MAGALHAES ) if Tc and omega : methods . append ( TEEGOTOSTEWARD2 ) if Tc : methods . append ( FLYNN ) methods . append ( BSLC ) methods . append ( TEEGOTOSTEWARD1 ) if Tb : methods . append ( BSLB ) if Tm : methods . append ( BSLM ) if Tc and Zc : methods . append ( STIELTHODOS ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == FLYNN : epsilon = epsilon_Flynn ( Tc ) elif Method == BSLC : epsilon = epsilon_Bird_Stewart_Lightfoot_critical ( Tc ) elif Method == BSLB : epsilon = epsilon_Bird_Stewart_Lightfoot_boiling ( Tb ) elif Method == BSLM : epsilon = epsilon_Bird_Stewart_Lightfoot_melting ( Tm ) elif Method == STIELTHODOS : epsilon = epsilon_Stiel_Thodos ( Tc , Zc ) elif Method == TEEGOTOSTEWARD1 : epsilon = epsilon_Tee_Gotoh_Steward_1 ( Tc ) elif Method == TEEGOTOSTEWARD2 : epsilon = epsilon_Tee_Gotoh_Steward_2 ( Tc , omega ) elif Method == MAGALHAES : epsilon = float ( MagalhaesLJ_data . at [ CASRN , "epsilon" ] ) elif Method == NONE : epsilon = None else : raise Exception ( 'Failure in in function' ) return epsilon
9533	def unsign ( self , signed_value , ttl = None ) : h_size , d_size = struct . calcsize ( '>cQ' ) , self . digest . digest_size fmt = '>cQ%ds%ds' % ( len ( signed_value ) - h_size - d_size , d_size ) try : version , timestamp , value , sig = struct . unpack ( fmt , signed_value ) except struct . error : raise BadSignature ( 'Signature is not valid' ) if version != self . version : raise BadSignature ( 'Signature version not supported' ) if ttl is not None : if isinstance ( ttl , datetime . timedelta ) : ttl = ttl . total_seconds ( ) # Check timestamp is not older than ttl age = abs ( time . time ( ) - timestamp ) if age > ttl + _MAX_CLOCK_SKEW : raise SignatureExpired ( 'Signature age %s > %s seconds' % ( age , ttl ) ) try : self . signature ( signed_value [ : - d_size ] ) . verify ( sig ) except InvalidSignature : raise BadSignature ( 'Signature "%s" does not match' % binascii . b2a_base64 ( sig ) ) return value
12917	def deref ( self , ctx ) : if self in ctx . call_nodes : raise CyclicReferenceError ( ctx , self ) if self in ctx . cached_results : return ctx . cached_results [ self ] try : ctx . call_nodes . add ( self ) ctx . call_stack . append ( self ) result = self . evaluate ( ctx ) ctx . cached_results [ self ] = result return result except : if ctx . exception_call_stack is None : ctx . exception_call_stack = list ( ctx . call_stack ) raise finally : ctx . call_stack . pop ( ) ctx . call_nodes . remove ( self )
10388	def calculate_average_scores_on_graph ( graph : BELGraph , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , use_tqdm : bool = False , ) : subgraphs = generate_bioprocess_mechanisms ( graph , key = key ) scores = calculate_average_scores_on_subgraphs ( subgraphs , key = key , tag = tag , default_score = default_score , runs = runs , use_tqdm = use_tqdm ) return scores
7493	def n_choose_k ( n , k ) : return int ( reduce ( MUL , ( Fraction ( n - i , i + 1 ) for i in range ( k ) ) , 1 ) )
13680	def get_translated_data ( self ) : j = { } for k in self . data : d = { } for l in self . data [ k ] : d [ self . translation_keys [ l ] ] = self . data [ k ] [ l ] j [ k ] = d return j
5852	def get_dataset_file ( self , dataset_id , file_path , version = None ) : return self . get_dataset_files ( dataset_id , "^{}$" . format ( file_path ) , version_number = version ) [ 0 ]
4571	def adapt_animation_layout ( animation ) : layout = animation . layout required = getattr ( animation , 'LAYOUT_CLASS' , None ) if not required or isinstance ( layout , required ) : return msg = LAYOUT_WARNING % ( type ( animation ) . __name__ , required . __name__ , type ( layout ) . __name__ ) setter = layout . set adaptor = None if required is strip . Strip : if isinstance ( layout , matrix . Matrix ) : width = layout . width def adaptor ( pixel , color = None ) : y , x = divmod ( pixel , width ) setter ( x , y , color or BLACK ) elif isinstance ( layout , cube . Cube ) : lx , ly = layout . x , layout . y def adaptor ( pixel , color = None ) : yz , x = divmod ( pixel , lx ) z , y = divmod ( yz , ly ) setter ( x , y , z , color or BLACK ) elif isinstance ( layout , circle . Circle ) : def adaptor ( pixel , color = None ) : layout . _set_base ( pixel , color or BLACK ) elif required is matrix . Matrix : if isinstance ( layout , strip . Strip ) : width = animation . width def adaptor ( x , y , color = None ) : setter ( x + y * width , color or BLACK ) if not adaptor : raise ValueError ( msg ) log . warning ( msg ) animation . layout . set = adaptor
2509	def parse_only_extr_license ( self , extr_lic ) : # Grab all possible values ident = self . get_extr_license_ident ( extr_lic ) text = self . get_extr_license_text ( extr_lic ) comment = self . get_extr_lics_comment ( extr_lic ) xrefs = self . get_extr_lics_xref ( extr_lic ) name = self . get_extr_lic_name ( extr_lic ) if not ident : # Must have identifier return # Set fields # FIXME: the constructor of the license should alwas accept a name lic = document . ExtractedLicense ( ident ) if text is not None : lic . text = text if name is not None : lic . full_name = name if comment is not None : lic . comment = comment lic . cross_ref = map ( lambda x : six . text_type ( x ) , xrefs ) return lic
9848	def _load_dx ( self , filename ) : dx = OpenDX . field ( 0 ) dx . read ( filename ) grid , edges = dx . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
8339	def _findAll ( self , name , attrs , text , limit , generator , * * kwargs ) : if isinstance ( name , SoupStrainer ) : strainer = name else : # Build a SoupStrainer strainer = SoupStrainer ( name , attrs , text , * * kwargs ) results = ResultSet ( strainer ) g = generator ( ) while True : try : i = g . next ( ) except StopIteration : break if i : found = strainer . search ( i ) if found : results . append ( found ) if limit and len ( results ) >= limit : break return results
832	def drawFile ( dataset , matrix , patterns , cells , w , fnum ) : score = 0 count = 0 assert len ( patterns ) == len ( cells ) for p in xrange ( len ( patterns ) - 1 ) : matrix [ p + 1 : , p ] = [ len ( set ( patterns [ p ] ) . intersection ( set ( q ) ) ) * 100 / w for q in patterns [ p + 1 : ] ] matrix [ p , p + 1 : ] = [ len ( set ( cells [ p ] ) . intersection ( set ( r ) ) ) * 5 / 2 for r in cells [ p + 1 : ] ] score += sum ( abs ( np . array ( matrix [ p + 1 : , p ] ) - np . array ( matrix [ p , p + 1 : ] ) ) ) count += len ( matrix [ p + 1 : , p ] ) print 'Score' , score / count fig = pyl . figure ( figsize = ( 10 , 10 ) , num = fnum ) pyl . matshow ( matrix , fignum = fnum ) pyl . colorbar ( ) pyl . title ( 'Coincidence Space' , verticalalignment = 'top' , fontsize = 12 ) pyl . xlabel ( 'The Mirror Image Visualization for ' + dataset , fontsize = 17 ) pyl . ylabel ( 'Encoding space' , fontsize = 12 )
9659	def get_levels ( G ) : levels = [ ] ends = get_sinks ( G ) levels . append ( ends ) while get_direct_ancestors ( G , ends ) : ends = get_direct_ancestors ( G , ends ) levels . append ( ends ) levels . reverse ( ) return levels
1526	def add_context ( self , err_context , succ_context = None ) : self . err_context = err_context self . succ_context = succ_context
526	def _inhibitColumnsLocal ( self , overlaps , density ) : activeArray = numpy . zeros ( self . _numColumns , dtype = "bool" ) for column , overlap in enumerate ( overlaps ) : if overlap >= self . _stimulusThreshold : neighborhood = self . _getColumnNeighborhood ( column ) neighborhoodOverlaps = overlaps [ neighborhood ] numBigger = numpy . count_nonzero ( neighborhoodOverlaps > overlap ) # When there is a tie, favor neighbors that are already selected as # active. ties = numpy . where ( neighborhoodOverlaps == overlap ) tiedNeighbors = neighborhood [ ties ] numTiesLost = numpy . count_nonzero ( activeArray [ tiedNeighbors ] ) numActive = int ( 0.5 + density * len ( neighborhood ) ) if numBigger + numTiesLost < numActive : activeArray [ column ] = True return activeArray . nonzero ( ) [ 0 ]
10773	def add_node ( self , node , offset ) : # calculate x,y from offset considering axis start and end points width = self . end [ 0 ] - self . start [ 0 ] height = self . end [ 1 ] - self . start [ 1 ] node . x = self . start [ 0 ] + ( width * offset ) node . y = self . start [ 1 ] + ( height * offset ) self . nodes [ node . ID ] = node
10452	def waittillguiexist ( self , window_name , object_name = '' , guiTimeOut = 30 , state = '' ) : timeout = 0 while timeout < guiTimeOut : if self . guiexist ( window_name , object_name ) : return 1 # Wait 1 second before retrying time . sleep ( 1 ) timeout += 1 # Object and/or window doesn't appear within the timeout period return 0
5947	def unlink_f ( path ) : try : os . unlink ( path ) except OSError as err : if err . errno != errno . ENOENT : raise
5034	def get_enterprise_customer_user_queryset ( self , request , search_keyword , customer_uuid , page_size = PAGE_SIZE ) : page = request . GET . get ( 'page' , 1 ) learners = EnterpriseCustomerUser . objects . filter ( enterprise_customer__uuid = customer_uuid ) user_ids = learners . values_list ( 'user_id' , flat = True ) matching_users = User . objects . filter ( pk__in = user_ids ) if search_keyword is not None : matching_users = matching_users . filter ( Q ( email__icontains = search_keyword ) | Q ( username__icontains = search_keyword ) ) matching_user_ids = matching_users . values_list ( 'pk' , flat = True ) learners = learners . filter ( user_id__in = matching_user_ids ) return paginated_list ( learners , page , page_size )
1929	def process_config_values ( parser : argparse . ArgumentParser , args : argparse . Namespace ) : # First, load a local config file, if passed or look for one in pwd if it wasn't. load_overrides ( args . config ) # Get a list of defined config vals. If these are passed on the command line, # update them in their correct group, not in the cli group defined_vars = list ( get_config_keys ( ) ) command_line_args = vars ( args ) # Bring in the options keys into args config_cli_args = get_group ( 'cli' ) # Place all command line args into the cli group (for saving in the workspace). If # the value is set on command line, then it takes precedence; otherwise we try to # read it from the config file's cli group. for k in command_line_args : default = parser . get_default ( k ) set_val = getattr ( args , k ) if default is not set_val : if k not in defined_vars : config_cli_args . update ( k , value = set_val ) else : # Update a var's native group group_name , key = k . split ( '.' ) group = get_group ( group_name ) setattr ( group , key , set_val ) else : if k in config_cli_args : setattr ( args , k , getattr ( config_cli_args , k ) )
5151	def merge_config ( template , config , list_identifiers = None ) : result = template . copy ( ) for key , value in config . items ( ) : if isinstance ( value , dict ) : node = result . get ( key , OrderedDict ( ) ) result [ key ] = merge_config ( node , value ) elif isinstance ( value , list ) and isinstance ( result . get ( key ) , list ) : result [ key ] = merge_list ( result [ key ] , value , list_identifiers ) else : result [ key ] = value return result
13784	def get_tm_session ( session_factory , transaction_manager ) : dbsession = session_factory ( ) zope . sqlalchemy . register ( dbsession , transaction_manager = transaction_manager ) return dbsession
12858	def from_date ( datetime_date ) : return BusinessDate . from_ymd ( datetime_date . year , datetime_date . month , datetime_date . day )
6642	def satisfyDependenciesRecursive ( self , available_components = None , search_dirs = None , update_installed = False , traverse_links = False , target = None , test = False ) : def provider ( dspec , available_components , search_dirs , working_directory , update_installed , dep_of = None ) : r = access . satisfyFromAvailable ( dspec . name , available_components ) if r : if r . isTestDependency ( ) and not dspec . is_test_dependency : logger . debug ( 'test dependency subsequently occurred as real dependency: %s' , r . getName ( ) ) r . setTestDependency ( False ) return r update_if_installed = False if update_installed is True : update_if_installed = True elif update_installed : update_if_installed = dspec . name in update_installed r = access . satisfyVersionFromSearchPaths ( dspec . name , dspec . versionReq ( ) , search_dirs , update_if_installed , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if r : r . setTestDependency ( dspec . is_test_dependency ) return r # before resorting to install this module, check if we have an # existing linked module (which wasn't picked up because it didn't # match the version specification) - if we do, then we shouldn't # try to install, but should return that anyway: default_path = os . path . join ( self . modulesPath ( ) , dspec . name ) if fsutils . isLink ( default_path ) : r = Component ( default_path , test_dependency = dspec . is_test_dependency , installed_linked = fsutils . isLink ( default_path ) , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if r : assert ( r . installedLinked ( ) ) return r else : logger . error ( 'linked module %s is invalid: %s' , dspec . name , r . getError ( ) ) return r r = access . satisfyVersionByInstalling ( dspec . name , dspec . versionReq ( ) , self . modulesPath ( ) , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if not r : logger . error ( 'could not install %s' % dspec . name ) if r is not None : r . setTestDependency ( dspec . is_test_dependency ) return r return self . __getDependenciesRecursiveWithProvider ( available_components = available_components , search_dirs = search_dirs , target = target , traverse_links = traverse_links , update_installed = update_installed , provider = provider , test = test )
2772	def load ( self ) : data = self . get_data ( 'load_balancers/%s' % self . id , type = GET ) load_balancer = data [ 'load_balancer' ] # Setting the attribute values for attr in load_balancer . keys ( ) : if attr == 'health_check' : health_check = HealthCheck ( * * load_balancer [ 'health_check' ] ) setattr ( self , attr , health_check ) elif attr == 'sticky_sessions' : sticky_ses = StickySesions ( * * load_balancer [ 'sticky_sessions' ] ) setattr ( self , attr , sticky_ses ) elif attr == 'forwarding_rules' : rules = list ( ) for rule in load_balancer [ 'forwarding_rules' ] : rules . append ( ForwardingRule ( * * rule ) ) setattr ( self , attr , rules ) else : setattr ( self , attr , load_balancer [ attr ] ) return self
9980	def extract_names ( source ) : if source is None : return None source = dedent ( source ) funcdef = find_funcdef ( source ) params = extract_params ( source ) names = [ ] if isinstance ( funcdef , ast . FunctionDef ) : stmts = funcdef . body elif isinstance ( funcdef , ast . Lambda ) : stmts = [ funcdef . body ] else : raise ValueError ( "must not happen" ) for stmt in stmts : for node in ast . walk ( stmt ) : if isinstance ( node , ast . Name ) : if node . id not in names and node . id not in params : names . append ( node . id ) return names
4825	def has_course_mode ( self , course_run_id , mode ) : course_modes = self . get_course_modes ( course_run_id ) return any ( course_mode for course_mode in course_modes if course_mode [ 'slug' ] == mode )
12456	def install ( env , requirements , args , ignore_activated = False , install_dev_requirements = False , quiet = False ) : if os . path . isfile ( requirements ) : args += ( '-r' , requirements ) label = 'project' else : args += ( '-U' , '-e' , '.' ) label = 'library' # Attempt to install development requirements if install_dev_requirements : dev_requirements = None dirname = os . path . dirname ( requirements ) basename , ext = os . path . splitext ( os . path . basename ( requirements ) ) # Possible dev requirements files: # # * <requirements>-dev.<ext> # * dev-<requirements>.<ext> # * <requirements>_dev.<ext> # * dev_<requirements>.<ext> # * <requirements>dev.<ext> # * dev<requirements>.<ext> # # Where <requirements> is basename of given requirements file to use # and <ext> is its extension. for delimiter in ( '-' , '_' , '' ) : filename = os . path . join ( dirname , '' . join ( ( basename , delimiter , 'dev' , ext ) ) ) if os . path . isfile ( filename ) : dev_requirements = filename break filename = os . path . join ( dirname , '' . join ( ( 'dev' , delimiter , basename , ext ) ) ) if os . path . isfile ( filename ) : dev_requirements = filename break # If at least one dev requirements file found, install dev requirements if dev_requirements : args += ( '-r' , dev_requirements ) if not quiet : print_message ( '== Step 2. Install {0} ==' . format ( label ) ) result = not pip_cmd ( env , ( 'install' , ) + args , ignore_activated , echo = not quiet ) if not quiet : print_message ( ) return result
12755	def enable_motors ( self , max_force ) : for joint in self . joints : amotor = getattr ( joint , 'amotor' , joint ) amotor . max_forces = max_force if max_force > 0 : amotor . enable_feedback ( ) else : amotor . disable_feedback ( )
8086	def strokewidth ( self , w = None ) : if w is not None : self . _canvas . strokewidth = w else : return self . _canvas . strokewidth
12306	def find_executable_files ( ) : files = glob . glob ( "*" ) + glob . glob ( "*/*" ) + glob . glob ( '*/*/*' ) files = filter ( lambda f : os . path . isfile ( f ) , files ) executable = stat . S_IEXEC | stat . S_IXGRP | stat . S_IXOTH final = [ ] for filename in files : if os . path . isfile ( filename ) : st = os . stat ( filename ) mode = st . st_mode if mode & executable : final . append ( filename ) if len ( final ) > 5 : break return final
7723	def get_password ( self ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "password" : return from_utf8 ( child . getContent ( ) ) return None
13655	def _matchRoute ( components , request , segments , partialMatching ) : if len ( components ) == 1 and isinstance ( components [ 0 ] , bytes ) : components = components [ 0 ] if components [ : 1 ] == '/' : components = components [ 1 : ] components = components . split ( '/' ) results = OrderedDict ( ) NO_MATCH = None , segments remaining = list ( segments ) # Handle the null route. if len ( segments ) == len ( components ) == 0 : return results , remaining for us , them in izip_longest ( components , segments ) : if us is None : if partialMatching : # We've matched all of our components, there might be more # segments for something else to process. break else : return NO_MATCH elif them is None : # We've run out of path segments to match, so this route can't be # the matching one. return NO_MATCH if callable ( us ) : name , match = us ( request , them ) if match is None : return NO_MATCH results [ name ] = match elif us != them : return NO_MATCH remaining . pop ( 0 ) return results , remaining
5371	def load_file ( file_path , credentials = None ) : if file_path . startswith ( 'gs://' ) : return _load_file_from_gcs ( file_path , credentials ) else : return open ( file_path , 'r' )
4222	def disable ( ) : root = platform . config_root ( ) try : os . makedirs ( root ) except OSError : pass filename = os . path . join ( root , 'keyringrc.cfg' ) if os . path . exists ( filename ) : msg = "Refusing to overwrite {filename}" . format ( * * locals ( ) ) raise RuntimeError ( msg ) with open ( filename , 'w' ) as file : file . write ( '[backend]\ndefault-keyring=keyring.backends.null.Keyring' )
13799	def log ( self , string ) : self . wfile . write ( json . dumps ( { 'log' : string } ) + NEWLINE )
3249	def get_base ( managed_policy , * * conn ) : managed_policy [ '_version' ] = 1 arn = _get_name_from_structure ( managed_policy , 'Arn' ) policy = get_policy ( arn , * * conn ) document = get_managed_policy_document ( arn , policy_metadata = policy , * * conn ) managed_policy . update ( policy [ 'Policy' ] ) managed_policy [ 'Document' ] = document # Fix the dates: managed_policy [ 'CreateDate' ] = get_iso_string ( managed_policy [ 'CreateDate' ] ) managed_policy [ 'UpdateDate' ] = get_iso_string ( managed_policy [ 'UpdateDate' ] ) return managed_policy
1418	def create_execution_state ( self , topologyName , executionState ) : if not executionState or not executionState . IsInitialized ( ) : raise_ ( StateException ( "Execution State protobuf not init properly" , StateException . EX_TYPE_PROTOBUF_ERROR ) , sys . exc_info ( ) [ 2 ] ) path = self . get_execution_state_path ( topologyName ) LOG . info ( "Adding topology: {0} to path: {1}" . format ( topologyName , path ) ) executionStateString = executionState . SerializeToString ( ) try : self . client . create ( path , value = executionStateString , makepath = True ) return True except NoNodeError : raise_ ( StateException ( "NoNodeError while creating execution state" , StateException . EX_TYPE_NO_NODE_ERROR ) , sys . exc_info ( ) [ 2 ] ) except NodeExistsError : raise_ ( StateException ( "NodeExistsError while creating execution state" , StateException . EX_TYPE_NODE_EXISTS_ERROR ) , sys . exc_info ( ) [ 2 ] ) except ZookeeperError : raise_ ( StateException ( "Zookeeper while creating execution state" , StateException . EX_TYPE_ZOOKEEPER_ERROR ) , sys . exc_info ( ) [ 2 ] ) except Exception : # Just re raise the exception. raise
1302	def keybd_event ( bVk : int , bScan : int , dwFlags : int , dwExtraInfo : int ) -> None : ctypes . windll . user32 . keybd_event ( bVk , bScan , dwFlags , dwExtraInfo )
8294	def clique ( graph , id ) : clique = [ id ] for n in graph . nodes : friend = True for id in clique : if n . id == id or graph . edge ( n . id , id ) == None : friend = False break if friend : clique . append ( n . id ) return clique
12712	def add_torque ( self , torque , relative = False ) : op = self . ode_body . addRelTorque if relative else self . ode_body . addTorque op ( torque )
3519	def matomo ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return MatomoNode ( )
2418	def write_package ( package , out ) : out . write ( '# Package\n\n' ) write_value ( 'PackageName' , package . name , out ) if package . has_optional_field ( 'version' ) : write_value ( 'PackageVersion' , package . version , out ) write_value ( 'PackageDownloadLocation' , package . download_location , out ) if package . has_optional_field ( 'summary' ) : write_text_value ( 'PackageSummary' , package . summary , out ) if package . has_optional_field ( 'source_info' ) : write_text_value ( 'PackageSourceInfo' , package . source_info , out ) if package . has_optional_field ( 'file_name' ) : write_value ( 'PackageFileName' , package . file_name , out ) if package . has_optional_field ( 'supplier' ) : write_value ( 'PackageSupplier' , package . supplier , out ) if package . has_optional_field ( 'originator' ) : write_value ( 'PackageOriginator' , package . originator , out ) if package . has_optional_field ( 'check_sum' ) : write_value ( 'PackageChecksum' , package . check_sum . to_tv ( ) , out ) write_value ( 'PackageVerificationCode' , format_verif_code ( package ) , out ) if package . has_optional_field ( 'description' ) : write_text_value ( 'PackageDescription' , package . description , out ) if isinstance ( package . license_declared , ( document . LicenseConjunction , document . LicenseDisjunction ) ) : write_value ( 'PackageLicenseDeclared' , u'({0})' . format ( package . license_declared ) , out ) else : write_value ( 'PackageLicenseDeclared' , package . license_declared , out ) if isinstance ( package . conc_lics , ( document . LicenseConjunction , document . LicenseDisjunction ) ) : write_value ( 'PackageLicenseConcluded' , u'({0})' . format ( package . conc_lics ) , out ) else : write_value ( 'PackageLicenseConcluded' , package . conc_lics , out ) # Write sorted list of licenses. for lics in sorted ( package . licenses_from_files ) : write_value ( 'PackageLicenseInfoFromFiles' , lics , out ) if package . has_optional_field ( 'license_comment' ) : write_text_value ( 'PackageLicenseComments' , package . license_comment , out ) # cr_text is either free form text or NONE or NOASSERTION. if isinstance ( package . cr_text , six . string_types ) : write_text_value ( 'PackageCopyrightText' , package . cr_text , out ) else : write_value ( 'PackageCopyrightText' , package . cr_text , out ) if package . has_optional_field ( 'homepage' ) : write_value ( 'PackageHomePage' , package . homepage , out ) # Write sorted files. for spdx_file in sorted ( package . files ) : write_separators ( out ) write_file ( spdx_file , out )
8823	def start_rpc_listeners ( self ) : self . _setup_rpc ( ) if not self . endpoints : return [ ] self . conn = n_rpc . create_connection ( ) self . conn . create_consumer ( self . topic , self . endpoints , fanout = False ) return self . conn . consume_in_threads ( )
3169	def resume ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/resume' ) )
4835	def get_paginated_catalog_courses ( self , catalog_id , querystring = None ) : return self . _load_data ( self . CATALOGS_COURSES_ENDPOINT . format ( catalog_id ) , default = [ ] , querystring = querystring , traverse_pagination = False , many = False , )
4170	def zpk2ss ( z , p , k ) : import scipy . signal return scipy . signal . zpk2ss ( z , p , k )
7257	def get_address_coords ( self , address ) : url = "https://maps.googleapis.com/maps/api/geocode/json?&address=" + address r = requests . get ( url ) r . raise_for_status ( ) results = r . json ( ) [ 'results' ] lat = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lat' ] lng = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lng' ] return lat , lng
267	def vectorize ( func ) : def wrapper ( df , * args , * * kwargs ) : if df . ndim == 1 : return func ( df , * args , * * kwargs ) elif df . ndim == 2 : return df . apply ( func , * args , * * kwargs ) return wrapper
7792	def remove_fetcher ( self , fetcher ) : self . _lock . acquire ( ) try : for t , f in list ( self . _active_fetchers ) : if f is fetcher : self . _active_fetchers . remove ( ( t , f ) ) f . _deactivated ( ) return finally : self . _lock . release ( )
8243	def guess_name ( clr ) : clr = Color ( clr ) if clr . is_transparent : return "transparent" if clr . is_black : return "black" if clr . is_white : return "white" if clr . is_black : return "black" for name in named_colors : try : r , g , b = named_colors [ name ] except : continue if r == clr . r and g == clr . g and b == clr . b : return name for shade in shades : if clr in shade : return shade . name + " " + clr . nearest_hue ( ) break return clr . nearest_hue ( )
12309	def auto_add ( repo , autooptions , files ) : # Get the mappings and keys. mapping = { "." : "" } if ( ( 'import' in autooptions ) and ( 'directory-mapping' in autooptions [ 'import' ] ) ) : mapping = autooptions [ 'import' ] [ 'directory-mapping' ] # Apply the longest prefix first... keys = mapping . keys ( ) keys = sorted ( keys , key = lambda k : len ( k ) , reverse = True ) count = 0 params = [ ] for f in files : # Find the destination relativepath = f for k in keys : v = mapping [ k ] if f . startswith ( k + "/" ) : #print("Replacing ", k) relativepath = f . replace ( k + "/" , v ) break # Now add to repository count += files_add ( repo = repo , args = [ f ] , targetdir = os . path . dirname ( relativepath ) ) return count
8352	def handle_charref ( self , ref ) : if self . convertEntities : data = unichr ( int ( ref ) ) else : data = '&#%s;' % ref self . handle_data ( data )
10229	def summarize_stability ( graph : BELGraph ) -> Mapping [ str , int ] : regulatory_pairs = get_regulatory_pairs ( graph ) chaotic_pairs = get_chaotic_pairs ( graph ) dampened_pairs = get_dampened_pairs ( graph ) contraditory_pairs = get_contradiction_summary ( graph ) separately_unstable_triples = get_separate_unstable_correlation_triples ( graph ) mutually_unstable_triples = get_mutually_unstable_correlation_triples ( graph ) jens_unstable_triples = get_jens_unstable ( graph ) increase_mismatch_triples = get_increase_mismatch_triplets ( graph ) decrease_mismatch_triples = get_decrease_mismatch_triplets ( graph ) chaotic_triples = get_chaotic_triplets ( graph ) dampened_triples = get_dampened_triplets ( graph ) return { 'Regulatory Pairs' : _count_or_len ( regulatory_pairs ) , 'Chaotic Pairs' : _count_or_len ( chaotic_pairs ) , 'Dampened Pairs' : _count_or_len ( dampened_pairs ) , 'Contradictory Pairs' : _count_or_len ( contraditory_pairs ) , 'Separately Unstable Triples' : _count_or_len ( separately_unstable_triples ) , 'Mutually Unstable Triples' : _count_or_len ( mutually_unstable_triples ) , 'Jens Unstable Triples' : _count_or_len ( jens_unstable_triples ) , 'Increase Mismatch Triples' : _count_or_len ( increase_mismatch_triples ) , 'Decrease Mismatch Triples' : _count_or_len ( decrease_mismatch_triples ) , 'Chaotic Triples' : _count_or_len ( chaotic_triples ) , 'Dampened Triples' : _count_or_len ( dampened_triples ) }
12197	def get_task_options ( ) : options = ( ) task_classes = get_tasks ( ) for cls in task_classes : options += cls . option_list return options
12676	def _escape_char ( c , escape_char = ESCAPE_CHAR ) : buf = [ ] for byte in c . encode ( 'utf8' ) : buf . append ( escape_char ) buf . append ( '%X' % _ord ( byte ) ) return '' . join ( buf )
2838	def setup ( self , pin , value ) : self . _validate_pin ( pin ) # Set bit to 1 for input or 0 for output. if value == GPIO . IN : self . iodir [ int ( pin / 8 ) ] |= 1 << ( int ( pin % 8 ) ) elif value == GPIO . OUT : self . iodir [ int ( pin / 8 ) ] &= ~ ( 1 << ( int ( pin % 8 ) ) ) else : raise ValueError ( 'Unexpected value. Must be GPIO.IN or GPIO.OUT.' ) self . write_iodir ( )
13285	def clone ( src , dst_path , skip_globals , skip_dimensions , skip_variables ) : if os . path . exists ( dst_path ) : os . unlink ( dst_path ) dst = netCDF4 . Dataset ( dst_path , 'w' ) # Global attributes for attname in src . ncattrs ( ) : if attname not in skip_globals : setattr ( dst , attname , getattr ( src , attname ) ) # Dimensions unlimdim = None unlimdimname = False for dimname , dim in src . dimensions . items ( ) : # Skip what we need to if dimname in skip_dimensions : continue if dim . isunlimited ( ) : unlimdim = dim unlimdimname = dimname dst . createDimension ( dimname , None ) else : dst . createDimension ( dimname , len ( dim ) ) # Variables for varname , ncvar in src . variables . items ( ) : # Skip what we need to if varname in skip_variables : continue hasunlimdim = False if unlimdimname and unlimdimname in ncvar . dimensions : hasunlimdim = True filler = None if hasattr ( ncvar , '_FillValue' ) : filler = ncvar . _FillValue if ncvar . chunking == "contiguous" : var = dst . createVariable ( varname , ncvar . dtype , ncvar . dimensions , fill_value = filler ) else : var = dst . createVariable ( varname , ncvar . dtype , ncvar . dimensions , fill_value = filler , chunksizes = ncvar . chunking ( ) ) # Attributes for attname in ncvar . ncattrs ( ) : if attname == '_FillValue' : continue else : setattr ( var , attname , getattr ( ncvar , attname ) ) # Data nchunk = 1000 if hasunlimdim : if nchunk : start = 0 stop = len ( unlimdim ) step = nchunk if step < 1 : step = 1 for n in range ( start , stop , step ) : nmax = n + nchunk if nmax > len ( unlimdim ) : nmax = len ( unlimdim ) idata = ncvar [ n : nmax ] var [ n : nmax ] = idata else : idata = ncvar [ : ] var [ 0 : len ( unlimdim ) ] = idata else : idata = ncvar [ : ] var [ : ] = idata dst . sync ( ) src . close ( ) dst . close ( )
2213	def delete ( path , verbose = False ) : if not os . path . exists ( path ) : # if the file does exists and is not a broken link if os . path . islink ( path ) : if verbose : # nocover print ( 'Deleting broken link="{}"' . format ( path ) ) os . unlink ( path ) elif os . path . isdir ( path ) : # nocover # Only on windows will a file be a directory and not exist if verbose : print ( 'Deleting broken directory link="{}"' . format ( path ) ) os . rmdir ( path ) elif os . path . isfile ( path ) : # nocover # This is a windows only case if verbose : print ( 'Deleting broken file link="{}"' . format ( path ) ) os . unlink ( path ) else : if verbose : # nocover print ( 'Not deleting non-existant path="{}"' . format ( path ) ) else : if os . path . islink ( path ) : if verbose : # nocover print ( 'Deleting symbolic link="{}"' . format ( path ) ) os . unlink ( path ) elif os . path . isfile ( path ) : if verbose : # nocover print ( 'Deleting file="{}"' . format ( path ) ) os . unlink ( path ) elif os . path . isdir ( path ) : if verbose : # nocover print ( 'Deleting directory="{}"' . format ( path ) ) if sys . platform . startswith ( 'win32' ) : # nocover # Workaround bug that prevents shutil from working if # the directory contains junctions from ubelt import _win32_links _win32_links . _win32_rmtree ( path , verbose = verbose ) else : import shutil shutil . rmtree ( path )
2069	def get_splice_data ( ) : df = pd . read_csv ( 'source_data/splice/splice.csv' ) X = df . reindex ( columns = [ x for x in df . columns . values if x != 'class' ] ) X [ 'dna' ] = X [ 'dna' ] . map ( lambda x : list ( str ( x ) . strip ( ) ) ) for idx in range ( 60 ) : X [ 'dna_%d' % ( idx , ) ] = X [ 'dna' ] . map ( lambda x : x [ idx ] ) del X [ 'dna' ] y = df . reindex ( columns = [ 'class' ] ) y = preprocessing . LabelEncoder ( ) . fit_transform ( y . values . reshape ( - 1 , ) ) # this data is truly categorical, with no known concept of ordering mapping = None return X , y , mapping
11938	def add_message_for ( users , level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) m = backend . create_message ( level , message_text , extra_tags , date , url ) backend . archive_store ( users , m ) backend . inbox_store ( users , m )
9072	def build_engine_session ( connection , echo = False , autoflush = None , autocommit = None , expire_on_commit = None , scopefunc = None ) : if connection is None : raise ValueError ( 'can not build engine when connection is None' ) engine = create_engine ( connection , echo = echo ) autoflush = autoflush if autoflush is not None else False autocommit = autocommit if autocommit is not None else False expire_on_commit = expire_on_commit if expire_on_commit is not None else True log . debug ( 'auto flush: %s, auto commit: %s, expire on commmit: %s' , autoflush , autocommit , expire_on_commit ) #: A SQLAlchemy session maker session_maker = sessionmaker ( bind = engine , autoflush = autoflush , autocommit = autocommit , expire_on_commit = expire_on_commit , ) #: A SQLAlchemy session object session = scoped_session ( session_maker , scopefunc = scopefunc ) return engine , session
10100	def get_snippet ( self , snippet_id , timeout = None ) : return self . _api_request ( self . SNIPPET_ENDPOINT % ( snippet_id ) , self . HTTP_GET , timeout = timeout )
13154	def cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , * * kwargs ) : with ( yield from cls . get_cursor ( ) ) as c : return ( yield from func ( cls , c , * args , * * kwargs ) ) return wrapper
12778	def render ( self , dt ) : for frame in self . _frozen : for body in frame : self . draw_body ( body ) for body in self . world . bodies : self . draw_body ( body ) if hasattr ( self . world , 'markers' ) : # draw line between anchor1 and anchor2 for marker joints. window . glColor4f ( 0.9 , 0.1 , 0.1 , 0.9 ) window . glLineWidth ( 3 ) for j in self . world . markers . joints . values ( ) : window . glBegin ( window . GL_LINES ) window . glVertex3f ( * j . getAnchor ( ) ) window . glVertex3f ( * j . getAnchor2 ( ) ) window . glEnd ( )
13070	def r_references ( self , objectId , lang = None ) : collection , reffs = self . get_reffs ( objectId = objectId , export_collection = True ) return { "template" : "main::references.html" , "objectId" : objectId , "citation" : collection . citation , "collections" : { "current" : { "label" : collection . get_label ( lang ) , "id" : collection . id , "model" : str ( collection . model ) , "type" : str ( collection . type ) , } , "parents" : self . make_parents ( collection , lang = lang ) } , "reffs" : reffs }
7360	def _check_hla_alleles ( alleles , valid_alleles = None ) : require_iterable_of ( alleles , string_types , "HLA alleles" ) # Don't run the MHC predictor twice for homozygous alleles, # only run it for unique alleles alleles = { normalize_allele_name ( allele . strip ( ) . upper ( ) ) for allele in alleles } if valid_alleles : # For some reason netMHCpan drops the '*' in names, so # 'HLA-A*03:01' becomes 'HLA-A03:01' missing_alleles = [ allele for allele in alleles if allele not in valid_alleles ] if len ( missing_alleles ) > 0 : raise UnsupportedAllele ( "Unsupported HLA alleles: %s" % missing_alleles ) return list ( alleles )
7680	def beat_position ( annotation , * * kwargs ) : times , values = annotation . to_interval_values ( ) labels = [ _ [ 'position' ] for _ in values ] # TODO: plot time signature, measure number return mir_eval . display . events ( times , labels = labels , * * kwargs )
3548	def _descriptor_changed ( self , descriptor ) : # Tell the descriptor it has a new value to read. desc = descriptor_list ( ) . get ( descriptor ) if desc is not None : desc . _value_read . set ( )
13713	def run ( self ) : self . log . debug ( 'consumer is running...' ) self . running = True while self . running : self . upload ( ) self . log . debug ( 'consumer exited.' )
1689	def InTemplateArgumentList ( self , clean_lines , linenum , pos ) : while linenum < clean_lines . NumLines ( ) : # Find the earliest character that might indicate a template argument line = clean_lines . elided [ linenum ] match = Match ( r'^[^{};=\[\]\.<>]*(.)' , line [ pos : ] ) if not match : linenum += 1 pos = 0 continue token = match . group ( 1 ) pos += len ( match . group ( 0 ) ) # These things do not look like template argument list: # class Suspect { # class Suspect x; } if token in ( '{' , '}' , ';' ) : return False # These things look like template argument list: # template <class Suspect> # template <class Suspect = default_value> # template <class Suspect[]> # template <class Suspect...> if token in ( '>' , '=' , '[' , ']' , '.' ) : return True # Check if token is an unmatched '<'. # If not, move on to the next character. if token != '<' : pos += 1 if pos >= len ( line ) : linenum += 1 pos = 0 continue # We can't be sure if we just find a single '<', and need to # find the matching '>'. ( _ , end_line , end_pos ) = CloseExpression ( clean_lines , linenum , pos - 1 ) if end_pos < 0 : # Not sure if template argument list or syntax error in file return False linenum = end_line pos = end_pos return False
13864	def tsms ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) ) * 1000 + int ( round ( when . microsecond / 1000.0 ) )
4855	def _update_transmissions ( self , content_metadata_item_map , transmission_map ) : for content_id , channel_metadata in content_metadata_item_map . items ( ) : transmission = transmission_map [ content_id ] transmission . channel_metadata = channel_metadata transmission . save ( )
7685	def clicks ( annotation , sr = 22050 , length = None , * * kwargs ) : interval , _ = annotation . to_interval_values ( ) return filter_kwargs ( mir_eval . sonify . clicks , interval [ : , 0 ] , fs = sr , length = length , * * kwargs )
13604	def system ( self , cmd , fake_code = False ) : try : if self . options . dry_run : def fake_system ( cmd ) : self . print_message ( cmd ) return fake_code return fake_system ( cmd ) except AttributeError : self . logger . warnning ( "fake mode enabled," "but you don't set '--dry-run' option " "in your argparser options" ) pass return os . system ( cmd )
810	def _finishLearning ( self ) : if self . _doSphering : self . _finishSphering ( ) self . _knn . finishLearning ( ) # Compute leave-one-out validation accuracy if # we actually received non-trivial partition info self . _accuracy = None
13768	def collect_files ( self ) : self . files = [ ] for bundle in self . bundles : bundle . init_build ( self , self . builder ) bundle_files = bundle . prepare ( ) self . files . extend ( bundle_files ) return self
5489	def from_file ( cls , file ) : if not os . path . exists ( file ) : raise ValueError ( "Config file not found." ) try : config_parser = configparser . ConfigParser ( ) config_parser . read ( file ) configuration = cls ( file , config_parser ) if not configuration . check_config_sanity ( ) : raise ValueError ( "Error in config file." ) else : return configuration except configparser . Error : raise ValueError ( "Config file is invalid." )
13745	def get_table ( self ) : if hasattr ( self , '_table' ) : table = self . _table else : try : table = self . conn . get_table ( self . get_table_name ( ) ) except boto . exception . DynamoDBResponseError : if self . auto_create_table : table = self . create_table ( ) else : raise self . _table = table return table
8334	def findAllPrevious ( self , name = None , attrs = { } , text = None , limit = None , * * kwargs ) : return self . _findAll ( name , attrs , text , limit , self . previousGenerator , * * kwargs )
6987	def _varfeatures_worker ( task ) : try : ( lcfile , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) = task return get_varfeatures ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , mindet = mindet , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
7614	def get_deck_link ( self , deck : BaseAttrDict ) : deck_link = 'https://link.clashroyale.com/deck/en?deck=' for i in deck : card = self . get_card_info ( i . name ) deck_link += '{0.id};' . format ( card ) return deck_link
1157	def acquire ( self , blocking = 1 ) : me = _get_ident ( ) if self . __owner == me : self . __count = self . __count + 1 if __debug__ : self . _note ( "%s.acquire(%s): recursive success" , self , blocking ) return 1 rc = self . __block . acquire ( blocking ) if rc : self . __owner = me self . __count = 1 if __debug__ : self . _note ( "%s.acquire(%s): initial success" , self , blocking ) else : if __debug__ : self . _note ( "%s.acquire(%s): failure" , self , blocking ) return rc
5747	def date_asn_block ( self , ip , announce_date = None ) : assignations , announce_date , keys = self . run ( ip , announce_date ) pos = next ( ( i for i , j in enumerate ( assignations ) if j is not None ) , None ) if pos is not None : block = keys [ pos ] if block != '0.0.0.0/0' : return announce_date , assignations [ pos ] , block return None
2122	def associate_failure_node ( self , parent , child = None , * * kwargs ) : return self . _assoc_or_create ( 'failure' , parent , child , * * kwargs )
7231	def create_from_wkt ( self , wkt , item_type , ingest_source , * * attributes ) : # verify the "depth" of the attributes is single layer geojson = load_wkt ( wkt ) . __geo_interface__ vector = { 'type' : "Feature" , 'geometry' : geojson , 'properties' : { 'item_type' : item_type , 'ingest_source' : ingest_source , 'attributes' : attributes } } return self . create ( vector ) [ 0 ]
8336	def findPreviousSiblings ( self , name = None , attrs = { } , text = None , limit = None , * * kwargs ) : return self . _findAll ( name , attrs , text , limit , self . previousSiblingGenerator , * * kwargs )
4793	def is_subset_of ( self , * supersets ) : if not isinstance ( self . val , Iterable ) : raise TypeError ( 'val is not iterable' ) if len ( supersets ) == 0 : raise ValueError ( 'one or more superset args must be given' ) missing = [ ] if hasattr ( self . val , 'keys' ) and callable ( getattr ( self . val , 'keys' ) ) and hasattr ( self . val , '__getitem__' ) : # flatten superset dicts superdict = { } for l , j in enumerate ( supersets ) : self . _check_dict_like ( j , check_values = False , name = 'arg #%d' % ( l + 1 ) ) for k in j . keys ( ) : superdict . update ( { k : j [ k ] } ) for i in self . val . keys ( ) : if i not in superdict : missing . append ( { i : self . val [ i ] } ) # bad key elif self . val [ i ] != superdict [ i ] : missing . append ( { i : self . val [ i ] } ) # bad val if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superdict ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) else : # flatten supersets superset = set ( ) for j in supersets : try : for k in j : superset . add ( k ) except Exception : superset . add ( j ) for i in self . val : if i not in superset : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superset ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) return self
2268	def to_dict ( self ) : return self . _base ( ( key , ( value . to_dict ( ) if isinstance ( value , AutoDict ) else value ) ) for key , value in self . items ( ) )
3157	def create ( self , list_id , data ) : return self . _mc_client . _post ( url = self . _build_path ( list_id , 'segments' ) , data = data )
9332	def empty_like ( array , dtype = None ) : array = numpy . asarray ( array ) if dtype is None : dtype = array . dtype return anonymousmemmap ( array . shape , dtype )
3484	def _create_bound ( model , reaction , bound_type , f_replace , units = None , flux_udef = None ) : value = getattr ( reaction , bound_type ) if value == config . lower_bound : return LOWER_BOUND_ID elif value == 0 : return ZERO_BOUND_ID elif value == config . upper_bound : return UPPER_BOUND_ID elif value == - float ( "Inf" ) : return BOUND_MINUS_INF elif value == float ( "Inf" ) : return BOUND_PLUS_INF else : # new parameter rid = reaction . id if f_replace and F_REACTION_REV in f_replace : rid = f_replace [ F_REACTION_REV ] ( rid ) pid = rid + "_" + bound_type _create_parameter ( model , pid = pid , value = value , sbo = SBO_FLUX_BOUND , units = units , flux_udef = flux_udef ) return pid
1477	def _get_instance_plans ( self , packing_plan , container_id ) : this_container_plan = None for container_plan in packing_plan . container_plans : if container_plan . id == container_id : this_container_plan = container_plan # When the executor runs in newly added container by `heron update`, # there is no plan for this container. In this situation, # return None to bypass instance processes. if this_container_plan is None : return None return this_container_plan . instance_plans
6839	def distrib_release ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : return run ( 'lsb_release -r --short' ) elif kernel == SUNOS : return run ( 'uname -v' )
1542	def queries_map ( ) : qs = _all_metric_queries ( ) return dict ( zip ( qs [ 0 ] , qs [ 1 ] ) + zip ( qs [ 2 ] , qs [ 3 ] ) )
7070	def precision ( ntp , nfp ) : if ( ntp + nfp ) > 0 : return ntp / ( ntp + nfp ) else : return np . nan
8879	def fit ( self , X , y = None ) : # Check data X = check_array ( X ) self . tree = BallTree ( X , leaf_size = self . leaf_size , metric = self . metric ) dist_train = self . tree . query ( X , k = 2 ) [ 0 ] if self . threshold == 'auto' : self . threshold_value = 0.5 * sqrt ( var ( dist_train [ : , 1 ] ) ) + mean ( dist_train [ : , 1 ] ) elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) data_test = safe_indexing ( dist_train [ : , 1 ] , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) AD . append ( data_test ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
7881	def _split_qname ( self , name , is_element ) : if name . startswith ( u"{" ) : namespace , name = name [ 1 : ] . split ( u"}" , 1 ) if namespace in STANZA_NAMESPACES : namespace = self . stanza_namespace elif is_element : raise ValueError ( u"Element with no namespace: {0!r}" . format ( name ) ) else : namespace = None return namespace , name
6786	def get_component_funcs ( self , components = None ) : current_tp = self . get_current_thumbprint ( components = components ) or { } previous_tp = self . get_previous_thumbprint ( components = components ) or { } if self . verbose : print ( 'Current thumbprint:' ) pprint ( current_tp , indent = 4 ) print ( 'Previous thumbprint:' ) pprint ( previous_tp , indent = 4 ) differences = list ( iter_dict_differences ( current_tp , previous_tp ) ) if self . verbose : print ( 'Differences:' ) pprint ( differences , indent = 4 ) component_order = get_component_order ( [ k for k , ( _ , _ ) in differences ] ) if self . verbose : print ( 'component_order:' ) pprint ( component_order , indent = 4 ) plan_funcs = list ( get_deploy_funcs ( component_order , current_tp , previous_tp ) ) return component_order , plan_funcs
10840	def publish ( self ) : url = PATHS [ 'PUBLISH' ] % self . id return self . api . post ( url = url )
4560	def stop ( self = None ) : if not self : instance = getattr ( Runner . instance ( ) , 'builder' , None ) self = instance and instance ( ) if not self : return self . _runner . stop ( ) if self . project : self . project . stop ( ) self . project = None
3192	def create ( self , list_id , data ) : self . list_id = list_id if 'status' not in data : raise KeyError ( 'The list member must have a status' ) if data [ 'status' ] not in [ 'subscribed' , 'unsubscribed' , 'cleaned' , 'pending' , 'transactional' ] : raise ValueError ( 'The list member status must be one of "subscribed", "unsubscribed", "cleaned", ' '"pending", or "transactional"' ) if 'email_address' not in data : raise KeyError ( 'The list member must have an email_address' ) check_email ( data [ 'email_address' ] ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' ) , data = data ) if response is not None : self . subscriber_hash = response [ 'id' ] else : self . subscriber_hash = None return response
2264	def dict_union ( * args ) : if not args : return { } else : dictclass = OrderedDict if isinstance ( args [ 0 ] , OrderedDict ) else dict return dictclass ( it . chain . from_iterable ( d . items ( ) for d in args ) )
4952	def get_no_record_response ( self , request ) : username , course_id , program_uuid , enterprise_customer_uuid = self . get_required_query_params ( request ) data = { self . REQUIRED_PARAM_USERNAME : username , self . REQUIRED_PARAM_ENTERPRISE_CUSTOMER : enterprise_customer_uuid , self . CONSENT_EXISTS : False , self . CONSENT_GRANTED : False , self . CONSENT_REQUIRED : False , } if course_id : data [ self . REQUIRED_PARAM_COURSE_ID ] = course_id if program_uuid : data [ self . REQUIRED_PARAM_PROGRAM_UUID ] = program_uuid return Response ( data , status = HTTP_200_OK )
5517	def limit ( self , value ) : self . _limit = value self . _start = None self . _sum = 0
3880	async def _handle_watermark_notification ( self , watermark_notification ) : conv_id = watermark_notification . conversation_id . id res = parsers . parse_watermark_notification ( watermark_notification ) await self . on_watermark_notification . fire ( res ) try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for watermark notification: %s' , conv_id ) else : await conv . on_watermark_notification . fire ( res )
13798	def handle ( self ) : while True : try : line = self . rfile . readline ( ) try : # All input data are lines of JSON like the following: # ["<cmd_name>" "<cmd_arg1>" "<cmd_arg2>" ...] # So I handle this by dispatching to various methods. cmd = json . loads ( line ) except Exception , exc : # Sometimes errors come up. Once again, I can't predict # anything, but can at least tell CouchDB about the error. self . wfile . write ( repr ( exc ) + NEWLINE ) continue else : # Automagically get the command handler. handler = getattr ( self , 'handle_' + cmd [ 0 ] , None ) if not handler : # We are ready to not find commands. It probably won't # happen, but fortune favours the prepared. self . wfile . write ( repr ( CommandNotFound ( cmd [ 0 ] ) ) + NEWLINE ) continue return_value = handler ( * cmd [ 1 : ] ) if not return_value : continue # We write the output back to CouchDB. self . wfile . write ( one_lineify ( json . dumps ( return_value ) ) + NEWLINE ) except Exception , exc : self . wfile . write ( repr ( exc ) + NEWLINE ) continue
7910	def __presence_unavailable ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_unavailable_presence ( MucPresence ( stanza ) ) return True
4645	def delete ( self , key ) : query = ( "DELETE FROM {} WHERE {}=?" . format ( self . __tablename__ , self . __key__ ) , ( key , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) connection . commit ( )
2419	def write_extracted_licenses ( lics , out ) : write_value ( 'LicenseID' , lics . identifier , out ) if lics . full_name is not None : write_value ( 'LicenseName' , lics . full_name , out ) if lics . comment is not None : write_text_value ( 'LicenseComment' , lics . comment , out ) for xref in sorted ( lics . cross_ref ) : write_value ( 'LicenseCrossReference' , xref , out ) write_text_value ( 'ExtractedText' , lics . text , out )
5434	def parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) : job_params = [ ] for col in header : # Reserve the "-" and "--" namespace. # If the column has no leading "-", treat it as an environment variable col_type = '--env' col_value = col if col . startswith ( '-' ) : col_type , col_value = split_pair ( col , ' ' , 1 ) if col_type == '--env' : job_params . append ( job_model . EnvParam ( col_value ) ) elif col_type == '--label' : job_params . append ( job_model . LabelParam ( col_value ) ) elif col_type == '--input' or col_type == '--input-recursive' : name = input_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . InputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) elif col_type == '--output' or col_type == '--output-recursive' : name = output_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . OutputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) else : raise ValueError ( 'Unrecognized column header: %s' % col ) return job_params
9373	def is_valid_url ( url ) : regex = re . compile ( r'^(?:http|ftp)s?://' r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+(?:[A-Z]{2,6}\.?|[A-Z0-9-]{2,}\.?)|' r'localhost|' r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})' r'(?::\d+)?' r'(?:/?|[/?]\S+)$' , re . IGNORECASE ) if regex . match ( url ) : logger . info ( "URL given as config" ) return True else : return False
2142	def process_extra_vars ( extra_vars_list , force_json = True ) : # Read from all the different sources and put into dictionary extra_vars = { } extra_vars_yaml = "" for extra_vars_opt in extra_vars_list : # Load file content if necessary if extra_vars_opt . startswith ( "@" ) : with open ( extra_vars_opt [ 1 : ] , 'r' ) as f : extra_vars_opt = f . read ( ) # Convert text markup to a dictionary conservatively opt_dict = string_to_dict ( extra_vars_opt , allow_kv = False ) else : # Convert text markup to a dictionary liberally opt_dict = string_to_dict ( extra_vars_opt , allow_kv = True ) # Rolling YAML-based string combination if any ( line . startswith ( "#" ) for line in extra_vars_opt . split ( '\n' ) ) : extra_vars_yaml += extra_vars_opt + "\n" elif extra_vars_opt != "" : extra_vars_yaml += yaml . dump ( opt_dict , default_flow_style = False ) + "\n" # Combine dictionary with cumulative dictionary extra_vars . update ( opt_dict ) # Return contents in form of a string if not force_json : try : # Conditions to verify it is safe to return rolling YAML string try_dict = yaml . load ( extra_vars_yaml , Loader = yaml . SafeLoader ) assert type ( try_dict ) is dict debug . log ( 'Using unprocessed YAML' , header = 'decision' , nl = 2 ) return extra_vars_yaml . rstrip ( ) except Exception : debug . log ( 'Failed YAML parsing, defaulting to JSON' , header = 'decison' , nl = 2 ) if extra_vars == { } : return "" return json . dumps ( extra_vars , ensure_ascii = False )
10760	def from_rectilinear ( cls , x , y , z , formatter = numpy_formatter ) : x = np . asarray ( x , dtype = np . float64 ) y = np . asarray ( y , dtype = np . float64 ) z = np . ma . asarray ( z , dtype = np . float64 ) # Check arguments. if x . ndim != 1 : raise TypeError ( "'x' must be a 1D array but is a {:d}D array" . format ( x . ndim ) ) if y . ndim != 1 : raise TypeError ( "'y' must be a 1D array but is a {:d}D array" . format ( y . ndim ) ) if z . ndim != 2 : raise TypeError ( "'z' must be a 2D array but it a {:d}D array" . format ( z . ndim ) ) if x . size != z . shape [ 1 ] : raise TypeError ( ( "the length of 'x' must be equal to the number of columns in " "'z' but the length of 'x' is {:d} and 'z' has {:d} " "columns" ) . format ( x . size , z . shape [ 1 ] ) ) if y . size != z . shape [ 0 ] : raise TypeError ( ( "the length of 'y' must be equal to the number of rows in " "'z' but the length of 'y' is {:d} and 'z' has {:d} " "rows" ) . format ( y . size , z . shape [ 0 ] ) ) # Convert to curvilinear format and call constructor. y , x = np . meshgrid ( y , x , indexing = 'ij' ) return cls ( x , y , z , formatter )
457	def _add_notice_to_docstring ( doc , no_doc_str , notice ) : if not doc : lines = [ no_doc_str ] else : lines = _normalize_docstring ( doc ) . splitlines ( ) notice = [ '' ] + notice if len ( lines ) > 1 : # Make sure that we keep our distance from the main body if lines [ 1 ] . strip ( ) : notice . append ( '' ) lines [ 1 : 1 ] = notice else : lines += notice return '\n' . join ( lines )
7769	def _stream_disconnected ( self , event ) : with self . lock : if event . stream != self . stream : return if self . stream is not None and event . stream == self . stream : if self . stream . transport in self . _ml_handlers : self . _ml_handlers . remove ( self . stream . transport ) self . main_loop . remove_handler ( self . stream . transport ) self . stream = None self . uplink = None
9687	def read_gsc_sfr ( self ) : config = [ ] data = { } # Send the command byte and sleep for 10 ms self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) # Read the config variables by sending 256 empty bytes for i in range ( 8 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) data [ "GSC" ] = self . _calculate_float ( config [ 0 : 4 ] ) data [ "SFR" ] = self . _calculate_float ( config [ 4 : ] ) return data
12498	def xfm_atlas_to_functional ( atlas_filepath , anatbrain_filepath , meanfunc_filepath , atlas2anat_nonlin_xfm_filepath , is_atlas2anat_inverted , anat2func_lin_xfm_filepath , atlasinanat_out_filepath , atlasinfunc_out_filepath , interp = 'nn' , rewrite = True , parallel = False ) : if is_atlas2anat_inverted : # I already have the inverted fields I need anat_to_mni_nl_inv = atlas2anat_nonlin_xfm_filepath else : # I am creating the inverted fields then...need output file path: output_dir = op . abspath ( op . dirname ( atlasinanat_out_filepath ) ) ext = get_extension ( atlas2anat_nonlin_xfm_filepath ) anat_to_mni_nl_inv = op . join ( output_dir , remove_ext ( op . basename ( atlas2anat_nonlin_xfm_filepath ) ) + '_inv' + ext ) # setup the commands to be called invwarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'invwarp' ) applywarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'applywarp' ) fslsub_cmd = op . join ( '${FSLDIR}' , 'bin' , 'fsl_sub' ) # add fsl_sub before the commands if parallel : invwarp_cmd = fslsub_cmd + ' ' + invwarp_cmd applywarp_cmd = fslsub_cmd + ' ' + applywarp_cmd # create the inverse fields if rewrite or ( not is_atlas2anat_inverted and not op . exists ( anat_to_mni_nl_inv ) ) : log . debug ( 'Creating {}.\n' . format ( anat_to_mni_nl_inv ) ) cmd = invwarp_cmd + ' ' cmd += '-w {} ' . format ( atlas2anat_nonlin_xfm_filepath ) cmd += '-o {} ' . format ( anat_to_mni_nl_inv ) cmd += '-r {} ' . format ( anatbrain_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) # transform the atlas to anatomical space if rewrite or not op . exists ( atlasinanat_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinanat_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlas_filepath ) cmd += '--ref={} ' . format ( anatbrain_filepath ) cmd += '--warp={} ' . format ( anat_to_mni_nl_inv ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinanat_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) # transform the atlas to functional space if rewrite or not op . exists ( atlasinfunc_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinfunc_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlasinanat_out_filepath ) cmd += '--ref={} ' . format ( meanfunc_filepath ) cmd += '--premat={} ' . format ( anat2func_lin_xfm_filepath ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinfunc_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd )
4460	def geo ( lat , lon , radius , unit = 'km' ) : return GeoValue ( lat , lon , radius , unit )
9576	def read_header ( fd , endian ) : flag_class , nzmax = read_elements ( fd , endian , [ 'miUINT32' ] ) header = { 'mclass' : flag_class & 0x0FF , 'is_logical' : ( flag_class >> 9 & 1 ) == 1 , 'is_global' : ( flag_class >> 10 & 1 ) == 1 , 'is_complex' : ( flag_class >> 11 & 1 ) == 1 , 'nzmax' : nzmax } header [ 'dims' ] = read_elements ( fd , endian , [ 'miINT32' ] ) header [ 'n_dims' ] = len ( header [ 'dims' ] ) if header [ 'n_dims' ] != 2 : raise ParseError ( 'Only matrices with dimension 2 are supported.' ) header [ 'name' ] = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) return header
6689	def groupuninstall ( group , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , str ) : options = [ options ] options = " " . join ( options ) run_as_root ( '%(manager)s %(options)s groupremove "%(group)s"' % locals ( ) )
4293	def _manage_args ( parser , args ) : for item in data . CONFIGURABLE_OPTIONS : action = parser . _option_string_actions [ item ] choices = default = '' input_value = getattr ( args , action . dest ) new_val = None # cannot count this until we find a way to test input if not args . noinput : # pragma: no cover if action . choices : choices = ' (choices: {0})' . format ( ', ' . join ( action . choices ) ) if input_value : if type ( input_value ) == list : default = ' [default {0}]' . format ( ', ' . join ( input_value ) ) else : default = ' [default {0}]' . format ( input_value ) while not new_val : prompt = '{0}{1}{2}: ' . format ( action . help , choices , default ) if action . choices in ( 'yes' , 'no' ) : new_val = utils . query_yes_no ( prompt ) else : new_val = compat . input ( prompt ) new_val = compat . clean ( new_val ) if not new_val and input_value : new_val = input_value if new_val and action . dest == 'templates' : if new_val != 'no' and not os . path . isdir ( new_val ) : sys . stdout . write ( 'Given directory does not exists, retry\n' ) new_val = False if new_val and action . dest == 'db' : action ( parser , args , new_val , action . option_strings ) new_val = getattr ( args , action . dest ) else : if not input_value and action . required : raise ValueError ( 'Option {0} is required when in no-input mode' . format ( action . dest ) ) new_val = input_value if action . dest == 'db' : action ( parser , args , new_val , action . option_strings ) new_val = getattr ( args , action . dest ) if action . dest == 'templates' and ( new_val == 'no' or not os . path . isdir ( new_val ) ) : new_val = False if action . dest in ( 'bootstrap' , 'starting_page' ) : new_val = ( new_val == 'yes' ) setattr ( args , action . dest , new_val ) return args
5	def clear_mpi_env_vars ( ) : removed_environment = { } for k , v in list ( os . environ . items ( ) ) : for prefix in [ 'OMPI_' , 'PMI_' ] : if k . startswith ( prefix ) : removed_environment [ k ] = v del os . environ [ k ] try : yield finally : os . environ . update ( removed_environment )
926	def generateDataset ( aggregationInfo , inputFilename , outputFilename = None ) : # Create the input stream inputFullPath = resource_filename ( "nupic.datafiles" , inputFilename ) inputObj = FileRecordStream ( inputFullPath ) # Instantiate the aggregator aggregator = Aggregator ( aggregationInfo = aggregationInfo , inputFields = inputObj . getFields ( ) ) # Is it a null aggregation? If so, just return the input file unmodified if aggregator . isNullAggregation ( ) : return inputFullPath # ------------------------------------------------------------------------ # If we were not given an output filename, create one based on the # aggregation settings if outputFilename is None : outputFilename = 'agg_%s' % os . path . splitext ( os . path . basename ( inputFullPath ) ) [ 0 ] timePeriods = 'years months weeks days ' 'hours minutes seconds milliseconds microseconds' for k in timePeriods . split ( ) : if aggregationInfo . get ( k , 0 ) > 0 : outputFilename += '_%s_%d' % ( k , aggregationInfo [ k ] ) outputFilename += '.csv' outputFilename = os . path . join ( os . path . dirname ( inputFullPath ) , outputFilename ) # ------------------------------------------------------------------------ # If some other process already started creating this file, simply # wait for it to finish and return without doing anything lockFilePath = outputFilename + '.please_wait' if os . path . isfile ( outputFilename ) or os . path . isfile ( lockFilePath ) : while os . path . isfile ( lockFilePath ) : print 'Waiting for %s to be fully written by another process' % lockFilePath time . sleep ( 1 ) return outputFilename # Create the lock file lockFD = open ( lockFilePath , 'w' ) # ------------------------------------------------------------------------- # Create the output stream outputObj = FileRecordStream ( streamID = outputFilename , write = True , fields = inputObj . getFields ( ) ) # ------------------------------------------------------------------------- # Write all aggregated records to the output while True : inRecord = inputObj . getNextRecord ( ) ( aggRecord , aggBookmark ) = aggregator . next ( inRecord , None ) if aggRecord is None and inRecord is None : break if aggRecord is not None : outputObj . appendRecord ( aggRecord ) return outputFilename
3694	def Tm ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ ] ) : def list_methods ( ) : methods = [ ] if CASRN in Tm_ON_data . index : methods . append ( OPEN_NTBKM ) if CASRN in CRC_inorganic_data . index and not np . isnan ( CRC_inorganic_data . at [ CASRN , 'Tm' ] ) : methods . append ( CRC_INORG ) if CASRN in CRC_organic_data . index and not np . isnan ( CRC_organic_data . at [ CASRN , 'Tm' ] ) : methods . append ( CRC_ORG ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == OPEN_NTBKM : return float ( Tm_ON_data . at [ CASRN , 'Tm' ] ) elif Method == CRC_INORG : return float ( CRC_inorganic_data . at [ CASRN , 'Tm' ] ) elif Method == CRC_ORG : return float ( CRC_organic_data . at [ CASRN , 'Tm' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
5756	def _strip_version_suffix ( version ) : global version_regex if not version : return version match = version_regex . search ( version ) return match . group ( 0 ) if match else version
11953	def _parse_dumb_push_output ( self , string ) : stack = 0 json_list = [ ] tmp_json = '' for char in string : if not char == '\r' and not char == '\n' : tmp_json += char if char == '{' : stack += 1 elif char == '}' : stack -= 1 if stack == 0 : if not len ( tmp_json ) == 0 : json_list . append ( tmp_json ) tmp_json = '' return json_list
11874	def put ( xy , * args ) : cmd = [ TermCursor . save , TermCursor . move ( * xy ) , '' . join ( args ) , TermCursor . restore ] write ( '' . join ( cmd ) )
2839	def pullup ( self , pin , enabled ) : self . _validate_pin ( pin ) if enabled : self . gppu [ int ( pin / 8 ) ] |= 1 << ( int ( pin % 8 ) ) else : self . gppu [ int ( pin / 8 ) ] &= ~ ( 1 << ( int ( pin % 8 ) ) ) self . write_gppu ( )
8238	def analogous ( clr , angle = 10 , contrast = 0.25 ) : contrast = max ( 0 , min ( contrast , 1.0 ) ) clr = color ( clr ) colors = colorlist ( clr ) for i , j in [ ( 1 , 2.2 ) , ( 2 , 1 ) , ( - 1 , - 0.5 ) , ( - 2 , 1 ) ] : c = clr . rotate_ryb ( angle * i ) t = 0.44 - j * 0.1 if clr . brightness - contrast * j < t : c . brightness = t else : c . brightness = clr . brightness - contrast * j c . saturation -= 0.05 colors . append ( c ) return colors
9426	def namelist ( self ) : names = [ ] for member in self . filelist : names . append ( member . filename ) return names
5703	def get_vehicle_hours_by_type ( gtfs , route_type ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT * , SUM(end_time_ds - start_time_ds)/3600 as vehicle_hours_type" " FROM" " (SELECT * FROM day_trips as q1" " INNER JOIN" " (SELECT route_I, type FROM routes) as q2" " ON q1.route_I = q2.route_I" " WHERE type = {route_type}" " AND date = '{day}')" . format ( day = day , route_type = route_type ) ) df = gtfs . execute_custom_query_pandas ( query ) return df [ 'vehicle_hours_type' ] . item ( )
13238	def _daily_periods ( self , range_start , range_end ) : specific = set ( self . exceptions . keys ( ) ) return heapq . merge ( self . exception_periods ( range_start , range_end ) , * [ sched . daily_periods ( range_start = range_start , range_end = range_end , exclude_dates = specific ) for sched in self . _recurring_schedules ] )
9801	def config ( list ) : # pylint:disable=redefined-builtin if list : _config = GlobalConfigManager . get_config_or_default ( ) Printer . print_header ( 'Current config:' ) dict_tabulate ( _config . to_dict ( ) )
5723	def _buffer_incomplete_responses ( raw_output , buf ) : if raw_output : if buf : # concatenate buffer and new output raw_output = b"" . join ( [ buf , raw_output ] ) buf = None if b"\n" not in raw_output : # newline was not found, so assume output is incomplete and store in buffer buf = raw_output raw_output = None elif not raw_output . endswith ( b"\n" ) : # raw output doesn't end in a newline, so store everything after the last newline (if anything) # in the buffer, and parse everything before it remainder_offset = raw_output . rindex ( b"\n" ) + 1 buf = raw_output [ remainder_offset : ] raw_output = raw_output [ : remainder_offset ] return ( raw_output , buf )
5060	def get_enterprise_customer ( uuid ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) # pylint: disable=invalid-name try : return EnterpriseCustomer . objects . get ( uuid = uuid ) # pylint: disable=no-member except EnterpriseCustomer . DoesNotExist : return None
250	def adjust_returns_for_slippage ( returns , positions , transactions , slippage_bps ) : slippage = 0.0001 * slippage_bps portfolio_value = positions . sum ( axis = 1 ) pnl = portfolio_value * returns traded_value = get_txn_vol ( transactions ) . txn_volume slippage_dollars = traded_value * slippage adjusted_pnl = pnl . add ( - slippage_dollars , fill_value = 0 ) adjusted_returns = returns * adjusted_pnl / pnl return adjusted_returns
13267	def gml_to_geojson ( el ) : if el . get ( 'srsName' ) not in ( 'urn:ogc:def:crs:EPSG::4326' , None ) : if el . get ( 'srsName' ) == 'EPSG:4326' : return _gmlv2_to_geojson ( el ) else : raise NotImplementedError ( "Unrecognized srsName %s" % el . get ( 'srsName' ) ) tag = el . tag . replace ( '{%s}' % NS_GML , '' ) if tag == 'Point' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}pos' % NS_GML ) ) [ 0 ] elif tag == 'LineString' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}posList' % NS_GML ) ) elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:exterior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) + el . xpath ( 'gml:interior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) : coordinates . append ( _reverse_gml_coords ( ring . text ) ) elif tag in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' ) : single_type = tag [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' coordinates = [ gml_to_geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member_tag , single_type ) , namespaces = NSMAP ) ] else : raise NotImplementedError return { 'type' : tag , 'coordinates' : coordinates }
3588	def cbuuid_to_uuid ( cbuuid ) : data = cbuuid . data ( ) . bytes ( ) template = '{:0>8}-0000-1000-8000-00805f9b34fb' if len ( data ) <= 4 else '{:0>32}' value = template . format ( hexlify ( data . tobytes ( ) [ : 16 ] ) . decode ( 'ascii' ) ) return uuid . UUID ( hex = value )
3637	def clubStaff ( self ) : method = 'GET' url = 'club/stats/staff' rc = self . __request__ ( method , url ) return rc
7946	def _write ( self , data ) : OUT_LOGGER . debug ( "OUT: %r" , data ) if self . _hup or not self . _socket : raise PyXMPPIOError ( u"Connection closed." ) try : while data : try : sent = self . _socket . send ( data ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : continue else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue if err . args [ 0 ] in BLOCKING_ERRORS : wait_for_write ( self . _socket ) continue raise data = data [ sent : ] except ( IOError , OSError , socket . error ) , err : raise PyXMPPIOError ( u"IO Error: {0}" . format ( err ) )
12682	def row ( self , idx ) : return DataFrameRow ( idx , [ x [ idx ] for x in self ] , self . colnames )
10109	def schema ( tg ) : tables = { } for tname , table in tg . tabledict . items ( ) : t = TableSpec . from_table_metadata ( table ) tables [ t . name ] = t for at in t . many_to_many . values ( ) : tables [ at . name ] = at # We must determine the order in which tables must be created! ordered = OrderedDict ( ) i = 0 # We loop through the tables repeatedly, and whenever we find one, which has all # referenced tables already in ordered, we move it from tables to ordered. while tables and i < 100 : i += 1 for table in list ( tables . keys ( ) ) : if all ( ( ref [ 1 ] in ordered ) or ref [ 1 ] == table for ref in tables [ table ] . foreign_keys ) : # All referenced tables are already created (or self-referential). ordered [ table ] = tables . pop ( table ) break if tables : # pragma: no cover raise ValueError ( 'there seem to be cyclic dependencies between the tables' ) return list ( ordered . values ( ) )
12457	def iteritems ( data , * * kwargs ) : return iter ( data . items ( * * kwargs ) ) if IS_PY3 else data . iteritems ( * * kwargs )
4941	def unlink_user ( self , enterprise_customer , user_email ) : try : existing_user = User . objects . get ( email = user_email ) # not capturing DoesNotExist intentionally to signal to view that link does not exist link_record = self . get ( enterprise_customer = enterprise_customer , user_id = existing_user . id ) link_record . delete ( ) if update_user : # Remove the SailThru flags for enterprise learner. update_user . delay ( sailthru_vars = { 'is_enterprise_learner' : False , 'enterprise_name' : None , } , email = user_email ) except User . DoesNotExist : # not capturing DoesNotExist intentionally to signal to view that link does not exist pending_link = PendingEnterpriseCustomerUser . objects . get ( enterprise_customer = enterprise_customer , user_email = user_email ) pending_link . delete ( ) LOGGER . info ( 'Enterprise learner {%s} successfully unlinked from Enterprise Customer {%s}' , user_email , enterprise_customer . name )
5386	def _format_task_name ( job_id , task_id , task_attempt ) : docker_name = '%s.%s' % ( job_id , 'task' if task_id is None else task_id ) if task_attempt is not None : docker_name += '.' + str ( task_attempt ) # Docker container names must match: [a-zA-Z0-9][a-zA-Z0-9_.-] # So 1) prefix it with "dsub-" and 2) change all invalid characters to "-". return 'dsub-{}' . format ( _convert_suffix_to_docker_chars ( docker_name ) )
11047	def _parse_field_value ( line ) : if line . startswith ( ':' ) : # Ignore the line return None , None if ':' not in line : # Treat the entire line as the field, use empty string as value return line , '' # Else field is before the ':' and value is after field , value = line . split ( ':' , 1 ) # If value starts with a space, remove it. value = value [ 1 : ] if value . startswith ( ' ' ) else value return field , value
8900	def max_parameter_substitution ( ) : if os . path . isfile ( SQLITE_VARIABLE_FILE_CACHE ) : return conn = sqlite3 . connect ( ':memory:' ) low = 1 high = 1000 # hard limit for SQLITE_MAX_VARIABLE_NUMBER <http://www.sqlite.org/limits.html> conn . execute ( 'CREATE TABLE T1 (id C1)' ) while low < high - 1 : guess = ( low + high ) // 2 try : statement = 'select * from T1 where id in (%s)' % ',' . join ( [ '?' for _ in range ( guess ) ] ) values = [ i for i in range ( guess ) ] conn . execute ( statement , values ) except sqlite3 . DatabaseError as ex : if 'too many SQL variables' in str ( ex ) : high = guess else : raise else : low = guess conn . close ( ) with open ( SQLITE_VARIABLE_FILE_CACHE , 'w' ) as file : file . write ( str ( low ) )
9371	def password ( at_least = 6 , at_most = 12 , lowercase = True , uppercase = True , digits = True , spaces = False , punctuation = False ) : return text ( at_least = at_least , at_most = at_most , lowercase = lowercase , uppercase = uppercase , digits = digits , spaces = spaces , punctuation = punctuation )
3593	def getHeaders ( self , upload_fields = False ) : if upload_fields : headers = self . deviceBuilder . getDeviceUploadHeaders ( ) else : headers = self . deviceBuilder . getBaseHeaders ( ) if self . gsfId is not None : headers [ "X-DFE-Device-Id" ] = "{0:x}" . format ( self . gsfId ) if self . authSubToken is not None : headers [ "Authorization" ] = "GoogleLogin auth=%s" % self . authSubToken if self . device_config_token is not None : headers [ "X-DFE-Device-Config-Token" ] = self . device_config_token if self . deviceCheckinConsistencyToken is not None : headers [ "X-DFE-Device-Checkin-Consistency-Token" ] = self . deviceCheckinConsistencyToken if self . dfeCookie is not None : headers [ "X-DFE-Cookie" ] = self . dfeCookie return headers
13579	def determine_type ( x ) : types = ( int , float , str ) _type = filter ( lambda a : is_type ( a , x ) , types ) [ 0 ] return _type ( x )
11033	def parse ( self , value : str , type_ : typing . Type [ typing . Any ] = str , subtype : typing . Type [ typing . Any ] = str , ) -> typing . Any : if type_ is bool : return type_ ( value . lower ( ) in self . TRUE_STRINGS ) try : if isinstance ( type_ , type ) and issubclass ( type_ , ( list , tuple , set , frozenset ) ) : return type_ ( self . parse ( v . strip ( " " ) , subtype ) for v in value . split ( "," ) if value . strip ( " " ) ) return type_ ( value ) except ValueError as e : raise ConfigError ( * e . args )
11402	def create_records ( marcxml , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , parser = '' , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : # Use the DOTALL flag to include newlines. regex = re . compile ( '<record.*?>.*?</record>' , re . DOTALL ) record_xmls = regex . findall ( marcxml ) return [ create_record ( record_xml , verbose = verbose , correct = correct , parser = parser , keep_singletons = keep_singletons ) for record_xml in record_xmls ]
7968	def _run_io_threads ( self , handler ) : reader = ReadingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) writter = WrittingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) self . io_threads += [ reader , writter ] reader . start ( ) writter . start ( )
6584	def station_selection_menu ( self , error = None ) : self . screen . clear ( ) if error : self . screen . print_error ( "{}\n" . format ( error ) ) for i , station in enumerate ( self . stations ) : i = "{:>3}" . format ( i ) print ( "{}: {}" . format ( Colors . yellow ( i ) , station . name ) ) return self . stations [ self . screen . get_integer ( "Station: " ) ]
2437	def add_review_date ( self , doc , reviewed ) : if len ( doc . reviews ) != 0 : if not self . review_date_set : self . review_date_set = True date = utils . datetime_from_iso_format ( reviewed ) if date is not None : doc . reviews [ - 1 ] . review_date = date return True else : raise SPDXValueError ( 'Review::ReviewDate' ) else : raise CardinalityError ( 'Review::ReviewDate' ) else : raise OrderError ( 'Review::ReviewDate' )
9625	def register ( self , cls ) : preview = cls ( site = self ) logger . debug ( 'Registering %r with %r' , preview , self ) index = self . __previews . setdefault ( preview . module , { } ) index [ cls . __name__ ] = preview
2209	def parse_requirements_alt ( fname = 'requirements.txt' ) : import requirements from os . path import dirname , join , exists require_fpath = join ( dirname ( __file__ ) , fname ) if exists ( require_fpath ) : # Dont use until this handles platform specific dependencies with open ( require_fpath , 'r' ) as file : requires = list ( requirements . parse ( file ) ) packages = [ r . name for r in requires ] return packages return [ ]
3520	def snapengage ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return SnapEngageNode ( )
269	def detect_intraday ( positions , transactions , threshold = 0.25 ) : daily_txn = transactions . copy ( ) daily_txn . index = daily_txn . index . date txn_count = daily_txn . groupby ( level = 0 ) . symbol . nunique ( ) . sum ( ) daily_pos = positions . drop ( 'cash' , axis = 1 ) . replace ( 0 , np . nan ) return daily_pos . count ( axis = 1 ) . sum ( ) / txn_count < threshold
10553	def create_helpingmaterial ( project_id , info , media_url = None , file_path = None ) : try : helping = dict ( project_id = project_id , info = info , media_url = None , ) if file_path : files = { 'file' : open ( file_path , 'rb' ) } payload = { 'project_id' : project_id } res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = payload , files = files ) else : res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = helping ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : # pragma: no cover raise
10718	def normalize_unitnumber ( unit_number ) : try : try : unit_number = int ( unit_number ) except ValueError : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) except TypeError : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) if not ( 1 <= unit_number <= 16 ) : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) return unit_number
5768	def _advapi32_interpret_dsa_key_blob ( bit_size , public_blob , private_blob ) : len1 = 20 len2 = bit_size // 8 q_offset = len2 g_offset = q_offset + len1 x_offset = g_offset + len2 y_offset = x_offset p = int_from_bytes ( private_blob [ 0 : q_offset ] [ : : - 1 ] ) q = int_from_bytes ( private_blob [ q_offset : g_offset ] [ : : - 1 ] ) g = int_from_bytes ( private_blob [ g_offset : x_offset ] [ : : - 1 ] ) x = int_from_bytes ( private_blob [ x_offset : x_offset + len1 ] [ : : - 1 ] ) y = int_from_bytes ( public_blob [ y_offset : y_offset + len2 ] [ : : - 1 ] ) public_key_info = keys . PublicKeyInfo ( { 'algorithm' : keys . PublicKeyAlgorithm ( { 'algorithm' : 'dsa' , 'parameters' : keys . DSAParams ( { 'p' : p , 'q' : q , 'g' : g , } ) } ) , 'public_key' : core . Integer ( y ) , } ) private_key_info = keys . PrivateKeyInfo ( { 'version' : 0 , 'private_key_algorithm' : keys . PrivateKeyAlgorithm ( { 'algorithm' : 'dsa' , 'parameters' : keys . DSAParams ( { 'p' : p , 'q' : q , 'g' : g , } ) } ) , 'private_key' : core . Integer ( x ) , } ) return ( public_key_info , private_key_info )
844	def _rebuildPartitionIdMap ( self , partitionIdList ) : self . _partitionIdMap = { } for row , partitionId in enumerate ( partitionIdList ) : indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( row ) self . _partitionIdMap [ partitionId ] = indices
618	def parseBool ( s ) : l = s . lower ( ) if l in ( "true" , "t" , "1" ) : return True if l in ( "false" , "f" , "0" ) : return False raise Exception ( "Unable to convert string '%s' to a boolean value" % s )
12421	def dump ( obj , fp , startindex = 1 , separator = DEFAULT , index_separator = DEFAULT ) : if startindex < 0 : raise ValueError ( 'startindex must be non-negative, but was {}' . format ( startindex ) ) try : firstkey = next ( iter ( obj . keys ( ) ) ) except StopIteration : return if isinstance ( firstkey , six . text_type ) : converter = six . u else : converter = six . b default_separator = converter ( '|' ) default_index_separator = converter ( '_' ) newline = converter ( '\n' ) if separator is DEFAULT : separator = default_separator if index_separator is DEFAULT : index_separator = default_index_separator for key , value in six . iteritems ( obj ) : if isinstance ( value , ( list , tuple , set ) ) : for index , item in enumerate ( value , start = startindex ) : fp . write ( key ) fp . write ( index_separator ) fp . write ( converter ( str ( index ) ) ) fp . write ( separator ) fp . write ( item ) fp . write ( newline ) else : fp . write ( key ) fp . write ( separator ) fp . write ( value ) fp . write ( newline )
2964	def insert_rule ( self , chain , src = None , dest = None , target = None ) : if not chain : raise ValueError ( "Invalid chain" ) if not target : raise ValueError ( "Invalid target" ) if not ( src or dest ) : raise ValueError ( "Need src, dest, or both" ) args = [ "-I" , chain ] if src : args += [ "-s" , src ] if dest : args += [ "-d" , dest ] args += [ "-j" , target ] self . call ( * args )
1327	def normalized_distance ( self , image ) : return self . __distance ( self . __original_image_for_distance , image , bounds = self . bounds ( ) )
7292	def set_post_data ( self ) : self . form . data = self . post_data_dict # Specifically adding list field keys to the form so they are included # in form.cleaned_data after the call to is_valid for field_key , field in self . form . fields . items ( ) : if has_digit ( field_key ) : # We have a list field. base_key = make_key ( field_key , exclude_last_string = True ) # Add new key value with field to form fields so validation # will work correctly for key in self . post_data_dict . keys ( ) : if base_key in key : self . form . fields . update ( { key : field } )
3828	async def query_presence ( self , query_presence_request ) : response = hangouts_pb2 . QueryPresenceResponse ( ) await self . _pb_request ( 'presence/querypresence' , query_presence_request , response ) return response
8686	def _decrypt ( self , hexified_value ) : encrypted_value = binascii . unhexlify ( hexified_value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" ) jsonified_value = self . cipher . decrypt ( encrypted_value ) . decode ( 'ascii' ) value = json . loads ( jsonified_value ) return value
1748	def _get_offset ( self , index ) : if not self . _in_range ( index ) : raise IndexError ( 'Map index out of range' ) if isinstance ( index , slice ) : index = slice ( index . start - self . start , index . stop - self . start ) else : index -= self . start return index
4919	def course_run_detail ( self , request , pk , course_id ) : # pylint: disable=invalid-name,unused-argument enterprise_customer_catalog = self . get_object ( ) course_run = enterprise_customer_catalog . get_course_run ( course_id ) if not course_run : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . CourseRunDetailSerializer ( course_run , context = context ) return Response ( serializer . data )
9201	def extract_cycles ( series , left = False , right = False ) : points = deque ( ) for x in reversals ( series , left = left , right = right ) : points . append ( x ) while len ( points ) >= 3 : # Form ranges X and Y from the three most recent points X = abs ( points [ - 2 ] - points [ - 1 ] ) Y = abs ( points [ - 3 ] - points [ - 2 ] ) if X < Y : # Read the next point break elif len ( points ) == 3 : # Y contains the starting point # Count Y as one-half cycle and discard the first point yield points [ 0 ] , points [ 1 ] , 0.5 points . popleft ( ) else : # Count Y as one cycle and discard the peak and the valley of Y yield points [ - 3 ] , points [ - 2 ] , 1.0 last = points . pop ( ) points . pop ( ) points . pop ( ) points . append ( last ) else : # Count the remaining ranges as one-half cycles while len ( points ) > 1 : yield points [ 0 ] , points [ 1 ] , 0.5 points . popleft ( )
9083	def get_by_uri ( self , uri ) : if not is_uri ( uri ) : raise ValueError ( '%s is not a valid URI.' % uri ) # Check if there's a provider that's more likely to have the URI csuris = [ csuri for csuri in self . concept_scheme_uri_map . keys ( ) if uri . startswith ( csuri ) ] for csuri in csuris : c = self . get_provider ( csuri ) . get_by_uri ( uri ) if c : return c # Check all providers for p in self . providers . values ( ) : c = p . get_by_uri ( uri ) if c : return c return False
6269	def on_resize ( self , width , height ) : self . width , self . height = width , height self . buffer_width , self . buffer_height = width , height self . resize ( width , height )
7201	def is_ordered ( cat_id ) : url = 'https://rda.geobigdata.io/v1/stripMetadata/{}' . format ( cat_id ) auth = Auth ( ) r = _req_with_retries ( auth . gbdx_connection , url ) if r is not None : return r . status_code == 200 return False
13282	def _parse_command ( self , source , start_index ) : parsed_elements = [ ] # Index of the parser in the source running_index = start_index for element in self . elements : opening_bracket = element [ 'bracket' ] closing_bracket = self . _brackets [ opening_bracket ] # Find the opening bracket. element_start = None element_end = None for i , c in enumerate ( source [ running_index : ] , start = running_index ) : if c == element [ 'bracket' ] : element_start = i break elif c == '\n' : # No starting bracket on the line. if element [ 'required' ] is True : # Try to parse a single single-word token after the # command, like '\input file' content = self . _parse_whitespace_argument ( source [ running_index : ] , self . name ) return ParsedCommand ( self . name , [ { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : content . strip ( ) } ] , start_index , source [ start_index : i ] ) else : # Give up on finding an optional element break # Handle cases when the opening bracket is never found. if element_start is None and element [ 'required' ] is False : # Optional element not found. Continue to next element, # not advancing the running_index of the parser. continue elif element_start is None and element [ 'required' ] is True : message = ( 'Parsing command {0} at index {1:d}, ' 'did not detect element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) # Find the closing bracket, keeping track of the number of times # the same type of bracket was opened and closed. balance = 1 for i , c in enumerate ( source [ element_start + 1 : ] , start = element_start + 1 ) : if c == opening_bracket : balance += 1 elif c == closing_bracket : balance -= 1 if balance == 0 : element_end = i break if balance > 0 : message = ( 'Parsing command {0} at index {1:d}, ' 'did not find closing bracket for required ' 'command element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) # Package the parsed element's content. element_content = source [ element_start + 1 : element_end ] parsed_element = { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : element_content . strip ( ) } parsed_elements . append ( parsed_element ) running_index = element_end + 1 command_source = source [ start_index : running_index ] parsed_command = ParsedCommand ( self . name , parsed_elements , start_index , command_source ) return parsed_command
1853	def SHR ( cpu , dest , src ) : OperandSize = dest . size count = Operators . ZEXTEND ( src . read ( ) & ( OperandSize - 1 ) , OperandSize ) value = dest . read ( ) res = dest . write ( value >> count ) # UNSIGNED Operators.UDIV2 !! TODO Check MASK = ( 1 << OperandSize ) - 1 SIGN_MASK = 1 << ( OperandSize - 1 ) if issymbolic ( count ) : cpu . CF = Operators . ITE ( count != 0 , ( ( value >> Operators . ZEXTEND ( count - 1 , OperandSize ) ) & 1 ) != 0 , cpu . CF ) else : if count != 0 : cpu . CF = Operators . EXTRACT ( value , count - 1 , 1 ) != 0 cpu . ZF = Operators . ITE ( count != 0 , res == 0 , cpu . ZF ) cpu . SF = Operators . ITE ( count != 0 , ( res & SIGN_MASK ) != 0 , cpu . SF ) # OF is only defined for count == 1, but in practice (unit tests from real cpu) it's calculated for count != 0 cpu . OF = Operators . ITE ( count != 0 , ( ( value >> ( OperandSize - 1 ) ) & 0x1 ) == 1 , cpu . OF ) cpu . PF = Operators . ITE ( count != 0 , cpu . _calculate_parity_flag ( res ) , cpu . PF )
8969	def encryptMessage ( self , message , ad = None ) : if ad == None : ad = self . __ad # Prepare the header for this message header = Header ( self . pub , self . __skr . sending_chain_length , self . __skr . previous_sending_chain_length ) # Encrypt the message ciphertext = self . __aead . encrypt ( message , self . __skr . nextEncryptionKey ( ) , self . _makeAD ( header , ad ) ) return { "header" : header , "ciphertext" : ciphertext }
4553	def set_colors ( self , colors , pos ) : self . _colors = colors self . _pos = pos end = self . _pos + self . numLEDs if end > len ( self . _colors ) : raise ValueError ( 'Needed %d colors but found %d' % ( end , len ( self . _colors ) ) )
1349	def write_error_response ( self , message ) : self . set_status ( 404 ) response = self . make_error_response ( str ( message ) ) now = time . time ( ) spent = now - self . basehandler_starttime response [ constants . RESPONSE_KEY_EXECUTION_TIME ] = spent self . write_json_response ( response )
1680	def SetVerboseLevel ( self , level ) : last_verbose_level = self . verbose_level self . verbose_level = level return last_verbose_level
13517	def resistance ( self ) : self . total_resistance_coef = frictional_resistance_coef ( self . length , self . speed ) + residual_resistance_coef ( self . slenderness_coefficient , self . prismatic_coefficient , froude_number ( self . speed , self . length ) ) RT = 1 / 2 * self . total_resistance_coef * 1025 * self . surface_area * self . speed ** 2 return RT
2478	def reset ( self ) : # FIXME: this state does not make sense self . reset_creation_info ( ) self . reset_document ( ) self . reset_package ( ) self . reset_file_stat ( ) self . reset_reviews ( ) self . reset_annotations ( ) self . reset_extr_lics ( )
2757	def get_all_floating_ips ( self ) : data = self . get_data ( "floating_ips" ) floating_ips = list ( ) for jsoned in data [ 'floating_ips' ] : floating_ip = FloatingIP ( * * jsoned ) floating_ip . token = self . token floating_ips . append ( floating_ip ) return floating_ips
587	def _constructClassificationRecord ( self ) : model = self . htm_prediction_model sp = model . _getSPRegion ( ) tm = model . _getTPRegion ( ) tpImp = tm . getSelf ( ) . _tfdr # Count the number of unpredicted columns activeColumns = sp . getOutputData ( "bottomUpOut" ) . nonzero ( ) [ 0 ] score = numpy . in1d ( activeColumns , self . _prevPredictedColumns ) . sum ( ) score = ( self . _activeColumnCount - score ) / float ( self . _activeColumnCount ) spSize = sp . getParameter ( 'activeOutputCount' ) tpSize = tm . getParameter ( 'cellsPerColumn' ) * tm . getParameter ( 'columnCount' ) classificationVector = numpy . array ( [ ] ) if self . _vectorType == 'tpc' : # Classification Vector: [---TM Cells---] classificationVector = numpy . zeros ( tpSize ) activeCellMatrix = tpImp . getLearnActiveStateT ( ) . reshape ( tpSize , 1 ) activeCellIdx = numpy . where ( activeCellMatrix > 0 ) [ 0 ] if activeCellIdx . shape [ 0 ] > 0 : classificationVector [ numpy . array ( activeCellIdx , dtype = numpy . uint16 ) ] = 1 elif self . _vectorType == 'sp_tpe' : # Classification Vecotr: [---SP---|---(TM-SP)----] classificationVector = numpy . zeros ( spSize + spSize ) if activeColumns . shape [ 0 ] > 0 : classificationVector [ activeColumns ] = 1.0 errorColumns = numpy . setdiff1d ( self . _prevPredictedColumns , activeColumns ) if errorColumns . shape [ 0 ] > 0 : errorColumnIndexes = ( numpy . array ( errorColumns , dtype = numpy . uint16 ) + spSize ) classificationVector [ errorColumnIndexes ] = 1.0 else : raise TypeError ( "Classification vector type must be either 'tpc' or" " 'sp_tpe', current value is %s" % ( self . _vectorType ) ) # Store the state for next time step numPredictedCols = len ( self . _prevPredictedColumns ) predictedColumns = tm . getOutputData ( "topDownOut" ) . nonzero ( ) [ 0 ] self . _prevPredictedColumns = copy . deepcopy ( predictedColumns ) if self . _anomalyVectorLength is None : self . _anomalyVectorLength = len ( classificationVector ) result = _CLAClassificationRecord ( ROWID = int ( model . getParameter ( '__numRunCalls' ) - 1 ) , #__numRunCalls called #at beginning of model.run anomalyScore = score , anomalyVector = classificationVector . nonzero ( ) [ 0 ] . tolist ( ) , anomalyLabel = [ ] ) return result
7886	def emit_stanza ( self , element ) : if not self . _head_emitted : raise RuntimeError ( ".emit_head() must be called first." ) string = self . _emit_element ( element , level = 1 , declared_prefixes = self . _root_prefixes ) return remove_evil_characters ( string )
6974	def rfepd_magseries ( times , mags , errs , externalparam_arrs , magsarefluxes = False , epdsmooth = True , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None , rf_subsample = 1.0 , rf_ntrees = 300 , rf_extraparams = { 'criterion' : 'mse' , 'oob_score' : False , 'n_jobs' : - 1 } ) : # get finite times, mags, errs finind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes , fmags , ferrs = times [ : : ] [ finind ] , mags [ : : ] [ finind ] , errs [ : : ] [ finind ] finalparam_arrs = [ ] for ep in externalparam_arrs : finalparam_arrs . append ( ep [ : : ] [ finind ] ) stimes , smags , serrs , eparams = sigclip_magseries_with_extparams ( times , mags , errs , externalparam_arrs , sigclip = epdsmooth_sigclip , magsarefluxes = magsarefluxes ) # smoothing is optional for RFR because we train on a fraction of the mag # series and so should not require a smoothed input to fit a function to if epdsmooth : # smooth the signal if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize , * * epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize ) else : smoothedmags = smags # set up the regressor if isinstance ( rf_extraparams , dict ) : RFR = RandomForestRegressor ( n_estimators = rf_ntrees , * * rf_extraparams ) else : RFR = RandomForestRegressor ( n_estimators = rf_ntrees ) # collect the features features = np . column_stack ( eparams ) # fit, then generate the predicted values, then get corrected values # we fit on a randomly selected subsample of all the mags if rf_subsample < 1.0 : featureindices = np . arange ( smoothedmags . size ) # these are sorted because time-order should be important training_indices = np . sort ( npr . choice ( featureindices , size = int ( rf_subsample * smoothedmags . size ) , replace = False ) ) else : training_indices = np . arange ( smoothedmags . size ) RFR . fit ( features [ training_indices , : ] , smoothedmags [ training_indices ] ) # predict on the full feature set flux_corrections = RFR . predict ( np . column_stack ( finalparam_arrs ) ) corrected_fmags = npmedian ( fmags ) + fmags - flux_corrections retdict = { 'times' : ftimes , 'mags' : corrected_fmags , 'errs' : ferrs , 'feature_importances' : RFR . feature_importances_ , 'regressor' : RFR , 'mags_median' : npmedian ( corrected_fmags ) , 'mags_mad' : npmedian ( npabs ( corrected_fmags - npmedian ( corrected_fmags ) ) ) } return retdict
9444	def bulk_call ( self , call_params ) : path = '/' + self . api_version + '/BulkCall/' method = 'POST' return self . request ( path , method , call_params )
5526	def grab ( bbox = None , childprocess = None , backend = None ) : if childprocess is None : childprocess = childprocess_default_value ( ) return _grab ( to_file = False , childprocess = childprocess , backend = backend , bbox = bbox )
13028	def exploit ( self ) : search = ServiceSearch ( ) host_search = HostSearch ( ) services = search . get_services ( tags = [ 'MS17-010' ] ) services = [ service for service in services ] if len ( services ) == 0 : print_error ( "No services found that are vulnerable for MS17-010" ) return if self . auto : print_success ( "Found {} services vulnerable for MS17-010" . format ( len ( services ) ) ) for service in services : print_success ( "Exploiting " + str ( service . address ) ) host = host_search . id_to_object ( str ( service . address ) ) system_os = '' if host . os : system_os = host . os else : system_os = self . detect_os ( str ( service . address ) ) host . os = system_os host . save ( ) text = self . exploit_single ( str ( service . address ) , system_os ) print_notification ( text ) else : service_list = [ ] for service in services : host = host_search . id_to_object ( str ( service . address ) ) system_os = '' if host . os : system_os = host . os else : system_os = self . detect_os ( str ( service . address ) ) host . os = system_os host . save ( ) service_list . append ( { 'ip' : service . address , 'os' : system_os , 'string' : "{ip} ({os}) {hostname}" . format ( ip = service . address , os = system_os , hostname = host . hostname ) } ) draw_interface ( service_list , self . callback , "Exploiting {ip} with OS: {os}" )
13464	def add_memory ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) form = MemoryForm ( request . POST or None , request . FILES or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . user = request . user instance . event = event instance . save ( ) msg = "Your thoughts were added. " if request . FILES : photo_list = request . FILES . getlist ( 'photos' ) photo_count = len ( photo_list ) for upload_file in photo_list : process_upload ( upload_file , instance , form , event , request ) if photo_count > 1 : msg += "{} images were added and should appear soon." . format ( photo_count ) else : msg += "{} image was added and should appear soon." . format ( photo_count ) messages . success ( request , msg ) return HttpResponseRedirect ( '../' ) return render ( request , 'happenings/add_memories.html' , { 'form' : form , 'event' : event } )
9270	def get_temp_tag_for_repo_creation ( self ) : tag_date = self . tag_times_dict . get ( REPO_CREATED_TAG_NAME , None ) if not tag_date : tag_name , tag_date = self . fetcher . fetch_repo_creation_date ( ) self . tag_times_dict [ tag_name ] = timestring_to_datetime ( tag_date ) return REPO_CREATED_TAG_NAME
9954	def custom_showwarning ( message , category , filename = "" , lineno = - 1 , file = None , line = None ) : if file is None : file = sys . stderr if file is None : # sys.stderr is None when run with pythonw.exe: # warnings get lost return text = "%s: %s\n" % ( category . __name__ , message ) try : file . write ( text ) except OSError : # the file (probably stderr) is invalid - this warning gets lost. pass
7112	def serve ( self , port = 62000 ) : from http . server import HTTPServer , CGIHTTPRequestHandler os . chdir ( self . log_folder ) httpd = HTTPServer ( ( '' , port ) , CGIHTTPRequestHandler ) print ( "Starting LanguageBoard on port: " + str ( httpd . server_port ) ) webbrowser . open ( 'http://0.0.0.0:{}' . format ( port ) ) httpd . serve_forever ( )
11290	def load_dict ( self , source , namespace = '' ) : for key , value in source . items ( ) : if isinstance ( key , str ) : nskey = ( namespace + '.' + key ) . strip ( '.' ) if isinstance ( value , dict ) : self . load_dict ( value , namespace = nskey ) else : self [ nskey ] = value else : raise TypeError ( 'Key has type %r (not a string)' % type ( key ) ) return self
8120	def intersection ( self , b ) : if not self . intersects ( b ) : return None mx , my = max ( self . x , b . x ) , max ( self . y , b . y ) return Bounds ( mx , my , min ( self . x + self . width , b . x + b . width ) - mx , min ( self . y + self . height , b . y + b . height ) - my )
13911	def new_user ( yaml_path ) : print 'Retrieve API Key from https://www.shirts.io/accounts/api_console/' api_key = raw_input ( 'Shirts.io API Key: ' ) tokens = { 'api_key' : api_key , } yaml_file = open ( yaml_path , 'w+' ) yaml . dump ( tokens , yaml_file , indent = 2 ) yaml_file . close ( ) return tokens
4790	def is_lower ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . lower ( ) : self . _err ( 'Expected <%s> to contain only lowercase chars, but did not.' % self . val ) return self
6722	def list_instances ( show = 1 , name = None , group = None , release = None , except_release = None ) : from burlap . common import shelf , OrderedDict , get_verbose verbose = get_verbose ( ) require ( 'vm_type' , 'vm_group' ) assert env . vm_type , 'No VM type specified.' env . vm_type = ( env . vm_type or '' ) . lower ( ) _name = name _group = group _release = release if verbose : print ( 'name=%s, group=%s, release=%s' % ( _name , _group , _release ) ) env . vm_elastic_ip_mappings = shelf . get ( 'vm_elastic_ip_mappings' ) data = type ( env ) ( ) if env . vm_type == EC2 : if verbose : print ( 'Checking EC2...' ) for instance in get_all_running_ec2_instances ( ) : name = instance . tags . get ( env . vm_name_tag ) group = instance . tags . get ( env . vm_group_tag ) release = instance . tags . get ( env . vm_release_tag ) if env . vm_group and env . vm_group != group : if verbose : print ( ( 'Skipping instance %s because its group "%s" ' 'does not match env.vm_group "%s".' ) % ( instance . public_dns_name , group , env . vm_group ) ) continue if _group and group != _group : if verbose : print ( ( 'Skipping instance %s because its group "%s" ' 'does not match local group "%s".' ) % ( instance . public_dns_name , group , _group ) ) continue if _name and name != _name : if verbose : print ( ( 'Skipping instance %s because its name "%s" ' 'does not match name "%s".' ) % ( instance . public_dns_name , name , _name ) ) continue if _release and release != _release : if verbose : print ( ( 'Skipping instance %s because its release "%s" ' 'does not match release "%s".' ) % ( instance . public_dns_name , release , _release ) ) continue if except_release and release == except_release : continue if verbose : print ( 'Adding instance %s (%s).' % ( name , instance . public_dns_name ) ) data . setdefault ( name , type ( env ) ( ) ) data [ name ] [ 'id' ] = instance . id data [ name ] [ 'public_dns_name' ] = instance . public_dns_name if verbose : print ( 'Public DNS: %s' % instance . public_dns_name ) if env . vm_elastic_ip_mappings and name in env . vm_elastic_ip_mappings : data [ name ] [ 'ip' ] = env . vm_elastic_ip_mappings [ name ] else : data [ name ] [ 'ip' ] = socket . gethostbyname ( instance . public_dns_name ) if int ( show ) : pprint ( data , indent = 4 ) return data elif env . vm_type == KVM : #virsh list pass else : raise NotImplementedError
13736	def get_param_values ( request , model = None ) : if type ( request ) == dict : return request params = get_payload ( request ) # support in-place editing formatted request try : del params [ 'pk' ] params [ params . pop ( 'name' ) ] = params . pop ( 'value' ) except KeyError : pass return { k . rstrip ( '[]' ) : safe_eval ( v ) if not type ( v ) == list else [ safe_eval ( sv ) for sv in v ] for k , v in params . items ( ) }
3363	def save_yaml_model ( model , filename , sort = False , * * kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ "version" ] = YAML_SPEC if isinstance ( filename , string_types ) : with io . open ( filename , "w" ) as file_handle : yaml . dump ( obj , file_handle , * * kwargs ) else : yaml . dump ( obj , filename , * * kwargs )
12672	def group ( * args ) : if args and isinstance ( args [ 0 ] , dataframe . DataFrame ) : return args [ 0 ] . group ( * args [ 1 : ] ) elif not args : raise ValueError ( "No arguments provided" ) else : return pipeable . Pipeable ( pipeable . PipingMethod . GROUP , * args )
10039	def pick_coda_from_decimal ( decimal ) : decimal = Decimal ( decimal ) __ , digits , exp = decimal . as_tuple ( ) if exp < 0 : return DIGIT_CODAS [ digits [ - 1 ] ] __ , digits , exp = decimal . normalize ( ) . as_tuple ( ) index = bisect_right ( EXP_INDICES , exp ) - 1 if index < 0 : return DIGIT_CODAS [ digits [ - 1 ] ] else : return EXP_CODAS [ EXP_INDICES [ index ] ]
13887	def CreateDirectory ( directory ) : from six . moves . urllib . parse import urlparse directory_url = urlparse ( directory ) # Handle local if _UrlIsLocal ( directory_url ) : if not os . path . exists ( directory ) : os . makedirs ( directory ) return directory # Handle FTP elif directory_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme ) else : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme )
380	def get_zca_whitening_principal_components_img ( X ) : flatX = np . reshape ( X , ( X . shape [ 0 ] , X . shape [ 1 ] * X . shape [ 2 ] * X . shape [ 3 ] ) ) tl . logging . info ( "zca : computing sigma .." ) sigma = np . dot ( flatX . T , flatX ) / flatX . shape [ 0 ] tl . logging . info ( "zca : computing U, S and V .." ) U , S , _ = linalg . svd ( sigma ) # USV tl . logging . info ( "zca : computing principal components .." ) principal_components = np . dot ( np . dot ( U , np . diag ( 1. / np . sqrt ( S + 10e-7 ) ) ) , U . T ) return principal_components
2856	def write ( self , data ) : #check for hardware limit of FT232H and similar MPSSE chips if ( len ( data ) > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) # Build command to write SPI data. command = 0x10 | ( self . lsbfirst << 3 ) | self . write_clock_ve logger . debug ( 'SPI write with command {0:2X}.' . format ( command ) ) # Compute length low and high bytes. # NOTE: Must actually send length minus one because the MPSSE engine # considers 0 a length of 1 and FFFF a length of 65536 # splitting into two lists for two commands to prevent buffer errors data1 = data [ : len ( data ) / 2 ] data2 = data [ len ( data ) / 2 : ] len_low1 = ( len ( data1 ) - 1 ) & 0xFF len_high1 = ( ( len ( data1 ) - 1 ) >> 8 ) & 0xFF len_low2 = ( len ( data2 ) - 1 ) & 0xFF len_high2 = ( ( len ( data2 ) - 1 ) >> 8 ) & 0xFF self . _assert_cs ( ) # Send command and length, then data, split into two commands, handle for length 1 if len ( data1 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low1 , len_high1 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data1 ) ) ) if len ( data2 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low2 , len_high2 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data2 ) ) ) self . _deassert_cs ( )
6212	def plane_xz ( size = ( 10 , 10 ) , resolution = ( 10 , 10 ) ) -> VAO : sx , sz = size rx , rz = resolution dx , dz = sx / rx , sz / rz # step ox , oz = - sx / 2 , - sz / 2 # start offset def gen_pos ( ) : for z in range ( rz ) : for x in range ( rx ) : yield ox + x * dx yield 0 yield oz + z * dz def gen_uv ( ) : for z in range ( rz ) : for x in range ( rx ) : yield x / ( rx - 1 ) yield 1 - z / ( rz - 1 ) def gen_normal ( ) : for _ in range ( rx * rz ) : yield 0.0 yield 1.0 yield 0.0 def gen_index ( ) : for z in range ( rz - 1 ) : for x in range ( rx - 1 ) : # quad poly left yield z * rz + x + 1 yield z * rz + x yield z * rz + x + rx # quad poly right yield z * rz + x + 1 yield z * rz + x + rx yield z * rz + x + rx + 1 pos_data = numpy . fromiter ( gen_pos ( ) , dtype = numpy . float32 ) uv_data = numpy . fromiter ( gen_uv ( ) , dtype = numpy . float32 ) normal_data = numpy . fromiter ( gen_normal ( ) , dtype = numpy . float32 ) index_data = numpy . fromiter ( gen_index ( ) , dtype = numpy . uint32 ) vao = VAO ( "plane_xz" , mode = moderngl . TRIANGLES ) vao . buffer ( pos_data , '3f' , [ 'in_position' ] ) vao . buffer ( uv_data , '2f' , [ 'in_uv' ] ) vao . buffer ( normal_data , '3f' , [ 'in_normal' ] ) vao . index_buffer ( index_data , index_element_size = 4 ) return vao
11349	def is_function ( self ) : if self . is_instance ( ) or self . is_class ( ) : return False return isinstance ( self . callback , ( Callable , classmethod ) )
4434	async def update_state ( self , data ) : guild_id = int ( data [ 'guildId' ] ) if guild_id in self . players : player = self . players . get ( guild_id ) player . position = data [ 'state' ] . get ( 'position' , 0 ) player . position_timestamp = data [ 'state' ] [ 'time' ]
1456	def valid_path ( path ) : # check if the suffic of classpath suffix exists as directory if path . endswith ( '*' ) : Log . debug ( 'Checking classpath entry suffix as directory: %s' , path [ : - 1 ] ) if os . path . isdir ( path [ : - 1 ] ) : return True return False # check if the classpath entry is a directory Log . debug ( 'Checking classpath entry as directory: %s' , path ) if os . path . isdir ( path ) : return True else : # check if the classpath entry is a file Log . debug ( 'Checking classpath entry as file: %s' , path ) if os . path . isfile ( path ) : return True return False
6748	def collect_genv ( self , include_local = True , include_global = True ) : e = type ( self . genv ) ( ) if include_global : e . update ( self . genv ) if include_local : for k , v in self . lenv . items ( ) : e [ '%s_%s' % ( self . obj . name . lower ( ) , k ) ] = v return e
10162	def setup ( app ) : lexer = MarkdownLexer ( ) for alias in lexer . aliases : app . add_lexer ( alias , lexer ) return dict ( version = __version__ )
10175	def _format_range_dt ( self , d ) : if not isinstance ( d , six . string_types ) : d = d . isoformat ( ) return '{0}||/{1}' . format ( d , self . dt_rounding_map [ self . aggregation_interval ] )
12576	def set_mask ( self , mask_img ) : mask = load_mask ( mask_img , allow_empty = True ) check_img_compatibility ( self . img , mask , only_check_3d = True ) # this will raise an exception if something is wrong self . mask = mask
9511	def replace_bases ( self , old , new ) : self . seq = self . seq . replace ( old , new )
9355	def money ( min = 0 , max = 10 ) : value = random . choice ( range ( min * 100 , max * 100 ) ) return "%1.2f" % ( float ( value ) / 100 )
12188	def message_is_to_me ( self , data ) : return ( data . get ( 'type' ) == 'message' and data . get ( 'text' , '' ) . startswith ( self . address_as ) )
9210	def capture ( target_url , user_agent = "archiveis (https://github.com/pastpages/archiveis)" , proxies = { } ) : # Put together the URL that will save our request domain = "http://archive.vn" save_url = urljoin ( domain , "/submit/" ) # Configure the request headers headers = { 'User-Agent' : user_agent , "host" : "archive.vn" , } # Request a unique identifier for our activity logger . debug ( "Requesting {}" . format ( domain + "/" ) ) get_kwargs = dict ( timeout = 120 , allow_redirects = True , headers = headers , ) if proxies : get_kwargs [ 'proxies' ] = proxies response = requests . get ( domain + "/" , * * get_kwargs ) response . raise_for_status ( ) # It will need to be parsed from the homepage response headers html = str ( response . content ) try : unique_id = html . split ( 'name="submitid' , 1 ) [ 1 ] . split ( 'value="' , 1 ) [ 1 ] . split ( '"' , 1 ) [ 0 ] logger . debug ( "Unique identifier: {}" . format ( unique_id ) ) except IndexError : logger . warn ( "Unable to extract unique identifier from archive.is. Submitting without it." ) unique_id = None # Send the capture request to archive.is with the unique id included data = { "url" : target_url , "anyway" : 1 , } if unique_id : data . update ( { "submitid" : unique_id } ) post_kwargs = dict ( timeout = 120 , allow_redirects = True , headers = headers , data = data ) if proxies : post_kwargs [ 'proxies' ] = proxies logger . debug ( "Requesting {}" . format ( save_url ) ) response = requests . post ( save_url , * * post_kwargs ) response . raise_for_status ( ) # There are a couple ways the header can come back if 'Refresh' in response . headers : memento = str ( response . headers [ 'Refresh' ] ) . split ( ';url=' ) [ 1 ] logger . debug ( "Memento from Refresh header: {}" . format ( memento ) ) return memento if 'Location' in response . headers : memento = response . headers [ 'Location' ] logger . debug ( "Memento from Location header: {}" . format ( memento ) ) return memento logger . debug ( "Memento not found in response headers. Inspecting history." ) for i , r in enumerate ( response . history ) : logger . debug ( "Inspecting history request #{}" . format ( i ) ) logger . debug ( r . headers ) if 'Location' in r . headers : memento = r . headers [ 'Location' ] logger . debug ( "Memento from the Location header of {} history response: {}" . format ( i + 1 , memento ) ) return memento # If there's nothing at this point, throw an error logger . error ( "No memento returned by archive.is" ) logger . error ( "Status code: {}" . format ( response . status_code ) ) logger . error ( response . headers ) logger . error ( response . text ) raise Exception ( "No memento returned by archive.is" )
7701	def groups ( self ) : groups = set ( ) for item in self . _items : groups |= item . groups return groups
488	def _trackInstanceAndCheckForConcurrencyViolation ( self ) : global g_max_concurrency , g_max_concurrency_raise_exception assert g_max_concurrency is not None assert self not in self . _clsOutstandingInstances , repr ( self ) # Populate diagnostic info self . _creationTracebackString = traceback . format_stack ( ) # Check for concurrency violation if self . _clsNumOutstanding >= g_max_concurrency : # NOTE: It's possible for _clsNumOutstanding to be greater than # len(_clsOutstandingInstances) if concurrency check was enabled after # unrelease allocations. errorMsg = ( "With numOutstanding=%r, exceeded concurrency limit=%r " "when requesting %r. OTHER TRACKED UNRELEASED " "INSTANCES (%s): %r" ) % ( self . _clsNumOutstanding , g_max_concurrency , self , len ( self . _clsOutstandingInstances ) , self . _clsOutstandingInstances , ) self . _logger . error ( errorMsg ) if g_max_concurrency_raise_exception : raise ConcurrencyExceededError ( errorMsg ) # Add self to tracked instance set self . _clsOutstandingInstances . add ( self ) self . _addedToInstanceSet = True return
12635	def levenshtein_analysis ( self , field_weights = None ) : if field_weights is None : if not isinstance ( self . field_weights , dict ) : raise ValueError ( 'Expected a dict for `field_weights` parameter, ' 'got {}' . format ( type ( self . field_weights ) ) ) key_dicoms = list ( self . dicom_groups . keys ( ) ) file_dists = calculate_file_distances ( key_dicoms , field_weights , self . _dist_method_cls ) return file_dists
4636	def derive_from_seed ( self , offset ) : seed = int ( hexlify ( bytes ( self ) ) . decode ( "ascii" ) , 16 ) z = int ( hexlify ( offset ) . decode ( "ascii" ) , 16 ) order = ecdsa . SECP256k1 . order secexp = ( seed + z ) % order secret = "%0x" % secexp if len ( secret ) < 64 : # left-pad with zeroes secret = ( "0" * ( 64 - len ( secret ) ) ) + secret return PrivateKey ( secret , prefix = self . pubkey . prefix )
3543	def python_source_files ( path , tests_dirs ) : if isdir ( path ) : for root , dirs , files in os . walk ( path ) : dirs [ : ] = [ d for d in dirs if os . path . join ( root , d ) not in tests_dirs ] for filename in files : if filename . endswith ( '.py' ) : yield os . path . join ( root , filename ) else : yield path
3237	def modify ( item , output = 'camelized' ) : if output == 'camelized' : return _modify ( item , camelize ) elif output == 'underscored' : return _modify ( item , underscore )
12105	def _qsub_block ( self , output_dir , error_dir , tid_specs ) : processes = [ ] job_names = [ ] for ( tid , spec ) in tid_specs : job_name = "%s_%s_tid_%d" % ( self . batch_name , self . job_timestamp , tid ) job_names . append ( job_name ) cmd_args = self . command ( self . command . _formatter ( spec ) , tid , self . _launchinfo ) popen_args = self . _qsub_args ( [ ( "-e" , error_dir ) , ( '-N' , job_name ) , ( "-o" , output_dir ) ] , cmd_args ) p = subprocess . Popen ( popen_args , stdout = subprocess . PIPE ) ( stdout , stderr ) = p . communicate ( ) self . debug ( stdout ) if p . poll ( ) != 0 : raise EnvironmentError ( "qsub command exit with code: %d" % p . poll ( ) ) processes . append ( p ) self . message ( "Invoked qsub for %d commands" % len ( processes ) ) if ( self . reduction_fn is not None ) or self . dynamic : self . _qsub_collate_and_launch ( output_dir , error_dir , job_names )
1888	def must_be_true ( self , constraints , expression ) -> bool : solutions = self . get_all_values ( constraints , expression , maxcnt = 2 , silent = True ) return solutions == [ True ]
4301	def setup_database ( config_data ) : with chdir ( config_data . project_directory ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) commands = [ ] commands . append ( [ sys . executable , '-W' , 'ignore' , 'manage.py' , 'migrate' ] , ) if config_data . verbose : sys . stdout . write ( 'Database setup commands: {0}\n' . format ( ', ' . join ( [ ' ' . join ( cmd ) for cmd in commands ] ) ) ) for command in commands : try : output = subprocess . check_output ( command , env = env , stderr = subprocess . STDOUT ) sys . stdout . write ( output . decode ( 'utf-8' ) ) except subprocess . CalledProcessError as e : # pragma: no cover if config_data . verbose : sys . stdout . write ( e . output . decode ( 'utf-8' ) ) raise if not config_data . no_user : sys . stdout . write ( 'Creating admin user\n' ) if config_data . noinput : create_user ( config_data ) else : subprocess . check_call ( ' ' . join ( [ sys . executable , '-W' , 'ignore' , 'manage.py' , 'createsuperuser' ] ) , shell = True , stderr = subprocess . STDOUT )
1259	def save_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) return component . save ( sess = self . session , save_path = save_path )
4077	def run_3to2 ( args = None ) : args = BASE_ARGS_3TO2 if args is None else BASE_ARGS_3TO2 + args try : proc = subprocess . Popen ( [ '3to2' ] + args , stderr = subprocess . PIPE ) except OSError : for path in glob . glob ( '*.egg' ) : if os . path . isdir ( path ) and path not in sys . path : sys . path . append ( path ) try : from lib3to2 . main import main as lib3to2_main except ImportError : raise OSError ( '3to2 script is unavailable.' ) else : if lib3to2_main ( 'lib3to2.fixes' , args ) : raise Exception ( 'lib3to2 parsing error' ) else : # HACK: workaround for 3to2 never returning non-zero # when using the -j option. num_errors = 0 while proc . poll ( ) is None : line = proc . stderr . readline ( ) sys . stderr . write ( line ) num_errors += line . count ( ': ParseError: ' ) if proc . returncode or num_errors : raise Exception ( 'lib3to2 parsing error' )
3455	def weight ( self ) : try : return sum ( [ count * elements_and_molecular_weights [ element ] for element , count in self . elements . items ( ) ] ) except KeyError as e : warn ( "The element %s does not appear in the periodic table" % e )
10188	def publish ( self , event_type , events ) : assert event_type in self . events current_queues . queues [ 'stats-{}' . format ( event_type ) ] . publish ( events )
479	def word_to_id ( self , word ) : if word in self . vocab : return self . vocab [ word ] else : return self . unk_id
12915	def filelist ( self ) : if len ( self . _filelist ) == 0 : for item in self . _data : if isinstance ( self . _data [ item ] , filetree ) : self . _filelist . extend ( self . _data [ item ] . filelist ( ) ) else : self . _filelist . append ( self . _data [ item ] ) return self . _filelist
11466	def ls ( self , folder = '' ) : current_folder = self . _ftp . pwd ( ) self . cd ( folder ) contents = [ ] self . _ftp . retrlines ( 'LIST' , lambda a : contents . append ( a ) ) files = filter ( lambda a : a . split ( ) [ 0 ] . startswith ( '-' ) , contents ) folders = filter ( lambda a : a . split ( ) [ 0 ] . startswith ( 'd' ) , contents ) files = map ( lambda a : ' ' . join ( a . split ( ) [ 8 : ] ) , files ) folders = map ( lambda a : ' ' . join ( a . split ( ) [ 8 : ] ) , folders ) self . _ftp . cwd ( current_folder ) return files , folders
11579	def system_reset ( self ) : data = chr ( self . SYSTEM_RESET ) self . pymata . transport . write ( data ) # response table re-initialization # for each pin set the mode to input and the last read data value to zero with self . pymata . data_lock : # remove all old entries from existing tables for _ in range ( len ( self . digital_response_table ) ) : self . digital_response_table . pop ( ) for _ in range ( len ( self . analog_response_table ) ) : self . analog_response_table . pop ( ) # reinitialize tables for pin in range ( 0 , self . total_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . digital_response_table . append ( response_entry ) for pin in range ( 0 , self . number_of_analog_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . analog_response_table . append ( response_entry )
2801	def convert_reduce_sum ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting reduce_sum ...' ) keepdims = params [ 'keepdims' ] > 0 axis = params [ 'axes' ] def target_layer ( x , keepdims = keepdims , axis = axis ) : import keras . backend as K return K . sum ( x , keepdims = keepdims , axis = axis ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
969	def _extractCallingMethodArgs ( ) : import inspect import copy callingFrame = inspect . stack ( ) [ 1 ] [ 0 ] argNames , _ , _ , frameLocalVarDict = inspect . getargvalues ( callingFrame ) argNames . remove ( "self" ) args = copy . copy ( frameLocalVarDict ) for varName in frameLocalVarDict : if varName not in argNames : args . pop ( varName ) return args
3375	def add_absolute_expression ( model , expression , name = "abs_var" , ub = None , difference = 0 , add = True ) : Components = namedtuple ( 'Components' , [ 'variable' , 'upper_constraint' , 'lower_constraint' ] ) variable = model . problem . Variable ( name , lb = 0 , ub = ub ) # The following constraints enforce variable > expression and # variable > -expression upper_constraint = model . problem . Constraint ( expression - variable , ub = difference , name = "abs_pos_" + name ) , lower_constraint = model . problem . Constraint ( expression + variable , lb = difference , name = "abs_neg_" + name ) to_add = Components ( variable , upper_constraint , lower_constraint ) if add : add_cons_vars_to_problem ( model , to_add ) return to_add
6023	def new_psf_with_renormalized_array ( self ) : return PSF ( array = self , pixel_scale = self . pixel_scale , renormalize = True )
6438	def dist ( self , src , tar , weights = 'exponential' , max_length = 8 ) : return self . dist_abs ( src , tar , weights , max_length , True )
9063	def unfix ( self , param ) : if param == "delta" : self . _unfix ( "logistic" ) else : self . _fix [ param ] = False
11370	def convert_date_from_iso_to_human ( value ) : try : year , month , day = value . split ( "-" ) except ValueError : # Not separated by "-". Space? try : year , month , day = value . split ( " " ) except ValueError : # What gives? OK, lets just return as is return value try : date_object = datetime ( int ( year ) , int ( month ) , int ( day ) ) except TypeError : return value return date_object . strftime ( "%d %b %Y" )
953	def closenessScores ( self , expValues , actValues , * * kwargs ) : ratio = 1.0 esum = int ( expValues . sum ( ) ) asum = int ( actValues . sum ( ) ) if asum > esum : diff = asum - esum if diff < esum : ratio = 1 - diff / float ( esum ) else : ratio = 1 / float ( diff ) olap = expValues & actValues osum = int ( olap . sum ( ) ) if esum == 0 : r = 0.0 else : r = osum / float ( esum ) r = r * ratio return numpy . array ( [ r ] )
4489	def might_need_auth ( f ) : @ wraps ( f ) def wrapper ( cli_args ) : try : return_value = f ( cli_args ) except UnauthorizedException as e : config = config_from_env ( config_from_file ( ) ) username = _get_username ( cli_args , config ) if username is None : sys . exit ( "Please set a username (run `osf -h` for details)." ) else : sys . exit ( "You are not authorized to access this project." ) return return_value return wrapper
12883	def _run_supervisor ( self ) : import time still_supervising = lambda : ( multiprocessing . active_children ( ) or not self . log_queue . empty ( ) or not self . exception_queue . empty ( ) ) try : while still_supervising ( ) : # When a log message is received, make a logger with the same # name in this process and use it to re-log the message. It # will get handled in this process. try : record = self . log_queue . get_nowait ( ) logger = logging . getLogger ( record . name ) logger . handle ( record ) except queue . Empty : pass # When an exception is received, immediately re-raise it. try : exception = self . exception_queue . get_nowait ( ) except queue . Empty : pass else : raise exception # Sleep for a little bit, and make sure that the workers haven't # outlived their time limit. time . sleep ( 1 / self . frame_rate ) self . elapsed_time += 1 / self . frame_rate if self . time_limit and self . elapsed_time > self . time_limit : raise RuntimeError ( "timeout" ) # Make sure the workers don't outlive the supervisor, no matter how the # polling loop ended (e.g. normal execution or an exception). finally : for process in multiprocessing . active_children ( ) : process . terminate ( )
8081	def rellineto ( self , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . rellineto ( x , y )
4118	def _swapsides ( data ) : N = len ( data ) return np . concatenate ( ( data [ N // 2 + 1 : ] , data [ 0 : N // 2 ] ) )
189	def copy ( self , line_strings = None , shape = None ) : lss = self . line_strings if line_strings is None else line_strings shape = self . shape if shape is None else shape return LineStringsOnImage ( line_strings = lss , shape = shape )
13391	def format_uuid ( uuid , max_length = 10 ) : if max_length <= 3 : raise ValueError ( "max length must be larger than 3" ) if len ( uuid ) > max_length : uuid = "{}..." . format ( uuid [ 0 : max_length - 3 ] ) return uuid
2720	def wait ( self , update_every_seconds = 1 ) : while self . status == u'in-progress' : sleep ( update_every_seconds ) self . load ( ) return self . status == u'completed'
3512	def optimizely ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return OptimizelyNode ( )
1824	def SETZ ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , 1 , 0 ) )
4742	def env_export ( prefix , exported , env ) : for exp in exported : ENV [ "_" . join ( [ prefix , exp ] ) ] = env [ exp ]
6569	def signature_matches ( func , args = ( ) , kwargs = { } ) : try : sig = inspect . signature ( func ) sig . bind ( * args , * * kwargs ) except TypeError : return False else : return True
6956	def _transit_model ( times , t0 , per , rp , a , inc , ecc , w , u , limb_dark , exp_time_minutes = 2 , supersample_factor = 7 ) : params = batman . TransitParams ( ) # object to store transit parameters params . t0 = t0 # time of periastron params . per = per # orbital period params . rp = rp # planet radius (in stellar radii) params . a = a # semi-major axis (in stellar radii) params . inc = inc # orbital inclination (in degrees) params . ecc = ecc # the eccentricity of the orbit params . w = w # longitude of periastron (in degrees) params . u = u # limb darkening coefficient list params . limb_dark = limb_dark # limb darkening model to use t = times m = batman . TransitModel ( params , t , exp_time = exp_time_minutes / 60. / 24. , supersample_factor = supersample_factor ) return params , m
883	def activateDendrites ( self , learn = True ) : ( numActiveConnected , numActivePotential ) = self . connections . computeActivity ( self . activeCells , self . connectedPermanence ) activeSegments = ( self . connections . segmentForFlatIdx ( i ) for i in xrange ( len ( numActiveConnected ) ) if numActiveConnected [ i ] >= self . activationThreshold ) matchingSegments = ( self . connections . segmentForFlatIdx ( i ) for i in xrange ( len ( numActivePotential ) ) if numActivePotential [ i ] >= self . minThreshold ) self . activeSegments = sorted ( activeSegments , key = self . connections . segmentPositionSortKey ) self . matchingSegments = sorted ( matchingSegments , key = self . connections . segmentPositionSortKey ) self . numActiveConnectedSynapsesForSegment = numActiveConnected self . numActivePotentialSynapsesForSegment = numActivePotential if learn : for segment in self . activeSegments : self . lastUsedIterationForSegment [ segment . flatIdx ] = self . iteration self . iteration += 1
1809	def SETE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , 1 , 0 ) )
3788	def set_user_method ( self , user_methods , forced = False ) : # Accept either a string or a list of methods, and whether # or not to only consider the false methods if isinstance ( user_methods , str ) : user_methods = [ user_methods ] # The user's order matters and is retained for use by select_valid_methods self . user_methods = user_methods self . forced = forced # Validate that the user's specified methods are actual methods if set ( self . user_methods ) . difference ( self . all_methods ) : raise Exception ( "One of the given methods is not available for this mixture" ) if not self . user_methods and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) # Remove previously selected methods self . method = None self . sorted_valid_methods = [ ] self . TP_zs_ws_cached = ( None , None , None , None )
8997	def url ( self , url , encoding = "UTF-8" ) : import urllib . request with urllib . request . urlopen ( url ) as file : webpage_content = file . read ( ) webpage_content = webpage_content . decode ( encoding ) return self . string ( webpage_content )
5704	def _scan_footpaths ( self , stop_id , walk_departure_time ) : for _ , neighbor , data in self . _walk_network . edges_iter ( nbunch = [ stop_id ] , data = True ) : d_walk = data [ "d_walk" ] arrival_time = walk_departure_time + d_walk / self . _walk_speed self . _update_stop_label ( neighbor , arrival_time )
80	def CoarseDropout ( p = 0 , size_px = None , size_percent = None , per_channel = False , min_size = 4 , name = None , deterministic = False , random_state = None ) : if ia . is_single_number ( p ) : p2 = iap . Binomial ( 1 - p ) elif ia . is_iterable ( p ) : ia . do_assert ( len ( p ) == 2 ) ia . do_assert ( p [ 0 ] < p [ 1 ] ) ia . do_assert ( 0 <= p [ 0 ] <= 1.0 ) ia . do_assert ( 0 <= p [ 1 ] <= 1.0 ) p2 = iap . Binomial ( iap . Uniform ( 1 - p [ 1 ] , 1 - p [ 0 ] ) ) elif isinstance ( p , iap . StochasticParameter ) : p2 = p else : raise Exception ( "Expected p to be float or int or StochasticParameter, got %s." % ( type ( p ) , ) ) if size_px is not None : p3 = iap . FromLowerResolution ( other_param = p2 , size_px = size_px , min_size = min_size ) elif size_percent is not None : p3 = iap . FromLowerResolution ( other_param = p2 , size_percent = size_percent , min_size = min_size ) else : raise Exception ( "Either size_px or size_percent must be set." ) if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return MultiplyElementwise ( p3 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
1664	def CheckMakePairUsesDeduction ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = _RE_PATTERN_EXPLICIT_MAKEPAIR . search ( line ) if match : error ( filename , linenum , 'build/explicit_make_pair' , 4 , # 4 = high confidence 'For C++11-compatibility, omit template arguments from make_pair' ' OR use pair directly OR if appropriate, construct a pair directly' )
13186	def download_observations ( observer_code ) : page_number = 1 observations = [ ] while True : logger . info ( 'Downloading page %d...' , page_number ) response = requests . get ( WEBOBS_RESULTS_URL , params = { 'obscode' : observer_code , 'num_results' : 200 , 'obs_types' : 'all' , 'page' : page_number , } ) logger . debug ( response . request . url ) parser = WebObsResultsParser ( response . text ) observations . extend ( parser . get_observations ( ) ) # kinda silly, but there's no need for lxml machinery here if '>Next</a>' not in response . text : break page_number += 1 return observations
6502	def strings_in_dictionary ( dictionary ) : strings = [ value for value in six . itervalues ( dictionary ) if not isinstance ( value , dict ) ] for child_dict in [ dv for dv in six . itervalues ( dictionary ) if isinstance ( dv , dict ) ] : strings . extend ( SearchResultProcessor . strings_in_dictionary ( child_dict ) ) return strings
12538	def get_unique_field_values ( dcm_file_list , field_name ) : field_values = set ( ) for dcm in dcm_file_list : field_values . add ( str ( DicomFile ( dcm ) . get_attributes ( field_name ) ) ) return field_values
6090	def transform_grid ( func ) : @ wraps ( func ) def wrapper ( profile , grid , * args , * * kwargs ) : """ Parameters ---------- profile : GeometryProfile The profiles that owns the function grid : ndarray PlaneCoordinates in either cartesian or profiles coordinate system args kwargs Returns ------- A value or coordinate in the same coordinate system as those passed in. """ if not isinstance ( grid , TransformedGrid ) : return func ( profile , profile . transform_grid_to_reference_frame ( grid ) , * args , * * kwargs ) else : return func ( profile , grid , * args , * * kwargs ) return wrapper
5946	def in_dir ( directory , create = True ) : startdir = os . getcwd ( ) try : try : os . chdir ( directory ) logger . debug ( "Working in {directory!r}..." . format ( * * vars ( ) ) ) except OSError as err : if create and err . errno == errno . ENOENT : os . makedirs ( directory ) os . chdir ( directory ) logger . info ( "Working in {directory!r} (newly created)..." . format ( * * vars ( ) ) ) else : logger . exception ( "Failed to start working in {directory!r}." . format ( * * vars ( ) ) ) raise yield os . getcwd ( ) finally : os . chdir ( startdir )
3458	def main ( argv ) : source , target , tag = argv if "a" in tag : bump = "alpha" if "b" in tag : bump = "beta" else : bump = find_bump ( target , tag ) filename = "{}.md" . format ( tag ) destination = copy ( join ( source , filename ) , target ) build_hugo_md ( destination , tag , bump )
5731	def advance_past_string_with_gdb_escapes ( self , chars_to_remove_gdb_escape = None ) : if chars_to_remove_gdb_escape is None : chars_to_remove_gdb_escape = [ '"' ] buf = "" while True : c = self . raw_text [ self . index ] self . index += 1 logging . debug ( "%s" , fmt_cyan ( c ) ) if c == "\\" : # We are on a backslash and there is another character after the backslash # to parse. Handle this case specially since gdb escaped it for us # Get the next char that is being escaped c2 = self . raw_text [ self . index ] self . index += 1 # only store the escaped character in the buffer; don't store the backslash # (don't leave it escaped) buf += c2 elif c == '"' : # Quote is closed. Exit (and don't include the end quote). break else : # capture this character, and keep capturing buf += c return buf
12446	def resource ( * * kwargs ) : def inner ( function ) : name = kwargs . pop ( 'name' , None ) if name is None : name = utils . dasherize ( function . __name__ ) methods = kwargs . pop ( 'methods' , None ) if isinstance ( methods , six . string_types ) : # Tuple-ify the method if we got just a string. methods = methods , # Construct a handler. handler = ( function , methods ) if name not in _resources : # Initiate the handlers list. _handlers [ name ] = [ ] # Construct a light-weight resource using the passed kwargs # as the arguments for the meta. from armet import resources kwargs [ 'name' ] = name class LightweightResource ( resources . Resource ) : Meta = type ( str ( 'Meta' ) , ( ) , kwargs ) def route ( self , request , response ) : for handler , methods in _handlers [ name ] : if methods is None or request . method in methods : return handler ( request , response ) resources . Resource . route ( self ) # Construct and add this resource. _resources [ name ] = LightweightResource # Add this to the handlers. _handlers [ name ] . append ( handler ) # Return the resource. return _resources [ name ] # Return the inner method. return inner
4459	def between ( a , b , inclusive_min = True , inclusive_max = True ) : return RangeValue ( a , b , inclusive_min = inclusive_min , inclusive_max = inclusive_max )
6877	def _gunzip_sqlitecurve ( sqlitecurve ) : # -k to keep the input .gz just in case something explodes cmd = 'gunzip -k %s' % sqlitecurve try : subprocess . check_output ( cmd , shell = True ) return sqlitecurve . replace ( '.gz' , '' ) except subprocess . CalledProcessError : return None
1877	def MOVSD ( cpu , dest , src ) : assert dest . type != 'memory' or src . type != 'memory' value = Operators . EXTRACT ( src . read ( ) , 0 , 64 ) if dest . size > src . size : value = Operators . ZEXTEND ( value , dest . size ) dest . write ( value )
4485	def write_to ( self , fp ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) response = self . _get ( self . _download_url , stream = True ) if response . status_code == 200 : response . raw . decode_content = True copyfileobj ( response . raw , fp , int ( response . headers [ 'Content-Length' ] ) ) else : raise RuntimeError ( "Response has status " "code {}." . format ( response . status_code ) )
1899	def can_be_true ( self , constraints , expression ) : if isinstance ( expression , bool ) : if not expression : return expression else : # if True check if constraints are feasible self . _reset ( constraints ) return self . _is_sat ( ) assert isinstance ( expression , Bool ) with constraints as temp_cs : temp_cs . add ( expression ) self . _reset ( temp_cs . to_string ( related_to = expression ) ) return self . _is_sat ( )
10289	def enrich_reactions ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add_has_reactant ( u , v ) for v in u . products : graph . add_has_product ( u , v )
2055	def ADDW ( cpu , dest , src , add ) : aligned_pc = ( cpu . instruction . address + 4 ) & 0xfffffffc if src . type == 'register' and src . reg in ( 'PC' , 'R15' ) : src = aligned_pc else : src = src . read ( ) dest . write ( src + add . read ( ) )
11184	def wrap_state_dict ( self , typename : str , state ) -> Dict [ str , Any ] : return { self . type_key : typename , self . state_key : state }
10742	def print_profile ( function ) : import memory_profiler def wrapper ( * args , * * kwargs ) : m = StringIO ( ) pr = cProfile . Profile ( ) pr . enable ( ) temp_func = memory_profiler . profile ( func = function , stream = m , precision = 4 ) output = temp_func ( * args , * * kwargs ) print ( m . getvalue ( ) ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort_stats ( 'cumulative' ) . print_stats ( '(?!.*memory_profiler.*)(^.*$)' , 20 ) m . close ( ) return output return wrapper
6500	def perform_search ( search_term , user = None , size = 10 , from_ = 0 , course_id = None ) : # field_, filter_ and exclude_dictionary(s) can be overridden by calling application # field_dictionary includes course if course_id provided ( field_dictionary , filter_dictionary , exclude_dictionary ) = SearchFilterGenerator . generate_field_filters ( user = user , course_id = course_id ) searcher = SearchEngine . get_search_engine ( getattr ( settings , "COURSEWARE_INDEX_NAME" , "courseware_index" ) ) if not searcher : raise NoSearchEngineError ( "No search engine specified in settings.SEARCH_ENGINE" ) results = searcher . search_string ( search_term , field_dictionary = field_dictionary , filter_dictionary = filter_dictionary , exclude_dictionary = exclude_dictionary , size = size , from_ = from_ , doc_type = "courseware_content" , ) # post-process the result for result in results [ "results" ] : result [ "data" ] = SearchResultProcessor . process_result ( result [ "data" ] , search_term , user ) results [ "access_denied_count" ] = len ( [ r for r in results [ "results" ] if r [ "data" ] is None ] ) results [ "results" ] = [ r for r in results [ "results" ] if r [ "data" ] is not None ] return results
8779	def _try_allocate ( self , context , segment_id , network_id ) : LOG . info ( "Attempting to allocate segment for network %s " "segment_id %s segment_type %s" % ( network_id , segment_id , self . segment_type ) ) filter_dict = { "segment_id" : segment_id , "segment_type" : self . segment_type , "do_not_use" : False } available_ranges = db_api . segment_allocation_range_find ( context , scope = db_api . ALL , * * filter_dict ) available_range_ids = [ r [ "id" ] for r in available_ranges ] try : with context . session . begin ( subtransactions = True ) : # Search for any deallocated segment ids for the # given segment. filter_dict = { "deallocated" : True , "segment_id" : segment_id , "segment_type" : self . segment_type , "segment_allocation_range_ids" : available_range_ids } # NOTE(morgabra) We select 100 deallocated segment ids from # the table here, and then choose 1 randomly. This is to help # alleviate the case where an uncaught exception might leave # an allocation active on a remote service but we do not have # a record of it locally. If we *do* end up choosing a # conflicted id, the caller should simply allocate another one # and mark them all as reserved. If a single object has # multiple reservations on the same segment, they will not be # deallocated, and the operator must resolve the conficts # manually. allocations = db_api . segment_allocation_find ( context , lock_mode = True , * * filter_dict ) . limit ( 100 ) . all ( ) if allocations : allocation = random . choice ( allocations ) # Allocate the chosen segment. update_dict = { "deallocated" : False , "deallocated_at" : None , "network_id" : network_id } allocation = db_api . segment_allocation_update ( context , allocation , * * update_dict ) LOG . info ( "Allocated segment %s for network %s " "segment_id %s segment_type %s" % ( allocation [ "id" ] , network_id , segment_id , self . segment_type ) ) return allocation except Exception : LOG . exception ( "Error in segment reallocation." ) LOG . info ( "Cannot find reallocatable segment for network %s " "segment_id %s segment_type %s" % ( network_id , segment_id , self . segment_type ) )
1997	def sync_svc ( state ) : syscall = state . cpu . R7 # Grab idx from manticore since qemu could have exited name = linux_syscalls . armv7 [ syscall ] logger . debug ( f"Syncing syscall: {name}" ) try : # Make sure mmap returns the same address if 'mmap' in name : returned = gdb . getR ( 'R0' ) logger . debug ( f"Syncing mmap ({returned:x})" ) state . cpu . write_register ( 'R0' , returned ) if 'exit' in name : return except ValueError : for reg in state . cpu . canonical_registers : print ( f'{reg}: {state.cpu.read_register(reg):x}' ) raise
3553	def start_scan ( self , timeout_sec = TIMEOUT_SEC ) : get_provider ( ) . _central_manager . scanForPeripheralsWithServices_options_ ( None , None ) self . _is_scanning = True
10060	def jsonschemas ( self ) : _jsonschemas = { k : v [ 'jsonschema' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'jsonschema' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] , _jsonschemas )
13651	def get_reference_data ( self , modified_since : Optional [ datetime . datetime ] = None ) -> GetReferenceDataResponse : if modified_since is None : modified_since = datetime . datetime ( year = 2010 , month = 1 , day = 1 ) response = requests . get ( '{}/lovs' . format ( API_URL_BASE ) , headers = { 'if-modified-since' : self . _format_dt ( modified_since ) , * * self . _get_headers ( ) , } , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) # return response.text return GetReferenceDataResponse . deserialize ( response . json ( ) )
2196	def flush ( self ) : # nocover if self . redirect is not None : self . redirect . flush ( ) super ( TeeStringIO , self ) . flush ( )
8534	def read ( cls , data , protocol = None , fallback_protocol = TBinaryProtocol , finagle_thrift = False , max_fields = MAX_FIELDS , max_list_size = MAX_LIST_SIZE , max_map_size = MAX_MAP_SIZE , max_set_size = MAX_SET_SIZE , read_values = False ) : # do we have enough data? if len ( data ) < cls . MIN_MESSAGE_SIZE : raise ValueError ( 'not enough data' ) if protocol is None : protocol = cls . detect_protocol ( data , fallback_protocol ) trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) # finagle-thrift prepends a RequestHeader # # See: http://git.io/vsziG header = None if finagle_thrift : try : header = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) except : # reset stream, maybe it's not finagle-thrift trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) # unpack the message method , mtype , seqid = proto . readMessageBegin ( ) mtype = cls . message_type_to_str ( mtype ) if len ( method ) == 0 or method . isspace ( ) or method . startswith ( ' ' ) : raise ValueError ( 'no method name' ) if len ( method ) > cls . MAX_METHOD_LENGTH : raise ValueError ( 'method name too long' ) # we might have made it until this point by mere chance, so filter out # suspicious method names valid = range ( 33 , 127 ) if any ( ord ( char ) not in valid for char in method ) : raise ValueError ( 'invalid method name' % method ) args = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) proto . readMessageEnd ( ) # Note: this is a bit fragile, the right thing would be to count bytes # as we read them (i.e.: when calling readI32, etc). msglen = trans . _buffer . tell ( ) return cls ( method , mtype , seqid , args , header , msglen ) , msglen
13095	def callback ( self , event ) : # IN_CLOSE_WRITE -> 0x00000008 if event . mask == 0x00000008 : if event . name . endswith ( '.json' ) : print_success ( "Ldapdomaindump file found" ) if event . name in [ 'domain_groups.json' , 'domain_users.json' ] : if event . name == 'domain_groups.json' : self . domain_groups_file = event . pathname if event . name == 'domain_users.json' : self . domain_users_file = event . pathname if self . domain_groups_file and self . domain_users_file : print_success ( "Importing users" ) subprocess . Popen ( [ 'jk-import-domaindump' , self . domain_groups_file , self . domain_users_file ] ) elif event . name == 'domain_computers.json' : print_success ( "Importing computers" ) subprocess . Popen ( [ 'jk-import-domaindump' , event . pathname ] ) # Ldap has been dumped, so remove the ldap targets. self . ldap_strings = [ ] self . write_targets ( ) if event . name . endswith ( '_samhashes.sam' ) : host = event . name . replace ( '_samhashes.sam' , '' ) # TODO import file. print_success ( "Secretsdump file, host ip: {}" . format ( host ) ) subprocess . Popen ( [ 'jk-import-secretsdump' , event . pathname ] ) # Remove this system from this ip list. self . ips . remove ( host ) self . write_targets ( )
9492	def compile_bytecode ( code : list ) -> bytes : bc = b"" for i , op in enumerate ( code ) : try : # Get the bytecode. if isinstance ( op , _PyteOp ) or isinstance ( op , _PyteAugmentedComparator ) : bc_op = op . to_bytes ( bc ) elif isinstance ( op , int ) : bc_op = op . to_bytes ( 1 , byteorder = "little" ) elif isinstance ( op , bytes ) : bc_op = op else : raise CompileError ( "Could not compile code of type {}" . format ( type ( op ) ) ) bc += bc_op except Exception as e : print ( "Fatal compiliation error on operator {i} ({op})." . format ( i = i , op = op ) ) raise e return bc
13437	def _cut_range ( self , line , start , current_position ) : result = [ ] try : for j in range ( start , len ( line ) ) : index = _setup_index ( j ) try : result . append ( line [ index ] ) except IndexError : result . append ( self . invalid_pos ) finally : result . append ( self . separator ) result . append ( line [ - 1 ] ) except IndexError : pass try : int ( self . positions [ current_position + 1 ] ) result . append ( self . separator ) except ( ValueError , IndexError ) : pass return result
11891	def set_all ( self , red , green , blue , brightness ) : command = "C {},{},{},{},{},\r\n" . format ( self . _zid , red , green , blue , brightness ) response = self . _hub . send_command ( command ) _LOGGER . debug ( "Set all %s: %s" , repr ( command ) , response ) return response
3640	def tradeStatus ( self , trade_id ) : method = 'GET' url = 'trade/status' if not isinstance ( trade_id , ( list , tuple ) ) : trade_id = ( trade_id , ) trade_id = ( str ( i ) for i in trade_id ) params = { 'tradeIds' : ',' . join ( trade_id ) } # multiple trade_ids not tested rc = self . __request__ ( method , url , params = params ) return [ itemParse ( i , full = False ) for i in rc [ 'auctionInfo' ] ]
13110	def lookup ( cls , key , get = False ) : if get : item = cls . _item_dict . get ( key ) return item . name if item else key return cls . _item_dict [ key ] . name
5252	def bdib ( self , ticker , start_datetime , end_datetime , event_type , interval , elms = None ) : elms = [ ] if not elms else elms # flush event queue in case previous call errored out logger = _get_logger ( self . debug ) while ( self . _session . tryNextEvent ( ) ) : pass # Create and fill the request for the historical data request = self . refDataService . createRequest ( 'IntradayBarRequest' ) request . set ( 'security' , ticker ) request . set ( 'eventType' , event_type ) request . set ( 'interval' , interval ) # bar interval in minutes request . set ( 'startDateTime' , start_datetime ) request . set ( 'endDateTime' , end_datetime ) for name , val in elms : request . set ( name , val ) logger . info ( 'Sending Request:\n{}' . format ( request ) ) # Send the request self . _session . sendRequest ( request , identity = self . _identity ) # Process received events data = [ ] flds = [ 'open' , 'high' , 'low' , 'close' , 'volume' , 'numEvents' ] for msg in self . _receive_events ( ) : d = msg [ 'element' ] [ 'IntradayBarResponse' ] for bar in d [ 'barData' ] [ 'barTickData' ] : data . append ( bar [ 'barTickData' ] ) data = pd . DataFrame ( data ) . set_index ( 'time' ) . sort_index ( ) . loc [ : , flds ] return data
13493	def read ( args ) : if args . config_file is None or not isfile ( args . config_file ) : return logging . info ( "Reading configure file: %s" % args . config_file ) config = cparser . ConfigParser ( ) config . read ( args . config_file ) if not config . has_section ( 'lrcloud' ) : raise RuntimeError ( "Configure file has no [lrcloud] section!" ) for ( name , value ) in config . items ( 'lrcloud' ) : if value == "True" : value = True elif value == "False" : value = False if getattr ( args , name ) is None : setattr ( args , name , value )
5147	def generate ( self ) : tar_bytes = BytesIO ( ) tar = tarfile . open ( fileobj = tar_bytes , mode = 'w' ) self . _generate_contents ( tar ) self . _process_files ( tar ) tar . close ( ) tar_bytes . seek ( 0 ) # set pointer to beginning of stream # `mtime` parameter of gzip file must be 0, otherwise any checksum operation # would return a different digest even when content is the same. # to achieve this we must use the python `gzip` library because the `tarfile` # library does not seem to offer the possibility to modify the gzip `mtime`. gzip_bytes = BytesIO ( ) gz = gzip . GzipFile ( fileobj = gzip_bytes , mode = 'wb' , mtime = 0 ) gz . write ( tar_bytes . getvalue ( ) ) gz . close ( ) gzip_bytes . seek ( 0 ) # set pointer to beginning of stream return gzip_bytes
10265	def collapse_entrez_equivalencies ( graph : BELGraph ) : relation_filter = build_relation_predicate ( EQUIVALENT_TO ) source_namespace_filter = build_source_namespace_filter ( [ 'EGID' , 'EG' , 'ENTREZ' ] ) edge_predicates = [ relation_filter , source_namespace_filter , ] _collapse_edge_passing_predicates ( graph , edge_predicates = edge_predicates )
3485	def _create_parameter ( model , pid , value , sbo = None , constant = True , units = None , flux_udef = None ) : parameter = model . createParameter ( ) # type: libsbml.Parameter parameter . setId ( pid ) parameter . setValue ( value ) parameter . setConstant ( constant ) if sbo : parameter . setSBOTerm ( sbo ) if units : parameter . setUnits ( flux_udef . getId ( ) )
6432	def encode ( self , word ) : word = word . upper ( ) # Rule 3 word = self . _delete_consecutive_repeats ( word ) # Rule 4 # Rule 5 i = 0 while i < len ( word ) : for match_len in range ( 4 , 1 , - 1 ) : if word [ i : i + match_len ] in self . _rules [ match_len ] : repl = self . _rules [ match_len ] [ word [ i : i + match_len ] ] word = word [ : i ] + repl + word [ i + match_len : ] i += len ( repl ) break else : i += 1 word = word [ : 1 ] + word [ 1 : ] . translate ( self . _del_trans ) # Rule 6 return word
8539	def get_disk_image_by_name ( pbclient , location , image_name ) : all_images = pbclient . list_images ( ) matching = [ i for i in all_images [ 'items' ] if i [ 'properties' ] [ 'name' ] == image_name and i [ 'properties' ] [ 'imageType' ] == "HDD" and i [ 'properties' ] [ 'location' ] == location ] return matching
3974	def _get_compose_volumes ( app_name , assembled_specs ) : volumes = [ ] volumes . append ( _get_cp_volume_mount ( app_name ) ) volumes += get_app_volume_mounts ( app_name , assembled_specs ) return volumes
1859	def CMPS ( cpu , dest , src ) : src_reg = { 8 : 'SI' , 32 : 'ESI' , 64 : 'RSI' } [ cpu . address_bit_size ] dest_reg = { 8 : 'DI' , 32 : 'EDI' , 64 : 'RDI' } [ cpu . address_bit_size ] base , _ , ty = cpu . get_descriptor ( cpu . DS ) src_addr = cpu . read_register ( src_reg ) + base dest_addr = cpu . read_register ( dest_reg ) + base size = dest . size # Compare arg1 = cpu . read_int ( dest_addr , size ) arg0 = cpu . read_int ( src_addr , size ) res = ( arg0 - arg1 ) & ( ( 1 << size ) - 1 ) cpu . _calculate_CMP_flags ( size , res , arg0 , arg1 ) #Advance EDI/ESI pointers increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
7188	def get_offset_and_prefix ( body , skip_assignments = False ) : assert body . type in ( syms . file_input , syms . suite ) _offset = 0 prefix = '' for _offset , child in enumerate ( body . children ) : if child . type == syms . simple_stmt : stmt = child . children [ 0 ] if stmt . type == syms . expr_stmt : expr = stmt . children if not skip_assignments : break if ( len ( expr ) != 2 or expr [ 0 ] . type != token . NAME or expr [ 1 ] . type != syms . annassign or _eq in expr [ 1 ] . children ) : break elif stmt . type not in ( syms . import_name , syms . import_from , token . STRING ) : break elif child . type == token . INDENT : assert isinstance ( child , Leaf ) prefix = child . value elif child . type != token . NEWLINE : break prefix , child . prefix = child . prefix , prefix return _offset , prefix
779	def _getOneMatchingRowNoRetries ( self , tableInfo , conn , fieldsToMatch , selectFieldNames ) : rows = self . _getMatchingRowsNoRetries ( tableInfo , conn , fieldsToMatch , selectFieldNames , maxRows = 1 ) if rows : assert len ( rows ) == 1 , repr ( len ( rows ) ) result = rows [ 0 ] else : result = None return result
12025	def remove ( self , line_data , root_type = None ) : roots = [ ld for ld in self . ancestors ( line_data ) if ( root_type and ld [ 'line_type' ] == root_type ) or ( not root_type and not ld [ 'parents' ] ) ] or [ line_data ] for root in roots : root [ 'line_status' ] = 'removed' root_descendants = self . descendants ( root ) for root_descendant in root_descendants : root_descendant [ 'line_status' ] = 'removed' root_ancestors = self . ancestors ( root ) # BFS, so we will process closer ancestors first for root_ancestor in root_ancestors : if len ( [ ld for ld in root_ancestor [ 'children' ] if ld [ 'line_status' ] != 'removed' ] ) == 0 : # if all children of a root_ancestor is removed # remove this root_ancestor root_ancestor [ 'line_status' ] = 'removed'
5188	def connect ( host = 'localhost' , port = 8080 , ssl_verify = False , ssl_key = None , ssl_cert = None , timeout = 10 , protocol = None , url_path = '/' , username = None , password = None , token = None ) : return BaseAPI ( host = host , port = port , timeout = timeout , ssl_verify = ssl_verify , ssl_key = ssl_key , ssl_cert = ssl_cert , protocol = protocol , url_path = url_path , username = username , password = password , token = token )
5455	def numeric_task_id ( task_id ) : # This function exists to support the legacy "task-id" format in the "google" # provider. Google labels originally could not be numeric. When the google # provider is completely replaced by the google-v2 provider, this function can # go away. if task_id is not None : if task_id . startswith ( 'task-' ) : return int ( task_id [ len ( 'task-' ) : ] ) else : return int ( task_id )
12992	def line_chunker ( text , getreffs , lines = 30 ) : level = len ( text . citation ) source_reffs = [ reff . split ( ":" ) [ - 1 ] for reff in getreffs ( level = level ) ] reffs = [ ] i = 0 while i + lines - 1 < len ( source_reffs ) : reffs . append ( tuple ( [ source_reffs [ i ] + "-" + source_reffs [ i + lines - 1 ] , source_reffs [ i ] ] ) ) i += lines if i < len ( source_reffs ) : reffs . append ( tuple ( [ source_reffs [ i ] + "-" + source_reffs [ len ( source_reffs ) - 1 ] , source_reffs [ i ] ] ) ) return reffs
7627	def add_namespace ( filename ) : with open ( filename , mode = 'r' ) as fileobj : __NAMESPACE__ . update ( json . load ( fileobj ) )
243	def daily_txns_with_bar_data ( transactions , market_data ) : transactions . index . name = 'date' txn_daily = pd . DataFrame ( transactions . assign ( amount = abs ( transactions . amount ) ) . groupby ( [ 'symbol' , pd . TimeGrouper ( 'D' ) ] ) . sum ( ) [ 'amount' ] ) txn_daily [ 'price' ] = market_data [ 'price' ] . unstack ( ) txn_daily [ 'volume' ] = market_data [ 'volume' ] . unstack ( ) txn_daily = txn_daily . reset_index ( ) . set_index ( 'date' ) return txn_daily
12284	def rootdir ( self , username , reponame , create = True ) : path = os . path . join ( self . workspace , 'datasets' , username , reponame ) if create : try : os . makedirs ( path ) except : pass return path
11259	def grep ( prev , pattern , * args , * * kw ) : inv = False if 'inv' not in kw else kw . pop ( 'inv' ) pattern_obj = re . compile ( pattern , * args , * * kw ) for data in prev : if bool ( inv ) ^ bool ( pattern_obj . match ( data ) ) : yield data
3214	def get_stats ( self ) : expired = sum ( [ x [ 'expired' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) miss = sum ( [ x [ 'miss' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) hit = sum ( [ x [ 'hit' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) return { 'totals' : { 'keys' : len ( self . _CACHE_STATS [ 'access_stats' ] ) , 'expired' : expired , 'miss' : miss , 'hit' : hit , } }
10673	def load_data_factsage ( path = '' ) : compounds . clear ( ) if path == '' : path = default_data_path if not os . path . exists ( path ) : warnings . warn ( 'The specified data file path does not exist. (%s)' % path ) return files = glob . glob ( os . path . join ( path , 'Compound_*.txt' ) ) for file in files : compound = Compound ( _read_compound_from_factsage_file_ ( file ) ) compounds [ compound . formula ] = compound
4515	def drawLine ( self , x0 , y0 , x1 , y1 , color = None , colorFunc = None , aa = False ) : md . draw_line ( self . set , x0 , y0 , x1 , y1 , color , colorFunc , aa )
4787	def matches ( self , pattern ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if not isinstance ( pattern , str_types ) : raise TypeError ( 'given pattern arg must be a string' ) if len ( pattern ) == 0 : raise ValueError ( 'given pattern arg must not be empty' ) if re . search ( pattern , self . val ) is None : self . _err ( 'Expected <%s> to match pattern <%s>, but did not.' % ( self . val , pattern ) ) return self
12513	def _crop_img_to ( image , slices , copy = True ) : img = check_img ( image ) data = img . get_data ( ) affine = img . get_affine ( ) cropped_data = data [ slices ] if copy : cropped_data = cropped_data . copy ( ) linear_part = affine [ : 3 , : 3 ] old_origin = affine [ : 3 , 3 ] new_origin_voxel = np . array ( [ s . start for s in slices ] ) new_origin = old_origin + linear_part . dot ( new_origin_voxel ) new_affine = np . eye ( 4 ) new_affine [ : 3 , : 3 ] = linear_part new_affine [ : 3 , 3 ] = new_origin new_img = nib . Nifti1Image ( cropped_data , new_affine ) return new_img
1225	def setup_components_and_tf_funcs ( self , custom_getter = None ) : custom_getter = super ( MemoryModel , self ) . setup_components_and_tf_funcs ( custom_getter ) # Memory self . memory = Memory . from_spec ( spec = self . memory_spec , kwargs = dict ( states = self . states_spec , internals = self . internals_spec , actions = self . actions_spec , summary_labels = self . summary_labels ) ) # Optimizer self . optimizer = Optimizer . from_spec ( spec = self . optimizer_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) # TensorFlow functions self . fn_discounted_cumulative_reward = tf . make_template ( name_ = 'discounted-cumulative-reward' , func_ = self . tf_discounted_cumulative_reward , custom_getter_ = custom_getter ) self . fn_reference = tf . make_template ( name_ = 'reference' , func_ = self . tf_reference , custom_getter_ = custom_getter ) self . fn_loss_per_instance = tf . make_template ( name_ = 'loss-per-instance' , func_ = self . tf_loss_per_instance , custom_getter_ = custom_getter ) self . fn_regularization_losses = tf . make_template ( name_ = 'regularization-losses' , func_ = self . tf_regularization_losses , custom_getter_ = custom_getter ) self . fn_loss = tf . make_template ( name_ = 'loss' , func_ = self . tf_loss , custom_getter_ = custom_getter ) self . fn_optimization = tf . make_template ( name_ = 'optimization' , func_ = self . tf_optimization , custom_getter_ = custom_getter ) self . fn_import_experience = tf . make_template ( name_ = 'import-experience' , func_ = self . tf_import_experience , custom_getter_ = custom_getter ) return custom_getter
8849	def setup_actions ( self ) : self . actionOpen . triggered . connect ( self . on_open ) self . actionNew . triggered . connect ( self . on_new ) self . actionSave . triggered . connect ( self . on_save ) self . actionSave_as . triggered . connect ( self . on_save_as ) self . actionQuit . triggered . connect ( QtWidgets . QApplication . instance ( ) . quit ) self . tabWidget . current_changed . connect ( self . on_current_tab_changed ) self . tabWidget . last_tab_closed . connect ( self . on_last_tab_closed ) self . actionAbout . triggered . connect ( self . on_about ) self . actionRun . triggered . connect ( self . on_run ) self . interactiveConsole . process_finished . connect ( self . on_process_finished ) self . actionConfigure_run . triggered . connect ( self . on_configure_run )
333	def plot_stoch_vol ( data , trace = None , ax = None ) : if trace is None : trace = model_stoch_vol ( data ) if ax is None : fig , ax = plt . subplots ( figsize = ( 15 , 8 ) ) data . abs ( ) . plot ( ax = ax ) ax . plot ( data . index , np . exp ( trace [ 's' , : : 30 ] . T ) , 'r' , alpha = .03 ) ax . set ( title = 'Stochastic volatility' , xlabel = 'Time' , ylabel = 'Volatility' ) ax . legend ( [ 'Abs returns' , 'Stochastic volatility process' ] , frameon = True , framealpha = 0.5 ) return ax
2854	def mpsse_gpio ( self ) : level_low = chr ( self . _level & 0xFF ) level_high = chr ( ( self . _level >> 8 ) & 0xFF ) dir_low = chr ( self . _direction & 0xFF ) dir_high = chr ( ( self . _direction >> 8 ) & 0xFF ) return str ( bytearray ( ( 0x80 , level_low , dir_low , 0x82 , level_high , dir_high ) ) )
12100	def _append_log ( self , specs ) : self . _spec_log += specs # This should be removed log_path = os . path . join ( self . root_directory , ( "%s.log" % self . batch_name ) ) core . Log . write_log ( log_path , [ spec for ( _ , spec ) in specs ] , allow_append = True )
3425	def get_metabolite_compartments ( self ) : warn ( 'use Model.compartments instead' , DeprecationWarning ) return { met . compartment for met in self . metabolites if met . compartment is not None }
4871	def create ( self , validated_data ) : ret = [ ] for attrs in validated_data : if 'non_field_errors' not in attrs and not any ( isinstance ( attrs [ field ] , list ) for field in attrs ) : ret . append ( self . child . create ( attrs ) ) else : ret . append ( attrs ) return ret
11945	def _store ( self , messages , response , * args , * * kwargs ) : contrib_messages = [ ] if self . user . is_authenticated ( ) : if not messages : # erase inbox self . backend . inbox_purge ( self . user ) else : for m in messages : try : self . backend . inbox_store ( [ self . user ] , m ) except MessageTypeNotSupported : contrib_messages . append ( m ) super ( StorageMixin , self ) . _store ( contrib_messages , response , * args , * * kwargs )
13214	def available ( self , timeout = 5 ) : host = self . _connect_args [ 'host' ] port = self . _connect_args [ 'port' ] try : sock = socket . create_connection ( ( host , port ) , timeout = timeout ) sock . close ( ) return True except socket . error : pass return False
4532	def clone ( self ) : args = { k : getattr ( self , k ) for k in self . CLONE_ATTRS } args [ 'color_list' ] = copy . copy ( self . color_list ) return self . __class__ ( [ ] , * * args )
4748	def import_parms ( self , args ) : for key , val in args . items ( ) : self . set_parm ( key , val )
10918	def find_best_step ( err_vals ) : if np . all ( np . isnan ( err_vals ) ) : raise ValueError ( 'All err_vals are nans!' ) return np . nanargmin ( err_vals )
10035	def add_arguments ( parser ) : parser . add_argument ( '-e' , '--environment' , help = 'Environment name' , required = False , nargs = '+' ) parser . add_argument ( '-w' , '--dont-wait' , help = 'Skip waiting for the app to be deleted' , action = 'store_true' )
3695	def Clapeyron ( T , Tc , Pc , dZ = 1 , Psat = 101325 ) : Tr = T / Tc return R * T * dZ * log ( Pc / Psat ) / ( 1. - Tr )
7368	async def read ( response , loads = loads , encoding = None ) : ctype = response . headers . get ( 'Content-Type' , "" ) . lower ( ) try : if "application/json" in ctype : logger . info ( "decoding data as json" ) return await response . json ( encoding = encoding , loads = loads ) if "text" in ctype : logger . info ( "decoding data as text" ) return await response . text ( encoding = encoding ) except ( UnicodeDecodeError , json . JSONDecodeError ) as exc : data = await response . read ( ) raise exceptions . PeonyDecodeError ( response = response , data = data , exception = exc ) return await response . read ( )
5586	def output_cleaned ( self , process_data ) : if self . METADATA [ "data_type" ] == "raster" : if is_numpy_or_masked_array ( process_data ) : return process_data elif is_numpy_or_masked_array_with_tags ( process_data ) : data , tags = process_data return self . output_cleaned ( data ) , tags elif self . METADATA [ "data_type" ] == "vector" : return list ( process_data )
988	def createTemporalAnomaly ( recordParams , spatialParams = _SP_PARAMS , temporalParams = _TM_PARAMS , verbosity = _VERBOSITY ) : inputFilePath = recordParams [ "inputFilePath" ] scalarEncoderArgs = recordParams [ "scalarEncoderArgs" ] dateEncoderArgs = recordParams [ "dateEncoderArgs" ] scalarEncoder = ScalarEncoder ( * * scalarEncoderArgs ) dateEncoder = DateEncoder ( * * dateEncoderArgs ) encoder = MultiEncoder ( ) encoder . addEncoder ( scalarEncoderArgs [ "name" ] , scalarEncoder ) encoder . addEncoder ( dateEncoderArgs [ "name" ] , dateEncoder ) network = Network ( ) network . addRegion ( "sensor" , "py.RecordSensor" , json . dumps ( { "verbosity" : verbosity } ) ) sensor = network . regions [ "sensor" ] . getSelf ( ) sensor . encoder = encoder sensor . dataSource = FileRecordStream ( streamID = inputFilePath ) # Create the spatial pooler region spatialParams [ "inputWidth" ] = sensor . encoder . getWidth ( ) network . addRegion ( "spatialPoolerRegion" , "py.SPRegion" , json . dumps ( spatialParams ) ) # Link the SP region to the sensor input network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" ) network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" , srcOutput = "resetOut" , destInput = "resetIn" ) network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , srcOutput = "spatialTopDownOut" , destInput = "spatialTopDownIn" ) network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , srcOutput = "temporalTopDownOut" , destInput = "temporalTopDownIn" ) # Add the TPRegion on top of the SPRegion network . addRegion ( "temporalPoolerRegion" , "py.TMRegion" , json . dumps ( temporalParams ) ) network . link ( "spatialPoolerRegion" , "temporalPoolerRegion" , "UniformLink" , "" ) network . link ( "temporalPoolerRegion" , "spatialPoolerRegion" , "UniformLink" , "" , srcOutput = "topDownOut" , destInput = "topDownIn" ) spatialPoolerRegion = network . regions [ "spatialPoolerRegion" ] # Make sure learning is enabled spatialPoolerRegion . setParameter ( "learningMode" , True ) # We want temporal anomalies so disable anomalyMode in the SP. This mode is # used for computing anomalies in a non-temporal model. spatialPoolerRegion . setParameter ( "anomalyMode" , False ) temporalPoolerRegion = network . regions [ "temporalPoolerRegion" ] # Enable topDownMode to get the predicted columns output temporalPoolerRegion . setParameter ( "topDownMode" , True ) # Make sure learning is enabled (this is the default) temporalPoolerRegion . setParameter ( "learningMode" , True ) # Enable inference mode so we get predictions temporalPoolerRegion . setParameter ( "inferenceMode" , True ) # Enable anomalyMode to compute the anomaly score. temporalPoolerRegion . setParameter ( "anomalyMode" , True ) return network
7295	def create_document_dictionary ( self , document , document_key = None , owner_document = None ) : doc_dict = self . create_doc_dict ( document , document_key , owner_document ) for doc_key , doc_field in doc_dict . items ( ) : # Base fields should not be evaluated if doc_key . startswith ( "_" ) : continue if isinstance ( doc_field , ListField ) : doc_dict [ doc_key ] = self . create_list_dict ( document , doc_field , doc_key ) elif isinstance ( doc_field , EmbeddedDocumentField ) : doc_dict [ doc_key ] = self . create_document_dictionary ( doc_dict [ doc_key ] . document_type_obj , doc_key ) else : doc_dict [ doc_key ] = { "_document" : document , "_key" : doc_key , "_document_field" : doc_field , "_widget" : get_widget ( doc_dict [ doc_key ] , getattr ( doc_field , 'disabled' , False ) ) } return doc_dict
4436	def destroy ( self ) : self . ws . destroy ( ) self . bot . remove_listener ( self . on_socket_response ) self . hooks . clear ( )
5546	def raster2pyramid ( input_file , output_dir , options ) : pyramid_type = options [ "pyramid_type" ] scale_method = options [ "scale_method" ] output_format = options [ "output_format" ] resampling = options [ "resampling" ] zoom = options [ "zoom" ] bounds = options [ "bounds" ] mode = "overwrite" if options [ "overwrite" ] else "continue" # Prepare process parameters minzoom , maxzoom = _get_zoom ( zoom , input_file , pyramid_type ) with rasterio . open ( input_file , "r" ) as input_raster : output_bands = input_raster . count input_dtype = input_raster . dtypes [ 0 ] output_dtype = input_raster . dtypes [ 0 ] nodataval = input_raster . nodatavals [ 0 ] nodataval = nodataval if nodataval else 0 if output_format == "PNG" and output_bands > 3 : output_bands = 3 output_dtype = 'uint8' scales_minmax = ( ) if scale_method == "dtype_scale" : for index in range ( 1 , output_bands + 1 ) : scales_minmax += ( DTYPE_RANGES [ input_dtype ] , ) elif scale_method == "minmax_scale" : for index in range ( 1 , output_bands + 1 ) : band = input_raster . read ( index ) scales_minmax += ( ( band . min ( ) , band . max ( ) ) , ) elif scale_method == "crop" : for index in range ( 1 , output_bands + 1 ) : scales_minmax += ( ( 0 , 255 ) , ) if input_dtype == "uint8" : scale_method = None scales_minmax = ( ) for index in range ( 1 , output_bands + 1 ) : scales_minmax += ( ( None , None ) , ) # Create configuration config = dict ( process = "mapchete.processes.pyramid.tilify" , output = { "path" : output_dir , "format" : output_format , "bands" : output_bands , "dtype" : output_dtype } , pyramid = dict ( pixelbuffer = 5 , grid = pyramid_type ) , scale_method = scale_method , scales_minmax = scales_minmax , input = { "raster" : input_file } , config_dir = os . getcwd ( ) , zoom_levels = dict ( min = minzoom , max = maxzoom ) , nodataval = nodataval , resampling = resampling , bounds = bounds , baselevel = { "zoom" : maxzoom , "resampling" : resampling } , mode = mode ) # create process with mapchete . open ( config , zoom = zoom , bounds = bounds ) as mp : # prepare output directory if not os . path . exists ( output_dir ) : os . makedirs ( output_dir ) # run process mp . batch_process ( zoom = [ minzoom , maxzoom ] )
998	def printConfidence ( self , aState , maxCols = 20 ) : def formatFPRow ( var , i ) : s = '' for c in range ( min ( maxCols , self . numberOfCols ) ) : if c > 0 and c % 10 == 0 : s += ' ' s += ' %5.3f' % var [ c , i ] s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatFPRow ( aState , i )
10276	def get_neurommsig_scores ( graph : BELGraph , genes : List [ Gene ] , annotation : str = 'Subgraph' , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None , preprocess : bool = False ) -> Optional [ Mapping [ str , float ] ] : if preprocess : graph = neurommsig_graph_preprocessor . run ( graph ) if not any ( gene in graph for gene in genes ) : logger . debug ( 'no genes mapping to graph' ) return subgraphs = get_subgraphs_by_annotation ( graph , annotation = annotation ) return get_neurommsig_scores_prestratified ( subgraphs = subgraphs , genes = genes , ora_weight = ora_weight , hub_weight = hub_weight , top_percent = top_percent , topology_weight = topology_weight , )
4535	def fillRGB ( self , r , g , b , start = 0 , end = - 1 ) : self . fill ( ( r , g , b ) , start , end )
2659	def initialize_scaling ( self ) : debug_opts = "--debug" if self . worker_debug else "" max_workers = "" if self . max_workers == float ( 'inf' ) else "--max_workers={}" . format ( self . max_workers ) worker_logdir = "{}/{}" . format ( self . run_dir , self . label ) if self . worker_logdir_root is not None : worker_logdir = "{}/{}" . format ( self . worker_logdir_root , self . label ) l_cmd = self . launch_cmd . format ( debug = debug_opts , prefetch_capacity = self . prefetch_capacity , task_url = self . worker_task_url , result_url = self . worker_result_url , cores_per_worker = self . cores_per_worker , max_workers = max_workers , nodes_per_block = self . provider . nodes_per_block , heartbeat_period = self . heartbeat_period , heartbeat_threshold = self . heartbeat_threshold , poll_period = self . poll_period , logdir = worker_logdir ) self . launch_cmd = l_cmd logger . debug ( "Launch command: {}" . format ( self . launch_cmd ) ) self . _scaling_enabled = self . provider . scaling_enabled logger . debug ( "Starting HighThroughputExecutor with provider:\n%s" , self . provider ) if hasattr ( self . provider , 'init_blocks' ) : try : self . scale_out ( blocks = self . provider . init_blocks ) except Exception as e : logger . error ( "Scaling out failed: {}" . format ( e ) ) raise e
2933	def write_meta_data ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'MetaData' ) config . set ( 'MetaData' , 'entry_point_process' , self . wf_spec . name ) if self . editor : config . set ( 'MetaData' , 'editor' , self . editor ) for k , v in self . meta_data : config . set ( 'MetaData' , k , v ) if not self . PARSER_CLASS == BpmnParser : config . set ( 'MetaData' , 'parser_class_module' , inspect . getmodule ( self . PARSER_CLASS ) . __name__ ) config . set ( 'MetaData' , 'parser_class' , self . PARSER_CLASS . __name__ ) ini = StringIO ( ) config . write ( ini ) self . write_to_package_zip ( self . METADATA_FILE , ini . getvalue ( ) )
13342	def concatenate ( tup , axis = 0 ) : from distob import engine if len ( tup ) is 0 : raise ValueError ( 'need at least one array to concatenate' ) first = tup [ 0 ] others = tup [ 1 : ] # allow subclasses to provide their own implementations of concatenate: if ( hasattr ( first , 'concatenate' ) and hasattr ( type ( first ) , '__array_interface__' ) ) : return first . concatenate ( others , axis ) # convert all arguments to arrays/RemoteArrays if they are not already: arrays = [ ] for ar in tup : if isinstance ( ar , DistArray ) : if axis == ar . _distaxis : arrays . extend ( ar . _subarrays ) else : # Since not yet implemented arrays distributed on more than # one axis, will fetch and re-scatter on the new axis: arrays . append ( gather ( ar ) ) elif isinstance ( ar , RemoteArray ) : arrays . append ( ar ) elif isinstance ( ar , Remote ) : arrays . append ( _remote_to_array ( ar ) ) elif hasattr ( type ( ar ) , '__array_interface__' ) : # then treat as a local ndarray arrays . append ( ar ) else : arrays . append ( np . array ( ar ) ) if all ( isinstance ( ar , np . ndarray ) for ar in arrays ) : return np . concatenate ( arrays , axis ) total_length = 0 # validate dimensions are same, except for axis of concatenation: commonshape = list ( arrays [ 0 ] . shape ) commonshape [ axis ] = None # ignore this axis for shape comparison for ar in arrays : total_length += ar . shape [ axis ] shp = list ( ar . shape ) shp [ axis ] = None if shp != commonshape : raise ValueError ( 'incompatible shapes for concatenation' ) # set sensible target block size if splitting subarrays further: blocksize = ( ( total_length - 1 ) // engine . nengines ) + 1 rarrays = [ ] for ar in arrays : if isinstance ( ar , DistArray ) : rarrays . extend ( ar . _subarrays ) elif isinstance ( ar , RemoteArray ) : rarrays . append ( ar ) else : da = _scatter_ndarray ( ar , axis , blocksize ) for ra in da . _subarrays : rarrays . append ( ra ) del da del arrays # At this point rarrays is a list of RemoteArray to be concatenated eid = rarrays [ 0 ] . _id . engine if all ( ra . _id . engine == eid for ra in rarrays ) : # Arrays to be joined are all on the same engine if eid == engine . eid : # Arrays are all local return concatenate ( [ gather ( r ) for r in rarrays ] , axis ) else : return call ( concatenate , rarrays , axis ) else : # Arrays to be joined are on different engines. # TODO: consolidate any consecutive arrays already on same engine return DistArray ( rarrays , axis )
3234	def list_targets_by_rule ( client = None , * * kwargs ) : result = client . list_targets_by_rule ( * * kwargs ) if not result . get ( "Targets" ) : result . update ( { "Targets" : [ ] } ) return result
13754	def read_from_file ( file_path , encoding = "utf-8" ) : with codecs . open ( file_path , "r" , encoding ) as f : return f . read ( )
904	def read ( cls , proto ) : # pylint: disable=W0212 anomalyLikelihood = object . __new__ ( cls ) anomalyLikelihood . _iteration = proto . iteration anomalyLikelihood . _historicalScores = collections . deque ( maxlen = proto . historicWindowSize ) for i , score in enumerate ( proto . historicalScores ) : anomalyLikelihood . _historicalScores . append ( ( i , score . value , score . anomalyScore ) ) if proto . distribution . name : # is "" when there is no distribution. anomalyLikelihood . _distribution = dict ( ) anomalyLikelihood . _distribution [ 'distribution' ] = dict ( ) anomalyLikelihood . _distribution [ 'distribution' ] [ "name" ] = proto . distribution . name anomalyLikelihood . _distribution [ 'distribution' ] [ "mean" ] = proto . distribution . mean anomalyLikelihood . _distribution [ 'distribution' ] [ "variance" ] = proto . distribution . variance anomalyLikelihood . _distribution [ 'distribution' ] [ "stdev" ] = proto . distribution . stdev anomalyLikelihood . _distribution [ "movingAverage" ] = { } anomalyLikelihood . _distribution [ "movingAverage" ] [ "windowSize" ] = proto . distribution . movingAverage . windowSize anomalyLikelihood . _distribution [ "movingAverage" ] [ "historicalValues" ] = [ ] for value in proto . distribution . movingAverage . historicalValues : anomalyLikelihood . _distribution [ "movingAverage" ] [ "historicalValues" ] . append ( value ) anomalyLikelihood . _distribution [ "movingAverage" ] [ "total" ] = proto . distribution . movingAverage . total anomalyLikelihood . _distribution [ "historicalLikelihoods" ] = [ ] for likelihood in proto . distribution . historicalLikelihoods : anomalyLikelihood . _distribution [ "historicalLikelihoods" ] . append ( likelihood ) else : anomalyLikelihood . _distribution = None anomalyLikelihood . _probationaryPeriod = proto . probationaryPeriod anomalyLikelihood . _learningPeriod = proto . learningPeriod anomalyLikelihood . _reestimationPeriod = proto . reestimationPeriod # pylint: enable=W0212 return anomalyLikelihood
2510	def handle_extracted_license ( self , extr_lic ) : lic = self . parse_only_extr_license ( extr_lic ) if lic is not None : self . doc . add_extr_lic ( lic ) return lic
8142	def scale ( self , w = 1.0 , h = 1.0 ) : from types import FloatType w0 , h0 = self . img . size if type ( w ) == FloatType : w = int ( w * w0 ) if type ( h ) == FloatType : h = int ( h * h0 ) self . img = self . img . resize ( ( w , h ) , INTERPOLATION ) self . w = w self . h = h
4447	def add_document ( self , doc_id , nosave = False , score = 1.0 , payload = None , replace = False , partial = False , language = None , * * fields ) : return self . _add_document ( doc_id , conn = None , nosave = nosave , score = score , payload = payload , replace = replace , partial = partial , language = language , * * fields )
9648	def parse_log_messages ( self , text ) : regex = r"commit ([0-9a-f]+)\nAuthor: (.*?)\n\n(.*?)(?:\n\n|$)" messages = re . findall ( regex , text , re . DOTALL ) parsed = [ ] for commit , author , message in messages : parsed . append ( ( commit [ : 10 ] , re . sub ( r"\s*<.*?>" , "" , author ) , # Remove email address if present message . strip ( ) ) ) return parsed
5711	def get_descriptor_base_path ( descriptor ) : # Infer from path/url if isinstance ( descriptor , six . string_types ) : if os . path . exists ( descriptor ) : base_path = os . path . dirname ( os . path . abspath ( descriptor ) ) else : # suppose descriptor is a URL base_path = os . path . dirname ( descriptor ) # Current dir by default else : base_path = '.' return base_path
1867	def PMOVMSKB ( cpu , op0 , op1 ) : arg0 = op0 . read ( ) arg1 = op1 . read ( ) res = 0 for i in reversed ( range ( 7 , op1 . size , 8 ) ) : res = ( res << 1 ) | ( ( arg1 >> i ) & 1 ) op0 . write ( Operators . EXTRACT ( res , 0 , op0 . size ) )
10817	def can_see_members ( self , user ) : if self . privacy_policy == PrivacyPolicy . PUBLIC : return True elif self . privacy_policy == PrivacyPolicy . MEMBERS : return self . is_member ( user ) or self . is_admin ( user ) elif self . privacy_policy == PrivacyPolicy . ADMINS : return self . is_admin ( user )
604	def _addBase ( self , position , xlabel = None , ylabel = None ) : ax = self . _fig . add_subplot ( position ) ax . set_xlabel ( xlabel ) ax . set_ylabel ( ylabel ) return ax
2536	def set_chksum ( self , doc , chk_sum ) : if chk_sum : doc . ext_document_references [ - 1 ] . check_sum = checksum . Algorithm ( 'SHA1' , chk_sum ) else : raise SPDXValueError ( 'ExternalDocumentRef::Checksum' )
8531	def of_messages ( cls , msg_a , msg_b ) : ok_to_diff , reason = cls . can_diff ( msg_a , msg_b ) if not ok_to_diff : raise ValueError ( reason ) return [ cls . of_structs ( x . value , y . value ) for x , y in zip ( msg_a . args , msg_b . args ) if x . field_type == 'struct' ]
1788	def DAS ( cpu ) : oldAL = cpu . AL oldCF = cpu . CF cpu . AF = Operators . OR ( ( cpu . AL & 0x0f ) > 9 , cpu . AF ) cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL - 6 , cpu . AL ) cpu . CF = Operators . ITE ( cpu . AF , Operators . OR ( oldCF , cpu . AL > oldAL ) , cpu . CF ) cpu . CF = Operators . ITE ( Operators . OR ( oldAL > 0x99 , oldCF ) , True , cpu . CF ) cpu . AL = Operators . ITEBV ( 8 , Operators . OR ( oldAL > 0x99 , oldCF ) , cpu . AL - 0x60 , cpu . AL ) # """ if (cpu.AL & 0x0f) > 9 or cpu.AF: cpu.AL = cpu.AL - 6; cpu.CF = Operators.OR(oldCF, cpu.AL > oldAL) cpu.AF = True else: cpu.AF = False if ((oldAL > 0x99) or oldCF): cpu.AL = cpu.AL - 0x60 cpu.CF = True """ cpu . ZF = cpu . AL == 0 cpu . SF = ( cpu . AL & 0x80 ) != 0 cpu . PF = cpu . _calculate_parity_flag ( cpu . AL )
9487	def pack_value ( index : int ) -> bytes : if PY36 : return index . to_bytes ( 1 , byteorder = "little" ) else : return index . to_bytes ( 2 , byteorder = "little" )
12252	def delete_keys ( self , * args , * * kwargs ) : ikeys = iter ( kwargs . get ( 'keys' , args [ 0 ] if args else [ ] ) ) while True : try : key = ikeys . next ( ) except StopIteration : break if isinstance ( key , basestring ) : mimicdb . backend . srem ( tpl . bucket % self . name , key ) mimicdb . backend . delete ( tpl . key % ( self . name , key ) ) elif isinstance ( key , BotoKey ) or isinstance ( key , Key ) : mimicdb . backend . srem ( tpl . bucket % self . name , key . name ) mimicdb . backend . delete ( tpl . key % ( self . name , key . name ) ) return super ( Bucket , self ) . delete_keys ( * args , * * kwargs )
7793	def set_fetcher ( self , fetcher_class ) : self . _lock . acquire ( ) try : self . _fetcher = fetcher_class finally : self . _lock . release ( )
10002	def add_path ( self , nodes , * * attr ) : if nx . __version__ [ 0 ] == "1" : return super ( ) . add_path ( nodes , * * attr ) else : return nx . add_path ( self , nodes , * * attr )
285	def plot_drawdown_underwater ( returns , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . percentage ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) running_max = np . maximum . accumulate ( df_cum_rets ) underwater = - 100 * ( ( running_max - df_cum_rets ) / running_max ) ( underwater ) . plot ( ax = ax , kind = 'area' , color = 'coral' , alpha = 0.7 , * * kwargs ) ax . set_ylabel ( 'Drawdown' ) ax . set_title ( 'Underwater plot' ) ax . set_xlabel ( '' ) return ax
10321	def microcanonical_averages ( graph , runs = 40 , spanning_cluster = True , model = 'bond' , alpha = alpha_1sigma , copy_result = True ) : try : runs = int ( runs ) except : raise ValueError ( "runs needs to be a positive integer" ) if runs <= 0 : raise ValueError ( "runs needs to be a positive integer" ) try : alpha = float ( alpha ) except : raise ValueError ( "alpha needs to be a float in the interval (0, 1)" ) if alpha <= 0.0 or alpha >= 1.0 : raise ValueError ( "alpha needs to be a float in the interval (0, 1)" ) # initial iteration # we do not need a copy of the result dictionary since we copy the values # anyway run_iterators = [ sample_states ( graph , spanning_cluster = spanning_cluster , model = model , copy_result = False ) for _ in range ( runs ) ] ret = dict ( ) for microcanonical_ensemble in zip ( * run_iterators ) : # merge cluster statistics ret [ 'n' ] = microcanonical_ensemble [ 0 ] [ 'n' ] ret [ 'N' ] = microcanonical_ensemble [ 0 ] [ 'N' ] ret [ 'M' ] = microcanonical_ensemble [ 0 ] [ 'M' ] max_cluster_size = np . empty ( runs ) moments = np . empty ( ( runs , 5 ) ) if spanning_cluster : has_spanning_cluster = np . empty ( runs ) for r , state in enumerate ( microcanonical_ensemble ) : assert state [ 'n' ] == ret [ 'n' ] assert state [ 'N' ] == ret [ 'N' ] assert state [ 'M' ] == ret [ 'M' ] max_cluster_size [ r ] = state [ 'max_cluster_size' ] moments [ r ] = state [ 'moments' ] if spanning_cluster : has_spanning_cluster [ r ] = state [ 'has_spanning_cluster' ] ret . update ( _microcanonical_average_max_cluster_size ( max_cluster_size , alpha ) ) ret . update ( _microcanonical_average_moments ( moments , alpha ) ) if spanning_cluster : ret . update ( _microcanonical_average_spanning_cluster ( has_spanning_cluster , alpha ) ) if copy_result : yield copy . deepcopy ( ret ) else : yield ret
12754	def joint_distances ( self ) : return [ ( ( np . array ( j . anchor ) - j . anchor2 ) ** 2 ) . sum ( ) for j in self . joints ]
9525	def sort_by_name ( infile , outfile ) : seqs = { } file_to_dict ( infile , seqs ) #seqs = list(seqs.values()) #seqs.sort() fout = utils . open_file_write ( outfile ) for name in sorted ( seqs ) : print ( seqs [ name ] , file = fout ) utils . close ( fout )
11422	def print_rec ( rec , format = 1 , tags = None ) : if tags is None : tags = [ ] if format == 1 : text = record_xml_output ( rec , tags ) else : return '' return text
5236	def file_modified_time ( file_name ) -> pd . Timestamp : return pd . to_datetime ( time . ctime ( os . path . getmtime ( filename = file_name ) ) )
9968	def copy ( self , space = None , name = None ) : return Cells ( space = space , name = name , formula = self . formula )
147	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_face = None , color_lines = None , color_points = None , alpha = 1.0 , alpha_face = None , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , raise_if_out_of_image = False ) : for poly in self . polygons : image = poly . draw_on_image ( image , color = color , color_face = color_face , color_lines = color_lines , color_points = color_points , alpha = alpha , alpha_face = alpha_face , alpha_lines = alpha_lines , alpha_points = alpha_points , size = size , size_lines = size_lines , size_points = size_points , raise_if_out_of_image = raise_if_out_of_image ) return image
3417	def _cell ( x ) : x_no_none = [ i if i is not None else "" for i in x ] return array ( x_no_none , dtype = np_object )
12226	def on_pref_update ( * args , * * kwargs ) : Preference . update_prefs ( * args , * * kwargs ) Preference . read_prefs ( get_prefs ( ) )
11610	def update_probability_at_read_level ( self , model = 3 ) : self . probability . reset ( ) # reset to alignment incidence matrix if model == 1 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . HAPLOGROUP , grouping_mat = self . t2t_mat ) haplogroup_sum_mat = self . allelic_expression * self . t2t_mat self . probability . multiply ( haplogroup_sum_mat , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( haplogroup_sum_mat . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 2 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . LOCUS ) self . probability . multiply ( self . allelic_expression . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( ( self . allelic_expression * self . t2t_mat ) . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 3 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( ( self . allelic_expression * self . t2t_mat ) . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 4 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . READ ) else : raise RuntimeError ( 'The read normalization model should be 1, 2, 3, or 4.' )
1987	def save_state ( self , state , key ) : with self . save_stream ( key , binary = True ) as f : self . _serializer . serialize ( state , f )
8275	def recombine ( self , other , d = 0.7 ) : a , b = self , other d1 = max ( 0 , min ( d , 1 ) ) d2 = d1 c = ColorTheme ( name = a . name [ : int ( len ( a . name ) * d1 ) ] + b . name [ int ( len ( b . name ) * d2 ) : ] , ranges = a . ranges [ : int ( len ( a . ranges ) * d1 ) ] + b . ranges [ int ( len ( b . ranges ) * d2 ) : ] , top = a . top , cache = os . path . join ( DEFAULT_CACHE , "recombined" ) , blue = a . blue , length = a . length * d1 + b . length * d2 ) c . tags = a . tags [ : int ( len ( a . tags ) * d1 ) ] c . tags += b . tags [ int ( len ( b . tags ) * d2 ) : ] return c
13303	def nmse ( a , b ) : return np . square ( a - b ) . mean ( ) / ( a . mean ( ) * b . mean ( ) )
866	def setCustomProperties ( cls , properties ) : _getLogger ( ) . info ( "Setting custom configuration properties=%r; caller=%r" , properties , traceback . format_stack ( ) ) _CustomConfigurationFileWrapper . edit ( properties ) for propertyName , value in properties . iteritems ( ) : cls . set ( propertyName , value )
1234	def atomic_observe ( self , states , actions , internals , reward , terminal ) : # TODO probably unnecessary here. self . current_terminal = terminal self . current_reward = reward # print('action = {}'.format(actions)) if self . unique_state : states = dict ( state = states ) if self . unique_action : actions = dict ( action = actions ) self . episode = self . model . atomic_observe ( states = states , actions = actions , internals = internals , terminal = self . current_terminal , reward = self . current_reward )
4495	def remove ( args ) : osf = _setup_osf ( args ) if osf . username is None or osf . password is None : sys . exit ( 'To remove a file you need to provide a username and' ' password.' ) project = osf . project ( args . project ) storage , remote_path = split_storage ( args . target ) store = project . storage ( storage ) for f in store . files : if norm_remote_path ( f . path ) == remote_path : f . remove ( )
13260	def main ( argv = None , white_list = None , load_yaz_extension = True ) : assert argv is None or isinstance ( argv , list ) , type ( argv ) assert white_list is None or isinstance ( white_list , list ) , type ( white_list ) assert isinstance ( load_yaz_extension , bool ) , type ( load_yaz_extension ) argv = sys . argv if argv is None else argv assert len ( argv ) > 0 , len ( argv ) if load_yaz_extension : load ( "~/.yaz" , "yaz_extension" ) parser = Parser ( prog = argv [ 0 ] ) parser . add_task_tree ( get_task_tree ( white_list ) ) task , kwargs = parser . parse_arguments ( argv ) if task : try : result = task ( * * kwargs ) # when the result is a boolean, exit with 0 (success) or 1 (failure) if isinstance ( result , bool ) : code = 0 if result else 1 output = None # when the result is an integer, exit with that integer value elif isinstance ( result , int ) : code = result % 256 output = None # otherwise exit with 0 (success) and print the result else : code = 0 output = result # when yaz.Error occurs, exit with the given return code and print the error message # when any other error occurs, let python handle the exception (i.e. exit(1) and print call stack) except Error as error : code = error . return_code output = error else : # when no task is found to execute, exit with 1 (failure) and print the help text code = 1 output = parser . format_help ( ) . rstrip ( ) if output is not None : print ( output ) sys . exit ( code )
3045	def _refresh ( self , http ) : if not self . store : self . _do_refresh_request ( http ) else : self . store . acquire_lock ( ) try : new_cred = self . store . locked_get ( ) if ( new_cred and not new_cred . invalid and new_cred . access_token != self . access_token and not new_cred . access_token_expired ) : logger . info ( 'Updated access_token read from Storage' ) self . _updateFromCredential ( new_cred ) else : self . _do_refresh_request ( http ) finally : self . store . release_lock ( )
8609	def list_resources ( self , resource_type = None , depth = 1 ) : if resource_type is not None : response = self . _perform_request ( '/um/resources/%s?depth=%s' % ( resource_type , str ( depth ) ) ) else : response = self . _perform_request ( '/um/resources?depth=' + str ( depth ) ) return response
9038	def bounding_box ( self ) : min_x , min_y , max_x , max_y = zip ( * list ( self . walk_rows ( lambda row : row . bounding_box ) ) ) return min ( min_x ) , min ( min_y ) , max ( max_x ) , max ( max_y )
9748	async def choose_qtm_instance ( interface ) : instances = { } print ( "Available QTM instances:" ) async for i , qtm_instance in AsyncEnumerate ( qtm . Discover ( interface ) , start = 1 ) : instances [ i ] = qtm_instance print ( "{} - {}" . format ( i , qtm_instance . info ) ) try : choice = int ( input ( "Connect to: " ) ) if choice not in instances : raise ValueError except ValueError : LOG . error ( "Invalid choice" ) return None return instances [ choice ] . host
8697	def __clear_buffers ( self ) : try : self . _port . reset_input_buffer ( ) self . _port . reset_output_buffer ( ) except AttributeError : #pySerial 2.7 self . _port . flushInput ( ) self . _port . flushOutput ( )
1593	def prepare ( self , context ) : for stream_id , targets in self . targets . items ( ) : for target in targets : target . prepare ( context , stream_id )
2525	def get_reviewer ( self , r_term ) : reviewer_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'reviewer' ] , None ) ) ) if len ( reviewer_list ) != 1 : self . error = True msg = 'Review must have exactly one reviewer' self . logger . log ( msg ) return try : return self . builder . create_entity ( self . doc , six . text_type ( reviewer_list [ 0 ] [ 2 ] ) ) except SPDXValueError : self . value_error ( 'REVIEWER_VALUE' , reviewer_list [ 0 ] [ 2 ] )
5263	def capitalcase ( string ) : string = str ( string ) if not string : return string return uppercase ( string [ 0 ] ) + string [ 1 : ]
12561	def create_rois_mask ( roislist , filelist ) : roifiles = [ ] for roi in roislist : try : roi_file = search_list ( roi , filelist ) [ 0 ] except Exception as exc : raise Exception ( 'Error creating list of roi files. \n {}' . format ( str ( exc ) ) ) else : roifiles . append ( roi_file ) return binarise ( roifiles )
12377	def make_response ( self , data = None ) : if data is not None : # Prepare the data for transmission. data = self . prepare ( data ) # Encode the data using a desired encoder. self . response . write ( data , serialize = True )
257	def print_round_trip_stats ( round_trips , hide_pos = False ) : stats = gen_round_trip_stats ( round_trips ) print_table ( stats [ 'summary' ] , float_format = '{:.2f}' . format , name = 'Summary stats' ) print_table ( stats [ 'pnl' ] , float_format = '${:.2f}' . format , name = 'PnL stats' ) print_table ( stats [ 'duration' ] , float_format = '{:.2f}' . format , name = 'Duration stats' ) print_table ( stats [ 'returns' ] * 100 , float_format = '{:.2f}%' . format , name = 'Return stats' ) if not hide_pos : stats [ 'symbols' ] . columns = stats [ 'symbols' ] . columns . map ( format_asset ) print_table ( stats [ 'symbols' ] * 100 , float_format = '{:.2f}%' . format , name = 'Symbol stats' )
2700	def text_rank ( path ) : graph = build_graph ( json_iter ( path ) ) ranks = nx . pagerank ( graph ) return graph , ranks
9062	def fix ( self , param ) : if param == "delta" : super ( ) . _fix ( "logistic" ) else : self . _fix [ param ] = True
8189	def eigenvector_centrality ( self , normalized = True , reversed = True , rating = { } , start = None , iterations = 100 , tolerance = 0.0001 ) : ec = proximity . eigenvector_centrality ( self , normalized , reversed , rating , start , iterations , tolerance ) for id , w in ec . iteritems ( ) : self [ id ] . _eigenvalue = w return ec
12452	def check_pre_requirements ( pre_requirements ) : pre_requirements = set ( pre_requirements or [ ] ) pre_requirements . add ( 'virtualenv' ) for requirement in pre_requirements : if not which ( requirement ) : print_error ( 'Requirement {0!r} is not found in system' . format ( requirement ) ) return False return True
11587	def object ( self , infotype , key ) : redisent = self . redises [ self . _getnodenamefor ( key ) + '_slave' ] return getattr ( redisent , 'object' ) ( infotype , key )
3264	def md_link ( node ) : mimetype = node . find ( "type" ) mdtype = node . find ( "metadataType" ) content = node . find ( "content" ) if None in [ mimetype , mdtype , content ] : return None else : return ( mimetype . text , mdtype . text , content . text )
3107	def _retrieve_info ( self , http ) : if self . invalid : info = _metadata . get_service_account_info ( http , service_account = self . service_account_email or 'default' ) self . invalid = False self . service_account_email = info [ 'email' ] self . scopes = info [ 'scopes' ]
11694	def full_analysis ( self ) : self . count ( ) self . verify_words ( ) self . verify_user ( ) if self . review_requested == 'yes' : self . label_suspicious ( 'Review requested' )
7218	def delete ( self , task_name ) : r = self . gbdx_connection . delete ( self . _base_url + '/' + task_name ) raise_for_status ( r ) return r . text
7821	def challenge ( self , challenge ) : # pylint: disable=R0911 if not challenge : logger . debug ( "Empty challenge" ) return Failure ( "bad-challenge" ) if self . _server_first_message : return self . _final_challenge ( challenge ) match = SERVER_FIRST_MESSAGE_RE . match ( challenge ) if not match : logger . debug ( "Bad challenge syntax: {0!r}" . format ( challenge ) ) return Failure ( "bad-challenge" ) self . _server_first_message = challenge mext = match . group ( "mext" ) if mext : logger . debug ( "Unsupported extension received: {0!r}" . format ( mext ) ) return Failure ( "bad-challenge" ) nonce = match . group ( "nonce" ) if not nonce . startswith ( self . _c_nonce ) : logger . debug ( "Nonce does not start with our nonce" ) return Failure ( "bad-challenge" ) salt = match . group ( "salt" ) try : salt = a2b_base64 ( salt ) except ValueError : logger . debug ( "Bad base64 encoding for salt: {0!r}" . format ( salt ) ) return Failure ( "bad-challenge" ) iteration_count = match . group ( "iteration_count" ) try : iteration_count = int ( iteration_count ) except ValueError : logger . debug ( "Bad iteration_count: {0!r}" . format ( iteration_count ) ) return Failure ( "bad-challenge" ) return self . _make_response ( nonce , salt , iteration_count )
2584	def get_tasks ( self , count ) : tasks = [ ] for i in range ( 0 , count ) : try : x = self . pending_task_queue . get ( block = False ) except queue . Empty : break else : tasks . append ( x ) return tasks
8257	def _average ( self ) : r , g , b , a = 0 , 0 , 0 , 0 for clr in self : r += clr . r g += clr . g b += clr . b a += clr . alpha r /= len ( self ) g /= len ( self ) b /= len ( self ) a /= len ( self ) return color ( r , g , b , a , mode = "rgb" )
4428	async def _now ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) song = 'Nothing' if player . current : position = lavalink . Utils . format_time ( player . position ) if player . current . stream : duration = '๐ด LIVE' else : duration = lavalink . Utils . format_time ( player . current . duration ) song = f'**[{player.current.title}]({player.current.uri})**\n({position}/{duration})' embed = discord . Embed ( color = discord . Color . blurple ( ) , title = 'Now Playing' , description = song ) await ctx . send ( embed = embed )
8612	def list_volumes ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/volumes?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
10896	def load_image ( self ) : try : image = initializers . load_tiff ( self . filename ) image = initializers . normalize ( image , invert = self . invert , scale = self . exposure , dtype = self . float_precision ) except IOError as e : log . error ( "Could not find image '%s'" % self . filename ) raise e return image
8613	def delete_volume ( self , datacenter_id , volume_id ) : response = self . _perform_request ( url = '/datacenters/%s/volumes/%s' % ( datacenter_id , volume_id ) , method = 'DELETE' ) return response
12353	def rename ( self , name , wait = True ) : return self . _action ( 'rename' , name = name , wait = wait )
8409	def _extend_breaks ( self , major ) : trans = self . trans trans = trans if isinstance ( trans , type ) else trans . __class__ # so far we are only certain about this extending stuff # making sense for log transform is_log = trans . __name__ . startswith ( 'log' ) diff = np . diff ( major ) step = diff [ 0 ] if is_log and all ( diff == step ) : major = np . hstack ( [ major [ 0 ] - step , major , major [ - 1 ] + step ] ) return major
13866	def fromtsms ( ts , tzin = None , tzout = None ) : if ts is None : return None when = datetime . utcfromtimestamp ( ts / 1000 ) . replace ( microsecond = ts % 1000 * 1000 ) when = when . replace ( tzinfo = tzin or utc ) return totz ( when , tzout )
659	def populationStability ( vectors , numSamples = None ) : # ---------------------------------------------------------------------- # Calculate the stability numVectors = len ( vectors ) if numSamples is None : numSamples = numVectors - 1 countOn = range ( numVectors - 1 ) else : countOn = numpy . random . randint ( 0 , numVectors - 1 , numSamples ) sigmap = 0.0 for i in countOn : match = checkMatch ( vectors [ i ] , vectors [ i + 1 ] , sparse = False ) # Ignore reset vectors (all 0's) if match [ 1 ] != 0 : sigmap += float ( match [ 0 ] ) / match [ 1 ] return sigmap / numSamples
9178	def obtain_licenses ( ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( """\ SELECT combined_row.url, row_to_json(combined_row) FROM ( SELECT "code", "version", "name", "url", "is_valid_for_publication" FROM licenses) AS combined_row""" ) licenses = { r [ 0 ] : r [ 1 ] for r in cursor . fetchall ( ) } return licenses
1199	def get_variables ( self , include_nontrainable = False ) : if include_nontrainable : return [ self . all_variables [ key ] for key in sorted ( self . all_variables ) ] else : return [ self . variables [ key ] for key in sorted ( self . variables ) ]
5787	def _advapi32_encrypt ( cipher , key , data , iv , padding ) : context_handle = None key_handle = None try : context_handle , key_handle = _advapi32_create_handles ( cipher , key , iv ) out_len = new ( advapi32 , 'DWORD *' , len ( data ) ) res = advapi32 . CryptEncrypt ( key_handle , null ( ) , True , 0 , null ( ) , out_len , 0 ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) write_to_buffer ( buffer , data ) pointer_set ( out_len , len ( data ) ) res = advapi32 . CryptEncrypt ( key_handle , null ( ) , True , 0 , buffer , out_len , buffer_len ) handle_error ( res ) output = bytes_from_buffer ( buffer , deref ( out_len ) ) # Remove padding when not required. CryptoAPI doesn't support this, so # we just manually remove it. if cipher == 'aes' and not padding : if output [ - 16 : ] != ( b'\x10' * 16 ) : raise ValueError ( 'Invalid padding generated by OS crypto library' ) output = output [ : - 16 ] return output finally : if key_handle : advapi32 . CryptDestroyKey ( key_handle ) if context_handle : close_context_handle ( context_handle )
9007	def transfer_to_row ( self , new_row ) : if new_row != self . _row : index = self . get_index_in_row ( ) if index is not None : self . _row . instructions . pop ( index ) self . _row = new_row
7114	def fit ( self , X , y ) : #################### # Data Loader #################### word_vector_transformer = WordVectorTransformer ( padding = 'max' ) X = word_vector_transformer . fit_transform ( X ) X = LongTensor ( X ) self . word_vector_transformer = word_vector_transformer y_transformer = LabelEncoder ( ) y = y_transformer . fit_transform ( y ) y = torch . from_numpy ( y ) self . y_transformer = y_transformer dataset = CategorizedDataset ( X , y ) dataloader = DataLoader ( dataset , batch_size = self . batch_size , shuffle = True , num_workers = 4 ) #################### # Model #################### KERNEL_SIZES = self . kernel_sizes NUM_KERNEL = self . num_kernel EMBEDDING_DIM = self . embedding_dim model = TextCNN ( vocab_size = word_vector_transformer . get_vocab_size ( ) , embedding_dim = EMBEDDING_DIM , output_size = len ( self . y_transformer . classes_ ) , kernel_sizes = KERNEL_SIZES , num_kernel = NUM_KERNEL ) if USE_CUDA : model = model . cuda ( ) #################### # Train #################### EPOCH = self . epoch LR = self . lr loss_function = nn . CrossEntropyLoss ( ) optimizer = optim . Adam ( model . parameters ( ) , lr = LR ) for epoch in range ( EPOCH ) : losses = [ ] for i , data in enumerate ( dataloader ) : X , y = data X , y = Variable ( X ) , Variable ( y ) optimizer . zero_grad ( ) model . train ( ) output = model ( X ) loss = loss_function ( output , y ) losses . append ( loss . data . tolist ( ) [ 0 ] ) loss . backward ( ) optimizer . step ( ) if i % 100 == 0 : print ( "[%d/%d] mean_loss : %0.2f" % ( epoch , EPOCH , np . mean ( losses ) ) ) losses = [ ] self . model = model
7640	def convert_jams ( jams_file , output_prefix , csv = False , comment_char = '#' , namespaces = None ) : if namespaces is None : raise ValueError ( 'No namespaces provided. Try ".*" for all namespaces.' ) jam = jams . load ( jams_file ) # Get all the annotations # Filter down to the unique ones # For each annotation # generate the comment string # generate the output filename # dump to csv # Make a counter object for each namespace type counter = collections . Counter ( ) annotations = [ ] for query in namespaces : annotations . extend ( jam . search ( namespace = query ) ) if csv : suffix = 'csv' sep = ',' else : suffix = 'lab' sep = '\t' for ann in annotations : index = counter [ ann . namespace ] counter [ ann . namespace ] += 1 filename = os . path . extsep . join ( [ get_output_name ( output_prefix , ann . namespace , index ) , suffix ] ) comment = get_comments ( jam , ann ) # Dump to disk lab_dump ( ann , comment , filename , sep , comment_char )
11212	def _hash ( secret : bytes , data : bytes , alg : str ) -> bytes : algorithm = get_algorithm ( alg ) return hmac . new ( secret , msg = data , digestmod = algorithm ) . digest ( )
2356	def is_element_present ( self , strategy , locator ) : return self . driver_adapter . is_element_present ( strategy , locator , root = self . root )
6886	def mdwarf_subtype_from_sdsscolor ( ri_color , iz_color ) : # calculate the spectral type index and the spectral type spread of the # object. sti is calculated by fitting a line to the locus in r-i and i-z # space for M dwarfs in West+ 2007 if np . isfinite ( ri_color ) and np . isfinite ( iz_color ) : obj_sti = 0.875274 * ri_color + 0.483628 * ( iz_color + 0.00438 ) obj_sts = - 0.483628 * ri_color + 0.875274 * ( iz_color + 0.00438 ) else : obj_sti = np . nan obj_sts = np . nan # possible M star if sti is >= 0.666 but <= 3.4559 if ( np . isfinite ( obj_sti ) and np . isfinite ( obj_sts ) and ( obj_sti > 0.666 ) and ( obj_sti < 3.4559 ) ) : # decide which M subclass object this is if ( ( obj_sti > 0.6660 ) and ( obj_sti < 0.8592 ) ) : m_class = 'M0' if ( ( obj_sti > 0.8592 ) and ( obj_sti < 1.0822 ) ) : m_class = 'M1' if ( ( obj_sti > 1.0822 ) and ( obj_sti < 1.2998 ) ) : m_class = 'M2' if ( ( obj_sti > 1.2998 ) and ( obj_sti < 1.6378 ) ) : m_class = 'M3' if ( ( obj_sti > 1.6378 ) and ( obj_sti < 2.0363 ) ) : m_class = 'M4' if ( ( obj_sti > 2.0363 ) and ( obj_sti < 2.2411 ) ) : m_class = 'M5' if ( ( obj_sti > 2.2411 ) and ( obj_sti < 2.4126 ) ) : m_class = 'M6' if ( ( obj_sti > 2.4126 ) and ( obj_sti < 2.9213 ) ) : m_class = 'M7' if ( ( obj_sti > 2.9213 ) and ( obj_sti < 3.2418 ) ) : m_class = 'M8' if ( ( obj_sti > 3.2418 ) and ( obj_sti < 3.4559 ) ) : m_class = 'M9' else : m_class = None return m_class , obj_sti , obj_sts
541	def __deleteModelCheckpoint ( self , modelID ) : checkpointID = self . _jobsDAO . modelsGetFields ( modelID , [ 'modelCheckpointId' ] ) [ 0 ] if checkpointID is None : return try : shutil . rmtree ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) except : self . _logger . warn ( "Failed to delete model checkpoint %s. " "Assuming that another worker has already deleted it" , checkpointID ) return self . _jobsDAO . modelSetFields ( modelID , { 'modelCheckpointId' : None } , ignoreUnchanged = True ) return
7543	def chunk_clusters ( data , sample ) : ## counter for split job submission num = 0 ## set optim size for chunks in N clusters. The first few chunks take longer ## because they contain larger clusters, so we create 4X as many chunks as ## processors so that they are split more evenly. optim = int ( ( sample . stats . clusters_total // data . cpus ) + ( sample . stats . clusters_total % data . cpus ) ) ## break up the file into smaller tmp files for each engine ## chunking by cluster is a bit trickier than chunking by N lines chunkslist = [ ] ## open to clusters with gzip . open ( sample . files . clusters , 'rb' ) as clusters : ## create iterator to sample 2 lines at a time pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) ## Use iterator to sample til end of cluster done = 0 while not done : ## grab optim clusters and write to file. done , chunk = clustdealer ( pairdealer , optim ) chunkhandle = os . path . join ( data . dirs . clusts , "tmp_" + str ( sample . name ) + "." + str ( num * optim ) ) if chunk : chunkslist . append ( ( optim , chunkhandle ) ) with open ( chunkhandle , 'wb' ) as outchunk : outchunk . write ( "//\n//\n" . join ( chunk ) + "//\n//\n" ) num += 1 return chunkslist
1202	def execute ( self , action ) : adjusted_action = list ( ) for action_spec in self . level . action_spec ( ) : if action_spec [ 'min' ] == - 1 and action_spec [ 'max' ] == 1 : adjusted_action . append ( action [ action_spec [ 'name' ] ] - 1 ) else : adjusted_action . append ( action [ action_spec [ 'name' ] ] ) # clip? action = np . array ( adjusted_action , dtype = np . intc ) reward = self . level . step ( action = action , num_steps = self . repeat_action ) state = self . level . observations ( ) [ 'RGB_INTERLACED' ] terminal = not self . level . is_running ( ) return state , terminal , reward
1779	def AAA ( cpu ) : cpu . AF = Operators . OR ( cpu . AL & 0x0F > 9 , cpu . AF ) cpu . CF = cpu . AF cpu . AH = Operators . ITEBV ( 8 , cpu . AF , cpu . AH + 1 , cpu . AH ) cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL + 6 , cpu . AL ) """ if (cpu.AL & 0x0F > 9) or cpu.AF == 1: cpu.AL = cpu.AL + 6 cpu.AH = cpu.AH + 1 cpu.AF = True cpu.CF = True else: cpu.AF = False cpu.CF = False """ cpu . AL = cpu . AL & 0x0f
2436	def add_reviewer ( self , doc , reviewer ) : # Each reviewer marks the start of a new review object. # FIXME: this state does not make sense self . reset_reviews ( ) if validations . validate_reviewer ( reviewer ) : doc . add_review ( review . Review ( reviewer = reviewer ) ) return True else : raise SPDXValueError ( 'Review::Reviewer' )
4991	def post ( self , request , * args , * * kwargs ) : # pylint: disable=unused-variable enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( * * kwargs ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_customer_uuid ) if course_key : context_data = get_global_context ( request , enterprise_customer ) try : kwargs [ 'course_id' ] = RouterView . get_course_run_id ( request . user , enterprise_customer , course_key ) except Http404 : error_code = 'ENTRV001' log_message = ( 'Could not find course run with id {course_run_id} ' 'for course key {course_key} and ' 'for enterprise_customer_uuid {enterprise_customer_uuid} ' 'and program {program_uuid}. ' 'Returned error code {error_code} to user {userid}' . format ( course_key = course_key , course_run_id = course_run_id , enterprise_customer_uuid = enterprise_customer_uuid , error_code = error_code , userid = request . user . id , program_uuid = program_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) return self . redirect ( request , * args , * * kwargs )
693	def loadExperiment ( path ) : if not os . path . isdir ( path ) : path = os . path . dirname ( path ) descriptionPyModule = loadExperimentDescriptionScriptFromDir ( path ) expIface = getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) return expIface . getModelDescription ( ) , expIface . getModelControl ( )
9718	async def stream_frames_stop ( self ) : self . _protocol . set_on_packet ( None ) cmd = "streamframes stop" await self . _protocol . send_command ( cmd , callback = False )
11127	def update_file ( self , value , relativePath , name = None , description = False , klass = False , dump = False , pull = False , ACID = None , verbose = False ) : # check ACID if ACID is None : ACID = self . __ACID assert isinstance ( ACID , bool ) , "ACID must be boolean" # get relative path normalized relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' assert name != '.pyrepinfo' , "'.pyrepinfo' is not allowed as file name in main repository directory" assert name != '.pyrepstate' , "'.pyrepstate' is not allowed as file name in main repository directory" assert name != '.pyreplock' , "'.pyreplock' is not allowed as file name in main repository directory" if name is None : assert len ( relativePath ) , "name must be given when relative path is given as empty string or as a simple dot '.'" relativePath , name = os . path . split ( relativePath ) # get file info dict fileInfoDict , errorMessage = self . get_file_info ( relativePath , name ) assert fileInfoDict is not None , errorMessage # get real path realPath = os . path . join ( self . __path , relativePath ) # check if file exists if verbose : if not os . path . isfile ( os . path . join ( realPath , name ) ) : warnings . warn ( "file '%s' is in repository but does not exist in the system. It is therefore being recreated." % os . path . join ( realPath , name ) ) # convert dump and pull methods to strings if not dump : dump = fileInfoDict [ "dump" ] if not pull : pull = fileInfoDict [ "pull" ] # get savePath if ACID : savePath = os . path . join ( tempfile . gettempdir ( ) , name ) else : savePath = os . path . join ( realPath , name ) # dump file try : exec ( dump . replace ( "$FILE_PATH" , str ( savePath ) ) ) except Exception as e : message = "unable to dump the file (%s)" % e if 'pickle.dump(' in dump : message += '\nmore info: %s' % str ( get_pickling_errors ( value ) ) raise Exception ( message ) # copy if ACID if ACID : try : shutil . copyfile ( savePath , os . path . join ( realPath , name ) ) except Exception as e : os . remove ( savePath ) if verbose : warnings . warn ( e ) return os . remove ( savePath ) # update timestamp fileInfoDict [ "timestamp" ] = datetime . utcnow ( ) if description is not False : fileInfoDict [ "description" ] = description if klass is not False : assert inspect . isclass ( klass ) , "klass must be a class definition" fileInfoDict [ "class" ] = klass # save repository self . save ( )
9027	def _width ( self ) : layout = self . _instruction . get ( GRID_LAYOUT ) if layout is not None : width = layout . get ( WIDTH ) if width is not None : return width return self . _instruction . number_of_consumed_meshes
9053	def posteriori_mean ( self ) : from numpy_sugar . linalg import rsolve Sigma = self . posteriori_covariance ( ) eta = self . _ep . _posterior . eta return dot ( Sigma , eta + rsolve ( GLMM . covariance ( self ) , self . mean ( ) ) )
4638	def shared_blockchain_instance ( self ) : if not self . _sharedInstance . instance : klass = self . get_instance_class ( ) self . _sharedInstance . instance = klass ( * * self . _sharedInstance . config ) return self . _sharedInstance . instance
1167	def _dump_registry ( cls , file = None ) : print >> file , "Class: %s.%s" % ( cls . __module__ , cls . __name__ ) print >> file , "Inv.counter: %s" % ABCMeta . _abc_invalidation_counter for name in sorted ( cls . __dict__ . keys ( ) ) : if name . startswith ( "_abc_" ) : value = getattr ( cls , name ) print >> file , "%s: %r" % ( name , value )
11412	def record_get_field ( rec , tag , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) for field in rec [ tag ] : if field [ 4 ] == field_position_global : return field raise InvenioBibRecordFieldError ( "No field has the tag '%s' and the " "global field position '%d'." % ( tag , field_position_global ) ) else : try : return rec [ tag ] [ field_position_local ] except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
629	def _bitForCoordinate ( cls , coordinate , n ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getUInt32 ( n )
8830	def segment_allocation_find ( context , lock_mode = False , * * filters ) : range_ids = filters . pop ( "segment_allocation_range_ids" , None ) query = context . session . query ( models . SegmentAllocation ) if lock_mode : query = query . with_lockmode ( "update" ) query = query . filter_by ( * * filters ) # Optionally filter by given list of range ids if range_ids : query . filter ( models . SegmentAllocation . segment_allocation_range_id . in_ ( range_ids ) ) return query
4225	def load_config ( ) : filename = 'keyringrc.cfg' keyring_cfg = os . path . join ( platform . config_root ( ) , filename ) if not os . path . exists ( keyring_cfg ) : return config = configparser . RawConfigParser ( ) config . read ( keyring_cfg ) _load_keyring_path ( config ) # load the keyring class name, and then load this keyring try : if config . has_section ( "backend" ) : keyring_name = config . get ( "backend" , "default-keyring" ) . strip ( ) else : raise configparser . NoOptionError ( 'backend' , 'default-keyring' ) except ( configparser . NoOptionError , ImportError ) : logger = logging . getLogger ( 'keyring' ) logger . warning ( "Keyring config file contains incorrect values.\n" + "Config file: %s" % keyring_cfg ) return return load_keyring ( keyring_name )
11474	def login ( email = None , password = None , api_key = None , application = 'Default' , url = None , verify_ssl_certificate = True ) : try : input_ = raw_input except NameError : input_ = input if url is None : url = input_ ( 'Server URL: ' ) url = url . rstrip ( '/' ) if session . communicator is None : session . communicator = Communicator ( url ) else : session . communicator . url = url session . communicator . verify_ssl_certificate = verify_ssl_certificate if email is None : email = input_ ( 'Email: ' ) session . email = email if api_key is None : if password is None : password = getpass . getpass ( ) session . api_key = session . communicator . get_default_api_key ( session . email , password ) session . application = 'Default' else : session . api_key = api_key session . application = application return renew_token ( )
5671	def plot_temporal_distance_cdf ( self ) : xvalues , cdf = self . profile_block_analyzer . _temporal_distance_cdf ( ) fig = plt . figure ( ) ax = fig . add_subplot ( 111 ) xvalues = numpy . array ( xvalues ) / 60.0 ax . plot ( xvalues , cdf , "-k" ) ax . fill_between ( xvalues , cdf , color = "red" , alpha = 0.2 ) ax . set_ylabel ( "CDF(t)" ) ax . set_xlabel ( "Temporal distance t (min)" ) return fig
6968	def smooth_magseries_savgol ( mags , windowsize , polyorder = 2 ) : smoothed = savgol_filter ( mags , windowsize , polyorder ) return smoothed
1371	def get_heron_dir ( ) : go_above_dirs = 9 path = "/" . join ( os . path . realpath ( __file__ ) . split ( '/' ) [ : - go_above_dirs ] ) return normalized_class_path ( path )
2662	def _hold_block ( self , block_id ) : managers = self . connected_managers for manager in managers : if manager [ 'block_id' ] == block_id : logger . debug ( "[HOLD_BLOCK]: Sending hold to manager:{}" . format ( manager [ 'manager' ] ) ) self . hold_worker ( manager [ 'manager' ] )
1566	def invoke_hook_spout_fail ( self , message_id , fail_latency_ns ) : if len ( self . task_hooks ) > 0 : spout_fail_info = SpoutFailInfo ( message_id = message_id , spout_task_id = self . get_task_id ( ) , fail_latency_ms = fail_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . spout_fail ( spout_fail_info )
6622	def configure ( self , component , all_dependencies ) : r = { } builddir = self . buildroot # only dependencies which are actually valid can contribute to the # config data (which includes the versions of all dependencies in its # build info) if the dependencies aren't available we can't tell what # version they are. Anything missing here should always be a test # dependency that isn't going to be used, otherwise the yotta build # command will fail before we get here available_dependencies = OrderedDict ( ( k , v ) for k , v in all_dependencies . items ( ) if v ) self . set_toplevel_definitions = '' if self . build_info_include_file is None : self . build_info_include_file , build_info_definitions = self . getBuildInfo ( component . path , builddir ) self . set_toplevel_definitions += build_info_definitions if self . config_include_file is None : self . config_include_file , config_definitions , self . config_json_file = self . _getConfigData ( available_dependencies , component , builddir , self . build_info_include_file ) self . set_toplevel_definitions += config_definitions self . configured = True return { 'merged_config_include' : self . config_include_file , 'merged_config_json' : self . config_json_file , 'build_info_include' : self . build_info_include_file }
3346	def guess_mime_type ( url ) : ( mimetype , _mimeencoding ) = mimetypes . guess_type ( url ) if not mimetype : ext = os . path . splitext ( url ) [ 1 ] mimetype = _MIME_TYPES . get ( ext ) _logger . debug ( "mimetype({}): {}" . format ( url , mimetype ) ) if not mimetype : mimetype = "application/octet-stream" return mimetype
8026	def getPaths ( roots , ignores = None ) : paths , count , ignores = [ ] , 0 , ignores or [ ] # Prepare the ignores list for most efficient use ignore_re = multiglob_compile ( ignores , prefix = False ) for root in roots : # For safety, only use absolute, real paths. root = os . path . realpath ( root ) # Handle directly-referenced filenames properly # (And override ignores to "do as I mean, not as I say") if os . path . isfile ( root ) : paths . append ( root ) continue for fldr in os . walk ( root ) : out . write ( "Gathering file paths to compare... (%d files examined)" % count ) # Don't even descend into IGNOREd directories. for subdir in fldr [ 1 ] : dirpath = os . path . join ( fldr [ 0 ] , subdir ) if ignore_re . match ( dirpath ) : fldr [ 1 ] . remove ( subdir ) for filename in fldr [ 2 ] : filepath = os . path . join ( fldr [ 0 ] , filename ) if ignore_re . match ( filepath ) : continue # Skip IGNOREd files. paths . append ( filepath ) count += 1 out . write ( "Found %s files to be compared for duplication." % ( len ( paths ) ) , newline = True ) return paths
1210	def table ( self , header , body ) : table = '\n.. list-table::\n' if header and not header . isspace ( ) : table = ( table + self . indent + ':header-rows: 1\n\n' + self . _indent_block ( header ) + '\n' ) else : table = table + '\n' table = table + self . _indent_block ( body ) + '\n\n' return table
10919	def do_levmarq ( s , param_names , damping = 0.1 , decrease_damp_factor = 10. , run_length = 6 , eig_update = True , collect_stats = False , rz_order = 0 , run_type = 2 , * * kwargs ) : if rz_order > 0 : aug = AugmentedState ( s , param_names , rz_order = rz_order ) lm = LMAugmentedState ( aug , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , * * kwargs ) else : lm = LMGlobals ( s , param_names , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , * * kwargs ) if run_type == 2 : lm . do_run_2 ( ) elif run_type == 1 : lm . do_run_1 ( ) else : raise ValueError ( 'run_type=1,2 only' ) if collect_stats : return lm . get_termination_stats ( )
12026	def abfIDfromFname ( fname ) : fname = os . path . abspath ( fname ) basename = os . path . basename ( fname ) return os . path . splitext ( basename ) [ 0 ]
3012	def locked_put ( self , credentials ) : filters = { self . key_name : self . key_value } query = self . session . query ( self . model_class ) . filter_by ( * * filters ) entity = query . first ( ) if not entity : entity = self . model_class ( * * filters ) setattr ( entity , self . property_name , credentials ) self . session . add ( entity )
11597	def _rc_dbsize ( self ) : result = 0 for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result += redisent . dbsize ( ) return result
11811	def score ( self , word , docid ) : ## There are many options; here we take a very simple approach return ( math . log ( 1 + self . index [ word ] [ docid ] ) / math . log ( 1 + self . documents [ docid ] . nwords ) )
11220	def compare ( self , jwt : 'Jwt' , compare_dates : bool = False ) -> bool : if self . secret != jwt . secret : return False if self . payload != jwt . payload : return False if self . alg != jwt . alg : return False if self . header != jwt . header : return False expected_claims = self . registered_claims actual_claims = jwt . registered_claims if not compare_dates : strip = [ 'exp' , 'nbf' , 'iat' ] expected_claims = { k : { v if k not in strip else None } for k , v in expected_claims . items ( ) } actual_claims = { k : { v if k not in strip else None } for k , v in actual_claims . items ( ) } if expected_claims != actual_claims : return False return True
603	def add2DArray ( self , data , position = 111 , xlabel = None , ylabel = None , cmap = None , aspect = "auto" , interpolation = "nearest" , name = None ) : if cmap is None : # The default colormodel is an ugly blue-red model. cmap = cm . Greys ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . imshow ( data , cmap = cmap , aspect = aspect , interpolation = interpolation ) if self . _show : plt . draw ( ) if name is not None : if not os . path . exists ( "log" ) : os . mkdir ( "log" ) plt . savefig ( "log/{name}.png" . format ( name = name ) , bbox_inches = "tight" , figsize = ( 8 , 6 ) , dpi = 400 )
7777	def __make_fn ( self ) : s = [ ] if self . n . prefix : s . append ( self . n . prefix ) if self . n . given : s . append ( self . n . given ) if self . n . middle : s . append ( self . n . middle ) if self . n . family : s . append ( self . n . family ) if self . n . suffix : s . append ( self . n . suffix ) s = u" " . join ( s ) self . content [ "FN" ] = VCardString ( "FN" , s , empty_ok = True )
2500	def validate ( self , messages ) : messages = self . validate_creators ( messages ) messages = self . validate_created ( messages ) return messages
963	def _getScaledValue ( self , inpt ) : if inpt == SENTINEL_VALUE_FOR_MISSING_DATA : return None else : val = inpt if val < self . minval : val = self . minval elif val > self . maxval : val = self . maxval scaledVal = math . log10 ( val ) return scaledVal
13582	def format_to_csv ( filename , skiprows = 0 , delimiter = "" ) : if not delimiter : delimiter = "\t" input_file = open ( filename , "r" ) if skiprows : [ input_file . readline ( ) for _ in range ( skiprows ) ] new_filename = os . path . splitext ( filename ) [ 0 ] + ".csv" output_file = open ( new_filename , "w" ) header = input_file . readline ( ) . split ( ) reader = csv . DictReader ( input_file , fieldnames = header , delimiter = delimiter ) writer = csv . DictWriter ( output_file , fieldnames = header , delimiter = "," ) # Write header writer . writerow ( dict ( ( x , x ) for x in header ) ) # Write rows for line in reader : if None in line : del line [ None ] writer . writerow ( line ) input_file . close ( ) output_file . close ( ) print "Saved %s." % new_filename
1389	def get_machines ( self ) : if self . physical_plan : stmgrs = list ( self . physical_plan . stmgrs ) return map ( lambda s : s . host_name , stmgrs ) return [ ]
11675	def bare ( self ) : if not self . meta : return self elif self . stacked : return Features ( self . stacked_features , self . n_pts , copy = False ) else : return Features ( self . features , copy = False )
10295	def get_undefined_namespaces ( graph : BELGraph ) -> Set [ str ] : return { exc . namespace for _ , exc , _ in graph . warnings if isinstance ( exc , UndefinedNamespaceWarning ) }
5617	def clean_geometry_type ( geometry , target_type , allow_multipart = True ) : multipart_geoms = { "Point" : MultiPoint , "LineString" : MultiLineString , "Polygon" : MultiPolygon , "MultiPoint" : MultiPoint , "MultiLineString" : MultiLineString , "MultiPolygon" : MultiPolygon } if target_type not in multipart_geoms . keys ( ) : raise TypeError ( "target type is not supported: %s" % target_type ) if geometry . geom_type == target_type : return geometry elif allow_multipart : target_multipart_type = multipart_geoms [ target_type ] if geometry . geom_type == "GeometryCollection" : return target_multipart_type ( [ clean_geometry_type ( g , target_type , allow_multipart ) for g in geometry ] ) elif any ( [ isinstance ( geometry , target_multipart_type ) , multipart_geoms [ geometry . geom_type ] == target_multipart_type ] ) : return geometry raise GeometryTypeError ( "geometry type does not match: %s, %s" % ( geometry . geom_type , target_type ) )
3862	def _get_default_delivery_medium ( self ) : medium_options = ( self . _conversation . self_conversation_state . delivery_medium_option ) try : default_medium = medium_options [ 0 ] . delivery_medium except IndexError : logger . warning ( 'Conversation %r has no delivery medium' , self . id_ ) default_medium = hangouts_pb2 . DeliveryMedium ( medium_type = hangouts_pb2 . DELIVERY_MEDIUM_BABEL ) for medium_option in medium_options : if medium_option . current_default : default_medium = medium_option . delivery_medium return default_medium
3235	def list_buckets ( client = None , * * kwargs ) : buckets = client . list_buckets ( * * kwargs ) return [ b . __dict__ for b in buckets ]
4083	def check ( self , text : str , srctext = None ) -> [ Match ] : root = self . _get_root ( self . _url , self . _encode ( text , srctext ) ) return [ Match ( e . attrib ) for e in root if e . tag == 'error' ]
5591	def tiles_from_bounds ( self , bounds , zoom ) : for tile in self . tiles_from_bbox ( box ( * bounds ) , zoom ) : yield self . tile ( * tile . id )
7591	def run ( self , force = False , ipyclient = None , name_fields = 30 , name_separator = "_" , dry_run = False ) : ## temporarily set directory for tmpfiles used by fastq-dump ## if this fails then just skip it. try : ## ensure output directory, also used as tmpdir if not os . path . exists ( self . workdir ) : os . makedirs ( self . workdir ) ## get original directory for sra files ## probably /home/ncbi/public/sra by default. self . _set_vdbconfig_path ( ) ## register ipyclient for cleanup if ipyclient : self . _ipcluster [ "pids" ] = { } for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : pid = engine . apply ( os . getpid ) . get ( ) self . _ipcluster [ "pids" ] [ eid ] = pid ## submit jobs to engines or local self . _submit_jobs ( force = force , ipyclient = ipyclient , name_fields = name_fields , name_separator = name_separator , dry_run = dry_run , ) except IPyradWarningExit as inst : print ( inst ) ## exceptions to catch, cleanup and handle ipyclient interrupts except KeyboardInterrupt : print ( "keyboard interrupt..." ) except Exception as inst : print ( "Exception in run() - {}" . format ( inst ) ) finally : ## reset working sra path self . _restore_vdbconfig_path ( ) ## if it made a new sra directory then it should be empty when ## we are finished if all .sra files were removed. If so, then ## let's also remove the dir. if not empty, leave it. sradir = os . path . join ( self . workdir , "sra" ) if os . path . exists ( sradir ) and ( not os . listdir ( sradir ) ) : shutil . rmtree ( sradir ) else : ## print warning try : print ( FAILED_DOWNLOAD . format ( os . listdir ( sradir ) ) ) except OSError as inst : ## If sra dir doesn't even exist something very bad is broken. raise IPyradWarningExit ( "Download failed. Exiting." ) ## remove fastq file matching to cached sra file for srr in os . listdir ( sradir ) : isrr = srr . split ( "." ) [ 0 ] ipath = os . path . join ( self . workdir , "*_{}*.gz" . format ( isrr ) ) ifile = glob . glob ( ipath ) [ 0 ] if os . path . exists ( ifile ) : os . remove ( ifile ) ## remove cache of sra files shutil . rmtree ( sradir ) ## cleanup ipcluster shutdown if ipyclient : ## send SIGINT (2) to all engines still running tasks try : ipyclient . abort ( ) time . sleep ( 0.5 ) for engine_id , pid in self . _ipcluster [ "pids" ] . items ( ) : if ipyclient . queue_status ( ) [ engine_id ] [ "tasks" ] : os . kill ( pid , 2 ) time . sleep ( 0.1 ) except ipp . NoEnginesRegistered : pass ## clean memory space if not ipyclient . outstanding : ipyclient . purge_everything ( ) ## uh oh, kill everything, something bad happened else : ipyclient . shutdown ( hub = True , block = False ) ipyclient . close ( ) print ( "\nwarning: ipcluster shutdown and must be restarted" )
10425	def infer_missing_backwards_edge ( graph , u , v , k ) : if u in graph [ v ] : for attr_dict in graph [ v ] [ u ] . values ( ) : if attr_dict == graph [ u ] [ v ] [ k ] : return graph . add_edge ( v , u , key = k , * * graph [ u ] [ v ] [ k ] )
8768	def add_job_to_context ( context , job_id ) : db_job = db_api . async_transaction_find ( context , id = job_id , scope = db_api . ONE ) if not db_job : return context . async_job = { "job" : v . _make_job_dict ( db_job ) }
535	def writeToProto ( self , proto ) : proto . implementation = self . implementation proto . steps = self . steps proto . alpha = self . alpha proto . verbosity = self . verbosity proto . maxCategoryCount = self . maxCategoryCount proto . learningMode = self . learningMode proto . inferenceMode = self . inferenceMode proto . recordNum = self . recordNum self . _sdrClassifier . write ( proto . sdrClassifier )
6209	def print_attrs ( data_file , node_name = '/' , which = 'user' , compress = False ) : node = data_file . get_node ( node_name ) print ( 'List of attributes for:\n %s\n' % node ) for attr in node . _v_attrs . _f_list ( ) : print ( '\t%s' % attr ) attr_content = repr ( node . _v_attrs [ attr ] ) if compress : attr_content = attr_content . split ( '\n' ) [ 0 ] print ( "\t %s" % attr_content )
5778	def _bcrypt_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = BcryptConst . BCRYPT_PAD_PKCS1 if rsa_oaep_padding is True : flags = BcryptConst . BCRYPT_PAD_OAEP padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_OAEP_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) # This has to be assigned to a variable to prevent cffi from gc'ing it hash_buffer = buffer_from_unicode ( BcryptConst . BCRYPT_SHA1_ALGORITHM ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . pbLabel = null ( ) padding_info_struct . cbLabel = 0 padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) else : padding_info = null ( ) out_len = new ( bcrypt , 'ULONG *' ) res = bcrypt . BCryptEncrypt ( certificate_or_public_key . key_handle , data , len ( data ) , padding_info , null ( ) , 0 , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) res = bcrypt . BCryptEncrypt ( certificate_or_public_key . key_handle , data , len ( data ) , padding_info , null ( ) , 0 , buffer , buffer_len , out_len , flags ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) )
4206	def levup ( acur , knxt , ecur = None ) : if acur [ 0 ] != 1 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) acur = acur [ 1 : ] # Drop the leading 1, it is not needed # Matrix formulation from Stoica is used to avoid looping anxt = numpy . concatenate ( ( acur , [ 0 ] ) ) + knxt * numpy . concatenate ( ( numpy . conj ( acur [ - 1 : : - 1 ] ) , [ 1 ] ) ) enxt = None if ecur is not None : # matlab version enxt = (1-knxt'.*knxt)*ecur enxt = ( 1. - numpy . dot ( numpy . conj ( knxt ) , knxt ) ) * ecur anxt = numpy . insert ( anxt , 0 , 1 ) return anxt , enxt
13741	def cached_httpbl_exempt ( view_func ) : # We could just do view_func.cached_httpbl_exempt = True, but decorators # are nicer if they don't have side-effects, so we return a new # function. def wrapped_view ( * args , * * kwargs ) : return view_func ( * args , * * kwargs ) wrapped_view . cached_httpbl_exempt = True return wraps ( view_func , assigned = available_attrs ( view_func ) ) ( wrapped_view )
11240	def copy_web_file_to_local ( file_path , target_path ) : response = urllib . request . urlopen ( file_path ) f = open ( target_path , 'w' ) f . write ( response . read ( ) ) f . close ( )
4486	def remove ( self ) : response = self . _delete ( self . _delete_url ) if response . status_code != 204 : raise RuntimeError ( 'Could not delete {}.' . format ( self . path ) )
3747	def calculate ( self , T , P , zs , ws , method ) : if method == MIXING_LOG_MOLAR : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( zs , mus ) elif method == MIXING_LOG_MASS : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( ws , mus ) elif method == LALIBERTE_MU : ws = list ( ws ) ws . pop ( self . index_w ) return Laliberte_viscosity ( T , ws , self . wCASs ) else : raise Exception ( 'Method not valid' )
7737	def prohibit ( self , data ) : for char in data : for lookup in self . prohibited : if lookup ( char ) : raise StringprepError ( "Prohibited character: {0!r}" . format ( char ) ) return data
12987	def keep_kwargs_partial ( func , * args , * * keywords ) : def newfunc ( * fargs , * * fkeywords ) : newkeywords = fkeywords . copy ( ) newkeywords . update ( keywords ) return func ( * ( args + fargs ) , * * newkeywords ) newfunc . func = func newfunc . args = args newfunc . keywords = keywords return newfunc
6446	def _cond_bb ( self , word , suffix_len ) : return ( len ( word ) - suffix_len >= 3 and word [ - suffix_len - 3 : - suffix_len ] != 'met' and word [ - suffix_len - 4 : - suffix_len ] != 'ryst' )
8759	def get_subnets ( context , limit = None , page_reverse = False , sorts = [ 'id' ] , marker = None , filters = None , fields = None ) : LOG . info ( "get_subnets for tenant %s with filters %s fields %s" % ( context . tenant_id , filters , fields ) ) filters = filters or { } subnets = db_api . subnet_find ( context , limit = limit , page_reverse = page_reverse , sorts = sorts , marker_obj = marker , join_dns = True , join_routes = True , join_pool = True , * * filters ) for subnet in subnets : cache = subnet . get ( "_allocation_pool_cache" ) if not cache : db_api . subnet_update_set_alloc_pool_cache ( context , subnet , subnet . allocation_pools ) return v . _make_subnets_list ( subnets , fields = fields )
31	def adjust_shape ( placeholder , data ) : if not isinstance ( data , np . ndarray ) and not isinstance ( data , list ) : return data if isinstance ( data , list ) : data = np . array ( data ) placeholder_shape = [ x or - 1 for x in placeholder . shape . as_list ( ) ] assert _check_shape ( placeholder_shape , data . shape ) , 'Shape of data {} is not compatible with shape of the placeholder {}' . format ( data . shape , placeholder_shape ) return np . reshape ( data , placeholder_shape )
6454	def stem ( self , word ) : # lowercase, normalize, and compose word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) # remove umlauts word = word . translate ( self . _accents ) # Step 1 wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'ern' : word = word [ : - 3 ] elif wlen > 3 and word [ - 2 : ] in { 'em' , 'en' , 'er' , 'es' } : word = word [ : - 2 ] elif wlen > 2 and ( word [ - 1 ] == 'e' or ( word [ - 1 ] == 's' and word [ - 2 ] in self . _st_ending ) ) : word = word [ : - 1 ] # Step 2 wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'est' : word = word [ : - 3 ] elif wlen > 3 and ( word [ - 2 : ] in { 'er' , 'en' } or ( word [ - 2 : ] == 'st' and word [ - 3 ] in self . _st_ending ) ) : word = word [ : - 2 ] return word
9145	def clear ( skip ) : for name in sorted ( MODULES ) : if name in skip : continue click . secho ( f'clearing cache for {name}' , fg = 'cyan' , bold = True ) clear_cache ( name )
6154	def position_CD ( Ka , out_type = 'fb_exact' ) : rs = 10 / ( 2 * np . pi ) # Load b and a ndarrays with the coefficients if out_type . lower ( ) == 'open_loop' : b = np . array ( [ Ka * 4000 * rs ] ) a = np . array ( [ 1 , 1275 , 31250 , 0 ] ) elif out_type . lower ( ) == 'fb_approx' : b = np . array ( [ 3.2 * Ka * rs ] ) a = np . array ( [ 1 , 25 , 3.2 * Ka * rs ] ) elif out_type . lower ( ) == 'fb_exact' : b = np . array ( [ 4000 * Ka * rs ] ) a = np . array ( [ 1 , 1250 + 25 , 25 * 1250 , 4000 * Ka * rs ] ) else : raise ValueError ( 'out_type must be: open_loop, fb_approx, or fc_exact' ) return b , a
462	def exit_tensorflow ( sess = None , port = 6006 ) : text = "[TL] Close tensorboard and nvidia-process if available" text2 = "[TL] Close tensorboard and nvidia-process not yet supported by this function (tl.ops.exit_tf) on " if sess is not None : sess . close ( ) if _platform == "linux" or _platform == "linux2" : tl . logging . info ( 'linux: %s' % text ) os . system ( 'nvidia-smi' ) os . system ( 'fuser ' + port + '/tcp -k' ) # kill tensorboard 6006 os . system ( "nvidia-smi | grep python |awk '{print $3}'|xargs kill" ) # kill all nvidia-smi python process _exit ( ) elif _platform == "darwin" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( "lsof -i tcp:" + str ( port ) + " | grep -v PID | awk '{print $2}' | xargs kill" , shell = True ) # kill tensorboard elif _platform == "win32" : raise NotImplementedError ( "this function is not supported on the Windows platform" ) else : tl . logging . info ( text2 + _platform )
7181	def fix_remaining_type_comments ( node ) : assert node . type == syms . file_input last_n = None for n in node . post_order ( ) : if last_n is not None : if n . type == token . NEWLINE and is_assignment ( last_n ) : fix_variable_annotation_type_comment ( n , last_n ) elif n . type == syms . funcdef and last_n . type == syms . suite : fix_signature_annotation_type_comment ( n , last_n , offset = 1 ) elif n . type == syms . async_funcdef and last_n . type == syms . suite : fix_signature_annotation_type_comment ( n , last_n , offset = 2 ) last_n = n
13313	def _activate ( self ) : old_syspath = set ( sys . path ) site . addsitedir ( self . site_path ) site . addsitedir ( self . bin_path ) new_syspaths = set ( sys . path ) - old_syspath for path in new_syspaths : sys . path . remove ( path ) sys . path . insert ( 1 , path ) if not hasattr ( sys , 'real_prefix' ) : sys . real_prefix = sys . prefix sys . prefix = self . path
6425	def tanimoto_coeff ( self , src , tar , qval = 2 ) : coeff = self . sim ( src , tar , qval ) if coeff != 0 : return log ( coeff , 2 ) return float ( '-inf' )
2034	def MSTORE8 ( self , address , value ) : if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . _allocate ( address , 1 ) self . _store ( address , Operators . EXTRACT ( value , 0 , 8 ) , 1 )
3087	def _get_entity ( self ) : if self . _is_ndb ( ) : return self . _model . get_by_id ( self . _key_name ) else : return self . _model . get_by_key_name ( self . _key_name )
3819	async def add_user ( self , add_user_request ) : response = hangouts_pb2 . AddUserResponse ( ) await self . _pb_request ( 'conversations/adduser' , add_user_request , response ) return response
13769	def get_minifier ( self ) : if self . minifier is None : if not self . has_bundles ( ) : raise Exception ( "Unable to get default minifier, no bundles in build group" ) minifier = self . get_first_bundle ( ) . get_default_minifier ( ) else : minifier = self . minifier if minifier : minifier . init_asset ( self ) return minifier
6323	def ac_encode ( text , probs ) : coder = Arithmetic ( ) coder . set_probs ( probs ) return coder . encode ( text )
343	def validation_metrics ( self ) : if ( self . _validation_iterator is None ) or ( self . _validation_metrics is None ) : raise AttributeError ( 'Validation is not setup.' ) n = 0.0 metric_sums = [ 0.0 ] * len ( self . _validation_metrics ) self . _sess . run ( self . _validation_iterator . initializer ) while True : try : metrics = self . _sess . run ( self . _validation_metrics ) for i , m in enumerate ( metrics ) : metric_sums [ i ] += m n += 1.0 except tf . errors . OutOfRangeError : break for i , m in enumerate ( metric_sums ) : metric_sums [ i ] = metric_sums [ i ] / n return zip ( self . _validation_metrics , metric_sums )
7952	def wait_for_writability ( self ) : with self . lock : while True : if self . _state in ( "closing" , "closed" , "aborted" ) : return False if self . _socket and bool ( self . _write_queue ) : return True self . _write_queue_cond . wait ( ) return False
13145	def remove_near_duplicate_relation ( triples , threshold = 0.97 ) : logging . debug ( "remove duplicate" ) _assert_threshold ( threshold ) duplicate_rel_counter = defaultdict ( list ) relations = set ( ) for t in triples : duplicate_rel_counter [ t . relation ] . append ( f"{t.head} {t.tail}" ) relations . add ( t . relation ) relations = list ( relations ) num_triples = len ( triples ) removal_relation_set = set ( ) for rel , values in duplicate_rel_counter . items ( ) : duplicate_rel_counter [ rel ] = Superminhash ( values ) for i in relations : for j in relations : if i == j or i in removal_relation_set or j in removal_relation_set : continue close_relations = [ i ] if _set_close_to ( duplicate_rel_counter [ i ] , duplicate_rel_counter [ j ] , threshold ) : close_relations . append ( j ) if len ( close_relations ) > 1 : close_relations . pop ( np . random . randint ( len ( close_relations ) ) ) removal_relation_set |= set ( close_relations ) logging . info ( "Removing {} relations: {}" . format ( len ( removal_relation_set ) , str ( removal_relation_set ) ) ) return list ( filterfalse ( lambda x : x . relation in removal_relation_set , triples ) )
4078	def write_py2k_header ( file_list ) : if not isinstance ( file_list , list ) : file_list = [ file_list ] python_re = re . compile ( br"^(#!.*\bpython)(.*)([\r\n]+)$" ) coding_re = re . compile ( br"coding[:=]\s*([-\w.]+)" ) new_line_re = re . compile ( br"([\r\n]+)$" ) version_3 = LooseVersion ( '3' ) for file in file_list : if not os . path . getsize ( file ) : continue rewrite_needed = False python_found = False coding_found = False lines = [ ] f = open ( file , 'rb' ) try : while len ( lines ) < 2 : line = f . readline ( ) match = python_re . match ( line ) if match : python_found = True version = LooseVersion ( match . group ( 2 ) . decode ( ) or '2' ) try : version_test = version >= version_3 except TypeError : version_test = True if version_test : line = python_re . sub ( br"\g<1>2\g<3>" , line ) rewrite_needed = True elif coding_re . search ( line ) : coding_found = True lines . append ( line ) if not coding_found : match = new_line_re . search ( lines [ 0 ] ) newline = match . group ( 1 ) if match else b"\n" line = b"# -*- coding: utf-8 -*-" + newline lines . insert ( 1 if python_found else 0 , line ) rewrite_needed = True if rewrite_needed : lines += f . readlines ( ) finally : f . close ( ) if rewrite_needed : f = open ( file , 'wb' ) try : f . writelines ( lines ) finally : f . close ( )
2325	def predict_dataset ( self , x , * * kwargs ) : printout = kwargs . get ( "printout" , None ) pred = [ ] res = [ ] x . columns = [ "A" , "B" ] for idx , row in x . iterrows ( ) : a = scale ( row [ 'A' ] . reshape ( ( len ( row [ 'A' ] ) , 1 ) ) ) b = scale ( row [ 'B' ] . reshape ( ( len ( row [ 'B' ] ) , 1 ) ) ) pred . append ( self . predict_proba ( a , b , idx = idx ) ) if printout is not None : res . append ( [ row [ 'SampleID' ] , pred [ - 1 ] ] ) DataFrame ( res , columns = [ 'SampleID' , 'Predictions' ] ) . to_csv ( printout , index = False ) return pred
6848	def find_working_password ( self , usernames = None , host_strings = None ) : r = self . local_renderer if host_strings is None : host_strings = [ ] if not host_strings : host_strings . append ( self . genv . host_string ) if usernames is None : usernames = [ ] if not usernames : usernames . append ( self . genv . user ) for host_string in host_strings : for username in usernames : passwords = [ ] passwords . append ( self . genv . user_default_passwords [ username ] ) passwords . append ( self . genv . user_passwords [ username ] ) passwords . append ( self . env . default_password ) for password in passwords : with settings ( warn_only = True ) : r . env . host_string = host_string r . env . password = password r . env . user = username ret = r . _local ( "sshpass -p '{password}' ssh -o StrictHostKeyChecking=no {user}@{host_string} echo hello" , capture = True ) #print('ret.return_code:', ret.return_code) # print('ret000:[%s]' % ret) #code 1 = good password, but prompts needed #code 5 = bad password #code 6 = good password, but host public key is unknown if ret . return_code in ( 1 , 6 ) or 'hello' in ret : # Login succeeded, so we haven't yet changed the password, so use the default password. return host_string , username , password raise Exception ( 'No working login found.' )
2690	def new_compiler ( * args , * * kwargs ) : make_silent = kwargs . pop ( 'silent' , True ) cc = _new_compiler ( * args , * * kwargs ) # If MSVC10, initialize the compiler here and add /MANIFEST to linker flags. # See Python issue 4431 (https://bugs.python.org/issue4431) if is_msvc ( cc ) : from distutils . msvc9compiler import get_build_version if get_build_version ( ) == 10 : cc . initialize ( ) for ldflags in [ cc . ldflags_shared , cc . ldflags_shared_debug ] : unique_extend ( ldflags , [ '/MANIFEST' ] ) # If MSVC14, do not silence. As msvc14 requires some custom # steps before the process is spawned, we can't monkey-patch this. elif get_build_version ( ) == 14 : make_silent = False # monkey-patch compiler to suppress stdout and stderr. if make_silent : cc . spawn = _CCompiler_spawn_silent return cc
4584	def image_to_colorlist ( image , container = list ) : deprecated . deprecated ( 'util.gif.image_to_colorlist' ) return container ( convert_mode ( image ) . getdata ( ) )
649	def generateSimpleCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : assert nCoinc * activity <= length , "can't generate non-overlapping coincidences" coincMatrix = SM32 ( 0 , length ) coinc = numpy . zeros ( length , dtype = 'int32' ) for i in xrange ( nCoinc ) : coinc [ : ] = 0 coinc [ i * activity : ( i + 1 ) * activity ] = 1 coincMatrix . addRow ( coinc ) return coincMatrix
12242	def bohachevsky1 ( theta ) : x , y = theta obj = x ** 2 + 2 * y ** 2 - 0.3 * np . cos ( 3 * np . pi * x ) - 0.4 * np . cos ( 4 * np . pi * y ) + 0.7 grad = np . array ( [ 2 * x + 0.3 * np . sin ( 3 * np . pi * x ) * 3 * np . pi , 4 * y + 0.4 * np . sin ( 4 * np . pi * y ) * 4 * np . pi , ] ) return obj , grad
1416	def get_execution_state ( self , topologyName , callback = None ) : isWatching = False # Temp dict used to return result # if callback is not provided. ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : """ Custom callback to get the topologies right now. """ ret [ "result" ] = data self . _get_execution_state_with_watch ( topologyName , callback , isWatching ) # The topologies are now populated with the data. return ret [ "result" ]
12166	def remove_listener ( self , event , listener ) : with contextlib . suppress ( ValueError ) : self . _listeners [ event ] . remove ( listener ) return True with contextlib . suppress ( ValueError ) : self . _once [ event ] . remove ( listener ) return True return False
6364	def to_tuple ( self ) : return self . _tp , self . _tn , self . _fp , self . _fn
10778	def update_field ( self , poses = None ) : m = np . clip ( self . particle_field , 0 , 1 ) part_color = np . zeros ( self . _image . shape ) for a in range ( 4 ) : part_color [ : , : , : , a ] = self . part_col [ a ] self . field = np . zeros ( self . _image . shape ) for a in range ( 4 ) : self . field [ : , : , : , a ] = m * part_color [ : , : , : , a ] + ( 1 - m ) * self . _image [ : , : , : , a ]
12691	def write_table_pair_potential ( func , dfunc = None , bounds = ( 1.0 , 10.0 ) , samples = 1000 , tollerance = 1e-6 , keyword = 'PAIR' ) : r_min , r_max = bounds if dfunc is None : dfunc = lambda r : ( func ( r + tollerance ) - func ( r - tollerance ) ) / ( 2 * tollerance ) i = np . arange ( 1 , samples + 1 ) r = np . linspace ( r_min , r_max , samples ) forces = func ( r ) energies = dfunc ( r ) lines = [ '%d %f %f %f\n' % ( index , radius , force , energy ) for index , radius , force , energy in zip ( i , r , forces , energies ) ] return "%s\nN %d\n\n" % ( keyword , samples ) + '' . join ( lines )
4140	def execute_script ( code_block , example_globals , image_path , fig_count , src_file , gallery_conf ) : time_elapsed = 0 stdout = '' # We need to execute the code print ( 'plotting code blocks in %s' % src_file ) plt . close ( 'all' ) cwd = os . getcwd ( ) # Redirect output to stdout and orig_stdout = sys . stdout try : # First cd in the original example dir, so that any file # created by the example get created in this directory os . chdir ( os . path . dirname ( src_file ) ) my_buffer = StringIO ( ) my_stdout = Tee ( sys . stdout , my_buffer ) sys . stdout = my_stdout t_start = time ( ) exec ( code_block , example_globals ) time_elapsed = time ( ) - t_start sys . stdout = orig_stdout my_stdout = my_buffer . getvalue ( ) . strip ( ) . expandtabs ( ) if my_stdout : stdout = CODE_OUTPUT . format ( indent ( my_stdout , ' ' * 4 ) ) os . chdir ( cwd ) figure_list = save_figures ( image_path , fig_count , gallery_conf ) # Depending on whether we have one or more figures, we're using a # horizontal list or a single rst call to 'image'. image_list = "" if len ( figure_list ) == 1 : figure_name = figure_list [ 0 ] image_list = SINGLE_IMAGE % figure_name . lstrip ( '/' ) elif len ( figure_list ) > 1 : image_list = HLIST_HEADER for figure_name in figure_list : image_list += HLIST_IMAGE_TEMPLATE % figure_name . lstrip ( '/' ) except Exception : formatted_exception = traceback . format_exc ( ) print ( 80 * '_' ) print ( '%s is not compiling:' % src_file ) print ( formatted_exception ) print ( 80 * '_' ) figure_list = [ ] image_list = codestr2rst ( formatted_exception , lang = 'pytb' ) # Overrides the output thumbnail in the gallery for easy identification broken_img = os . path . join ( glr_path_static ( ) , 'broken_example.png' ) shutil . copyfile ( broken_img , os . path . join ( cwd , image_path . format ( 1 ) ) ) fig_count += 1 # raise count to avoid overwriting image # Breaks build on first example error if gallery_conf [ 'abort_on_example_error' ] : raise finally : os . chdir ( cwd ) sys . stdout = orig_stdout print ( " - time elapsed : %.2g sec" % time_elapsed ) code_output = "\n{0}\n\n{1}\n\n" . format ( image_list , stdout ) return code_output , time_elapsed , fig_count + len ( figure_list )
6601	def put_package ( self , package ) : self . last_package_index += 1 package_index = self . last_package_index package_fullpath = self . package_fullpath ( package_index ) # e.g., '{path}/tpd_20161129_122841_HnpcmF/task_00009.p.gz' with gzip . open ( package_fullpath , 'wb' ) as f : pickle . dump ( package , f , protocol = pickle . HIGHEST_PROTOCOL ) f . close ( ) result_fullpath = self . result_fullpath ( package_index ) # e.g., '{path}/tpd_20161129_122841_HnpcmF/results/task_00009/result.p.gz' result_dir = os . path . dirname ( result_fullpath ) # e.g., '{path}/tpd_20161129_122841_HnpcmF/results/task_00009' alphatwirl . mkdir_p ( result_dir ) return package_index
6362	def encode ( self , word , max_length = - 1 ) : # uppercase, normalize, and decompose, filter to A-Z minus vowels & W word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _uc_set ) # merge repeated Ls & Rs word = word . replace ( 'LL' , 'L' ) word = word . replace ( 'R' , 'R' ) # apply the Soundex algorithm sdx = word . translate ( self . _trans ) if max_length > 0 : sdx = ( sdx + ( '0' * max_length ) ) [ : max_length ] return sdx
4190	def window_poisson_hanning ( N , alpha = 2 ) : w1 = window_hann ( N ) w2 = window_poisson ( N , alpha = alpha ) return w1 * w2
5581	def _get_contour_values ( min_val , max_val , base = 0 , interval = 100 ) : i = base out = [ ] if min_val < base : while i >= min_val : i -= interval while i <= max_val : if i >= min_val : out . append ( i ) i += interval return out
9203	def render ( node , strict = False ) : if isinstance ( node , list ) : return render_list ( node ) elif isinstance ( node , dict ) : return render_node ( node , strict = strict ) else : raise NotImplementedError ( "You tried to render a %s. Only list and dicts can be rendered." % node . __class__ . __name__ )
6497	def index ( self , doc_type , sources , * * kwargs ) : try : actions = [ ] for source in sources : self . _check_mappings ( doc_type , source ) id_ = source [ 'id' ] if 'id' in source else None log . debug ( "indexing %s object with id %s" , doc_type , id_ ) action = { "_index" : self . index_name , "_type" : doc_type , "_id" : id_ , "_source" : source } actions . append ( action ) # bulk() returns a tuple with summary information # number of successfully executed actions and number of errors if stats_only is set to True. _ , indexing_errors = bulk ( self . _es , actions , * * kwargs ) if indexing_errors : ElasticSearchEngine . log_indexing_error ( indexing_errors ) # Broad exception handler to protect around bulk call except Exception as ex : # log information and re-raise log . exception ( "error while indexing - %s" , str ( ex ) ) raise
9357	def sentences ( quantity = 2 , as_list = False ) : result = [ sntc . strip ( ) for sntc in random . sample ( get_dictionary ( 'lorem_ipsum' ) , quantity ) ] if as_list : return result else : return ' ' . join ( result )
12664	def union_mask ( filelist ) : firstimg = check_img ( filelist [ 0 ] ) mask = np . zeros_like ( firstimg . get_data ( ) ) # create space for all features and read from subjects try : for volf in filelist : roiimg = check_img ( volf ) check_img_compatibility ( firstimg , roiimg ) mask += get_img_data ( roiimg ) except Exception as exc : raise ValueError ( 'Error joining mask {} and {}.' . format ( repr_imgs ( firstimg ) , repr_imgs ( volf ) ) ) from exc else : return as_ndarray ( mask > 0 , dtype = bool )
5135	def minimal_random_graph ( num_vertices , seed = None , * * kwargs ) : if isinstance ( seed , numbers . Integral ) : np . random . seed ( seed ) points = np . random . random ( ( num_vertices , 2 ) ) * 10 edges = [ ] for k in range ( num_vertices - 1 ) : for j in range ( k + 1 , num_vertices ) : v = points [ k ] - points [ j ] edges . append ( ( k , j , v [ 0 ] ** 2 + v [ 1 ] ** 2 ) ) mytype = [ ( 'n1' , int ) , ( 'n2' , int ) , ( 'distance' , np . float ) ] edges = np . array ( edges , dtype = mytype ) edges = np . sort ( edges , order = 'distance' ) unionF = UnionFind ( [ k for k in range ( num_vertices ) ] ) g = nx . Graph ( ) for n1 , n2 , dummy in edges : unionF . union ( n1 , n2 ) g . add_edge ( n1 , n2 ) if unionF . nClusters == 1 : break pos = { j : p for j , p in enumerate ( points ) } g = QueueNetworkDiGraph ( g . to_directed ( ) ) g . set_pos ( pos ) return g
13678	def filenumber_handle ( self ) : self . __results = [ ] self . __dirs = [ ] self . __files = [ ] self . __ftp = self . connect ( ) self . __ftp . dir ( self . args . path , self . __results . append ) self . logger . debug ( "dir results: {}" . format ( self . __results ) ) self . quit ( ) status = self . ok for data in self . __results : if "<DIR>" in data : self . __dirs . append ( str ( data . split ( ) [ 3 ] ) ) else : self . __files . append ( str ( data . split ( ) [ 2 ] ) ) self . __result = len ( self . __files ) self . logger . debug ( "result: {}" . format ( self . __result ) ) # Compare the vlaue. if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical # Output self . shortoutput = "Found {0} files in {1}." . format ( self . __result , self . args . path ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , path = self . args . path ) ) self . logger . debug ( "Return status and output." ) status ( self . output ( ) )
10942	def update_function ( self , param_vals ) : self . opt_obj . update_function ( param_vals ) return self . opt_obj . get_error ( )
12516	def get_h5file ( file_path , mode = 'r' ) : if not op . exists ( file_path ) : raise IOError ( 'Could not find file {}.' . format ( file_path ) ) try : h5file = h5py . File ( file_path , mode = mode ) except : raise else : return h5file
6170	def filter ( self , x ) : y = signal . sosfilt ( self . sos , x ) return y
7430	def _resolveambig ( subseq ) : N = [ ] for col in subseq : rand = np . random . binomial ( 1 , 0.5 ) N . append ( [ _AMBIGS [ i ] [ rand ] for i in col ] ) return np . array ( N )
7184	def copy_arguments_to_annotations ( args , type_comment , * , is_method = False ) : if isinstance ( type_comment , ast3 . Ellipsis ) : return expected = len ( args . args ) if args . vararg : expected += 1 expected += len ( args . kwonlyargs ) if args . kwarg : expected += 1 actual = len ( type_comment ) if isinstance ( type_comment , list ) else 1 if expected != actual : if is_method and expected - actual == 1 : pass # fine, we're just skipping `self`, `cls`, etc. else : raise ValueError ( f"number of arguments in type comment doesn't match; " + f"expected {expected}, found {actual}" ) if isinstance ( type_comment , list ) : next_value = type_comment . pop else : # If there's just one value, only one of the loops and ifs below will # be populated. We ensure this with the expected/actual length check # above. _tc = type_comment def next_value ( index : int = 0 ) -> ast3 . expr : return _tc for arg in args . args [ expected - actual : ] : ensure_no_annotation ( arg . annotation ) arg . annotation = next_value ( 0 ) if args . vararg : ensure_no_annotation ( args . vararg . annotation ) args . vararg . annotation = next_value ( 0 ) for arg in args . kwonlyargs : ensure_no_annotation ( arg . annotation ) arg . annotation = next_value ( 0 ) if args . kwarg : ensure_no_annotation ( args . kwarg . annotation ) args . kwarg . annotation = next_value ( 0 )
5945	def convert_aa_code ( x ) : if len ( x ) == 1 : return amino_acid_codes [ x . upper ( ) ] elif len ( x ) == 3 : return inverse_aa_codes [ x . upper ( ) ] else : raise ValueError ( "Can only convert 1-letter or 3-letter amino acid codes, " "not %r" % x )
9799	def stop ( ctx , yes , pending ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) if not yes and not click . confirm ( "Are sure you want to stop experiments " "in group `{}`" . format ( _group ) ) : click . echo ( 'Existing without stopping experiments in group.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . experiment_group . stop ( user , project_name , _group , pending = pending ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop experiments in group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiments in group are being stopped." )
4364	def encode ( data , json_dumps = default_json_dumps ) : payload = '' msg = str ( MSG_TYPES [ data [ 'type' ] ] ) if msg in [ '0' , '1' ] : # '1::' [path] [query] msg += '::' + data [ 'endpoint' ] if 'qs' in data and data [ 'qs' ] != '' : msg += ':' + data [ 'qs' ] elif msg == '2' : # heartbeat msg += '::' elif msg in [ '3' , '4' , '5' ] : # '3:' [id ('+')] ':' [endpoint] ':' [data] # '4:' [id ('+')] ':' [endpoint] ':' [json] # '5:' [id ('+')] ':' [endpoint] ':' [json encoded event] # The message id is an incremental integer, required for ACKs. # If the message id is followed by a +, the ACK is not handled by # socket.io, but by the user instead. if msg == '3' : payload = data [ 'data' ] if msg == '4' : payload = json_dumps ( data [ 'data' ] ) if msg == '5' : d = { } d [ 'name' ] = data [ 'name' ] if 'args' in data and data [ 'args' ] != [ ] : d [ 'args' ] = data [ 'args' ] payload = json_dumps ( d ) if 'id' in data : msg += ':' + str ( data [ 'id' ] ) if data [ 'ack' ] == 'data' : msg += '+' msg += ':' else : msg += '::' if 'endpoint' not in data : data [ 'endpoint' ] = '' if payload != '' : msg += data [ 'endpoint' ] + ':' + payload else : msg += data [ 'endpoint' ] elif msg == '6' : # '6:::' [id] '+' [data] msg += '::' + data . get ( 'endpoint' , '' ) + ':' + str ( data [ 'ackId' ] ) if 'args' in data and data [ 'args' ] != [ ] : msg += '+' + json_dumps ( data [ 'args' ] ) elif msg == '7' : # '7::' [endpoint] ':' [reason] '+' [advice] msg += ':::' if 'reason' in data and data [ 'reason' ] != '' : msg += str ( ERROR_REASONS [ data [ 'reason' ] ] ) if 'advice' in data and data [ 'advice' ] != '' : msg += '+' + str ( ERROR_ADVICES [ data [ 'advice' ] ] ) msg += data [ 'endpoint' ] # NoOp, used to close a poll after the polling duration time elif msg == '8' : msg += '::' return msg
12148	def convertImages ( self ) : # copy over JPGs (and such) exts = [ '.jpg' , '.png' ] for fname in [ x for x in self . files1 if cm . ext ( x ) in exts ] : ID = "UNKNOWN" if len ( fname ) > 8 and fname [ : 8 ] in self . IDs : ID = fname [ : 8 ] fname2 = ID + "_jpg_" + fname if not fname2 in self . files2 : self . log . info ( "copying over [%s]" % fname2 ) shutil . copy ( os . path . join ( self . folder1 , fname ) , os . path . join ( self . folder2 , fname2 ) ) if not fname [ : 8 ] + ".abf" in self . files1 : self . log . error ( "orphan image: %s" , fname ) # convert TIFs (and such) to JPGs exts = [ '.tif' , '.tiff' ] for fname in [ x for x in self . files1 if cm . ext ( x ) in exts ] : ID = "UNKNOWN" if len ( fname ) > 8 and fname [ : 8 ] in self . IDs : ID = fname [ : 8 ] fname2 = ID + "_tif_" + fname + ".jpg" if not fname2 in self . files2 : self . log . info ( "converting micrograph [%s]" % fname2 ) imaging . TIF_to_jpg ( os . path . join ( self . folder1 , fname ) , saveAs = os . path . join ( self . folder2 , fname2 ) ) if not fname [ : 8 ] + ".abf" in self . files1 : self . log . error ( "orphan image: %s" , fname )
2310	def predict_proba ( self , a , b , * * kwargs ) : return self . b_fit_score ( b , a ) - self . b_fit_score ( a , b )
4577	def get_server ( self , key , * * kwds ) : kwds = dict ( self . kwds , * * kwds ) server = self . servers . get ( key ) if server : # Make sure it's the right server. server . check_keywords ( self . constructor , kwds ) else : # Make a new server server = _CachedServer ( self . constructor , key , kwds ) self . servers [ key ] = server return server
12323	def api_call_action ( func ) : def _inner ( * args , * * kwargs ) : return func ( * args , * * kwargs ) _inner . __name__ = func . __name__ _inner . __doc__ = func . __doc__ return _inner
2020	def ADDMOD ( self , a , b , c ) : try : result = Operators . ITEBV ( 256 , c == 0 , 0 , ( a + b ) % c ) except ZeroDivisionError : result = 0 return result
2084	def parse_args ( self , ctx , args ) : if not args and self . no_args_is_help and not ctx . resilient_parsing : click . echo ( ctx . get_help ( ) ) ctx . exit ( ) return super ( ActionSubcommand , self ) . parse_args ( ctx , args )
11271	def to_str ( prev , encoding = None ) : first = next ( prev ) if isinstance ( first , str ) : if encoding is None : yield first for s in prev : yield s else : yield first . encode ( encoding ) for s in prev : yield s . encode ( encoding ) else : if encoding is None : encoding = sys . stdout . encoding or 'utf-8' yield first . decode ( encoding ) for s in prev : yield s . decode ( encoding )
10763	def get_unique_token ( self ) : if self . _unique_token is None : self . _unique_token = self . _random_token ( ) return self . _unique_token
7751	def process_message ( self , stanza ) : stanza_type = stanza . stanza_type if stanza_type is None : stanza_type = "normal" if self . __try_handlers ( self . _message_handlers , stanza , stanza_type = stanza_type ) : return True if stanza_type not in ( "error" , "normal" ) : # try 'normal' handler additionaly to the regular handler return self . __try_handlers ( self . _message_handlers , stanza , stanza_type = "normal" ) return False
9806	def deploy ( file , manager_path , check , dry_run ) : # pylint:disable=redefined-builtin config = read_deployment_config ( file ) manager = DeployManager ( config = config , filepath = file , manager_path = manager_path , dry_run = dry_run ) exception = None if check : manager . check ( ) Printer . print_success ( 'Polyaxon deployment file is valid.' ) else : try : manager . install ( ) except Exception as e : Printer . print_error ( 'Polyaxon could not be installed.' ) exception = e if exception : Printer . print_error ( 'Error message `{}`.' . format ( exception ) )
3392	def prune_unused_reactions ( cobra_model ) : output_model = cobra_model . copy ( ) reactions_to_prune = [ r for r in output_model . reactions if len ( r . metabolites ) == 0 ] output_model . remove_reactions ( reactions_to_prune ) return output_model , reactions_to_prune
4139	def save_thumbnail ( image_path , base_image_name , gallery_conf ) : first_image_file = image_path . format ( 1 ) thumb_dir = os . path . join ( os . path . dirname ( first_image_file ) , 'thumb' ) if not os . path . exists ( thumb_dir ) : os . makedirs ( thumb_dir ) thumb_file = os . path . join ( thumb_dir , 'sphx_glr_%s_thumb.png' % base_image_name ) if os . path . exists ( first_image_file ) : scale_image ( first_image_file , thumb_file , 400 , 280 ) elif not os . path . exists ( thumb_file ) : # create something to replace the thumbnail default_thumb_file = os . path . join ( glr_path_static ( ) , 'no_image.png' ) default_thumb_file = gallery_conf . get ( "default_thumb_file" , default_thumb_file ) scale_image ( default_thumb_file , thumb_file , 200 , 140 )
2550	def system ( cmd , data = None ) : import subprocess s = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stdin = subprocess . PIPE ) out , err = s . communicate ( data ) return out . decode ( 'utf8' )
9005	def add_new_pattern ( self , id_ , name = None ) : if name is None : name = id_ pattern = self . _parser . new_pattern ( id_ , name ) self . _patterns . append ( pattern ) return pattern
5528	def open ( config , mode = "continue" , zoom = None , bounds = None , single_input_file = None , with_cache = False , debug = False ) : return Mapchete ( MapcheteConfig ( config , mode = mode , zoom = zoom , bounds = bounds , single_input_file = single_input_file , debug = debug ) , with_cache = with_cache )
9427	def getinfo ( self , name ) : rarinfo = self . NameToInfo . get ( name ) if rarinfo is None : raise KeyError ( 'There is no item named %r in the archive' % name ) return rarinfo
3177	def create ( self , list_id , data ) : self . list_id = list_id if 'name' not in data : raise KeyError ( 'The list merge field must have a name' ) if 'type' not in data : raise KeyError ( 'The list merge field must have a type' ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'merge-fields' ) , data = data ) if response is not None : self . merge_id = response [ 'merge_id' ] else : self . merge_id = None return response
6983	def _legendre_dtr ( x , y , y_err , legendredeg = 10 ) : try : p = Legendre . fit ( x , y , legendredeg ) fit_y = p ( x ) except Exception as e : fit_y = npzeros_like ( y ) fitchisq = npsum ( ( ( fit_y - y ) * ( fit_y - y ) ) / ( y_err * y_err ) ) nparams = legendredeg + 1 fitredchisq = fitchisq / ( len ( y ) - nparams - 1 ) LOGINFO ( 'legendre detrend applied. chisq = %.5f, reduced chisq = %.5f' % ( fitchisq , fitredchisq ) ) return fit_y , fitchisq , fitredchisq
7889	def update_presence ( self , presence ) : self . presence = MucPresence ( presence ) t = presence . get_type ( ) if t == "unavailable" : self . role = "none" self . affiliation = "none" self . room_jid = self . presence . get_from ( ) self . nick = self . room_jid . resource mc = self . presence . get_muc_child ( ) if isinstance ( mc , MucUserX ) : items = mc . get_items ( ) for item in items : if not isinstance ( item , MucItem ) : continue if item . role : self . role = item . role if item . affiliation : self . affiliation = item . affiliation if item . jid : self . real_jid = item . jid if item . nick : self . new_nick = item . nick break
12320	def add_files ( self , repo , files ) : rootdir = repo . rootdir for f in files : relativepath = f [ 'relativepath' ] sourcepath = f [ 'localfullpath' ] if sourcepath is None : # This can happen if the relative path is a URL continue # # Prepare the target path targetpath = os . path . join ( rootdir , relativepath ) try : os . makedirs ( os . path . dirname ( targetpath ) ) except : pass # print(sourcepath," => ", targetpath) print ( "Updating: {}" . format ( relativepath ) ) shutil . copyfile ( sourcepath , targetpath ) with cd ( repo . rootdir ) : self . _run ( [ 'add' , relativepath ] )
673	def runHotgym ( numRecords ) : # Create a data source for the network. dataSource = FileRecordStream ( streamID = _INPUT_FILE_PATH ) numRecords = min ( numRecords , dataSource . getDataRowCount ( ) ) network = createNetwork ( dataSource ) # Set predicted field network . regions [ "sensor" ] . setParameter ( "predictedField" , "consumption" ) # Enable learning for all regions. network . regions [ "SP" ] . setParameter ( "learningMode" , 1 ) network . regions [ "TM" ] . setParameter ( "learningMode" , 1 ) network . regions [ "classifier" ] . setParameter ( "learningMode" , 1 ) # Enable inference for all regions. network . regions [ "SP" ] . setParameter ( "inferenceMode" , 1 ) network . regions [ "TM" ] . setParameter ( "inferenceMode" , 1 ) network . regions [ "classifier" ] . setParameter ( "inferenceMode" , 1 ) results = [ ] N = 1 # Run the network, N iterations at a time. for iteration in range ( 0 , numRecords , N ) : network . run ( N ) predictionResults = getPredictionResults ( network , "classifier" ) oneStep = predictionResults [ 1 ] [ "predictedValue" ] oneStepConfidence = predictionResults [ 1 ] [ "predictionConfidence" ] fiveStep = predictionResults [ 5 ] [ "predictedValue" ] fiveStepConfidence = predictionResults [ 5 ] [ "predictionConfidence" ] result = ( oneStep , oneStepConfidence * 100 , fiveStep , fiveStepConfidence * 100 ) print "1-step: {:16} ({:4.4}%)\t 5-step: {:16} ({:4.4}%)" . format ( * result ) results . append ( result ) return results
8872	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : for t in self . regex : m = t . Regex . search ( l ) if m != None : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}' . format ( str ( truePosition + 1 ) ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . failed = True self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) raise DirectiveException ( self )
10493	def clickMouseButtonLeft ( self , coord , interval = None ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) if interval : self . _postQueuedEvents ( interval = interval ) else : self . _postQueuedEvents ( )
4989	def redirect ( self , request , * args , * * kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( * * kwargs ) resource_id = course_key or course_run_id or program_uuid # Replace enterprise UUID and resource ID with '{}', to easily match with a path in RouterView.VIEWS. Example: # /enterprise/fake-uuid/course/course-v1:cool+course+2017/enroll/ -> /enterprise/{}/course/{}/enroll/ path = re . sub ( '{}|{}' . format ( enterprise_customer_uuid , re . escape ( resource_id ) ) , '{}' , request . path ) # Remove course_key from kwargs if it exists because delegate views are not expecting it. kwargs . pop ( 'course_key' , None ) return self . VIEWS [ path ] . as_view ( ) ( request , * args , * * kwargs )
12555	def sav_to_pandas_savreader ( input_file ) : from savReaderWriter import SavReader lines = [ ] with SavReader ( input_file , returnHeader = True ) as reader : header = next ( reader ) for line in reader : lines . append ( line ) return pd . DataFrame ( data = lines , columns = header )
13375	def ensure_path_exists ( path , * args ) : if os . path . exists ( path ) : return os . makedirs ( path , * args )
11570	def set_brightness ( self , brightness ) : if brightness > 15 : brightness = 15 brightness |= 0xE0 self . brightness = brightness self . firmata . i2c_write ( 0x70 , brightness )
5529	def _get_zoom_level ( zoom , process ) : if zoom is None : return reversed ( process . config . zoom_levels ) if isinstance ( zoom , int ) : return [ zoom ] elif len ( zoom ) == 2 : return reversed ( range ( min ( zoom ) , max ( zoom ) + 1 ) ) elif len ( zoom ) == 1 : return zoom
12287	def shellcmd ( repo , args ) : with cd ( repo . rootdir ) : result = run ( args ) return result
7056	def ec2_ssh ( ip_address , keypem_file , username = 'ec2-user' , raiseonfail = False ) : c = paramiko . client . SSHClient ( ) c . load_system_host_keys ( ) c . set_missing_host_key_policy ( paramiko . client . AutoAddPolicy ) # load the private key from the AWS keypair pem privatekey = paramiko . RSAKey . from_private_key_file ( keypem_file ) # connect to the server try : c . connect ( ip_address , pkey = privatekey , username = 'ec2-user' ) return c except Exception as e : LOGEXCEPTION ( 'could not connect to EC2 instance at %s ' 'using keyfile: %s and user: %s' % ( ip_address , keypem_file , username ) ) if raiseonfail : raise return None
10959	def set_mem_level ( self , mem_level = 'hi' ) : #A little thing to parse strings for convenience: key = '' . join ( [ c if c in 'mlh' else '' for c in mem_level ] ) if key not in [ 'h' , 'mh' , 'm' , 'ml' , 'm' , 'l' ] : raise ValueError ( 'mem_level must be one of hi, med-hi, med, med-lo, lo.' ) mem_levels = { 'h' : [ np . float64 , np . float64 ] , 'mh' : [ np . float64 , np . float32 ] , 'm' : [ np . float32 , np . float32 ] , 'ml' : [ np . float32 , np . float16 ] , 'l' : [ np . float16 , np . float16 ] } hi_lvl , lo_lvl = mem_levels [ key ] cat_lvls = { 'obj' : lo_lvl , 'ilm' : hi_lvl , 'bkg' : lo_lvl } #no psf... self . image . float_precision = hi_lvl self . image . image = self . image . image . astype ( lo_lvl ) self . set_image ( self . image ) for cat in cat_lvls . keys ( ) : obj = self . get ( cat ) #check if it's a component collection if hasattr ( obj , 'comps' ) : for c in obj . comps : c . float_precision = lo_lvl else : obj . float_precision = lo_lvl self . _model = self . _model . astype ( hi_lvl ) self . _residuals = self . _model . astype ( hi_lvl ) self . reset ( )
7	def observation_placeholder ( ob_space , batch_size = None , name = 'Ob' ) : assert isinstance ( ob_space , Discrete ) or isinstance ( ob_space , Box ) or isinstance ( ob_space , MultiDiscrete ) , 'Can only deal with Discrete and Box observation spaces for now' dtype = ob_space . dtype if dtype == np . int8 : dtype = np . uint8 return tf . placeholder ( shape = ( batch_size , ) + ob_space . shape , dtype = dtype , name = name )
5138	def process_file ( self , file ) : if sys . version_info [ 0 ] >= 3 : nxt = file . __next__ else : nxt = file . next for token in tokenize . generate_tokens ( nxt ) : self . process_token ( * token ) self . make_index ( )
12330	def run ( cmd ) : cmd = [ pipes . quote ( c ) for c in cmd ] cmd = " " . join ( cmd ) cmd += "; exit 0" # print("Running {} in {}".format(cmd, os.getcwd())) try : output = subprocess . check_output ( cmd , stderr = subprocess . STDOUT , shell = True ) except subprocess . CalledProcessError as e : output = e . output output = output . decode ( 'utf-8' ) output = output . strip ( ) return output
6947	def jhk_to_sdssu ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSU_JHK , SDSSU_JH , SDSSU_JK , SDSSU_HK , SDSSU_J , SDSSU_H , SDSSU_K )
1169	def rlecode_hqx ( s ) : if not s : return '' result = [ ] prev = s [ 0 ] count = 1 # Add a dummy character to get the loop to go one extra round. # The dummy must be different from the last character of s. # In the same step we remove the first character, which has # already been stored in prev. if s [ - 1 ] == '!' : s = s [ 1 : ] + '?' else : s = s [ 1 : ] + '!' for c in s : if c == prev and count < 255 : count += 1 else : if count == 1 : if prev != '\x90' : result . append ( prev ) else : result += [ '\x90' , '\x00' ] elif count < 4 : if prev != '\x90' : result += [ prev ] * count else : result += [ '\x90' , '\x00' ] * count else : if prev != '\x90' : result += [ prev , '\x90' , chr ( count ) ] else : result += [ '\x90' , '\x00' , '\x90' , chr ( count ) ] count = 1 prev = c return '' . join ( result )
6271	def sphere ( radius = 0.5 , sectors = 32 , rings = 16 ) -> VAO : R = 1.0 / ( rings - 1 ) S = 1.0 / ( sectors - 1 ) vertices = [ 0 ] * ( rings * sectors * 3 ) normals = [ 0 ] * ( rings * sectors * 3 ) uvs = [ 0 ] * ( rings * sectors * 2 ) v , n , t = 0 , 0 , 0 for r in range ( rings ) : for s in range ( sectors ) : y = math . sin ( - math . pi / 2 + math . pi * r * R ) x = math . cos ( 2 * math . pi * s * S ) * math . sin ( math . pi * r * R ) z = math . sin ( 2 * math . pi * s * S ) * math . sin ( math . pi * r * R ) uvs [ t ] = s * S uvs [ t + 1 ] = r * R vertices [ v ] = x * radius vertices [ v + 1 ] = y * radius vertices [ v + 2 ] = z * radius normals [ n ] = x normals [ n + 1 ] = y normals [ n + 2 ] = z t += 2 v += 3 n += 3 indices = [ 0 ] * rings * sectors * 6 i = 0 for r in range ( rings - 1 ) : for s in range ( sectors - 1 ) : indices [ i ] = r * sectors + s indices [ i + 1 ] = ( r + 1 ) * sectors + ( s + 1 ) indices [ i + 2 ] = r * sectors + ( s + 1 ) indices [ i + 3 ] = r * sectors + s indices [ i + 4 ] = ( r + 1 ) * sectors + s indices [ i + 5 ] = ( r + 1 ) * sectors + ( s + 1 ) i += 6 vbo_vertices = numpy . array ( vertices , dtype = numpy . float32 ) vbo_normals = numpy . array ( normals , dtype = numpy . float32 ) vbo_uvs = numpy . array ( uvs , dtype = numpy . float32 ) vbo_elements = numpy . array ( indices , dtype = numpy . uint32 ) vao = VAO ( "sphere" , mode = mlg . TRIANGLES ) # VBOs vao . buffer ( vbo_vertices , '3f' , [ 'in_position' ] ) vao . buffer ( vbo_normals , '3f' , [ 'in_normal' ] ) vao . buffer ( vbo_uvs , '2f' , [ 'in_uv' ] ) vao . index_buffer ( vbo_elements , index_element_size = 4 ) return vao
4231	def run_subcommand ( netgear , args ) : subcommand = args . subcommand if subcommand == "block_device" or subcommand == "allow_device" : return netgear . allow_block_device ( args . mac_addr , BLOCK if subcommand == "block_device" else ALLOW ) if subcommand == "attached_devices" : if args . verbose : return netgear . get_attached_devices_2 ( ) else : return netgear . get_attached_devices ( ) if subcommand == 'traffic_meter' : return netgear . get_traffic_meter ( ) if subcommand == 'login' : return netgear . login ( ) print ( "Unknown subcommand" )
12694	def is_disjoint ( set1 , set2 , warn ) : for elem in set2 : if elem in set1 : raise ValueError ( warn ) return True
3009	def has_credentials ( self ) : credentials = _credentials_from_request ( self . request ) return ( credentials and not credentials . invalid and credentials . has_scopes ( self . _get_scopes ( ) ) )
7845	def get_items ( self ) : ret = [ ] l = self . xpath_ctxt . xpathEval ( "d:item" ) if l is not None : for i in l : ret . append ( DiscoItem ( self , i ) ) return ret
13842	def arkt_to_unixt ( ark_timestamp ) : res = datetime . datetime ( 2017 , 3 , 21 , 15 , 55 , 44 ) + datetime . timedelta ( seconds = ark_timestamp ) return res . timestamp ( )
158	def InColorspace ( to_colorspace , from_colorspace = "RGB" , children = None , name = None , deterministic = False , random_state = None ) : return WithColorspace ( to_colorspace , from_colorspace , children , name , deterministic , random_state )
10663	def elements ( compounds ) : elementlist = [ parse_compound ( compound ) . count ( ) . keys ( ) for compound in compounds ] return set ( ) . union ( * elementlist )
2890	def create_task ( self ) : return self . spec_class ( self . spec , self . get_task_spec_name ( ) , lane = self . get_lane ( ) , description = self . node . get ( 'name' , None ) )
8528	def get_ip_packet ( data , client_port , server_port , is_loopback = False ) : header = _loopback if is_loopback else _ethernet try : header . unpack ( data ) except Exception as ex : raise ValueError ( 'Bad header: %s' % ex ) tcp_p = getattr ( header . data , 'data' , None ) if type ( tcp_p ) != dpkt . tcp . TCP : raise ValueError ( 'Not a TCP packet' ) if tcp_p . dport == server_port : if client_port != 0 and tcp_p . sport != client_port : raise ValueError ( 'Request from different client' ) elif tcp_p . sport == server_port : if client_port != 0 and tcp_p . dport != client_port : raise ValueError ( 'Reply for different client' ) else : raise ValueError ( 'Packet not for/from client/server' ) return header . data
10004	def rename ( self , name ) : if is_valid_name ( name ) : if name not in self . system . models : self . name = name return True # Rename success else : # Model name already exists return False else : raise ValueError ( "Invalid name '%s'." % name )
10744	def print_runtime ( function ) : def wrapper ( * args , * * kwargs ) : pr = cProfile . Profile ( ) pr . enable ( ) output = function ( * args , * * kwargs ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort_stats ( 'tot' ) . print_stats ( 20 ) return output return wrapper
1829	def JA ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , target . read ( ) , cpu . PC )
12719	def axes ( self ) : return [ np . array ( self . ode_obj . getAxis ( i ) ) for i in range ( self . ADOF or self . LDOF ) ]
5460	def get_file_environment_variables ( file_params ) : env = { } for param in file_params : # We have no cases where the environment variable provided to user # scripts have a trailing slash, so be sure to always strip it. # The case that this is specifically handling is --input-recursive and # --output-recursive variables, which are directory values. env [ param . name ] = os . path . join ( DATA_MOUNT_POINT , param . docker_path . rstrip ( '/' ) ) if param . value else '' return env
6570	def last_arg_decorator ( func ) : @ wraps ( func ) def decorator ( * args , * * kwargs ) : if signature_matches ( func , args , kwargs ) : return func ( * args , * * kwargs ) else : return lambda last : func ( * ( args + ( last , ) ) , * * kwargs ) return decorator
1166	def join ( self , timeout = None ) : if not self . __initialized : raise RuntimeError ( "Thread.__init__() not called" ) if not self . __started . is_set ( ) : raise RuntimeError ( "cannot join thread before it is started" ) if self is current_thread ( ) : raise RuntimeError ( "cannot join current thread" ) if __debug__ : if not self . __stopped : self . _note ( "%s.join(): waiting until thread stops" , self ) self . __block . acquire ( ) try : if timeout is None : while not self . __stopped : self . __block . wait ( ) if __debug__ : self . _note ( "%s.join(): thread stopped" , self ) else : deadline = _time ( ) + timeout while not self . __stopped : delay = deadline - _time ( ) if delay <= 0 : if __debug__ : self . _note ( "%s.join(): timed out" , self ) break self . __block . wait ( delay ) else : if __debug__ : self . _note ( "%s.join(): thread stopped" , self ) finally : self . __block . release ( )
6076	def einstein_mass_in_units ( self , unit_mass = 'angular' , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . einstein_mass_in_units ( unit_mass = unit_mass , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
8321	def parse_important ( self , markup ) : important = [ ] table_titles = [ table . title for table in self . tables ] m = re . findall ( self . re [ "bold" ] , markup ) for bold in m : bold = self . plain ( bold ) if not bold in table_titles : important . append ( bold . lower ( ) ) return important
4612	def block_timestamp ( self , block_num ) : return int ( self . block_class ( block_num , blockchain_instance = self . blockchain ) . time ( ) . timestamp ( ) )
5736	def cleanup ( self ) : if self . subscription : logger . info ( "Deleting worker subscription..." ) self . subscriber_client . delete_subscription ( self . subscription )
4530	def _addLoggingLevel ( levelName , levelNum , methodName = None ) : if not methodName : methodName = levelName . lower ( ) if hasattr ( logging , levelName ) : raise AttributeError ( '{} already defined in logging module' . format ( levelName ) ) if hasattr ( logging , methodName ) : raise AttributeError ( '{} already defined in logging module' . format ( methodName ) ) if hasattr ( logging . getLoggerClass ( ) , methodName ) : raise AttributeError ( '{} already defined in logger class' . format ( methodName ) ) # This method was inspired by the answers to Stack Overflow post # http://stackoverflow.com/q/2183233/2988730, especially # http://stackoverflow.com/a/13638084/2988730 def logForLevel ( self , message , * args , * * kwargs ) : if self . isEnabledFor ( levelNum ) : self . _log ( levelNum , message , args , * * kwargs ) def logToRoot ( message , * args , * * kwargs ) : logging . log ( levelNum , message , * args , * * kwargs ) logging . addLevelName ( levelNum , levelName ) setattr ( logging , levelName , levelNum ) setattr ( logging . getLoggerClass ( ) , methodName , logForLevel ) setattr ( logging , methodName , logToRoot )
7756	def _set_response_handlers ( self , stanza , res_handler , err_handler , timeout_handler = None , timeout = None ) : # pylint: disable-msg=R0913 self . fix_out_stanza ( stanza ) to_jid = stanza . to_jid if to_jid : to_jid = unicode ( to_jid ) if timeout_handler : def callback ( dummy1 , dummy2 ) : """Wrapper for the timeout handler to make it compatible with the `ExpiringDictionary` """ timeout_handler ( ) self . _iq_response_handlers . set_item ( ( stanza . stanza_id , to_jid ) , ( res_handler , err_handler ) , timeout , callback ) else : self . _iq_response_handlers . set_item ( ( stanza . stanza_id , to_jid ) , ( res_handler , err_handler ) , timeout )
5655	def _finalize_profiles ( self ) : for stop , stop_profile in self . _stop_profiles . items ( ) : assert ( isinstance ( stop_profile , NodeProfileMultiObjective ) ) neighbor_label_bags = [ ] walk_durations_to_neighbors = [ ] departure_arrival_stop_pairs = [ ] if stop_profile . get_walk_to_target_duration ( ) != 0 and stop in self . _walk_network . node : neighbors = networkx . all_neighbors ( self . _walk_network , stop ) for neighbor in neighbors : neighbor_profile = self . _stop_profiles [ neighbor ] assert ( isinstance ( neighbor_profile , NodeProfileMultiObjective ) ) neighbor_real_connection_labels = neighbor_profile . get_labels_for_real_connections ( ) neighbor_label_bags . append ( neighbor_real_connection_labels ) walk_durations_to_neighbors . append ( int ( self . _walk_network . get_edge_data ( stop , neighbor ) [ "d_walk" ] / self . _walk_speed ) ) departure_arrival_stop_pairs . append ( ( stop , neighbor ) ) stop_profile . finalize ( neighbor_label_bags , walk_durations_to_neighbors , departure_arrival_stop_pairs )
6993	def flare_model_residual ( flareparams , times , mags , errs ) : modelmags , _ , _ , _ = flare_model ( flareparams , times , mags , errs ) return ( mags - modelmags ) / errs
8853	def on_open ( self ) : filename , filter = QtWidgets . QFileDialog . getOpenFileName ( self , 'Open' ) if filename : self . open_file ( filename ) self . actionRun . setEnabled ( True ) self . actionConfigure_run . setEnabled ( True )
1954	def symbolic_run_get_cons ( trace ) : m2 = Manticore . linux ( prog , workspace_url = 'mem:' ) f = Follower ( trace ) m2 . verbosity ( VERBOSITY ) m2 . register_plugin ( f ) def on_term_testcase ( mcore , state , stateid , err ) : with m2 . locked_context ( ) as ctx : readdata = [ ] for name , fd , data in state . platform . syscall_trace : if name in ( '_receive' , '_read' ) and fd == 0 : readdata . append ( data ) ctx [ 'readdata' ] = readdata ctx [ 'constraints' ] = list ( state . constraints . constraints ) m2 . subscribe ( 'will_terminate_state' , on_term_testcase ) m2 . run ( ) constraints = m2 . context [ 'constraints' ] datas = m2 . context [ 'readdata' ] return constraints , datas
7186	def maybe_replace_any_if_equal ( name , expected , actual ) : is_equal = expected == actual if not is_equal and Config . replace_any : actual_str = minimize_whitespace ( str ( actual ) ) if actual_str and actual_str [ 0 ] in { '"' , "'" } : actual_str = actual_str [ 1 : - 1 ] is_equal = actual_str in { 'Any' , 'typing.Any' , 't.Any' } if not is_equal : expected_annotation = minimize_whitespace ( str ( expected ) ) actual_annotation = minimize_whitespace ( str ( actual ) ) raise ValueError ( f"incompatible existing {name}. " + f"Expected: {expected_annotation!r}, actual: {actual_annotation!r}" ) return expected or actual
1790	def IDIV ( cpu , src ) : reg_name_h = { 8 : 'AH' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ src . size ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ src . size ] dividend = Operators . CONCAT ( src . size * 2 , cpu . read_register ( reg_name_h ) , cpu . read_register ( reg_name_l ) ) divisor = src . read ( ) if isinstance ( divisor , int ) and divisor == 0 : raise DivideByZeroError ( ) dst_size = src . size * 2 divisor = Operators . SEXTEND ( divisor , src . size , dst_size ) mask = ( 1 << dst_size ) - 1 sign_mask = 1 << ( dst_size - 1 ) dividend_sign = ( dividend & sign_mask ) != 0 divisor_sign = ( divisor & sign_mask ) != 0 if isinstance ( divisor , int ) : if divisor_sign : divisor = ( ( ~ divisor ) + 1 ) & mask divisor = - divisor if isinstance ( dividend , int ) : if dividend_sign : dividend = ( ( ~ dividend ) + 1 ) & mask dividend = - dividend quotient = Operators . SDIV ( dividend , divisor ) if ( isinstance ( dividend , int ) and isinstance ( dividend , int ) ) : # handle the concrete case remainder = dividend - ( quotient * divisor ) else : # symbolic case -- optimize via SREM remainder = Operators . SREM ( dividend , divisor ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( quotient , 0 , src . size ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( remainder , 0 , src . size ) )
9562	def _apply_skips ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for skip in self . _skips : try : result = skip ( r ) if result is True : yield True except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( skip . __name__ , skip . __doc__ ) if context is not None : p [ 'context' ] = context yield p
9892	def _uptime_minix ( ) : try : f = open ( '/proc/uptime' , 'r' ) up = float ( f . read ( ) ) f . close ( ) return up except ( IOError , ValueError ) : return None
6141	def in_out_check ( self ) : devices = available_devices ( ) if not self . in_idx in devices : raise OSError ( "Input device is unavailable" ) in_check = devices [ self . in_idx ] if not self . out_idx in devices : raise OSError ( "Output device is unavailable" ) out_check = devices [ self . out_idx ] if ( ( in_check [ 'inputs' ] == 0 ) and ( out_check [ 'outputs' ] == 0 ) ) : raise StandardError ( 'Invalid input and output devices' ) elif ( in_check [ 'inputs' ] == 0 ) : raise ValueError ( 'Selected input device has no inputs' ) elif ( out_check [ 'outputs' ] == 0 ) : raise ValueError ( 'Selected output device has no outputs' ) return True
12409	def package_info ( cls , package ) : if package not in cls . package_info_cache : package_json_url = 'https://pypi.python.org/pypi/%s/json' % package try : logging . getLogger ( 'requests' ) . setLevel ( logging . WARN ) response = requests . get ( package_json_url ) response . raise_for_status ( ) cls . package_info_cache [ package ] = simplejson . loads ( response . text ) except Exception as e : log . debug ( 'Could not get package info from %s: %s' , package_json_url , e ) cls . package_info_cache [ package ] = None return cls . package_info_cache [ package ]
2208	def ensuredir ( dpath , mode = 0o1777 , verbose = None ) : if verbose is None : # nocover verbose = 0 if isinstance ( dpath , ( list , tuple ) ) : # nocover dpath = join ( * dpath ) if not exists ( dpath ) : if verbose : # nocover print ( 'Ensuring new directory (%r)' % dpath ) if sys . version_info . major == 2 : # nocover os . makedirs ( normpath ( dpath ) , mode = mode ) else : os . makedirs ( normpath ( dpath ) , mode = mode , exist_ok = True ) else : if verbose : # nocover print ( 'Ensuring existing directory (%r)' % dpath ) return dpath
10708	def delete_vacation ( _id ) : arequest = requests . delete ( VACATIONS_URL + "/" + _id , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "Failed to delete vacation. " + status_code ) return False return True
2521	def p_file_type ( self , f_term , predicate ) : try : for _ , _ , ftype in self . graph . triples ( ( f_term , predicate , None ) ) : try : if ftype . endswith ( 'binary' ) : ftype = 'BINARY' elif ftype . endswith ( 'source' ) : ftype = 'SOURCE' elif ftype . endswith ( 'other' ) : ftype = 'OTHER' elif ftype . endswith ( 'archive' ) : ftype = 'ARCHIVE' self . builder . set_file_type ( self . doc , ftype ) except SPDXValueError : self . value_error ( 'FILE_TYPE' , ftype ) except CardinalityError : self . more_than_one_error ( 'file type' )
10501	def waitForWindowToDisappear ( self , winName , timeout = 10 ) : callback = AXCallbacks . elemDisappearedCallback retelem = None args = ( retelem , self ) # For some reason for the AXUIElementDestroyed notification to fire, # we need to have a reference to it first win = self . findFirst ( AXRole = 'AXWindow' , AXTitle = winName ) return self . waitFor ( timeout , 'AXUIElementDestroyed' , callback = callback , args = args , AXRole = 'AXWindow' , AXTitle = winName )
13465	def __register_library ( self , module_name : str , attr : str , fallback : str = None ) : # Import the module Named in the string try : module = importlib . import_module ( module_name ) # If module is not found it checks if an alternative is is listed # If it is then it substitutes it, just so that the code can run except ImportError : if fallback is not None : module = importlib . import_module ( fallback ) self . __logger . warn ( module_name + " not available: Replaced with " + fallback ) else : self . __logger . warn ( module_name + " not available: No Replacement Specified" ) # Cram the module into the __sketch in the form of module -> "attr" # AKA the same as `import module as attr` if not attr in dir ( self . __sketch ) : setattr ( self . __sketch , attr , module ) else : self . __logger . warn ( attr + " could not be imported as it's label is already used in the sketch" )
10164	def get_personalities ( self , line ) : return [ split ( '\W+' , i ) [ 1 ] for i in line . split ( ':' ) [ 1 ] . split ( ' ' ) if i . startswith ( '[' ) ]
1582	def read ( self , dispatcher ) : try : if not self . is_header_read : # try reading header to_read = HeronProtocol . HEADER_SIZE - len ( self . header ) self . header += dispatcher . recv ( to_read ) if len ( self . header ) == HeronProtocol . HEADER_SIZE : self . is_header_read = True else : Log . debug ( "Header read incomplete; read %d bytes of header" % len ( self . header ) ) return if self . is_header_read and not self . is_complete : # try reading data to_read = self . get_datasize ( ) - len ( self . data ) self . data += dispatcher . recv ( to_read ) if len ( self . data ) == self . get_datasize ( ) : self . is_complete = True except socket . error as e : if e . errno == socket . errno . EAGAIN or e . errno == socket . errno . EWOULDBLOCK : # Try again later -> call continue_read later Log . debug ( "Try again error" ) else : # Fatal error Log . debug ( "Fatal error when reading IncomingPacket" ) raise RuntimeError ( "Fatal error occured in IncomingPacket.read()" )
13618	def get_branches ( self ) : return [ self . _sanitize ( branch ) for branch in self . _git . branch ( color = "never" ) . splitlines ( ) ]
9900	def data ( self , data ) : if self . is_caching : self . cache = data else : fcontents = self . file_contents with open ( self . path , "w" ) as f : try : # Write the file. Keep user settings about indentation, etc indent = self . indent if self . pretty else None json . dump ( data , f , sort_keys = self . sort_keys , indent = indent ) except Exception as e : # Rollback to prevent data loss f . seek ( 0 ) f . truncate ( ) f . write ( fcontents ) # And re-raise the exception raise e self . _updateType ( )
6510	def _parse ( self , filename ) : self . names = { } with codecs . open ( filename , encoding = "iso8859-1" ) as f : for line in f : if any ( map ( lambda c : 128 < ord ( c ) < 160 , line ) ) : line = line . encode ( "iso8859-1" ) . decode ( "windows-1252" ) self . _eat_name_line ( line . strip ( ) )
3800	def Bahadori_gas ( T , MW ) : A = [ 4.3931323468E-1 , - 3.88001122207E-2 , 9.28616040136E-4 , - 6.57828995724E-6 ] B = [ - 2.9624238519E-3 , 2.67956145820E-4 , - 6.40171884139E-6 , 4.48579040207E-8 ] C = [ 7.54249790107E-6 , - 6.46636219509E-7 , 1.5124510261E-8 , - 1.0376480449E-10 ] D = [ - 6.0988433456E-9 , 5.20752132076E-10 , - 1.19425545729E-11 , 8.0136464085E-14 ] X , Y = T , MW a = A [ 0 ] + B [ 0 ] * X + C [ 0 ] * X ** 2 + D [ 0 ] * X ** 3 b = A [ 1 ] + B [ 1 ] * X + C [ 1 ] * X ** 2 + D [ 1 ] * X ** 3 c = A [ 2 ] + B [ 2 ] * X + C [ 2 ] * X ** 2 + D [ 2 ] * X ** 3 d = A [ 3 ] + B [ 3 ] * X + C [ 3 ] * X ** 2 + D [ 3 ] * X ** 3 return a + b * Y + c * Y ** 2 + d * Y ** 3
7411	def sample_loci ( self ) : ## store idx of passing loci idxs = np . random . choice ( self . idxs , self . ntests ) ## open handle, make a proper generator to reduce mem with open ( self . data ) as indata : liter = ( indata . read ( ) . strip ( ) . split ( "|\n" ) ) ## store data as dict seqdata = { i : "" for i in self . samples } ## put chunks into a list for idx , loc in enumerate ( liter ) : if idx in idxs : ## parse chunk lines = loc . split ( "\n" ) [ : - 1 ] names = [ i . split ( ) [ 0 ] for i in lines ] seqs = [ i . split ( ) [ 1 ] for i in lines ] dd = { i : j for i , j in zip ( names , seqs ) } ## add data to concatenated seqdict for name in seqdata : if name in names : seqdata [ name ] += dd [ name ] else : seqdata [ name ] += "N" * len ( seqs [ 0 ] ) ## concatenate into a phylip file return seqdata
2639	def cancel ( self , job_ids ) : statuses = [ ] for job_id in job_ids : try : self . delete_instance ( job_id ) statuses . append ( True ) self . provisioned_blocks -= 1 except Exception : statuses . append ( False ) return statuses
2146	def _separate ( self , kwargs ) : self . _pop_none ( kwargs ) result = { } for field in Resource . config_fields : if field in kwargs : result [ field ] = kwargs . pop ( field ) if field in Resource . json_fields : # If result[field] is not a string we can continue on if not isinstance ( result [ field ] , six . string_types ) : continue try : data = json . loads ( result [ field ] ) result [ field ] = data except ValueError : raise exc . TowerCLIError ( 'Provided json file format ' 'invalid. Please recheck.' ) return result
10573	def get_local_songs ( filepaths , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None , max_depth = float ( 'inf' ) ) : logger . info ( "Loading local songs..." ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS , max_depth = max_depth ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
5742	def service_start ( service = None , param = None ) : if service is not None : to_run = [ "python" , service ] if param is not None : to_run += param return subprocess . Popen ( to_run ) return False
1338	def crossentropy ( label , logits ) : assert logits . ndim == 1 # for numerical reasons we subtract the max logit # (mathematically it doesn't matter!) # otherwise exp(logits) might become too large or too small logits = logits - np . max ( logits ) e = np . exp ( logits ) s = np . sum ( e ) ce = np . log ( s ) - logits [ label ] return ce
13022	def query ( self , sql_string , * args , * * kwargs ) : commit = None columns = None if kwargs . get ( 'commit' ) is not None : commit = kwargs . pop ( 'commit' ) if kwargs . get ( 'columns' ) is not None : columns = kwargs . pop ( 'columns' ) query = self . _assemble_simple ( sql_string , * args , * * kwargs ) return self . _execute ( query , commit = commit , working_columns = columns )
11588	def _rc_brpoplpush ( self , src , dst , timeout = 0 ) : rpop = self . brpop ( src , timeout ) if rpop is not None : self . lpush ( dst , rpop [ 1 ] ) return rpop [ 1 ] return None
3365	def add_pfba ( model , objective = None , fraction_of_optimum = 1.0 ) : if objective is not None : model . objective = objective if model . solver . objective . name == '_pfba_objective' : raise ValueError ( 'The model already has a pFBA objective.' ) sutil . fix_objective_as_constraint ( model , fraction = fraction_of_optimum ) reaction_variables = ( ( rxn . forward_variable , rxn . reverse_variable ) for rxn in model . reactions ) variables = chain ( * reaction_variables ) model . objective = model . problem . Objective ( Zero , direction = 'min' , sloppy = True , name = "_pfba_objective" ) model . objective . set_linear_coefficients ( { v : 1.0 for v in variables } )
3396	def gapfill ( model , universal = None , lower_bound = 0.05 , penalties = None , demand_reactions = True , exchange_reactions = False , iterations = 1 ) : gapfiller = GapFiller ( model , universal = universal , lower_bound = lower_bound , penalties = penalties , demand_reactions = demand_reactions , exchange_reactions = exchange_reactions ) return gapfiller . fill ( iterations = iterations )
1230	def tf_optimization ( self , states , internals , actions , terminal , reward , next_states = None , next_internals = None ) : arguments = self . optimizer_arguments ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) return self . optimizer . minimize ( * * arguments )
10625	def _calculate_Hfr_coal ( self , T ) : m_C = 0 # kg/h m_H = 0 # kg/h m_O = 0 # kg/h m_N = 0 # kg/h m_S = 0 # kg/h Hfr = 0.0 # kWh/h for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) formula = compound . split ( '[' ) [ 0 ] if stoich . element_mass_fraction ( formula , 'C' ) == 1.0 : m_C += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'H' ) == 1.0 : m_H += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'O' ) == 1.0 : m_O += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'N' ) == 1.0 : m_N += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'S' ) == 1.0 : m_S += self . _compound_mfrs [ index ] else : dHfr = thermo . H ( compound , T , self . _compound_mfrs [ index ] ) Hfr += dHfr m_total = m_C + m_H + m_O + m_N + m_S # kg/h y_C = m_C / m_total y_H = m_H / m_total y_O = m_O / m_total y_N = m_N / m_total y_S = m_S / m_total hmodel = coals . DafHTy ( ) H = hmodel . calculate ( T = T + 273.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 # kWh/kg H298 = hmodel . calculate ( T = 298.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 # kWh/kg Hdaf = H - H298 + self . _DH298 # kWh/kg Hdaf *= m_total # kWh/h Hfr += Hdaf return Hfr
11181	def acquire ( self , * args , * * kwargs ) : with self . _stat_lock : self . _waiting += 1 self . _lock . acquire ( * args , * * kwargs ) with self . _stat_lock : self . _locked = True self . _waiting -= 1
10948	def reset ( self , * * kwargs ) : self . aug_state . reset ( ) super ( LMAugmentedState , self ) . reset ( * * kwargs )
10503	def waitForFocusedWindowToChange ( self , nextWinName , timeout = 10 ) : callback = AXCallbacks . returnElemCallback retelem = None return self . waitFor ( timeout , 'AXFocusedWindowChanged' , AXTitle = nextWinName )
11124	def move_directory ( self , relativePath , relativeDestination , replace = False , verbose = True ) : # normalize path relativePath = os . path . normpath ( relativePath ) relativeDestination = os . path . normpath ( relativeDestination ) # get files and directories filesInfo = list ( self . walk_files_info ( relativePath = relativePath ) ) dirsPath = list ( self . walk_directories_relative_path ( relativePath = relativePath ) ) dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage # remove directory info only self . remove_directory ( relativePath = relativePath , removeFromSystem = False ) # create new relative path self . add_directory ( relativeDestination ) # move files for RP , info in filesInfo : source = os . path . join ( self . __path , relativePath , RP ) destination = os . path . join ( self . __path , relativeDestination , RP ) # add directory newDirRP , fileName = os . path . split ( os . path . join ( relativeDestination , RP ) ) dirInfoDict = self . add_directory ( newDirRP ) # move file if os . path . isfile ( destination ) : if replace : os . remove ( destination ) if verbose : warnings . warn ( "file '%s' is copied replacing existing one in destination '%s'." % ( fileName , newDirRP ) ) else : if verbose : warnings . warn ( "file '%s' is not copied because the same file exists in destination '%s'." % ( fileName , destination ) ) continue os . rename ( source , destination ) # set file information dict . __getitem__ ( dirInfoDict , "files" ) [ fileName ] = info # save repository self . save ( )
2975	def cmd_destroy ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . destroy ( )
12568	def create_dataset ( self , ds_name , data , attrs = None , dtype = None ) : if ds_name in self . _datasets : ds = self . _datasets [ ds_name ] if ds . dtype != data . dtype : warnings . warn ( 'Dataset and data dtype are different!' ) else : if dtype is None : dtype = data . dtype ds = self . _group . create_dataset ( ds_name , data . shape , dtype = dtype ) if attrs is not None : for key in attrs : setattr ( ds . attrs , key , attrs [ key ] ) ds . read_direct ( data ) self . _datasets [ ds_name ] = ds return ds
7674	def slice ( self , start_time , end_time , strict = False ) : # Make sure duration is set in file metadata if self . file_metadata . duration is None : raise JamsError ( 'Duration must be set (jam.file_metadata.duration) before ' 'slicing can be performed.' ) # Make sure start and end times are within the file start/end times if ( start_time < 0 or start_time > float ( self . file_metadata . duration ) or end_time < start_time or end_time > float ( self . file_metadata . duration ) ) : raise ParameterError ( 'start_time and end_time must be within the original file ' 'duration ({:f}) and end_time cannot be smaller than ' 'start_time.' . format ( float ( self . file_metadata . duration ) ) ) # Create a new jams jam_sliced = JAMS ( annotations = None , file_metadata = self . file_metadata , sandbox = self . sandbox ) # trim annotations jam_sliced . annotations = self . annotations . slice ( start_time , end_time , strict = strict ) # adjust dutation jam_sliced . file_metadata . duration = end_time - start_time # Document jam-level trim in top level sandbox if 'slice' not in jam_sliced . sandbox . keys ( ) : jam_sliced . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time } ] ) else : jam_sliced . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time } ) return jam_sliced
13573	def skip ( course , num = 1 ) : sel = None try : sel = Exercise . get_selected ( ) if sel . course . tid != course . tid : sel = None except NoExerciseSelected : pass if sel is None : sel = course . exercises . first ( ) else : try : sel = Exercise . get ( Exercise . id == sel . id + num ) except peewee . DoesNotExist : print ( "There are no more exercises in this course." ) return False sel . set_select ( ) list_all ( single = sel )
7375	def code ( self , code ) : def decorator ( exception ) : self [ code ] = exception return exception return decorator
5485	def _eval_arg_type ( arg_type , T = Any , arg = None , sig = None ) : try : T = eval ( arg_type ) except Exception as e : raise ValueError ( 'The type of {0} could not be evaluated in {1} for {2}: {3}' . format ( arg_type , arg , sig , text_type ( e ) ) ) else : if type ( T ) not in ( type , Type ) : raise TypeError ( '{0} is not a valid type in {1} for {2}' . format ( repr ( T ) , arg , sig ) ) return T
1512	def distribute_package ( roles , cl_args ) : Log . info ( "Distributing heron package to nodes (this might take a while)..." ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] tar_file = tempfile . NamedTemporaryFile ( suffix = ".tmp" ) . name Log . debug ( "TAR file %s to %s" % ( cl_args [ "heron_dir" ] , tar_file ) ) make_tarfile ( tar_file , cl_args [ "heron_dir" ] ) dist_nodes = masters . union ( slaves ) scp_package ( tar_file , dist_nodes , cl_args )
8003	def get_form ( self , form_type = "form" ) : if self . form : if self . form . type != form_type : raise ValueError ( "Bad form type in the jabber:iq:register element" ) return self . form form = Form ( form_type , instructions = self . instructions ) form . add_field ( "FORM_TYPE" , [ u"jabber:iq:register" ] , "hidden" ) for field in legacy_fields : field_type , field_label = legacy_fields [ field ] value = getattr ( self , field ) if value is None : continue if form_type == "form" : if not value : value = None form . add_field ( name = field , field_type = field_type , label = field_label , value = value , required = True ) else : form . add_field ( name = field , value = value ) return form
1869	def MOVZX ( cpu , op0 , op1 ) : op0 . write ( Operators . ZEXTEND ( op1 . read ( ) , op0 . size ) )
2030	def CODECOPY ( self , mem_offset , code_offset , size ) : self . _allocate ( mem_offset , size ) GCOPY = 3 # cost to copy one 32 byte word copyfee = self . safe_mul ( GCOPY , Operators . UDIV ( self . safe_add ( size , 31 ) , 32 ) ) self . _consume ( copyfee ) if issymbolic ( size ) : max_size = solver . max ( self . constraints , size ) else : max_size = size for i in range ( max_size ) : if issymbolic ( i < size ) : default = Operators . ITEBV ( 8 , i < size , 0 , self . _load ( mem_offset + i , 1 ) ) # Fixme. unnecessary memory read else : if i < size : default = 0 else : default = self . _load ( mem_offset + i , 1 ) if issymbolic ( code_offset ) : value = Operators . ITEBV ( 8 , code_offset + i >= len ( self . bytecode ) , default , self . bytecode [ code_offset + i ] ) else : if code_offset + i >= len ( self . bytecode ) : value = default else : value = self . bytecode [ code_offset + i ] self . _store ( mem_offset + i , value ) self . _publish ( 'did_evm_read_code' , code_offset , size )
2504	def get_extr_license_ident ( self , extr_lic ) : identifier_tripples = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'licenseId' ] , None ) ) ) if not identifier_tripples : self . error = True msg = 'Extracted license must have licenseId property.' self . logger . log ( msg ) return if len ( identifier_tripples ) > 1 : self . more_than_one_error ( 'extracted license identifier_tripples' ) return identifier_tripple = identifier_tripples [ 0 ] _s , _p , identifier = identifier_tripple return identifier
12017	def _dump_field ( self , fd ) : v = { } v [ 'label' ] = Pbd . LABELS [ fd . label ] v [ 'type' ] = fd . type_name if len ( fd . type_name ) > 0 else Pbd . TYPES [ fd . type ] v [ 'name' ] = fd . name v [ 'number' ] = fd . number v [ 'default' ] = '[default = {}]' . format ( fd . default_value ) if len ( fd . default_value ) > 0 else '' f = '{label} {type} {name} = {number} {default};' . format ( * * v ) f = ' ' . join ( f . split ( ) ) self . _print ( f ) if len ( fd . type_name ) > 0 : self . uses . append ( fd . type_name )
7678	def pitch_contour ( annotation , * * kwargs ) : ax = kwargs . pop ( 'ax' , None ) # If the annotation is empty, we need to construct a new axes ax = mir_eval . display . __get_axes ( ax = ax ) [ 0 ] times , values = annotation . to_interval_values ( ) indices = np . unique ( [ v [ 'index' ] for v in values ] ) for idx in indices : rows = [ i for ( i , v ) in enumerate ( values ) if v [ 'index' ] == idx ] freqs = np . asarray ( [ values [ r ] [ 'frequency' ] for r in rows ] ) unvoiced = ~ np . asarray ( [ values [ r ] [ 'voiced' ] for r in rows ] ) freqs [ unvoiced ] *= - 1 ax = mir_eval . display . pitch ( times [ rows , 0 ] , freqs , unvoiced = True , ax = ax , * * kwargs ) return ax
728	def numberMapForBits ( self , bits ) : numberMap = dict ( ) for bit in bits : numbers = self . numbersForBit ( bit ) for number in numbers : if not number in numberMap : numberMap [ number ] = set ( ) numberMap [ number ] . add ( bit ) return numberMap
9595	def execute_async_script ( self , script , * args ) : return self . _execute ( Command . EXECUTE_ASYNC_SCRIPT , { 'script' : script , 'args' : list ( args ) } )
13013	def strip_labels ( filename ) : labels = [ ] with open ( filename ) as f , open ( 'processed_labels.txt' , 'w' ) as f1 : for l in f : if l . startswith ( '#' ) : next l = l . replace ( " ." , '' ) l = l . replace ( ">\tskos:prefLabel\t" , ' ' ) l = l . replace ( "<" , '' ) l = l . replace ( ">\trdfs:label\t" , ' ' ) f1 . write ( l )
12475	def remove_all ( filelist , folder = '' ) : if not folder : for f in filelist : os . remove ( f ) else : for f in filelist : os . remove ( op . join ( folder , f ) )
13750	def one_to_many ( clsname , * * kw ) : @ declared_attr def o2m ( cls ) : cls . _references ( ( clsname , cls . __name__ ) ) return relationship ( clsname , * * kw ) return o2m
8549	def create_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule ) : properties = { "name" : firewall_rule . name } if firewall_rule . protocol : properties [ 'protocol' ] = firewall_rule . protocol # Optional Properties if firewall_rule . source_mac : properties [ 'sourceMac' ] = firewall_rule . source_mac if firewall_rule . source_ip : properties [ 'sourceIp' ] = firewall_rule . source_ip if firewall_rule . target_ip : properties [ 'targetIp' ] = firewall_rule . target_ip if firewall_rule . port_range_start : properties [ 'portRangeStart' ] = firewall_rule . port_range_start if firewall_rule . port_range_end : properties [ 'portRangeEnd' ] = firewall_rule . port_range_end if firewall_rule . icmp_type : properties [ 'icmpType' ] = firewall_rule . icmp_type if firewall_rule . icmp_code : properties [ 'icmpCode' ] = firewall_rule . icmp_code data = { "properties" : properties } response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules' % ( datacenter_id , server_id , nic_id ) , method = 'POST' , data = json . dumps ( data ) ) return response
3772	def phase_select_property ( phase = None , s = None , l = None , g = None , V_over_F = None ) : if phase == 's' : return s elif phase == 'l' : return l elif phase == 'g' : return g elif phase == 'two-phase' : return None #TODO: all two-phase properties? elif phase is None : return None else : raise Exception ( 'Property not recognized' )
5222	def market_info ( ticker : str ) -> dict : t_info = ticker . split ( ) assets = param . load_info ( 'assets' ) # ========================== # # Equity # # ========================== # if ( t_info [ - 1 ] == 'Equity' ) and ( '=' not in t_info [ 0 ] ) : exch = t_info [ - 2 ] for info in assets . get ( 'Equity' , [ dict ( ) ] ) : if 'exch_codes' not in info : continue if exch in info [ 'exch_codes' ] : return info return dict ( ) # ============================ # # Currency # # ============================ # if t_info [ - 1 ] == 'Curncy' : for info in assets . get ( 'Curncy' , [ dict ( ) ] ) : if 'tickers' not in info : continue if ( t_info [ 0 ] . split ( '+' ) [ 0 ] in info [ 'tickers' ] ) or ( t_info [ 0 ] [ - 1 ] . isdigit ( ) and ( t_info [ 0 ] [ : - 1 ] in info [ 'tickers' ] ) ) : return info return dict ( ) if t_info [ - 1 ] == 'Comdty' : for info in assets . get ( 'Comdty' , [ dict ( ) ] ) : if 'tickers' not in info : continue if t_info [ 0 ] [ : - 1 ] in info [ 'tickers' ] : return info return dict ( ) # =================================== # # Index / Futures # # =================================== # if ( t_info [ - 1 ] == 'Index' ) or ( ( t_info [ - 1 ] == 'Equity' ) and ( '=' in t_info [ 0 ] ) ) : if t_info [ - 1 ] == 'Equity' : tck = t_info [ 0 ] . split ( '=' ) [ 0 ] else : tck = ' ' . join ( t_info [ : - 1 ] ) for info in assets . get ( 'Index' , [ dict ( ) ] ) : if 'tickers' not in info : continue if ( tck [ : 2 ] == 'UX' ) and ( 'UX' in info [ 'tickers' ] ) : return info if tck in info [ 'tickers' ] : if t_info [ - 1 ] == 'Equity' : return info if not info . get ( 'is_fut' , False ) : return info if tck [ : - 1 ] . rstrip ( ) in info [ 'tickers' ] : if info . get ( 'is_fut' , False ) : return info return dict ( ) if t_info [ - 1 ] == 'Corp' : for info in assets . get ( 'Corp' , [ dict ( ) ] ) : if 'ticker' not in info : continue return dict ( )
1991	def ls ( self , glob_str ) : path = os . path . join ( self . uri , glob_str ) return [ os . path . split ( s ) [ 1 ] for s in glob . glob ( path ) ]
91	def derive_random_states ( random_state , n = 1 ) : seed_ = random_state . randint ( SEED_MIN_VALUE , SEED_MAX_VALUE , 1 ) [ 0 ] return [ new_random_state ( seed_ + i ) for i in sm . xrange ( n ) ]
12266	def docstring ( docstr ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : return func ( * args , * * kwargs ) wrapper . __doc__ = docstr return wrapper return decorator
1352	def make_success_response ( self , result ) : response = self . make_response ( constants . RESPONSE_STATUS_SUCCESS ) response [ constants . RESPONSE_KEY_RESULT ] = result return response
1881	def constrain ( self , constraint ) : constraint = self . migrate_expression ( constraint ) self . _constraints . add ( constraint )
3161	def get ( self , conversation_id , * * queryparams ) : self . conversation_id = conversation_id return self . _mc_client . _get ( url = self . _build_path ( conversation_id ) , * * queryparams )
892	def columnForCell ( self , cell ) : self . _validateCell ( cell ) return int ( cell / self . cellsPerColumn )
10670	def _split_compound_string_ ( compound_string ) : formula = compound_string . replace ( ']' , '' ) . split ( '[' ) [ 0 ] phase = compound_string . replace ( ']' , '' ) . split ( '[' ) [ 1 ] return formula , phase
11164	def atime ( self ) : try : return self . _stat . st_atime except : # pragma: no cover self . _stat = self . stat ( ) return self . atime
1275	def tf_retrieve_indices ( self , indices ) : states = dict ( ) for name in sorted ( self . states_memory ) : states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = indices ) internals = dict ( ) for name in sorted ( self . internals_memory ) : internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = indices ) actions = dict ( ) for name in sorted ( self . actions_memory ) : actions [ name ] = tf . gather ( params = self . actions_memory [ name ] , indices = indices ) terminal = tf . gather ( params = self . terminal_memory , indices = indices ) reward = tf . gather ( params = self . reward_memory , indices = indices ) if self . include_next_states : assert util . rank ( indices ) == 1 next_indices = ( indices + 1 ) % self . capacity next_states = dict ( ) for name in sorted ( self . states_memory ) : next_states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = next_indices ) next_internals = dict ( ) for name in sorted ( self . internals_memory ) : next_internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = next_indices ) return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) else : return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
10416	def data_contains_key_builder ( key : str ) -> NodePredicate : # noqa: D202 def data_contains_key ( _ : BELGraph , node : BaseEntity ) -> bool : """Pass only for a node that contains the enclosed key in its data dictionary. :return: If the node contains the enclosed key in its data dictionary """ return key in node return data_contains_key
9761	def logs ( ctx , job , past , follow , hide_time ) : def get_experiment_logs ( ) : if past : try : response = PolyaxonClient ( ) . experiment . logs ( user , project_name , _experiment , stream = False ) get_logs_handler ( handle_job_info = True , show_timestamp = not hide_time , stream = False ) ( response . content . decode ( ) . split ( '\n' ) ) print ( ) if not follow : return except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : if not follow : Printer . print_error ( 'Could not get logs for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) try : PolyaxonClient ( ) . experiment . logs ( user , project_name , _experiment , message_handler = get_logs_handler ( handle_job_info = True , show_timestamp = not hide_time ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get logs for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) def get_experiment_job_logs ( ) : if past : try : response = PolyaxonClient ( ) . experiment_job . logs ( user , project_name , _experiment , _job , stream = False ) get_logs_handler ( handle_job_info = True , show_timestamp = not hide_time , stream = False ) ( response . content . decode ( ) . split ( '\n' ) ) print ( ) if not follow : return except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : if not follow : Printer . print_error ( 'Could not get logs for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) try : PolyaxonClient ( ) . experiment_job . logs ( user , project_name , _experiment , _job , message_handler = get_logs_handler ( handle_job_info = True , show_timestamp = not hide_time ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_logs ( ) else : get_experiment_logs ( )
9407	def read_file ( path , session = None ) : try : data = loadmat ( path , struct_as_record = True ) except UnicodeDecodeError as e : raise Oct2PyError ( str ( e ) ) out = dict ( ) for ( key , value ) in data . items ( ) : out [ key ] = _extract ( value , session ) return out
4556	def all_named_colors ( ) : yield from _TO_COLOR_USER . items ( ) for name , color in _TO_COLOR . items ( ) : if name not in _TO_COLOR_USER : yield name , color
12208	def add ( self , * entries ) : for entry in entries : if isinstance ( entry , string_types ) : self . _add_entries ( database . parse_string ( entry , bib_format = 'bibtex' ) ) else : self . _add_entries ( entry )
11690	def get_area ( self , geojson ) : geojson = json . load ( open ( geojson , 'r' ) ) self . area = Polygon ( geojson [ 'features' ] [ 0 ] [ 'geometry' ] [ 'coordinates' ] [ 0 ] )
6548	def move_to ( self , ypos , xpos ) : # the screen's co-ordinates are 1 based, but the command is 0 based xpos -= 1 ypos -= 1 self . exec_command ( "MoveCursor({0}, {1})" . format ( ypos , xpos ) . encode ( "ascii" ) )
11958	def is_dec ( ip ) : try : dec = int ( str ( ip ) ) except ValueError : return False if dec > 4294967295 or dec < 0 : return False return True
11098	def select_by_size ( self , min_size = 0 , max_size = 1 << 40 , recursive = True ) : def filters ( p ) : return min_size <= p . size <= max_size return self . select_file ( filters , recursive )
10022	def update_environment ( self , environment_name , description = None , option_settings = [ ] , tier_type = None , tier_name = None , tier_version = '1.0' ) : out ( "Updating environment: " + str ( environment_name ) ) messages = self . ebs . validate_configuration_settings ( self . app_name , option_settings , environment_name = environment_name ) messages = messages [ 'ValidateConfigurationSettingsResponse' ] [ 'ValidateConfigurationSettingsResult' ] [ 'Messages' ] ok = True for message in messages : if message [ 'Severity' ] == 'error' : ok = False out ( "[" + message [ 'Severity' ] + "] " + str ( environment_name ) + " - '" + message [ 'Namespace' ] + ":" + message [ 'OptionName' ] + "': " + message [ 'Message' ] ) self . ebs . update_environment ( environment_name = environment_name , description = description , option_settings = option_settings , tier_type = tier_type , tier_name = tier_name , tier_version = tier_version )
2781	def create ( self ) : input_params = { "type" : self . type , "data" : self . data , "name" : self . name , "priority" : self . priority , "port" : self . port , "ttl" : self . ttl , "weight" : self . weight , "flags" : self . flags , "tags" : self . tags } data = self . get_data ( "domains/%s/records" % ( self . domain ) , type = POST , params = input_params , ) if data : self . id = data [ 'domain_record' ] [ 'id' ]
10239	def count_citations_by_annotation ( graph : BELGraph , annotation : str ) -> Mapping [ str , typing . Counter [ str ] ] : citations = defaultdict ( lambda : defaultdict ( set ) ) for u , v , data in graph . edges ( data = True ) : if not edge_has_annotation ( data , annotation ) or CITATION not in data : continue k = data [ ANNOTATIONS ] [ annotation ] citations [ k ] [ u , v ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) ) return { k : Counter ( itt . chain . from_iterable ( v . values ( ) ) ) for k , v in citations . items ( ) }
10629	def HHV ( self , HHV ) : self . _HHV = HHV # MJ/kg coal if self . isCoal : self . _DH298 = self . _calculate_DH298_coal ( )
9753	def experiment ( ctx , project , experiment ) : # pylint:disable=redefined-outer-name ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'experiment' ] = experiment
5257	def block_to_fork ( block_number ) : forks_by_block = { 0 : "frontier" , 1150000 : "homestead" , # 1920000 Dao 2463000 : "tangerine_whistle" , 2675000 : "spurious_dragon" , 4370000 : "byzantium" , #7280000: "constantinople", # Same Block as petersburg, commented to avoid conflicts 7280000 : "petersburg" , 9999999 : "serenity" # to be replaced after Serenity launch } fork_names = list ( forks_by_block . values ( ) ) fork_blocks = list ( forks_by_block . keys ( ) ) return fork_names [ bisect ( fork_blocks , block_number ) - 1 ]
2562	def recv_task_request_from_workers ( self ) : info = MPI . Status ( ) comm . recv ( source = MPI . ANY_SOURCE , tag = TASK_REQUEST_TAG , status = info ) worker_rank = info . Get_source ( ) logger . info ( "Received task request from worker:{}" . format ( worker_rank ) ) return worker_rank
489	def close ( self ) : self . _logger . info ( "Closing" ) if self . _conn is not None : self . _conn . close ( ) self . _conn = None else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
13135	def autocomplete ( query , country = None , hurricanes = False , cities = True , timeout = 5 ) : data = { } data [ 'query' ] = quote ( query ) data [ 'country' ] = country or '' data [ 'hurricanes' ] = 1 if hurricanes else 0 data [ 'cities' ] = 1 if cities else 0 data [ 'format' ] = 'JSON' r = requests . get ( AUTOCOMPLETE_URL . format ( * * data ) , timeout = timeout ) results = json . loads ( r . content ) [ 'RESULTS' ] return results
9545	def add_value_predicate ( self , field_name , value_predicate , code = VALUE_PREDICATE_FALSE , message = MESSAGES [ VALUE_PREDICATE_FALSE ] , modulus = 1 ) : assert field_name in self . _field_names , 'unexpected field name: %s' % field_name assert callable ( value_predicate ) , 'value predicate must be a callable function' t = field_name , value_predicate , code , message , modulus self . _value_predicates . append ( t )
12103	def summary ( self ) : print ( "Type: %s" % self . __class__ . __name__ ) print ( "Batch Name: %r" % self . batch_name ) if self . tag : print ( "Tag: %s" % self . tag ) print ( "Root directory: %r" % self . get_root_directory ( ) ) print ( "Maximum concurrency: %s" % self . max_concurrency ) if self . description : print ( "Description: %s" % self . description )
7916	def get_arg_parser ( cls , settings = None , option_prefix = u'--' , add_help = False ) : # pylint: disable-msg=R0914,R0912 parser = argparse . ArgumentParser ( add_help = add_help , prefix_chars = option_prefix [ 0 ] ) if settings is None : settings = cls . list_all ( basic = True ) if sys . version_info . major < 3 : # pylint: disable-msg=W0404 from locale import getpreferredencoding encoding = getpreferredencoding ( ) def decode_string_option ( value ) : """Decode a string option.""" return value . decode ( encoding ) for name in settings : if name not in cls . _defs : logger . debug ( "get_arg_parser: ignoring unknown option {0}" . format ( name ) ) return setting = cls . _defs [ name ] if not setting . cmdline_help : logger . debug ( "get_arg_parser: option {0} has no cmdline" . format ( name ) ) return if sys . version_info . major < 3 : name = name . encode ( encoding , "replace" ) option = option_prefix + name . replace ( "_" , "-" ) dest = "pyxmpp2_" + name if setting . validator : opt_type = setting . validator elif setting . type is unicode and sys . version_info . major < 3 : opt_type = decode_string_option else : opt_type = setting . type if setting . default_d : default_s = setting . default_d if sys . version_info . major < 3 : default_s = default_s . encode ( encoding , "replace" ) elif setting . default is not None : default_s = repr ( setting . default ) else : default_s = None opt_help = setting . cmdline_help if sys . version_info . major < 3 : opt_help = opt_help . encode ( encoding , "replace" ) if default_s : opt_help += " (Default: {0})" . format ( default_s ) if opt_type is bool : opt_action = _YesNoAction else : opt_action = "store" parser . add_argument ( option , action = opt_action , default = setting . default , type = opt_type , help = opt_help , metavar = name . upper ( ) , dest = dest ) return parser
2998	def marketYesterdayDF ( token = '' , version = '' ) : x = marketYesterday ( token , version ) data = [ ] for key in x : data . append ( x [ key ] ) data [ - 1 ] [ 'symbol' ] = key df = pd . DataFrame ( data ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
13291	def get_variables_by_attributes ( self , * * kwargs ) : vs = [ ] has_value_flag = False for vname in self . variables : var = self . variables [ vname ] for k , v in kwargs . items ( ) : if callable ( v ) : has_value_flag = v ( getattr ( var , k , None ) ) if has_value_flag is False : break elif hasattr ( var , k ) and getattr ( var , k ) == v : has_value_flag = True else : has_value_flag = False break if has_value_flag is True : vs . append ( self . variables [ vname ] ) return vs
10169	def register_receivers ( app , config ) : for event_name , event_config in config . items ( ) : event_builders = [ obj_or_import_string ( func ) for func in event_config . get ( 'event_builders' , [ ] ) ] signal = obj_or_import_string ( event_config [ 'signal' ] ) signal . connect ( EventEmmiter ( event_name , event_builders ) , sender = app , weak = False )
5882	def nodes_to_check ( self , docs ) : nodes_to_check = [ ] for doc in docs : for tag in [ 'p' , 'pre' , 'td' ] : items = self . parser . getElementsByTag ( doc , tag = tag ) nodes_to_check += items return nodes_to_check
289	def plot_rolling_returns ( returns , factor_returns = None , live_start_date = None , logy = False , cone_std = None , legend_loc = 'best' , volatility_match = False , cone_function = timeseries . forecast_cone_bootstrap , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Cumulative returns' ) ax . set_yscale ( 'log' if logy else 'linear' ) if volatility_match and factor_returns is None : raise ValueError ( 'volatility_match requires passing of ' 'factor_returns.' ) elif volatility_match and factor_returns is not None : bmark_vol = factor_returns . loc [ returns . index ] . std ( ) returns = ( returns / returns . std ( ) ) * bmark_vol cum_rets = ep . cum_returns ( returns , 1.0 ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) if factor_returns is not None : cum_factor_returns = ep . cum_returns ( factor_returns [ cum_rets . index ] , 1.0 ) cum_factor_returns . plot ( lw = 2 , color = 'gray' , label = factor_returns . name , alpha = 0.60 , ax = ax , * * kwargs ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_cum_returns = cum_rets . loc [ cum_rets . index < live_start_date ] oos_cum_returns = cum_rets . loc [ cum_rets . index >= live_start_date ] else : is_cum_returns = cum_rets oos_cum_returns = pd . Series ( [ ] ) is_cum_returns . plot ( lw = 3 , color = 'forestgreen' , alpha = 0.6 , label = 'Backtest' , ax = ax , * * kwargs ) if len ( oos_cum_returns ) > 0 : oos_cum_returns . plot ( lw = 4 , color = 'red' , alpha = 0.6 , label = 'Live' , ax = ax , * * kwargs ) if cone_std is not None : if isinstance ( cone_std , ( float , int ) ) : cone_std = [ cone_std ] is_returns = returns . loc [ returns . index < live_start_date ] cone_bounds = cone_function ( is_returns , len ( oos_cum_returns ) , cone_std = cone_std , starting_value = is_cum_returns [ - 1 ] ) cone_bounds = cone_bounds . set_index ( oos_cum_returns . index ) for std in cone_std : ax . fill_between ( cone_bounds . index , cone_bounds [ float ( std ) ] , cone_bounds [ float ( - std ) ] , color = 'steelblue' , alpha = 0.5 ) if legend_loc is not None : ax . legend ( loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . axhline ( 1.0 , linestyle = '--' , color = 'black' , lw = 2 ) return ax
9616	def is_displayed ( target ) : is_displayed = getattr ( target , 'is_displayed' , None ) if not is_displayed or not callable ( is_displayed ) : raise TypeError ( 'Target has no attribute \'is_displayed\' or not callable' ) if not is_displayed ( ) : raise WebDriverException ( 'element not visible' )
4841	def get_program_by_uuid ( self , program_uuid ) : return self . _load_data ( self . PROGRAMS_ENDPOINT , resource_id = program_uuid , default = None )
10005	def clear_descendants ( self , source , clear_source = True ) : removed = self . cellgraph . clear_descendants ( source , clear_source ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
11865	def pointwise_product ( self , other , bn ) : vars = list ( set ( self . vars ) | set ( other . vars ) ) cpt = dict ( ( event_values ( e , vars ) , self . p ( e ) * other . p ( e ) ) for e in all_events ( vars , bn , { } ) ) return Factor ( vars , cpt )
3842	async def sync_recent_conversations ( self , sync_recent_conversations_request ) : response = hangouts_pb2 . SyncRecentConversationsResponse ( ) await self . _pb_request ( 'conversations/syncrecentconversations' , sync_recent_conversations_request , response ) return response
703	def _getStreamDef ( self , modelDescription ) : #-------------------------------------------------------------------------- # Generate the string containing the aggregation settings. aggregationPeriod = { 'days' : 0 , 'hours' : 0 , 'microseconds' : 0 , 'milliseconds' : 0 , 'minutes' : 0 , 'months' : 0 , 'seconds' : 0 , 'weeks' : 0 , 'years' : 0 , } # Honor any overrides provided in the stream definition aggFunctionsDict = { } if 'aggregation' in modelDescription [ 'streamDef' ] : for key in aggregationPeriod . keys ( ) : if key in modelDescription [ 'streamDef' ] [ 'aggregation' ] : aggregationPeriod [ key ] = modelDescription [ 'streamDef' ] [ 'aggregation' ] [ key ] if 'fields' in modelDescription [ 'streamDef' ] [ 'aggregation' ] : for ( fieldName , func ) in modelDescription [ 'streamDef' ] [ 'aggregation' ] [ 'fields' ] : aggFunctionsDict [ fieldName ] = str ( func ) # Do we have any aggregation at all? hasAggregation = False for v in aggregationPeriod . values ( ) : if v != 0 : hasAggregation = True break # Convert the aggFunctionsDict to a list aggFunctionList = aggFunctionsDict . items ( ) aggregationInfo = dict ( aggregationPeriod ) aggregationInfo [ 'fields' ] = aggFunctionList streamDef = copy . deepcopy ( modelDescription [ 'streamDef' ] ) streamDef [ 'aggregation' ] = copy . deepcopy ( aggregationInfo ) return streamDef
8758	def get_subnet ( context , id , fields = None ) : LOG . info ( "get_subnet %s for tenant %s with fields %s" % ( id , context . tenant_id , fields ) ) subnet = db_api . subnet_find ( context = context , limit = None , page_reverse = False , sorts = [ 'id' ] , marker_obj = None , fields = None , id = id , join_dns = True , join_routes = True , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) cache = subnet . get ( "_allocation_pool_cache" ) if not cache : new_cache = subnet . allocation_pools db_api . subnet_update_set_alloc_pool_cache ( context , subnet , new_cache ) return v . _make_subnet_dict ( subnet )
11066	def add_user_to_allow ( self , name , user ) : # Clear user from both allow and deny before adding if not self . remove_user_from_acl ( name , user ) : return False if name not in self . _acl : return False self . _acl [ name ] [ 'allow' ] . append ( user ) return True
299	def plot_slippage_sweep ( returns , positions , transactions , slippage_params = ( 3 , 8 , 10 , 12 , 15 , 20 , 50 ) , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) slippage_sweep = pd . DataFrame ( ) for bps in slippage_params : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) label = str ( bps ) + " bps" slippage_sweep [ label ] = ep . cum_returns ( adj_returns , 1 ) slippage_sweep . plot ( alpha = 1.0 , lw = 0.5 , ax = ax ) ax . set_title ( 'Cumulative returns given additional per-dollar slippage' ) ax . set_ylabel ( '' ) ax . legend ( loc = 'center left' , frameon = True , framealpha = 0.5 ) return ax
135	def change_first_point_by_coords ( self , x , y , max_distance = 1e-4 , raise_if_too_far_away = True ) : if len ( self . exterior ) == 0 : raise Exception ( "Cannot reorder polygon points, because it contains no points." ) closest_idx , closest_dist = self . find_closest_point_index ( x = x , y = y , return_distance = True ) if max_distance is not None and closest_dist > max_distance : if not raise_if_too_far_away : return self . deepcopy ( ) closest_point = self . exterior [ closest_idx , : ] raise Exception ( "Closest found point (%.9f, %.9f) exceeds max_distance of %.9f exceeded" % ( closest_point [ 0 ] , closest_point [ 1 ] , closest_dist ) ) return self . change_first_point_by_index ( closest_idx )
3539	def hubspot ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return HubSpotNode ( )
1091	def encode_basestring ( s ) : def replace ( match ) : return ESCAPE_DCT [ match . group ( 0 ) ] return '"' + ESCAPE . sub ( replace , s ) + '"'
6049	def set_defaults ( key ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( phase , new_value ) : new_value = new_value or [ ] for item in new_value : # noinspection PyTypeChecker galaxy = new_value [ item ] if isinstance ( item , str ) else item galaxy . redshift = galaxy . redshift or conf . instance . general . get ( "redshift" , key , float ) return func ( phase , new_value ) return wrapper return decorator
11382	def do_url_scheme ( parser , token ) : args = token . split_contents ( ) if len ( args ) != 1 : raise template . TemplateSyntaxError ( '%s takes no parameters.' % args [ 0 ] ) return OEmbedURLSchemeNode ( )
6591	def receive ( self ) : ret = [ ] # a list of (pkgid, result) while True : if self . runid_pkgidx_map : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret . extend ( self . _collect_all_finished_pkgidx_result_pairs ( ) ) if not self . runid_pkgidx_map : break time . sleep ( self . sleep ) ret = sorted ( ret , key = itemgetter ( 0 ) ) return ret
3360	def elements ( self ) : tmp_formula = self . formula if tmp_formula is None : return { } # necessary for some old pickles which use the deprecated # Formula class tmp_formula = str ( self . formula ) # commonly occurring characters in incorrectly constructed formulas if "*" in tmp_formula : warn ( "invalid character '*' found in formula '%s'" % self . formula ) tmp_formula = tmp_formula . replace ( "*" , "" ) if "(" in tmp_formula or ")" in tmp_formula : warn ( "invalid formula (has parenthesis) in '%s'" % self . formula ) return None composition = { } parsed = element_re . findall ( tmp_formula ) for ( element , count ) in parsed : if count == '' : count = 1 else : try : count = float ( count ) int_count = int ( count ) if count == int_count : count = int_count else : warn ( "%s is not an integer (in formula %s)" % ( count , self . formula ) ) except ValueError : warn ( "failed to parse %s (in formula %s)" % ( count , self . formula ) ) return None if element in composition : composition [ element ] += count else : composition [ element ] = count return composition
2105	def version ( ) : # Print out the current version of Tower CLI. click . echo ( 'Tower CLI %s' % __version__ ) # Print out the current API version of the current code base. click . echo ( 'API %s' % CUR_API_VERSION ) # Attempt to connect to the Ansible Tower server. # If we succeed, print a version; if not, generate a failure. try : r = client . get ( '/config/' ) except RequestException as ex : raise exc . TowerCLIError ( 'Could not connect to Ansible Tower.\n%s' % six . text_type ( ex ) ) config = r . json ( ) license = config . get ( 'license_info' , { } ) . get ( 'license_type' , 'open' ) if license == 'open' : server_type = 'AWX' else : server_type = 'Ansible Tower' click . echo ( '%s %s' % ( server_type , config [ 'version' ] ) ) # Print out Ansible version of server click . echo ( 'Ansible %s' % config [ 'ansible_version' ] )
9288	def _connect ( self ) : self . logger . info ( "Attempting connection to %s:%s" , self . server [ 0 ] , self . server [ 1 ] ) try : self . _open_socket ( ) peer = self . sock . getpeername ( ) self . logger . info ( "Connected to %s" , str ( peer ) ) # 5 second timeout to receive server banner self . sock . setblocking ( 1 ) self . sock . settimeout ( 5 ) self . sock . setsockopt ( socket . SOL_SOCKET , socket . SO_KEEPALIVE , 1 ) banner = self . sock . recv ( 512 ) if is_py3 : banner = banner . decode ( 'latin-1' ) if banner [ 0 ] == "#" : self . logger . debug ( "Banner: %s" , banner . rstrip ( ) ) else : raise ConnectionError ( "invalid banner from server" ) except ConnectionError as e : self . logger . error ( str ( e ) ) self . close ( ) raise except ( socket . error , socket . timeout ) as e : self . close ( ) self . logger . error ( "Socket error: %s" % str ( e ) ) if str ( e ) == "timed out" : raise ConnectionError ( "no banner from server" ) else : raise ConnectionError ( e ) self . _connected = True
7624	def pattern_to_mireval ( ann ) : # It's easier to work with dictionaries, since we can't assume # sequential pattern or occurrence identifiers patterns = defaultdict ( lambda : defaultdict ( list ) ) # Iterate over the data in interval-value format for time , observation in zip ( * ann . to_event_values ( ) ) : pattern_id = observation [ 'pattern_id' ] occurrence_id = observation [ 'occurrence_id' ] obs = ( time , observation [ 'midi_pitch' ] ) # Push this note observation into the correct pattern/occurrence patterns [ pattern_id ] [ occurrence_id ] . append ( obs ) # Convert to list-list-tuple format for mir_eval return [ list ( _ . values ( ) ) for _ in six . itervalues ( patterns ) ]
4060	def item_template ( self , itemtype ) : # if we have a template and it hasn't been updated since we stored it template_name = "item_template_" + itemtype query_string = "/items/new?itemType={i}" . format ( i = itemtype ) if self . templates . get ( template_name ) and not self . _updated ( query_string , self . templates [ template_name ] , template_name ) : return copy . deepcopy ( self . templates [ template_name ] [ "tmplt" ] ) # otherwise perform a normal request and cache the response retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , template_name )
10564	def get_supported_filepaths ( filepaths , supported_extensions , max_depth = float ( 'inf' ) ) : supported_filepaths = [ ] for path in filepaths : if os . name == 'nt' and CYGPATH_RE . match ( path ) : path = convert_cygwin_path ( path ) if os . path . isdir ( path ) : for root , __ , files in walk_depth ( path , max_depth ) : for f in files : if f . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( os . path . join ( root , f ) ) elif os . path . isfile ( path ) and path . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( path ) return supported_filepaths
10520	def oneup ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarvertical ( window_name , object_name ) : raise LdtpServerException ( 'Object not vertical scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 minValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue <= 0 : raise LdtpServerException ( 'Minimum limit reached' ) object_handle . AXValue -= minValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to decrease scrollbar' )
86	def is_single_float ( val ) : return isinstance ( val , numbers . Real ) and not is_single_integer ( val ) and not isinstance ( val , bool )
10970	def get_group_name ( id_group ) : group = Group . query . get ( id_group ) if group is not None : return group . name
752	def setResultsPerChoice ( self , resultsPerChoice ) : # Keep track of the results obtained for each choice. self . _resultsPerChoice = [ [ ] ] * len ( self . choices ) for ( choiceValue , values ) in resultsPerChoice : choiceIndex = self . choices . index ( choiceValue ) self . _resultsPerChoice [ choiceIndex ] = list ( values )
3565	def write_value ( self , value , write_type = 0 ) : data = NSData . dataWithBytes_length_ ( value , len ( value ) ) self . _device . _peripheral . writeValue_forCharacteristic_type_ ( data , self . _characteristic , write_type )
2582	def load_checkpoints ( self , checkpointDirs ) : self . memo_lookup_table = None if not checkpointDirs : return { } if type ( checkpointDirs ) is not list : raise BadCheckpoint ( "checkpointDirs expects a list of checkpoints" ) return self . _load_checkpoints ( checkpointDirs )
10681	def H ( self , T ) : result = self . DHref for Tmax in sorted ( [ float ( TT ) for TT in self . _Cp_records . keys ( ) ] ) : result += self . _Cp_records [ str ( Tmax ) ] . H ( T ) if T <= Tmax : return result + self . H_mag ( T ) # Extrapolate beyond the upper limit by using a constant heat capacity. Tmax = max ( [ float ( TT ) for TT in self . _Cp_records . keys ( ) ] ) result += self . Cp ( Tmax ) * ( T - Tmax ) return result + self . H_mag ( T )
3898	def compile_protofile ( proto_file_path ) : out_file = tempfile . mkstemp ( ) [ 1 ] try : subprocess . check_output ( [ 'protoc' , '--include_source_info' , '--descriptor_set_out' , out_file , proto_file_path ] ) except subprocess . CalledProcessError as e : sys . exit ( 'protoc returned status {}' . format ( e . returncode ) ) return out_file
2868	def get_platform_gpio ( * * keywords ) : plat = Platform . platform_detect ( ) if plat == Platform . RASPBERRY_PI : import RPi . GPIO return RPiGPIOAdapter ( RPi . GPIO , * * keywords ) elif plat == Platform . BEAGLEBONE_BLACK : import Adafruit_BBIO . GPIO return AdafruitBBIOAdapter ( Adafruit_BBIO . GPIO , * * keywords ) elif plat == Platform . MINNOWBOARD : import mraa return AdafruitMinnowAdapter ( mraa , * * keywords ) elif plat == Platform . JETSON_NANO : import Jetson . GPIO return RPiGPIOAdapter ( Jetson . GPIO , * * keywords ) elif plat == Platform . UNKNOWN : raise RuntimeError ( 'Could not determine platform.' )
4853	def _get_transmissions ( self ) : # pylint: disable=invalid-name ContentMetadataItemTransmission = apps . get_model ( 'integrated_channel' , 'ContentMetadataItemTransmission' ) return ContentMetadataItemTransmission . objects . filter ( enterprise_customer = self . enterprise_configuration . enterprise_customer , integrated_channel_code = self . enterprise_configuration . channel_code ( ) )
11732	def registerGoodClass ( self , class_ ) : # Class itself added to "good" list self . _valid_classes . append ( class_ ) # Recurse into any inner classes for name , cls in class_members ( class_ ) : if self . isValidClass ( cls ) : self . registerGoodClass ( cls )
5304	def sanitize_color_palette ( colorpalette ) : new_palette = { } def __make_valid_color_name ( name ) : """ Convert the given name into a valid colorname """ if len ( name ) == 1 : name = name [ 0 ] return name [ : 1 ] . lower ( ) + name [ 1 : ] return name [ 0 ] . lower ( ) + '' . join ( word . capitalize ( ) for word in name [ 1 : ] ) for key , value in colorpalette . items ( ) : if isinstance ( value , str ) : # we assume it's a hex RGB value value = utils . hex_to_rgb ( value ) new_palette [ __make_valid_color_name ( key . split ( ) ) ] = value return new_palette
5792	def _cert_callback ( callback , der_cert , reason ) : if not callback : return callback ( x509 . Certificate . load ( der_cert ) , reason )
685	def getTotalw ( self ) : w = sum ( [ field . w for field in self . fields ] ) return w
6148	def unique_cpx_roots ( rlist , tol = 0.001 ) : uniq = [ rlist [ 0 ] ] mult = [ 1 ] for k in range ( 1 , len ( rlist ) ) : N_uniq = len ( uniq ) for m in range ( N_uniq ) : if abs ( rlist [ k ] - uniq [ m ] ) <= tol : mult [ m ] += 1 uniq [ m ] = ( uniq [ m ] * ( mult [ m ] - 1 ) + rlist [ k ] ) / float ( mult [ m ] ) break uniq = np . hstack ( ( uniq , rlist [ k ] ) ) mult = np . hstack ( ( mult , [ 1 ] ) ) return np . array ( uniq ) , np . array ( mult )
9086	def _sort ( self , concepts , sort = None , language = 'any' , reverse = False ) : sorted = copy . copy ( concepts ) if sort : sorted . sort ( key = methodcaller ( '_sortkey' , sort , language ) , reverse = reverse ) return sorted
7921	def __prepare_domain ( data ) : # pylint: disable=R0912 if not data : raise JIDError ( "Domain must be given" ) data = unicode ( data ) if not data : raise JIDError ( "Domain must be given" ) if u'[' in data : if data [ 0 ] == u'[' and data [ - 1 ] == u']' : try : addr = _validate_ip_address ( socket . AF_INET6 , data [ 1 : - 1 ] ) return "[{0}]" . format ( addr ) except ValueError , err : logger . debug ( "ValueError: {0}" . format ( err ) ) raise JIDError ( u"Invalid IPv6 literal in JID domainpart" ) else : raise JIDError ( u"Invalid use of '[' or ']' in JID domainpart" ) elif data [ 0 ] . isdigit ( ) and data [ - 1 ] . isdigit ( ) : try : addr = _validate_ip_address ( socket . AF_INET , data ) except ValueError , err : logger . debug ( "ValueError: {0}" . format ( err ) ) data = UNICODE_DOT_RE . sub ( u"." , data ) data = data . rstrip ( u"." ) labels = data . split ( u"." ) try : labels = [ idna . nameprep ( label ) for label in labels ] except UnicodeError : raise JIDError ( u"Domain name invalid" ) for label in labels : if not STD3_LABEL_RE . match ( label ) : raise JIDError ( u"Domain name invalid" ) try : idna . ToASCII ( label ) except UnicodeError : raise JIDError ( u"Domain name invalid" ) domain = u"." . join ( labels ) if len ( domain . encode ( "utf-8" ) ) > 1023 : raise JIDError ( u"Domain name too long" ) return domain
7752	def process_presence ( self , stanza ) : stanza_type = stanza . stanza_type return self . __try_handlers ( self . _presence_handlers , stanza , stanza_type )
2340	def GNN_instance ( x , idx = 0 , device = None , nh = 20 , * * kwargs ) : device = SETTINGS . get_default ( device = device ) xy = scale ( x ) . astype ( 'float32' ) inputx = th . FloatTensor ( xy [ : , [ 0 ] ] ) . to ( device ) target = th . FloatTensor ( xy [ : , [ 1 ] ] ) . to ( device ) GNNXY = GNN_model ( x . shape [ 0 ] , device = device , nh = nh ) . to ( device ) GNNYX = GNN_model ( x . shape [ 0 ] , device = device , nh = nh ) . to ( device ) GNNXY . reset_parameters ( ) GNNYX . reset_parameters ( ) XY = GNNXY . run ( inputx , target , * * kwargs ) YX = GNNYX . run ( target , inputx , * * kwargs ) return [ XY , YX ]
11207	def gettz_db_metadata ( ) : warnings . warn ( "zoneinfo.gettz_db_metadata() will be removed in future " "versions, to use the dateutil-provided zoneinfo files, " "ZoneInfoFile object and query the 'metadata' attribute " "instead. See the documentation for details." , DeprecationWarning ) if len ( _CLASS_ZONE_INSTANCE ) == 0 : _CLASS_ZONE_INSTANCE . append ( ZoneInfoFile ( getzoneinfofile_stream ( ) ) ) return _CLASS_ZONE_INSTANCE [ 0 ] . metadata
6258	def update ( self , aspect_ratio = None , fov = None , near = None , far = None ) : self . aspect_ratio = aspect_ratio or self . aspect_ratio self . fov = fov or self . fov self . near = near or self . near self . far = far or self . far self . matrix = Matrix44 . perspective_projection ( self . fov , self . aspect_ratio , self . near , self . far )
6435	def sim_eudex ( src , tar , weights = 'exponential' , max_length = 8 ) : return Eudex ( ) . sim ( src , tar , weights , max_length )
9496	def parse_module ( path , excludes = None ) : file = path / MODULE_FILENAME if not file . exists ( ) : raise MissingFile ( file ) id = _parse_document_id ( etree . parse ( file . open ( ) ) ) excludes = excludes or [ ] excludes . extend ( [ lambda filepath : filepath . name == MODULE_FILENAME , ] ) resources_paths = _find_resources ( path , excludes = excludes ) resources = tuple ( _resource_from_path ( res ) for res in resources_paths ) return Module ( id , file , resources )
10759	def from_curvilinear ( cls , x , y , z , formatter = numpy_formatter ) : return cls ( x , y , z , formatter )
9828	def write ( self , file , optstring = "" , quote = False ) : classid = str ( self . id ) if quote : classid = '"' + classid + '"' # Only use a *single* space between tokens; both chimera's and pymol's DX parser # does not properly implement the OpenDX specs and produces garbage with multiple # spaces. (Chimera 1.4.1, PyMOL 1.3) file . write ( 'object ' + classid + ' class ' + str ( self . name ) + ' ' + optstring + '\n' )
4482	def storages ( self ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : yield Storage ( store , self . session )
3328	def get_lock ( self , token , key = None ) : assert key in ( None , "type" , "scope" , "depth" , "owner" , "root" , "timeout" , "principal" , "token" , ) lock = self . storage . get ( token ) if key is None or lock is None : return lock return lock [ key ]
5587	def extract_subset ( self , input_data_tiles = None , out_tile = None ) : if self . METADATA [ "data_type" ] == "raster" : mosaic = create_mosaic ( input_data_tiles ) return extract_from_array ( in_raster = prepare_array ( mosaic . data , nodata = self . nodata , dtype = self . output_params [ "dtype" ] ) , in_affine = mosaic . affine , out_tile = out_tile ) elif self . METADATA [ "data_type" ] == "vector" : return [ feature for feature in list ( chain . from_iterable ( [ features for _ , features in input_data_tiles ] ) ) if shape ( feature [ "geometry" ] ) . intersects ( out_tile . bbox ) ]
10310	def safe_add_edge ( graph , u , v , key , attr_dict , * * attr ) : if key < 0 : graph . add_edge ( u , v , key = key , attr_dict = attr_dict , * * attr ) else : graph . add_edge ( u , v , attr_dict = attr_dict , * * attr )
8174	def goal ( self , x , y , z , d = 50.0 ) : return ( x - self . x ) / d , ( y - self . y ) / d , ( z - self . z ) / d
3069	def wrap_http_for_jwt_access ( credentials , http ) : orig_request_method = http . request wrap_http_for_auth ( credentials , http ) # The new value of ``http.request`` set by ``wrap_http_for_auth``. authenticated_request_method = http . request # The closure that will replace 'httplib2.Http.request'. def new_request ( uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : if 'aud' in credentials . _kwargs : # Preemptively refresh token, this is not done for OAuth2 if ( credentials . access_token is None or credentials . access_token_expired ) : credentials . refresh ( None ) return request ( authenticated_request_method , uri , method , body , headers , redirections , connection_type ) else : # If we don't have an 'aud' (audience) claim, # create a 1-time token with the uri root as the audience headers = _initialize_headers ( headers ) _apply_user_agent ( headers , credentials . user_agent ) uri_root = uri . split ( '?' , 1 ) [ 0 ] token , unused_expiry = credentials . _create_token ( { 'aud' : uri_root } ) headers [ 'Authorization' ] = 'Bearer ' + token return request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) # Replace the request method with our own closure. http . request = new_request # Set credentials as a property of the request method. http . request . credentials = credentials
4101	def mdl_eigen ( s , N ) : import numpy as np kmdl = [ ] n = len ( s ) for k in range ( 0 , n - 1 ) : ak = 1. / ( n - k ) * np . sum ( s [ k + 1 : ] ) gk = np . prod ( s [ k + 1 : ] ** ( 1. / ( n - k ) ) ) kmdl . append ( - ( n - k ) * N * np . log ( gk / ak ) + 0.5 * k * ( 2. * n - k ) * np . log ( N ) ) return kmdl
10960	def scramble_positions ( p , delete_frac = 0.1 ) : probs = [ 1 - delete_frac , delete_frac ] m = np . random . choice ( [ True , False ] , p . shape [ 0 ] , p = probs ) jumble = np . random . randn ( m . sum ( ) , 3 ) return p [ m ] + jumble
9305	def handle_date_mismatch ( self , req ) : req_datetime = self . get_request_date ( req ) new_key_date = req_datetime . strftime ( '%Y%m%d' ) self . regenerate_signing_key ( date = new_key_date )
1692	def CheckCompletedBlocks ( self , filename , error ) : # Note: This test can result in false positives if #ifdef constructs # get in the way of brace matching. See the testBuildClass test in # cpplint_unittest.py for an example of this. for obj in self . stack : if isinstance ( obj , _ClassInfo ) : error ( filename , obj . starting_linenum , 'build/class' , 5 , 'Failed to find complete declaration of class %s' % obj . name ) elif isinstance ( obj , _NamespaceInfo ) : error ( filename , obj . starting_linenum , 'build/namespaces' , 5 , 'Failed to find complete declaration of namespace %s' % obj . name )
5880	def get_siblings_content ( self , current_sibling , baselinescore_siblings_para ) : if current_sibling . tag == 'p' and self . parser . getText ( current_sibling ) : tmp = current_sibling if tmp . tail : tmp = deepcopy ( tmp ) tmp . tail = '' return [ tmp ] else : potential_paragraphs = self . parser . getElementsByTag ( current_sibling , tag = 'p' ) if potential_paragraphs is None : return None paragraphs = list ( ) for first_paragraph in potential_paragraphs : text = self . parser . getText ( first_paragraph ) if text : # no len(text) > 0 word_stats = self . stopwords_class ( language = self . get_language ( ) ) . get_stopword_count ( text ) paragraph_score = word_stats . get_stopword_count ( ) sibling_baseline_score = float ( .30 ) high_link_density = self . is_highlink_density ( first_paragraph ) score = float ( baselinescore_siblings_para * sibling_baseline_score ) if score < paragraph_score and not high_link_density : para = self . parser . createElement ( tag = 'p' , text = text , tail = None ) paragraphs . append ( para ) return paragraphs
10547	def find_taskruns ( project_id , * * kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'taskrun' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ TaskRun ( taskrun ) for taskrun in res ] else : return res except : # pragma: no cover raise
7798	def sasl_mechanism ( name , secure , preference = 50 ) : # pylint: disable-msg=W0212 def decorator ( klass ) : """The decorator.""" klass . _pyxmpp_sasl_secure = secure klass . _pyxmpp_sasl_preference = preference if issubclass ( klass , ClientAuthenticator ) : _register_client_authenticator ( klass , name ) elif issubclass ( klass , ServerAuthenticator ) : _register_server_authenticator ( klass , name ) else : raise TypeError ( "Not a ClientAuthenticator" " or ServerAuthenticator class" ) return klass return decorator
5831	def update ( self , id , configuration , name , description ) : data = { "configuration" : configuration , "name" : name , "description" : description } failure_message = "Dataview creation failed" self . _patch_json ( 'v1/data_views/' + id , data , failure_message = failure_message )
13080	def register ( self ) : if self . app is not None : if not self . blueprint : self . blueprint = self . create_blueprint ( ) self . app . register_blueprint ( self . blueprint ) if self . cache is None : # We register a fake cache extension. setattr ( self . app . jinja_env , "_fake_cache_extension" , self ) self . app . jinja_env . add_extension ( FakeCacheExtension ) return self . blueprint return None
5429	def _name_for_command ( command ) : lines = command . splitlines ( ) for line in lines : line = line . strip ( ) if line and not line . startswith ( '#' ) and line != '\\' : return os . path . basename ( re . split ( r'\s' , line ) [ 0 ] ) return 'command'
10148	def from_schema_mapping ( self , schema_mapping ) : responses = { } for status , response_schema in schema_mapping . items ( ) : response = { } if response_schema . description : response [ 'description' ] = response_schema . description else : raise CorniceSwaggerException ( 'Responses must have a description.' ) for field_schema in response_schema . children : location = field_schema . name if location == 'body' : title = field_schema . __class__ . __name__ if title == 'body' : title = response_schema . __class__ . __name__ + 'Body' field_schema . title = title response [ 'schema' ] = self . definitions . from_schema ( field_schema ) elif location in ( 'header' , 'headers' ) : header_schema = self . type_converter ( field_schema ) headers = header_schema . get ( 'properties' ) if headers : # Response headers doesn't accept titles for header in headers . values ( ) : header . pop ( 'title' ) response [ 'headers' ] = headers pointer = response_schema . __class__ . __name__ if self . ref : response = self . _ref ( response , pointer ) responses [ status ] = response return responses
4863	def to_representation ( self , instance ) : request = self . context [ 'request' ] enterprise_customer = instance . enterprise_customer representation = super ( EnterpriseCustomerCatalogDetailSerializer , self ) . to_representation ( instance ) # Retrieve the EnterpriseCustomerCatalog search results from the discovery service. paginated_content = instance . get_paginated_content ( request . GET ) count = paginated_content [ 'count' ] search_results = paginated_content [ 'results' ] for item in search_results : content_type = item [ 'content_type' ] marketing_url = item . get ( 'marketing_url' ) if marketing_url : item [ 'marketing_url' ] = utils . update_query_parameters ( marketing_url , utils . get_enterprise_utm_context ( enterprise_customer ) ) # Add the Enterprise enrollment URL to each content item returned from the discovery service. if content_type == 'course' : item [ 'enrollment_url' ] = instance . get_course_enrollment_url ( item [ 'key' ] ) if content_type == 'courserun' : item [ 'enrollment_url' ] = instance . get_course_run_enrollment_url ( item [ 'key' ] ) if content_type == 'program' : item [ 'enrollment_url' ] = instance . get_program_enrollment_url ( item [ 'uuid' ] ) # Build pagination URLs previous_url = None next_url = None page = int ( request . GET . get ( 'page' , '1' ) ) request_uri = request . build_absolute_uri ( ) if paginated_content [ 'previous' ] : previous_url = utils . update_query_parameters ( request_uri , { 'page' : page - 1 } ) if paginated_content [ 'next' ] : next_url = utils . update_query_parameters ( request_uri , { 'page' : page + 1 } ) representation [ 'count' ] = count representation [ 'previous' ] = previous_url representation [ 'next' ] = next_url representation [ 'results' ] = search_results return representation
3304	def _run_paste ( app , config , mode ) : from paste import httpserver version = "WsgiDAV/{} {} Python {}" . format ( __version__ , httpserver . WSGIHandler . server_version , util . PYTHON_VERSION ) _logger . info ( "Running {}..." . format ( version ) ) # See http://pythonpaste.org/modules/httpserver.html for more options server = httpserver . serve ( app , host = config [ "host" ] , port = config [ "port" ] , server_version = version , # This option enables handling of keep-alive # and expect-100: protocol_version = "HTTP/1.1" , start_loop = False , ) if config [ "verbose" ] >= 5 : __handle_one_request = server . RequestHandlerClass . handle_one_request def handle_one_request ( self ) : __handle_one_request ( self ) if self . close_connection == 1 : _logger . debug ( "HTTP Connection : close" ) else : _logger . debug ( "HTTP Connection : continue" ) server . RequestHandlerClass . handle_one_request = handle_one_request # __handle = server.RequestHandlerClass.handle # def handle(self): # _logger.debug("open HTTP connection") # __handle(self) server . RequestHandlerClass . handle_one_request = handle_one_request host , port = server . server_address if host == "0.0.0.0" : _logger . info ( "Serving on 0.0.0.0:{} view at {}://127.0.0.1:{}" . format ( port , "http" , port ) ) else : _logger . info ( "Serving on {}://{}:{}" . format ( "http" , host , port ) ) try : server . serve_forever ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
1011	def trimSegments ( self , minPermanence = None , minNumSyns = None ) : # Fill in defaults if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold # Loop through all cells totalSegsRemoved , totalSynsRemoved = 0 , 0 for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : ( segsRemoved , synsRemoved ) = self . _trimSegmentsInCell ( colIdx = c , cellIdx = i , segList = self . cells [ c ] [ i ] , minPermanence = minPermanence , minNumSyns = minNumSyns ) totalSegsRemoved += segsRemoved totalSynsRemoved += synsRemoved # Print all cells if verbosity says to if self . verbosity >= 5 : print "Cells, all segments:" self . printCells ( predictedOnly = False ) return totalSegsRemoved , totalSynsRemoved
7212	def layers ( self ) : layers = [ self . _layer_def ( style ) for style in self . styles ] return layers
3836	async def set_conversation_notification_level ( self , set_conversation_notification_level_request ) : response = hangouts_pb2 . SetConversationNotificationLevelResponse ( ) await self . _pb_request ( 'conversations/setconversationnotificationlevel' , set_conversation_notification_level_request , response ) return response
6392	def _get_qgrams ( self , src , tar , qval = 0 , skip = 0 ) : if isinstance ( src , Counter ) and isinstance ( tar , Counter ) : return src , tar if qval > 0 : return QGrams ( src , qval , '$#' , skip ) , QGrams ( tar , qval , '$#' , skip ) return Counter ( src . strip ( ) . split ( ) ) , Counter ( tar . strip ( ) . split ( ) )
11464	def download ( self , source_file , target_folder = '' ) : current_folder = self . _ftp . pwd ( ) if not target_folder . startswith ( '/' ) : # relative path target_folder = join ( getcwd ( ) , target_folder ) folder = os . path . dirname ( source_file ) self . cd ( folder ) if folder . startswith ( "/" ) : folder = folder [ 1 : ] destination_folder = join ( target_folder , folder ) if not os . path . exists ( destination_folder ) : print ( "Creating folder" , destination_folder ) os . makedirs ( destination_folder ) source_file = os . path . basename ( source_file ) destination = join ( destination_folder , source_file ) try : with open ( destination , 'wb' ) as result : self . _ftp . retrbinary ( 'RETR %s' % ( source_file , ) , result . write ) except error_perm as e : # source_file is a folder print ( e ) remove ( join ( target_folder , source_file ) ) raise self . _ftp . cwd ( current_folder )
4122	def centerdc_2_twosided ( data ) : N = len ( data ) newpsd = np . concatenate ( ( data [ N // 2 : ] , ( cshift ( data [ 0 : N // 2 ] , - 1 ) ) ) ) return newpsd
9177	def _dissect_roles ( metadata ) : for role_key in cnxepub . ATTRIBUTED_ROLE_KEYS : for user in metadata . get ( role_key , [ ] ) : if user [ 'type' ] != 'cnx-id' : raise ValueError ( "Archive only accepts Connexions users." ) uid = parse_user_uri ( user [ 'id' ] ) yield uid , role_key raise StopIteration ( )
5271	def lcs ( self , stringIdxs = - 1 ) : if stringIdxs == - 1 or not isinstance ( stringIdxs , list ) : stringIdxs = set ( range ( len ( self . word_starts ) ) ) else : stringIdxs = set ( stringIdxs ) deepestNode = self . _find_lcs ( self . root , stringIdxs ) start = deepestNode . idx end = deepestNode . idx + deepestNode . depth return self . word [ start : end ]
5339	def __create_dashboard_menu ( self , dash_menu , kibiter_major ) : logger . info ( "Adding dashboard menu" ) if kibiter_major == "6" : menu_resource = ".kibana/doc/metadashboard" mapping_resource = ".kibana/_mapping/doc" mapping = { "dynamic" : "true" } menu = { 'metadashboard' : dash_menu } else : menu_resource = ".kibana/metadashboard/main" mapping_resource = ".kibana/_mapping/metadashboard" mapping = { "dynamic" : "true" } menu = dash_menu menu_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , menu_resource ) mapping_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , mapping_resource ) logger . debug ( "Adding mapping for metadashboard" ) res = self . grimoire_con . put ( mapping_url , data = json . dumps ( mapping ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create mapping for Kibiter menu." ) res = self . grimoire_con . post ( menu_url , data = json . dumps ( menu ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create Kibiter menu." ) logger . error ( res . json ( ) ) raise
9292	def python_value ( self , value ) : value = super ( OrderedUUIDField , self ) . python_value ( value ) u = binascii . b2a_hex ( value ) value = u [ 8 : 16 ] + u [ 4 : 8 ] + u [ 0 : 4 ] + u [ 16 : 22 ] + u [ 22 : 32 ] return UUID ( value . decode ( ) )
1144	def init ( self ) : self . length = 0 self . input = [ ] # Initial 160 bit message digest (5 times 32 bit). self . H0 = 0x67452301 self . H1 = 0xEFCDAB89 self . H2 = 0x98BADCFE self . H3 = 0x10325476 self . H4 = 0xC3D2E1F0
4594	def make_matrix_coord_map ( dx , dy , serpentine = True , offset = 0 , rotation = 0 , y_flip = False ) : result = [ ] for y in range ( dy ) : if not serpentine or y % 2 == 0 : result . append ( [ ( dx * y ) + x + offset for x in range ( dx ) ] ) else : result . append ( [ dx * ( y + 1 ) - 1 - x + offset for x in range ( dx ) ] ) result = rotate_and_flip ( result , rotation , y_flip ) return result
1247	def recv ( self , socket_ , encoding = None ) : unpacker = msgpack . Unpacker ( encoding = encoding ) # Wait for an immediate response. response = socket_ . recv ( 8 ) # get the length of the message if response == b"" : raise TensorForceError ( "No data received by socket.recv in call to method `recv` " + "(listener possibly closed)!" ) orig_len = int ( response ) received_len = 0 while True : data = socket_ . recv ( min ( orig_len - received_len , self . max_msg_len ) ) # There must be a response. if not data : raise TensorForceError ( "No data of len {} received by socket.recv in call to method `recv`!" . format ( orig_len - received_len ) ) data_len = len ( data ) received_len += data_len unpacker . feed ( data ) if received_len == orig_len : break # Get the data. for message in unpacker : sts = message . get ( "status" , message . get ( b"status" ) ) if sts : if sts == "ok" or sts == b"ok" : return message else : raise TensorForceError ( "RemoteEnvironment server error: {}" . format ( message . get ( "message" , "not specified" ) ) ) else : raise TensorForceError ( "Message without field 'status' received!" ) raise TensorForceError ( "No message encoded in data stream (data stream had len={})" . format ( orig_len ) )
8224	def _mouse_pointer_moved ( self , x , y ) : self . _namespace [ 'MOUSEX' ] = x self . _namespace [ 'MOUSEY' ] = y
12547	def abs_img ( img ) : bool_img = np . abs ( read_img ( img ) . get_data ( ) ) return bool_img . astype ( int )
5149	def _add_file ( self , tar , name , contents , mode = DEFAULT_FILE_MODE ) : byte_contents = BytesIO ( contents . encode ( 'utf8' ) ) info = tarfile . TarInfo ( name = name ) info . size = len ( contents ) # mtime must be 0 or any checksum operation # will return a different digest even when content is the same info . mtime = 0 info . type = tarfile . REGTYPE info . mode = int ( mode , 8 ) # permissions converted to decimal notation tar . addfile ( tarinfo = info , fileobj = byte_contents )
2511	def parse_package ( self , p_term ) : # Check there is a pacakge name if not ( p_term , self . spdx_namespace [ 'name' ] , None ) in self . graph : self . error = True self . logger . log ( 'Package must have a name.' ) # Create dummy package so that we may continue parsing the rest of # the package fields. self . builder . create_package ( self . doc , 'dummy_package' ) else : for _s , _p , o in self . graph . triples ( ( p_term , self . spdx_namespace [ 'name' ] , None ) ) : try : self . builder . create_package ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'Package name' ) break self . p_pkg_vinfo ( p_term , self . spdx_namespace [ 'versionInfo' ] ) self . p_pkg_fname ( p_term , self . spdx_namespace [ 'packageFileName' ] ) self . p_pkg_suppl ( p_term , self . spdx_namespace [ 'supplier' ] ) self . p_pkg_originator ( p_term , self . spdx_namespace [ 'originator' ] ) self . p_pkg_down_loc ( p_term , self . spdx_namespace [ 'downloadLocation' ] ) self . p_pkg_homepg ( p_term , self . doap_namespace [ 'homepage' ] ) self . p_pkg_chk_sum ( p_term , self . spdx_namespace [ 'checksum' ] ) self . p_pkg_src_info ( p_term , self . spdx_namespace [ 'sourceInfo' ] ) self . p_pkg_verif_code ( p_term , self . spdx_namespace [ 'packageVerificationCode' ] ) self . p_pkg_lic_conc ( p_term , self . spdx_namespace [ 'licenseConcluded' ] ) self . p_pkg_lic_decl ( p_term , self . spdx_namespace [ 'licenseDeclared' ] ) self . p_pkg_lics_info_from_files ( p_term , self . spdx_namespace [ 'licenseInfoFromFiles' ] ) self . p_pkg_comments_on_lics ( p_term , self . spdx_namespace [ 'licenseComments' ] ) self . p_pkg_cr_text ( p_term , self . spdx_namespace [ 'copyrightText' ] ) self . p_pkg_summary ( p_term , self . spdx_namespace [ 'summary' ] ) self . p_pkg_descr ( p_term , self . spdx_namespace [ 'description' ] )
7597	def get_top_war_clans ( self , country_key = '' , * * params : keys ) : url = self . api . TOP + '/war/' + str ( country_key ) return self . _get_model ( url , PartialClan , * * params )
8401	def rescale_mid ( x , to = ( 0 , 1 ) , _from = None , mid = 0 ) : array_like = True try : len ( x ) except TypeError : array_like = False x = [ x ] if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) if _from is None : _from = np . array ( [ np . min ( x ) , np . max ( x ) ] ) else : _from = np . asarray ( _from ) if ( zero_range ( _from ) or zero_range ( to ) ) : out = np . repeat ( np . mean ( to ) , len ( x ) ) else : extent = 2 * np . max ( np . abs ( _from - mid ) ) out = ( x - mid ) / extent * np . diff ( to ) + np . mean ( to ) if not array_like : out = out [ 0 ] return out
10651	def get_activity ( self , name ) : return [ a for a in self . activities if a . name == name ] [ 0 ]
13524	def safe_joinall ( greenlets , timeout = None , raise_error = False ) : greenlets = list ( greenlets ) try : gevent . joinall ( greenlets , timeout = timeout , raise_error = raise_error ) except gevent . GreenletExit : [ greenlet . kill ( ) for greenlet in greenlets if not greenlet . ready ( ) ] raise return greenlets
2673	def invoke ( src , event_file = 'event.json' , config_file = 'config.yaml' , profile_name = None , verbose = False , ) : # Load and parse the config file. path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) # Set AWS_PROFILE environment variable based on `--profile` option. if profile_name : os . environ [ 'AWS_PROFILE' ] = profile_name # Load environment variables from the config file into the actual # environment. env_vars = cfg . get ( 'environment_variables' ) if env_vars : for key , value in env_vars . items ( ) : os . environ [ key ] = get_environment_variable_value ( value ) # Load and parse event file. path_to_event_file = os . path . join ( src , event_file ) event = read ( path_to_event_file , loader = json . loads ) # Tweak to allow module to import local modules try : sys . path . index ( src ) except ValueError : sys . path . append ( src ) handler = cfg . get ( 'handler' ) # Inspect the handler string (<module>.<function name>) and translate it # into a function we can execute. fn = get_callable_handler_function ( src , handler ) timeout = cfg . get ( 'timeout' ) if timeout : context = LambdaContext ( cfg . get ( 'function_name' ) , timeout ) else : context = LambdaContext ( cfg . get ( 'function_name' ) ) start = time . time ( ) results = fn ( event , context ) end = time . time ( ) print ( '{0}' . format ( results ) ) if verbose : print ( '\nexecution time: {:.8f}s\nfunction execution ' 'timeout: {:2}s' . format ( end - start , cfg . get ( 'timeout' , 15 ) ) )
8710	def __write_chunk ( self , chunk ) : log . debug ( 'writing %d bytes chunk' , len ( chunk ) ) data = BLOCK_START + chr ( len ( chunk ) ) + chunk if len ( chunk ) < 128 : padding = 128 - len ( chunk ) log . debug ( 'pad with %d characters' , padding ) data = data + ( ' ' * padding ) log . debug ( "packet size %d" , len ( data ) ) self . __write ( data ) self . _port . flush ( ) return self . __got_ack ( )
266	def format_asset ( asset ) : try : import zipline . assets except ImportError : return asset if isinstance ( asset , zipline . assets . Asset ) : return asset . symbol else : return asset
7827	def stream_element_handler ( element_name , usage_restriction = None ) : def decorator ( func ) : """The decorator""" func . _pyxmpp_stream_element_handled = element_name func . _pyxmpp_usage_restriction = usage_restriction return func return decorator
1919	def linux ( cls , path , argv = None , envp = None , entry_symbol = None , symbolic_files = None , concrete_start = '' , pure_symbolic = False , stdin_size = None , * * kwargs ) : if stdin_size is None : stdin_size = consts . stdin_size try : return cls ( _make_linux ( path , argv , envp , entry_symbol , symbolic_files , concrete_start , pure_symbolic , stdin_size ) , * * kwargs ) except elftools . common . exceptions . ELFError : raise Exception ( f'Invalid binary: {path}' )
2750	def get_all_sizes ( self ) : data = self . get_data ( "sizes/" ) sizes = list ( ) for jsoned in data [ 'sizes' ] : size = Size ( * * jsoned ) size . token = self . token sizes . append ( size ) return sizes
6472	def color_ramp ( self , size ) : color = PALETTE . get ( self . option . palette , { } ) color = color . get ( self . term . colors , None ) color_ramp = [ ] if color is not None : ratio = len ( color ) / float ( size ) for i in range ( int ( size ) ) : color_ramp . append ( self . term . color ( color [ int ( ratio * i ) ] ) ) return color_ramp
10180	def get ( self , timeout = None ) : result = None try : result = self . _result . get ( True , timeout = timeout ) except Empty : raise Timeout ( ) if isinstance ( result , Failure ) : six . reraise ( * result . exc_info ) else : return result
8235	def split_complementary ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) clr = clr . complement colors . append ( clr . rotate_ryb ( - 30 ) . lighten ( 0.1 ) ) colors . append ( clr . rotate_ryb ( 30 ) . lighten ( 0.1 ) ) return colors
12684	def query ( self , input = '' , params = { } ) : # Get and construct query parameters # Default parameters payload = { 'input' : input , 'appid' : self . appid } # Additional parameters (from params), formatted for url for key , value in params . items ( ) : # Check if value is list or tuple type (needs to be comma joined) if isinstance ( value , ( list , tuple ) ) : payload [ key ] = ',' . join ( value ) else : payload [ key ] = value # Catch any issues with connecting to Wolfram Alpha API try : r = requests . get ( "http://api.wolframalpha.com/v2/query" , params = payload ) # Raise Exception (to be returned as error) if r . status_code != 200 : raise Exception ( 'Invalid response status code: %s' % ( r . status_code ) ) if r . encoding != 'utf-8' : raise Exception ( 'Invalid encoding: %s' % ( r . encoding ) ) except Exception , e : return Result ( error = e ) return Result ( xml = r . text )
9158	def version ( ) : with io . open ( 'pgmagick/_version.py' ) as input_file : for line in input_file : if line . startswith ( '__version__' ) : return ast . parse ( line ) . body [ 0 ] . value . s
5009	def create_course_completion ( self , user_id , payload ) : url = self . enterprise_configuration . sapsf_base_url + self . global_sap_config . completion_status_api_path return self . _call_post_with_user_override ( user_id , url , payload )
1632	def CheckForBadCharacters ( filename , lines , error ) : for linenum , line in enumerate ( lines ) : if unicode_escape_decode ( '\ufffd' ) in line : error ( filename , linenum , 'readability/utf8' , 5 , 'Line contains invalid UTF-8 (or Unicode replacement character).' ) if '\0' in line : error ( filename , linenum , 'readability/nul' , 5 , 'Line contains NUL byte.' )
10728	def _handle_struct ( toks ) : subtrees = toks [ 1 : - 1 ] signature = '' . join ( s for ( _ , s ) in subtrees ) funcs = [ f for ( f , _ ) in subtrees ] def the_func ( a_list , variant = 0 ) : """ Function for generating a Struct from a list. :param a_list: the list to transform :type a_list: list or tuple :param int variant: variant index :returns: a dbus Struct of transformed values and variant level :rtype: Struct * int :raises IntoDPValueError: """ if isinstance ( a_list , dict ) : raise IntoDPValueError ( a_list , "a_list" , "must be a simple sequence, is a dict" ) if len ( a_list ) != len ( funcs ) : raise IntoDPValueError ( a_list , "a_list" , "must have exactly %u items, has %u" % ( len ( funcs ) , len ( a_list ) ) ) elements = [ f ( x ) for ( f , x ) in zip ( funcs , a_list ) ] level = 0 if elements == [ ] else max ( x for ( _ , x ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Struct ( ( x for ( x , _ ) in elements ) , signature = signature , variant_level = obj_level ) , func_level ) return ( the_func , '(' + signature + ')' )
4216	def get_credential ( self , service , username ) : # The default implementation requires a username here. if username is not None : password = self . get_password ( service , username ) if password is not None : return credentials . SimpleCredential ( username , password , ) return None
6242	def load_shader ( self , shader_type : str , path : str ) : if path : resolved_path = self . find_program ( path ) if not resolved_path : raise ValueError ( "Cannot find {} shader '{}'" . format ( shader_type , path ) ) print ( "Loading:" , path ) with open ( resolved_path , 'r' ) as fd : return fd . read ( )
5307	def rgb_to_ansi16 ( r , g , b , use_bright = False ) : ansi_b = round ( b / 255.0 ) << 2 ansi_g = round ( g / 255.0 ) << 1 ansi_r = round ( r / 255.0 ) ansi = ( 90 if use_bright else 30 ) + ( ansi_b | ansi_g | ansi_r ) return ansi
2613	def pack_apply_message ( f , args , kwargs , buffer_threshold = MAX_BYTES , item_threshold = MAX_ITEMS ) : arg_bufs = list ( chain . from_iterable ( serialize_object ( arg , buffer_threshold , item_threshold ) for arg in args ) ) kw_keys = sorted ( kwargs . keys ( ) ) kwarg_bufs = list ( chain . from_iterable ( serialize_object ( kwargs [ key ] , buffer_threshold , item_threshold ) for key in kw_keys ) ) info = dict ( nargs = len ( args ) , narg_bufs = len ( arg_bufs ) , kw_keys = kw_keys ) msg = [ pickle . dumps ( can ( f ) , PICKLE_PROTOCOL ) ] msg . append ( pickle . dumps ( info , PICKLE_PROTOCOL ) ) msg . extend ( arg_bufs ) msg . extend ( kwarg_bufs ) return msg
9216	def file ( self , filename ) : with open ( filename ) as f : self . lexer . input ( f . read ( ) ) return self
3908	def put ( self , coro ) : # Avoid logging when a coroutine is queued or executed to avoid log # spam from coroutines that are started on every keypress. assert asyncio . iscoroutine ( coro ) self . _queue . put_nowait ( coro )
11310	def get_object ( self , url , month_format = '%b' , day_format = '%d' ) : params = self . get_params ( url ) try : year = params [ self . _meta . year_part ] month = params [ self . _meta . month_part ] day = params [ self . _meta . day_part ] except KeyError : try : # named lookups failed, so try to get the date using the first # three parameters year , month , day = params [ '_0' ] , params [ '_1' ] , params [ '_2' ] except KeyError : raise OEmbedException ( 'Error extracting date from url parameters' ) try : tt = time . strptime ( '%s-%s-%s' % ( year , month , day ) , '%s-%s-%s' % ( '%Y' , month_format , day_format ) ) date = datetime . date ( * tt [ : 3 ] ) except ValueError : raise OEmbedException ( 'Error parsing date from: %s' % url ) # apply the date-specific lookups if isinstance ( self . _meta . model . _meta . get_field ( self . _meta . date_field ) , DateTimeField ) : min_date = datetime . datetime . combine ( date , datetime . time . min ) max_date = datetime . datetime . combine ( date , datetime . time . max ) query = { '%s__range' % self . _meta . date_field : ( min_date , max_date ) } else : query = { self . _meta . date_field : date } # apply the regular search lookups for key , value in self . _meta . fields_to_match . iteritems ( ) : try : query [ value ] = params [ key ] except KeyError : raise OEmbedException ( '%s was not found in the urlpattern parameters. Valid names are: %s' % ( key , ', ' . join ( params . keys ( ) ) ) ) try : obj = self . get_queryset ( ) . get ( * * query ) except self . _meta . model . DoesNotExist : raise OEmbedException ( 'Requested object not found' ) return obj
13818	def _ConvertMessage ( value , message ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : _ConvertWrapperMessage ( value , message ) elif full_name in _WKTJSONMETHODS : _WKTJSONMETHODS [ full_name ] [ 1 ] ( value , message ) else : _ConvertFieldValuePair ( value , message )
1429	def convert_args_dict_to_list ( dict_extra_args ) : list_extra_args = [ ] if 'component_parallelism' in dict_extra_args : list_extra_args += [ "--component_parallelism" , ',' . join ( dict_extra_args [ 'component_parallelism' ] ) ] if 'runtime_config' in dict_extra_args : list_extra_args += [ "--runtime_config" , ',' . join ( dict_extra_args [ 'runtime_config' ] ) ] if 'container_number' in dict_extra_args : list_extra_args += [ "--container_number" , ',' . join ( dict_extra_args [ 'container_number' ] ) ] if 'dry_run' in dict_extra_args and dict_extra_args [ 'dry_run' ] : list_extra_args += [ '--dry_run' ] if 'dry_run_format' in dict_extra_args : list_extra_args += [ '--dry_run_format' , dict_extra_args [ 'dry_run_format' ] ] return list_extra_args
2608	def _nbytes ( buf ) : if isinstance ( buf , memoryview ) : if PY3 : # py3 introduces nbytes attribute return buf . nbytes else : # compute nbytes on py2 size = buf . itemsize for dim in buf . shape : size *= dim return size else : # not a memoryview, raw bytes/ py2 buffer return len ( buf )
6614	def receive_one ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . receive_one ( )
8326	def setup ( self , parent = None , previous = None ) : self . parent = parent self . previous = previous self . next = None self . previousSibling = None self . nextSibling = None if self . parent and self . parent . contents : self . previousSibling = self . parent . contents [ - 1 ] self . previousSibling . nextSibling = self
3710	def calculate_P ( self , T , P , method ) : if method == COSTALD_COMPRESSED : Vm = self . T_dependent_property ( T ) Psat = self . Psat ( T ) if hasattr ( self . Psat , '__call__' ) else self . Psat Vm = COSTALD_compressed ( T , P , Psat , self . Tc , self . Pc , self . omega , Vm ) elif method == COOLPROP : Vm = 1. / PropsSI ( 'DMOLAR' , 'T' , T , 'P' , P , self . CASRN ) elif method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP ( T = T , P = P ) Vm = self . eos [ 0 ] . V_l elif method in self . tabular_data : Vm = self . interpolate_P ( T , P , method ) return Vm
4442	def add_suggestions ( self , * suggestions , * * kwargs ) : pipe = self . redis . pipeline ( ) for sug in suggestions : args = [ AutoCompleter . SUGADD_COMMAND , self . key , sug . string , sug . score ] if kwargs . get ( 'increment' ) : args . append ( AutoCompleter . INCR ) if sug . payload : args . append ( 'PAYLOAD' ) args . append ( sug . payload ) pipe . execute_command ( * args ) return pipe . execute ( ) [ - 1 ]
8832	def if_ ( * args ) : for i in range ( 0 , len ( args ) - 1 , 2 ) : if args [ i ] : return args [ i + 1 ] if len ( args ) % 2 : return args [ - 1 ] else : return None
12654	def convert_dcm2nii ( input_dir , output_dir , filename ) : # a few checks before doing the job if not op . exists ( input_dir ) : raise IOError ( 'Expected an existing folder in {}.' . format ( input_dir ) ) if not op . exists ( output_dir ) : raise IOError ( 'Expected an existing output folder in {}.' . format ( output_dir ) ) # create a temporary folder for dcm2nii export tmpdir = tempfile . TemporaryDirectory ( prefix = 'dcm2nii_' ) # call dcm2nii arguments = '-o "{}" -i y' . format ( tmpdir . name ) try : call_out = call_dcm2nii ( input_dir , arguments ) except : raise else : log . info ( 'Converted "{}" to nifti.' . format ( input_dir ) ) # get the filenames of the files that dcm2nii produced filenames = glob ( op . join ( tmpdir . name , '*.nii*' ) ) # cleanup `filenames`, using only the post-processed (reoriented, cropped, etc.) images by dcm2nii cleaned_filenames = remove_dcm2nii_underprocessed ( filenames ) # copy files to the output_dir filepaths = [ ] for srcpath in cleaned_filenames : dstpath = op . join ( output_dir , filename ) realpath = copy_w_plus ( srcpath , dstpath ) filepaths . append ( realpath ) # copy any other file produced by dcm2nii that is not a NifTI file, e.g., *.bvals, *.bvecs, etc. basename = op . basename ( remove_ext ( srcpath ) ) aux_files = set ( glob ( op . join ( tmpdir . name , '{}.*' . format ( basename ) ) ) ) - set ( glob ( op . join ( tmpdir . name , '{}.nii*' . format ( basename ) ) ) ) for aux_file in aux_files : aux_dstpath = copy_w_ext ( aux_file , output_dir , remove_ext ( op . basename ( realpath ) ) ) filepaths . append ( aux_dstpath ) return filepaths
11581	def run ( self ) : # To add a command to the command dispatch table, append here. self . command_dispatch . update ( { self . REPORT_VERSION : [ self . report_version , 2 ] } ) self . command_dispatch . update ( { self . REPORT_FIRMWARE : [ self . report_firmware , 1 ] } ) self . command_dispatch . update ( { self . ANALOG_MESSAGE : [ self . analog_message , 2 ] } ) self . command_dispatch . update ( { self . DIGITAL_MESSAGE : [ self . digital_message , 2 ] } ) self . command_dispatch . update ( { self . ENCODER_DATA : [ self . encoder_data , 3 ] } ) self . command_dispatch . update ( { self . SONAR_DATA : [ self . sonar_data , 3 ] } ) self . command_dispatch . update ( { self . STRING_DATA : [ self . _string_data , 2 ] } ) self . command_dispatch . update ( { self . I2C_REPLY : [ self . i2c_reply , 2 ] } ) self . command_dispatch . update ( { self . CAPABILITY_RESPONSE : [ self . capability_response , 2 ] } ) self . command_dispatch . update ( { self . PIN_STATE_RESPONSE : [ self . pin_state_response , 2 ] } ) self . command_dispatch . update ( { self . ANALOG_MAPPING_RESPONSE : [ self . analog_mapping_response , 2 ] } ) self . command_dispatch . update ( { self . STEPPER_DATA : [ self . stepper_version_response , 2 ] } ) while not self . is_stopped ( ) : if len ( self . pymata . command_deque ) : # get next byte from the deque and process it data = self . pymata . command_deque . popleft ( ) # this list will be populated with the received data for the command command_data = [ ] # process sysex commands if data == self . START_SYSEX : # next char is the actual sysex command # wait until we can get data from the deque while len ( self . pymata . command_deque ) == 0 : pass sysex_command = self . pymata . command_deque . popleft ( ) # retrieve the associated command_dispatch entry for this command dispatch_entry = self . command_dispatch . get ( sysex_command ) # get a "pointer" to the method that will process this command method = dispatch_entry [ 0 ] # now get the rest of the data excluding the END_SYSEX byte end_of_sysex = False while not end_of_sysex : # wait for more data to arrive while len ( self . pymata . command_deque ) == 0 : pass data = self . pymata . command_deque . popleft ( ) if data != self . END_SYSEX : command_data . append ( data ) else : end_of_sysex = True # invoke the method to process the command method ( command_data ) # go to the beginning of the loop to process the next command continue # is this a command byte in the range of 0x80-0xff - these are the non-sysex messages elif 0x80 <= data <= 0xff : # look up the method for the command in the command dispatch table # for the digital reporting the command value is modified with port number # the handler needs the port to properly process, so decode that from the command and # place in command_data if 0x90 <= data <= 0x9f : port = data & 0xf command_data . append ( port ) data = 0x90 # the pin number for analog data is embedded in the command so, decode it elif 0xe0 <= data <= 0xef : pin = data & 0xf command_data . append ( pin ) data = 0xe0 else : pass dispatch_entry = self . command_dispatch . get ( data ) # this calls the method retrieved from the dispatch table method = dispatch_entry [ 0 ] # get the number of parameters that this command provides num_args = dispatch_entry [ 1 ] # look at the number of args that the selected method requires # now get that number of bytes to pass to the called method for i in range ( num_args ) : while len ( self . pymata . command_deque ) == 0 : pass data = self . pymata . command_deque . popleft ( ) command_data . append ( data ) # go execute the command with the argument list method ( command_data ) # go to the beginning of the loop to process the next command continue else : time . sleep ( .1 )
6052	def sparse_to_unmasked_sparse_from_mask_and_pixel_centres ( total_sparse_pixels , mask , unmasked_sparse_grid_pixel_centres ) : pix_to_full_pix = np . zeros ( total_sparse_pixels ) pixel_index = 0 for full_pixel_index in range ( unmasked_sparse_grid_pixel_centres . shape [ 0 ] ) : y = unmasked_sparse_grid_pixel_centres [ full_pixel_index , 0 ] x = unmasked_sparse_grid_pixel_centres [ full_pixel_index , 1 ] if not mask [ y , x ] : pix_to_full_pix [ pixel_index ] = full_pixel_index pixel_index += 1 return pix_to_full_pix
7119	def filter_dict ( unfiltered , filter_keys ) : filtered = DotDict ( ) for k in filter_keys : filtered [ k ] = unfiltered [ k ] return filtered
12722	def velocities ( self , velocities ) : _set_params ( self . ode_obj , 'Vel' , velocities , self . ADOF + self . LDOF )
12406	def serialize ( self , data = None ) : if data is not None and self . response is not None : # Set the content type. self . response [ 'Content-Type' ] = self . media_types [ 0 ] # Write the encoded and prepared data to the response. self . response . write ( data ) # Return the serialized data. # This has normally been transformed by a base class. return data
6302	def add_package ( self , name ) : name , cls_name = parse_package_string ( name ) if name in self . package_map : return package = EffectPackage ( name ) package . load ( ) self . packages . append ( package ) self . package_map [ package . name ] = package # Load effect package dependencies self . polulate ( package . effect_packages )
13759	def _create_api_uri ( self , * parts ) : return urljoin ( self . API_URI , '/' . join ( map ( quote , parts ) ) )
8048	def _parse_from_import_names ( self , is_future_import ) : if self . current . value == "(" : self . consume ( tk . OP ) expected_end_kinds = ( tk . OP , ) else : expected_end_kinds = ( tk . NEWLINE , tk . ENDMARKER ) while self . current . kind not in expected_end_kinds and not ( self . current . kind == tk . OP and self . current . value == ";" ) : if self . current . kind != tk . NAME : self . stream . move ( ) continue self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , ) if is_future_import : self . log . debug ( "found future import: %s" , self . current . value ) self . future_imports . add ( self . current . value ) self . consume ( tk . NAME ) self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , ) if self . current . kind == tk . NAME and self . current . value == "as" : self . consume ( tk . NAME ) # as if self . current . kind == tk . NAME : self . consume ( tk . NAME ) # new name, irrelevant if self . current . value == "," : self . consume ( tk . OP ) self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , )
7434	def _bufcountlines ( filename , gzipped ) : if gzipped : fin = gzip . open ( filename ) else : fin = open ( filename ) nlines = 0 buf_size = 1024 * 1024 read_f = fin . read # loop optimization buf = read_f ( buf_size ) while buf : nlines += buf . count ( '\n' ) buf = read_f ( buf_size ) fin . close ( ) return nlines
13909	def show_version ( self ) : class ShowVersionAction ( argparse . Action ) : def __init__ ( inner_self , nargs = 0 , * * kw ) : super ( ShowVersionAction , inner_self ) . __init__ ( nargs = nargs , * * kw ) def __call__ ( inner_self , parser , args , value , option_string = None ) : print ( "{parser_name} version: {version}" . format ( parser_name = self . config . get ( "parser" , { } ) . get ( "prog" ) , version = self . prog_version ) ) return ShowVersionAction
7335	async def upload_media ( self , file_ , media_type = None , media_category = None , chunked = None , size_limit = None , * * params ) : if isinstance ( file_ , str ) : url = urlparse ( file_ ) if url . scheme . startswith ( 'http' ) : media = await self . _session . get ( file_ ) else : path = urlparse ( file_ ) . path . strip ( " \"'" ) media = await utils . execute ( open ( path , 'rb' ) ) elif hasattr ( file_ , 'read' ) or isinstance ( file_ , bytes ) : media = file_ else : raise TypeError ( "upload_media input must be a file object or a " "filename or binary data or an aiohttp request" ) media_size = await utils . get_size ( media ) if chunked is not None : size_test = False else : size_test = await self . _size_test ( media_size , size_limit ) if isinstance ( media , aiohttp . ClientResponse ) : # send the content of the response media = media . content if chunked or ( size_test and chunked is None ) : args = media , media_size , file_ , media_type , media_category response = await self . _chunked_upload ( * args , * * params ) else : response = await self . upload . media . upload . post ( media = media , * * params ) if not hasattr ( file_ , 'read' ) and not getattr ( media , 'closed' , True ) : media . close ( ) return response
8976	def file ( self , file = None ) : if file is None : file = StringIO ( ) self . _file ( file ) return file
6088	def scaled_noise_map_from_hyper_galaxies_and_contribution_maps ( contribution_maps , hyper_galaxies , noise_map ) : scaled_noise_maps = list ( map ( lambda hyper_galaxy , contribution_map : hyper_galaxy . hyper_noise_from_contributions ( noise_map = noise_map , contributions = contribution_map ) , hyper_galaxies , contribution_maps ) ) return noise_map + sum ( scaled_noise_maps )
5102	def get_edge_type ( self , edge_type ) : edges = [ ] for e in self . edges ( ) : if self . adj [ e [ 0 ] ] [ e [ 1 ] ] . get ( 'edge_type' ) == edge_type : edges . append ( e ) return edges
6050	def run ( self , data , results = None , mask = None , positions = None ) : model_image = results . last . unmasked_model_image galaxy_tuples = results . last . constant . name_instance_tuples_for_class ( g . Galaxy ) results_copy = copy . copy ( results . last ) for name , galaxy in galaxy_tuples : optimizer = self . optimizer . copy_with_name_extension ( name ) optimizer . variable . hyper_galaxy = g . HyperGalaxy galaxy_image = results . last . unmasked_image_for_galaxy ( galaxy ) optimizer . fit ( self . __class__ . Analysis ( data , model_image , galaxy_image ) ) getattr ( results_copy . variable , name ) . hyper_galaxy = optimizer . variable . hyper_galaxy getattr ( results_copy . constant , name ) . hyper_galaxy = optimizer . constant . hyper_galaxy return results_copy
12299	def register ( self , what , obj ) : # print("Registering pattern", name, pattern) name = obj . name version = obj . version enable = obj . enable if enable == 'n' : return key = Key ( name , version ) self . plugins [ what ] [ key ] = obj
2934	def merge_option_and_config_str ( cls , option_name , config , options ) : opt = getattr ( options , option_name , None ) if opt : config . set ( CONFIG_SECTION_NAME , option_name , opt ) elif config . has_option ( CONFIG_SECTION_NAME , option_name ) : setattr ( options , option_name , config . get ( CONFIG_SECTION_NAME , option_name ) )
11644	def fit ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) # TODO: only get negative eigs somehow? memory = get_memory ( self . memory ) vals , vecs = memory . cache ( scipy . linalg . eigh , ignore = [ 'overwrite_a' ] ) ( X , overwrite_a = not self . copy ) vals = vals . reshape ( - 1 , 1 ) if self . min_eig == 0 : inner = vals > self . min_eig else : with np . errstate ( divide = 'ignore' ) : inner = np . where ( vals >= self . min_eig , 1 , np . where ( vals == 0 , 0 , self . min_eig / vals ) ) self . clip_ = np . dot ( vecs , inner * vecs . T ) return self
4113	def rc2is ( k ) : assert numpy . isrealobj ( k ) , 'Inverse sine parameters not defined for complex reflection coefficients.' if max ( numpy . abs ( k ) ) >= 1 : raise ValueError ( 'All reflection coefficients should have magnitude less than unity.' ) return ( 2 / numpy . pi ) * numpy . arcsin ( k )
1489	def save_file ( self , obj ) : # pylint: disable=too-many-branches try : import StringIO as pystringIO #we can't use cStringIO as it lacks the name attribute except ImportError : import io as pystringIO # pylint: disable=reimported if not hasattr ( obj , 'name' ) or not hasattr ( obj , 'mode' ) : raise pickle . PicklingError ( "Cannot pickle files that do not map to an actual file" ) if obj is sys . stdout : return self . save_reduce ( getattr , ( sys , 'stdout' ) , obj = obj ) if obj is sys . stderr : return self . save_reduce ( getattr , ( sys , 'stderr' ) , obj = obj ) if obj is sys . stdin : raise pickle . PicklingError ( "Cannot pickle standard input" ) if hasattr ( obj , 'isatty' ) and obj . isatty ( ) : raise pickle . PicklingError ( "Cannot pickle files that map to tty objects" ) if 'r' not in obj . mode : raise pickle . PicklingError ( "Cannot pickle files that are not opened for reading" ) name = obj . name try : fsize = os . stat ( name ) . st_size except OSError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be stat" % name ) if obj . closed : #create an empty closed string io retval = pystringIO . StringIO ( "" ) retval . close ( ) elif not fsize : #empty file retval = pystringIO . StringIO ( "" ) try : tmpfile = file ( name ) tst = tmpfile . read ( 1 ) except IOError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be read" % name ) tmpfile . close ( ) if tst != '' : raise pickle . PicklingError ( "Cannot pickle file %s as it does not appear to map to a physical, real file" % name ) else : try : tmpfile = file ( name ) contents = tmpfile . read ( ) tmpfile . close ( ) except IOError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be read" % name ) retval = pystringIO . StringIO ( contents ) curloc = obj . tell ( ) retval . seek ( curloc ) retval . name = name self . save ( retval ) self . memoize ( obj )
5358	def es_version ( self , url ) : try : res = self . grimoire_con . get ( url ) res . raise_for_status ( ) major = res . json ( ) [ 'version' ] [ 'number' ] . split ( "." ) [ 0 ] except Exception : logger . error ( "Error retrieving Elasticsearch version: " + url ) raise return major
4106	def MINEIGVAL ( T0 , T , TOL ) : M = len ( T ) eigval = 10 eigvalold = 1 eigvec = numpy . zeros ( M + 1 , dtype = complex ) for k in range ( 0 , M + 1 ) : eigvec [ k ] = 1 + 0j it = 0 #print 'initialisation',T0, T, eigval, eigvec maxit = 15 while abs ( eigvalold - eigval ) > TOL * eigvalold and it < maxit : it = it + 1 eigvalold = eigval #print 'iteration ',it, 'eigvalold=',eigvalold, 'eigval=', eigval eig = toeplitz . HERMTOEP ( T0 , T , eigvec ) SUM = 0 save = 0. + 0j for k in range ( 0 , M + 1 ) : SUM = SUM + eig [ k ] . real ** 2 + eig [ k ] . imag ** 2 save = save + eig [ k ] * eigvec [ k ] . conjugate ( ) SUM = 1. / SUM eigval = save . real * SUM for k in range ( 0 , M + 1 ) : eigvec [ k ] = SUM * eig [ k ] if it == maxit : print ( 'warning reached max number of iteration (%s)' % maxit ) return eigval , eigvec
3409	def knock_out ( self ) : self . functional = False for reaction in self . reactions : if not reaction . functional : reaction . bounds = ( 0 , 0 )
912	def write ( self , proto ) : super ( PreviousValueModel , self ) . writeBaseToProto ( proto . modelBase ) proto . fieldNames = self . _fieldNames proto . fieldTypes = self . _fieldTypes if self . _predictedField : proto . predictedField = self . _predictedField proto . predictionSteps = self . _predictionSteps
13896	def ExpandUser ( path ) : if six . PY2 : encoding = sys . getfilesystemencoding ( ) path = path . encode ( encoding ) result = os . path . expanduser ( path ) if six . PY2 : result = result . decode ( encoding ) return result
7315	def parse_filter ( self , filters ) : for filter_type in filters : if filter_type == 'or' or filter_type == 'and' : conditions = [ ] for field in filters [ filter_type ] : if self . is_field_allowed ( field ) : conditions . append ( self . create_query ( self . parse_field ( field , filters [ filter_type ] [ field ] ) ) ) if filter_type == 'or' : self . model_query = self . model_query . filter ( or_ ( * conditions ) ) elif filter_type == 'and' : self . model_query = self . model_query . filter ( and_ ( * conditions ) ) else : if self . is_field_allowed ( filter_type ) : conditions = self . create_query ( self . parse_field ( filter_type , filters [ filter_type ] ) ) self . model_query = self . model_query . filter ( conditions ) return self . model_query
8432	def manual_pal ( values ) : max_n = len ( values ) def _manual_pal ( n ) : if n > max_n : msg = ( "Palette can return a maximum of {} values. " "{} were requested from it." ) warnings . warn ( msg . format ( max_n , n ) ) return values [ : n ] return _manual_pal
10634	def get_compound_afr ( self , compound ) : index = self . material . get_compound_index ( compound ) return stoich . amount ( compound , self . _compound_mfrs [ index ] )
5282	def construct_formset ( self ) : formset_class = self . get_formset ( ) if hasattr ( self , 'get_extra_form_kwargs' ) : klass = type ( self ) . __name__ raise DeprecationWarning ( 'Calling {0}.get_extra_form_kwargs is no longer supported. ' 'Set `form_kwargs` in {0}.formset_kwargs or override ' '{0}.get_formset_kwargs() directly.' . format ( klass ) , ) return formset_class ( * * self . get_formset_kwargs ( ) )
11855	def predictor ( self , ( i , j , A , alpha , Bb ) ) : B = Bb [ 0 ] if B in self . grammar . rules : for rhs in self . grammar . rewrites_for ( B ) : self . add_edge ( [ j , j , B , [ ] , rhs ] )
8462	def set_cmd_env_var ( value ) : def func_decorator ( function ) : @ functools . wraps ( function ) def wrapper ( * args , * * kwargs ) : previous_cmd_env_var = os . getenv ( temple . constants . TEMPLE_ENV_VAR ) os . environ [ temple . constants . TEMPLE_ENV_VAR ] = value try : ret_val = function ( * args , * * kwargs ) finally : if previous_cmd_env_var is None : del os . environ [ temple . constants . TEMPLE_ENV_VAR ] else : os . environ [ temple . constants . TEMPLE_ENV_VAR ] = previous_cmd_env_var return ret_val return wrapper return func_decorator
8792	def validate ( self , value ) : try : vlan_id_int = int ( value ) assert vlan_id_int >= self . MIN_VLAN_ID assert vlan_id_int <= self . MAX_VLAN_ID except Exception : msg = ( "Invalid vlan_id. Got '%(vlan_id)s'. " "vlan_id should be an integer between %(min)d and %(max)d " "inclusive." % { 'vlan_id' : value , 'min' : self . MIN_VLAN_ID , 'max' : self . MAX_VLAN_ID } ) raise TagValidationError ( value , msg ) return True
11942	def stored_messages_archive ( context , num_elements = 10 ) : if "user" in context : user = context [ "user" ] if user . is_authenticated ( ) : qs = MessageArchive . objects . select_related ( "message" ) . filter ( user = user ) return { "messages" : qs [ : num_elements ] , "count" : qs . count ( ) , }
9550	def ivalidate ( self , data , expect_header_row = True , ignore_lines = 0 , summarize = False , context = None , report_unexpected_exceptions = True ) : unique_sets = self . _init_unique_sets ( ) # used for unique checks for i , r in enumerate ( data ) : if expect_header_row and i == ignore_lines : # r is the header row for p in self . _apply_header_checks ( i , r , summarize , context ) : yield p elif i >= ignore_lines : # r is a data row skip = False for p in self . _apply_skips ( i , r , summarize , report_unexpected_exceptions , context ) : if p is True : skip = True else : yield p if not skip : for p in self . _apply_each_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p # may yield a problem if an exception is raised for p in self . _apply_value_checks ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_length_checks ( i , r , summarize , context ) : yield p for p in self . _apply_value_predicates ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_checks ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_predicates ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_unique_checks ( i , r , unique_sets , summarize ) : yield p for p in self . _apply_check_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_assert_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_finally_assert_methods ( summarize , report_unexpected_exceptions , context ) : yield p
7536	def derep_concat_split ( data , sample , nthreads , force ) : ## report location for debugging LOGGER . info ( "INSIDE derep %s" , sample . name ) ## MERGED ASSEMBIES ONLY: ## concatenate edits files within Samples. Returns a new sample.files.edits ## with the concat file. No change if not merged Assembly. mergefile = os . path . join ( data . dirs . edits , sample . name + "_merged_.fastq" ) if not force : if not os . path . exists ( mergefile ) : sample . files . edits = concat_multiple_edits ( data , sample ) else : LOGGER . info ( "skipped concat_multiple_edits: {} exists" . format ( mergefile ) ) else : sample . files . edits = concat_multiple_edits ( data , sample ) ## PAIRED DATA ONLY: ## Denovo: merge or concat fastq pairs [sample.files.pairs] ## Reference: only concat fastq pairs [] ## Denovo + Reference: ... if 'pair' in data . paramsdict [ 'datatype' ] : ## the output file handle for merged reads ## modify behavior of merging vs concating if reference if "reference" in data . paramsdict [ "assembly_method" ] : nmerged = merge_pairs ( data , sample . files . edits , mergefile , 0 , 0 ) else : nmerged = merge_pairs ( data , sample . files . edits , mergefile , 1 , 1 ) ## store results sample . files . edits = [ ( mergefile , ) ] sample . stats . reads_merged = nmerged ## 3rad uses random adapters to identify pcr duplicates. We will ## remove pcr dupes here. Basically append the radom adapter to ## each sequence, do a regular old vsearch derep, then trim ## off the adapter, and push it down the pipeline. This will ## remove all identical seqs with identical random i5 adapters. if "3rad" in data . paramsdict [ "datatype" ] : declone_3rad ( data , sample ) derep_and_sort ( data , os . path . join ( data . dirs . edits , sample . name + "_declone.fastq" ) , os . path . join ( data . dirs . edits , sample . name + "_derep.fastq" ) , nthreads ) else : ## convert fastq to fasta, then derep and sort reads by their size. ## we pass in only one file b/c paired should be merged by now. derep_and_sort ( data , sample . files . edits [ 0 ] [ 0 ] , os . path . join ( data . dirs . edits , sample . name + "_derep.fastq" ) , nthreads )
1294	def tf_combined_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : q_model_loss = self . fn_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) demo_loss = self . fn_demo_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , update = update , reference = reference ) return q_model_loss + self . supervised_weight * demo_loss
5837	def tsne ( self , data_view_id ) : analysis = self . _data_analysis ( data_view_id ) projections = analysis [ 'projections' ] tsne = Tsne ( ) for k , v in projections . items ( ) : projection = Projection ( xs = v [ 'x' ] , ys = v [ 'y' ] , responses = v [ 'label' ] , tags = v [ 'inputs' ] , uids = v [ 'uid' ] ) tsne . add_projection ( k , projection ) return tsne
3636	def club ( self , sort = 'desc' , ctype = 'player' , defId = '' , start = 0 , count = None , page_size = itemsPerPage [ 'club' ] , level = None , category = None , assetId = None , league = None , club = None , position = None , zone = None , nationality = None , rare = False , playStyle = None ) : method = 'GET' url = 'club' if count : # backward compatibility, will be removed in future page_size = count params = { 'sort' : sort , 'type' : ctype , 'defId' : defId , 'start' : start , 'count' : page_size } if level : params [ 'level' ] = level if category : params [ 'cat' ] = category if assetId : params [ 'maskedDefId' ] = assetId if league : params [ 'leag' ] = league if club : params [ 'team' ] = club if position : params [ 'pos' ] = position if zone : params [ 'zone' ] = zone if nationality : params [ 'nat' ] = nationality if rare : params [ 'rare' ] = 'SP' if playStyle : params [ 'playStyle' ] = playStyle rc = self . __request__ ( method , url , params = params ) # pinEvent if start == 0 : if ctype == 'player' : pgid = 'Club - Players - List View' elif ctype == 'staff' : pgid = 'Club - Staff - List View' elif ctype in ( 'item' , 'kit' , 'ball' , 'badge' , 'stadium' ) : pgid = 'Club - Club Items - List View' # else: # TODO: THIS IS probably WRONG, detect all ctypes # pgid = 'Club - Club Items - List View' events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) , self . pin . event ( 'page_view' , pgid ) ] if rc [ 'itemData' ] : events . append ( self . pin . event ( 'page_view' , 'Item - Detail View' ) ) self . pin . send ( events ) return [ itemParse ( { 'itemData' : i } ) for i in rc [ 'itemData' ] ]
12367	def create ( self , name , ip_address ) : return ( self . post ( name = name , ip_address = ip_address ) . get ( self . singular , None ) )
4385	def remove_binaries ( ) : patterns = ( "adslib/*.a" , "adslib/*.o" , "adslib/obj/*.o" , "adslib/*.bin" , "adslib/*.so" , ) for f in functools . reduce ( operator . iconcat , [ glob . glob ( p ) for p in patterns ] ) : os . remove ( f )
927	def getFilename ( aggregationInfo , inputFile ) : # Find the actual file, with an absolute path inputFile = resource_filename ( "nupic.datafiles" , inputFile ) a = defaultdict ( lambda : 0 , aggregationInfo ) outputDir = os . path . dirname ( inputFile ) outputFile = 'agg_%s' % os . path . splitext ( os . path . basename ( inputFile ) ) [ 0 ] noAggregation = True timePeriods = 'years months weeks days ' 'hours minutes seconds milliseconds microseconds' for k in timePeriods . split ( ) : if a [ k ] > 0 : noAggregation = False outputFile += '_%s_%d' % ( k , a [ k ] ) if noAggregation : return inputFile outputFile += '.csv' outputFile = os . path . join ( outputDir , outputFile ) return outputFile
11667	def linear ( Ks , dim , num_q , rhos , nus ) : return _get_linear ( Ks , dim ) ( num_q , rhos , nus )
9498	def parse_litezip ( path ) : struct = [ parse_collection ( path ) ] struct . extend ( [ parse_module ( x ) for x in path . iterdir ( ) if x . is_dir ( ) and x . name . startswith ( 'm' ) ] ) return tuple ( sorted ( struct ) )
12609	def _query_data ( data , field_names = None , operators = '__eq__' ) : if field_names is None : field_names = list ( data . keys ( ) ) if isinstance ( field_names , str ) : field_names = [ field_names ] # using OrderedDict by default, in case operators has different operators for each field. sample = OrderedDict ( [ ( fn , data [ fn ] ) for fn in field_names ] ) return _query_sample ( sample , operators = operators )
1285	def footnote_item ( self , key , text ) : back = ( '<a href="#fnref-%s" class="footnote">&#8617;</a>' ) % escape ( key ) text = text . rstrip ( ) if text . endswith ( '</p>' ) : text = re . sub ( r'<\/p>$' , r'%s</p>' % back , text ) else : text = '%s<p>%s</p>' % ( text , back ) html = '<li id="fn-%s">%s</li>\n' % ( escape ( key ) , text ) return html
10518	def setmin ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) object_handle . AXValue = 0 return 1
6334	def dist ( self , src , tar ) : if src == tar : return 0.0 return self . dist_abs ( src , tar ) / ( len ( src ) + len ( tar ) )
12294	def annotate_metadata_platform ( repo ) : print ( "Added platform information" ) package = repo . package mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) package [ 'platform' ] = repomgr . get_metadata ( )
13369	def is_home_environment ( path ) : home = unipath ( os . environ . get ( 'CPENV_HOME' , '~/.cpenv' ) ) path = unipath ( path ) return path . startswith ( home )
536	def readFromProto ( cls , proto ) : instance = cls ( ) instance . implementation = proto . implementation instance . steps = proto . steps instance . stepsList = [ int ( i ) for i in proto . steps . split ( "," ) ] instance . alpha = proto . alpha instance . verbosity = proto . verbosity instance . maxCategoryCount = proto . maxCategoryCount instance . _sdrClassifier = SDRClassifierFactory . read ( proto ) instance . learningMode = proto . learningMode instance . inferenceMode = proto . inferenceMode instance . recordNum = proto . recordNum return instance
12989	def setup_notebook ( debug = False ) : output_notebook ( INLINE , hide_banner = True ) if debug : _setup_logging ( logging . DEBUG ) logging . debug ( 'Running notebook in debug mode.' ) else : _setup_logging ( logging . WARNING ) # If JUPYTERHUB_SERVICE_PREFIX environment variable isn't set, # this means that you're running JupyterHub not with Hub in k8s, # and not using run_local.sh (which sets it to empty). if 'JUPYTERHUB_SERVICE_PREFIX' not in os . environ : global jupyter_proxy_url jupyter_proxy_url = 'localhost:8888' logging . info ( 'Setting jupyter proxy to local mode.' )
11065	def acl_show ( self , msg , args ) : name = args [ 0 ] if len ( args ) > 0 else None if name is None : return "%s: The following ACLs are defined: %s" % ( msg . user , ', ' . join ( self . _acl . keys ( ) ) ) if name not in self . _acl : return "Sorry, couldn't find an acl named '%s'" % name return '\n' . join ( [ "%s: ACL '%s' is defined as follows:" % ( msg . user , name ) , "allow: %s" % ', ' . join ( self . _acl [ name ] [ 'allow' ] ) , "deny: %s" % ', ' . join ( self . _acl [ name ] [ 'deny' ] ) ] )
4693	def cmd ( command ) : env ( ) ipmi = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) command = "ipmitool -U %s -P %s -H %s -p %s %s" % ( ipmi [ "USER" ] , ipmi [ "PASS" ] , ipmi [ "HOST" ] , ipmi [ "PORT" ] , command ) cij . info ( "ipmi.command: %s" % command ) return cij . util . execute ( command , shell = True , echo = True )
5845	def kill_design_run ( self , data_view_id , run_uuid ) : url = routes . kill_data_view_design_run ( data_view_id , run_uuid ) response = self . _delete ( url ) . json ( ) return response [ "data" ] [ "uid" ]
1949	def write_back_register ( self , reg , val ) : if self . write_backs_disabled : return if issymbolic ( val ) : logger . warning ( "Skipping Symbolic write-back" ) return if reg in self . flag_registers : self . _emu . reg_write ( self . _to_unicorn_id ( 'EFLAGS' ) , self . _cpu . read_register ( 'EFLAGS' ) ) return self . _emu . reg_write ( self . _to_unicorn_id ( reg ) , val )
4943	def get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = None , program_uuid = None ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) # pylint: disable=invalid-name try : if course_id : return get_course_data_sharing_consent ( username , course_id , enterprise_customer_uuid ) return get_program_data_sharing_consent ( username , program_uuid , enterprise_customer_uuid ) except EnterpriseCustomer . DoesNotExist : return None
10705	def get_locations ( ) : arequest = requests . get ( LOCATIONS_URL , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
6047	def map_to_2d_keep_padded ( self , padded_array_1d ) : return mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = padded_array_1d , shape = self . mask . shape )
387	def obj_box_imresize ( im , coords = None , size = None , interp = 'bicubic' , mode = None , is_rescale = False ) : if coords is None : coords = [ ] if size is None : size = [ 100 , 100 ] imh , imw = im . shape [ 0 : 2 ] imh = imh * 1.0 # * 1.0 for python2 : force division to be float point imw = imw * 1.0 im = imresize ( im , size = size , interp = interp , mode = mode ) if is_rescale is False : coords_new = list ( ) for coord in coords : if len ( coord ) != 4 : raise AssertionError ( "coordinate should be 4 values : [x, y, w, h]" ) # x' = x * (imw'/imw) x = int ( coord [ 0 ] * ( size [ 1 ] / imw ) ) # y' = y * (imh'/imh) # tl.logging.info('>>', coord[1], size[0], imh) y = int ( coord [ 1 ] * ( size [ 0 ] / imh ) ) # w' = w * (imw'/imw) w = int ( coord [ 2 ] * ( size [ 1 ] / imw ) ) # h' = h * (imh'/imh) h = int ( coord [ 3 ] * ( size [ 0 ] / imh ) ) coords_new . append ( [ x , y , w , h ] ) return im , coords_new else : return im , coords
7537	def branch_assembly ( args , parsedict ) : ## Get the current assembly data = getassembly ( args , parsedict ) ## get arguments to branch command bargs = args . branch ## get new name, trim off .txt if it was accidentally added newname = bargs [ 0 ] if newname . endswith ( ".txt" ) : newname = newname [ : - 4 ] ## look for subsamples if len ( bargs ) > 1 : ## Branching and subsampling at step 6 is a bad idea, it messes up ## indexing into the hdf5 cluster file. Warn against this. if any ( [ x . stats . state == 6 for x in data . samples . values ( ) ] ) : pass ## TODODODODODO #print("wat") ## are we removing or keeping listed samples? subsamples = bargs [ 1 : ] ## drop the matching samples if bargs [ 1 ] == "-" : ## check drop names fails = [ i for i in subsamples [ 1 : ] if i not in data . samples . keys ( ) ] if any ( fails ) : raise IPyradWarningExit ( "\ \n Failed: unrecognized names requested, check spelling:\n {}" . format ( "\n " . join ( [ i for i in fails ] ) ) ) print ( " dropping {} samples" . format ( len ( subsamples ) - 1 ) ) subsamples = list ( set ( data . samples . keys ( ) ) - set ( subsamples ) ) ## If the arg after the new param name is a file that exists if os . path . exists ( bargs [ 1 ] ) : new_data = data . branch ( newname , infile = bargs [ 1 ] ) else : new_data = data . branch ( newname , subsamples ) ## keeping all samples else : new_data = data . branch ( newname , None ) print ( " creating a new branch called '{}' with {} Samples" . format ( new_data . name , len ( new_data . samples ) ) ) print ( " writing new params file to {}" . format ( "params-" + new_data . name + ".txt\n" ) ) new_data . write_params ( "params-" + new_data . name + ".txt" , force = args . force )
1936	def constructor_abi ( self ) -> Dict [ str , Any ] : item = self . _constructor_abi_item if item : return dict ( item ) return { 'inputs' : [ ] , 'payable' : False , 'stateMutability' : 'nonpayable' , 'type' : 'constructor' }
440	def print_layers ( self ) : for i , layer in enumerate ( self . all_layers ) : # logging.info(" layer %d: %s" % (i, str(layer))) logging . info ( " layer {:3}: {:20} {:15} {}" . format ( i , layer . name , str ( layer . get_shape ( ) ) , layer . dtype . name ) )
7912	def add_setting ( cls , name , type = unicode , default = None , factory = None , cache = False , default_d = None , doc = None , cmdline_help = None , validator = None , basic = False ) : # pylint: disable-msg=W0622,R0913 setting_def = _SettingDefinition ( name , type , default , factory , cache , default_d , doc , cmdline_help , validator , basic ) if name not in cls . _defs : cls . _defs [ name ] = setting_def return duplicate = cls . _defs [ name ] if duplicate . type != setting_def . type : raise ValueError ( "Setting duplicate, with a different type" ) if duplicate . default != setting_def . default : raise ValueError ( "Setting duplicate, with a different default" ) if duplicate . factory != setting_def . factory : raise ValueError ( "Setting duplicate, with a different factory" )
10094	def get_template ( self , template_id , version = None , timeout = None ) : if ( version ) : return self . _api_request ( self . TEMPLATES_VERSION_ENDPOINT % ( template_id , version ) , self . HTTP_GET , timeout = timeout ) else : return self . _api_request ( self . TEMPLATES_SPECIFIC_ENDPOINT % template_id , self . HTTP_GET , timeout = timeout )
2159	def _format_yaml ( self , payload ) : return parser . ordered_dump ( payload , Dumper = yaml . SafeDumper , default_flow_style = False )
11262	def sub ( prev , pattern , repl , * args , * * kw ) : count = 0 if 'count' not in kw else kw . pop ( 'count' ) pattern_obj = re . compile ( pattern , * args , * * kw ) for s in prev : yield pattern_obj . sub ( repl , s , count = count )
11991	def decode_html_entities ( html ) : if not html : return html for entity , char in six . iteritems ( html_entity_map ) : html = html . replace ( entity , char ) return html
2260	def group_items ( items , groupids ) : if callable ( groupids ) : keyfunc = groupids pair_list = ( ( keyfunc ( item ) , item ) for item in items ) else : pair_list = zip ( groupids , items ) # Initialize a dict of lists groupid_to_items = defaultdict ( list ) # Insert each item into the correct group for key , item in pair_list : groupid_to_items [ key ] . append ( item ) return groupid_to_items
5797	def _get_func_info ( docstring , def_lineno , code_lines , prefix ) : def_index = def_lineno - 1 definition = code_lines [ def_index ] definition = definition . rstrip ( ) while not definition . endswith ( ':' ) : def_index += 1 definition += '\n' + code_lines [ def_index ] . rstrip ( ) definition = textwrap . dedent ( definition ) . rstrip ( ':' ) definition = definition . replace ( '\n' , '\n' + prefix ) description = '' found_colon = False params = '' for line in docstring . splitlines ( ) : if line and line [ 0 ] == ':' : found_colon = True if not found_colon : if description : description += '\n' description += line else : if params : params += '\n' params += line description = description . strip ( ) description_md = '' if description : description_md = "%s%s" % ( prefix , description . replace ( '\n' , '\n' + prefix ) ) description_md = re . sub ( '\n>(\\s+)\n' , '\n>\n' , description_md ) params = params . strip ( ) if params : definition += ( ':\n%s """\n%s ' % ( prefix , prefix ) ) definition += params . replace ( '\n' , '\n%s ' % prefix ) definition += ( '\n%s """' % prefix ) definition = re . sub ( '\n>(\\s+)\n' , '\n>\n' , definition ) for search , replace in definition_replacements . items ( ) : definition = definition . replace ( search , replace ) return ( definition , description_md )
13485	def showhtml ( ) : import webbrowser # copy from paver opts = options docroot = path ( opts . get ( 'docroot' , 'docs' ) ) if not docroot . exists ( ) : raise BuildFailure ( "Sphinx documentation root (%s) does not exist." % docroot ) builddir = docroot / opts . get ( "builddir" , ".build" ) # end of copy builddir = builddir / 'html' if not builddir . exists ( ) : raise BuildFailure ( "Sphinx build directory (%s) does not exist." % builddir ) webbrowser . open ( builddir / 'index.html' )
9857	def get_devices ( self ) : retn = [ ] api_devices = self . api_call ( 'devices' ) self . log ( 'DEVICES:' ) self . log ( api_devices ) for device in api_devices : retn . append ( AmbientWeatherStation ( self , device ) ) self . log ( 'DEVICE INSTANCE LIST:' ) self . log ( retn ) return retn
9209	def get_codec ( bytes_ ) : prefix = extract_prefix ( bytes_ ) try : return CODE_TABLE [ prefix ] except KeyError : raise ValueError ( 'Prefix {} not present in the lookup table' . format ( prefix ) )
5604	def _get_warped_array ( input_file = None , indexes = None , dst_bounds = None , dst_shape = None , dst_crs = None , resampling = None , src_nodata = None , dst_nodata = None ) : try : return _rasterio_read ( input_file = input_file , indexes = indexes , dst_bounds = dst_bounds , dst_shape = dst_shape , dst_crs = dst_crs , resampling = resampling , src_nodata = src_nodata , dst_nodata = dst_nodata ) except Exception as e : logger . exception ( "error while reading file %s: %s" , input_file , e ) raise
5509	def get_permissions ( self , path ) : path = pathlib . PurePosixPath ( path ) parents = filter ( lambda p : p . is_parent ( path ) , self . permissions ) perm = min ( parents , key = lambda p : len ( path . relative_to ( p . path ) . parts ) , default = Permission ( ) , ) return perm
14	def obs_space_info ( obs_space ) : if isinstance ( obs_space , gym . spaces . Dict ) : assert isinstance ( obs_space . spaces , OrderedDict ) subspaces = obs_space . spaces else : subspaces = { None : obs_space } keys = [ ] shapes = { } dtypes = { } for key , box in subspaces . items ( ) : keys . append ( key ) shapes [ key ] = box . shape dtypes [ key ] = box . dtype return keys , shapes , dtypes
12970	def _doCascadeFetch ( obj ) : obj . validateModel ( ) if not obj . foreignFields : return # NOTE: Currently this fetches using one transaction per object. Implementation for actual resolution is in # IndexedRedisModel.__getattribute__ for foreignField in obj . foreignFields : subObjsData = object . __getattribute__ ( obj , foreignField ) if not subObjsData : setattr ( obj , str ( foreignField ) , irNull ) continue subObjs = subObjsData . getObjs ( ) for subObj in subObjs : if isIndexedRedisModel ( subObj ) : IndexedRedisQuery . _doCascadeFetch ( subObj )
319	def calc_distribution_stats ( x ) : return pd . Series ( { 'mean' : np . mean ( x ) , 'median' : np . median ( x ) , 'std' : np . std ( x ) , '5%' : np . percentile ( x , 5 ) , '25%' : np . percentile ( x , 25 ) , '75%' : np . percentile ( x , 75 ) , '95%' : np . percentile ( x , 95 ) , 'IQR' : np . subtract . reduce ( np . percentile ( x , [ 75 , 25 ] ) ) , } )
3671	def identify_phase ( T , P , Tm = None , Tb = None , Tc = None , Psat = None ) : if Tm and T <= Tm : return 's' elif Tc and T >= Tc : # No special return value for the critical point return 'g' elif Psat : # Do not allow co-existence of phases; transition to 'l' directly under if P <= Psat : return 'g' elif P > Psat : return 'l' elif Tb : # Crude attempt to model phases without Psat # Treat Tb as holding from 90 kPa to 110 kPa if 9E4 < P < 1.1E5 : if T < Tb : return 'l' else : return 'g' elif P > 1.1E5 and T <= Tb : # For the higher-pressure case, it is definitely liquid if under Tb # Above the normal boiling point, impossible to say - return None return 'l' else : return None else : return None
8151	def _addvar ( self , v ) : oldvar = self . _oldvars . get ( v . name ) if oldvar is not None : if isinstance ( oldvar , Variable ) : if oldvar . compliesTo ( v ) : v . value = oldvar . value else : # Set from commandline v . value = v . sanitize ( oldvar ) else : for listener in VarListener . listeners : listener . var_added ( v ) self . _vars [ v . name ] = v self . _namespace [ v . name ] = v . value self . _oldvars [ v . name ] = v return v
4327	def downsample ( self , factor = 2 ) : if not isinstance ( factor , int ) or factor < 1 : raise ValueError ( 'factor must be a positive integer.' ) effect_args = [ 'downsample' , '{}' . format ( factor ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'downsample' ) return self
929	def _getFuncPtrAndParams ( self , funcName ) : params = None if isinstance ( funcName , basestring ) : if funcName == 'sum' : fp = _aggr_sum elif funcName == 'first' : fp = _aggr_first elif funcName == 'last' : fp = _aggr_last elif funcName == 'mean' : fp = _aggr_mean elif funcName == 'max' : fp = max elif funcName == 'min' : fp = min elif funcName == 'mode' : fp = _aggr_mode elif funcName . startswith ( 'wmean:' ) : fp = _aggr_weighted_mean paramsName = funcName [ 6 : ] params = [ f [ 0 ] for f in self . _inputFields ] . index ( paramsName ) else : fp = funcName return ( fp , params )
4350	def vol ( self , gain , gain_type = 'amplitude' , limiter_gain = None ) : if not is_number ( gain ) : raise ValueError ( 'gain must be a number.' ) if limiter_gain is not None : if ( not is_number ( limiter_gain ) or limiter_gain <= 0 or limiter_gain >= 1 ) : raise ValueError ( 'limiter gain must be a positive number less than 1' ) if gain_type in [ 'amplitude' , 'power' ] and gain < 0 : raise ValueError ( "If gain_type = amplitude or power, gain must be positive." ) effect_args = [ 'vol' ] effect_args . append ( '{:f}' . format ( gain ) ) if gain_type == 'amplitude' : effect_args . append ( 'amplitude' ) elif gain_type == 'power' : effect_args . append ( 'power' ) elif gain_type == 'db' : effect_args . append ( 'dB' ) else : raise ValueError ( 'gain_type must be one of amplitude power or db' ) if limiter_gain is not None : if gain_type in [ 'amplitude' , 'power' ] and gain > 1 : effect_args . append ( '{:f}' . format ( limiter_gain ) ) elif gain_type == 'db' and gain > 0 : effect_args . append ( '{:f}' . format ( limiter_gain ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'vol' ) return self
5824	def add_descriptor ( self , descriptor , role = 'ignore' , group_by_key = False ) : descriptor . validate ( ) if descriptor . key in self . configuration [ "roles" ] : raise ValueError ( "Cannot add a descriptor with the same name twice" ) self . configuration [ 'descriptors' ] . append ( descriptor . as_dict ( ) ) self . configuration [ "roles" ] [ descriptor . key ] = role if group_by_key : self . configuration [ "group_by" ] . append ( descriptor . key )
6709	def check ( self ) : self . _validate_settings ( ) r = self . local_renderer r . env . alias = r . env . aliases [ 0 ] r . sudo ( r . env . check_command_template )
248	def make_transaction_frame ( transactions ) : transaction_list = [ ] for dt in transactions . index : txns = transactions . loc [ dt ] if len ( txns ) == 0 : continue for txn in txns : txn = map_transaction ( txn ) transaction_list . append ( txn ) df = pd . DataFrame ( sorted ( transaction_list , key = lambda x : x [ 'dt' ] ) ) df [ 'txn_dollars' ] = - df [ 'amount' ] * df [ 'price' ] df . index = list ( map ( pd . Timestamp , df . dt . values ) ) return df
9622	def buttons ( self ) : return [ name for name , value in rController . _buttons . items ( ) if self . gamepad . wButtons & value == value ]
875	def copyVarStatesFrom ( self , particleState , varNames ) : # Set this to false if you don't want the variable to move anymore # after we set the state allowedToMove = True for varName in particleState [ 'varStates' ] : if varName in varNames : # If this particle doesn't include this field, don't copy it if varName not in self . permuteVars : continue # Set the best position to the copied position state = copy . deepcopy ( particleState [ 'varStates' ] [ varName ] ) state [ '_position' ] = state [ 'position' ] state [ 'bestPosition' ] = state [ 'position' ] if not allowedToMove : state [ 'velocity' ] = 0 # Set the state now self . permuteVars [ varName ] . setState ( state ) if allowedToMove : # Let the particle move in both directions from the best position # it found previously and set it's initial velocity to a known # fraction of the total distance. self . permuteVars [ varName ] . resetVelocity ( self . _rng )
313	def sharpe_ratio ( returns , risk_free = 0 , period = DAILY ) : return ep . sharpe_ratio ( returns , risk_free = risk_free , period = period )
11289	def strip_xml_namespace ( root ) : try : root . tag = root . tag . split ( '}' ) [ 1 ] except IndexError : pass for element in root . getchildren ( ) : strip_xml_namespace ( element )
13862	def totz ( when , tz = None ) : if when is None : return None when = to_datetime ( when ) if when . tzinfo is None : when = when . replace ( tzinfo = localtz ) return when . astimezone ( tz or utc )
10639	def extract ( self , other ) : # Extract the specified mass flow rate. if type ( other ) is float or type ( other ) is numpy . float64 or type ( other ) is numpy . float32 : return self . _extract_mfr ( other ) # Extract the specified mass flow rateof the specified compound. elif self . _is_compound_mfr_tuple ( other ) : return self . _extract_compound_mfr ( other [ 0 ] , other [ 1 ] ) # Extract all of the specified compound. elif type ( other ) is str : return self . _extract_compound ( other ) # TODO: Test # Extract all of the compounds of the specified material. elif type ( other ) is Material : return self . _extract_material ( other ) # If not one of the above, it must be an invalid argument. else : raise TypeError ( "Invalid extraction argument." )
10371	def build_pmid_exclusion_filter ( pmids : Strings ) -> EdgePredicate : if isinstance ( pmids , str ) : @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : """Fail for edges with PubMed citations matching the contained PubMed identifier. :return: If the edge has a PubMed citation with the contained PubMed identifier """ return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] != pmids elif isinstance ( pmids , Iterable ) : pmids = set ( pmids ) @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : """Pass for edges with PubMed citations matching one of the contained PubMed identifiers. :return: If the edge has a PubMed citation with one of the contained PubMed identifiers """ return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] not in pmids else : raise TypeError return pmid_exclusion_filter
6525	def get_grouped_issues ( self , keyfunc = None , sortby = None ) : if not keyfunc : keyfunc = default_group if not sortby : sortby = self . DEFAULT_SORT self . _ensure_cleaned_issues ( ) return self . _group_issues ( self . _cleaned_issues , keyfunc , sortby )
6348	def _apply_final_rules ( self , phonetic , final_rules , language_arg , strip ) : # optimization to save time if not final_rules : return phonetic # expand the result phonetic = self . _expand_alternates ( phonetic ) phonetic_array = phonetic . split ( '|' ) for k in range ( len ( phonetic_array ) ) : phonetic = phonetic_array [ k ] phonetic2 = '' phoneticx = self . _normalize_lang_attrs ( phonetic , True ) i = 0 while i < len ( phonetic ) : found = False if phonetic [ i ] == '[' : # skip over language attribute attrib_start = i i += 1 while True : if phonetic [ i ] == ']' : i += 1 phonetic2 += phonetic [ attrib_start : i ] break i += 1 continue for rule in final_rules : pattern = rule [ _PATTERN_POS ] pattern_length = len ( pattern ) lcontext = rule [ _LCONTEXT_POS ] rcontext = rule [ _RCONTEXT_POS ] right = '^' + rcontext left = lcontext + '$' # check to see if next sequence in phonetic matches the # string in the rule if ( pattern_length > len ( phoneticx ) - i ) or phoneticx [ i : i + pattern_length ] != pattern : continue # check that right context is satisfied if rcontext != '' : if not search ( right , phoneticx [ i + pattern_length : ] ) : continue # check that left context is satisfied if lcontext != '' : if not search ( left , phoneticx [ : i ] ) : continue # check for incompatible attributes candidate = self . _apply_rule_if_compat ( phonetic2 , rule [ _PHONETIC_POS ] , language_arg ) # The below condition shouldn't ever be false if candidate is not None : # pragma: no branch phonetic2 = candidate found = True break if not found : # character in name for which there is no substitution in # the table phonetic2 += phonetic [ i ] pattern_length = 1 i += pattern_length phonetic_array [ k ] = self . _expand_alternates ( phonetic2 ) phonetic = '|' . join ( phonetic_array ) if strip : phonetic = self . _normalize_lang_attrs ( phonetic , True ) if '|' in phonetic : phonetic = '(' + self . _remove_dupes ( phonetic ) + ')' return phonetic
12465	def read_config ( filename , args ) : # Initial vars config = defaultdict ( dict ) splitter = operator . methodcaller ( 'split' , ' ' ) converters = { __script__ : { 'env' : safe_path , 'pre_requirements' : splitter , } , 'pip' : { 'allow_external' : splitter , 'allow_unverified' : splitter , } } default = copy . deepcopy ( CONFIG ) sections = set ( iterkeys ( default ) ) # Append download-cache for old pip versions if int ( getattr ( pip , '__version__' , '1.x' ) . split ( '.' ) [ 0 ] ) < 6 : default [ 'pip' ] [ 'download_cache' ] = safe_path ( os . path . expanduser ( os . path . join ( '~' , '.{0}' . format ( __script__ ) , 'pip-cache' ) ) ) # Expand user and environ vars in config filename is_default = filename == DEFAULT_CONFIG filename = os . path . expandvars ( os . path . expanduser ( filename ) ) # Read config if it exists on disk if not is_default and not os . path . isfile ( filename ) : print_error ( 'Config file does not exist at {0!r}' . format ( filename ) ) return None parser = ConfigParser ( ) try : parser . read ( filename ) except ConfigParserError : print_error ( 'Cannot parse config file at {0!r}' . format ( filename ) ) return None # Apply config for each possible section for section in sections : if not parser . has_section ( section ) : continue items = parser . items ( section ) # Make auto convert here for integers and boolean values for key , value in items : try : value = int ( value ) except ( TypeError , ValueError ) : try : value = bool ( strtobool ( value ) ) except ValueError : pass if section in converters and key in converters [ section ] : value = converters [ section ] [ key ] ( value ) config [ section ] [ key ] = value # Update config with default values if necessary for section , data in iteritems ( default ) : if section not in config : config [ section ] = data else : for key , value in iteritems ( data ) : config [ section ] . setdefault ( key , value ) # Update bootstrap config from parsed args keys = set ( ( 'env' , 'hook' , 'install_dev_requirements' , 'ignore_activated' , 'pre_requirements' , 'quiet' , 'recreate' , 'requirements' ) ) for key in keys : value = getattr ( args , key ) config [ __script__ ] . setdefault ( key , value ) if key == 'pre_requirements' and not value : continue if value is not None : config [ __script__ ] [ key ] = value return config
8776	def start_api_and_rpc_workers ( self ) : pool = eventlet . GreenPool ( ) quark_rpc = self . serve_rpc ( ) pool . spawn ( quark_rpc . wait ) pool . waitall ( )
10447	def activatewindow ( self , window_name ) : window_handle = self . _get_window_handle ( window_name ) self . _grabfocus ( window_handle ) return 1
12593	def execute_reliabledictionary ( client , application_name , service_name , input_file ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) # call get service with headers and params with open ( input_file ) as json_file : json_data = json . load ( json_file ) service . execute ( json_data ) return
166	def project ( self , from_shape , to_shape ) : coords_proj = project_coords ( self . coords , from_shape , to_shape ) return self . copy ( coords = coords_proj )
1712	def ConstructArray ( self , py_arr ) : arr = self . NewArray ( len ( py_arr ) ) arr . _init ( py_arr ) return arr
3461	def single_gene_deletion ( model , gene_list = None , method = "fba" , solution = None , processes = None , * * kwargs ) : return _multi_deletion ( model , 'gene' , element_lists = _element_lists ( model . genes , gene_list ) , method = method , solution = solution , processes = processes , * * kwargs )
12827	def add_data ( self , data ) : if not self . _data : self . _data = { } self . _data . update ( data )
7157	def assign_prompter ( self , prompter ) : if is_string ( prompter ) : if prompter not in prompters : eprint ( "Error: '{}' is not a core prompter" . format ( prompter ) ) sys . exit ( ) self . prompter = prompters [ prompter ] else : self . prompter = prompter
8104	def update ( self ) : try : self . manager . handle ( self . socket . recv ( 1024 ) ) except socket . error : pass
1159	def wait ( self , timeout = None ) : if not self . _is_owned ( ) : raise RuntimeError ( "cannot wait on un-acquired lock" ) waiter = _allocate_lock ( ) waiter . acquire ( ) self . __waiters . append ( waiter ) saved_state = self . _release_save ( ) try : # restore state no matter what (e.g., KeyboardInterrupt) if timeout is None : waiter . acquire ( ) if __debug__ : self . _note ( "%s.wait(): got it" , self ) else : # Balancing act: We can't afford a pure busy loop, so we # have to sleep; but if we sleep the whole timeout time, # we'll be unresponsive. The scheme here sleeps very # little at first, longer as time goes on, but never longer # than 20 times per second (or the timeout time remaining). endtime = _time ( ) + timeout delay = 0.0005 # 500 us -> initial delay of 1 ms while True : gotit = waiter . acquire ( 0 ) if gotit : break remaining = endtime - _time ( ) if remaining <= 0 : break delay = min ( delay * 2 , remaining , .05 ) _sleep ( delay ) if not gotit : if __debug__ : self . _note ( "%s.wait(%s): timed out" , self , timeout ) try : self . __waiters . remove ( waiter ) except ValueError : pass else : if __debug__ : self . _note ( "%s.wait(%s): got it" , self , timeout ) finally : self . _acquire_restore ( saved_state )
6554	def flip_variable ( self , v ) : try : idx = self . variables . index ( v ) except ValueError : raise ValueError ( "variable {} is not a variable in constraint {}" . format ( v , self . name ) ) if self . vartype is dimod . BINARY : original_func = self . func def func ( * args ) : new_args = list ( args ) new_args [ idx ] = 1 - new_args [ idx ] # negate v return original_func ( * new_args ) self . func = func self . configurations = frozenset ( config [ : idx ] + ( 1 - config [ idx ] , ) + config [ idx + 1 : ] for config in self . configurations ) else : # SPIN original_func = self . func def func ( * args ) : new_args = list ( args ) new_args [ idx ] = - new_args [ idx ] # negate v return original_func ( * new_args ) self . func = func self . configurations = frozenset ( config [ : idx ] + ( - config [ idx ] , ) + config [ idx + 1 : ] for config in self . configurations ) self . name = '{} ({} flipped)' . format ( self . name , v )
2469	def set_file_copyright ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_copytext_set : self . file_copytext_set = True if validations . validate_file_cpyright ( text ) : if isinstance ( text , string_types ) : self . file ( doc ) . copyright = str_from_text ( text ) else : self . file ( doc ) . copyright = text # None or NoAssert return True else : raise SPDXValueError ( 'File::CopyRight' ) else : raise CardinalityError ( 'File::CopyRight' ) else : raise OrderError ( 'File::CopyRight' )
1760	def read_bytes ( self , where , size , force = False ) : result = [ ] for i in range ( size ) : result . append ( Operators . CHR ( self . read_int ( where + i , 8 , force ) ) ) return result
9646	def get_attributes ( var ) : is_valid = partial ( is_valid_in_template , var ) return list ( filter ( is_valid , dir ( var ) ) )
11870	def color_from_rgb ( red , green , blue ) : r = min ( red , 255 ) g = min ( green , 255 ) b = min ( blue , 255 ) if r > 1 or g > 1 or b > 1 : r = r / 255.0 g = g / 255.0 b = b / 255.0 return color_from_hls ( * rgb_to_hls ( r , g , b ) )
7688	def pitch_contour ( annotation , sr = 22050 , length = None , * * kwargs ) : # Map contours to lists of observations times = defaultdict ( list ) freqs = defaultdict ( list ) for obs in annotation : times [ obs . value [ 'index' ] ] . append ( obs . time ) freqs [ obs . value [ 'index' ] ] . append ( obs . value [ 'frequency' ] * ( - 1 ) ** ( ~ obs . value [ 'voiced' ] ) ) y_out = 0.0 for ix in times : y_out = y_out + filter_kwargs ( mir_eval . sonify . pitch_contour , np . asarray ( times [ ix ] ) , np . asarray ( freqs [ ix ] ) , fs = sr , length = length , * * kwargs ) if length is None : length = len ( y_out ) return y_out
1420	def _get_scheduler_location_with_watch ( self , topologyName , callback , isWatching ) : path = self . get_scheduler_location_path ( topologyName ) if isWatching : LOG . info ( "Adding data watch for path: " + path ) # pylint: disable=unused-variable, unused-argument @ self . client . DataWatch ( path ) def watch_scheduler_location ( data , stats ) : """ invoke callback to watch scheduler location """ if data : scheduler_location = SchedulerLocation ( ) scheduler_location . ParseFromString ( data ) callback ( scheduler_location ) else : callback ( None ) # Returning False will result in no future watches # being triggered. If isWatching is True, then # the future watches will be triggered. return isWatching
6647	def baseTargetSpec ( self ) : inherits = self . description . get ( 'inherits' , { } ) if len ( inherits ) == 1 : name , version_req = list ( inherits . items ( ) ) [ 0 ] shrinkwrap_version_req = self . getShrinkwrapMapping ( 'targets' ) . get ( name , None ) if shrinkwrap_version_req is not None : logger . debug ( 'respecting shrinkwrap version %s for %s' , shrinkwrap_version_req , name ) return pack . DependencySpec ( name , version_req , shrinkwrap_version_req = shrinkwrap_version_req ) elif len ( inherits ) > 1 : logger . error ( 'target %s specifies multiple base targets, but only one is allowed' , self . getName ( ) ) return None
13056	def _plugin_endpoint_rename ( fn_name , instance ) : if instance and instance . namespaced : fn_name = "r_{0}_{1}" . format ( instance . name , fn_name [ 2 : ] ) return fn_name
579	def dictDiff ( da , db ) : different = False resultDict = dict ( ) resultDict [ 'inAButNotInB' ] = set ( da ) - set ( db ) if resultDict [ 'inAButNotInB' ] : different = True resultDict [ 'inBButNotInA' ] = set ( db ) - set ( da ) if resultDict [ 'inBButNotInA' ] : different = True resultDict [ 'differentValues' ] = [ ] for key in ( set ( da ) - resultDict [ 'inAButNotInB' ] ) : comparisonResult = da [ key ] == db [ key ] if isinstance ( comparisonResult , bool ) : isEqual = comparisonResult else : # This handles numpy arrays (but only at the top level) isEqual = comparisonResult . all ( ) if not isEqual : resultDict [ 'differentValues' ] . append ( key ) different = True assert ( ( ( resultDict [ 'inAButNotInB' ] or resultDict [ 'inBButNotInA' ] or resultDict [ 'differentValues' ] ) and different ) or not different ) return resultDict if different else None
12745	def pid ( kp = 0. , ki = 0. , kd = 0. , smooth = 0.1 ) : state = dict ( p = 0 , i = 0 , d = 0 ) def control ( error , dt = 1 ) : state [ 'd' ] = smooth * state [ 'd' ] + ( 1 - smooth ) * ( error - state [ 'p' ] ) / dt state [ 'i' ] += error * dt state [ 'p' ] = error return kp * state [ 'p' ] + ki * state [ 'i' ] + kd * state [ 'd' ] return control
8793	def get_all ( self , model ) : tags = { } for name , tag in self . tags . items ( ) : for mtag in model . tags : if tag . is_tag ( mtag ) : tags [ name ] = tag . get ( model ) return tags
4174	def window_kaiser ( N , beta = 8.6 , method = 'numpy' ) : if N == 1 : return ones ( 1 ) if method == 'numpy' : from numpy import kaiser return kaiser ( N , beta ) else : return _kaiser ( N , beta )
192	def SimplexNoiseAlpha ( first = None , second = None , per_channel = False , size_px_max = ( 2 , 16 ) , upscale_method = None , iterations = ( 1 , 3 ) , aggregation_method = "max" , sigmoid = True , sigmoid_thresh = None , name = None , deterministic = False , random_state = None ) : upscale_method_default = iap . Choice ( [ "nearest" , "linear" , "cubic" ] , p = [ 0.05 , 0.6 , 0.35 ] ) sigmoid_thresh_default = iap . Normal ( 0.0 , 5.0 ) noise = iap . SimplexNoise ( size_px_max = size_px_max , upscale_method = upscale_method if upscale_method is not None else upscale_method_default ) if iterations != 1 : noise = iap . IterativeNoiseAggregator ( noise , iterations = iterations , aggregation_method = aggregation_method ) if sigmoid is False or ( ia . is_single_number ( sigmoid ) and sigmoid <= 0.01 ) : noise = iap . Sigmoid . create_for_noise ( noise , threshold = sigmoid_thresh if sigmoid_thresh is not None else sigmoid_thresh_default , activated = sigmoid ) if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return AlphaElementwise ( factor = noise , first = first , second = second , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
9325	def _validate_server ( self ) : if not self . _title : msg = "No 'title' in Server Discovery for request '{}'" raise ValidationError ( msg . format ( self . url ) )
11200	def fromutc ( self , dt ) : if not isinstance ( dt , datetime ) : raise TypeError ( "fromutc() requires a datetime argument" ) if dt . tzinfo is not self : raise ValueError ( "dt.tzinfo is not self" ) # Get transitions - if there are none, fixed offset transitions = self . transitions ( dt . year ) if transitions is None : return dt + self . utcoffset ( dt ) # Get the transition times in UTC dston , dstoff = transitions dston -= self . _std_offset dstoff -= self . _std_offset utc_transitions = ( dston , dstoff ) dt_utc = dt . replace ( tzinfo = None ) isdst = self . _naive_isdst ( dt_utc , utc_transitions ) if isdst : dt_wall = dt + self . _dst_offset else : dt_wall = dt + self . _std_offset _fold = int ( not isdst and self . is_ambiguous ( dt_wall ) ) return enfold ( dt_wall , fold = _fold )
9770	def update ( ctx , name , description , tags ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the job.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . job . update_job ( user , project_name , _job , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Job updated." ) get_job_details ( response )
2627	def show_summary ( self ) : self . get_instance_state ( ) status_string = "EC2 Summary:\n\tVPC IDs: {}\n\tSubnet IDs: \ {}\n\tSecurity Group ID: {}\n\tRunning Instance IDs: {}\n" . format ( self . vpc_id , self . sn_ids , self . sg_id , self . instances ) status_string += "\tInstance States:\n\t\t" self . get_instance_state ( ) for state in self . instance_states . keys ( ) : status_string += "Instance ID: {} State: {}\n\t\t" . format ( state , self . instance_states [ state ] ) status_string += "\n" logger . info ( status_string ) return status_string
7880	def emit_head ( self , stream_from , stream_to , stream_id = None , version = u'1.0' , language = None ) : # pylint: disable-msg=R0913 self . _root_prefixes = dict ( STANDARD_PREFIXES ) self . _root_prefixes [ self . stanza_namespace ] = None for namespace , prefix in self . _root_prefixes . items ( ) : if not prefix or prefix == "stream" : continue if namespace in STANDARD_PREFIXES or namespace in STANZA_NAMESPACES : continue self . _root_prefixes [ namespace ] = prefix tag = u"<{0}:stream version={1}" . format ( STANDARD_PREFIXES [ STREAM_NS ] , quoteattr ( version ) ) if stream_from : tag += u" from={0}" . format ( quoteattr ( stream_from ) ) if stream_to : tag += u" to={0}" . format ( quoteattr ( stream_to ) ) if stream_id is not None : tag += u" id={0}" . format ( quoteattr ( stream_id ) ) if language is not None : tag += u" xml:lang={0}" . format ( quoteattr ( language ) ) for namespace , prefix in self . _root_prefixes . items ( ) : if prefix == "xml" : continue if prefix : tag += u' xmlns:{0}={1}' . format ( prefix , quoteattr ( namespace ) ) else : tag += u' xmlns={1}' . format ( prefix , quoteattr ( namespace ) ) tag += u">" self . _head_emitted = True return tag
9096	def upload_bel_namespace ( self , update : bool = False ) -> Namespace : if not self . is_populated ( ) : self . populate ( ) namespace = self . _get_default_namespace ( ) if namespace is None : log . info ( 'making namespace for %s' , self . _get_namespace_name ( ) ) return self . _make_namespace ( ) if update : self . _update_namespace ( namespace ) return namespace
11693	def label_suspicious ( self , reason ) : self . suspicion_reasons . append ( reason ) self . is_suspect = True
13787	def flush ( self ) : queue = self . queue size = queue . qsize ( ) queue . join ( ) self . log . debug ( 'successfully flushed %s items.' , size )
6352	def _remove_dupes ( self , phonetic ) : alt_string = phonetic alt_array = alt_string . split ( '|' ) result = '|' for i in range ( len ( alt_array ) ) : alt = alt_array [ i ] if alt and '|' + alt + '|' not in result : result += alt + '|' return result [ 1 : - 1 ]
3077	def has_credentials ( self ) : if not self . credentials : return False # Is the access token expired? If so, do we have an refresh token? elif ( self . credentials . access_token_expired and not self . credentials . refresh_token ) : return False else : return True
6240	def render_lights_debug ( self , camera_matrix , projection ) : self . ctx . enable ( moderngl . BLEND ) self . ctx . blend_func = moderngl . SRC_ALPHA , moderngl . ONE_MINUS_SRC_ALPHA for light in self . point_lights : m_mv = matrix44 . multiply ( light . matrix , camera_matrix ) light_size = light . radius self . debug_shader [ "m_proj" ] . write ( projection . tobytes ( ) ) self . debug_shader [ "m_mv" ] . write ( m_mv . astype ( 'f4' ) . tobytes ( ) ) self . debug_shader [ "size" ] . value = light_size self . unit_cube . render ( self . debug_shader , mode = moderngl . LINE_STRIP ) self . ctx . disable ( moderngl . BLEND )
1718	def fix_js_args ( func ) : fcode = six . get_function_code ( func ) fargs = fcode . co_varnames [ fcode . co_argcount - 2 : fcode . co_argcount ] if fargs == ( 'this' , 'arguments' ) or fargs == ( 'arguments' , 'var' ) : return func code = append_arguments ( six . get_function_code ( func ) , ( 'this' , 'arguments' ) ) return types . FunctionType ( code , six . get_function_globals ( func ) , func . __name__ , closure = six . get_function_closure ( func ) )
5478	def get_operation_full_job_id ( op ) : job_id = op . get_field ( 'job-id' ) task_id = op . get_field ( 'task-id' ) if task_id : return '%s.%s' % ( job_id , task_id ) else : return job_id
217	def append ( self , key : str , value : str ) -> None : append_key = key . lower ( ) . encode ( "latin-1" ) append_value = value . encode ( "latin-1" ) self . _list . append ( ( append_key , append_value ) )
5572	def is_valid_with_config ( self , config ) : validate_values ( config , [ ( "schema" , dict ) , ( "path" , str ) ] ) validate_values ( config [ "schema" ] , [ ( "properties" , dict ) , ( "geometry" , str ) ] ) if config [ "schema" ] [ "geometry" ] not in [ "Geometry" , "Point" , "MultiPoint" , "Line" , "MultiLine" , "Polygon" , "MultiPolygon" ] : raise TypeError ( "invalid geometry type" ) return True
12198	def to_cldf ( self , dest , mdname = 'cldf-metadata.json' ) : dest = Path ( dest ) if not dest . exists ( ) : dest . mkdir ( ) data = self . read ( ) if data [ self . source_table_name ] : sources = Sources ( ) for src in data [ self . source_table_name ] : sources . add ( Source ( src [ 'genre' ] , src [ 'id' ] , * * { k : v for k , v in src . items ( ) if k not in [ 'id' , 'genre' ] } ) ) sources . write ( dest / self . dataset . properties . get ( 'dc:source' , 'sources.bib' ) ) for table_type , items in data . items ( ) : try : table = self . dataset [ table_type ] table . common_props [ 'dc:extent' ] = table . write ( [ self . retranslate ( table , item ) for item in items ] , base = dest ) except KeyError : assert table_type == self . source_table_name , table_type return self . dataset . write_metadata ( dest / mdname )
4333	def noisered ( self , profile_path , amount = 0.5 ) : if not os . path . exists ( profile_path ) : raise IOError ( "profile_path {} does not exist." . format ( profile_path ) ) if not is_number ( amount ) or amount < 0 or amount > 1 : raise ValueError ( "amount must be a number between 0 and 1." ) effect_args = [ 'noisered' , profile_path , '{:f}' . format ( amount ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'noisered' ) return self
4866	def save ( self ) : # pylint: disable=arguments-differ enterprise_customer = self . validated_data [ 'enterprise_customer' ] ecu = models . EnterpriseCustomerUser ( user_id = self . user . pk , enterprise_customer = enterprise_customer , ) ecu . save ( )
2169	def get_command ( self , ctx , name ) : # First, attempt to get a basic command from `tower_cli.api.misc`. if name in misc . __all__ : return getattr ( misc , name ) # No command was found; try to get a resource. try : resource = tower_cli . get_resource ( name ) return ResSubcommand ( resource ) except ImportError : pass # Okay, we weren't able to find a command. secho ( 'No such command: %s.' % name , fg = 'red' , bold = True ) sys . exit ( 2 )
1439	def update_sent_packet ( self , sent_pkt_size_bytes ) : self . update_count ( self . SENT_PKT_COUNT ) self . update_count ( self . SENT_PKT_SIZE , incr_by = sent_pkt_size_bytes )
3902	def _input_filter ( self , keys , _ ) : if keys == [ self . _keys [ 'menu' ] ] : if self . _urwid_loop . widget == self . _tabbed_window : self . _show_menu ( ) else : self . _hide_menu ( ) elif keys == [ self . _keys [ 'quit' ] ] : self . _coroutine_queue . put ( self . _client . disconnect ( ) ) else : return keys
13115	def parse_ips ( ips , netmask , include_public ) : hs = HostSearch ( ) rs = RangeSearch ( ) ranges = [ ] ips = list ( set ( ips ) ) included_ips = [ ] print_success ( "Found {} ips" . format ( len ( ips ) ) ) for ip in ips : ip_address = ipaddress . ip_address ( ip ) if include_public or ip_address . is_private : # To stop the screen filling with ranges. if len ( ips ) < 15 : print_success ( "Found ip: {}" . format ( ip ) ) host = hs . id_to_object ( ip ) host . add_tag ( 'dns_discover' ) host . save ( ) r = str ( ipaddress . IPv4Network ( "{}/{}" . format ( ip , netmask ) , strict = False ) ) ranges . append ( r ) included_ips . append ( ip ) else : print_notification ( "Excluding ip {}" . format ( ip ) ) ranges = list ( set ( ranges ) ) print_success ( "Found {} ranges" . format ( len ( ranges ) ) ) for rng in ranges : # To stop the screen filling with ranges. if len ( ranges ) < 15 : print_success ( "Found range: {}" . format ( rng ) ) r = rs . id_to_object ( rng ) r . add_tag ( 'dns_discover' ) r . save ( ) stats = { } stats [ 'ips' ] = included_ips stats [ 'ranges' ] = ranges return stats
4221	def set_keyring ( keyring ) : global _keyring_backend if not isinstance ( keyring , backend . KeyringBackend ) : raise TypeError ( "The keyring must be a subclass of KeyringBackend" ) _keyring_backend = keyring
633	def createSynapse ( self , segment , presynapticCell , permanence ) : idx = len ( segment . _synapses ) synapse = Synapse ( segment , presynapticCell , permanence , self . _nextSynapseOrdinal ) self . _nextSynapseOrdinal += 1 segment . _synapses . add ( synapse ) self . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) self . _numSynapses += 1 return synapse
87	def is_integer_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . integer )
5484	def execute ( api ) : try : return api . execute ( ) except Exception as exception : now = datetime . now ( ) . strftime ( '%Y-%m-%d %H:%M:%S.%f' ) _print_error ( '%s: Exception %s: %s' % ( now , type ( exception ) . __name__ , str ( exception ) ) ) # Re-raise exception to be handled by retry logic raise exception
620	def parseSdr ( s ) : assert isinstance ( s , basestring ) sdr = [ int ( c ) for c in s if c in ( "0" , "1" ) ] if len ( sdr ) != len ( s ) : raise ValueError ( "The provided string %s is malformed. The string should " "have only 0's and 1's." ) return sdr
12346	def compress ( self , delete_tif = False , folder = None ) : return compress ( self . images , delete_tif , folder )
10544	def update_task ( task ) : try : task_id = task . id task = _forbidden_attributes ( task ) res = _pybossa_req ( 'put' , 'task' , task_id , payload = task . data ) if res . get ( 'id' ) : return Task ( res ) else : return res except : # pragma: no cover raise
7765	def disconnect ( self ) : with self . lock : if self . stream : if self . settings [ u"initial_presence" ] : self . send ( Presence ( stanza_type = "unavailable" ) ) self . stream . disconnect ( )
5212	def bdh ( tickers , flds = None , start_date = None , end_date = 'today' , adjust = None , * * kwargs ) -> pd . DataFrame : logger = logs . get_logger ( bdh , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) # Dividend adjustments if isinstance ( adjust , str ) and adjust : if adjust == 'all' : kwargs [ 'CshAdjNormal' ] = True kwargs [ 'CshAdjAbnormal' ] = True kwargs [ 'CapChg' ] = True else : kwargs [ 'CshAdjNormal' ] = 'normal' in adjust or 'dvd' in adjust kwargs [ 'CshAdjAbnormal' ] = 'abn' in adjust or 'dvd' in adjust kwargs [ 'CapChg' ] = 'split' in adjust con , _ = create_connection ( ) elms = assist . proc_elms ( * * kwargs ) ovrds = assist . proc_ovrds ( * * kwargs ) if isinstance ( tickers , str ) : tickers = [ tickers ] if flds is None : flds = [ 'Last_Price' ] if isinstance ( flds , str ) : flds = [ flds ] e_dt = utils . fmt_dt ( end_date , fmt = '%Y%m%d' ) if start_date is None : start_date = pd . Timestamp ( e_dt ) - relativedelta ( months = 3 ) s_dt = utils . fmt_dt ( start_date , fmt = '%Y%m%d' ) logger . info ( f'loading historical data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) logger . debug ( f'\nflds={flds}\nelms={elms}\novrds={ovrds}\nstart_date={s_dt}\nend_date={e_dt}' ) res = con . bdh ( tickers = tickers , flds = flds , elms = elms , ovrds = ovrds , start_date = s_dt , end_date = e_dt ) res . index . name = None if ( len ( flds ) == 1 ) and kwargs . get ( 'keep_one' , False ) : return res . xs ( flds [ 0 ] , axis = 1 , level = 1 ) return res
12044	def where_cross ( data , threshold ) : Is = np . where ( data > threshold ) [ 0 ] Is = np . concatenate ( ( [ 0 ] , Is ) ) Ds = Is [ : - 1 ] - Is [ 1 : ] + 1 return Is [ np . where ( Ds ) [ 0 ] + 1 ]
7660	def validate ( self , strict = True ) : # Get the schema for this annotation ann_schema = schema . namespace_array ( self . namespace ) valid = True try : jsonschema . validate ( self . __json_light__ ( data = False ) , schema . JAMS_SCHEMA ) # validate each record in the frame data_ser = [ serialize_obj ( obs ) for obs in self . data ] jsonschema . validate ( data_ser , ann_schema ) except jsonschema . ValidationError as invalid : if strict : raise SchemaError ( str ( invalid ) ) else : warnings . warn ( str ( invalid ) ) valid = False return valid
10135	def dump ( grids , mode = MODE_ZINC ) : if isinstance ( grids , Grid ) : return dump_grid ( grids , mode = mode ) _dump = functools . partial ( dump_grid , mode = mode ) if mode == MODE_ZINC : return '\n' . join ( map ( _dump , grids ) ) elif mode == MODE_JSON : return '[%s]' % ',' . join ( map ( _dump , grids ) ) else : # pragma: no cover raise NotImplementedError ( 'Format not implemented: %s' % mode )
2905	def _add_child ( self , task_spec , state = MAYBE ) : if task_spec is None : raise ValueError ( self , '_add_child() requires a TaskSpec' ) if self . _is_predicted ( ) and state & self . PREDICTED_MASK == 0 : msg = 'Attempt to add non-predicted child to predicted task' raise WorkflowException ( self . task_spec , msg ) task = Task ( self . workflow , task_spec , self , state = state ) task . thread_id = self . thread_id if state == self . READY : task . _ready ( ) return task
13123	def argparser ( self ) : core_parser = self . core_parser core_parser . add_argument ( '-r' , '--range' , type = str , help = "The range to search for use" ) return core_parser
7313	def process_request ( self , request ) : if not request : return if not db_loaded : load_db ( ) tz = request . session . get ( 'django_timezone' ) if not tz : # use the default timezone (settings.TIME_ZONE) for localhost tz = timezone . get_default_timezone ( ) client_ip = get_ip_address_from_request ( request ) ip_addrs = client_ip . split ( ',' ) for ip in ip_addrs : if is_valid_ip ( ip ) and not is_local_ip ( ip ) : if ':' in ip : tz = db_v6 . time_zone_by_addr ( ip ) break else : tz = db . time_zone_by_addr ( ip ) break if tz : timezone . activate ( tz ) request . session [ 'django_timezone' ] = str ( tz ) if getattr ( settings , 'AUTH_USER_MODEL' , None ) and getattr ( request , 'user' , None ) : detected_timezone . send ( sender = get_user_model ( ) , instance = request . user , timezone = tz ) else : timezone . deactivate ( )
1390	def get_status ( self ) : status = None if self . physical_plan and self . physical_plan . topology : status = self . physical_plan . topology . state if status == 1 : return "Running" elif status == 2 : return "Paused" elif status == 3 : return "Killed" else : return "Unknown"
450	def compute_alpha ( x ) : threshold = _compute_threshold ( x ) alpha1_temp1 = tf . where ( tf . greater ( x , threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha1_temp2 = tf . where ( tf . less ( x , - threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha_array = tf . add ( alpha1_temp1 , alpha1_temp2 , name = None ) alpha_array_abs = tf . abs ( alpha_array ) alpha_array_abs1 = tf . where ( tf . greater ( alpha_array_abs , 0 ) , tf . ones_like ( alpha_array_abs , tf . float32 ) , tf . zeros_like ( alpha_array_abs , tf . float32 ) ) alpha_sum = tf . reduce_sum ( alpha_array_abs ) n = tf . reduce_sum ( alpha_array_abs1 ) alpha = tf . div ( alpha_sum , n ) return alpha
13179	def get_or_default ( func = None , default = None ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : try : return func ( * args , * * kwargs ) except ObjectDoesNotExist : if callable ( default ) : return default ( ) else : return default return wrapper if func is None : return decorator else : return decorator ( func )
5128	def transitions ( self , return_matrix = True ) : if return_matrix : mat = np . zeros ( ( self . nV , self . nV ) ) for v in self . g . nodes ( ) : ind = [ e [ 1 ] for e in sorted ( self . g . out_edges ( v ) ) ] mat [ v , ind ] = self . _route_probs [ v ] else : mat = { k : { e [ 1 ] : p for e , p in zip ( sorted ( self . g . out_edges ( k ) ) , value ) } for k , value in enumerate ( self . _route_probs ) } return mat
11182	def release ( self ) : self . _lock . release ( ) with self . _stat_lock : self . _locked = False self . _last_released = datetime . now ( )
11199	def _validate_fromutc_inputs ( f ) : @ wraps ( f ) def fromutc ( self , dt ) : if not isinstance ( dt , datetime ) : raise TypeError ( "fromutc() requires a datetime argument" ) if dt . tzinfo is not self : raise ValueError ( "dt.tzinfo is not self" ) return f ( self , dt ) return fromutc
4361	def _watcher ( self ) : while True : gevent . sleep ( 1.0 ) if not self . connected : for ns_name , ns in list ( six . iteritems ( self . active_ns ) ) : ns . recv_disconnect ( ) # Killing Socket-level jobs gevent . killall ( self . jobs ) break
11902	def serve_dir ( dir_path ) : # Create index files, and store the list of their paths for cleanup later # This time, force no processing - this gives us a fast first-pass in terms # of page generation, but potentially slow serving for large image files print ( 'Performing first pass index file generation' ) created_files = _create_index_files ( dir_path , True ) if ( PIL_ENABLED ) : # If PIL is enabled, we'd like to process the HTML indexes to include # generated thumbnails - this slows down generation so we don't do it # first time around, but now we're serving it's good to do in the # background print ( 'Performing PIL-enchanced optimised index file generation in background' ) background_indexer = BackgroundIndexFileGenerator ( dir_path ) background_indexer . run ( ) # Run the server in the current location - this blocks until it's stopped _run_server ( ) # Clean up the index files created earlier so we don't make a mess of # the image directories _clean_up ( created_files )
11880	def scanAllProcessesForCwd ( searchPortion , isExactMatch = False ) : pids = getAllRunningPids ( ) cwdResults = [ scanProcessForCwd ( pid , searchPortion , isExactMatch ) for pid in pids ] ret = { } for i in range ( len ( pids ) ) : if cwdResults [ i ] is not None : ret [ pids [ i ] ] = cwdResults [ i ] return ret
2940	def deserialize_condition ( self , workflow , start_node ) : # Collect all information. condition = None spec_name = None for node in start_node . childNodes : if node . nodeType != minidom . Node . ELEMENT_NODE : continue if node . nodeName . lower ( ) == 'successor' : if spec_name is not None : _exc ( 'Duplicate task name %s' % spec_name ) if node . firstChild is None : _exc ( 'Successor tag without a task name' ) spec_name = node . firstChild . nodeValue elif node . nodeName . lower ( ) in _op_map : if condition is not None : _exc ( 'Multiple conditions are not yet supported' ) condition = self . deserialize_logical ( node ) else : _exc ( 'Unknown node: %s' % node . nodeName ) if condition is None : _exc ( 'Missing condition in conditional statement' ) if spec_name is None : _exc ( 'A %s has no task specified' % start_node . nodeName ) return condition , spec_name
5130	def find ( self , s ) : pSet = [ s ] parent = self . _leader [ s ] while parent != self . _leader [ parent ] : pSet . append ( parent ) parent = self . _leader [ parent ] if len ( pSet ) > 1 : for a in pSet : self . _leader [ a ] = parent return parent
3753	def Ceiling ( CASRN , AvailableMethods = False , Method = None ) : # pragma: no cover def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits and ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] or _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] ) : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : if _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] : _Ceiling = ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] , 'ppm' ) elif _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] : _Ceiling = ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] , 'mg/m^3' ) elif Method == NONE : _Ceiling = None else : raise Exception ( 'Failure in in function' ) return _Ceiling
2021	def EXP_gas ( self , base , exponent ) : EXP_SUPPLEMENTAL_GAS = 10 # cost of EXP exponent per byte def nbytes ( e ) : result = 0 for i in range ( 32 ) : result = Operators . ITEBV ( 512 , Operators . EXTRACT ( e , i * 8 , 8 ) != 0 , i + 1 , result ) return result return EXP_SUPPLEMENTAL_GAS * nbytes ( exponent )
6829	def pull ( self , path , use_sudo = False , user = None , force = False ) : if path is None : raise ValueError ( "Path to the working copy is needed to pull from a remote repository." ) options = [ ] if force : options . append ( '--force' ) options = ' ' . join ( options ) cmd = 'git pull %s' % options with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
5057	def build_notification_message ( template_context , template_configuration = None ) : if ( template_configuration is not None and template_configuration . html_template and template_configuration . plaintext_template ) : plain_msg , html_msg = template_configuration . render_all_templates ( template_context ) else : plain_msg = render_to_string ( 'enterprise/emails/user_notification.txt' , template_context ) html_msg = render_to_string ( 'enterprise/emails/user_notification.html' , template_context ) return plain_msg , html_msg
8301	def dispatch ( self , message , source = None ) : msgtype = "" try : if type ( message [ 0 ] ) == str : # got a single message address = message [ 0 ] self . callbacks [ address ] ( message ) elif type ( message [ 0 ] ) == list : for msg in message : self . dispatch ( msg ) except KeyError , key : print 'address %s not found, %s: %s' % ( address , key , message ) pprint . pprint ( message ) except IndexError , e : print '%s: %s' % ( e , message ) pass except None , e : print "Exception in" , address , "callback :" , e return
4668	def encrypt ( privkey , passphrase ) : if isinstance ( privkey , str ) : privkey = PrivateKey ( privkey ) else : privkey = PrivateKey ( repr ( privkey ) ) privkeyhex = repr ( privkey ) # hex addr = format ( privkey . bitcoin . address , "BTC" ) a = _bytes ( addr ) salt = hashlib . sha256 ( hashlib . sha256 ( a ) . digest ( ) ) . digest ( ) [ 0 : 4 ] if SCRYPT_MODULE == "scrypt" : # pragma: no cover key = scrypt . hash ( passphrase , salt , 16384 , 8 , 8 ) elif SCRYPT_MODULE == "pylibscrypt" : # pragma: no cover key = scrypt . scrypt ( bytes ( passphrase , "utf-8" ) , salt , 16384 , 8 , 8 ) else : # pragma: no cover raise ValueError ( "No scrypt module loaded" ) # pragma: no cover ( derived_half1 , derived_half2 ) = ( key [ : 32 ] , key [ 32 : ] ) aes = AES . new ( derived_half2 , AES . MODE_ECB ) encrypted_half1 = _encrypt_xor ( privkeyhex [ : 32 ] , derived_half1 [ : 16 ] , aes ) encrypted_half2 = _encrypt_xor ( privkeyhex [ 32 : ] , derived_half1 [ 16 : ] , aes ) " flag byte is forced 0xc0 because Graphene only uses compressed keys " payload = b"\x01" + b"\x42" + b"\xc0" + salt + encrypted_half1 + encrypted_half2 " Checksum " checksum = hashlib . sha256 ( hashlib . sha256 ( payload ) . digest ( ) ) . digest ( ) [ : 4 ] privatkey = hexlify ( payload + checksum ) . decode ( "ascii" ) return Base58 ( privatkey )
6025	def geometry_from_grid ( self , grid , buffer = 1e-8 ) : y_min = np . min ( grid [ : , 0 ] ) - buffer y_max = np . max ( grid [ : , 0 ] ) + buffer x_min = np . min ( grid [ : , 1 ] ) - buffer x_max = np . max ( grid [ : , 1 ] ) + buffer pixel_scales = ( float ( ( y_max - y_min ) / self . shape [ 0 ] ) , float ( ( x_max - x_min ) / self . shape [ 1 ] ) ) origin = ( ( y_max + y_min ) / 2.0 , ( x_max + x_min ) / 2.0 ) pixel_neighbors , pixel_neighbors_size = self . neighbors_from_pixelization ( ) return self . Geometry ( shape = self . shape , pixel_scales = pixel_scales , origin = origin , pixel_neighbors = pixel_neighbors , pixel_neighbors_size = pixel_neighbors_size )
10715	def sendCommands ( comPort , commands ) : mutex . acquire ( ) try : try : port = serial . Serial ( port = comPort ) header = '11010101 10101010' footer = '10101101' for command in _translateCommands ( commands ) : _sendBinaryData ( port , header + command + footer ) except serial . SerialException : print ( 'Unable to open serial port %s' % comPort ) print ( '' ) raise finally : mutex . release ( )
9843	def __tokenize ( self , string ) : for m in self . dx_regex . finditer ( string . strip ( ) ) : code = m . lastgroup text = m . group ( m . lastgroup ) tok = Token ( code , text ) if not tok . iscode ( 'WHITESPACE' ) : self . tokens . append ( tok )
11026	def sort_pem_objects ( pem_objects ) : keys , certs , ca_certs = [ ] , [ ] , [ ] for pem_object in pem_objects : if isinstance ( pem_object , pem . Key ) : keys . append ( pem_object ) else : # This assumes all pem objects provided are either of type pem.Key # or pem.Certificate. Technically, there are CSR and CRL types, but # we should never be passed those. if _is_ca ( pem_object ) : ca_certs . append ( pem_object ) else : certs . append ( pem_object ) [ key ] , [ cert ] = keys , certs return key , cert , ca_certs
1311	def KeyboardInput ( wVk : int , wScan : int , dwFlags : int = KeyboardEventFlag . KeyDown , time_ : int = 0 ) -> INPUT : return _CreateInput ( KEYBDINPUT ( wVk , wScan , dwFlags , time_ , None ) )
7705	def remove_item ( self , jid ) : if jid not in self . _jids : raise KeyError ( jid ) index = self . _jids [ jid ] for i in range ( index , len ( self . _jids ) ) : self . _jids [ self . _items [ i ] . jid ] -= 1 del self . _jids [ jid ] del self . _items [ index ]
1691	def InnermostClass ( self ) : for i in range ( len ( self . stack ) , 0 , - 1 ) : classinfo = self . stack [ i - 1 ] if isinstance ( classinfo , _ClassInfo ) : return classinfo return None
13693	def register ( self , service , name = '' ) : try : is_model = issubclass ( service , orb . Model ) except StandardError : is_model = False # expose an ORB table dynamically as a service if is_model : self . services [ service . schema ( ) . dbname ( ) ] = ( ModelService , service ) else : super ( OrbApiFactory , self ) . register ( service , name = name )
9463	def conference_deaf ( self , call_params ) : path = '/' + self . api_version + '/ConferenceDeaf/' method = 'POST' return self . request ( path , method , call_params )
13456	def open_s3 ( bucket ) : conn = boto . connect_s3 ( options . paved . s3 . access_id , options . paved . s3 . secret ) try : bucket = conn . get_bucket ( bucket ) except boto . exception . S3ResponseError : bucket = conn . create_bucket ( bucket ) return bucket
971	def _copyAllocatedStates ( self ) : # Get learn states if we need to print them out if self . verbosity > 1 or self . retrieveLearningStates : ( activeT , activeT1 , predT , predT1 ) = self . cells4 . getLearnStates ( ) self . lrnActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) if self . allocateStatesInCPP : assert False ( activeT , activeT1 , predT , predT1 , colConfidenceT , colConfidenceT1 , confidenceT , confidenceT1 ) = self . cells4 . getStates ( ) self . cellConfidence [ 't' ] = confidenceT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . cellConfidence [ 't-1' ] = confidenceT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . colConfidence [ 't' ] = colConfidenceT . reshape ( self . numberOfCols ) self . colConfidence [ 't-1' ] = colConfidenceT1 . reshape ( self . numberOfCols ) self . infActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) )
3129	def get_subscriber_hash ( member_email ) : check_email ( member_email ) member_email = member_email . lower ( ) . encode ( ) m = hashlib . md5 ( member_email ) return m . hexdigest ( )
13120	def argument_count ( self ) : arguments , _ = self . argparser . parse_known_args ( ) return self . count ( * * vars ( arguments ) )
2012	def instruction ( self ) : # FIXME check if pc points to invalid instruction # if self.pc >= len(self.bytecode): # return InvalidOpcode('Code out of range') # if self.pc in self.invalid: # raise InvalidOpcode('Opcode inside a PUSH immediate') try : _decoding_cache = getattr ( self , '_decoding_cache' ) except Exception : _decoding_cache = self . _decoding_cache = { } pc = self . pc if isinstance ( pc , Constant ) : pc = pc . value if pc in _decoding_cache : return _decoding_cache [ pc ] def getcode ( ) : bytecode = self . bytecode for pc_i in range ( pc , len ( bytecode ) ) : yield simplify ( bytecode [ pc_i ] ) . value while True : yield 0 instruction = EVMAsm . disassemble_one ( getcode ( ) , pc = pc , fork = DEFAULT_FORK ) _decoding_cache [ pc ] = instruction return instruction
770	def _getGroundTruth ( self , inferenceElement ) : sensorInputElement = InferenceElement . getInputElement ( inferenceElement ) if sensorInputElement is None : return None return getattr ( self . __currentGroundTruth . sensorInput , sensorInputElement )
11311	def get_record ( self ) : self . recid = self . get_recid ( ) self . remove_controlfields ( ) self . update_system_numbers ( ) self . add_systemnumber ( "Inspire" , recid = self . recid ) self . add_control_number ( "003" , "SzGeCERN" ) self . update_collections ( ) self . update_languages ( ) self . update_reportnumbers ( ) self . update_authors ( ) self . update_journals ( ) self . update_subject_categories ( "INSPIRE" , "SzGeCERN" , "categories_cds" ) self . update_pagenumber ( ) self . update_notes ( ) self . update_experiments ( ) self . update_isbn ( ) self . update_dois ( ) self . update_links_and_ffts ( ) self . update_date ( ) self . update_date_year ( ) self . update_hidden_notes ( ) self . update_oai_info ( ) self . update_cnum ( ) self . update_conference_info ( ) self . fields_list = [ "909" , "541" , "961" , "970" , "690" , "695" , "981" , ] self . strip_fields ( ) if "ANNOUNCEMENT" in self . collections : self . update_conference_111 ( ) self . update_conference_links ( ) record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) if "THESIS" in self . collections : self . update_thesis_information ( ) self . update_thesis_supervisors ( ) if "PROCEEDINGS" in self . collections : # Special proceeding syntax self . update_title_to_proceeding ( ) self . update_author_to_proceeding ( ) record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) # 690 tags if self . tag_as_cern : record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CERN" ) ] ) return self . record
678	def generateRecords ( self , records ) : if self . verbosity > 0 : print 'Generating' , len ( records ) , 'records...' for record in records : self . generateRecord ( record )
9912	def send ( self ) : context = { "verification_url" : app_settings . EMAIL_VERIFICATION_URL . format ( key = self . key ) } email_utils . send_email ( context = context , from_email = settings . DEFAULT_FROM_EMAIL , recipient_list = [ self . email . email ] , subject = _ ( "Please Verify Your Email Address" ) , template_name = "rest_email_auth/emails/verify-email" , ) logger . info ( "Sent confirmation email to %s for user #%d" , self . email . email , self . email . user . id , )
11849	def percept ( self , agent ) : return [ self . thing_percept ( thing , agent ) for thing in self . things_near ( agent . location ) ]
2080	def callback ( self , pk = None , host_config_key = '' , extra_vars = None ) : url = self . endpoint + '%s/callback/' % pk if not host_config_key : host_config_key = client . get ( url ) . json ( ) [ 'host_config_key' ] post_data = { 'host_config_key' : host_config_key } if extra_vars : post_data [ 'extra_vars' ] = parser . process_extra_vars ( list ( extra_vars ) , force_json = True ) r = client . post ( url , data = post_data , auth = None ) if r . status_code == 201 : return { 'changed' : True }
6704	def togroups ( self , user , groups ) : r = self . local_renderer if isinstance ( groups , six . string_types ) : groups = [ _ . strip ( ) for _ in groups . split ( ',' ) if _ . strip ( ) ] for group in groups : r . env . username = user r . env . group = group r . sudo ( 'groupadd --force {group}' ) r . sudo ( 'adduser {username} {group}' )
5931	def scale_dihedrals ( mol , dihedrals , scale , banned_lines = None ) : if banned_lines is None : banned_lines = [ ] new_dihedrals = [ ] for dh in mol . dihedrals : atypes = dh . atom1 . get_atomtype ( ) , dh . atom2 . get_atomtype ( ) , dh . atom3 . get_atomtype ( ) , dh . atom4 . get_atomtype ( ) atypes = [ a . replace ( "_" , "" ) . replace ( "=" , "" ) for a in atypes ] # special-case: this is a [ dihedral ] override in molecule block, continue and don't match if dh . gromacs [ 'param' ] != [ ] : for p in dh . gromacs [ 'param' ] : p [ 'kch' ] *= scale new_dihedrals . append ( dh ) continue for iswitch in range ( 32 ) : if ( iswitch % 2 == 0 ) : a1 = atypes [ 0 ] a2 = atypes [ 1 ] a3 = atypes [ 2 ] a4 = atypes [ 3 ] else : a1 = atypes [ 3 ] a2 = atypes [ 2 ] a3 = atypes [ 1 ] a4 = atypes [ 0 ] if ( ( iswitch // 2 ) % 2 == 1 ) : a1 = "X" if ( ( iswitch // 4 ) % 2 == 1 ) : a2 = "X" if ( ( iswitch // 8 ) % 2 == 1 ) : a3 = "X" if ( ( iswitch // 16 ) % 2 == 1 ) : a4 = "X" key = "{0}-{1}-{2}-{3}-{4}" . format ( a1 , a2 , a3 , a4 , dh . gromacs [ 'func' ] ) if ( key in dihedrals ) : for i , dt in enumerate ( dihedrals [ key ] ) : dhA = copy . deepcopy ( dh ) param = copy . deepcopy ( dt . gromacs [ 'param' ] ) # Only check the first dihedral in a list if not dihedrals [ key ] [ 0 ] . line in banned_lines : for p in param : p [ 'kchi' ] *= scale dhA . gromacs [ 'param' ] = param #if key == "CT3-C-NH1-CT1-9": print i, dt, key if i == 0 : dhA . comment = "; banned lines {0} found={1}\n" . format ( " " . join ( map ( str , banned_lines ) ) , 1 if dt . line in banned_lines else 0 ) dhA . comment += "; parameters for types {}-{}-{}-{}-9 at LINE({})\n" . format ( dhA . atom1 . atomtype , dhA . atom2 . atomtype , dhA . atom3 . atomtype , dhA . atom4 . atomtype , dt . line ) . replace ( "_" , "" ) name = "{}-{}-{}-{}-9" . format ( dhA . atom1 . atomtype , dhA . atom2 . atomtype , dhA . atom3 . atomtype , dhA . atom4 . atomtype ) . replace ( "_" , "" ) #if name == "CL-CTL2-CTL2-HAL2-9": print dihedrals[key], key new_dihedrals . append ( dhA ) break mol . dihedrals = new_dihedrals #assert(len(mol.dihedrals) == new_dihedrals) return mol
8347	def _getAttrMap ( self ) : if not getattr ( self , 'attrMap' ) : self . attrMap = { } for ( key , value ) in self . attrs : self . attrMap [ key ] = value return self . attrMap
11410	def record_move_fields ( rec , tag , field_positions_local , field_position_local = None ) : fields = record_delete_fields ( rec , tag , field_positions_local = field_positions_local ) return record_add_fields ( rec , tag , fields , field_position_local = field_position_local )
13697	def try_read_file ( s ) : try : with open ( s , 'r' ) as f : data = f . read ( ) except FileNotFoundError : # Not a file name. return s except EnvironmentError as ex : print_err ( '\nFailed to read file: {}\n {}' . format ( s , ex ) ) return None return data
8305	def live_source_load ( self , source ) : source = source . rstrip ( '\n' ) if source != self . source : self . source = source b64_source = base64 . b64encode ( bytes ( bytearray ( source , "ascii" ) ) ) self . send_command ( CMD_LOAD_BASE64 , b64_source )
10751	def validate_sceneInfo ( self ) : if self . sceneInfo . prefix not in self . __prefixesValid : raise WrongSceneNameError ( 'AWS: Prefix of %s (%s) is invalid' % ( self . sceneInfo . name , self . sceneInfo . prefix ) )
1099	def _count_leading ( line , ch ) : i , n = 0 , len ( line ) while i < n and line [ i ] == ch : i += 1 return i
12679	def can_send ( self , user , notice_type ) : from notification . models import NoticeSetting return NoticeSetting . for_user ( user , notice_type , self . medium_id ) . send
12404	def bump ( self , bump_reqs = None , * * kwargs ) : bumps = { } for existing_req in sorted ( self . requirements ( ) , key = lambda r : r . project_name ) : if bump_reqs and existing_req . project_name not in bump_reqs : continue bump_reqs . check ( existing_req ) try : bump = self . _bump ( existing_req , bump_reqs . get ( existing_req . project_name ) ) if bump : bumps [ bump . name ] = bump bump_reqs . check ( bump ) except Exception as e : if bump_reqs and bump_reqs . get ( existing_req . project_name ) and all ( r . required_by is None for r in bump_reqs . get ( existing_req . project_name ) ) : raise else : log . warn ( e ) for reqs in bump_reqs . required_requirements ( ) . values ( ) : name = reqs [ 0 ] . project_name if name not in bumps and self . should_add ( name ) : try : bump = self . _bump ( None , reqs ) if bump : bumps [ bump . name ] = bump bump_reqs . check ( bump ) except Exception as e : if all ( r . required_by is None for r in reqs ) : raise else : log . warn ( e ) self . bumps . update ( bumps . values ( ) ) return bumps . values ( )
8707	def exec_file ( self , path ) : filename = os . path . basename ( path ) log . info ( 'Execute %s' , filename ) content = from_file ( path ) . replace ( '\r' , '' ) . split ( '\n' ) res = '> ' for line in content : line = line . rstrip ( '\n' ) retlines = ( res + self . __exchange ( line ) ) . splitlines ( ) # Log all but the last line res = retlines . pop ( ) for lin in retlines : log . info ( lin ) # last line log . info ( res )
9306	def encode_body ( req ) : if isinstance ( req . body , text_type ) : split = req . headers . get ( 'content-type' , 'text/plain' ) . split ( ';' ) if len ( split ) == 2 : ct , cs = split cs = cs . split ( '=' ) [ 1 ] req . body = req . body . encode ( cs ) else : ct = split [ 0 ] if ( ct == 'application/x-www-form-urlencoded' or 'x-amz-' in ct ) : req . body = req . body . encode ( ) else : req . body = req . body . encode ( 'utf-8' ) req . headers [ 'content-type' ] = ct + '; charset=utf-8'
9140	def find_best_label_for_type ( labels , language , labeltype ) : typelabels = [ l for l in labels if l . type == labeltype ] if not typelabels : return False if language == 'any' : return typelabels [ 0 ] exact = filter_labels_by_language ( typelabels , language ) if exact : return exact [ 0 ] inexact = filter_labels_by_language ( typelabels , language , True ) if inexact : return inexact [ 0 ] return False
12352	def rebuild ( self , image , wait = True ) : return self . _action ( 'rebuild' , image = image , wait = wait )
4849	def _serialize_items ( self , channel_metadata_items ) : return json . dumps ( self . _prepare_items_for_transmission ( channel_metadata_items ) , sort_keys = True ) . encode ( 'utf-8' )
2988	def ensure_iterable ( inst ) : if isinstance ( inst , string_types ) : return [ inst ] elif not isinstance ( inst , collections . Iterable ) : return [ inst ] else : return inst
7774	def _compute_response ( urp_hash , nonce , cnonce , nonce_count , authzid , digest_uri ) : # pylint: disable-msg=C0103,R0913 logger . debug ( "_compute_response{0!r}" . format ( ( urp_hash , nonce , cnonce , nonce_count , authzid , digest_uri ) ) ) if authzid : a1 = b":" . join ( ( urp_hash , nonce , cnonce , authzid ) ) else : a1 = b":" . join ( ( urp_hash , nonce , cnonce ) ) a2 = b"AUTHENTICATE:" + digest_uri return b2a_hex ( _kd_value ( b2a_hex ( _h_value ( a1 ) ) , b":" . join ( ( nonce , nonce_count , cnonce , b"auth" , b2a_hex ( _h_value ( a2 ) ) ) ) ) )
4455	def apply ( self , * * kwexpr ) : for alias , expr in kwexpr . items ( ) : self . _projections . append ( [ alias , expr ] ) return self
2281	def to_csv ( self , fname_radical , * * kwargs ) : if self . data is not None : self . data . to_csv ( fname_radical + '_data.csv' , index = False , * * kwargs ) pd . DataFrame ( self . adjacency_matrix ) . to_csv ( fname_radical + '_target.csv' , index = False , * * kwargs ) else : raise ValueError ( "Graph has not yet been generated. \ Use self.generate() to do so." )
3831	async def search_entities ( self , search_entities_request ) : response = hangouts_pb2 . SearchEntitiesResponse ( ) await self . _pb_request ( 'contacts/searchentities' , search_entities_request , response ) return response
9636	def _getCallingContext ( ) : frames = inspect . stack ( ) if len ( frames ) > 4 : context = frames [ 5 ] else : context = frames [ 0 ] modname = context [ 1 ] lineno = context [ 2 ] if context [ 3 ] : funcname = context [ 3 ] else : funcname = "" # python docs say you don't want references to # frames lying around. Bad things can happen. del context del frames return modname , funcname , lineno
10342	def overlay_data ( graph : BELGraph , data : Mapping [ BaseEntity , Any ] , label : Optional [ str ] = None , overwrite : bool = False , ) -> None : if label is None : label = WEIGHT for node , value in data . items ( ) : if node not in graph : log . debug ( '%s not in graph' , node ) continue if label in graph . nodes [ node ] and not overwrite : log . debug ( '%s already on %s' , label , node ) continue graph . nodes [ node ] [ label ] = value
9926	def handle ( self , * args , * * kwargs ) : cutoff = timezone . now ( ) cutoff -= app_settings . CONFIRMATION_EXPIRATION cutoff -= app_settings . CONFIRMATION_SAVE_PERIOD queryset = models . EmailConfirmation . objects . filter ( created_at__lte = cutoff ) count = queryset . count ( ) queryset . delete ( ) if count : self . stdout . write ( self . style . SUCCESS ( "Removed {count} old email confirmation(s)" . format ( count = count ) ) ) else : self . stdout . write ( "No email confirmations to remove." )
11595	def _rc_renamenx ( self , src , dst ) : if self . exists ( dst ) : return False return self . _rc_rename ( src , dst )
12579	def mask_and_flatten ( self ) : self . _check_for_mask ( ) return self . get_data ( smoothed = True , masked = True , safe_copy = False ) [ self . get_mask_indices ( ) ] , self . get_mask_indices ( ) , self . mask . shape
12991	def create_hierarchy ( hierarchy , level ) : if level not in hierarchy : hierarchy [ level ] = OrderedDict ( ) return hierarchy [ level ]
13051	def import_nmap ( result , tag , check_function = all_hosts , import_services = False ) : host_search = HostSearch ( arguments = False ) service_search = ServiceSearch ( ) parser = NmapParser ( ) report = parser . parse_fromstring ( result ) imported_hosts = 0 imported_services = 0 for nmap_host in report . hosts : if check_function ( nmap_host ) : imported_hosts += 1 host = host_search . id_to_object ( nmap_host . address ) host . status = nmap_host . status host . add_tag ( tag ) if nmap_host . os_fingerprinted : host . os = nmap_host . os_fingerprint if nmap_host . hostnames : host . hostname . extend ( nmap_host . hostnames ) if import_services : for service in nmap_host . services : imported_services += 1 serv = Service ( * * service . get_dict ( ) ) serv . address = nmap_host . address service_id = service_search . object_to_id ( serv ) if service_id : # Existing object, save the banner and script results. serv_old = Service . get ( service_id ) if service . banner : serv_old . banner = service . banner # TODO implement # if service.script_results: # serv_old.script_results.extend(service.script_results) serv_old . save ( ) else : # New object serv . address = nmap_host . address serv . save ( ) if service . state == 'open' : host . open_ports . append ( service . port ) if service . state == 'closed' : host . closed_ports . append ( service . port ) if service . state == 'filtered' : host . filtered_ports . append ( service . port ) host . save ( ) if imported_hosts : print_success ( "Imported {} hosts, with tag {}" . format ( imported_hosts , tag ) ) else : print_error ( "No hosts found" ) return { 'hosts' : imported_hosts , 'services' : imported_services }
9324	def refresh_collections ( self , accept = MEDIA_TYPE_TAXII_V20 ) : url = self . url + "collections/" response = self . _conn . get ( url , headers = { "Accept" : accept } ) self . _collections = [ ] for item in response . get ( "collections" , [ ] ) : # optional collection_url = url + item [ "id" ] + "/" collection = Collection ( collection_url , conn = self . _conn , collection_info = item ) self . _collections . append ( collection ) self . _loaded_collections = True
972	def _setStatePointers ( self ) : if not self . allocateStatesInCPP : self . cells4 . setStatePointers ( self . infActiveState [ "t" ] , self . infActiveState [ "t-1" ] , self . infPredictedState [ "t" ] , self . infPredictedState [ "t-1" ] , self . colConfidence [ "t" ] , self . colConfidence [ "t-1" ] , self . cellConfidence [ "t" ] , self . cellConfidence [ "t-1" ] )
3404	def normalize_cutoff ( model , zero_cutoff = None ) : if zero_cutoff is None : return model . tolerance else : if zero_cutoff < model . tolerance : raise ValueError ( "The chosen zero cutoff cannot be less than the model's " "tolerance value." ) else : return zero_cutoff
5744	def run_splitted_processing ( max_simultaneous_processes , process_name , filenames ) : pids = [ ] while len ( filenames ) > 0 : while len ( filenames ) > 0 and len ( pids ) < max_simultaneous_processes : filename = filenames . pop ( ) pids . append ( service_start ( service = process_name , param = [ '-f' , filename , '-d' , imported_day ] ) ) while len ( pids ) == max_simultaneous_processes : time . sleep ( sleep_timer ) pids = update_running_pids ( pids ) while len ( pids ) > 0 : # Wait until all the processes are finished time . sleep ( sleep_timer ) pids = update_running_pids ( pids )
2076	def main ( loader , name ) : scores = [ ] raw_scores_ds = { } # first get the dataset X , y , mapping = loader ( ) clf = linear_model . LogisticRegression ( solver = 'lbfgs' , multi_class = 'auto' , max_iter = 200 , random_state = 0 ) # try each encoding method available, which works on multiclass problems encoders = ( set ( category_encoders . __all__ ) - { 'WOEEncoder' } ) # WoE is currently only for binary targets for encoder_name in encoders : encoder = getattr ( category_encoders , encoder_name ) start_time = time . time ( ) score , stds , raw_scores , dim = score_models ( clf , X , y , encoder ) scores . append ( [ encoder_name , name , dim , score , stds , time . time ( ) - start_time ] ) raw_scores_ds [ encoder_name ] = raw_scores gc . collect ( ) results = pd . DataFrame ( scores , columns = [ 'Encoding' , 'Dataset' , 'Dimensionality' , 'Avg. Score' , 'Score StDev' , 'Elapsed Time' ] ) raw = pd . DataFrame . from_dict ( raw_scores_ds ) ax = raw . plot ( kind = 'box' , return_type = 'axes' ) plt . title ( 'Scores for Encodings on %s Dataset' % ( name , ) ) plt . ylabel ( 'Score (higher is better)' ) for tick in ax . get_xticklabels ( ) : tick . set_rotation ( 90 ) plt . grid ( ) plt . tight_layout ( ) plt . show ( ) return results , raw
7065	def delete_ec2_nodes ( instance_id_list , client = None ) : if not client : client = boto3 . client ( 'ec2' ) resp = client . terminate_instances ( InstanceIds = instance_id_list ) return resp
11698	def _unwrap_stream ( uri , timeout , scanner , requests_session ) : original_uri = uri seen_uris = set ( ) deadline = time . time ( ) + timeout while time . time ( ) < deadline : if uri in seen_uris : logger . info ( 'Unwrapping stream from URI (%s) failed: ' 'playlist referenced itself' , uri ) return None else : seen_uris . add ( uri ) logger . debug ( 'Unwrapping stream from URI: %s' , uri ) try : scan_timeout = deadline - time . time ( ) if scan_timeout < 0 : logger . info ( 'Unwrapping stream from URI (%s) failed: ' 'timed out in %sms' , uri , timeout ) return None scan_result = scanner . scan ( uri , timeout = scan_timeout ) except exceptions . ScannerError as exc : logger . debug ( 'GStreamer failed scanning URI (%s): %s' , uri , exc ) scan_result = None if scan_result is not None and not ( scan_result . mime . startswith ( 'text/' ) or scan_result . mime . startswith ( 'application/' ) ) : logger . debug ( 'Unwrapped potential %s stream: %s' , scan_result . mime , uri ) return uri download_timeout = deadline - time . time ( ) if download_timeout < 0 : logger . info ( 'Unwrapping stream from URI (%s) failed: timed out in %sms' , uri , timeout ) return None content = http . download ( requests_session , uri , timeout = download_timeout ) if content is None : logger . info ( 'Unwrapping stream from URI (%s) failed: ' 'error downloading URI %s' , original_uri , uri ) return None uris = playlists . parse ( content ) if not uris : logger . debug ( 'Failed parsing URI (%s) as playlist; found potential stream.' , uri ) return uri # TODO Test streams and return first that seems to be playable logger . debug ( 'Parsed playlist (%s) and found new URI: %s' , uri , uris [ 0 ] ) uri = uris [ 0 ]
12011	def extractFile ( self , filename ) : files = [ x for x in self . tableOfContents if x [ 'filename' ] == filename ] if len ( files ) == 0 : raise FileNotFoundException ( ) fileRecord = files [ 0 ] # got here? need to fetch the file size metaheadroom = 1024 # should be enough request = urllib2 . Request ( self . zipURI ) start = fileRecord [ 'filestart' ] end = fileRecord [ 'filestart' ] + fileRecord [ 'compressedsize' ] + metaheadroom request . headers [ 'Range' ] = "bytes=%s-%s" % ( start , end ) handle = urllib2 . urlopen ( request ) # make sure the response is ranged return_range = handle . headers . get ( 'Content-Range' ) if return_range != "bytes %d-%d/%s" % ( start , end , self . filesize ) : raise Exception ( "Ranged requests are not supported for this URI" ) filedata = handle . read ( ) # find start of raw file data zip_n = unpack ( "H" , filedata [ 26 : 28 ] ) [ 0 ] zip_m = unpack ( "H" , filedata [ 28 : 30 ] ) [ 0 ] # check compressed size has_data_descriptor = bool ( unpack ( "H" , filedata [ 6 : 8 ] ) [ 0 ] & 8 ) comp_size = unpack ( "I" , filedata [ 18 : 22 ] ) [ 0 ] if comp_size == 0 and has_data_descriptor : # assume compressed size in the Central Directory is correct comp_size = fileRecord [ 'compressedsize' ] elif comp_size != fileRecord [ 'compressedsize' ] : raise Exception ( "Something went wrong. Directory and file header disagree of compressed file size" ) raw_zip_data = filedata [ 30 + zip_n + zip_m : 30 + zip_n + zip_m + comp_size ] uncompressed_data = "" # can't decompress if stored without compression compression_method = unpack ( "H" , filedata [ 8 : 10 ] ) [ 0 ] if compression_method == 0 : return raw_zip_data dec = zlib . decompressobj ( - zlib . MAX_WBITS ) for chunk in raw_zip_data : rv = dec . decompress ( chunk ) if rv : uncompressed_data = uncompressed_data + rv return uncompressed_data
12429	def create_virtualenv ( self ) : if check_command ( 'virtualenv' ) : ve_dir = os . path . join ( self . _ve_dir , self . _project_name ) if os . path . exists ( ve_dir ) : if self . _force : logging . warn ( 'Removing existing virtualenv' ) shutil . rmtree ( ve_dir ) else : logging . warn ( 'Found existing virtualenv; not creating (use --force to overwrite)' ) return logging . info ( 'Creating virtualenv' ) p = subprocess . Popen ( 'virtualenv --no-site-packages {0} > /dev/null' . format ( ve_dir ) , shell = True ) os . waitpid ( p . pid , 0 ) # install modules for m in self . _modules : self . log . info ( 'Installing module {0}' . format ( m ) ) p = subprocess . Popen ( '{0} install {1} > /dev/null' . format ( os . path . join ( self . _ve_dir , self . _project_name ) + os . sep + 'bin' + os . sep + 'pip' , m ) , shell = True ) os . waitpid ( p . pid , 0 )
7970	def _add_timeout_handler ( self , handler ) : self . timeout_handlers . append ( handler ) if self . event_thread is None : return self . _run_timeout_threads ( handler )
2966	def _sm_to_pain ( self , * args , * * kwargs ) : _logger . info ( "Starting chaos for blockade %s" % self . _blockade_name ) self . _do_blockade_event ( ) # start the timer to end the pain millisec = random . randint ( self . _run_min_time , self . _run_max_time ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
901	def mmPrettyPrintMetrics ( metrics , sigFigs = 5 ) : assert len ( metrics ) > 0 , "No metrics found" table = PrettyTable ( [ "Metric" , "mean" , "standard deviation" , "min" , "max" , "sum" , ] ) for metric in metrics : table . add_row ( [ metric . prettyPrintTitle ( ) ] + metric . getStats ( ) ) return table . get_string ( ) . encode ( "utf-8" )
4889	def update_course ( self , course , enterprise_customer , enterprise_context ) : course [ 'course_runs' ] = self . update_course_runs ( course_runs = course . get ( 'course_runs' ) or [ ] , enterprise_customer = enterprise_customer , enterprise_context = enterprise_context , ) # Update marketing urls in course metadata to include enterprise related info (i.e. our global context). marketing_url = course . get ( 'marketing_url' ) if marketing_url : query_parameters = dict ( enterprise_context , * * utils . get_enterprise_utm_context ( enterprise_customer ) ) course . update ( { 'marketing_url' : utils . update_query_parameters ( marketing_url , query_parameters ) } ) # Finally, add context to the course as a whole. course . update ( enterprise_context ) return course
7432	def _write_nex ( self , mdict , nlocus ) : ## create matrix as a string max_name_len = max ( [ len ( i ) for i in mdict ] ) namestring = "{:<" + str ( max_name_len + 1 ) + "} {}\n" matrix = "" for i in mdict . items ( ) : matrix += namestring . format ( i [ 0 ] , i [ 1 ] ) ## ensure dir minidir = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) if not os . path . exists ( minidir ) : os . makedirs ( minidir ) ## write nexus block handle = os . path . join ( minidir , "{}.nex" . format ( nlocus ) ) with open ( handle , 'w' ) as outnex : outnex . write ( NEXBLOCK . format ( * * { "ntax" : len ( mdict ) , "nchar" : len ( mdict . values ( ) [ 0 ] ) , "matrix" : matrix , "ngen" : self . params . mb_mcmc_ngen , "sfreq" : self . params . mb_mcmc_sample_freq , "burnin" : self . params . mb_mcmc_burnin , } ) )
2423	def set_doc_version ( self , doc , value ) : if not self . doc_version_set : self . doc_version_set = True m = self . VERS_STR_REGEX . match ( value ) if m is None : raise SPDXValueError ( 'Document::Version' ) else : doc . version = version . Version ( major = int ( m . group ( 1 ) ) , minor = int ( m . group ( 2 ) ) ) return True else : raise CardinalityError ( 'Document::Version' )
10307	def calculate_global_tanimoto_set_distances ( dict_of_sets : Mapping [ X , Set ] ) -> Mapping [ X , Mapping [ X , float ] ] : universe = set ( itt . chain . from_iterable ( dict_of_sets . values ( ) ) ) universe_size = len ( universe ) result : Dict [ X , Dict [ X , float ] ] = defaultdict ( dict ) for x , y in itt . combinations ( dict_of_sets , 2 ) : result [ x ] [ y ] = result [ y ] [ x ] = 1.0 - len ( dict_of_sets [ x ] | dict_of_sets [ y ] ) / universe_size for x in dict_of_sets : result [ x ] [ x ] = 1.0 - len ( x ) / universe_size return dict ( result )
5847	def get_credentials_from_file ( filepath ) : try : creds = load_file_as_yaml ( filepath ) except Exception : creds = { } profile_name = os . environ . get ( citr_env_vars . CITRINATION_PROFILE ) if profile_name is None or len ( profile_name ) == 0 : profile_name = DEFAULT_CITRINATION_PROFILE api_key = None site = None try : profile = creds [ profile_name ] api_key = profile [ CREDENTIALS_API_KEY_KEY ] site = profile [ CREDENTIALS_SITE_KEY ] except KeyError : pass return ( api_key , site )
3274	def get_user_info ( self ) : if self . value in ERROR_DESCRIPTIONS : s = "{}" . format ( ERROR_DESCRIPTIONS [ self . value ] ) else : s = "{}" . format ( self . value ) if self . context_info : s += ": {}" . format ( self . context_info ) elif self . value in ERROR_RESPONSES : s += ": {}" . format ( ERROR_RESPONSES [ self . value ] ) if self . src_exception : s += "\n Source exception: '{}'" . format ( self . src_exception ) if self . err_condition : s += "\n Error condition: '{}'" . format ( self . err_condition ) return s
4879	def get_paginated_response ( data , request ) : url = urlparse ( request . build_absolute_uri ( ) ) . _replace ( query = None ) . geturl ( ) next_page = None previous_page = None if data [ 'next' ] : next_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'next' ] ) . query , ) next_page = next_page . rstrip ( '?' ) if data [ 'previous' ] : previous_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'previous' ] or "" ) . query , ) previous_page = previous_page . rstrip ( '?' ) return Response ( OrderedDict ( [ ( 'count' , data [ 'count' ] ) , ( 'next' , next_page ) , ( 'previous' , previous_page ) , ( 'results' , data [ 'results' ] ) ] ) )
5857	def get_available_columns ( self , dataset_ids ) : if not isinstance ( dataset_ids , list ) : dataset_ids = [ dataset_ids ] data = { "dataset_ids" : dataset_ids } failure_message = "Failed to get available columns in dataset(s) {}" . format ( dataset_ids ) return self . _get_success_json ( self . _post_json ( 'v1/datasets/get-available-columns' , data , failure_message = failure_message ) ) [ 'data' ]
8330	def findAllNext ( self , name = None , attrs = { } , text = None , limit = None , * * kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextGenerator , * * kwargs )
3907	def _on_event ( self , conv_event ) : conv = self . _conv_list . get ( conv_event . conversation_id ) user = conv . get_user ( conv_event . user_id ) show_notification = all ( ( isinstance ( conv_event , hangups . ChatMessageEvent ) , not user . is_self , not conv . is_quiet , ) ) if show_notification : self . add_conversation_tab ( conv_event . conversation_id ) if self . _discreet_notifications : notification = DISCREET_NOTIFICATION else : notification = notifier . Notification ( user . full_name , get_conv_name ( conv ) , conv_event . text ) self . _notifier . send ( notification )
1723	def translate_file ( input_path , output_path ) : js = get_file_contents ( input_path ) py_code = translate_js ( js ) lib_name = os . path . basename ( output_path ) . split ( '.' ) [ 0 ] head = '__all__ = [%s]\n\n# Don\'t look below, you will not understand this Python code :) I don\'t.\n\n' % repr ( lib_name ) tail = '\n\n# Add lib to the module scope\n%s = var.to_python()' % lib_name out = head + py_code + tail write_file_contents ( output_path , out )
9162	def processor ( ) : # pragma: no cover registry = get_current_registry ( ) settings = registry . settings connection_string = settings [ CONNECTION_STRING ] channels = _get_channels ( settings ) # Code adapted from # http://initd.org/psycopg/docs/advanced.html#asynchronous-notifications with psycopg2 . connect ( connection_string ) as conn : conn . set_isolation_level ( ISOLATION_LEVEL_AUTOCOMMIT ) with conn . cursor ( ) as cursor : for channel in channels : cursor . execute ( 'LISTEN {}' . format ( channel ) ) logger . debug ( 'Waiting for notifications on channel "{}"' . format ( channel ) ) registry . notify ( ChannelProcessingStartUpEvent ( ) ) rlist = [ conn ] # wait until ready for reading wlist = [ ] # wait until ready for writing xlist = [ ] # wait for an "exceptional condition" timeout = 5 while True : if select . select ( rlist , wlist , xlist , timeout ) != ( [ ] , [ ] , [ ] ) : conn . poll ( ) while conn . notifies : notif = conn . notifies . pop ( 0 ) logger . debug ( 'Got NOTIFY: pid={} channel={} payload={}' . format ( notif . pid , notif . channel , notif . payload ) ) event = create_pg_notify_event ( notif ) try : registry . notify ( event ) except Exception : logger . exception ( 'Logging an uncaught exception' )
4456	def limit ( self , offset , num ) : limit = Limit ( offset , num ) if self . _groups : self . _groups [ - 1 ] . limit = limit else : self . _limit = limit return self
9217	def input ( self , file ) : if isinstance ( file , string_types ) : with open ( file ) as f : self . lexer . input ( f . read ( ) ) else : self . lexer . input ( file . read ( ) )
990	def scale ( reader , writer , column , start , stop , multiple ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( multiple ) ( row [ column ] ) * multiple writer . appendRecord ( row )
9311	def amz_cano_querystring ( qs ) : safe_qs_amz_chars = '&=+' safe_qs_unresvd = '-_.~' # If Python 2, switch to working entirely in str # as quote() has problems with Unicode if PY2 : qs = qs . encode ( 'utf-8' ) safe_qs_amz_chars = safe_qs_amz_chars . encode ( ) safe_qs_unresvd = safe_qs_unresvd . encode ( ) qs = unquote ( qs ) space = b' ' if PY2 else ' ' qs = qs . split ( space ) [ 0 ] qs = quote ( qs , safe = safe_qs_amz_chars ) qs_items = { } for name , vals in parse_qs ( qs , keep_blank_values = True ) . items ( ) : name = quote ( name , safe = safe_qs_unresvd ) vals = [ quote ( val , safe = safe_qs_unresvd ) for val in vals ] qs_items [ name ] = vals qs_strings = [ ] for name , vals in qs_items . items ( ) : for val in vals : qs_strings . append ( '=' . join ( [ name , val ] ) ) qs = '&' . join ( sorted ( qs_strings ) ) if PY2 : qs = unicode ( qs ) return qs
4928	def transform_image ( self , content_metadata_item ) : image_url = '' if content_metadata_item [ 'content_type' ] in [ 'course' , 'program' ] : image_url = content_metadata_item . get ( 'card_image_url' ) elif content_metadata_item [ 'content_type' ] == 'courserun' : image_url = content_metadata_item . get ( 'image_url' ) return image_url
2272	def _win32_is_junction ( path ) : if not exists ( path ) : if os . path . isdir ( path ) : if not os . path . islink ( path ) : return True return False return jwfs . is_reparse_point ( path ) and not os . path . islink ( path )
2649	def monitor ( pid , task_id , monitoring_hub_url , run_id , sleep_dur = 10 ) : import psutil radio = UDPRadio ( monitoring_hub_url , source_id = task_id ) # these values are simple to log. Other information is available in special formats such as memory below. simple = [ "cpu_num" , 'cpu_percent' , 'create_time' , 'cwd' , 'exe' , 'memory_percent' , 'nice' , 'name' , 'num_threads' , 'pid' , 'ppid' , 'status' , 'username' ] # values that can be summed up to see total resources used by task process and its children summable_values = [ 'cpu_percent' , 'memory_percent' , 'num_threads' ] pm = psutil . Process ( pid ) pm . cpu_percent ( ) first_msg = True while True : try : d = { "psutil_process_" + str ( k ) : v for k , v in pm . as_dict ( ) . items ( ) if k in simple } d [ "run_id" ] = run_id d [ "task_id" ] = task_id d [ 'resource_monitoring_interval' ] = sleep_dur d [ 'first_msg' ] = first_msg d [ 'timestamp' ] = datetime . datetime . now ( ) children = pm . children ( recursive = True ) d [ "psutil_cpu_count" ] = psutil . cpu_count ( ) d [ 'psutil_process_memory_virtual' ] = pm . memory_info ( ) . vms d [ 'psutil_process_memory_resident' ] = pm . memory_info ( ) . rss d [ 'psutil_process_time_user' ] = pm . cpu_times ( ) . user d [ 'psutil_process_time_system' ] = pm . cpu_times ( ) . system d [ 'psutil_process_children_count' ] = len ( children ) try : d [ 'psutil_process_disk_write' ] = pm . io_counters ( ) . write_bytes d [ 'psutil_process_disk_read' ] = pm . io_counters ( ) . read_bytes except psutil . _exceptions . AccessDenied : # occassionally pid temp files that hold this information are unvailable to be read so set to zero d [ 'psutil_process_disk_write' ] = 0 d [ 'psutil_process_disk_read' ] = 0 for child in children : for k , v in child . as_dict ( attrs = summable_values ) . items ( ) : d [ 'psutil_process_' + str ( k ) ] += v d [ 'psutil_process_time_user' ] += child . cpu_times ( ) . user d [ 'psutil_process_time_system' ] += child . cpu_times ( ) . system d [ 'psutil_process_memory_virtual' ] += child . memory_info ( ) . vms d [ 'psutil_process_memory_resident' ] += child . memory_info ( ) . rss try : d [ 'psutil_process_disk_write' ] += child . io_counters ( ) . write_bytes d [ 'psutil_process_disk_read' ] += child . io_counters ( ) . read_bytes except psutil . _exceptions . AccessDenied : # occassionally pid temp files that hold this information are unvailable to be read so add zero d [ 'psutil_process_disk_write' ] += 0 d [ 'psutil_process_disk_read' ] += 0 finally : radio . send ( MessageType . TASK_INFO , task_id , d ) time . sleep ( sleep_dur ) first_msg = False
5941	def transform_args ( self , * args , * * kwargs ) : newargs = self . _combineargs ( * args , * * kwargs ) return self . _build_arg_list ( * * newargs )
6781	def get_current_thumbprint ( self , components = None ) : components = str_to_component_list ( components ) if self . verbose : print ( 'deploy.get_current_thumbprint.components:' , components ) manifest_data = { } # {component:data} for component_name , func in sorted ( manifest_recorder . items ( ) ) : self . vprint ( 'Checking thumbprint for component %s...' % component_name ) manifest_key = assert_valid_satchel ( component_name ) service_name = clean_service_name ( component_name ) if service_name not in self . genv . services : self . vprint ( 'Skipping unused component:' , component_name ) continue elif components and service_name not in components : self . vprint ( 'Skipping non-matching component:' , component_name ) continue try : self . vprint ( 'Retrieving manifest for %s...' % component_name ) manifest_data [ manifest_key ] = func ( ) if self . verbose : pprint ( manifest_data [ manifest_key ] , indent = 4 ) except exceptions . AbortDeployment as e : raise return manifest_data
6843	def set_permissions ( self ) : r = self . local_renderer for path in r . env . paths_owned : r . env . path_owned = path r . sudo ( 'chown {celery_daemon_user}:{celery_daemon_user} {celery_path_owned}' )
2524	def get_review_date ( self , r_term ) : reviewed_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'reviewDate' ] , None ) ) ) if len ( reviewed_list ) != 1 : self . error = True msg = 'Review must have exactlyone review date' self . logger . log ( msg ) return return six . text_type ( reviewed_list [ 0 ] [ 2 ] )
11625	def generate ( grammar = None , num = 1 , output = sys . stdout , max_recursion = 10 , seed = None ) : if seed is not None : gramfuzz . rand . seed ( seed ) fuzzer = gramfuzz . GramFuzzer ( ) fuzzer . load_grammar ( grammar ) cat_group = os . path . basename ( grammar ) . replace ( ".py" , "" ) results = fuzzer . gen ( cat_group = cat_group , num = num , max_recursion = max_recursion ) for res in results : output . write ( res )
12094	def proto_VC_50_MT_IV ( abf = exampleABF ) : swhlab . memtest . memtest ( abf ) #do membrane test on every sweep swhlab . memtest . checkSweep ( abf ) #see all MT values swhlab . plot . save ( abf , tag = '02-check' , resize = False ) av1 , sd1 = swhlab . plot . IV ( abf , 1.2 , 1.4 , True , 'b' ) swhlab . plot . save ( abf , tag = 'iv' ) Xs = abf . clampValues ( 1.2 ) #generate IV clamp values abf . saveThing ( [ Xs , av1 ] , '01_iv' )
9544	def add_value_check ( self , field_name , value_check , code = VALUE_CHECK_FAILED , message = MESSAGES [ VALUE_CHECK_FAILED ] , modulus = 1 ) : # guard conditions assert field_name in self . _field_names , 'unexpected field name: %s' % field_name assert callable ( value_check ) , 'value check must be a callable function' t = field_name , value_check , code , message , modulus self . _value_checks . append ( t )
13788	def open ( name = None , fileobj = None , closefd = True ) : return Guesser ( ) . open ( name = name , fileobj = fileobj , closefd = closefd )
5821	def version ( self ) : ver = Version ( ) ver . conn = self . conn ver . attrs = { # Parent params 'service_id' : self . attrs [ 'id' ] , } ver . save ( ) return ver
7854	def add_identity ( self , item_name , item_category = None , item_type = None ) : return DiscoIdentity ( self , item_name , item_category , item_type )
7993	def _restart_stream ( self ) : self . _input_state = "restart" self . _output_state = "restart" self . features = None self . transport . restart ( ) if self . initiator : self . _send_stream_start ( self . stream_id )
7299	def get_qset ( self , queryset , q ) : if self . mongoadmin . search_fields and q : params = { } for field in self . mongoadmin . search_fields : if field == 'id' : # check to make sure this is a valid ID, otherwise we just continue if is_valid_object_id ( q ) : return queryset . filter ( pk = q ) continue search_key = "{field}__icontains" . format ( field = field ) params [ search_key ] = q queryset = queryset . filter ( * * params ) return queryset
8828	def sg_gather_associated_ports ( context , group ) : if not group : return None if not hasattr ( group , "ports" ) or len ( group . ports ) <= 0 : return [ ] return group . ports
10343	def overlay_type_data ( graph : BELGraph , data : Mapping [ str , float ] , func : str , namespace : str , label : Optional [ str ] = None , overwrite : bool = False , impute : Optional [ float ] = None , ) -> None : new_data = { node : data . get ( node [ NAME ] , impute ) for node in filter_nodes ( graph , function_namespace_inclusion_builder ( func , namespace ) ) } overlay_data ( graph , new_data , label = label , overwrite = overwrite )
3344	def read_timeout_value_header ( timeoutvalue ) : timeoutsecs = 0 timeoutvaluelist = timeoutvalue . split ( "," ) for timeoutspec in timeoutvaluelist : timeoutspec = timeoutspec . strip ( ) if timeoutspec . lower ( ) == "infinite" : return - 1 else : listSR = reSecondsReader . findall ( timeoutspec ) for secs in listSR : timeoutsecs = int ( secs ) if timeoutsecs > MAX_FINITE_TIMEOUT_LIMIT : return - 1 if timeoutsecs != 0 : return timeoutsecs return None
3454	def add_SBO ( model ) : for r in model . reactions : # don't annotate already annotated reactions if r . annotation . get ( "sbo" ) : continue # only doing exchanges if len ( r . metabolites ) != 1 : continue met_id = list ( r . _metabolites ) [ 0 ] . id if r . id . startswith ( "EX_" ) and r . id == "EX_" + met_id : r . annotation [ "sbo" ] = "SBO:0000627" elif r . id . startswith ( "DM_" ) and r . id == "DM_" + met_id : r . annotation [ "sbo" ] = "SBO:0000628"
7426	def refmap_stats ( data , sample ) : ## shorter names mapf = os . path . join ( data . dirs . refmapping , sample . name + "-mapped-sorted.bam" ) umapf = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) ## get from unmapped cmd1 = [ ipyrad . bins . samtools , "flagstat" , umapf ] proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) result1 = proc1 . communicate ( ) [ 0 ] ## get from mapped cmd2 = [ ipyrad . bins . samtools , "flagstat" , mapf ] proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE ) result2 = proc2 . communicate ( ) [ 0 ] ## store results ## If PE, samtools reports the _actual_ number of reads mapped, both ## R1 and R2, so here if PE divide the results by 2 to stay consistent ## with how we've been reporting R1 and R2 as one "read pair" if "pair" in data . paramsdict [ "datatype" ] : sample . stats [ "refseq_unmapped_reads" ] = int ( result1 . split ( ) [ 0 ] ) / 2 sample . stats [ "refseq_mapped_reads" ] = int ( result2 . split ( ) [ 0 ] ) / 2 else : sample . stats [ "refseq_unmapped_reads" ] = int ( result1 . split ( ) [ 0 ] ) sample . stats [ "refseq_mapped_reads" ] = int ( result2 . split ( ) [ 0 ] ) sample_cleanup ( data , sample )
8674	def export_keys ( output_path , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Exporting stash to {0}...' . format ( output_path ) ) stash . export ( output_path = output_path ) click . echo ( 'Export complete!' ) except GhostError as ex : sys . exit ( ex )
1529	def establish_ssh_tunnel ( self ) : localportlist = [ ] for ( host , port ) in self . hostportlist : localport = self . pick_unused_port ( ) self . tunnel . append ( subprocess . Popen ( ( 'ssh' , self . tunnelhost , '-NL127.0.0.1:%d:%s:%d' % ( localport , host , port ) ) ) ) localportlist . append ( ( '127.0.0.1' , localport ) ) return localportlist
5770	def rsa_pkcs1v15_verify ( certificate_or_public_key , signature , data , hash_algorithm ) : if certificate_or_public_key . algorithm != 'rsa' : raise ValueError ( 'The key specified is not an RSA public key' ) return _verify ( certificate_or_public_key , signature , data , hash_algorithm )
4249	def netspeed_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . netspeed_by_addr ( addr )
5848	def get_preferred_credentials ( api_key , site , cred_file = DEFAULT_CITRINATION_CREDENTIALS_FILE ) : profile_api_key , profile_site = get_credentials_from_file ( cred_file ) if api_key is None : api_key = os . environ . get ( citr_env_vars . CITRINATION_API_KEY ) if api_key is None or len ( api_key ) == 0 : api_key = profile_api_key if site is None : site = os . environ . get ( citr_env_vars . CITRINATION_SITE ) if site is None or len ( site ) == 0 : site = profile_site if site is None : site = "https://citrination.com" return api_key , site
5024	def get_enterprise_customer ( uuid ) : if uuid is None : return None try : return EnterpriseCustomer . active_customers . get ( uuid = uuid ) except EnterpriseCustomer . DoesNotExist : raise CommandError ( _ ( 'Enterprise customer {uuid} not found, or not active' ) . format ( uuid = uuid ) )
3813	async def upload_image ( self , image_file , filename = None , * , return_uploaded_image = False ) : image_filename = filename or os . path . basename ( image_file . name ) image_data = image_file . read ( ) # request an upload URL res = await self . _base_request ( IMAGE_UPLOAD_URL , 'application/x-www-form-urlencoded;charset=UTF-8' , 'json' , json . dumps ( { "protocolVersion" : "0.8" , "createSessionRequest" : { "fields" : [ { "external" : { "name" : "file" , "filename" : image_filename , "put" : { } , "size" : len ( image_data ) } } ] } } ) ) try : upload_url = self . _get_upload_session_status ( res ) [ 'externalFieldTransfers' ] [ 0 ] [ 'putInfo' ] [ 'url' ] except KeyError : raise exceptions . NetworkError ( 'image upload failed: can not acquire an upload url' ) # upload the image data using the upload_url to get the upload info res = await self . _base_request ( upload_url , 'application/octet-stream' , 'json' , image_data ) try : raw_info = ( self . _get_upload_session_status ( res ) [ 'additionalInfo' ] [ 'uploader_service.GoogleRupioAdditionalInfo' ] [ 'completionInfo' ] [ 'customerSpecificInfo' ] ) image_id = raw_info [ 'photoid' ] url = raw_info [ 'url' ] except KeyError : raise exceptions . NetworkError ( 'image upload failed: can not fetch upload info' ) result = UploadedImage ( image_id = image_id , url = url ) return result if return_uploaded_image else result . image_id
12203	def _compile ( self , parselet_node , level = 0 ) : if self . DEBUG : debug_offset = "" . join ( [ " " for x in range ( level ) ] ) if self . DEBUG : print ( debug_offset , "%s::compile(%s)" % ( self . __class__ . __name__ , parselet_node ) ) if isinstance ( parselet_node , dict ) : parselet_tree = ParsleyNode ( ) for k , v in list ( parselet_node . items ( ) ) : # we parse the key raw elements but without much # interpretation (which is done by the SelectorHandler) try : m = self . REGEX_PARSELET_KEY . match ( k ) if not m : if self . DEBUG : print ( debug_offset , "could not parse key" , k ) raise InvalidKeySyntax ( k ) except : raise InvalidKeySyntax ( "Key %s is not valid" % k ) key = m . group ( 'key' ) # by default, fields are required key_required = True operator = m . group ( 'operator' ) if operator == '?' : key_required = False # FIXME: "!" operator not supported (complete array) scope = m . group ( 'scope' ) # example: get list of H3 tags # { "titles": ["h3"] } # FIXME: should we support multiple selectors in list? # e.g. { "titles": ["h1", "h2", "h3", "h4"] } if isinstance ( v , ( list , tuple ) ) : v = v [ 0 ] iterate = True else : iterate = False # keys in the abstract Parsley trees are of type `ParsleyContext` try : parsley_context = ParsleyContext ( key , operator = operator , required = key_required , scope = self . selector_handler . make ( scope ) if scope else None , iterate = iterate ) except SyntaxError : if self . DEBUG : print ( "Invalid scope:" , k , scope ) raise if self . DEBUG : print ( debug_offset , "current context:" , parsley_context ) # go deeper in the Parsley tree... try : child_tree = self . _compile ( v , level = level + 1 ) except SyntaxError : if self . DEBUG : print ( "Invalid value: " , v ) raise except : raise if self . DEBUG : print ( debug_offset , "child tree:" , child_tree ) parselet_tree [ parsley_context ] = child_tree return parselet_tree # a string leaf should match some kind of selector, # let the selector handler deal with it elif isstr ( parselet_node ) : return self . selector_handler . make ( parselet_node ) else : raise ValueError ( "Unsupported type(%s) for Parselet node <%s>" % ( type ( parselet_node ) , parselet_node ) )
7512	def padnames ( names ) : ## get longest name longname_len = max ( len ( i ) for i in names ) ## Padding distance between name and seq. padding = 5 ## add pad to names pnames = [ name + " " * ( longname_len - len ( name ) + padding ) for name in names ] snppad = "//" + " " * ( longname_len - 2 + padding ) return np . array ( pnames ) , snppad
8369	def create_canvas ( src , format = None , outputfile = None , multifile = False , buff = None , window = False , title = None , fullscreen = None , show_vars = False ) : from core import CairoCanvas , CairoImageSink # https://github.com/shoebot/shoebot/issues/206 if outputfile : sink = CairoImageSink ( outputfile , format , multifile , buff ) elif window or show_vars : from gui import ShoebotWindow if not title : if src and os . path . isfile ( src ) : title = os . path . splitext ( os . path . basename ( src ) ) [ 0 ] + ' - Shoebot' else : title = 'Untitled - Shoebot' sink = ShoebotWindow ( title , show_vars , fullscreen = fullscreen ) else : if src and isinstance ( src , cairo . Surface ) : outputfile = src format = 'surface' elif src and os . path . isfile ( src ) : outputfile = os . path . splitext ( os . path . basename ( src ) ) [ 0 ] + '.' + ( format or 'svg' ) else : outputfile = 'output.svg' sink = CairoImageSink ( outputfile , format , multifile , buff ) canvas = CairoCanvas ( sink ) return canvas
726	def addNoise ( self , bits , amount ) : newBits = set ( ) for bit in bits : if self . _random . getReal64 ( ) < amount : newBits . add ( self . _random . getUInt32 ( self . _n ) ) else : newBits . add ( bit ) return newBits
8094	def edges ( s , edges , alpha = 1.0 , weighted = False , directed = False ) : p = s . _ctx . BezierPath ( ) if directed and s . stroke : pd = s . _ctx . BezierPath ( ) if weighted and s . fill : pw = [ s . _ctx . BezierPath ( ) for i in range ( 11 ) ] # Draw the edges in a single BezierPath for speed. # Weighted edges are divided into ten BezierPaths, # depending on their weight rounded between 0 and 10. if len ( edges ) == 0 : return for e in edges : try : s2 = e . node1 . graph . styles [ e . node1 . style ] except : s2 = s if s2 . edge : s2 . edge ( s2 , p , e , alpha ) if directed and s . stroke : s2 . edge_arrow ( s2 , pd , e , radius = 10 ) if weighted and s . fill : s2 . edge ( s2 , pw [ int ( e . weight * 10 ) ] , e , alpha ) s . _ctx . autoclosepath ( False ) s . _ctx . nofill ( ) s . _ctx . nostroke ( ) # All weighted edges use the default fill. if weighted and s . fill : r = e . node1 . __class__ ( None ) . r s . _ctx . stroke ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * 0.65 * alpha ) for w in range ( 1 , len ( pw ) ) : s . _ctx . strokewidth ( r * w * 0.1 ) s . _ctx . drawpath ( pw [ w ] . copy ( ) ) # All edges use the default stroke. if s . stroke : s . _ctx . strokewidth ( s . strokewidth ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * 0.65 * alpha ) s . _ctx . drawpath ( p . copy ( ) ) if directed and s . stroke : #clr = s._ctx.stroke().copy() clr = s . _ctx . color ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * 0.65 * alpha ) clr . a *= 1.3 s . _ctx . stroke ( clr ) s . _ctx . drawpath ( pd . copy ( ) ) for e in edges : try : s2 = self . styles [ e . node1 . style ] except : s2 = s if s2 . edge_label : s2 . edge_label ( s2 , e , alpha )
2097	def cancel ( self , pk = None , fail_if_not_running = False , * * kwargs ) : # Search for the record if pk not given if not pk : existing_data = self . get ( * * kwargs ) pk = existing_data [ 'id' ] cancel_endpoint = '%s%s/cancel/' % ( self . endpoint , pk ) # Attempt to cancel the job. try : client . post ( cancel_endpoint ) changed = True except exc . MethodNotAllowed : changed = False if fail_if_not_running : raise exc . TowerCLIError ( 'Job not running.' ) # Return a success. return { 'status' : 'canceled' , 'changed' : changed }
9569	def build_message ( self , data ) : if not data : return None return Message ( id = data [ 'message' ] [ 'mid' ] , platform = self . platform , text = data [ 'message' ] [ 'text' ] , user = data [ 'sender' ] [ 'id' ] , timestamp = data [ 'timestamp' ] , raw = data , chat = None , # TODO: Refactor build_messages and Message class )
12758	def labels ( self ) : return sorted ( self . channels , key = lambda c : self . channels [ c ] )
4504	def put_edit ( self , f , * args , * * kwds ) : self . put_nowait ( functools . partial ( f , * args , * * kwds ) )
2585	def migrate_tasks_to_internal ( self , kill_event ) : logger . info ( "[TASK_PULL_THREAD] Starting" ) task_counter = 0 poller = zmq . Poller ( ) poller . register ( self . task_incoming , zmq . POLLIN ) while not kill_event . is_set ( ) : try : msg = self . task_incoming . recv_pyobj ( ) except zmq . Again : # We just timed out while attempting to receive logger . debug ( "[TASK_PULL_THREAD] {} tasks in internal queue" . format ( self . pending_task_queue . qsize ( ) ) ) continue if msg == 'STOP' : kill_event . set ( ) break else : self . pending_task_queue . put ( msg ) task_counter += 1 logger . debug ( "[TASK_PULL_THREAD] Fetched task:{}" . format ( task_counter ) )
9118	def _add_admin ( self , app , * * kwargs ) : from flask_admin import Admin from flask_admin . contrib . sqla import ModelView admin = Admin ( app , * * kwargs ) for flask_admin_model in self . flask_admin_models : if isinstance ( flask_admin_model , tuple ) : # assume its a 2 tuple if len ( flask_admin_model ) != 2 : raise TypeError model , view = flask_admin_model admin . add_view ( view ( model , self . session ) ) else : admin . add_view ( ModelView ( flask_admin_model , self . session ) ) return admin
3685	def set_from_PT ( self , Vs ) : # All roots will have some imaginary component; ignore them if > 1E-9 good_roots = [ ] bad_roots = [ ] for i in Vs : j = i . real if abs ( i . imag ) > 1E-9 or j < 0 : bad_roots . append ( i ) else : good_roots . append ( j ) if len ( bad_roots ) == 2 : V = good_roots [ 0 ] self . phase = self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) if self . phase == 'l' : self . V_l = V else : self . V_g = V else : # Even in the case of three real roots, it is still the min/max that make sense self . V_l , self . V_g = min ( good_roots ) , max ( good_roots ) [ self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) for V in [ self . V_l , self . V_g ] ] self . phase = 'l/g'
10932	def get_termination_stats ( self , get_cos = True ) : delta_vals = self . _last_vals - self . param_vals delta_err = self . _last_error - self . error frac_err = delta_err / self . error to_return = { 'delta_vals' : delta_vals , 'delta_err' : delta_err , 'num_iter' : 1 * self . _num_iter , 'frac_err' : frac_err , 'error' : self . error , 'exp_err' : self . _exp_err } if get_cos : model_cosine = self . calc_model_cosine ( ) to_return . update ( { 'model_cosine' : model_cosine } ) return to_return
8486	def get ( self , name , default , allow_default = True ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) if name not in self . settings : if not allow_default : raise LookupError ( 'No setting "{name}"' . format ( name = name ) ) self . settings [ name ] = default return self . settings [ name ]
12587	def all_childnodes_to_nifti1img ( h5group ) : child_nodes = [ ] def append_parent_if_dataset ( name , obj ) : if isinstance ( obj , h5py . Dataset ) : if name . split ( '/' ) [ - 1 ] == 'data' : child_nodes . append ( obj . parent ) vols = [ ] h5group . visititems ( append_parent_if_dataset ) for c in child_nodes : vols . append ( hdfgroup_to_nifti1image ( c ) ) return vols
4807	def create_char_dataframe ( words ) : char_dict = [ ] for word in words : for i , char in enumerate ( word ) : if i == 0 : char_dict . append ( { 'char' : char , 'type' : CHAR_TYPE_FLATTEN . get ( char , 'o' ) , 'target' : True } ) else : char_dict . append ( { 'char' : char , 'type' : CHAR_TYPE_FLATTEN . get ( char , 'o' ) , 'target' : False } ) return pd . DataFrame ( char_dict )
4317	def info ( filepath ) : info_dictionary = { 'channels' : channels ( filepath ) , 'sample_rate' : sample_rate ( filepath ) , 'bitrate' : bitrate ( filepath ) , 'duration' : duration ( filepath ) , 'num_samples' : num_samples ( filepath ) , 'encoding' : encoding ( filepath ) , 'silent' : silent ( filepath ) } return info_dictionary
9609	def find_exception_by_code ( code ) : errorName = None for error in WebDriverError : if error . value . code == code : errorName = error break return errorName
8719	def backup ( self , path ) : log . info ( 'Backing up in ' + path ) # List file to backup files = self . file_list ( ) # then download each of then self . prepare ( ) for f in files : self . read_file ( f [ 0 ] , os . path . join ( path , f [ 0 ] ) )
521	def _initPermanence ( self , potential , connectedPct ) : # Determine which inputs bits will start out as connected # to the inputs. Initially a subset of the input bits in a # column's potential pool will be connected. This number is # given by the parameter "connectedPct" perm = numpy . zeros ( self . _numInputs , dtype = realDType ) for i in xrange ( self . _numInputs ) : if ( potential [ i ] < 1 ) : continue if ( self . _random . getReal64 ( ) <= connectedPct ) : perm [ i ] = self . _initPermConnected ( ) else : perm [ i ] = self . _initPermNonConnected ( ) # Clip off low values. Since we use a sparse representation # to store the permanence values this helps reduce memory # requirements. perm [ perm < self . _synPermTrimThreshold ] = 0 return perm
3696	def Watson ( T , Hvap_ref , T_Ref , Tc , exponent = 0.38 ) : Tr = T / Tc Trefr = T_Ref / Tc H2 = Hvap_ref * ( ( 1 - Tr ) / ( 1 - Trefr ) ) ** exponent return H2
12872	def one_of ( these ) : ch = peek ( ) try : if ( ch is EndOfFile ) or ( ch not in these ) : fail ( list ( these ) ) except TypeError : if ch != these : fail ( [ these ] ) next ( ) return ch
10495	def clickMouseButtonRightWithMods ( self , coord , modifiers ) : modFlags = self . _pressModifiers ( modifiers ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonRight , modFlags ) self . _releaseModifiers ( modifiers ) self . _postQueuedEvents ( )
4501	def project ( * descs , root_file = None ) : load . ROOT_FILE = root_file desc = merge . merge ( merge . DEFAULT_PROJECT , * descs ) path = desc . get ( 'path' , '' ) if root_file : project_path = os . path . dirname ( root_file ) if path : path += ':' + project_path else : path = project_path with load . extender ( path ) : desc = recurse . recurse ( desc ) project = construct . construct ( * * desc ) project . desc = desc return project
1564	def get_metrics_collector ( self ) : if self . metrics_collector is None or not isinstance ( self . metrics_collector , MetricsCollector ) : raise RuntimeError ( "Metrics collector is not registered in this context" ) return self . metrics_collector
2877	def one ( nodes , or_none = False ) : if not nodes and or_none : return None assert len ( nodes ) == 1 , 'Expected 1 result. Received %d results.' % ( len ( nodes ) ) return nodes [ 0 ]
1586	def _handle_state_change_msg ( self , new_helper ) : assert self . my_pplan_helper is not None assert self . my_instance is not None and self . my_instance . py_class is not None if self . my_pplan_helper . get_topology_state ( ) != new_helper . get_topology_state ( ) : # handle state change # update the pplan_helper self . my_pplan_helper = new_helper if new_helper . is_topology_running ( ) : if not self . is_instance_started : self . start_instance_if_possible ( ) self . my_instance . py_class . invoke_activate ( ) elif new_helper . is_topology_paused ( ) : self . my_instance . py_class . invoke_deactivate ( ) else : raise RuntimeError ( "Unexpected TopologyState update: %s" % new_helper . get_topology_state ( ) ) else : Log . info ( "Topology state remains the same." )
7605	def get_clan ( self , tag : crtag , timeout : int = None ) : url = self . api . CLAN + '/' + tag return self . _get_model ( url , FullClan , timeout = timeout )
11170	def read_docs ( self , docsfiles ) : updates = DocParser ( ) for docsfile in _list ( docsfiles ) : if os . path . isfile ( docsfile ) : updates . parse ( docsfile ) self . docs . update ( ( k , _docs ( updates [ k ] , self . docvars ) ) for k in self . docs if updates . blocks [ k ] ) for name , text in updates [ 'parameters' ] . items ( ) : if name in self : self . getparam ( name ) . docs = text [ 0 ] % self . docvars elif name not in self . ignore : raise ValueError ( "parameter %r does not exist" % name )
4299	def create_project ( config_data ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) kwargs = { } args = [ ] if config_data . template : kwargs [ 'template' ] = config_data . template args . append ( config_data . project_name ) if config_data . project_directory : args . append ( config_data . project_directory ) if not os . path . exists ( config_data . project_directory ) : os . makedirs ( config_data . project_directory ) base_cmd = 'django-admin.py' start_cmds = [ os . path . join ( os . path . dirname ( sys . executable ) , base_cmd ) ] start_cmd_pnodes = [ 'Scripts' ] start_cmds . extend ( [ os . path . join ( os . path . dirname ( sys . executable ) , pnode , base_cmd ) for pnode in start_cmd_pnodes ] ) start_cmd = [ base_cmd ] for p in start_cmds : if os . path . exists ( p ) : start_cmd = [ sys . executable , p ] break cmd_args = start_cmd + [ 'startproject' ] + args if config_data . verbose : sys . stdout . write ( 'Project creation command: {0}\n' . format ( ' ' . join ( cmd_args ) ) ) try : output = subprocess . check_output ( cmd_args , stderr = subprocess . STDOUT ) sys . stdout . write ( output . decode ( 'utf-8' ) ) except subprocess . CalledProcessError as e : # pragma: no cover if config_data . verbose : sys . stdout . write ( e . output . decode ( 'utf-8' ) ) raise
8046	def parse_definitions ( self , class_ , all = False ) : while self . current is not None : self . log . debug ( "parsing definition list, current token is %r (%s)" , self . current . kind , self . current . value , ) self . log . debug ( "got_newline: %s" , self . stream . got_logical_newline ) if all and self . current . value == "__all__" : self . parse_all ( ) elif ( self . current . kind == tk . OP and self . current . value == "@" and self . stream . got_logical_newline ) : self . consume ( tk . OP ) self . parse_decorators ( ) elif self . current . value in [ "def" , "class" ] : yield self . parse_definition ( class_ . _nest ( self . current . value ) ) elif self . current . kind == tk . INDENT : self . consume ( tk . INDENT ) for definition in self . parse_definitions ( class_ ) : yield definition elif self . current . kind == tk . DEDENT : self . consume ( tk . DEDENT ) return elif self . current . value == "from" : self . parse_from_import_statement ( ) else : self . stream . move ( )
851	def rewind ( self ) : # Superclass rewind super ( FileRecordStream , self ) . rewind ( ) self . close ( ) self . _file = open ( self . _filename , self . _mode ) self . _reader = csv . reader ( self . _file , dialect = "excel" ) # Skip header rows self . _reader . next ( ) self . _reader . next ( ) self . _reader . next ( ) # Reset record count, etc. self . _recordCount = 0
11844	def run ( self , steps = 1000 ) : for step in range ( steps ) : if self . is_done ( ) : return self . step ( )
8093	def node_label ( s , node , alpha = 1.0 ) : if s . text : #s._ctx.lineheight(1) s . _ctx . font ( s . font ) s . _ctx . fontsize ( s . fontsize ) s . _ctx . nostroke ( ) s . _ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha ) # Cache an outlined label text and translate it. # This enhances the speed and avoids wiggling text. try : p = node . _textpath except : txt = node . label try : txt = unicode ( txt ) except : try : txt = txt . decode ( "utf-8" ) except : pass # Abbreviation. #root = node.graph.root #if txt != root and txt[-len(root):] == root: # txt = txt[:len(txt)-len(root)]+root[0]+"." dx , dy = 0 , 0 if s . align == 2 : #CENTER dx = - s . _ctx . textwidth ( txt , s . textwidth ) / 2 dy = s . _ctx . textheight ( txt ) / 2 node . _textpath = s . _ctx . textpath ( txt , dx , dy , width = s . textwidth ) p = node . _textpath if s . depth : try : __colors . shadow ( dx = 2 , dy = 4 , blur = 5 , alpha = 0.3 * alpha ) except : pass s . _ctx . push ( ) s . _ctx . translate ( node . x , node . y ) s . _ctx . scale ( alpha ) s . _ctx . drawpath ( p . copy ( ) ) s . _ctx . pop ( )
9121	def dropbox_submission ( dropbox , request ) : try : data = dropbox_schema . deserialize ( request . POST ) except Exception : return HTTPFound ( location = request . route_url ( 'dropbox_form' ) ) # set the message dropbox . message = data . get ( 'message' ) # recognize submission from watchdog if 'testing_secret' in dropbox . settings : dropbox . from_watchdog = is_equal ( unicode ( dropbox . settings [ 'test_submission_secret' ] ) , data . pop ( 'testing_secret' , u'' ) ) # a non-js client might have uploaded an attachment via the form's fileupload field: if data . get ( 'upload' ) is not None : dropbox . add_attachment ( data [ 'upload' ] ) # now we can call the process method dropbox . submit ( ) drop_url = request . route_url ( 'dropbox_view' , drop_id = dropbox . drop_id ) print ( "Created dropbox %s" % drop_url ) return HTTPFound ( location = drop_url )
9020	def _connect_rows ( self , connections ) : for connection in connections : from_row_id = self . _to_id ( connection [ FROM ] [ ID ] ) from_row = self . _id_cache [ from_row_id ] from_row_start_index = connection [ FROM ] . get ( START , DEFAULT_START ) from_row_number_of_possible_meshes = from_row . number_of_produced_meshes - from_row_start_index to_row_id = self . _to_id ( connection [ TO ] [ ID ] ) to_row = self . _id_cache [ to_row_id ] to_row_start_index = connection [ TO ] . get ( START , DEFAULT_START ) to_row_number_of_possible_meshes = to_row . number_of_consumed_meshes - to_row_start_index meshes = min ( from_row_number_of_possible_meshes , to_row_number_of_possible_meshes ) # TODO: test all kinds of connections number_of_meshes = connection . get ( MESHES , meshes ) from_row_stop_index = from_row_start_index + number_of_meshes to_row_stop_index = to_row_start_index + number_of_meshes assert 0 <= from_row_start_index <= from_row_stop_index produced_meshes = from_row . produced_meshes [ from_row_start_index : from_row_stop_index ] assert 0 <= to_row_start_index <= to_row_stop_index consumed_meshes = to_row . consumed_meshes [ to_row_start_index : to_row_stop_index ] assert len ( produced_meshes ) == len ( consumed_meshes ) mesh_pairs = zip ( produced_meshes , consumed_meshes ) for produced_mesh , consumed_mesh in mesh_pairs : produced_mesh . connect_to ( consumed_mesh )
6061	def convolve_mapping_matrix ( self , mapping_matrix ) : return self . convolve_matrix_jit ( mapping_matrix , self . image_frame_indexes , self . image_frame_psfs , self . image_frame_lengths )
6024	def convolve ( self , array ) : if self . shape [ 0 ] % 2 == 0 or self . shape [ 1 ] % 2 == 0 : raise exc . KernelException ( "PSF Kernel must be odd" ) return scipy . signal . convolve2d ( array , self , mode = 'same' )
6132	def toJSON ( self ) : return { "id" : self . id , "compile" : self . compile , "position" : self . position , "version" : self . version }
3531	def is_internal_ip ( context , prefix = None ) : try : request = context [ 'request' ] remote_ip = request . META . get ( 'HTTP_X_FORWARDED_FOR' , '' ) if not remote_ip : remote_ip = request . META . get ( 'REMOTE_ADDR' , '' ) if not remote_ip : return False internal_ips = None if prefix is not None : internal_ips = getattr ( settings , '%s_INTERNAL_IPS' % prefix , None ) if internal_ips is None : internal_ips = getattr ( settings , 'ANALYTICAL_INTERNAL_IPS' , None ) if internal_ips is None : internal_ips = getattr ( settings , 'INTERNAL_IPS' , None ) return remote_ip in ( internal_ips or [ ] ) except ( KeyError , AttributeError ) : return False
8477	def run ( self ) : self . checkProperties ( ) self . debug ( "[*] Iniciando escaneo de AtomShields con las siguientes propiedades. . . " ) self . showScanProperties ( ) self . loadConfig ( ) # Init time counter init_ts = datetime . now ( ) # Execute plugins cwd = os . getcwd ( ) os . chdir ( self . path ) issues = self . executeCheckers ( ) os . chdir ( cwd ) # Finish time counter end_ts = datetime . now ( ) duration = '{}' . format ( end_ts - init_ts ) # Process and set issues for plugin in issues . keys ( ) : value = issues [ plugin ] if isinstance ( value , list ) : map ( self . saveIssue , value ) else : self . saveIssue ( value ) # Execute reports print "" self . executeReports ( ) # Print summary output. self . debug ( "" ) self . debug ( "Duration: {t}" . format ( t = duration ) ) self . showSummary ( ) return self . issues
4355	def _pop_ack_callback ( self , msgid ) : if msgid not in self . ack_callbacks : return None return self . ack_callbacks . pop ( msgid )
10334	def build_expand_node_neighborhood_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , BELGraph , str ] , None ] : @ uni_in_place_transformation def expand_node_neighborhood_by_hash ( universe : BELGraph , graph : BELGraph , node_hash : str ) -> None : """Expand around the neighborhoods of a node by identifier.""" node = manager . get_dsl_by_hash ( node_hash ) return expand_node_neighborhood ( universe , graph , node ) return expand_node_neighborhood_by_hash
2590	def stage_in ( self , file , executor ) : if file . scheme == 'ftp' : working_dir = self . dfk . executors [ executor ] . working_dir stage_in_app = self . _ftp_stage_in_app ( executor = executor ) app_fut = stage_in_app ( working_dir , outputs = [ file ] ) return app_fut . _outputs [ 0 ] elif file . scheme == 'http' or file . scheme == 'https' : working_dir = self . dfk . executors [ executor ] . working_dir stage_in_app = self . _http_stage_in_app ( executor = executor ) app_fut = stage_in_app ( working_dir , outputs = [ file ] ) return app_fut . _outputs [ 0 ] elif file . scheme == 'globus' : globus_ep = self . _get_globus_endpoint ( executor ) stage_in_app = self . _globus_stage_in_app ( ) app_fut = stage_in_app ( globus_ep , outputs = [ file ] ) return app_fut . _outputs [ 0 ] else : raise Exception ( 'Staging in with unknown file scheme {} is not supported' . format ( file . scheme ) )
3047	def _do_retrieve_scopes ( self , http , token ) : logger . info ( 'Refreshing scopes' ) query_params = { 'access_token' : token , 'fields' : 'scope' } token_info_uri = _helpers . update_query_params ( self . token_info_uri , query_params ) resp , content = transport . request ( http , token_info_uri ) content = _helpers . _from_bytes ( content ) if resp . status == http_client . OK : d = json . loads ( content ) self . scopes = set ( _helpers . string_to_scopes ( d . get ( 'scope' , '' ) ) ) else : error_msg = 'Invalid response {0}.' . format ( resp . status ) try : d = json . loads ( content ) if 'error_description' in d : error_msg = d [ 'error_description' ] except ( TypeError , ValueError ) : pass raise Error ( error_msg )
3486	def _check_required ( sbase , value , attribute ) : if ( value is None ) or ( value == "" ) : msg = "Required attribute '%s' cannot be found or parsed in '%s'" % ( attribute , sbase ) if hasattr ( sbase , "getId" ) and sbase . getId ( ) : msg += " with id '%s'" % sbase . getId ( ) elif hasattr ( sbase , "getName" ) and sbase . getName ( ) : msg += " with name '%s'" % sbase . getName ( ) elif hasattr ( sbase , "getMetaId" ) and sbase . getMetaId ( ) : msg += " with metaId '%s'" % sbase . getName ( ) raise CobraSBMLError ( msg ) return value
4508	def error ( self , fail = True , action = '' ) : e = 'There was an unknown error communicating with the device.' if action : e = 'While %s: %s' % ( action , e ) log . error ( e ) if fail : raise IOError ( e )
1335	def best_other_class ( logits , exclude ) : other_logits = logits - onehot_like ( logits , exclude , value = np . inf ) return np . argmax ( other_logits )
796	def getActiveJobsForClientInfo ( self , clientInfo , fields = [ ] ) : # Form the sequence of field name strings that will go into the # request dbFields = [ self . _jobs . pubToDBNameDict [ x ] for x in fields ] dbFieldsStr = ',' . join ( [ 'job_id' ] + dbFields ) with ConnectionFactory . get ( ) as conn : query = 'SELECT %s FROM %s ' 'WHERE client_info = %%s ' ' AND status != %%s' % ( dbFieldsStr , self . jobsTableName ) conn . cursor . execute ( query , [ clientInfo , self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return rows
4901	def get_course_enrollments ( self , enterprise_customer , days ) : return CourseEnrollment . objects . filter ( created__gt = datetime . datetime . now ( ) - datetime . timedelta ( days = days ) ) . filter ( user_id__in = enterprise_customer . enterprise_customer_users . values_list ( 'user_id' , flat = True ) )
8119	def transform_path ( self , path ) : p = path . __class__ ( ) # Create a new BezierPath. for pt in path : if pt . cmd == "close" : p . closepath ( ) elif pt . cmd == "moveto" : p . moveto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == "lineto" : p . lineto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == "curveto" : vx1 , vy1 = self . apply ( pt . ctrl1 . x , pt . ctrl1 . y ) vx2 , vy2 = self . apply ( pt . ctrl2 . x , pt . ctrl2 . y ) x , y = self . apply ( pt . x , pt . y ) p . curveto ( vx1 , vy1 , vx2 , vy2 , x , y ) return p
6516	def execute_tools ( config , path , progress = None ) : progress = progress or QuietProgress ( ) progress . on_start ( ) manager = SyncManager ( ) manager . start ( ) num_tools = 0 tools = manager . Queue ( ) for name , cls in iteritems ( get_tools ( ) ) : if config [ name ] [ 'use' ] and cls . can_be_used ( ) : num_tools += 1 tools . put ( { 'name' : name , 'config' : config [ name ] , } ) collector = Collector ( config ) if not num_tools : progress . on_finish ( ) return collector notifications = manager . Queue ( ) environment = manager . dict ( { 'finder' : Finder ( path , config ) , } ) workers = [ ] for _ in range ( config [ 'workers' ] ) : worker = Worker ( args = ( tools , notifications , environment , ) , ) worker . start ( ) workers . append ( worker ) while num_tools : try : notification = notifications . get ( True , 0.25 ) except Empty : pass else : if notification [ 'type' ] == 'start' : progress . on_tool_start ( notification [ 'tool' ] ) elif notification [ 'type' ] == 'complete' : collector . add_issues ( notification [ 'issues' ] ) progress . on_tool_finish ( notification [ 'tool' ] ) num_tools -= 1 progress . on_finish ( ) return collector
12533	def from_set ( self , fileset , check_if_dicoms = True ) : if check_if_dicoms : self . items = [ ] for f in fileset : if is_dicom_file ( f ) : self . items . append ( f ) else : self . items = fileset
10240	def count_author_publications ( graph : BELGraph ) -> typing . Counter [ str ] : authors = group_as_dict ( _iter_author_publiations ( graph ) ) return Counter ( count_dict_values ( count_defaultdict ( authors ) ) )
2918	def _eval_kwargs ( kwargs , my_task ) : results = { } for kwarg , value in list ( kwargs . items ( ) ) : if isinstance ( value , Attrib ) or isinstance ( value , PathAttrib ) : results [ kwarg ] = valueof ( my_task , value ) else : results [ kwarg ] = value return results
5148	def write ( self , name , path = './' ) : byte_object = self . generate ( ) file_name = '{0}.tar.gz' . format ( name ) if not path . endswith ( '/' ) : path += '/' f = open ( '{0}{1}' . format ( path , file_name ) , 'wb' ) f . write ( byte_object . getvalue ( ) ) f . close ( )
6401	def _undouble ( self , word ) : if ( len ( word ) > 1 and word [ - 1 ] == word [ - 2 ] and word [ - 1 ] in { 'd' , 'k' , 't' } ) : return word [ : - 1 ] return word
12233	def pref_group ( title , prefs , help_text = '' , static = True , readonly = False ) : bind_proxy ( prefs , title , help_text = help_text , static = static , readonly = readonly ) for proxy in prefs : # For preferences already marked by pref(). if isinstance ( proxy , PrefProxy ) : proxy . category = title
692	def _initializeEncoders ( self , encoderSpec ) : #Initializing scalar encoder if self . encoderType in [ 'adaptiveScalar' , 'scalar' ] : if 'minval' in encoderSpec : self . minval = encoderSpec . pop ( 'minval' ) else : self . minval = None if 'maxval' in encoderSpec : self . maxval = encoderSpec . pop ( 'maxval' ) else : self . maxval = None self . encoder = adaptive_scalar . AdaptiveScalarEncoder ( name = 'AdaptiveScalarEncoder' , w = self . w , n = self . n , minval = self . minval , maxval = self . maxval , periodic = False , forced = True ) #Initializing category encoder elif self . encoderType == 'category' : self . encoder = sdr_category . SDRCategoryEncoder ( name = 'categoryEncoder' , w = self . w , n = self . n ) #Initializing date encoder elif self . encoderType in [ 'date' , 'datetime' ] : self . encoder = date . DateEncoder ( name = 'dateEncoder' ) else : raise RuntimeError ( 'Error in constructing class object. Either encoder type' 'or dataType must be specified' )
249	def get_txn_vol ( transactions ) : txn_norm = transactions . copy ( ) txn_norm . index = txn_norm . index . normalize ( ) amounts = txn_norm . amount . abs ( ) prices = txn_norm . price values = amounts * prices daily_amounts = amounts . groupby ( amounts . index ) . sum ( ) daily_values = values . groupby ( values . index ) . sum ( ) daily_amounts . name = "txn_shares" daily_values . name = "txn_volume" return pd . concat ( [ daily_values , daily_amounts ] , axis = 1 )
5231	def create_folder ( path_name : str , is_file = False ) : path_sep = path_name . replace ( '\\' , '/' ) . split ( '/' ) for i in range ( 1 , len ( path_sep ) + ( 0 if is_file else 1 ) ) : cur_path = '/' . join ( path_sep [ : i ] ) if not os . path . exists ( cur_path ) : os . mkdir ( cur_path )
922	def _filterRecord ( filterList , record ) : for ( fieldIdx , fp , params ) in filterList : x = dict ( ) x [ 'value' ] = record [ fieldIdx ] x [ 'acceptValues' ] = params [ 'acceptValues' ] x [ 'min' ] = params [ 'min' ] x [ 'max' ] = params [ 'max' ] if not fp ( x ) : return False # None of the field filters triggered, accept the record as a good one return True
895	def getPredictiveCells ( self ) : previousCell = None predictiveCells = [ ] for segment in self . activeSegments : if segment . cell != previousCell : predictiveCells . append ( segment . cell ) previousCell = segment . cell return predictiveCells
7318	def parsemail ( raw_message ) : message = email . parser . Parser ( ) . parsestr ( raw_message ) # Detect encoding detected = chardet . detect ( bytearray ( raw_message , "utf-8" ) ) encoding = detected [ "encoding" ] print ( ">>> encoding {}" . format ( encoding ) ) for part in message . walk ( ) : if part . get_content_maintype ( ) == 'multipart' : continue part . set_charset ( encoding ) # Extract recipients addrs = email . utils . getaddresses ( message . get_all ( "TO" , [ ] ) ) + email . utils . getaddresses ( message . get_all ( "CC" , [ ] ) ) + email . utils . getaddresses ( message . get_all ( "BCC" , [ ] ) ) recipients = [ x [ 1 ] for x in addrs ] message . __delitem__ ( "bcc" ) message . __setitem__ ( 'Date' , email . utils . formatdate ( ) ) sender = message [ "from" ] return ( message , sender , recipients )
430	def read_images ( img_list , path = '' , n_threads = 10 , printable = True ) : imgs = [ ] for idx in range ( 0 , len ( img_list ) , n_threads ) : b_imgs_list = img_list [ idx : idx + n_threads ] b_imgs = tl . prepro . threading_data ( b_imgs_list , fn = read_image , path = path ) # tl.logging.info(b_imgs.shape) imgs . extend ( b_imgs ) if printable : tl . logging . info ( 'read %d from %s' % ( len ( imgs ) , path ) ) return imgs
12411	def close ( self ) : # Ensure we're not closed. self . require_not_closed ( ) if not self . streaming or self . asynchronous : # We're not streaming, auto-write content-length if not # already set. if 'Content-Length' not in self . headers : self . headers [ 'Content-Length' ] = self . tell ( ) # Flush out the current buffer. self . flush ( ) # We're done with the response; inform the HTTP connector # to close the response stream. self . _closed = True
7643	def convert ( annotation , target_namespace ) : # First, validate the input. If this fails, we can't auto-convert. annotation . validate ( strict = True ) # If we're already in the target namespace, do nothing if annotation . namespace == target_namespace : return annotation if target_namespace in __CONVERSION__ : # Otherwise, make a copy to mangle annotation = deepcopy ( annotation ) # Look for a way to map this namespace to the target for source in __CONVERSION__ [ target_namespace ] : if annotation . search ( namespace = source ) : return __CONVERSION__ [ target_namespace ] [ source ] ( annotation ) # No conversion possible raise NamespaceError ( 'Unable to convert annotation from namespace=' '"{0}" to "{1}"' . format ( annotation . namespace , target_namespace ) )
13511	def http_exception_error_handler ( exception ) : assert issubclass ( type ( exception ) , HTTPException ) , type ( exception ) assert hasattr ( exception , "code" ) assert hasattr ( exception , "description" ) return response ( exception . code , exception . description )
1501	def template_slave_hcl ( cl_args , masters ) : slave_config_template = "%s/standalone/templates/slave.template.hcl" % cl_args [ "config_path" ] slave_config_actual = "%s/standalone/resources/slave.hcl" % cl_args [ "config_path" ] masters_in_quotes = [ '"%s"' % master for master in masters ] template_file ( slave_config_template , slave_config_actual , { "<nomad_masters:master_port>" : ", " . join ( masters_in_quotes ) } )
4986	def get_path_variables ( * * kwargs ) : enterprise_customer_uuid = kwargs . get ( 'enterprise_uuid' , '' ) course_run_id = kwargs . get ( 'course_id' , '' ) course_key = kwargs . get ( 'course_key' , '' ) program_uuid = kwargs . get ( 'program_uuid' , '' ) return enterprise_customer_uuid , course_run_id , course_key , program_uuid
13064	def make_coins ( self , collection , text , subreference = "" , lang = None ) : if lang is None : lang = self . __default_lang__ return "url_ver=Z39.88-2004" "&ctx_ver=Z39.88-2004" "&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" "&rft_id={cid}" "&rft.genre=bookitem" "&rft.btitle={title}" "&rft.edition={edition}" "&rft.au={author}" "&rft.atitle={pages}" "&rft.language={language}" "&rft.pages={pages}" . format ( title = quote ( str ( text . get_title ( lang ) ) ) , author = quote ( str ( text . get_creator ( lang ) ) ) , cid = url_for ( ".r_collection" , objectId = collection . id , _external = True ) , language = collection . lang , pages = quote ( subreference ) , edition = quote ( str ( text . get_description ( lang ) ) ) )
10248	def update_context ( universe : BELGraph , graph : BELGraph ) : for namespace in get_namespaces ( graph ) : if namespace in universe . namespace_url : graph . namespace_url [ namespace ] = universe . namespace_url [ namespace ] elif namespace in universe . namespace_pattern : graph . namespace_pattern [ namespace ] = universe . namespace_pattern [ namespace ] else : log . warning ( 'namespace: %s missing from universe' , namespace ) for annotation in get_annotations ( graph ) : if annotation in universe . annotation_url : graph . annotation_url [ annotation ] = universe . annotation_url [ annotation ] elif annotation in universe . annotation_pattern : graph . annotation_pattern [ annotation ] = universe . annotation_pattern [ annotation ] elif annotation in universe . annotation_list : graph . annotation_list [ annotation ] = universe . annotation_list [ annotation ] else : log . warning ( 'annotation: %s missing from universe' , annotation )
10729	def _handle_base_case ( klass , symbol ) : def the_func ( value , variant = 0 ) : """ Base case. :param int variant: variant level for this object :returns: a tuple of a dbus object and the variant level :rtype: dbus object * int """ ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( 0 , variant ) return ( klass ( value , variant_level = obj_level ) , func_level ) return lambda : ( the_func , symbol )
10607	def run ( self ) : self . prepare_to_run ( ) for i in range ( 0 , self . period_count ) : for e in self . entities : e . run ( self . clock ) self . clock . tick ( )
13711	def invalidate_ip ( self , ip ) : if self . _use_cache : key = self . _make_cache_key ( ip ) self . _cache . delete ( key , version = self . _cache_version )
12280	def find_matching_files ( self , includes ) : if len ( includes ) == 0 : return [ ] files = [ f [ 'relativepath' ] for f in self . package [ 'resources' ] ] includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) # Match both the file name as well the path.. files = [ f for f in files if re . match ( includes , os . path . basename ( f ) ) ] + [ f for f in files if re . match ( includes , f ) ] files = list ( set ( files ) ) return files
9704	def checkSerial ( self ) : for item in self . rxSerial ( self . _TUN . _tun . mtu ) : # print("about to send: {0}".format(item)) try : self . _TUN . _tun . write ( item ) except pytun . Error as error : print ( "pytun error writing: {0}" . format ( item ) ) print ( error )
8685	def _encrypt ( self , value ) : value = json . dumps ( value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" ) encrypted_value = self . cipher . encrypt ( value . encode ( 'utf8' ) ) hexified_value = binascii . hexlify ( encrypted_value ) . decode ( 'ascii' ) return hexified_value
10863	def add_particle ( self , pos , rad ) : rad = listify ( rad ) # add some zero mass particles to the list (same as not having these # particles in the image, which is true at this moment) inds = np . arange ( self . N , self . N + len ( rad ) ) self . pos = np . vstack ( [ self . pos , pos ] ) self . rad = np . hstack ( [ self . rad , np . zeros ( len ( rad ) ) ] ) # update the parameters globally self . setup_variables ( ) self . trigger_parameter_change ( ) # now request a drawing of the particle plz params = self . param_particle_rad ( inds ) self . trigger_update ( params , rad ) return inds
9221	def mix ( self , color1 , color2 , weight = 50 , * args ) : if color1 and color2 : if isinstance ( weight , string_types ) : weight = float ( weight . strip ( '%' ) ) weight = ( ( weight / 100.0 ) * 2 ) - 1 rgb1 = self . _hextorgb ( color1 ) rgb2 = self . _hextorgb ( color2 ) alpha = 0 w1 = ( ( ( weight if weight * alpha == - 1 else weight + alpha ) / ( 1 + weight * alpha ) ) + 1 ) w1 = w1 / 2.0 w2 = 1 - w1 rgb = [ rgb1 [ 0 ] * w1 + rgb2 [ 0 ] * w2 , rgb1 [ 1 ] * w1 + rgb2 [ 1 ] * w2 , rgb1 [ 2 ] * w1 + rgb2 [ 2 ] * w2 , ] return self . _rgbatohex ( rgb ) raise ValueError ( 'Illegal color values' )
1362	def get_argument_query ( self ) : try : query = self . get_argument ( constants . PARAM_QUERY ) return query except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
12975	def _doSave ( self , obj , isInsert , conn , pipeline = None ) : if pipeline is None : pipeline = conn newDict = obj . asDict ( forStorage = True ) key = self . _get_key_for_id ( obj . _id ) if isInsert is True : for thisField in self . fields : fieldValue = newDict . get ( thisField , thisField . getDefaultValue ( ) ) pipeline . hset ( key , thisField , fieldValue ) # Update origData with the new data if fieldValue == IR_NULL_STR : obj . _origData [ thisField ] = irNull else : obj . _origData [ thisField ] = object . __getattribute__ ( obj , str ( thisField ) ) self . _add_id_to_keys ( obj . _id , pipeline ) for indexedField in self . indexedFields : self . _add_id_to_index ( indexedField , obj . _id , obj . _origData [ indexedField ] , pipeline ) else : updatedFields = obj . getUpdatedFields ( ) for thisField , fieldValue in updatedFields . items ( ) : ( oldValue , newValue ) = fieldValue oldValueForStorage = thisField . toStorage ( oldValue ) newValueForStorage = thisField . toStorage ( newValue ) pipeline . hset ( key , thisField , newValueForStorage ) if thisField in self . indexedFields : self . _rem_id_from_index ( thisField , obj . _id , oldValueForStorage , pipeline ) self . _add_id_to_index ( thisField , obj . _id , newValueForStorage , pipeline ) # Update origData with the new data obj . _origData [ thisField ] = newValue
3432	def add_groups ( self , group_list ) : def existing_filter ( group ) : if group . id in self . groups : LOGGER . warning ( "Ignoring group '%s' since it already exists." , group . id ) return False return True if isinstance ( group_list , string_types ) or hasattr ( group_list , "id" ) : warn ( "need to pass in a list" ) group_list = [ group_list ] pruned = DictList ( filter ( existing_filter , group_list ) ) for group in pruned : group . _model = self for member in group . members : # If the member is not associated with the model, add it if isinstance ( member , Metabolite ) : if member not in self . metabolites : self . add_metabolites ( [ member ] ) if isinstance ( member , Reaction ) : if member not in self . reactions : self . add_reactions ( [ member ] ) # TODO(midnighter): `add_genes` method does not exist. # if isinstance(member, Gene): # if member not in self.genes: # self.add_genes([member]) self . groups += [ group ]
10878	def calculate_polychrome_linescan_psf ( x , y , z , normalize = False , kfki = 0.889 , sigkf = 0.1 , zint = 100. , nkpts = 3 , dist_type = 'gaussian' , wrap = True , * * kwargs ) : kfkipts , wts = get_polydisp_pts_wts ( kfki , sigkf , dist_type = dist_type , nkpts = nkpts ) #0. Set up vecs if wrap : xpts = vec_to_halfvec ( x ) ypts = vec_to_halfvec ( y ) x3 , y3 , z3 = np . meshgrid ( xpts , ypts , z , indexing = 'ij' ) else : x3 , y3 , z3 = np . meshgrid ( x , y , z , indexing = 'ij' ) rho3 = np . sqrt ( x3 * x3 + y3 * y3 ) #1. Hilm if wrap : y2 , z2 = np . meshgrid ( ypts , z , indexing = 'ij' ) hilm0 = calculate_linescan_ilm_psf ( y2 , z2 , zint = zint , * * kwargs ) if ypts [ 0 ] == 0 : hilm = np . append ( hilm0 [ - 1 : 0 : - 1 ] , hilm0 , axis = 0 ) else : hilm = np . append ( hilm0 [ : : - 1 ] , hilm0 , axis = 0 ) else : y2 , z2 = np . meshgrid ( y , z , indexing = 'ij' ) hilm = calculate_linescan_ilm_psf ( y2 , z2 , zint = zint , * * kwargs ) #2. Hdet if wrap : #Lambda function that ignores its args but still returns correct values func = lambda x , y , z , kfki = 1. : get_hsym_asym ( rho3 * kfki , z3 * kfki , zint = kfki * zint , get_hdet = True , * * kwargs ) [ 0 ] hdet_func = lambda kfki : wrap_and_calc_psf ( xpts , ypts , z , func , kfki = kfki ) else : hdet_func = lambda kfki : get_hsym_asym ( rho3 * kfki , z3 * kfki , zint = kfki * zint , get_hdet = True , * * kwargs ) [ 0 ] ##### inner = [ wts [ a ] * hdet_func ( kfkipts [ a ] ) for a in range ( nkpts ) ] hdet = np . sum ( inner , axis = 0 ) if normalize : hilm /= hilm . sum ( ) hdet /= hdet . sum ( ) for a in range ( x . size ) : hdet [ a ] *= hilm return hdet if normalize else hdet / hdet . sum ( )
1079	def replace ( self , year = None , month = None , day = None ) : if year is None : year = self . _year if month is None : month = self . _month if day is None : day = self . _day return date . __new__ ( type ( self ) , year , month , day )
11518	def generate_upload_token ( self , token , item_id , filename , checksum = None ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemid' ] = item_id parameters [ 'filename' ] = filename if checksum is not None : parameters [ 'checksum' ] = checksum response = self . request ( 'midas.upload.generatetoken' , parameters ) return response [ 'token' ]
13646	def hump_to_underscore ( name ) : new_name = '' pos = 0 for c in name : if pos == 0 : new_name = c . lower ( ) elif 65 <= ord ( c ) <= 90 : new_name += '_' + c . lower ( ) pass else : new_name += c pos += 1 pass return new_name
5543	def clip ( self , array , geometries , inverted = False , clip_buffer = 0 ) : return commons_clip . clip_array_with_vector ( array , self . tile . affine , geometries , inverted = inverted , clip_buffer = clip_buffer * self . tile . pixel_x_size )
1540	def set_config ( self , config ) : if not isinstance ( config , dict ) : raise TypeError ( "Argument to set_config needs to be dict, given: %s" % str ( config ) ) self . _topology_config = config
11705	def reproduce_asexually ( self , egg_word , sperm_word ) : egg = self . generate_gamete ( egg_word ) sperm = self . generate_gamete ( sperm_word ) self . genome = list ( set ( egg + sperm ) ) # Eliminate duplicates self . generation = 1 self . divinity = god
12961	def count ( self ) : conn = self . _get_connection ( ) numFilters = len ( self . filters ) numNotFilters = len ( self . notFilters ) if numFilters + numNotFilters == 0 : return conn . scard ( self . _get_ids_key ( ) ) if numNotFilters == 0 : if numFilters == 1 : ( filterFieldName , filterValue ) = self . filters [ 0 ] return conn . scard ( self . _get_key_for_index ( filterFieldName , filterValue ) ) indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] return len ( conn . sinter ( indexKeys ) ) notIndexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . notFilters ] if numFilters == 0 : return len ( conn . sdiff ( self . _get_ids_key ( ) , * notIndexKeys ) ) indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] tempKey = self . _getTempKey ( ) pipeline = conn . pipeline ( ) pipeline . sinterstore ( tempKey , * indexKeys ) pipeline . sdiff ( tempKey , * notIndexKeys ) pipeline . delete ( tempKey ) pks = pipeline . execute ( ) [ 1 ] # sdiff return len ( pks )
11185	def publish ( quiet , dataset_uri ) : access_uri = http_publish ( dataset_uri ) if not quiet : click . secho ( "Dataset accessible at " , nl = False , fg = "green" ) click . secho ( access_uri )
11109	def walk_directories_info ( self , relativePath = "" ) : def walk_directories ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) for fname in sorted ( directories ) : info = dict . __getitem__ ( directories , fname ) yield os . path . join ( relativePath , fname ) , info for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = dict . __getitem__ ( directories , k ) for e in walk_directories ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_directories ( dir , relativePath = '' )
12631	def group_dicom_files ( dicom_file_paths , header_fields ) : dist = SimpleDicomFileDistance ( field_weights = header_fields ) path_list = dicom_file_paths . copy ( ) path_groups = DefaultOrderedDict ( DicomFileSet ) while len ( path_list ) > 0 : file_path1 = path_list . pop ( ) file_subgroup = [ file_path1 ] dist . set_dicom_file1 ( file_path1 ) j = len ( path_list ) - 1 while j >= 0 : file_path2 = path_list [ j ] dist . set_dicom_file2 ( file_path2 ) if dist . transform ( ) : file_subgroup . append ( file_path2 ) path_list . pop ( j ) j -= 1 path_groups [ file_path1 ] . from_set ( file_subgroup , check_if_dicoms = False ) return path_groups
10085	def delete ( self , force = True , pid = None ) : pid = pid or self . pid if self [ '_deposit' ] . get ( 'pid' ) : raise PIDInvalidAction ( ) if pid : pid . delete ( ) return super ( Deposit , self ) . delete ( force = force )
8880	def predict_proba ( self , X ) : # Check is fit had been called check_is_fitted ( self , [ 'tree' ] ) # Check data X = check_array ( X ) return self . tree . query ( X ) [ 0 ] . flatten ( )
11749	def attach_bundle ( self , bundle ) : if not isinstance ( bundle , BlueprintBundle ) : raise IncompatibleBundle ( 'BlueprintBundle object passed to attach_bundle must be of type {0}' . format ( BlueprintBundle ) ) elif len ( bundle . blueprints ) == 0 : raise MissingBlueprints ( "Bundles must contain at least one flask.Blueprint" ) elif self . _bundle_exists ( bundle . path ) : raise ConflictingPath ( "Duplicate bundle path {0}" . format ( bundle . path ) ) elif self . _journey_path == bundle . path == '/' : raise ConflictingPath ( "Bundle path and Journey path cannot both be {0}" . format ( bundle . path ) ) self . _attached_bundles . append ( bundle )
2483	def write_document ( document , out , validate = True ) : if validate : messages = [ ] messages = document . validate ( messages ) if messages : raise InvalidDocumentError ( messages ) writer = Writer ( document , out ) writer . write ( )
2920	def _send_call ( self , my_task ) : args , kwargs = None , None if self . args : args = _eval_args ( self . args , my_task ) if self . kwargs : kwargs = _eval_kwargs ( self . kwargs , my_task ) LOG . debug ( "%s (task id %s) calling %s" % ( self . name , my_task . id , self . call ) , extra = dict ( data = dict ( args = args , kwargs = kwargs ) ) ) async_call = default_app . send_task ( self . call , args = args , kwargs = kwargs ) my_task . _set_internal_data ( task_id = async_call . task_id ) my_task . async_call = async_call LOG . debug ( "'%s' called: %s" % ( self . call , my_task . async_call . task_id ) )
12811	def lineReceived ( self , line ) : while self . _in_header : if line : self . _headers . append ( line ) else : http , status , message = self . _headers [ 0 ] . split ( " " , 2 ) status = int ( status ) if status == 200 : self . factory . get_stream ( ) . connected ( ) else : self . factory . continueTrying = 0 self . transport . loseConnection ( ) self . factory . get_stream ( ) . disconnected ( RuntimeError ( status , message ) ) return self . _in_header = False break else : try : self . _len_expected = int ( line , 16 ) self . setRawMode ( ) except : pass
13584	def _obj_display ( obj , display = '' ) : result = '' if not display : result = str ( obj ) else : template = Template ( display ) context = Context ( { 'obj' : obj } ) result = template . render ( context ) return result
3688	def solve_T ( self , P , V , quick = True ) : Tc , a , b , kappa0 , kappa1 = self . Tc , self . a , self . b , self . kappa0 , self . kappa1 if quick : x0 = V - b R_x0 = R / x0 x3 = ( 100. * ( V * ( V + b ) + b * x0 ) ) x4 = 10. * kappa0 kappa110 = kappa1 * 10. kappa17 = kappa1 * 7. def to_solve ( T ) : x1 = T / Tc x2 = x1 ** 0.5 return ( T * R_x0 - a * ( ( x4 - ( kappa110 * x1 - kappa17 ) * ( x2 + 1. ) ) * ( x2 - 1. ) - 10. ) ** 2 / x3 ) - P else : def to_solve ( T ) : P_calc = R * T / ( V - b ) - a * ( ( kappa0 + kappa1 * ( sqrt ( T / Tc ) + 1 ) * ( - T / Tc + 7 / 10 ) ) * ( - sqrt ( T / Tc ) + 1 ) + 1 ) ** 2 / ( V * ( V + b ) + b * ( V - b ) ) return P_calc - P return newton ( to_solve , Tc * 0.5 )
5715	def _slugify_foreign_key ( schema ) : for foreign_key in schema . get ( 'foreignKeys' , [ ] ) : foreign_key [ 'reference' ] [ 'resource' ] = _slugify_resource_name ( foreign_key [ 'reference' ] . get ( 'resource' , '' ) ) return schema
7824	def finish ( self , data ) : if not self . _server_first_message : logger . debug ( "Got success too early" ) return Failure ( "bad-success" ) if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : ret = self . _final_challenge ( data ) if isinstance ( ret , Failure ) : return ret if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : logger . debug ( "Something went wrong when processing additional" " data with success?" ) return Failure ( "bad-success" )
7571	def revcomp ( sequence ) : sequence = sequence [ : : - 1 ] . strip ( ) . replace ( "A" , "t" ) . replace ( "T" , "a" ) . replace ( "C" , "g" ) . replace ( "G" , "c" ) . upper ( ) return sequence
1131	def urldefrag ( url ) : if '#' in url : s , n , p , a , q , frag = urlparse ( url ) defrag = urlunparse ( ( s , n , p , a , q , '' ) ) return defrag , frag else : return url , ''
4892	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , grade = None , is_passing = False ) : # pylint: disable=invalid-name LearnerDataTransmissionAudit = apps . get_model ( 'integrated_channel' , 'LearnerDataTransmissionAudit' ) completed_timestamp = None course_completed = False if completed_date is not None : completed_timestamp = parse_datetime_to_epoch_millis ( completed_date ) course_completed = is_passing return [ LearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , course_id = enterprise_enrollment . course_id , course_completed = course_completed , completed_timestamp = completed_timestamp , grade = grade , ) ]
13181	def _get_printable_columns ( columns , row ) : if not columns : return row # Extract the column values, in the order specified. return tuple ( row [ c ] for c in columns )
1628	def GetIndentLevel ( line ) : indent = Match ( r'^( *)\S' , line ) if indent : return len ( indent . group ( 1 ) ) else : return 0
6100	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . divide ( self . intensity , self . sigma * np . sqrt ( 2.0 * np . pi ) ) , np . exp ( - 0.5 * np . square ( np . divide ( grid_radii , self . sigma ) ) ) )
3423	def resettable ( f ) : def wrapper ( self , new_value ) : context = get_context ( self ) if context : old_value = getattr ( self , f . __name__ ) # Don't clutter the context with unchanged variables if old_value == new_value : return context ( partial ( f , self , old_value ) ) f ( self , new_value ) return wrapper
7036	def cone_search ( lcc_server , center_ra , center_decl , radiusarcmin = 5.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , samplespec = None , limitspec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : # turn the input into a param dict coords = '%.5f %.5f %.1f' % ( center_ra , center_decl , radiusarcmin ) params = { 'coords' : coords } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done # we won't wait for the LC ZIP to complete if email_when_done = True if email_when_done : download_data = False # check if we have an API key already have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) # if not, get a new one if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) # hit the server api_url = '%s/api/conesearch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) # check the status of the search status = searchresult [ 0 ] # now we'll check if we want to download the data if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
4340	def remix ( self , remix_dictionary = None , num_output_channels = None ) : if not ( isinstance ( remix_dictionary , dict ) or remix_dictionary is None ) : raise ValueError ( "remix_dictionary must be a dictionary or None." ) if remix_dictionary is not None : if not all ( [ isinstance ( i , int ) and i > 0 for i in remix_dictionary . keys ( ) ] ) : raise ValueError ( "remix dictionary must have positive integer keys." ) if not all ( [ isinstance ( v , list ) for v in remix_dictionary . values ( ) ] ) : raise ValueError ( "remix dictionary values must be lists." ) for v_list in remix_dictionary . values ( ) : if not all ( [ isinstance ( v , int ) and v > 0 for v in v_list ] ) : raise ValueError ( "elements of remix dictionary values must " "be positive integers" ) if not ( ( isinstance ( num_output_channels , int ) and num_output_channels > 0 ) or num_output_channels is None ) : raise ValueError ( "num_output_channels must be a positive integer or None." ) effect_args = [ 'remix' ] if remix_dictionary is None : effect_args . append ( '-' ) else : if num_output_channels is None : num_output_channels = max ( remix_dictionary . keys ( ) ) for channel in range ( 1 , num_output_channels + 1 ) : if channel in remix_dictionary . keys ( ) : out_channel = ',' . join ( [ str ( i ) for i in remix_dictionary [ channel ] ] ) else : out_channel = '0' effect_args . append ( out_channel ) self . effects . extend ( effect_args ) self . effects_log . append ( 'remix' ) return self
13296	def decode_jsonld ( jsonld_text ) : decoder = json . JSONDecoder ( object_pairs_hook = _decode_object_pairs ) return decoder . decode ( jsonld_text )
11213	def decode ( secret : Union [ str , bytes ] , token : Union [ str , bytes ] , alg : str = default_alg ) -> Tuple [ dict , dict ] : secret = util . to_bytes ( secret ) token = util . to_bytes ( token ) pre_signature , signature_segment = token . rsplit ( b'.' , 1 ) header_b64 , payload_b64 = pre_signature . split ( b'.' ) try : header_json = util . b64_decode ( header_b64 ) header = json . loads ( util . from_bytes ( header_json ) ) except ( json . decoder . JSONDecodeError , UnicodeDecodeError , ValueError ) : raise InvalidHeaderError ( 'Invalid header' ) try : payload_json = util . b64_decode ( payload_b64 ) payload = json . loads ( util . from_bytes ( payload_json ) ) except ( json . decoder . JSONDecodeError , UnicodeDecodeError , ValueError ) : raise InvalidPayloadError ( 'Invalid payload' ) if not isinstance ( header , dict ) : raise InvalidHeaderError ( 'Invalid header: {}' . format ( header ) ) if not isinstance ( payload , dict ) : raise InvalidPayloadError ( 'Invalid payload: {}' . format ( payload ) ) signature = util . b64_decode ( signature_segment ) calculated_signature = _hash ( secret , pre_signature , alg ) if not compare_signature ( signature , calculated_signature ) : raise InvalidSignatureError ( 'Invalid signature' ) return header , payload
2626	def cancel ( self , job_ids ) : if self . linger is True : logger . debug ( "Ignoring cancel requests due to linger mode" ) return [ False for x in job_ids ] try : self . client . terminate_instances ( InstanceIds = list ( job_ids ) ) except Exception as e : logger . error ( "Caught error while attempting to remove instances: {0}" . format ( job_ids ) ) raise e else : logger . debug ( "Removed the instances: {0}" . format ( job_ids ) ) for job_id in job_ids : self . resources [ job_id ] [ "status" ] = "COMPLETED" for job_id in job_ids : self . instances . remove ( job_id ) return [ True for x in job_ids ]
10270	def get_unweighted_upstream_leaves ( graph : BELGraph , key : Optional [ str ] = None ) -> Iterable [ BaseEntity ] : if key is None : key = WEIGHT return filter_nodes ( graph , [ node_is_upstream_leaf , data_missing_key_builder ( key ) ] )
2120	def associate_success_node ( self , parent , child = None , * * kwargs ) : return self . _assoc_or_create ( 'success' , parent , child , * * kwargs )
632	def destroySegment ( self , segment ) : # Remove the synapses from all data structures outside this Segment. for synapse in segment . _synapses : self . _removeSynapseFromPresynapticMap ( synapse ) self . _numSynapses -= len ( segment . _synapses ) # Remove the segment from the cell's list. segments = self . _cells [ segment . cell ] . _segments i = segments . index ( segment ) del segments [ i ] # Free the flatIdx and remove the final reference so the Segment can be # garbage-collected. self . _freeFlatIdxs . append ( segment . flatIdx ) self . _segmentForFlatIdx [ segment . flatIdx ] = None
8526	def find_match ( self ) : for pattern , callback in self . rules : match = pattern . match ( self . source , pos = self . pos ) if not match : continue try : node = callback ( match ) except IgnoredMatchException : pass else : self . seen . append ( node ) return match raise NoMatchException ( 'None of the known patterns match for {}' '' . format ( self . source [ self . pos : ] ) )
6361	def sim_matrix ( src , tar , mat = None , mismatch_cost = 0 , match_cost = 1 , symmetric = True , alphabet = None , ) : if alphabet : alphabet = tuple ( alphabet ) for i in src : if i not in alphabet : raise ValueError ( 'src value not in alphabet' ) for i in tar : if i not in alphabet : raise ValueError ( 'tar value not in alphabet' ) if src == tar : if mat and ( src , src ) in mat : return mat [ ( src , src ) ] return match_cost if mat and ( src , tar ) in mat : return mat [ ( src , tar ) ] elif symmetric and mat and ( tar , src ) in mat : return mat [ ( tar , src ) ] return mismatch_cost
6183	def git_path_valid ( git_path = None ) : if git_path is None and GIT_PATH is None : return False if git_path is None : git_path = GIT_PATH try : call ( [ git_path , '--version' ] ) return True except OSError : return False
6835	def vagrant_settings ( self , name = '' , * args , * * kwargs ) : config = self . ssh_config ( name ) extra_args = self . _settings_dict ( config ) kwargs . update ( extra_args ) return self . settings ( * args , * * kwargs )
13557	def get_all_images ( self ) : self_imgs = self . image_set . all ( ) update_ids = self . update_set . values_list ( 'id' , flat = True ) u_images = UpdateImage . objects . filter ( update__id__in = update_ids ) return list ( chain ( self_imgs , u_images ) )
13677	def prepare ( self ) : result_files = self . collect_files ( ) chain = self . prepare_handlers_chain if chain is None : # default handlers chain = [ LessCompilerPrepareHandler ( ) ] for prepare_handler in chain : result_files = prepare_handler . prepare ( result_files , self ) return result_files
2602	def engine_file ( self ) : return os . path . join ( self . ipython_dir , 'profile_{0}' . format ( self . profile ) , 'security/ipcontroller-engine.json' )
6992	def flare_model ( flareparams , times , mags , errs ) : ( amplitude , flare_peak_time , rise_gaussian_stdev , decay_time_constant ) = flareparams zerolevel = np . median ( mags ) modelmags = np . full_like ( times , zerolevel ) # before peak gaussian rise... modelmags [ times < flare_peak_time ] = ( mags [ times < flare_peak_time ] + amplitude * np . exp ( - ( ( times [ times < flare_peak_time ] - flare_peak_time ) * ( times [ times < flare_peak_time ] - flare_peak_time ) ) / ( 2.0 * rise_gaussian_stdev * rise_gaussian_stdev ) ) ) # after peak exponential decay... modelmags [ times > flare_peak_time ] = ( mags [ times > flare_peak_time ] + amplitude * np . exp ( - ( ( times [ times > flare_peak_time ] - flare_peak_time ) ) / ( decay_time_constant ) ) ) return modelmags , times , mags , errs
1410	def filter_bolts ( table , header ) : bolts_info = [ ] for row in table : if row [ 0 ] == 'bolt' : bolts_info . append ( row ) return bolts_info , header
13094	def start_processes ( self ) : self . relay = subprocess . Popen ( [ 'ntlmrelayx.py' , '-6' , '-tf' , self . targets_file , '-w' , '-l' , self . directory , '-of' , self . output_file ] , cwd = self . directory ) self . responder = subprocess . Popen ( [ 'responder' , '-I' , self . interface_name ] )
12432	def create ( self ) : # create virtualenv self . create_virtualenv ( ) # create project self . create_project ( ) # generate uwsgi script self . create_uwsgi_script ( ) # generate nginx config self . create_nginx_config ( ) # generate management scripts self . create_manage_scripts ( ) logging . info ( '** Make sure to set proper permissions for the webserver user account on the var and log directories in the project root' )
12322	def to_holvi_dict ( self ) : self . _jsondata [ "items" ] = [ ] for item in self . items : self . _jsondata [ "items" ] . append ( item . to_holvi_dict ( ) ) self . _jsondata [ "issue_date" ] = self . issue_date . isoformat ( ) self . _jsondata [ "due_date" ] = self . due_date . isoformat ( ) self . _jsondata [ "receiver" ] = self . receiver . to_holvi_dict ( ) return { k : v for ( k , v ) in self . _jsondata . items ( ) if k in self . _valid_keys }
6524	def get_issues ( self , sortby = None ) : self . _ensure_cleaned_issues ( ) return self . _sort_issues ( self . _cleaned_issues , sortby )
1384	def trigger_watches ( self ) : to_remove = [ ] for uid , callback in self . watches . items ( ) : try : callback ( self ) except Exception as e : Log . error ( "Caught exception while triggering callback: " + str ( e ) ) Log . debug ( traceback . format_exc ( ) ) to_remove . append ( uid ) for uid in to_remove : self . unregister_watch ( uid )
13294	def convert_text ( content , from_fmt , to_fmt , deparagraph = False , mathjax = False , smart = True , extra_args = None ) : logger = logging . getLogger ( __name__ ) if extra_args is not None : extra_args = list ( extra_args ) else : extra_args = [ ] if mathjax : extra_args . append ( '--mathjax' ) if smart : extra_args . append ( '--smart' ) if deparagraph : extra_args . append ( '--filter=lsstprojectmeta-deparagraph' ) extra_args . append ( '--wrap=none' ) # de-dupe extra args extra_args = set ( extra_args ) logger . debug ( 'Running pandoc from %s to %s with extra_args %s' , from_fmt , to_fmt , extra_args ) output = pypandoc . convert_text ( content , to_fmt , format = from_fmt , extra_args = extra_args ) return output
1596	def format_mtime ( mtime ) : now = datetime . now ( ) dt = datetime . fromtimestamp ( mtime ) return '%s %2d %5s' % ( dt . strftime ( '%b' ) , dt . day , dt . year if dt . year != now . year else dt . strftime ( '%H:%M' ) )
8423	def hls_palette ( n_colors = 6 , h = .01 , l = .6 , s = .65 ) : hues = np . linspace ( 0 , 1 , n_colors + 1 ) [ : - 1 ] hues += h hues %= 1 hues -= hues . astype ( int ) palette = [ colorsys . hls_to_rgb ( h_i , l , s ) for h_i in hues ] return palette
6830	def get_logs_between_commits ( self , a , b ) : print ( 'REAL' ) ret = self . local ( 'git --no-pager log --pretty=oneline %s...%s' % ( a , b ) , capture = True ) if self . verbose : print ( ret ) return str ( ret )
7629	def namespace_array ( ns_key ) : obs_sch = namespace ( ns_key ) obs_sch [ 'title' ] = 'Observation' sch = copy . deepcopy ( JAMS_SCHEMA [ 'definitions' ] [ 'SparseObservationList' ] ) sch [ 'items' ] = obs_sch return sch
11232	def run_excel_to_html ( ) : # Capture commandline arguments. prog='' argument must # match the command name in setup.py entry_points parser = argparse . ArgumentParser ( prog = 'excel_to_html' ) parser . add_argument ( '-p' , nargs = '?' , help = 'Path to an excel file for conversion.' ) parser . add_argument ( '-s' , nargs = '?' , help = 'The name of a sheet in our excel file. Defaults to "Sheet1".' , ) parser . add_argument ( '-css' , nargs = '?' , help = 'Space separated css classes to append to the table.' ) parser . add_argument ( '-m' , action = 'store_true' , help = 'Merge, attempt to combine merged cells.' ) parser . add_argument ( '-c' , nargs = '?' , help = 'Caption for creating an accessible table.' ) parser . add_argument ( '-d' , nargs = '?' , help = 'Two strings separated by a | character. The first string \ is for the html "summary" attribute and the second string is for the html "details" attribute. \ both values must be provided and nothing more.' , ) parser . add_argument ( '-r' , action = 'store_true' , help = 'Row headers. Does the table have row headers?' ) args = parser . parse_args ( ) inputs = { 'p' : args . p , 's' : args . s , 'css' : args . css , 'm' : args . m , 'c' : args . c , 'd' : args . d , 'r' : args . r , } p = inputs [ 'p' ] s = inputs [ 's' ] if inputs [ 's' ] else 'Sheet1' css = inputs [ 'css' ] if inputs [ 'css' ] else '' m = inputs [ 'm' ] if inputs [ 'm' ] else False c = inputs [ 'c' ] if inputs [ 'c' ] else '' d = inputs [ 'd' ] . split ( '|' ) if inputs [ 'd' ] else [ ] r = inputs [ 'r' ] if inputs [ 'r' ] else False html = fp . excel_to_html ( p , sheetname = s , css_classes = css , caption = c , details = d , row_headers = r , merge = m ) print ( html )
13044	def main ( ) : config = Config ( ) pipes_dir = config . get ( 'pipes' , 'directory' ) pipes_config = config . get ( 'pipes' , 'config_file' ) pipes_config_path = os . path . join ( config . config_dir , pipes_config ) if not os . path . exists ( pipes_config_path ) : print_error ( "Please configure the named pipes first" ) return workers = create_pipe_workers ( pipes_config_path , pipes_dir ) if workers : for worker in workers : worker . start ( ) try : for worker in workers : worker . join ( ) except KeyboardInterrupt : print_notification ( "Shutting down" ) for worker in workers : worker . terminate ( ) worker . join ( )
10714	def _setRTSDTR ( port , RTS , DTR ) : port . setRTS ( RTS ) port . setDTR ( DTR )
3124	def _verify_time_range ( payload_dict ) : # Get the current time to use throughout. now = int ( time . time ( ) ) # Make sure issued at and expiration are in the payload. issued_at = payload_dict . get ( 'iat' ) if issued_at is None : raise AppIdentityError ( 'No iat field in token: {0}' . format ( payload_dict ) ) expiration = payload_dict . get ( 'exp' ) if expiration is None : raise AppIdentityError ( 'No exp field in token: {0}' . format ( payload_dict ) ) # Make sure the expiration gives an acceptable token lifetime. if expiration >= now + MAX_TOKEN_LIFETIME_SECS : raise AppIdentityError ( 'exp field too far in future: {0}' . format ( payload_dict ) ) # Make sure (up to clock skew) that the token wasn't issued in the future. earliest = issued_at - CLOCK_SKEW_SECS if now < earliest : raise AppIdentityError ( 'Token used too early, {0} < {1}: {2}' . format ( now , earliest , payload_dict ) ) # Make sure (up to clock skew) that the token isn't already expired. latest = expiration + CLOCK_SKEW_SECS if now > latest : raise AppIdentityError ( 'Token used too late, {0} > {1}: {2}' . format ( now , latest , payload_dict ) )
4176	def window_gaussian ( N , alpha = 2.5 ) : t = linspace ( - ( N - 1 ) / 2. , ( N - 1 ) / 2. , N ) #t = linspace(-(N)/2., (N)/2., N) w = exp ( - 0.5 * ( alpha * t / ( N / 2. ) ) ** 2. ) return w
8428	def gradient_n_pal ( colors , values = None , name = 'gradientn' ) : # Note: For better results across devices and media types, # it would be better to do the interpolation in # Lab color space. if values is None : colormap = mcolors . LinearSegmentedColormap . from_list ( name , colors ) else : colormap = mcolors . LinearSegmentedColormap . from_list ( name , list ( zip ( values , colors ) ) ) def _gradient_n_pal ( vals ) : return ratios_to_colors ( vals , colormap ) return _gradient_n_pal
6376	def dist_abs ( self , src , tar , max_offset = 5 ) : if not src : return len ( tar ) if not tar : return len ( src ) src_len = len ( src ) tar_len = len ( tar ) src_cur = 0 tar_cur = 0 lcss = 0 local_cs = 0 while ( src_cur < src_len ) and ( tar_cur < tar_len ) : if src [ src_cur ] == tar [ tar_cur ] : local_cs += 1 else : lcss += local_cs local_cs = 0 if src_cur != tar_cur : src_cur = tar_cur = max ( src_cur , tar_cur ) for i in range ( max_offset ) : if not ( ( src_cur + i < src_len ) or ( tar_cur + i < tar_len ) ) : break if ( src_cur + i < src_len ) and ( src [ src_cur + i ] == tar [ tar_cur ] ) : src_cur += i local_cs += 1 break if ( tar_cur + i < tar_len ) and ( src [ src_cur ] == tar [ tar_cur + i ] ) : tar_cur += i local_cs += 1 break src_cur += 1 tar_cur += 1 lcss += local_cs return round ( max ( src_len , tar_len ) - lcss )
11859	def sum_out ( var , factors , bn ) : result , var_factors = [ ] , [ ] for f in factors : ( var_factors if var in f . vars else result ) . append ( f ) result . append ( pointwise_product ( var_factors , bn ) . sum_out ( var , bn ) ) return result
8000	def fix_out_stanza ( self , stanza ) : StreamBase . fix_out_stanza ( self , stanza ) if self . initiator : if stanza . from_jid : stanza . from_jid = None else : if not stanza . from_jid : stanza . from_jid = self . me
7949	def send_stream_tail ( self ) : with self . lock : if not self . _socket or self . _hup : logger . debug ( u"Cannot send stream closing tag: already closed" ) return data = self . _serializer . emit_tail ( ) try : self . _write ( data . encode ( "utf-8" ) ) except ( IOError , SystemError , socket . error ) , err : logger . debug ( u"Sending stream closing tag failed: {0}" . format ( err ) ) self . _serializer = None self . _hup = True if self . _tls_state is None : try : self . _socket . shutdown ( socket . SHUT_WR ) except socket . error : pass self . _set_state ( "closing" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
11943	def _get ( self , * args , * * kwargs ) : messages , all_retrieved = super ( StorageMixin , self ) . _get ( * args , * * kwargs ) if self . user . is_authenticated ( ) : inbox_messages = self . backend . inbox_list ( self . user ) else : inbox_messages = [ ] return messages + inbox_messages , all_retrieved
8226	def _makeInstance ( self , clazz , args , kwargs ) : inst = clazz ( self , * args , * * kwargs ) return inst
10725	def _variant_levels ( level , variant ) : return ( level + variant , level + variant ) if variant != 0 else ( variant , level )
4632	def child ( self , offset256 ) : a = bytes ( self ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . add ( s )
1425	def initialize ( self , config , context ) : self . logger . info ( "Initializing PulsarSpout with the following" ) self . logger . info ( "Component-specific config: \n%s" % str ( config ) ) self . logger . info ( "Context: \n%s" % str ( context ) ) self . emit_count = 0 self . ack_count = 0 self . fail_count = 0 if not PulsarSpout . serviceUrl in config or not PulsarSpout . topicName in config : self . logger . fatal ( "Need to specify both serviceUrl and topicName" ) self . pulsar_cluster = str ( config [ PulsarSpout . serviceUrl ] ) self . topic = str ( config [ PulsarSpout . topicName ] ) mode = config [ api_constants . TOPOLOGY_RELIABILITY_MODE ] if mode == api_constants . TopologyReliabilityMode . ATLEAST_ONCE : self . acking_timeout = 1000 * int ( config [ api_constants . TOPOLOGY_MESSAGE_TIMEOUT_SECS ] ) else : self . acking_timeout = 30000 if PulsarSpout . receiveTimeoutMs in config : self . receive_timeout_ms = config [ PulsarSpout . receiveTimeoutMs ] else : self . receive_timeout_ms = 10 if PulsarSpout . deserializer in config : self . deserializer = config [ PulsarSpout . deserializer ] if not callable ( self . deserializer ) : self . logger . fatal ( "Pulsar Message Deserializer needs to be callable" ) else : self . deserializer = self . default_deserializer # First generate the config self . logConfFileName = GenerateLogConfig ( context ) self . logger . info ( "Generated LogConf at %s" % self . logConfFileName ) # We currently use the high level consumer API # For supporting effectively once, we will need to switch # to using lower level Reader API, when it becomes # available in python self . client = pulsar . Client ( self . pulsar_cluster , log_conf_file_path = self . logConfFileName ) self . logger . info ( "Setup Client with cluster %s" % self . pulsar_cluster ) try : self . consumer = self . client . subscribe ( self . topic , context . get_topology_name ( ) , consumer_type = pulsar . ConsumerType . Failover , unacked_messages_timeout_ms = self . acking_timeout ) except Exception as e : self . logger . fatal ( "Pulsar client subscription failed: %s" % str ( e ) ) self . logger . info ( "Subscribed to topic %s" % self . topic )
6123	def instance_for_arguments ( self , arguments ) : profiles = { * * { key : value . instance_for_arguments ( arguments ) for key , value in self . profile_prior_model_dict . items ( ) } , * * self . constant_profile_dict } try : redshift = self . redshift . instance_for_arguments ( arguments ) except AttributeError : redshift = self . redshift pixelization = self . pixelization . instance_for_arguments ( arguments ) if isinstance ( self . pixelization , pm . PriorModel ) else self . pixelization regularization = self . regularization . instance_for_arguments ( arguments ) if isinstance ( self . regularization , pm . PriorModel ) else self . regularization hyper_galaxy = self . hyper_galaxy . instance_for_arguments ( arguments ) if isinstance ( self . hyper_galaxy , pm . PriorModel ) else self . hyper_galaxy return galaxy . Galaxy ( redshift = redshift , pixelization = pixelization , regularization = regularization , hyper_galaxy = hyper_galaxy , * * profiles )
13312	def _pre_activate ( self ) : if 'CPENV_CLEAN_ENV' not in os . environ : if platform == 'win' : os . environ [ 'PROMPT' ] = '$P$G' else : os . environ [ 'PS1' ] = '\\u@\\h:\\w\\$' clean_env_path = utils . get_store_env_tmp ( ) os . environ [ 'CPENV_CLEAN_ENV' ] = clean_env_path utils . store_env ( path = clean_env_path ) else : utils . restore_env_from_file ( os . environ [ 'CPENV_CLEAN_ENV' ] )
12979	def deleteMultiple ( self , objs ) : conn = self . _get_connection ( ) pipeline = conn . pipeline ( ) numDeleted = 0 for obj in objs : numDeleted += self . deleteOne ( obj , pipeline ) pipeline . execute ( ) return numDeleted
11686	def get_user_details ( user_id ) : reasons = [ ] try : url = OSM_USERS_API . format ( user_id = requests . compat . quote ( user_id ) ) user_request = requests . get ( url ) if user_request . status_code == 200 : user_data = user_request . content xml_data = ET . fromstring ( user_data ) . getchildren ( ) [ 0 ] . getchildren ( ) changesets = [ i for i in xml_data if i . tag == 'changesets' ] [ 0 ] blocks = [ i for i in xml_data if i . tag == 'blocks' ] [ 0 ] if int ( changesets . get ( 'count' ) ) <= 5 : reasons . append ( 'New mapper' ) elif int ( changesets . get ( 'count' ) ) <= 30 : url = MAPBOX_USERS_API . format ( user_id = requests . compat . quote ( user_id ) ) user_request = requests . get ( url ) if user_request . status_code == 200 : mapping_days = int ( user_request . json ( ) . get ( 'extra' ) . get ( 'mapping_days' ) ) if mapping_days <= 5 : reasons . append ( 'New mapper' ) if int ( blocks . getchildren ( ) [ 0 ] . get ( 'count' ) ) > 1 : reasons . append ( 'User has multiple blocks' ) except Exception as e : message = 'Could not verify user of the changeset: {}, {}' print ( message . format ( user_id , str ( e ) ) ) return reasons
5909	def parse_groups ( output ) : groups = [ ] for line in output . split ( '\n' ) : m = NDXGROUP . match ( line ) if m : d = m . groupdict ( ) groups . append ( { 'name' : d [ 'GROUPNAME' ] , 'nr' : int ( d [ 'GROUPNUMBER' ] ) , 'natoms' : int ( d [ 'NATOMS' ] ) } ) return groups
2211	def inject_method ( self , func , name = None ) : # TODO: if func is a bound method we should probably unbind it new_method = func . __get__ ( self , self . __class__ ) if name is None : name = func . __name__ setattr ( self , name , new_method )
2588	def get_data_manager ( cls ) : from parsl . dataflow . dflow import DataFlowKernelLoader dfk = DataFlowKernelLoader . dfk ( ) return dfk . executors [ 'data_manager' ]
4143	def _numpy_solver ( A , B ) : x = numpy . linalg . solve ( A , B ) return x
4008	def _increase_file_handle_limit ( ) : logging . info ( 'Increasing file handle limit to {}' . format ( constants . FILE_HANDLE_LIMIT ) ) resource . setrlimit ( resource . RLIMIT_NOFILE , ( constants . FILE_HANDLE_LIMIT , resource . RLIM_INFINITY ) )
8171	def alignment ( self , d = 5 ) : vx = vy = vz = 0 for b in self . boids : if b != self : vx , vy , vz = vx + b . vx , vy + b . vy , vz + b . vz n = len ( self . boids ) - 1 vx , vy , vz = vx / n , vy / n , vz / n return ( vx - self . vx ) / d , ( vy - self . vy ) / d , ( vz - self . vz ) / d
9237	def open ( self ) : if self . is_open : return try : os . chdir ( self . working_directory ) if self . chroot_directory : os . chroot ( self . chroot_directory ) os . setgid ( self . gid ) os . setuid ( self . uid ) os . umask ( self . umask ) except OSError as err : raise DaemonError ( 'Setting up Environment failed: {0}' . format ( err ) ) if self . prevent_core : try : resource . setrlimit ( resource . RLIMIT_CORE , ( 0 , 0 ) ) except Exception as err : raise DaemonError ( 'Could not disable core files: {0}' . format ( err ) ) if self . detach_process : try : if os . fork ( ) > 0 : os . _exit ( 0 ) except OSError as err : raise DaemonError ( 'First fork failed: {0}' . format ( err ) ) os . setsid ( ) try : if os . fork ( ) > 0 : os . _exit ( 0 ) except OSError as err : raise DaemonError ( 'Second fork failed: {0}' . format ( err ) ) for ( signal_number , handler ) in self . _signal_handler_map . items ( ) : signal . signal ( signal_number , handler ) close_filenos ( self . _files_preserve ) redirect_stream ( sys . stdin , self . stdin ) redirect_stream ( sys . stdout , self . stdout ) redirect_stream ( sys . stderr , self . stderr ) if self . pidfile : self . pidfile . acquire ( ) self . _is_open = True
5418	def format_logging_uri ( uri , job_metadata , task_metadata ) : # If the user specifies any formatting (with curly braces), then use that # as the format string unchanged. fmt = str ( uri ) if '{' not in fmt : if uri . endswith ( '.log' ) : # URI includes a filename. Trim the extension and just use the prefix. fmt = os . path . splitext ( uri ) [ 0 ] else : # URI is a path to a directory. The job-id becomes the filename prefix. fmt = os . path . join ( uri , '{job-id}' ) # If this is a task job, add the task-id. if task_metadata . get ( 'task-id' ) is not None : fmt += '.{task-id}' # If this is a retryable task, add the task-attempt. if task_metadata . get ( 'task-attempt' ) is not None : fmt += '.{task-attempt}' fmt += '.log' return _format_task_uri ( fmt , job_metadata , task_metadata )
3446	def add_mip_obj ( model ) : if len ( model . variables ) > 1e4 : LOGGER . warning ( "the MIP version of minimal media is extremely slow for" " models that large :(" ) exchange_rxns = find_boundary_types ( model , "exchange" ) big_m = max ( abs ( b ) for r in exchange_rxns for b in r . bounds ) prob = model . problem coefs = { } to_add = [ ] for rxn in exchange_rxns : export = len ( rxn . reactants ) == 1 indicator = prob . Variable ( "ind_" + rxn . id , lb = 0 , ub = 1 , type = "binary" ) if export : vrv = rxn . reverse_variable indicator_const = prob . Constraint ( vrv - indicator * big_m , ub = 0 , name = "ind_constraint_" + rxn . id ) else : vfw = rxn . forward_variable indicator_const = prob . Constraint ( vfw - indicator * big_m , ub = 0 , name = "ind_constraint_" + rxn . id ) to_add . extend ( [ indicator , indicator_const ] ) coefs [ indicator ] = 1 model . add_cons_vars ( to_add ) model . solver . update ( ) model . objective . set_linear_coefficients ( coefs ) model . objective . direction = "min"
9695	def findall ( text ) : results = TIMESTRING_RE . findall ( text ) dates = [ ] for date in results : if re . compile ( '((next|last)\s(\d+|couple(\sof))\s(weeks|months|quarters|years))|(between|from)' , re . I ) . match ( date [ 0 ] ) : dates . append ( ( date [ 0 ] . strip ( ) , Range ( date [ 0 ] ) ) ) else : dates . append ( ( date [ 0 ] . strip ( ) , Date ( date [ 0 ] ) ) ) return dates
4467	def serialize ( transform , * * kwargs ) : params = transform . get_params ( ) return jsonpickle . encode ( params , * * kwargs )
1350	def write_json_response ( self , response ) : self . write ( tornado . escape . json_encode ( response ) ) self . set_header ( "Content-Type" , "application/json" )
1497	def parse_query_string ( self , query ) : if not query : return None # Just braces do not matter if query [ 0 ] == '(' : index = self . find_closing_braces ( query ) # This must be the last index, since this was an NOP starting brace if index != len ( query ) - 1 : raise Exception ( "Invalid syntax" ) else : return self . parse_query_string ( query [ 1 : - 1 ] ) start_index = query . find ( "(" ) # There must be a ( in the query if start_index < 0 : # Otherwise it must be a constant try : constant = float ( query ) return constant except ValueError : raise Exception ( "Invalid syntax" ) token = query [ : start_index ] if token not in self . operators : raise Exception ( "Invalid token: " + token ) # Get sub components rest_of_the_query = query [ start_index : ] braces_end_index = self . find_closing_braces ( rest_of_the_query ) if braces_end_index != len ( rest_of_the_query ) - 1 : raise Exception ( "Invalid syntax" ) parts = self . get_sub_parts ( rest_of_the_query [ 1 : - 1 ] ) # parts are simple strings in this case if token == "TS" : # This will raise exception if parts are not syntactically correct return self . operators [ token ] ( parts ) children = [ ] for part in parts : children . append ( self . parse_query_string ( part ) ) # Make a node for the current token node = self . operators [ token ] ( children ) return node
13849	def get_time ( filename ) : ts = os . stat ( filename ) . st_mtime return datetime . datetime . utcfromtimestamp ( ts )
2036	def SSTORE ( self , offset , value ) : storage_address = self . address self . _publish ( 'will_evm_write_storage' , storage_address , offset , value ) #refund = Operators.ITEBV(256, # previous_value != 0, # Operators.ITEBV(256, value != 0, 0, GSTORAGEREFUND), # 0) if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . world . set_storage_data ( storage_address , offset , value ) self . _publish ( 'did_evm_write_storage' , storage_address , offset , value )
1152	def warn ( message , category = None , stacklevel = 1 ) : # Check if message is already a Warning object if isinstance ( message , Warning ) : category = message . __class__ # Check category argument if category is None : category = UserWarning assert issubclass ( category , Warning ) # Get context information try : caller = sys . _getframe ( stacklevel ) except ValueError : globals = sys . __dict__ lineno = 1 else : globals = caller . f_globals lineno = caller . f_lineno if '__name__' in globals : module = globals [ '__name__' ] else : module = "<string>" filename = globals . get ( '__file__' ) if filename : fnl = filename . lower ( ) if fnl . endswith ( ( ".pyc" , ".pyo" ) ) : filename = filename [ : - 1 ] else : if module == "__main__" : try : filename = sys . argv [ 0 ] except AttributeError : # embedded interpreters don't have sys.argv, see bug #839151 filename = '__main__' if not filename : filename = module registry = globals . setdefault ( "__warningregistry__" , { } ) warn_explicit ( message , category , filename , lineno , module , registry , globals )
1454	def add_data_tuple ( self , stream_id , new_data_tuple , tuple_size_in_bytes ) : if ( self . current_data_tuple_set is None ) or ( self . current_data_tuple_set . stream . id != stream_id ) or ( len ( self . current_data_tuple_set . tuples ) >= self . data_tuple_set_capacity ) or ( self . current_data_tuple_size_in_bytes >= self . max_data_tuple_size_in_bytes ) : self . _init_new_data_tuple ( stream_id ) added_tuple = self . current_data_tuple_set . tuples . add ( ) added_tuple . CopyFrom ( new_data_tuple ) self . current_data_tuple_size_in_bytes += tuple_size_in_bytes self . total_data_emitted_in_bytes += tuple_size_in_bytes
11178	def get_separator ( self , i ) : return i and self . separator [ min ( i - 1 , len ( self . separator ) - 1 ) ] or ''
11061	def send_message ( self , channel , text , thread = None , reply_broadcast = None ) : # This doesn't want the # in the channel name if isinstance ( channel , SlackRoomIMBase ) : channel = channel . id self . log . debug ( "Trying to send to %s: %s" , channel , text ) self . sc . rtm_send_message ( channel , text , thread = thread , reply_broadcast = reply_broadcast )
28	def mpi_fork ( n , extra_mpi_args = [ ] ) : if n <= 1 : return "child" if os . getenv ( "IN_MPI" ) is None : env = os . environ . copy ( ) env . update ( MKL_NUM_THREADS = "1" , OMP_NUM_THREADS = "1" , IN_MPI = "1" ) # "-bind-to core" is crucial for good performance args = [ "mpirun" , "-np" , str ( n ) ] + extra_mpi_args + [ sys . executable ] args += sys . argv subprocess . check_call ( args , env = env ) return "parent" else : install_mpi_excepthook ( ) return "child"
10137	def _assert_version ( self , version ) : if self . nearest_version < version : if self . _version_given : raise ValueError ( 'Data type requires version %s' % version ) else : self . _version = version
10106	def _process_tabs ( self , tabs , current_tab , group_current_tab ) : # Update references to the current tab for t in tabs : t . current_tab = current_tab t . group_current_tab = group_current_tab # Filter out hidden tabs tabs = list ( filter ( lambda t : t . tab_visible , tabs ) ) # Sort remaining tabs in-place tabs . sort ( key = lambda t : t . weight ) return tabs
8314	def parse ( self , light = False ) : markup = self . markup self . disambiguation = self . parse_disambiguation ( markup ) self . categories = self . parse_categories ( markup ) self . links = self . parse_links ( markup ) if not light : # Conversion of HTML markup to Wikipedia markup. markup = self . convert_pre ( markup ) markup = self . convert_li ( markup ) markup = self . convert_table ( markup ) markup = replace_entities ( markup ) # Harvest references from the markup # and replace them by footnotes. markup = markup . replace ( "{{Cite" , "{{cite" ) markup = re . sub ( "\{\{ {1,2}cite" , "{{cite" , markup ) self . references , markup = self . parse_references ( markup ) # Make sure there are no legend linebreaks in image links. # Then harvest images and strip them from the markup. markup = re . sub ( "\n+(\{\{legend)" , "\\1" , markup ) self . images , markup = self . parse_images ( markup ) self . images . extend ( self . parse_gallery_images ( markup ) ) self . paragraphs = self . parse_paragraphs ( markup ) self . tables = self . parse_tables ( markup ) self . translations = self . parse_translations ( markup ) self . important = self . parse_important ( markup )
1842	def JNP ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . PF , target . read ( ) , cpu . PC )
11560	def i2c_stop_reading ( self , address ) : data = [ address , self . I2C_STOP_READING ] self . _command_handler . send_sysex ( self . _command_handler . I2C_REQUEST , data )
7599	def get_popular_players ( self , * * params : keys ) : url = self . api . POPULAR + '/players' return self . _get_model ( url , PartialPlayerClan , * * params )
7180	def reapply_all ( ast_node , lib2to3_node ) : late_processing = reapply ( ast_node , lib2to3_node ) for lazy_func in reversed ( late_processing ) : lazy_func ( )
6278	def draw ( self , current_time , frame_time ) : self . set_default_viewport ( ) self . timeline . draw ( current_time , frame_time , self . fbo )
11453	def convert_all ( cls , records ) : out = [ "<collection>" ] for rec in records : conversion = cls ( rec ) out . append ( conversion . convert ( ) ) out . append ( "</collection>" ) return "\n" . join ( out )
8291	def _keywords ( self ) : meta = self . find ( "meta" , { "name" : "keywords" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : keywords = [ k . strip ( ) for k in meta [ "content" ] . split ( "," ) ] else : keywords = [ ] return keywords
3418	def load_matlab_model ( infile_path , variable_name = None , inf = inf ) : if not scipy_io : raise ImportError ( 'load_matlab_model requires scipy' ) data = scipy_io . loadmat ( infile_path ) possible_names = [ ] if variable_name is None : # skip meta variables meta_vars = { "__globals__" , "__header__" , "__version__" } possible_names = sorted ( i for i in data if i not in meta_vars ) if len ( possible_names ) == 1 : variable_name = possible_names [ 0 ] if variable_name is not None : return from_mat_struct ( data [ variable_name ] , model_id = variable_name , inf = inf ) for possible_name in possible_names : try : return from_mat_struct ( data [ possible_name ] , model_id = possible_name , inf = inf ) except ValueError : pass # If code here is executed, then no model was found. raise IOError ( "no COBRA model found" )
3330	def acquire_write ( self , timeout = None ) : if timeout is not None : endtime = time ( ) + timeout me , upgradewriter = currentThread ( ) , False self . __condition . acquire ( ) try : if self . __writer is me : # If we are the writer, grant a new write lock, always. self . __writercount += 1 return elif me in self . __readers : # If we are a reader, no need to add us to pendingwriters, # we get the upgradewriter slot. if self . __upgradewritercount : # If we are a reader and want to upgrade, and someone # else also wants to upgrade, there is no way we can do # this except if one of us releases all his read locks. # Signal this to user. raise ValueError ( "Inevitable dead lock, denying write lock" ) upgradewriter = True self . __upgradewritercount = self . __readers . pop ( me ) else : # We aren't a reader, so add us to the pending writers queue # for synchronization with the readers. self . __pendingwriters . append ( me ) while True : if not self . __readers and self . __writer is None : # Only test anything if there are no readers and writers. if self . __upgradewritercount : if upgradewriter : # There is a writer to upgrade, and it's us. Take # the write lock. self . __writer = me self . __writercount = self . __upgradewritercount + 1 self . __upgradewritercount = 0 return # There is a writer to upgrade, but it's not us. # Always leave the upgrade writer the advance slot, # because he presumes he'll get a write lock directly # from a previously held read lock. elif self . __pendingwriters [ 0 ] is me : # If there are no readers and writers, it's always # fine for us to take the writer slot, removing us # from the pending writers queue. # This might mean starvation for readers, though. self . __writer = me self . __writercount = 1 self . __pendingwriters = self . __pendingwriters [ 1 : ] return if timeout is not None : remaining = endtime - time ( ) if remaining <= 0 : # Timeout has expired, signal caller of this. if upgradewriter : # Put us back on the reader queue. No need to # signal anyone of this change, because no other # writer could've taken our spot before we got # here (because of remaining readers), as the test # for proper conditions is at the start of the # loop, not at the end. self . __readers [ me ] = self . __upgradewritercount self . __upgradewritercount = 0 else : # We were a simple pending writer, just remove us # from the FIFO list. self . __pendingwriters . remove ( me ) raise RuntimeError ( "Acquiring write lock timed out" ) self . __condition . wait ( remaining ) else : self . __condition . wait ( ) finally : self . __condition . release ( )
108	def draw_grid ( images , rows = None , cols = None ) : nb_images = len ( images ) do_assert ( nb_images > 0 ) if is_np_array ( images ) : do_assert ( images . ndim == 4 ) else : do_assert ( is_iterable ( images ) and is_np_array ( images [ 0 ] ) and images [ 0 ] . ndim == 3 ) dts = [ image . dtype . name for image in images ] nb_dtypes = len ( set ( dts ) ) do_assert ( nb_dtypes == 1 , ( "All images provided to draw_grid() must have the same dtype, " + "found %d dtypes (%s)" ) % ( nb_dtypes , ", " . join ( dts ) ) ) cell_height = max ( [ image . shape [ 0 ] for image in images ] ) cell_width = max ( [ image . shape [ 1 ] for image in images ] ) channels = set ( [ image . shape [ 2 ] for image in images ] ) do_assert ( len ( channels ) == 1 , "All images are expected to have the same number of channels, " + "but got channel set %s with length %d instead." % ( str ( channels ) , len ( channels ) ) ) nb_channels = list ( channels ) [ 0 ] if rows is None and cols is None : rows = cols = int ( math . ceil ( math . sqrt ( nb_images ) ) ) elif rows is not None : cols = int ( math . ceil ( nb_images / rows ) ) elif cols is not None : rows = int ( math . ceil ( nb_images / cols ) ) do_assert ( rows * cols >= nb_images ) width = cell_width * cols height = cell_height * rows dt = images . dtype if is_np_array ( images ) else images [ 0 ] . dtype grid = np . zeros ( ( height , width , nb_channels ) , dtype = dt ) cell_idx = 0 for row_idx in sm . xrange ( rows ) : for col_idx in sm . xrange ( cols ) : if cell_idx < nb_images : image = images [ cell_idx ] cell_y1 = cell_height * row_idx cell_y2 = cell_y1 + image . shape [ 0 ] cell_x1 = cell_width * col_idx cell_x2 = cell_x1 + image . shape [ 1 ] grid [ cell_y1 : cell_y2 , cell_x1 : cell_x2 , : ] = image cell_idx += 1 return grid
2622	def spin_up_instance ( self , command , job_name ) : command = Template ( template_string ) . substitute ( jobname = job_name , user_script = command , linger = str ( self . linger ) . lower ( ) , worker_init = self . worker_init ) instance_type = self . instance_type subnet = self . sn_ids [ 0 ] ami_id = self . image_id total_instances = len ( self . instances ) if float ( self . spot_max_bid ) > 0 : spot_options = { 'MarketType' : 'spot' , 'SpotOptions' : { 'MaxPrice' : str ( self . spot_max_bid ) , 'SpotInstanceType' : 'one-time' , 'InstanceInterruptionBehavior' : 'terminate' } } else : spot_options = { } if total_instances > self . max_nodes : logger . warn ( "Exceeded instance limit ({}). Cannot continue\n" . format ( self . max_nodes ) ) return [ None ] try : tag_spec = [ { "ResourceType" : "instance" , "Tags" : [ { 'Key' : 'Name' , 'Value' : job_name } ] } ] instance = self . ec2 . create_instances ( MinCount = 1 , MaxCount = 1 , InstanceType = instance_type , ImageId = ami_id , KeyName = self . key_name , SubnetId = subnet , SecurityGroupIds = [ self . sg_id ] , TagSpecifications = tag_spec , InstanceMarketOptions = spot_options , InstanceInitiatedShutdownBehavior = 'terminate' , IamInstanceProfile = { 'Arn' : self . iam_instance_profile_arn } , UserData = command ) except ClientError as e : print ( e ) logger . error ( e . response ) return [ None ] except Exception as e : logger . error ( "Request for EC2 resources failed : {0}" . format ( e ) ) return [ None ] self . instances . append ( instance [ 0 ] . id ) logger . info ( "Started up 1 instance {} . Instance type:{}" . format ( instance [ 0 ] . id , instance_type ) ) return instance
8374	def var_added ( self , v ) : self . add_variable ( v ) self . window . set_size_request ( 400 , 35 * len ( self . widgets . keys ( ) ) ) self . window . show_all ( )
1615	def ReplaceAll ( pattern , rep , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . sub ( rep , s )
6480	def null ( self ) : if not self . option . axis : return - 1 else : return self . screen . height - ( - self . minimum * 4.0 / self . extents * self . size . y )
13309	def fmt ( a , b ) : return 100 * np . min ( [ a , b ] , axis = 0 ) . sum ( ) / np . max ( [ a , b ] , axis = 0 ) . sum ( )
3632	def baseId ( resource_id , return_version = False ) : version = 0 resource_id = resource_id + 0xC4000000 # 3288334336 # TODO: version is broken due ^^, needs refactoring while resource_id > 0x01000000 : # 16777216 version += 1 if version == 1 : resource_id -= 0x80000000 # 2147483648 # 0x50000000 # 1342177280 ? || 0x2000000 # 33554432 elif version == 2 : resource_id -= 0x03000000 # 50331648 else : resource_id -= 0x01000000 # 16777216 if return_version : return resource_id , version - 67 # just correct "magic number" return resource_id
7380	async def run ( self , * args , data ) : cmd = self . _get ( data . text ) try : if cmd is not None : command = self [ cmd ] ( * args , data = data ) return await peony . utils . execute ( command ) except : fmt = "Error occurred while running function {cmd}:" peony . utils . log_error ( fmt . format ( cmd = cmd ) )
1528	def pick_unused_port ( self ) : s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind ( ( '127.0.0.1' , 0 ) ) _ , port = s . getsockname ( ) s . close ( ) return port
4349	def vad ( self , location = 1 , normalize = True , activity_threshold = 7.0 , min_activity_duration = 0.25 , initial_search_buffer = 1.0 , max_gap = 0.25 , initial_pad = 0.0 ) : if location not in [ - 1 , 1 ] : raise ValueError ( "location must be -1 or 1." ) if not isinstance ( normalize , bool ) : raise ValueError ( "normalize muse be a boolean." ) if not is_number ( activity_threshold ) : raise ValueError ( "activity_threshold must be a number." ) if not is_number ( min_activity_duration ) or min_activity_duration < 0 : raise ValueError ( "min_activity_duration must be a positive number" ) if not is_number ( initial_search_buffer ) or initial_search_buffer < 0 : raise ValueError ( "initial_search_buffer must be a positive number" ) if not is_number ( max_gap ) or max_gap < 0 : raise ValueError ( "max_gap must be a positive number." ) if not is_number ( initial_pad ) or initial_pad < 0 : raise ValueError ( "initial_pad must be a positive number." ) effect_args = [ ] if normalize : effect_args . append ( 'norm' ) if location == - 1 : effect_args . append ( 'reverse' ) effect_args . extend ( [ 'vad' , '-t' , '{:f}' . format ( activity_threshold ) , '-T' , '{:f}' . format ( min_activity_duration ) , '-s' , '{:f}' . format ( initial_search_buffer ) , '-g' , '{:f}' . format ( max_gap ) , '-p' , '{:f}' . format ( initial_pad ) ] ) if location == - 1 : effect_args . append ( 'reverse' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'vad' ) return self
9329	def post ( self , url , headers = None , params = None , * * kwargs ) : if len ( kwargs ) > 1 : raise InvalidArgumentsError ( "Too many extra args ({} > 1)" . format ( len ( kwargs ) ) ) if kwargs : kwarg = next ( iter ( kwargs ) ) if kwarg not in ( "json" , "data" ) : raise InvalidArgumentsError ( "Invalid kwarg: " + kwarg ) resp = self . session . post ( url , headers = headers , params = params , * * kwargs ) resp . raise_for_status ( ) return _to_json ( resp )
6073	def mass_within_circle_in_units ( self , radius , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . mass_within_circle_in_units ( radius = radius , unit_mass = unit_mass , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
10539	def update_category ( category ) : try : res = _pybossa_req ( 'put' , 'category' , category . id , payload = category . data ) if res . get ( 'id' ) : return Category ( res ) else : return res except : # pragma: no cover raise
2217	def _list_itemstrs ( list_ , * * kwargs ) : items = list ( list_ ) kwargs [ '_return_info' ] = True _tups = [ repr2 ( item , * * kwargs ) for item in items ] itemstrs = [ t [ 0 ] for t in _tups ] max_height = max ( [ t [ 1 ] [ 'max_height' ] for t in _tups ] ) if _tups else 0 _leaf_info = { 'max_height' : max_height + 1 , } sort = kwargs . get ( 'sort' , None ) if sort is None : # Force orderings on sets. sort = isinstance ( list_ , ( set , frozenset ) ) if sort : itemstrs = _sort_itemstrs ( items , itemstrs ) return itemstrs , _leaf_info
10458	def clearContents ( cls ) : log_msg = 'Request to clear contents of pasteboard: general' logging . debug ( log_msg ) pb = AppKit . NSPasteboard . generalPasteboard ( ) pb . clearContents ( ) return True
11490	def _download_item ( item_id , path = '.' , item = None ) : session . token = verify_credentials ( ) filename , content_iter = session . communicator . download_item ( item_id , session . token ) item_path = os . path . join ( path , filename ) print ( 'Creating file at {0}' . format ( item_path ) ) out_file = open ( item_path , 'wb' ) for block in content_iter : out_file . write ( block ) out_file . close ( ) for callback in session . item_download_callbacks : if not item : item = session . communicator . item_get ( session . token , item_id ) callback ( session . communicator , session . token , item , item_path )
2245	def hzcat ( args , sep = '' ) : import unicodedata if '\n' in sep or '\r' in sep : raise ValueError ( '`sep` cannot contain newline characters' ) # TODO: ensure unicode data works correctly for python2 args = [ unicodedata . normalize ( 'NFC' , ensure_unicode ( val ) ) for val in args ] arglines = [ a . split ( '\n' ) for a in args ] height = max ( map ( len , arglines ) ) # Do vertical padding arglines = [ lines + [ '' ] * ( height - len ( lines ) ) for lines in arglines ] # Initialize output all_lines = [ '' for _ in range ( height ) ] width = 0 n_args = len ( args ) for sx , lines in enumerate ( arglines ) : # Concatenate the new string for lx , line in enumerate ( lines ) : all_lines [ lx ] += line # Find the new maximum horizontal width width = max ( width , max ( map ( len , all_lines ) ) ) if sx < n_args - 1 : # Horizontal padding on all but last iter for lx , line in list ( enumerate ( all_lines ) ) : residual = width - len ( line ) all_lines [ lx ] = line + ( ' ' * residual ) + sep width += len ( sep ) # Clean up trailing whitespace all_lines = [ line . rstrip ( ' ' ) for line in all_lines ] ret = '\n' . join ( all_lines ) return ret
414	def delete_model ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) self . db . Model . delete_many ( kwargs ) logging . info ( "[Database] Delete Model SUCCESS" )
716	def __loadHyperSearchJobID ( cls , permWorkDir , outputLabel ) : filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , outputLabel = outputLabel ) jobID = None with open ( filePath , "r" ) as jobIdPickleFile : jobInfo = pickle . load ( jobIdPickleFile ) jobID = jobInfo [ "hyperSearchJobID" ] return jobID
11885	def connect ( self ) : try : self . _socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) self . _socket . settimeout ( TIMEOUT_SECONDS ) self . _socket . connect ( ( self . _ip , self . _port ) ) _LOGGER . debug ( "Successfully created Hub at %s:%s :)" , self . _ip , self . _port ) except socket . error as error : _LOGGER . error ( "Error creating Hub: %s :(" , error ) self . _socket . close ( )
10951	def update ( self , params , values ) : return super ( State , self ) . update ( params , values )
12804	def get_user ( self , id = None ) : if not id : id = self . _user . id if id not in self . _users : self . _users [ id ] = self . _user if id == self . _user . id else User ( self , id ) return self . _users [ id ]
12209	def get_cache_key ( user_or_username , size , prefix ) : if isinstance ( user_or_username , get_user_model ( ) ) : user_or_username = user_or_username . username return '%s_%s_%s' % ( prefix , user_or_username , size )
4518	def fillTriangle ( self , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : md . fill_triangle ( self . set , x0 , y0 , x1 , y1 , x2 , y2 , color , aa )
6360	def needleman_wunsch ( src , tar , gap_cost = 1 , sim_func = sim_ident ) : return NeedlemanWunsch ( ) . dist_abs ( src , tar , gap_cost , sim_func )
3307	def _run_cheroot ( app , config , mode ) : assert mode == "cheroot" try : from cheroot import server , wsgi # from cheroot.ssl.builtin import BuiltinSSLAdapter # import cheroot.ssl.pyopenssl except ImportError : _logger . error ( "*" * 78 ) _logger . error ( "ERROR: Could not import Cheroot." ) _logger . error ( "Try `pip install cheroot` or specify another server using the --server option." ) _logger . error ( "*" * 78 ) raise server_name = "WsgiDAV/{} {} Python/{}" . format ( __version__ , wsgi . Server . version , util . PYTHON_VERSION ) wsgi . Server . version = server_name # Support SSL ssl_certificate = _get_checked_path ( config . get ( "ssl_certificate" ) , config ) ssl_private_key = _get_checked_path ( config . get ( "ssl_private_key" ) , config ) ssl_certificate_chain = _get_checked_path ( config . get ( "ssl_certificate_chain" ) , config ) ssl_adapter = config . get ( "ssl_adapter" , "builtin" ) protocol = "http" if ssl_certificate and ssl_private_key : ssl_adapter = server . get_ssl_adapter_class ( ssl_adapter ) wsgi . Server . ssl_adapter = ssl_adapter ( ssl_certificate , ssl_private_key , ssl_certificate_chain ) protocol = "https" _logger . info ( "SSL / HTTPS enabled. Adapter: {}" . format ( ssl_adapter ) ) elif ssl_certificate or ssl_private_key : raise RuntimeError ( "Option 'ssl_certificate' and 'ssl_private_key' must be used together." ) # elif ssl_adapter: # print("WARNING: Ignored option 'ssl_adapter' (requires 'ssl_certificate').") _logger . info ( "Running {}" . format ( server_name ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , "server_name" : server_name , } # Override or add custom args server_args . update ( config . get ( "server_args" , { } ) ) server = wsgi . Server ( * * server_args ) # If the caller passed a startup event, monkey patch the server to set it # when the request handler loop is entered startup_event = config . get ( "startup_event" ) if startup_event : def _patched_tick ( ) : server . tick = org_tick # undo the monkey patch _logger . info ( "wsgi.Server is ready" ) startup_event . set ( ) org_tick ( ) org_tick = server . tick server . tick = _patched_tick try : server . start ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) finally : server . stop ( ) return
10432	def selectrowindex ( self , window_name , object_name , row_index ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) count = len ( object_handle . AXRows ) if row_index < 0 or row_index > count : raise LdtpServerException ( 'Row index out of range: %d' % row_index ) cell = object_handle . AXRows [ row_index ] if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : # Selected pass return 1
4908	def _post ( self , url , data , scope ) : self . _create_session ( scope ) response = self . session . post ( url , data = data ) return response . status_code , response . text
9794	def _ignore_path ( cls , path , ignore_list = None , white_list = None ) : ignore_list = ignore_list or [ ] white_list = white_list or [ ] return ( cls . _matches_patterns ( path , ignore_list ) and not cls . _matches_patterns ( path , white_list ) )
6889	def _parallel_bls_worker ( task ) : try : times , mags , errs = task [ : 3 ] magsarefluxes = task [ 3 ] minfreq , nfreq , stepsize = task [ 4 : 7 ] ndurations , mintransitduration , maxtransitduration = task [ 7 : 10 ] blsobjective , blsmethod , blsoversample = task [ 10 : ] frequencies = minfreq + nparange ( nfreq ) * stepsize periods = 1.0 / frequencies # astropy's BLS requires durations in units of time durations = nplinspace ( mintransitduration * periods . min ( ) , maxtransitduration * periods . min ( ) , ndurations ) # set up the correct units for the BLS model if magsarefluxes : blsmodel = BoxLeastSquares ( times * u . day , mags * u . dimensionless_unscaled , dy = errs * u . dimensionless_unscaled ) else : blsmodel = BoxLeastSquares ( times * u . day , mags * u . mag , dy = errs * u . mag ) blsresult = blsmodel . power ( periods * u . day , durations * u . day , objective = blsobjective , method = blsmethod , oversample = blsoversample ) return { 'blsresult' : blsresult , 'blsmodel' : blsmodel , 'durations' : durations , 'power' : nparray ( blsresult . power ) } except Exception as e : LOGEXCEPTION ( 'BLS for frequency chunk: (%.6f, %.6f) failed.' % ( frequencies [ 0 ] , frequencies [ - 1 ] ) ) return { 'blsresult' : None , 'blsmodel' : None , 'durations' : durations , 'power' : nparray ( [ npnan for x in range ( nfreq ) ] ) , }
5462	def _emit_search_criteria ( user_ids , job_ids , task_ids , labels ) : print ( 'Delete running jobs:' ) print ( ' user:' ) print ( ' %s\n' % user_ids ) print ( ' job-id:' ) print ( ' %s\n' % job_ids ) if task_ids : print ( ' task-id:' ) print ( ' %s\n' % task_ids ) # Labels are in a LabelParam namedtuple and must be reformated for printing. if labels : print ( ' labels:' ) print ( ' %s\n' % repr ( labels ) )
8861	def defined_names ( request_data ) : global _old_definitions ret_val = [ ] path = request_data [ 'path' ] toplvl_definitions = jedi . names ( request_data [ 'code' ] , path , 'utf-8' ) for d in toplvl_definitions : definition = _extract_def ( d , path ) if d . type != 'import' : ret_val . append ( definition ) ret_val = [ d . to_dict ( ) for d in ret_val ] return ret_val
9618	def UnPlug ( self , force = False ) : if force : _xinput . UnPlugForce ( c_uint ( self . id ) ) else : _xinput . UnPlug ( c_uint ( self . id ) ) while self . id not in self . available_ids ( ) : if self . id == 0 : break
460	def evaluation ( y_test = None , y_predict = None , n_classes = None ) : c_mat = confusion_matrix ( y_test , y_predict , labels = [ x for x in range ( n_classes ) ] ) f1 = f1_score ( y_test , y_predict , average = None , labels = [ x for x in range ( n_classes ) ] ) f1_macro = f1_score ( y_test , y_predict , average = 'macro' ) acc = accuracy_score ( y_test , y_predict ) tl . logging . info ( 'confusion matrix: \n%s' % c_mat ) tl . logging . info ( 'f1-score : %s' % f1 ) tl . logging . info ( 'f1-score(macro) : %f' % f1_macro ) # same output with > f1_score(y_true, y_pred, average='macro') tl . logging . info ( 'accuracy-score : %f' % acc ) return c_mat , f1 , acc , f1_macro
72	def deepcopy ( self ) : # Manual copy is far faster than deepcopy for BoundingBoxesOnImage, # so use manual copy here too bbs = [ bb . deepcopy ( ) for bb in self . bounding_boxes ] return BoundingBoxesOnImage ( bbs , tuple ( self . shape ) )
12816	def _send_to_consumer ( self , block ) : self . _consumer . write ( block ) self . _sent += len ( block ) if self . _callback : self . _callback ( self . _sent , self . length )
13370	def is_redirecting ( path ) : candidate = unipath ( path , '.cpenv' ) return os . path . exists ( candidate ) and os . path . isfile ( candidate )
3997	def copy_between_containers ( source_name , source_path , dest_name , dest_path ) : if not container_path_exists ( source_name , source_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist inside container {}.' . format ( source_path , source_name ) ) temp_path = os . path . join ( tempfile . mkdtemp ( ) , str ( uuid . uuid1 ( ) ) ) with _cleanup_path ( temp_path ) : copy_to_local ( temp_path , source_name , source_path , demote = False ) copy_from_local ( temp_path , dest_name , dest_path , demote = False )
7475	def dask_chroms ( data , samples ) : ## example concatenating with dask h5s = [ os . path . join ( data . dirs . across , s . name + ".tmp.h5" ) for s in samples ] handles = [ h5py . File ( i ) for i in h5s ] dsets = [ i [ '/ichrom' ] for i in handles ] arrays = [ da . from_array ( dset , chunks = ( 10000 , 3 ) ) for dset in dsets ] stack = da . stack ( arrays , axis = 2 ) ## max chrom (should we check for variable hits? if so, things can get wonk) maxchrom = da . max ( stack , axis = 2 ) [ : , 0 ] ## max pos maxpos = da . max ( stack , axis = 2 ) [ : , 2 ] ## min pos mask = stack == 0 stack [ mask ] = 9223372036854775807 ## max int64 value minpos = da . min ( stack , axis = 2 ) [ : , 1 ] final = da . stack ( [ maxchrom , minpos , maxpos ] , axis = 1 ) final . to_hdf5 ( data . clust_database , "/chroms" ) ## close the h5 handles _ = [ i . close ( ) for i in handles ]
13262	def task ( func , * * config ) : if func . __name__ == func . __qualname__ : assert not func . __qualname__ in _task_list , "Can not define the same task \"{}\" twice" . format ( func . __qualname__ ) logger . debug ( "Found task %s" , func ) _task_list [ func . __qualname__ ] = Task ( plugin_class = None , func = func , config = config ) else : func . yaz_task_config = config return func
11785	def attrnum ( self , attr ) : if attr < 0 : return len ( self . attrs ) + attr elif isinstance ( attr , str ) : return self . attrnames . index ( attr ) else : return attr
4362	def _heartbeat ( self ) : interval = self . config [ 'heartbeat_interval' ] while self . connected : gevent . sleep ( interval ) # TODO: this process could use a timeout object like the disconnect # timeout thing, and ONLY send packets when none are sent! # We would do that by calling timeout.set() for a "sending" # timeout. If we're sending 100 messages a second, there is # no need to push some heartbeats in there also. self . put_client_msg ( "2::" )
7670	def add ( self , jam , on_conflict = 'fail' ) : if on_conflict not in [ 'overwrite' , 'fail' , 'ignore' ] : raise ParameterError ( "on_conflict='{}' is not in ['fail', " "'overwrite', 'ignore']." . format ( on_conflict ) ) if not self . file_metadata == jam . file_metadata : if on_conflict == 'overwrite' : self . file_metadata = jam . file_metadata elif on_conflict == 'fail' : raise JamsError ( "Metadata conflict! " "Resolve manually or force-overwrite it." ) self . annotations . extend ( jam . annotations ) self . sandbox . update ( * * jam . sandbox )
1644	def GetPreviousNonBlankLine ( clean_lines , linenum ) : prevlinenum = linenum - 1 while prevlinenum >= 0 : prevline = clean_lines . elided [ prevlinenum ] if not IsBlankLine ( prevline ) : # if not a blank line... return ( prevline , prevlinenum ) prevlinenum -= 1 return ( '' , - 1 )
6462	def filter_symlog ( y , base = 10.0 ) : log_base = np . log ( base ) sign = np . sign ( y ) logs = np . log ( np . abs ( y ) / log_base ) return sign * logs
8421	def _format ( formatter , x ) : # For MPL to play nice formatter . create_dummy_axis ( ) # For sensible decimal places formatter . set_locs ( [ val for val in x if ~ np . isnan ( val ) ] ) try : oom = int ( formatter . orderOfMagnitude ) except AttributeError : oom = 0 labels = [ formatter ( tick ) for tick in x ] # Remove unnecessary decimals pattern = re . compile ( r'\.0+$' ) for i , label in enumerate ( labels ) : match = pattern . search ( label ) if match : labels [ i ] = pattern . sub ( '' , label ) # MPL does not add the exponential component if oom : labels = [ '{}e{}' . format ( s , oom ) if s != '0' else s for s in labels ] return labels
5639	def _temporal_distance_pdf ( self ) : temporal_distance_split_points_ordered , norm_cdf = self . _temporal_distance_cdf ( ) delta_peak_loc_to_probability_mass = { } non_delta_peak_split_points = [ temporal_distance_split_points_ordered [ 0 ] ] non_delta_peak_densities = [ ] for i in range ( 0 , len ( temporal_distance_split_points_ordered ) - 1 ) : left = temporal_distance_split_points_ordered [ i ] right = temporal_distance_split_points_ordered [ i + 1 ] width = right - left prob_mass = norm_cdf [ i + 1 ] - norm_cdf [ i ] if width == 0.0 : delta_peak_loc_to_probability_mass [ left ] = prob_mass else : non_delta_peak_split_points . append ( right ) non_delta_peak_densities . append ( prob_mass / float ( width ) ) assert ( len ( non_delta_peak_densities ) == len ( non_delta_peak_split_points ) - 1 ) return numpy . array ( non_delta_peak_split_points ) , numpy . array ( non_delta_peak_densities ) , delta_peak_loc_to_probability_mass
546	def _writePrediction ( self , result ) : self . __predictionCache . append ( result ) if self . _isBestModel : self . __flushPredictionCache ( )
2258	def argsort ( indexable , key = None , reverse = False ) : # Create an iterator of value/key pairs if isinstance ( indexable , collections_abc . Mapping ) : vk_iter = ( ( v , k ) for k , v in indexable . items ( ) ) else : vk_iter = ( ( v , k ) for k , v in enumerate ( indexable ) ) # Sort by values and extract the indices if key is None : indices = [ k for v , k in sorted ( vk_iter , reverse = reverse ) ] else : # If key is provided, call it using the value as input indices = [ k for v , k in sorted ( vk_iter , key = lambda vk : key ( vk [ 0 ] ) , reverse = reverse ) ] return indices
13576	def submit ( course , tid = None , pastebin = False , review = False ) : if tid is not None : return submit_exercise ( Exercise . byid ( tid ) , pastebin = pastebin , request_review = review ) else : sel = Exercise . get_selected ( ) if not sel : raise NoExerciseSelected ( ) return submit_exercise ( sel , pastebin = pastebin , request_review = review )
3469	def copy ( self ) : # no references to model when copying model = self . _model self . _model = None for i in self . _metabolites : i . _model = None for i in self . _genes : i . _model = None # now we can copy new_reaction = deepcopy ( self ) # restore the references self . _model = model for i in self . _metabolites : i . _model = model for i in self . _genes : i . _model = model return new_reaction
10227	def get_triangles ( graph : DiGraph ) -> SetOfNodeTriples : return { tuple ( sorted ( [ a , b , c ] , key = str ) ) for a , b in graph . edges ( ) for c in graph . successors ( b ) if graph . has_edge ( c , a ) }
6405	def cmp_features ( feat1 , feat2 ) : if feat1 < 0 or feat2 < 0 : return - 1.0 if feat1 == feat2 : return 1.0 magnitude = len ( _FEATURE_MASK ) featxor = feat1 ^ feat2 diffbits = 0 # print(featxor) while featxor : if featxor & 0b1 : diffbits += 1 featxor >>= 1 # print(diffbits) return 1 - ( diffbits / ( 2 * magnitude ) )
5785	def _raw_read ( self ) : data = self . _raw_bytes try : data += self . _socket . recv ( 8192 ) except ( socket_ . error ) : pass output = data written = libssl . BIO_write ( self . _rbio , data , len ( data ) ) self . _raw_bytes = data [ written : ] return output
11843	def step ( self ) : if not self . is_done ( ) : actions = [ agent . program ( self . percept ( agent ) ) for agent in self . agents ] for ( agent , action ) in zip ( self . agents , actions ) : self . execute_action ( agent , action ) self . exogenous_change ( )
4591	def to_triplets ( colors ) : try : colors [ 0 ] [ 0 ] return colors except : pass # It's a 1-dimensional list extra = len ( colors ) % 3 if extra : colors = colors [ : - extra ] return list ( zip ( * [ iter ( colors ) ] * 3 ) )
9070	def value ( self ) : from numpy_sugar . linalg import ddot , sum2diag if self . _cache [ "value" ] is not None : return self . _cache [ "value" ] scale = exp ( self . logscale ) delta = 1 / ( 1 + exp ( - self . logitdelta ) ) v0 = scale * ( 1 - delta ) v1 = scale * delta mu = self . eta / self . tau n = len ( mu ) if self . _QS is None : K = zeros ( ( n , n ) ) else : Q0 = self . _QS [ 0 ] [ 0 ] S0 = self . _QS [ 1 ] K = dot ( ddot ( Q0 , S0 ) , Q0 . T ) A = sum2diag ( sum2diag ( v0 * K , v1 ) , 1 / self . tau ) m = mu - self . mean ( ) v = - n * log ( 2 * pi ) v -= slogdet ( A ) [ 1 ] v -= dot ( m , solve ( A , m ) ) self . _cache [ "value" ] = v / 2 return self . _cache [ "value" ]
3247	def get_users ( group , * * conn ) : group_details = get_group_api ( group [ 'GroupName' ] , * * conn ) user_list = [ ] for user in group_details . get ( 'Users' , [ ] ) : user_list . append ( user [ 'UserName' ] ) return user_list
9065	def value ( self ) : if not self . _fix [ "beta" ] : self . _update_beta ( ) if not self . _fix [ "scale" ] : self . _update_scale ( ) return self . lml ( )
5329	def get_raw ( config , backend_section , arthur ) : if arthur : task = TaskRawDataArthurCollection ( config , backend_section = backend_section ) else : task = TaskRawDataCollection ( config , backend_section = backend_section ) TaskProjects ( config ) . execute ( ) try : task . execute ( ) logging . info ( "Loading raw data finished!" ) except Exception as e : logging . error ( str ( e ) ) sys . exit ( - 1 )
11301	def populate ( self ) : self . _registry = { } for provider_class in self . _registered_providers : instance = provider_class ( ) self . _registry [ instance ] = instance . regex for stored_provider in StoredProvider . objects . active ( ) : self . _registry [ stored_provider ] = stored_provider . regex self . _populated = True
6745	def iter_sites ( sites = None , site = None , renderer = None , setter = None , no_secure = False , verbose = None ) : if verbose is None : verbose = get_verbose ( ) hostname = get_current_hostname ( ) target_sites = env . available_sites_by_host . get ( hostname , None ) if sites is None : site = site or env . SITE or ALL if site == ALL : sites = list ( six . iteritems ( env . sites ) ) else : sys . stderr . flush ( ) sites = [ ( site , env . sites . get ( site ) ) ] renderer = renderer #or render_remote_paths env_default = save_env ( ) for _site , site_data in sorted ( sites ) : if no_secure and _site . endswith ( '_secure' ) : continue # Only load site configurations that are allowed for this host. if target_sites is None : pass else : assert isinstance ( target_sites , ( tuple , list ) ) if _site not in target_sites : if verbose : print ( 'Skipping site %s because not in among target sites.' % _site ) continue env . update ( env_default ) env . update ( env . sites . get ( _site , { } ) ) env . SITE = _site if callable ( renderer ) : renderer ( ) if setter : setter ( _site ) yield _site , site_data # Revert modified keys. env . update ( env_default ) # Remove keys that were added, not simply updated. added_keys = set ( env ) . difference ( env_default ) for key in added_keys : # Don't remove internally maintained variables, because these are used to cache hostnames # used by iter_sites(). if key . startswith ( '_' ) : continue del env [ key ]
8434	def apply ( cls , x , palette , na_value = None , trans = None ) : if trans is not None : x = trans . transform ( x ) limits = cls . train ( x ) return cls . map ( x , palette , limits , na_value )
11727	def ppdict ( dict_to_print , br = '\n' , html = False , key_align = 'l' , sort_keys = True , key_preffix = '' , key_suffix = '' , value_prefix = '' , value_suffix = '' , left_margin = 3 , indent = 2 ) : if dict_to_print : if sort_keys : dic = dict_to_print . copy ( ) keys = list ( dic . keys ( ) ) keys . sort ( ) dict_to_print = OrderedDict ( ) for k in keys : dict_to_print [ k ] = dic [ k ] tmp = [ '{' ] ks = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . keys ( ) ] vs = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . values ( ) ] max_key_len = max ( [ len ( str ( x ) ) for x in ks ] ) for i in range ( len ( ks ) ) : k = { 1 : str ( ks [ i ] ) . ljust ( max_key_len ) , key_align == 'r' : str ( ks [ i ] ) . rjust ( max_key_len ) } [ 1 ] v = vs [ i ] tmp . append ( ' ' * indent + '{}{}{}:{}{}{},' . format ( key_preffix , k , key_suffix , value_prefix , v , value_suffix ) ) tmp [ - 1 ] = tmp [ - 1 ] [ : - 1 ] # remove the ',' in the last item tmp . append ( '}' ) if left_margin : tmp = [ ' ' * left_margin + x for x in tmp ] if html : return '<code>{}</code>' . format ( br . join ( tmp ) . replace ( ' ' , '&nbsp;' ) ) else : return br . join ( tmp ) else : return '{}'
1048	def print_tb ( tb , limit = None , file = None ) : if file is None : file = sys . stderr if limit is None : if hasattr ( sys , 'tracebacklimit' ) : limit = sys . tracebacklimit n = 0 while tb is not None and ( limit is None or n < limit ) : f = tb . tb_frame lineno = tb . tb_lineno co = f . f_code filename = co . co_filename name = co . co_name _print ( file , ' File "%s", line %d, in %s' % ( filename , lineno , name ) ) linecache . checkcache ( filename ) line = linecache . getline ( filename , lineno , f . f_globals ) if line : _print ( file , ' ' + line . strip ( ) ) tb = tb . tb_next n = n + 1
7304	def set_permissions_in_context ( self , context = { } ) : context [ 'has_view_permission' ] = self . mongoadmin . has_view_permission ( self . request ) context [ 'has_edit_permission' ] = self . mongoadmin . has_edit_permission ( self . request ) context [ 'has_add_permission' ] = self . mongoadmin . has_add_permission ( self . request ) context [ 'has_delete_permission' ] = self . mongoadmin . has_delete_permission ( self . request ) return context
1785	def CMPXCHG ( cpu , dest , src ) : size = dest . size reg_name = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ size ] accumulator = cpu . read_register ( reg_name ) sval = src . read ( ) dval = dest . read ( ) cpu . write_register ( reg_name , dval ) dest . write ( Operators . ITEBV ( size , accumulator == dval , sval , dval ) ) # Affected Flags o..szapc cpu . _calculate_CMP_flags ( size , accumulator - dval , accumulator , dval )
9491	def _get_name_info ( name_index , name_list ) : argval = name_index if name_list is not None : try : argval = name_list [ name_index ] except IndexError : raise ValidationError ( "Names value out of range: {}" . format ( name_index ) ) from None argrepr = argval else : argrepr = repr ( argval ) return argval , argrepr
9139	def label ( labels = [ ] , language = 'any' , sortLabel = False ) : if not labels : return None if not language : language = 'und' labels = [ dict_to_label ( l ) for l in labels ] l = False if sortLabel : l = find_best_label_for_type ( labels , language , 'sortLabel' ) if not l : l = find_best_label_for_type ( labels , language , 'prefLabel' ) if not l : l = find_best_label_for_type ( labels , language , 'altLabel' ) if l : return l else : return label ( labels , 'any' , sortLabel ) if language != 'any' else None
3138	def get ( self , app_id , * * queryparams ) : self . app_id = app_id return self . _mc_client . _get ( url = self . _build_path ( app_id ) , * * queryparams )
10743	def declaration ( function ) : function , name = _strip_function ( function ) if not function . __code__ . co_code in [ empty_function . __code__ . co_code , doc_string_only_function . __code__ . co_code ] : raise ValueError ( 'Declaration requires empty function definition' ) def not_implemented_function ( * args , * * kwargs ) : raise ValueError ( 'Argument \'{}\' did not specify how \'{}\' should act on it' . format ( args [ 0 ] , name ) ) not_implemented_function . __qualname__ = not_implemented_function . __name__ return default ( not_implemented_function , name = name )
4359	def spawn ( self , fn , * args , * * kwargs ) : log . debug ( "Spawning sub-Socket Greenlet: %s" % fn . __name__ ) job = gevent . spawn ( fn , * args , * * kwargs ) self . jobs . append ( job ) return job
11770	def name ( object ) : return ( getattr ( object , 'name' , 0 ) or getattr ( object , '__name__' , 0 ) or getattr ( getattr ( object , '__class__' , 0 ) , '__name__' , 0 ) or str ( object ) )
1799	def CMOVO ( cpu , dest , src ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF , src . read ( ) , dest . read ( ) ) )
4647	def create ( self ) : # pragma: no cover query = ( """ CREATE TABLE {} ( id INTEGER PRIMARY KEY AUTOINCREMENT, {} STRING(256), {} STRING(256) )""" ) . format ( self . __tablename__ , self . __key__ , self . __value__ ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( query ) connection . commit ( )
11603	def parse_byteranges ( cls , environ ) : r = [ ] s = environ . get ( cls . header_range , '' ) . replace ( ' ' , '' ) . lower ( ) if s : l = s . split ( '=' ) if len ( l ) == 2 : unit , vals = tuple ( l ) if unit == 'bytes' and vals : gen_rng = ( tuple ( rng . split ( '-' ) ) for rng in vals . split ( ',' ) if '-' in rng ) for start , end in gen_rng : if start or end : r . append ( ( int ( start ) if start else None , int ( end ) if end else None ) ) return r
8864	def icon_from_typename ( name , icon_type ) : ICONS = { 'CLASS' : ICON_CLASS , 'IMPORT' : ICON_NAMESPACE , 'STATEMENT' : ICON_VAR , 'FORFLOW' : ICON_VAR , 'FORSTMT' : ICON_VAR , 'WITHSTMT' : ICON_VAR , 'GLOBALSTMT' : ICON_VAR , 'MODULE' : ICON_NAMESPACE , 'KEYWORD' : ICON_KEYWORD , 'PARAM' : ICON_VAR , 'ARRAY' : ICON_VAR , 'INSTANCEELEMENT' : ICON_VAR , 'INSTANCE' : ICON_VAR , 'PARAM-PRIV' : ICON_VAR , 'PARAM-PROT' : ICON_VAR , 'FUNCTION' : ICON_FUNC , 'DEF' : ICON_FUNC , 'FUNCTION-PRIV' : ICON_FUNC_PRIVATE , 'FUNCTION-PROT' : ICON_FUNC_PROTECTED } ret_val = None icon_type = icon_type . upper ( ) # jedi 0.8 introduced NamedPart class, which have a string instead of being # one if hasattr ( name , "string" ) : name = name . string if icon_type == "FORFLOW" or icon_type == "STATEMENT" : icon_type = "PARAM" if icon_type == "PARAM" or icon_type == "FUNCTION" : if name . startswith ( "__" ) : icon_type += "-PRIV" elif name . startswith ( "_" ) : icon_type += "-PROT" if icon_type in ICONS : ret_val = ICONS [ icon_type ] elif icon_type : _logger ( ) . warning ( "Unimplemented completion icon_type: %s" , icon_type ) return ret_val
13451	def imgmax ( self ) : if not hasattr ( self , '_imgmax' ) : imgmax = _np . max ( self . images [ 0 ] ) for img in self . images : imax = _np . max ( img ) if imax > imgmax : imgmax = imax self . _imgmax = imgmax return self . _imgmax
4777	def is_empty ( self ) : if len ( self . val ) != 0 : if isinstance ( self . val , str_types ) : self . _err ( 'Expected <%s> to be empty string, but was not.' % self . val ) else : self . _err ( 'Expected <%s> to be empty, but was not.' % self . val ) return self
3210	def get ( self , key , delete_if_expired = True ) : self . _update_cache_stats ( key , None ) if key in self . _CACHE : ( expiration , obj ) = self . _CACHE [ key ] if expiration > self . _now ( ) : self . _update_cache_stats ( key , 'hit' ) return obj else : if delete_if_expired : self . delete ( key ) self . _update_cache_stats ( key , 'expired' ) return None self . _update_cache_stats ( key , 'miss' ) return None
6285	def toggle_pause ( self ) : self . controller . playing = not self . controller . playing self . music . toggle_pause ( )
2809	def convert_constant ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting constant ...' ) params_list = params [ 'value' ] . numpy ( ) def target_layer ( x , value = params_list ) : return tf . constant ( value . tolist ( ) , shape = value . shape ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name + '_np' ] = params_list # ad-hoc layers [ scope_name ] = lambda_layer ( layers [ list ( layers . keys ( ) ) [ 0 ] ] )
10791	def save_wisdom ( wisdomfile ) : if wisdomfile is None : return if wisdomfile : pickle . dump ( pyfftw . export_wisdom ( ) , open ( wisdomfile , 'wb' ) , protocol = 2 )
389	def sequences_get_mask ( sequences , pad_val = 0 ) : mask = np . ones_like ( sequences ) for i , seq in enumerate ( sequences ) : for i_w in reversed ( range ( len ( seq ) ) ) : if seq [ i_w ] == pad_val : mask [ i , i_w ] = 0 else : break # <-- exit the for loop, prepcess next sequence return mask
13021	def process_columns ( self , columns ) : if type ( columns ) == list : self . columns = columns elif type ( columns ) == str : self . columns = [ c . strip ( ) for c in columns . split ( ) ] elif type ( columns ) == IntEnum : self . columns = [ str ( c ) for c in columns ] else : raise RawlException ( "Unknown format for columns" )
3448	def minimal_medium ( model , min_objective_value = 0.1 , exports = False , minimize_components = False , open_exchanges = False ) : exchange_rxns = find_boundary_types ( model , "exchange" ) if isinstance ( open_exchanges , bool ) : open_bound = 1000 else : open_bound = open_exchanges with model as mod : if open_exchanges : LOGGER . debug ( "Opening exchanges for %d imports." , len ( exchange_rxns ) ) for rxn in exchange_rxns : rxn . bounds = ( - open_bound , open_bound ) LOGGER . debug ( "Applying objective value constraints." ) obj_const = mod . problem . Constraint ( mod . objective . expression , lb = min_objective_value , name = "medium_obj_constraint" ) mod . add_cons_vars ( [ obj_const ] ) mod . solver . update ( ) mod . objective = Zero LOGGER . debug ( "Adding new media objective." ) tol = mod . solver . configuration . tolerances . feasibility if minimize_components : add_mip_obj ( mod ) if isinstance ( minimize_components , bool ) : minimize_components = 1 seen = set ( ) best = num_components = mod . slim_optimize ( ) if mod . solver . status != OPTIMAL : LOGGER . warning ( "Minimization of medium was infeasible." ) return None exclusion = mod . problem . Constraint ( Zero , ub = 0 ) mod . add_cons_vars ( [ exclusion ] ) mod . solver . update ( ) media = [ ] for i in range ( minimize_components ) : LOGGER . info ( "Finding alternative medium #%d." , ( i + 1 ) ) vars = [ mod . variables [ "ind_" + s ] for s in seen ] if len ( seen ) > 0 : exclusion . set_linear_coefficients ( dict . fromkeys ( vars , 1 ) ) exclusion . ub = best - 1 num_components = mod . slim_optimize ( ) if mod . solver . status != OPTIMAL or num_components > best : break medium = _as_medium ( exchange_rxns , tol , exports = exports ) media . append ( medium ) seen . update ( medium [ medium > 0 ] . index ) if len ( media ) > 1 : medium = pd . concat ( media , axis = 1 , sort = True ) . fillna ( 0.0 ) medium . sort_index ( axis = 1 , inplace = True ) else : medium = media [ 0 ] else : add_linear_obj ( mod ) mod . slim_optimize ( ) if mod . solver . status != OPTIMAL : LOGGER . warning ( "Minimization of medium was infeasible." ) return None medium = _as_medium ( exchange_rxns , tol , exports = exports ) return medium
4478	def split_storage ( path , default = 'osfstorage' ) : path = norm_remote_path ( path ) for provider in KNOWN_PROVIDERS : if path . startswith ( provider + '/' ) : if six . PY3 : return path . split ( '/' , maxsplit = 1 ) else : return path . split ( '/' , 1 ) return ( default , path )
9242	def detect_actual_closed_dates ( self , issues , kind ) : if self . options . verbose : print ( "Fetching closed dates for {} {}..." . format ( len ( issues ) , kind ) ) all_issues = copy . deepcopy ( issues ) for issue in all_issues : if self . options . verbose > 2 : print ( "." , end = "" ) if not issues . index ( issue ) % 30 : print ( "" ) self . find_closed_date_by_commit ( issue ) if not issue . get ( 'actual_date' , False ) : if issue . get ( 'closed_at' , False ) : print ( "Skipping closed non-merged issue: #{0} {1}" . format ( issue [ "number" ] , issue [ "title" ] ) ) all_issues . remove ( issue ) if self . options . verbose > 2 : print ( "." ) return all_issues
11258	def pack ( prev , n , rest = False , * * kw ) : if 'padding' in kw : use_padding = True padding = kw [ 'padding' ] else : use_padding = False padding = None items = [ ] for i , data in enumerate ( prev , 1 ) : items . append ( data ) if ( i % n ) == 0 : yield items items = [ ] if len ( items ) != 0 and rest : if use_padding : items . extend ( [ padding , ] * ( n - ( i % n ) ) ) yield items
6427	def dist ( self , src , tar ) : if src == tar : return 0.0 src = src . encode ( 'utf-8' ) tar = tar . encode ( 'utf-8' ) src_comp = bz2 . compress ( src , self . _level ) [ 10 : ] tar_comp = bz2 . compress ( tar , self . _level ) [ 10 : ] concat_comp = bz2 . compress ( src + tar , self . _level ) [ 10 : ] concat_comp2 = bz2 . compress ( tar + src , self . _level ) [ 10 : ] return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
6703	def enter_password_change ( self , username = None , old_password = None ) : from fabric . state import connections from fabric . network import disconnect_all r = self . local_renderer # print('self.genv.user:', self.genv.user) # print('self.env.passwords:', self.env.passwords) r . genv . user = r . genv . user or username r . pc ( 'Changing password for user {user} via interactive prompts.' ) r . env . old_password = r . env . default_passwords [ self . genv . user ] # print('self.genv.user:', self.genv.user) # print('self.env.passwords:', self.env.passwords) r . env . new_password = self . env . passwords [ self . genv . user ] if old_password : r . env . old_password = old_password prompts = { '(current) UNIX password: ' : r . env . old_password , 'Enter new UNIX password: ' : r . env . new_password , 'Retype new UNIX password: ' : r . env . new_password , #"Login password for '%s': " % r.genv.user: r.env.new_password, # "Login password for '%s': " % r.genv.user: r.env.old_password, } print ( 'prompts:' , prompts ) r . env . password = r . env . old_password with self . settings ( warn_only = True ) : ret = r . _local ( "sshpass -p '{password}' ssh -o StrictHostKeyChecking=no {user}@{host_string} echo hello" , capture = True ) #code 1 = good password, but prompts needed #code 5 = bad password #code 6 = good password, but host public key is unknown if ret . return_code in ( 1 , 6 ) or 'hello' in ret : # Login succeeded, so we haven't yet changed the password, so use the default password. self . genv . password = r . env . old_password elif self . genv . user in self . genv . user_passwords : # Otherwise, use the password or key set in the config. self . genv . password = r . env . new_password else : # Default password fails and there's no current password, so clear. self . genv . password = None print ( 'using password:' , self . genv . password ) # Note, the correct current password should be set in host.initrole(), not here. #r.genv.password = r.env.new_password #r.genv.password = r.env.new_password with self . settings ( prompts = prompts ) : ret = r . _run ( 'echo checking for expired password' ) print ( 'ret:[%s]' % ret ) do_disconnect = 'passwd: password updated successfully' in ret print ( 'do_disconnect:' , do_disconnect ) if do_disconnect : # We need to disconnect to reset the session or else Linux will again prompt # us to change our password. disconnect_all ( ) # Further logins should require the new password. self . genv . password = r . env . new_password
10499	def waitFor ( self , timeout , notification , * * kwargs ) : return self . _waitFor ( timeout , notification , * * kwargs )
7713	def handle_roster_push ( self , stanza ) : if self . server is None and stanza . from_jid : logger . debug ( u"Server address not known, cannot verify roster push" " from {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) if self . server and stanza . from_jid and stanza . from_jid != self . server : logger . debug ( u"Roster push from invalid source: {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) payload = stanza . get_payload ( RosterPayload ) if len ( payload ) != 1 : logger . warning ( "Bad roster push received ({0} items)" . format ( len ( payload ) ) ) return stanza . make_error_response ( u"bad-request" ) if self . roster is None : logger . debug ( "Dropping roster push - no roster here" ) return True item = payload [ 0 ] item . verify_roster_push ( True ) old_item = self . roster . get ( item . jid ) if item . subscription == "remove" : if old_item : self . roster . remove_item ( item . jid ) else : self . roster . add_item ( item , replace = True ) self . _event_queue . put ( RosterUpdatedEvent ( self , old_item , item ) ) return stanza . make_result_response ( )
4764	def is_equal_to ( self , other , * * kwargs ) : if self . _check_dict_like ( self . val , check_values = False , return_as_bool = True ) and self . _check_dict_like ( other , check_values = False , return_as_bool = True ) : if self . _dict_not_equal ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) : self . _dict_err ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) else : if self . val != other : self . _err ( 'Expected <%s> to be equal to <%s>, but was not.' % ( self . val , other ) ) return self
3825	async def get_group_conversation_url ( self , get_group_conversation_url_request ) : response = hangouts_pb2 . GetGroupConversationUrlResponse ( ) await self . _pb_request ( 'conversations/getgroupconversationurl' , get_group_conversation_url_request , response ) return response
11356	def escape_for_xml ( data , tags_to_keep = None ) : data = re . sub ( "&" , "&amp;" , data ) if tags_to_keep : data = re . sub ( r"(<)(?![\/]?({0})\b)" . format ( "|" . join ( tags_to_keep ) ) , '&lt;' , data ) else : data = re . sub ( "<" , "&lt;" , data ) return data
1373	def defaults_cluster_role_env ( cluster_role_env ) : if len ( cluster_role_env [ 1 ] ) == 0 and len ( cluster_role_env [ 2 ] ) == 0 : return ( cluster_role_env [ 0 ] , getpass . getuser ( ) , ENVIRON ) return ( cluster_role_env [ 0 ] , cluster_role_env [ 1 ] , cluster_role_env [ 2 ] )
2233	def _register_builtin_class_extensions ( self ) : @ self . register ( uuid . UUID ) def _hash_uuid ( data ) : hashable = data . bytes prefix = b'UUID' return prefix , hashable @ self . register ( OrderedDict ) def _hash_ordered_dict ( data ) : """ Note, we should not be hashing dicts because they are unordered """ hashable = b'' . join ( _hashable_sequence ( list ( data . items ( ) ) ) ) prefix = b'ODICT' return prefix , hashable
12964	def all ( self , cascadeFetch = False ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultiple ( matchedKeys , cascadeFetch = cascadeFetch ) return IRQueryableList ( [ ] , mdl = self . mdl )
10222	def get_nift_values ( ) -> Mapping [ str , str ] : r = get_bel_resource ( NIFT ) return { name . lower ( ) : name for name in r [ 'Values' ] }
9137	def clear_cache ( module_name : str , keep_database : bool = True ) -> None : data_dir = get_data_dir ( module_name ) if not os . path . exists ( data_dir ) : return for name in os . listdir ( data_dir ) : if name in { 'config.ini' , 'cfg.ini' } : continue if name == 'cache.db' and keep_database : continue path = os . path . join ( data_dir , name ) if os . path . isdir ( path ) : shutil . rmtree ( path ) else : os . remove ( path ) os . rmdir ( data_dir )
6522	def add_issues ( self , issues ) : if not isinstance ( issues , ( list , tuple ) ) : issues = [ issues ] with self . _lock : self . _all_issues . extend ( issues ) self . _cleaned_issues = None
1907	def all_events ( cls ) : all_evts = set ( ) for cls , evts in cls . __all_events__ . items ( ) : all_evts . update ( evts ) return all_evts
9978	def find_funcdef ( source ) : try : module_node = compile ( source , "<string>" , mode = "exec" , flags = ast . PyCF_ONLY_AST ) except SyntaxError : return find_funcdef ( fix_lamdaline ( source ) ) for node in ast . walk ( module_node ) : if isinstance ( node , ast . FunctionDef ) or isinstance ( node , ast . Lambda ) : return node raise ValueError ( "function definition not found" )
9730	def get_force ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . plate_count ) : component_position , plate = QRTPacket . _get_exact ( RTForcePlate , data , component_position ) force_list = [ ] for _ in range ( plate . force_count ) : component_position , force = QRTPacket . _get_exact ( RTForce , data , component_position ) force_list . append ( force ) append_components ( ( plate , force_list ) ) return components
201	def pad_to_aspect_ratio ( self , aspect_ratio , mode = "constant" , cval = 0.0 , return_pad_amounts = False ) : arr_padded , pad_amounts = ia . pad_to_aspect_ratio ( self . arr , aspect_ratio = aspect_ratio , mode = mode , cval = cval , return_pad_amounts = True ) segmap = SegmentationMapOnImage ( arr_padded , shape = self . shape ) segmap . input_was = self . input_was if return_pad_amounts : return segmap , pad_amounts else : return segmap
11146	def get_file_info ( self , relativePath ) : relativePath = self . to_repo_relative_path ( path = relativePath , split = False ) fileName = os . path . basename ( relativePath ) isRepoFile , fileOnDisk , infoOnDisk , classOnDisk = self . is_repository_file ( relativePath ) if not isRepoFile : return None , "file is not a registered repository file." if not infoOnDisk : return None , "file is a registered repository file but info file missing" fileInfoPath = os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileInfo % fileName ) try : with open ( fileInfoPath , 'rb' ) as fd : info = pickle . load ( fd ) except Exception as err : return None , "Unable to read file info from disk (%s)" % str ( err ) return info , ''
938	def _getModelPickleFilePath ( saveModelDir ) : path = os . path . join ( saveModelDir , "model.pkl" ) path = os . path . abspath ( path ) return path
817	def Distribution ( pos , size , counts , dtype ) : x = numpy . zeros ( size , dtype = dtype ) if hasattr ( pos , '__iter__' ) : # calculate normalization constant total = 0 for i in pos : total += counts [ i ] total = float ( total ) # set included positions to normalized probability for i in pos : x [ i ] = counts [ i ] / total # If we don't have a set of positions, assume there's only one position else : x [ pos ] = 1 return x
11928	def get_files_stat ( self ) : if not exists ( Post . src_dir ) : logger . error ( SourceDirectoryNotFound . __doc__ ) sys . exit ( SourceDirectoryNotFound . exit_code ) paths = [ ] for fn in ls ( Post . src_dir ) : if fn . endswith ( src_ext ) : paths . append ( join ( Post . src_dir , fn ) ) # config.toml if exists ( config . filepath ) : paths . append ( config . filepath ) # files: a <filepath to updated time> dict files = dict ( ( p , stat ( p ) . st_mtime ) for p in paths ) return files
6977	def kepler_lcdict_to_pkl ( lcdict , outfile = None ) : if not outfile : outfile = '%s-keplc.pkl' % lcdict [ 'objectid' ] . replace ( ' ' , '-' ) # we're using pickle.HIGHEST_PROTOCOL here, this will make Py3 pickles # unreadable for Python 2.7 with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) return os . path . abspath ( outfile )
12812	def rawDataReceived ( self , data ) : if self . _len_expected is not None : data , extra = data [ : self . _len_expected ] , data [ self . _len_expected : ] self . _len_expected -= len ( data ) else : extra = "" self . _buffer += data if self . _len_expected == 0 : data = self . _buffer . strip ( ) if data : lines = data . split ( "\r" ) for line in lines : try : message = self . factory . get_stream ( ) . get_connection ( ) . parse ( line ) if message : self . factory . get_stream ( ) . received ( [ message ] ) except ValueError : pass self . _buffer = "" self . _len_expected = None self . setLineMode ( extra )
2683	def get_function_config ( cfg ) : function_name = cfg . get ( 'function_name' ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) try : return client . get_function ( FunctionName = function_name ) except client . exceptions . ResourceNotFoundException as e : if 'Function not found' in str ( e ) : return False
6686	def is_installed ( pkg_name ) : manager = MANAGER with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = run ( "rpm --query %(pkg_name)s" % locals ( ) ) if res . succeeded : return True return False
10224	def get_chaotic_pairs ( graph : BELGraph ) -> SetOfNodePairs : cg = get_causal_subgraph ( graph ) results = set ( ) for u , v , d in cg . edges ( data = True ) : if d [ RELATION ] not in CAUSAL_INCREASE_RELATIONS : continue if cg . has_edge ( v , u ) and any ( dd [ RELATION ] in CAUSAL_INCREASE_RELATIONS for dd in cg [ v ] [ u ] . values ( ) ) : results . add ( tuple ( sorted ( [ u , v ] , key = str ) ) ) return results
3897	def generate_message_doc ( message_descriptor , locations , path , name_prefix = '' ) : # message_type is 4 prefixed_name = name_prefix + message_descriptor . name print ( make_subsection ( prefixed_name ) ) location = locations [ path ] if location . HasField ( 'leading_comments' ) : print ( textwrap . dedent ( location . leading_comments ) ) row_tuples = [ ] for field_index , field in enumerate ( message_descriptor . field ) : field_location = locations [ path + ( 2 , field_index ) ] if field . type not in [ 11 , 14 ] : type_str = TYPE_TO_STR [ field . type ] else : type_str = make_link ( field . type_name . lstrip ( '.' ) ) row_tuples . append ( ( make_code ( field . name ) , field . number , type_str , LABEL_TO_STR [ field . label ] , textwrap . fill ( get_comment_from_location ( field_location ) , INFINITY ) , ) ) print_table ( ( 'Field' , 'Number' , 'Type' , 'Label' , 'Description' ) , row_tuples ) # Generate nested messages nested_types = enumerate ( message_descriptor . nested_type ) for index , nested_message_desc in nested_types : generate_message_doc ( nested_message_desc , locations , path + ( 3 , index ) , name_prefix = prefixed_name + '.' ) # Generate nested enums for index , nested_enum_desc in enumerate ( message_descriptor . enum_type ) : generate_enum_doc ( nested_enum_desc , locations , path + ( 4 , index ) , name_prefix = prefixed_name + '.' )
2684	def cached_download ( url , name ) : clean_name = os . path . normpath ( name ) if clean_name != name : raise ValueError ( "{} is not normalized." . format ( name ) ) for dir_ in iter_data_dirs ( ) : path = os . path . join ( dir_ , name ) if os . path . exists ( path ) : return path dir_ = next ( iter_data_dirs ( True ) ) path = os . path . join ( dir_ , name ) log . info ( "Downloading {} to {}" . format ( url , path ) ) response = urlopen ( url ) if response . getcode ( ) != 200 : raise ValueError ( "HTTP {}" . format ( response . getcode ( ) ) ) dir_ = os . path . dirname ( path ) try : os . makedirs ( dir_ ) except OSError as e : if e . errno != errno . EEXIST : raise tmp_path = path + '.tmp' with open ( tmp_path , 'wb' ) as fh : while True : chunk = response . read ( 8196 ) if chunk : fh . write ( chunk ) else : break os . rename ( tmp_path , path ) return path
4618	def parse_time ( block_time ) : return datetime . strptime ( block_time , timeFormat ) . replace ( tzinfo = timezone . utc )
1415	def create_pplan ( self , topologyName , pplan ) : if not pplan or not pplan . IsInitialized ( ) : raise_ ( StateException ( "Physical Plan protobuf not init properly" , StateException . EX_TYPE_PROTOBUF_ERROR ) , sys . exc_info ( ) [ 2 ] ) path = self . get_pplan_path ( topologyName ) LOG . info ( "Adding topology: {0} to path: {1}" . format ( topologyName , path ) ) pplanString = pplan . SerializeToString ( ) try : self . client . create ( path , value = pplanString , makepath = True ) return True except NoNodeError : raise_ ( StateException ( "NoNodeError while creating pplan" , StateException . EX_TYPE_NO_NODE_ERROR ) , sys . exc_info ( ) [ 2 ] ) except NodeExistsError : raise_ ( StateException ( "NodeExistsError while creating pplan" , StateException . EX_TYPE_NODE_EXISTS_ERROR ) , sys . exc_info ( ) [ 2 ] ) except ZookeeperError : raise_ ( StateException ( "Zookeeper while creating pplan" , StateException . EX_TYPE_ZOOKEEPER_ERROR ) , sys . exc_info ( ) [ 2 ] ) except Exception : # Just re raise the exception. raise
7330	def stream_request ( self , method , url , headers = None , _session = None , * args , * * kwargs ) : return StreamResponse ( method = method , url = url , client = self , headers = headers , session = _session , proxy = self . proxy , * * kwargs )
12158	def abfSort ( IDs ) : IDs = list ( IDs ) monO = [ ] monN = [ ] monD = [ ] good = [ ] for ID in IDs : if ID is None : continue if 'o' in ID : monO . append ( ID ) elif 'n' in ID : monN . append ( ID ) elif 'd' in ID : monD . append ( ID ) else : good . append ( ID ) return sorted ( good ) + sorted ( monO ) + sorted ( monN ) + sorted ( monD )
6394	def sim_levenshtein ( src , tar , mode = 'lev' , cost = ( 1 , 1 , 1 , 1 ) ) : return Levenshtein ( ) . sim ( src , tar , mode , cost )
12805	def search ( self , terms ) : messages = self . _connection . get ( "search/%s" % urllib . quote_plus ( terms ) , key = "messages" ) if messages : messages = [ Message ( self , message ) for message in messages ] return messages
3337	def join_uri ( uri , * segments ) : sub = "/" . join ( segments ) if not sub : return uri return uri . rstrip ( "/" ) + "/" + sub
8245	def aggregated ( cache = DEFAULT_CACHE ) : global _aggregated_name , _aggregated_dict if _aggregated_name != cache : _aggregated_name = cache _aggregated_dict = { } for path in glob ( os . path . join ( cache , "*" ) ) : if os . path . isdir ( path ) : p = os . path . basename ( path ) _aggregated_dict [ p ] = glob ( os . path . join ( path , "*" ) ) _aggregated_dict [ p ] = [ os . path . basename ( f ) [ : - 4 ] for f in _aggregated_dict [ p ] ] return _aggregated_dict
12651	def where_is ( strings , pattern , n = 1 , lookup_func = re . match ) : count = 0 for idx , item in enumerate ( strings ) : if lookup_func ( pattern , item ) : count += 1 if count == n : return idx return - 1
2294	def integral_approx_estimator ( x , y ) : a , b = ( 0. , 0. ) x = np . array ( x ) y = np . array ( y ) idx , idy = ( np . argsort ( x ) , np . argsort ( y ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idx ] ] [ : - 1 ] , x [ [ idx ] ] [ 1 : ] , y [ [ idx ] ] [ : - 1 ] , y [ [ idx ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : a = a + np . log ( np . abs ( ( y2 - y1 ) / ( x2 - x1 ) ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idy ] ] [ : - 1 ] , x [ [ idy ] ] [ 1 : ] , y [ [ idy ] ] [ : - 1 ] , y [ [ idy ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : b = b + np . log ( np . abs ( ( x2 - x1 ) / ( y2 - y1 ) ) ) return ( a - b ) / len ( x )
11231	def replace ( self , * * kwargs ) : new_kwargs = { "interval" : self . _interval , "count" : self . _count , "dtstart" : self . _dtstart , "freq" : self . _freq , "until" : self . _until , "wkst" : self . _wkst , "cache" : False if self . _cache is None else True } new_kwargs . update ( self . _original_rule ) new_kwargs . update ( kwargs ) return rrule ( * * new_kwargs )
6418	def sim_editex ( src , tar , cost = ( 0 , 1 , 2 ) , local = False ) : return Editex ( ) . sim ( src , tar , cost , local )
2447	def set_pkg_vers ( self , doc , version ) : self . assert_package_exists ( ) if not self . package_vers_set : self . package_vers_set = True doc . package . version = version return True else : raise CardinalityError ( 'Package::Version' )
2079	def disassociate_notification_template ( self , job_template , notification_template , status ) : return self . _disassoc ( 'notification_templates_%s' % status , job_template , notification_template )
6424	def sim ( self , src , tar , qval = 2 ) : return super ( self . __class__ , self ) . sim ( src , tar , qval , 1 , 1 )
10402	def calculate_score ( self , node : BaseEntity ) -> float : score = ( self . graph . nodes [ node ] [ self . tag ] if self . tag in self . graph . nodes [ node ] else self . default_score ) for predecessor , _ , d in self . graph . in_edges ( node , data = True ) : if d [ RELATION ] in CAUSAL_INCREASE_RELATIONS : score += self . graph . nodes [ predecessor ] [ self . tag ] elif d [ RELATION ] in CAUSAL_DECREASE_RELATIONS : score -= self . graph . nodes [ predecessor ] [ self . tag ] return score
10206	def run ( self , start_date = None , end_date = None , * * kwargs ) : start_date = self . extract_date ( start_date ) if start_date else None end_date = self . extract_date ( end_date ) if end_date else None self . validate_arguments ( start_date , end_date , * * kwargs ) agg_query = self . build_query ( start_date , end_date , * * kwargs ) query_result = agg_query . execute ( ) . to_dict ( ) res = self . process_query_result ( query_result , start_date , end_date ) return res
8399	def transform ( x ) : try : x = date2num ( x ) except AttributeError : # numpy datetime64 # This is not ideal because the operations do not # preserve the np.datetime64 type. May be need # a datetime64_trans x = [ pd . Timestamp ( item ) for item in x ] x = date2num ( x ) return x
4097	def AKICc ( N , rho , k ) : from numpy import log , array p = k res = log ( rho ) + p / N / ( N - p ) + ( 3. - ( p + 2. ) / N ) * ( p + 1. ) / ( N - p - 2. ) return res
10435	def gettablerowindex ( self , window_name , object_name , row_text ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) index = 0 for cell in object_handle . AXRows : if re . match ( row_text , cell . AXChildren [ 0 ] . AXValue ) : return index index += 1 raise LdtpServerException ( u"Unable to find row: %s" % row_text )
12434	def redirect ( cls , request , response ) : if cls . meta . legacy_redirect : if request . method in ( 'GET' , 'HEAD' , ) : # A SAFE request is allowed to redirect using a 301 response . status = http . client . MOVED_PERMANENTLY else : # All other requests must use a 307 response . status = http . client . TEMPORARY_REDIRECT else : # Modern redirects are allowed. Let's have some fun. # Hopefully you're client supports this. # The RFC explicitly discourages UserAgent sniffing. response . status = http . client . PERMANENT_REDIRECT # Terminate the connection. response . close ( )
1438	def update_received_packet ( self , received_pkt_size_bytes ) : self . update_count ( self . RECEIVED_PKT_COUNT ) self . update_count ( self . RECEIVED_PKT_SIZE , incr_by = received_pkt_size_bytes )
13832	def _SkipFieldValue ( tokenizer ) : # String/bytes tokens can come in multiple adjacent string literals. # If we can consume one, consume as many as we can. if tokenizer . TryConsumeByteString ( ) : while tokenizer . TryConsumeByteString ( ) : pass return if ( not tokenizer . TryConsumeIdentifier ( ) and not tokenizer . TryConsumeInt64 ( ) and not tokenizer . TryConsumeUint64 ( ) and not tokenizer . TryConsumeFloat ( ) ) : raise ParseError ( 'Invalid field value: ' + tokenizer . token )
11203	def picknthweekday ( year , month , dayofweek , hour , minute , whichweek ) : first = datetime . datetime ( year , month , 1 , hour , minute ) # This will work if dayofweek is ISO weekday (1-7) or Microsoft-style (0-6), # Because 7 % 7 = 0 weekdayone = first . replace ( day = ( ( dayofweek - first . isoweekday ( ) ) % 7 ) + 1 ) wd = weekdayone + ( ( whichweek - 1 ) * ONEWEEK ) if ( wd . month != month ) : wd -= ONEWEEK return wd
1437	def update_reduced_metric ( self , name , value , key = None ) : if name not in self . metrics : Log . error ( "In update_reduced_metric(): %s is not registered in the metric" , name ) if key is None and isinstance ( self . metrics [ name ] , ReducedMetric ) : self . metrics [ name ] . update ( value ) elif key is not None and isinstance ( self . metrics [ name ] , MultiReducedMetric ) : self . metrics [ name ] . update ( key , value ) else : Log . error ( "In update_count(): %s is registered but not supported with this method" , name )
4797	def is_before ( self , other ) : if type ( self . val ) is not datetime . datetime : raise TypeError ( 'val must be datetime, but was type <%s>' % type ( self . val ) . __name__ ) if type ( other ) is not datetime . datetime : raise TypeError ( 'given arg must be datetime, but was type <%s>' % type ( other ) . __name__ ) if self . val >= other : self . _err ( 'Expected <%s> to be before <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) return self
2831	def set_training ( model , mode ) : if mode is None : yield return old_mode = model . training if old_mode != mode : model . train ( mode ) try : yield finally : if old_mode != mode : model . train ( old_mode )
10938	def update_eig_J ( self ) : CLOG . debug ( 'Eigen update.' ) vls , vcs = np . linalg . eigh ( self . JTJ ) res0 = self . calc_residuals ( ) for a in range ( min ( [ self . num_eig_dirs , vls . size ] ) ) : #1. Finding stiff directions stif_dir = vcs [ - ( a + 1 ) ] #already normalized #2. Evaluating derivative along that direction, we'll use dl=5e-4: dl = self . eig_dl #1e-5 _ = self . update_function ( self . param_vals + dl * stif_dir ) res1 = self . calc_residuals ( ) #3. Updating grad_stif = ( res1 - res0 ) / dl self . _rank_1_J_update ( stif_dir , grad_stif ) self . JTJ = np . dot ( self . J , self . J . T ) #Putting the parameters back: _ = self . update_function ( self . param_vals )
439	def print_params ( self , details = True , session = None ) : for i , p in enumerate ( self . all_params ) : if details : try : val = p . eval ( session = session ) logging . info ( " param {:3}: {:20} {:15} {} (mean: {:<18}, median: {:<18}, std: {:<18}) " . format ( i , p . name , str ( val . shape ) , p . dtype . name , val . mean ( ) , np . median ( val ) , val . std ( ) ) ) except Exception as e : logging . info ( str ( e ) ) raise Exception ( "Hint: print params details after tl.layers.initialize_global_variables(sess) " "or use network.print_params(False)." ) else : logging . info ( " param {:3}: {:20} {:15} {}" . format ( i , p . name , str ( p . get_shape ( ) ) , p . dtype . name ) ) logging . info ( " num of params: %d" % self . count_params ( ) )
13835	def _ParseOrMerge ( self , lines , message ) : tokenizer = _Tokenizer ( lines ) while not tokenizer . AtEnd ( ) : self . _MergeField ( tokenizer , message )
4162	def _parse_dict_recursive ( dict_str ) : dict_out = dict ( ) pos_last = 0 pos = dict_str . find ( ':' ) while pos >= 0 : key = dict_str [ pos_last : pos ] if dict_str [ pos + 1 ] == '[' : # value is a list pos_tmp = dict_str . find ( ']' , pos + 1 ) if pos_tmp < 0 : raise RuntimeError ( 'error when parsing dict' ) value = dict_str [ pos + 2 : pos_tmp ] . split ( ',' ) # try to convert elements to int for i in range ( len ( value ) ) : try : value [ i ] = int ( value [ i ] ) except ValueError : pass elif dict_str [ pos + 1 ] == '{' : # value is another dictionary subdict_str = _select_block ( dict_str [ pos : ] , '{' , '}' ) value = _parse_dict_recursive ( subdict_str ) pos_tmp = pos + len ( subdict_str ) else : raise ValueError ( 'error when parsing dict: unknown elem' ) key = key . strip ( '"' ) if len ( key ) > 0 : dict_out [ key ] = value pos_last = dict_str . find ( ',' , pos_tmp ) if pos_last < 0 : break pos_last += 1 pos = dict_str . find ( ':' , pos_last ) return dict_out
9460	def conference_unmute ( self , call_params ) : path = '/' + self . api_version + '/ConferenceUnmute/' method = 'POST' return self . request ( path , method , call_params )
4341	def repeat ( self , count = 1 ) : if not isinstance ( count , int ) or count < 1 : raise ValueError ( "count must be a postive integer." ) effect_args = [ 'repeat' , '{}' . format ( count ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'repeat' )
13205	def _parse_title ( self ) : command = LatexCommand ( 'title' , { 'name' : 'short_title' , 'required' : False , 'bracket' : '[' } , { 'name' : 'long_title' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no title' ) self . _title = None self . _short_title = None self . _title = parsed [ 'long_title' ] try : self . _short_title = parsed [ 'short_title' ] except KeyError : self . _logger . warning ( 'lsstdoc has no short title' ) self . _short_title = None
13087	def config_dir ( self ) : home = expanduser ( '~' ) config_dir = os . path . join ( home , '.jackal' ) return config_dir
1305	def GetConsoleOriginalTitle ( ) -> str : if IsNT6orHigher : arrayType = ctypes . c_wchar * MAX_PATH values = arrayType ( ) ctypes . windll . kernel32 . GetConsoleOriginalTitleW ( values , MAX_PATH ) return values . value else : raise RuntimeError ( 'GetConsoleOriginalTitle is not supported on Windows XP or lower.' )
5738	def enqueue ( self , f , * args , * * kwargs ) : task = Task ( uuid4 ( ) . hex , f , args , kwargs ) self . storage . put_task ( task ) return self . enqueue_task ( task )
9257	def exclude_issues_by_labels ( self , issues ) : if not self . options . exclude_labels : return copy . deepcopy ( issues ) remove_issues = set ( ) exclude_labels = self . options . exclude_labels include_issues = [ ] for issue in issues : for label in issue [ "labels" ] : if label [ "name" ] in exclude_labels : remove_issues . add ( issue [ "number" ] ) break for issue in issues : if issue [ "number" ] not in remove_issues : include_issues . append ( issue ) return include_issues
4329	def flanger ( self , delay = 0 , depth = 2 , regen = 0 , width = 71 , speed = 0.5 , shape = 'sine' , phase = 25 , interp = 'linear' ) : if not is_number ( delay ) or delay < 0 or delay > 30 : raise ValueError ( "delay must be a number between 0 and 30." ) if not is_number ( depth ) or depth < 0 or depth > 10 : raise ValueError ( "depth must be a number between 0 and 10." ) if not is_number ( regen ) or regen < - 95 or regen > 95 : raise ValueError ( "regen must be a number between -95 and 95." ) if not is_number ( width ) or width < 0 or width > 100 : raise ValueError ( "width must be a number between 0 and 100." ) if not is_number ( speed ) or speed < 0.1 or speed > 10 : raise ValueError ( "speed must be a number between 0.1 and 10." ) if shape not in [ 'sine' , 'triangle' ] : raise ValueError ( "shape must be one of 'sine' or 'triangle'." ) if not is_number ( phase ) or phase < 0 or phase > 100 : raise ValueError ( "phase must be a number between 0 and 100." ) if interp not in [ 'linear' , 'quadratic' ] : raise ValueError ( "interp must be one of 'linear' or 'quadratic'." ) effect_args = [ 'flanger' , '{:f}' . format ( delay ) , '{:f}' . format ( depth ) , '{:f}' . format ( regen ) , '{:f}' . format ( width ) , '{:f}' . format ( speed ) , '{}' . format ( shape ) , '{:f}' . format ( phase ) , '{}' . format ( interp ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'flanger' ) return self
3643	def quickSell ( self , item_id ) : method = 'DELETE' url = 'item' if not isinstance ( item_id , ( list , tuple ) ) : item_id = ( item_id , ) item_id = ( str ( i ) for i in item_id ) params = { 'itemIds' : ',' . join ( item_id ) } self . __request__ ( method , url , params = params ) # {"items":[{"id":280607437106}],"totalCredits":18136} return True
8138	def contrast ( self , value = 1.0 ) : c = ImageEnhance . Contrast ( self . img ) self . img = c . enhance ( value )
10630	def clone ( self ) : result = copy . copy ( self ) result . _compound_mfrs = copy . deepcopy ( self . _compound_mfrs ) return result
9048	def gradient ( self ) : grad = { } for i , f in enumerate ( self . _covariances ) : for varname , g in f . gradient ( ) . items ( ) : grad [ f"{self._name}[{i}].{varname}" ] = g return grad
8008	def activate ( self ) : obj = self . find_paypal_object ( ) if obj . state == enums . BillingPlanState . CREATED : success = obj . activate ( ) if not success : raise PaypalApiError ( "Failed to activate plan: %r" % ( obj . error ) ) # Resync the updated data to the database self . get_or_update_from_api_data ( obj , always_sync = True ) return obj
9614	def element_or_none ( self , using , value ) : try : return self . _execute ( Command . FIND_CHILD_ELEMENT , { 'using' : using , 'value' : value } ) except : return None
7362	async def connect ( self ) : with async_timeout . timeout ( self . timeout ) : self . response = await self . _connect ( ) if self . response . status in range ( 200 , 300 ) : self . _error_timeout = 0 self . state = NORMAL elif self . response . status == 500 : self . state = DISCONNECTION elif self . response . status in range ( 501 , 600 ) : self . state = RECONNECTION elif self . response . status in ( 420 , 429 ) : self . state = ENHANCE_YOUR_CALM else : logger . debug ( "raising error during stream connection" ) raise await exceptions . throw ( self . response , loads = self . client . _loads , url = self . kwargs [ 'url' ] ) logger . debug ( "stream state: %d" % self . state )
11067	def create_acl ( self , name ) : if name in self . _acl : return False self . _acl [ name ] = { 'allow' : [ ] , 'deny' : [ ] } return True
2686	def curated ( name ) : return cached_download ( 'https://docs.mikeboers.com/pyav/samples/' + name , os . path . join ( 'pyav-curated' , name . replace ( '/' , os . path . sep ) ) )
2228	def hash_data ( data , hasher = NoParam , base = NoParam , types = False , hashlen = NoParam , convert = False ) : if convert and isinstance ( data , six . string_types ) : # nocover try : data = json . dumps ( data ) except TypeError as ex : # import warnings # warnings.warn('Unable to encode input as json due to: {!r}'.format(ex)) pass base = _rectify_base ( base ) hashlen = _rectify_hashlen ( hashlen ) hasher = _rectify_hasher ( hasher ) ( ) # Feed the data into the hasher _update_hasher ( hasher , data , types = types ) # Get the hashed representation text = _digest_hasher ( hasher , hashlen , base ) return text
6207	def merge_da ( self ) : print ( ' - Merging D and A timestamps' , flush = True ) ts_d , ts_par_d = self . S . get_timestamps_part ( self . name_timestamps_d ) ts_a , ts_par_a = self . S . get_timestamps_part ( self . name_timestamps_a ) ts , a_ch , part = merge_da ( ts_d , ts_par_d , ts_a , ts_par_a ) assert a_ch . sum ( ) == ts_a . shape [ 0 ] assert ( ~ a_ch ) . sum ( ) == ts_d . shape [ 0 ] assert a_ch . size == ts_a . shape [ 0 ] + ts_d . shape [ 0 ] self . ts , self . a_ch , self . part = ts , a_ch , part self . clk_p = ts_d . attrs [ 'clk_p' ]
12549	def spatial_map ( icc , thr , mode = '+' ) : return thr_img ( icc_img_to_zscore ( icc ) , thr = thr , mode = mode ) . get_data ( )
415	def save_dataset ( self , dataset = None , dataset_name = None , * * kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) try : dataset_id = self . dataset_fs . put ( self . _serialization ( dataset ) ) kwargs . update ( { 'dataset_id' : dataset_id , 'time' : datetime . utcnow ( ) } ) self . db . Dataset . insert_one ( kwargs ) # print("[Database] Save params: {} SUCCESS, took: {}s".format(file_name, round(time.time()-s, 2))) print ( "[Database] Save dataset: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save dataset: FAIL" ) return False
8585	def get_attached_cdroms ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/cdroms?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
11321	def update_languages ( self ) : language_fields = record_get_field_instances ( self . record , '041' ) language = "eng" record_delete_fields ( self . record , "041" ) for field in language_fields : subs = field_get_subfields ( field ) if 'a' in subs : language = self . get_config_item ( subs [ 'a' ] [ 0 ] , "languages" ) break new_subs = [ ( 'a' , language ) ] record_add_field ( self . record , "041" , subfields = new_subs )
2515	def p_file_contributor ( self , f_term , predicate ) : for _ , _ , contributor in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . add_file_contribution ( self . doc , six . text_type ( contributor ) )
8495	def _get_module_filename ( module ) : # Split up the module and its containing package, if it has one module = module . split ( '.' ) package = '.' . join ( module [ : - 1 ] ) module = module [ - 1 ] try : if not package : # We aren't accessing a module within a package, but rather a top # level package, so it's a straight up import module = __import__ ( module ) else : # Import the package containing our desired module package = __import__ ( package , fromlist = [ module ] ) # Get the module from that package module = getattr ( package , module , None ) filename = getattr ( module , '__file__' , None ) if not filename : # No filename? Nothing to do here return Unparseable ( ) # If we get a .pyc, strip the c to get .py so we can parse the source if filename . endswith ( '.pyc' ) : filename = filename [ : - 1 ] if not os . path . exists ( filename ) and os . path . isfile ( filename ) : # If there's only a .pyc and no .py it's a compile package or # egg and we can't get at the source for parsing return Unparseable ( ) # If we have a package, we want the directory not the init file if filename . endswith ( '__init__.py' ) : filename = filename [ : - 11 ] # Yey, we found it return filename except ImportError : # Definitely not a valid module or package return
9923	def save ( self ) : try : email = models . EmailAddress . objects . get ( email = self . validated_data [ "email" ] , is_verified = False ) logger . debug ( "Resending verification email to %s" , self . validated_data [ "email" ] , ) email . send_confirmation ( ) except models . EmailAddress . DoesNotExist : logger . debug ( "Not resending verification email to %s because the address " "doesn't exist in the database." , self . validated_data [ "email" ] , )
3371	def get_solver_name ( mip = False , qp = False ) : if len ( solvers ) == 0 : raise SolverNotFound ( "no solvers installed" ) # Those lists need to be updated as optlang implements more solvers mip_order = [ "gurobi" , "cplex" , "glpk" ] lp_order = [ "glpk" , "cplex" , "gurobi" ] qp_order = [ "gurobi" , "cplex" ] if mip is False and qp is False : for solver_name in lp_order : if solver_name in solvers : return solver_name # none of them are in the list order - so return the first one return list ( solvers ) [ 0 ] elif qp : # mip does not yet matter for this determination for solver_name in qp_order : if solver_name in solvers : return solver_name raise SolverNotFound ( "no qp-capable solver found" ) else : for solver_name in mip_order : if solver_name in solvers : return solver_name raise SolverNotFound ( "no mip-capable solver found" )
11893	def retrieve_document ( file_path , directory = 'sec_filings' ) : ftp = FTP ( 'ftp.sec.gov' , timeout = None ) ftp . login ( ) name = file_path . replace ( '/' , '_' ) if not os . path . exists ( directory ) : os . makedirs ( directory ) with tempfile . TemporaryFile ( ) as temp : ftp . retrbinary ( 'RETR %s' % file_path , temp . write ) temp . seek ( 0 ) with open ( '{}/{}' . format ( directory , name ) , 'w+' ) as f : f . write ( temp . read ( ) . decode ( "utf-8" ) ) f . closed records = temp retry = False ftp . close ( )
1385	def set_physical_plan ( self , physical_plan ) : if not physical_plan : self . physical_plan = None self . id = None else : self . physical_plan = physical_plan self . id = physical_plan . topology . id self . trigger_watches ( )
10070	def preserve ( method = None , result = True , fields = None ) : if method is None : return partial ( preserve , result = result , fields = fields ) fields = fields or ( '_deposit' , ) @ wraps ( method ) def wrapper ( self , * args , * * kwargs ) : """Check current deposit status.""" data = { field : self [ field ] for field in fields if field in self } result_ = method ( self , * args , * * kwargs ) replace = result_ if result else self for field in data : replace [ field ] = data [ field ] return result_ return wrapper
5783	def read_exactly ( self , num_bytes ) : output = b'' remaining = num_bytes while remaining > 0 : output += self . read ( remaining ) remaining = num_bytes - len ( output ) return output
8885	def fit ( self , x , y = None ) : if self . _dtype is not None : iter2array ( x , dtype = self . _dtype ) else : iter2array ( x ) return self
8522	def add_int ( self , name , min , max , warp = None ) : min , max = map ( int , ( min , max ) ) if max < min : raise ValueError ( 'variable %s: max < min error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or "log",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = IntVariable ( name , min , max , warp )
2232	def _register_numpy_extensions ( self ) : # system checks import numpy as np numpy_floating_types = ( np . float16 , np . float32 , np . float64 ) if hasattr ( np , 'float128' ) : # nocover numpy_floating_types = numpy_floating_types + ( np . float128 , ) @ self . add_iterable_check def is_object_ndarray ( data ) : # ndarrays of objects cannot be hashed directly. return isinstance ( data , np . ndarray ) and data . dtype . kind == 'O' @ self . register ( np . ndarray ) def hash_numpy_array ( data ) : """ Example: >>> import ubelt as ub >>> if not ub.modname_to_modpath('numpy'): ... raise pytest.skip() >>> import numpy as np >>> data_f32 = np.zeros((3, 3, 3), dtype=np.float64) >>> data_i64 = np.zeros((3, 3, 3), dtype=np.int64) >>> data_i32 = np.zeros((3, 3, 3), dtype=np.int32) >>> hash_f64 = _hashable_sequence(data_f32, types=True) >>> hash_i64 = _hashable_sequence(data_i64, types=True) >>> hash_i32 = _hashable_sequence(data_i64, types=True) >>> assert hash_i64 != hash_f64 >>> assert hash_i64 != hash_i32 """ if data . dtype . kind == 'O' : msg = 'directly hashing ndarrays with dtype=object is unstable' raise TypeError ( msg ) else : # tobytes() views the array in 1D (via ravel()) # encode the shape as well header = b'' . join ( _hashable_sequence ( ( len ( data . shape ) , data . shape ) ) ) dtype = b'' . join ( _hashable_sequence ( data . dtype . descr ) ) hashable = header + dtype + data . tobytes ( ) prefix = b'NDARR' return prefix , hashable @ self . register ( ( np . int64 , np . int32 , np . int16 , np . int8 ) + ( np . uint64 , np . uint32 , np . uint16 , np . uint8 ) ) def _hash_numpy_int ( data ) : return _convert_to_hashable ( int ( data ) ) @ self . register ( numpy_floating_types ) def _hash_numpy_float ( data ) : return _convert_to_hashable ( float ( data ) ) @ self . register ( np . random . RandomState ) def _hash_numpy_random_state ( data ) : """ Example: >>> import ubelt as ub >>> if not ub.modname_to_modpath('numpy'): ... raise pytest.skip() >>> import numpy as np >>> rng = np.random.RandomState(0) >>> _hashable_sequence(rng, types=True) """ hashable = b'' . join ( _hashable_sequence ( data . get_state ( ) ) ) prefix = b'RNG' return prefix , hashable
11086	def whoami ( self , msg , args ) : output = [ "Hello %s" % msg . user ] if hasattr ( self . _bot . dispatcher , 'auth_manager' ) and msg . user . is_admin is True : output . append ( "You are a *bot admin*." ) output . append ( "Bot version: %s-%s" % ( self . _bot . version , self . _bot . commit ) ) return '\n' . join ( output )
12218	def _bind_args ( sig , param_matchers , args , kwargs ) : #Bind to signature. May throw its own TypeError bound = sig . bind ( * args , * * kwargs ) if not all ( param_matcher ( bound . arguments [ param_name ] ) for param_name , param_matcher in param_matchers ) : raise TypeError return bound
3005	def _get_storage_model ( ) : storage_model_settings = getattr ( django . conf . settings , 'GOOGLE_OAUTH2_STORAGE_MODEL' , None ) if storage_model_settings is not None : return ( storage_model_settings [ 'model' ] , storage_model_settings [ 'user_property' ] , storage_model_settings [ 'credentials_property' ] ) else : return None , None , None
5654	def makedirs ( path ) : if not os . path . isdir ( path ) : os . makedirs ( path ) return path
1125	def Rule ( name , loc = None ) : @ llrule ( loc , lambda parser : getattr ( parser , name ) . expected ( parser ) ) def rule ( parser ) : return getattr ( parser , name ) ( ) return rule
9269	def version_of_first_item ( self ) : try : sections = read_changelog ( self . options ) return sections [ 0 ] [ "version" ] except ( IOError , TypeError ) : return self . get_temp_tag_for_repo_creation ( )
209	def invert ( self ) : arr_inv = HeatmapsOnImage . from_0to1 ( 1 - self . arr_0to1 , shape = self . shape , min_value = self . min_value , max_value = self . max_value ) arr_inv . arr_was_2d = self . arr_was_2d return arr_inv
13825	def FromJsonString ( self , value ) : if len ( value ) < 1 or value [ - 1 ] != 's' : raise ParseError ( 'Duration must end with letter "s": {0}.' . format ( value ) ) try : pos = value . find ( '.' ) if pos == - 1 : self . seconds = int ( value [ : - 1 ] ) self . nanos = 0 else : self . seconds = int ( value [ : pos ] ) if value [ 0 ] == '-' : self . nanos = int ( round ( float ( '-0{0}' . format ( value [ pos : - 1 ] ) ) * 1e9 ) ) else : self . nanos = int ( round ( float ( '0{0}' . format ( value [ pos : - 1 ] ) ) * 1e9 ) ) except ValueError : raise ParseError ( 'Couldn\'t parse duration: {0}.' . format ( value ) )
1924	def binary_symbols ( binary ) : def substr_after ( string , delim ) : return string . partition ( delim ) [ 2 ] with open ( binary , 'rb' ) as f : elffile = ELFFile ( f ) for section in elffile . iter_sections ( ) : if not isinstance ( section , SymbolTableSection ) : continue symbols = [ sym . name for sym in section . iter_symbols ( ) if sym ] return [ substr_after ( name , PREPEND_SYM ) for name in symbols if name . startswith ( PREPEND_SYM ) ]
12929	def as_dict ( self ) : self_as_dict = { 'chrom' : self . chrom , 'start' : self . start , 'ref_allele' : self . ref_allele , 'alt_alleles' : self . alt_alleles , 'alleles' : [ x . as_dict ( ) for x in self . alleles ] } try : self_as_dict [ 'info' ] = self . info except AttributeError : pass return self_as_dict
4932	def transform_courserun_schedule ( self , content_metadata_item ) : start = content_metadata_item . get ( 'start' ) or UNIX_MIN_DATE_STRING end = content_metadata_item . get ( 'end' ) or UNIX_MAX_DATE_STRING return [ { 'startDate' : parse_datetime_to_epoch_millis ( start ) , 'endDate' : parse_datetime_to_epoch_millis ( end ) , 'active' : current_time_is_in_interval ( start , end ) } ]
6714	def _install_from_scratch ( python_cmd , use_sudo ) : with cd ( "/tmp" ) : download ( EZ_SETUP_URL ) command = '%(python_cmd)s ez_setup.py' % locals ( ) if use_sudo : run_as_root ( command ) else : run ( command ) run ( 'rm -f ez_setup.py' )
6078	def convolve_image ( self , image_array , blurring_array ) : return self . convolve_jit ( image_array , self . image_frame_indexes , self . image_frame_psfs , self . image_frame_lengths , blurring_array , self . blurring_frame_indexes , self . blurring_frame_psfs , self . blurring_frame_lengths )
6736	def reboot_or_dryrun ( * args , * * kwargs ) : from fabric . state import connections verbose = get_verbose ( ) dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) # Use 'wait' as max total wait time kwargs . setdefault ( 'wait' , 120 ) wait = int ( kwargs [ 'wait' ] ) command = kwargs . get ( 'command' , 'reboot' ) now = int ( kwargs . get ( 'now' , 0 ) ) print ( 'now:' , now ) if now : command += ' now' # Shorter timeout for a more granular cycle than the default. timeout = int ( kwargs . get ( 'timeout' , 30 ) ) reconnect_hostname = kwargs . pop ( 'new_hostname' , env . host_string ) if 'dryrun' in kwargs : del kwargs [ 'dryrun' ] if dryrun : print ( '%s sudo: %s' % ( render_command_prefix ( ) , command ) ) else : if is_local ( ) : if raw_input ( 'reboot localhost now? ' ) . strip ( ) [ 0 ] . lower ( ) != 'y' : return attempts = int ( round ( float ( wait ) / float ( timeout ) ) ) # Don't bleed settings, since this is supposed to be self-contained. # User adaptations will probably want to drop the "with settings()" and # just have globally set timeout/attempts values. with settings ( warn_only = True ) : _sudo ( command ) env . host_string = reconnect_hostname success = False for attempt in xrange ( attempts ) : # Try to make sure we don't slip in before pre-reboot lockdown if verbose : print ( 'Waiting for %s seconds, wait %i of %i' % ( timeout , attempt + 1 , attempts ) ) time . sleep ( timeout ) # This is actually an internal-ish API call, but users can simply drop # it in real fabfile use -- the next run/sudo/put/get/etc call will # automatically trigger a reconnect. # We use it here to force the reconnect while this function is still in # control and has the above timeout settings enabled. try : if verbose : print ( 'Reconnecting to:' , env . host_string ) # This will fail until the network interface comes back up. connections . connect ( env . host_string ) # This will also fail until SSH is running again. with settings ( timeout = timeout ) : _run ( 'echo hello' ) success = True break except Exception as e : print ( 'Exception:' , e ) if not success : raise Exception ( 'Reboot failed or took longer than %s seconds.' % wait )
5297	def get_first_of_week ( self ) : if self . first_of_week is None : raise ImproperlyConfigured ( "%s.first_of_week is required." % self . __class__ . __name__ ) if self . first_of_week not in range ( 7 ) : raise ImproperlyConfigured ( "%s.first_of_week must be an integer between 0 and 6." % self . __class__ . __name__ ) return self . first_of_week
9617	def PlugIn ( self ) : ids = self . available_ids ( ) if len ( ids ) == 0 : raise MaxInputsReachedError ( 'Max Inputs Reached' ) self . id = ids [ 0 ] _xinput . PlugIn ( self . id ) while self . id in self . available_ids ( ) : pass
7260	def get_data_location ( self , catalog_id ) : try : record = self . get ( catalog_id ) except : return None # Handle Landsat8 if 'Landsat8' in record [ 'type' ] and 'LandsatAcquisition' in record [ 'type' ] : bucket = record [ 'properties' ] [ 'bucketName' ] prefix = record [ 'properties' ] [ 'bucketPrefix' ] return 's3://' + bucket + '/' + prefix # Handle DG Acquisition if 'DigitalGlobeAcquisition' in record [ 'type' ] : o = Ordering ( ) res = o . location ( [ catalog_id ] ) return res [ 'acquisitions' ] [ 0 ] [ 'location' ] return None
12021	def add_line_error ( self , line_data , error_info , log_level = logging . ERROR ) : if not error_info : return try : line_data [ 'line_errors' ] . append ( error_info ) except KeyError : line_data [ 'line_errors' ] = [ error_info ] except TypeError : # no line_data pass try : self . logger . log ( log_level , Gff3 . error_format . format ( current_line_num = line_data [ 'line_index' ] + 1 , error_type = error_info [ 'error_type' ] , message = error_info [ 'message' ] , line = line_data [ 'line_raw' ] . rstrip ( ) ) ) except AttributeError : # no logger pass
1781	def AAM ( cpu , imm = None ) : if imm is None : imm = 10 else : imm = imm . read ( ) cpu . AH = Operators . UDIV ( cpu . AL , imm ) cpu . AL = Operators . UREM ( cpu . AL , imm ) # Defined flags: ...sz.p. cpu . _calculate_logic_flags ( 8 , cpu . AL )
13633	def _negotiateHandler ( self , request ) : accept = _parseAccept ( request . requestHeaders . getRawHeaders ( 'Accept' ) ) for contentType in accept . keys ( ) : handler = self . _acceptHandlers . get ( contentType . lower ( ) ) if handler is not None : return handler , handler . contentType if self . _fallback : handler = self . _handlers [ 0 ] return handler , handler . contentType return NotAcceptable ( ) , None
13188	async def process_lander_page ( session , github_api_token , ltd_product_data , mongo_collection = None ) : logger = logging . getLogger ( __name__ ) # Try to download metadata.jsonld from the Landing page site. published_url = ltd_product_data [ 'published_url' ] jsonld_url = urljoin ( published_url , '/metadata.jsonld' ) try : async with session . get ( jsonld_url ) as response : logger . debug ( '%s response status %r' , jsonld_url , response . status ) response . raise_for_status ( ) json_data = await response . text ( ) except aiohttp . ClientResponseError as err : logger . debug ( 'Tried to download %s, got status %d' , jsonld_url , err . code ) raise NotLanderPageError ( ) # Use our own json parser to get datetimes metadata = decode_jsonld ( json_data ) if mongo_collection is not None : await _upload_to_mongodb ( mongo_collection , metadata ) return metadata
7879	def add_prefix ( self , namespace , prefix ) : if prefix == "xml" and namespace != XML_NS : raise ValueError , "Cannot change 'xml' prefix meaning" self . _prefixes [ namespace ] = prefix
560	def anyGoodSprintsActive ( self ) : if self . _state [ 'lastGoodSprint' ] is not None : goodSprints = self . _state [ 'sprints' ] [ 0 : self . _state [ 'lastGoodSprint' ] + 1 ] else : goodSprints = self . _state [ 'sprints' ] for sprint in goodSprints : if sprint [ 'status' ] == 'active' : anyActiveSprints = True break else : anyActiveSprints = False return anyActiveSprints
1903	def colored_level_name ( self , levelname ) : if self . colors_disabled : return self . plain_levelname_format . format ( levelname ) else : return self . colored_levelname_format . format ( self . color_map [ levelname ] , levelname )
3269	def md_jdbc_virtual_table ( key , node ) : name = node . find ( "name" ) sql = node . find ( "sql" ) escapeSql = node . find ( "escapeSql" ) escapeSql = escapeSql . text if escapeSql is not None else None keyColumn = node . find ( "keyColumn" ) keyColumn = keyColumn . text if keyColumn is not None else None n_g = node . find ( "geometry" ) geometry = JDBCVirtualTableGeometry ( n_g . find ( "name" ) , n_g . find ( "type" ) , n_g . find ( "srid" ) ) parameters = [ ] for n_p in node . findall ( "parameter" ) : p_name = n_p . find ( "name" ) p_defaultValue = n_p . find ( "defaultValue" ) p_defaultValue = p_defaultValue . text if p_defaultValue is not None else None p_regexpValidator = n_p . find ( "regexpValidator" ) p_regexpValidator = p_regexpValidator . text if p_regexpValidator is not None else None parameters . append ( JDBCVirtualTableParam ( p_name , p_defaultValue , p_regexpValidator ) ) return JDBCVirtualTable ( name , sql , escapeSql , geometry , keyColumn , parameters )
8010	def webhook_handler ( * event_types ) : # First expand all wildcards and verify the event types are valid event_types_to_register = set ( ) for event_type in event_types : # Always convert to lowercase event_type = event_type . lower ( ) if "*" in event_type : # expand it for t in WEBHOOK_EVENT_TYPES : if fnmatch ( t , event_type ) : event_types_to_register . add ( t ) elif event_type not in WEBHOOK_EVENT_TYPES : raise ValueError ( "Unknown webhook event: %r" % ( event_type ) ) else : event_types_to_register . add ( event_type ) # Now register them def decorator ( func ) : for event_type in event_types_to_register : WEBHOOK_SIGNALS [ event_type ] . connect ( func ) return func return decorator
7015	def concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , recursive = True ) : LOGINFO ( 'looking for light curves for %s, aperture %s in directory: %s' % ( objectid , aperture , lcbasedir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcbasedir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) ) else : # use recursive glob for Python 3.5+ if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcbasedir , '**' , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) , recursive = True ) LOGINFO ( 'found %s files: %s' % ( len ( matching ) , repr ( matching ) ) ) # otherwise, use os.walk and glob else : # use os.walk to go through the directories walker = os . walk ( lcbasedir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) LOGINFO ( 'found %s in dir: %s' % ( repr ( foundfiles ) , os . path . join ( root , sdir ) ) ) # now that we have all the files, concatenate them # a single file will be returned as normalized if matching and len ( matching ) > 0 : clcdict = concatenate_textlcs ( matching , sortby = sortby , normalize = normalize ) return clcdict else : LOGERROR ( 'did not find any light curves for %s and aperture %s' % ( objectid , aperture ) ) return None
8241	def compound ( clr , flip = False ) : def _wrap ( x , min , threshold , plus ) : if x - min < threshold : return x + plus else : return x - min d = 1 if flip : d = - 1 clr = color ( clr ) colors = colorlist ( clr ) c = clr . rotate_ryb ( 30 * d ) c . brightness = _wrap ( clr . brightness , 0.25 , 0.6 , 0.25 ) colors . append ( c ) c = clr . rotate_ryb ( 30 * d ) c . saturation = _wrap ( clr . saturation , 0.4 , 0.1 , 0.4 ) c . brightness = _wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) colors . append ( c ) c = clr . rotate_ryb ( 160 * d ) c . saturation = _wrap ( clr . saturation , 0.25 , 0.1 , 0.25 ) c . brightness = max ( 0.2 , clr . brightness ) colors . append ( c ) c = clr . rotate_ryb ( 150 * d ) c . saturation = _wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = _wrap ( clr . brightness , 0.3 , 0.6 , 0.3 ) colors . append ( c ) c = clr . rotate_ryb ( 150 * d ) c . saturation = _wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = _wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) # colors.append(c) return colors
6310	def load ( self ) : self . _open_image ( ) components , data = image_data ( self . image ) texture = self . ctx . texture ( self . image . size , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build_mipmaps ( ) self . _close_image ( ) return texture
6913	def generate_sinusoidal_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.04 , scale = 500.0 ) , 'fourierorder' : [ 2 , 10 ] , 'amplitude' : sps . uniform ( loc = 0.1 , scale = 0.9 ) , 'phioffset' : 0.0 , } , magsarefluxes = False ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) # choose the epoch epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) # choose the period, fourierorder, and amplitude period = paramdists [ 'period' ] . rvs ( size = 1 ) fourierorder = npr . randint ( paramdists [ 'fourierorder' ] [ 0 ] , high = paramdists [ 'fourierorder' ] [ 1 ] ) amplitude = paramdists [ 'amplitude' ] . rvs ( size = 1 ) # fix the amplitude if it needs to be flipped if magsarefluxes and amplitude < 0.0 : amplitude = - amplitude elif not magsarefluxes and amplitude > 0.0 : amplitude = - amplitude # generate the amplitudes and phases of the Fourier components ampcomps = [ abs ( amplitude / 2.0 ) / float ( x ) for x in range ( 1 , fourierorder + 1 ) ] phacomps = [ paramdists [ 'phioffset' ] * float ( x ) for x in range ( 1 , fourierorder + 1 ) ] # now that we have our amp and pha components, generate the light curve modelmags , phase , ptimes , pmags , perrs = sinusoidal . sine_series_sum ( [ period , epoch , ampcomps , phacomps ] , times , mags , errs ) # resort in original time order timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] mphase = phase [ timeind ] # return a dict with everything modeldict = { 'vartype' : 'sinusoidal' , 'params' : { x : y for x , y in zip ( [ 'period' , 'epoch' , 'amplitude' , 'fourierorder' , 'fourieramps' , 'fourierphases' ] , [ period , epoch , amplitude , fourierorder , ampcomps , phacomps ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'phase' : mphase , # these are standard keys that help with later characterization of # variability as a function period, variability amplitude, object mag, # ndet, etc. 'varperiod' : period , 'varamplitude' : amplitude } return modeldict
11051	def listen_events ( self , reconnects = 0 ) : self . log . info ( 'Listening for events from Marathon...' ) self . _attached = False def on_finished ( result , reconnects ) : # If the callback fires then the HTTP request to the event stream # went fine, but the persistent connection for the SSE stream was # dropped. Just reconnect for now- if we can't actually connect # then the errback will fire rather. self . log . warn ( 'Connection lost listening for events, ' 'reconnecting... ({reconnects} so far)' , reconnects = reconnects ) reconnects += 1 return self . listen_events ( reconnects ) def log_failure ( failure ) : self . log . failure ( 'Failed to listen for events' , failure ) return failure return self . marathon_client . get_events ( { 'event_stream_attached' : self . _sync_on_event_stream_attached , 'api_post_event' : self . _sync_on_api_post_event } ) . addCallbacks ( on_finished , log_failure , callbackArgs = [ reconnects ] )
7586	def _get_boots ( arr , nboots ) : ## hold results (nboots, [dstat, ]) boots = np . zeros ( ( nboots , ) ) ## iterate to fill boots for bidx in xrange ( nboots ) : ## sample with replacement lidx = np . random . randint ( 0 , arr . shape [ 0 ] , arr . shape [ 0 ] ) tarr = arr [ lidx ] _ , _ , dst = _prop_dstat ( tarr ) boots [ bidx ] = dst ## return bootarr return boots
9845	def ndmeshgrid ( * arrs ) : #arrs = tuple(reversed(arrs)) <-- wrong on stackoverflow.com arrs = tuple ( arrs ) lens = list ( map ( len , arrs ) ) dim = len ( arrs ) sz = 1 for s in lens : sz *= s ans = [ ] for i , arr in enumerate ( arrs ) : slc = [ 1 ] * dim slc [ i ] = lens [ i ] arr2 = numpy . asanyarray ( arr ) . reshape ( slc ) for j , sz in enumerate ( lens ) : if j != i : arr2 = arr2 . repeat ( sz , axis = j ) ans . append ( arr2 ) return tuple ( ans )
7086	def _single_true ( iterable ) : # return True if exactly one true found iterator = iter ( iterable ) # consume from "i" until first true or it's exhausted has_true = any ( iterator ) # carry on consuming until another true value / exhausted has_another_true = any ( iterator ) return has_true and not has_another_true
5382	def _build_pipeline_request ( self , task_view ) : job_metadata = task_view . job_metadata job_params = task_view . job_params job_resources = task_view . job_resources task_metadata = task_view . task_descriptors [ 0 ] . task_metadata task_params = task_view . task_descriptors [ 0 ] . task_params task_resources = task_view . task_descriptors [ 0 ] . task_resources script = task_view . job_metadata [ 'script' ] reserved_labels = google_base . build_pipeline_labels ( job_metadata , task_metadata , task_id_pattern = 'task-%d' ) # Build the ephemeralPipeline for this job. # The ephemeralPipeline definition changes for each job because file # parameters localCopy.path changes based on the remote_uri. pipeline = _Pipelines . build_pipeline ( project = self . _project , zones = job_resources . zones , min_cores = job_resources . min_cores , min_ram = job_resources . min_ram , disk_size = job_resources . disk_size , boot_disk_size = job_resources . boot_disk_size , preemptible = job_resources . preemptible , accelerator_type = job_resources . accelerator_type , accelerator_count = job_resources . accelerator_count , image = job_resources . image , script_name = script . name , envs = job_params [ 'envs' ] | task_params [ 'envs' ] , inputs = job_params [ 'inputs' ] | task_params [ 'inputs' ] , outputs = job_params [ 'outputs' ] | task_params [ 'outputs' ] , pipeline_name = job_metadata [ 'pipeline-name' ] ) # Build the pipelineArgs for this job. logging_uri = task_resources . logging_path . uri scopes = job_resources . scopes or google_base . DEFAULT_SCOPES pipeline . update ( _Pipelines . build_pipeline_args ( self . _project , script . value , job_params , task_params , reserved_labels , job_resources . preemptible , logging_uri , scopes , job_resources . keep_alive ) ) return pipeline
8216	def hide_variables_window ( self ) : if self . var_window is not None : self . var_window . window . destroy ( ) self . var_window = None
6234	def start ( self ) : if self . initialized : mixer . music . unpause ( ) else : mixer . music . play ( ) # FIXME: Calling play twice to ensure the music is actually playing mixer . music . play ( ) self . initialized = True self . paused = False
12876	def many ( parser ) : results = [ ] terminate = object ( ) while local_ps . value : result = optional ( parser , terminate ) if result == terminate : break results . append ( result ) return results
9949	def new_space ( self , name = None , bases = None , formula = None , * , refs = None , source = None , is_derived = False , prefix = "" ) : from modelx . core . space import StaticSpaceImpl if name is None : name = self . spacenamer . get_next ( self . namespace , prefix ) if name in self . namespace : raise ValueError ( "Name '%s' already exists." % name ) if not prefix and not is_valid_name ( name ) : raise ValueError ( "Invalid name '%s'." % name ) space = self . _new_space ( name = name , formula = formula , refs = refs , source = source , is_derived = is_derived , ) self . _set_space ( space ) self . model . spacegraph . add_space ( space ) # Set up direct base spaces and mro if bases is not None : if isinstance ( bases , StaticSpaceImpl ) : bases = [ bases ] space . add_bases ( bases ) return space
12621	def repr_imgs ( imgs ) : if isinstance ( imgs , string_types ) : return imgs if isinstance ( imgs , collections . Iterable ) : return '[{}]' . format ( ', ' . join ( repr_imgs ( img ) for img in imgs ) ) # try get_filename try : filename = imgs . get_filename ( ) if filename is not None : img_str = "{}('{}')" . format ( imgs . __class__ . __name__ , filename ) else : img_str = "{}(shape={}, affine={})" . format ( imgs . __class__ . __name__ , repr ( get_shape ( imgs ) ) , repr ( imgs . get_affine ( ) ) ) except Exception as exc : log . error ( 'Error reading attributes from img.get_filename()' ) return repr ( imgs ) else : return img_str
8464	def deploy ( target ) : # Ensure proper environment if not os . getenv ( CIRCLECI_ENV_VAR ) : # pragma: no cover raise EnvironmentError ( 'Must be on CircleCI to run this script' ) current_branch = os . getenv ( 'CIRCLE_BRANCH' ) if ( target == 'PROD' ) and ( current_branch != 'master' ) : raise EnvironmentError ( ( 'Refusing to deploy to production from branch {current_branch!r}. ' 'Production deploys can only be made from master.' ) . format ( current_branch = current_branch ) ) if target in ( 'PROD' , 'TEST' ) : pypi_username = os . getenv ( '{target}_PYPI_USERNAME' . format ( target = target ) ) pypi_password = os . getenv ( '{target}_PYPI_PASSWORD' . format ( target = target ) ) else : raise ValueError ( "Deploy target must be 'PROD' or 'TEST', got {target!r}." . format ( target = target ) ) if not ( pypi_username and pypi_password ) : # pragma: no cover raise EnvironmentError ( ( "Missing '{target}_PYPI_USERNAME' and/or '{target}_PYPI_PASSWORD' " "environment variables. These are required to push to PyPI." ) . format ( target = target ) ) # Twine requires these environment variables to be set. Subprocesses will # inherit these when we invoke them, so no need to pass them on the command # line. We want to avoid that in case something's logging each command run. os . environ [ 'TWINE_USERNAME' ] = pypi_username os . environ [ 'TWINE_PASSWORD' ] = pypi_password # Set up git on circle to push to the current branch _shell ( 'git config --global user.email "oss@cloverhealth.com"' ) _shell ( 'git config --global user.name "Circle CI"' ) _shell ( 'git config push.default current' ) # Obtain the version to deploy ret = _shell ( 'make version' , stdout = subprocess . PIPE ) version = ret . stdout . decode ( 'utf-8' ) . strip ( ) print ( 'Deploying version {version!r}...' . format ( version = version ) ) # Tag the version _shell ( 'git tag -f -a {version} -m "Version {version}"' . format ( version = version ) ) # Update the version _shell ( 'sed -i.bak "s/^__version__ = .*/__version__ = {version!r}/" */version.py' . format ( version = version ) ) # Create a standard distribution and a wheel _shell ( 'python setup.py sdist bdist_wheel' ) # Add the updated ChangeLog and AUTHORS _shell ( 'git add ChangeLog AUTHORS */version.py' ) # Start the commit message with "Merge" so that PBR will ignore it in the # ChangeLog. Use [skip ci] to ensure CircleCI doesn't recursively deploy. _shell ( 'git commit --no-verify -m "Merge autogenerated files [skip ci]"' ) # Push the distributions to PyPI. _pypi_push ( 'dist' ) # Push the tag and AUTHORS / ChangeLog after successful PyPI deploy _shell ( 'git push --follow-tags' ) print ( 'Deployment complete. Latest version is {version}.' . format ( version = version ) )
12912	def append ( self , item ) : if self . meta_type == 'dict' : raise AssertionError ( 'Cannot append to object of `dict` base type!' ) if self . meta_type == 'list' : self . _list . append ( item ) return
4435	async def get_tracks ( self , query ) : log . debug ( 'Requesting tracks for query {}' . format ( query ) ) async with self . http . get ( self . rest_uri + quote ( query ) , headers = { 'Authorization' : self . password } ) as res : return await res . json ( content_type = None )
7559	def set_mkl_thread_limit ( cores ) : if "linux" in sys . platform : mkl_rt = ctypes . CDLL ( 'libmkl_rt.so' ) else : mkl_rt = ctypes . CDLL ( 'libmkl_rt.dylib' ) oldlimit = mkl_rt . mkl_get_max_threads ( ) mkl_rt . mkl_set_num_threads ( ctypes . byref ( ctypes . c_int ( cores ) ) ) return oldlimit
4905	def create_course_completion ( self , user_id , payload ) : # pylint: disable=unused-argument return self . _post ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . completion_status_api_path ) , payload , self . COMPLETION_PROVIDER_SCOPE )
3873	async def leave_conversation ( self , conv_id ) : logger . info ( 'Leaving conversation: {}' . format ( conv_id ) ) await self . _conv_dict [ conv_id ] . leave ( ) del self . _conv_dict [ conv_id ]
13563	def repack ( self ) : items = self . grouped_filter ( ) . order_by ( 'rank' ) . select_for_update ( ) for count , item in enumerate ( items ) : item . rank = count + 1 item . save ( rerank = False )
13589	def json_post_required ( * decorator_args ) : def decorator ( method ) : @ wraps ( method ) def wrapper ( * args , * * kwargs ) : field = decorator_args [ 0 ] if len ( decorator_args ) == 2 : request_name = decorator_args [ 1 ] else : request_name = field request = args [ 0 ] if request . method != 'POST' : logger . error ( 'POST required for this url' ) raise Http404 ( 'only POST allowed for this url' ) if field not in request . POST : s = 'Expected field named %s in POST' % field logger . error ( s ) raise Http404 ( s ) # deserialize the JSON and put it in the request setattr ( request , request_name , json . loads ( request . POST [ field ] ) ) # everything verified, run the view return method ( * args , * * kwargs ) return wrapper return decorator
224	async def receive ( self ) -> Message : if self . client_state == WebSocketState . CONNECTING : message = await self . _receive ( ) message_type = message [ "type" ] assert message_type == "websocket.connect" self . client_state = WebSocketState . CONNECTED return message elif self . client_state == WebSocketState . CONNECTED : message = await self . _receive ( ) message_type = message [ "type" ] assert message_type in { "websocket.receive" , "websocket.disconnect" } if message_type == "websocket.disconnect" : self . client_state = WebSocketState . DISCONNECTED return message else : raise RuntimeError ( 'Cannot call "receive" once a disconnect message has been received.' )
10675	def list_compounds ( ) : print ( 'Compounds currently loaded:' ) for compound in sorted ( compounds . keys ( ) ) : phases = compounds [ compound ] . get_phase_list ( ) print ( '%s: %s' % ( compound , ', ' . join ( phases ) ) )
1543	def get_clusters ( ) : instance = tornado . ioloop . IOLoop . instance ( ) # pylint: disable=unnecessary-lambda try : return instance . run_sync ( lambda : API . get_clusters ( ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
12851	def _unlock_temporarily ( self ) : if not self . _is_locked : yield else : try : self . _is_locked = False yield finally : self . _is_locked = True
12058	def TK_askPassword ( title = "input" , msg = "type here:" ) : root = tkinter . Tk ( ) root . withdraw ( ) #hide tk window root . attributes ( "-topmost" , True ) #always on top root . lift ( ) #bring to top value = tkinter . simpledialog . askstring ( title , msg ) root . destroy ( ) return value
6014	def load_positions ( positions_path ) : with open ( positions_path ) as f : position_string = f . readlines ( ) positions = [ ] for line in position_string : position_list = ast . literal_eval ( line ) positions . append ( position_list ) return positions
6512	def _set ( self , name , gender , country_values ) : if '+' in name : for replacement in [ '' , ' ' , '-' ] : self . _set ( name . replace ( '+' , replacement ) , gender , country_values ) else : if name not in self . names : self . names [ name ] = { } self . names [ name ] [ gender ] = country_values
9025	def kivy_svg ( self ) : from kivy . graphics . svg import Svg path = self . temporary_path ( ".svg" ) try : return Svg ( path ) finally : remove_file ( path )
3663	def calculate_integral_over_T ( self , T1 , T2 , method ) : if method == ZABRANSKY_SPLINE : return self . Zabransky_spline . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_SPLINE_C : return self . Zabransky_spline_iso . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_SPLINE_SAT : return self . Zabransky_spline_sat . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL : return self . Zabransky_quasipolynomial . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_C : return self . Zabransky_quasipolynomial_iso . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_SAT : return self . Zabransky_quasipolynomial_sat . calculate_integral_over_T ( T1 , T2 ) elif method == POLING_CONST : return self . POLING_constant * log ( T2 / T1 ) elif method == CRCSTD : return self . CRCSTD_constant * log ( T2 / T1 ) elif method == DADGOSTAR_SHAW : dS = ( Dadgostar_Shaw_integral_over_T ( T2 , self . similarity_variable ) - Dadgostar_Shaw_integral_over_T ( T1 , self . similarity_variable ) ) return property_mass_to_molar ( dS , self . MW ) elif method in self . tabular_data or method == COOLPROP or method in [ ROWLINSON_POLING , ROWLINSON_BONDI ] : return float ( quad ( lambda T : self . calculate ( T , method ) / T , T1 , T2 ) [ 0 ] ) else : raise Exception ( 'Method not valid' )
10965	def get ( self ) : fields = [ c . get ( ) for c in self . comps ] return self . field_reduce_func ( fields )
10530	def get_project ( project_id ) : try : res = _pybossa_req ( 'get' , 'project' , project_id ) if res . get ( 'id' ) : return Project ( res ) else : return res except : # pragma: no cover raise
12730	def axes ( self ) : return [ np . array ( self . ode_obj . getAxis1 ( ) ) , np . array ( self . ode_obj . getAxis2 ( ) ) ]
581	def _setRandomEncoderResolution ( minResolution = 0.001 ) : encoder = ( model_params . MODEL_PARAMS [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] [ "value" ] ) if encoder [ "type" ] == "RandomDistributedScalarEncoder" : rangePadding = abs ( _INPUT_MAX - _INPUT_MIN ) * 0.2 minValue = _INPUT_MIN - rangePadding maxValue = _INPUT_MAX + rangePadding resolution = max ( minResolution , ( maxValue - minValue ) / encoder . pop ( "numBuckets" ) ) encoder [ "resolution" ] = resolution
3951	def _read_comparator ( self , mux , gain , data_rate , mode , high_threshold , low_threshold , active_low , traditional , latching , num_readings ) : assert num_readings == 1 or num_readings == 2 or num_readings == 4 , 'Num readings must be 1, 2, or 4!' # Set high and low threshold register values. self . _device . writeList ( ADS1x15_POINTER_HIGH_THRESHOLD , [ ( high_threshold >> 8 ) & 0xFF , high_threshold & 0xFF ] ) self . _device . writeList ( ADS1x15_POINTER_LOW_THRESHOLD , [ ( low_threshold >> 8 ) & 0xFF , low_threshold & 0xFF ] ) # Now build up the appropriate config register value. config = ADS1x15_CONFIG_OS_SINGLE # Go out of power-down mode for conversion. # Specify mux value. config |= ( mux & 0x07 ) << ADS1x15_CONFIG_MUX_OFFSET # Validate the passed in gain and then set it in the config. if gain not in ADS1x15_CONFIG_GAIN : raise ValueError ( 'Gain must be one of: 2/3, 1, 2, 4, 8, 16' ) config |= ADS1x15_CONFIG_GAIN [ gain ] # Set the mode (continuous or single shot). config |= mode # Get the default data rate if none is specified (default differs between # ADS1015 and ADS1115). if data_rate is None : data_rate = self . _data_rate_default ( ) # Set the data rate (this is controlled by the subclass as it differs # between ADS1015 and ADS1115). config |= self . _data_rate_config ( data_rate ) # Enable window mode if required. if not traditional : config |= ADS1x15_CONFIG_COMP_WINDOW # Enable active high mode if required. if not active_low : config |= ADS1x15_CONFIG_COMP_ACTIVE_HIGH # Enable latching mode if required. if latching : config |= ADS1x15_CONFIG_COMP_LATCHING # Set number of comparator hits before alerting. config |= ADS1x15_CONFIG_COMP_QUE [ num_readings ] # Send the config value to start the ADC conversion. # Explicitly break the 16-bit value down to a big endian pair of bytes. self . _device . writeList ( ADS1x15_POINTER_CONFIG , [ ( config >> 8 ) & 0xFF , config & 0xFF ] ) # Wait for the ADC sample to finish based on the sample rate plus a # small offset to be sure (0.1 millisecond). time . sleep ( 1.0 / data_rate + 0.0001 ) # Retrieve the result. result = self . _device . readList ( ADS1x15_POINTER_CONVERSION , 2 ) return self . _conversion_value ( result [ 1 ] , result [ 0 ] )
3163	def create ( self , workflow_id , email_id , data ) : self . workflow_id = workflow_id self . email_id = email_id if 'email_address' not in data : raise KeyError ( 'The automation email queue must have an email_address' ) check_email ( data [ 'email_address' ] ) response = self . _mc_client . _post ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'queue' ) , data = data ) if response is not None : self . subscriber_hash = response [ 'id' ] else : self . subscriber_hash = None return response
12787	def check_file ( self , filename ) : # type: (str) -> bool if not exists ( filename ) : return False # Check if the file is version-compatible with this instance. new_config = ConfigResolverBase ( ) new_config . read ( filename ) if self . version and not new_config . has_option ( 'meta' , 'version' ) : # self.version is set, so we MUST have a version in the file! raise NoVersionError ( "The config option 'meta.version' is missing in {}. The " "application expects version {}!" . format ( filename , self . version ) ) elif not self . version and new_config . has_option ( 'meta' , 'version' ) : # Automatically "lock-in" a version number if one is found. # This prevents loading a chain of config files with incompatible # version numbers! self . version = StrictVersion ( new_config . get ( 'meta' , 'version' ) ) self . _log . info ( '%r contains a version number, but the config ' 'instance was not created with a version ' 'restriction. Will set version number to "%s" to ' 'prevent accidents!' , filename , self . version ) elif self . version : # This instance expected a certain version. We need to check the # version in the file and compare. file_version = new_config . get ( 'meta' , 'version' ) major , minor , _ = StrictVersion ( file_version ) . version expected_major , expected_minor , _ = self . version . version if expected_major != major : self . _log . error ( 'Invalid major version number in %r. Expected %r, got %r!' , abspath ( filename ) , str ( self . version ) , file_version ) return False if expected_minor != minor : self . _log . warning ( 'Mismatching minor version number in %r. ' 'Expected %r, got %r!' , abspath ( filename ) , str ( self . version ) , file_version ) return True return True
8096	def edge_label ( s , edge , alpha = 1.0 ) : if s . text and edge . label != "" : s . _ctx . nostroke ( ) s . _ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha * 0.75 ) s . _ctx . lineheight ( 1 ) s . _ctx . font ( s . font ) s . _ctx . fontsize ( s . fontsize * 0.75 ) # Cache an outlined label text and translate it. # This enhances the speed and avoids wiggling text. try : p = edge . _textpath except : try : txt = unicode ( edge . label ) except : try : txt = edge . label . decode ( "utf-8" ) except : pass edge . _textpath = s . _ctx . textpath ( txt , s . _ctx . textwidth ( " " ) , 0 , width = s . textwidth ) p = edge . _textpath # Position the label centrally along the edge line. a = degrees ( atan2 ( edge . node2 . y - edge . node1 . y , edge . node2 . x - edge . node1 . x ) ) d = sqrt ( ( edge . node2 . x - edge . node1 . x ) ** 2 + ( edge . node2 . y - edge . node1 . y ) ** 2 ) d = abs ( d - s . _ctx . textwidth ( edge . label ) ) * 0.5 s . _ctx . push ( ) s . _ctx . transform ( CORNER ) s . _ctx . translate ( edge . node1 . x , edge . node1 . y ) s . _ctx . rotate ( - a ) s . _ctx . translate ( d , s . fontsize * 1.0 ) s . _ctx . scale ( alpha ) # Flip labels on the left hand side so they are legible. if 90 < a % 360 < 270 : s . _ctx . translate ( s . _ctx . textwidth ( edge . label ) , - s . fontsize * 2.0 ) s . _ctx . transform ( CENTER ) s . _ctx . rotate ( 180 ) s . _ctx . transform ( CORNER ) s . _ctx . drawpath ( p . copy ( ) ) s . _ctx . pop ( )
3188	def create ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash if 'note' not in data : raise KeyError ( 'The list member note must have a note' ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' , subscriber_hash , 'notes' ) , data = data ) if response is not None : self . note_id = response [ 'id' ] else : self . note_id = None return response
12764	def attach ( self , frame_no ) : assert not self . joints for label , j in self . channels . items ( ) : target = self . targets . get ( label ) if target is None : continue if self . visibility [ frame_no , j ] < 0 : continue if np . linalg . norm ( self . velocities [ frame_no , j ] ) > 10 : continue joint = ode . BallJoint ( self . world . ode_world , self . jointgroup ) joint . attach ( self . bodies [ label ] . ode_body , target . ode_body ) joint . setAnchor1Rel ( [ 0 , 0 , 0 ] ) joint . setAnchor2Rel ( self . offsets [ label ] ) joint . setParam ( ode . ParamCFM , self . cfms [ frame_no , j ] ) joint . setParam ( ode . ParamERP , self . erp ) joint . name = label self . joints [ label ] = joint self . _frame_no = frame_no
8265	def _interpolate ( self , colors , n = 100 ) : gradient = [ ] for i in _range ( n ) : l = len ( colors ) - 1 x = int ( 1.0 * i / n * l ) x = min ( x + 0 , l ) y = min ( x + 1 , l ) base = 1.0 * n / l * x d = ( i - base ) / ( 1.0 * n / l ) r = colors [ x ] . r * ( 1 - d ) + colors [ y ] . r * d g = colors [ x ] . g * ( 1 - d ) + colors [ y ] . g * d b = colors [ x ] . b * ( 1 - d ) + colors [ y ] . b * d a = colors [ x ] . a * ( 1 - d ) + colors [ y ] . a * d gradient . append ( color ( r , g , b , a , mode = "rgb" ) ) gradient . append ( colors [ - 1 ] ) return gradient
