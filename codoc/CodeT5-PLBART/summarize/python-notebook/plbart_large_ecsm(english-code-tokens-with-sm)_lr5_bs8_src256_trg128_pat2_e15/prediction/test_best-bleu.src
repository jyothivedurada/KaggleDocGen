0	import pandas as pd import numpy as np import matplotlib import matplotlib pyplot as plt import seaborn as sns color sns color_palette import plotly offline as py py init_notebook_mode connected True from plotly offline import init_notebook_mode iplot init_notebook_mode connected True import plotly graph_objs as go import plotly offline as offline offline init_notebook_mode import cufflinks as cf cf go_offline
1	train_labels application_train_dummies 'TARGET' application_train_dummies application_test_dummies application_train_dummies align application_test_dummies join 'inner' axis application_train_dummies 'TARGET' train_labels print application_train_dummies shape print application_test_dummies shape
2	import seaborn as sns color sns color_palette plt figure figsize plt title ax sns distplot "AMT_INCOME_TOTAL"
3	from scipy stats import boxcox from matplotlib import pyplot np log application_train 'AMT_INCOME_TOTAL' iplot kind 'histogram' bins xTitle yTitle title
4	import seaborn as sns color sns color_palette plt figure figsize plt title ax sns distplot application_train "AMT_CREDIT"
5	original_train_data pd read_csv '../input/home-credit-default-risk/application_train.csv' contract_val original_train_data 'NAME_CONTRACT_TYPE' value_counts contract_df pd DataFrame 'labels' contract_val index 'values' contract_val values contract_df iplot kind 'pie' labels 'labels' values 'values' title
6	original_train_data "NAME_TYPE_SUITE" iplot kind "histogram" bins theme "white" title xTitle yTitle
7	education_val original_train_data 'NAME_FAMILY_STATUS' value_counts education_val_y0 education_val_y1 for val in education_val index education_val_y1 append np sum original_train_data 'TARGET' original_train_data 'NAME_FAMILY_STATUS' val education_val_y0 append np sum original_train_data 'TARGET' original_train_data 'NAME_FAMILY_STATUS' val data go Bar education_val index education_val_y1 education_val sum name go Bar education_val index education_val_y0 education_val sum name layout go Layout title xaxis dict title yaxis dict title fig go Figure data data layout layout fig layout template 'plotly_dark' py iplot fig
8	original_train_data "DAYS_BIRTH" iplot kind "histogram" bins theme "white" title xTitle yTitle
9	application_train_dummies application_train_dummies 'AMT_INCOME_TOTAL' application_train_dummies 'AMT_CREDIT' application_train_dummies application_train_dummies 'DAYS_EMPLOYED' application_train_dummies 'DAYS_BIRTH' application_train_dummies application_train_dummies 'AMT_ANNUITY' application_train_dummies 'AMT_INCOME_TOTAL' application_train_dummies application_train_dummies 'AMT_CREDIT' application_train_dummies 'AMT_INCOME_TOTAL' application_test_dummies application_test_dummies 'AMT_INCOME_TOTAL' application_test_dummies 'AMT_CREDIT' application_test_dummies application_test_dummies 'DAYS_EMPLOYED' application_test_dummies 'DAYS_BIRTH' application_test_dummies application_test_dummies 'AMT_ANNUITY' application_test_dummies 'AMT_INCOME_TOTAL' application_test_dummies application_test_dummies 'AMT_CREDIT' application_test_dummies 'AMT_INCOME_TOTAL'
10	grp bureau drop 'SK_ID_BUREAU' axis groupby by 'SK_ID_CURR' mean reset_index grp columns 'BUREAU_' column if column 'SK_ID_CURR' else column for column in grp columns application_bureau application_train_dummies merge grp on 'SK_ID_CURR' how 'left' application_bureau update application_bureau grp columns fillna application_bureau_test application_test_dummies merge grp on 'SK_ID_CURR' how 'left' application_bureau_test update application_bureau_test grp columns fillna
11	grp bureau groupby by 'SK_ID_CURR' 'SK_ID_BUREAU' count reset_index rename columns 'SK_ID_BUREAU' 'BUREAU_LOAN_COUNT' application_bureau application_bureau merge grp on 'SK_ID_CURR' how 'left' application_bureau 'BUREAU_LOAN_COUNT' application_bureau 'BUREAU_LOAN_COUNT' fillna application_bureau_test application_bureau_test merge grp on 'SK_ID_CURR' how 'left' application_bureau_test 'BUREAU_LOAN_COUNT' application_bureau_test 'BUREAU_LOAN_COUNT' fillna
12	def isOneToOne df col1 col2 first df drop_duplicates col1 col2 groupby col1 col2 count max second df drop_duplicates col1 col2 groupby col2 col1 count max return first second isOneToOne previous_application 'SK_ID_CURR' 'SK_ID_PREV'
13	credit_card credit_card_balance grp credit_card drop 'SK_ID_PREV' axis groupby by 'SK_ID_CURR' mean reset_index prev_columns 'CREDIT_' column if column 'SK_ID_CURR' else column for column in grp columns grp columns prev_columns application_bureau_prev application_bureau_prev merge grp on 'SK_ID_CURR' how 'left' application_bureau_prev update application_bureau_prev grp columns fillna application_bureau_prev_test application_bureau_prev_test merge grp on 'SK_ID_CURR' how 'left' application_bureau_prev_test update application_bureau_prev_test grp columns fillna
14	from sklearn preprocessing import MinMaxScaler StandardScaler PolynomialFeatures def preprocessing degree poly PolynomialFeatures degree scaler MinMaxScaler lin_scaler StandardScaler poly_df pd DataFrame lin_scaler fit_transform poly fit_transform scaler fit_transform poly_df 'SK_ID_CURR' index poly_df set_index 'SK_ID_CURR' inplace True drop True return poly_df
15	import lightgbm as lgb def kfold_lightgbm trn_x trn_y num_folds in_folds StratifiedShuffleSplit n_splits num_folds random_state seed_val for train_idx valid_idx in in_folds split trn_x trn_y dtrain lgb Dataset data trn_x train_idx label trn_y train_idx free_raw_data False silent True dvalid lgb Dataset data trn_x valid_idx label trn_y valid_idx free_raw_data False silent True params 'objective' 'binary' 'boosting_type' 'gbdt' 'nthread' 'learning_rate' 'num_leaves' 'colsample_bytree' 'subsample' 'subsample_freq' 'max_depth' 'reg_alpha' 'reg_lambda' 'min_split_gain' 'min_child_weight' 'seed' seed_val 'verbose' 'metric' 'auc' clf lgb train params params train_set dtrain num_boost_round valid_sets dtrain dvalid early_stopping_rounds verbose_eval False del dtrain dvalid gc collect return clf
16	reverse_list
17	from sklearn preprocessing import StandardScaler values scaler StandardScaler x_scaled scaler fit_transform pd DataFrame x_scaled set_index index
18	for in range np std train 'var_' str np mean train 'var_' str def getp len train1 train1 'var_' str train1 'var_' str len train0 train0 'var_' str train0 'var_' str if return return def smooth st for in range st x2 np ones len for in range len x2 x2 copy return
19	from sklearn linear_model import Ridge import sklearn linear_model def ridge trn_x trn_y clf Ridge alpha copy_X True fit_intercept True solver 'auto' max_iter normalize False random_state tol clf fit trn_x trn_y return clf
20	def kfold_lightgbm trn_x trn_y num_folds in_folds StratifiedShuffleSplit n_splits num_folds random_state seed_val for train_idx valid_idx in in_folds split trn_x trn_y dtrain lgb Dataset data trn_x values train_idx label trn_y train_idx free_raw_data False silent True dvalid lgb Dataset data trn_x values valid_idx label trn_y valid_idx free_raw_data False silent True params 'objective' 'binary' 'boosting_type' 'gbdt' 'nthread' 'learning_rate' 'num_leaves' 'colsample_bytree' 'subsample' 'subsample_freq' 'max_depth' 'reg_alpha' 'reg_lambda' 'min_split_gain' 'min_child_weight' 'seed' seed_val 'verbose' 'metric' 'auc' clf lgb train params params train_set dtrain num_boost_round valid_sets dtrain dvalid early_stopping_rounds verbose_eval False del dtrain dvalid gc collect return clf
21	data_pass '/kaggle/input/m5-forecasting-accuracy/' sales pd read_csv data_pass 'sales_train_validation.csv' calendar pd read_csv data_pass 'calendar.csv' calendar reduce_mem_usage calendar sell_prices pd read_csv data_pass 'sell_prices.csv' sell_prices reduce_mem_usage sell_prices
22	cols "d_{}" format for in range data sales "id" 'store_id' 'item_id' cols data data melt id_vars "id" 'store_id' 'item_id' var_name "d" value_name "sale" data pd merge data calendar how 'left' left_on 'd' right_on 'd' data data "id" 'store_id' 'item_id' "sale" "d" "wm_yr_wk" data data merge sell_prices on 'store_id' 'item_id' 'wm_yr_wk' how 'left' data drop columns 'wm_yr_wk' inplace True data 'sale_usd' data 'sale' data 'sell_price' data head
23	pd DataFrame index roll_index columns 'w' data_pass '/kaggle/input/original-weights/' pd read_csv data_pass 'weights_validation.csv' set_index index 'diff' Weight Predicted Weight values values
24	def rollup return roll_mat_csr def wrmsse preds y_true score_only False sw SW if score_only return np sum np sqrt np mean np square rollup preds values y_true values axis sw else score_matrix np square rollup preds values y_true values np square None None score np sum np sqrt np mean score_matrix axis return score score_matrix
25	file_pass '/kaggle/working/' sw_df pd read_pickle file_pass 'sw_df.pkl' sw_df values sw_df values SW sw_df sw values roll_mat_df pd read_pickle file_pass 'roll_mat_df.pkl' roll_index roll_mat_df index roll_mat_csr csr_matrix roll_mat_df values del roll_mat_df
26	sub pd read_csv '/kaggle/input/m5-forecasting-accuracy/sample_submission.csv' sub sub sub id str endswith 'validation' sub drop 'id' axis inplace True DAYS_PRED sub shape dayCols "d_{}" format for in range DAYS_PRED y_true sales dayCols
27	def centroids lbls centroids np zeros len np unique lbls for in np unique lbls mask lbls centroids np mean mask axis return centroids
28	itx items iloc reset_index plt figure ax andrews_curves itx class_column 'index' colormap 'cubehelix' ax spines 'top' set_visible False ax spines 'right' set_visible False ax grid color 'grey' linestyle linewidth alpha plt show
29	plt figure ax autocorrelation_plot items iloc ax grid color 'grey' linestyle linewidth alpha plt show
30	plt figure ax lag_plot items iloc ax grid color 'grey' linestyle linewidth alpha plt show
31	categ_2 np hstack np zeros np ones categ_3 np hstack np zeros np ones np ones fig ax plt subplots figsize sns distplot categ_2 ax ax sns distplot categ_3 ax ax ax set_xlabel "3 categories" ax set_xlabel "2 categories"
32	diff_categ_2 np sort noise_categ_2 np sort np random normal noise_categ_2 mean noise_categ_2 std diff_categ_3 np sort noise_categ_3 np sort np random normal noise_categ_3 mean noise_categ_3 std fig ax plt subplots figsize sns distplot diff_categ_2 ax ax sns distplot diff_categ_3 ax ax ax set_xlabel ax set_xlabel
33	train_file '../input/train.csv' test_file '../input/test.csv' train pd read_csv train_file index_col pd read_csv test_file index_col
34	for in range np std train 'var_' str np mean train 'var_' str def getp len train1 train1 'var_' str train1 'var_' str len train0 train0 'var_' str train0 'var_' str if return return def smooth st for in range st x2 np ones len for in range len x2 x2 copy return
35	not_null_sex train train 'sex' notnull reset_index drop True nan_sex train train 'sex' isnull reset_index drop True
36	def create_dist df title fig plt figure figsize df "age_approx" value_counts normalize True to_frame reset_index ax sns barplot data 'age_approx' 'index' ax set xlabel ylabel ax set title title
37	test_x test 'image_name' test_x 'image_name' test_x 'image_name' apply lambda '.jpg'
38	train pd read_csv '/kaggle/input/liverpool-ion-switching/train.csv' submission pd read_csv '/kaggle/input/liverpool-ion-switching/sample_submission.csv' test pd read_csv '/kaggle/input/liverpool-ion-switching/test.csv'
39	import seaborn as sns cmap cmap sns diverging_palette as_cmap True def magnify return dict selector "th" props "font-size" "7pt" dict selector "td" props 'padding' "0em 0em" dict selector "th:hover" props "font-size" "12pt" dict selector "tr:hover td:hover" props 'max-width' '200px' 'font-size' '12pt'
40	def movingaverage df df 'cummax' df 'signal' cummax df 'cummin' df 'signal' cummin for in range df 'MA_{}' format df 'signal' rolling window mean df fillna inplace True df reset_index drop True inplace True return df
41	dr pd concat train 'MA_2' for in range test 'MA_2' for in range axis dr columns 'train_b1' 'train_b2' 'train_b3' 'train_b4' 'train_b5' 'train_b6' 'train_b7' 'train_b8' 'train_b9' 'train_b10' 'test_b1' 'test_b2' 'test_b3' 'test_b4' corr dr corr corr style background_gradient cmap axis set_properties 'max-width' '100px' 'font-size' '10pt' set_caption set_precision set_table_styles magnify
42	import pandas as pd import warnings warnings filterwarnings 'ignore' train pd read_csv "../input/train.csv" train head
43	plt figure figsize sns distplot np log train price_doc values bins kde True plt xlabel fontsize plt show
44	def remove_outlier df column_list lower_percentile upper_percentile for in range len column_list df df df column_list np percentile df column_list upper_percentile df column_list np percentile df column_list lower_percentile return df outlier_removal_list market_data_no_outlier remove_outlier market_train_df outlier_removal_list print len market_train_df " to " len market_data_no_outlier market_data_no_outlier corr fig plt figure figsize sb heatmap vmax square True annot True plt show
45	features temp_show market_data_no_outlier_scaled features temp_show 'target' market_data_no_outlier_target temp_show corr fig plt figure figsize sb heatmap vmax square True annot True plt show del temp_show
46	from keras callbacks import ModelCheckpoint EarlyStopping early_stopping EarlyStopping monitor 'val_loss' patience verbose mode 'auto' restore_best_weights True callbacks_list early_stopping
47	def make_my_prediction my_pred model predict reshape my_pred my_pred my_pred my_pred return my_pred
48	for market_obs_df predictions_template_df in env get_prediction_days features market_obs_df_scaled scale_data market_obs_df features x_submission market_obs_df_scaled features copy for in range len features x_submission features x_submission features fillna x_submission features mean predictions_template_df make_my_prediction x_submission env predict predictions_template_df del x_submission print env write_submission_file
49	path Path '../input' train_df pd read_csv path 'train.csv' var_names col for col in train_df if 'var_' in col
50	def count_dist_peaks series bins prominence width count division np histogram series bins bins peaks props find_peaks count prominence prominence width width return peaks
51	train_df_num_list train_df_num columns tolist for in train_df_num_list make_histogram
52	img_name predictions gtf Infer img_name img_name from IPython display import Image Image filename img_name
53	import pandas as pd from tqdm import tqdm_notebook as tqdm from scipy special import softmax df pd read_csv "/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv"
54	from keras models import Model from keras layers import Input Dense Reshape concatenate Flatten from keras layers import BatchNormalization Dropout def simple_cnn pic_input Input shape ang_input Input shape cnn BatchNormalization pic_input for in range cnn kernel_size activation 'relu' cnn cnn cnn cnn cnn cnn concatenate cnn ang_input cnn Dense activation 'relu' cnn cnn Dense activation 'sigmoid' cnn simple_cnn Model inputs pic_input ang_input outputs cnn simple_cnn compile optimizer 'adam' loss 'binary_crossentropy' metrics 'accuracy' return simple_cnn
55	model simple_cnn model fit_generator gen_flow validation_data y_valid steps_per_epoch len batch_size epochs
56	test_img_file_path train_dogs_filepaths img_array cv2 imread test_img_file_path cv2 IMREAD_COLOR plt imshow img_array plt show
57	ROW_DIMENSION COLUMN_DIMENSION CHANNELS new_array cv2 resize img_array_gray ROW_DIMENSION COLUMN_DIMENSION plt imshow new_array cmap 'gray' plt show
58	def read_converted_img to_read_img_array plt imshow to_read_img_array cmap 'gray' plt show def prep_img single_image_path img_array_to_resize cv2 imread single_image_path cv2 IMREAD_COLOR resized cv2 resize img_array_to_resize ROW_DIMENSION COLUMN_DIMENSION interpolation cv2 INTER_CUBIC return resized def prep_data list_of_image_paths size len list_of_image_paths preped_data for image_file_path in enumerate list_of_image_paths preped_data append cv2 resize cv2 imread image_file_path ROW_DIMENSION COLUMN_DIMENSION interpolation cv2 INTER_CUBIC if print "of" size return preped_data
59	from tensorflow import keras from keras models import Sequential from keras layers import Dense Flatten Dropout print
60	dvc_classifier compile loss keras losses binary_crossentropy optimizer 'adam' metrics 'accuracy'
61	for in range if prediction_probabilities print format prediction_probabilities else print format prediction_probabilities plt imshow arr_test plt show
62	class TGSSaltDataset data Dataset def __init__ self root_path file_list self root_path root_path self file_list file_list def __len__ self return len self file_list def __getitem__ self index if index not in range len self file_list return self __getitem__ np random randint self __len__ file_id self file_list index image_folder os path join self root_path "images" image_path os path join image_folder file_id ".png" mask_folder os path join self root_path "masks" mask_path os path join mask_folder file_id ".png" image np array imageio imread image_path dtype np uint8 mask np array imageio imread mask_path dtype np uint8 return image mask
63	def plot2x2Array image mask axarr plt subplots axarr imshow image axarr imshow mask axarr grid axarr grid axarr set_title axarr set_title
64	def decode_image filename label None image_size IMG_SIZE IMG_SIZE bits tf io read_file filename image tf image decode_jpeg bits channels image tf cast image tf float32 image tf image resize image image_size if label is None return image else return image label def data_augment image label None seed image tf image random_flip_left_right image seed seed image tf image random_flip_up_down image seed seed if label is None return image else return image label
65	train_dataset tf data Dataset from_tensor_slices x_train y_train map decode_image num_parallel_calls AUTO map data_augment num_parallel_calls AUTO repeat shuffle batch BATCH_SIZE prefetch AUTO
66	print import numpy as np import pandas as pd import matplotlib pyplot as plt from scipy optimize import curve_fit from statsmodels tsa statespace sarimax import SARIMAX from statsmodels tsa arima_model import ARIMA from random import random
67	print "read in train file" df pd read_csv "/kaggle/input/covid19-global-forecasting-week-2/train.csv" usecols
68	sns set_style 'whitegrid' sns set rc 'figure.figsize'
69	plt figure figsize sns distplot train_df trip_duration values bins kde True plt xlabel 'trip_duration' fontsize plt show
70	plt figure figsize sns distplot np log train_df trip_duration values bins kde True plt xlabel 'trip_duration' fontsize plt show
71	from mpl_toolkits basemap import Basemap from matplotlib import cm west south east north samples train_df sample fig plt figure figsize ax fig add_subplot Basemap projection 'merc' llcrnrlat south urcrnrlat north llcrnrlon west urcrnrlon east lat_ts south resolution 'i' samples 'pickup_longitude' values samples 'pickup_latitude' values hexbin gridsize bins 'log' cmap cm
72	def fix_location data data pickup_latitude data pickup_latitude data pickup_latitude data pickup_longitude data pickup_longitude data pickup_longitude data dropoff_latitude data dropoff_latitude data dropoff_latitude data dropoff_longitude data dropoff_longitude data dropoff_longitude return data train_df fix_location train_df test_df fix_location test_df
73	plt figure figsize sns countplot "pickup_hour" data train_df plt ylabel fontsize plt xlabel 'pick up hour' fontsize plt xticks rotation 'vertical' plt show
74	import tensorflow as tf from tensorflow python data import Dataset import numpy as np import sklearn metrics as metrics
75	N_TRAINING N_VALIDATION training_examples train_data head N_TRAINING my_feature_name copy training_targets train_data head N_TRAINING my_target_name copy validation_examples train_data tail N_VALIDATION my_feature_name copy validation_targets train_data tail N_VALIDATION my_target_name copy
76	test_file_path '../input/test.csv' test_data pd read_csv test_file_path sep my_feature_name 'teacher_number_of_previously_posted_projects' test_examples test_data my_feature_name copy placeholder_label_vals for in range test_labels pd DataFrame "project_is_approved" placeholder_label_vals predict_test_input_fn lambda my_input_fn test_examples test_labels num_epochs shuffle False predictions_generator linear_classifier predict input_fn predict_test_input_fn predictions_list list predictions_generator probabilities "probabilities" for in predictions_list print
77	pneumonia_locations with open os path join '../input/stage_2_train_labels.csv' mode 'r' as infile reader csv reader infile next reader None for rows in reader filename rows location rows pneumonia rows if pneumonia location int float for in location if filename in pneumonia_locations pneumonia_locations filename append location else pneumonia_locations filename location
78	train pd read_csv '../input/train.csv' test pd read_csv '../input/test.csv' test_ids test Id
79	def split_data train households test_percentage seed None train2 train copy cv_hhs np random choice households size int len households test_percentage replace False cv_idx np isin households cv_hhs train2 cv_idx y_test cv_idx train2 cv_idx y_train cv_idx return y_train y_test
80	opt_parameters 'colsample_bytree' 'min_child_samples' 'num_leaves' 'subsample' 'reg_lambda' opt_parameters 'colsample_bytree' 'min_child_samples' 'num_leaves' 'subsample' 'reg_lambda' def evaluate_macroF1_lgb truth predictions pred_labels predictions reshape len np unique truth argmax axis f1 f1_score truth pred_labels average 'macro' return f1 True fit_params "early_stopping_rounds" "eval_metric" evaluate_macroF1_lgb "eval_set" y_train y_test 'eval_names' 'train' 'valid' 'verbose' False 'categorical_feature' 'auto' def learning_rate_power_0997 current_iter base_learning_rate min_learning_rate lr base_learning_rate np power current_iter return max lr min_learning_rate fit_params 'verbose' fit_params 'callbacks' lgb reset_parameter learning_rate learning_rate_power_0997
81	train pd read_csv '../input/train.csv' test pd read_csv '../input/test.csv' test_ids test Id
82	def split_data train sample_weight None households None test_percentage seed None train2 train copy cv_hhs np random choice households size int len households test_percentage replace False cv_idx np isin households cv_hhs train2 cv_idx y_test cv_idx train2 cv_idx y_train cv_idx if sample_weight is not None y_train_weights sample_weight cv_idx return y_train y_test y_train_weights return y_train y_test
83	opt_parameters 'max_depth' 'eta' 'silent' 'objective' 'multi:softmax' 'min_child_weight' 'num_class' 'gamma' 'colsample_bylevel' 'subsample' 'colsample_bytree' 'reg_lambda' opt_parameters 'max_depth' 'eta' 'silent' 'objective' 'multi:softmax' 'min_child_weight' 'num_class' 'gamma' 'colsample_bylevel' 'subsample' 'colsample_bytree' 'reg_lambda' def evaluate_macroF1_lgb predictions truth pred_labels predictions argmax axis truth truth get_label f1 f1_score truth pred_labels average 'macro' return f1 fit_params "early_stopping_rounds" "eval_metric" evaluate_macroF1_lgb "eval_set" y_train y_test 'verbose' False def learning_rate_power_0997 current_iter base_learning_rate min_learning_rate lr base_learning_rate np power current_iter return max lr min_learning_rate fit_params 'verbose'
84	def clean 'words' re sub data lower split for data in 'text' 'words' re sub data lower split for data in 'text' return clean
85	import torch import time import numpy as np import pandas as pd from numba import njit torch manual_seed N_DAYS N_FAMILIES MAX_OCCUPANCY MIN_OCCUPANCY INPUT_PATH '/kaggle/input/santa-workshop-tour-2019/' OUTPUT_PATH DEFAULT_DEVICE torch device 'cpu'
86	MAX_ACTION SOFT_PENALTY_PER_PERSON PENALTY_RAMP_TIME BATCH_SIZE N_BATCHES LR GRADIENT_CLIP MAX_PREFERENCE USE_ADAM True ADAM_BETA_M ADAM_BETA_V ADAM_EPSILON MOMENTUM
87	clf LGBMClassifier n_estimators learning_rate num_leaves colsample_bytree subsample max_depth reg_alpha reg_lambda min_split_gain min_child_weight silent verbose clf fit data_train y_train eval_set data_train y_train data_valid y_valid eval_metric 'auc' verbose early_stopping_rounds
88	fig axes plt subplots figsize plt ylim axes set_ylabel axes set_title results_df results_df index "RF" plot kind "bar" ax axes
89	market_train_full_df env get_training_data sample_market_df pd read_csv "../input/marketdata_sample.csv" sample_news_df pd read_csv "../input/news_sample.csv"
90	from wordcloud import WordCloud wordcloud WordCloud width height margin generate join market_train_full_df assetName fig ax plt subplots figsize ax imshow wordcloud interpolation 'bilinear' ax axis "off" ax margins plt show
91	fig axes plt subplots figsize axes set_title axes set_ylabel "volume" axes set_xlabel "records" axes plot market_train_full_df "volume"
92	from wordcloud import WordCloud import matplotlib pyplot as plt wordcloud WordCloud width height margin generate join list news_train_full_df head headline fig ax plt subplots figsize ax imshow wordcloud interpolation 'bilinear' ax axis "off" ax margins plt show
93	text join tmp_list replace wordcloud WordCloud width height margin generate text fig ax plt subplots figsize ax imshow wordcloud interpolation 'bilinear' ax axis "off" ax margins plt show
94	import os import pandas as pd import numpy as np import json import matplotlib pyplot as plt import datetime as datetime from datetime import timedelta date import seaborn as sns import matplotlib cm as CM import lightgbm as lgb from sklearn import preprocessing from sklearn metrics import mean_squared_error from sklearn model_selection import GridSearchCV train_test_split
95	list_of_devices train_data device apply json loads tolist keys for devices_iter in list_of_devices for list_element in list devices_iter keys if list_element not in keys keys append list_element
96	revenue_datetime_df train_data "revenue" "date" dropna revenue_datetime_df "revenue" revenue_datetime_df revenue astype np int64 revenue_datetime_df head
97	daily_revenue_df revenue_datetime_df groupby by "date" axis sum import matplotlib pyplot as plt fig axes plt subplots figsize axes set_title axes set_ylabel axes set_xlabel "date" axes plot daily_revenue_df "revenue"
98	fig axes plt subplots figsize traffic_source_df "keyword" value_counts head plot kind "bar" ax axes title "keywords (total)" color "orange" traffic_source_df traffic_source_df "keyword" "(not provided)" "keyword" value_counts head plot kind "bar" ax axes title color "c"
99	target_intervals_list intervals_visitors for tmp_tuple in target_intervals_list intervals_visitors append tmp_churn_df tmp_churn_df yaer tmp_tuple tmp_churn_df month tmp_tuple format len intervals_visitors
100	datetime_revenue_visits_df pd concat total_revenue_daily_df total_visitNumber_daily_df axis fig ax1 plt subplots figsize datetime_revenue_visits_df index s1 datetime_revenue_visits_df ax1 plot s1 'b-' ax1 set_xlabel 'day' ax1 set_ylabel color 'b' ax1 tick_params 'y' colors 'b' ax2 ax1 twinx s2 datetime_revenue_visits_df "revenue" ax2 plot s2 'r--' ax2 set_ylabel 'revenue' color 'r' ax2 tick_params 'y' colors 'r' fig tight_layout
101	train train_transaction merge train_identity how 'left' left_index True right_index True test test_transaction merge test_identity how 'left' left_index True right_index True print "shape of train is ....." str train shape print "shape of test is ....." str test shape y_train train copy train drop axis test copy fillna fillna
102	method "sklearn_stratified" start time time train valid split dataset_path DATASET_PATH test_size TEST_SIZE stratification method print f"Dataset split done for {time.time() - start} seconds"
103	def train2vw data features_extractor valid_rate train_output 'train' valid_output 'valid' yvalid_output 'yvalid' writer_train open train_output 'w' writer_val open valid_output 'w' writer_yval open yvalid_output 'w' for row in tqdm data iterrows total data shape miniters label row target_col features features_extractor row output_line '{0:.6} {1}\n' format label features if np random rand valid_rate writer_train write output_line else writer_val write output_line writer_yval write '%s\n' label writer_train close writer_val close writer_yval close train2vw df train_len vw_extractor
104	def get_rmse ytest_input 'ytest' pred_input 'pred' loss reader_ytest open ytest_input 'r' reader_pred open pred_input 'r' for label pred in tqdm zip reader_ytest reader_pred true_score float label pred_score float pred loss np square pred_score true_score reader_ytest close reader_pred close return np sqrt loss
105	stage1_names 'takato_original_seresnext' '../input/hemoorhagesubsfirststage/se_resnext50_32x4d_102105_hflip.csv' 'takato_original_effb1' '../input/hemoorhagesubsfirststage/efficientnet-b1_102407_hflip.csv' 'takato_original_effb2' '../input/hemoorhagesubsfirststage/efficientnet-b2_102000_hflip.csv' 'takato_original_effb3' '../input/hemoorhagesubsfirststage/efficientnet-b3_102112_hflip.csv' 'takato_original_effb4' '../input/hemoorhagesubsfirststage/efficientnet-b4_102100_hflip.csv' 'agro_b7' '../input/hemoorhagesubsfirststage/agroeffnet_stage1_test_tta8.csv' 'sneddy_b7' '../input/hemoorhagesubsfirststage/effnet-b7-e1.csv' 'resnet152' '../input/hemoorhagesubsfirststage/resnet152_stage1.csv' 'effnet_b5' '../input/hemoorhagesubsfirststage/effnetb5_stage1_test_tta8.csv' 'balanced_effb4' '../input/hemoorhagesubsfirststage/balansed_effb4_stage1.csv' 'inceptionv4' '../input/hemoorhagesubsfirststage/inceptionv4_stage1.csv' 'effnet_b0' '../input/hemoorhagesubsfirststage/effb0_stage1.csv' 'densenet121' '../input/hemoorhagesubsfirststage/densenet121_stage1.csv' stage1 pd read_csv '../input/hemorrhagehelpers/stage_1_sample_submission.csv' sort_values 'ID' stage1 'image_id' stage1 ID apply lambda join split stage1 'group_id' stage1 ID apply lambda split stage1 drop inplace True helpers_cols 'image_id' 'group_id' stage1 set_index 'ID' inplace True for name fpath in tqdm stage1_names items add_table stage1 fpath name stage1 head
106	stage1_meta_df pd read_csv '../input/hemoorrhagemetadata/stage_1_test_meta.csv' stage1_meta_df stage1_meta_df apply lambda float split stage1_meta_df stage1_meta_df sort_values reset_index drop True stage1_meta_df stage1_meta_df stage1_meta_df stage1_meta_df groupby Axial cumcount stage1_meta_df stage1_meta_df SOPInstanceUID shift stage1_meta_df loc stage1_meta_df InstancePosition None stage1_meta_df stage1_meta_df InstancePosition shift stage1_meta_df stage1_meta_df SOPInstanceUID shift stage1_meta_df loc stage1_meta_df NextPosition None stage1_meta_df drop inplace True stage1_meta_df columns 'image_id' list stage1_meta_df columns stage1_meta_df iloc
107	stage2_names 'takato_original_seresnext' '../input/hemorrhagesubssecondstage/se_resnext50_32x4d_102105_hflip.csv' 'takato_original_effb1' '../input/hemorrhagesubssecondstage/efficientnet-b1_102407_hflip.csv' 'takato_original_effb2' '../input/hemorrhagesubssecondstage/efficientnet-b2_102000_hflip.csv' 'takato_original_effb3' '../input/hemorrhagesubssecondstage/efficientnet-b3_102112_hflip.csv' 'takato_original_effb4' '../input/hemorrhagesubssecondstage/efficientnet-b4_102100_hflip.csv' 'agro_b7' '../input/hemorrhagesubssecondstage/agroeffnet_stage2_test_tta10.csv' 'sneddy_b7' '../input/hemorrhagesubssecondstage/effnet-b7-new_stage2_test_tta10.csv' 'resnet152' '../input/hemorrhagesubssecondstage/resnet152_stage2.csv' 'effnet_b5' '../input/hemorrhagesubssecondstage/effnetb5_stage2_test_tta8.csv' 'balanced_effb4' '../input/hemorrhagesubssecondstage/balansed_effb4_stage2.csv' 'inceptionv4' '../input/hemorrhagesubssecondstage/inceptionv4_stage2.csv' 'effnet_b0' '../input/hemorrhagesubssecondstage/effb0_stage2.csv' 'densenet121' '../input/hemorrhagesubssecondstage/densenet121_stage2.csv' stage2 pd read_csv '../input/hemorrhagehelpers/stage_2_sample_submission.csv' sort_values 'ID' stage2 'image_id' stage2 ID apply lambda join split stage2 'group_id' stage2 ID apply lambda split stage2 drop inplace True helpers_cols 'image_id' 'group_id' stage2 set_index 'ID' inplace True for name fpath in tqdm stage2_names items add_table stage2 fpath name stage2 head
108	stage2_meta_df pd read_csv '../input/hemoorrhagemetadata/stage_2_test_meta.csv' stage2_meta_df stage2_meta_df apply lambda float split stage2_meta_df stage2_meta_df sort_values reset_index drop True stage2_meta_df stage2_meta_df stage2_meta_df stage2_meta_df groupby Axial cumcount stage2_meta_df stage2_meta_df SOPInstanceUID shift stage2_meta_df loc stage2_meta_df InstancePosition None stage2_meta_df stage2_meta_df InstancePosition shift stage2_meta_df stage2_meta_df SOPInstanceUID shift stage2_meta_df loc stage2_meta_df NextPosition None stage2_meta_df drop inplace True stage2_meta_df columns 'image_id' list stage2_meta_df columns stage2_meta_df iloc
109	postproc_cols col for col in stage2 columns if col not in helpers_cols for col in postproc_cols print format col add_custom_postproc stage2 col stage2_meta_df optimal_triplets
110	import gc import os import librosa import numpy as np import pandas as pd from glob import glob from pathlib import Path from librosa import display import matplotlib pyplot as plt from scipy io wavfile import read from IPython display import HTML Audio display_html from IPython display import display as display_ipython pd set_option 'display.max_colwidth'
111	train_path Path data_folder 'train' train_df pd read_csv Path data_folder 'train.csv' train_df 'target' train_df 'label' apply lambda label_dict get assert set os path relpath path data_folder '/train' for path in glob f'{train_path}/train/*/*.wav' set train_df wav_path values train_df 'wav_path' train_df 'wav_path' apply lambda str Path data_folder Path 'train'
112	PROJECT_ID 'geultto' from google cloud import bigquery client bigquery Client project PROJECT_ID location "US" dataset client create_dataset 'bqml_example' exists_ok True from google cloud bigquery import magics from kaggle gcp import KaggleKernelCredentials magics context credentials KaggleKernelCredentials magics context project PROJECT_ID
113	table client get_table "kaggle-competition-datasets.geotab_intersection_congestion.train" client list_rows table max_results to_dataframe
114	SELECT FROM ML TRAINING_INFO MODEL bqml_example distance_p20 ORDER BY iteration
115	try tpu tf distribute cluster_resolver TPUClusterResolver print tpu master except ValueError tpu None if tpu tf config experimental_connect_to_cluster tpu tf tpu experimental initialize_tpu_system tpu strategy tf distribute experimental TPUStrategy tpu else strategy tf distribute get_strategy print "REPLICAS: " strategy num_replicas_in_sync
116	with strategy scope model tf keras Sequential irv include_top False weights 'imagenet' input_shape Dense activation 'sigmoid' model compile optimizer 'adam' loss 'binary_crossentropy' metrics 'accuracy' model summary
117	import numpy as np import pandas as pd import matplotlib pyplot as plt import seaborn as sns import glob import cv2 import os print os listdir "../input"
118	plt rcParams "axes.grid" True axarr plt subplots figsize curr_row for in range example cv2 imread test_list example example col axarr col curr_row imshow example if col curr_row
119	plt rcParams "axes.grid" True axarr plt subplots figsize curr_row for in range example cv2 imread index_list example example col axarr col curr_row imshow example if col curr_row
120	plt rcParams "axes.grid" True axarr plt subplots figsize curr_row for in range example cv2 imread train_list example example col axarr col curr_row imshow example if col curr_row
121	temp pd DataFrame train_data landmark_id value_counts head temp reset_index inplace True temp columns 'landmark_id' 'count' temp
122	temp pd DataFrame train_data landmark_id value_counts tail temp reset_index inplace True temp columns 'landmark_id' 'count' temp
123	import numpy as np import pandas as pd import os for dirname filenames in os walk '/kaggle/input' for filename in filenames print os path join dirname filename
124	from datetime import datetime from os import scandir def convert_date timestamp datetime utcfromtimestamp timestamp formated_date strftime return formated_date def get_files dir_entries scandir 'my_directory/' for entry in dir_entries if entry is_file info entry stat print f'{entry.name}\t Last Modified: {convert_date(info.st_mtime)}'
125	import openslide import skimage io import random import seaborn as sns import cv2 import pandas as pd import numpy as np import matplotlib import matplotlib pyplot as plt import PIL import os
126	from nilearn import datasets rest_dataset datasets fetch_development_fmri n_subjects func_filenames rest_dataset func confounds rest_dataset confounds
127	import numpy as np import pandas as pd import random import seaborn as sns import matplotlib pyplot as plt import gc import seaborn as sns sns set style 'whitegrid' color_codes True import scipy stats as st import statsmodels formula api as smf from sklearn ensemble import RandomForestRegressor from sklearn cross_validation import train_test_split import xgboost as xgb import operator
128	train_items pd merge df_train item how 'inner' train_items1 pd merge df_train item how 'inner' train_items2 pd merge df_train item how 'inner'
129	fig axis1 plt subplots sharex True figsize ax1 oil plot legend True ax axis1 marker 'o' title
130	average_sales train groupby 'date' "unit_sales" mean average_promo train groupby 'date' "onpromotion" mean fig axis1 axis2 plt subplots figsize ax1 average_sales plot legend True ax axis1 marker 'o' title ax2 average_promo plot legend True ax axis2 marker 'o' rot colormap "summer" title
131	import os import numpy as np import pandas as pd import warnings from bayes_opt import BayesianOptimization from skopt import BayesSearchCV import matplotlib pyplot as plt import seaborn as sns import lightgbm as lgb import xgboost as xgb from sklearn model_selection import train_test_split StratifiedKFold cross_val_score import time import sys from sklearn metrics import roc_auc_score roc_curve import shap warnings simplefilter action 'ignore' category FutureWarning
132	opt_params "num_leaves" int round opt_params "num_leaves" opt_params 'max_depth' int round opt_params 'max_depth' opt_params 'min_data_in_leaf' int round opt_params 'min_data_in_leaf' opt_params 'max_bin' int round opt_params 'max_bin' opt_params 'objective' 'binary' opt_params 'metric' 'auc' opt_params 'is_unbalance' True opt_params 'boost_from_average' False opt_params opt_params opt_params
133	graph lgb create_tree_digraph clf tree_index name graph graph_attr update size graph
134	pubs_map folium Map location zoom_start data for in np array bars HeatMap data radius add_to pubs_map pubs_map
135	t4_params 'boosting_type' 'gbdt' 'objective' 'multiclass' 'nthread' 'silent' True 'num_leaves' 'learning_rate' 'max_depth' 'max_bin' 'subsample_for_bin' 'subsample' 'subsample_freq' 'colsample_bytree' 'reg_alpha' 'reg_lambda' 'min_split_gain' 'min_child_weight' 'min_child_samples' 'scale_pos_weight' t4 lgbm sklearn LGBMClassifier n_estimators seed t4_params
136	def cross_validate_lgbm filename_str preprocess_func preprocess_1 lgbm_params t4_params copy lgbm_params 'num_class' train_X train_y test_df preprocess_func train test dset lgbm Dataset train_X train_y silent True cv_results lgbm cv lgbm_params dset num_boost_round nfold stratified False shuffle True metrics 'multi_logloss' early_stopping_rounds verbose_eval show_stdv True seed json dump cv_results open filename_str 'w' print filename_str print 'best n_estimators:' len cv_results 'multi_logloss-mean' print 'best cv score:' cv_results 'multi_logloss-mean' cross_validate_lgbm 'lgbm_1.json'
137	plt figure figsize plt title tmp dataset groupby agg 'count' sns countplot tmp
138	sub pd DataFrame sub test_df sub lgb_sub_preds sub sub sub to_csv 'lgb_submission.csv' index False
139	import numpy as np import pandas as pd import os import matplotlib pyplot as plt from IPython core interactiveshell import InteractiveShell InteractiveShell ast_node_interactivity 'all'
140	df pd read_csv '../input/train_labels.csv' print df shape df head
141	df df 'id' 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2' df df 'id' '9369c7278ec8bcc6c880d99194de09fc2bd4efbe' df head
142	df_train 'label' df_train df_val train_test_split df_train test_size random_state stratify
143	base_dir 'base_dir' os mkdir base_dir train_dir os path join base_dir 'train_dir' os mkdir train_dir val_dir os path join base_dir 'val_dir' os mkdir val_dir no_tumor os path join train_dir os mkdir no_tumor has_tumor os path join train_dir os mkdir has_tumor no_tumor os path join val_dir os mkdir no_tumor has_tumor os path join val_dir os mkdir has_tumor print os listdir 'base_dir/train_dir' print os listdir 'base_dir/val_dir'
144	df set_index 'id' inplace True train_list list df_train 'id' val_list list df_val 'id' for image in train_list file_name image '.tif' target df loc image 'label' if target label elif target label src os path join '../input/train' file_name dest os path join train_dir label file_name shutil copyfile src dest for image in val_list file_name image '.tif' target df loc image 'label' if target label elif target label src os path join '../input/train' file_name dest os path join val_dir label file_name shutil copyfile src dest
145	model Net build width height depth classes from keras optimizers import SGD Adam Adagrad model compile optimizer Adam lr loss 'binary_crossentropy' metrics 'accuracy'
146	from keras callbacks import ModelCheckpoint ReduceLROnPlateau filepath "checkpoint.h5" checkpoint ModelCheckpoint filepath monitor 'val_acc' verbose save_best_only True mode 'max' reduce_lr ReduceLROnPlateau monitor 'val_acc' factor patience verbose mode 'max' min_lr callbacks_list checkpoint reduce_lr history model fit_generator train_gen steps_per_epoch train_steps validation_data val_gen validation_steps val_steps epochs verbose callbacks callbacks_list
147	plt plot history history 'acc' plt plot history history 'val_acc' plt title plt ylabel plt xlabel plt legend loc 'best' plt show plt plot history history 'loss' plt plot history history 'val_loss' plt title plt ylabel plt xlabel plt legend loc 'best' plt show
148	model load_weights 'checkpoint.h5' val_loss val_acc model evaluate_generator test_gen steps len df_val print 'val_loss:' val_loss print 'val_acc:' val_acc
149	test_filenames test_gen filenames df_preds 'file_names' test_filenames def extract_id split split extracted_id return extracted_id df_preds 'id' df_preds 'file_names' apply extract_id df_preds head
150	submission pd DataFrame 'id' image_id 'label' y_pred set_index 'id' submission to_csv 'submission.csv' columns 'label'
151	from sklearn ensemble import ExtraTreesClassifier from xgboost import XGBClassifier from xgboost import plot_importance from matplotlib import pyplot numerical_features train 'bathrooms' 'bedrooms' 'price' 'price_room' 'latitude' 'longitude' 'nb_images' 'nb_features' 'nb_description' 'description_len' 'sentiment' 'b_counts' 'm_counts' 'b_count_log' 'm_count_log' model ExtraTreesClassifier model fit numerical_features train 'interest_level' print model feature_importances_ plt subplots figsize plt title fontsize plt ylabel fontsize feature_names numerical_features columns plt xticks range numerical_features shape feature_names fontsize pyplot bar range len model feature_importances_ model feature_importances_ pyplot show
152	def submit predictions submit pd read_csv '../input/sample_submission.csv' submit "target" predictions submit to_csv "submission.csv" index False def fallback_auc y_true y_pred try return metrics roc_auc_score y_true y_pred except return def auc y_true y_pred return tf py_function fallback_auc y_true y_pred tf double
153	df_tr target ids df_tr id values train df_tr drop 'id' 'target' axis test_ids df_test id values test df_test train columns oof_preds np zeros len train test_preds np zeros len test scl preprocessing StandardScaler scl fit pd concat train test train scl transform train test scl transform test
154	import numpy as np import pandas as pd import os import time import datetime import gc from sklearn preprocessing import LabelEncoder from sklearn model_selection import GroupKFold from sklearn metrics import mean_absolute_error import matplotlib pyplot as plt import seaborn as sns from tqdm import tqdm_notebook as tqdm from catboost import CatBoostRegressor Pool import warnings warnings filterwarnings "ignore"
155	train pd read_csv '../input/train.csv' structures pd read_csv '../input/structures.csv' print format train shape train shape print format structures shape structures shape
156	def permutation_importance model y_val metric threshold minimize True verbose True results y_pred model predict results 'base_score' metric y_val y_pred if verbose print f'Base score {results["base_score"]:.5}' for col in tqdm columns freezed_col col copy col np random permutation col preds model predict results col metric y_val preds col freezed_col if verbose print f'column: {col} - {results[col]:.5}' if minimize bad_features for in results if results results 'base_score' threshold else bad_features for in results if results results 'base_score' threshold bad_features remove 'base_score' return results bad_features
157	def catboost_fit model y_train y_val train_pool Pool y_train val_pool Pool y_val model fit train_pool eval_set val_pool return model model CatBoostRegressor iterations max_depth objective 'MAE' task_type 'GPU' verbose False model catboost_fit model tr_X tr_y val_X val_y
158	forest replace inplace True forest forest rename columns
159	temp forest forest fig px histogram temp color height width title barmode "group" fig show temp forest forest fig px histogram temp color height width title barmode "group" fig show temp forest forest fig px histogram temp color height width title barmode "group" fig show temp forest forest fig px histogram temp color height width title barmode "group" fig show
160	temp forest groupby as_index False median fig px bar temp color barmode 'group' height width fig show fig px treemap temp path values height width fig show temp style background_gradient cmap
161	fig px histogram forest color marginal 'rug' title height width fig show
162	fig px histogram forest color marginal 'rug' title height width fig show
163	fig px histogram forest color marginal 'rug' title height width fig show
164	temp forest groupby as_index False median fig px bar temp sort_values by ascending False color orientation 'h' height width fig show
165	fig px histogram forest color marginal 'rug' title height width fig show
166	fig px histogram forest color marginal 'box' title height width fig show
167	import numpy as np pandas as pd from sklearn model_selection import StratifiedKFold from sklearn metrics import roc_auc_score from sklearn preprocessing import StandardScaler from sklearn feature_selection import VarianceThreshold from sklearn discriminant_analysis import QuadraticDiscriminantAnalysis from tqdm import tqdm_notebook import warnings warnings filterwarnings 'ignore'
168	batch_size lrG lrD beta1 epochs real_label fake_label nz device torch device "cuda" if torch cuda is_available else "cpu"
169	transform1 transforms Compose transforms Resize transforms CenterCrop random_transforms transforms RandomRotation degrees transform2 transforms Compose transforms RandomHorizontalFlip transforms RandomApply random_transforms transforms ToTensor transforms Normalize train_dataset DogDataset img_dir '../input/all-dogs/all-dogs/' annotations_dir transform1 transform1 transform2 transform2 train_loader DataLoader dataset train_dataset batch_size batch_size shuffle True num_workers
170	next iter train_loader fig plt figure figsize for ii img in enumerate ax fig add_subplot ii xticks yticks img img numpy transpose plt imshow img
171	if not os path exists '../output_images' os mkdir '../output_images' im_batch_size n_images for i_batch in range n_images im_batch_size gen_z torch randn im_batch_size nz device device gen_images netG gen_z images gen_images to "cpu" clone detach images images numpy transpose for i_image in range gen_images size save_image gen_images i_image os path join '../output_images' f'image_{i_batch+i_image:05d}.png' import shutil shutil make_archive 'images' 'zip' '../output_images'
172	MAGIC_N train_subset train train 'wheezy-copper-turtle-magic' MAGIC_N test_subset test test 'wheezy-copper-turtle-magic' MAGIC_N concated pd concat train_subset test_subset train_subset std cols idx for idx in index if idx concated concated cols 'target'
173	example openslide OpenSlide os path join BASE_FOLDER "train_images" '005e66f06bce9c2e49142536caf2f6ee.tiff' patch example read_region display patch example close
174	pen_marked_images 'fd6fe1a3985b17d067f2cb4d5bc1e6e1' 'ebb6a080d72e09f6481721ef9f88c472' 'ebb6d5ca45942536f78beb451ee43cc4' 'ea9d52d65500acc9b9d89eb6b82cdcdf' 'e726a8eac36c3d91c3c4f9edba8ba713' 'e90abe191f61b6fed6d6781c8305fe4b' 'fd0bb45eba479a7f7d953f41d574bf9f' 'ff10f937c3d52eff6ad4dd733f2bc3ac' 'feee2e895355a921f2b75b54debad328' overlay_mask_on_slide pen_marked_images
175	import numpy as np import pandas as pd import os import matplotlib pyplot as plt import torch from torch import nn optim import torch nn functional as from torchvision import datasets transforms from torchvision utils import save_image from torch utils data import Dataset DataLoader from torch autograd import Variable from PIL import Image from tqdm import tqdm_notebook as tqdm
176	next iter train_loader fig plt figure figsize for ii img in enumerate ax fig add_subplot ii xticks yticks img img numpy transpose plt imshow img
177	reconstructed mu model to device reconstructed reconstructed view detach cpu numpy transpose fig plt figure figsize for ii img in enumerate reconstructed ax fig add_subplot ii xticks yticks plt imshow img
178	first_dog_idx second_dog_idx dz mu second_dog_idx mu first_dog_idx walk Variable torch randn latent_dim to device walk mu first_dog_idx for in range walk walk dz walk model decoder walk detach cpu numpy transpose fig plt figure figsize for ii img in enumerate walk ax fig add_subplot ii xticks yticks plt imshow img
179	samples Variable torch randn latent_dim to device samples model decoder samples detach cpu numpy transpose fig plt figure figsize for ii img in enumerate samples ax fig add_subplot ii xticks yticks plt imshow img
180	def seed_everything seed random seed seed os environ 'PYTHONHASHSEED' str seed np random seed seed torch manual_seed seed torch cuda manual_seed seed torch backends cudnn deterministic True seed_everything
181	df_train pd read_csv "../input/train.csv" df_test pd read_csv "../input/test.csv" df pd concat df_train df_test sort True
182	np save "x_train" x_train np save "x_test" x_test np save "y_train" y_train np save "features" features np save "test_features" test_features np save "word_index.npy" word_index
183	x_train np load "x_train.npy" x_test np load "x_test.npy" y_train np load "y_train.npy" features np load "features.npy" test_features np load "test_features.npy" word_index np load "word_index.npy" item
184	seed_everything glove_embeddings load_glove word_index paragram_embeddings load_para word_index embedding_matrix np mean glove_embeddings paragram_embeddings axis del glove_embeddings paragram_embeddings gc collect np shape embedding_matrix
185	class MyDataset Dataset def __init__ self dataset self dataset dataset def __getitem__ self index data target self dataset index return data target index def __len__ self return len self dataset
186	def bestThresshold y_train train_preds tmp delta for tmp in tqdm np arange tmp f1_score y_train np array train_preds tmp if tmp tmp delta tmp tmp tmp print format delta tmp return delta delta bestThresshold y_train train_preds
187	import numpy as np import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import os merchants pd read_csv "../input/merchants.csv"
188	merchants 'category_2' merchants 'category_2' fillna astype int merchants loc merchants 'city_id' 'city_id' merchants loc merchants 'state_id' 'state_id'
189	def rating if np isfinite and if elif and elif elif and else else return
190	merchants 'most_recent_sales_range' merchants 'most_recent_sales_range' map 'A' 'B' 'C' 'D' 'E' merchants 'most_recent_purchases_range' merchants 'most_recent_purchases_range' map 'A' 'B' 'C' 'D' 'E'
191	from sklearn preprocessing import LabelEncoder cat_features encoder LabelEncoder encoded data cat_features apply encoder fit_transform
192	import matplotlib pyplot as plt import seaborn as seabornInstance from sklearn model_selection import train_test_split from sklearn linear_model import LinearRegression from sklearn import metrics
193	test pd read_csv '../input/nomad2018-predict-transparent-conductors/test.csv' test_id test id train pd read_csv '../input/nomad2018-predict-transparent-conductors/train.csv'
194	def get_prop_list path_to_element_data return for in os listdir path_to_element_data path_to_element_data '../input/elemental-properties/' properties get_prop_list path_to_element_data print sorted properties
195	for col in 'a' 'b' 'c' 'vol' 'atomic_density' for in all_data sg unique sns distplot all_data all_data 'sg' col plt title col plt show
196	for col in 'E' sns distplot train col plt title col plt show
197	def rmsle return np sqrt np square np log np log mean
198	import keras import tensorflow as tf import keras backend as from keras regularizers import l2 from keras optimizers import Adam from keras models import Model from keras models import Sequential from keras layers import Dense Dropout BatchNormalization Input
199	def assess_early_stop_light y_list params final best_iter for idx in enumerate y_list kfold KFold n_splits shuffle True out for train_index test_index in kfold split train_index test_index y_train y_test train_index test_index lgb_train lgb Dataset y_train lgb_valid lgb Dataset y_test reference lgb_train model lgb train params idx lgb_train num_boost_round valid_sets lgb_valid early_stopping_rounds verbose_eval best_iter idx append model best_iteration model predict num_iteration model best_iteration rmsle np expm1 np expm1 y_test print 'RMSLE: {}' format out append final append np array out mean print 'y1 best iteration: {}' format np mean best_iter print 'y2 best iteration: {}' format np mean best_iter return np array final mean np array final std
200	from sklearn model_selection import train_test_split catboost_cv CatBoostRegressor iterations learning_rate depth loss_function 'RMSE' eval_metric 'RMSE' random_seed od_type od_wait def assess_cv_catboost y_list final best_iter for idx in enumerate y_list kfold KFold n_splits shuffle True out for train_index test_index in kfold split train_index test_index y_iter y_test train_index test_index y_train y_valid train_test_split y_iter test_size model catboost_cv model fit y_train eval_set y_valid use_best_model True verbose False model predict rmsle np expm1 np expm1 y_test print 'RMSLE: {}' format out append final append np array out mean return np array final mean np array final std
201	import os def read_all_dcm PatientID Path "/kaggle/input/osic-pulmonary-fibrosis-progression/test/" PatientID path dirs files next os walk Path file_count len files PathList file_count for in range len PathList try PathList Path str ".dcm" plt imshow pydicom dcmread PathList pixel_array cmap plt cm bone plt show print PathList except pass return
202	dict_columns 'belongs_to_collection' 'genres' 'production_companies' 'production_countries' 'spoken_languages' 'cast' 'crew' def text_to_dict df for column in dict_columns df column df column apply lambda if pd isna else ast literal_eval return df train text_to_dict train test text_to_dict test
203	fig axes plt subplots len sample_maps1 figsize for ii in enumerate sample_maps1 keys plotting plot_glass_brain sample_maps1 ii title ii axes axes plot_abs False plotting plot_glass_brain sample_maps2 ii title ii axes axes plot_abs False axes set_title axes set_title
204	icn_mat_idx icn_number to_dict 'list' icn_mat_idx for in zip icn_mat_idx values icn_mat_idx keys name_matrix for fnco in fnc10_cols name_matrix fnco np int icn_mat_idx np int split for in fnco split con_matrix1 np zeros con_matrix2 np zeros for in fnc10_cols r_ c_ name_matrix con_matrix1 c_ r_ fnc_10 iloc con_matrix2 c_ r_ fnc_10 iloc con_matrix1 con_matrix1 con_matrix2 con_matrix2 col_halves np array jj split for jj in name_matrix keys idx np unique col_halves return_index True col_labels col_halves np sort idx
205	from sklearn model_selection import train_test_split train_idx train_test_split scores index train_size random_state stratify scores stratify
206	from nilearn import input_data basc197_masker input_data NiftiLabelsMasker basc_197 mask_img brain_mask def load_matlab participant_id masker path mat np array h5py File f'{path}{participant_id}.mat' mode 'r' get mat masker fit_transform nb mat transpose affine masker mask_img affine return mat flatten
207	components2 pd DataFrame components2 index scores_stat index pca2_corr for kk in range pca2_corr append scores_stat 'age' 'domain1_var1' 'domain1_var2' 'domain2_var1' 'domain2_var2' corrwith components2 loc kk pca2_scorr pd concat pca2_corr axis pca2_scorr
208	inds list train groupby groups values xs ys cs for in range n_train ind inds play train loc ind extract_feature play xs append ys append cs append xs ys cs np vstack xs np hstack ys np array cs astype np int ys np maximum ys
209	model sm PHReg ys xs cs result model fit baseline_cum_hazard_func result baseline_cumulative_hazard_function pred_index np arange
210	from matplotlib import colors fig ax plt subplots figsize n_bins bins patches plt hist df_unique n_bins alpha fracs max norm colors Normalize fracs min fracs max for thisfrac thispatch in zip fracs patches color plt cm viridis norm thisfrac thispatch set_facecolor color
211	fig ax plt subplots figsize n_bins bins patches plt hist df_train 'FVC' n_bins alpha fracs max norm colors Normalize fracs min fracs max for thisfrac thispatch in zip fracs patches color plt cm inferno norm thisfrac thispatch set_facecolor color print df_train 'FVC' min print df_train 'FVC' max print df_train 'FVC' mean print df_train 'FVC' median
212	def image_to_hu image_path image_id dicom pydicom read_file image_path 'ID_' image_id '.dcm' image dicom pixel_array astype np float64 intercept dicom RescaleIntercept slope dicom RescaleSlope if slope image slope image astype np float64 image image astype np float64 image np float64 intercept image image return image dicom
213	def image_pad image new_height new_width height width image shape im_bg np zeros new_height new_width pad_left int new_width width pad_top int new_height height im_bg pad_top pad_top height pad_left pad_left width image return im_bg
214	import numpy as np import pandas as pd import matplotlib pyplot as plt import seaborn as sns from itertools import cycle color_cycle cycle plt rcParams 'axes.prop_cycle' by_key 'color' import os for dirname filenames in os walk '/kaggle/input' for filename in filenames print os path join dirname filename
215	def image_ids_in root_dir is_train_data False ids for id in os listdir root_dir if id in TRAIN_ERROR_IDS print id else ids append id return ids TRAIN_IMAGE_IDS image_ids_in TRAIN_DIR is_train_data True TEST_IMAGE_IDS image_ids_in TEST_DIR print TRAIN_IMAGE_IDS TEST_IMAGE_IDS
216	import pandas as pd from sklearn feature_extraction text import CountVectorizer from nltk corpus import stopwords from sklearn import preprocessing from sklearn naive_bayes import MultinomialNB
217	train pd read_csv '../input/train.csv' test pd read_csv '../input/test.csv' sample pd read_csv '../input/sample_submission.csv' train head
218	xtrain_ctv_all ctv fit_transform train text values xtest_ctv_all ctv transform test text values clf MultinomialNB alpha clf fit xtrain_ctv_all
219	train_text train_data 'question_text' test_text test_data 'question_text' train_target train_data 'target' all_text train_text append test_text
220	import os gc import datetime import numpy as np import pandas as pd import category_encoders from sklearn impute import SimpleImputer from sklearn metrics import mean_squared_error from sklearn model_selection import KFold from sklearn preprocessing import LabelEncoder from sklearn linear_model import Lasso from sklearn linear_model import Ridge from lightgbm import LGBMRegressor from mlxtend regressor import StackingRegressor from pandas api types import is_categorical_dtype from pandas api types import is_datetime64_any_dtype as is_datetime
221	predictions for model in models predictions np expm1 model predict np array test len models del model gc collect del test models gc collect
222	fig plt figure ax fig add_subplot plot_offset plt xlim plot_offset plot_offset plt ylim plot_offset plot_offset ax scatter pc points pc points pc points crds sample_to_explore center ax scatter crds crds 'red' sample_to_explore wlh angles_to_rotate sample_to_explore orientation yaw_pitch_roll rect_angle math degrees angles_to_rotate mpl_rotate mpl transforms rotate_deg_around crds crds rect_angle ax transData rect patches Rectangle crds crds fill False rect set_transform mpl_rotate ax add_patch rect
223	noise train train batch signal values mean_predict train train batch open_channels values fs fig ax plt subplots nrows ncols fig subplots_adjust hspace fft np fft fft noise psd np abs fft fftfreq np fft fftfreq len psd fs abs fftfreq ax grid ax plot fftfreq np log10 psd linewidth ax set_xlabel ax set_ylabel plt show
224	cs1 combinatorial_synthesis train train group flip False means mean_predict noise_factor for sig ch in cs1 train append_dataset train sig ch plt plot test test group signal values plt title plt show plt plot train train group signal values plt title plt show distplot train train group signal values label distplot test test group signal values label plt legend plt show
225	noise train train batch signal values mean_predict train train batch open_channels values fs fig ax plt subplots nrows ncols fig subplots_adjust hspace fft np fft fft noise psd np abs fft fftfreq np fft fftfreq len psd fs abs fftfreq ax grid ax plot fftfreq np log10 psd linewidth ax set_xlabel ax set_ylabel plt show
226	train signal plot plt title plt show test signal plot plt title plt show train to_csv "train_synthetic.csv" index False float_format '%.4f' test to_csv "test_synthetic.csv" index False float_format '%.4f'
227	def load_data_kfold train pd read_json '../input/train.json' train inc_angle train inc_angle replace 'na' x_band1 np array np array band astype np float32 reshape for band in train "band_1" x_band2 np array np array band astype np float32 reshape for band in train "band_2" x_band3 x_band1 x_band2 np concatenate x_band1 np newaxis x_band2 np newaxis x_band3 np newaxis axis y_train np array train "is_iceberg" folds list StratifiedKFold n_splits shuffle True random_state split y_train return folds y_train folds y_train load_data_kfold
228	def get_model Input model BatchNormalization axis model filters kernel_size strides padding 'same' activation 'relu' model model model model BatchNormalization axis model model filters kernel_size strides padding 'same' activation 'relu' model model model model BatchNormalization axis model model filters kernel_size strides padding 'same' activation 'relu' model model model model BatchNormalization axis model model filters kernel_size strides padding 'same' activation 'relu' model model model model Dense activation 'sigmoid' model model Model input output model opt_adam keras optimizers Adam lr beta_1 beta_2 epsilon 1e-08 decay model compile opt_adam loss 'binary_crossentropy' metrics 'accuracy' return model
229	batch_size gen ImageDataGenerator horizontal_flip True vertical_flip True width_shift_range height_shift_range zoom_range rotation_range
230	for train_idx val_idx in enumerate folds print train_idx y_train_cv y_train train_idx val_idx y_valid_cv y_train val_idx name_weights "final_model_fold" str "_weights.h5" callbacks get_callbacks name_weights name_weights patience_lr generator gen flow y_train_cv batch_size batch_size model get_model model fit_generator generator steps_per_epoch len batch_size epochs shuffle True verbose validation_data y_valid_cv callbacks callbacks print model evaluate y_valid_cv
231	from sklearn model_selection import train_test_split y_train y_validation train_test_split test_size random_state print format type print 'it has shape {}' format shape print 'y_train is a {} object' format type y_train print 'it has {} elements' format len y_train
232	import warnings warnings filterwarnings "ignore" import numpy as np import pandas as pd from datetime import datetime import matplotlib pyplot as plt import seaborn as sns from scipy import stats import itertools from sklearn import model_selection from sklearn ensemble import RandomForestRegressor from sklearn import metrics
233	df_train pd read_csv "../input/train.csv" sep parse_dates date_parser str_to_date low_memory False df_store pd read_csv "../input/store.csv" low_memory False
234	df_test pd read_csv "../input/test.csv" sep parse_dates date_parser str_to_date low_memory False print format str df_test shape str df_test shape
235	rfr_val RandomForestRegressor n_estimators criterion 'mse' max_depth min_samples_split min_samples_leaf min_weight_fraction_leaf max_features 'auto' max_leaf_nodes None min_impurity_decrease min_impurity_split None bootstrap True oob_score False n_jobs random_state verbose warm_start False model_RF_test rfr_val fit y_train
236	import mxnet as mx from mxnet import gluon import numpy as np import pandas as pd import matplotlib pyplot as plt import json import os from tqdm autonotebook import tqdm from pathlib import Path
237	from gluonts model deepar import DeepAREstimator from gluonts distribution neg_binomial import NegativeBinomialOutput from gluonts trainer import Trainer estimator DeepAREstimator prediction_length prediction_length freq "D" distr_output NegativeBinomialOutput use_feat_dynamic_real True use_feat_static_cat True cardinality stat_cat_cardinalities trainer Trainer learning_rate 1e-3 epochs num_batches_per_epoch batch_size predictor estimator train train_ds
238	if submission True forecasts_acc_sub np zeros len forecasts single_prediction_length forecasts_acc_sub len forecasts forecasts_acc single_prediction_length forecasts_acc_sub len forecasts forecasts_acc single_prediction_length
239	plot_log_path "./plots/" directory os path dirname plot_log_path if not os path exists directory os makedirs directory def plot_prob_forecasts ts_entry forecast_entry path sample_id inline True plot_length prediction_intervals legend "observations" "median prediction" f"{k}% prediction interval" for in prediction_intervals ax plt subplots figsize ts_entry plot_length plot ax ax forecast_entry plot prediction_intervals prediction_intervals color 'g' ax axvline ts_entry index prediction_length color 'r' plt legend legend loc "upper left" if inline plt show plt clf else plt savefig '{}forecast_{}.pdf' format path sample_id plt close print for in tqdm range ts_entry tss forecast_entry forecasts plot_prob_forecasts ts_entry forecast_entry plot_log_path
240	import mxnet as mx from mxnet import gluon import numpy as np import pandas as pd import matplotlib pyplot as plt import json import os from tqdm autonotebook import tqdm from pathlib import Path
241	from gluonts model deepar import DeepAREstimator from gluonts distribution neg_binomial import NegativeBinomialOutput from gluonts trainer import Trainer estimator DeepAREstimator prediction_length prediction_length freq "D" distr_output NegativeBinomialOutput use_feat_dynamic_real True use_feat_static_cat True cardinality stat_cat_cardinalities trainer Trainer learning_rate 1e-3 epochs num_batches_per_epoch batch_size predictor estimator train train_ds
242	plot_log_path "./plots/" directory os path dirname plot_log_path if not os path exists directory os makedirs directory def plot_prob_forecasts ts_entry forecast_entry path sample_id inline True plot_length prediction_intervals legend "observations" "median prediction" f"{k}% prediction interval" for in prediction_intervals ax plt subplots figsize ts_entry plot_length plot ax ax forecast_entry plot prediction_intervals prediction_intervals color 'g' ax axvline ts_entry index prediction_length color 'r' plt legend legend loc "upper left" if inline plt show plt clf else plt savefig '{}forecast_{}.pdf' format path sample_id plt close print for in tqdm range ts_entry tss forecast_entry forecasts plot_prob_forecasts ts_entry forecast_entry plot_log_path
243	print format im shape from skimage color import rgb2gray im_gray rgb2gray im print format im_gray shape
244	from scipy import ndimage labels nlabels ndimage label mask label_arrays for label_num in range nlabels label_mask np where labels label_num label_arrays append label_mask print format nlabels
245	def rle_encoding dots np where flatten run_lengths prev for in dots if prev run_lengths extend run_lengths prev return join str for in run_lengths print format rle_encoding label_mask
246	import pandas as pd def analyze_image im_path im_id im_path parts im imageio imread str im_path im_gray rgb2gray im thresh_val threshold_otsu im_gray mask np where im_gray thresh_val if np sum mask np sum mask mask np where mask labels nlabels ndimage label mask labels nlabels ndimage label mask im_df pd DataFrame for label_num in range nlabels label_mask np where labels label_num if label_mask flatten sum rle rle_encoding label_mask pd Series im_id rle im_df im_df append ignore_index True return im_df def analyze_list_of_images im_path_list all_df pd DataFrame for im_path in im_path_list im_df analyze_image im_path all_df all_df append im_df ignore_index True return all_df
247	import numpy as np import pandas as pd import matplotlib pyplot as plt df_train pd read_csv '../input/train.csv' subset df_train loc df_train 'crew' df_train 'experiment' 'CA' subset sort_values by 'time' plt plot subset 'r'
248	subset df_train loc df_train 'crew' freqs times Sx signal spectrogram subset 'fz_cz' fs window 'hanning' nperseg noverlap detrend False scaling 'spectrum' ax plt subplots figsize ax pcolormesh times freqs np log10 Sx cmap 'viridis' ax set_ylabel ax set_xlabel
249	y_train y_test train_test_split processedtext sentiment test_size random_state print f'Data Split done.'
250	vectoriser transform vectoriser transform print f'Data Transformed.'
251	def model_Evaluate model y_pred model predict print classification_report y_test y_pred cf_matrix confusion_matrix y_test y_pred categories group_names group_percentages format value for value in cf_matrix flatten np sum cf_matrix labels f'{v1}\n{v2}' for v1 v2 in zip group_names group_percentages labels np asarray labels reshape sns heatmap cf_matrix annot labels cmap fmt xticklabels categories yticklabels categories plt xlabel fontdict 'size' labelpad plt ylabel fontdict 'size' labelpad plt title fontdict 'size' pad
252	LRmodel LogisticRegression max_iter n_jobs LRmodel fit y_train model_Evaluate LRmodel
253	file open 'vectoriser-ngram-(1,2).pickle' 'wb' pickle dump vectoriser file file close file open 'wb' pickle dump LRmodel file file close file open 'wb' pickle dump BNBmodel file file close
254	def load_models file open '..path/vectoriser-ngram-(1,2).pickle' 'rb' vectoriser pickle load file file close file open 'rb' LRmodel pickle load file file close return vectoriser LRmodel def predict vectoriser model text textdata vectoriser transform preprocess text sentiment model predict textdata data for text pred in zip text sentiment data append text pred df pd DataFrame data columns 'text' 'sentiment' df df replace return df if __name__ "__main__" text df predict vectoriser LRmodel text print df head
255	import os import numpy as np import pandas as pd import matplotlib pyplot as plt import seaborn as sns from subprocess import check_output print check_output "ls" "../input" decode "utf8"
256	librosa stft data Xdb librosa amplitude_to_db abs plt figure figsize librosa display specshow Xdb sr sr x_axis 'time' y_axis 'hz' plt colorbar
257	n0 n1 plt figure figsize plt plot data n0 n1 plt grid
258	import sklearn spectral_centroids librosa feature spectral_centroid data sr sr print spectral_centroids shape frames range len spectral_centroids librosa frames_to_time frames def normalize axis return sklearn preprocessing minmax_scale axis axis plt figure figsize librosa display waveplot data sr sr alpha plt plot normalize spectral_centroids color 'r'
259	def sitk_show img title None margin dpi nda SimpleITK GetArrayFromImage img spacing img GetSpacing figsize margin nda shape dpi margin nda shape dpi extent nda shape spacing nda shape spacing fig plt figure figsize figsize dpi dpi ax fig add_axes margin margin margin margin plt set_cmap "gray" ax imshow nda extent extent interpolation None if title plt title title plt show
260	title_grp train_labels groupby 'title' 'game_session' count reset_index display title_grp fig go Figure data go Pie labels title_grp title values title_grp game_session fig show
261	print cate_cols train_data select_dtypes include 'object' columns print cate_cols print num_cols train_data select_dtypes exclude 'object' columns print num_cols
262	event_count test_data 'title' value_counts reset_index event_count 'index' event_count 'index' astype 'category' fig px bar event_count 'index' 'title' hover_data 'title' color 'index' labels 'title' height fig show
263	type_count test_data 'type' value_counts reset_index total len test_data type_count 'percent' round type_count 'type' total print type_count fig px bar type_count 'index' 'type' hover_data 'index' 'percent' color 'type' labels 'type' height fig show
264	type_count train_data 'world' value_counts reset_index total len train_data type_count 'percent' round type_count 'world' total print type_count fig px bar type_count 'index' 'world' hover_data 'index' 'percent' color 'world' labels 'world' height fig show
265	type_count test_data 'world' value_counts reset_index total len test_data type_count 'percent' round type_count 'world' total print type_count fig px bar type_count 'index' 'world' hover_data 'index' 'percent' color 'world' labels 'world' height fig show
266	date_count train_data groupby 'date' 'installation_id' count reset_index fig go Figure data go Scatter date_count 'date' date_count 'installation_id' fig show
267	week_type_count train_data groupby 'weekofyear' 'type' 'installation_id' count reset_index fig px line week_type_count "weekofyear" "installation_id" color 'type' fig show
268	date_title_count test_data groupby 'title' 'game_time' count reset_index date_title_count sort_values by 'game_time' inplace True ascending True print date_title_count fig px line date_title_count "title" "game_time" fig show
269	data train_data train_data 'event_code' data shape print data 'title' value_counts print data 'type' value_counts print data 'world' value_counts
270	game train_data groupby 'title' 'game_time' max reset_index game sort_values by 'game_time' inplace True ascending False fig px bar game 'game_time' 'title' orientation 'h' hover_data 'title' color 'game_time' labels 'game_time' height fig show
271	world_type train_data groupby 'world' 'type' 'game_time' mean reset_index world_type plt figure figsize ax sns barplot "world" "game_time" hue "type" data world_type
272	world_type train_data groupby 'world' 'type' 'event_count' count reset_index world_type plt figure figsize ax sns barplot "world" "event_count" hue "type" data world_type
273	def get_unique data feat return data feat nunique for col in train_labels columns values print "unique number of values in " col print get_unique train_labels col
274	dd test_data groupby 'date' 'world' value_counts dd dd reset_index name 'count' dd fig px line dd "date" "count" color 'world' fig show
275	corr data corr mask np zeros_like corr dtype np bool mask np triu_indices_from mask True ax plt subplots figsize cmap sns diverging_palette as_cmap True sns heatmap corr mask mask cmap cmap vmax center square True linewidths cbar_kws "shrink"
276	train pd read_csv '/kaggle/input/cat-in-the-dat/train.csv' test pd read_csv '/kaggle/input/cat-in-the-dat/test.csv' target train 'target' train_id train 'id' test_id test 'id' train drop 'target' 'id' axis inplace True test drop 'id' axis inplace True
277	WOEEncoder train_woe fit_transform train feature_list target test_woe transform test feature_list
278	null_rate train isna sum len train for in train columns fig ax plt subplots figsize sns barplot train columns null_rate ax ax color 'gray' ax set_title ax set_xticklabels train columns rotation ax axhline color 'red' plt show
279	fig ax plt subplots figsize for in range sns countplot f'nom_{i}' data train ax ax order train f'nom_{i}' value_counts index ax set_ylim ax set_title f'nom_{i}' fontsize fig suptitle fontsize plt show
280	fig ax plt subplots figsize for in range sns countplot f'bin_{i}' hue 'target' data train_df ax ax ax set_title f'bin_{i} feature countplot' print percentage_of_feature_target train_df f'bin_{i}' 'target' plt show
281	fig ax plt subplots figsize for in range sns countplot sorted train_df f'nom_{i}' ax ax plt setp ax get_xticklabels rotation plt show
282	xgb_params 'eta' 'max_depth' 'subsample' 'colsample_bytree' 'objective' 'reg:linear' 'eval_metric' 'rmse' 'min_child_weight' 'silent' 'seed' xgtrain xgb DMatrix dev_X dev_y feature_names dev_X columns xgtest xgb DMatrix val_X val_y feature_names val_X columns watchlist xgtrain 'train' xgtest 'test' num_rounds model xgb train xgb_params xgtrain num_rounds watchlist early_stopping_rounds verbose_eval
283	train_df pd read_csv '../input/nyc-taxi-trip-duration/train.csv' parse_dates 'pickup_datetime' test_df pd read_csv '../input/nyc-taxi-trip-duration/test.csv' parse_dates 'pickup_datetime' print train_df shape print test_df shape
284	fig ax plt subplots figsize sns distplot train_df 'X' ax ax sns distplot train_df 'Y' ax ax plt show
285	temp_data StringIO temp_df pd read_csv temp_data train_df pd merge train_df temp_df on "category_name" how "left"
286	temp_series train_df 'user_type' value_counts labels np array temp_series index sizes np array temp_series temp_series sum trace go Pie labels labels values sizes layout go Layout title width height data trace fig go Figure data data layout layout py iplot fig filename "usertype"
287	train_df "description" fillna "NA" inplace True test_df "description" fillna "NA" inplace True train_df "desc_nwords" train_df "description" apply lambda len split test_df "desc_nwords" test_df "description" apply lambda len split cnt_srs train_df 'desc_nwords' value_counts head trace go Bar cnt_srs index cnt_srs values marker dict color "blue" reversescale True layout go Layout title data trace fig go Figure data data layout layout py iplot fig filename "desc_nwords"
288	def run_lgb train_X train_y val_X val_y test_X params "objective" "regression" "metric" "rmse" "num_leaves" "learning_rate" "bagging_fraction" "feature_fraction" "bagging_frequency" "bagging_seed" "verbosity" lgtrain lgb Dataset train_X label train_y lgval lgb Dataset val_X label val_y evals_result model lgb train params lgtrain valid_sets lgval early_stopping_rounds verbose_eval evals_result evals_result pred_test_y model predict test_X num_iteration model best_iteration return pred_test_y model evals_result
289	fig ax plt subplots figsize lgb plot_importance model max_num_features height ax ax ax grid False plt title fontsize plt show
290	def run_lgb train_X train_y val_X val_y test_X params "objective" "regression" "metric" "rmse" "num_leaves" "learning_rate" "bagging_fraction" "feature_fraction" "bagging_frequency" "bagging_seed" "verbosity" lgtrain lgb Dataset train_X label train_y lgval lgb Dataset val_X label val_y evals_result model lgb train params lgtrain valid_sets lgval early_stopping_rounds verbose_eval evals_result evals_result pred_test_y model predict test_X num_iteration model best_iteration return pred_test_y model evals_result
291	target_col "target" plt figure figsize plt scatter range train_df shape np sort train_df target_col values plt xlabel 'index' fontsize plt ylabel fontsize plt show
292	cnt_srs train_df 'first_active_month' dt date value_counts cnt_srs cnt_srs sort_index plt figure figsize sns barplot cnt_srs index cnt_srs values alpha color 'green' plt xticks rotation 'vertical' plt xlabel fontsize plt ylabel fontsize plt title plt show cnt_srs test_df 'first_active_month' dt date value_counts cnt_srs cnt_srs sort_index plt figure figsize sns barplot cnt_srs index cnt_srs values alpha color 'green' plt xticks rotation 'vertical' plt xlabel fontsize plt ylabel fontsize plt title plt show
293	gdf hist_df groupby "card_id" gdf gdf "purchase_amount" agg 'sum' 'mean' 'std' 'min' 'max' reset_index gdf columns "card_id" "sum_hist_trans" "mean_hist_trans" "std_hist_trans" "min_hist_trans" "max_hist_trans" train_df pd merge train_df gdf on "card_id" how "left" test_df pd merge test_df gdf on "card_id" how "left"
294	fig ax plt subplots figsize sns distplot weather_train 'wind_direction' ax ax sns distplot weather_train 'wind_speed' ax ax plt show
295	def cloud_graph df df df sort_values 'timestamp' fig go Figure fig add_trace go Scatter df 'timestamp' df 'cloud_coverage' name line_color 'lightskyblue' opacity fig update_layout template 'plotly_dark' title_text xaxis_rangeslider_visible True fig show
296	cnt_srs train_df 'bedrooms' value_counts plt figure figsize sns barplot cnt_srs index cnt_srs values alpha color color plt ylabel fontsize plt xlabel 'bedrooms' fontsize plt show
297	plt figure figsize plt scatter range train_df shape np sort train_df price values plt xlabel 'index' fontsize plt ylabel 'price' fontsize plt show
298	ulimit np percentile train_df price values train_df 'price' ix train_df 'price' ulimit ulimit plt figure figsize sns distplot train_df price values bins kde True plt xlabel 'price' fontsize plt show
299	llimit np percentile train_df latitude values ulimit np percentile train_df latitude values train_df 'latitude' ix train_df 'latitude' llimit llimit train_df 'latitude' ix train_df 'latitude' ulimit ulimit plt figure figsize sns distplot train_df latitude values bins kde False plt xlabel 'latitude' fontsize plt show
300	llimit np percentile train_df longitude values ulimit np percentile train_df longitude values train_df 'longitude' ix train_df 'longitude' llimit llimit train_df 'longitude' ix train_df 'longitude' ulimit ulimit plt figure figsize sns distplot train_df longitude values bins kde False plt xlabel 'longitude' fontsize plt show
301	from mpl_toolkits basemap import Basemap from matplotlib import cm west south east north fig plt figure figsize ax fig add_subplot Basemap projection 'merc' llcrnrlat south urcrnrlat north llcrnrlon west urcrnrlon east lat_ts south resolution 'i' train_df 'longitude' values train_df 'latitude' values hexbin gridsize bins 'log' cmap cm
302	from wordcloud import WordCloud text text_da text_desc for ind row in train_df iterrows for feature in row 'features' text join text join feature strip split text_da join text_da join row 'display_address' strip split text text strip text_da text_da strip text_desc text_desc strip plt figure figsize wordcloud WordCloud background_color 'white' width height max_font_size max_words generate text wordcloud recolor random_state plt imshow wordcloud plt title fontsize plt axis "off" plt show plt figure figsize wordcloud WordCloud background_color 'white' width height max_font_size max_words generate text_da wordcloud recolor random_state plt imshow wordcloud plt title fontsize plt axis "off" plt show
303	ulimit train_df 'y' ix train_df 'y' ulimit ulimit plt figure figsize sns distplot train_df values bins kde False plt xlabel 'y value' fontsize plt show
304	missing_df train_df isnull sum axis reset_index missing_df columns 'column_name' 'missing_count' missing_df missing_df ix missing_df 'missing_count' missing_df missing_df sort_values by 'missing_count' missing_df
305	plt figure figsize train_df 'eval_set' "train" test_df 'eval_set' "test" full_df pd concat train_df "ID" "eval_set" test_df "ID" "eval_set" axis plt figure figsize sns violinplot "eval_set" 'ID' data full_df plt xlabel "eval_set" fontsize plt ylabel 'y' fontsize plt title fontsize plt show
306	for in "X0" "X1" "X2" "X3" "X4" "X5" "X6" "X8" lbl preprocessing LabelEncoder lbl fit list train_df values train_df lbl transform list train_df values train_y train_df 'y' values train_X train_df drop "ID" "y" "eval_set" axis def xgb_r2_score preds dtrain labels dtrain get_label return 'r2' r2_score labels preds xgb_params 'eta' 'max_depth' 'subsample' 'colsample_bytree' 'objective' 'reg:linear' 'silent' dtrain xgb DMatrix train_X train_y feature_names train_X columns values model xgb train dict xgb_params silent dtrain num_boost_round feval xgb_r2_score maximize True fig ax plt subplots figsize xgb plot_importance model max_num_features height ax ax plt show
307	from sklearn import ensemble model ensemble RandomForestRegressor n_estimators max_depth min_samples_leaf max_features n_jobs random_state model fit train_X train_y feat_names train_X columns values importances model feature_importances_ std np std tree feature_importances_ for tree in model estimators_ axis indices np argsort importances plt figure figsize plt title plt bar range len indices importances indices color "r" align "center" plt xticks range len indices feat_names indices rotation 'vertical' plt xlim len indices plt show
308	train_y train_df "target" values def runModel train_X train_y test_X test_y test_X2 model linear_model LogisticRegression solver 'sag' model fit train_X train_y pred_test_y model predict_proba test_X pred_test_y2 model predict_proba test_X2 return pred_test_y pred_test_y2 model print cv_scores pred_full_test pred_train np zeros train_df shape kf model_selection KFold n_splits shuffle True random_state for dev_index val_index in kf split train_df dev_X val_X train_tfidf dev_index train_tfidf val_index dev_y val_y train_y dev_index train_y val_index pred_val_y pred_test_y model runModel dev_X dev_y val_X val_y test_tfidf pred_full_test pred_full_test pred_test_y pred_train val_index pred_val_y cv_scores append metrics log_loss val_y pred_val_y break
309	for thresh in np arange thresh np round thresh print format thresh metrics f1_score val_y pred_val_y thresh astype int
310	plt figure figsize sns countplot "days_since_prior_order" data orders_df color color plt ylabel fontsize plt xlabel fontsize plt xticks rotation 'vertical' plt title fontsize plt show
311	cnt_srs order_products_prior_df 'aisle' value_counts head plt figure figsize sns barplot cnt_srs index cnt_srs values alpha color color plt ylabel fontsize plt xlabel fontsize plt xticks rotation 'vertical' plt show
312	plt figure figsize temp_series order_products_prior_df 'department' value_counts labels np array temp_series index sizes np array temp_series temp_series sum plt pie sizes labels labels autopct '%1.1f%%' startangle plt title fontsize plt show
313	grouped_df order_products_prior_df groupby "department" "reordered" aggregate "mean" reset_index plt figure figsize sns pointplot grouped_df 'department' values grouped_df 'reordered' values alpha color color plt ylabel fontsize plt xlabel fontsize plt title fontsize plt xticks rotation 'vertical' plt show
314	grouped_df order_products_prior_df groupby "department_id" "aisle" "reordered" aggregate "mean" reset_index fig ax plt subplots figsize ax scatter grouped_df reordered values grouped_df department_id values for txt in enumerate grouped_df aisle values ax annotate txt grouped_df reordered values grouped_df department_id values rotation ha 'center' va 'center' color 'green' plt xlabel plt ylabel 'department_id' plt title fontsize plt show
315	from subprocess import check_output print check_output "ls" "../input" decode "utf8"
316	train_variants_df pd read_csv "../input/training_variants" test_variants_df pd read_csv "../input/test_variants" train_text_df pd read_csv "../input/training_text" sep engine 'python' header None skiprows names "ID" test_text_df pd read_csv "../input/test_text" sep engine 'python' header None skiprows names "ID" print train_variants_df shape test_variants_df shape print train_text_df shape test_text_df shape
317	print f"The total number of games in the training data is {train_df['GameId'].nunique()}" print f"The total number of plays in the training data is {train_df['PlayId'].nunique()}" print f"The NFL seasons in the training data are {train_df['Season'].unique().tolist()}"
318	from plotly import tools import plotly offline as py py init_notebook_mode connected True import plotly graph_objs as go import plotly express as px temp_df train_df query fig px histogram temp_df layout go Layout title go layout Title text font dict size width height fig update_layout layout fig show
319	plt figure figsize temp_df train_df query sns catplot data temp_df kind "boxen" plt xlabel fontsize plt ylabel fontsize plt title fontsize plt show
320	import random
321	train_df pd read_csv "../input/train.csv" parse_dates 'timestamp' dtype_df train_df dtypes reset_index dtype_df columns dtype_df groupby aggregate 'count' reset_index
322	col "life_sq" train_df col fillna inplace True ulimit np percentile train_df col values llimit np percentile train_df col values train_df col ix train_df col ulimit ulimit train_df col ix train_df col llimit llimit plt figure figsize sns jointplot np log1p train_df life_sq values np log1p train_df price_doc values kind 'kde' size plt ylabel fontsize plt xlabel fontsize plt show
323	plt figure figsize sns countplot "floor" data train_df plt ylabel fontsize plt xlabel 'floor number' fontsize plt xticks rotation 'vertical' plt show
324	grouped_df train_df groupby 'floor' 'price_doc' aggregate np median reset_index plt figure figsize sns pointplot grouped_df floor values grouped_df price_doc values alpha color color plt ylabel fontsize plt xlabel fontsize plt xticks rotation 'vertical' plt show
325	train_df 'transaction_month' train_df 'transactiondate' dt month cnt_srs train_df 'transaction_month' value_counts plt figure figsize sns barplot cnt_srs index cnt_srs values alpha color color plt xticks rotation 'vertical' plt xlabel fontsize plt ylabel fontsize plt show
326	missing_df prop_df isnull sum axis reset_index missing_df columns 'column_name' 'missing_count' missing_df missing_df ix missing_df 'missing_count' missing_df missing_df sort_values by 'missing_count' ind np arange missing_df shape width fig ax plt subplots figsize rects ax barh ind missing_df missing_count values color 'blue' ax set_yticks ind ax set_yticklabels missing_df column_name values rotation 'horizontal' ax set_xlabel ax set_title plt show
327	plt figure figsize sns jointplot prop_df latitude values prop_df longitude values size plt ylabel fontsize plt xlabel fontsize plt show
328	pd options display max_rows dtype_df train_df dtypes reset_index dtype_df columns dtype_df
329	missing_df train_df isnull sum axis reset_index missing_df columns 'column_name' 'missing_count' missing_df 'missing_ratio' missing_df 'missing_count' train_df shape missing_df ix missing_df 'missing_ratio'
330	plt figure figsize sns countplot "bathroomcnt" data train_df plt ylabel fontsize plt xlabel fontsize plt xticks rotation 'vertical' plt title fontsize plt show
331	train_df 'bedroomcnt' ix train_df 'bedroomcnt' plt figure figsize sns violinplot 'bedroomcnt' 'logerror' data train_df plt xlabel fontsize plt ylabel fontsize plt show
332	from ggplot import ggplot aes 'yearbuilt' 'logerror' data train_df geom_point color 'steelblue' size stat_smooth
333	ggplot aes 'latitude' 'longitude' color 'logerror' data train_df geom_point scale_color_gradient low 'red' high 'blue'
334	train_df 'num_punctuations' loc train_df 'num_punctuations' plt figure figsize sns violinplot 'author' 'num_punctuations' data train_df plt xlabel fontsize plt ylabel fontsize plt title fontsize plt show
335	cv_scores pred_full_test pred_train np zeros train_df shape kf model_selection KFold n_splits shuffle True random_state for dev_index val_index in kf split train_X dev_X val_X train_tfidf dev_index train_tfidf val_index dev_y val_y train_y dev_index train_y val_index pred_val_y pred_test_y model runMNB dev_X dev_y val_X val_y test_tfidf pred_full_test pred_full_test pred_test_y pred_train val_index pred_val_y cv_scores append metrics log_loss val_y pred_val_y print np mean cv_scores pred_full_test pred_full_test
336	tfidf_vec CountVectorizer stop_words 'english' ngram_range tfidf_vec fit train_df 'text' values tolist test_df 'text' values tolist train_tfidf tfidf_vec transform train_df 'text' values tolist test_tfidf tfidf_vec transform test_df 'text' values tolist
337	cv_scores pred_full_test pred_train np zeros train_df shape kf model_selection KFold n_splits shuffle True random_state for dev_index val_index in kf split train_X dev_X val_X train_tfidf dev_index train_tfidf val_index dev_y val_y train_y dev_index train_y val_index pred_val_y pred_test_y model runMNB dev_X dev_y val_X val_y test_tfidf pred_full_test pred_full_test pred_test_y pred_train val_index pred_val_y cv_scores append metrics log_loss val_y pred_val_y print np mean cv_scores pred_full_test pred_full_test train_df "nb_cvec_eap" pred_train train_df "nb_cvec_hpl" pred_train train_df "nb_cvec_mws" pred_train test_df "nb_cvec_eap" pred_full_test test_df "nb_cvec_hpl" pred_full_test test_df "nb_cvec_mws" pred_full_test
338	cnf_matrix confusion_matrix val_y np argmax pred_val_y axis np set_printoptions precision plt figure figsize plot_confusion_matrix cnf_matrix classes 'EAP' 'HPL' 'MWS' title plt show
339	env kagglegym make observation env reset train observation train
340	is_dup train_df 'is_duplicate' value_counts plt figure figsize sns barplot is_dup index is_dup values alpha color color plt ylabel fontsize plt xlabel fontsize plt show
341	plt figure figsize grouped_df train_df groupby 'q1_freq' 'is_duplicate' aggregate np mean reset_index sns barplot grouped_df "q1_freq" values grouped_df "is_duplicate" values alpha color color plt ylabel fontsize plt xlabel fontsize plt xticks rotation 'vertical' plt show
342	cols_to_use 'q1_q2_intersect' 'q1_freq' 'q2_freq' temp_df train_df cols_to_use corrmat temp_df corr method 'spearman' ax plt subplots figsize sns heatmap corrmat vmax square True plt title fontsize plt show
343	def runXGB train_X train_y test_X test_y None feature_names None seed_val num_rounds param param 'objective' 'multi:softprob' param 'eta' param 'max_depth' param 'silent' param 'num_class' param 'eval_metric' "mlogloss" param 'min_child_weight' param 'subsample' param 'colsample_bytree' param 'seed' seed_val num_rounds num_rounds plst list param items xgtrain xgb DMatrix train_X label train_y if test_y is not None xgtest xgb DMatrix test_X label test_y watchlist xgtrain 'train' xgtest 'test' model xgb train plst xgtrain num_rounds watchlist early_stopping_rounds else xgtest xgb DMatrix test_X model xgb train plst xgtrain num_rounds pred_test_y model predict xgtest return pred_test_y model
344	data_path "../input/" train_file data_path "train.json" test_file data_path "test.json" train_df pd read_json train_file test_df pd read_json test_file print train_df shape print test_df shape
345	cv_scores kf model_selection KFold n_splits shuffle True random_state for dev_index val_index in kf split range train_X shape dev_X val_X train_X dev_index train_X val_index dev_y val_y train_y dev_index train_y val_index preds model runXGB dev_X dev_y val_X val_y cv_scores append log_loss val_y preds print cv_scores break
346	path '../input/' train pd read_csv path 'train.csv' test pd read_csv path 'test.csv' print train shape print test shape
347	fig ax plt subplots figsize ax1 ax2 ax3 ax4 ax5 ax6 ax flatten sns countplot train 'toxic' palette 'magma' ax ax1 sns countplot train 'severe_toxic' palette 'viridis' ax ax2 sns countplot train 'obscene' palette ax ax3 sns countplot train 'threat' palette 'viridis' ax ax4 sns countplot train 'insult' palette 'magma' ax ax5 sns countplot train 'identity_hate' palette ax ax6
348	vect_word TfidfVectorizer max_features lowercase True analyzer 'word' stop_words 'english' ngram_range dtype np float32 vect_char TfidfVectorizer max_features lowercase True analyzer 'char' stop_words 'english' ngram_range dtype np float32
349	col 'identity_hate' print col pred lr predict print confusion_matrix col pred print classification_report col pred
350	col 'identity_hate' print col pred_pro lr predict_proba frp trp thres roc_curve col pred_pro auc_val auc frp trp plt figure figsize plt plot color 'b' plt plot frp trp color 'r' label auc_val plt legend loc 'lower right' plt xlabel plt ylabel plt title
351	path '../input/' train pd read_csv path 'train.csv' test pd read_csv path 'test.csv' print train shape print test shape
352	fig ax plt subplots figsize ax1 ax2 ax flatten sns distplot train 'formation_energy_ev_natom' bins ax ax1 color 'b' sns distplot train 'bandgap_energy_ev' bins ax ax2 color 'r'
353	cor train corr plt figure figsize sns heatmap cor cmap annot True
354	def OHE df1 df2 columns len df1 shape df pd concat df1 df2 axis c2 c3 print columns for in columns c2 append c3 'ohe_' df pd get_dummies data df columns c2 prefix c3 df1 df iloc len df2 df iloc len print df1 shape df2 shape return df1 df2
355	print train 'text' review re sub train 'text' print review
356	review word_tokenize train 'text' print review review word for word in str train 'text' lower split if word not in set stopwords words 'english' print review review word for word in str train 'text' lower split if word in set stopwords words 'english' print review ps PorterStemmer review ps stem word for word in str train 'text' lower split print review
357	def clean_text df ps PorterStemmer corpus for in range df shape review re sub df 'text' review word_tokenize review review word for word in review if word lower not in set stopwords words 'english' review ps stem word for word in review review join review corpus append review return corpus
358	cv CountVectorizer max_features ngram_range dtype np int8 stop_words 'english' cv fit_transform train 'clean_text' toarray cv fit_transform test 'clean_text' toarray
359	mNB MultinomialNB kf KFold n_splits shuffle True random_state seed pred_test_full cv_score for train_index test_index in kf split print format kf n_splits xtr xvl train_index test_index ytr yvl train_index test_index mNB fit xtr ytr y_mNB mNB predict xvl cv_score append log_loss yvl mNB predict_proba xvl pred_test_full mNB predict_proba print cv_score print np mean cv_score print 'confusion matrix:\n' confusion_matrix yvl y_mNB del xtr ytr xvl yvl
360	y_pred pred_test_full submit pd DataFrame test 'id' submit submit join pd DataFrame y_pred submit columns 'id' 'EAP' 'HPL' 'MWS' submit to_csv 'spooky_pred1.csv' index False
361	mNB MultinomialNB kf KFold n_splits shuffle True random_state seed pred_test_full cv_score for train_index test_index in kf split print format kf n_splits xtr xvl train_index test_index ytr yvl train_index test_index mNB fit xtr ytr y_mNB mNB predict xvl cv_score append log_loss yvl mNB predict_proba xvl pred_test_full mNB predict_proba print cv_score print np mean cv_score print 'confusion matrix:\n' confusion_matrix yvl y_mNB del xtr ytr xvl yvl
362	y_pred pred_test_full submit pd DataFrame test 'id' submit submit join pd DataFrame y_pred submit columns 'id' 'EAP' 'HPL' 'MWS' submit to_csv 'spooky_pred2.csv' index False
363	y_pred pred_test_full submit pd DataFrame test 'id' submit submit join pd DataFrame y_pred submit columns 'id' 'EAP' 'HPL' 'MWS' submit to_csv 'spooky_pred3.csv' index False
364	dropna as_matrix None if np percentile or np percentile else for in df_na pd DataFrame 'ds' index values 'y'
365	path '../input/' train pd read_csv path 'train.csv' na_values test pd read_csv path 'test.csv' na_values print train shape print test shape
366	pd DataFrame 'train' train isnull sum 'test' test isnull sum fig ax plt subplots figsize plot kind 'bar' ax ax
367	def missing_value df col df columns for in col if df isnull sum df fillna df mode inplace True
368	def basic_details df pd DataFrame df isnull sum df nunique 'dtype' df dtypes return basic_details train
369	def descrictive_stat_feat df df pd DataFrame df dcol for in train columns if train nunique dcol remove 'id' d_median df dcol median axis d_mean df dcol mean axis q1 df dcol apply np float32 quantile q3 df dcol apply np float32 quantile for in dcol df str '_median_range' df astype np float32 values d_median astype np int8 df str '_mean_range' df astype np float32 values d_mean astype np int8 df str '_q1' df astype np float32 values q1 astype np int8 df str '_q3' df astype np float32 values q3 astype np int8 return df
370	def outlier df columns for in columns quartile_1 quartile_3 np percentile df quartile_f quartile_l np percentile df IQR quartile_3 quartile_1 lower_bound quartile_1 IQR upper_bound quartile_3 IQR print lower_bound upper_bound quartile_f quartile_l df loc df lower_bound quartile_f df loc df upper_bound quartile_l outlier train num_col outlier test num_col
371	def OHE df1 df2 column cat_col column len_df1 df1 shape df pd concat df1 df2 ignore_index True c2 c3 print len column for in cat_col if df nunique c2 append c3 'ohe_' df pd get_dummies df prefix c3 columns c2 drop_first True df1 df loc len_df1 df2 df loc len_df1 print df1 shape print df2 shape return df1 df2
372	train1 drop 'target' 'id' axis train1 'target' astype 'category' x_test test1 drop 'target' 'id' axis del train1 test1
373	kf StratifiedKFold n_splits random_state seed shuffle True pred_test_full cv_score for train_index test_index in kf split print '\n{} of kfold {}' format kf n_splits xtr xvl loc train_index loc test_index ytr yvl train_index test_index lr LogisticRegression class_weight 'balanced' lr fit xtr ytr pred_test lr predict_proba xvl score roc_auc_score yvl pred_test print 'roc_auc_score' score cv_score append score pred_test_full lr predict_proba x_test
374	proba lr predict_proba xvl fpr tpr threshold roc_curve yvl proba auc_val auc fpr tpr plt figure figsize plt title plt plot fpr tpr 'b' label auc_val plt legend loc 'lower right' plt plot 'r--' plt ylabel plt xlabel
375	y_pred pred_test_full submit pd DataFrame 'id' test 'id' 'target' y_pred submit to_csv 'lr_porto.csv' index False
376	path '../input/' train pd read_csv path 'train.csv' na_values test pd read_csv path 'test.csv' na_values print train shape print test shape
377	pd DataFrame 'train' train isnull sum 'test' test isnull sum
378	def descrictive_stat_feat df df pd DataFrame df dcol for in train columns if train nunique dcol remove 'id' d_median df dcol median axis d_mean df dcol mean axis q1 df dcol apply np float32 quantile q3 df dcol apply np float32 quantile for in dcol df str '_median_range' df astype np float32 values d_median astype np int8 df str '_mean_range' df astype np float32 values d_mean astype np int8 df str '_q1' df astype np float32 values q1 astype np int8 df str '_q3' df astype np float32 values q3 astype np int8 return df
379	def OHE df1 df2 column cat_col column len_df1 df1 shape df pd concat df1 df2 ignore_index True c2 c3 print len column for in cat_col if df nunique c2 append c3 'ohe_' df pd get_dummies df prefix c3 columns c2 drop_first True df1 df loc len_df1 df2 df loc len_df1 print df1 shape print df2 shape return df1 df2
380	train drop 'target' 'id' axis train 'target' astype 'category' x_test test drop 'id' axis test_id test 'id'
381	y_pred pred_xgb submit pd DataFrame 'id' test_id 'target' y_pred submit to_csv 'xgb_porto.csv' index False
382	def get_model bn_model p_activation "elu" input_layer Input shape name "X_1" dense_layer Dropout BatchNormalization momentum bn_model Dense activation p_activation input_layer dense_layer Dropout BatchNormalization momentum bn_model Dense activation p_activation dense_layer output Dense activation "sigmoid" dense_layer model Model input_layer output optimizer Adam lr epsilon 1e-08 model compile loss "binary_crossentropy" optimizer optimizer metrics "accuracy" return model
383	def gini actual pred cmpcol sortcol assert len actual len pred all np asarray np c_ actual pred np arange len actual dtype np float all all np lexsort all all totalLosses all sum giniSum all cumsum sum totalLosses giniSum len actual return giniSum len actual def gini_normalized return gini gini def gini_xgb preds dtrain labels dtrain get_label gini_score gini_normalized labels preds return 'gini' gini_score
384	train drop 'id' 'target' axis values train target values test_id test id values test test drop 'id' axis
385	sub pd DataFrame sub 'id' test_id sub 'target' np zeros_like test_id
386	def train_pred model epochs for in range epochs model fit train_X train_y batch_size epochs validation_data val_X val_y pred_val_y model predict val_X batch_size verbose best_thresh best_score for thresh in np arange thresh np round thresh score metrics f1_score val_y pred_val_y thresh astype int if score best_score best_thresh thresh best_score score print format best_score pred_test_y model predict test_X batch_size verbose return pred_val_y pred_test_y best_score
387	if isfile P2SIZE print with open P2SIZE 'rb' as p2size pickle load else p2size for in tqdm join size pil_image open expand_path size p2size size
388	preds_v y_v learn TTA is_test False n_aug preds_v np stack preds_v axis preds_v np exp preds_v preds_v preds_v mean axis y_v
389	sample_df pd read_csv SAMPLE_SUB sample_list list sample_df Image labels_list "new_whale" labels_list pred_list labels_list for in argsort for in preds_t pred_dic dict key value for key value in zip learn data test_ds fnames pred_list pred_list_cor join pred_dic id for id in sample_list df pd DataFrame sample_list pred_list_cor df to_csv 'submission.csv' format MODEL_PATH header True index False df head
390	sample_df pd read_csv SAMPLE_SUB sample_list list sample_df id pred_list for in preds_t pred_dic dict key value for key value in zip learn data test_ds fnames pred_list pred_list_cor pred_dic id for id in sample_list df pd DataFrame 'id' sample_list 'label' pred_list_cor df to_csv 'submission.csv' format MODEL_PATH header True index False
391	df df apply lambda if else df df apply lambda if len re split else df df apply lambda if else len re split df df apply lambda if else re split df df apply lambda if else re split df value_counts
392	df_cabin df df notnull df_cabin df_cabin apply lambda df_cabin df_cabin apply lambda split df_cabin replace inplace True df_cabin df_cabin apply lambda int df_cabin groupby mean
393	from sklearn import preprocessing df df drop axis categorical lbl preprocessing LabelEncoder for col in categorical df col fillna df col lbl fit_transform df col astype str df head
394	train_identity pd read_csv '../input/ieee-fraud-detection/train_identity.csv' train_transaction pd read_csv '../input/ieee-fraud-detection/train_transaction.csv' test_identity pd read_csv '../input/ieee-fraud-detection/test_identity.csv' test_transaction pd read_csv '../input/ieee-fraud-detection/test_transaction.csv' sub pd read_csv '../input/ieee-fraud-detection/sample_submission.csv' train pd merge train_transaction train_identity on how 'left' test pd merge test_transaction test_identity on how 'left'
395	from sklearn utils import resample not_fraud train train isFraud fraud train train isFraud not_fraud_downsampled resample not_fraud replace False n_samples random_state downsampled pd concat not_fraud_downsampled fraud downsampled isFraud value_counts
396	def uid_fe df df 'uid' df 'card1' astype str df 'card2' astype str df 'uid2' df 'uid' astype str df 'card3' astype str df 'card5' astype str df 'uid3' df 'uid2' astype str df 'addr1' astype str df 'addr2' astype str df 'uid4' df 'uid3' astype str df astype str df 'uid5' df 'uid3' astype str df astype str return df train uid_fe train test uid_fe test
397	for feature in 'card1' 'card2' 'card3' 'card4' 'card5' 'card6' 'id_36' train feature '_count_full' train feature map pd concat train feature test feature ignore_index True value_counts dropna False test feature '_count_full' test feature map pd concat train feature test feature ignore_index True value_counts dropna False
398	from scipy import stats from scipy stats import normaltest resid arima_mod6 resid print normaltest resid fig plt figure figsize ax0 fig add_subplot sns distplot resid fit stats norm ax ax0 mu sigma stats norm fit resid plt legend format mu sigma loc 'best' plt ylabel plt title fig plt figure figsize ax1 fig add_subplot fig sm graphics tsa plot_acf arima_mod6 resid lags ax ax1 ax2 fig add_subplot fig sm graphics tsa plot_pacf arima_mod6 resid lags ax ax2
399	start_index end_index train_df 'forecast' sarima_mod6 predict start start_index end end_index dynamic True train_df start_index end_index 'sales' 'forecast' plot figsize
400	gdf gdf drop gdf index gdf loc gdf 'country' 'country' gdf loc gdf 'country' 'country' gdf loc gdf 'country' 'country' gdf loc gdf 'country' 'country' gdf loc gdf 'country' 'country' gdf loc gdf 'country' 'country' gdf loc gdf 'country' 'country' 'US' gdf loc gdf 'country' 'country' gdf loc gdf 'country' 'country' gdf loc gdf 'country' 'country'
401	times_series_df plot figsize title plt legend loc prop 'size' plt show
402	from tensorflow python keras import optimizers sgd optimizers SGD lr decay 1e-6 momentum nesterov True model compile optimizer sgd loss OBJECTIVE_FUNCTION metrics LOSS_METRICS
403	from keras applications resnet50 import preprocess_input from keras preprocessing image import ImageDataGenerator image_size IMAGE_RESIZE data_generator ImageDataGenerator preprocessing_function preprocess_input train_generator data_generator flow_from_directory '../input/catsdogs-trainvalid-80pc-prepd/trainvalidfull4keras/trainvalidfull4keras/train' target_size image_size image_size batch_size BATCH_SIZE_TRAINING class_mode 'categorical' validation_generator data_generator flow_from_directory '../input/catsdogs-trainvalid-80pc-prepd/trainvalidfull4keras/trainvalidfull4keras/valid' target_size image_size image_size batch_size BATCH_SIZE_VALIDATION class_mode 'categorical'
404	test_generator reset pred model predict_generator test_generator steps len test_generator verbose predicted_class_indices np argmax pred axis
405	def latex_tag_in_text text text lower return ' [ math ] ' in train 'latex_tag_in_text' train 'question_text' apply lambda latex_tag_in_text
406	EMBED_SIZE MAX_WORDS_LEN MAX_VOCAB_FEATURES
407	def clean_latex_tag text corr_t for in text split strip if corr_t append text join corr_t text re sub '(\[ math \]).+(\[ / math \])' 'mathematical formula' text return text
408	train_df pd read_csv "../input/train.csv" encoding 'utf8' test_df pd read_csv "../input/test.csv" encoding 'utf8' all_test_texts join test_df question_text values tolist print train_df shape print test_df shape
409	def imshow inp title None inp inp numpy transpose mean np array std np array inp std inp mean inp np clip inp plt figure figsize plt imshow inp if title is not None plt title title plt show
410	model models resnet18 pretrained True fc_in_features model fc in_features model fc nn Linear fc_in_features model model to device loss_fn nn CrossEntropyLoss optimizer optim SGD model parameters lr momentum
411	test_pids test_df PetID values input_tensor torch zeros test_image_features for petid in tqdm test_pids test_img f"../input/test_images/{petid}-1.jpg" if not os path exists test_img continue test_img Image open test_img test_img extract_transform test_img input_tensor test_img input_tensor input_tensor cuda model input_tensor test_image_features petid image_features image_features clear
412	plt hist train_transaction label 'train' plt hist test_transaction label 'test' plt legend plt title
413	plt figure figsize total len train_full_cat plt subplot sns countplot data train_full_cat set_title fontsize set_xlabel fontsize set_ylabel fontsize for in patches height get_height text get_x get_width height '{:1.2f}%' format height total ha "center" fontsize plt subplot g1 sns countplot hue data train_full g1 set_title fontsize g1 set_xlabel fontsize g1 set_ylabel fontsize plt legend title loc 'best' labels
414	protonmail_fraud len train_full train_full "protonmail" train_full protonmail_non_fraud len train_full train_full "protonmail" train_full protonmail_fraud_rate protonmail_fraud protonmail_fraud protonmail_non_fraud print protonmail_fraud print protonmail_non_fraud print protonmail_fraud_rate
415	train_full 'major_os' train_full "id_30" str split expand True visualize_cat_cariable 'major_os'
416	train_features train_df drop 'target' axis test_features test_df drop axis train_target train_df 'target'
417	my_submission_nn pd DataFrame id_code_test "target" test_preds my_submission_lbgm pd DataFrame id_code_test "target" predictions my_submission_esemble pd DataFrame id_code_test "target" esemble_pred
418	data_dir '/kaggle/input/prostate-cancer-grade-assessment/train_images' mask_dir '/kaggle/input/prostate-cancer-grade-assessment/train_label_masks' train_labels pd read_csv '/kaggle/input/prostate-cancer-grade-assessment/train.csv'
419	label_map label for label in enumerate train_labels 'gleason_score' for label_name in enumerate label_map label_map label_name map_label label_map index index for index in enumerate label_map isup_map 'negative'
420	def map_atom_info df atom_idx df pd merge df structures how 'left' left_on 'molecule_name' f'atom_index_{atom_idx}' right_on 'molecule_name' 'atom_index' df df drop 'atom_index' axis df df rename columns 'atom' f'atom_{atom_idx}' 'x' f'x_{atom_idx}' 'y' f'y_{atom_idx}' 'z' f'z_{atom_idx}' return df train map_atom_info train train map_atom_info train test map_atom_info test test map_atom_info test
421	cols feature_importance_df "feature" "importance" groupby "feature" mean sort_values by "importance" ascending False index best_features feature_importance_df loc feature_importance_df feature isin cols plt figure figsize sns barplot "importance" "feature" data best_features sort_values by "importance" ascending False plt title plt tight_layout plt savefig 'lgbm_importances.png'
422	import os import gc print os listdir "../input" import numpy as np import pandas as pd import time
423	train 'exist_ship' train fillna train loc train 'exist_ship' 'exist_ship' del train
424	print len train print train value_counts shape train_gp train groupby sum reset_index train_gp loc train_gp 'exist_ship' 'exist_ship'
425	print train_gp 'exist_ship' value_counts train_gp train_gp sort_values by 'exist_ship' train_gp train_gp drop train_gp index
426	print train_gp 'exist_ship' value_counts train_sample train_gp sample print train_sample 'exist_ship' value_counts print train_sample shape
427	from sklearn preprocessing import OneHotEncoder targets data_target reshape len data_target enc OneHotEncoder enc fit targets targets enc transform targets toarray print targets shape
428	from sklearn model_selection import train_test_split x_train x_val y_train y_val train_test_split data targets test_size x_train shape x_val shape y_train shape y_val shape
429	from keras layers import Dropout Flatten Dense from keras models import Sequential Model for layer in model layers layer trainable False model output Flatten Dense activation "relu" Dropout Dense activation "relu" predictions Dense activation "softmax" model_final Model input model input output predictions
430	from keras import optimizers epochs lrate decay lrate epochs sgd optimizers SGD lr lrate momentum decay decay nesterov False model_final compile loss 'categorical_crossentropy' optimizer sgd metrics 'accuracy' model_final summary
431	from PIL import Image data_predict np empty len train_predict_sample dtype np uint8 data_target_predict np empty len train_predict_sample dtype np uint8 image_name_list os listdir index for image_name in image_name_list if image_name in list train_predict_sample imageA Image open image_name resize convert 'RGB' data_predict index imageA data_target_predict index train_predict_sample train_gp str contains image_name 'exist_ship' iloc index print data_predict shape print data_target_predict shape
432	from sklearn preprocessing import OneHotEncoder targets_predict data_target_predict reshape len data_target_predict enc OneHotEncoder enc fit targets_predict targets_predict enc transform targets_predict toarray print targets_predict shape
433	print str len train unique print str len train unique print train unique " to " train unique print str len test unique print str len test unique print test unique " to " test unique
434	for country in train unique country_pd_train train train country if country_pd_train isna unique True plt_title country plot_trend_by_date country_pd_train value title plt_title else state_count len country_pd_train unique row state_count column fig plt figure figsize row index for state in country_pd_train unique state_pd country_pd_train country_pd_train state plt_title country state ax fig add_subplot row column index xaxis state_pd tolist yaxis state_pd xaxis dates date2num xaxis hfmt dates DateFormatter '%m\n%d' ax xaxis set_major_formatter hfmt plt xlabel plt ylabel plt title plt_title ax plot xaxis yaxis index plt show
435	for country in train unique country_pd_train train train country if country_pd_train isna unique True plt_title country plot_trend_by_date country_pd_train value title plt_title else state_count len country_pd_train unique row state_count column fig plt figure figsize row index for state in country_pd_train unique state_pd country_pd_train country_pd_train state plt_title country state ax fig add_subplot row column index xaxis state_pd tolist yaxis state_pd xaxis dates date2num xaxis hfmt dates DateFormatter '%m\n%d' ax xaxis set_major_formatter hfmt plt xlabel plt ylabel plt title plt_title ax plot xaxis yaxis index plt show
436	drop_cols "bin_0" ddall "ord_5a" ddall "ord_5" str ddall "ord_5b" ddall "ord_5" str drop_cols append "ord_5"
437	for col in "nom_5" "nom_6" "nom_7" "nom_8" "nom_9" train_vals set dd0 col unique test_vals set ddtest0 col unique xor_cat_vals train_vals test_vals if xor_cat_vals ddall loc ddall col isin xor_cat_vals col "xor"
438	from sklearn base import TransformerMixin from itertools import repeat import scipy class ThermometerEncoder TransformerMixin def __init__ self sort_key None self sort_key sort_key self value_map_ None def fit self None self value_map_ val for val in enumerate sorted unique key self sort_key return self def transform self None values map self value_map_ possible_values sorted self value_map_ values idx1 idx2 all_indices np arange len for idx val in enumerate possible_values new_idxs all_indices values val idx1 extend new_idxs idx2 extend repeat idx len new_idxs result scipy sparse coo_matrix len idx1 idx1 idx2 shape len len possible_values dtype "int8" return result
439	ohc scipy sparse hstack ohc1 thermos tocsr display ohc ohc num_train ohc num_train y_train dd0 "target" values
440	drop_cols "bin_0" ddall "ord_5a" ddall "ord_5" str ddall "ord_5b" ddall "ord_5" str drop_cols append "ord_5"
441	for col in "nom_5" "nom_6" "nom_7" "nom_8" "nom_9" train_vals set dd0 col unique test_vals set ddtest0 col unique xor_cat_vals train_vals test_vals if xor_cat_vals ddall loc ddall col isin xor_cat_vals col "xor"
442	from sklearn base import TransformerMixin from itertools import repeat import scipy class ThermometerEncoder TransformerMixin def __init__ self sort_key None self sort_key sort_key self value_map_ None def fit self None self value_map_ val for val in enumerate sorted unique key self sort_key return self def transform self None values map self value_map_ possible_values sorted self value_map_ values idx1 idx2 all_indices np arange len for idx val in enumerate possible_values new_idxs all_indices values val idx1 extend new_idxs idx2 extend repeat idx len new_idxs result scipy sparse coo_matrix len idx1 idx1 idx2 shape len len possible_values dtype "int8" return result
443	ohc scipy sparse hstack ohc1 thermos tocsr display ohc ohc num_train ohc num_train y_train dd0 "target" values
444	def train_step images labels with tf GradientTape as tape predictions model images labels training True loss compute_loss labels predictions gradients tape gradient loss model trainable_variables optimizer apply_gradients zip gradients model trainable_variables train_loss loss tpu_strategy num_replicas_in_sync train_accuracy labels predictions return loss tf function def distributed_train_steps training_set_iter steps_per_call for in tf range steps_per_call per_replica_losses tpu_strategy run train_step next training_set_iter def test_step images labels predictions model images training False t_loss loss_object labels predictions test_loss t_loss test_accuracy labels predictions tf function def distributed_test_step images labels return tpu_strategy run test_step args images labels
445	show_cols "datetime" "card1" "card2" "card3" "id_31" sort_cols fraud dd query cat_cols dd dtypes loc lambda "category" index colors mpl cm tab20 colors n_colors len colors color_spec 'background: rgb({})' format join str int colval for colval in color for color in enumerate colors def color_hash value color_idx hash value n_colors return color_spec color_idx all_show_cols set sort_cols set cat_cols fraud sort_values sort_cols show_cols sorted all_show_cols set show_cols head style applymap color_hash
446	import numpy as np from sklearn metrics import log_loss from sklearn base import BaseEstimator from scipy optimize import minimize
447	n_classes data labels make_classification n_samples n_features n_informative n_classes n_classes random_state random_state y_test train_test_split data labels test_size random_state random_state y_train y_valid train_test_split test_size random_state random_state print print shape shape shape
448	from tabulate import tabulate print print wA np round w_enA decimals reshape print tabulate wA headers clfs keys tablefmt "orgtbl" print print print wB np round w_enB reshape n_classes decimals wB np hstack np array list clfs keys dtype str reshape wB print tabulate wB headers 'y%s' for in range n_classes tablefmt "orgtbl"
449	lr LogisticRegressionCV Cs dual False fit_intercept True intercept_scaling max_iter multi_class 'ovr' n_jobs penalty 'l2' random_state random_state solver 'lbfgs' tol lr fit XV y_valid y_lr lr predict_proba XT print '{:20s} {:2s} {:1.7f}' format 'logloss =>' log_loss y_test y_lr
450	import matplotlib pylab as plt plt figure figsize plt plot classes ll_sc color 'black' label for in range plt plot classes ll_sc color 'black' plt plot classes ll_lr 'bo-' label plt plot classes ll_gb 'mo-' label plt plot classes ll_eA 'yo-' label plt plot classes ll_eB 'go-' label plt plot classes ll_e3 'ro-' label plt title plt xlabel plt ylabel plt grid True plt legend loc plt show
451	def mirrorAction axis if new_a else if axis "h" new_a if axis "v" new_a not return new_a def augmentData transitions length len transitions for in range length transitions copy tf reverse tf reverse mirrorAction "h" transitions append copy tf reverse tf reverse mirrorAction "v" transitions append copy tf reverse tf reverse mirrorAction "h" transitions append
452	print len memory augmentData memory print len memory
453	labels sizes len train_fns len test_fns explode fig ax plt subplots figsize ax pie sizes explode explode labels labels autopct '%1.1f%%' shadow True startangle ax axis 'equal' ax set_title plt show
454	print len train_df unique print len train_df train_df unique
455	def get_img_size train True if train path TRAIN_PATH else path TEST_PATH widths heights imgs sorted glob path '*.jpg' max_img Image open imgs min_img Image open imgs for img in range len imgs image Image open imgs width height image size if len widths if width max widths max_img image if width min widths min_img image widths append width heights append height return widths heights max_img min_img
456	def get_labels image_id im_df train_df train_df image_id fillna im_df im_df im_df groupby count index im_df index all_labels labels for label in all_labels if label in index labels labels label return labels def plot_training_images width height images sorted glob TRAIN_PATH '*.jpg' fig axs plt subplots height width figsize width height rnd_indices rnd_indices np random choice range len images for in range height width for im in range height width image Image open images rnd_indices im im width im width axs imshow image axs axis 'off' axs set_title get_labels images rnd_indices im split plt suptitle plt show
457	def rle_to_mask rle_string width height rows cols height width if rle_string return np zeros height width else rle_numbers int num_string for num_string in rle_string split rle_pairs np array rle_numbers reshape img np zeros rows cols dtype np uint8 for index length in rle_pairs index img index index length img img reshape cols rows img img return img
458	def plot_training_images_and_masks width height images sorted glob TRAIN_PATH '*.jpg' fig axs plt subplots height width figsize rnd_indices rnd_indices np random choice range len images for in range height width for im in range height width image Image open images rnd_indices im image draw_segmentation_maps images rnd_indices im im width im width axs imshow image axs axis 'off' axs set_title get_labels images rnd_indices im split plt suptitle plt show
459	from tensorflow keras applications inception_resnet_v2 import def get_model base_model model backend tf keras backend layers tf keras layers weights 'imagenet' models tf keras models utils tf keras utils base_model output y_pred Dense activation 'sigmoid' return Model inputs base_model input outputs y_pred model get_model
460	def plot_with_dots ax np_array ax scatter list range len np_array np_array ax plot list range len np_array np_array
461	store fillna inplace True store fillna store mode inplace True store fillna store mode inplace True
462	size for img in hair_images image cv2 imread BASE_PATH '/jpeg/train/' img '.jpg' image_resize cv2 resize image size size image_resize cv2 cvtColor image_resize cv2 COLOR_BGR2RGB plt imshow image_resize plt show
463	rate train "species" value_counts sort_values print f'{"Target" :-<40} {"rate":-<20}' for in range len rate print f'{rate.index[n] :-<40} {rate[n]}'
464	longitude pd to_numeric train 'longitude' errors 'coerce' latitude pd to_numeric train 'latitude' errors 'coerce' df pd concat longitude latitude axis
465	ebird_code_simple sample list train "ebird_code" unique AudioProcessing PlotSampleWave nrows captions ebird_code_simple df train
466	categorical_list numerical_list for in application columns tolist if application dtype 'object' categorical_list append else numerical_list append print str len categorical_list print str len numerical_list
467	application drop 'SK_ID_CURR' 'TARGET' axis application TARGET feature_name columns tolist
468	PATH '/kaggle/input/covid19-global-forecasting-week-4/' train_df pd read_csv PATH 'train.csv' parse_dates test_df pd read_csv PATH 'test.csv' parse_dates add_datepart train_df drop False add_datepart test_df drop False
469	idx_group def day_reached_cases df name no_cases gb df df no_cases groupby idx_group return gb Dayofyear first reset_index rename columns name def area_fatality_rate df gb df df groupby idx_group res_df gb Fatalities last gb ConfirmedCases last reset_index return res_df rename columns
470	def with_new_features df train True res df copy add_datepart res 'quarantine' prefix 'qua' add_datepart res 'schools' prefix 'sql' res res res res res res res res res res res res if train res return res train_df with_new_features train_df test_df with_new_features test_df train False
471	df1 df copy df1 np log df1 df1 np log df1
472	to_tst to new test_df to_tst process to_tst all_cols head
473	import seaborn as sns min_date test_df Date min max_date train_df Date max axes plt subplots figsize def plot_preds country ax targets train_df train_df country train_df min_date ConfirmedCases subset test_df test_df country test_df max_date idx subset index dates subset Date predicted submit iloc idx ConfirmedCases targets index dates predicted index dates combined pd DataFrame 'real' targets 'pred' predicted sns lineplot data combined ax axes ax set_title country plot_preds plot_preds plot_preds plot_preds plot_preds plot_preds plot_preds plot_preds plot_preds plot_preds
474	import pandas as pd pd set_option 'display.max_columns' None import numpy as np import matplotlib pyplot as plt import seaborn as sns color sns color_palette import gc import warnings import time warnings filterwarnings "ignore"
475	print '------------main------------' print 'application_train:' application_train shape "rows and" application_train shape 'columns' print 'application_test:' application_test shape "rows and" application_test shape 'columns' print print '------------others------------' print shape "rows and" shape 'columns' print 'bureau:' bureau shape "rows and" bureau shape 'columns' print 'bureau_balance:' bureau_balance shape "rows and" bureau_balance shape 'columns' print 'previous_application:' previous_application shape "rows and" previous_application shape 'columns' print 'installments_payments:' installments_payments shape "rows and" installments_payments shape 'columns' print 'credit_card_balance:' credit_card_balance shape "rows and" credit_card_balance shape 'columns'
476	def plot_categorical data col size xlabel_angle title plotdata data col value_counts plt figure figsize size sns barplot plotdata index plotdata values plt title title if xlabel_angle plt xticks rotation xlabel_angle plt show plot_categorical data application_train col 'TARGET' size xlabel_angle title 'train set: label'
477	def plot_categorical_bylabel data col size xlabel_angle title plt figure figsize size l1 data loc data TARGET col value_counts l0 data loc data TARGET col value_counts plt subplot sns barplot l1 index l1 values plt title title plt xticks rotation xlabel_angle plt subplot sns barplot l0 index l0 values plt title title plt xticks rotation xlabel_angle plt show plot_categorical_bylabel application_train 'CODE_GENDER' title
478	def plot_numerical_bylabel data col size plt figure figsize size corr data 'TARGET' corr data col avg_repaid data ix data 'TARGET' col median avg_not_repaid data ix data 'TARGET' col median plt figure figsize sns kdeplot data ix data 'TARGET' col label 'TARGET == 0' sns kdeplot data ix data 'TARGET' col label 'TARGET == 1' plt xlabel col plt ylabel plt title col plt legend print col corr plot_numerical_bylabel application_train 'EXT_SOURCE_1'
479	from sklearn impute import SimpleImputer MICEImputer application_train pd read_csv '../input/application_train.csv' application_test pd read_csv '../input/application_test.csv'
480	def feature_type_split data special_list cat_list dis_num_list num_list for in data columns tolist if data dtype 'object' cat_list append elif data nunique dis_num_list append elif in special_list dis_num_list append else num_list append return cat_list dis_num_list num_list cat_list dis_num_list num_list feature_type_split application_train special_list 'AMT_REQ_CREDIT_BUREAU_YEAR'
481	import os import time import gc import warnings warnings filterwarnings "ignore" import json from pandas io json import json_normalize import numpy as np import pandas as pd import matplotlib pyplot as plt import seaborn as sns color sns color_palette from sklearn preprocessing import LabelEncoder from sklearn model_selection import KFold from sklearn metrics import mean_squared_error import lightgbm as lgb
482	def find_missing data count_missing data isnull sum values total data shape ratio_missing count_missing total return pd DataFrame data 'missing_count' count_missing 'missing_ratio' ratio_missing index data columns values train_missing find_missing train test_missing find_missing test
483	if test fullVisitorId nunique len sub print else print
484	data_path Path '/kaggle/input/abstraction-and-reasoning-challenge/' training_path data_path 'training' evaluation_path data_path 'evaluation' test_path data_path 'test' training_tasks sorted os listdir training_path eval_tasks sorted os listdir evaluation_path
485	evaluation_examples for in range task Evals basic_task Create task Function basic_task if and task 'test' 'output' plot_picture plot_task task print evaluation_examples append
486	geometry Geometry SIZE_X SIZE_Y dist geometry dist get_prox geometry get_prox pos_towards geometry pos_towards int_to_pos geometry int_to_pos _diff_to geometry _diff_to dataclass class ClosestDist idx int pos Pos dist int def find_closest pos Pos dest_poses Iterable Pos Optional ClosestDist dists dest_pos dist pos dest_pos for dest_pos in enumerate dest_poses if not dists return None closest min dists key itemgetter return ClosestDist closest def is_unique elems Iterable cnts Counter elems if not cnts return True return cnts most_common
487	strategies Dict Id Strategy obs Observation None mine_scores Dict Pos MineScore def agent raw_obs Dict str str obs Observation from_obs raw_obs update_new_state obs assert obs my_ships keys obs my_shipyards keys strategies keys obs my_ships obs my_shipyards strategies bot_plans list chain from_iterable bot make_plans num len strategies for bot in strategies values best_plans solve_assign bot_plans possible_actions list chain from_iterable plan actions for plan in best_plans best_actions solve_assign possible_actions for action in best_actions strategies action id notify_action action halite_actions for action in best_actions halite_actions update action halite_command return halite_actions
488	from kaggle_environments import evaluate make import random import numpy random seed env make "halite" debug True trainer env train None "random" obs_ trainer reset while not env done player_info obs_ "players" if player_info player_info key "sy" val for key val in player_info items action agent obs_ obs_ reward done info trainer step action env render mode "ipython" width height
489	import torch from detectron2 config import get_cfg from detectron2 engine import DefaultPredictor if torch cuda is_available device torch device "cuda:{}" format else device torch device "cpu" print cfg get_cfg cfg merge_from_file cfg MODEL DEVICE str device cfg MODEL RPN NMS_THRESH cfg MODEL ROI_HEADS SCORE_THRESH_TEST cfg MODEL WEIGHTS "../input/parameters/model_final_f10217.pkl" model DefaultPredictor cfg
490	import numpy as np mask torch stack mask mask mask dim mask mask cpu numpy astype "uint8" instances cv2 multiply image mask plt imshow instances plt show
491	import os import cv2 import pdb import glob import argparse import numpy as np
492	import nltk import re from nltk corpus import stopwords from nltk stem porter import PorterStemmer
493	voc_size onehot_repr one_hot words voc_size for words in corpus onehot_repr
494	fig ax plt subplots figsize ax plot train index values train quaketime values ax set_xlabel ax set_ylabel ax set_title ax plot train index values np diff train quaketime values ax set_xlabel ax set_ylabel ax set_title ax plot train index values train quaketime values ax set_xlabel ax set_ylabel ax set_title
495	fig ax plt subplots figsize sns distplot train signal values ax ax color bins kde False ax set_xlabel ax set_ylabel ax set_title low train signal mean train signal std high train signal mean train signal std sns distplot train loc train signal low train signal high "signal" values ax ax color bins kde False ax set_xlabel ax set_ylabel ax set_title
496	from sklearn model_selection import TimeSeriesSplit cv TimeSeriesSplit n_splits
497	oof_qda1 oof_qda1 reshape oof_qda2 oof_qda2 reshape oof_gmm oof_gmm reshape oof_lr oof_lr reshape oof_ls oof_ls reshape oof_knn oof_knn reshape oof_nn oof_nn reshape oof_qda3 oof_qda3 reshape
498	fig ax plt subplots nrows ncols figsize spaceboy circle_crop '../input/train_images/1df0a4c23c95.png' ax imshow cv2 cvtColor spaceboy cv2 COLOR_BGR2RGB ax axis 'off' cropboy circle_crop '../input/train_images/0a1076183736.png' ax imshow cv2 cvtColor cropboy cv2 COLOR_BGR2RGB ax axis 'off' squareboy circle_crop '../input/train_images/0e3572b5884a.png' ax imshow cv2 cvtColor squareboy cv2 COLOR_BGR2RGB ax axis 'off' supercropboy circle_crop '../input/train_images/698d6e422a80.png' ax imshow cv2 cvtColor supercropboy cv2 COLOR_BGR2RGB ax axis 'off'
499	fig ax plt subplots nrows ncols figsize spaceboy circle_crop_v2 '../input/train_images/1df0a4c23c95.png' ax imshow cv2 cvtColor spaceboy cv2 COLOR_BGR2RGB ax axis 'off' cropboy circle_crop_v2 '../input/train_images/0a1076183736.png' ax imshow cv2 cvtColor cropboy cv2 COLOR_BGR2RGB ax axis 'off' squareboy circle_crop_v2 '../input/train_images/0e3572b5884a.png' ax imshow cv2 cvtColor squareboy cv2 COLOR_BGR2RGB ax axis 'off' supercropboy circle_crop_v2 '../input/train_images/698d6e422a80.png' ax imshow cv2 cvtColor supercropboy cv2 COLOR_BGR2RGB ax axis 'off'
500	df pd concat train 'id' 'comment_text' test axis del train test gc collect
501	MAX_NUM_WORDS TOXICITY_COLUMN 'target' TEXT_COLUMN 'comment_text' tokenizer Tokenizer num_words MAX_NUM_WORDS tokenizer fit_on_texts train_df TEXT_COLUMN MAX_SEQUENCE_LENGTH def pad_text texts tokenizer return pad_sequences tokenizer texts_to_sequences texts maxlen MAX_SEQUENCE_LENGTH
502	data pd concat train test np random seed data data iloc np random permutation len data data reset_index drop True inplace True data drop 'target' 'train_test' axis data train_test
503	fig_unigram_positive npt_positive bar_ngram title 'uni-gram' xaxis_label 'word_count' yaxis_label 'word' ngram top_n width height stopwords stopwords fig_unigram_neutral npt_neutral bar_ngram title 'uni-gram' xaxis_label 'word_count' yaxis_label 'word' ngram top_n width height stopwords stopwords fig_unigram_negative npt_negative bar_ngram title 'uni-gram' xaxis_label 'word_count' yaxis_label 'word' ngram top_n width height stopwords stopwords
504	npt word_distribution title 'number of words distribution'
505	import tensorflow as tf import numpy as np from keras import backend as from keras losses import binary_crossentropy def mean_iou y_true y_pred prec for in np arange y_pred_ tf to_int32 y_pred score up_opt tf metrics mean_iou y_true y_pred_ get_session run tf local_variables_initializer with tf control_dependencies up_opt score tf identity score prec append score return mean stack prec def dice_coef y_true y_pred smooth y_true_f flatten y_true y_pred_f flatten y_pred intersection sum y_true_f y_pred_f return intersection smooth sum y_true_f sum y_pred_f smooth def bce_dice_loss y_true y_pred return binary_crossentropy y_true y_pred dice_coef y_true y_pred
506	sns set train 'revenue' train 'popularity' plt figure figsize sns regplot plt xlabel 'popularity' plt ylabel 'revenue' plt title
507	fig ax plt subplots figsize plt subplot np random seed ax plt subplot projection '3d' size len train colors np random rand size xs np array train ys np array train 'budget' zs np array train 'revenue' ax scatter xs ys zs colors marker 'o' ax set_xlabel ax set_ylabel ax set_zlabel plt show
508	lgbmodel lgb LGBMRegressor n_estimators objective 'regression' metric 'rmse' max_depth num_leaves min_child_samples learning_rate boosting 'gbdt' min_data_in_leaf feature_fraction bagging_freq bagging_fraction importance_type 'gain' lambda_l1 bagging_seed random_seed subsample colsample_bytree use_best_model True
509	from sklearn metrics import mean_squared_error import lightgbm as lgb model lgb LGBMRegressor model fit xtrain ytrain print math sqrt mean_squared_error yval model predict xval
510	summ pd DataFrame 'data' 'train.csv' 'test.csv' 'sample_submission.csv' 'rows' len trainset len testset len sample_sub 'patient' trainset nunique testset nunique sample_sub nunique summ set_index 'data' inplace True display summ
511	fig px histogram trainset color marginal 'box' histnorm 'probability density' opacity fig update_layout title width height fig show
512	parti_patient trainset drop_duplicates subset fig px histogram parti_patient facet_row facet_col fig for_each_annotation lambda update text text replace fig update_layout title autosize True width height font_size fig show
513	age_range pd cut trainset np arange pivot_smkstat_sex pd pivot_table trainset index age_range aggfunc 'FVC' 'max' 'min' np mean np std 'max' 'min' np mean np std mean_of_fvc pivot_smkstat_sex 'FVC' 'mean' 'std' round reset_index mean_of_fvc mean_of_fvc sort_values mean_of_fvc mean_of_fvc astype str map mean_of_fvc mean_of_fvc rename columns "mean" 'std' display mean_of_fvc
514	fig px density_contour trainset 'FVC' marginal_x "histogram" marginal_y "histogram" color fig update_layout title width height fig show
515	base_image_dir os path join 'input/aptos2019-blindness-detection/' train_dir os path join base_image_dir 'train_images/' df pd read_csv os path join base_image_dir 'train.csv' df 'path' df 'id_code' map lambda os path join train_dir '{}.png' format df df drop columns 'id_code' df df sample frac reset_index drop True df head
516	import os import numpy as np import pandas as pd import lightgbm as lgb from sklearn model_selection import train_test_split from sklearn linear_model import LogisticRegression from sklearn metrics import classification_report
517	import numpy as np import pandas as pd import os import matplotlib pylab as plt import plotly import plotly graph_objs as go from plotly offline import download_plotlyjs init_notebook_mode plot iplot from sklearn linear_model import LinearRegression import datetime import colorlover as cl plt style use 'ggplot' color_pal 'color' for in plt rcParams 'axes.prop_cycle' df pd read_csv '../input/aptos2019-public-lb/publicleaderboarddata/aptos2019-blindness-detection-publicleaderboard.csv' df pd to_datetime df df df set_index unstack df columns name for name in df columns FIFTEENTH_SCORE df max sort_values ascending False FIFTYTH_SCORE df max sort_values ascending False TOP_SCORE df max sort_values ascending False
518	plt rcParams "font.size" ALL_TEAMS df columns values df ALL_TEAMS ffill plot figsize color color_pal legend False alpha ylim TOP_SCORE title df ffill max axis plot color color_pal label legend True plt show
519	plt rcParams "font.size" ax df ffill count axis plot figsize title color color_pal lw ax set_ylabel plt show
520	plt rcParams "font.size" TOP_TEAMS df max loc df max FIFTYTH_SCORE index values df TOP_TEAMS max sort_values ascending True plot kind 'barh' xlim TOP_SCORE FIFTYTH_SCORE title figsize color color_pal plt show
521	plt rcParams "font.size" df TOP_TEAMS nunique sort_values plot kind 'barh' figsize color color_pal title plt show
522	plt rcParams "font.size" n_days datetime date today datetime date days fig axes plt subplots n_days figsize sharex True plt subplots_adjust top bottom for in range n_days date2 df loc df index date datetime date datetime timedelta index max num_teams len df ffill loc date2 dropna max_cutoff df ffill loc date2 df ffill loc date2 loc max_cutoff plot kind 'hist' bins ax axes title format date2 date isoformat num_teams y_axis axes yaxis y_axis set_label_text y_axis label set_visible False
523	engineering_feats 'var_0' 'var_2' 'var_198' 'var_179' 'var_191' 'var_22' 'var_2' 'var_0' 'var_179' 'var_198' 'var_146' 'var_22' 'var_115' 'var_26' 'var_44' 'var_155' 'var_157' 'var_163' 'var_180' 'var_123' 'var_87' 'var_44' 'var_26' 'var_123' 'var_173' 'var_180' 'var_87' 'var_155' 'var_157' 'var_163' 'var_35' 'var_196' 'var_75' 'var_86' 'var_21' 'var_51' 'var_40' 'var_135' 'var_139' 'var_67' 'var_167' 'var_76' 'var_139' 'var_21' 'var_80' 'var_86' 'var_174' 'var_40' 'var_76' 'var_172' 'var_83' 'var_167' 'var_19' 'var_67' 'var_118'
524	df_e values astype np float32 df_e values astype np float32 print shape
525	all_roc all_cnt for fold_id in enumerate KFold n_splits random_state shuffle True split clf make_pipeline QuantileTransformer output_distribution 'normal' random_state GaussianNB clf fit clf predict_proba all_roc roc_auc_score all_cnt print all_roc all_cnt
526	for clzid in range len clz_attr_num if clz_attr_num clzid if not os path isfile MODEL_FILE_DIR "attrmodel_%d-%d.model" attr_image_size clzid model train_attr_net clzid torch save model state_dict MODEL_FILE_DIR "attrmodel_%d-%d.model" attr_image_size clzid
527	data_mask dict while len data_cache cid mask imag imgid data_cache pop mask ztop mask if imgid not in data_mask imag ztop imag data_mask imgid ptoz imag transpose astype np float32 np zeros attr_image_size dtype np int data_mask imgid mask cid
528	class MaskDataset object def __init__ self keys self keys keys def __getitem__ self idx self keys idx return ztop data_mask ztop data_mask def __len__ self return len self keys
529	class MaskDataset object def __init__ self folder self imgids split for in os listdir folder self folder folder def __getitem__ self idx imag cv2 imread self folder self imgids idx ".jpg" imag cv2 resize imag attr_image_size return imag transpose astype np float32 def __len__ self return len self imgids
530	predict_imgeid predict_imgeid for in set uses_index predict_mask predict_mask for in set uses_index predict_rle predict_rle for in set uses_index predict_classid predict_classid for in set uses_index predict_attr predict_attr for in set uses_index predict_attri_str predict_attri_str for in set uses_index
531	def seed_everything seed random seed seed os environ 'PYTHONHASHSEED' str seed np random seed seed torch manual_seed seed torch cuda manual_seed seed torch backends cudnn deterministic True
532	base_image_dir os path join 'input/aptos2019-blindness-detection/' train_dir os path join base_image_dir 'train_images/' df pd read_csv os path join base_image_dir 'train.csv' df 'path' df 'id_code' map lambda os path join train_dir '{}.png' format df df drop columns 'id_code' df df sample frac reset_index drop True df head
533	idx im cl learn data dl DatasetType Valid dataset idx cl int cl im show title f"pred. class: {interp.pred_class[idx]}, actual class: {learn.data.classes[cl]}"
534	import os import random import pandas as pd import numpy as np import glob import matplotlib pyplot as plt import cv2 import IPython display as ipd import librosa from albumentations core transforms_interface import DualTransform BasicTransform
535	class TimeShifting AudioTransform def __init__ self always_apply False super TimeShifting self __init__ always_apply def apply self data params start_ int np random uniform if start_ audio_time_shift np r_ data start_ np random uniform start_ else audio_time_shift np r_ np random uniform start_ data start_ return audio_time_shift
536	class SpeedTuning AudioTransform def __init__ self always_apply False speed_rate None super SpeedTuning self __init__ always_apply if speed_rate self speed_rate speed_rate else self speed_rate np random uniform def apply self data params audio_speed_tune cv2 resize data int len data self speed_rate squeeze if len audio_speed_tune len data pad_len len data len audio_speed_tune audio_speed_tune np r_ np random uniform int pad_len audio_speed_tune np random uniform int np ceil pad_len else cut_len len audio_speed_tune len data audio_speed_tune audio_speed_tune int cut_len int cut_len len data return audio_speed_tune
537	class StretchAudio AudioTransform def __init__ self always_apply False rate None super StretchAudio self __init__ always_apply if rate self rate rate else self rate np random uniform def apply self data params input_length len data data librosa effects time_stretch data self rate if len data input_length data data input_length else data np pad data max input_length len data "constant" return data
538	class PitchShift AudioTransform def __init__ self always_apply False n_steps None super PitchShift self __init__ always_apply self n_steps n_steps def apply self data params return librosa effects pitch_shift data sr n_steps self n_steps
539	class AddGaussianNoise AudioTransform def __init__ self always_apply False super AddGaussianNoise self __init__ always_apply def apply self data params noise np random randn len data data_wn data noise return data_wn
540	class AddCustomNoise AudioTransform def __init__ self file_dir always_apply False super AddCustomNoise self __init__ always_apply self noise_files glob glob file_dir def apply self data params nf self noise_files int np random uniform len self noise_files noise librosa load nf if len noise len data start_ np random randint len noise len data noise noise start_ start_ len data else noise np pad noise len data len noise "constant" data_wn data noise return data_wn
541	import albumentations def get_train_transforms return albumentations Compose TimeShifting albumentations OneOf AddCustomNoise file_dir '../input/freesound-audio-tagging/audio_train' SpeedTuning AddGaussianNoise PitchShift n_steps Gain PolarityInversion StretchAudio
542	def roc_auc predictions target fpr tpr thresholds metrics roc_curve target predictions roc_auc metrics auc fpr tpr return roc_auc
543	with strategy scope model Sequential model add Embedding len word_index weights embedding_matrix input_length max_len trainable False model add Bidirectional LSTM dropout recurrent_dropout model add Dense activation 'sigmoid' model compile loss 'binary_crossentropy' optimizer 'adam' metrics 'accuracy' model summary
544	import os import tensorflow as tf from tensorflow keras layers import Dense Input from tensorflow keras optimizers import Adam from tensorflow keras models import Model from tensorflow keras callbacks import ModelCheckpoint from kaggle_datasets import KaggleDatasets import transformers from tokenizers import BertWordPieceTokenizer
545	tokenizer transformers DistilBertTokenizer from_pretrained 'distilbert-base-multilingual-cased' tokenizer save_pretrained fast_tokenizer BertWordPieceTokenizer 'vocab.txt' lowercase False fast_tokenizer
546	import matplotlib def display_masks slides ax plt subplots figsize for slide in enumerate slides mask openslide OpenSlide os path join mask_dir f'{slide}_mask.tiff' mask_data mask read_region mask level_count mask level_dimensions cmap matplotlib colors ListedColormap 'black' 'gray' 'green' 'yellow' 'orange' 'red' ax imshow np asarray mask_data cmap cmap interpolation 'nearest' vmin vmax mask close ax axis 'off' image_id slide data_provider train loc slide 'data_provider' isup_grade train loc slide 'isup_grade' gleason_score train loc slide 'gleason_score' ax set_title f"ID: {image_id}\nSource: {data_provider} ISUP: {isup_grade} Gleason: {gleason_score}" tight_layout plt show
547	l_in torch randn device xm xla_device linear torch nn Linear to xm xla_device l_out linear l_in print l_out
548	class config MAX_LEN TRAIN_BATCH_SIZE VALID_BATCH_SIZE EPOCHS MODEL_PATH "model.bin" TRAINING_FILE '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv' TOKENIZER transformers BertTokenizer from_pretrained 'bert-base-uncased' do_lower_case True
549	def train_fn data_loader model optimizer device scheduler epoch num_steps model train for bi in enumerate data_loader ids "ids" mask "mask" targets "targets" ids ids to device dtype torch long mask mask to device dtype torch long targets targets to device dtype torch float optimizer zero_grad outputs model ids ids mask mask loss loss_fn outputs targets loss backward xm optimizer_step optimizer barrier True if scheduler is not None scheduler step if bi print format epoch bi num_steps loss item
550	BATCH_SIZE EPOCHS LR seed patience device torch device 'cuda' FOLDS
551	df pd read_csv '../input/fe-using-only-competition-data-melanoma/melanoma_folds.csv' df drop 'image_id' 'stratify_group' 'center' 'diagnosis' 'benign_malignant' axis inplace True target 'target' unused_feat 'patient_id' 'fold' features col for col in df columns if col not in unused_feat target categorical_columns for col in df columns df dtypes object if col not in unused_feat print col df col nunique l_enc LabelEncoder df col l_enc fit_transform df col values output open f'{col}_encoder.pkl' 'wb' pickle dump l_enc output output close categorical_columns append col
552	class CustomTabnet nn Module def __init__ self input_dim output_dim n_d n_a n_steps gamma cat_idxs cat_dims cat_emb_dim n_independent n_shared momentum mask_type "sparsemax" super CustomTabnet self __init__ self tabnet TabNet input_dim input_dim output_dim output_dim n_d n_d n_a n_a n_steps n_steps gamma gamma cat_idxs cat_idxs cat_dims cat_dims cat_emb_dim cat_emb_dim n_independent n_independent n_shared n_shared momentum momentum mask_type "sparsemax" def forward self return self tabnet
553	class SoftMarginFocalLoss nn Module def __init__ self margin gamma super SoftMarginFocalLoss self __init__ self gamma gamma self margin margin self weight_pos self weight_neg def forward self inputs targets em np exp self margin log_pos logsigmoid inputs log_neg logsigmoid inputs log_prob targets log_pos targets log_neg prob torch exp log_prob margin torch log em em prob weight targets self weight_pos targets self weight_neg loss self margin weight prob self gamma log_prob loss loss mean return loss
554	def train_fn dataloader model criterion optimizer device scheduler epoch model train train_targets train_outputs for bi in enumerate dataloader features 'features' target 'target' features features to device dtype torch float target target to device dtype torch float optimizer zero_grad output model features loss criterion output target loss backward optimizer step if scheduler is not None scheduler step output softmax output dim cpu detach numpy train_targets extend target cpu detach numpy argmax axis astype int tolist train_outputs extend output return loss item train_outputs train_targets
555	def eval_fn data_loader model criterion device fin_targets fin_outputs model eval with torch no_grad for bi in enumerate data_loader features "features" target "target" features features to device dtype torch float target target to device dtype torch float outputs model features loss_eval criterion outputs target outputs softmax outputs dim cpu detach numpy fin_targets extend target cpu detach numpy argmax axis astype int tolist fin_outputs extend outputs return loss_eval item fin_outputs fin_targets
556	fig ax plt subplots nrows ncols figsize for in range cvimg sk imread cover_images_path uniimg sk imread jpodimg sk imread uerdimg sk imread ax imshow cvimg ax set_title train_imageids ax imshow uniimg ax set_title 'JNIWARD_IMG' train_imageids ax imshow jpodimg ax set_title train_imageids ax imshow uerdimg ax set_title 'UERD_IMG' train_imageids
557	labels 'greek' 'southern_us' 'filipino' 'indian' 'jamaican' 'spanish' 'italian' 'mexican' 'chinese' 'british' 'thai' 'vietnamese' 'cajun_creole' 'brazilian' 'french' 'japanese' 'irish' 'korean' 'moroccan' 'russian' templist for cus in labels lisofing for lis in train train 'cuisine' cus 'ingredients' for ing in lis lisofing append ing templist append cus len list set lisofing pd DataFrame templist columns 'cuisine' 'unique_ing' sort_values by 'unique_ing' ascending False
558	for ingredient expected in 'egg' 'all-purpose flour' 'all purpose flour' 'purée' 'puree' '1% low-fat milk' 'low fat milk' 'half & half' 'half half' 'safetida (powder)' 'safetida (powder)' actual preprocess ingredient assert actual expected f'"{expected}" is excpected but got "{actual}"'
559	merge gp merge gk on feature how 'left' sns lmplot "mean_download_delay_time" "download_rate" data merge plt title plt ylim plt xlim
560	train pd read_csv '/kaggle/input/tweet-sentiment-extraction/train.csv' test pd read_csv '/kaggle/input/tweet-sentiment-extraction/test.csv' ss pd read_csv '/kaggle/input/tweet-sentiment-extraction/sample_submission.csv'
561	def clean_text text text str text lower text re sub text text re sub text text re sub text text re sub '[%s]' re escape string punctuation text text re sub '\n' text text re sub '\w*\d\w*' text return text
562	train train 'sentiment' 'positive' train train 'sentiment' 'negative' train train 'sentiment' 'neutral'
563	df_train pd read_csv '/kaggle/input/tweet-sentiment-extraction/train.csv' df_test pd read_csv '/kaggle/input/tweet-sentiment-extraction/test.csv' df_submission pd read_csv '/kaggle/input/tweet-sentiment-extraction/sample_submission.csv'
564	sentiment 'positive' train_data get_training_data sentiment model_path get_model_out_path sentiment train train_data model_path n_iter model None
565	def predict_entities text model doc model text ent_array for ent in doc ents start text find ent text end start len ent text new_int start end ent label_ if new_int not in ent_array ent_array append start end ent label_ selected_text text ent_array ent_array if len ent_array else text return selected_text
566	train pd read_csv '../input/train.csv' index_col 'plaintext_id' test pd read_csv '../input/test.csv' index_col 'ciphertext_id' sub pd read_csv '../input/sample_submission.csv' index_col 'ciphertext_id'
567	plain_dict for p_id row in train iterrows text row 'text' plain_dict text p_id print len plain_dict
568	c_id index sub loc c_id index
569	from collections import Counter import matplotlib pyplot as plt plt rcParams "figure.figsize"
570	fullcipher3 join test3 "ciphertext" values dict_fullcipher3 Counter fullcipher3 split df_fullcipher3 pd DataFrame from_dict dict_fullcipher3 orient 'index' df_fullcipher3 df_fullcipher3 reset_index df_fullcipher3 columns "num" "nb" df_fullcipher3 sort_values "nb" ascending False inplace True print df_fullcipher3 shape df_fullcipher3 head
571	from xgboost import XGBRegressor report_cv XGBRegressor random_state random_seed
572	import os import gc import numpy as np import pandas as pd import seaborn as sns import matplotlib pyplot as plt import warnings warnings filterwarnings 'ignore'
573	sns distplot dipole_moments color 'mediumseagreen' plt title plt show sns distplot dipole_moments color 'seagreen' plt title plt show sns distplot dipole_moments color 'green' plt title plt show
574	plt figure figsize for col in enumerate typelist plt subplot sns distplot dipole_moments train 'type' col 'X' color 'orange' kde False sns distplot dipole_moments train 'type' col 'Y' color 'red' kde False sns distplot dipole_moments train 'type' col 'Z' color 'blue' kde False plt title col
575	plt figure figsize for col in enumerate typelist plt subplot sns distplot potential_energy train 'type' col 'potential_energy' color 'orangered' plt title col
576	def is_outlier points thresh if len points shape points points None median np median points axis diff np sum points median axis diff np sqrt diff med_abs_deviation np median diff modified_z_score diff med_abs_deviation return modified_z_score thresh
577	sns distplot mulliken_charges loc mulliken_charges atom_index mulliken_charge color 'blue' plt title plt show sns distplot mulliken_charges loc mulliken_charges atom_index mulliken_charge color 'darkblue' plt title plt show sns distplot mulliken_charges loc mulliken_charges atom_index mulliken_charge color 'blueviolet' plt title plt show sns distplot mulliken_charges loc mulliken_charges atom_index mulliken_charge color 'purple' plt title plt show sns distplot mulliken_charges loc mulliken_charges atom_index mulliken_charge color 'indigo' plt title plt show
578	import os import gc import cv2 import json import time import numpy as np import pandas as pd from pathlib import Path from keras utils import to_categorical import seaborn as sns import plotly express as px from matplotlib import colors import matplotlib pyplot as plt import plotly figure_factory as ff import torch torch Tensor import torch nn as nn from torch optim import Adam from torch utils data import Dataset DataLoader
579	test_task_files sorted os listdir TEST_PATH test_tasks for task_file in test_task_files with open str TEST_PATH task_file 'r' as task json load test_tasks append task
580	ys_train for task in test_tasks y_train for pair in task "test" append pair "input" for pair in task "train" append pair "input" y_train append pair "output" append append ys_train append y_train
581	means np mean for in matrices fig ff create_distplot means group_labels colors "green" fig update_layout title_text
582	def flattener pred str_pred str row for row in pred str_pred str_pred replace str_pred str_pred replace str_pred str_pred replace str_pred str_pred replace return str_pred
583	test_predictions list pred for pred in test_pred for test_pred in test_predictions for idx pred in enumerate test_predictions test_predictions idx flattener pred submission pd read_csv SUBMISSION_PATH submission "output" test_predictions
584	import os import gc import numpy as np import pandas as pd from tqdm import tqdm_notebook as tqdm import seaborn as sns from collections import Counter import matplotlib pyplot as plt from IPython display import SVG import warnings warnings filterwarnings 'ignore' import lightgbm import xgboost import catboost import keras from keras models import Model from keras utils vis_utils import model_to_dot from keras layers import Input Dense Dropout BatchNormalization from sklearn preprocessing import MinMaxScaler
585	DATA_PATH '../input/ieee-fraud-detection/' TRAIN_PATH DATA_PATH 'train_transaction.csv' TEST_PATH DATA_PATH 'test_transaction.csv'
586	fig ax plt subplots figsize plot sns countplot data train_df palette reversed 'aquamarine' 'mediumaquamarine' 'mediumseagreen' 'seagreen' 'darkgreen' set_title fontsize plt show plot
587	fig ax plt subplots figsize props train_df query query groupby value_counts normalize True unstack sns set_palette 'lightblue' 'darkblue' props plot kind 'bar' stacked ax ax set_ylabel plt show plot
588	fig ax plt subplots figsize props train_df query query groupby value_counts normalize True unstack sns set_palette 'pink' 'crimson' props plot kind 'bar' stacked ax ax set_ylabel plt show plot
589	fig ax plt subplots figsize plot sns countplot "card4" data train_df query palette reversed 'orangered' 'darkorange' 'orange' 'peachpuff' 'navajowhite' set_title 'card4' fontsize plt show plot
590	fig ax plt subplots figsize props train_df query groupby "card4" value_counts normalize True unstack sns set_palette 'peachpuff' 'darkorange' props plot kind 'bar' stacked ax ax set_ylabel plt show plot
591	fig ax plt subplots figsize plot sns countplot "card6" data train_df query query "card6 == 'credit' or card6 == 'debit'" palette reversed 'red' 'crimson' 'mediumvioletred' 'darkmagenta' 'indigo' set_title 'card6' fontsize plt show plot
592	fig ax plt subplots figsize props train_df query query "card6 == 'credit' or card6 == 'debit'" groupby "card6" value_counts normalize True unstack sns set_palette 'plum' 'purple' props plot kind 'bar' stacked ax ax set_ylabel plt show plot
593	cat_cols 'id_12' 'id_13' 'id_14' 'id_15' 'id_16' 'id_17' 'id_18' 'id_19' 'id_20' 'id_21' 'id_22' 'id_23' 'id_24' 'id_25' 'id_26' 'id_27' 'id_28' 'id_29' 'id_30' 'id_31' 'id_32' 'id_33' 'id_34' 'id_35' 'id_36' 'id_37' 'id_38' 'card4' 'card6' 'M4' 'card1' 'card2' 'card3' 'card5' 'addr1' 'addr2' 'M1' 'M2' 'M3' 'M5' 'M6' 'M7' 'M8' 'M9' cat_cols col for col in cat_cols if col in train_df columns
594	def prepare_data df cat_cols cat_cols cat_cols col for col in cat_cols if col in df columns for col in tqdm cat_cols df col pd factorize df col return df
595	train_data sort_values drop axis train_data sort_values del train_data
596	parameters 'application' 'binary' 'objective' 'binary' 'metric' 'auc' 'is_unbalance' 'true' 'boosting' 'gbdt' 'num_leaves' 'feature_fraction' 'bagging_fraction' 'bagging_freq' 'learning_rate' 'verbose' train_data lightgbm Dataset label y_train categorical_feature cat_cols val_data lightgbm Dataset label y_val model lightgbm train parameters train_data valid_sets val_data num_boost_round early_stopping_rounds
597	plt rcParams "axes.titlesize" plt rcParams "axes.labelsize" plt rcParams "xtick.labelsize" plt rcParams "ytick.labelsize" plot lightgbm plot_importance model max_num_features figsize grid False color sns color_palette "husl" plt show plot
598	fig ax plt subplots figsize plt plot history history 'acc' color 'blue' plt plot history history 'val_acc' color 'orangered' plt title plt ylabel plt xlabel plt legend loc 'upper left' plt show
599	fig ax plt subplots figsize plt plot history history 'loss' color 'blue' plt plot history history 'val_loss' color 'orangered' plt title plt ylabel plt xlabel plt legend loc 'upper left' plt show
600	TEXT_COL 'comment_text' EMB_PATH MAXLEN ENDLEN MAX_FEATURES EMBED_SIZE BATCH_SIZE NUM_EPOCHS
601	lengths train_df TEXT_COL apply len train_df 'lengths' lengths lengths train_df loc train_df 'lengths' 'lengths' sns distplot lengths color 'r' plt show
602	words train_df TEXT_COL apply lambda len len join split train_df 'words' words words train_df loc train_df 'words' 'words' sns distplot words color 'g' plt show
603	avg_word_len train_df TEXT_COL apply lambda len join split len len join split train_df 'avg_word_len' avg_word_len avg_word_len train_df loc train_df 'avg_word_len' 'avg_word_len' sns distplot avg_word_len color 'b' plt show
604	tokenizer Tokenizer num_words MAX_FEATURES lower True tokenizer fit_on_texts list train_df TEXT_COL list test_df TEXT_COL word_index tokenizer word_index
605	def squash axis s_squared_norm sum square axis keepdims True epsilon scale sqrt s_squared_norm s_squared_norm return scale
606	def get_model inp Input shape MAXLEN embed_inp Embedding EMBED_SIZE input_length MAXLEN trainable False inp drop_inp embed_inp bi_lstm Bidirectional CuDNNLSTM return_sequences True drop_inp max_pool_lstm bi_lstm attention_lstm AttentionWeightedAverage bi_lstm capsule Capsule num_capsule dim_capsule routings activation squash bi_lstm capsule Flatten capsule concatenate max_pool_lstm attention_lstm capsule axis outp Dense activation 'sigmoid' model Model inp outp model compile loss 'binary_crossentropy' optimizer Adam lr decay metrics 'acc' return model
607	with open 'word_index.json' 'w' as json dump word_index
608	import os import gc import pandas as pd import numpy as np from sklearn metrics import accuracy_score mean_absolute_error mean_squared_error import matplotlib pyplot as plt import seaborn as sns from langdetect import detect import markdown import json import requests import warnings import time from colorama import Fore Back Style init
609	train_df pd read_csv '../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv' comments train_df 'comment_text' targets train_df 'target' severe_toxicities train_df 'severe_toxicity' obscenities train_df 'obscene' del train_df gc collect
610	with open as google_api_key readline client Perspective google_api_key
611	print str accuracy_score np round targets len toxicity_scores np round toxicity_scores len toxicity_scores print str accuracy_score np round obscenities len toxicity_scores np round obscenity_scores len toxicity_scores print str accuracy_score np round severe_toxicities len toxicity_scores np round severe_toxicity_scores len toxicity_scores
612	print str mean_absolute_error targets len toxicity_scores toxicity_scores len toxicity_scores print str mean_absolute_error obscenities len toxicity_scores obscenity_scores len toxicity_scores print str mean_absolute_error severe_toxicities len toxicity_scores severe_toxicity_scores len toxicity_scores
613	print str mean_squared_error targets len toxicity_scores toxicity_scores len toxicity_scores print str mean_squared_error obscenities len toxicity_scores obscenity_scores len toxicity_scores print str mean_squared_error severe_toxicities len toxicity_scores severe_toxicity_scores len toxicity_scores
614	keys set train_df ebird_code values np arange len keys code_dict dict zip sorted keys values
615	def to_imagenet mean None std None norm_max None norm_min None eps 1e-6 mean mean or mean mean std std or std Xstd std eps _min _max Xstd min Xstd max norm_max norm_max or _max norm_min norm_min or _min if _max _min eps Xstd norm_min norm_min norm_max norm_max norm_min norm_max norm_min else np zeros_like Xstd dtype np uint8 return np stack axis
616	class BirdDataset Dataset def __init__ self df path self code_dict code_dict self classes len code_dict self df self path df path self dataset_length len df def __len__ self return self dataset_length def __getitem__ self file_name self df filename ebird_code self df ebird_code num_code self code_dict ebird_code default_signal np random random MAXLEN AMPLITUDE default_values SR np int32 np round default_signal values read self path ebird_code file_name data values if len values else default_values code to_categorical num_code num_classes self classes return to_tensor np nan_to_num get_melsp_img data np repeat code CHUNKS
617	class BirdNet nn Module def __init__ self super BirdNet self __init__ self dropout nn Dropout DROP self dense_output nn Linear self resnet resnet34 pretrained True self resnet_head list self resnet children self resnet_head nn Sequential self resnet_head def forward self self resnet_head return self dense_output self dropout view
618	len code_dict network BirdNet optimizer Adam 'params' network resnet parameters 'lr' LR 'params' network dense_output parameters 'lr' LR
619	def cel y_true y_pred y_true torch argmax y_true axis return nn CrossEntropyLoss y_pred y_true squeeze def accuracy y_true y_pred y_true torch argmax y_true axis squeeze y_pred torch argmax y_pred axis squeeze return y_true y_pred float sum len y_true
620	def get_time site start_time if site 'site_3' return None if site 'site_1' or site 'site_2' return int start_time TSR int start_time TSR class BirdTestDataset Dataset def __init__ self df path self df self path df path self dataset_length len df self normalize Normalize def __len__ self return self dataset_length def __getitem__ self site self df site start_time self df seconds get_time site start_time default_values TSR np zeros MAXLEN values read self path self df audio_id out values if len values else default_values return FloatTensor self normalize image get_melsp_img out 'image'
621	def softmax return np exp np sum np exp axis None def decision_fn probs probs np argsort probs MAX_OUTPUTS if np max probs MIN_THRESHOLD return condition probs np max probs probs THRESHOLD return np where np logical_or condition tolist
622	network eval test_preds test_set BirdTestDataset test_df TEST_AUDIO_PATH test_loader DataLoader test_set batch_size VAL_BATCH_SIZE if os path exists TEST_AUDIO_PATH for test_X in tqdm test_loader test_pred network forward test_X view to device test_preds extend softmax test_pred detach cpu numpy flatten
623	code_dict 'nocall' keys code_dict keys values code_dict values code_dict dict zip values keys if os path exists TEST_AUDIO_PATH test_preds np array test_preds test_preds test_preds reshape CHUNKS mean axis test_preds decision_fn test_pred for test_pred in test_preds test_preds code_dict pred for pred in test_pred for test_pred in test_preds
624	import os import gc import re import numpy as np import pandas as pd import nltk from nltk corpus import wordnet stopwords from nltk stem import WordNetLemmatizer from nltk stem porter import PorterStemmer from colorama import Fore Back Style
625	def example_cleaning_results function select_comments for comment in enumerate comments if comment function comment select_comments append comment if len select_comments break print f'{Style.DIM}' "EXAMPLE WORKING OF TEXT CLEANING FUNCTION" f'{Style.RESET_ALL}' print f'{Style.DIM}' f'{Style.RESET_ALL}' print for comment in select_comments print f'{Fore.YELLOW}{Style.DIM}' comment f'{Style.RESET_ALL}' '\n\n' 'CHANGES TO' '\n\n' f'{Fore.CYAN}{Style.DIM}' function comment f'{Style.RESET_ALL}' print print f'{Fore.WHITE}{Style.DIM}' f'{Style.RESET_ALL}'
626	def remove_numbers text text join for in text if not isdigit return text
627	def replace_multi_exclamation_mark text text re sub r"(\!)\1+" text return text def replace_multi_question_mark text text re sub r"(\?)\1+" text return text def replace_multi_stop_mark text text re sub r"(\.)\1+" text return text
628	contraction_patterns r'won\'t' 'will not' r'can\'t' 'cannot' r'i\'m' 'i am' r'ain\'t' 'is not' r'(\w+)\'ll' '\g<1> will' r'(\w+)n\'t' '\g<1> not' r'(\w+)\'ve' '\g<1> have' r'(\w+)\'s' '\g<1> is' r'(\w+)\'re' '\g<1> are' r'(\w+)\'d' '\g<1> would' r'&' 'and' r'dammit' 'damn it' r'dont' 'do not' r'wont' 'will not' def replace_contraction text patterns re compile regex repl for regex repl in contraction_patterns for pattern repl in patterns text count re subn pattern repl text return text
629	def replace word pos None antonyms set for syn in wordnet synsets word pos pos for lemma in syn lemmas for antonym in lemma antonyms antonyms add antonym name if len antonyms return antonyms pop else return None def replace_negations text len text words while word text if word 'not' and ant replace text if ant words append ant continue words append word return words def tokenize_and_replace_negations text tokens nltk word_tokenize text tokens replace_negations tokens text join tokens return text
630	def replace_elongated word repeat_regexp re compile r'(\w*)(\w)\2(\w*)' repl r'\1\2\3' if wordnet synsets word return word repl_word repeat_regexp sub repl word if repl_word word return replace_elongated repl_word else return repl_word def replace_elongated_words text finalTokens tokens nltk word_tokenize text for in tokens finalTokens append replace_elongated text join finalTokens return text
631	import os import gc import numpy as np import pandas as pd from tqdm import tqdm_notebook as tqdm import seaborn as sns from collections import Counter import matplotlib pyplot as plt from IPython display import SVG import warnings warnings filterwarnings 'ignore' import keras from keras models import Model from keras utils vis_utils import model_to_dot from keras layers import Input Dense Dropout BatchNormalization import tensorflow as tf from numpy random import seed seed from tensorflow import set_random_seed set_random_seed from sklearn preprocessing import MinMaxScaler from sklearn metrics import accuracy_score import plotly graph_objects as go
632	def get_neural_network inputs Input shape shape dense_1 Dense activation 'relu' inputs dense_2 Dense activation 'relu' dense_1 outputs Dense activation 'sigmoid' dense_2 model Model inputs inputs outputs outputs model compile loss 'binary_crossentropy' optimizer 'adam' metrics 'acc' return model model get_neural_network
633	split np int32 len split y_train np int32 split split y_val np int32 split
634	preds_one_val model_one predict preds_two_val model_two predict preds_one_train model_one predict preds_two_train model_two predict
635	import plotly graph_objects as go labels fig go Figure data go Bar name 'val' labels accuracy_score y_val np round preds_one_val accuracy_score y_val np round preds_two_val go Bar name 'train' labels accuracy_score y_train np round preds_one_train accuracy_score y_train np round preds_two_train fig update_layout barmode 'group' yaxis_title yaxis_type "log" fig show
636	preds_one_val model_one predict preds_two_val model_two predict preds_one_train model_one predict preds_two_train model_two predict
637	import plotly graph_objects as go labels fig go Figure data go Bar name 'val' labels accuracy_score y_val np round preds_one_val accuracy_score y_val np round preds_two_val go Bar name 'train' labels accuracy_score y_train np round preds_one_train accuracy_score y_train np round preds_two_train fig update_layout barmode 'group' yaxis_title yaxis_type "log" fig show
638	import os import numpy as np import pandas as pd from tqdm import tqdm tqdm pandas from nltk import word_tokenize pos_tag from collections import Counter import matplotlib pyplot as plt import seaborn as sns import warnings warnings filterwarnings 'ignore'
639	SIGNAL_LEN MIN_NUM MAX_NUM
640	acoustic_data seismic_signals acoustic_data time_to_failure seismic_signals time_to_failure data_len len seismic_signals del seismic_signals gc collect
641	signals targets for in range data_len SIGNAL_LEN min_lim SIGNAL_LEN max_lim min SIGNAL_LEN data_len signals append list acoustic_data min_lim max_lim targets append time_to_failure max_lim del acoustic_data del time_to_failure gc collect signals np array signals targets np array targets
642	def min_max_transfer ts min_value max_value range_needed ts_std ts min_value max_value min_value if range_needed return ts_std range_needed abs range_needed range_needed else return ts_std range_needed range_needed range_needed
643	def transform_ts ts n_dim min_max ts_std min_max_transfer ts min_value MIN_NUM max_value MAX_NUM bucket_size int SIGNAL_LEN n_dim new_ts for in range SIGNAL_LEN bucket_size ts_range ts_std bucket_size mean ts_range mean std ts_range std std_top mean std std_bot mean std percentil_calc ts_range quantile max_range ts_range quantile ts_range quantile relative_percentile percentil_calc mean new_ts append np concatenate np asarray mean std std_top std_bot max_range percentil_calc relative_percentile return np asarray new_ts
644	def prepare_data start end train pd DataFrame np transpose signals int start int end for id_measurement in tqdm train index int start int end transform_ts train id_measurement append np asarray return
645	def load_all total_size len signals for start end in int total_size prepare_data start end append load_all np concatenate
646	plot sns jointplot perm_entropies targets kind 'kde' color 'orangered' plot set_axis_labels 'perm_entropy' 'time_to_failure' fontsize plt show
647	plot sns jointplot perm_entropies targets kind 'reg' color 'orangered' plot set_axis_labels 'perm_entropy' 'time_to_failure' fontsize plt show
648	plot sns jointplot app_entropies targets kind 'kde' color 'magenta' plot set_axis_labels 'app_entropy' 'time_to_failure' fontsize plt show
649	plot sns jointplot app_entropies targets kind 'reg' color 'magenta' plot set_axis_labels 'app_entropy' 'time_to_failure' fontsize plt show
650	plot sns jointplot higuchi_fds targets kind 'kde' color 'crimson' plot set_axis_labels 'higuchi_fd' 'time_to_failure' fontsize plt show
651	def katz_fd np array dists np abs np ediff1d ll dists sum ln np log10 np divide ll dists mean aux_d np max np abs aux_d return np divide ln np add ln np log10 np divide ll
652	plot sns jointplot katz_fds targets kind 'reg' color 'forestgreen' plot set_axis_labels 'katz_fd' 'time_to_failure' fontsize plt show
653	import os import gc import numpy as np from numpy fft import import pandas as pd import matplotlib pyplot as plt import seaborn as sns import pywt from statsmodels robust import mad import scipy from scipy import signal from scipy signal import butter deconvolve import warnings warnings filterwarnings 'ignore'
654	acoustic_data seismic_signals acoustic_data time_to_failure seismic_signals time_to_failure data_len len seismic_signals del seismic_signals gc collect
655	def maddest axis None return np mean np absolute np mean axis axis
656	SIGNAL_LEN MIN_NUM MAX_NUM
657	acoustic_data seismic_signals acoustic_data time_to_failure seismic_signals time_to_failure data_len len seismic_signals del seismic_signals gc collect
658	signals targets for in range data_len SIGNAL_LEN min_lim SIGNAL_LEN max_lim min SIGNAL_LEN data_len signals append list acoustic_data min_lim max_lim targets append time_to_failure max_lim del acoustic_data del time_to_failure gc collect signals np array signals targets np array targets
659	def min_max_transfer ts min_value max_value range_needed ts_std ts min_value max_value min_value if range_needed return ts_std range_needed abs range_needed range_needed else return ts_std range_needed range_needed range_needed
660	def transform_ts ts n_dim min_max ts_std min_max_transfer ts min_value MIN_NUM max_value MAX_NUM bucket_size int SIGNAL_LEN n_dim new_ts for in range SIGNAL_LEN bucket_size ts_range ts_std bucket_size mean ts_range mean std ts_range std std_top mean std std_bot mean std percentil_calc ts_range quantile max_range ts_range quantile ts_range quantile relative_percentile percentil_calc mean new_ts append np concatenate np asarray mean std std_top std_bot max_range percentil_calc relative_percentile return np asarray new_ts
661	def prepare_data start end train pd DataFrame np transpose signals int start int end for id_measurement in tqdm train index int start int end transform_ts train id_measurement append np asarray return
662	def load_all total_size len signals for start end in int total_size prepare_data start end append load_all np concatenate
663	plot sns jointplot spectral_entropies targets kind 'kde' color 'blueviolet' plot set_axis_labels 'spectral_entropy' 'time_to_failure' fontsize plt show
664	plot sns jointplot spectral_entropies targets kind 'reg' color 'blueviolet' plot set_axis_labels 'spectral_entropy' 'time_to_failure' fontsize plt show
665	plot sns jointplot sample_entropies targets kind 'kde' color 'mediumvioletred' plot set_axis_labels 'sample_entropy' 'time_to_failure' fontsize plt show
666	plot sns jointplot sample_entropies targets kind 'reg' color 'mediumvioletred' plot set_axis_labels 'sample_entropy' 'time_to_failure' fontsize plt show
667	plot sns jointplot detrended_fluctuations targets kind 'kde' color 'mediumblue' plot set_axis_labels 'detrended_fluctuation' 'time_to_failure' fontsize plt show
668	plot sns jointplot detrended_fluctuations targets kind 'reg' color 'mediumblue' plot set_axis_labels 'detrended_fluctuation' 'time_to_failure' fontsize plt show
669	INPUT_DIR '../input/m5-forecasting-accuracy' calendar pd read_csv f'{INPUT_DIR}/calendar.csv' selling_prices pd read_csv f'{INPUT_DIR}/sell_prices.csv' sample_submission pd read_csv f'{INPUT_DIR}/sample_submission.csv' sales_train_val pd read_csv f'{INPUT_DIR}/sales_train_validation.csv'
670	fig ax plt subplots nrows ncols figsize ax plot x_1 color 'seagreen' marker 'o' ax set_title fontsize ax plot y_w1 color 'red' marker ax set_title fontsize ax plot x_2 color 'seagreen' marker 'o' ax set_title fontsize ax plot y_w2 color 'red' marker ax set_title fontsize ax plot x_3 color 'seagreen' marker 'o' ax set_title fontsize ax plot y_w3 color 'red' marker ax set_title fontsize plt show
671	fig ax plt subplots nrows ncols figsize ax plot x_1 color 'seagreen' marker 'o' ax set_title fontsize ax plot y_a1 color 'red' marker ax set_title fontsize ax plot x_2 color 'seagreen' marker 'o' ax set_title fontsize ax plot y_a2 color 'red' marker ax set_title fontsize ax plot x_3 color 'seagreen' marker 'o' ax set_title fontsize ax plot y_a3 color 'red' marker ax set_title fontsize plt show
672	past_sales sales_train_val set_index 'id' d_cols merge calendar set_index 'd' 'date' left_index True right_index True validate set_index 'date' store_list selling_prices 'store_id' unique means fig go Figure for in store_list store_items for in past_sales columns if in data past_sales store_items sum axis rolling mean means append np mean past_sales store_items sum axis fig add_trace go Scatter np arange len data data name fig update_layout yaxis_title xaxis_title title
673	greens "mediumaquamarine" "mediumseagreen" "seagreen" "green" store_list selling_prices 'store_id' unique fig go Figure means stores for in enumerate store_list if "ca" in or "CA" in store_items for in past_sales columns if in data past_sales store_items sum axis rolling mean means append np mean past_sales store_items sum axis stores append fig add_trace go Scatter np arange len data data name marker dict color greens fig update_layout yaxis_title xaxis_title title
674	purples "thistle" "violet" "purple" "indigo" store_list selling_prices 'store_id' unique fig go Figure means stores for in enumerate store_list if "wi" in or "WI" in store_items for in past_sales columns if in data past_sales store_items sum axis rolling mean means append np mean past_sales store_items sum axis stores append fig add_trace go Scatter np arange len data data name marker dict color purples len purples fig update_layout yaxis_title xaxis_title title
675	blues "skyblue" "dodgerblue" "darkblue" store_list selling_prices 'store_id' unique fig go Figure means stores for in enumerate store_list if "tx" in or "TX" in store_items for in past_sales columns if in data past_sales store_items sum axis rolling mean means append np mean past_sales store_items sum axis stores append fig add_trace go Scatter np arange len data data name marker dict color blues len blues fig update_layout yaxis_title xaxis_title title
676	error error_naive error_avg error_holt error_exponential error_arima error_prophet names "ARIMA" df pd DataFrame np transpose error names df columns px bar df color title
677	import os import gc import numpy as np import pandas as pd from tqdm import tqdm tqdm pandas from collections import Counter from operator import itemgetter import scipy import cv2 from cv2 import imread import matplotlib import matplotlib pyplot as plt import seaborn as sns
678	train_images image_dirs np take os listdir '../input/train' select_rows for image_dir in tqdm sorted image_dirs image imread '../input/train/' image_dir train_images append image del image gc collect train_images np array train_images
679	labels_df pd read_csv '../input/labels.csv' label_dict dict zip labels_df attribute_id labels_df attribute_name for key in label_dict if 'culture' in label_dict key label_dict key label_dict key if 'tag' in label_dict key label_dict key label_dict key
680	train_targets for targets in targets_df attribute_ids target targets split target list map lambda label_dict int target train_targets append target train_targets np array train_targets
681	fig ax plt subplots nrows ncols figsize count for in range for in range ax imshow cv2 cvtColor train_images count cv2 COLOR_BGR2RGB ax set_title str train_targets count fontsize count count
682	FOLDS EPOCHS RRC FLIP NORM ROTATE LR 1e-4 1e-3 MODEL_SAVE_PATH "resnet_model" WIDTH HEIGHT BATCH_SIZE VAL_BATCH_SIZE DATA_PATH '../input/prostate-cancer-grade-assessment/' RESIZED_PATH '../input/panda-resized-train-data-512x512/train_images/'
683	test_df pd read_csv TEST_DATA_PATH train_df pd read_csv TRAIN_DATA_PATH sample_submission pd read_csv SAMPLE_SUB_PATH
684	gleason_replace_dict def process_gleason gleason if gleason 'negative' gs else gs tuple gleason split return gleason_replace_dict int for in gs train_df gleason_score train_df gleason_score apply process_gleason
685	def display_images num sq_num np sqrt num assert sq_num int sq_num sq_num int sq_num fig ax plt subplots nrows sq_num ncols sq_num figsize for in range int sq_num for in range int sq_num idx sq_num path TRAIN_IMG_PATH train_df image_id idx path '.png' ax imshow cv2 imread path ax set_title format idx fontsize plt show
686	class ResNetDetector nn Module def __init__ self super ResNetDetector self __init__ self softmax nn Softmax dim self dense_1 nn Linear self dense_2 nn Linear self dense_3 nn Linear self resnet resnet34 pretrained True self resnet nn Sequential list self resnet children def forward self img feat self resnet img squeeze isup_logit self dense_1 feat gleason_logit_0 self dense_2 feat gleason_logit_1 self dense_3 feat isup_prob self softmax isup_logit gleason_prob_0 self softmax gleason_logit_0 gleason_prob_1 self softmax gleason_logit_1 return torch cat isup_prob gleason_prob_0 gleason_prob_1 axis
687	model ResNetDetector torch randn requires_grad_ True model make_dot params dict list model named_parameters 'x'
688	def cel inp targ labels targ max dim return nn CrossEntropyLoss inp labels def acc inp targ inp_idx inp max axis indices targ_idx targ max axis indices return inp_idx targ_idx float sum axis len inp_idx
689	def panda_cel inp targ isup_loss cel inp targ gleason_loss_0 cel inp targ gleason_loss_1 cel inp targ return isup_loss gleason_loss_0 gleason_loss_1 isup_loss gleason_loss_0 gleason_loss_1 def panda_acc inp targ isup_accuracy acc inp targ gleason_accuracy_0 acc inp targ gleason_accuracy_1 acc inp targ return np array isup_accuracy gleason_accuracy_0 gleason_accuracy_1
690	def print_metric data fold start end metric typ Fore RESET "ISUP" "G-0" "G-1" time np round end start time format time Fore CYAN Fore YELLOW Fore MAGENTA tick Fore GREEN '\u2714' Fore RESET prefix "FOLD {} " format fold tick string prefix for idx in range value np round data idx detach cpu numpy typ idx metric idx np round value Fore RESET string string format print string time
691	EPOCHS BATCH_SIZE DATA_PATH '../input/nfl-big-data-bowl-2020/'
692	data train_df sample frac quantile data quantile data data loc data quantile plot sns jointplot data "X" data kind 'kde' color 'forestgreen' height plot set_axis_labels fontsize plt show plot
693	data train_df sample frac quantile data quantile data data loc data quantile plot sns jointplot data "Y" data kind 'kde' color height plot set_axis_labels fontsize plt show plot
694	data train_df sample frac plot sns jointplot data "X" data "Y" kind 'kde' color 'mediumvioletred' height plot set_axis_labels fontsize plt show plot
695	fig ff create_distplot hist_data train_df sample frac "S" group_labels "S" colors 'rgb(230, 0, 191)' fig update_layout title "S" yaxis dict title xaxis dict title "S" fig show
696	cat_cols value_dicts for feature in cat_cols values set train_df feature value_dicts append dict zip values np arange len values
697	def indices data feat_index value_dict value_dicts feat_index return data cat_cols feat_index apply lambda value_dict def one_hot indices feat_index return to_categorical indices num_classes len value_dicts feat_index
698	num_cols 'X' 'S' 'A' def get_numerical_features sample return sample num_cols values
699	hl_graph hl build_graph torch zeros hl_graph theme hl graph THEMES "blue" copy hl_graph
700	mean std nb_samples for data in tqdm train_loader batch_samples data size data data view batch_samples data size mean data mean std data std nb_samples batch_samples mean nb_samples std nb_samples
701	def nonan if type str return replace "\n" else return text join nonan abstract for abstract in train_data "comment_text" wordcloud WordCloud max_font_size None background_color 'black' collocations False width height generate text fig px imshow wordcloud fig update_layout title_text
702	def new_len if type is str return len split else return train_data "comment_words" train_data "comment_text" apply new_len nums train_data query "comment_words != 0 and comment_words < 200" sample frac "comment_words" fig ff create_distplot hist_data nums group_labels colors "coral" fig update_layout title_text xaxis_title template "simple_white" showlegend False fig show
703	df pd DataFrame np transpose lang_list train_data groupby "lang" mean "comment_words" df columns df df apply float df df query fig go Figure go Bar df df fig update_layout xaxis_title yaxis_title title_text template "plotly_white" fig show
704	df "country" df apply get_country df df query fig px choropleth df locations "country" hover_name "country" projection "natural earth" locationmode "country names" title color template "plotly" color_continuous_scale "aggrnyl" fig
705	fig go Figure go Histogram pols "compound" for pols in train_data "polarity" if pols "compound" marker dict color 'orchid' fig update_layout xaxis_title title_text template "simple_white" fig show
706	train_data "compound" train_data "polarity" apply lambda "compound" df pd DataFrame np transpose lang_list train_data groupby "lang" mean "compound" tolist df columns df df apply float df df query df "country" df apply get_country df df query fig px choropleth df locations "country" hover_name "country" projection "natural earth" locationmode "country names" title color template "plotly" color_continuous_scale "purples" fig show
707	nums_1 train_data sample frac query "toxic == 1" "compound" nums_2 train_data sample frac query "toxic == 0" "compound" fig ff create_distplot hist_data nums_1 nums_2 group_labels colors "darkorange" "dodgerblue" show_hist False fig update_layout title_text xaxis_title template "simple_white" fig show
708	fig go Figure go Histogram train_data query "flesch_reading_ease > 0" "flesch_reading_ease" marker dict color 'darkorange' fig update_layout xaxis_title title_text template "simple_white" fig show
709	df pd DataFrame np transpose lang_list train_data groupby "lang" mean "flesch_reading_ease" tolist df columns "flesch_reading_ease" df "flesch_reading_ease" df "flesch_reading_ease" apply float df df query "flesch_reading_ease > 0" df "country" df apply get_country df df query fig px choropleth df locations "country" hover_name "country" projection "natural earth" locationmode "country names" title color "flesch_reading_ease" template "plotly" color_continuous_scale "oranges" fig show
710	nums_1 train_data sample frac query "toxic == 1" "flesch_reading_ease" nums_2 train_data sample frac query "toxic == 0" "flesch_reading_ease" fig ff create_distplot hist_data nums_1 nums_2 group_labels colors "darkorange" "dodgerblue" show_hist False fig update_layout title_text xaxis_title template "simple_white" fig show
711	fig go Figure go Histogram train_data query "automated_readability < 100" "automated_readability" marker dict color 'mediumaquamarine' fig update_layout xaxis_title title_text template "simple_white" fig show
712	df pd DataFrame np transpose lang_list train_data groupby "lang" mean "automated_readability" tolist df columns "automated_readability" df "automated_readability" df "automated_readability" apply float df df query "automated_readability < 100" df "country" df apply get_country df df query fig px choropleth df locations "country" hover_name "country" projection "natural earth" locationmode "country names" title color "automated_readability" template "plotly" color_continuous_scale fig show
713	nums_1 train_data sample frac query "toxic == 1" "automated_readability" nums_2 train_data sample frac query "toxic == 0" "automated_readability" fig ff create_distplot hist_data nums_1 nums_2 group_labels colors "darkorange" "dodgerblue" show_hist False fig update_layout title_text xaxis_title template "simple_white" fig show
714	fig go Figure data go Pie labels train_data columns values train_data iloc sum values marker dict colors px colors qualitative Plotly fig update_traces textposition 'outside' textfont dict color "black" fig update_layout title_text fig show
715	fig go Figure data go Bar train_data columns train_data iloc sum values marker dict color px colors qualitative Plotly fig data marker line color 'rgb(0, 0, 0)' fig data marker line width fig update_traces orientation "h" fig update_layout title_text template "plotly_white" fig show
716	AUTO tf data experimental AUTOTUNE tpu tf distribute cluster_resolver TPUClusterResolver tf config experimental_connect_to_cluster tpu tf tpu experimental initialize_tpu_system tpu strategy tf distribute experimental TPUStrategy tpu GCS_DS_PATH KaggleDatasets get_gcs_path 'jigsaw-multilingual-toxic-comment-classification' EPOCHS BATCH_SIZE strategy num_replicas_in_sync
717	tokenizer transformers DistilBertTokenizer from_pretrained 'distilbert-base-multilingual-cased' save_path '/kaggle/working/distilbert_base_uncased/' if not os path exists save_path os makedirs save_path tokenizer save_pretrained save_path fast_tokenizer BertWordPieceTokenizer 'distilbert_base_uncased/vocab.txt' lowercase True
718	x_train fast_encode train comment_text astype str fast_tokenizer maxlen x_valid fast_encode val_data comment_text astype str values fast_tokenizer maxlen x_test fast_encode test_data content astype str values fast_tokenizer maxlen y_valid val toxic values y_train train toxic values
719	train_dataset tf data Dataset from_tensor_slices x_train y_train repeat shuffle batch BATCH_SIZE prefetch AUTO valid_dataset tf data Dataset from_tensor_slices x_valid y_valid batch BATCH_SIZE cache prefetch AUTO test_dataset tf data Dataset from_tensor_slices x_test batch BATCH_SIZE
720	def build_vnn_model transformer max_len input_word_ids Input shape max_len dtype tf int32 name "input_word_ids" embed transformer weights numpy embedding Embedding np shape embed np shape embed input_length max_len weights embed trainable False input_word_ids conc sum embedding axis conc Dense activation 'relu' conc conc Dense activation 'sigmoid' conc model Model inputs input_word_ids outputs conc model compile Adam lr loss 'binary_crossentropy' metrics 'accuracy' return model
721	with strategy scope transformer_layer transformers TFDistilBertModel from_pretrained 'distilbert-base-multilingual-cased' model_vnn build_vnn_model transformer_layer max_len model_vnn summary
722	def callback cb reduceLROnPlat ReduceLROnPlateau monitor 'val_loss' factor patience verbose mode 'auto' epsilon cooldown min_lr cb append reduceLROnPlat log CSVLogger 'log.csv' cb append log RocAuc RocAucEvaluation validation_data x_valid y_valid interval cb append RocAuc return cb
723	N_STEPS x_train shape BATCH_SIZE calls callback train_history model_vnn fit train_dataset steps_per_epoch N_STEPS validation_data valid_dataset callbacks calls epochs EPOCHS
724	def build_cnn_model transformer max_len input_word_ids Input shape max_len dtype tf int32 name "input_word_ids" embed transformer weights numpy embedding Embedding np shape embed np shape embed input_length max_len weights embed trainable False input_word_ids embedding embedding conv_1 embedding conv_2 embedding conv_3 embedding conv_4 embedding maxpool_1 conv_1 maxpool_2 conv_2 maxpool_3 conv_3 maxpool_4 conv_4 conc concatenate maxpool_1 maxpool_2 maxpool_3 maxpool_4 axis conc Dense activation 'relu' conc conc Dense activation 'sigmoid' conc model Model inputs input_word_ids outputs conc model compile Adam lr loss 'binary_crossentropy' metrics 'accuracy' return model
725	with strategy scope model_cnn build_cnn_model transformer_layer max_len model_cnn summary
726	train_history model_cnn fit train_dataset steps_per_epoch N_STEPS validation_data valid_dataset callbacks calls epochs EPOCHS
727	def build_lstm_model transformer max_len input_word_ids Input shape max_len dtype tf int32 name "input_word_ids" embed transformer weights numpy embedding Embedding np shape embed np shape embed input_length max_len weights embed trainable False input_word_ids embedding embedding lstm_1 LSTM return_sequences True embedding lstm_2 LSTM return_sequences True lstm_1 attention AttentionWeightedAverage lstm_2 conc Dense activation 'relu' attention conc Dense activation 'sigmoid' conc model Model inputs input_word_ids outputs conc model compile Adam lr loss 'binary_crossentropy' metrics 'accuracy' return model
728	with strategy scope model_lstm build_lstm_model transformer_layer max_len model_lstm summary
729	train_history model_lstm fit train_dataset steps_per_epoch N_STEPS validation_data valid_dataset callbacks calls epochs EPOCHS
730	def build_capsule_model transformer max_len input_word_ids Input shape max_len dtype tf int32 name "input_word_ids" embed transformer weights numpy embedding Embedding np shape embed np shape embed input_length max_len weights embed trainable False input_word_ids embedding embedding capsule Capsule num_capsule dim_capsule routings activation squash embedding capsule Flatten capsule output Dense activation 'relu' capsule output Dense activation 'sigmoid' output model Model inputs input_word_ids outputs output model compile Adam lr 1.5e-5 loss 'binary_crossentropy' metrics 'accuracy' return model
731	with strategy scope model_capsule build_capsule_model transformer_layer max_len model_capsule summary
732	train_history model_capsule fit train_dataset steps_per_epoch N_STEPS validation_data valid_dataset callbacks calls epochs EPOCHS
733	def build_distilbert_model transformer max_len input_word_ids Input shape max_len dtype tf int32 name "input_word_ids" sequence_output transformer input_word_ids cls_token sequence_output cls_token Dense activation "elu" cls_token cls_token Dropout cls_token out Dense activation 'sigmoid' cls_token model Model inputs input_word_ids outputs out model compile Adam lr 1.5e-5 loss 'binary_crossentropy' metrics 'accuracy' return model
734	with strategy scope model_distilbert build_distilbert_model transformer_layer max_len model_distilbert summary
735	train_history model_distilbert fit train_dataset steps_per_epoch N_STEPS validation_data valid_dataset callbacks calls epochs EPOCHS
736	EPOCHS MAXLEN SPLIT DROP_RATE LR 4e-5 1e-2 BATCH_SIZE VAL_BATCH_SIZE MODEL_SAVE_PATH 'insincerity_model.pt'
737	class QuoraDataset Dataset def __init__ self data tokenizer self text data question_text self data self tokenizer data tokenizer self target data target if "target" in data columns else len data def __len__ self return len self data def __getitem__ self pg tg 'post' 'post' target self target question str self text quest_ids self tokenizer encode question strip attention_mask_idx len quest_ids if not in quest_ids quest_ids quest_ids quest_ids pad quest_ids maxlen MAXLEN value padding pg truncating tg attention_mask np zeros MAXLEN attention_mask attention_mask_idx attention_mask attention_mask reshape if not in quest_ids quest_ids attention_mask return FloatTensor target LongTensor quest_ids LongTensor attention_mask
738	EPOCHS SAMPLE_LEN IMAGE_PATH "../input/plant-pathology-2020-fgvc7/images/" TEST_PATH "../input/plant-pathology-2020-fgvc7/test.csv" TRAIN_PATH "../input/plant-pathology-2020-fgvc7/train.csv" SUB_PATH "../input/plant-pathology-2020-fgvc7/sample_submission.csv" sub pd read_csv SUB_PATH test_data pd read_csv TEST_PATH train_data pd read_csv TRAIN_PATH
739	def load_image image_id file_path image_id ".jpg" image cv2 imread IMAGE_PATH file_path return cv2 cvtColor image cv2 COLOR_BGR2RGB train_images train_data "image_id" SAMPLE_LEN progress_apply load_image
740	fig ff create_distplot values group_labels colors "purple" fig update_layout showlegend False template "simple_white" fig update_layout title_text fig data marker line color 'rgb(0, 0, 0)' fig data marker line width fig
741	fig ff create_distplot red_values group_labels "R" colors "red" fig update_layout showlegend False template "simple_white" fig update_layout title_text fig data marker line color 'rgb(0, 0, 0)' fig data marker line width fig
742	fig ff create_distplot green_values group_labels "G" colors "green" fig update_layout showlegend False template "simple_white" fig update_layout title_text fig data marker line color 'rgb(0, 0, 0)' fig data marker line width fig
743	fig ff create_distplot blue_values group_labels "B" colors "blue" fig update_layout showlegend False template "simple_white" fig update_layout title_text fig data marker line color 'rgb(0, 0, 0)' fig data marker line width fig
744	train_data train_data "multiple_diseases" apply bool apply str fig px histogram train_data color title color_discrete_map px colors qualitative Plotly px colors qualitative Plotly fig update_layout template "simple_white" fig data marker line color 'rgb(0, 0, 0)' fig data marker line width fig data marker line color 'rgb(0, 0, 0)' fig data marker line width fig
745	AUTO tf data experimental AUTOTUNE tpu tf distribute cluster_resolver TPUClusterResolver tf config experimental_connect_to_cluster tpu tf tpu experimental initialize_tpu_system tpu strategy tf distribute experimental TPUStrategy tpu BATCH_SIZE strategy num_replicas_in_sync GCS_DS_PATH KaggleDatasets get_gcs_path
746	def format_path st return GCS_DS_PATH '/images/' st '.jpg' test_paths test_data image_id apply format_path values train_paths train_data image_id apply format_path values train_labels np float32 train_data loc 'healthy' 'scab' values train_paths valid_paths train_labels valid_labels train_test_split train_paths train_labels test_size random_state
747	train_dataset tf data Dataset from_tensor_slices train_paths train_labels map decode_image num_parallel_calls AUTO map data_augment num_parallel_calls AUTO repeat shuffle batch BATCH_SIZE prefetch AUTO valid_dataset tf data Dataset from_tensor_slices valid_paths valid_labels map decode_image num_parallel_calls AUTO batch BATCH_SIZE cache prefetch AUTO test_dataset tf data Dataset from_tensor_slices test_paths map decode_image num_parallel_calls AUTO batch BATCH_SIZE
748	lrfn build_lrfn STEPS_PER_EPOCH train_labels shape BATCH_SIZE lr_schedule tf keras callbacks LearningRateScheduler lrfn verbose
749	EPOCHS SPLIT MAXLEN DROP_RATE np random seed OUTPUT_UNITS BATCH_SIZE LR 4e-5 1e-2 ROBERTA_UNITS VAL_BATCH_SIZE MODEL_SAVE_PATH 'sentiment_model.pt'
750	def cel inp target labels target max dim return nn CrossEntropyLoss inp labels len inp def accuracy inp target inp_ind inp max axis indices target_ind target max axis indices return inp_ind target_ind float sum axis
751	val_losses torch load 'val_loss_{}.pt' format for in range EPOCHS train_losses torch load 'train_loss_{}.pt' format for in range EPOCHS val_accuracies torch load 'val_acc_{}.pt' format for in range EPOCHS train_accuracies torch load 'train_acc_{}.pt' format for in range EPOCHS
752	device xm xla_device network network to device def predict_sentiment tweet pg tg 'post' 'post' tweet_ids tokenizer encode tweet strip sent 'positive' 'neutral' 'negative' att_mask_idx len tweet_ids if not in tweet_ids tweet_ids tweet_ids tweet_ids pad tweet_ids maxlen MAXLEN value padding pg truncating tg att_mask np zeros MAXLEN att_mask att_mask_idx att_mask att_mask reshape if not in tweet_ids tweet_ids att_mask tweet_ids att_mask torch LongTensor tweet_ids torch LongTensor att_mask return sent np argmax network forward tweet_ids to device att_mask to device detach cpu numpy
753	def generate_txt_file dataframe folder DIR_PATH folder ".zip" zipObj ZipFile DIR_PATH 'w' for in tqdm range len dataframe label DIR_PATH dataframe iloc 'image_id' ".txt" open label "w" line "{} {} {} {} {}\n" format dataframe iloc 'x_center' dataframe iloc 'y_center' dataframe iloc 'w' dataframe iloc 'h' write line close zipObj write label zipObj close
754	SPLIT SAMPLE True MU SIGMA EPOCHS LR 1e-3 1e-3 BATCH_SIZE VAL_BATCH_SIZE MODEL 'efficientnet-b3' IMG_PATHS '../working/test' '../working/train_1' '../working/train_2'
755	PATH_DICT for folder_path in tqdm IMG_PATHS for img_path in os listdir folder_path PATH_DICT img_path folder_path
756	def display_images num sq_num np sqrt num assert sq_num int sq_num sq_num int sq_num image_ids os listdir IMG_PATHS fig ax plt subplots nrows sq_num ncols sq_num figsize for in range sq_num for in range sq_num idx sq_num ax axis 'off' img cv2 imread IMG_PATHS image_ids idx ax imshow img ax set_title format idx fontsize plt show
757	def to_tensor data return FloatTensor point for point in data def set_image_transformations dataset aug norm Normalize mean MU std SIGMA vflip hflip VerticalFlip HorizontalFlip dataset transformation Compose norm vflip hflip if aug else norm class SIIMDataset Dataset def __init__ self df aug targ ids set_image_transformations self aug self df self targ self aug self image_ids df targ aug ids def __len__ self return len self image_ids def __getitem__ self image_id self image_ids target self df target if self targ else image cv2 imread PATH_DICT image_id image_id return to_tensor self transformation image image 'image' target
758	def bce y_true y_pred return nn BCEWithLogitsLoss y_pred y_true def acc y_true y_pred y_true y_true squeeze y_pred nn Sigmoid y_pred squeeze return y_true torch round y_pred float sum len y_true
759	def print_metric data batch epoch start end metric typ typ metric "%s" data "%s" if typ pre str batch "%s " if typ pre str epoch "%s " time np round end start time format time fonts fg attr 'reset' fg attr 'reset' fg attr 'reset' print pre fonts format fonts time fonts
760	np array ones len train_df query 'target == 1' zeros len train_df query 'target == 0' weightage_fn zeros ones weights weightage_fn target for target in train_df target
761	length len train_df val_ids val_df image_name apply lambda '.jpg' train_ids train_df image_name apply lambda '.jpg' val_set SIIMDataset val_df False True ids val_ids train_set SIIMDataset train_df True True ids train_ids
762	train_sampler WeightedRandomSampler weights length if_sample if_shuffle train_sampler False None True sample_fn lambda is_sample sampler if_sample if is_sample else if_shuffle sampler shuffler sample_fn SAMPLE train_sampler val_loader DataLoader val_set VAL_BATCH_SIZE shuffle False train_loader DataLoader train_set BATCH_SIZE sampler sampler shuffle shuffler
763	device xm xla_device network CancerNet features to device optimizer Adam 'params' network efn parameters 'lr' LR 'params' network dense_output parameters 'lr' LR
764	def display_preds num sq_num np sqrt num assert sq_num int sq_num sq_num int sq_num image_ids os listdir IMG_PATHS few_preds sigmoid np array test_preds num pred_dict fig ax plt subplots nrows sq_num ncols sq_num figsize norm Normalize mean std for in range sq_num for in range sq_num idx sq_num ax axis 'off' pred few_preds idx img cv2 imread IMG_PATHS image_ids idx ax imshow img ax set_title format pred_dict round pred item fontsize plt show
765	bpps_files os listdir '../input/stanford-covid-vaccine/bpps/' example_bpps np load f'../input/stanford-covid-vaccine/bpps/{bpps_files[0]}' print 'bpps file shape:' example_bpps shape
766	loss 'hinge' 'log' 'modified_huber' 'squared_hinge' 'perceptron' penalty 'l1' 'l2' 'elasticnet' alpha learning_rate 'constant' 'optimal' 'invscaling' 'adaptive' class_weight eta0 param_distributions dict loss loss penalty penalty alpha alpha learning_rate learning_rate class_weight class_weight eta0 eta0 random RandomizedSearchCV estimator sgd param_distributions param_distributions scoring 'roc_auc' verbose n_jobs n_iter random_result random fit y_train print random_result best_score_ print random_result best_params_
767	labels_df pd read_json os path join path 'train_sample_videos/metadata.json' labels_df labels_df print labels_df shape labels_df head
768	import numpy as np import pandas as pd import matplotlib pyplot as plt import plotly offline as py py init_notebook_mode connected True import plotly tools as tls import warnings import seaborn as sns plt style use 'fivethirtyeight' from collections import Counter warnings filterwarnings 'ignore' import plotly graph_objs as go import plotly tools as tls import plotly plotly as plpl
769	object_columns 'sample_id' 'object_id' 'center_x' 'center_y' 'center_z' 'width' 'length' 'height' 'yaw' 'class_name' objects for sample_id ps in tqdm train values object_params ps split n_objects len object_params for in range n_objects yaw tuple object_params objects append sample_id yaw train_objects pd DataFrame objects columns object_columns
770	fig ax plt subplots figsize sns distplot train_objects 'yaw' color 'darkgreen' ax ax set_title 'yaw' fontsize plt xlabel 'yaw' fontsize plt show
771	fig ax plt subplots figsize plot sns countplot "class_name" data train_objects query 'class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"' palette 'navy' 'darkblue' 'blue' 'dodgerblue' 'skyblue' 'lightblue' set_title fontsize plt yticks fontsize plt xlabel fontsize plt ylabel fontsize plt show plot
772	def render_scene index my_scene lyft_dataset scene index my_sample_token my_scene "first_sample_token" lyft_dataset render_sample my_sample_token
773	sensor_channel 'CAM_BACK' my_sample_data lyft_dataset get 'sample_data' my_sample 'data' sensor_channel lyft_dataset render_sample_data my_sample_data 'token'
774	my_scene lyft_dataset scene my_sample_token my_scene "first_sample_token" my_sample lyft_dataset get 'sample' my_sample_token lyft_dataset render_sample_data my_sample 'data' 'LIDAR_TOP' nsweeps
775	df pd DataFrame plt figure figsize plt subplots_adjust top hspace colors 'red' 'red' 'blue' 'pink' 'gold' 'brown' 'blue' 'pink' 'brown' 'gold' for in train 'batch' unique df f'batch:{i}' train iloc ROW_PER_BATCH ROW_PER_BATCH reset_index signal plt subplot sns distplot df f'batch:{i}' color colors set_title f"(Signal) median: {df[f'batch:{i}'].median():.2f}"
776	test 'batch' for in range test shape ROW_PER_BATCH test iloc ROW_PER_BATCH ROW_PER_BATCH
777	plt figure figsize plt plot train signal plt show
778	print format train signal mean train signal std train signal median train 'signal_undrifted' train signal train loc train index 'signal_undrifted' train signal values train time values print format train signal_undrifted mean train signal_undrifted std train signal_undrifted median
779	def low high mid return low high mid high low batch batch batch train loc train index 'signal_undrifted' train signal values train time values batch batch batch train loc train index 'signal_undrifted' train signal values train time values batch batch batch train loc train index 'signal_undrifted' train signal values train time values batch batch batch train loc train index 'signal_undrifted' train signal values train time values
780	plt figure figsize sns lineplot test time test signal color 'r' set_title sns lineplot test time test signal_undrifted color 'g' set_title plt legend title loc 'upper right' labels plt show
781	import numpy as np import pandas as pd import matplotlib pyplot as plt import seaborn as sns from kmodes kmodes import KModes from sklearn import preprocessing from sklearn decomposition import PCA pd set_option 'mode.chained_assignment' None
782	train_nom train nom_features for col in nom_features le preprocessing LabelEncoder train_nom col le fit_transform train_nom col train_nom head
783	plt figure figsize plt plot range train shape res train signal res for in range num_batches plt plot batch_size batch_size 'r' for in range num_batches plt text batch_size num_batches str size plt xlabel size plt ylabel size plt title size plt show
784	fig ax plt subplots nrows ncols figsize fig subplots_adjust hspace ax ax ravel colors plt rcParams "axes.prop_cycle" for batch in range num_batches fft sp fftpack fft train signal batch_size batch batch_size batch psd np abs fft fftfreq sp fftpack fftfreq len psd fs fftfreq next colors "color" ax batch plot fftfreq np log10 psd color ax batch set_title f'Batch {batch+1}' ax batch set_xlabel ax batch set_ylabel
785	butter order lpf_cutoff nyq btype 'low' analog False freqz fs fs plt figure figsize plt plot np log10 abs 'b' plt ylabel color 'b' plt xlabel plt title
786	batch signal_lpf_batch_5 butter_lowpass_filter train signal batch_size batch batch_size batch lpf_cutoff fs order fig ax plt subplots nrows ncols figsize ax plot range batch_size res train open_channels batch_size batch batch_size batch res color 'g' ax plot range batch_size res train signal batch_size batch batch_size batch res ax plot range batch_size res signal_lpf_batch_5 res ax legend 'open_channels' ax legend 'signal' 'filtered signal'
787	batch signal_lpf_batch_9 butter_lowpass_filter train signal batch_size batch batch_size batch lpf_cutoff fs order fig ax plt subplots nrows ncols figsize ax plot range batch_size res train open_channels batch_size batch batch_size batch res color 'g' ax plot range batch_size res train signal batch_size batch batch_size batch res ax plot range batch_size res signal_lpf_batch_9 res ax legend 'open_channels' ax legend 'signal' 'filtered signal'
788	batch signal_lpf_batch_10 butter_lowpass_filter train signal batch_size batch batch_size batch lpf_cutoff fs order fig ax plt subplots nrows ncols figsize ax plot range batch_size res train open_channels batch_size batch batch_size batch res color 'g' ax plot range batch_size res train signal batch_size batch batch_size batch res ax plot range batch_size res signal_lpf_batch_10 res ax legend 'open_channels' ax legend 'signal' 'filtered signal'
789	import numpy as np import pandas as pd import os for dirname filenames in os walk '/kaggle/input' for filename in filenames print os path join dirname filename from time import time from tqdm import tqdm_notebook as tqdm from collections import Counter from scipy import stats import lightgbm as lgb from sklearn metrics import cohen_kappa_score from sklearn model_selection import KFold StratifiedKFold import gc import json pd set_option 'display.max_columns'
790	def read_data print train pd read_csv '/kaggle/input/data-science-bowl-2019/train.csv' print format train shape train shape print test pd read_csv '/kaggle/input/data-science-bowl-2019/test.csv' print format test shape test shape print train_labels pd read_csv '/kaggle/input/data-science-bowl-2019/train_labels.csv' print format train_labels shape train_labels shape print specs pd read_csv '/kaggle/input/data-science-bowl-2019/specs.csv' print format specs shape specs shape print sample_submission pd read_csv '/kaggle/input/data-science-bowl-2019/sample_submission.csv' print format sample_submission shape sample_submission shape return train test train_labels specs sample_submission
791	cities pd read_csv gamecities pd read_csv tourneycompactresults pd read_csv tourneyseeds pd read_csv tourneyslots pd read_csv regseasoncompactresults pd read_csv seasons pd read_csv teamspellings pd read_csv engine 'python' teams pd read_csv
792	gamecities gamecities merge cities how 'left' on tourneycompactresults tourneycompactresults merge tourneyseeds left_on right_on how 'left' tourneycompactresults tourneycompactresults merge tourneyseeds left_on right_on how 'left' tourneycompactresults tourneycompactresults merge gamecities how 'left' on regseasoncompactresults regseasoncompactresults merge tourneyseeds left_on right_on how 'left' regseasoncompactresults regseasoncompactresults merge tourneyseeds left_on right_on how 'left' regseasoncompactresults regseasoncompactresults merge gamecities how 'left' on
793	binary_columns for in list train_master columns values if '_bin' in categorical_columns for in list train_master columns values if '_cat' in non_continuous_feature_subs '_cat' '_bin' 'target' 'id' continuous_columns for in list train_master columns values if all not in for in non_continuous_feature_subs target_column 'target' ind_columns for in list train_master columns values if '_ind' in car_columns for in list train_master columns values if '_car' in calc_columns for in list train_master columns values if '_calc' in reg_columns for in list train_master columns values if '_reg' in
794	def gini pred fpr tpr thr metrics roc_curve pred pos_label metrics auc fpr tpr return
795	plt figure figsize train test plt hist np ceil train_trans bins plt hist np ceil test_trans bins
796	import numpy as np import pandas as pd train pd read_csv '/kaggle/input/covid19-global-forecasting-week-1/train.csv' test pd read_csv '/kaggle/input/covid19-global-forecasting-week-1/test.csv'
797	train_agg train groupby as_index False agg 'sum' 'sum' train_agg pd to_datetime train_agg
798	import pycountry_convert as pc import pycountry def do_fuzzy_search country try result pycountry countries search_fuzzy country except Exception return np nan else return result alpha_2 train_continent train_agg train_continent loc train_continent train_continent loc train_continent iso_map country do_fuzzy_search country for country in train_continent unique train_continent 'iso' train_continent map iso_map
799	fig px line train_agg color hover_name fig update_layout autosize False width height title fig show
800	import geopandas as gpd shapefile '/kaggle/input/natural-earth-maps/ne_110m_admin_0_countries.shp' gdf gpd read_file shapefile gdf gdf drop gdf index
801	import seaborn as sns sns regplot 'hits' data cc_google scatter_kws 's' fit_reg True line_kws "color" "black"
802	from sklearn import metrics def gini_xgb preds dtrain labels dtrain get_label gini_score gini_normalizedc labels preds return 'gini' gini_score def gini_lgb actuals preds return 'gini' gini_normalizedc actuals preds True gini_sklearn metrics make_scorer gini_normalizedc True True
803	import numpy as np import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import os print os listdir "../input" from PIL import Image import random from tqdm import tqdm_notebook
804	def load_img PATH return np array Image open PATH def plots ims figsize rows titles None plt figure figsize figsize for in range len ims sp add_subplot rows len ims rows sp axis if titles is not None sp set_title titles fontsize plt imshow ims
805	fre_sincere np array quora_train "question_text" quora_train "target" progress_apply textstat flesch_reading_ease fre_insincere np array quora_train "question_text" quora_train "target" progress_apply textstat flesch_reading_ease plot_readability fre_sincere fre_insincere
806	def consensus_all text return textstat text_standard text float_output True con_sincere np array quora_train "question_text" quora_train "target" progress_apply consensus_all con_insincere np array quora_train "question_text" quora_train "target" progress_apply consensus_all plot_readability con_sincere con_insincere
807	vectorizer_sincere CountVectorizer min_df max_df stop_words 'english' lowercase True token_pattern sincere_questions_vectorized vectorizer_sincere fit_transform sincere_questions vectorizer_insincere CountVectorizer min_df max_df stop_words 'english' lowercase True token_pattern insincere_questions_vectorized vectorizer_insincere fit_transform insincere_questions
808	pyLDAvis enable_notebook dash pyLDAvis sklearn prepare lda_sincere sincere_questions_vectorized vectorizer_sincere mds 'tsne' dash
809	pyLDAvis enable_notebook dash pyLDAvis sklearn prepare lda_insincere insincere_questions_vectorized vectorizer_insincere mds 'tsne' dash
810	model Sequential model add LSTM return_sequences True recurrent_dropout input_shape None sequence_length model add LSTM recurrent_dropout model add Dense activation 'relu' model add Dropout model add Dense activation 'relu' model add Dropout model add Dense len ebird_to_id keys activation "softmax" model summary callbacks ReduceLROnPlateau monitor 'val_loss' patience verbose factor EarlyStopping monitor 'val_loss' patience ModelCheckpoint filepath 'best_model.h5' monitor 'val_loss' save_best_only True model compile loss "categorical_crossentropy" optimizer 'adam'
811	sample_df pd read_csv PATH 'sample_submission.csv' learn data add_test ImageList from_df sample_df PATH folder 'test_images' suffix '.png' preds learn get_preds DatasetType Test sample_df diagnosis preds argmax sample_df head sample_df to_csv 'submission.csv' index False
812	def missing_values_table df mis_val df isnull sum mis_val_percent df isnull sum len df mis_val_table pd concat mis_val mis_val_percent axis mis_val_table_ren_columns mis_val_table rename columns mis_val_table_ren_columns mis_val_table_ren_columns mis_val_table_ren_columns iloc sort_values ascending False round print str df shape " columns.\n" str mis_val_table_ren_columns shape " columns that have missing values." return mis_val_table_ren_columns
813	fig plt figure figsize columns rows for in range columns rows ds pydicom dcmread train_images_dir train train 'benign_malignant' 'benign' 'image_name' '.dcm' fig add_subplot rows columns plt imshow ds pixel_array cmap plt cm bone fig add_subplot
814	vals train train 'benign_malignant' 'malignant' 'image_name' index values fig plt figure figsize columns rows for in range columns rows ds pydicom dcmread train_images_dir train train 'benign_malignant' 'malignant' 'image_name' vals '.dcm' fig add_subplot rows columns plt imshow ds pixel_array cmap plt cm bone fig add_subplot
815	fgbg cv createBackgroundSubtractorMOG2 def view_images_aug images title aug None width height fig axs plt subplots height width figsize for im in range height width data pydicom read_file os path join train_images_dir list images im '.dcm' image data pixel_array image cv2 cvtColor image cv2 COLOR_BGR2RGB image cv2 resize image image fgbg apply image im width im width axs imshow image cmap plt cm bone axs axis 'off' plt suptitle title view_images_aug train train 'diagnosis' 'image_name' title
816	image_folder_path "/kaggle/input/siim-isic-melanoma-classification/jpeg/train/" chosen_image cv2 imread os path join image_folder_path albumentation_list RandomSunFlare GaussNoise CLAHE RandomRain Rotate limit RGBShift RandomSnow HorizontalFlip VerticalFlip RandomContrast limit HueSaturationValue hue_shift_limit sat_shift_limit val_shift_limit img_matrix_list bboxes_list for aug_type in albumentation_list img aug_type image chosen_image 'image' img_matrix_list append img img_matrix_list insert chosen_image titles_list "CLAHE" "HSV" def plot_multiple_img img_matrix_list title_list ncols main_title fig myaxes plt subplots figsize nrows ncols ncols squeeze False fig suptitle main_title fontsize fig subplots_adjust wspace fig subplots_adjust hspace for img title in enumerate zip img_matrix_list title_list myaxes ncols ncols imshow img myaxes ncols ncols set_title title fontsize plt show plot_multiple_img img_matrix_list titles_list ncols main_title
817	def view_images_aug images title aug None width height fig axs plt subplots height width figsize for im in range height width data pydicom read_file os path join train_images_dir list images im '.dcm' image data pixel_array image cv2 cvtColor image cv2 COLOR_BGR2RGB image cv2 resize image kernel np ones np uint8 img_erosion cv2 erode image kernel iterations img_erosion cv2 dilate image kernel iterations im width im width axs imshow image cmap plt cm bone axs axis 'off' plt suptitle title view_images_aug train train 'diagnosis' 'image_name' title
818	class Config BATCH_SIZE EPOCHS WARMUP_EPOCHS LEARNING_RATE 1e-4 WARMUP_LEARNING_RATE 1e-3 HEIGHT WIDTH CANAL N_CLASSES train 'target' nunique ES_PATIENCE RLROP_PATIENCE DECAY_DROP
819	def id_values row overlap for key value in row items if key in overlap print key value
820	def filter_signal signal threshold 1e8 fourier rfft signal frequencies rfftfreq signal size 1e-5 fourier frequencies threshold return irfft fourier
821	import pandas as pd import numpy as np import matplotlib pyplot as plt import seaborn as sns plt style use 'dark_background' from IPython display import display from IPython core interactiveshell import InteractiveShell InteractiveShell ast_node_interactivity "all" import os for dirname filenames in os walk '/kaggle/input' for filename in filenames print os path join dirname filename
822	print train shape print train shape
823	train 'release_date' pd to_datetime train 'release_date' infer_datetime_format True train 'release_day' train 'release_date' apply lambda day train 'release_weekday' train 'release_date' apply lambda weekday train 'release_month' train 'release_date' apply lambda month train 'release_year' train 'release_date' apply lambda year if year else year
824	plt figure figsize edgecolor sns countplot train 'release_year' sort_values palette edgecolor plt title fontsize plt xlabel plt ylabel plt xticks fontsize rotation plt show
825	plt figure figsize edgecolor sns distplot train 'popularity' kde False plt title fontsize plt xlabel plt ylabel plt xticks fontsize rotation plt show
826	plt figure figsize edgecolor sns countplot train 'release_day' sort_values palette edgecolor plt title fontsize plt xlabel plt ylabel plt xticks fontsize plt show
827	plt figure figsize sns countplot train 'release_weekday' sort_values palette loc np array range len train 'release_weekday' unique day_labels plt xlabel plt ylabel plt xticks loc day_labels fontsize plt show
828	def get_couples structure opened idx for idx in enumerate structure if closed idx for idx in enumerate structure if assert len opened len closed assigned couples for close_idx in closed for open_idx in opened if open_idx close_idx if open_idx not in assigned candidate open_idx else break assigned append candidate couples append candidate close_idx assert len couples len opened return couples
829	def sieve_eratosthenes primes False False True for in range while if primes True for in range primes False return primes
830	def build_vocab texts sentences texts apply lambda split values vocab for sentence in sentences for word in sentence try vocab word except KeyError vocab word return vocab
831	def add_lower embedding vocab count for word in vocab if word in embedding and word lower not in embedding embedding word lower embedding word count print f"Added {count} words to embedding"
832	def clean_contractions text mapping specials for in specials text text replace text join mapping if in mapping else for in text split return text
833	def build_vocab texts sentences texts apply lambda split values vocab for sentence in sentences for word in sentence try vocab word except KeyError vocab word return vocab
834	def clean_contractions text mapping specials for in specials text text replace text join mapping if in mapping else for in text split return text
835	def make_treated_data Tokenizer num_words len_voc filters fit_on_texts texts_to_sequences pad_sequences maxlen max_len return word_index
836	for wav in pywt wavelist try filtered wavelet_denoising signal wavelet wav level except pass plt figure figsize plt plot signal label plt plot filtered label plt legend plt title f"DWT Denoising with {wav} Wavelet" size plt show
837	def build_model inp Input shape max_len feature_size BatchNormalization inp Bidirectional CuDNNGRU return_sequences True Bidirectional CuDNNGRU return_sequences True avg_pool max_pool concat concatenate avg_pool max_pool concat Dense activation "relu" concat concat Dropout concat output Dense activation "sigmoid" concat model Model inputs inp outputs output model compile loss 'binary_crossentropy' optimizer Adam lr metrics 'accuracy' return model
838	dtypes "crew" "int8" "experiment" "category" "time" "float32" "seat" "int8" "eeg_fp1" "float32" "eeg_f7" "float32" "eeg_f8" "float32" "eeg_t4" "float32" "eeg_t6" "float32" "eeg_t5" "float32" "eeg_t3" "float32" "eeg_fp2" "float32" "eeg_o1" "float32" "eeg_p3" "float32" "eeg_pz" "float32" "eeg_f3" "float32" "eeg_fz" "float32" "eeg_f4" "float32" "eeg_c4" "float32" "eeg_p4" "float32" "eeg_poz" "float32" "eeg_c3" "float32" "eeg_cz" "float32" "eeg_o2" "float32" "ecg" "float32" "r" "float32" "gsr" "float32" "event" "category"
839	plt figure figsize sns countplot 'event' hue 'seat' data train_df plt xlabel fontsize plt ylabel fontsize plt yscale 'log' plt title fontsize plt show
840	plt figure figsize sns violinplot 'event' 'time' data train_df sample plt ylabel fontsize plt xlabel fontsize plt title fontsize plt show
841	plt figure figsize sns violinplot 'event' 'gsr' data train_df sample plt ylabel fontsize plt xlabel fontsize plt title fontsize plt show
842	features "crew" "seat" features_n def run_lgb df_train df_test dic 'A' 'B' 'C' 'D' try df_train "event" df_train "event" apply lambda dic df_test "event" df_test "event" apply lambda dic except pass params "objective" "multiclass" "num_class" "metric" "multi_error" "num_leaves" "min_child_weight" "learning_rate" "bagging_fraction" "feature_fraction" "bagging_seed" "verbosity" lg_train lgb Dataset df_train features label df_train "event" lg_test lgb Dataset df_test features label df_test "event" model lgb train params lg_train valid_sets lg_test early_stopping_rounds verbose_eval return model
843	plt figure figsize plt plot ts rolling window center False mean label plt plot ts rolling window center False std label plt legend
844	plt figure figsize plt subplot plt scatter iloc iloc plt xlabel plt ylabel plt plot plt title plt subplot plt scatter iloc iloc plt xlabel plt ylabel plt plot plt title plt subplot plt scatter iloc iloc plt xlabel plt ylabel plt plot plt title plt subplot plt scatter iloc iloc plt xlabel plt ylabel plt plot plt title
845	plt figure figsize plt subplot adjustable 'box' aspect plt plot cities cities 'k,' alpha plt plot cities cities 'bx' plt xlim plt ylim plt xlabel 'X' fontsize plt ylabel 'Y' fontsize plt title fontsize plt show
846	plt figure figsize plt subplot adjustable 'box' aspect plt plot cities cities 'k,' alpha plt plot prime_cities prime_cities 'r.' markersize alpha plt plot cities cities 'bx' plt xlim plt ylim plt xlabel 'X' fontsize plt ylabel 'Y' fontsize plt title fontsize plt show
847	def concorde_tsp seed cities pd read_csv '../input/cities.csv' solver TSPSolver from_data cities cities norm "EUC_2D" tour_data solver solve time_bound verbose True random_seed seed if tour_data found_tour path np append tour_data tour make_submission 'concorde' path return path else return None path_cc concorde_tsp
848	cities pd read_csv '../input/cities.csv' cities cities CityId apply isprime prime_cities cities loc cities CityId cities isPrime solver TSPSolver from_data prime_cities prime_cities norm "EUC_2D" tour_data solver solve time_bound verbose True random_seed prime_path np append tour_data tour
849	from xgboost import XGBRegressor regressor XGBRegressor n_estimators regressor fit
850	import glob import matplotlib pyplot as plt import seaborn as sns import pandas as pd import pydicom import numpy as np import warnings import multiprocessing import os from skimage import morphology from skimage import feature from skimage import measure from skimage import util from skimage import transform warnings filterwarnings 'ignore'
851	boxes_per_patient tr groupby sum ax boxes_per_patient value_counts plot bar ax set_title ax set_xlabel ax set_ylabel ax xaxis set_tick_params rotation
852	ax boxes_per_patient value_counts plot bar ax set_title ax set_xlabel ax xaxis set_tick_params rotation
853	centers tr dropna subset 'x' assign center_x tr tr width center_y tr tr height ax sns jointplot "center_x" "center_y" data centers height alpha ax fig suptitle
854	sns FacetGrid col hue 'gender' data tr drop_duplicates subset height palette dict "red" "blue" map sns distplot 'age' hist_kws 'alpha' add_legend fig suptitle fontsize
855	areas tr dropna subset 'area' sns FacetGrid hue 'gender' data areas height palette dict "red" "blue" aspect map sns distplot 'area' hist_kws 'alpha' add_legend fig suptitle
856	pixel_vc tr drop_duplicates 'pixel_spacing' value_counts ax pixel_vc iloc plot bar ax set_xticklabels f'{ps:.4f}' for ps in pixel_vc index ax set_xlabel ax set_ylabel ax set_title fontsize
857	areas_with_count areas merge pd DataFrame boxes_per_patient rename columns 'bbox_count' on sns FacetGrid hue 'bbox_count' data areas_with_count height aspect map sns distplot 'area' add_legend fig suptitle
858	from sklearn mixture import GaussianMixture clf GaussianMixture n_components clf fit centers 'center_x' 'center_y' center_probs clf predict_proba centers 'center_x' 'center_y' clf score_samples centers 'center_x' 'center_y' outliers centers iloc fig ax plt subplots centers plot scatter 'center_x' 'center_y' alpha cmap 'viridis' ax ax outliers plot scatter 'center_x' 'center_y' 'red' marker 'x' ax ax ax set_title fontsize
859	ax sns boxplot tr mean_black_pixels ax set_xlabel ax set_title
860	high_black_pixel_patientIds tr loc tr mean_black_pixels drop_duplicates fig axes plt subplots for patient_id ax in enumerate zip high_black_pixel_patientIds axes flatten row tr loc tr patientId patient_id img get_image row patientId iloc bbs row 'x' 'y' 'width' 'height' draw_image img bbs ax fig tight_layout pad
861	high_white_pixel_patientIds tr loc tr mean_black_pixels drop_duplicates fig axes plt subplots for patient_id ax in zip high_white_pixel_patientIds axes flatten row tr loc tr patientId patient_id img get_image row patientId iloc bbs row 'x' 'y' 'width' 'height' draw_image img bbs ax fig tight_layout pad
862	high_black_pixel_images np empty shape high_black_pixel_patientIds shape for patient_id in enumerate high_black_pixel_patientIds row tr loc tr patientId patient_id img get_image row patientId iloc high_black_pixel_images img high_black_pixel_contours for img in high_black_pixel_images img2 feature canny img img2 morphology convex_hull_image img2 measure find_contours img2 measure approximate_polygon high_black_pixel_contours append fig axes plt subplots contours for img ax in zip high_black_pixel_contours high_black_pixel_images axes flatten draw_image img None ax ax plot '-b' linewidth fig tight_layout pad
863	ax sns distplot tr 'aspect_ratio' dropna norm_hist True ax set_title ax set_xlabel
864	aspect_ratios tr 'aspect_ratio' dropna high_aspect_ratio_tr tr iloc aspect_ratios aspect_ratios aspect_ratios quantile index drop_duplicates fig axes plt subplots for row ax in zip high_aspect_ratio_tr itertuples axes flatten img get_image row patientId bbs tr loc tr patientId row patientId 'x' 'y' 'width' 'height' draw_image img bbs ax fig tight_layout pad
865	from sklearn discriminant_analysis import LinearDiscriminantAnalysis as LDA lda LDA lda fit_transform astype int lda transform
866	import seaborn as sns fig ax plt subplots figsize sns heatmap train_correlations annot True vmin vmax center ax ax
867	import pandas as pd import matplotlib pyplot as plt import seaborn as sns import numpy as np from scipy stats import norm from sklearn preprocessing import StandardScaler from scipy import stats import warnings warnings filterwarnings 'ignore' import gc from sklearn model_selection import KFold from sklearn metrics import mean_squared_error import lightgbm as lgb import xgboost as xgb from scipy optimize import minimize
868	print print print XGB_BO res 'max' 'max_val' print XGB_BO res 'max' 'max_params' print file log_file print file log_file print XGB_BO res 'max' 'max_val' file log_file print XGB_BO res 'max' 'max_params' file log_file log_file flush log_file close history_df pd DataFrame XGB_BO res 'all' 'params' history_df2 pd DataFrame XGB_BO res 'all' 'values' history_df pd concat history_df history_df2 axis history_df rename columns 'gini' inplace True history_df 'AUC' history_df 'gini' history_df to_csv
869	print pca PCA n_components n_comp svd_solver 'full' random_state pca fit_transform print pca explained_variance_ratio_ sum print for in range n_comp print pca explained_variance_ratio_ plt figure figsize for color target_name in zip colors target_names plt scatter color color alpha label target_name marker plt legend loc 'best' shadow False scatterpoints plt title "and 2nd principal components" plt xlabel pca explained_variance_ratio_ plt ylabel pca explained_variance_ratio_ plt savefig 'pca-porto-02.png' dpi plt show
870	starttime timer None start_time timer None rfecv fit timer start_time
871	plt figure figsize plt xlabel plt ylabel plt plot range len rfecv grid_scores_ rfecv grid_scores_ plt savefig dpi plt show
872	ranking pd DataFrame all_features ranking np asarray rfecv ranking_ ranking sort_values inplace True ranking to_csv index False
873	def timer start_time None if not start_time start_time datetime now return start_time elif start_time thour temp_sec divmod datetime now start_time total_seconds tmin tsec divmod temp_sec print thour tmin round tsec train_df pd read_csv '../input/train.csv' dtype 'id' np int32 'target' np int8 train_df 'target' values train_df drop 'target' 'id' axis test_df pd read_csv '../input/test.csv' dtype 'id' np int32 test test_df drop 'id' axis
874	params 'min_child_weight' 'gamma' 'subsample' 'colsample_bytree' 'max_depth'
875	log_loss avreal avpred print cv_LL folds print roc_auc_score avreal avpred print cv_AUC folds print print cv_gini folds print score str round timer starttime mpred pred folds
876	result pd DataFrame mpred columns 'target' result 'id' te_ids result result set_index 'id' print print result head sub_file 'submission_5fold-average-keras-run-01-v1_' str score str now strftime '.csv' print sub_file result to_csv sub_file index True index_label 'id'
877	print print print XGB_BO res 'max' 'max_val' print XGB_BO res 'max' 'max_params' grid_file print grid_file XGB_BO points_to_csv grid_file
878	mtcnn MTCNN margin keep_all True factor device device eval resnet pretrained 'vggface2' device device eval
879	class FastMTCNN object def __init__ self stride resize args kwargs self stride stride self resize resize self mtcnn MTCNN args kwargs def __call__ self frames if self resize frames cv2 resize int shape self resize int shape self resize for in frames boxes probs self mtcnn detect frames self stride faces for frame in enumerate frames box_ind int self stride if boxes box_ind is None continue for box in boxes box_ind box int for in box faces append frame box box box box return faces
880	fast_mtcnn FastMTCNN stride resize margin factor keep_all True device device
881	fast_mtcnn FastMTCNN stride resize margin factor keep_all True device device
882	from dlib import get_frontal_face_detector detector get_frontal_face_detector def detect_dlib detector images faces for image in images image_gray cv2 cvtColor image cv2 COLOR_BGR2GRAY boxes detector image_gray box boxes face image box top box bottom box left box right faces append face return faces times_dlib
883	from mtcnn import MTCNN detector MTCNN def detect_mtcnn detector images faces for image in images boxes detector detect_faces image box boxes 'box' face image box box box box box box faces append face return faces times_mtcnn
884	lr epochs netD Discriminator to device optimizerD optim Adam netD parameters lr lr criteria nn BCELoss netD conv1 weight nn Parameter torch Tensor to device for param in netD conv1 parameters param requires_grad False
885	for in range plt figure figsize for in range xx torch Tensor np zeros to device xx np random randint plt subplot img netD torch Tensor zeros to device reshape xx reshape reshape img img detach cpu numpy img Image fromarray img astype 'uint8' reshape plt imshow img plt show
886	zipfile PyZipFile 'images.zip' mode 'w' DogGenerator for in range img getDog np random normal str '.png' img save 'PNG' write os remove close
887	random seed daily_sales_item_lookup_scaled_clustered loc iloc random sample range daily_sales_item_lookup_scaled_clustered loc shape plot figsize
888	daily_sales_item_lookup_scaled_weekly merge dtw_clusters loc dtw_clusters cluster left_index True right_index True plot figsize
889	audio_path '../input/train/audio/' pict_Path '../input/picts/train/' test_pict_Path '../input/picts/test/' test_audio_path '../input/test/audio/' samples
890	sample_audio total for in subFolderList all_files for in os listdir audio_path if '.wav' in total len all_files sample_audio append audio_path all_files print 'count: %d : %s' len all_files print total
891	def log_specgram audio sample_rate window_size step_size eps 1e-10 nperseg int round window_size sample_rate 1e3 noverlap int round step_size sample_rate 1e3 freqs spec signal spectrogram audio fs sample_rate window 'hann' nperseg nperseg noverlap noverlap detrend False return freqs np log spec astype np float32 eps
892	fig plt figure figsize for filepath in enumerate sample_audio plt subplot samplerate test_sound wavfile read filepath plt title filepath split plt axis 'off' plt plot test_sound
893	fig plt figure figsize for filepath in enumerate five_samples plt subplot samplerate test_sound wavfile read filepath plt title filepath split plt axis 'off' plt plot test_sound
894	def wav2img wav_path targetdir figsize fig plt figure figsize figsize samplerate test_sound wavfile read filepath spectrogram log_specgram test_sound samplerate output_file wav_path split split '.wav' output_file targetdir output_file plt imsave '%s.png' output_file spectrogram plt close
895	import shutil shutil make_archive 'train_zipped' 'zip' '/kaggle/working/train' shutil make_archive 'test_zipped' 'zip' '/kaggle/working/test' shutil make_archive 'exa_test_zipped' 'zip' '/kaggle/working/exa_test'
896	from sklearn import preprocessing import matplotlib pyplot as plt plt rc "font" size from sklearn linear_model import LogisticRegression from sklearn cross_validation import train_test_split y_train y_test train_test_split y_smt test_size random_state from sklearn import metrics logreg LogisticRegression logreg fit y_train
897	def visualize_classifier model ax None cmap 'rainbow' ax ax or plt gca ax scatter cmap cmap clim min max zorder ax axis 'tight' ax axis 'off' xlim ax get_xlim ylim ax get_ylim model fit xx yy np meshgrid np linspace xlim num np linspace ylim num model predict np c_ xx ravel yy ravel reshape xx shape n_classes len np unique contours ax contourf xx yy alpha levels np arange n_classes cmap cmap clim min max zorder ax set xlim xlim ylim ylim
898	fig ax plt subplots nrows fig set_size_inches sns lineplot data df ax ax sns lineplot 'FVC' data df ax ax fig savefig "weeksvsfvc.jpeg"
899	fig ax plt subplots fig set_size_inches sns lineplot ex_smoker_male ex_smoker_male "FVC" label 'ex_smoker_male' sns lineplot ex_smoker_female ex_smoker_female "FVC" label 'ex_smoker_female' sns lineplot non_smoker_male non_smoker_male "FVC" label 'non_smoker_male' sns lineplot non_smoker_female non_smoker_female "FVC" label 'non_smoker_female' sns lineplot current_smoker_male current_smoker_male "FVC" label 'current_smoker_male' sns lineplot current_smoker_female current_smoker_female "FVC" label 'current_smoker_female' fig savefig "smoker_current_fvc.jpeg"
900	files for dirname filenames in os walk '../input/osic-pulmonary-fibrosis-progression/train' for filename in filenames files append os path join dirname filename
901	imp pd DataFrame index feature_names imp 'train' pd Series bst get_score importance_type 'gain' index feature_names imp 'OOB' pd Series bst_after get_score importance_type 'gain' index feature_names imp imp fillna
902	import plotly offline as pyo import plotly plotly as py from plotly graph_objs import import pandas as pd import plotly plotly offline init_notebook_mode from scipy import signal pyo offline init_notebook_mode import plotly plotly as py from plotly graph_objs import import plotly plotly as py from plotly graph_objs import
903	def prep edge num_of_adjacencies text nb edge_trace Scatter line Line width color ' hoverinfo=' none ', mode=' lines ') for i in range(len(edge)): e1=edge[i][0]-1 e2=edge[i][1]-1 x0, y0 = nb[' longitude '][e1],nb[' latitude '][e1] x1, y1 = nb[' longitude '][e2],nb[' latitude '][e2] edge_trace[' markers ', hoverinfo=' text YIGnBu Node Connections ', xanchor=' left ', titleside=' right ' ), line=dict(width=2))) for i in range(len(nb)): x, y = nb[' longitude '][i],nb[' latitude '][i] node_trace[' '].append(x) node_trace[' '].append(y) for i in range(len(nb)): node_info = text[i] node_trace[' text '].append(node_info) node_trace[' marker color br NYC texi trip neighborhood interactions closest margin dict annotations dict showarrow False xref "paper" yref "paper" xaxis XAxis showgrid False zeroline False showticklabels False yaxis YAxis showgrid False zeroline False showticklabels False return fig
904	edge get_edge nb data num_of_adjacencies get_numbers_of_adjcs edge nb text for in range len nb 'neighborhood:' '<b>' str nb 'neighborhood_name' '</b>' '<br>' 'boro:' '<b>' str nb 'boro' '</b>' '<br>' text append fig prep edge num_of_adjacencies text nb pyo iplot fig
905	edge get_edge nb data num_of_adjacencies get_numbers_of_adjcs edge nb text for in range len nb 'neighborhood:' '<b>' str nb 'neighborhood_name' '</b>' '<br>' 'boro:' '<b>' str nb 'boro' '</b>' '<br>' text append fig prep edge num_of_adjacencies text nb pyo iplot fig
906	df_train pd read_csv 'train.csv' df_test pd read_csv 'test.csv' df_struct pd read_csv 'structures.csv' df_train_sub_charge pd read_csv 'mulliken_charges.csv' df_train_sub_tensor pd read_csv 'magnetic_shielding_tensors.csv'
907	def create_nn_model input_shape inp Input shape input_shape Dense inp BatchNormalization LeakyReLU alpha Dropout Dense BatchNormalization LeakyReLU alpha Dropout Dense BatchNormalization LeakyReLU alpha Dropout Dense BatchNormalization LeakyReLU alpha Dropout Dense BatchNormalization LeakyReLU alpha Dense BatchNormalization LeakyReLU alpha Dropout out1 Dense activation "linear" out2 Dense activation "linear" out3 Dense activation "linear" Dense BatchNormalization LeakyReLU alpha Dropout Dense BatchNormalization LeakyReLU alpha Dense BatchNormalization LeakyReLU alpha Dropout out Dense activation "linear" model Model inputs inp outputs out out1 out2 out3 return model
908	def submit predictions submit pd read_csv 'sample_submission.csv' print len submit len predictions submit "scalar_coupling_constant" predictions submit to_csv "/kaggle/working/workingsubmission-test.csv" index False submit test_prediction print datetime now start_time for mol_type in mol_types print mol_type ": cv score is " cv_score print "total cv score is" cv_score_total
909	CUR_X_FILES CUR_X list training_curated_df fname values def open_fat2019_image fn convert_mode after_open Image idx CUR_X_FILES index fn split PIL Image fromarray CUR_X idx time_dim base_dim size crop_x crop crop_x crop_x base_dim base_dim return Image pil2tensor np float32 div_ vision data open_image open_fat2019_image
910	import numpy as np import pandas as pd import os import json from pathlib import Path import matplotlib pyplot as plt from matplotlib import colors import numpy as np for dirname filenames in os walk '/kaggle/input' print dirname from pathlib import Path
911	def task_train020 shape bese_color row_count column_count for yy in range if yy bese_color row_count for xx in range if xx bese_color column_count bese_color np ones row_count column_count return task get_data str training_path training_tasks check task task_train020
912	import numpy as np import pandas as pd import matplotlib pyplot as plt import itertools import string import re import os plt style use pd set_option "display.max_columns"
913	print print print print resource_stats isnull sum axis resource_stats isnull sum axis
914	from csv import QUOTE_ALL for text_col in text_cols test_train text_col test_train text_col str replace
915	num_colsplitter ColSplitter cols numeric_cols logtransform LogTransform alpha scaler RobustScaler quantile_range numerical_pipe Pipeline 'num_colsplitter' num_colsplitter 'logtransform' logtransform 'scaler' scaler dummy_colsplitter ColSplitter cols dummy_categorical_cols categorical_pipe Pipeline 'dummy_colsplitter' dummy_colsplitter text_subpipes for text_col in text_cols text_colsplitter ColSplitter cols text_col ravel True tf_idf TfidfVectorizer sublinear_tf False norm 'l2' stop_words None ngram_range max_features None text_col_pipe Pipeline 'text_colsplitter' text_colsplitter 'tf_idf' tf_idf text_subpipes append text_col_pipe text_pipe ParallelPipe text_subpipes estimator LogisticRegression penalty "l2" pipeline_logreg Pipeline 'union' FeatureUnion transformer_list 'numerical_pipe' numerical_pipe 'categorical_pipe' categorical_pipe 'text_pipe' text_pipe 'estimator' estimator
916	subset_A test_train loc lambda df df project_is_approved sample subset_B test_train loc lambda df df project_is_approved sample test_train_subset pd concat subset_A subset_B test_X test_train_subset test_y test_X project_is_approved
917	import os for dirname filenames in os walk '/kaggle/input' for filename in filenames print os path join dirname filename
918	def addNewFeatures data data 'uid' data 'card1' astype str data 'card2' astype str data 'uid2' data 'uid' astype str data 'card3' astype str data 'card5' astype str data 'uid3' data 'uid2' astype str data 'addr1' astype str data 'addr2' astype str data 'D9' np where data 'D9' isna return data train addNewFeatures train test addNewFeatures test
919	uknown 'email_not_provided' def setDomain df df df fillna uknown df df fillna uknown df 'email_check' np where df df df uknown df '_prefix' df apply lambda split df '_prefix' df apply lambda split return df train setDomain train test setDomain test
920	import numpy as np import pandas as pd import matplotlib pyplot as plt import warnings warnings filterwarnings "ignore"
921	def get_rdf hist_x hist_r density natom dr hist_r hist_r factor np pi dr density natom rad rdf for in range len hist_x hist_r hist_r rad append factor hist_x rdf append return rad rdf
922	def get_angles Rij atom1 angles crdn1 list neighbors atom1 crdn1_indeces int atom split for atom in crdn1 i1 int atom1 split for in range len crdn1 i2 crdn1_indeces v2 Rij i2 i1 for in range i3 crdn1_indeces v3 Rij i3 i1 angle angle_deg_between v2 v3 angles append angle return angles
923	def get_dihedral_angles Rij atom1 atom2 dihedral_angles crdn1 list neighbors atom1 crdn2 list neighbors atom2 crdn1 remove atom2 crdn2 remove atom1 for c1 in crdn1 for c2 in crdn2 if c1 c2 continue j1 int atom1 split j2 int atom2 split i1 int c1 split i2 int c2 split v0 Rij i1 i2 v1 Rij i1 j1 v2 Rij i2 j2 uv0 unit_vector v0 w1 v1 np dot v1 uv0 uv0 w2 v2 np dot v2 uv0 uv0 if length w1 1e-8 or length w2 1e-8 continue angle angle_deg_between w1 w2 dihedral_angles append angle return dihedral_angles
924	import pandas as pd import torch import torch nn functional as from torch optim import Adam import schnetpack as spk import schnetpack atomistic as atm import schnetpack representation as rep from schnetpack datasets import device torch device "cuda"
925	import os for dirname filenames in os walk '/kaggle/input' for filename in filenames print os path join dirname filename
926	plt pie train value_counts labels autopct "%.1f%%" plt title plt show
927	import pandas as pd import torch import torch nn functional as from torch optim import Adam import schnetpack as spk import schnetpack atomistic as atm import schnetpack representation as rep from schnetpack datasets import device torch device "cuda"
928	import pandas as pd import torch import torch nn functional as from torch optim import Adam import schnetpack as spk import schnetpack atomistic as atm import schnetpack representation as rep from schnetpack datasets import device torch device "cuda" data QM9 "qm9/" properties QM9 U0 remove_uncharacterized True energies data QM9 U0 item for in range len data energies pd Series energies name QM9 U0 display energies describe ax energies hist bins ax set_xlabel QM9 U0
929	def get_size_list targets dir_target result list for target in tqdm targets img np array Image open os path join dir_target target result append str img shape return result
930	data pd read_csv '../input/train.csv' data 'size_info' get_size_list data Image tolist dir_target '../input/train' data to_csv './size_train.csv' index False
931	counts data size_info value_counts agg data groupby 'size_info' Id agg 'number_sample' len 'rate_new_whale' lambda np mean 'new_whale' agg agg sort_values 'number_sample' ascending False agg to_csv 'result.csv' print agg head
932	imageio imwrite 'quality-70.jpg' original_img quality new_img skimage io imread 'quality-70.jpg' plt figure figsize plt imshow new_img plt title "quality-70" plt show
933	imageio imwrite 'quality-90.jpg' original_img quality new_img skimage io imread 'quality-90.jpg' plt figure figsize plt imshow new_img plt title "quality-90" plt show
934	import random import numpy as np import pandas as pd import chainer import chainer_chemistry from IPython display import display
935	list_atoms list set structures 'atom' print 'list of atoms' print list_atoms train_graphs list train_targets list print 'preprocess training molecules ...' for mole in train_moles train_graphs append Graph structures_groups get_group mole list_atoms train_targets append train_gp get_group mole valid_graphs list valid_targets list print 'preprocess validation molecules ...' for mole in valid_moles valid_graphs append Graph structures_groups get_group mole list_atoms valid_targets append valid_gp get_group mole test_graphs list test_targets list print 'preprocess test molecules ...' for mole in test_moles test_graphs append Graph structures_groups get_group mole list_atoms test_targets append test_gp get_group mole
936	train_iter chainer iterators SerialIterator train_dataset batch_size order_sampler train_sampler valid_iter chainer iterators SerialIterator valid_dataset batch_size repeat False order_sampler valid_sampler test_iter chainer iterators SerialIterator test_dataset batch_size repeat False order_sampler test_sampler
937	from chainer import optimizers optimizer optimizers Adam alpha 1e-3 optimizer setup model
938	for im in df ImageId unique image df df ImageId im for cType in image ClassType unique polygonsList cType loads image image ClassType cType MultipolygonWKT values fig ax plt subplots figsize for in polygonsList for polygon in polygonsList mpl_poly Polygon np array polygon exterior color plt cm lw alpha ax add_patch mpl_poly ax relim ax autoscale_view plt show
939	from PIL import Image import os os listdir '../input/three_band' with open '../input/three_band/6120_2_2.tif' encoding 'utf-8' errors 'ignore' as print readlines
940	model Sampler train test model compute_bounds model compute_samples sample_sub pd read_csv '/kaggle/input/liverpool-ion-switching/sample_submission.csv' sample_sub 'open_channels' np array astype 'int64' sample_sub to_csv 'submission_0.csv' index False float_format '%.4f'
941	train_sales pd read_csv '/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv' sell_prices pd read_csv '/kaggle/input/m5-forecasting-accuracy/sell_prices.csv' calendar pd read_csv '/kaggle/input/m5-forecasting-accuracy/calendar.csv' submission_file pd read_csv '/kaggle/input/m5-forecasting-accuracy/sample_submission.csv'
942	def disp_boxplot data title xlabel ylabel sns set_style 'whitegrid' sns set_context 'poster' palette sns color_palette "mako_r" ax sns boxplot data data palette palette ax set title title xlabel xlabel ylabel ylabel try ax axhline data mean mean color 'b' label linestyle linewidth ax ahline data median median color 'g' label linestyle linewidth except pass ax set_xticklabels ax get_xticklabels rotation plt legend plt show
943	n_items_dept train_sales 'dept_id' value_counts mean_of_total_sales_per_dept dept_sum mean axis ax sns regplot n_items_dept mean_of_total_sales_per_dept ax set title xlabel ylabel plt show
944	cat_sum train_sales groupby 'cat_id' sum reset_index drop True disp_boxplot data cat_sum title xlabel ylabel
945	state_sum train_sales groupby 'state_id' sum reset_index drop True state_mean train_sales groupby 'state_id' mean reset_index drop True disp_boxplot data state_sum title xlabel ylabel disp_boxplot data state_mean title xlabel ylabel
946	store_sum train_sales groupby 'store_id' sum reset_index drop True store_mean train_sales groupby 'store_id' mean reset_index drop True disp_boxplot data store_sum title xlabel ylabel disp_boxplot data store_mean title xlabel ylabel
947	ax sns regplot np arange dept_sales shape dept_sales scatter_kws 'color' 'blue' 'alpha' order line_kws 'color' 'green' ax set title xlabel ylabel plt show
948	from statsmodels tsa statespace sarimax import SARIMAX def sarima_train_test t_series NUM_TO_FORECAST do_plot_results True NUM_TO_FORECAST NUM_TO_FORECAST dates np arange t_series shape model SARIMAX t_series order trend 'c' results model fit results plot_diagnostics figsize plt show forecast results get_prediction start NUM_TO_FORECAST mean_forecast forecast predicted_mean conf_int forecast conf_int print mean_forecast shape plt figure figsize plt plot dates NUM_TO_FORECAST mean_forecast values color 'red' label 'forecast' plt plot dates NUM_TO_FORECAST t_series iloc NUM_TO_FORECAST color 'blue' label 'actual' plt legend plt title plt show residuals results resid mae_sarima np mean np abs residuals print mae_sarima print results summary
949	submission_df to_csv 'submission.csv' index False print submission_df shape print
950	columns train columns for cc in tqdm_notebook columns train cc train cc fillna train cc mode test cc test cc fillna test cc mode
951	train copy test copy ohe OneHotEncoder dtype 'uint16' handle_unknown "ignore" ohe fit train ohe transform train ohe transform test
952	pd read_csv "/kaggle/input/cat-in-the-dat-ii/train.csv" Xt pd read_csv "/kaggle/input/cat-in-the-dat-ii/test.csv" target values id_train id id_test Xt id drop 'id' 'target' axis inplace True Xt drop 'id' axis inplace True binary_vars for in columns if 'bin_' in nominal_vars for in columns if 'nom_' in high_cardinality for in nominal_vars if len unique low_cardinality for in nominal_vars if len unique ordinal_vars for in columns if 'ord_' in time_vars 'day' 'month'
953	ent np zeros shape ent_temp np zeros cv KFold shuffle False for idx in tqdm_notebook range shape for idx2 train_idx test_idx in enumerate cv split idx ent_temp idx2 entropy_fast idx test_idx ent idx np mean ent_temp
954	plt figure figsize plt plot ent plt xlabel 'time' plt ylabel 'entropy'
955	plt figure figsize plt plot feature 'r' plt plot 'k' plt ylabel 'TTF' plt xlabel 'time' plt grid
956	import matplotlib pyplot as plt import numpy as np import pandas as pd from math import floor log from scipy stats import skew kurtosis from scipy io import loadmat
957	def openfile_dialog return '../input/train_1/1_25_1.mat'
958	def convertMatToDictionary path try mat loadmat path names mat dtype names ndata mat for in names except ValueError print path ndata None return ndata
959	def defineEEGFreqs return np array
960	def hjorthFD Kmax len for in range Kmax Lk for in range Lmk for in range floor Lmk np abs Lmk Lmk floor Lk append Lmk append np log np nanmean Lk append np log float r1 r2 np linalg lstsq return
961	def petrosianFD None if is None np diff for in range len if len return np log10 np log10 np log10
962	def katzFD epoch np abs epoch epoch max len epoch return np log np log
963	def higuchiFD epoch Kmax len epoch Lmk np zeros Kmax Kmax for in range Kmax for in range Lmki maxI floor for in range maxI Lmki Lmki np abs epoch epoch normFactor maxI Lmk normFactor Lmki Lk np zeros Kmax for in range Kmax Lk np nansum Lmk range lnLk np log Lk lnk np log np divide range Kmax fit np polyfit lnk lnLk return fit
964	def replaceZeroRuns df return df replace np nan fillna
965	from sklearn import preprocessing def normalizeFeatures df min_max_scaler preprocessing MinMaxScaler x_scaled min_max_scaler fit_transform df df_normalized pd DataFrame x_scaled columns df columns return df_normalized def normalizePanel pf pf2 for in range pf shape pf2 normalizeFeatures pf ix return pd Panel pf2
966	from os import listdir def ieegGetFilePaths directory extension '.mat' filenames sorted listdir directory files_with_extension directory for in filenames if endswith extension and not startswith return files_with_extension
967	train pd read_csv '../input/osic-pulmonary-fibrosis-progression/train.csv' test pd read_csv '../input/osic-pulmonary-fibrosis-progression/test.csv' submission pd read_csv '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv'
968	def calculate_height row height if row or height row row return int height all_data all_data apply calculate_height axis def FEV1 row FEV if row FEV row else FEV row return FEV all_data 'FEV' all_data apply FEV1 axis
969	mse mean_squared_error train 'FVC' predictions squared False mae mean_absolute_error train 'FVC' predictions print format mse print format mae
970	new_df train_x groupby train_x Patient train_x Age train_x Sex train_x SmokingStatus count new_df index new_df index set_names 'id' new_df new_df reset_index new_df rename columns 'freq' inplace True fig px bar new_df 'id' 'freq' color 'freq' fig update_layout xaxis 'categoryorder' 'total ascending' title fig update_xaxes showticklabels False fig show
971	fig px histogram new_df nbins fig update_traces marker_color 'rgb(158,202,225)' marker_line_color 'rgb(8,48,107)' marker_line_width opacity fig update_layout title fig show
972	fig px histogram train_x color color_discrete_map 'yellow' 'cyan' 'green' hover_data train_x columns fig update_layout title fig update_traces marker_line_color 'black' marker_line_width opacity fig show
973	def load_scan path slices pydicom read_file path for in os listdir path slices sort key lambda int InstanceNumber try slice_thickness np abs slices ImagePositionPatient slices ImagePositionPatient except slice_thickness np abs slices SliceLocation slices SliceLocation for in slices SliceThickness slice_thickness return slices def get_pixels_hu scans image np stack pixel_array for in scans image image astype np int16 image image intercept scans RescaleIntercept slope scans RescaleSlope if slope image slope image astype np float64 image image astype np int16 image np int16 intercept return np array image dtype np int16
974	test_patient_scans load_scan data_path patients test_patient_images get_pixels_hu test_patient_scans for imgs in range len test_patient_images ax1 ax2 ax3 plt subplots sharey True figsize ax1 imshow test_patient_images imgs cmap plt cm bone ax1 set_title ax2 imshow test_patient_images imgs cmap plt cm bone ax2 set_title ax3 imshow test_patient_images imgs cmap plt cm bone ax3 set_title plt show
975	def eval_metric FVC sigma len sigma np empty fill sigma_clipped np maximum sigma delta np minimum np abs FVC eval_metric np sqrt delta sigma_clipped np log np sqrt sigma_clipped return eval_metric
976	sub_df pd read_csv '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv' print f"The sample submission contains: {sub_df.shape[0]} rows and {sub_df.shape[1]} columns."
977	from sklearn preprocessing import OneHotEncoder LabelEncoder from sklearn preprocessing import StandardScaler MinMaxScaler RobustScaler from sklearn compose import ColumnTransformer no_transform_attribs 'min_week' num_attribs 'FVC' 'baselined_week' cat_attribs
978	train train 'sigma_clipped' train apply lambda max train 'diff' abs train 'FVC' train train 'delta' train 'diff' apply lambda min train 'score' math sqrt train 'delta' train 'sigma_clipped' np log math sqrt train 'sigma_clipped' score train 'score' mean print score
979	import random from tqdm notebook import tqdm from sklearn model_selection import train_test_split KFold from sklearn metrics import mean_absolute_error from tensorflow_addons optimizers import RectifiedAdam from tensorflow keras import Model import tensorflow keras backend as import tensorflow keras layers as import tensorflow keras models as from tensorflow keras optimizers import Nadam import seaborn as sns import plotly express as px import plotly graph_objects as go from PIL import Image import tensorflow as tf
980	def region_plot df data df copy data 'time_to_failure' data 'time_to_failure' data 'time' data index data 'time' data 'time' 4e6 data data 'time' data 'time' min data 'acoustic_data' 'time_to_failure' plot figsize return
981	df1 depths set_index 'id' df2 train_masks set_index 'id' dataset pd concat df1 df2 axis join 'inner' dataset dataset reset_index
982	def apply_all_aug img no_grid False DIM IMAGE_SIZE if tf random uniform if tf random uniform img transform img else img dropout img PROBABILITY else if not no_grid if tf random uniform img apply_grid_mask img else img img else if tf random uniform img transform img else img img return tf reshape img DIM DIM
983	from fastai2 basics import from fastai2 vision all import from fastai2 medical imaging import
984	file_dir Path dicom_meta pd DataFrame from_dicoms file_dir ls print f"Extracted DICOM data is of dimension: {dicom_meta.shape}" dicom_meta head
985	index dicom_meta pivot_table values 'img_mean' 'img_max' 'img_min' index index aggfunc 'img_mean' 'mean' 'img_max' 'max' 'img_min' 'min' 'count'
986	def plot_learning_curves results fig ax plt subplots len results 'histories' figsize for result in enumerate results 'histories' for history in result ax plot history history 'loss' color 'C0' ax plot history history 'val_loss' color 'C1' ax set_title f"{results['models'][i]}" ax set_ylabel 'MCRMSE' ax set_xlabel ax legend 'train' 'validation' loc 'upper right' results "models" 'GRU' 'LSTM' "histories" gru_histories lstm_histories
987	n_groups train "group" for in range n_groups ids np arange train loc ids "group" n_groups test "group" for in range n_groups ids np arange test loc ids "group" train 'signal_scaled' test 'signal_scaled' train 'signal_2' test 'signal_2'
