651	Create a submission
1241	Read the image from image id
1125	Verify that the model has not overfit
212	Compute mean for all features
257	Fitting and predicting
1167	Import required libraries
637	If you like it , Please upvote
514	NCAA Tourney Summary
764	create a list of markers
834	Define an ECDF function
1134	Visualize Validation Predictions
28	MODEL WITH VECTOR MACHINE
911	Take one sample of categorical values
171	Pooling and final linear layer
354	printing the result
1506	FIND ORIGIN PIXEL VALUES
939	And here is a table of results
1479	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1101	Run LightGBM model
857	Distribution of Validation Sets
1464	Create Testing Data Set
779	We can see that we are almost equal
96	Save the before.pbz data for plotting
964	Load sample sample
287	Evaluate the model
1193	Go to actual revenues
1116	Returns the counts of each type of rating that a rater made
1392	Save the dataframe to output path
139	How to Check For Any Type
1316	fill missing values with zero
520	Train the model
1475	MAKE MIXUP IMAGE
1153	Lets look at the data
1380	drop ridge from y
1246	plot the training history
1815	Load the Lyft dataset
348	highlight high correlation features
1336	Skip connection and drop connect
648	Load the data
1178	take a look of .dcm extension
432	Predict in batches of folds
320	Fitting the gradient boosting model
930	Save results as csv
1481	Distribuitions of the Provolutional Features
405	calculate the fit vector
667	Add log transformation
1594	Tokenize the sentences
1306	Importing relevant Libraries
540	Create pipeline objects
1017	Sort the table by percentage of missing descending
1529	Read candidates with real multiple processes
447	Encoding the Regions
1677	Class Imbalance Problem
1221	Get the index of the best score
665	Create date column
402	Credits and comments on changes
318	Decision Tree Regression
1800	Plot the predictions
1646	WBF over TPU
1123	LGBM Dataset Formatting
603	Prepare the data for modeling
484	import the necessary packages
668	Load and Preprocess Data
438	Data Exploration and Feature Engineering
579	Calculate logmel spectrogram using pytorch
1326	Round number of filters based on depth multiplier
971	Now we will read our data
1544	Override the data frame with the input data
819	then apply the reductions to the dataframe
945	iteration score 疸번갱
833	Plotting the distribution of fare
1595	Pad the sentences
1559	Linear Corellation check
946	altair is a very nice plotting library by the way
1621	Sometimes , a type can also be identified
570	Order Count Across Days of the Week
666	Create dataframe with aggregated features
325	LOAD TRAIN AND TEST
417	Preview of the data
1237	Decoding the image
4	Remove Unused Columns
393	What are the most frequent clusters
888	Merging previous training and testing sets
1218	we need to compute the world and type stats
573	Iterest per Bathrooms
1637	Cumulative count of each feature
1275	set color to background
1377	Visualizing the augmented images
802	Adding missing values
1335	Squeeze and Excitation
558	convert list of lists to integer
332	iterate over all samples in the batch
912	The code for this section is copied from
1395	Draw the graphs
990	Using installments due dates
5	Encode Categorical Data
758	Find the households where the family members do not have the same target
1672	Initialize patient entry into parsed
122	Define function for cleaning special characters
1683	contrast , etc
198	Binary Thresholding ..
707	Compute CV score
874	Fitting and predicting
1295	Prediction of Train Data
1740	How many enemies DBNOs are there
1766	cross validation and metrics
1250	Save the best model
467	Precision and Recall curve
1074	Exploring the data
652	Importing necessary modules and Reading the data
1759	define a matrix of features
380	Check if the length of the item is correct
17	Now extract the data from the new transactions
1788	Plot the peak frequency
676	append the data to our list
496	extracting target data
632	Load the data
1390	Creating train dataset
1206	Define the densenet layer
1664	Download and Import Dependencies
1688	Check if all the pixmaps are the same
621	get all contour areas
1618	average the predictions from different folds
1186	the current session is a new action
378	compute mean absolute error
1197	We create a dataframe that contains all the predictions
994	The most common client type
1676	Sample Patient ID
1174	Define a color for the bkg
517	Get the seeds as integers
1432	Link count and node count
1600	Which we can plot up ..
1102	Submit to Kaggle
200	Importing Packages and Functions
1802	Converting image to world coordinates
1145	Active Curve Fit
1213	Create strategy from tpu
1235	draw rectangle around image
1543	STAIN NORMALIZATION FUNCTIONS
1474	Iterate through the dataset
1027	get best score for each class
1558	create watershed Marker
374	Avoid division by zero by setting zero values to tiny float
739	Check if the autocorrelation is high enough
440	Plotting the most commmon Paths
248	Drop target variable
1384	normalize the image
177	Most common category
442	Latitude and Longitude
1188	Calculate average accuracy of each assessment for each title
1749	Create entity from dataframe
835	New number of observations
424	Scatter plot of meter reading
262	Parameters and LB score visualization
1091	Create a submission
864	Fitting the baseline model on the test data
90	fast less accurate
156	create dataframe from train.csv file
1512	size and spacing
1465	Define dataset and model
836	Zoom to images
1731	Function to create a video from a list of images
582	Calculate batch probability
757	Plot the label counts
1588	Find the labels that have the highest probability
1297	Predicting the Test Set
768	Which walls do we have
155	Download rate evolution over the day
797	convert list to numpy array
1442	Process Each Patient
67	split into train and validation filenames
1501	Detect hardware , return appropriate distribution strategy
1023	encode ohe features
58	Apply data augmentation to the images
1279	reset and identify the objects
1484	It is from the public kernel
631	Computes the prime variables of the model
806	cast floats of subsample parameters to int
777	Plot the kurtosis
1259	Using original generator
1276	Iterate over examples
340	Creating Prediction dataframe
1563	Evaluate XGBoost model on train set
509	Here is the custom function that evaluate the threshold
1765	Plot the results
1477	Iterate through data
1319	get train and test data
1711	Read data from the CSV file
1681	the accurace is the all time wins divided by the all time attempts
1014	Clean up memory
829	Read the image on which data augmentaion is to be performed
321	Define ExtraTrees Regressor
1089	Resize test predictions if necessary
553	Predict on test set
546	Create pipeline objects
1037	Compute the memory usage of a dataframe
349	highlight value as a color
1108	Load image file
1468	Find max value of a feature for both train and test data
1189	generate data sets
769	Create roof column
242	Filter Albania , run the RReg workflow
1070	Converting image to RGB
1143	Create empty arrays to store results
1562	Split data into train and test
197	Plotting the blackhat
353	over all five training folds
75	create train and validation generators
625	Reordered Cases by Day
195	inpaint with threshold image
1610	Log transformation of target variable y
986	Extracting date features from start dates
1440	Preparing the training data
1289	Get the input shape of the image
1585	fill all na as
1439	Loading the data
817	Run method for this fold
27	Looking at the distribution of the target variable
217	MinMax scale all features
1190	Build a list of event codes
1142	sum of absolute values
1818	Check if an index is one of the indecies
351	Random Forest Regressor
114	Add state lag features
714	Count the missing values in each column
554	Add RUC metric to monitor NN
64	Lets Plot the distribution of values in the continuous variables
1627	Display country predictions
506	Create a function to draw a boxplot of transactions revenue
608	path is relative to the base directory
1813	summarize history for loss
1078	Set values for various parameters
1674	Add boxes with random color if present
1446	Order does not matter since we will be shuffling the data anyway
889	realligning the shapes
674	Setting X and y
152	Plot the heatmap
74	cosine learning rate annealing
789	Ignore the warnings
688	draw box over face
1010	Adding the column to the list
1777	plot the heatmap
247	Apply exponential transf
245	Rreg on Andorra
972	Plot random search and Bayesian Optimization results
1085	If original image size is different to target image size
1350	iterate through all the columns of a dataframe and modify the data type
855	Stratified Train and Validation
65	save pneumonia location in dictionary
978	huge difference in the importances
1214	Order does not matter since we will be shuffling the data anyway
1038	Previous aggregation numeric features
283	Sample out data
1763	Create train and test df with missing values
1025	Create a model
322	Voting Regression Model
1741	HANDLE MISSING VALUES
1668	Total Sales by Store
1591	Load the best weights and check the performance
1126	Apply mask to the image
717	We can remove outliers in the test set
1695	Plot some examples
1518	Load the raw training dataset
499	handle .ahi files
253	Split training set to validation set
1047	Reading and preparing data
699	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
713	PREDICTIONS LSTM
1454	inverse transform test set
837	set alpha on legendHandle
1117	Compute QWK based on OOF train predictions
390	Set some parameters
1722	The mean of the two is used as the final embedding matrix
650	Define some network parameters
11	Compute the STA and the LTA
479	Hashing the text
218	MinMax scale all feature
1154	Checking the difference between train and test set
1003	define a matrix of features
57	Ensure determinism in the results
1369	create a annotations for each district
1623	Logistic Regression Model
117	Submit to Kaggle
1260	Combine the filename column with the variable column
50	Create pandas dataframe with the grouped variables
1636	Extracting birds from click hour
1811	Printing the crosstab that we did above
1346	Visualizing the results
685	Create a dataframe with the labels count
299	Resizing the image
1461	Make a Baseline model
593	factorize categorical features
61	Exploratory Data Analysis
576	Set some parameters
1066	First dense layer
196	Show original and grayscale
842	Heat Correlation with Fare Amount
1691	Original code from
1810	from sklearn.metrics import x
490	ploting the distribution of application training values
931	count possible combinations
1341	plot the measured and unmeasured lines
385	Lets validate the test files
726	We can deal great wildlife picures
357	Looking at the data
1043	Print some summary information
1552	Average Day of Year
1807	Load in the data
282	Looking at the data
1081	Fit the model
1016	Bureau balance by loans
1209	Save model and weights
316	Fitting a linear SVR model
422	Scatter plot of meter reading hour
1362	Preprocess date column
1542	STAIN NORMALIZATION FUNCTIONS
1596	Checking for Null values
1030	huge difference in the importances
100	code takesn from
1019	Merging All the dataset
934	Fitting and predicting
1632	Get the fulltable us dataframe
755	replace some datatypes
1399	Handle missing values
1702	Evaluate each candidate with the given task
997	DETERMINE SEED FEATURES
115	define a grid of parameters
1344	Plot samples from the train set
745	build a dict to convert surface names into numbers
727	Most of the categories of items
404	initialise empty arrays for storing results
41	Null values in the training and testing data
923	Cumulative importance plot
1535	Load and preview Data
585	Yeah , there are two stories per year
1173	convert to HU
285	set label for exit tissue
1385	suppose all instances are not crowd
420	ELECTRICITY THE MOST FREQUENT METER TYPE
360	Visualizing the training dataset
289	NumtaDB Classification Report
295	Defining the target variable
1737	The competition metric relies only on the order of recods ignoring IDs
969	Split into train and test
1592	some config values
51	place on the left , right and bottom borders
985	Now lets visualize the example credit
606	Only the classes that are true for each sample will be filled in
703	STRATIFIED K FOLD
395	Ekush Confusion Matrix
1405	Finally , we need to compute the rolling mean
1020	pairs of removed columns
878	ROC AUC scores
1498	Get the names of the decay variables
725	Logistic Regression without Standardization
221	highlight value as a color
1315	Combining all the digits from the string
1379	apply warp to image
937	Create a file and open a connection
1572	Number of data per each diagnosis
444	Latitude and Longitude
944	add the hyperparameters to the dataframe
138	Lets view some predictions and detected objects
1705	Search for a best candidate
619	get all contour areas
263	Parameters to check
1602	Moving Average model with window size
1042	Sort the table by percentage of missing descending
1191	create submission file
583	Compute batch probability
1516	Detect hardware , return appropriate distribution strategy
1729	Add train leak
877	Evaluate the models
1525	Span logits minus the cls logits seems to be close to the best
311	Just a list of all the columns that are target
1292	train all layers
1773	for numerical stability in the loss
1226	Plotting some random images to check how cleaning works
744	Map image id to filepath
709	FIND ORIGIN PIXEL VALUES
239	Filter Italy , run the RReg workflow
673	And finally , build the training and validation sets
1058	Initialize mode and load trained weights
225	Scatter plot of COVID
869	Create a file and open a connection
1071	load the face on the image
999	loop over all the sequences and calculate the longest repetition
886	Load raw data
1271	Check objectness of each image
704	MODEL AND PREDICT WITH QDA
87	paths , labels , fake
610	path is relative to the base directory
46	Create pandas dataframe with the month count
792	merge with predictions
1458	checking missing data
743	Create new DataFrame by pivoting
103	Split into features and targets
39	Read a batch
1697	if image list is empty return empty list
661	get different test sets and process each
1072	You can access the actual attribute
1366	load the districts
1099	predict validation and test
1208	Create model and training parameters
1124	predict oof and test
677	filtering out outliers
1580	MAKE EVALUATE FEATURES
1269	otherwise use original image as background
476	Vectorize the texts
706	MODEL AND PREDICT WITH QDA
1243	to truncate it
84	Class Distribution and Frequency
780	Aggregate some features
365	SVR and AUC
1725	The method for training is borrowed from
1601	checking missing data
1069	Write the prediction to file for submission
1077	Optionally , remove stop words
1780	The wordcloud of the raven for Edgar Allen Poe
1301	Process test set
1724	text version of squash , slight different from original one
790	RF model on Countvectorizer
1011	count categorical features
974	add the hyperparameters to the dataframe
846	Baseline Training and Validation metrics
900	bad encoding , must be ohe or le
1330	Encodes a list of BlockArgs to a list of strings
1437	Number of Patients and Images in Test Images Folder
1312	create a dataframe for validation and train
133	Spliting the data into training and validation set
1655	Draw the heatmap using seaborn
1287	Display the images we have to blurry
1352	Leak Data loading and concat
414	Importing necessary libraries
1820	Check for missing values
107	unique value counts
698	Applying CRF seems to have smoothed the model output
1240	Reading the submission file
820	Random Forest Classifier
1307	Mask for padding
288	from sklearn.metrics import auc
297	Spliting the training and validation sets
690	iterating over the dataset
1433	Plot the heatmap
1333	Squeeze and Excitation layer , if desired
550	Create pipeline objects
243	Filter Albania , run the RReg workflow
369	Split training set to validation set
596	set bird to 1
1473	MAKE CUTMIX LABEL
1272	check if all pairs are in succession
1041	Clean up memory
1252	Load Model Weights
392	Resize images to expected size
246	Set the dataframe where we will update the predictions
1104	Credit card balance
1561	Columns to be engineered
1786	read train.parquet file
1575	More is coming Soon
375	build model on all data
1577	Create the continuous features list
1393	Converting categorical features to cateogry
497	reducing sample data
224	Convert LB score to integer
1505	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1450	Create train and test datasets
233	Create date columns
1801	k is camera instrinsic matrix
345	Now let us define a generator that allocates large objects
35	Generate an array of embeddings from train text
189	Number of characters in the description
1365	Download the data
52	create colors based on the count
383	setup the paths of the data
1772	always call this before training for deterministic results
1288	Load dataset info
149	Now , we can look at the distribution of the values
1109	Unique IDs from train and test
732	Read and preprocess an image
76	load and shuffle filenames
167	Only the classes that are true for each sample will be filled in
449	Exploring the data
1273	in the future I plan to change and remove this object
919	Reading cash data
1347	iterate through all the columns of a dataframe and modify the data type
1690	return a sorted list of the elements in the list
1254	Predicting on test set
1617	remove layter activation layer and use losvasz loss
60	Predict and Submit
1423	function to transform sentence to word list
293	function to extract the id of a file from a string
256	Voting Regression Model
1718	Tokenize the sentences
1404	Create submission file
746	take one sample of labels
1494	Generate predictions for test set
19	Imputations and Data Transformation
22	Impute any values will significantly affect the RMSE score for test set
589	No of storesys Vs Log error
733	Read and preprocess an image
1758	Relationship between applications with pos balance
1614	Split the train dataset into development and valid based on time
229	Calculate the average ensembles for each target
1359	Fast data loading
1671	Preparing the data
186	Shipping dependencies of prices
44	We can see there is no missing values
1304	Clear GPU memory
1599	checking missing data
1082	Pad images to make the dataset smaller
1257	Helper function for building new train and test data
901	parameter value is copied from
1622	Print the feature ranking
1713	Now , define the model
341	Change columns names
670	Define a function to transpose a dataframe
241	Filter Germany , run the RReg workflow
1567	Pinball loss for multiple quantiles
1445	correct the confidence
73	create network and compiler
352	Load the data
48	create colors based on the count
729	Coefficient of variation for prices in different recognized image categories
1130	define training dataset loader
113	Join the two new features to the main dataframe
1004	define a matrix of features
132	Create Testing Generator
1417	Load Model into TPU
1210	Pads the image
386	Check if the length of the item is correct
1756	Relationship between applications
538	Running the best algorithm
270	It is good to keep these scaling values for later
1308	The first frame is the original frame
1784	Compute the STA and the LTA
1709	Importing sklearn libraries
1267	Now we can compute the text and questions from the test set
1151	convert training set to binary
723	Sort ordinal feature values
1106	Load metadata file
1778	Import train and test csv data
446	Encoding the Regions
1318	Train the model
1665	fill in mean for floats
1699	iterate over all samples
730	Add time calculation features
1353	iterate through all the columns of a dataframe and modify the data type
434	OSIC Pseudo Labeling with Disease
1248	Load and preprocess data
120	Function to create vocabulary from text
694	remove activation layer and use losvasz loss
1135	load timestamps data
69	add trailing channel dimension
1500	Printing results to file
1686	FIND ORIGIN PIXEL VALUES
1483	Features to use for training
1105	load mapping dictionaries
16	To plot pretty figures
849	get sorted feature importances
1554	Go Go Go
1277	reset and identify the objects
364	Now scale each feature in dataframe
355	How many duplicate clicks have different target values
1694	Plotting multiple images using subplots
762	create a scatter plot
915	return unique info row
1754	Relationship between applications and installments
1420	Building the model
1649	extra data used in competition
1222	collect parameters from grid
605	Pads audio to make it unified length to L
1388	add boxes to target
330	Actual run of the model
290	Clean up base directory
1035	Take one sample of categorical values
870	Write column names
1743	EXTRACT DEVELOPTMENT TEST
1814	Copy predictions to submission file
8	merge with building info
1555	Build lightgbm model
1165	Computes and stores the mean and standard deviation of the model
421	APPLY DEFINED READINGS FOR WEEKDAYS
822	Read the image on which data augmentaion is to be performed
1480	batch grid mask
1441	Preprocess test images
613	Importing the libraries
343	Create a generator that iterate over data
101	load the image file using cv
884	Loading the data
1299	Function for generating language embeddings
23	Remove the Outliers if any
701	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
903	get best score for each class
601	And the above one is our final working image
1706	Print best candidates info
1084	to not overload memory
957	Create features for the previous installments and credit
641	Common positive words
1161	FIND ORIGIN PIXEL VALUES
958	We can see there are no missing values
611	Save images to directory
398	NumtaDB Confusion Matrix
1533	Plot mean ROC
124	Change percent of each value
1539	CLEAN TRAINING DATA
481	Tokenize the sentences
1309	The first frame is the original frame
693	Load the model from the previous run
428	all other columns
896	huge difference in the importances
296	Split the data into two sets
1537	Breakdown Topic Engineering
15	Common data processors
1300	Performing some cleaning and engineering on text
897	Cumulative importance plot
384	to reduce memory usage , dtype is specified
379	Calculate mean square error
510	process remaining batch
204	Remove other air pockets insided body
882	Fit the model
1495	If not , prepare the data
980	Define Ordinal app test types
466	ROC curve and AUC
1050	bad encoding , must be ohe or le
1410	Gaussian Mixture Clustering
85	so we have to train on fake data
1746	Replace some outliers
1553	Average values for all day weeks
1052	Train the model with early stopping
620	get all contour areas
165	Otsu Method for Masking
858	plot the validation results
1079	loop for every word
1680	the time spent in the app so far
36	Target Variable Analysis
599	selecting a sample image
1034	Take one row from the dataset
1132	predict on validation set
1080	Divide the result by the number of words to get the average
1783	Generating the wordcloud with the values under the category dataframe
1521	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1092	Applying CRF seems to have smoothed the model output
574	Count the bedrooms having high interest level
860	Set up the model
1343	Plot SN filter error
1586	Dropping duplicate columns
1127	Read in the data
747	Make all layers following the layer name layer trainable
1515	Load the data
1805	Lets look at the memory usage of each dataframe
222	Fitting a Linear SVR model
1744	FITTING THE MODEL
1236	Loop over text texts
675	grid search for this estimator
1036	Average values for all repaid and not paid variables
543	Predict on test set
1111	Extract processed data and format them as DFs
1032	Drop all variables except for the parent variable
1647	folds should be a list
656	combine the categorical features into one dataframe
259	Convert LB score to integer
106	without any limitations and dtype modification
425	I will see the distribution of the primary use
1266	Preparing the test data
110	Plotting sales volume per year
735	Classify an image with different models
258	Train and predict
55	Get the number of clusters
1367	Plot the districts
917	Previous Application Features
865	cast floats of hyperparameters to int
1183	Get the configs for this classifier
907	Compute the size of each column
175	load the image data
816	Fast cappa eval function for lgb
495	Setup the paths to the files
1403	Get the decode dates
367	SGD regressor model
607	Return a normalized weight vector for the contributions of each class
102	grid mask augmentation
838	Plot the dropoff locations
511	Extracting wins and loses
591	Noise augmentation with Gaussian target noise
1513	Order does not matter since we will be shuffling the data anyway
1097	Check if train and test overlap
692	Load the model from the previous run
1394	Read in the masks
1767	Tokenize the sentences
437	Preview of the Data
314	Drop target variable
1261	Create test generator
1224	select proper model parameters
880	Reading the datasets
334	Looking some shapes
230	Implementing the SIR model
954	Creating the training and testing sets
1652	Process to prepare the data
174	Loading the image resizing function
141	look at the histogram plot for isFraud
1176	Save images to a list
1391	Plot bounding boxes on the image
298	Resizing the image
1692	Lets return a list of results from each function
731	Pairplot of full hits table
970	Final Training and Testing Data
329	iterate over all samples in the batch
1661	Get the list of alphabetically sorted labels
10	merge weather data
809	Fit the model with early stopping
867	Extract nested conditional parameters
1673	Add box if opacity is present
996	DETERMINE SEED FEATURES
564	Convert item descriptions to integers
1460	checking missing data
1509	LIST DESTINATION PIXEL INDICES
818	add the columns for the test reduction
530	Running the best algorithm
202	Determine current pixel spacing
848	Create random Forest Object using the mentioned parameters
873	Write column names
1298	between train and test
1425	Total number of tokens
1654	Select columns with correlations
1753	Add Bureau as a Relationship
1140	Plot the dependence plot
93	Set values before aft
1620	checking missing data
615	An optimizer for rounding thresholds
811	Create a file and open a connection
959	Aggregate all features by type
1511	define the input layers
1482	If not , prepare the data
1382	removing common words and stemming
687	Get the image and label for the given index
1662	Show some images
1239	Run the model
1148	Define some image and input sizes
273	get lead and lags features
1234	Model initialization and fitting on train and valid sets
1488	Eval data available for a single example
303	Preparing the train and validation sets
82	Create the submission file
1363	You can change this and make it more efficient
1073	Exploring the data
1578	simplest NaN imputation
1570	Convert DICOM to PNG
1524	Eval data available for a single example
756	Plot the distribution of target variable i.e
1076	Comment Length Analysis
362	Load image from image id and type
363	Drop target variable
1798	shift train predictions for plotting
249	SVR and AUC
277	reorder the input data
159	Extracting labels from the mask
1504	LIST DESTINATION PIXEL INDICES
1231	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
634	Run the model
557	No. of times Feature interacted
199	inpaint with threshold image
1607	one hot encode features
549	Predict on test set
131	Prepare Testing Data
1060	Split the training data to train and validation set
1497	Get the pretrained model
119	Pulmonary Condition Progression by Sex
1466	Turn off gradients
286	Define train and validation paths
832	We submit the solution
281	The number of train and test files
1669	gather input and output parts of the pattern
584	concatenates all and prints the max
30	Read the data
916	Bureau info table
927	get score and hyperparameters
995	The most common client type
412	Retrieving the Data
793	Choose and initialize a selector
841	Euclidean distance by Fare Amount
587	Vs Log Error
419	Shape of our train and test data
929	Analysis of learning rate
1590	Load the data
532	Create pipeline objects
56	Load the dependancies
678	using outliers column as labels instead of target column
1100	get the best fold AUC
595	Create the model
719	SAVE MODEL TO DISK
1548	Merge with weather data
1018	Print some summary information
788	Plot the vertical lines
803	Plot of confidence by Target
213	create dummies for all columns
187	No description yet
1657	Segregating positive , negative , neutral sentiment data
429	Converting the built year column to uint
38	Get the train data
1566	minify test dataset by week
535	Predict on test set
1305	calculate the score
736	Removing all zero features
533	Fit Grid Search Cross Validation
423	monthly reading sales
696	Exclude background from the analysis
284	set label for exit tissue
1462	Create dataset for training and Validation
1727	Shuffling happens when splitting for kfolds
1291	Squeeze and add to the list
1087	Convert coverage column to class
705	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
950	iterate through all the hyperparameters
856	Filter Features for Dimension Reduction
45	Create the quad chart
989	Adding new features
465	Split into training and validation sets
1162	Order does not matter since we will be shuffling the data anyway
502	Rescaling the Image Most image preprocessing functions want the image as grayscale
800	df to hold feature importances
700	MODEL AND PREDICT WITH QDA
163	Applying CRF seems to have smoothed the model output
1175	Add the actor to the world
1137	Merge train and test data with months
433	Ignore the warnings
577	Get the sample
483	Build the model
6	eliminate bad rows
1457	checking missing data
1424	Creating a list of the clean words
34	Loading Train and Test Data
473	Loading the data
1408	predict on test data
526	Running the best algorithm
193	Plotting the blackhat
1609	Convert our data into XGBoost format
1055	to set up scoring parameters
740	normalize each point by its number of pixels
1389	StratifiedKFold On Source
1656	written by MJ Bahmani
244	Rreg on Andorra
1794	Train the model
852	Fare Amount versus Time since Start of Records
1065	Model Hyper Parameters
1253	Save the best model during the training
1761	Label encode categorical features
1402	Get the data using exogenous indices
904	Clean up memory
121	Check the current coverage
512	rename column header back to meter categories
105	without any limitations and dtype modification
275	Setting the Hyperparameters for our model
918	installments payments features
1274	loop over all colors
1205	Move to the real folder
1645	Logistic Regression parameter
710	Pseudo Labeled QDA
1463	CNN Model for multiclass classification
1732	Samples which have unique values are real the others are fake
1146	load mapping dictionaries
26	Visualization of Target values
1531	Previous applications categorical features
1438	Create image generator
492	Not suprisingly we overfit
31	define TfidfVectorizer
1228	Load Train , Validation and Test data
1007	Average values for all repaid and not paid variables
184	Brand name with minimum price over time
376	compute mean absolute error
1263	Using original generator
1606	Custom binary encoding
695	remove layter activation layer and use losvasz loss
1400	Apply log transformation
1710	Keras Libraries for Neural Networks
426	Distribution with log transformation
79	if read with PIL
572	Order count by user
1795	fit model to full training set
1720	SAVE DATASET TO DISK
315	SVR and AUC
306	Combine train and test
891	Drop unwanted columns
1028	Clean up memory
1286	In loop to minimize memory use
1242	Run the prediction
630	calculation of the model parameters
1204	Create fake folder
1750	Create entity from dataframe
1358	iterate through all the columns of a dataframe and modify the data type
618	get all contour areas
1584	Now we will check the rest of the columns
1172	create the reader
539	Predict on test set
1232	Load Train , Validation and Test data
234	Filter selected features
1528	Join examples with features and raw results
504	Create a function to draw a lineplot for transactions revenue
32	Identity Hate Classification
1635	Spliting the data
399	Print the confusion matrix
1470	Scale the data
1396	Get the date field from var
781	Create new columns with the same levels
480	Lets look at the length of each word
228	Calculate the average ensembles for each target
403	Train the model
1313	Light GBM Results
1170	Create the training and testing columns
536	Create pipeline objects
20	Imputations and Data Transformation
671	Preparing the Data
1219	create title mode
144	get the data fields ready for stacking
254	Fitting the gradient boosting model
1430	make training features
272	This is the main training part
1059	Show result of prediction
598	Images with ship
1245	Tokenize the train data
752	Downcasting the data
711	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1371	Train the model
59	Unfreeze the model and search for a good learning rate
782	Create new columns with the same levels
528	Create pipeline objects
1489	Reading the examples from a zip file
1675	Sample Patient Image
150	Set empty string for categorical DL
1056	Split into Training and Validation
372	Voting Regression Model
1422	creating a total set of sentences
1748	Bureau balance
1701	Create a list of candidate words
83	Getting the training text data
43	left , right , bottom
1630	Optimize on all Province
1723	missing entries in the embedding are set using np.random.normal
965	Remove rows with missing values
346	Now let us define a generator that allocates large objects
137	Clear the output
501	show the graphs
1006	Removing low information features from training and testing set
179	Mean price by category distribution
410	Training One Vs SGDClassifier
1200	plot best score
1282	get train and test dataframes
1147	Unique IDs from train and test
168	reset for next epoch
1503	of the TPU during training
1005	define a matrix of features
236	Filter Spain , run the RReg workflow
1152	convert validation set to binary
146	Number of different values
784	Random Forest Classifier
1187	We can skip this if we are in test set
1141	Plot the dependence plot
267	Player College Name Validation
712	ADD PSEUDO LABELED DATA
1526	Default empty prediction
1083	Load data from csv files
81	Adding custom Dropout layers
487	Reading all data into respective dataframes
7	declare target , categorical and numeric columns
486	Importing the Keras libraries and packages
1633	Plotly distribution of hospital deaths
18	impute missing values
772	Lets see the correlation between two variables
872	Create a file and open a connection
760	Plotting heads of household
1604	remove rows with null values
1044	Drop missing columns
478	Vectorize the data
1325	Calculate and round number of filters based on depth multiplier
851	Add Elapsed Time
475	Applying the vectorizer on text
1202	Load Model into TPU
291	Create Testing Generator
507	Split the train dataset into train and validation set
377	print out the mean square error
1094	Get the input dimensions of the image
448	Updated train and test data
684	Most frequent labels in a dataframe
1817	Splitting the data into train and test files
300	In the validation set
1812	Create numpy array with features and test data
1660	Import cities and convert to integers
1556	Set some parameters
787	Cumulative importance plot
261	Scatter plot of parameters and LB score visualization
161	Check if the label size is too small
737	Do the same thing with test
876	iteration score 疸번갱
1158	size and spacing
1644	Distribution of Clicks
1648	Generate the inputs for the validation
1215	Only load those columns in order to save space
1549	Merge Weather Data
949	Plotting learning rate distribution
1031	Suppress warnings due to deprecation of methods used
1345	Plot the results
180	Zoom on the second level of categories
1492	Generate predictions for validation set
1774	Shuffling happens when splitting for kfolds
588	room count vs logerror
1203	Get the original fake paths
1022	if this is a new column
331	iterate over all batch images and create DICOM files
1760	Label encode categoricals
2	Add new Features
9	fill test weather data
1348	Fast data loading
1064	Generate data for the BERT model
1375	create empty arrays to store the results
153	Download by click
1775	This enables operations which are only applied during training like dropout
1453	Drop rows with NaN values
415	Pitch Transcription Exercise
338	Plot the training and validation losses
527	Predict on test set
1118	Manually adjusted coefficients
1726	for numerical stability in the loss
963	define feature matrix and feature names
1658	Tokenize the text
660	Computes and stores the average and current value
1519	Number of repetitions per class
183	Number of items per brand
1086	Training set ready
210	get list of categorical columns
104	Compile and fit model
162	Find the indices of the second cell
672	Importing all the models we want to use
370	Fitting the gradient boosting model
1374	Getting the X and y values
1779	Generate the Mask for EAP
145	create dataframe from train.csv file
1510	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
617	Reduce training set sizes
445	Extracting informations from street features
580	Logmel feature extractor
508	Here is the custom function that evaluate the threshold
1131	define the dataloaders with the previous dataset
142	get the data fields ready for stacking
136	This is saved in the same directory as your notebook
252	Decision Tree Regression
1383	Split into train and validation sets
1115	Check if columns between the two DFs are the same
1120	Function to rename columns
786	Plot normalized importance
644	Test data analysis
1546	Revenue based on month
1057	apply transforms to image
1159	LIST DESTINATION PIXEL INDICES
563	Lenght of Items
827	Read the image on which data augmentaion is to be performed
49	Create the quad chart
181	Prices of the first level of categories
54	Understanding the train set
1258	so we have to pad the missing values
319	Split training set to validation set
624	group italy , by country
206	CONVERT DEGREES TO RADIANS
1587	Create categorical features list
1372	Save these predictions
1448	split into train and test
1409	Gaussian Mixture Clustering
116	define a grid of parameters
366	Fitting a linear SVR model
1582	MAKE MIXUP FEATURES
1738	Importing the data
223	Convert LB score to integer
25	Load train and test data
491	Heatmap of features
1615	show mask class example
955	Load the data
1136	Split into public and private test data
575	Correlation between bedrooms and bathrooms
1517	Load the raw training dataset
894	standardize the feature importances
344	Create a generator that iterate over data
1793	Plot the heatmap
1244	Load the data
94	Compute and store the unique values set for each column
62	Exploring the categorical variables
1122	create a baseline model with all features
1281	Load Libraries and Data
456	Load the packages
209	FIND ORIGIN PIXEL VALUES
1338	The first block needs to take care of stride and filter size increase
1574	First look at the Data
1628	Optimize based on confirmed cases
1547	Override the data frame with the input data
1742	SCALE target variable
1015	Read the data
1340	Remove samples with too many noise
830	We can see the distribution of label surface
1455	inverse transform test set
1444	Square the full data
1469	join the bins to create a new dataframe
1356	meter split based
1491	Computes predictions for all examples in the training set
960	Get the feature names
359	Applying the mask to the original image
879	iterate through all the hyperparameters
926	return the score , hyperparameters , iteration
216	MinMax scale all features
1534	Fill the intervals between mean and standard deviation
1045	Join to main dataframe
885	Importing the librarys and datasets
691	Computes gradient of the Lovasz extension w.r.t sorted errors
33	prophet expects the folllwing label names
292	Load the model and make predictions on the test set
552	Running the best algorithm
796	Initiate the model
623	Replace the mainland china with China
1487	Load latest checkpoint
309	Create an embedding matrix of words in the data
542	Running the best algorithm
1133	remove lo intensity pixels as noise
895	Extract the features with zero importance
427	first column only
1262	Load an image
1349	Leak Data loading and concat
1169	Clean up and return the dataframe
471	ROC curve and AUC
578	Calculate spectrogram using pytorch
1039	Previous count features
522	First , we merge the final result of the model
1062	Apply each of the above layers
1569	Initialize Bayesian Optimization
702	ADD PSEUDO LABELED DATA
941	Save the results of the objective function
500	Separate the zone and subject id into a df
494	Plot the distributions of application train set
397	fitting random forest on the dataset
1476	MAKE CUTMIX LABEL
770	Separate the bonus variables from the heads
1411	Set up the layout
921	Normalize the importances
1797	Plot rolling statistics
1581	multiply all the columns in the train and test set
850	Get the datatype of the field
913	Create aggregate categorical dataframe
157	Print final result
561	Which columns have nan
1684	Loading the data
783	Build a scorer
1728	This enables operations which are only applied during training like dropout
1164	DISPLAY TRAIN IMAGES AND VALIDATION ERROR
1598	checking missing data
268	Filter Game Features
1530	Previous app data
622	Examples for usage and understanding
1265	Create train and validation generators
1678	convert text into datetime
12	This block is SPPED UP
274	Set some parameters
541	Fit Grid Search Cross Validation
72	define iou or jaccard loss function
1192	for numerical columns , replace the missing values by
255	Define ExtraTrees Regressor
759	Households without a head
761	Home Ownership Status and Missing Rent Payments
521	We merge the three dataframes baesd on tournament winners
388	A function to compute the histogram of the image
751	To set a style to all graphs
844	Fit the model
1471	Order does not matter since we will be shuffling the data anyway
1046	Join to main dataframe
1747	Amount loaned relative to salary
1048	Credit card balance
807	Now we define the features and labels
1502	Order does not matter since we will be shuffling the data anyway
681	Most frequent labels in training data set
810	Store scores in list for later use
898	Extract the features with zero importance
1776	Breakdown of this notebook
1427	Set values for various parameters
356	Read the image from the given id and type
1451	make predictions on test
932	Save the results of the hyperopt Function
1181	normalize conf and out prod
523	insert it into the tournament
468	Loading the data
708	ADD PSEUDO LABELED DATA
1355	iterate through all the columns of a dataframe and modify the data type
1185	Add to user samples
1536	Check for the number of records
646	summarize history for mean
335	Looking some shapes
823	Custom Cutout augmentation with handling of bounding boxes
1799	shift test predictions for plotting
1597	checking missing data
991	Load the data
1816	Load the data
382	The number of train masks in the CSV
1576	checking missing data
1194	Make a submission
1033	Extract numeric values from all parents
1283	Split the data into train and test
477	How to Compute
1284	converting to numpy array
1264	Load Model Weights
1715	Ensure determinism in the results
37	Plot the distribution of log values
1685	Loading the data
658	Random Forest Regressor
645	Set values for various parameters
158	Converting image to grayscale
1088	remove padding from images
1792	Plot the heatmap
1522	FIND ORIGIN PIXEL VALUES
1735	Plotting the colors of the data
1593	fill up the missing values
350	Fitting a Linear SVR model
1782	Calling our overwritten Count vectorizer
1357	Find Best Weight
1294	warm up model
1687	Check if all the pixmaps are the same
389	Check for empty images
226	Scatter plot of COVID
1755	Add Bureau balance relationship
659	Feature Augmentation using Fagg model
813	Plot the predicted labels
337	Train the model
276	sort the validation data
826	Read the image on which data augmentaion is to be performed
1564	Encoding the month , year , day
140	Make PyTorch deterministic
1233	Build datasets objects
301	Save image to dst
191	VS price of the description length
636	Fit the model
1589	to truncate it
767	drop high correlation columns
1207	Load train and test data
1230	Load model into the TPU
987	Previous Loan Amounts
581	resize batch to desired size
771	Adding some new features from the heads
1751	Create entity from dataframe
1434	use unknown word as label
1324	Change namedtuple defaults
669	Load in the data
431	Separating target and train
1486	Get the pretrained model
902	Train the model with early stopping
566	Keras Neural Network Model
871	Evaluate the hyperopt Function
1806	Read in the data
649	Creating the model
515	Create dataframe of ConfStrength by Season
1616	Computes gradient of the Lovasz extension w.r.t sorted errors
853	Pickup Day of Week
280	Show the label distribution of the clusters
1431	Save the data
640	Common Word frequency analysis for positive text
1323	Parameters for an individual model block
560	Plot Gain importances
1024	bad encoding , must be ohe or le
976	Add Random Search to hypotesis
749	It is from the public kernel
1757	Relationship between applications
905	to set up scoring parameters
1302	Load model into the TPU
128	Prepare Traning Data
13	Load train and test data
1008	Calculate the correlation matrix
1573	Prepare data and model
1364	Function to change addr from one image to another
1220	Drop target , fill in NaNs
98	This is just a simple function that compares two sets of values
742	We need the same for our test data later
942	sort by score
1579	MAKE MIXUP FEATURES
307	We apply tokenization for our documents
868	Sample data from the data
123	A function to clean up the text
910	Take one row from the dataset
1514	size and spacing
1090	Convert test predictions to rle
609	Save images to directory
721	Categorical with few uniques
278	Lets Plot a word cloud
1227	Load the data
80	Create Layers of Covnet
99	declare some parameter
799	Train the model with early stopping
235	Clean Id columns and keep ForecastId as index
1613	Split the train dataset into development and valid based on time
776	Create a pair grid
1177	set color palette
1012	Add the column name
450	Learning Control Parameters
791	Fitting the model
452	import required libraries
1121	Separating target and features
194	Binary Thresholding ..
1407	Train the model
1426	length of clean words
1707	Build and return the model
1532	Choose and initialize a model
519	Split the data into train and test set
556	Create a dictionary with the feature names
1378	randomly choose a random label
1068	Print CV scores , as well as score on the test data
126	Prepare data for the model
892	Columns with more than 75% missing values
943	Fitting and predicting
147	Number of click by IP
336	Fitting and Validation
1679	get some sessions information
983	Adding new features from start date
1406	Get just the digits from the seeding
828	Read the image on which data augmentaion is to be performed
1360	Leak Data loading and concat
1320	fill missing values with zero
925	Fitting the baseline model on the test data
1452	Logarithmic transform of train set
1415	What are the labels in each class
118	Patient line , weeks , FVC
302	Resizing the image
151	How many people download the app
294	Create submission file
1061	Create a DownConvolution
664	Count bookings over time
1478	LIST DESTINATION PIXEL INDICES
396	Ekush Confusion Matrix
232	Double check that there are no informed ConfirmedCases and Fatalities after
1523	Oversampled training dataset
890	Lets check Correlation Matrix
679	Split the labels
1791	Create some time features
545	Classif feature selection
66	load and shuffle filenames
525	Fit Grid Search Cross Validation
1435	take a look of .dcm extension
1762	Missing Ratio in Feature Matrix
0	Load the DICOM image and convert to pixel array
1093	Create the parser
127	Find the dimensions of categorical variables
436	Preview of the Data
1096	Encode test set and generate submission
1138	Shap interaction values
1789	Forceasting with decompasable model
1310	Get feature importances
968	Remove the low information features
977	Prepare data for modeling
1571	Find the best loss
460	Fitting the random search
1026	Train the model with early stopping
1639	Distribution of Clicks
211	Label encode Categorical Columns
824	Applies the cutout augmentation on the given image
1296	Pixel Normalization and Image Augmentation
604	Read in raw data
933	sort by score
109	plot raw data
443	Latitude and Longitude
1629	Simple MA Model
922	Plot normalized importance
612	Ok , as expected
815	parameter value is copied from
629	Grouping USA Data
488	get categorical and numerical features
936	Fitting and predicting
1520	LIST DESTINATION PIXEL INDICES
1053	get best score for each class
1270	each color of the image
847	Fitting the model
537	Fit Grid Search Cross Validation
774	and calculate scorr and pvalues
1583	scale all lugar features by instancelevel
1381	split the dataset in train and test set
1095	Predict on validation and test set
1568	Pinball loss for multiple quantiles
371	Define ExtraTrees Regressor
887	Get the list of original features
220	highlight high correlation features
323	calculate the iou score for a batch of samples
655	Check missing values
1790	importing all the required ML packages
308	padded train and test sets
14	Visualization of Target values
1103	Import the datasets
875	Append the hyperparameters to the dataframe
1223	Predicting with the best parameters
715	Remove the Outliers
814	Spliting the data into train and validation set
750	Combinations of TTA
1217	get summation and other summary stats
3	Reset Index for Fast Update
125	Loading the data
208	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1666	StackNetClassifier with GPU
1110	Extract processed data and format them as DFs
1339	Final linear layer
170	First component of main path
773	Correlations between train and test set
984	Days with Days
862	Standard deviation of best score
391	Show some example images
464	Merge datasets into full training and test dataframe
347	Import libraries and data
643	Basic Pytorch Model
1337	Update block input and output filters based on depth multiplier
1098	Create the train and validation sets
1212	Applying Quadratic Spline
1736	Target variable exploration
1667	Predict on test data
1171	Save images to a list
1354	Fast data loading
169	Build the model
1166	Preparing test data
547	Fit Grid Search Cross Validation
1293	Load dataset info
1456	Number of rooms and price per document
559	Create a dictionary with the ordered values
881	Drop unwanted columns
86	Total number of train and validation samples
1180	Using the same conf.mtx as above
1641	Most frequent IPs in training data set
381	Get the item from the dataset
409	Feature extraction from question text
518	Make a DF with just the wins and losses
1659	Copy Neutral tweets to Selected Text
663	Count number of bookings per year
227	Scatter plot of COVID
1285	members of each model
1163	Order does not matter since we will be shuffling the data anyway
498	read in header and get dimensions
457	Importing the libraries
1663	kick off the animation
1199	Plot of evaluation metrics vs boosting iterations
95	Function for extracting indices of numeric values from training data
462	Load libs and funcs
975	iteration score 疸번갱
129	See sample image
883	extract hyperparameters from bayesian optimization
91	Detect face in this frame
741	Local Deform function
1317	Create a tf.Session
938	Write column names
470	Merge datasets into full training and test dataframe
728	Read in the labels
544	Dividing the predictions into train and test set
908	Drop all variables except for the parent variable
1640	Load the Data
361	Read the image data
130	Look at how data generator augment the data
305	Evaluate the model
1196	Save a dict that contains all the folds
1507	loop over data
1557	Creation of the External Marker
529	Fit Grid Search Cross Validation
571	Hour of the Day Reorders
548	Running the best algorithm
63	iterate through all the columns and create a distribution
201	Thanks to with the preprocessing part
266	Find any missing values
1611	First some columns of the categorical variables
738	Do the same thing with test
866	specify your configurations as a dict
1527	Computes official answer key from raw logits
565	Get feature importances
1107	Load sentiment file
304	Weights of the classes
154	Attributed Time Exploration
148	Click by IP
1545	Create new column name
628	How many data are there per day
188	wordcloud for items description
416	Import and Preview Data
682	Create a dataframe with the labels count
1467	Fill in missing values with zero
1054	Clean up memory
967	huge difference in the importances
831	Now our data is ready to go
1739	distribution of winPlacePerc
1643	Proportion of clicks by device
1398	The function for Data Processing is borrowed from
1714	cross validation and metrics
134	Initializing the model
1119	Distribution inspection of original target and predicted train and test
616	Add Random Forest Classifier
408	Generating the word cloud
1429	make training features
1625	Drop Unused Columns
439	Intersection Number of the most commmon IntersectionID
1351	Fast data loading
143	Build and fit the model
899	encode ohe features
657	Get the X and y variables
633	Some libraries we need to get things done
988	Looking at the example credit data
1419	Basic data manipulation libraries
1698	Check if the program has any images
1322	Split train and validation data
569	Hour of the Day
1182	Compute the Keras loss
469	Data processing , metrics and modeling
951	Reading the datasets
1168	Converting the dist columns to log
697	Precision helper function
765	Create a scatter plot
1696	Function to evaluate program
914	Create aggregated dataframe
1386	If using imgaug we want to know the image size
182	Top level categories with highest price
1000	DETERMINE CUSTOM FEATURES
21	Check for missing values in training set
956	Load from dataframes
953	Importing the librarys and datasets
1361	Importing the libraries
313	Label encode categorical variables
1051	Create a model
785	Normalize the importances
237	Filter Spain , run the RReg workflow
928	Randomly sample the results
240	Filter Germany , run the RReg workflow
602	Read in the data
1634	Find the difference between all the columns
854	Fare Amount vs Pickup Fare Amount
190	Plotting the distribution of the description length
1485	Reading the test tf records
1704	replace the best candidate with the best score
1449	Create X and Y dataframes
597	Examine the masks
920	Credit card balance
778	Get the columns with correlations
1414	Count the number of items in each class
1216	Group by installation id and compute the mean and standard deviation
567	A simple Keras implementation that mimics that of
1	Resize image to desired size
1314	Plot the distribution of shap values for each variable
839	set alpha on legendHandle
176	Visualizing the images
108	Plotting sales volume per year
312	find categorical columns
1021	Add to list of seen columns
947	search in random
1733	Importing the required libraries
562	Create out of fold feature
1251	ONLY TRAIN WITH DATA WHERE CATEGORIES LIKE
1397	Distribution of var
592	Read in the data
935	sort by score
1155	Create strategy from tpu
326	Initialize patient entry into parsed
97	Making sure that the before values are highly correlated
1290	Squeeze and add
516	Join the two new features to the main dataframe
534	Running the best algorithm
1717	LOAD PROCESSED TRAINING DATA FROM DISK
859	run randomized search
269	Merge the players into one vector
1619	checking missing data
1787	FIND ORIGIN PIXEL VALUES
42	Create a dataframe that contains the counts of each official variable
387	Get the item from the dataset
328	iterate over all batch images and create DICOM files
1551	Average day of month
503	scale pixel values to grayscale
160	Plotting label images
1321	Create a tf.Session
600	Load mask dir
801	Adding missing values
324	LOAD TRAIN AND TEST
455	Visualize the boxes on the image
407	Run it in parallel
373	Compute the STA and the LTA
505	Lets plot the joint plot of all columns
401	obtain one batch of training images
47	left , right , bottom
454	Read in the binary file
1370	Label Encoding for LightGBM
1129	Preparing the data
1329	Encodes a block to a string
948	Boosting Type for Random Search
1682	An optimizer for rounding thresholds
748	Load the last checkpoint
952	Drop unwanted columns
453	draw rectangle around image
260	Convert LB score to integer
639	Segregating positive , negative , neutral sentiment data
1708	Importing standard libraries
1650	Cluster the hits , stds , events
251	SGD regressor model
906	Visualize Scree plot
1225	Make a picture format from flat vector
940	sort by score
1541	STAIN NORMALIZATION FUNCTIONS
638	Generating the wordcloud with the given text
1412	Number of items in each class
626	Order the cases by day
493	Merging Applicatoin train data
250	Fitting a linear SVR model
1198	Create submission file
754	Plot the distribution of target variable i.e
1114	Remove missing target column from test
1443	Square the full data
1128	Calculate the mean coverage for each feature
310	Splitting the data
718	Load model , classifier , and preprocess
1493	write predictions to file
463	Data processing , metrics and modeling
804	Extract nested conditional parameters
654	Function to read test data
1693	Replace the unlifted function with lifted function
459	Fitting the random search
1809	Handling NaN values in categorical features
683	Create a dataframe with the labels count
614	Weight of the class is inversely proportional to the population of the class
724	create dataframe with mean imputation
1149	Get the preprocessing function
24	Remove the Outliers if any
1342	Calculate the mean ratio
1769	SAVE DATASET TO DISK
981	Remove the Outliers
1009	in the future I plan to change and add numeric columns
794	make a dataframe with the selected features
1421	Training and Testing Sentences
1819	Join market data with news data
1550	week of year average
485	First component of main path
594	Construct text feature from text features
1712	Since the labels are textual , so we encode them categorically
327	Add box if opacity is present
1560	Correlation with macro features
70	add trailing channel dimension
1768	shifting the data
1332	Depthwise convolution phase
53	Draw the quad plot
1781	Calling our overwritten Count vectorizer
482	Define the model
68	if augment then horizontal flip half the time
840	Plotting Manhattan Distance by Fare Amount
1249	DataFrame for trials
1565	Deepfake Detection Challenge
1605	Custom binary encoding
207	LIST DESTINATION PIXEL INDICES
1703	sort best candidates according to score
1001	Get the most recent day
430	Encode Categorical Data
78	save dictionary as csv file
333	iterate over all batch images
1280	A task of interest
192	Show original and grayscale
77	retrieve x , y , height and width
1013	Clean up memory
92	Pickle seems to be more efficient in this competition
451	Only the classes that are true for each sample will be filled in
368	Decision Tree Regression
686	Create train data generator
1303	Clear GPU memory
1752	Ensembling the entity
238	Filter Italy , run the RReg workflow
973	iteration score with random set
1540	STAIN NORMALIZATION FUNCTIONS
982	Converting the datetime columns to number of days
40	Load the data
845	Fill NaNs in train and validation set
1387	Apply transforms to sample
458	Create out of fold feature
1653	Target Variable Analysis
1490	Read candidates with real multiple processes
1624	Importing the libraries
1157	numpy and matplotlib defaults
1785	Avoid division by zero by setting zero values to tiny float
1255	Load Model into TPU
1139	Plot the distribution of returnsClosePrevRaw features
1796	Different Time Series Modelling
924	Plot the vertical lines
215	converting data into XGBoost format
1049	encode ohe features
775	Plot the heatmap
825	Set to instance variables to use this later
1112	extract different column types
1626	Calculate stats for all Province
586	Plot the error vs bedroom count
1040	Previous applications categorical features
993	Get a feeling for the interesting features
1144	iterate through each game feature and calculate the growth rate
808	Split train and validation sets
812	Write column names
1113	Subset text features
1247	to truncate it
1327	Convolutions like TensorFlow , for a fixed image size
1716	FUNCTIONS TAKEN FROM
1334	Expansion and Depthwise Convolution
1436	Number of Patients and Images in Training Images Folder
441	Latitude and Longitude
998	Normalize the mode count
1459	checking missing data
1160	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
135	Fbeta score on predicted labels
1447	Create submission file
766	Add a legend and label
722	Sort ordinal feature values
1428	Add PAD to each sentence
1631	Confirmed and Deaths Full Table
411	Training Logistic Regression on Multilabel data
1771	text version of squash , slight different from original one
1745	using sieve eratosthenes
1150	validation image augmentation
1700	Apply each fitness function to the score
164	Reading the image
1229	Build datasets objects
317	SGD regressor model
1416	Detect hardware , return appropriate distribution strategy
29	Load train and test data
279	Applying the K means on the given data
214	split training set to validation set
265	We scale the train and test data
979	App data features
1472	size and spacing
966	Labeling of plot
689	functions to show an image
178	Product prices by category
1331	Loads pretrained weights , and downloads if loading for the first time
647	Load Libraries and Data
962	define feature matrix and feature names
716	Train a Random Forest Regressor
568	Loading the data
1376	Machine Learning to Neural Networks
88	CREATE LIST OF MODELS
1508	loop over data
653	Function to read the training data
662	Aggregate by date
413	In this Section , I import necessary modules
339	make predictions on test set
1638	Let us now look at the boxplot and violinplot of Minute distribution
1721	LOAD DATASET FROM DISK
489	count distinct values by a single type
203	For every slice we determine the largest solid structure
358	sieve skin mask
231	Merge train and test , exclude overlap
112	Merge the three dataframes into one DataFrame
861	Split into training and testing data
1238	Define the initialization operations
734	Classify image and return top matches
342	Create submission file
435	Pitch Transcription Exercise
1201	Detect hardware , return appropriate distribution strategy
1002	define a list of features
795	del n_estimators
524	Create pipeline objects
513	Summary of the Conference Tourney Game
1770	missing entries in the embedding are set using np.random.normal
394	Decision Tree Classifier
753	Import Train and Test dataset
635	Initialize the fitting model
1184	Initialize all the variables
71	create numpy batch
555	Set the data types
172	Save the model to JSON file
461	Fitting the random search
1156	watch out for overfitting
1067	split training and validation data
264	Prepare Training Data
798	Split into train and validation sets
1612	Split the train dataset into development and valid based on time
219	Import libraries and data
472	Precision and Recall curve
1278	reset and identify the objects
720	Importing the libraries
1670	Set up seeds again
1418	Importing the libraries
627	Spain Cases by Day
590	Gaussian target noise
1401	Apply exponential transf
805	Initialize subsample parameters
551	Fit Grid Search Cross Validation
111	Join the two new features to the main dataframe
1734	fill in missing values
1075	Import test and train data
863	Calculate baseline model AUC
1804	Plot ROC Curve
185	Price of zero item
961	Create a matrix of features
173	Creating a dictionary of all the unique classes
1642	Most frequent IPs in training data set
1373	Fitting the model
1063	We will use the most basic of all of them
1328	Gets a block through a string notation of arguments
1368	retrieve the base map
205	Loading Dependencies and Dataset
1764	Create train and test df with missing values
406	created a new dataframe with the ID code and the distribution
1029	to set up scoring parameters
1179	Calculate the confusion matrix
1499	Load latest checkpoint
474	Create submission file
1803	scale to world
1256	create train , test folders
400	Function to convert images to numpy array
89	Median Absolute Deviation
1496	Features to use for training
1311	Display current run and time used
763	create a new annotation
821	Random Forest Classifier
893	Drop missing values and align
1195	Make a prediction
1413	Not in train set
1719	shuffling the data
992	Create features for the previous installments and credit
271	from functools import partial
1608	define random hyperparammeters
1603	Checking any missing values ,
418	Preview of the data
531	Predict on test set
166	Create dataframe with mask for each label
1808	Number of transactions
1730	Add leak to test
1211	Pad image paths to make sure that they are aligned
680	Splitting the data into train and test set
1689	Sort the list of weights
642	neutral top words
909	Extract numeric values from all parents
843	Stratified Train and Validation
1268	Linear Weighted Kappa
1651	Cluster the hits , stds , events
1538	Preparing the data
