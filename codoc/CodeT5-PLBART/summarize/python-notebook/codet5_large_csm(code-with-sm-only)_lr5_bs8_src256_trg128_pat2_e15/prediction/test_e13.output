345	Split into Training and Validation Sets
180	Correlation Heatmap of Features
26	Detect faces in this frame
449	Improvement clearly visible
577	There are also many primes
243	Aggregate the bookings by date
38	Preparing the training data
60	Let us check the memory usage again
541	Diff Each Dx Values
150	ELECTRICITY OF MOST FREQUENT METER TYPE
489	Batch Cut Mixing
279	Concatinating all the heads
128	Run it in parallel
267	D Surface plot
360	Load the data
197	Linear Corellation check
508	Pearson correlation between features
219	Reordered china cases by day
482	Num Rooms and price
512	Base FVC and Weeks
262	Sort ordinal feature values
6	Load the data
475	Reshape images for each patient
565	LOAD DATASET FROM DISK
268	The number of binary features
352	Applying CRF seems to have smoothed the model output
1	Imputations and Data Transformation
295	Zoom into NYC
221	Iran Cases by Day
108	Glimpse of Data
356	Cred Card Balance Data
173	Lets try to remove these one at a time
590	Use only sklearn metrics for visualization
118	Random Forest Regressor
292	and then finally create our submission
369	load mapping dictionaries
461	TPU Strategy and other configs
354	First , we try to run the model
424	Test Data Exploration
293	What is the distribution of fare amount
285	Behind the scenes
522	Create categorical and object features
331	Remove Low Information Features
127	Voting Regressor
156	Distribution after log transformation
576	How many enemies DBNOs are there
551	Install and import necessary libraries
158	Categorical Encoding using Label Encoder
368	Plotting Cases by Date
51	Import libraries and data
101	Take Sample Images for training
584	SAVE DATASET TO DISK
397	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
530	Now lets take a look at the distribution
436	Fast data loading
450	Loading Dependencies and Dataset
429	Generate the video id
61	Converting images to grayscale
423	Generate predictions for submission ,
257	Filter Data Set
226	Peek of the input data folder
126	Linear SVR model
179	Distribution of the values
102	Train and Validation Split
223	population of the world
314	Set up the correlation matrix
445	Feature Slicing in Time Series Data
261	Categorical in top
230	Counter of negative words in positive train set
131	Load the Data
123	Looking at the image data
265	Getting all the image labels
234	Load and Preprocessing Steps
503	Load the training dataset
417	Quadratic Weighted Kappa
527	Visualization of missing data
563	LOAD TRAINING DATA FROM DISK
298	Fitting the learning rate
210	Looking at the masks
206	Compose the augmentations
258	Feature importance with Random Forest
458	Class Distribution and Null values
33	Patient Condition Progression by Sex
477	Split the data into train and test
497	Predicting on test set
515	Read the data
44	Prepare Testing Data
444	Charts and cool stuff
305	Fitting and Evaluating the Model
303	First pickup columns
139	Random Forest Classifier
326	Random Search Bar chart
181	Applicatoin merge
217	Looking at italy data
493	It creates a generator for every jsonl file
529	One hot encoding the columns
408	Making the necessary folders
146	Multilabel features vectorization
505	Oversampling the training dataset
282	age distribution vs escolari
579	Building a feature matrix
415	Build Test and Submit
18	Finetuning the baseline model
296	Fare Value Correlation
411	Create test generator
239	Feature Augmentation using Fagg
558	Exploring the data
347	Split into train and validation sets
105	Linear SVR model
68	No description yet
65	First level categories
427	Generate predictions for submission
405	DataFrame of all trials
575	Is there a home team advantage
185	Reducing the shape of a dataframe
47	Now we define the model
531	Checking for Null values
140	NumtaDB Confusion Matrix
374	We can add one more useless feature
454	Lets compute the rolling mean per store
523	Load the data
29	Split into features and targets
233	Load data and libraries
220	Spain cases by day
373	Import required libraries
88	MLP for Time Series Forecasting
500	Get the list of decay variables
147	OneVsRest Classifier
49	Save the model
377	Overview of DICOM files and medical images
434	Visualizing the predictions
381	Get the original fake paths
80	Ensemble with averaging
36	Funtion to clean special characters
471	Create Data Generator
379	TPU Strategy and other configs
90	Folders in input directory
187	Set up the evaluate function
524	Check for the Missing Values
511	Encoding the Date Features
453	How many data are there in the dataset
216	Replace the country with China
20	Training Text Data
542	Split into Train and Test
327	Reading in the datasets
504	Set global parameters
329	Calculate feature matrix and feature names
553	Set up seeds again
478	Splitting the data
50	Clear the output
55	The number of click by IP
380	Load Model into TPU
343	Reading in the data
407	Load Model into TPU
74	Dropping unnecessary rows and columns
378	Create submission file
64	Mean price by category distribution
291	Now our data set is ready to go
525	Checking for Null values
581	Train Set Missing Values
214	And the final output
254	Load the model
472	Loading the data
135	Show the best clusters for test data
124	Complete training dataset
43	Create a generator for training
357	load mapping dictionaries
428	Importing relevant Libraries
413	Load the model and do predictions on test set
24	Preprocess the fake data
433	Plots of training and testing samples
455	Train the model
403	Training History Plots
336	Detecting the boolean variables
52	How fraudent transactions is distributed
348	Evaluating the Target Variable
340	Reducing the memory usage
447	Now let us see what our model looks like
112	Creating Submission File
588	Loading the data
439	Leak Data loading and concat
485	CNN Model for multiclass classification
30	Compile and fit model
251	filtering out outliers
278	Plotting the walls
160	Podemos visualizar o resultado
148	Exploratory Data Analysis
238	Random Forest Regressor
399	Build datasets objects
474	Predicting the test data
174	Tokenizing the text
183	Load Train Data
487	Define dataset and model
312	Loading the data
520	Create new features
9	Create embeddings for train set
554	Data Preprocessing Helper Functions
571	Importing necessary libraries
67	Prices paid by seller or buyer
84	Converting columns format
498	Load the pretrained models
492	Load the pretrained models
119	Load the data
481	Logistic Regression model
359	Remove unwanted features
232	Perfect submission and target vectors
46	Spliting the training data
526	Time Series Impact on Melanoma
98	Apply model to test set and output predictions
209	Prepare Full Text Data
172	Vectorizing the data
532	Exploring the data
463	Importing all the basic python libraries
151	LOWEST READINGS IS LESS THAN MALES
218	Reordered Cases by Day
315	Drop Unhelpful Columns
443	Leak Data loading and concat
325	UpVote if this was helpful
21	Class Distribution Over Entries
276	Look at the households without a head
555	Sample Patient 1 - Normal Image
297	Split into Train and Test
466	No null values present in the train set
313	Merge Training and Testing Bureau
66	Top 10 categories of items with a price of zero
572	Exploratory Data Analysis
387	Resize to desired injest
189	We define the model parameters
323	Analysis of learning rate
301	Fare Value by Day of Week
63	RLE Encoding for the current mask
334	Random Search and Bayesian
144	Train the model
111	Creating Prediction dataframe
513	And then finally , create the converted files
113	Now let us define a generator function
164	Loading the data
384	Define the model
287	Modelling with Random Forest
207	Read the data
96	Remove unwanted files
215	Ok , as expected
465	Adding PAD to each sentence
517	Create continuous features list
22	Preprocess the data
365	SHAP Interaction Values
392	Plotting some random images to check how cleaning works
75	Display results in a bar chart
506	Random Forest Classifier
528	Create list of features
162	Encoding the Regions
190	Loading the data
332	Final Training and Testing Data
109	Plot the evaluation metrics over epochs
564	SAVE DATASET TO DISK
16	Ensure determinism in the results
149	Peeping into the Data
152	How hours do the meter readings vary across the day
393	Read the data
546	Here we evaluate the clustering and score the event
244	Aggregate the bookings by date
41	Prepare Traning Data
550	Load the market data
587	Bad results overall for the baseline
307	Some functions to modify the data
25	Checking for Class Imbalance
284	Join the levels for the index
510	Taking a look at the test data
414	Create dataloader for mini batch training
14	Overview of Missing Values
376	Cylinder Actor Setup
557	Sample Patients and FVC
79	Set the best score
389	Game time stats
78	Linear SVR on columns
264	We can see there are some categories with price
177	Checking data for type features
561	Build the model
457	Gaussian Mixture Clustering
321	Loading the credit data
228	Example of sentiment
304	Distribution of Validation Fares
521	Check if there are any values with only one value
212	Using python OpenCV
442	Fast data loading
540	Exploration of Hospital Deaths
306	Baseline model scores
167	Loading the data
473	Preparing the training data
320	Getting Late Payment Data
77	High Correlation Matrix
188	What are the data types
344	Loading the data
390	Create title mode
231	Top 20 words in neutral training set
568	Add train leak
17	Creating a DataBunch
35	Function to count words from each sentence
355	Loading Data Into Memory
37	The function for cleaning is borrowed from
259	Save model and preprocess functions
339	Rearrange categorical variables
224	Test if the model uses to run other models
85	Prepare Training Data
484	Create dataset for training and Validation
514	Predictions class distribution
419	Load packages and data
432	This plot indicates a strong correlation between the error columns
416	Create sequences from test text and questions
448	Training and Evaluating the Model
204	No of Storeys Vs Log Error
363	Spliting the test data
507	Breakdown topic analysis
366	Vast majority of the data is just car
559	This is something I learnt from fast.ai
383	Read in the real data
456	Predicting probabilities with clipping
302	Split into Train and Test
509	Split into train and test
459	Which attributes are not in train labels
195	Bathrooms interest level
89	Lets generate a word cloud for each sentence
426	Clear GPU memory
191	Hour of the Day Order Count
182	Distribution of the missing values
548	Replace neutral with text
404	Load and preprocess data
7	I thought about vectorization
117	Linear SVR on columns
169	Loading the data
153	Aggregating by season clearly lack in granularity
196	Interest Levels vs bedrooms
11	Lets plot the distribution of the training data
241	Aggregate the data for buildings
364	Month of year
227	Word Cloud for tweets
32	A unique identifier for each store and item
199	Transforming the probs to a dataframe
273	Combinations of TTA
13	Read the data
318	Load previous application data
229	Most common words in positive data set
222	Uniting the cases by day
361	Preparing the data
0	visualization of Target values
582	Train and predict
567	The method for training is borrowed from
351	Comment Length Analysis
250	Exploratory Data Analysis
270	We need the same for our test set
591	Plot the evaluation metrics over epochs
483	Make a Baseline model
420	Train and Test Data
549	Saving the cities as integers
200	The number of stories built VS year
543	We can see there is no missing data
430	Sample valid set
271	Converting images to filepath
116	High Correlation Matrix
201	Vs log error
294	Theoretical microlensing curve
163	Train and Test data
464	Total Training and Test Sentences
583	Plots of Quaketime vs Signal
395	Build datasets objects
240	Inference and Submission
469	Number of Patients and Images in Training Images Folder
141	NumtaDB Confusion Matrix
176	Exploration Road Map
106	Voting Regressor
208	We factorize the categorical variables
346	Loading the model
99	Submit to Kaggle
593	Lets plot some of the data
533	Linear Model with Logistic Regression
386	Saving the model
95	Ekush Classification Report
310	Cross Validating the Model
120	Train the model
225	Argmax of infection peak
114	Now let us define a generator that allocates large objects
205	Gaussian Target Noise
470	Number of Patients and Images in Test Images Folder
402	Load the data
143	Credits and comments on changes
246	Next we need to add the distribution to the dataframe
539	Province and State
375	Visualize Bkg Color
249	Now we can transpose the data
435	Plot test samples from train tasks
446	Can we run adversarial validation on this data
28	Save the before and sets data
31	Loading the data
573	The competition metric relies only on the order of recods ignoring IDs
277	drop high correlation columns
53	Creating a dataframe
412	Validate code and path
370	Show some data
59	Creating a dataframe
19	Test prediction and submission
578	Lets take a weighted average of individual values
248	Load and preview Data
372	Process the test data
501	Listing the contents of the directory
100	Set the target variable
73	Importing relevant Libraries
40	Get the categorical variables
255	Applying CRF seems to have smoothed the model output
192	Days of the week
400	Model initialization and fitting on train and validation
479	Create Train and Test datasets
2	Impute any values will significantly affect the RMSE performance
184	Extract target data
333	Applying to random and opt
42	See sample image
165	Loading the data
97	Create test generator
86	Filter out dense game and categorical features
159	Predict on test set
566	The mean of the two is used as the final embedding matrix
476	Reshape the data
72	Lets plot some more images at random
198	Setting up some hyperparameters
94	Making user metric for objective function
23	Count the fake and real samples
441	Find Best Weight
480	Predict and Submit
406	Save the best model
324	We can look at how many param combinations are there
62	Here are some examples of the labels
281	drop high correlation columns
256	Lets check the missing values in each file
15	Modeling with Fastai Library
495	Generate predictions for validation set
12	Prepare the train data
462	Training the model
54	There are a lot of different values
574	Charts and cool stuff
115	Importing the required libraries
580	Prepare the data
121	ok lets go for it
545	Add in extra variables
154	DIFFERENCES BETWEEN TIMES AND METER Reading
69	Most common words in Items Descriptions
538	Time Series Analysis
585	Breakdown of this notebook
83	Voting Regressor
421	Blurring and Unblurred Images
170	Vectorizing the text
437	Leak Data loading and concat
418	Analyzing a random task
71	VS description length VS price
536	Training and Prediction
237	There are missing values in the dataframe
341	Aggregating the bureau balance by client
145	Tagging and Counting the words
186	Rescaling the Image Most image preprocessing functions want the image as grayscale
319	Exploratory Data Analysis
260	Importing the Libraries
502	Define the model
544	Number of clicks and proportion of download by device
4	visualization of Target values
132	Lets validate the test files
552	Sales by store
308	Bayes and Random Search
283	Target variable distribution
311	Cross validation on the full dataset
385	Create model and train
317	Merging Bureau Data
371	Show some examples
330	Train and test set features
396	Load model into the TPU
5	Lets look at the distribution of the distribution of the data
468	take a look of .dcm extension
499	Get the pretrained model
92	Take Sample Images for training
589	Now we can remove inf values
58	Download rate evolution over the day
263	Train the model and predict
45	Create Testing Generator
518	Now we can replace the inf values with zero values
394	Load Train and Validation data
494	Get the pretrained model
235	Define the model
138	NumtaDB Confusion Matrix
338	Target and Feature Correlation
388	Resizing the Images
178	Imbalanced dataset Check
467	Here is how often the model uses each link
342	Merging All the data
350	Import the Data
136	Decision Tree Classifier
166	Identity dataset merge
10	Understanding the target variable
70	Coms Length Analysis
367	Growth Rate Percentage
440	Fast data loading
491	Run the model
82	Linear SVR model
272	Bounding Boxes and scores
194	Dealing with the Order Count
535	Exploratory Data Analysis
110	Make predictions on the test set
516	Examine Missing Values
452	Add date features
168	Identity dataset merge
142	Converting to image
337	Building the feature matrix
161	Extracting informations from street features
247	Load and view data
137	NumtaDB Confusion Matrix
242	Aggregate the bookings by year
309	Reading in the datasets
537	Training and Prediction
496	Validate the model
562	Ensure determinism in the results
300	Fare Amount versus Time since Start of Records
586	Import train and test csv data
335	Extract target variable
193	Hour of the Day Reorders
155	I WOULD LIKE TO HAVE DONE , IF I HAD MORE TIME
213	The number of masks per image
290	Lets start with the label surface
451	Visualizing the augmented images
203	Vs log error
328	Aggregated Feature Selection
488	Predictions on Test set
76	Importing the required libraries
266	Vast majority of prices in different image categories
103	Define train and validation paths
81	Implementing the SIR model
570	Create a video file
556	Lung Opacity Examples
27	This is something I learnt from fast.ai
490	mixup for each image
274	Read Train and Test Data
382	Create a save directory
252	using outliers column as labels instead of target column
534	Charts and cool stuff
569	Add leak to test
398	Load Train and Validation data
157	Converting date features to uint8
3	Detect and Correct Outliers
202	Vs Bathroom Count Vs Log Error
592	Read the dataset
289	Train a Random Forest on the training set
401	Predicting on the test set
56	IP Distribution and Quantile
48	Fbeta model metrics
409	Build the new dataframe
134	Predicting the test data
422	Display some samples of blurry dataset
129	The number of masks per image
316	Aggregating the child variables
425	Load model into TPU
519	Multiply all the columns
91	Filter Data Set
122	How many duplicates with different target values in train data
104	Setting X and y
130	Load the data
560	The function below create a list of evaluated images
362	Show a random validation mask and prediction
438	Fast data loading
93	Set up train and validation paths
288	Random Forest Classifier
133	Computing the histogram
269	Image drawing with opencv
8	Identity Hate Classification
107	Load train and test data
236	Some basic model specs
431	Scatter plot of SHAP Values
299	Fitting and evaluating the learning rate
286	Random Forest Classifier
34	Pulmonary Condition Progression by Sex
358	extract different column types
39	Prepare Data for KNN Model
175	Build the model
171	Vectorizing the text
322	Define the hyperparameters for the model
275	Which households do not have the same target
410	Resizing the Images
353	Test out the model results
125	Pillow Image Data Generator
391	Predicting with the best params Xgb
57	There are some missing values in the data
547	See why the model fails
87	Setting the Paths
253	Split the string into labels
280	Lets see some examples of the data
486	Predicting on Test set
211	We will keep only the images with masks
349	Exploring the Income Total
460	Class counts by label
245	Final resultado de datos
