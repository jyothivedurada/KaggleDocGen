0	import pandas as pd import numpy as np import matplotlib import matplotlib . pyplot as plt import seaborn as sns color = sns . color_palette ( ) import plotly . offline as py py . init_notebook_mode ( connected = True ) from plotly . offline import init_notebook_mode , iplot init_notebook_mode ( connected = True ) import plotly . graph_objs as go import plotly . offline as offline offline . init_notebook_mode ( ) import cufflinks as cf cf . go_offline ( )
1	train_labels = application_train_dummies [ 'TARGET' ] application_train_dummies , application_test_dummies = application_train_dummies . align ( application_test_dummies , join = 'inner' , axis = 1 ) application_train_dummies [ 'TARGET' ] = train_labels print ( 'Training Features shape: ' , application_train_dummies . shape ) print ( 'Testing Features shape: ' , application_test_dummies . shape )
2	import seaborn as sns color = sns . color_palette ( ) plt . figure ( figsize = ( 12 , 5 ) ) plt . title ( "Distribution of AMT_INCOME_TOTAL" ) ax = sns . distplot ( X_new [ "AMT_INCOME_TOTAL" ] )
3	from scipy . stats import boxcox from matplotlib import pyplot np . log ( application_train [ 'AMT_INCOME_TOTAL' ] ) . iplot ( kind = 'histogram' , bins = 100 , xTitle = 'log(INCOME_TOTAL)' , yTitle = 'Count corresponding to Incomes' , title = 'Distribution of log(AMT_INCOME_TOTAL)' )
4	import seaborn as sns color = sns . color_palette ( ) plt . figure ( figsize = ( 12 , 5 ) ) plt . title ( "Distribution of AMT_CREDIT" ) ax = sns . distplot ( application_train [ "AMT_CREDIT" ] )
5	original_train_data = pd . read_csv ( '../input/home-credit-default-risk/application_train.csv' ) contract_val = original_train_data [ 'NAME_CONTRACT_TYPE' ] . value_counts ( ) contract_df = pd . DataFrame ( { 'labels' : contract_val . index , 'values' : contract_val . values } ) contract_df . iplot ( kind = 'pie' , labels = 'labels' , values = 'values' , title = 'Types of Loan' )
6	original_train_data [ "NAME_TYPE_SUITE" ] . iplot ( kind = "histogram" , bins = 20 , theme = "white" , title = "Accompanying Person" , xTitle = 'People accompanying' , yTitle = 'Count' )
7	( original_train_data [ "DAYS_BIRTH" ] / - 365 ) . iplot ( kind = "histogram" , bins = 20 , theme = "white" , title = "Customer's Ages" , xTitle = 'Age of customer' , yTitle = 'Count' )
8	grp = bureau . groupby ( by = [ 'SK_ID_CURR' ] ) [ 'SK_ID_BUREAU' ] . count ( ) . reset_index ( ) . rename ( columns = { 'SK_ID_BUREAU' : 'BUREAU_LOAN_COUNT' } ) application_bureau = application_bureau . merge ( grp , on = 'SK_ID_CURR' , how = 'left' ) application_bureau [ 'BUREAU_LOAN_COUNT' ] = application_bureau [ 'BUREAU_LOAN_COUNT' ] . fillna ( 0 ) application_bureau_test = application_bureau_test . merge ( grp , on = 'SK_ID_CURR' , how = 'left' ) application_bureau_test [ 'BUREAU_LOAN_COUNT' ] = application_bureau_test [ 'BUREAU_LOAN_COUNT' ] . fillna ( 0 )
9	def isOneToOne ( df , col1 , col2 ) : first = df . drop_duplicates ( [ col1 , col2 ] ) . groupby ( col1 ) [ col2 ] . count ( ) . max ( ) second = df . drop_duplicates ( [ col1 , col2 ] ) . groupby ( col2 ) [ col1 ] . count ( ) . max ( ) return first + second == 2 isOneToOne ( previous_application , 'SK_ID_CURR' , 'SK_ID_PREV' )
10	from sklearn . preprocessing import MinMaxScaler , StandardScaler , PolynomialFeatures def preprocessing ( X , degree ) : poly = PolynomialFeatures ( degree ) scaler = MinMaxScaler ( ) lin_scaler = StandardScaler ( ) poly_df = pd . DataFrame ( lin_scaler . fit_transform ( poly . fit_transform ( scaler . fit_transform ( X ) ) ) ) poly_df [ 'SK_ID_CURR' ] = X . index poly_df . set_index ( 'SK_ID_CURR' , inplace = True , drop = True ) return poly_df
11	from sklearn . preprocessing import StandardScaler x = X_all . values scaler = StandardScaler ( ) x_scaled = scaler . fit_transform ( x ) X_all = pd . DataFrame ( x_scaled ) . set_index ( X_all . index )
12	from sklearn . linear_model import Ridge import sklearn . linear_model def ridge ( trn_x , trn_y ) : clf = Ridge ( alpha = 20 , copy_X = True , fit_intercept = True , solver = 'auto' , max_iter = 10000 , normalize = False , random_state = 0 , tol = 0.0025 ) clf . fit ( trn_x , trn_y ) return clf
13	data_pass = '/kaggle/input/m5-forecasting-accuracy/' sales = pd . read_csv ( data_pass + 'sales_train_validation.csv' ) calendar = pd . read_csv ( data_pass + 'calendar.csv' ) calendar = reduce_mem_usage ( calendar ) sell_prices = pd . read_csv ( data_pass + 'sell_prices.csv' ) sell_prices = reduce_mem_usage ( sell_prices )
14	W_df = pd . DataFrame ( W , index = roll_index , columns = [ 'w' ] ) data_pass = '/kaggle/input/original-weights/' W_original_df = pd . read_csv ( data_pass + 'weights_validation.csv' ) W_original_df = W_original_df . set_index ( W_df . index ) W_original_df [ 'Predicted' ] = W_df . w W_original_df [ 'diff' ] = W_original_df . Weight - W_original_df . Predicted m = W_original_df . Weight . values - W_df . w . values > 0.000001 W_original_df [ m ]
15	file_pass = '/kaggle/working/' sw_df = pd . read_pickle ( file_pass + 'sw_df.pkl' ) S = sw_df . s . values W = sw_df . w . values SW = sw_df . sw . values roll_mat_df = pd . read_pickle ( file_pass + 'roll_mat_df.pkl' ) roll_index = roll_mat_df . index roll_mat_csr = csr_matrix ( roll_mat_df . values ) del roll_mat_df
16	sub = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/sample_submission.csv' ) sub = sub [ sub . id . str . endswith ( 'validation' ) ] sub . drop ( [ 'id' ] , axis = 1 , inplace = True ) DAYS_PRED = sub . shape [ 1 ] dayCols = [ "d_{}" . format ( i ) for i in range ( 1914 - DAYS_PRED , 1914 ) ] y_true = sales [ dayCols ]
17	def centroids ( X , lbls ) : centroids = np . zeros ( ( len ( np . unique ( lbls ) ) , 2 ) ) for l in np . unique ( lbls ) : mask = lbls == l centroids [ l ] = np . mean ( X [ mask ] , axis = 0 ) return centroids
18	itx = items . T . iloc [ : 5 ] . reset_index ( ) plt . figure ( ) ax = andrews_curves ( itx , class_column = 'index' , colormap = 'cubehelix' ) ax . spines [ 'top' ] . set_visible ( False ) ax . spines [ 'right' ] . set_visible ( False ) ax . grid ( color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) plt . show ( )
19	plt . figure ( ) ax = autocorrelation_plot ( items . T . iloc [ 1 ] ) ax . grid ( color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) plt . show ( )
20	plt . figure ( ) ax = lag_plot ( items . T . iloc [ 1 ] ) ax . grid ( color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) plt . show ( )
21	train_file = '../input/train.csv' test_file = '../input/test.csv' train = pd . read_csv ( train_file , index_col = 'ID_code' ) X_test = pd . read_csv ( test_file , index_col = 'ID_code' )
22	not_null_sex = train [ train [ 'sex' ] . notnull ( ) ] . reset_index ( drop = True ) nan_sex = train [ train [ 'sex' ] . isnull ( ) ] . reset_index ( drop = True )
23	def create_dist ( df , title ) : fig = plt . figure ( figsize = ( 15 , 6 ) ) x = df [ "age_approx" ] . value_counts ( normalize = True ) . to_frame ( ) x = x . reset_index ( ) ax = sns . barplot ( data = x , y = 'age_approx' , x = 'index' ) ax . set ( xlabel = 'Age' , ylabel = 'Percentage' ) ax . set ( title = title ) ;
24	test_x = test [ [ 'image_name' ] ] test_x [ 'image_name' ] = test_x [ 'image_name' ] . apply ( lambda x : x + '.jpg' )
25	train = pd . read_csv ( '/kaggle/input/liverpool-ion-switching/train.csv' ) submission = pd . read_csv ( '/kaggle/input/liverpool-ion-switching/sample_submission.csv' ) test = pd . read_csv ( '/kaggle/input/liverpool-ion-switching/test.csv' )
26	MAX_roll = 6 def movingaverage ( df ) : df [ 'cummax' ] = df [ 'signal' ] . cummax ( ) df [ 'cummin' ] = df [ 'signal' ] . cummin ( ) for i in range ( 2 , MAX_roll ) : df [ 'MA_{}' . format ( i ) ] = df [ 'signal' ] . rolling ( window = i ) . mean ( ) df . fillna ( - 999 , inplace = True ) df . reset_index ( drop = True , inplace = True ) return df
27	import pandas as pd import warnings warnings . filterwarnings ( 'ignore' ) train = pd . read_csv ( "../input/train.csv" ) train . head ( )
28	plt . figure ( figsize = ( 10 , 8 ) ) sns . distplot ( np . log ( train . price_doc . values ) , bins = 60 , kde = True ) plt . xlabel ( 'Price' , fontsize = 12 ) plt . show ( )
29	features = [ 'returnsClosePrevRaw1' , 'returnsOpenPrevRaw1' , 'returnsClosePrevMktres1' , 'returnsOpenPrevMktres1' , 'returnsClosePrevRaw10' , 'returnsOpenPrevRaw10' , 'returnsClosePrevMktres10' , 'returnsOpenPrevMktres10' ] temp_show = market_data_no_outlier_scaled [ features ] temp_show [ 'target' ] = market_data_no_outlier_target [ 'returnsOpenNextMktres10' ] C_mat = temp_show . corr ( ) fig = plt . figure ( figsize = ( 15 , 15 ) ) sb . heatmap ( C_mat , vmax = 0.5 , square = True , annot = True ) plt . show ( ) del temp_show
30	from keras . callbacks import ModelCheckpoint , EarlyStopping early_stopping = EarlyStopping ( monitor = 'val_loss' , patience = 3 , verbose = 1 , mode = 'auto' , restore_best_weights = True ) callbacks_list = [ early_stopping ]
31	def make_my_prediction ( x ) : my_pred = ( model . predict ( x ) ) . reshape ( 1 , - 1 ) [ 0 ] my_pred [ my_pred > 0 ] = 1 my_pred [ my_pred < 0 ] = - 1 return my_pred
32	path = Path ( '../input' ) train_df = pd . read_csv ( path / 'train.csv' ) var_names = [ col for col in train_df if 'var_' in col ]
33	def count_dist_peaks ( series , bins , prominence , width ) : count , division = np . histogram ( series , bins = bins ) peaks , props = find_peaks ( count , prominence = prominence , width = width ) return peaks
34	train_df_num_list = train_df_num . columns . tolist ( ) for i in train_df_num_list : make_histogram ( i )
35	img_name = "/kaggle/input/plant-pathology-2020-fgvc7/images/Test_0.jpg" ; predictions = gtf . Infer ( img_name = img_name ) ; from IPython . display import Image Image ( filename = img_name )
36	import pandas as pd from tqdm import tqdm_notebook as tqdm from scipy . special import softmax df = pd . read_csv ( "/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv" )
37	model = simple_cnn ( ) model . fit_generator ( gen_flow , validation_data = ( [ X_valid , X_angle_valid ] , y_valid ) , steps_per_epoch = len ( X_train ) / batch_size , epochs = 20 )
38	test_img_file_path = train_dogs_filepaths [ 0 ] img_array = cv2 . imread ( test_img_file_path , cv2 . IMREAD_COLOR ) plt . imshow ( img_array ) plt . show ( )
39	ROW_DIMENSION = 60 COLUMN_DIMENSION = 60 CHANNELS = 3 new_array = cv2 . resize ( img_array_gray , ( ROW_DIMENSION , COLUMN_DIMENSION ) ) plt . imshow ( new_array , cmap = 'gray' ) plt . show ( )
40	from tensorflow import keras from keras . models import Sequential from keras . layers import Dense , Flatten , Conv2D , Dropout print ( "Import Successful" )
41	dvc_classifier . compile ( loss = keras . losses . binary_crossentropy , optimizer = 'adam' , metrics = [ 'accuracy' ] )
42	for i in range ( 5 , 11 ) : if prediction_probabilities [ i , 0 ] >= 0.5 : print ( 'I am {:.2%} sure this is a Dog' . format ( prediction_probabilities [ i ] [ 0 ] ) ) else : print ( 'I am {:.2%} sure this is a Cat' . format ( 1 - prediction_probabilities [ i ] [ 0 ] ) ) plt . imshow ( arr_test [ i ] ) plt . show ( )
43	def plot2x2Array ( image , mask ) : f , axarr = plt . subplots ( 1 , 2 ) axarr [ 0 ] . imshow ( image ) axarr [ 1 ] . imshow ( mask ) axarr [ 0 ] . grid ( ) axarr [ 1 ] . grid ( ) axarr [ 0 ] . set_title ( 'Image' ) axarr [ 1 ] . set_title ( 'Mask' )
44	train_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) . map ( decode_image , num_parallel_calls = AUTO ) . map ( data_augment , num_parallel_calls = AUTO ) . repeat ( ) . shuffle ( 512 ) . batch ( BATCH_SIZE ) . prefetch ( AUTO ) )
45	print ( "Read in libraries" ) import numpy as np import pandas as pd import matplotlib . pyplot as plt from scipy . optimize import curve_fit from statsmodels . tsa . statespace . sarimax import SARIMAX from statsmodels . tsa . arima_model import ARIMA from random import random
46	print ( "read in train file" ) df = pd . read_csv ( "/kaggle/input/covid19-global-forecasting-week-2/train.csv" , usecols = [ 'Province_State' , 'Country_Region' , 'Date' , 'ConfirmedCases' , 'Fatalities' ] )
47	sns . set_style ( 'whitegrid' ) sns . set ( rc = { 'figure.figsize' : ( 11.7 , 8.27 ) } )
48	plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( train_df . trip_duration . values , bins = 50 , kde = True ) plt . xlabel ( 'trip_duration' , fontsize = 12 ) plt . show ( )
49	plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( np . log ( train_df . trip_duration . values ) , bins = 50 , kde = True ) plt . xlabel ( 'trip_duration' , fontsize = 12 ) plt . show ( )
50	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "pickup_hour" , data = train_df ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'pick up hour' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
51	import tensorflow as tf from tensorflow . python . data import Dataset import numpy as np import sklearn . metrics as metrics
52	N_TRAINING = 160000 N_VALIDATION = 100000 training_examples = train_data . head ( N_TRAINING ) [ [ my_feature_name ] ] . copy ( ) training_targets = train_data . head ( N_TRAINING ) [ [ my_target_name ] ] . copy ( ) validation_examples = train_data . tail ( N_VALIDATION ) [ [ my_feature_name ] ] . copy ( ) validation_targets = train_data . tail ( N_VALIDATION ) [ [ my_target_name ] ] . copy ( )
53	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) test_ids = test . Id
54	def split_data ( train , y , households , test_percentage = 0.20 , seed = None ) : train2 = train . copy ( ) cv_hhs = np . random . choice ( households , size = int ( len ( households ) * test_percentage ) , replace = False ) cv_idx = np . isin ( households , cv_hhs ) X_test = train2 [ cv_idx ] y_test = y [ cv_idx ] X_train = train2 [ ~ cv_idx ] y_train = y [ ~ cv_idx ] return X_train , y_train , X_test , y_test
55	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) test_ids = test . Id
56	def clean ( X_train , X_test ) : X_train [ 'words' ] = [ re . sub ( "[^a-zA-Z]" , " " , data ) . lower ( ) . split ( ) for data in X_train [ 'text' ] ] X_test [ 'words' ] = [ re . sub ( "[^a-zA-Z]" , " " , data ) . lower ( ) . split ( ) for data in X_test [ 'text' ] ] return X_train , X_test X_train , X_test = clean ( X_train , X_test )
57	import torch import time import numpy as np import pandas as pd from numba import njit torch . manual_seed ( 20191210 ) N_DAYS = 100 N_FAMILIES = 5000 MAX_OCCUPANCY = 300 MIN_OCCUPANCY = 125 INPUT_PATH = '/kaggle/input/santa-workshop-tour-2019/' OUTPUT_PATH = '' DEFAULT_DEVICE = torch . device ( 'cpu' )
58	MAX_ACTION = 4 SOFT_PENALTY_PER_PERSON = 1000 PENALTY_RAMP_TIME = 2000 BATCH_SIZE = 1000 N_BATCHES = 6000 LR = 0.025 GRADIENT_CLIP = 100.0 MAX_PREFERENCE = 8.5 USE_ADAM = True ADAM_BETA_M = 0.9 ADAM_BETA_V = 0.99 ADAM_EPSILON = 0.000001 MOMENTUM = 0.95
59	clf = LGBMClassifier ( n_estimators = 400 , learning_rate = 0.03 , num_leaves = 30 , colsample_bytree = .8 , subsample = .9 , max_depth = 7 , reg_alpha = .1 , reg_lambda = .1 , min_split_gain = .01 , min_child_weight = 2 , silent = - 1 , verbose = - 1 , ) clf . fit ( data_train , y_train , eval_set = [ ( data_train , y_train ) , ( data_valid , y_valid ) ] , eval_metric = 'auc' , verbose = 100 , early_stopping_rounds = 30 )
60	fig , axes = plt . subplots ( 1 , 1 , figsize = ( 15 , 8 ) ) plt . ylim ( ( .5 , 1 ) ) axes . set_ylabel ( "Accuracy" ) axes . set_title ( "Traditional Classfieris Results for 67% Training and 23% Testing with Two Types of Embedding" ) results_df [ results_df . index != "RF" ] . plot ( kind = "bar" , ax = axes )
61	market_train_full_df = env . get_training_data ( ) [ 0 ] sample_market_df = pd . read_csv ( "../input/marketdata_sample.csv" ) sample_news_df = pd . read_csv ( "../input/news_sample.csv" )
62	from wordcloud import WordCloud wordcloud = WordCloud ( width = 1024 , height = 1024 , margin = 0 ) . generate ( " " . join ( market_train_full_df . assetName ) ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 20 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
63	fig , axes = plt . subplots ( figsize = ( 20 , 10 ) ) axes . set_title ( "Volume" ) axes . set_ylabel ( "volume" ) axes . set_xlabel ( "records" ) axes . plot ( market_train_full_df [ "volume" ] )
64	text = " " . join ( tmp_list ) . replace ( "'" , "" ) wordcloud = WordCloud ( width = 1024 , height = 1024 , margin = 0 ) . generate ( text ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
65	import os import pandas as pd import numpy as np import json import matplotlib . pyplot as plt import datetime as datetime from datetime import timedelta , date import seaborn as sns import matplotlib . cm as CM import lightgbm as lgb from sklearn import preprocessing from sklearn . metrics import mean_squared_error from sklearn . model_selection import GridSearchCV , train_test_split
66	list_of_devices = train_data . device . apply ( json . loads ) . tolist ( ) keys = [ ] for devices_iter in list_of_devices : for list_element in list ( devices_iter . keys ( ) ) : if list_element not in keys : keys . append ( list_element )
67	revenue_datetime_df = train_data [ [ "revenue" , "date" ] ] . dropna ( ) revenue_datetime_df [ "revenue" ] = revenue_datetime_df . revenue . astype ( np . int64 ) revenue_datetime_df . head ( )
68	daily_revenue_df = revenue_datetime_df . groupby ( by = [ "date" ] , axis = 0 ) . sum ( ) import matplotlib . pyplot as plt fig , axes = plt . subplots ( figsize = ( 20 , 10 ) ) axes . set_title ( "Daily Revenue" ) axes . set_ylabel ( "Revenue" ) axes . set_xlabel ( "date" ) axes . plot ( daily_revenue_df [ "revenue" ] )
69	fig , axes = plt . subplots ( 1 , 2 , figsize = ( 15 , 10 ) ) traffic_source_df [ "keyword" ] . value_counts ( ) . head ( 10 ) . plot ( kind = "bar" , ax = axes [ 0 ] , title = "keywords (total)" , color = "orange" ) traffic_source_df [ traffic_source_df [ "keyword" ] != "(not provided)" ] [ "keyword" ] . value_counts ( ) . head ( 15 ) . plot ( kind = "bar" , ax = axes [ 1 ] , title = "keywords (dropping NA)" , color = "c" )
70	method = "sklearn_stratified" start = time . time ( ) train , valid = split ( dataset_path = DATASET_PATH , test_size = TEST_SIZE , stratification = method ) print ( f"Dataset split done for {time.time() - start} seconds" )
71	def get_rmse ( ytest_input = 'ytest' , pred_input = 'pred' ) : n , loss = 0 , 0 reader_ytest = open ( ytest_input , 'r' ) reader_pred = open ( pred_input , 'r' ) for label , pred in tqdm ( zip ( reader_ytest , reader_pred ) ) : n += 1 true_score = float ( label ) pred_score = float ( pred ) loss += np . square ( pred_score - true_score ) reader_ytest . close ( ) reader_pred . close ( ) return np . sqrt ( loss / n )
72	postproc_cols = [ col for col in stage2 . columns if col not in helpers_cols ] for col in postproc_cols : print ( 'Make postprocessing for {}' . format ( col ) ) add_custom_postproc ( stage2 , col , stage2_meta_df , optimal_triplets )
73	import gc import os import librosa import numpy as np import pandas as pd from glob import glob from pathlib import Path from librosa import display import matplotlib . pyplot as plt from scipy . io . wavfile import read from IPython . display import HTML , Audio , display_html from IPython . display import display as display_ipython pd . set_option ( 'display.max_colwidth' , 500 )
74	PROJECT_ID = 'geultto' from google . cloud import bigquery client = bigquery . Client ( project = PROJECT_ID , location = "US" ) dataset = client . create_dataset ( 'bqml_example' , exists_ok = True ) from google . cloud . bigquery import magics from kaggle . gcp import KaggleKernelCredentials magics . context . credentials = KaggleKernelCredentials ( ) magics . context . project = PROJECT_ID
75	table = client . get_table ( "kaggle-competition-datasets.geotab_intersection_congestion.train" ) client . list_rows ( table , max_results = 5 ) . to_dataframe ( )
76	SELECT * FROM ML . TRAINING_INFO ( MODEL ` bqml_example . distance_p20 ` ) ORDER BY iteration
77	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync )
78	with strategy . scope ( ) : model = tf . keras . Sequential ( [ irv . InceptionResNetV2 ( include_top = False , weights = 'imagenet' , input_shape = ( 299 , 299 , 3 ) ) , L . GlobalAveragePooling2D ( ) , L . Dense ( 1 , activation = 'sigmoid' ) ] ) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) model . summary ( )
79	import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns import glob import cv2 import os print ( os . listdir ( "../input" ) )
80	plt . rcParams [ "axes.grid" ] = True f , axarr = plt . subplots ( 6 , 5 , figsize = ( 24 , 22 ) ) curr_row = 0 for i in range ( 30 ) : example = cv2 . imread ( test_list [ i ] ) example = example [ : , : , : : - 1 ] col = i % 6 axarr [ col , curr_row ] . imshow ( example ) if col == 5 : curr_row += 1
81	plt . rcParams [ "axes.grid" ] = True f , axarr = plt . subplots ( 6 , 5 , figsize = ( 24 , 22 ) ) curr_row = 0 for i in range ( 30 ) : example = cv2 . imread ( index_list [ i ] ) example = example [ : , : , : : - 1 ] col = i % 6 axarr [ col , curr_row ] . imshow ( example ) if col == 5 : curr_row += 1
82	plt . rcParams [ "axes.grid" ] = True f , axarr = plt . subplots ( 6 , 5 , figsize = ( 24 , 22 ) ) curr_row = 0 for i in range ( 30 ) : example = cv2 . imread ( train_list [ i ] ) example = example [ : , : , : : - 1 ] col = i % 6 axarr [ col , curr_row ] . imshow ( example ) if col == 5 : curr_row += 1
83	temp = pd . DataFrame ( train_data . landmark_id . value_counts ( ) . head ( 10 ) ) temp . reset_index ( inplace = True ) temp . columns = [ 'landmark_id' , 'count' ] temp
84	temp = pd . DataFrame ( train_data . landmark_id . value_counts ( ) . tail ( 10 ) ) temp . reset_index ( inplace = True ) temp . columns = [ 'landmark_id' , 'count' ] temp
85	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
86	from datetime import datetime from os import scandir def convert_date ( timestamp ) : d = datetime . utcfromtimestamp ( timestamp ) formated_date = d . strftime ( '%d %b %Y' ) return formated_date def get_files ( ) : dir_entries = scandir ( 'my_directory/' ) for entry in dir_entries : if entry . is_file ( ) : info = entry . stat ( ) print ( f'{entry.name}\t Last Modified: {convert_date(info.st_mtime)}' )
87	import openslide import skimage . io import random import seaborn as sns import cv2 import pandas as pd import numpy as np import matplotlib import matplotlib . pyplot as plt import PIL import os
88	from nilearn import datasets rest_dataset = datasets . fetch_development_fmri ( n_subjects = 20 ) func_filenames = rest_dataset . func confounds = rest_dataset . confounds
89	import numpy as np import pandas as pd import random import seaborn as sns import matplotlib . pyplot as plt import gc import seaborn as sns sns . set ( style = 'whitegrid' , color_codes = True ) import scipy . stats as st import statsmodels . formula . api as smf from sklearn . ensemble import RandomForestRegressor from sklearn . cross_validation import train_test_split import xgboost as xgb import operator
90	train_items = pd . merge ( df_train , item , how = 'inner' ) train_items1 = pd . merge ( df_train , item , how = 'inner' ) train_items2 = pd . merge ( df_train , item , how = 'inner' )
91	fig , ( axis1 ) = plt . subplots ( 1 , 1 , sharex = True , figsize = ( 15 , 8 ) ) ax1 = oil . plot ( legend = True , ax = axis1 , marker = 'o' , title = "Oil Price" )
92	import os import numpy as np import pandas as pd import warnings from bayes_opt import BayesianOptimization from skopt import BayesSearchCV import matplotlib . pyplot as plt import seaborn as sns import lightgbm as lgb import xgboost as xgb from sklearn . model_selection import train_test_split , StratifiedKFold , cross_val_score import time import sys from sklearn . metrics import roc_auc_score , roc_curve import shap warnings . simplefilter ( action = 'ignore' , category = FutureWarning )
93	graph = lgb . create_tree_digraph ( clf , tree_index = 3 , name = 'Tree3' ) graph . graph_attr . update ( size = "110,110" ) graph
94	pubs_map = folium . Map ( location = [ 40.742459 , - 73.971765 ] , zoom_start = 12 ) data = [ [ x [ 0 ] , x [ 1 ] , 1 ] for x in np . array ( bars [ [ 'Latitude' , 'Longitude' ] ] ) ] HeatMap ( data , radius = 20 ) . add_to ( pubs_map ) pubs_map
95	t4_params = { 'boosting_type' : 'gbdt' , 'objective' : 'multiclass' , 'nthread' : - 1 , 'silent' : True , 'num_leaves' : 2 ** 4 , 'learning_rate' : 0.05 , 'max_depth' : - 1 , 'max_bin' : 255 , 'subsample_for_bin' : 50000 , 'subsample' : 0.8 , 'subsample_freq' : 1 , 'colsample_bytree' : 0.6 , 'reg_alpha' : 1 , 'reg_lambda' : 0 , 'min_split_gain' : 0.5 , 'min_child_weight' : 1 , 'min_child_samples' : 10 , 'scale_pos_weight' : 1 } t4 = lgbm . sklearn . LGBMClassifier ( n_estimators = 1000 , seed = 0 , ** t4_params )
96	plt . figure ( figsize = ( 12 , 6 ) ) plt . title ( 'Number of Team Members' ) tmp = dataset . groupby ( [ 'matchId' , 'groupId' ] ) [ 'Id' ] . agg ( 'count' ) sns . countplot ( tmp )
97	sub = pd . DataFrame ( ) sub [ 'Id' ] = test_df [ 'Id' ] sub [ 'winPlacePerc' ] = lgb_sub_preds sub [ 'winPlacePerc' ] [ sub [ 'winPlacePerc' ] > 1 ] = 1 sub . to_csv ( 'lgb_submission.csv' , index = False )
98	import numpy as np import pandas as pd import os import matplotlib . pyplot as plt from IPython . core . interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = 'all'
99	df = pd . read_csv ( '../input/train_labels.csv' ) print ( 'Shape of DataFrame' , df . shape ) df . head ( )
100	df [ df [ 'id' ] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2' ] df [ df [ 'id' ] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe' ] df . head ( )
101	y = df_train [ 'label' ] df_train , df_val = train_test_split ( df_train , test_size = 0.1 , random_state = 0 , stratify = y )
102	model = Net . build ( width = 96 , height = 96 , depth = 3 , classes = 2 ) from keras . optimizers import SGD , Adam , Adagrad model . compile ( optimizer = Adam ( lr = 0.0001 ) , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] )
103	model . load_weights ( 'checkpoint.h5' ) val_loss , val_acc = \ model . evaluate_generator ( test_gen , steps = len ( df_val ) ) print ( 'val_loss:' , val_loss ) print ( 'val_acc:' , val_acc )
104	test_filenames = test_gen . filenames df_preds [ 'file_names' ] = test_filenames def extract_id ( x ) : a = x . split ( '/' ) b = a [ 1 ] . split ( '.' ) extracted_id = b [ 0 ] return extracted_id df_preds [ 'id' ] = df_preds [ 'file_names' ] . apply ( extract_id ) df_preds . head ( )
105	submission = pd . DataFrame ( { 'id' : image_id , 'label' : y_pred , } ) . set_index ( 'id' ) submission . to_csv ( 'submission.csv' , columns = [ 'label' ] )
106	def submit ( predictions ) : submit = pd . read_csv ( '../input/sample_submission.csv' ) submit [ "target" ] = predictions submit . to_csv ( "submission.csv" , index = False ) def fallback_auc ( y_true , y_pred ) : try : return metrics . roc_auc_score ( y_true , y_pred ) except : return 0.5 def auc ( y_true , y_pred ) : return tf . py_function ( fallback_auc , ( y_true , y_pred ) , tf . double )
107	import numpy as np import pandas as pd import os import time import datetime import gc from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import GroupKFold from sklearn . metrics import mean_absolute_error import matplotlib . pyplot as plt import seaborn as sns from tqdm import tqdm_notebook as tqdm from catboost import CatBoostRegressor , Pool import warnings warnings . filterwarnings ( "ignore" )
108	train = pd . read_csv ( '../input/train.csv' ) structures = pd . read_csv ( '../input/structures.csv' ) print ( 'Train dataset shape is -> rows: {} cols:{}' . format ( train . shape [ 0 ] , train . shape [ 1 ] ) ) print ( 'Structures dataset shape is -> rows: {} cols:{}' . format ( structures . shape [ 0 ] , structures . shape [ 1 ] ) )
109	def catboost_fit ( model , X_train , y_train , X_val , y_val ) : train_pool = Pool ( X_train , y_train ) val_pool = Pool ( X_val , y_val ) model . fit ( train_pool , eval_set = val_pool ) return model model = CatBoostRegressor ( iterations = 20000 , max_depth = 9 , objective = 'MAE' , task_type = 'GPU' , verbose = False ) model = catboost_fit ( model , tr_X , tr_y , val_X , val_y )
110	forest [ 'Cover_Type' ] . replace ( { 1 : 'Spruce/Fir' , 2 : 'Lodgepole Pine' , 3 : 'Ponderosa Pine' , 4 : 'Cottonwood/Willow' , 5 : 'Aspen' , 6 : 'Douglas-fir' , 7 : 'Krummholz' } , inplace = True ) forest = forest . rename ( columns = { "Wilderness_Area1" : "Rawah_WA" , "Wilderness_Area2" : "Neota_WA" , "Wilderness_Area3" : "Comanche_Peak_WA" , "Wilderness_Area4" : "Cache_la_Poudre_WA" , "Horizontal_Distance_To_Hydrology" : "HD_Hydrology" , "Vertical_Distance_To_Hydrology" : "VD_Hydrology" , "Horizontal_Distance_To_Roadways" : "HD_Roadways" , "Horizontal_Distance_To_Fire_Points" : "HD_Fire_Points" } )
111	fig = px . histogram ( forest , x = "HD_Hydrology" , color = "Cover_Type" , marginal = 'rug' , title = "HD_Hydrology Histogram" , height = 500 , width = 800 ) fig . show ( )
112	fig = px . histogram ( forest , x = "VD_Hydrology" , color = "Cover_Type" , marginal = 'rug' , title = "VD_Hydrology Histogram" , height = 500 , width = 800 ) fig . show ( )
113	fig = px . histogram ( forest , x = "HD_Roadways" , color = "Cover_Type" , marginal = 'rug' , title = "HD_Roadways Histogram" , height = 500 , width = 800 ) fig . show ( )
114	temp = forest . groupby ( [ 'Cover_Type' ] , as_index = False ) [ [ 'HD_Roadways' ] ] . median ( ) fig = px . bar ( temp . sort_values ( by = "HD_Roadways" , ascending = False ) , x = "HD_Roadways" , y = "Cover_Type" , color = 'Cover_Type' , orientation = 'h' , height = 300 , width = 900 ) fig . show ( )
115	fig = px . histogram ( forest , x = "HD_Fire_Points" , color = "Cover_Type" , marginal = 'rug' , title = "HD Fire Points Histogram" , height = 500 , width = 800 ) fig . show ( )
116	fig = px . histogram ( forest , x = "Hillshade_Noon" , color = "Cover_Type" , marginal = 'box' , title = "Hillshade at Noon Histogram" , height = 500 , width = 800 ) fig . show ( )
117	import numpy as np , pandas as pd from sklearn . model_selection import StratifiedKFold from sklearn . metrics import roc_auc_score from sklearn . preprocessing import StandardScaler from sklearn . feature_selection import VarianceThreshold from sklearn . discriminant_analysis import QuadraticDiscriminantAnalysis from tqdm import tqdm_notebook import warnings warnings . filterwarnings ( 'ignore' )
118	batch_size = 16 lrG = 0.001 lrD = 0.001 beta1 = 0.5 epochs = 300 real_label = 0.5 fake_label = 0 nz = 128 device = torch . device ( "cuda" if torch . cuda . is_available ( ) else "cpu" )
119	x = next ( iter ( train_loader ) ) fig = plt . figure ( figsize = ( 25 , 16 ) ) for ii , img in enumerate ( x ) : ax = fig . add_subplot ( 4 , 8 , ii + 1 , xticks = [ ] , yticks = [ ] ) img = img . numpy ( ) . transpose ( 1 , 2 , 0 ) plt . imshow ( ( img + 1 ) / 2 )
120	MAGIC_N = 42 train_subset = train [ train [ 'wheezy-copper-turtle-magic' ] == MAGIC_N ] test_subset = test [ test [ 'wheezy-copper-turtle-magic' ] == MAGIC_N ] concated = pd . concat ( [ train_subset , test_subset ] ) a = train_subset . std ( ) > 1.2 cols = [ idx for idx in a . index if a [ idx ] ] concated = concated [ cols + [ 'target' ] ]
121	example = openslide . OpenSlide ( os . path . join ( BASE_FOLDER + "train_images" , '005e66f06bce9c2e49142536caf2f6ee.tiff' ) ) patch = example . read_region ( ( 17800 , 19500 ) , 0 , ( 256 , 256 ) ) display ( patch ) example . close ( )
122	pen_marked_images = [ 'fd6fe1a3985b17d067f2cb4d5bc1e6e1' , 'ebb6a080d72e09f6481721ef9f88c472' , 'ebb6d5ca45942536f78beb451ee43cc4' , 'ea9d52d65500acc9b9d89eb6b82cdcdf' , 'e726a8eac36c3d91c3c4f9edba8ba713' , 'e90abe191f61b6fed6d6781c8305fe4b' , 'fd0bb45eba479a7f7d953f41d574bf9f' , 'ff10f937c3d52eff6ad4dd733f2bc3ac' , 'feee2e895355a921f2b75b54debad328' , ] overlay_mask_on_slide ( pen_marked_images )
123	import numpy as np import pandas as pd import os import matplotlib . pyplot as plt import torch from torch import nn , optim import torch . nn . functional as F from torchvision import datasets , transforms from torchvision . utils import save_image from torch . utils . data import Dataset , DataLoader from torch . autograd import Variable from PIL import Image from tqdm import tqdm_notebook as tqdm
124	x = next ( iter ( train_loader ) ) fig = plt . figure ( figsize = ( 25 , 16 ) ) for ii , img in enumerate ( x ) : ax = fig . add_subplot ( 4 , 8 , ii + 1 , xticks = [ ] , yticks = [ ] ) img = img . numpy ( ) . transpose ( 1 , 2 , 0 ) plt . imshow ( ( img + 1. ) / 2. )
125	def seed_everything ( seed = 1029 ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True seed_everything ( )
126	df_train = pd . read_csv ( "../input/train.csv" ) df_test = pd . read_csv ( "../input/test.csv" ) df = pd . concat ( [ df_train , df_test ] , sort = True )
127	np . save ( "x_train" , x_train ) np . save ( "x_test" , x_test ) np . save ( "y_train" , y_train ) np . save ( "features" , features ) np . save ( "test_features" , test_features ) np . save ( "word_index.npy" , word_index )
128	x_train = np . load ( "x_train.npy" ) x_test = np . load ( "x_test.npy" ) y_train = np . load ( "y_train.npy" ) features = np . load ( "features.npy" ) test_features = np . load ( "test_features.npy" ) word_index = np . load ( "word_index.npy" ) . item ( )
129	seed_everything ( ) glove_embeddings = load_glove ( word_index ) paragram_embeddings = load_para ( word_index ) embedding_matrix = np . mean ( [ glove_embeddings , paragram_embeddings ] , axis = 0 ) del glove_embeddings , paragram_embeddings gc . collect ( ) np . shape ( embedding_matrix )
130	class MyDataset ( Dataset ) : def __init__ ( self , dataset ) : self . dataset = dataset def __getitem__ ( self , index ) : data , target = self . dataset [ index ] return data , target , index def __len__ ( self ) : return len ( self . dataset )
131	import numpy as np import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import os merchants = pd . read_csv ( "../input/merchants.csv" )
132	merchants [ 'category_2' ] = merchants [ 'category_2' ] . fillna ( 0 ) . astype ( int ) merchants . loc [ merchants [ 'city_id' ] == - 1 , 'city_id' ] = 0 merchants . loc [ merchants [ 'state_id' ] == - 1 , 'state_id' ] = 0
133	def rating ( x ) : if np . isfinite ( x ) and x > 0 : x = ( 1 / x ) - 1 if x > 1 : r = 1 elif x <= 1 and x > 0 : r = 2 elif x == 0 : r = 3 elif x < 0 and x >= - 1 : r = 4 else : r = 5 else : r = 5 return r
134	merchants [ 'most_recent_sales_range' ] = merchants [ 'most_recent_sales_range' ] \ . map ( { 'A' : 1 , 'B' : 2 , 'C' : 3 , 'D' : 4 , 'E' : 5 } ) merchants [ 'most_recent_purchases_range' ] = merchants [ 'most_recent_purchases_range' ] \ . map ( { 'A' : 1 , 'B' : 2 , 'C' : 3 , 'D' : 4 , 'E' : 5 } )
135	from sklearn . preprocessing import LabelEncoder cat_features = [ 'Sex' , 'SmokingStatus' ] encoder = LabelEncoder ( ) encoded = data [ cat_features ] . apply ( encoder . fit_transform )
136	import matplotlib . pyplot as plt import seaborn as seabornInstance from sklearn . model_selection import train_test_split from sklearn . linear_model import LinearRegression from sklearn import metrics
137	test = pd . read_csv ( '../input/nomad2018-predict-transparent-conductors/test.csv' ) test_id = test . id train = pd . read_csv ( '../input/nomad2018-predict-transparent-conductors/train.csv' )
138	def get_prop_list ( path_to_element_data ) : return [ f [ : - 4 ] for f in os . listdir ( path_to_element_data ) ] path_to_element_data = '../input/elemental-properties/' properties = get_prop_list ( path_to_element_data ) print ( sorted ( properties ) )
139	for col in [ 'x_Al' , 'x_Ga' , 'x_In' , 'a' , 'b' , 'c' , 'vol' , 'atomic_density' ] : for x in all_data . sg . unique ( ) : sns . distplot ( all_data [ all_data [ 'sg' ] == x ] [ col ] ) plt . title ( col ) plt . show ( )
140	for col in [ 'E' , 'Eg' ] : sns . distplot ( ( train [ col ] ) ) plt . title ( col ) plt . show ( )
141	def rmsle ( h , y ) : return np . sqrt ( np . square ( np . log ( h + 1 ) - np . log ( y + 1 ) ) . mean ( ) )
142	import keras import tensorflow as tf import keras . backend as K from keras . regularizers import l2 from keras . optimizers import Adam from keras . models import Model from keras . models import Sequential from keras . layers import Dense , Dropout , BatchNormalization , Input
143	dict_columns = [ 'belongs_to_collection' , 'genres' , 'production_companies' , 'production_countries' , 'spoken_languages' , 'Keywords' , 'cast' , 'crew' ] def text_to_dict ( df ) : for column in dict_columns : df [ column ] = df [ column ] . apply ( lambda x : { } if pd . isna ( x ) else ast . literal_eval ( x ) ) return df train = text_to_dict ( train ) test = text_to_dict ( test )
144	from sklearn . model_selection import train_test_split train_idx , _ = train_test_split ( scores . index , train_size = 0.2 , random_state = 223 , stratify = scores . stratify )
145	from nilearn import input_data basc197_masker = input_data . NiftiLabelsMasker ( basc_197 , mask_img = brain_mask ) def load_matlab ( participant_id , masker , path = '../input/trends-assessment-prediction/fMRI_train/' ) : mat = np . array ( h5py . File ( f'{path}{participant_id}.mat' , mode = 'r' ) . get ( 'SM_feature' ) ) mat = masker . fit_transform ( nb . Nifti1Image ( mat . transpose ( [ 3 , 2 , 1 , 0 ] ) , affine = masker . mask_img . affine ) ) return mat . flatten ( )
146	components2 = pd . DataFrame ( components2 , index = scores_stat . index ) pca2_corr = [ ] for kk in range ( 20 ) : pca2_corr . append ( scores_stat [ [ 'age' , 'domain1_var1' , 'domain1_var2' , 'domain2_var1' , 'domain2_var2' ] ] . corrwith ( components2 . loc [ : , kk ] ) ) pca2_scorr = pd . concat ( pca2_corr , axis = 1 ) pca2_scorr
147	model = sm . PHReg ( ys , xs , cs ) result = model . fit ( ) baseline_cum_hazard_func = result . baseline_cumulative_hazard_function [ 0 ] pred_index = np . arange ( - 99 , 100 )
148	def image_to_hu ( image_path , image_id ) : dicom = pydicom . read_file ( image_path + 'ID_' + image_id + '.dcm' ) image = dicom . pixel_array . astype ( np . float64 ) intercept = dicom . RescaleIntercept slope = dicom . RescaleSlope if slope != 1 : image = slope * image . astype ( np . float64 ) image = image . astype ( np . float64 ) image += np . float64 ( intercept ) image [ image < - 1024 ] = - 1024 return image , dicom
149	def image_pad ( image , new_height , new_width ) : height , width = image . shape im_bg = np . zeros ( ( new_height , new_width ) ) pad_left = int ( ( new_width - width ) / 2 ) pad_top = int ( ( new_height - height ) / 2 ) im_bg [ pad_top : pad_top + height , pad_left : pad_left + width ] = image return im_bg
150	import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns from itertools import cycle color_cycle = cycle ( plt . rcParams [ 'axes.prop_cycle' ] . by_key ( ) [ 'color' ] ) import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
151	def image_ids_in ( root_dir , is_train_data = False ) : ids = [ ] for id in os . listdir ( root_dir ) : if id in TRAIN_ERROR_IDS : print ( 'Skipping ID due to bad training data:' , id ) else : ids . append ( id ) return ids TRAIN_IMAGE_IDS = image_ids_in ( TRAIN_DIR , is_train_data = True ) TEST_IMAGE_IDS = image_ids_in ( TEST_DIR ) print ( 'Examples:' , TRAIN_IMAGE_IDS [ 22 ] , TEST_IMAGE_IDS [ 22 ] )
152	import pandas as pd from sklearn . feature_extraction . text import CountVectorizer from nltk . corpus import stopwords from sklearn import preprocessing from sklearn . naive_bayes import MultinomialNB
153	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) sample = pd . read_csv ( '../input/sample_submission.csv' ) train . head ( 3 )
154	xtrain_ctv_all = ctv . fit_transform ( train . text . values ) xtest_ctv_all = ctv . transform ( test . text . values ) clf = MultinomialNB ( alpha = 1.0 ) clf . fit ( xtrain_ctv_all , y )
155	train_text = train_data [ 'question_text' ] test_text = test_data [ 'question_text' ] train_target = train_data [ 'target' ] all_text = train_text . append ( test_text )
156	import os , gc import datetime import numpy as np import pandas as pd import category_encoders from sklearn . impute import SimpleImputer from sklearn . metrics import mean_squared_error from sklearn . model_selection import KFold from sklearn . preprocessing import LabelEncoder from sklearn . linear_model import Lasso from sklearn . linear_model import Ridge from lightgbm import LGBMRegressor from mlxtend . regressor import StackingRegressor from pandas . api . types import is_categorical_dtype from pandas . api . types import is_datetime64_any_dtype as is_datetime
157	predictions = 0 for model in models : predictions += np . expm1 ( model . predict ( np . array ( test ) ) ) / len ( models ) del model ; gc . collect ( ) del test , models ; gc . collect ( )
158	train . signal . plot ( ) plt . title ( 'Train' ) plt . show ( ) test . signal . plot ( ) plt . title ( 'Test' ) plt . show ( ) train . to_csv ( "train_synthetic.csv" , index = False , float_format = '%.4f' ) test . to_csv ( "test_synthetic.csv" , index = False , float_format = '%.4f' )
159	batch_size = 64 gen = ImageDataGenerator ( horizontal_flip = True , vertical_flip = True , width_shift_range = 0.1 , height_shift_range = 0.1 , zoom_range = 0.1 , rotation_range = 10 )
160	from sklearn . model_selection import train_test_split X_train , X_validation , y_train , y_validation = train_test_split ( X , y [ 0 : 99 ] , test_size = 0.40 , random_state = 14113 ) print ( 'X_train is a {} object' . format ( type ( X_train ) ) ) print ( 'it has shape {}' . format ( X_train . shape ) ) print ( 'y_train is a {} object' . format ( type ( y_train ) ) ) print ( 'it has {} elements' . format ( len ( y_train ) ) )
161	import warnings warnings . filterwarnings ( "ignore" ) import numpy as np import pandas as pd from datetime import datetime import matplotlib . pyplot as plt import seaborn as sns from scipy import stats import itertools from sklearn import model_selection from sklearn . ensemble import RandomForestRegressor from sklearn import metrics
162	df_train = pd . read_csv ( "../input/train.csv" , sep = ',' , parse_dates = [ 'Date' ] , date_parser = str_to_date , low_memory = False ) df_store = pd . read_csv ( "../input/store.csv" , low_memory = False )
163	df_test = pd . read_csv ( "../input/test.csv" , sep = ',' , parse_dates = [ 'Date' ] , date_parser = str_to_date , low_memory = False ) print ( "The Test dataset has {} Rows and {} Variables" . format ( str ( df_test . shape [ 0 ] ) , str ( df_test . shape [ 1 ] ) ) )
164	rfr_val = RandomForestRegressor ( n_estimators = 128 , criterion = 'mse' , max_depth = 20 , min_samples_split = 10 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , min_impurity_split = None , bootstrap = True , oob_score = False , n_jobs = 4 , random_state = 35 , verbose = 0 , warm_start = False ) model_RF_test = rfr_val . fit ( X_train , y_train )
165	import mxnet as mx from mxnet import gluon import numpy as np import pandas as pd import matplotlib . pyplot as plt import json import os from tqdm . autonotebook import tqdm from pathlib import Path
166	from gluonts . model . deepar import DeepAREstimator from gluonts . distribution . neg_binomial import NegativeBinomialOutput from gluonts . trainer import Trainer estimator = DeepAREstimator ( prediction_length = prediction_length , freq = "D" , distr_output = NegativeBinomialOutput ( ) , use_feat_dynamic_real = True , use_feat_static_cat = True , cardinality = stat_cat_cardinalities , trainer = Trainer ( learning_rate = 1e-3 , epochs = 100 , num_batches_per_epoch = 50 , batch_size = 32 ) ) predictor = estimator . train ( train_ds )
167	if submission == True : forecasts_acc_sub = np . zeros ( ( len ( forecasts ) * 2 , single_prediction_length ) ) forecasts_acc_sub [ : len ( forecasts ) ] = forecasts_acc [ : , : single_prediction_length ] forecasts_acc_sub [ len ( forecasts ) : ] = forecasts_acc [ : , single_prediction_length : ]
168	import mxnet as mx from mxnet import gluon import numpy as np import pandas as pd import matplotlib . pyplot as plt import json import os from tqdm . autonotebook import tqdm from pathlib import Path
169	from gluonts . model . deepar import DeepAREstimator from gluonts . distribution . neg_binomial import NegativeBinomialOutput from gluonts . trainer import Trainer estimator = DeepAREstimator ( prediction_length = prediction_length , freq = "D" , distr_output = NegativeBinomialOutput ( ) , use_feat_dynamic_real = True , use_feat_static_cat = True , cardinality = stat_cat_cardinalities , trainer = Trainer ( learning_rate = 1e-3 , epochs = 100 , num_batches_per_epoch = 200 , batch_size = 100 ) ) predictor = estimator . train ( train_ds )
170	print ( 'Original image shape: {}' . format ( im . shape ) ) from skimage . color import rgb2gray im_gray = rgb2gray ( im ) print ( 'New image shape: {}' . format ( im_gray . shape ) )
171	from scipy import ndimage labels , nlabels = ndimage . label ( mask ) label_arrays = [ ] for label_num in range ( 1 , nlabels + 1 ) : label_mask = np . where ( labels == label_num , 1 , 0 ) label_arrays . append ( label_mask ) print ( 'There are {} separate components / objects detected.' . format ( nlabels ) )
172	def rle_encoding ( x ) : dots = np . where ( x . T . flatten ( ) == 1 ) [ 0 ] run_lengths = [ ] prev = - 2 for b in dots : if ( b > prev + 1 ) : run_lengths . extend ( ( b + 1 , 0 ) ) run_lengths [ - 1 ] += 1 prev = b return " " . join ( [ str ( i ) for i in run_lengths ] ) print ( 'RLE Encoding for the current mask is: {}' . format ( rle_encoding ( label_mask ) ) )
173	import numpy as np import pandas as pd import matplotlib . pyplot as plt df_train = pd . read_csv ( '../input/train.csv' ) subset = df_train . loc [ ( df_train [ 'crew' ] == 1 ) & ( df_train [ 'experiment' ] == 'CA' ) ] subset . sort_values ( by = 'time' ) plt . plot ( subset [ 'r' ] [ 3000 : 4024 ] )
174	X_train , X_test , y_train , y_test = train_test_split ( processedtext , sentiment , test_size = 0.05 , random_state = 0 ) print ( f'Data Split done.' )
175	X_train = vectoriser . transform ( X_train ) X_test = vectoriser . transform ( X_test ) print ( f'Data Transformed.' )
176	LRmodel = LogisticRegression ( C = 2 , max_iter = 1000 , n_jobs = - 1 ) LRmodel . fit ( X_train , y_train ) model_Evaluate ( LRmodel )
177	file = open ( 'vectoriser-ngram-(1,2).pickle' , 'wb' ) pickle . dump ( vectoriser , file ) file . close ( ) file = open ( 'Sentiment-LR.pickle' , 'wb' ) pickle . dump ( LRmodel , file ) file . close ( ) file = open ( 'Sentiment-BNB.pickle' , 'wb' ) pickle . dump ( BNBmodel , file ) file . close ( )
178	import os import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
179	X = librosa . stft ( data ) Xdb = librosa . amplitude_to_db ( abs ( X ) ) plt . figure ( figsize = ( 14 , 5 ) ) librosa . display . specshow ( Xdb , sr = sr , x_axis = 'time' , y_axis = 'hz' ) plt . colorbar ( )
180	n0 = 10000 n1 = 10100 plt . figure ( figsize = ( 14 , 5 ) ) plt . plot ( data [ n0 : n1 ] ) plt . grid ( )
181	title_grp = train_labels . groupby ( [ 'title' ] ) [ 'game_session' ] . count ( ) . reset_index ( ) display ( title_grp ) fig = go . Figure ( data = [ go . Pie ( labels = title_grp . title , values = title_grp . game_session ) ] ) fig . show ( )
182	print ( "Qualitative/Categorical Columns:" ) cate_cols = train_data . select_dtypes ( include = [ 'object' ] ) . columns print ( cate_cols ) print ( "\nQuntitative/Numerical Columns:" ) num_cols = train_data . select_dtypes ( exclude = [ 'object' ] ) . columns print ( num_cols )
183	event_count = test_data [ 'title' ] . value_counts ( ) . reset_index ( ) event_count [ 'index' ] = event_count [ 'index' ] . astype ( 'category' ) fig = px . bar ( event_count [ 0 : 10 ] , x = 'index' , y = 'title' , hover_data = [ 'title' ] , color = 'index' , labels = { 'title' : 'Event Count' } , height = 400 ) fig . show ( )
184	type_count = test_data [ 'type' ] . value_counts ( ) . reset_index ( ) total = len ( test_data ) type_count [ 'percent' ] = round ( ( type_count [ 'type' ] / total ) * 100 , 2 ) print ( type_count ) fig = px . bar ( type_count , x = 'index' , y = 'type' , hover_data = [ 'index' , 'percent' ] , color = 'type' , labels = { 'type' : 'Type Count' } , height = 400 ) fig . show ( )
185	type_count = train_data [ 'world' ] . value_counts ( ) . reset_index ( ) total = len ( train_data ) type_count [ 'percent' ] = round ( ( type_count [ 'world' ] / total ) * 100 , 2 ) print ( type_count ) fig = px . bar ( type_count , x = 'index' , y = 'world' , hover_data = [ 'index' , 'percent' ] , color = 'world' , labels = { 'world' : 'World Count' } , height = 400 ) fig . show ( )
186	type_count = test_data [ 'world' ] . value_counts ( ) . reset_index ( ) total = len ( test_data ) type_count [ 'percent' ] = round ( ( type_count [ 'world' ] / total ) * 100 , 2 ) print ( type_count ) fig = px . bar ( type_count , x = 'index' , y = 'world' , hover_data = [ 'index' , 'percent' ] , color = 'world' , labels = { 'world' : 'World Count' } , height = 400 ) fig . show ( )
187	date_count = train_data . groupby ( [ 'date' ] ) [ 'installation_id' ] . count ( ) . reset_index ( ) fig = go . Figure ( data = go . Scatter ( x = date_count [ 'date' ] , y = date_count [ 'installation_id' ] ) ) fig . show ( )
188	week_type_count = train_data . groupby ( [ 'weekofyear' , 'type' ] ) [ 'installation_id' ] . count ( ) . reset_index ( ) fig = px . line ( week_type_count , x = "weekofyear" , y = "installation_id" , color = 'type' ) fig . show ( )
189	date_title_count = test_data . groupby ( [ 'title' ] ) [ 'game_time' ] . count ( ) . reset_index ( ) date_title_count . sort_values ( by = [ 'game_time' ] , inplace = True , ascending = True ) print ( date_title_count ) fig = px . line ( date_title_count , x = "title" , y = "game_time" ) fig . show ( )
190	data = train_data [ train_data [ 'event_code' ] == 4030 ] data . shape print ( data [ 'title' ] . value_counts ( ) ) print ( data [ 'type' ] . value_counts ( ) ) print ( data [ 'world' ] . value_counts ( ) )
191	game = train_data . groupby ( [ 'title' ] ) [ 'game_time' ] . max ( ) . reset_index ( ) game . sort_values ( by = [ 'game_time' ] , inplace = True , ascending = False ) fig = px . bar ( game [ 0 : 10 ] , x = 'game_time' , y = 'title' , orientation = 'h' , hover_data = [ 'title' ] , color = 'game_time' , labels = { 'game_time' : 'Game Time' } , height = 400 ) fig . show ( )
192	world_type = train_data . groupby ( [ 'world' , 'type' ] ) [ 'game_time' ] . mean ( ) . reset_index ( ) world_type plt . figure ( figsize = ( 20 , 7 ) ) ax = sns . barplot ( x = "world" , y = "game_time" , hue = "type" , data = world_type )
193	world_type = train_data . groupby ( [ 'world' , 'type' ] ) [ 'event_count' ] . count ( ) . reset_index ( ) world_type plt . figure ( figsize = ( 20 , 7 ) ) ax = sns . barplot ( x = "world" , y = "event_count" , hue = "type" , data = world_type )
194	def get_unique ( data , feat ) : return data [ feat ] . nunique ( ) for col in train_labels . columns . values : print ( "unique number of values in " , col ) print ( get_unique ( train_labels , col ) )
195	dd = test_data . groupby ( 'date' ) [ 'world' ] . value_counts ( ) dd = dd . reset_index ( name = 'count' ) dd fig = px . line ( dd , x = "date" , y = "count" , color = 'world' ) fig . show ( )
196	train = pd . read_csv ( '/kaggle/input/cat-in-the-dat/train.csv' ) test = pd . read_csv ( '/kaggle/input/cat-in-the-dat/test.csv' ) target = train [ 'target' ] train_id = train [ 'id' ] test_id = test [ 'id' ] train . drop ( [ 'target' , 'id' ] , axis = 1 , inplace = True ) test . drop ( 'id' , axis = 1 , inplace = True )
197	WOE_encoder = WOEEncoder ( ) train_woe = WOE_encoder . fit_transform ( train [ feature_list ] , target ) test_woe = WOE_encoder . transform ( test [ feature_list ] )
198	fig , ax = plt . subplots ( 1 , 4 , figsize = ( 20 , 5 ) ) for i in range ( 4 ) : sns . countplot ( f'bin_{i}' , hue = 'target' , data = train_df , ax = ax [ i ] ) ax [ i ] . set_title ( f'bin_{i} feature countplot' ) print ( percentage_of_feature_target ( train_df , f'bin_{i}' , 'target' , 1 ) ) plt . show ( )
199	fig , ax = plt . subplots ( 4 , 1 , figsize = ( 40 , 40 ) ) for i in range ( 5 , 9 ) : sns . countplot ( sorted ( train_df [ f'nom_{i}' ] ) , ax = ax [ i - 5 ] ) plt . setp ( ax [ i - 5 ] . get_xticklabels ( ) , rotation = 90 ) plt . show ( ) ;
200	train_df = pd . read_csv ( '../input/nyc-taxi-trip-duration/train.csv' , parse_dates = [ 'pickup_datetime' ] ) test_df = pd . read_csv ( '../input/nyc-taxi-trip-duration/test.csv' , parse_dates = [ 'pickup_datetime' ] ) print ( "Train dataframe shape : " , train_df . shape ) print ( "Test dataframe shape : " , test_df . shape )
201	fig , ax = plt . subplots ( 1 , 2 , figsize = ( 18 , 7 ) ) sns . distplot ( train_df [ 'X' ] , ax = ax [ 0 ] ) sns . distplot ( train_df [ 'Y' ] , ax = ax [ 1 ] ) plt . show ( )
202	temp_data = StringIO ( ) temp_df = pd . read_csv ( temp_data ) train_df = pd . merge ( train_df , temp_df , on = "category_name" , how = "left" )
203	fig , ax = plt . subplots ( figsize = ( 12 , 18 ) ) lgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) ax . grid ( False ) plt . title ( "LightGBM - Feature Importance" , fontsize = 15 ) plt . show ( )
204	target_col = "target" plt . figure ( figsize = ( 8 , 6 ) ) plt . scatter ( range ( train_df . shape [ 0 ] ) , np . sort ( train_df [ target_col ] . values ) ) plt . xlabel ( 'index' , fontsize = 12 ) plt . ylabel ( 'Loyalty Score' , fontsize = 12 ) plt . show ( )
205	gdf = hist_df . groupby ( "card_id" ) gdf = gdf [ "purchase_amount" ] . agg ( [ 'sum' , 'mean' , 'std' , 'min' , 'max' ] ) . reset_index ( ) gdf . columns = [ "card_id" , "sum_hist_trans" , "mean_hist_trans" , "std_hist_trans" , "min_hist_trans" , "max_hist_trans" ] train_df = pd . merge ( train_df , gdf , on = "card_id" , how = "left" ) test_df = pd . merge ( test_df , gdf , on = "card_id" , how = "left" )
206	fig , ax = plt . subplots ( 1 , 2 , figsize = ( 17 , 5 ) ) sns . distplot ( weather_train [ 'wind_direction' ] , ax = ax [ 0 ] ) sns . distplot ( weather_train [ 'wind_speed' ] , ax = ax [ 1 ] ) plt . show ( )
207	def cloud_graph ( df ) : df = df . sort_values ( 'timestamp' ) fig = go . Figure ( ) fig . add_trace ( go . Scatter ( x = df [ 'timestamp' ] , y = df [ 'cloud_coverage' ] , name = "Cloud Coverage" , line_color = 'lightskyblue' , opacity = 0.7 ) ) fig . update_layout ( template = 'plotly_dark' , title_text = 'Cloud' , xaxis_rangeslider_visible = True ) fig . show ( )
208	cnt_srs = train_df [ 'bedrooms' ] . value_counts ( ) plt . figure ( figsize = ( 8 , 4 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 2 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'bedrooms' , fontsize = 12 ) plt . show ( )
209	plt . figure ( figsize = ( 8 , 6 ) ) plt . scatter ( range ( train_df . shape [ 0 ] ) , np . sort ( train_df . price . values ) ) plt . xlabel ( 'index' , fontsize = 12 ) plt . ylabel ( 'price' , fontsize = 12 ) plt . show ( )
210	ulimit = np . percentile ( train_df . price . values , 99 ) train_df [ 'price' ] . ix [ train_df [ 'price' ] > ulimit ] = ulimit plt . figure ( figsize = ( 8 , 6 ) ) sns . distplot ( train_df . price . values , bins = 50 , kde = True ) plt . xlabel ( 'price' , fontsize = 12 ) plt . show ( )
211	ulimit = 180 train_df [ 'y' ] . ix [ train_df [ 'y' ] > ulimit ] = ulimit plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( train_df . y . values , bins = 50 , kde = False ) plt . xlabel ( 'y value' , fontsize = 12 ) plt . show ( )
212	missing_df = train_df . isnull ( ) . sum ( axis = 0 ) . reset_index ( ) missing_df . columns = [ 'column_name' , 'missing_count' ] missing_df = missing_df . ix [ missing_df [ 'missing_count' ] > 0 ] missing_df = missing_df . sort_values ( by = 'missing_count' ) missing_df
213	for thresh in np . arange ( 0.1 , 0.201 , 0.01 ) : thresh = np . round ( thresh , 2 ) print ( "F1 score at threshold {0} is {1}" . format ( thresh , metrics . f1_score ( val_y , ( pred_val_y > thresh ) . astype ( int ) ) ) )
214	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "days_since_prior_order" , data = orders_df , color = color [ 3 ] ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'Days since prior order' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . title ( "Frequency distribution by days since prior order" , fontsize = 15 ) plt . show ( )
215	cnt_srs = order_products_prior_df [ 'aisle' ] . value_counts ( ) . head ( 20 ) plt . figure ( figsize = ( 12 , 8 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 5 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'Aisle' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
216	plt . figure ( figsize = ( 10 , 10 ) ) temp_series = order_products_prior_df [ 'department' ] . value_counts ( ) labels = ( np . array ( temp_series . index ) ) sizes = ( np . array ( ( temp_series / temp_series . sum ( ) ) * 100 ) ) plt . pie ( sizes , labels = labels , autopct = '%1.1f%%' , startangle = 200 ) plt . title ( "Departments distribution" , fontsize = 15 ) plt . show ( )
217	from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
218	print ( f"The total number of games in the training data is {train_df['GameId'].nunique()}" ) print ( f"The total number of plays in the training data is {train_df['PlayId'].nunique()}" ) print ( f"The NFL seasons in the training data are {train_df['Season'].unique().tolist()}" )
219	plt . figure ( figsize = ( 16 , 12 ) ) temp_df = train_df . query ( "NflIdRusher == NflId" ) sns . catplot ( data = temp_df , x = "Quarter" , y = "Yards" , kind = "boxen" ) plt . xlabel ( 'Quarter' , fontsize = 12 ) plt . ylabel ( 'Yards (Target)' , fontsize = 12 ) plt . title ( "Quarter Vs Yards (target)" , fontsize = 20 ) plt . show ( )
220	import random
221	train_df = pd . read_csv ( "../input/train.csv" , parse_dates = [ 'timestamp' ] ) dtype_df = train_df . dtypes . reset_index ( ) dtype_df . columns = [ "Count" , "Column Type" ] dtype_df . groupby ( "Column Type" ) . aggregate ( 'count' ) . reset_index ( )
222	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "floor" , data = train_df ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'floor number' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
223	grouped_df = train_df . groupby ( 'floor' ) [ 'price_doc' ] . aggregate ( np . median ) . reset_index ( ) plt . figure ( figsize = ( 12 , 8 ) ) sns . pointplot ( grouped_df . floor . values , grouped_df . price_doc . values , alpha = 0.8 , color = color [ 2 ] ) plt . ylabel ( 'Median Price' , fontsize = 12 ) plt . xlabel ( 'Floor number' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
224	train_df [ 'transaction_month' ] = train_df [ 'transactiondate' ] . dt . month cnt_srs = train_df [ 'transaction_month' ] . value_counts ( ) plt . figure ( figsize = ( 12 , 6 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 3 ] ) plt . xticks ( rotation = 'vertical' ) plt . xlabel ( 'Month of transaction' , fontsize = 12 ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . show ( )
225	plt . figure ( figsize = ( 12 , 12 ) ) sns . jointplot ( x = prop_df . latitude . values , y = prop_df . longitude . values , size = 10 ) plt . ylabel ( 'Longitude' , fontsize = 12 ) plt . xlabel ( 'Latitude' , fontsize = 12 ) plt . show ( )
226	pd . options . display . max_rows = 65 dtype_df = train_df . dtypes . reset_index ( ) dtype_df . columns = [ "Count" , "Column Type" ] dtype_df
227	missing_df = train_df . isnull ( ) . sum ( axis = 0 ) . reset_index ( ) missing_df . columns = [ 'column_name' , 'missing_count' ] missing_df [ 'missing_ratio' ] = missing_df [ 'missing_count' ] / train_df . shape [ 0 ] missing_df . ix [ missing_df [ 'missing_ratio' ] > 0.999 ]
228	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "bathroomcnt" , data = train_df ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'Bathroom' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . title ( "Frequency of Bathroom count" , fontsize = 15 ) plt . show ( )
229	train_df [ 'bedroomcnt' ] . ix [ train_df [ 'bedroomcnt' ] > 7 ] = 7 plt . figure ( figsize = ( 12 , 8 ) ) sns . violinplot ( x = 'bedroomcnt' , y = 'logerror' , data = train_df ) plt . xlabel ( 'Bedroom count' , fontsize = 12 ) plt . ylabel ( 'Log Error' , fontsize = 12 ) plt . show ( )
230	from ggplot import * ggplot ( aes ( x = 'yearbuilt' , y = 'logerror' ) , data = train_df ) + \ geom_point ( color = 'steelblue' , size = 1 ) + \ stat_smooth ( )
231	ggplot ( aes ( x = 'latitude' , y = 'longitude' , color = 'logerror' ) , data = train_df ) + \ geom_point ( ) + \ scale_color_gradient ( low = 'red' , high = 'blue' )
232	train_df [ 'num_punctuations' ] . loc [ train_df [ 'num_punctuations' ] > 10 ] = 10 plt . figure ( figsize = ( 12 , 8 ) ) sns . violinplot ( x = 'author' , y = 'num_punctuations' , data = train_df ) plt . xlabel ( 'Author Name' , fontsize = 12 ) plt . ylabel ( 'Number of puntuations in text' , fontsize = 12 ) plt . title ( "Number of punctuations by author" , fontsize = 15 ) plt . show ( )
233	tfidf_vec = CountVectorizer ( stop_words = 'english' , ngram_range = ( 1 , 3 ) ) tfidf_vec . fit ( train_df [ 'text' ] . values . tolist ( ) + test_df [ 'text' ] . values . tolist ( ) ) train_tfidf = tfidf_vec . transform ( train_df [ 'text' ] . values . tolist ( ) ) test_tfidf = tfidf_vec . transform ( test_df [ 'text' ] . values . tolist ( ) )
234	cnf_matrix = confusion_matrix ( val_y , np . argmax ( pred_val_y , axis = 1 ) ) np . set_printoptions ( precision = 2 ) plt . figure ( figsize = ( 8 , 8 ) ) plot_confusion_matrix ( cnf_matrix , classes = [ 'EAP' , 'HPL' , 'MWS' ] , title = 'Confusion matrix of XGB, without normalization' ) plt . show ( )
235	env = kagglegym . make ( ) observation = env . reset ( ) train = observation . train
236	is_dup = train_df [ 'is_duplicate' ] . value_counts ( ) plt . figure ( figsize = ( 8 , 4 ) ) sns . barplot ( is_dup . index , is_dup . values , alpha = 0.8 , color = color [ 1 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'Is Duplicate' , fontsize = 12 ) plt . show ( )
237	plt . figure ( figsize = ( 12 , 8 ) ) grouped_df = train_df . groupby ( 'q1_freq' ) [ 'is_duplicate' ] . aggregate ( np . mean ) . reset_index ( ) sns . barplot ( grouped_df [ "q1_freq" ] . values , grouped_df [ "is_duplicate" ] . values , alpha = 0.8 , color = color [ 4 ] ) plt . ylabel ( 'Mean is_duplicate' , fontsize = 12 ) plt . xlabel ( 'Q1 frequency' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
238	cols_to_use = [ 'q1_q2_intersect' , 'q1_freq' , 'q2_freq' ] temp_df = train_df [ cols_to_use ] corrmat = temp_df . corr ( method = 'spearman' ) f , ax = plt . subplots ( figsize = ( 8 , 8 ) ) sns . heatmap ( corrmat , vmax = 1. , square = True ) plt . title ( "Leaky variables correlation map" , fontsize = 15 ) plt . show ( )
239	data_path = "../input/" train_file = data_path + "train.json" test_file = data_path + "test.json" train_df = pd . read_json ( train_file ) test_df = pd . read_json ( test_file ) print ( train_df . shape ) print ( test_df . shape )
240	path = '../input/' train = pd . read_csv ( path + 'train.csv' ) test = pd . read_csv ( path + 'test.csv' ) print ( 'Number of rows and columns in the train data set:' , train . shape ) print ( 'Number of rows and columns in the test data set:' , test . shape )
241	vect_word = TfidfVectorizer ( max_features = 20000 , lowercase = True , analyzer = 'word' , stop_words = 'english' , ngram_range = ( 1 , 3 ) , dtype = np . float32 ) vect_char = TfidfVectorizer ( max_features = 40000 , lowercase = True , analyzer = 'char' , stop_words = 'english' , ngram_range = ( 3 , 6 ) , dtype = np . float32 )
242	col = 'identity_hate' print ( "Column:" , col ) pred = lr . predict ( X ) print ( '\nConfusion matrix\n' , confusion_matrix ( y [ col ] , pred ) ) print ( classification_report ( y [ col ] , pred ) )
243	path = '../input/' train = pd . read_csv ( path + 'train.csv' ) test = pd . read_csv ( path + 'test.csv' ) print ( 'Number of rows and columns in train data set:' , train . shape ) print ( 'Number of rows and columns in test data set:' , test . shape )
244	fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 4 ) ) ax1 , ax2 = ax . flatten ( ) sns . distplot ( train [ 'formation_energy_ev_natom' ] , bins = 50 , ax = ax1 , color = 'b' ) sns . distplot ( train [ 'bandgap_energy_ev' ] , bins = 50 , ax = ax2 , color = 'r' )
245	cor = train . corr ( ) plt . figure ( figsize = ( 12 , 8 ) ) sns . heatmap ( cor , cmap = 'Set1' , annot = True )
246	print ( 'Original text:\n' , train [ 'text' ] [ 0 ] ) review = re . sub ( '[^A-Za-z0-9]' , " " , train [ 'text' ] [ 0 ] ) print ( '\nAfter removal of punctuation:\n' , review )
247	cv = CountVectorizer ( max_features = 2000 , ngram_range = ( 1 , 3 ) , dtype = np . int8 , stop_words = 'english' ) X_cv = cv . fit_transform ( train [ 'clean_text' ] ) . toarray ( ) X_test_cv = cv . fit_transform ( test [ 'clean_text' ] ) . toarray ( )
248	y_pred = pred_test_full / 10 submit = pd . DataFrame ( test [ 'id' ] ) submit = submit . join ( pd . DataFrame ( y_pred ) ) submit . columns = [ 'id' , 'EAP' , 'HPL' , 'MWS' ] submit . to_csv ( 'spooky_pred1.csv' , index = False )
249	y_pred = pred_test_full / 10 submit = pd . DataFrame ( test [ 'id' ] ) submit = submit . join ( pd . DataFrame ( y_pred ) ) submit . columns = [ 'id' , 'EAP' , 'HPL' , 'MWS' ] submit . to_csv ( 'spooky_pred2.csv' , index = False )
250	y_pred = pred_test_full / 2 submit = pd . DataFrame ( test [ 'id' ] ) submit = submit . join ( pd . DataFrame ( y_pred ) ) submit . columns = [ 'id' , 'EAP' , 'HPL' , 'MWS' ] submit . to_csv ( 'spooky_pred3.csv' , index = False )
251	y = X_train . dropna ( 0 ) . as_matrix ( ) [ 0 ] y = [ None if i >= np . percentile ( y , 95 ) or i <= np . percentile ( y , 5 ) else i for i in y ] df_na = pd . DataFrame ( { 'ds' : X_train . T . index . values , 'y' : y } )
252	path = '../input/' train = pd . read_csv ( path + 'train.csv' , na_values = - 1 ) test = pd . read_csv ( path + 'test.csv' , na_values = - 1 ) print ( 'Number rows and columns:' , train . shape ) print ( 'Number rows and columns:' , test . shape )
253	k = pd . DataFrame ( ) k [ 'train' ] = train . isnull ( ) . sum ( ) k [ 'test' ] = test . isnull ( ) . sum ( ) fig , ax = plt . subplots ( figsize = ( 16 , 5 ) ) k . plot ( kind = 'bar' , ax = ax )
254	def missing_value ( df ) : col = df . columns for i in col : if df [ i ] . isnull ( ) . sum ( ) > 0 : df [ i ] . fillna ( df [ i ] . mode ( ) [ 0 ] , inplace = True )
255	def basic_details ( df ) : b = pd . DataFrame ( ) b [ 'Missing value' ] = df . isnull ( ) . sum ( ) b [ 'N unique value' ] = df . nunique ( ) b [ 'dtype' ] = df . dtypes return b basic_details ( train )
256	X = train1 . drop ( [ 'target' , 'id' ] , axis = 1 ) y = train1 [ 'target' ] . astype ( 'category' ) x_test = test1 . drop ( [ 'target' , 'id' ] , axis = 1 ) del train1 , test1
257	y_pred = pred_test_full / 5 submit = pd . DataFrame ( { 'id' : test [ 'id' ] , 'target' : y_pred } ) submit . to_csv ( 'lr_porto.csv' , index = False )
258	path = '../input/' train = pd . read_csv ( path + 'train.csv' , na_values = - 1 ) test = pd . read_csv ( path + 'test.csv' , na_values = - 1 ) print ( 'Number rows and columns:' , train . shape ) print ( 'Number rows and columns:' , test . shape )
259	k = pd . DataFrame ( ) k [ 'train' ] = train . isnull ( ) . sum ( ) k [ 'test' ] = test . isnull ( ) . sum ( ) k
260	X = train . drop ( [ 'target' , 'id' ] , axis = 1 ) y = train [ 'target' ] . astype ( 'category' ) x_test = test . drop ( 'id' , axis = 1 ) test_id = test [ 'id' ]
261	y_pred = pred_xgb submit = pd . DataFrame ( { 'id' : test_id , 'target' : y_pred } ) submit . to_csv ( 'xgb_porto.csv' , index = False )
262	X = train . drop ( [ 'id' , 'target' ] , axis = 1 ) . values y = train . target . values test_id = test . id . values test = test . drop ( 'id' , axis = 1 )
263	sub = pd . DataFrame ( ) sub [ 'id' ] = test_id sub [ 'target' ] = np . zeros_like ( test_id )
264	if isfile ( P2SIZE ) : print ( "P2SIZE exists." ) with open ( P2SIZE , 'rb' ) as f : p2size = pickle . load ( f ) else : p2size = { } for p in tqdm ( join ) : size = pil_image . open ( expand_path ( p ) ) . size p2size [ p ] = size
265	preds_v , y_v = learn . TTA ( is_test = False , n_aug = 2 ) preds_v = np . stack ( preds_v , axis = - 1 ) preds_v = np . exp ( preds_v ) preds_v = preds_v . mean ( axis = - 1 ) y_v += 1
266	from sklearn import preprocessing df = df . drop ( [ "Name" , "Ticket" ] , axis = 1 ) categorical = [ "Sex" , "Embarked" , "Ticket_code" , "Ticket_code_HEAD" , "Ticket_code_TAIL" , "Initial" ] lbl = preprocessing . LabelEncoder ( ) for col in categorical : df [ col ] . fillna ( 'Unknown' ) df [ col ] = lbl . fit_transform ( df [ col ] . astype ( str ) ) df . head ( )
267	train_identity = pd . read_csv ( '../input/ieee-fraud-detection/train_identity.csv' ) train_transaction = pd . read_csv ( '../input/ieee-fraud-detection/train_transaction.csv' ) test_identity = pd . read_csv ( '../input/ieee-fraud-detection/test_identity.csv' ) test_transaction = pd . read_csv ( '../input/ieee-fraud-detection/test_transaction.csv' ) sub = pd . read_csv ( '../input/ieee-fraud-detection/sample_submission.csv' ) train = pd . merge ( train_transaction , train_identity , on = 'TransactionID' , how = 'left' ) test = pd . merge ( test_transaction , test_identity , on = 'TransactionID' , how = 'left' )
268	from sklearn . utils import resample not_fraud = train [ train . isFraud == 0 ] fraud = train [ train . isFraud == 1 ] not_fraud_downsampled = resample ( not_fraud , replace = False , n_samples = 400000 , random_state = 27 ) downsampled = pd . concat ( [ not_fraud_downsampled , fraud ] ) downsampled . isFraud . value_counts ( )
269	start_index = 1730 end_index = 1826 train_df [ 'forecast' ] = sarima_mod6 . predict ( start = start_index , end = end_index , dynamic = True ) train_df [ start_index : end_index ] [ [ 'sales' , 'forecast' ] ] . plot ( figsize = ( 12 , 8 ) )
270	times_series_df . plot ( figsize = ( 20 , 10 ) , title = "The Cumulative total of Confirmed cases" ) plt . legend ( loc = 2 , prop = { 'size' : 20 } ) plt . show ( )
271	from tensorflow . python . keras import optimizers sgd = optimizers . SGD ( lr = 0.01 , decay = 1e-6 , momentum = 0.9 , nesterov = True ) model . compile ( optimizer = sgd , loss = OBJECTIVE_FUNCTION , metrics = LOSS_METRICS )
272	from keras . applications . resnet50 import preprocess_input from keras . preprocessing . image import ImageDataGenerator image_size = IMAGE_RESIZE data_generator = ImageDataGenerator ( preprocessing_function = preprocess_input ) train_generator = data_generator . flow_from_directory ( '../input/catsdogs-trainvalid-80pc-prepd/trainvalidfull4keras/trainvalidfull4keras/train' , target_size = ( image_size , image_size ) , batch_size = BATCH_SIZE_TRAINING , class_mode = 'categorical' ) validation_generator = data_generator . flow_from_directory ( '../input/catsdogs-trainvalid-80pc-prepd/trainvalidfull4keras/trainvalidfull4keras/valid' , target_size = ( image_size , image_size ) , batch_size = BATCH_SIZE_VALIDATION , class_mode = 'categorical' )
273	test_generator . reset ( ) pred = model . predict_generator ( test_generator , steps = len ( test_generator ) , verbose = 1 ) predicted_class_indices = np . argmax ( pred , axis = 1 )
274	def latex_tag_in_text ( text ) : x = text . lower ( ) return ' [ math ] ' in x train [ 'latex_tag_in_text' ] = train [ 'question_text' ] . apply ( lambda x : latex_tag_in_text ( x ) )
275	EMBED_SIZE = 300 MAX_WORDS_LEN = 70 MAX_VOCAB_FEATURES = 200000
276	def clean_latex_tag ( text ) : corr_t = [ ] for t in text . split ( " " ) : t = t . strip ( ) if t != '' : corr_t . append ( t ) text = ' ' . join ( corr_t ) text = re . sub ( '(\[ math \]).+(\[ / math \])' , 'mathematical formula' , text ) return text
277	train_df = pd . read_csv ( "../input/train.csv" , encoding = 'utf8' ) test_df = pd . read_csv ( "../input/test.csv" , encoding = 'utf8' ) all_test_texts = '' . join ( test_df . question_text . values . tolist ( ) ) print ( 'Train:' , train_df . shape ) print ( 'Test:' , test_df . shape )
278	model = models . resnet18 ( pretrained = True ) fc_in_features = model . fc . in_features model . fc = nn . Linear ( fc_in_features , 2 ) model = model . to ( device ) loss_fn = nn . CrossEntropyLoss ( ) optimizer = optim . SGD ( model . parameters ( ) , lr = 0.001 , momentum = 0.9 )
279	test_pids = test_df . PetID . values input_tensor = torch . zeros ( 1 , 3 , 224 , 224 ) test_image_features = { } for petid in tqdm ( test_pids ) : test_img = f"../input/test_images/{petid}-1.jpg" if not os . path . exists ( test_img ) : continue test_img = Image . open ( test_img ) test_img = extract_transform ( test_img ) input_tensor [ 0 , : , : , : ] = test_img input_tensor = input_tensor . cuda ( ) model ( input_tensor ) test_image_features [ petid ] = image_features [ 0 ] image_features . clear ( )
280	plt . hist ( train_transaction [ 'TransactionDT' ] , label = 'train' ) plt . hist ( test_transaction [ 'TransactionDT' ] , label = 'test' ) plt . legend ( ) plt . title ( 'Distribution of TransactionDT' )
281	protonmail_fraud = len ( train_full [ ( train_full [ 'P_parent_emaildomain' ] == "protonmail" ) & ( train_full [ 'isFraud' ] == 1 ) ] ) protonmail_non_fraud = len ( train_full [ ( train_full [ 'P_parent_emaildomain' ] == "protonmail" ) & ( train_full [ 'isFraud' ] == 0 ) ] ) protonmail_fraud_rate = protonmail_fraud / ( protonmail_fraud + protonmail_non_fraud ) print ( "Number of protonmail fraud transactions:" , protonmail_fraud ) print ( "Number of protonmail non-fraud transactions:" , protonmail_non_fraud ) print ( "Protonmail fraud rate:" , protonmail_fraud_rate )
282	train_full [ 'major_os' ] = train_full [ "id_30" ] . str . split ( ' ' , expand = True ) [ [ 0 ] ] visualize_cat_cariable ( 'major_os' )
283	train_features = train_df . drop ( [ 'target' , 'ID_code' ] , axis = 1 ) test_features = test_df . drop ( [ 'ID_code' ] , axis = 1 ) train_target = train_df [ 'target' ]
284	my_submission_nn = pd . DataFrame ( { "ID_code" : id_code_test , "target" : test_preds } ) my_submission_lbgm = pd . DataFrame ( { "ID_code" : id_code_test , "target" : predictions } ) my_submission_esemble = pd . DataFrame ( { "ID_code" : id_code_test , "target" : esemble_pred } )
285	data_dir = '/kaggle/input/prostate-cancer-grade-assessment/train_images' mask_dir = '/kaggle/input/prostate-cancer-grade-assessment/train_label_masks' train_labels = pd . read_csv ( '/kaggle/input/prostate-cancer-grade-assessment/train.csv' )
286	import os import gc print ( os . listdir ( "../input" ) ) import numpy as np import pandas as pd import time
287	train [ 'exist_ship' ] = train [ 'EncodedPixels' ] . fillna ( 0 ) train . loc [ train [ 'exist_ship' ] != 0 , 'exist_ship' ] = 1 del train [ 'EncodedPixels' ]
288	print ( len ( train [ 'ImageId' ] ) ) print ( train [ 'ImageId' ] . value_counts ( ) . shape [ 0 ] ) train_gp = train . groupby ( 'ImageId' ) . sum ( ) . reset_index ( ) train_gp . loc [ train_gp [ 'exist_ship' ] > 0 , 'exist_ship' ] = 1
289	print ( train_gp [ 'exist_ship' ] . value_counts ( ) ) train_gp = train_gp . sort_values ( by = 'exist_ship' ) train_gp = train_gp . drop ( train_gp . index [ 0 : 100000 ] )
290	print ( train_gp [ 'exist_ship' ] . value_counts ( ) ) train_sample = train_gp . sample ( 5000 ) print ( train_sample [ 'exist_ship' ] . value_counts ( ) ) print ( train_sample . shape )
291	from sklearn . preprocessing import OneHotEncoder targets = data_target . reshape ( len ( data_target ) , - 1 ) enc = OneHotEncoder ( ) enc . fit ( targets ) targets = enc . transform ( targets ) . toarray ( ) print ( targets . shape )
292	from sklearn . model_selection import train_test_split x_train , x_val , y_train , y_val = train_test_split ( data , targets , test_size = 0.2 ) x_train . shape , x_val . shape , y_train . shape , y_val . shape
293	from keras import optimizers epochs = 10 lrate = 0.001 decay = lrate / epochs sgd = optimizers . SGD ( lr = lrate , momentum = 0.9 , decay = decay , nesterov = False ) model_final . compile ( loss = 'categorical_crossentropy' , optimizer = sgd , metrics = [ 'accuracy' ] ) model_final . summary ( )
294	from sklearn . preprocessing import OneHotEncoder targets_predict = data_target_predict . reshape ( len ( data_target_predict ) , - 1 ) enc = OneHotEncoder ( ) enc . fit ( targets_predict ) targets_predict = enc . transform ( targets_predict ) . toarray ( ) print ( targets_predict . shape )
295	drop_cols = [ "bin_0" ] ddall [ "ord_5a" ] = ddall [ "ord_5" ] . str [ 0 ] ddall [ "ord_5b" ] = ddall [ "ord_5" ] . str [ 1 ] drop_cols . append ( "ord_5" )
296	for col in [ "nom_5" , "nom_6" , "nom_7" , "nom_8" , "nom_9" ] : train_vals = set ( dd0 [ col ] . unique ( ) ) test_vals = set ( ddtest0 [ col ] . unique ( ) ) xor_cat_vals = train_vals ^ test_vals if xor_cat_vals : ddall . loc [ ddall [ col ] . isin ( xor_cat_vals ) , col ] = "xor"
297	ohc = scipy . sparse . hstack ( [ ohc1 ] + thermos ) . tocsr ( ) display ( ohc ) X_train = ohc [ : num_train ] X_test = ohc [ num_train : ] y_train = dd0 [ "target" ] . values
298	drop_cols = [ "bin_0" ] ddall [ "ord_5a" ] = ddall [ "ord_5" ] . str [ 0 ] ddall [ "ord_5b" ] = ddall [ "ord_5" ] . str [ 1 ] drop_cols . append ( "ord_5" )
299	for col in [ "nom_5" , "nom_6" , "nom_7" , "nom_8" , "nom_9" ] : train_vals = set ( dd0 [ col ] . unique ( ) ) test_vals = set ( ddtest0 [ col ] . unique ( ) ) xor_cat_vals = train_vals ^ test_vals if xor_cat_vals : ddall . loc [ ddall [ col ] . isin ( xor_cat_vals ) , col ] = "xor"
300	ohc = scipy . sparse . hstack ( [ ohc1 ] + thermos ) . tocsr ( ) display ( ohc ) X_train = ohc [ : num_train ] X_test = ohc [ num_train : ] y_train = dd0 [ "target" ] . values
301	import numpy as np from sklearn . metrics import log_loss from sklearn . base import BaseEstimator from scipy . optimize import minimize
302	n_classes = 12 data , labels = make_classification ( n_samples = 2000 , n_features = 100 , n_informative = 50 , n_classes = n_classes , random_state = random_state ) X , X_test , y , y_test = train_test_split ( data , labels , test_size = 0.2 , random_state = random_state ) X_train , X_valid , y_train , y_valid = train_test_split ( X , y , test_size = 0.25 , random_state = random_state ) print ( 'Data shape:' ) print ( 'X_train: %s, X_valid: %s, X_test: %s \n' % ( X_train . shape , X_valid . shape , X_test . shape ) )
303	lr = LogisticRegressionCV ( Cs = 10 , dual = False , fit_intercept = True , intercept_scaling = 1.0 , max_iter = 100 , multi_class = 'ovr' , n_jobs = 1 , penalty = 'l2' , random_state = random_state , solver = 'lbfgs' , tol = 0.0001 ) lr . fit ( XV , y_valid ) y_lr = lr . predict_proba ( XT ) print ( '{:20s} {:2s} {:1.7f}' . format ( 'Log_Reg:' , 'logloss =>' , log_loss ( y_test , y_lr ) ) )
304	print ( len ( memory ) ) augmentData ( memory ) print ( len ( memory ) )
305	labels = 'Train' , 'Test' sizes = [ len ( train_fns ) , len ( test_fns ) ] explode = ( 0 , 0.1 ) fig , ax = plt . subplots ( figsize = ( 6 , 6 ) ) ax . pie ( sizes , explode = explode , labels = labels , autopct = '%1.1f%%' , shadow = True , startangle = 90 ) ax . axis ( 'equal' ) ax . set_title ( 'Train and Test Sets' ) plt . show ( )
306	print ( 'Total number of images: %s' % len ( train_df [ 'Image' ] . unique ( ) ) ) print ( 'Images with at least one label: %s' % len ( train_df [ train_df [ 'EncodedPixels' ] != 'NaN' ] [ 'Image' ] . unique ( ) ) )
307	from tensorflow . keras . applications . inception_resnet_v2 import InceptionResNetV2 def get_model ( ) : base_model = model = ResNeXt101 ( ... , backend = tf . keras . backend , layers = tf . keras . layers , weights = 'imagenet' , models = tf . keras . models , utils = tf . keras . utils ) x = base_model . output y_pred = Dense ( 4 , activation = 'sigmoid' ) ( x ) return Model ( inputs = base_model . input , outputs = y_pred ) model = get_model ( )
308	def plot_with_dots ( ax , np_array ) : ax . scatter ( list ( range ( 1 , len ( np_array ) + 1 ) ) , np_array , s = 50 ) ax . plot ( list ( range ( 1 , len ( np_array ) + 1 ) ) , np_array )
309	store [ 'Promo2SinceWeek' ] . fillna ( 0 , inplace = True ) store [ 'Promo2SinceYear' ] . fillna ( store [ 'Promo2SinceYear' ] . mode ( ) [ 0 ] , inplace = True ) store [ 'PromoInterval' ] . fillna ( store [ 'PromoInterval' ] . mode ( ) [ 0 ] , inplace = True )
310	size = 1024 for img in hair_images : image = cv2 . imread ( BASE_PATH + '/jpeg/train/' + img + '.jpg' ) image_resize = cv2 . resize ( image , ( size , size ) ) image_resize = cv2 . cvtColor ( image_resize , cv2 . COLOR_BGR2RGB ) plt . imshow ( image_resize ) plt . show ( )
311	rate = train [ "species" ] . value_counts ( ) . sort_values ( ) / 264 print ( f'{"Target" :-<40} {"rate":-<20}' ) for n in range ( len ( rate ) ) : print ( f'{rate.index[n] :-<40} {rate[n]}' )
312	longitude = pd . to_numeric ( train [ 'longitude' ] , errors = 'coerce' ) latitude = pd . to_numeric ( train [ 'latitude' ] , errors = 'coerce' ) df = pd . concat ( [ longitude , latitude ] , axis = 1 )
313	N = 5 ebird_code_simple = sample ( list ( train [ "ebird_code" ] . unique ( ) ) , N ) AudioProcessing ( ) . PlotSampleWave ( nrows = N , captions = ebird_code_simple , df = train )
314	categorical_list = [ ] numerical_list = [ ] for i in application . columns . tolist ( ) : if application [ i ] . dtype == 'object' : categorical_list . append ( i ) else : numerical_list . append ( i ) print ( 'Number of categorical features:' , str ( len ( categorical_list ) ) ) print ( 'Number of numerical features:' , str ( len ( numerical_list ) ) )
315	X = application . drop ( [ 'SK_ID_CURR' , 'TARGET' ] , axis = 1 ) y = application . TARGET feature_name = X . columns . tolist ( )
316	PATH = '/kaggle/input/covid19-global-forecasting-week-4/' train_df = pd . read_csv ( PATH + 'train.csv' , parse_dates = [ 'Date' ] ) test_df = pd . read_csv ( PATH + 'test.csv' , parse_dates = [ 'Date' ] ) add_datepart ( train_df , 'Date' , drop = False ) add_datepart ( test_df , 'Date' , drop = False )
317	df1 = df . copy ( ) df1 [ 'ConfirmedCases' ] = np . log ( df1 [ 'ConfirmedCases' ] ) df1 [ 'Fatalities' ] = np . log ( df1 [ 'Fatalities' ] + 1 )
318	to_tst = to . new ( test_df ) to_tst . process ( ) to_tst . all_cols . head ( )
319	import pandas as pd pd . set_option ( 'display.max_columns' , None ) import numpy as np import matplotlib . pyplot as plt import seaborn as sns color = sns . color_palette ( ) import gc import warnings import time warnings . filterwarnings ( "ignore" )
320	from sklearn . impute import SimpleImputer , MICEImputer application_train = pd . read_csv ( '../input/application_train.csv' ) application_test = pd . read_csv ( '../input/application_test.csv' )
321	import os import time import gc import warnings warnings . filterwarnings ( "ignore" ) import json from pandas . io . json import json_normalize import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns color = sns . color_palette ( ) from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import KFold from sklearn . metrics import mean_squared_error import lightgbm as lgb
322	def find_missing ( data ) : count_missing = data . isnull ( ) . sum ( ) . values total = data . shape [ 0 ] ratio_missing = count_missing / total return pd . DataFrame ( data = { 'missing_count' : count_missing , 'missing_ratio' : ratio_missing } , index = data . columns . values ) train_missing = find_missing ( train ) test_missing = find_missing ( test )
323	if test . fullVisitorId . nunique ( ) == len ( sub ) : print ( 'Till now, the number of fullVisitorId is equal to the rows in submission. Everything goes well!' ) else : print ( 'Check it again' )
324	data_path = Path ( '/kaggle/input/abstraction-and-reasoning-challenge/' ) training_path = data_path / 'training' evaluation_path = data_path / 'evaluation' test_path = data_path / 'test' training_tasks = sorted ( os . listdir ( training_path ) ) eval_tasks = sorted ( os . listdir ( evaluation_path ) )
325	evaluation_examples = [ ] for i in range ( 400 ) : task = Evals [ i ] basic_task = Create ( task , 0 ) a = Function ( basic_task ) if a != - 1 and task [ 'test' ] [ 0 ] [ 'output' ] == a : plot_picture ( a ) plot_task ( task ) print ( i ) evaluation_examples . append ( i )
326	import numpy as np mask = torch . stack ( [ mask , mask , mask ] , dim = 2 ) mask = mask . cpu ( ) . numpy ( ) . astype ( "uint8" ) instances = cv2 . multiply ( image , mask ) plt . imshow ( instances ) plt . show ( )
327	import os import cv2 import pdb import glob import argparse import numpy as np
328	import nltk import re from nltk . corpus import stopwords from nltk . stem . porter import PorterStemmer
329	voc_size = 5000 onehot_repr = [ one_hot ( words , voc_size ) for words in corpus ] onehot_repr
330	from sklearn . model_selection import TimeSeriesSplit cv = TimeSeriesSplit ( n_splits = 5 )
331	oof_qda1 = oof_qda1 . reshape ( - 1 , 1 ) oof_qda2 = oof_qda2 . reshape ( - 1 , 1 ) oof_gmm = oof_gmm . reshape ( - 1 , 1 ) oof_lr = oof_lr . reshape ( - 1 , 1 ) oof_ls = oof_ls . reshape ( - 1 , 1 ) oof_knn = oof_knn . reshape ( - 1 , 1 ) oof_nn = oof_nn . reshape ( - 1 , 1 ) oof_qda3 = oof_qda3 . reshape ( - 1 , 1 )
332	df = pd . concat ( [ train [ [ 'id' , 'comment_text' ] ] , test ] , axis = 0 ) del ( train , test ) gc . collect ( )
333	MAX_NUM_WORDS = 100000 TOXICITY_COLUMN = 'target' TEXT_COLUMN = 'comment_text' tokenizer = Tokenizer ( num_words = MAX_NUM_WORDS ) tokenizer . fit_on_texts ( train_df [ TEXT_COLUMN ] ) MAX_SEQUENCE_LENGTH = 256 def pad_text ( texts , tokenizer ) : return pad_sequences ( tokenizer . texts_to_sequences ( texts ) , maxlen = MAX_SEQUENCE_LENGTH )
334	data = pd . concat ( ( train , test ) ) np . random . seed ( 42 ) data = data . iloc [ np . random . permutation ( len ( data ) ) ] data . reset_index ( drop = True , inplace = True ) x = data . drop ( [ 'target' , 'ID_code' , 'train_test' ] , axis = 1 ) y = data . train_test
335	npt . word_distribution ( title = 'number of words distribution' )
336	sns . set ( ) x = train [ 'revenue' ] y = train [ 'popularity' ] plt . figure ( figsize = ( 15 , 8 ) ) sns . regplot ( x , y ) plt . xlabel ( 'popularity' ) plt . ylabel ( 'revenue' ) plt . title ( 'Relationship between popularity and revenue of a movie' )
337	lgbmodel = lgb . LGBMRegressor ( n_estimators = 10000 , objective = 'regression' , metric = 'rmse' , max_depth = 5 , num_leaves = 30 , min_child_samples = 100 , learning_rate = 0.01 , boosting = 'gbdt' , min_data_in_leaf = 10 , feature_fraction = 0.9 , bagging_freq = 1 , bagging_fraction = 0.9 , importance_type = 'gain' , lambda_l1 = 0.2 , bagging_seed = random_seed , subsample = .8 , colsample_bytree = .9 , use_best_model = True )
338	from sklearn . metrics import mean_squared_error import lightgbm as lgb model = lgb . LGBMRegressor ( ) model . fit ( xtrain , ytrain ) print ( "RMSE of Validation Data using Light GBM: %.2f" % math . sqrt ( mean_squared_error ( yval , model . predict ( xval ) ) ) )
339	summ = pd . DataFrame ( { 'data' : [ 'train.csv' , 'test.csv' , 'sample_submission.csv' ] , 'rows' : [ len ( trainset ) , len ( testset ) , len ( sample_sub ) ] , 'patient' : [ trainset [ 'Patient' ] . nunique ( ) , testset [ 'Patient' ] . nunique ( ) , sample_sub [ 'Patient_Week' ] . nunique ( ) ] } ) summ . set_index ( 'data' , inplace = True ) display ( summ )
340	fig = px . histogram ( trainset , x = 'Age' , color = 'Sex' , marginal = 'box' , histnorm = 'probability density' , opacity = 0.7 ) fig . update_layout ( title = 'Distribution of Age between Male and Female' , width = 800 , height = 500 ) fig . show ( )
341	parti_patient = trainset . drop_duplicates ( subset = 'Patient' ) fig = px . histogram ( parti_patient , x = 'Age' , facet_row = 'SmokingStatus' , facet_col = 'Sex' , ) fig . for_each_annotation ( lambda a : a . update ( text = a . text . replace ( "SmokingStatus=" , "" ) ) ) fig . update_layout ( title = 'Distribution of Age sperated by Sex (col) and Smoking Status (row)' , autosize = True , width = 800 , height = 600 , font_size = 14 ) fig . show ( )
342	fig = px . density_contour ( trainset , x = 'Percent' , y = 'FVC' , marginal_x = "histogram" , marginal_y = "histogram" , color = 'SmokingStatus' , ) fig . update_layout ( title = 'Relationship between Percent and FVC' , width = 800 , height = 400 ) fig . show ( )
343	import os import numpy as np import pandas as pd import lightgbm as lgb from sklearn . model_selection import train_test_split from sklearn . linear_model import LogisticRegression from sklearn . metrics import classification_report
344	plt . rcParams [ "font.size" ] = "12" ax = df . ffill ( ) \ . count ( axis = 1 ) \ . plot ( figsize = ( 20 , 8 ) , title = 'Number of Teams in the Competition by Date' , color = color_pal [ 5 ] , lw = 5 ) ax . set_ylabel ( 'Number of Teams' ) plt . show ( )
345	plt . rcParams [ "font.size" ] = "12" TOP_TEAMS = df . max ( ) . loc [ df . max ( ) > FIFTYTH_SCORE ] . index . values df [ TOP_TEAMS ] . max ( ) . sort_values ( ascending = True ) . plot ( kind = 'barh' , xlim = ( TOP_SCORE - 0.1 , FIFTYTH_SCORE + 0.1 ) , title = 'Top 50 Public LB Teams' , figsize = ( 12 , 15 ) , color = color_pal [ 3 ] ) plt . show ( )
346	plt . rcParams [ "font.size" ] = "12" df [ TOP_TEAMS ] . nunique ( ) . sort_values ( ) . plot ( kind = 'barh' , figsize = ( 12 , 15 ) , color = color_pal [ 1 ] , title = 'Count of Submissions improving LB score by Team' ) plt . show ( )
347	X = df_e . values [ : , 2 : ] . astype ( np . float32 ) Y = df_e . values [ : , 1 ] . astype ( np . float32 ) print ( X . shape )
348	for clzid in range ( len ( clz_attr_num ) ) : if clz_attr_num [ clzid ] > 0 : if not os . path . isfile ( MODEL_FILE_DIR + "attrmodel_%d-%d.model" % ( attr_image_size [ 0 ] , clzid ) ) : model = train_attr_net ( clzid , 32 ) torch . save ( model . state_dict ( ) , MODEL_FILE_DIR + "attrmodel_%d-%d.model" % ( attr_image_size [ 0 ] , clzid ) )
349	class MaskDataset ( object ) : def __init__ ( self , keys ) : self . keys = keys def __getitem__ ( self , idx ) : k = self . keys [ idx ] return ztop ( data_mask [ k ] [ 0 ] ) , ztop ( data_mask [ k ] [ 1 ] ) def __len__ ( self ) : return len ( self . keys )
350	predict_imgeid = [ predict_imgeid [ i ] for i in set ( uses_index ) ] predict_mask = [ predict_mask [ i ] for i in set ( uses_index ) ] predict_rle = [ predict_rle [ i ] for i in set ( uses_index ) ] predict_classid = [ predict_classid [ i ] for i in set ( uses_index ) ] predict_attr = [ predict_attr [ i ] for i in set ( uses_index ) ] predict_attri_str = [ predict_attri_str [ i ] for i in set ( uses_index ) ]
351	def seed_everything ( seed ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True
352	idx = 1 im , cl = learn . data . dl ( DatasetType . Valid ) . dataset [ idx ] cl = int ( cl ) im . show ( title = f"pred. class: {interp.pred_class[idx]}, actual class: {learn.data.classes[cl]}" )
353	import os import random import pandas as pd import numpy as np import glob import matplotlib . pyplot as plt import cv2 import IPython . display as ipd import librosa from albumentations . core . transforms_interface import DualTransform , BasicTransform
354	class PitchShift ( AudioTransform ) : def __init__ ( self , always_apply = False , p = 0.5 , n_steps = None ) : super ( PitchShift , self ) . __init__ ( always_apply , p ) self . n_steps = n_steps def apply ( self , data , ** params ) : return librosa . effects . pitch_shift ( data , sr = 22050 , n_steps = self . n_steps )
355	class AddGaussianNoise ( AudioTransform ) : def __init__ ( self , always_apply = False , p = 0.5 ) : super ( AddGaussianNoise , self ) . __init__ ( always_apply , p ) def apply ( self , data , ** params ) : noise = np . random . randn ( len ( data ) ) data_wn = data + 0.005 * noise return data_wn
356	import albumentations def get_train_transforms ( ) : return albumentations . Compose ( [ TimeShifting ( p = 0.9 ) , albumentations . OneOf ( [ AddCustomNoise ( file_dir = '../input/freesound-audio-tagging/audio_train' , p = 0.8 ) , SpeedTuning ( p = 0.8 ) , ] ) , AddGaussianNoise ( p = 0.8 ) , PitchShift ( p = 0.5 , n_steps = 4 ) , Gain ( p = 0.9 ) , PolarityInversion ( p = 0.9 ) , StretchAudio ( p = 0.1 ) , ] )
357	def roc_auc ( predictions , target ) : fpr , tpr , thresholds = metrics . roc_curve ( target , predictions ) roc_auc = metrics . auc ( fpr , tpr ) return roc_auc
358	with strategy . scope ( ) : model = Sequential ( ) model . add ( Embedding ( len ( word_index ) + 1 , 300 , weights = [ embedding_matrix ] , input_length = max_len , trainable = False ) ) model . add ( Bidirectional ( LSTM ( 300 , dropout = 0.3 , recurrent_dropout = 0.3 ) ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) model . summary ( )
359	import os import tensorflow as tf from tensorflow . keras . layers import Dense , Input from tensorflow . keras . optimizers import Adam from tensorflow . keras . models import Model from tensorflow . keras . callbacks import ModelCheckpoint from kaggle_datasets import KaggleDatasets import transformers from tokenizers import BertWordPieceTokenizer
360	tokenizer = transformers . DistilBertTokenizer . from_pretrained ( 'distilbert-base-multilingual-cased' ) tokenizer . save_pretrained ( '.' ) fast_tokenizer = BertWordPieceTokenizer ( 'vocab.txt' , lowercase = False ) fast_tokenizer
361	l_in = torch . randn ( 10 , device = xm . xla_device ( ) ) linear = torch . nn . Linear ( 10 , 20 ) . to ( xm . xla_device ( ) ) l_out = linear ( l_in ) print ( l_out )
362	class config : MAX_LEN = 224 TRAIN_BATCH_SIZE = 32 VALID_BATCH_SIZE = 8 EPOCHS = 1 MODEL_PATH = "model.bin" TRAINING_FILE = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv' TOKENIZER = transformers . BertTokenizer . from_pretrained ( 'bert-base-uncased' , do_lower_case = True )
363	BATCH_SIZE = 1024 EPOCHS = 150 LR = 0.02 seed = 2020 patience = 50 device = torch . device ( 'cuda' ) FOLDS = 5
364	for ingredient , expected in [ ( 'Eggs' , 'egg' ) , ( 'all-purpose flour' , 'all purpose flour' ) , ( 'purée' , 'puree' ) , ( '1% low-fat milk' , 'low fat milk' ) , ( 'half & half' , 'half half' ) , ( 'safetida (powder)' , 'safetida (powder)' ) ] : actual = preprocess ( [ ingredient ] ) assert actual == expected , f'"{expected}" is excpected but got "{actual}"'
365	merge = gp . merge ( gk , on = [ feature ] , how = 'left' ) sns . lmplot ( x = "mean_download_delay_time" , y = "download_rate" , data = merge ) plt . title ( 'Download-rate vs. Download_delay_time' ) plt . ylim ( 0 , 1 ) plt . xlim ( 0 , 24 )
366	train = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/train.csv' ) test = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/test.csv' ) ss = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/sample_submission.csv' )
367	def clean_text ( text ) : text = str ( text ) . lower ( ) text = re . sub ( '\[.*?\]' , '' , text ) text = re . sub ( 'https?://\S+|www\.\S+' , '' , text ) text = re . sub ( '<.*?>+' , '' , text ) text = re . sub ( '[%s]' % re . escape ( string . punctuation ) , '' , text ) text = re . sub ( '\n' , '' , text ) text = re . sub ( '\w*\d\w*' , '' , text ) return text
368	Positive_sent = train [ train [ 'sentiment' ] == 'positive' ] Negative_sent = train [ train [ 'sentiment' ] == 'negative' ] Neutral_sent = train [ train [ 'sentiment' ] == 'neutral' ]
369	df_train = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/train.csv' ) df_test = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/test.csv' ) df_submission = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/sample_submission.csv' )
370	sentiment = 'positive' train_data = get_training_data ( sentiment ) model_path = get_model_out_path ( sentiment ) train ( train_data , model_path , n_iter = 3 , model = None )
371	train = pd . read_csv ( '../input/train.csv' , index_col = 'plaintext_id' ) test = pd . read_csv ( '../input/test.csv' , index_col = 'ciphertext_id' ) sub = pd . read_csv ( '../input/sample_submission.csv' , index_col = 'ciphertext_id' )
372	plain_dict = { } for p_id , row in train . iterrows ( ) : text = row [ 'text' ] plain_dict [ text ] = p_id print ( len ( plain_dict ) )
373	c_id = 'ID_0414884b0' index = 42677 sub . loc [ c_id ] = index
374	from collections import Counter import matplotlib . pyplot as plt plt . rcParams [ "figure.figsize" ] = ( 20 , 10 )
375	fullcipher3 = " " . join ( ( test3 [ "ciphertext" ] . values ) ) dict_fullcipher3 = Counter ( fullcipher3 . split ( " " ) ) df_fullcipher3 = pd . DataFrame . from_dict ( dict_fullcipher3 , orient = 'index' ) df_fullcipher3 = df_fullcipher3 . reset_index ( ) df_fullcipher3 . columns = [ "num" , "nb" ] df_fullcipher3 . sort_values ( "nb" , ascending = False , inplace = True ) print ( df_fullcipher3 . shape ) df_fullcipher3 . head ( )
376	from xgboost import XGBRegressor report_cv ( XGBRegressor ( random_state = random_seed ) )
377	import os import gc import numpy as np import pandas as pd import seaborn as sns import matplotlib . pyplot as plt import warnings warnings . filterwarnings ( 'ignore' )
378	sns . distplot ( dipole_moments . X , color = 'mediumseagreen' ) plt . title ( 'Dipole moment along X-axis' ) plt . show ( ) sns . distplot ( dipole_moments . Y , color = 'seagreen' ) plt . title ( 'Dipole moment along Y-axis' ) plt . show ( ) sns . distplot ( dipole_moments . Z , color = 'green' ) plt . title ( 'Dipole moment along Z-axis' ) plt . show ( )
379	plt . figure ( figsize = ( 26 , 24 ) ) for i , col in enumerate ( typelist ) : plt . subplot ( 4 , 2 , i + 1 ) sns . distplot ( potential_energy [ train [ 'type' ] == col ] [ 'potential_energy' ] , color = 'orangered' ) plt . title ( col )
380	def is_outlier ( points , thresh = 3.5 ) : if len ( points . shape ) == 1 : points = points [ : , None ] median = np . median ( points , axis = 0 ) diff = np . sum ( ( points - median ) ** 2 , axis = - 1 ) diff = np . sqrt ( diff ) med_abs_deviation = np . median ( diff ) modified_z_score = 0.6745 * diff / med_abs_deviation return modified_z_score > thresh
381	import os import gc import cv2 import json import time import numpy as np import pandas as pd from pathlib import Path from keras . utils import to_categorical import seaborn as sns import plotly . express as px from matplotlib import colors import matplotlib . pyplot as plt import plotly . figure_factory as ff import torch T = torch . Tensor import torch . nn as nn from torch . optim import Adam from torch . utils . data import Dataset , DataLoader
382	test_task_files = sorted ( os . listdir ( TEST_PATH ) ) test_tasks = [ ] for task_file in test_task_files : with open ( str ( TEST_PATH / task_file ) , 'r' ) as f : task = json . load ( f ) test_tasks . append ( task )
383	Xs_test , Xs_train , ys_train = [ ] , [ ] , [ ] for task in test_tasks : X_test , X_train , y_train = [ ] , [ ] , [ ] for pair in task [ "test" ] : X_test . append ( pair [ "input" ] ) for pair in task [ "train" ] : X_train . append ( pair [ "input" ] ) y_train . append ( pair [ "output" ] ) Xs_test . append ( X_test ) Xs_train . append ( X_train ) ys_train . append ( y_train )
384	means = [ np . mean ( X ) for X in matrices ] fig = ff . create_distplot ( [ means ] , group_labels = [ "Means" ] , colors = [ "green" ] ) fig . update_layout ( title_text = "Distribution of matrix mean values" )
385	def flattener ( pred ) : str_pred = str ( [ row for row in pred ] ) str_pred = str_pred . replace ( ', ' , '' ) str_pred = str_pred . replace ( '[[' , '|' ) str_pred = str_pred . replace ( '][' , '|' ) str_pred = str_pred . replace ( ']]' , '|' ) return str_pred
386	test_predictions = [ [ list ( pred ) for pred in test_pred ] for test_pred in test_predictions ] for idx , pred in enumerate ( test_predictions ) : test_predictions [ idx ] = flattener ( pred ) submission = pd . read_csv ( SUBMISSION_PATH ) submission [ "output" ] = test_predictions
387	import os import gc import numpy as np import pandas as pd from tqdm import tqdm_notebook as tqdm import seaborn as sns from collections import Counter import matplotlib . pyplot as plt from IPython . display import SVG import warnings warnings . filterwarnings ( 'ignore' ) import lightgbm import xgboost import catboost import keras from keras . models import Model from keras . utils . vis_utils import model_to_dot from keras . layers import Input , Dense , Dropout , BatchNormalization from sklearn . preprocessing import MinMaxScaler
388	DATA_PATH = '../input/ieee-fraud-detection/' TRAIN_PATH = DATA_PATH + 'train_transaction.csv' TEST_PATH = DATA_PATH + 'test_transaction.csv'
389	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "ProductCD" , data = train_df , palette = reversed ( [ 'aquamarine' , 'mediumaquamarine' , 'mediumseagreen' , 'seagreen' , 'darkgreen' ] ) ) . set_title ( 'ProductCD' , fontsize = 16 ) plt . show ( plot )
390	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "P_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']" ) . query ( "TransactionAmt < 500" ) \ . groupby ( "P_emaildomain" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'lightblue' , 'darkblue' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
391	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "R_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']" ) . query ( "TransactionAmt < 500" ) \ . groupby ( "R_emaildomain" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'pink' , 'crimson' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
392	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "card4" , data = train_df . query ( "TransactionAmt < 500" ) , palette = reversed ( [ 'orangered' , 'darkorange' , 'orange' , 'peachpuff' , 'navajowhite' ] ) ) . set_title ( 'card4' , fontsize = 16 ) plt . show ( plot )
393	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "TransactionAmt < 500" ) \ . groupby ( "card4" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'peachpuff' , 'darkorange' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
394	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "card6" , data = train_df . query ( "TransactionAmt < 500" ) . query ( "card6 == 'credit' or card6 == 'debit'" ) , palette = reversed ( [ 'red' , 'crimson' , 'mediumvioletred' , 'darkmagenta' , 'indigo' ] ) ) . set_title ( 'card6' , fontsize = 16 ) plt . show ( plot )
395	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "TransactionAmt < 500" ) . query ( "card6 == 'credit' or card6 == 'debit'" ) \ . groupby ( "card6" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'plum' , 'purple' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
396	def prepare_data ( df , cat_cols = cat_cols ) : cat_cols = [ col for col in cat_cols if col in df . columns ] for col in tqdm ( cat_cols ) : \ df [ col ] = pd . factorize ( df [ col ] ) [ 0 ] return df
397	X = train_data . sort_values ( 'TransactionDT' ) . drop ( [ 'isFraud' , 'TransactionDT' , 'TransactionID' ] , axis = 1 ) y = train_data . sort_values ( 'TransactionDT' ) [ 'isFraud' ] del train_data
398	parameters = { 'application' : 'binary' , 'objective' : 'binary' , 'metric' : 'auc' , 'is_unbalance' : 'true' , 'boosting' : 'gbdt' , 'num_leaves' : 31 , 'feature_fraction' : 0.5 , 'bagging_fraction' : 0.5 , 'bagging_freq' : 20 , 'learning_rate' : 0.05 , 'verbose' : 0 } train_data = lightgbm . Dataset ( X_train , label = y_train , categorical_feature = cat_cols ) val_data = lightgbm . Dataset ( X_val , label = y_val ) model = lightgbm . train ( parameters , train_data , valid_sets = val_data , num_boost_round = 5000 , early_stopping_rounds = 100 )
399	plt . rcParams [ "axes.titlesize" ] = 16 plt . rcParams [ "axes.labelsize" ] = 15 plt . rcParams [ "xtick.labelsize" ] = 13 plt . rcParams [ "ytick.labelsize" ] = 13 plot = lightgbm . plot_importance ( model , max_num_features = 10 , figsize = ( 20 , 20 ) , grid = False , color = sns . color_palette ( "husl" , 20 ) ) plt . show ( plot )
400	fig , ax = plt . subplots ( figsize = ( 7 , 7 ) ) plt . plot ( history . history [ 'acc' ] , color = 'blue' ) plt . plot ( history . history [ 'val_acc' ] , color = 'orangered' ) plt . title ( 'Model accuracy' ) plt . ylabel ( 'Accuracy' ) plt . xlabel ( 'Epoch' ) plt . legend ( [ 'Train' , 'Validation' ] , loc = 'upper left' ) plt . show ( )
401	fig , ax = plt . subplots ( figsize = ( 7 , 7 ) ) plt . plot ( history . history [ 'loss' ] , color = 'blue' ) plt . plot ( history . history [ 'val_loss' ] , color = 'orangered' ) plt . title ( 'Model loss' ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'Epoch' ) plt . legend ( [ 'Train' , 'Validation' ] , loc = 'upper left' ) plt . show ( )
402	TEXT_COL = 'comment_text' EMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec' MAXLEN = 128 ENDLEN = 32 MAX_FEATURES = 100000 EMBED_SIZE = 300 BATCH_SIZE = 2048 NUM_EPOCHS = 100
403	lengths = train_df [ TEXT_COL ] . apply ( len ) train_df [ 'lengths' ] = lengths lengths = train_df . loc [ train_df [ 'lengths' ] < 1125 ] [ 'lengths' ] sns . distplot ( lengths , color = 'r' ) plt . show ( )
404	words = train_df [ TEXT_COL ] . apply ( lambda x : len ( x ) - len ( '' . join ( x . split ( ) ) ) + 1 ) train_df [ 'words' ] = words words = train_df . loc [ train_df [ 'words' ] < 200 ] [ 'words' ] sns . distplot ( words , color = 'g' ) plt . show ( )
405	avg_word_len = train_df [ TEXT_COL ] . apply ( lambda x : 1.0 * len ( '' . join ( x . split ( ) ) ) / ( len ( x ) - len ( '' . join ( x . split ( ) ) ) + 1 ) ) train_df [ 'avg_word_len' ] = avg_word_len avg_word_len = train_df . loc [ train_df [ 'avg_word_len' ] < 10 ] [ 'avg_word_len' ] sns . distplot ( avg_word_len , color = 'b' ) plt . show ( )
406	tokenizer = Tokenizer ( num_words = MAX_FEATURES , lower = True ) tokenizer . fit_on_texts ( list ( train_df [ TEXT_COL ] ) + list ( test_df [ TEXT_COL ] ) ) word_index = tokenizer . word_index
407	def squash ( x , axis = - 1 ) : s_squared_norm = K . sum ( K . square ( x ) , axis , keepdims = True ) + K . epsilon ( ) scale = K . sqrt ( s_squared_norm ) / ( 0.5 + s_squared_norm ) return scale * x
408	with open ( 'word_index.json' , 'w' ) as f : json . dump ( word_index , f )
409	import os import gc import pandas as pd import numpy as np from sklearn . metrics import accuracy_score , mean_absolute_error , mean_squared_error import matplotlib . pyplot as plt import seaborn as sns from langdetect import detect import markdown import json import requests import warnings import time from colorama import Fore , Back , Style , init
410	train_df = pd . read_csv ( '../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv' ) comments = train_df [ 'comment_text' ] targets = train_df [ 'target' ] severe_toxicities = train_df [ 'severe_toxicity' ] obscenities = train_df [ 'obscene' ] del train_df gc . collect ( )
411	with open ( '../input/google-api-information/Google API Key.txt' ) as f : google_api_key = f . readline ( ) [ : - 1 ] client = Perspective ( google_api_key )
412	print ( "Toxicity Mean Absolute Error : " + \ str ( mean_absolute_error ( targets [ : len ( toxicity_scores ) ] , toxicity_scores [ : len ( toxicity_scores ) ] ) ) ) print ( "Obscneity Mean Absolute Error : " + \ str ( mean_absolute_error ( obscenities [ : len ( toxicity_scores ) ] , obscenity_scores [ : len ( toxicity_scores ) ] ) ) ) print ( "Severe Toxicity Mean Absolute Error : " + \ str ( mean_absolute_error ( severe_toxicities [ : len ( toxicity_scores ) ] , severe_toxicity_scores [ : len ( toxicity_scores ) ] ) ) )
413	print ( "Toxicity Squared Absolute Error : " + \ str ( mean_squared_error ( targets [ : len ( toxicity_scores ) ] , toxicity_scores [ : len ( toxicity_scores ) ] ) ) ) print ( "Obscneity Squared Absolute Error : " + \ str ( mean_squared_error ( obscenities [ : len ( toxicity_scores ) ] , obscenity_scores [ : len ( toxicity_scores ) ] ) ) ) print ( "Severe Toxicity Squared Absolute Error : " + \ str ( mean_squared_error ( severe_toxicities [ : len ( toxicity_scores ) ] , severe_toxicity_scores [ : len ( toxicity_scores ) ] ) ) )
414	keys = set ( train_df . ebird_code ) values = np . arange ( 0 , len ( keys ) ) code_dict = dict ( zip ( sorted ( keys ) , values ) )
415	O = len ( code_dict ) network = BirdNet ( f = F , o = O ) optimizer = Adam ( [ { 'params' : network . resnet . parameters ( ) , 'lr' : LR [ 0 ] } , { 'params' : network . dense_output . parameters ( ) , 'lr' : LR [ 1 ] } ] )
416	def cel ( y_true , y_pred ) : y_true = torch . argmax ( y_true , axis = - 1 ) return nn . CrossEntropyLoss ( ) ( y_pred , y_true . squeeze ( ) ) def accuracy ( y_true , y_pred ) : y_true = torch . argmax ( y_true , axis = - 1 ) . squeeze ( ) y_pred = torch . argmax ( y_pred , axis = - 1 ) . squeeze ( ) return ( y_true == y_pred ) . float ( ) . sum ( ) / len ( y_true )
417	network . eval ( ) test_preds = [ ] test_set = BirdTestDataset ( test_df , TEST_AUDIO_PATH ) test_loader = DataLoader ( test_set , batch_size = VAL_BATCH_SIZE ) if os . path . exists ( TEST_AUDIO_PATH ) : for test_X in tqdm ( test_loader ) : test_pred = network . forward ( test_X . view ( - 1 , * D ) . to ( device ) ) test_preds . extend ( softmax ( test_pred . detach ( ) . cpu ( ) . numpy ( ) ) . flatten ( ) )
418	import os import gc import re import numpy as np import pandas as pd import nltk from nltk . corpus import wordnet , stopwords from nltk . stem import WordNetLemmatizer from nltk . stem . porter import PorterStemmer from colorama import Fore , Back , Style
419	def remove_numbers ( text ) : text = '' . join ( [ i for i in text if not i . isdigit ( ) ] ) return text
420	def replace_multi_exclamation_mark ( text ) : text = re . sub ( r"(\!)\1+" , ' multiExclamation ' , text ) return text def replace_multi_question_mark ( text ) : text = re . sub ( r"(\?)\1+" , ' multiQuestion ' , text ) return text def replace_multi_stop_mark ( text ) : text = re . sub ( r"(\.)\1+" , ' multiStop ' , text ) return text
421	def replace_elongated ( word ) : repeat_regexp = re . compile ( r'(\w*)(\w)\2(\w*)' ) repl = r'\1\2\3' if wordnet . synsets ( word ) : return word repl_word = repeat_regexp . sub ( repl , word ) if repl_word != word : return replace_elongated ( repl_word ) else : return repl_word def replace_elongated_words ( text ) : finalTokens = [ ] tokens = nltk . word_tokenize ( text ) for w in tokens : finalTokens . append ( replace_elongated ( w ) ) text = " " . join ( finalTokens ) return text
422	def get_neural_network ( ) : inputs = Input ( shape = ( X . shape [ 1 ] , ) ) dense_1 = Dense ( 10 , activation = 'relu' ) ( inputs ) dense_2 = Dense ( 10 , activation = 'relu' ) ( dense_1 ) outputs = Dense ( 1 , activation = 'sigmoid' ) ( dense_2 ) model = Model ( inputs = inputs , outputs = outputs ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ] ) return model model = get_neural_network ( )
423	split = np . int32 ( 0.8 * len ( X ) ) X_train = X [ : split ] y_train = np . int32 ( y ) [ : split ] X_val = X [ split : ] y_val = np . int32 ( y ) [ split : ]
424	preds_one_val = model_one . predict ( X_val ) preds_two_val = model_two . predict ( X_val ) preds_one_train = model_one . predict ( X_train ) preds_two_train = model_two . predict ( X_train )
425	preds_one_val = model_one . predict ( X_val ) preds_two_val = model_two . predict ( X_val ) preds_one_train = model_one . predict ( X_train ) preds_two_train = model_two . predict ( X_train )
426	import os import numpy as np import pandas as pd from tqdm import tqdm tqdm . pandas ( ) from nltk import word_tokenize , pos_tag from collections import Counter import matplotlib . pyplot as plt import seaborn as sns import warnings warnings . filterwarnings ( 'ignore' )
427	SIGNAL_LEN = 150000 MIN_NUM = - 27 MAX_NUM = 28
428	acoustic_data = seismic_signals . acoustic_data time_to_failure = seismic_signals . time_to_failure data_len = len ( seismic_signals ) del seismic_signals gc . collect ( )
429	signals = [ ] targets = [ ] for i in range ( data_len // SIGNAL_LEN ) : min_lim = SIGNAL_LEN * i max_lim = min ( [ SIGNAL_LEN * ( i + 1 ) , data_len ] ) signals . append ( list ( acoustic_data [ min_lim : max_lim ] ) ) targets . append ( time_to_failure [ max_lim ] ) del acoustic_data del time_to_failure gc . collect ( ) signals = np . array ( signals ) targets = np . array ( targets )
430	def min_max_transfer ( ts , min_value , max_value , range_needed = ( - 1 , 1 ) ) : ts_std = ( ts - min_value ) / ( max_value - min_value ) if range_needed [ 0 ] < 0 : return ts_std * ( range_needed [ 1 ] + abs ( range_needed [ 0 ] ) ) + range_needed [ 0 ] else : return ts_std * ( range_needed [ 1 ] - range_needed [ 0 ] ) + range_needed [ 0 ]
431	def prepare_data ( start , end ) : train = pd . DataFrame ( np . transpose ( signals [ int ( start ) : int ( end ) ] ) ) X = [ ] for id_measurement in tqdm ( train . index [ int ( start ) : int ( end ) ] ) : X_signal = transform_ts ( train [ id_measurement ] ) X . append ( X_signal ) X = np . asarray ( X ) return X
432	X = [ ] def load_all ( ) : total_size = len ( signals ) for start , end in [ ( 0 , int ( total_size ) ) ] : X_temp = prepare_data ( start , end ) X . append ( X_temp ) load_all ( ) X = np . concatenate ( X )
433	plot = sns . jointplot ( x = perm_entropies , y = targets , kind = 'kde' , color = 'orangered' ) plot . set_axis_labels ( 'perm_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
434	plot = sns . jointplot ( x = perm_entropies , y = targets , kind = 'reg' , color = 'orangered' ) plot . set_axis_labels ( 'perm_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
435	plot = sns . jointplot ( x = app_entropies , y = targets , kind = 'kde' , color = 'magenta' ) plot . set_axis_labels ( 'app_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
436	plot = sns . jointplot ( x = app_entropies , y = targets , kind = 'reg' , color = 'magenta' ) plot . set_axis_labels ( 'app_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
437	plot = sns . jointplot ( x = higuchi_fds , y = targets , kind = 'kde' , color = 'crimson' ) plot . set_axis_labels ( 'higuchi_fd' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
438	plot = sns . jointplot ( x = katz_fds , y = targets , kind = 'reg' , color = 'forestgreen' ) plot . set_axis_labels ( 'katz_fd' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
439	import os import gc import numpy as np from numpy . fft import * import pandas as pd import matplotlib . pyplot as plt import seaborn as sns import pywt from statsmodels . robust import mad import scipy from scipy import signal from scipy . signal import butter , deconvolve import warnings warnings . filterwarnings ( 'ignore' )
440	acoustic_data = seismic_signals . acoustic_data time_to_failure = seismic_signals . time_to_failure data_len = len ( seismic_signals ) del seismic_signals gc . collect ( )
441	def maddest ( d , axis = None ) : return np . mean ( np . absolute ( d - np . mean ( d , axis ) ) , axis )
442	SIGNAL_LEN = 150000 MIN_NUM = - 27 MAX_NUM = 28
443	acoustic_data = seismic_signals . acoustic_data time_to_failure = seismic_signals . time_to_failure data_len = len ( seismic_signals ) del seismic_signals gc . collect ( )
444	signals = [ ] targets = [ ] for i in range ( data_len // SIGNAL_LEN ) : min_lim = SIGNAL_LEN * i max_lim = min ( [ SIGNAL_LEN * ( i + 1 ) , data_len ] ) signals . append ( list ( acoustic_data [ min_lim : max_lim ] ) ) targets . append ( time_to_failure [ max_lim ] ) del acoustic_data del time_to_failure gc . collect ( ) signals = np . array ( signals ) targets = np . array ( targets )
445	def min_max_transfer ( ts , min_value , max_value , range_needed = ( - 1 , 1 ) ) : ts_std = ( ts - min_value ) / ( max_value - min_value ) if range_needed [ 0 ] < 0 : return ts_std * ( range_needed [ 1 ] + abs ( range_needed [ 0 ] ) ) + range_needed [ 0 ] else : return ts_std * ( range_needed [ 1 ] - range_needed [ 0 ] ) + range_needed [ 0 ]
446	def prepare_data ( start , end ) : train = pd . DataFrame ( np . transpose ( signals [ int ( start ) : int ( end ) ] ) ) X = [ ] for id_measurement in tqdm ( train . index [ int ( start ) : int ( end ) ] ) : X_signal = transform_ts ( train [ id_measurement ] ) X . append ( X_signal ) X = np . asarray ( X ) return X
447	X = [ ] def load_all ( ) : total_size = len ( signals ) for start , end in [ ( 0 , int ( total_size ) ) ] : X_temp = prepare_data ( start , end ) X . append ( X_temp ) load_all ( ) X = np . concatenate ( X )
448	plot = sns . jointplot ( x = spectral_entropies , y = targets , kind = 'kde' , color = 'blueviolet' ) plot . set_axis_labels ( 'spectral_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
449	plot = sns . jointplot ( x = spectral_entropies , y = targets , kind = 'reg' , color = 'blueviolet' ) plot . set_axis_labels ( 'spectral_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
450	plot = sns . jointplot ( x = sample_entropies , y = targets , kind = 'kde' , color = 'mediumvioletred' ) plot . set_axis_labels ( 'sample_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
451	plot = sns . jointplot ( x = sample_entropies , y = targets , kind = 'reg' , color = 'mediumvioletred' ) plot . set_axis_labels ( 'sample_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
452	plot = sns . jointplot ( x = detrended_fluctuations , y = targets , kind = 'kde' , color = 'mediumblue' ) plot . set_axis_labels ( 'detrended_fluctuation' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
453	plot = sns . jointplot ( x = detrended_fluctuations , y = targets , kind = 'reg' , color = 'mediumblue' ) plot . set_axis_labels ( 'detrended_fluctuation' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
454	INPUT_DIR = '../input/m5-forecasting-accuracy' calendar = pd . read_csv ( f'{INPUT_DIR}/calendar.csv' ) selling_prices = pd . read_csv ( f'{INPUT_DIR}/sell_prices.csv' ) sample_submission = pd . read_csv ( f'{INPUT_DIR}/sample_submission.csv' ) sales_train_val = pd . read_csv ( f'{INPUT_DIR}/sales_train_validation.csv' )
455	error = [ error_naive , error_avg , error_holt , error_exponential , error_arima , error_prophet ] names = [ "Naive approach" , "Moving average" , "Holt linear" , "Exponential smoothing" , "ARIMA" , "Prophet" ] df = pd . DataFrame ( np . transpose ( [ error , names ] ) ) df . columns = [ "RMSE Loss" , "Model" ] px . bar ( df , y = "RMSE Loss" , x = "Model" , color = "Model" , title = "RMSE Loss vs. Model" )
456	import os import gc import numpy as np import pandas as pd from tqdm import tqdm tqdm . pandas ( ) from collections import Counter from operator import itemgetter import scipy import cv2 from cv2 import imread import matplotlib import matplotlib . pyplot as plt import seaborn as sns
457	train_images = [ ] image_dirs = np . take ( os . listdir ( '../input/train' ) , select_rows ) for image_dir in tqdm ( sorted ( image_dirs ) ) : image = imread ( '../input/train/' + image_dir ) train_images . append ( image ) del image gc . collect ( ) train_images = np . array ( train_images )
458	labels_df = pd . read_csv ( '../input/labels.csv' ) label_dict = dict ( zip ( labels_df . attribute_id , labels_df . attribute_name ) ) for key in label_dict : if 'culture' in label_dict [ key ] : label_dict [ key ] = label_dict [ key ] [ 9 : ] if 'tag' in label_dict [ key ] : label_dict [ key ] = label_dict [ key ] [ 5 : ]
459	train_targets = [ ] for targets in targets_df . attribute_ids : target = targets . split ( ) target = list ( map ( lambda x : label_dict [ int ( x ) ] , target ) ) train_targets . append ( target ) train_targets = np . array ( train_targets )
460	fig , ax = plt . subplots ( nrows = 4 , ncols = 4 , figsize = ( 50 , 50 ) ) count = 0 for i in range ( 4 ) : for j in range ( 4 ) : ax [ i , j ] . imshow ( cv2 . cvtColor ( train_images [ count ] , cv2 . COLOR_BGR2RGB ) ) ax [ i , j ] . set_title ( str ( train_targets [ count ] ) , fontsize = 24 ) count = count + 1
461	FOLDS = 8 EPOCHS = 4 RRC = 1.0 FLIP = 1.0 NORM = 1.0 ROTATE = 1.0 LR = ( 1e-4 , 1e-3 ) MODEL_SAVE_PATH = "resnet_model" WIDTH = 512 HEIGHT = 512 BATCH_SIZE = 128 VAL_BATCH_SIZE = 128 DATA_PATH = '../input/prostate-cancer-grade-assessment/' RESIZED_PATH = '../input/panda-resized-train-data-512x512/train_images/'
462	test_df = pd . read_csv ( TEST_DATA_PATH ) train_df = pd . read_csv ( TRAIN_DATA_PATH ) sample_submission = pd . read_csv ( SAMPLE_SUB_PATH )
463	gleason_replace_dict = { 0 : 0 , 1 : 1 , 3 : 2 , 4 : 3 , 5 : 4 } def process_gleason ( gleason ) : if gleason == 'negative' : gs = ( 1 , 1 ) else : gs = tuple ( gleason . split ( '+' ) ) return [ gleason_replace_dict [ int ( g ) ] for g in gs ] train_df . gleason_score = train_df . gleason_score . apply ( process_gleason )
464	model = ResNetDetector ( ) x = torch . randn ( 2 , 3 , 32 , 32 ) . requires_grad_ ( True ) y = model ( x ) make_dot ( y , params = dict ( list ( model . named_parameters ( ) ) + [ ( 'x' , x ) ] ) )
465	def cel ( inp , targ ) : _ , labels = targ . max ( dim = 1 ) return nn . CrossEntropyLoss ( ) ( inp , labels ) def acc ( inp , targ ) : inp_idx = inp . max ( axis = 1 ) . indices targ_idx = targ . max ( axis = 1 ) . indices return ( inp_idx == targ_idx ) . float ( ) . sum ( axis = 0 ) / len ( inp_idx )
466	EPOCHS = 8 BATCH_SIZE = 128 DATA_PATH = '../input/nfl-big-data-bowl-2020/'
467	data = train_df . sample ( frac = 0.025 ) quantile = data [ "Yards" ] . quantile ( 0.95 ) data = data . loc [ data [ "Yards" ] < quantile ] plot = sns . jointplot ( x = data [ "X" ] , y = data [ "Yards" ] , kind = 'kde' , color = 'forestgreen' , height = 7 ) plot . set_axis_labels ( 'X coordinate' , 'Yards' , fontsize = 16 ) plt . show ( plot )
468	data = train_df . sample ( frac = 0.025 ) quantile = data [ "Yards" ] . quantile ( 0.95 ) data = data . loc [ data [ "Yards" ] < quantile ] plot = sns . jointplot ( x = data [ "Y" ] , y = data [ "Yards" ] , kind = 'kde' , color = ( 179 / 255 , 0 , 30 / 255 ) , height = 7 ) plot . set_axis_labels ( 'Y coordinate' , 'Yards' , fontsize = 16 ) plt . show ( plot )
469	data = train_df . sample ( frac = 0.025 ) plot = sns . jointplot ( x = data [ "X" ] , y = data [ "Y" ] , kind = 'kde' , color = 'mediumvioletred' , height = 7 ) plot . set_axis_labels ( 'X coordinate' , 'Y coordinate' , fontsize = 16 ) plt . show ( plot )
470	fig = ff . create_distplot ( hist_data = [ train_df . sample ( frac = 0.025 ) [ "S" ] ] , group_labels = "S" , colors = [ 'rgb(230, 0, 191)' ] ) fig . update_layout ( title = "S" , yaxis = dict ( title = "Probability Density" ) , xaxis = dict ( title = "S" ) ) fig . show ( )
471	cat_cols = [ 'Team' , 'FieldPosition' , 'OffenseFormation' ] value_dicts = [ ] for feature in cat_cols : values = set ( train_df [ feature ] ) value_dicts . append ( dict ( zip ( values , np . arange ( len ( values ) ) ) ) )
472	def indices ( data , feat_index ) : value_dict = value_dicts [ feat_index ] return data [ cat_cols [ feat_index ] ] . apply ( lambda x : value_dict [ x ] ) def one_hot ( indices , feat_index ) : return to_categorical ( indices , num_classes = len ( value_dicts [ feat_index ] ) )
473	num_cols = [ 'X' , 'S' , 'A' , 'Dis' , 'Orientation' , 'Dir' , 'YardLine' , 'Quarter' , 'Down' , 'Distance' , 'HomeScoreBeforePlay' , 'VisitorScoreBeforePlay' , 'DefendersInTheBox' , 'PlayerWeight' , 'Week' , 'Temperature' , 'Humidity' ] def get_numerical_features ( sample ) : return sample [ num_cols ] . values
474	hl_graph = hl . build_graph ( CNN1DNetwork ( ) , torch . zeros ( [ 1 , 25 , 17 ] ) ) hl_graph . theme = hl . graph . THEMES [ "blue" ] . copy ( ) hl_graph
475	mean = 0. std = 0. nb_samples = 0. for data , _ in tqdm ( train_loader ) : batch_samples = data . size ( 0 ) data = data . view ( batch_samples , data . size ( 1 ) , - 1 ) mean += data . mean ( ( 0 , 1 ) ) std += data . std ( ( 0 , 1 ) ) nb_samples += batch_samples mean /= nb_samples std /= nb_samples
476	def nonan ( x ) : if type ( x ) == str : return x . replace ( "\n" , "" ) else : return "" text = ' ' . join ( [ nonan ( abstract ) for abstract in train_data [ "comment_text" ] ] ) wordcloud = WordCloud ( max_font_size = None , background_color = 'black' , collocations = False , width = 1200 , height = 1000 ) . generate ( text ) fig = px . imshow ( wordcloud ) fig . update_layout ( title_text = 'Common words in comments' )
477	df [ "country" ] = df [ "Language" ] . apply ( get_country ) df = df . query ( "country != 'None'" ) fig = px . choropleth ( df , locations = "country" , hover_name = "country" , projection = "natural earth" , locationmode = "country names" , title = "Average comment length vs. Country" , color = "Average_comment_words" , template = "plotly" , color_continuous_scale = "aggrnyl" ) fig
478	fig = go . Figure ( go . Histogram ( x = [ pols [ "compound" ] for pols in train_data [ "polarity" ] if pols [ "compound" ] != 0 ] , marker = dict ( color = 'orchid' ) ) ) fig . update_layout ( xaxis_title = "Compound sentiment" , title_text = "Compound sentiment" , template = "simple_white" ) fig . show ( )
479	nums_1 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 1" ) [ "compound" ] nums_2 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 0" ) [ "compound" ] fig = ff . create_distplot ( hist_data = [ nums_1 , nums_2 ] , group_labels = [ "Toxic" , "Non-toxic" ] , colors = [ "darkorange" , "dodgerblue" ] , show_hist = False ) fig . update_layout ( title_text = "Compound vs. Toxicity" , xaxis_title = "Compound" , template = "simple_white" ) fig . show ( )
480	fig = go . Figure ( go . Histogram ( x = train_data . query ( "flesch_reading_ease > 0" ) [ "flesch_reading_ease" ] , marker = dict ( color = 'darkorange' ) ) ) fig . update_layout ( xaxis_title = "Flesch reading ease" , title_text = "Flesch reading ease" , template = "simple_white" ) fig . show ( )
481	nums_1 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 1" ) [ "flesch_reading_ease" ] nums_2 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 0" ) [ "flesch_reading_ease" ] fig = ff . create_distplot ( hist_data = [ nums_1 , nums_2 ] , group_labels = [ "Toxic" , "Non-toxic" ] , colors = [ "darkorange" , "dodgerblue" ] , show_hist = False ) fig . update_layout ( title_text = "Flesch reading ease vs. Toxicity" , xaxis_title = "Flesch reading ease" , template = "simple_white" ) fig . show ( )
482	fig = go . Figure ( go . Histogram ( x = train_data . query ( "automated_readability < 100" ) [ "automated_readability" ] , marker = dict ( color = 'mediumaquamarine' ) ) ) fig . update_layout ( xaxis_title = "Automated readability" , title_text = "Automated readability" , template = "simple_white" ) fig . show ( )
483	nums_1 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 1" ) [ "automated_readability" ] nums_2 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 0" ) [ "automated_readability" ] fig = ff . create_distplot ( hist_data = [ nums_1 , nums_2 ] , group_labels = [ "Toxic" , "Non-toxic" ] , colors = [ "darkorange" , "dodgerblue" ] , show_hist = False ) fig . update_layout ( title_text = "Automated readability vs. Toxicity" , xaxis_title = "Automated readability" , template = "simple_white" ) fig . show ( )
484	fig = go . Figure ( data = [ go . Pie ( labels = train_data . columns [ 2 : 7 ] , values = train_data . iloc [ : , 2 : 7 ] . sum ( ) . values , marker = dict ( colors = px . colors . qualitative . Plotly ) ) ] ) fig . update_traces ( textposition = 'outside' , textfont = dict ( color = "black" ) ) fig . update_layout ( title_text = "Pie chart of labels" ) fig . show ( )
485	AUTO = tf . data . experimental . AUTOTUNE tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) GCS_DS_PATH = KaggleDatasets ( ) . get_gcs_path ( 'jigsaw-multilingual-toxic-comment-classification' ) EPOCHS = 2 BATCH_SIZE = 32 * strategy . num_replicas_in_sync
486	tokenizer = transformers . DistilBertTokenizer . from_pretrained ( 'distilbert-base-multilingual-cased' ) save_path = '/kaggle/working/distilbert_base_uncased/' if not os . path . exists ( save_path ) : os . makedirs ( save_path ) tokenizer . save_pretrained ( save_path ) fast_tokenizer = BertWordPieceTokenizer ( 'distilbert_base_uncased/vocab.txt' , lowercase = True )
487	x_train = fast_encode ( train . comment_text . astype ( str ) , fast_tokenizer , maxlen = 512 ) x_valid = fast_encode ( val_data . comment_text . astype ( str ) . values , fast_tokenizer , maxlen = 512 ) x_test = fast_encode ( test_data . content . astype ( str ) . values , fast_tokenizer , maxlen = 512 ) y_valid = val . toxic . values y_train = train . toxic . values
488	train_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) . repeat ( ) . shuffle ( 2048 ) . batch ( BATCH_SIZE ) . prefetch ( AUTO ) ) valid_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_valid , y_valid ) ) . batch ( BATCH_SIZE ) . cache ( ) . prefetch ( AUTO ) ) test_dataset = ( tf . data . Dataset . from_tensor_slices ( x_test ) . batch ( BATCH_SIZE ) )
489	with strategy . scope ( ) : transformer_layer = transformers . TFDistilBertModel . \ from_pretrained ( 'distilbert-base-multilingual-cased' ) model_vnn = build_vnn_model ( transformer_layer , max_len = 512 ) model_vnn . summary ( )
490	def callback ( ) : cb = [ ] reduceLROnPlat = ReduceLROnPlateau ( monitor = 'val_loss' , factor = 0.3 , patience = 3 , verbose = 1 , mode = 'auto' , epsilon = 0.0001 , cooldown = 1 , min_lr = 0.000001 ) cb . append ( reduceLROnPlat ) log = CSVLogger ( 'log.csv' ) cb . append ( log ) RocAuc = RocAucEvaluation ( validation_data = ( x_valid , y_valid ) , interval = 1 ) cb . append ( RocAuc ) return cb
491	N_STEPS = x_train . shape [ 0 ] // BATCH_SIZE calls = callback ( ) train_history = model_vnn . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
492	with strategy . scope ( ) : model_cnn = build_cnn_model ( transformer_layer , max_len = 512 ) model_cnn . summary ( )
493	train_history = model_cnn . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
494	with strategy . scope ( ) : model_lstm = build_lstm_model ( transformer_layer , max_len = 512 ) model_lstm . summary ( )
495	train_history = model_lstm . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
496	with strategy . scope ( ) : model_capsule = build_capsule_model ( transformer_layer , max_len = 512 ) model_capsule . summary ( )
497	train_history = model_capsule . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
498	with strategy . scope ( ) : model_distilbert = build_distilbert_model ( transformer_layer , max_len = 512 ) model_distilbert . summary ( )
499	train_history = model_distilbert . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
500	EPOCHS = 5 MAXLEN = 64 SPLIT = 0.8 DROP_RATE = 0.3 LR = ( 4e-5 , 1e-2 ) BATCH_SIZE = 256 VAL_BATCH_SIZE = 8192 MODEL_SAVE_PATH = 'insincerity_model.pt'
501	EPOCHS = 20 SAMPLE_LEN = 100 IMAGE_PATH = "../input/plant-pathology-2020-fgvc7/images/" TEST_PATH = "../input/plant-pathology-2020-fgvc7/test.csv" TRAIN_PATH = "../input/plant-pathology-2020-fgvc7/train.csv" SUB_PATH = "../input/plant-pathology-2020-fgvc7/sample_submission.csv" sub = pd . read_csv ( SUB_PATH ) test_data = pd . read_csv ( TEST_PATH ) train_data = pd . read_csv ( TRAIN_PATH )
502	def load_image ( image_id ) : file_path = image_id + ".jpg" image = cv2 . imread ( IMAGE_PATH + file_path ) return cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) train_images = train_data [ "image_id" ] [ : SAMPLE_LEN ] . progress_apply ( load_image )
503	fig = ff . create_distplot ( [ values ] , group_labels = [ "Channels" ] , colors = [ "purple" ] ) fig . update_layout ( showlegend = False , template = "simple_white" ) fig . update_layout ( title_text = "Distribution of channel values" ) fig . data [ 0 ] . marker . line . color = 'rgb(0, 0, 0)' fig . data [ 0 ] . marker . line . width = 0.5 fig
504	fig = ff . create_distplot ( [ red_values ] , group_labels = [ "R" ] , colors = [ "red" ] ) fig . update_layout ( showlegend = False , template = "simple_white" ) fig . update_layout ( title_text = "Distribution of red channel values" ) fig . data [ 0 ] . marker . line . color = 'rgb(0, 0, 0)' fig . data [ 0 ] . marker . line . width = 0.5 fig
505	fig = ff . create_distplot ( [ green_values ] , group_labels = [ "G" ] , colors = [ "green" ] ) fig . update_layout ( showlegend = False , template = "simple_white" ) fig . update_layout ( title_text = "Distribution of green channel values" ) fig . data [ 0 ] . marker . line . color = 'rgb(0, 0, 0)' fig . data [ 0 ] . marker . line . width = 0.5 fig
506	fig = ff . create_distplot ( [ blue_values ] , group_labels = [ "B" ] , colors = [ "blue" ] ) fig . update_layout ( showlegend = False , template = "simple_white" ) fig . update_layout ( title_text = "Distribution of blue channel values" ) fig . data [ 0 ] . marker . line . color = 'rgb(0, 0, 0)' fig . data [ 0 ] . marker . line . width = 0.5 fig
507	AUTO = tf . data . experimental . AUTOTUNE tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) BATCH_SIZE = 16 * strategy . num_replicas_in_sync GCS_DS_PATH = KaggleDatasets ( ) . get_gcs_path ( )
508	def format_path ( st ) : return GCS_DS_PATH + '/images/' + st + '.jpg' test_paths = test_data . image_id . apply ( format_path ) . values train_paths = train_data . image_id . apply ( format_path ) . values train_labels = np . float32 ( train_data . loc [ : , 'healthy' : 'scab' ] . values ) train_paths , valid_paths , train_labels , valid_labels = \ train_test_split ( train_paths , train_labels , test_size = 0.15 , random_state = 2020 )
509	lrfn = build_lrfn ( ) STEPS_PER_EPOCH = train_labels . shape [ 0 ] // BATCH_SIZE lr_schedule = tf . keras . callbacks . LearningRateScheduler ( lrfn , verbose = 1 )
510	EPOCHS = 20 SPLIT = 0.8 MAXLEN = 48 DROP_RATE = 0.3 np . random . seed ( 42 ) OUTPUT_UNITS = 3 BATCH_SIZE = 384 LR = ( 4e-5 , 1e-2 ) ROBERTA_UNITS = 768 VAL_BATCH_SIZE = 384 MODEL_SAVE_PATH = 'sentiment_model.pt'
511	def cel ( inp , target ) : _ , labels = target . max ( dim = 1 ) return nn . CrossEntropyLoss ( ) ( inp , labels ) * len ( inp ) def accuracy ( inp , target ) : inp_ind = inp . max ( axis = 1 ) . indices target_ind = target . max ( axis = 1 ) . indices return ( inp_ind == target_ind ) . float ( ) . sum ( axis = 0 )
512	val_losses = [ torch . load ( 'val_loss_{}.pt' . format ( i ) ) for i in range ( EPOCHS ) ] train_losses = [ torch . load ( 'train_loss_{}.pt' . format ( i ) ) for i in range ( EPOCHS ) ] val_accuracies = [ torch . load ( 'val_acc_{}.pt' . format ( i ) ) for i in range ( EPOCHS ) ] train_accuracies = [ torch . load ( 'train_acc_{}.pt' . format ( i ) ) for i in range ( EPOCHS ) ]
513	W = 512 H = 512 B = 0.5 SPLIT = 0.8 SAMPLE = True MU = [ 0.485 , 0.456 , 0.406 ] SIGMA = [ 0.229 , 0.224 , 0.225 ] EPOCHS = 5 LR = 1e-3 , 1e-3 BATCH_SIZE = 32 VAL_BATCH_SIZE = 32 MODEL = 'efficientnet-b3' IMG_PATHS = [ '../working/test' , '../working/train_1' , '../working/train_2' ]
514	PATH_DICT = { } for folder_path in tqdm ( IMG_PATHS ) : for img_path in os . listdir ( folder_path ) : PATH_DICT [ img_path ] = folder_path + '/'
515	def bce ( y_true , y_pred ) : return nn . BCEWithLogitsLoss ( ) ( y_pred , y_true ) def acc ( y_true , y_pred ) : y_true = y_true . squeeze ( ) y_pred = nn . Sigmoid ( ) ( y_pred ) . squeeze ( ) return ( y_true == torch . round ( y_pred ) ) . float ( ) . sum ( ) / len ( y_true )
516	C = np . array ( [ B , ( 1 - B ) ] ) * 2 ones = len ( train_df . query ( 'target == 1' ) ) zeros = len ( train_df . query ( 'target == 0' ) ) weightage_fn = { 0 : C [ 1 ] / zeros , 1 : C [ 0 ] / ones } weights = [ weightage_fn [ target ] for target in train_df . target ]
517	length = len ( train_df ) val_ids = val_df . image_name . apply ( lambda x : x + '.jpg' ) train_ids = train_df . image_name . apply ( lambda x : x + '.jpg' ) val_set = SIIMDataset ( val_df , False , True , ids = val_ids ) train_set = SIIMDataset ( train_df , True , True , ids = train_ids )
518	train_sampler = WeightedRandomSampler ( weights , length ) if_sample , if_shuffle = ( train_sampler , False ) , ( None , True ) sample_fn = lambda is_sample , sampler : if_sample if is_sample else if_shuffle sampler , shuffler = sample_fn ( SAMPLE , train_sampler ) val_loader = DataLoader ( val_set , VAL_BATCH_SIZE , shuffle = False ) train_loader = DataLoader ( train_set , BATCH_SIZE , sampler = sampler , shuffle = shuffler )
519	device = xm . xla_device ( ) network = CancerNet ( features = 1536 ) . to ( device ) optimizer = Adam ( [ { 'params' : network . efn . parameters ( ) , 'lr' : LR [ 0 ] } , { 'params' : network . dense_output . parameters ( ) , 'lr' : LR [ 1 ] } ] )
520	bpps_files = os . listdir ( '../input/stanford-covid-vaccine/bpps/' ) example_bpps = np . load ( f'../input/stanford-covid-vaccine/bpps/{bpps_files[0]}' ) print ( 'bpps file shape:' , example_bpps . shape )
521	labels_df = pd . read_json ( os . path . join ( path , 'train_sample_videos/metadata.json' ) ) labels_df = labels_df . T print ( labels_df . shape ) labels_df . head ( )
522	import numpy as np import pandas as pd import matplotlib . pyplot as plt import plotly . offline as py py . init_notebook_mode ( connected = True ) import plotly . tools as tls import warnings import seaborn as sns plt . style . use ( 'fivethirtyeight' ) from collections import Counter warnings . filterwarnings ( 'ignore' ) import plotly . graph_objs as go import plotly . tools as tls import plotly . plotly as plpl
523	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . distplot ( train_objects [ 'yaw' ] , color = 'darkgreen' , ax = ax ) . set_title ( 'yaw' , fontsize = 16 ) plt . xlabel ( 'yaw' , fontsize = 15 ) plt . show ( )
524	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "class_name" , data = train_objects . query ( 'class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"' ) , palette = [ 'navy' , 'darkblue' , 'blue' , 'dodgerblue' , 'skyblue' , 'lightblue' ] ) . set_title ( 'Object Frequencies' , fontsize = 16 ) plt . yticks ( fontsize = 14 ) plt . xlabel ( "Count" , fontsize = 15 ) plt . ylabel ( "Class Name" , fontsize = 15 ) plt . show ( plot )
525	def render_scene ( index ) : my_scene = lyft_dataset . scene [ index ] my_sample_token = my_scene [ "first_sample_token" ] lyft_dataset . render_sample ( my_sample_token )
526	sensor_channel = 'CAM_BACK' my_sample_data = lyft_dataset . get ( 'sample_data' , my_sample [ 'data' ] [ sensor_channel ] ) lyft_dataset . render_sample_data ( my_sample_data [ 'token' ] )
527	my_scene = lyft_dataset . scene [ 0 ] my_sample_token = my_scene [ "first_sample_token" ] my_sample = lyft_dataset . get ( 'sample' , my_sample_token ) lyft_dataset . render_sample_data ( my_sample [ 'data' ] [ 'LIDAR_TOP' ] , nsweeps = 5 )
528	test [ 'batch' ] = 0 for i in range ( 0 , test . shape [ 0 ] // ROW_PER_BATCH ) : test . iloc [ i * ROW_PER_BATCH : ( i + 1 ) * ROW_PER_BATCH , 2 ] = i
529	plt . figure ( figsize = ( 20 , 5 ) ) plt . plot ( train . signal [ 500000 : 1000000 ] [ : : 100 ] ) plt . show ( )
530	import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns from kmodes . kmodes import KModes from sklearn import preprocessing from sklearn . decomposition import PCA pd . set_option ( 'mode.chained_assignment' , None )
531	train_nom = train [ nom_features ] for col in nom_features : le = preprocessing . LabelEncoder ( ) train_nom [ col ] = le . fit_transform ( train_nom [ col ] ) train_nom . head ( )
532	b , a = butter ( order , lpf_cutoff / nyq , btype = 'low' , analog = False ) w , h = freqz ( b , a , fs = fs ) plt . figure ( figsize = ( 16 , 8 ) ) ; plt . plot ( w , 20 * np . log10 ( abs ( h ) ) , 'b' ) plt . ylabel ( 'Amplitude [dB]' , color = 'b' ) plt . xlabel ( 'Frequency [Hz]' ) plt . title ( 'Low-pass Butterworth Filter, cutoff @ 600Hz' )
533	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) ) from time import time from tqdm import tqdm_notebook as tqdm from collections import Counter from scipy import stats import lightgbm as lgb from sklearn . metrics import cohen_kappa_score from sklearn . model_selection import KFold , StratifiedKFold import gc import json pd . set_option ( 'display.max_columns' , 1000 )
534	cities = pd . read_csv ( '../input/WCities.csv' ) gamecities = pd . read_csv ( '../input/WGameCities.csv' ) tourneycompactresults = pd . read_csv ( '../input/WNCAATourneyCompactResults.csv' ) tourneyseeds = pd . read_csv ( '../input/WNCAATourneySeeds.csv' ) tourneyslots = pd . read_csv ( '../input/WNCAATourneySlots.csv' ) regseasoncompactresults = pd . read_csv ( '../input/WRegularSeasonCompactResults.csv' ) seasons = pd . read_csv ( '../input/WSeasons.csv' ) teamspellings = pd . read_csv ( '../input/WTeamSpellings.csv' , engine = 'python' ) teams = pd . read_csv ( '../input/WTeams.csv' )
535	def gini ( y , pred ) : fpr , tpr , thr = metrics . roc_curve ( y , pred , pos_label = 1 ) g = 2 * metrics . auc ( fpr , tpr ) - 1 return g
536	plt . figure ( figsize = ( 20 , 10 ) ) train , test = plt . hist ( np . ceil ( train_trans [ 'TransactionDT' ] / 86400 ) , bins = 182 ) , plt . hist ( np . ceil ( test_trans [ 'TransactionDT' ] / 86400 ) , bins = 182 )
537	import numpy as np import pandas as pd train = pd . read_csv ( '/kaggle/input/covid19-global-forecasting-week-1/train.csv' ) test = pd . read_csv ( '/kaggle/input/covid19-global-forecasting-week-1/test.csv' )
538	train_agg = train [ [ 'Country_Region' , 'Date' , 'ConfirmedCases' , 'Fatalities' ] ] . groupby ( [ 'Country_Region' , 'Date' ] , as_index = False ) . agg ( { 'ConfirmedCases' : 'sum' , 'Fatalities' : 'sum' } ) train_agg [ 'Date' ] = pd . to_datetime ( train_agg [ 'Date' ] )
539	fig = px . line ( train_agg , x = 'Date' , y = 'ConfirmedCases' , color = "Country_Region" , hover_name = "Country_Region" ) fig . update_layout ( autosize = False , width = 1000 , height = 500 , title = 'Confirmed Cases Over Time for Each Country' ) fig . show ( )
540	import geopandas as gpd shapefile = '/kaggle/input/natural-earth-maps/ne_110m_admin_0_countries.shp' gdf = gpd . read_file ( shapefile ) gdf = gdf . drop ( gdf . index [ 159 ] )
541	import seaborn as sns sns . regplot ( x = 'hits' , y = 'ConfirmedCases_log10' , data = cc_google , scatter_kws = { 's' : 25 } , fit_reg = True , line_kws = { "color" : "black" } )
542	from sklearn import metrics def gini_xgb ( preds , dtrain ) : labels = dtrain . get_label ( ) gini_score = gini_normalizedc ( labels , preds ) return [ ( 'gini' , gini_score ) ] def gini_lgb ( actuals , preds ) : return 'gini' , gini_normalizedc ( actuals , preds ) , True gini_sklearn = metrics . make_scorer ( gini_normalizedc , True , True )
543	import numpy as np import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import os print ( os . listdir ( "../input" ) ) from PIL import Image import random from tqdm import tqdm_notebook
544	fre_sincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 0 ] . progress_apply ( textstat . flesch_reading_ease ) ) fre_insincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 1 ] . progress_apply ( textstat . flesch_reading_ease ) ) plot_readability ( fre_sincere , fre_insincere , "Flesch Reading Ease" , 20 )
545	def consensus_all ( text ) : return textstat . text_standard ( text , float_output = True ) con_sincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 0 ] . progress_apply ( consensus_all ) ) con_insincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 1 ] . progress_apply ( consensus_all ) ) plot_readability ( con_sincere , con_insincere , "Readability Consensus based upon all the above tests" , 2 )
546	vectorizer_sincere = CountVectorizer ( min_df = 5 , max_df = 0.9 , stop_words = 'english' , lowercase = True , token_pattern = '[a-zA-Z\-][a-zA-Z\-]{2,}' ) sincere_questions_vectorized = vectorizer_sincere . fit_transform ( sincere_questions ) vectorizer_insincere = CountVectorizer ( min_df = 5 , max_df = 0.9 , stop_words = 'english' , lowercase = True , token_pattern = '[a-zA-Z\-][a-zA-Z\-]{2,}' ) insincere_questions_vectorized = vectorizer_insincere . fit_transform ( insincere_questions )
547	pyLDAvis . enable_notebook ( ) dash = pyLDAvis . sklearn . prepare ( lda_sincere , sincere_questions_vectorized , vectorizer_sincere , mds = 'tsne' ) dash
548	pyLDAvis . enable_notebook ( ) dash = pyLDAvis . sklearn . prepare ( lda_insincere , insincere_questions_vectorized , vectorizer_insincere , mds = 'tsne' ) dash
549	sample_df = pd . read_csv ( PATH / 'sample_submission.csv' ) learn . data . add_test ( ImageList . from_df ( sample_df , PATH , folder = 'test_images' , suffix = '.png' ) ) preds , y = learn . get_preds ( DatasetType . Test ) sample_df . diagnosis = preds . argmax ( 1 ) sample_df . head ( ) sample_df . to_csv ( 'submission.csv' , index = False )
550	fig = plt . figure ( figsize = ( 15 , 10 ) ) columns = 4 ; rows = 5 for i in range ( 1 , columns * rows + 1 ) : ds = pydicom . dcmread ( train_images_dir + train [ train [ 'benign_malignant' ] == 'benign' ] [ 'image_name' ] [ i ] + '.dcm' ) fig . add_subplot ( rows , columns , i ) plt . imshow ( ds . pixel_array , cmap = plt . cm . bone ) fig . add_subplot
551	class Config : BATCH_SIZE = 8 EPOCHS = 40 WARMUP_EPOCHS = 2 LEARNING_RATE = 1e-4 WARMUP_LEARNING_RATE = 1e-3 HEIGHT = 224 WIDTH = 224 CANAL = 3 N_CLASSES = train [ 'target' ] . nunique ( ) ES_PATIENCE = 5 RLROP_PATIENCE = 3 DECAY_DROP = 0.5
552	def id_values ( row , overlap ) : for key , value in row . items ( ) : if key in overlap : print ( key , value )
553	def filter_signal ( signal , threshold = 1e8 ) : fourier = rfft ( signal ) frequencies = rfftfreq ( signal . size , d = 1e-5 ) fourier [ frequencies > threshold ] = 0 return irfft ( fourier )
554	import pandas as pd import numpy as np import matplotlib . pyplot as plt import seaborn as sns plt . style . use ( 'dark_background' ) from IPython . display import display from IPython . core . interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = "all" import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
555	print ( "Shape of data is " ) train . shape print ( 'The total number of movies are' , train . shape [ 0 ] )
556	plt . figure ( figsize = ( 20 , 12 ) ) edgecolor = ( 0 , 0 , 0 ) , sns . countplot ( train [ 'release_year' ] . sort_values ( ) , palette = "Dark2" , edgecolor = ( 0 , 0 , 0 ) ) plt . title ( "Movie Release count by Year" , fontsize = 20 ) plt . xlabel ( 'Release Year' ) plt . ylabel ( 'Number of Movies Release' ) plt . xticks ( fontsize = 12 , rotation = 90 ) plt . show ( )
557	plt . figure ( figsize = ( 20 , 12 ) ) edgecolor = ( 0 , 0 , 0 ) , sns . distplot ( train [ 'popularity' ] , kde = False ) plt . title ( "Movie Popularity Count" , fontsize = 20 ) plt . xlabel ( 'Popularity' ) plt . ylabel ( 'Count' ) plt . xticks ( fontsize = 12 , rotation = 90 ) plt . show ( )
558	plt . figure ( figsize = ( 20 , 12 ) ) edgecolor = ( 0 , 0 , 0 ) , sns . countplot ( train [ 'release_day' ] . sort_values ( ) , palette = "Dark2" , edgecolor = ( 0 , 0 , 0 ) ) plt . title ( "Movie Release count by Day of Month" , fontsize = 20 ) plt . xlabel ( 'Release Day' ) plt . ylabel ( 'Number of Movies Release' ) plt . xticks ( fontsize = 12 ) plt . show ( )
559	plt . figure ( figsize = ( 20 , 12 ) ) sns . countplot ( train [ 'release_weekday' ] . sort_values ( ) , palette = 'Dark2' ) loc = np . array ( range ( len ( train [ 'release_weekday' ] . unique ( ) ) ) ) day_labels = [ 'Mon' , 'Tue' , 'Wed' , 'Thu' , 'Fri' , 'Sat' , 'Sun' ] plt . xlabel ( 'Release Day of Week' ) plt . ylabel ( 'Number of Movies Release' ) plt . xticks ( loc , day_labels , fontsize = 12 ) plt . show ( )
560	def sieve_eratosthenes ( n ) : primes = [ False , False ] + [ True for i in range ( n - 1 ) ] p = 2 while ( p * p <= n ) : if ( primes [ p ] == True ) : for i in range ( p * 2 , n + 1 , p ) : primes [ i ] = False p += 1 return primes
561	def build_vocab ( texts ) : sentences = texts . apply ( lambda x : x . split ( ) ) . values vocab = { } for sentence in sentences : for word in sentence : try : vocab [ word ] += 1 except KeyError : vocab [ word ] = 1 return vocab
562	def add_lower ( embedding , vocab ) : count = 0 for word in vocab : if word in embedding and word . lower ( ) not in embedding : embedding [ word . lower ( ) ] = embedding [ word ] count += 1 print ( f"Added {count} words to embedding" )
563	def clean_contractions ( text , mapping ) : specials = [ "’" , "‘" , "´" , "`" ] for s in specials : text = text . replace ( s , "'" ) text = ' ' . join ( [ mapping [ t ] if t in mapping else t for t in text . split ( " " ) ] ) return text
564	def build_vocab ( texts ) : sentences = texts . apply ( lambda x : x . split ( ) ) . values vocab = { } for sentence in sentences : for word in sentence : try : vocab [ word ] += 1 except KeyError : vocab [ word ] = 1 return vocab
565	def clean_contractions ( text , mapping ) : specials = [ "’" , "‘" , "´" , "`" ] for s in specials : text = text . replace ( s , "'" ) text = ' ' . join ( [ mapping [ t ] if t in mapping else t for t in text . split ( " " ) ] ) return text
566	def make_treated_data ( X ) : t = Tokenizer ( num_words = len_voc , filters = '' ) t . fit_on_texts ( X ) X = t . texts_to_sequences ( X ) X = pad_sequences ( X , maxlen = max_len ) return X , t . word_index
567	for wav in pywt . wavelist ( ) : try : filtered = wavelet_denoising ( signal , wavelet = wav , level = 1 ) except : pass plt . figure ( figsize = ( 10 , 6 ) ) plt . plot ( signal , label = 'Raw' ) plt . plot ( filtered , label = 'Filtered' ) plt . legend ( ) plt . title ( f"DWT Denoising with {wav} Wavelet" , size = 15 ) plt . show ( )
568	plt . figure ( figsize = ( 15 , 10 ) ) sns . countplot ( 'event' , hue = 'seat' , data = train_df ) plt . xlabel ( "Seat and state of the pilot" , fontsize = 12 ) plt . ylabel ( "Count (log)" , fontsize = 12 ) plt . yscale ( 'log' ) plt . title ( "Left seat or right seat ?" , fontsize = 15 ) plt . show ( )
569	plt . figure ( figsize = ( 15 , 10 ) ) sns . violinplot ( x = 'event' , y = 'time' , data = train_df . sample ( 50000 ) ) plt . ylabel ( "Time (s)" , fontsize = 12 ) plt . xlabel ( "Event" , fontsize = 12 ) plt . title ( "Which time do events occur at ?" , fontsize = 15 ) plt . show ( )
570	plt . figure ( figsize = ( 15 , 10 ) ) sns . violinplot ( x = 'event' , y = 'gsr' , data = train_df . sample ( 50000 ) ) plt . ylabel ( "Electrodermal activity measure (µV)" , fontsize = 12 ) plt . xlabel ( "Event" , fontsize = 12 ) plt . title ( "Electrodermal activity influence" , fontsize = 15 ) plt . show ( )
571	plt . figure ( figsize = ( 12 , 12 ) ) plt . plot ( ts . rolling ( window = 30 , center = False ) . mean ( ) , label = 'Rolling Mean' ) ; plt . plot ( ts . rolling ( window = 30 , center = False ) . std ( ) , label = 'Rolling sd' ) ; plt . legend ( ) ;
572	def concorde_tsp ( seed = 42 ) : cities = pd . read_csv ( '../input/cities.csv' ) solver = TSPSolver . from_data ( cities . X , cities . Y , norm = "EUC_2D" ) tour_data = solver . solve ( time_bound = 60.0 , verbose = True , random_seed = seed ) if tour_data . found_tour : path = np . append ( tour_data . tour , [ 0 ] ) make_submission ( 'concorde' , path ) return path else : return None path_cc = concorde_tsp ( )
573	cities = pd . read_csv ( '../input/cities.csv' ) cities [ 'isPrime' ] = cities . CityId . apply ( isprime ) prime_cities = cities . loc [ ( cities . CityId == 0 ) | ( cities . isPrime ) ] solver = TSPSolver . from_data ( prime_cities . X , prime_cities . Y , norm = "EUC_2D" ) tour_data = solver . solve ( time_bound = 5.0 , verbose = True , random_seed = 42 ) prime_path = np . append ( tour_data . tour , [ 0 ] )
574	from xgboost import XGBRegressor regressor = XGBRegressor ( n_estimators = 300 ) regressor . fit ( X , y )
575	import glob import matplotlib . pyplot as plt import seaborn as sns import pandas as pd import pydicom import numpy as np import warnings import multiprocessing import os from skimage import morphology from skimage import feature from skimage import measure from skimage import util from skimage import transform warnings . filterwarnings ( 'ignore' )
576	boxes_per_patient = tr . groupby ( 'patientId' ) [ 'Target' ] . sum ( ) ax = ( boxes_per_patient > 0 ) . value_counts ( ) . plot . bar ( ) _ = ax . set_title ( 'Are the classes imbalanced?' ) _ = ax . set_xlabel ( 'Has Pneumonia' ) _ = ax . set_ylabel ( 'Count' ) _ = ax . xaxis . set_tick_params ( rotation = 0 )
577	ax = boxes_per_patient . value_counts ( ) . plot . bar ( ) _ = ax . set_title ( 'How many cases are there per image?' ) _ = ax . set_xlabel ( 'Number of cases' ) _ = ax . xaxis . set_tick_params ( rotation = 0 )
578	centers = ( tr . dropna ( subset = [ 'x' ] ) . assign ( center_x = tr . x + tr . width / 2 , center_y = tr . y + tr . height / 2 ) ) ax = sns . jointplot ( "center_x" , "center_y" , data = centers , height = 9 , alpha = 0.1 ) _ = ax . fig . suptitle ( "Where is Pneumonia located?" , y = 1.01 )
579	g = sns . FacetGrid ( col = 'Target' , hue = 'gender' , data = tr . drop_duplicates ( subset = [ 'patientId' ] ) , height = 9 , palette = dict ( F = "red" , M = "blue" ) ) _ = g . map ( sns . distplot , 'age' , hist_kws = { 'alpha' : 0.3 } ) . add_legend ( ) _ = g . fig . suptitle ( "What is the age distribution by gender and target?" , y = 1.02 , fontsize = 20 )
580	areas = tr . dropna ( subset = [ 'area' ] ) g = sns . FacetGrid ( hue = 'gender' , data = areas , height = 9 , palette = dict ( F = "red" , M = "blue" ) , aspect = 1.4 ) _ = g . map ( sns . distplot , 'area' , hist_kws = { 'alpha' : 0.3 } ) . add_legend ( ) _ = g . fig . suptitle ( 'What are the areas of the bounding boxes by gender?' , y = 1.01 )
581	pixel_vc = tr . drop_duplicates ( 'patientId' ) [ 'pixel_spacing' ] . value_counts ( ) ax = pixel_vc . iloc [ : 6 ] . plot . bar ( ) _ = ax . set_xticklabels ( [ f'{ps:.4f}' for ps in pixel_vc . index [ : 6 ] ] ) _ = ax . set_xlabel ( 'Pixel Spacing' ) _ = ax . set_ylabel ( 'Count' ) _ = ax . set_title ( 'How is the pixel spacing distributed?' , fontsize = 20 )
582	areas_with_count = areas . merge ( pd . DataFrame ( boxes_per_patient ) . rename ( columns = { 'Target' : 'bbox_count' } ) , on = 'patientId' ) g = sns . FacetGrid ( hue = 'bbox_count' , data = areas_with_count , height = 8 , aspect = 1.4 ) _ = g . map ( sns . distplot , 'area' ) . add_legend ( ) _ = g . fig . suptitle ( "How are the bounding box areas distributed by the number of boxes?" , y = 1.01 )
583	ax = sns . boxplot ( tr . mean_black_pixels ) _ = ax . set_xlabel ( 'Percentage of black pixels' ) _ = ax . set_title ( 'Are there images with mostly black pixels?' )
584	ax = sns . distplot ( tr [ 'aspect_ratio' ] . dropna ( ) , norm_hist = True ) _ = ax . set_title ( "What does the distribution of bounding aspect ratios look like?" ) _ = ax . set_xlabel ( "Aspect Ratio" )
585	from sklearn . discriminant_analysis import LinearDiscriminantAnalysis as LDA lda = LDA ( ) X = lda . fit_transform ( X , y . astype ( int ) ) X_Test = lda . transform ( X_Test )
586	import seaborn as sns fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . heatmap ( train_correlations , annot = True , vmin = - 0.23 , vmax = 0.23 , center = 0.0 , ax = ax )
587	import pandas as pd import matplotlib . pyplot as plt import seaborn as sns import numpy as np from scipy . stats import norm from sklearn . preprocessing import StandardScaler from scipy import stats import warnings warnings . filterwarnings ( 'ignore' ) import gc from sklearn . model_selection import KFold from sklearn . metrics import mean_squared_error import lightgbm as lgb import xgboost as xgb from scipy . optimize import minimize
588	starttime = timer ( None ) start_time = timer ( None ) rfecv . fit ( X , y ) timer ( start_time )
589	plt . figure ( figsize = ( 12 , 9 ) ) plt . xlabel ( 'Number of features tested x 2' ) plt . ylabel ( 'Cross-validation score (AUC)' ) plt . plot ( range ( 1 , len ( rfecv . grid_scores_ ) + 1 ) , rfecv . grid_scores_ ) plt . savefig ( 'Porto-RFECV-01.png' , dpi = 150 ) plt . show ( )
590	ranking = pd . DataFrame ( { 'Features' : all_features } ) ranking [ 'Rank' ] = np . asarray ( rfecv . ranking_ ) ranking . sort_values ( 'Rank' , inplace = True ) ranking . to_csv ( 'Porto-RFECV-ranking-01.csv' , index = False )
591	params = { 'min_child_weight' : [ 1 , 5 , 10 ] , 'gamma' : [ 0.5 , 1 , 1.5 , 2 , 5 ] , 'subsample' : [ 0.6 , 0.8 , 1.0 ] , 'colsample_bytree' : [ 0.6 , 0.8 , 1.0 ] , 'max_depth' : [ 3 , 4 , 5 ] }
592	LL_oof = log_loss ( avreal , avpred ) print ( '\n Average Log-loss: %.5f' % ( cv_LL / folds ) ) print ( ' Out-of-fold Log-loss: %.5f' % LL_oof ) AUC_oof = roc_auc_score ( avreal , avpred ) print ( '\n Average AUC: %.5f' % ( cv_AUC / folds ) ) print ( ' Out-of-fold AUC: %.5f' % AUC_oof ) print ( '\n Average normalized gini: %.5f' % ( cv_gini / folds ) ) print ( ' Out-of-fold normalized gini: %.5f' % ( AUC_oof * 2 - 1 ) ) score = str ( round ( ( AUC_oof * 2 - 1 ) , 5 ) ) timer ( starttime ) mpred = pred / folds
593	result = pd . DataFrame ( mpred , columns = [ 'target' ] ) result [ 'id' ] = te_ids result = result . set_index ( 'id' ) print ( '\n First 10 lines of your 5-fold average prediction:\n' ) print ( result . head ( 10 ) ) sub_file = 'submission_5fold-average-keras-run-01-v1_' + str ( score ) + '_' + str ( now . strftime ( '%Y-%m-%d-%H-%M' ) ) + '.csv' print ( '\n Writing submission: %s' % sub_file ) result . to_csv ( sub_file , index = True , index_label = 'id' )
594	print ( '-' * 126 ) print ( '\n Final Results' ) print ( ' Maximum XGBOOST value: %f' % XGB_BO . res [ 'max' ] [ 'max_val' ] ) print ( ' Best XGBOOST parameters: ' , XGB_BO . res [ 'max' ] [ 'max_params' ] ) grid_file = 'Bayes-gini-5fold-XGB-target-enc-run-04-v1-grid.csv' print ( ' Saving grid search parameters to %s' % grid_file ) XGB_BO . points_to_csv ( grid_file )
595	mtcnn = MTCNN ( margin = 14 , keep_all = True , factor = 0.5 , device = device ) . eval ( ) resnet = InceptionResnetV1 ( pretrained = 'vggface2' , device = device ) . eval ( )
596	fast_mtcnn = FastMTCNN ( stride = 4 , resize = 1 , margin = 14 , factor = 0.6 , keep_all = True , device = device )
597	fast_mtcnn = FastMTCNN ( stride = 4 , resize = 0.5 , margin = 14 , factor = 0.5 , keep_all = True , device = device )
598	from dlib import get_frontal_face_detector detector = get_frontal_face_detector ( ) def detect_dlib ( detector , images ) : faces = [ ] for image in images : image_gray = cv2 . cvtColor ( image , cv2 . COLOR_BGR2GRAY ) boxes = detector ( image_gray ) box = boxes [ 0 ] face = image [ box . top ( ) : box . bottom ( ) , box . left ( ) : box . right ( ) ] faces . append ( face ) return faces times_dlib = [ ]
599	from mtcnn import MTCNN detector = MTCNN ( ) def detect_mtcnn ( detector , images ) : faces = [ ] for image in images : boxes = detector . detect_faces ( image ) box = boxes [ 0 ] [ 'box' ] face = image [ box [ 1 ] : box [ 3 ] + box [ 1 ] , box [ 0 ] : box [ 2 ] + box [ 0 ] ] faces . append ( face ) return faces times_mtcnn = [ ]
600	lr = 0.005 epochs = 360 netD = Discriminator ( ) . to ( device ) optimizerD = optim . Adam ( netD . parameters ( ) , lr = lr ) criteria = nn . BCELoss ( ) netD . conv1 . weight = nn . Parameter ( torch . Tensor ( [ [ [ [ - 1.0 ] , [ 1.0 ] ] ] ] ) . to ( device ) ) for param in netD . conv1 . parameters ( ) : param . requires_grad = False
601	z = zipfile . PyZipFile ( 'images.zip' , mode = 'w' ) d = DogGenerator ( ) for k in range ( 10000 ) : img = d . getDog ( np . random . normal ( 0 , 1 , 100 ) ) f = str ( k ) + '.png' img . save ( f , 'PNG' ) ; z . write ( f ) ; os . remove ( f ) z . close ( )
602	random . seed ( 1 ) daily_sales_item_lookup_scaled_clustered . loc [ 1 ] \ . T \ . iloc [ : , random . sample ( range ( daily_sales_item_lookup_scaled_clustered . loc [ 1 ] . shape [ 0 ] ) , 10 ) ] \ . plot ( figsize = ( 12 , 6 ) )
603	daily_sales_item_lookup_scaled_weekly . T . merge ( dtw_clusters . loc [ dtw_clusters . cluster == 5 ] , left_index = True , right_index = True ) \ . T \ . plot ( figsize = ( 12 , 4 ) )
604	audio_path = '../input/train/audio/' pict_Path = '../input/picts/train/' test_pict_Path = '../input/picts/test/' test_audio_path = '../input/test/audio/' samples = [ ]
605	sample_audio = [ ] total = 0 for x in subFolderList : all_files = [ y for y in os . listdir ( audio_path + x ) if '.wav' in y ] total += len ( all_files ) sample_audio . append ( audio_path + x + '/' + all_files [ 0 ] ) print ( 'count: %d : %s' % ( len ( all_files ) , x ) ) print ( total )
606	def log_specgram ( audio , sample_rate , window_size = 20 , step_size = 10 , eps = 1e-10 ) : nperseg = int ( round ( window_size * sample_rate / 1e3 ) ) noverlap = int ( round ( step_size * sample_rate / 1e3 ) ) freqs , _ , spec = signal . spectrogram ( audio , fs = sample_rate , window = 'hann' , nperseg = nperseg , noverlap = noverlap , detrend = False ) return freqs , np . log ( spec . T . astype ( np . float32 ) + eps )
607	fig = plt . figure ( figsize = ( 8 , 20 ) ) for i , filepath in enumerate ( sample_audio [ : 6 ] ) : plt . subplot ( 9 , 1 , i + 1 ) samplerate , test_sound = wavfile . read ( filepath ) plt . title ( filepath . split ( '/' ) [ - 2 ] ) plt . axis ( 'off' ) plt . plot ( test_sound )
608	fig = plt . figure ( figsize = ( 8 , 20 ) ) for i , filepath in enumerate ( five_samples ) : plt . subplot ( 9 , 1 , i + 1 ) samplerate , test_sound = wavfile . read ( filepath ) plt . title ( filepath . split ( '/' ) [ - 2 ] ) plt . axis ( 'off' ) plt . plot ( test_sound )
609	def wav2img ( wav_path , targetdir = '' , figsize = ( 4 , 4 ) ) : fig = plt . figure ( figsize = figsize ) samplerate , test_sound = wavfile . read ( filepath ) _ , spectrogram = log_specgram ( test_sound , samplerate ) output_file = wav_path . split ( '/' ) [ - 1 ] . split ( '.wav' ) [ 0 ] output_file = targetdir + '/' + output_file plt . imsave ( '%s.png' % output_file , spectrogram ) plt . close ( )
610	import shutil shutil . make_archive ( 'train_zipped' , 'zip' , '/kaggle/working/train' ) shutil . make_archive ( 'test_zipped' , 'zip' , '/kaggle/working/test' ) shutil . make_archive ( 'exa_test_zipped' , 'zip' , '/kaggle/working/exa_test' )
611	from sklearn import preprocessing import matplotlib . pyplot as plt plt . rc ( "font" , size = 14 ) from sklearn . linear_model import LogisticRegression from sklearn . cross_validation import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X_smt , y_smt , test_size = 0.2 , random_state = 0 ) from sklearn import metrics logreg = LogisticRegression ( ) logreg . fit ( X_train , y_train )
612	fig , ax = plt . subplots ( nrows = 2 ) fig . set_size_inches ( 22 , 8.27 ) sns . lineplot ( x = 'Weeks' , y = 'Percent' , data = df , ax = ax [ 0 ] ) sns . lineplot ( x = 'Weeks' , y = 'FVC' , data = df , ax = ax [ 1 ] ) fig . savefig ( "weeksvsfvc.jpeg" )
613	files = [ ] for dirname , _ , filenames in os . walk ( '../input/osic-pulmonary-fibrosis-progression/train' ) : for filename in filenames : files . append ( os . path . join ( dirname , filename ) )
614	imp = pd . DataFrame ( index = feature_names ) imp [ 'train' ] = pd . Series ( bst . get_score ( importance_type = 'gain' ) , index = feature_names ) imp [ 'OOB' ] = pd . Series ( bst_after . get_score ( importance_type = 'gain' ) , index = feature_names ) imp = imp . fillna ( 0 )
615	import plotly . offline as pyo import plotly . plotly as py from plotly . graph_objs import * import pandas as pd import plotly plotly . offline . init_notebook_mode ( ) from scipy import signal pyo . offline . init_notebook_mode ( ) import plotly . plotly as py from plotly . graph_objs import * import plotly . plotly as py from plotly . graph_objs import *
616	edge = get_edge ( nb , data , 500 ) num_of_adjacencies = get_numbers_of_adjcs ( edge , nb ) text = [ ] for i in range ( len ( nb ) ) : t = 'neighborhood:' + '<b>' + str ( nb [ 'neighborhood_name' ] [ i ] ) + '</b>' + '<br>' + 'boro:' + '<b>' + str ( nb [ 'boro' ] [ i ] ) + '</b>' + '<br>' + ' text . append ( t ) fig = prep ( edge , num_of_adjacencies , text , nb ) pyo . iplot ( fig )
617	edge = get_edge ( nb , data , 2000 ) num_of_adjacencies = get_numbers_of_adjcs ( edge , nb ) text = [ ] for i in range ( len ( nb ) ) : t = 'neighborhood:' + '<b>' + str ( nb [ 'neighborhood_name' ] [ i ] ) + '</b>' + '<br>' + 'boro:' + '<b>' + str ( nb [ 'boro' ] [ i ] ) + '</b>' + '<br>' + ' text . append ( t ) fig = prep ( edge , num_of_adjacencies , text , nb ) pyo . iplot ( fig )
618	df_train = pd . read_csv ( 'train.csv' ) df_test = pd . read_csv ( 'test.csv' ) df_struct = pd . read_csv ( 'structures.csv' ) df_train_sub_charge = pd . read_csv ( 'mulliken_charges.csv' ) df_train_sub_tensor = pd . read_csv ( 'magnetic_shielding_tensors.csv' )
619	def submit ( predictions ) : submit = pd . read_csv ( 'sample_submission.csv' ) print ( len ( submit ) , len ( predictions ) ) submit [ "scalar_coupling_constant" ] = predictions submit . to_csv ( "/kaggle/working/workingsubmission-test.csv" , index = False ) submit ( test_prediction ) print ( 'Total training time: ' , datetime . now ( ) - start_time ) i = 0 for mol_type in mol_types : print ( mol_type , ": cv score is " , cv_score [ i ] ) i += 1 print ( "total cv score is" , cv_score_total )
620	import numpy as np import pandas as pd import os import json from pathlib import Path import matplotlib . pyplot as plt from matplotlib import colors import numpy as np for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : print ( dirname ) from pathlib import Path
621	import numpy as np import pandas as pd import matplotlib . pyplot as plt import itertools import string import re import os plt . style . use ( 'Solarize_Light2' ) pd . set_option ( "display.max_columns" , 2 ** 10 )
622	print ( "=" * 30 ) print ( "Detecting NaN values in data:" ) print ( "=" * 30 ) print ( resource_stats . isnull ( ) . sum ( axis = 0 ) [ resource_stats . isnull ( ) . sum ( axis = 0 ) > 0 ] )
623	from csv import QUOTE_ALL for text_col in text_cols : test_train [ text_col ] = test_train [ text_col ] . str . replace ( '"' , ' ' )
624	n = 25000 subset_A = test_train . loc [ lambda df : ( df . project_is_approved == 1 ) ] . sample ( n ) subset_B = test_train . loc [ lambda df : ( df . project_is_approved == 0 ) ] . sample ( n ) test_train_subset = pd . concat ( ( subset_A , subset_B ) ) test_X = test_train_subset test_y = test_X . project_is_approved
625	import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
626	import numpy as np import pandas as pd import matplotlib . pyplot as plt import warnings warnings . filterwarnings ( "ignore" )
627	import pandas as pd import torch import torch . nn . functional as F from torch . optim import Adam import schnetpack as spk import schnetpack . atomistic as atm import schnetpack . representation as rep from schnetpack . datasets import * device = torch . device ( "cuda" )
628	import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
629	plt . pie ( train [ "SmokingStatus" ] . value_counts ( ) , labels = [ "Ex-smoker" , "Never smoked" , "Currently smokes" ] , autopct = "%.1f%%" ) plt . title ( "SmokingStatus" ) plt . show ( )
630	import pandas as pd import torch import torch . nn . functional as F from torch . optim import Adam import schnetpack as spk import schnetpack . atomistic as atm import schnetpack . representation as rep from schnetpack . datasets import * device = torch . device ( "cuda" )
631	def get_size_list ( targets , dir_target ) : result = list ( ) for target in tqdm ( targets ) : img = np . array ( Image . open ( os . path . join ( dir_target , target ) ) ) result . append ( str ( img . shape ) ) return result
632	data = pd . read_csv ( '../input/train.csv' ) data [ 'size_info' ] = get_size_list ( data . Image . tolist ( ) , dir_target = '../input/train' ) data . to_csv ( './size_train.csv' , index = False )
633	counts = data . size_info . value_counts ( ) agg = data . groupby ( 'size_info' ) . Id . agg ( { 'number_sample' : len , 'rate_new_whale' : lambda g : np . mean ( g == 'new_whale' ) } ) agg = agg . sort_values ( 'number_sample' , ascending = False ) agg . to_csv ( 'result.csv' ) print ( agg . head ( 20 ) )
634	imageio . imwrite ( 'quality-70.jpg' , original_img , quality = 70 ) new_img = skimage . io . imread ( 'quality-70.jpg' ) plt . figure ( figsize = ( 6 , 6 ) ) plt . imshow ( new_img ) plt . title ( "quality-70" ) plt . show ( )
635	imageio . imwrite ( 'quality-90.jpg' , original_img , quality = 90 ) new_img = skimage . io . imread ( 'quality-90.jpg' ) plt . figure ( figsize = ( 6 , 6 ) ) plt . imshow ( new_img ) plt . title ( "quality-90" ) plt . show ( )
636	import random import numpy as np import pandas as pd import chainer import chainer_chemistry from IPython . display import display
637	train_iter = chainer . iterators . SerialIterator ( train_dataset , batch_size , order_sampler = train_sampler ) valid_iter = chainer . iterators . SerialIterator ( valid_dataset , batch_size , repeat = False , order_sampler = valid_sampler ) test_iter = chainer . iterators . SerialIterator ( test_dataset , batch_size , repeat = False , order_sampler = test_sampler )
638	from chainer import optimizers optimizer = optimizers . Adam ( alpha = 1e-3 ) optimizer . setup ( model )
639	from PIL import Image import os os . listdir ( '../input/three_band' ) with open ( '../input/three_band/6120_2_2.tif' , encoding = 'utf-8' , errors = 'ignore' ) as f : print ( f . readlines ( ) )
640	model = Sampler ( train , test ) model . compute_bounds ( ) y = model . compute_samples ( ) sample_sub = pd . read_csv ( '/kaggle/input/liverpool-ion-switching/sample_submission.csv' ) sample_sub [ 'open_channels' ] = np . array ( y ) . astype ( 'int64' ) sample_sub . to_csv ( 'submission_0.csv' , index = False , float_format = '%.4f' )
641	train_sales = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv' ) sell_prices = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/sell_prices.csv' ) calendar = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/calendar.csv' ) submission_file = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/sample_submission.csv' )
642	n_items_dept = train_sales [ 'dept_id' ] . value_counts ( ) mean_of_total_sales_per_dept = dept_sum . mean ( axis = 0 ) ax = sns . regplot ( n_items_dept , mean_of_total_sales_per_dept ) ax . set ( title = 'Do departments with more items sell more? - No' , xlabel = 'Number of items Per Department' , ylabel = 'Mean total sales per department.' ) plt . show ( )
643	cat_sum = train_sales . groupby ( [ 'cat_id' ] ) . sum ( ) . T . reset_index ( drop = True ) disp_boxplot ( data = cat_sum , title = 'Total Sales by Category' , xlabel = "Category" , ylabel = "Total Sales" )
644	state_sum = train_sales . groupby ( [ 'state_id' ] ) . sum ( ) . T . reset_index ( drop = True ) state_mean = train_sales . groupby ( [ 'state_id' ] ) . mean ( ) . T . reset_index ( drop = True ) disp_boxplot ( data = state_sum , title = 'Total Sales by State ID' , xlabel = "State ID" , ylabel = "Total Sales" ) disp_boxplot ( data = state_mean , title = 'Mean Sales by State ID' , xlabel = "State ID" , ylabel = "Mean Sales" )
645	store_sum = train_sales . groupby ( [ 'store_id' ] ) . sum ( ) . T . reset_index ( drop = True ) store_mean = train_sales . groupby ( [ 'store_id' ] ) . mean ( ) . T . reset_index ( drop = True ) disp_boxplot ( data = store_sum , title = 'Total Sales by Store ID' , xlabel = "Store ID" , ylabel = "Total Sales" ) disp_boxplot ( data = store_mean , title = 'Mean Sales Per Day by Store ID' , xlabel = "Store ID" , ylabel = "Total Sales" )
646	ax = sns . regplot ( x = np . arange ( dept_sales . shape [ 0 ] ) , y = dept_sales , scatter_kws = { 'color' : 'blue' , 'alpha' : 0.1 } , order = 3 , line_kws = { 'color' : 'green' } , ) ax . set ( title = "Mean Total Sales Per Item Per Day Over Time" , xlabel = 'Day ID' , ylabel = 'Total sale per item per day' ) plt . show ( )
647	submission_df . to_csv ( 'submission.csv' , index = False ) print ( submission_df . shape ) print ( "Submission file created" )
648	columns = train . columns for cc in tqdm_notebook ( columns ) : train [ cc ] = train [ cc ] . fillna ( train [ cc ] . mode ( ) [ 0 ] ) test [ cc ] = test [ cc ] . fillna ( test [ cc ] . mode ( ) [ 0 ] )
649	X_train = train . copy ( ) X_test = test . copy ( ) ohe = OneHotEncoder ( dtype = 'uint16' , handle_unknown = "ignore" ) ohe . fit ( train ) X_train = ohe . transform ( train ) X_test = ohe . transform ( test )
650	ent = np . zeros ( X . shape [ 0 ] ) n = 2000 ent_temp = np . zeros ( n ) cv = KFold ( n , shuffle = False ) for idx in tqdm_notebook ( range ( X . shape [ 0 ] ) ) : for idx2 , ( train_idx , test_idx ) in enumerate ( cv . split ( X [ idx ] ) ) : ent_temp [ idx2 ] = entropy_fast ( X [ idx , test_idx ] , 300 ) ent [ idx ] = np . mean ( ent_temp )
651	plt . figure ( figsize = ( 15 , 5 ) ) plt . plot ( ent ) plt . xlabel ( 'time' ) plt . ylabel ( 'entropy' ) ;
652	plt . figure ( figsize = ( 15 , 5 ) ) plt . plot ( feature , 'r' ) plt . plot ( y , 'k' ) plt . ylabel ( 'TTF' ) plt . xlabel ( 'time' ) plt . grid ( )
653	import matplotlib . pyplot as plt import numpy as np import pandas as pd from math import floor , log from scipy . stats import skew , kurtosis from scipy . io import loadmat
654	def openfile_dialog ( ) : return '../input/train_1/1_25_1.mat'
655	def convertMatToDictionary ( path ) : try : mat = loadmat ( path ) names = mat [ 'dataStruct' ] . dtype . names ndata = { n : mat [ 'dataStruct' ] [ n ] [ 0 , 0 ] for n in names } except ValueError : print ( 'File ' + path + ' is corrupted. Will skip this file in the analysis.' ) ndata = None return ndata
656	def defineEEGFreqs ( ) : return ( np . array ( [ 0.1 , 4 , 8 , 14 , 30 , 45 , 70 , 180 ] ) )
657	def petrosianFD ( X , D = None ) : if D is None : D = np . diff ( X ) N_delta = 0 ; for i in range ( 1 , len ( D ) ) : if D [ i ] * D [ i - 1 ] < 0 : N_delta += 1 n = len ( X ) return np . log10 ( n ) / ( np . log10 ( n ) + np . log10 ( n / n + 0.4 * N_delta ) )
658	def katzFD ( epoch ) : L = np . abs ( epoch - epoch [ 0 ] ) . max ( ) d = len ( epoch ) return ( np . log ( L ) / np . log ( d ) )
659	def replaceZeroRuns ( df ) : return ( df . replace ( 0 , np . nan ) . fillna ( ) )
660	from sklearn import preprocessing def normalizeFeatures ( df ) : min_max_scaler = preprocessing . MinMaxScaler ( ) x_scaled = min_max_scaler . fit_transform ( df ) df_normalized = pd . DataFrame ( x_scaled , columns = df . columns ) return df_normalized def normalizePanel ( pf ) : pf2 = { } for i in range ( pf . shape [ 2 ] ) : pf2 [ i ] = normalizeFeatures ( pf . ix [ : , : , i ] ) return pd . Panel ( pf2 )
661	from os import listdir def ieegGetFilePaths ( directory , extension = '.mat' ) : filenames = sorted ( listdir ( directory ) ) files_with_extension = [ directory + '/' + f for f in filenames if f . endswith ( extension ) and not f . startswith ( '.' ) ] return files_with_extension
662	train = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/train.csv' ) test = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/test.csv' ) submission = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv' )
663	mse = mean_squared_error ( train [ 'FVC' ] , predictions , squared = False ) mae = mean_absolute_error ( train [ 'FVC' ] , predictions ) print ( 'MSE Loss: {0:.2f}' . format ( mse ) ) print ( 'MAE Loss: {0:.2f}' . format ( mae ) )
664	fig = px . histogram ( new_df , x = 'Age' , nbins = 42 ) fig . update_traces ( marker_color = 'rgb(158,202,225)' , marker_line_color = 'rgb(8,48,107)' , marker_line_width = 1.5 , opacity = 0.6 ) fig . update_layout ( title = 'Distribution of Age' ) fig . show ( )
665	fig = px . histogram ( train_x , x = 'Age' , color = 'SmokingStatus' , color_discrete_map = { 'Never smoked' : 'yellow' , 'Currently smokes' : 'cyan' , 'Ex-smoker' : 'green' , } , hover_data = train_x . columns ) fig . update_layout ( title = 'Distribution of Age w.r.t. SmokingStatus for unique patients' ) fig . update_traces ( marker_line_color = 'black' , marker_line_width = 1.5 , opacity = 0.85 ) fig . show ( )
666	def eval_metric ( FVC , FVC_Pred , sigma ) : n = len ( sigma ) a = np . empty ( n ) a . fill ( 70 ) sigma_clipped = np . maximum ( sigma , a ) delta = np . minimum ( np . abs ( FVC , FVC_Pred ) , 1000 ) eval_metric = - np . sqrt ( 2 ) * delta / sigma_clipped - np . log ( np . sqrt ( 2 ) * sigma_clipped ) return eval_metric
667	sub_df = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv' ) print ( f"The sample submission contains: {sub_df.shape[0]} rows and {sub_df.shape[1]} columns." )
668	from sklearn . preprocessing import OneHotEncoder , LabelEncoder from sklearn . preprocessing import StandardScaler , MinMaxScaler , RobustScaler from sklearn . compose import ColumnTransformer no_transform_attribs = [ 'Patient' , 'Weeks' , 'min_week' ] num_attribs = [ 'FVC' , 'Percent' , 'Age' , 'baselined_week' , 'base_FVC' ] cat_attribs = [ 'Sex' , 'SmokingStatus' ]
669	import random from tqdm . notebook import tqdm from sklearn . model_selection import train_test_split , KFold from sklearn . metrics import mean_absolute_error from tensorflow_addons . optimizers import RectifiedAdam from tensorflow . keras import Model import tensorflow . keras . backend as K import tensorflow . keras . layers as L import tensorflow . keras . models as M from tensorflow . keras . optimizers import Nadam import seaborn as sns import plotly . express as px import plotly . graph_objects as go from PIL import Image import tensorflow as tf
670	def region_plot ( df ) : data = df . copy ( ) data [ 'time_to_failure' ] = data [ 'time_to_failure' ] * 100 data [ 'time' ] = data . index data [ 'time' ] = data [ 'time' ] * ( 1 / 4e6 ) data [ 'Time [sec]' ] = data [ 'time' ] - data [ 'time' ] . min ( ) data [ [ 'acoustic_data' , 'time_to_failure' , 'Time [sec]' ] ] . plot ( x = 'Time [sec]' , figsize = ( 8 , 5 ) ) return
671	df1 = depths . set_index ( 'id' ) df2 = train_masks . set_index ( 'id' ) dataset = pd . concat ( [ df1 , df2 ] , axis = 1 , join = 'inner' ) dataset = dataset . reset_index ( )
672	from fastai2 . basics import * from fastai2 . vision . all import * from fastai2 . medical . imaging import *
673	file_dir = Path ( '../input/osic-pulmonary-fibrosis-progression/train/ID00007637202177411956430/' ) dicom_meta = pd . DataFrame . from_dicoms ( file_dir . ls ( ) ) print ( f"Extracted DICOM data is of dimension: {dicom_meta.shape}" ) dicom_meta . head ( )
674	index = [ 'BitsStored' , 'PixelRepresentation' ] dicom_meta . pivot_table ( values = [ 'img_mean' , 'img_max' , 'img_min' , 'PatientID' ] , index = index , aggfunc = { 'img_mean' : 'mean' , 'img_max' : 'max' , 'img_min' : 'min' , 'PatientID' : 'count' } )
