1074	Specify the pretrain weights
396	Let 's fix that for the test set .
316	Create test generator
574	China and Mainland COVID
275	First of all , there 's a chance that one of these features has a high FVC_weight . I think it 's good to assume that it 's a good candidate for predicting FVC_weight . Let 's assume that this is a good candidate for predicting FVC_weight .
1509	Add leak to test
1227	Prepare for modeling
578	Italy - European countries
979	Let 's see the distribution of patient data
403	Find the indices for where the earthquakes occur , then plotting may be performed in the region around failure .
1250	Mixing up the Images
1576	Autonomous Driving Model
1034	Predicting on the Test Set
509	Zone 1 ? Really Looking at this distribution I 'm left to wonder whether the stage 2 data might be substantially different , and certainly one might think real world data might be different . Zone 1 , the right arm , seems unlikely to be the population 's most likely zone to hide contraband . Zone 2 , the right arm , seems unlikely to be the population 's most likely zone to hide contraband . To begin with , you would have to put your contraband in place with your left hand . To begin , you would have to put your contraband in place with your
1495	A function to create a description of a program
1430	Exploring the data
651	Remove rows with -1s
863	Adding to the data
784	Now we will extract the date information for the test set .
1249	Train the model
153	Let 's see the FB score
925	Distribution of AMT_INCOME_TOTAL application data
114	Clone Datasets
957	Test Predictions
1516	Let 's take a look at the distribution of ` v2a1 ` .
111	Preparing data for Neural Network
1046	Load Model into TPU
118	First , let 's look at the data .
1178	DICOM ( Digital Imaging and COmmunications in Medicine ) is the de-facto standard that establishes rules that allow medical images ( X-Ray , MRI , CT ) and associated information to be exchanged between imaging equipment from different vendors , computers , and hospitals . The DICOM format provides a suitable means that meets health information exchange ( HIE ) standards for transmission of health related data among facilities and HL7 standards which is the messaging standard that enables clinical applications to exchange data . image.png ] (
362	Example of output from Google Vision API
1330	Missing Values
1255	Pretrain models
1486	Consolidations and Ground-Glass Opacities
535	Mel-Frequency Cepstral Coefficients ( MFCCs
817	Baseline Model ( CV
988	Create a Display object
1399	Numeric features
1359	Let 's look at the distribution of numeric features for the target .
1432	Difference between d1 and h1 features
1356	Numeric features
1150	Let 's see the distribution of the Test Data .
1192	Load the data
746	Let 's run the model and submit the predictions .
731	Cross Validation CV
128	Perform the following function to calculate the distribution of the unsegmented data
820	First , we will import the required libraries .
1086	Generate predictions for submission
1180	Load the data
612	Set some hyperparameters for our model
107	It 's always a good idea to save the data as a pickle file and not make it easy to use . It takes a while to run , so it 's always a good idea to save the data .
1088	Run the four models and check the output .
1082	Let 's create a submission file .
564	Submit to Kaggle
1096	Let 's see what happens if SN_filter is 1 .
1110	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags wind_speed + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross
321	Binary target values are 0 or 1 , so we will take a random sample of data from the first two columns .
1444	Let 's read in a chunk by chunk and filter it .
554	factorize ` , ` trn_cat ` and ` tst_cat
1070	First of all , let 's identify an object in an image using the ARC solver .
1579	Plot the evaluation metrics over epochs
904	One-hot encode categorical data
927	Import the Data
452	Wind Speed
166	There are 6 features with different values
538	Interest Levels
1181	As a starter , we are going to define a function to preprocess an image . The function returns an image as a numpy array . In this case , we will resize the image .
1106	Leak Data loading and concat
470	Not looking to seriously compete in this competition so figured I 'd at least share the small experiments I was toying with . This kernel serves as just a starter to look at how to handle the various numerical and categorical variables in a NN with Keras .
1244	Box plot of Type and Weekly Sales
1491	Let 's check the distribution of unclear Abnormality and normality for a single patient .
997	Leakage Data ( Site
1575	Split the data into a training set and a test set We will use ` times_series_means ` to train the model .
381	Apply models to test data
1524	It is very important to keep the same order of ids as in the sample submission The competition metric relies only on the order of recods ignoring IDs .
481	Fit the LGBM model
1589	From the above plot we can see that for every row we need to predict 'volume ' , 'close ' , 'open ' , 'returnsPrevRaw1 ' , 'returnsPrevMktres1 ' , 'returnsClosePrevRaw10 ' , 'returnsOpenPrevMktres10 ' . But for every row we need to predict 'volume ' , 'close ' , 'open ' , 'returnsPrevMktres10 ' , and 'returnsPrevMktres
251	Let 's try to see results when training with a single country Spain
1285	Here is a list comprehension that summarizes the squared elements of a list .
1361	Let 's look at the distribution of values for the numeric features .
807	Write output to file
583	Reordered cases by day of the week
83	Outcome Type and Neutered
495	Exploratory Data Analysis
642	filtering out outliers
1054	filtered_sub_df ` contains all of the images with at least one mask . ` null_sub_df ` contains all the images with exactly 4 missing masks .
943	Cred Card Balance
144	Checking the categorical dimension
1127	Model : LGBM
1068	So now we have test text and questions . Let 's do the same for the test text and the questions .
1061	filtered_sub_df ` contains all of the images with at least one mask . ` null_sub_df ` contains all the images with exactly 4 missing masks .
920	Inference
599	Gini on random submission
177	Brightness Manipulation
749	Train and predict
1260	F1 score for validation set
331	Decision Tree
966	Evaluation of Growth Rate Percentage
71	Reading in the data
1397	Numeric features
1329	Load libraries
314	The classification report is one of the best ways to summarize the model .
303	Setting up some basic model specs
1565	Hann & Convolutional Neural Network ( CNN
729	Model fitting : Random Forest Classifier
747	For recording our result of hyperopt
187	Let 's take a look at the first level of categories
639	Setting up some basic paths
1354	Let 's look at the distribution of values for the numeric features .
1572	Visits by day
1577	Prepare the data for the Neural Network
1290	Also , let 's run XGBoost on 10 rounds
23	Text Features - Bag of Words
1215	Predict Test Set
1201	Fitting the model
1348	Merging Applicatoin data
1179	Number of Patients and Images in Test Data
1084	Model initialization
1550	More To Come . Stay Tuned .
756	We 've our image ready , let 's create an array of bounding boxes for all the wheat heads in the above image and the array of labels ( we 've only 2 class here : wheat head and background ) . As all bounding boxes are of same class , labels array will contain only 1 's .
45	Let 's look at the distribution of the target values
1132	Missing value for V319 & V321
63	Exploratory Data Analysis ( EDA
803	boosting_typeの特徴量を計算
1353	As a first step , I make a list of features that will be used as features . Here , I make a list of all categorical features .
742	Random Forest
1404	EMA & MACD
117	As we can see , there are more than 99.5 % of the data for each state . I 'll drop them .
696	A look at the distribution of data types
467	Let 's see the performance of this kernel
39	Let 's now add some non-image features . We can start with sex , and one-hot encode it .
228	Let 's see what happens when we select one commit .
881	From the plot below , we see the different values of n_estimators and learning rate .
1041	Writing out the trials data to the file
439	Visualizing MOST FREQUENT METER TYPE
498	As we can see , there are a lot of missing values in the data . Let 's do that .
860	Simple Feature Engineering
985	Cleaning the data
258	SVR
1362	Let 's look at the distribution of values for the numeric features .
1546	SAVE DATASET TO DISK
1376	Let 's look at the distribution of numeric features for the 23th feature .
259	Linear SVR
1326	Create categorical features list
826	Missing Value Exploration
545	The correlations between the top features is very similar . Let 's check the correlation between the top features .
944	load mapping dictionaries
515	Normalize and Zero Center With the data cropped , we can normalize and zero center . Note that more work needs to be done to confirm a reasonable pixel mean for the zero center .
1386	Let 's look at the distribution of numeric features for the name 34 .
143	Fixing random state
311	Now , before we look at the data , we will sample from the training set ( 0 for train and 1 for test ) . We will keep this in mind that the training set is imbalanced .
146	See sample image
617	RandomForestRegressor
573	Next I 'll create a new feature 'active ' , which is the difference between the confirmed and deaths .
1268	Prepare training data
624	Again , thank you to [ Xhlulu ] ( notebook [ here ] ( for providing this submission-formatting code
819	Baseline Model ( CV
385	Run it in parallel
1416	We need to remove all the color features from our dataset .
1382	Let 's look at the distribution of values for the numeric features .
739	Finally , I 'm going to create a submission
754	Random Forest Classifier
483	Now let 's vectorize the text using our vectorizer and check the output .
458	Make a new column Intersection ID + City name
1391	Numeric features
1126	Creating a Submission
162	Pushout + Median Stacking
1471	First , we import necessary modules .
1476	This kernel is for beginners only . Please upvote that kernel if you find it helpful .
334	Prepare Training and Validation Sets
1350	Checking for Null values
81	Let 's try to find the ` Mix ` animals that are breed .
849	learning_rate ` を使った可視化
1083	Getting the Test Data
938	LightGBM - LightGBM model
714	Now , let 's see how correlogram is used to predict the labels .
884	High Correlation Heatmap
1505	Two embedding matrices have been used . Glove , and paragram . The mean of the two is used as the final embedding matrix
224	Let 's see what happens when we select one commit .
1137	Model with Augmentation
893	There are many features that can be found in the app_train dataset . I 'd love to explore them , but it 's not so good to explore them . I 'd love to write a function to find all the features , but it 's not so easy to write a function to find all the features , so it 's not a good choice . Some of these features can be found in the app_train dataset . I 'd love to write a function to find all the features , but it 's not too bad at all . Some of these features can be found in the
640	The accuracy of our model is quite high , so let 's try some random sampling
681	Exploratory Data Analysis ( EDA
265	baggingRegressor
1091	We define the hyperparameters for the model .
829	Remove features with less than 95 % importance
1298	Categorical and numerical variables
1519	t-SNE visualization in 3 dimensions
875	Let 's look at the hyperparameters
73	Modelling with Fastai Library
1152	A lot of code in this kernel is directly inspired and taken from . It would have been so easy to get this up and running .
84	Outcome Type
410	Let 's check for duplicate images .
586	As you can see , the steps with either run_sir , run_seird , or run_sirq .
744	Macro F1 metric
138	month_temperature month_temperature
46	Let 's look at the distribution of log 1+target values .
895	Late Payment Feature
987	Setup Directory and Read Data
245	Picking the highest LB score as a feature
419	Decision Tree
618	KNN Regressor
1506	The method for training is borrowed from
1060	Predicting the Test Set
1492	More To Come . Stay Tuned .
65	Prepare the data for training .
1453	Load the training and testing data
1548	Two embedding matrices have been used . Glove , and paragram . The mean of the two is used as the final embedding matrix
266	ExtraTreesRegressor
77	Training the Model
313	Image : An example of an ROC curve . AOC is a typo and should be AUC .
1271	Visualizing the training dataset
69	Distance function is defined as : $ $ \frac { \sum\limits_ { i=1 } ^ { n } \sum\limits_ { i=1 } ^ { n-1 } ^ { n-1 } + ( 1-n ) \sum\limits_ { i=1 } ^ { n-1 } ^ { n-1 } ^ { n-1 } + ( 1-n ) + ( 2-n ) + ( 2-n ) + ( 2-n ) + ( 2-n ) + ( 1-n ) + ( 2-n ) + ( 2
241	Let 's see what happens if we select the ` commit_num ` which is 22 .
730	We have to create a Pipeline object that will combine features using 'imputer ' and 'scaler ' .
1146	Using [ Fastai Vision ] ( library
236	Let 's see what happens when we select one commit from the dataset .
1121	Outcome Type , Neutered , Animal Type
213	Let 's take a look at a sample of 5000 images .
1539	Prepare data for processing by Label Encoder
474	Set some hyperparameters for the model
763	Let 's read a sample first ( 5M rows
279	First of all , there 's a chance that one of these features has a high FVC_weight of 0 . We will choose a value of 0.36 for Dropout model , 0.225 for FVC_weight .
636	Split the data into train and test
1172	Total number of tokens and unique tokens
26	The datasets can be distinguished quite good . Let 's take a look at the feature importances .
53	Let 's take a log transform of the dataframe to get a cumulative count of the training samples .
1109	Fast data loading
638	First , we import the necessary libraries .
852	The best hyperparameters were in the param_grid .
786	Fare Amount by Hour of Day
453	Looking at the above map , there is a huge increase in the number of houses built since 1900 . April and September have high increase in the number of houses built .
131	Specail Signs and punctuations
1318	We have added some new features , so we can replace with 0s .
455	Predicting Chunks
656	Prepare for data exploration
795	Set the number of jobs to -1 and fit the model .
962	We can see that there is a big difference between importance and test importance , and that there is a big difference between importance and test importance . We will use SHAP library ( for interaction values
221	Let 's see what happens when we select one commit .
548	Bathroom Count Vs Log Error
622	Fagg model : feature agglomeration
1049	Pad and resize images for training and testing .
1428	Exploratory Data Analysis
1574	Time Series Forecasting
372	Decision Tree
1232	For the second cross_validate function , we will compare the predictions of the second model with the first model .
589	Plot the infection peak of the crisis days
1393	Let 's look at the distribution of values for the numeric features .
56	Let 's now look at the distribution of train zeros .
733	First , Importing the required libraries .
357	First , we import the necessary libraries .
418	Find the best number of clusters , using the MinMaxScaler to find the nine clusters
672	Let 's check the distribution of price variance within the parent categories and log ( price ) .
1053	Create test generator
1065	Load the model and make predictions
628	Let 's see the total number of bookings per day .
1541	Split the data into train and test sets
867	Let 's create a function that calculates the feature matrix and the feature names . You can read the documentation [ here
176	Let us take a look at the memory usage of the dataframe
1229	Bernoulli Naive Bayes
223	Let 's see what happens when we select one commit .
391	Most common level
1503	SAVE DATASET TO DISK
1277	Train a Random Forest
195	t-SNE with cervix indicators
670	We can see that most of the items have price of 10 \u20BD ( top
961	Month of the date
575	Daily Deaths by date
669	The top n ingredients in the dataset is 100 . Let 's look at the most common ingredients in the dataset .
1573	Lagged predictions based on last_date
1206	Let 's take a look at the price of rooms
924	CNT_CHILDREN ` - number of children of an application
588	Running the solving process
325	Get back to building a CNN using Keras . Much better frameworks then others . You will enjoy for sure .
1388	Let 's look at the distribution of values for the numeric features
238	Let 's see what happens if we select the ` commit_num ` to be 19 .
1207	Ploting product category of the owner and investment
1418	First , we import necessary modules .
1052	Load the U-Net++ model trained in the previous kernel .
1385	Let 's look at the distribution of values for the numeric features
1321	Blend by multiplying by the number of features
745	Confidence by Fold and Target
237	Let 's see what happens when we select the ` commit_num ` .
793	Now we will make predictions on the validation set . Finally we will make predictions on the test set .
113	calendar.csv - Contains information about the dates on which the products are sold . sales_train_validation.csv - Contains the historical daily unit sales data per product and store [ d_1 - d sample_submission.csv - The correct format for submissions . Reference the Evaluation tab for more info .
984	Let 's get started
743	Plot Feature Selection Scores
133	Let 's free up some memory
1320	Concating the public and noelec features into a new dataset
1125	From this kernel , I think that addr1 and addr2 are the same . I think that addr1 and addr2 seem to be the same . I think that addr1 and addr2 seem to be the same . I think that addr1 and addr2 seem to be the same . I think it would be nice to leave it as is . Let 's see what it does
1368	Numeric features
413	Create a DataGenOsic object
239	Let 's see what happens if we select one commit from the dataset .
1557	Let 's tokenize the text in the training data to get word lists .
534	Order Count
1195	Most common toxicity_annotator_count
916	Prepare for data exploration
244	Let 's pick a single commit , and see how it looks like .
54	Let 's check the distribution of the nonzero values in the test set .
1555	The number of words in the training set is the total number of characters in the sentence . This can be achieved by examining the number of characters present in the sentence and examining the number of occurrences of each word . We can do this by splitting the sentence into individual words . We can do this by using the ` split ( ) ` method , which yields a list of all the words in the given text . We can then do the same thing using the ` unstack ( ) ` method .
972	DICOM files can be read and processed easily with pydicom.pydicom
1170	Training and Testing Sentences
1183	Create Data Generator
675	Now let 's check the coefficient of variation ( CV ) for prices in different recognized image categories .
191	Let 's create a feature called 'no_descrip ' to identify the items with no description .
48	As we can see in the train data , the evaluation metric is Root Mean Square Logarithmic error . Let 's convert the target value to the log , i.e . ( 1+target ) function .
145	Prepare Traning Data
839	agregating cash data into individual children
447	It seems we do n't have any strong correlations . Let 's check the correlation between features .
91	Gene Frequency Plot
1592	Remove columns with type ` object ` .
1316	Create continuous features list
1357	Numeric features
652	Remove outliers with different quantiles
328	SVR
119	Expected FVC = \frac { \sum\limits_ { i=1 } ^ { N } \sum\limits_ { i=1 } ^ { N } \sum\limits_ { i=1 } ^ { N }
775	Linear Regression
1171	Now , we 'll do the same for the rest of the sentences
302	Set CV-fit model parameters
650	Observations ConfirmedCases '' and `` Fatalities '' are now only informed for dates previous to The dataset includes all countries and dates , which is required for the lag/trend step Missing values for `` ConfirmedCases '' and `` Fatalities '' have been replaced by 0 , which may be dangerous if we do not remember it at the end of the process . Lets look at how many missing values are present .
637	Now , we need to create a new column called 'lag_1 ' and 'lag_2 ' . These are the variables that we will need later .
339	Let 's try some models
1455	Convert result to submission format
1480	Introduction to Quadratic Weighted Kappa
928	Comment Length Analysis
1380	Let 's look at the distribution of values for the numeric features
1044	For each sample , we take the predicted tensors of shape ( 107 , 5 ) or ( 130 , 5 ) and convert them to the long format ( i.e . $ 629 \times 107 , 5 $ or $ 3005 \times 130 ,
1263	Pretrain models
615	Before checking missing values , let 's check for missing values in the dataframe .
882	Number of Estimators vs Learning Rate
1128	For class
601	Plot of public vs private score vs samples spoiled
1076	Convolutional Neural Network
1159	Make Predictions
993	Vamos analisar alguns dos métodos e ver o que podemos extrair dos métodos ( lags ) da série temporal da competição podemos extrair dos métodos ( lags ) da série temporal da competição podemos extrair dos métodos ( lags ) da série temporal da competição podemos extrair dos métodos (
1459	Example of sentiment
1344	So it does n't look like there 's a clear difference between the number of houses with target = 0 and number of houses with target = 1 . Let 's see if we can distinguish the two .
1532	Now , let 's check the correlation with winPlacePerc .
1498	Let 's build a model and visualize it .
212	Loading and preparing data
1115	Fast data loading
432	WordCloud from tag to count map
995	Submission
329	Linear SVR
1542	Time between measurements Let 's visualize the acoustic data .
1067	Simplified NQ Test
541	Common parameters for the model
1075	Splitting the data into train , validation and test sets
994	Let 's take a look at the DICOM files
368	Linear Regression
868	Variable Correlations
482	Importing Librosa libraries
126	Let 's take a look at the Hounsfield Units ( HU ) and the Frequency ( C ) .
1515	Map the 'Household_type ' values to the format expected by the competition .
1272	If > 0 , then this function will return the value to repeat for each class only . Otherwise it will return 1 .
429	Let 's plot a few of the data . I 'm going to use the bayesian blocks to visualize the data . I 'm going to plot the bayesian blocks as follows
741	drop high correlation columns
1377	Let 's take a look at the hist of numeric features for the target
1458	Adding start and end positions to data
250	As you see , the process is really fast . An example of some of the lag/trend columns for Spain
1469	Let 's take a look at the sales distribution .
88	A simple kernel that uses a Keras model trained in my local system .
398	Designed and run in a subprocess .
288	Let 's see what happens if we select one commit from our dataset .
310	Looking at the data
814	Boosting Type
1369	Numeric features
619	Linear Regression
1556	Finally plotting the word clouds via the following function
125	Let 's take a look at one patient for more information .
1507	Add train leak
202	Pretty cool , no Anyway , when you want to use this mask , remember to first apply a dilation morphological operation on it ( i.e . with a circular kernel ) . This expands the mask in all directions . The air + structures in the lung alone will not contain all nodules , in particular it will miss those that are stuck to the side of the lung , where they often appear ! So expand the mask a little This segmentation may fail for some edge cases . It relies on the fact that the air outside the patient is not connected to the air in the lungs . If the
180	Let 's check if there are any separate components / objects detected .
300	Building Model
585	Italy Cases by day
1173	Set global parameters
1554	Data Cleaning & EDA
941	Reading in the data
760	First of all let 's run the same model on the training set and validation set .
277	First of all , there 's a chance that one of these features was missing in the training data . I 'm not going to do it yet , but it 's worth pointing out that it 's not going to be particularly good . Let 's check it .
100	Take a random sample from the training set
366	Computing histogram
234	Let 's see what happens if we select the ` drop_model ` to 0 .
724	Let 's create a custom function to calculate the range of the features
1535	Let 's take a look at the distance matrix
1175	Now , let 's look at the distribution of titles in our dataset . I 'm also going to explore the number of links and the number of nodes in our dataset .
1104	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so it is better to consider time-splitting . However this notebook is for simple tutorial , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
975	Let 's take a look at the DICOM image
897	Building the feature matrix
837	Feature Engineering : installments_info
94	Now , let 's analyze the top 100 words in each sentence .
842	As this dataset is huge reading all the data would require a lot of memory . Therefore I 'll reset the index and check each time .
1105	Fast data loading
1551	Since we 've already melted the time series , we can just convert it into numeric values .
665	Simple imputer
367	Helper functions
140	Encode the labels
450	Air Temperature
537	Use librosa.piptrack to calculate the pitches and the magnitudes
1089	In this kernel , we will be working on accuracy_group_target feature . The target feature is set to 'accuracy_group_target ' .
771	Now , let 's take a look at fare amount by number of passengers .
963	Look at ` returnsClosePrevRaw10 ` distribution
848	log 均匀分布
1345	We can see that the correlation coefficient is highly imbalanced , but it is highly imbalanced if the source is repaying ( 0 ) or not ( 1 ) . Let 's see what the correlation coefficient is for the two variables
442	Looking at the distribution of meter reading values per primary_use
1310	First , we will import the required libraries .
1312	Augmented Dataset
582	Reorder the dataframe by day of the year
1292	It is obvious that the FVC for a patient is significantly higher than the FVC for all the patients . It is obvious that the FVC for a patient is significantly higher than the FVC for all the patients .
120	FVC Difference
315	The model performs very poorly on uncommon classes . The boxes are imprecise : the input has a very low resolution ( one pixel is 40x40cm in the real world ! ) , and we arbitrarily threshold the predictions and fit boxes around these boxes . As we evaluate with IoUs between 0.4 and 0.75 , we can expect that to hurt the score . The model is barely converged - we could train for longer . We only use LIDAR data , and we only use one lidar sweep . We compress the height dimension into only 3 channels . We assume
544	Let see what is the data type of the data .
609	Prepare the model
369	SVR
210	Feature Score
1080	Now that we have all predictions ready we can proceed to preprocessing . Let 's do the same for train_df .
105	Pickle and Save
1029	Now that we have pretty much saturated the learning rate , we train it one more epoch on the validation set , which is significantly smaller but contains a mixture of diffferent languages .
1188	Let 's take a look at each patient 's images .
148	Flowing data into a generator
294	Converting the score to numeric and calculating the max value of the feature .
982	Let 's check how many images are in each validation set .
1426	Save the data to a pandas dataframe
97	Load test data
264	Modeling with RidgeCV
422	Random Forest
718	Let 's calculate the difference between the p-correlation and the scorrs .
186	First level of categories
379	AdaBoost
1217	We define the trainer and evaluator . Since we have multiple devices we need to create a supervised model , we can use only one device .
592	See why the model fails
1291	We see that we have mo_ye ( 可以看到 item_id ) and mo_ye ( 根据对比
887	Ordinal Variable Types
426	Most of the code on feature engineering are from this link
487	To train Word2Vec , we can use keras.preprocessing.text
8	Let 's load the data .
1435	Selecting the features we need to calculate the number of unique and other features .
197	Lets visualize the data using neato
152	Train the model
614	Let 's read all the files .
513	Masking the Region of Interest Using the slice lists from above , getting a set of masked images for a given threat zone is straight forward . The same note applies here as in the 4x4 visualization above , I used a cv2.resize to get around a size constraint ( see above ) , therefore the images are quite blurry at this resolution . Note that the blurriness only applies to the unit test and visualization . The data returned by this function is at full resolution .
108	TPU or GPU detection
1160	Pre-processing the categories
170	Download by click ratio
921	Train - Test split
1307	Train a Random Forest model
1372	Let 's look at the target distribution for the numeric features .
1324	Let 's multiply all the other features by their product .
1360	Let 's look at the distribution of values for the numeric features
1439	Now we will read the data and change the type to string
1208	feature_3 has 1 when feautre_1 high than
86	Add a new column : AgeCategory
222	Let 's see what happens when we select a specific commit .
1142	Training a Wheat Model
154	Save the model
252	Italy
1544	Let us learn on a example
1303	Null values for the test set
1254	Load packages
1331	I 'll add a new category to the categories we have seen so far .
680	Exploring the data
549	Room Count Vs Log Error
425	Pretty good , no Anyway , when you want to convert an image into a 2D numpy array .
9	Imputations and Data Transformation
280	First of all , there 's a chance that one of these features has a high FVC_weight . I think it 's good to assume that it 's good to assume that it 's good at this point . Let 's see what happens when we use all of these features .
1284	Let 's plot the validation score for each model .
1265	Let 's find all the variables that are decaying in the bert_nq model .
1093	We can clearly see the distribution of var_0 , var_1 and var_2 .
1304	Missing values in categorical variables are replaced with `` missing '' .
485	Vectorize the data using TfidfVectorizer
208	Transform data using MinMaxScaler
959	Load data
233	Let 's see what happens if we select one commit from this dataset .
462	MinMax Scaling the lat and long
1384	Let 's look at the distribution of numeric features for the target .
1079	Let 's check out these images .
836	Exploratory Data Analysis
1085	Clear model and training
558	We take a look at the masks csv file , and read their summary
1226	It is very important to convert the prediction from probability to rank .
34	identity_hate
502	Applicatoin data merge
1424	Now , let 's take a look at the time series data for each country . We will use the optdisplay model to perform prediction .
1007	Load the model and train it
1144	Categorical Features
922	Let 's visualize the keypoints
31	Checking for the optimal K in Kclusters
253	Germany
276	First of all , there 's a chance that one of these features was missing in the training data . I 'll assign it to 10 and see if it improves performance .
134	A little more memory clean up and some other data clean up ....
1006	Train the model
1138	Given an image name ( an integer ) , we will create a new column called image_name + '.jpg
274	First of all , there 's a chance that one of these features was missing in the training data . I am going to drop them . Let 's increase the number of features and increase the score .
1212	Make a Baseline model
1111	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so it is better to consider time-splitting . However this notebook is for simple tutorial , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
1447	Convert categorical data to category
480	Step 1 : Prepare the data analysis
1241	Now , let 's check the shape and unique value of stores data set .
935	Using all feature engineering
61	Now let 's have a look at ProductCD .
376	Modeling with Ridge
1023	Now that we have pretty much saturated the learning rate , we train it one more epoch on the validation set , which is significantly smaller but contains a mixture of diffferent languages .
569	We can now add the ` DataGenerator ` to our training , validation and test datasets .
445	Meter Reading Recovered from MAY TO OCTOBER
90	Dataset for training text
1220	Predictions on Test set
1230	Backward Elimination
919	Split the masks into training and validation sets .
356	Random Forest
906	Feature Engineering - Bureau Balance
1402	Load libraries
673	Now let 's check the coefficient of variation for prices in different categories ( category_name ) .
896	Find the most recent data point in a time-series
783	Random Forest Prediction
1383	Let 's look at the distribution of values for the numeric features
317	Now , we load the model and generate predictions
1322	Now , we 'll multiply all the categorical features by the average value of these features .
28	Let 's start by plotting the data . I 'm not sure how to do that - but I 'm going to try it out for the purpose of this notebook . I 'll begin by plotting a histogram of the data .
1051	Since we are only interested in categories , we need to get the most common label . To do that , we need to get the most common label ( filename , type ) . To do that , we need to get the most common label ( filename , type ) , so we can use pandas ' powerful ` pivot_df ` method .
539	Interest level of the bedrooms
1196	Annotators and comments
1015	Title Mode
1570	Importing Necessary Packages
1342	We also see the distribution of percentage of application_train features for an object type .
794	Fitting the model on a sample of data
870	Specification Feature Importances
606	Load packages
1114	Find Best Weight
667	Train and predict Logistic Regression
87	Load libraries
382	Prepare for data analysis
257	Linear Regression
1222	Encoding Categorical features
679	Due to Kaggle 's disk space restrictions , we will only extract a few images .
1513	Let 's now look at the categorical and numerical features in the test set .
584	Let 's load the data and take a look at it
261	Decision Tree
59	Step 4 ) Feature Engineering
885	Dropping ` SK_ID_CURR ` and ` TARGET ` columns
155	Clear the output
64	t-SNE with 2 components
1396	Numeric features
1234	Cross Validation for Logistic Regression
801	boosting_type为goss，subsample就是所以要把两个参数放到一起设定
719	Correlation matrix of train heads
750	Confusion Matrix
411	Let 's look at all the images with the same ` hash ` .
272	First of all , there 's one commit with a number of `` commit_num '' equal to 4 . We 'll set the ` dropout_model ` to 0 .
695	Distribution of the number of unique values in Integer Columns
664	Applying one-hot encoding
691	I define a function in order can be reused
214	Creating an EntitySet from the dataframe dataframe .
705	Let 's look at the heads of household
1518	t-SNE embedding
654	Train a Random ForestRegressor on the full dataset .
414	Computing histogram
21	Let 's take a look at the distribution of ` wheezy-copper-turtle-magic ` values
725	We have reduced the number of unique values in the dataset and we can now create a new column with all unique values .
282	Let 's add one more feature , ` Dropout_model ` , ` FVC_weight ` .
653	Train a Random Forest model on the training data and check the score on the test set .
978	This function is called when the output area is scrolled .
1547	Let 's have a look at the output of this notebook
708	Look at the predictions , we can see that some walls are highly correlated with the `` epared1 '' , 'epared2 ' , 'epared3 '' . Let 's take the max to find a walls .
977	Thanks to this [ kernel ] ( for uploading the dicom files . To make this a lot of time , I will use the ` np.unique ( ) ` function , which returns an array of unique values . In this example , I will use the ` np.unique ( ) ` function , to get the unique values for each series .
1191	Train and validation split
1042	Save best hyperparameters
934	Predicting on Validation and Test
861	Now I 'll use the same number of estimators as in the previous kernel .
129	Let 's check the data memory
454	Before we go any further , we need to deal with categorical variables . A machine learning model unfortunately can not deal with categorical variables ( except for some models such as [ LightGBM ] ( Encoding Variables ) . Therefore , we have to find a way to encode ( represent ) these variables as numbers before handing them off to the model . There are two main ways to carry out this process You can see [ this Label Encoding Label encoding assigns each unique value to a different integer . This approach assumes an ordering of the categories : `` Never '' ( 0 ) < `` Rarely '' ( 1 ) < `` Never
225	Let 's pick a single commit , and see how it looks like .
200	Let 's take a look at one of the patients .
340	Apply models to test data
70	How can you read these files
809	Running the optimizer
1028	First , we train in the subset of taining set , which is completely in English .
1261	Create submission file
378	ExtraTreesRegressor
1545	Importing the data
304	Build Model
1366	Let 's look at the distribution of values for the numeric features .
688	Transforming an image id to a filepath
717	Most correlations
130	The following function counts the number of words in a sentence and creates a dictionary .
466	Let 's define a function in order to get an image file path .
1504	LOAD DATASET FROM DISK
255	Andorra
931	Applying CRF seems to have smoothed the model output .
345	Predicting on Test Set
613	Plot of training and validation loss and accuracy
1246	Box plot of Store and Weekly Sales
844	Feature Engineering and RNN
420	BanglaLekha Confusion Matrix
581	Reorder Spain Cases by Day
1584	Let 's split the filename into three variables , host , cam and timestamp .
883	High Correlation Heatmap
101	Count the number of fake samples and the number of real samples
1141	Take a look at the Efficient Detection model
1156	First , we 'll simplify the datasets to remove the columns we wo n't be using and convert the seedings to the needed format ( stripping the regional abbreviation in front of the seed ) .
283	First of all , there 's a chance that one of these features has a high FVC_weight of 0.38 when it comes to 0 . We 'll set the number to 12 and use it for prediction .
1466	Dependencies
459	Extracting informations from street features
219	Let 's see what happens when we select a specific commit .
392	Level2 the most frequent category
913	Now , let 's remove the correlation between train and test .
1279	Null Analysis
598	The metric for this competition is called `` roc_auc_score '' . The metric used for this competition is `` gini '' on the perfect submission .
301	Select the features with a standard deviation above 1e-8 .
1078	Data Augmentation
477	Build and re-install LightGBM with GPU support
785	Fare Amount versus the start of the record
1024	Instancing the tokenizer from DistilBERT model and then applying WordPieceTokenizer
389	Ok , let 's look at the images for an individual item .
322	Train and validation split
551	Define the noise
734	Train the model
514	Cropping the Images Using the crop lists from above , getting a set of cropped images for a given threat zone is also straight forward . The same note applies here as in the 4x4 visualization above , I used a cv2.resize to get around a size constraint ( see above ) , therefore the images are quite blurry at this resolution . Note that the blurriness only applies the unit test and visualization . The data returned by this function is at full resolution .
67	Load packages
149	Prepare Testing Data
371	SGD Regressor
687	Let 's split the ID into two columns and check the shape .
199	Lets visualize the data using neato
1327	Load the data
720	drop high correlation columns
1155	Import Library & Load Data
1523	Similar to validation , additional adjustment may be done based on public LB probing results ( to predict approximately the same fraction of images of a particular class as expected from the public LB .
1112	Leak Validation for public kernels ( not used leak data
690	Let 's look at the DICOM files for patient data .
1489	Let 's check out the distribution of patient id 's
853	Fitting the best model from the grid search
1370	Numeric features
911	Let 's plot all variables with a threshold of 0.8 .
1062	Preparing final submission data
475	Submission
115	store_id & item_id price data
704	Let 's see what we got
75	The images are actually quite big , so we will resize to a much smaller size .
377	baggingRegressor
1149	It is obvious that there is a significant difference between var_68 and var_69 . Let 's convert it into date .
945	extract different column types
937	Filter Data for Train and Test
1482	Patient 1 - Normal Image
727	Final features aggregation
1301	Load Test Data
230	Let 's see how that works out for the next 11 commits .
135	The first thing we can do is to get the state and country information from Covid-19 .
1151	Let 's plot now the number of var_91 for train and test .
15	Padding sequences for text length
611	Embedding Datasetup
13	Parameters and preprocessing
627	Let 's see the total number of bookings per year .
1587	Highest Volume by Asset
1514	Plotting the Plot
769	Zooming out the images
759	We replace with 0 NAs and $ \infty $ .
109	Data augmentation
1203	Logistic Regression
623	Very less values in the test set for the training set will be less than the train set for the VT model . Furthermore the accuracies in the test set will be less than the train set for the VT model .
956	Let 's check the stacking results .
800	log 均匀分布
1389	Let 's look at the distribution of target for numeric features
363	Let 's check if there are any duplicates with different target values
122	Pulmonary Condition Progression by Sex
940	Basic aggregation
1446	Let 's load some data .
1521	Evaluate the score with using 4-fold TTA ( test time augmentation ) .
644	Let 's split the label into three parts , so we can have 5 parts of label
472	Spliting training data into train and validation sets
1010	Saving the model
1339	We also see the distribution of percentage of missing values for an application_train feature .
682	The shape of train and test data is 6x6 x
66	Let 's split the data into train and test sets . I will fill missing values with mean .
1337	We also see the distribution of percentage of missing values for an application_train feature .
980	Let 's take a look at the DICOM file for a single patient .
1102	Leak Data loading and concat
269	Apply models to test data
352	Let 's have a look at 10,000 samples
888	A simple way to replace missing values with np.nan
630	We can see that the ` hotel_cluster ` is one of the most popular clusters in the data . Let 's try to aggregate on the weekdays .
469	Our target variable is y_test . Let 's see the shape of y_test
1395	Numeric features
451	First , let 's see the dew temperature
110	Define Callbacks
659	Correlation in the 0s
543	This is a collection of scripts which can be useful for this and next competitions , as I think . There is an example of baseline at the end of this notebook .
1064	Function to load images and define helper functions .
298	Prepare Training Data
1566	And finally , create the submission file .
1027	Model initialization and fitting on train and valid sets
1407	Let 's get started
1352	As we can see there are some null values , so we will try to remove them .
1066	Now , we will split our data into train and validation sets . We will define a BATCH_SIZE of 8 , to make our model 32 samples per iteration .
546	yearbuild : Year building was taken
1478	Now , we will read in the data and pre-process it .
25	Make a submission
1412	Categorize the target ( target ) by bins
707	Distribution of heads by area
1025	Load Train , Validation and Test data
503	Problem 1 some variables are missing in the training data
24	Simple NLP
421	BanglaLekha Confusion Matrix
1472	Let 's have a look at how many groups of 1108 sirna are there in the train set . The first group is ( 0 , 1 ) , and the last group is ( 3 ,
766	ECDF is a function that calculates the probability that an image is evident from the other two images . The function returns an array of arrays ( x , y ) . The function returns an array ( x , y ) . The function returns an array ( x , y ) where x and y are integers .
190	Let 's check which products are shipping depending on the prices .
1124	From the above plot we can see that addr1 and addr2 are different . I think that this means that addr1 and addr2 are the same . But if it 's different ( 60.0 , 96.0 ) or ( np.nan , np.nan ) , we can just replace it with np.nan .
866	Running DFS with default parameters
631	Next we will merge the products into a single data frame .
563	And the final mask
1130	Dropping V109 , V329 and V330 features
536	Mel-Frequency Cepstral Coefficients ( MFCCs
736	First , let 's try a few nearest neighbors .
437	Loading the data
1449	Let 's see the distribution of IPs in the dataset .
1417	Logistic Regression
6	Check for Class Imbalance
648	Train the model
211	Load the data
1071	Not bad at all ! Let 's try another task
1209	card_id : Card identifier month_lag : month lag to reference date purchase_date : Purchase date authorized_flag : Y ' if approved , 'N ' if denied category_3 : anonymized category installments : number of installments of purchase category_1 : anonymized category merchant_category_id : Merchant category identifier ( anonymized subsector_id : Merchant category group identifier ( anonymized merchant_id : Merchant identifier ( anonymized purchase_amount : Normalized purchase amount city_id : City identifier ( anonymized state_id : State identifier ( an
1238	Create submission file
1437	The next_click feature is a timedelta from a given reference datetime ( not an actual timestamp ) . The maximum value in the next_click feature is 3600000 .
1018	Read the data
1531	Let 's see the distribution of kills
668	Top n Labels
330	SGD Regressor
496	Type-Based Feature Engineering
285	First of all , there 's a chance that one of these features has a high FVC_weight of 0.2 , and a low FVC_weight of 0.37 . We 'll set the value of this feature to 0.2 . Let 's check it .
602	Density of public-private difference between public score and private score
423	Confusion Matrix
232	Let 's see what happens if we select one commit from this dataframe .
1035	Load the data
68	Read the data and define the tour
999	Session level CV score : 0 if the model is overfitting . User level CV score : 0 if the model is overfitting .
1087	Augmented Dicky Fuller Test Set
1296	Plot the evaluation metrics over epochs
764	Let 's check the distribution of fare
568	Using variance threshold to select the features
1163	Let 's check all the labels to make sure they are all in the training set .
332	Random Forest
297	First , Importing the required libraries .
1122	More To Come . Stay Tuned .
57	Let 's calculate the mean squared error for the test set .
507	Reducing the sample of target
335	Modeling with Ridge
1236	Backward Elimination
1100	Visualize the test samples
1275	Feature Engineering - Previous Applications
78	Next use ` lr_find ` again to to select a different learning rate .
996	Making a prediction on the first site
983	Preparing test data
571	Cleaning the Data
967	Plotistic Growth Curve for each confirmed and deaths
397	Mark each image as in_train and in_test .
1415	Let 's see the distribution of target variable for each type
579	Reorder Brazil Cases by Day
1423	Hong Kong , Hubei ...
150	Create Testing Generator
810	Saving the trials as json file
1379	Let 's look at the distribution of kde and target for numeric features
577	Looking at COVID for China
1536	There are missing values in the previous data set for 'DAYS_LAST_DUE ' , 'DAYS_TERMINATION ' , 'DAYS_FIRST_DRAWING ' , 'DAYS_LAST_DUE_1ST_VERSION ' . Replace these values with np.nan
1252	Label Encoding the Sexo features
1442	Listing of skiplines ( starting from 1 ) and sorting them in descending order
49	Get the list of columns to use for training
50	Let 's plot a histogram of the train data .
60	Let 's look at the number of connected components
416	Sales by State
1400	Numeric features
1308	Setting the Paths
1436	Let 's check the distribution of Minute 's time
1182	Train Validation Split
1072	Importing necessary libraries
106	The ` loadPickleBZ ` function will load the ` before.pbz ` file , and convert it to a numpy array .
524	Now we can see that 0.32 is the best metric for class 0 .
404	Data Preparation
188	Top 10 brand names
347	Make a submission
1038	Build Model for Public Model and Private Model
19	Let 's look at the distribution of the target variable
324	Cohen 's Quadratic Weighted Kappa
156	Clear the output
95	Word Distribution Over Whole Text
890	Bureau Balance Value over Time
1189	square of full data
876	Random Search and Bayesian Optimization
1431	Let 's see the distribution of age , gender and hospital_death
634	Deaths and Confirmed Time Series
1014	Let 's look at the distribution of game time per installation id .
0	Let 's see the distribution of the target values .
160	Ploting the distribution of isFraud
401	Load the data , this takes a while . There are over 629 million data rows . This data requires over 10GB of storage space on the computer 's hard drive .
1493	I will use the ` abstraction_and_reasoning_challenge ` dataset as the training dataset .
1267	Let 's take a look at the results
1302	Fill missing values in test set
1043	Again , thank you to [ Xhlulu ] ( notebook [ here ] ( for providing this submission-formatting code
127	Let 's calculate the Lung volume , which is defined by the Slice Thickness and Pixel Spacing
626	Let 's take a look at the total number of bookings per level .
1228	Logistic Regression
525	Mean Squared Error
415	Predict on Test Set
386	We split the raw data into train , test and scale fields .
512	Spreading the Spectrum From the histogram , you can see that most pixels are found between a value of 0 and and about 25 . The entire range of grayscale values in the scan is less than ~125 . You can also see a fair amount of ghosting or noise around the core image . Maybe the millimeter wave technology scatters some noise ? Not sure ... Anyway , if someone knows what this is caused by , drop a note in the comments . That said , let 's see what we can do to clean the image up . In the following function , I first threshold the background .
969	Process to prepare the data
555	We need to find the optimal scale for real features , i.e . from [ 0 , 1 ] to [ 0 , 2 ] .
792	Get the list of features .
1560	Vectorizing Raw Text
781	Heatmap showing correlation between variables
550	No Of Storeys Vs Log Error
1300	We can see that we have columns with values between -32767 and 256 . We can also see columns with values between -32768 and 32767 .
1347	First , we take the most important features .
10	Impute any values will significantly affect the RMSE score for test set . So Imputations have been excluded
42	Model Training with Spearman
722	Let 's look at the age distribution .
1166	Load the ` sample_submission.csv ` file and visualize it .
693	More To Come . Stay Tuned .
1133	Technique 4 - Android browser
1240	Extracting date features
518	Let 's create a class that will serve as a base classifier for prediction .
1564	Let 's take a look at each of these topics .
812	ROC-AUC Score
3	Listing the available files
1468	store_id , total_sales , cat_id
16	Create a dataframe with prediction of all the questions
824	Correlation
387	Lets look at some of the training data
37	Let 's now look at the distributions of various `` features
616	SVR
818	Random Search for Submission
52	First of all , let 's see the distribution of the target values in the training set .
732	Fitting the model and calculating the feature importances
184	Top 10 categories
270	Dropout Model : CV
1540	There 's still data missing in the encoded data . Let 's see how much missing data is present in the encoded data .
2	Modelling with Ftrl
1257	Build dataset objects
869	Default Risk Feature Tools
1040	Load and preprocess data
1186	Let 's take a look at each patient 's images .
1273	Oversampling the training dataset
828	Since there are several columns with zero values , we will drop them from the data .
1475	TurnOff You can not use the internet in this competition . Turn it off . SettingsからイントをOFFにします
1479	Build the Tabular Model
58	Load Data
1569	Ploting the number of images with error
1410	Extra columns are ps_ind_01 to ps_ind_03 , ps_ind_14 to ps_calc
646	Let 's split the training data in 5 groups ( 4 or 5 ) into multiple groups ( 4 or 5 ) . The first group is ( 0 , 1 , 2 ) and the last group is ( 3 , 4 , 5 ) .
247	Ensembles are averaged . Let 's average the ensembles .
103	Median Absolute Deviation
242	Let 's select a single commit from this dataframe .
635	In order to get a better understanding of the data we need to transpose it so that we can have a better view of the data . Basically , it is fairly straightforward to take each country/state combination .
560	And then finally , create a dataframe with all the bounding boxes
527	Data Preparation
1467	Plotting Sales over all the three states
1176	Let 's plot the number of links per patient .
349	Now it 's time to create a generator object
227	Let 's see what happens when we select one commit .
165	Below is the code to create a dataframe . I will be doing this only for the first 59633310 rows .
1568	Now , we will read our data , and convert it to a pandas DataFrame . To do this , we first need to load our data ( 999 rows ) and then convert it to a pandas DataFrame .
1218	Visualize Validation Results
633	Understanding the Data
1262	Load packages
123	Observation : From the above plot we observe that Pulmonary Condition is Progression by Sex .
1413	Data generator
112	Compile and fit model
740	Random Forest Submission
1522	Instead of 0.5 , one can adjust the values of the threshold for each class individually to boost the score . However , it should be done for each model individually .
89	Let 's remove stop words from train_clean_data.csv
1490	Let 's check the distribution of sample patient 12 with normality and unclear Abnormality
632	Check the distribution of ` Demanda_uni_equil_sum ` distribution
1000	TPU or GPU detection
215	High Correlation Matrix
986	Label encode all object columns
1328	Predicting on test and output
762	Submission
1123	Converting the datetime field to match localized date and time
791	Let 's take a look at the feature importances .
643	using outliers as labels instead of target column
1245	Size vs Weekly Sales
946	adapted from
758	The number of surfaces for each label is
1187	Let 's test the same for the test set .
291	First of all , let 's pick a single commit .
1534	Dumbest Path : Go in the order of magnitude : 0 , 1 , 2 .. etc . and come back to zero when you reach the end .
374	Train a XGBoost model
182	To be able to feed the mask into the MaskRCNN , we need to encode them into RLE . What is RLE encoding for the current mask is
412	Let 's look at the image , and the mask .
463	Let 's look at the updated data
476	Merging transaction and identity dataset
565	Create an iterator for submission
394	Plotting a histogram
1325	Let 's find all the columns with only one value .
1450	Device information and download percentage
7	Let 's plot the distribution of feature_1 values
243	Let 's see what happens if we select one of the most important feature for each commit .
62	Plotting Frauds and non-Frauds
780	Fitting and Evaluating the model
1030	Convert result to submission format
139	Split 'ord
894	Previous Credit Let 's check the average term of previous credit
289	Now , we 've got our commits_df , we 're done . Let 's check it .
1168	Word Embedding with Gensim
1526	Distribution of winPlacePerc
855	The best model from random search scores is located in the test set .
218	Set some constants we want to use for dropout
1231	Backward Elimination
33	I will also define a vectorizer for words and characters . I will use a TfidfVectorizer object and pass it to the constructor of the class .
124	First let 's take a look at the metadata we can find in the DICOM files . The competition organizers are hosting this data in Google Cloud Platform . The competition organizers are hosting this data in Google Cloud Platform . The competition organizers are hosting this data in Google Cloud Platform . The competition organizers are hosting this data in Google Cloud Platform . The competition organizers are hosting this data in Google Cloud Platform . The competition organizers are hosting this data in Google Cloud Platform . Downloading this data is very simple and downloading data from Google Cloud Platform . Downloading this data is very simple
1289	We define parameters for the model .
1473	Model
830	Feature Importance
1578	Augmented metric
918	Credit Card Balance
580	Reordered China Cases by Day
1147	Distribution of number of masks per image
307	As we can see in this competition , the model is quite good at this stage . Let 's try some hyperparameters .
424	Confusion Matrix
163	MinMax + Mean Stacking
547	Bedroom Count Vs Log Error
1563	Corpus - Document - Word : Topic Generation In LDA , the modelling process revolves around three things : the text corpus , its collection of documents , D and the words W in the documents . Therefore the algorithm attempts to uncover K topics from this corpus via the following way ( illustrated by the diagram Three_Level Bayesian Model Model each topic , $ \kappa $ via a Dirichlet prior distribution given by $ \beta_ { k Model each document d by another Dirichlet distribution parameterized by $ \alpha Subsequently for document d , we generate a topic via a
72	Let 's check the number of rows and columns .
318	Let 's submit the solution .
1477	Set the seed for generating random numbers
1048	Concatenate both train and test data with a new format
570	First , we import the necessary libraries .
41	Loading and preparing data
217	Load packages
909	Merging Bureau Data
716	Correlations in train/test sets
1427	Provice/State Model
522	We will now output the classification report for each class .
1465	Before moving further , let 's group visitStartTime by fullVisitorId , and create previous_visitStartTime feature . The feature is called 'previous_visitStartTime ' . The default value is -1 . We want to keep the previous visitStartTime as -1 .
361	On the right hand side , our prediction is 0.25 . Let 's see what our prediction looks like . It looks like our model 's predictions are 0.25 and 0.75 . Let 's see what we can do with that .
409	Checking for Duplicates
346	Create Prediction dataframe
168	How many clicks do we have in each category
169	Let 's check the quantiles for 99 % and 99 % on the whole set .
1269	Create the model
172	As we can see , there 's a lot of missing values for the ` attributed_time ` and ` click_time ` columns . Let 's see what happens with that .
955	Create train-validation split
137	Let 's take a look at the unique values and their distribution .
1494	Now , we 've got a function that can be lifted . We 'll replace all unlifted functions with the original function .
1161	Let 's create a dataset with 10,000 observations
607	Loading and preparing data
517	Now let 's apply the log transformation to the revenue field .
1517	Let 's look at the meaneduc for different target
12	Preparing the data for modeling
529	Convolutional Neural Network
1012	Pad and resize images for training and testing .
494	Once connected , we define a Model object and specify the input and output layers . The visible layer will be the second hidden layer .
873	We will now align the train and test datasets with the column ` TARGET ` .
1143	Let 's take a look at the unique values for columns with numeric values . For columns with numeric values , we will take a random sample of 5 rows .
441	Distribution of meter reading hours
531	Ploting the Order Count Across the Hour Of The Day
886	From the above graph we can see that most of the values are either 'true ' or 'false ' . Let 's check how many different values are in the dataset .
1013	Filter convolutional signal
491	Compile the model
104	Detect Face In this frame
171	Distribution of Download by click Ratio
821	Read bureau and previous features
1334	Dropping not used columns
1457	Ensure determinism in the results
964	Looking at ` returnsClosePrevRaw10 ` and ` returnsOpenPrevMktres10 ` plot
320	As we can see there are some missing values . We are going to treat them as one-hot encodings . Let 's convert them into binary targets .
333	Train a XGBoost model
974	Let 's look at the rest of the keywords .
976	Looks like there is some missing values in the DICOM file . Let 's try to extract the tag from it
595	Top 20 neutral words in selected_text
1537	Both datasets have a lot of missing values . Let 's check them .
1481	Predict on the test set
808	Running the optimizer
692	Combinations of TTA
1411	One-hot encoding has been my solution before but this time around I stopped and pondered : there must be a better way to handle it .
406	Okay , so it 's allready ` img_pil ` . We can do a very simple box-blur on it , and a CNN for it .
262	Random Forest
1205	Mode by Owners and Investments
790	Linear Regression
748	Saving the trials as json file
32	Read the data
1365	Let 's take a look at the data for the numeric features .
1580	I formulate this task as an extractive question answering problem , such as SQuAD . Given a question and context , the model is trained to find the answer spans in the context . Therefore , I use sentiment as question , text as context , selected_text as answer . Question : sentiment Context : text Answer : selected_text
1530	killPlace Variable
1002	The original fake paths
889	Extracting dates from bureau features
427	Credits and comments on changes
1433	Cross-validation with sklearn
173	This plot shows the number of clicks over the day . The number of clicks increases until the end of the year .
823	One hot encoder
1264	Get the pre trained model .
874	More To Come . Stay Tuned .
1153	Lets take a look at the mean of values per store , and compute the rolling mean per store .
752	Random Forest Classifier
1022	First , we train in the subset of taining set , which is completely in English .
1309	Load the model
689	Let 's take a look at the DICOM files
164	MinMax + Median Stacking
1588	Assets with unknown assetName
506	Plotting samples for the target
709	Looking at the walls and floors , we can see that the number of categoricals has significantly higher proportion as compared to the total number of categoricals .
1003	Create Fake data directory
312	Setting up some basic model specs
1131	Label encode the categorical variables
35	First , we import necessary packages .
1456	Load libraries
1145	Open mask with RLE
590	More To Come . Stay Tuned .
354	High Correlation Matrix
1371	Let 's look at the distribution of values for the numeric features .
1031	Visualizing the result as an image
674	Next we will load the image labels and concatenate them to get a single dataframe containing all the image labels .
751	To build the network , we need to first create the UMAP , PCA , and FastICA .
175	Below is the code to create a dataframe . I will be doing this only for the first 59633310 rows .
1295	Plot the accuracy and validation accuracy of the model
370	Linear SVR
965	Shap values and average importance
698	Let 's look at the head of the households without a head .
857	Lets evaluate the hyperparameters
449	Wow ! The dataset contains buildings that were built in the year
576	Looking at the above grouped data for a particular country
267	AdaBoost
1103	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags wind_speed + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross
949	Let 's take a look at the grouped merchant_id features .
1512	Exploratory Data Analysis
1351	Group battery by type
1190	Mel-Frequency Cepstral Coefficients
604	Let 's create a submission with random weights .
293	First of all , let 's select a commit number , and compute the score for that commit .
1462	Saving the model
683	One of the most important features have all 0 values . Let 's check how many of those features have all 0 values .
116	Let 's take a look at the price data
663	We can also create some time features , such as day , month , year and month .
235	Let 's pick a single commit , and see how it looks like .
557	Calculate embedding sizes for categorical features , real and text
1420	China
798	Create the model and load the data
805	Hyperopt 提供了记录结果
17	Load the predictions for the H2O model
433	Frequency of top 20 tags
296	We 're going to use the following parameters to control the learning rate of the model : 2014 ,000 ,
203	As a final preprocessing step , it is advisory to zero center your data so that your mean value is 0 . To do this you simply subtract the mean pixel value from all pixels . To determine this mean you simply average all images in the whole dataset . If that sounds like a lot of work , we found this to be around 0.25 in the LUNA16 competition . Warning : Do not zero center with the mean per image ( like is done in some kernels on here ) . The CT scanners are calibrated to return accurate HU measurements . There is no such thing as an
914	I recommend initially setting MAX_ROUNDS fairly high and using OPTIMIZE_ROUNDS to get an idea of the appropriate number of rounds ( which , in my judgment , should be close to the maximum value of best_ntree_limit among all folds , maybe even a bit higher if your model is adequately regularized ... or alternatively , you could set verbose=True and look at the details to try to find a number of rounds that works well for all folds ) . Then I would turn off OPTIMIZE_ROUNDS and set MAX_RO
508	Next I collect the constants . You 'll need to replace the various file name reference constants with a path to your corresponding folder structure .
854	Let 's generate some random parameters from the grid
1502	LOAD TRAINING DATA FROM DISK
380	Let 's try some models
1387	Let 's look at the distribution of values for the numeric features .
939	Write a submission file
1008	Loading the data
1107	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags wind_speed + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross
490	Now we need to add at the top of the base model some fully connected layers . Alsowe can use the BatchNormalization and the Dropout layers as usual in case we want to . For this I have used a Keras sequential model and build our entire model on top of it ; comprising of the VGG model as the base model + our own fully connected layers .
1026	Build dataset objects
562	Lets take a look at the masks for this image .
478	Loading the data
1319	Let 's multiply all the features by each other and create a new feature name .
761	As this is a multi class classification problem . Lets try Random Forest Classifier algorithm .
726	Dimension reduction .
806	Hyperopt 提供了记录结果的工具，可以方便实时监控
843	Next we will create a pandas dataframe with the feature importances of the model .
308	Word Cloud Plot
1406	Loading the data
802	boosting_type ` 各个参数放到最好
1390	Let 's look at the target distribution for the numeric features
1077	Permutation
1281	Extracting series from train and test set
970	load mapping dictionaries
249	Implementing the SIR model
1157	Make a new DF with just the wins and losses
1454	Looks good . Let 's check the score again .
1297	Let 's check how many data is present per diagnosis in the submission .
908	Bureau Balance by Loan
254	Albania
542	Concatenate all probabilities to a new dataframe
858	Let 's start with altair
796	How well does the model make the predictions on the test data
1500	More To Come . Stay Tuned .
556	Concatenate all text features together
1562	Here we have utilised some subtle concepts from Object-Oriented Programming ( OOP ) . We have essentially inherited and subclassed the original Sklearn 's CountVectorizer class and overwritten the build_analyzer method by implementing the lemmatizer for each list in the raw text matrix .
1135	More To Come . Stay Tuned .
246	Load and preprocess data
179	Now , we will convert the pixel values into numeric labels . To do that , we will use the [ scipy.ndimage.label ` ] ( package . Then , we will use the [ ndimage.label ` ] ( package . Then , we will store the labels in an array , with the value being 1 .
797	I recommend initially setting MAX_FOLDS to 5 and using only 5 folds . I recommend initially setting MAX_EVALS to 5 .
229	Let 's see what happens when we select a specific commit .
685	Let 's look at the distribution of the target values
1223	Let 's encode the categorical features using the binary encoding . The ` binary_encoding ` function will encode the categorical features using one-hot encoding .
30	Making submission
323	Setting up some basic model specs
96	Read training data
1306	Split the data into a training sample and a validation sample
981	Lets display some of the bottom up image in a gif format .
1081	Display Blurry Sample
1559	Lemmatization to the rescue
1177	Let 's take a look at the DICOM files
167	Let 's see the number of click by IP
1162	Let 's now look at the number of each class
489	Tokenization
20	Now we can see the distribution of muggy-smalt-axolotl-pembus values
1097	So the train and test data have the same structure as the sample_struc . Let 's check if it 's the same
1167	Model
260	SGD Regressor
835	Previous Application Data Table ` previous_application.csv
712	Let 's check the distribution of the heads .
840	Credit Card Balance
1019	Load Train , Validation and Test data
278	Now , we 've got our commits_df , we 're done . Let 's see what we can do with this .
559	Masks have ships , let 's remove them .
1237	Logistic Regression
770	Lets take a look at the absolute latitude and longitude difference
1398	Let 's look at the distribution of target for numeric features
350	Importing required libraries .
1120	Great ! It is clear that there is a difference between the spayed and intact female animals . I think it is clear that some of them are different from the unknown .
735	Linear Discriminant Analysis
231	Let 's see what happens if we select one commit from this dataset .
594	The most common words in the selected_text is 'neutral ' .
728	Education by Target and Female Head of Household
384	Define high-pass and low-pass filter
1591	Let 's create a dictionary to aggregate the news variables
1259	Generate predictions for the validation set
205	OneHotEncoding
1497	The least common product is less than the last one , so let 's take a look .
1037	Training History
1174	Adding PAD to each sequence ...
973	Let 's take a look at the patient name .
1011	We can see that most of the images are 4x4x512x1080x1080x
457	Intersection ID 's
510	Digging into the Images When we get to running a pipeline , we will want to pull the scans out one at a time so that they can be processed . So here 's a function to return the nth image . In the unit test , I added a histogram of the values in the image .
1338	We also have some missing values , so there 's no need to deal with them .
1346	We can see that the correlation coefficient is highly imbalanced , but it is highly imbalanced at all . This means that if we take values between 0 and 1 , then the correlation coefficient is highly imbalanced
1419	Preparing the data for submission
273	Let 's add one more feature , ` Dropout_model ` , ` FVC_weight ` .
1590	Preparing the data
930	Train the model
661	nominal variables
915	Top 100 Features from the bureau data
1090	Reducing the validation set
11	Detect and Correct Outliers
678	Initial position or vertex ( in millimeters ) in global coordinates . ' X ' and ' Y ' axis with number of hits generated by particular particle
1169	Visualizing the data
1036	Again , thank you to [ Xhlulu ] ( notebook [ here ] ( for providing this submission-formatting code
1136	Exploring the data
1148	Load the data
610	Create train and validation sets
1315	Replace edjefa with float values
992	Visualizing the image
990	As we can see , the position of the cylinder is around 30 and to the right of the cylinder . The angle of the cylinder is about 45 degrees . cylinderActor
532	Now , let 's plot the order counts across the weeks .
701	For the heads of households , we can see that most of the households are heads of households only
1585	In the data file description , About this file This is just a sample of the market data . You should not use this data directly . Instead , call env.get_training_data ( ) from the twosigmanews package to get the full training sets in your Kernel . So you download directly below . I using DJ sterling kernel ( thnaks
1129	UpVote if this was helpful
1194	Train-Test Split
51	Let 's plot a log histogram of the train counts and the log value .
484	Now let us transform our text using our vectorizer and output it as an array
305	Construction of the Lattice Neural Network
500	Correlation Heatmap of Application Features
174	Exploratory Data Analysis
488	Hashing the text
1219	Update learning rate
1020	Build dataset objects
201	Please note that when you apply this , to save the new spacing ! Due to rounding this may be slightly off from the desired spacing ( above script picks the best possible spacing with rounding ) . Let 's resample our patient 's pixels to an isomorphic resolution of 1 by 1 mm .
1282	We will also need a plot function to plot the predictions and the actual values .
147	Set a learning rate annealer
1443	HOURLY CLICK FREQUENCY
1278	Straight away we can see that the predictions are very innacurate , this could be due to the fact that there are so many 0 values and that there are so many 0 values and that there is a lot of noise and barely any signal . One thing to note is that even using SARIMA , the predictions are not very similar . One thing to note is that even using SARIMA , the predictions are very similar . One thing to note is that even using SARIMA , the predictions are very similar . One thing to note is that even using SARIMA , the predictions
1527	Let 's check the distribution of assists .
468	Prepare for data exploration
121	Let 's see the correlation between features .
337	ExtraTreesRegressor
1452	Some functions that might be useful .
713	Extracting capabilities from all the images
1461	Fixing neutral sentiments
1358	Let 's look at the distribution of values for the numeric features .
1538	In order to get a better understanding of the features , using the ft.dfs method , we need to limit the max_depth to 2 . We can do this with an entityset as follows
1487	Let 's look at a sample patient
1073	Load packages
359	tanh ( tanh
402	Lets check the test files . This verifies that they all contain 150,000 samples as expected .
933	Building the model
206	This kernel is specifically is for Beginners who want 's to experiment building CNN using XGBoost . I recommend initially setting MAX_ROUNDS fairly high and using OPTIMIZE_ROUNDS to get an idea of the appropriate number of rounds ( which , in my judgment , should be close to the maximum value of best_ntree_limit among all folds , maybe even a bit higher if your model is adequately regularized ... or alternatively , you can turn off OPTIMIZE_ROUNDS to get an idea of the appropriate number of rounds
159	The data comes from two separate sites Hot Pepper Gourmet ( hpg ) : similar to Yelp , here users can search restaurants and also make a reservation online AirREGI / Restaurant Board ( air ) : similar to Square , a reservation control and cash register system
290	Now we can see that there are no missing values ( ` FVC_weight ` =0.38 & ` FVC_weight ` =0.2 ) and ` lb_score ` is - 6 .
460	turn direction The cardinal directions can be expressed using the equation : $ $ \frac { \theta } { \pi Where $ \theta $ is the angle between the direction we want to encode and the north compass direction , measured clockwise .
684	The plot above shows that there are some binary features with more than 1 value . Let 's see if there are any binary features with more than 1 value .
960	Splitting the test data into public and private test data
597	Perfect submission looks like this vector looks like
608	Building Keras LSTM model
1582	Let 's take a look at the JSON data
240	Let 's see what happens if we select the ` drop_model ` to 0 .
1045	Building Model and calculating predictions
341	I define a function to calculate the IoU .
220	Let 's see what happens if we select a single commit .
1332	I 'll combine similar categories into one .
671	Lets take top 10 categories with price > 1M .
1253	Let 's have a look at the distribution of data in train and test sets .
989	Bkg Color
1140	Load Image Data
1403	Mel-Frequency Cepstral Coefficients ( Mel-Frequency Cepstral Coefficients
526	Here is one of the most important features . OLS ( OLS ) is a linear regression model . We can add a constant to the data , then call the estimated OLS ( OLS ) method .
1198	Split the data into train and test .
947	Get the list of input files
196	Lets visualize the structure of the bulge graph
1197	First , let 's sort by distance to the target=0 .
721	Education Distribution by Target
353	Creating an EntitySet from the dataframe dataframe .
157	The version of torchvision and mmdet are respectively 1.0.58 and 2.5.1 .
600	Evaluate public LB score with first 30 % score
226	Let 's see what happens if we select one commit from this dataset .
905	One hot encoding
1483	Lung Opacity Visualization
1200	Create Train and Test datasets
192	Description of the Item
351	Loading and preparing data
1496	Evaluate the model using a list of functions . The function must take a single image as an input . The function must return a list of tensors .
1256	Let 's create an iterator using the JSONL files .
1113	A one idea how we can use LV usefull is blending . We probably can find best blending method without LB probing .
1213	Create dataset for training and Validation
596	We will now analyze the class distribution
778	Baseline Model ( baseline
1364	Let 's take a look at the distribution of values for the numeric features
587	Preparing the data
834	Feature Engineering - Bureau Data
76	Create the function to calculate F1 score
1501	Ensure determinism in the results
757	Data Import
1094	First , I 'll calculate the SNR ratio of each error and the mes column .
711	Warning variable vs Target
1119	SexuponOutcome
1185	Reading the data
405	We can see the differences between the images in the training set and the test set . The differences are pretty close .
942	Let 's aggregate the features on the Bureau_balance dataset .
295	Average prediction
641	First , we import the required libraries .
1401	Numeric features
444	Distribution of meter reading values across the week days
443	Looking at the distribution of meter readings for each primary_use
1581	Loading the dataset and basic visualization
281	Let 's add one more feature : commit_num , dropout_model , FVC_weight
1294	To use this kernel , we need to create a directory where the converted images will be stored . If the directory does n't exist it will be created .
629	Let 's see the total number of bookings per day .
519	Cross-validation with logreg , SGD and rfc
1005	Define the densenet
1445	Let 's read in the training data
1274	Feature Engineering - Bureau Data
1225	Since 'ps_calc ' features do not show any have zero relationship with other features
1375	Let 's look at the distribution of values for the numeric features .
1063	Histogram of predictions
838	Reading POS_CASH_balance.csv
1460	Prepare test data
1511	Pulmonary Fibrosis Progression
5	Let 's look at the distribution of the target variable
348	Now it 's time to create a generator object
1525	How to Process the data
1117	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags wind_speed + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross
183	Looking at the data
1101	Fast data loading
465	MNCAATourney & MRegularSeason Detailed Results
1235	Both train and test prediction lists have been converted to a pandas dataframe so we can use it for prediction .
1317	Create a list of new features , one for each family_size features .
194	Scatter plot of description length VS price
1434	Now it 's time to split the data into training and testing sets .
846	Hyperparameters search for optimal hyperparameters
952	Prepare for modeling
1508	Select some features ( threshold is not optimized
1451	Let 's plot as well the ratio of click hour to is_attributed .
953	Read the data .
36	Load OOF and submission data
178	We can see that there are 2 prominent peaks . The number of pixels with intensity values around 0 is extrememly low ( 250000 ) . We would expect this to occur as the nuclei cover a smaller portion of the picture as compared to the background which is primarily black . Our job here is to seperate the nuclei from the background . To seperate the nuclei , we should use Otsu 's method . Otsu 's method is implemented in [ here
971	Visualizing the data
132	To make this easier to use , I am going to make a function that will clean up all the characters that occur at the beginning and end of the sentence . The function will do all the processing except for the contractions , special characters and small capital letters .
1422	World COVID-19 Model without China Data
1533	Let 's look at the distribution of winPlacePerc values per column .
1033	We can see that there are first 10 detection scores for each image in the test set .
655	SAVE MODEL TO DISK
1056	Train a NN with 9 neighbors
1529	Now let 's see the distribution of headshotKills
1448	Converting the object type to category
912	Let 's find all the above threshold variables which are not in the above set .
1474	Selecting the group with respect to experiment
816	Simple Feature Engineering
136	Number of unique values
1242	First , let 's take a look at the stores .
1405	Mel-Frequency Cepstral Coefficients ( VMA
1323	Area and Instance Levels
878	Next , we will add the ` set ` to our data , ignoring the index .
1484	Lung Nodules and Masses
773	Now we can calculate the minkowski distance between the pickup and dropoff coordinates , and also the euclidean distance .
1499	Understanding created date
1032	Lets look at the tensors and see what they look like
901	Let 's create a list of all the variables which are important for the analysis
877	Now we can merge the results with the random and opt datasets .
880	Scatter plot of scoring as function of Learning Rate and Estimators
703	Check for missing values in age and rez_esc
1349	Now we 'll be dealing with time series data that is interesting to us . Interestingly , one can think of time series data to be overdue ( e.g . x=30 , y=60 , x=90 and y=365 ) . We 'll be dealing with time series data that are not overdue ( e.g . x=180 and y=365 ) .
360	Let 's compute the importance of each feature based on the cross-validation scores .
1224	Since 'ps_calc ' features do not show any have zero relationship with other features
1216	Define dataset and model
902	Xây dựng mô hình
1528	DBNO - EDA
492	Visualize the visible layer
158	UpVote if this was helpful
204	Let 's import the necessary libraries .
1583	Extracting image and labels from data
440	UNDERSTANDING TARGET FEATURE meter_reading
998	Leakage Data
1221	Read the data
464	Data I/O
827	Model - LightGBM
430	Label Encoding the categorical variables
473	Loading the data
1039	For each sample , we take the predicted tensors of shape ( 107 , 5 ) or ( 130 , 5 ) and convert them to the long format ( i.e . $ 629 \times 107 , 5 $ or $ 3005 \times 130 ,
82	Plot the number of animals per outcome type and sex
1258	Get the pre trained model .
859	Boosting Type for Random Search
774	What about correlation with the Fare amount
686	And lastly , let 's see the result
528	We define the model parameters .
1336	I will use a random color generator to get a subset of data .
1098	We solve many of the tasks within the training set using our Neural Cellular Automata model ! I did test on the validation set as well , and it correctly solved 17 of the tasks . There are a number of ways this model could be improved . Please let me know if you 'd be interested in collaboration Solved Tasks
1154	In order to properly compare the trends in training data , we can now create a dictionary with store_id and trend .
958	And finally , create the submission file .
782	Random Forest
43	Understanding the Question Asker
991	Let 's now add the cylinderActor to the model . This will set the color to be BkgColor and reset the camera .
1211	card_id : Card identifier month_lag : month lag to reference date purchase_date : Purchase date authorized_flag : Y ' if approved , 'N ' if denied category_3 : anonymized category installments : number of installments of purchase category_1 : anonymized category merchant_category_id : Merchant category identifier ( anonymized subsector_id : Merchant category group identifier ( anonymized merchant_id : Merchant identifier ( anonymized purchase_amount : Normalized purchase amount city_id : City identifier ( anonymized state_id : State identifier ( an
831	Applying PCA with imputer
862	LGBM Classifier
1414	Checking for Null values
822	Merging Training and Testing Data
851	Combinations of Parameters
657	Read the data
22	Data Cleaning
1343	We can also see the distribution of count percent for integer features
431	There are two titles with duplicate questions . We need to remove them .
189	Top 10 categories of items with a price of 0
572	First day entry & last day reported
98	Stage 2 - Prediction
776	Being careful about memory management , which is critical when running the entire dataset .
804	Train the model
436	Multilabel classification
407	Now it 's time to compare the images returned by stage_2 to see if they are the same
1092	Let 's take a look at the feature importances of the model .
706	drop high correlation columns
1251	Run the model for 100 epochs .
493	How can you distinguish the visible layer from the hidden one
461	One hot encoding
14	Keras Embedding
767	Let 's see what happens with this data . For example , let 's see the 500 % ECDF .
1333	Concatenate both train and test data
1341	We also see the distribution of percentage of missing values for an application_train feature .
923	CNT_CHILDREN Variable
1429	Provice/State Modelling
715	First , let 's look at the correlations between the start and end points . There are some common correlations between the start and end points ( -19 , 20 ) and the end points ( -19 , 20 ) .
856	Write out the result to a csv file
1367	Let 's look at the data for the numeric features .
1016	Predicting with the best parameters
926	Prepare for data exploration
358	Read in the data
566	Predict on Test Set
1058	Plot of logloss on longitude and latitude
1305	Handling category columns
516	Some missing values in Train and Test Datasets
1288	Pearson correlation with Spearman
344	Plot of training and validation loss over epochs
1571	Average of page 's visits
1239	Data Exploration and Feature Engineering
811	Bayesian and Random Search
1286	Let 's split the data into train and validation sets .
662	Sort ordinal feature values
841	Joining credit_info features
326	Get the Padded Data
1266	Adam optimizer
1270	Predict for one iteration
438	Lets take a look at the head of the data
185	Mean price by category distribution
306	Below tokenization code logic is inspired by Abhishek 's PyTorch notebook [ here
216	Linear SVR for features selection
388	Lets look at the test set .
1549	The method for training is borrowed from
521	Set the threshold for our model to evaluate .
393	Importing the training data
40	The datasets can be distinguished quite good . Let 's take a look at the feature importances .
1586	Let 's remove data before 2012 ( optional
753	Limited tree visualization
141	Splitting Train and Test
1485	Lung Opacity vs Lung Nodules and Masses
390	Checking for duplicate categories
399	Prepare for data exploration
1553	Preparations
1050	To get a sample dataset , we 'll just select 4000 images to train the model .
847	Boosting type and subsample
1118	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so it is better to consider time-splitting . However this notebook is for simple tutorial , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
540	Bedrooms & bathrooms
932	SaltParser
29	Let 's also calculate the Gini coefficient for each image .
1421	World COVID-19 Model
1363	Let 's look at the distribution of numeric features for the 10 features .
1409	Null values
1543	Now we 've got the quaketimes for both train and test data . Quaketimes are defined as follows Quaketimes for train and test data . Quaketimes for train and test are defined as mean ( measurement value over 68 nts ) /mean ( statistical error in measurement value over 68 nts ) . These can be achieved by plotting two plots .
79	Submit predictions
4	Load train and test data .
1392	Let 's look at the distribution of values for the numeric features .
456	PandasのdataFrameをきれいに表示する
677	Scatter plot of full hits table
954	Setting the Paths
142	Find out which columns are categorical and continuous .
181	There are several ways to open the mask in two cells . We can use ` binary_openning ` from the ndimage library . Let 's open a mask with two cells only
948	Let 's check for the missing values again .
1021	Model initialization and fitting on train and valid sets
865	Running DFS with default parameters
434	Train and test split
1464	Read Train order
38	Let 's take a look at some images .
1381	Numeric features
899	As we can see , there are very few features with low information ( 0.9 ) . Let 's try the selection algorithm to remove these features .
400	Setting the MaskRCNN
1099	We solve many of the tasks in the training set using our Neural Cellular Automata model ! I did test on the validation set as well , and it correctly solved 17 of the tasks . There are a number of ways this model could be improved . Please let me know if you 'd be interested in collaboration Solved Tasks
658	Correlation
1283	From [ 4 ] ( [ Basic EDA Face Detection , we can easily make a function to read from each of the folders and create a data frame . Read from each frame into a new data frame . Sort the frame so we have the same order .
700	Check for missing values
161	The idea of Blend is taken from Paulo 's Kernel
561	Lets visualize one image
621	Ridge Regression
910	Những không xuất hiến không xuất hiến không xuất hiến không xuến không xuất hiến không xuất hiến không . Chúng ta cần loại bỏ nhất hiến
1374	Let 's look at the distribution of values for the numeric features .
520	Cross Validating the Classifier
936	Feature engineering
365	Now , let 's take a look at the dataset
364	Type_1 & Type
929	A minor detail to note is the difference between the `` += '' and `` append '' when it comes to Python lists . In many applications the two are interchangeable , but here they are not . If you are appending a list of lists to another list of lists , `` append '' will only append the first list ; you need to use `` += '' in order to join all of the lists at once . If you are appending a list of lists , `` append '' will only append the first list ; you need to use `` += '' in order to join all of the lists at once . If you do n't want to
1	Submissions will be evaluated on the roc_auc_score with the roc_auc_score calculated using the roc_auc_score module .
1520	Classification Report
1510	Create video
1394	Let 's check the target distribution for the numeric features .
950	Let 's check the column types again .
591	Word Cloud
338	AdaBoost
768	Latitude and Longitude Clean-up Looking into it , the range is ( 40 , 42 ) .
1293	Step 1 : Prepare for data analysis
845	Let 's try LightGBM .
777	Fitting the model
1214	CNN Model for multiclass classification
523	Since the y_decision_function_score is much higher than the threshold , let 's use it for prediction .
1340	We also see the distribution of percentage of missing values for an application_train feature .
99	Load libraries and data
499	Distribution of application features AUC
1164	We can now look at the class distribution in descending order to get a better sense of the most common label and count .
299	Training the LGBM model
1193	As our data is ready for preprocessing , we will open an image and resize it to the desired size .
93	Dropping the missing values
309	Let 's check how many images are in train and test folders .
789	Prepare time and features for model training
1438	More To Come . Stay Tuned .
755	There are two major formats of bounding boxes pascal_voc , which is [ x_min , y_min , x_max , y_max COCO , which is [ x_min , y_min , x_max COCO , which is [ x_min , y_min , width , height We 'll see how to perform image augmentations for both the formats . Let 's first start with pascal_voc format .
1280	Wikipedia агригированного проданного в штататего в штатататататегорованно
567	Data Cleaning & Visualization
723	We can create a new feature called 'inst/age ' and 'tech/v18q ' . Let 's create a new feature called 'tech ' and 'mobilephone ' .
1017	Plotting some random images to check how cleaning works
647	Using previous sucessful run 's model
336	baggingRegressor
1552	The toxicity is highly spread out across classes .
815	Boosting type
1276	Preparing the data for the competition
872	Selection for features with low information
1184	Part_1 : Exploratory Data Analysis ( EDA
286	Let 's add one more feature : ` commit_num ` , ` dropout_model ` , ` FVC_weight ` .
676	Learned how to import trackml from
1055	Loading the data
699	Which households do not have the same target value
479	Submission
892	Checking the distribution of Trends in Credit Sum
448	Now let 's apply log transformation to the square feet variable .
1311	Loading Json for training and testing
27	Listing the available files
813	ROC AUC
428	Train a model
1001	Load Model into TPU
772	Let 's load test data and check it
552	Now let 's do the same for all augmentations .
864	Next , let 's take a look at the aggregation type .
193	Coms length
1378	Let 's look at the distribution of values for the numeric features .
85	Converting the ages into numbers in the years
1108	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so it is better to consider time-splitting . However this notebook is for simple tutorial , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
871	Featuretools - Create top 100 features
74	Ensure determinism in the results
47	Target Variation ( log ( 1+target.values
968	Italy and China have slightly better correlation with China w/o Hubei . Italy and South Korea have slightly different correlation with China .
497	checking missing data in bureau_balance
435	Multilabeled questions
799	Baseline Model AUC
44	Create embeddings for training data
501	Heatmap showing correlation between features with high correlation value
207	Modelling with XGBoost
553	Let 's load the data .
1313	Checking for Null values
1204	Fitting a multi-model
55	Let 's also create a dataframe with the mean value of the missing values for the training set .
1355	Let 's look at the distribution of numeric features for the target .
779	Now we 'll do the same for the test set .
833	Aggregating the child variables
832	Ploting PCA values by Target
1470	LSTM
1165	TPU or GPU detection
209	Score with Linear Regression
511	Rescaling the Image Most image preprocessing functions want the image as grayscale . So here 's a function that rescales to the normal grayscale range .
1233	Backward Elimination
292	First of all , let 's pick a single commit .
649	Applying CRF seems to have smoothed the model output .
383	Setting up some basic model specs
1488	Lung Nodules and Masses
287	First of all , there are a lot of missing values . We might as well use these as a feature . Let 's use all of them for our prediction .
891	Simple Feature Engineering
102	Now we have a list of fake paths and a list of real paths
850	First of all , let 's create some dataframe for results
373	Random Forest
1009	First , I 'll save the model , so that we can reuse it later .
620	Lasso for Classification
660	Day of the week distribution
1425	Time series prediction for each country
319	Alright , let 's see the output file name .
343	Glimpse of Data
787	Fare Amount by Day of Week
1199	Now , we 've got a list of tuples ( dataX , dataY ) . For each tuple , we 've got a list of tuples ( x_true , y_true ) and a list ( y_true , y_false ) .
18	Load train and test data .
1558	To filter out stop words from our tokenized list of words , we can simply remove them from the list .
1057	Neural Network Model
1287	This kernel is for beginners only . Please upvote this kernel which motivates me to do more .
355	Linear SVR for features selection
1314	Replace EDA with float values
1441	Couting the length of train.csv
1408	Id is not unique Let 's check if the data contains any missing values .
788	Being careful about memory management , which is critical when running the entire dataset .
327	Linear Regression
1561	Putting all the preprocessing steps together
446	What is the distribution of meter reading for each primary_use
1243	Type and Size Distribution
271	Let 's see what happens when we select a single commit .
408	Image and Mask Viz .
1069	The Kaggle competition used the Cohen 's quadratic weighted kappa so I have that here to calculate the score .
1247	Analyzing FVC vs Weekly Sales
737	ExtraTrees Classifier
92	The distribution of data is highly imbalanced . Lets check the distribution of data over each class .
903	Target Correlation
284	Now we can see that there are no missing values ( ` FVC_weight ` =0.39 & ` FVC_weight ` =0.2 ) and ( ` LB_score ` = - 6 .
593	The most common words in the selected_text field is in positive data .
666	Applying the above function on the full data
1095	SN_filter
471	Merging transaction and identity dataset
342	Load the data
605	Let 's find a solution with fixed number of samples .
248	Get started with the Data Preparation
505	Get the data for target = 0 and target
1440	Let 's load some data
1210	merchant_id : Unique merchant identifier merchant_group_id : Merchant group ( anonymized merchant_category_id : Unique identifier for merchant category ( anonymized subsector_id : Merchant category group ( anonymized numerical_1 : anonymized measure numerical_2 : anonymized measure category_1 : anonymized category most_recent_sales_range : Range of revenue ( monetary units ) in last active month -- > A > B > C > D > E most_recent_purchases_range : Range of quantity of transactions in last active month -- >
1463	Converting the cities to xy_int format
951	Join the datasets , with the new merchant_card_id feature .
1139	Let 's visualize the augmented images
765	Does n't seem to be the same thing that I can see here , but the amount of cab rides are different . Maybe the amount of cab rides are different . Let 's try binning the fare
625	ignored_feat
1299	First , we 'll make all the numeric columns into integers .
80	Lets look at the data for sex , Neutered and Intact .
697	Now , let 's check if the family members all have the same target .
645	Now we have labels and a translation file ( unicode_translation.csv ) . Let 's check how many unique labels we have
900	Same as before , we need to align the train and test matrices . Lets do the same for the test set .
898	Running DFS with app_test features
151	Train-Test Split
603	Now let 's plot the public-private absolute difference
486	Now we have 6 features : best of times , worst of times , wisdom , foolishness
917	Reading POS_CASH_balance.csv
1373	Let 's look at the distribution of values for the numeric features .
198	Plotting the structure of the bulge graph
694	Loading and viewing data
710	Now , let 's create a new feature 'warning ' . The warning value is calculated as the sum of 'sanitario1 ' , 'elec ' , 'pisonotiene ' , 'abastaguano ' and 'cielorazo ' .
417	Load the training data , and create the features .
530	Data loading and inspection checks
702	Fixing missing values in v2a
268	Let 's try some models
1158	Train the model
879	It seems that the functions of Reg Lambda and Alpha are the same . Let 's plot the scores as functions of Reg Lambda and Alpha
1059	Function to load images and define helper functions .
1134	Loading Libraries
1335	Reading in the data
533	Hour of The Day Reorder Count
1116	Leak Data loading and concat
825	Now we can drop the unwanted columns .
1248	Plotting Weekly Sales vs Dept
395	Train Masks CSV
263	Prepare Training and Validation Sets
1202	As mentioned earlier , the model 's predict method outputs an array with shape ( -1 , 1 ) . However , it returns an array with shape ( -1 , 1 ) . We can then use the scaler.inverse_transform method to do that .
375	Prepare Training and Validation Sets
1004	First we need to evaluted on the LB
907	Bureau Balance Analysis
1047	Create folders for the training and test folders
504	Let 's check what data files are available .
738	Train the model and generate predictions
256	Prepare ` train.csv ` for data exploration
1567	Process the training , testing and 'other ' datasets , and then process the data .
