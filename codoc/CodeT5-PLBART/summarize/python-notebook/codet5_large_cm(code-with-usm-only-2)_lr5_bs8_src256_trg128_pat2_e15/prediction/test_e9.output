153	Let 's see the FB score for validation set
62	Distribution of Frauds and Non-Frauds
570	I 've added imports that will be used in the following sections .
832	PCA values by Target
1508	Select some features ( threshold is not optimized
1590	Preparing the data
1009	We will use keras.cnn_model.h5 for training
1354	Let 's look at the distribution of numeric features .
1034	Predicting on the Test Set
981	Lets display some of the bottom up image in a gif format .
634	Deaths and Confirmed Countries
108	TPU or GPU detection
291	First of all , we need to know how many different models do we have in our dataset . I 'm not sure how to calculate this value , but it 's definitely good . Let 's look at a single commit .
996	Replace the submission file with the correct site_id
1171	Now we 'll do the same for the rest of the sentences
752	Random Forest Classifier
577	Looking at China
830	Feature Importance and Forecasting
619	Linear Regression
1551	Let 's put it all together in a single data frame and convert it into numeric .
1360	Let 's look at the distribution of numeric features .
579	Reordered Cases by Day
1320	Concating public , planpri , noelec , coopele andenergcocinar features
1555	The number of words in the training data is the total number of words in the dataset . This is the same for the test set .
1297	Number of data points in each diagnosis
723	Most popular techniques are v18q - inst , age , mobilephone
1429	United States COVID-19 Prediction
1526	Let 's check this distribution .
489	Tokenization
1512	Import Packages and Data
709	From the plot below we see that the walls and floors are similar , but we can see a combination of floor and walls
708	Also , let 's see if there is a correlation between epared1 and epared
25	Submission
1138	Now we will add the .jpg extension .
930	Train the model
58	Load Data
1199	Now we 've got a list of tuples ( train_data , test_data ) . For this we need to split the dataset into train_data ( train_data ) and test_data ( test_data ) . We then create a new list of tuples ( train_data , test_data ) .
848	log 均匀分布
1018	Let 's load the data
247	Ensembles ( all ensembles
724	Let 's calculate the range of labels
1068	So now we can use our tokenizer to generate sequences from the test data
560	Transforming the bboxes dictionary into a dataframe
1562	Here we have utilised some subtle concepts from Object-Oriented Programming ( OOP ) . We have essentially inherited and subclassed the original Sklearn 's CountVectorizer class and overwritten the build_analyzer method by implementing the lemmatizer for each list in the raw text matrix .
563	And the final mask
876	Bayesian Optimization Result
905	One-hot encode categorical data
1575	Split the data into a train and a test set
1144	Most of the features are categorical ( ` category_3 ` ) , let 's convert them to category
1585	In the data file description , About this file This is just a sample of the market data . You should not use this data directly . Instead , call env.get_training_data ( ) from the twosigmanews package to get the full training sets in your Kernel . So you download directly below . I using DJ sterling kernel ( thnaks
1492	Loading the data
978	When I scroll to the top of the notebook I set ` _should_scroll ` to false so that the notebook does n't scroll too much
755	There are two major formats of bounding boxes pascal_voc , which is [ x_min , y_min , x_max , y_max COCO , which is [ x_min , y_min , width , height We 'll see how to perform image augmentations for both the formats . Let 's first start with pascal_voc format .
1455	Convert to a submission format
1065	Load the model and do a prediction
334	Prepare Training and Validation Sets
1046	Model
1217	We define the trainer and evaluator . We will use the same device as the supervised model
1040	Load and preprocess data
944	load mapping dictionaries
1173	Set global parameters
1509	Add leak to test
1091	We define the hyperparameters for the model .
36	Load OOF and submission data
1255	Pretrain models
824	We can extract some interesting features from Corr Matrix . I leave this out to the reader to find out some tricky features from Corr Matrix .
105	Pickle and Save
82	OutcomeType ` : EDA
650	Observations ConfirmedCases '' and `` Fatalities '' are now only informed for dates previous to The dataset includes all countries and dates , which is required for the lag/trend step Missing values for `` ConfirmedCases '' and `` Fatalities '' have been replaced by 0 , which may be dangerous if we do not remember it at the end of the process . However , since we will train only on dates previous to 2020-03-12 , this wo n't impact our prediction algorithm A new feature `` ConfirmedCases '' have been created , as a new feature `` ConfirmedCases '' is
694	Loading and Describing the Data
1581	Loading the dataset and basic visualization
1399	Numeric features
928	Comment Length Analysis
782	Random Forest
1487	Sample Patient 6 - Normal Pleural Effusion
1005	Define the network
919	Split into Training and Validation
556	Concatenate all text features together
1412	Logistic Regression
281	Let 's see what happens if we use all of these features to predict the relationship between CV and LB score
1531	Let 's see the distribution of kills
1365	Let 's look at the distribution of numeric features .
568	Using variance threshold from sklearn
1308	Let 's declare PATH variables
1096	Let 's see what happens if SN_filter is 1 .
148	Here we will take a look at one sample from the training set .
698	Let 's look at the households without a head .
1038	Build Model for Public Model and Private Model
316	Create test generator
11	Detect and Correct Outliers
192	Item Description
1564	Let 's extract the first 5 topics , which we will be using with the LDA model .
510	Digging into the Images When we get to running a pipeline , we will want to pull the scans out one at a time so that they can be processed . So here 's a function to return the nth image . In the unit test , I added a histogram of the values in the image .
1329	Load libraries
639	Setting up hyper-parameters
1140	Load Image Data
1576	This competition uses a simple autocorrelation model trained in [ this Kaggle competition ] ( The idea behind this competition is that , it predicts the probability that a patient is a fraudulent with a given set of patients . What I do next is to predict the probability that a patient is fraudulent with a given set of patients . What I do next is to predict the probability that a patient is fraudulent with a given set of patients . What I do next is to predict the probability that a patient is fraudulent with
1162	Number of each class
394	Category_count vs Image_count
267	AdaBoost Regressor
1435	The features we need to keep in mind that we do n't have too many features , but we do n't need them any more .
451	Dew Temperature
739	Finally , I 'll prepare the submission .
1584	Let 's split the ` filename ` into two columns and strip the ` host ` , ` cam ` and ` timestamp
1264	Let 's load the pre trained model .
1289	Feature importance with sklearn
1574	Times Series Forecasting
983	Preparing test data
1459	Lets split the data into positive , negative and neutral ones
1363	Let 's look at the distribution of numeric features for the target
228	Let 's see what happens if we select a single commit from this dataset .
359	tanh ( tanh
447	It seems we do n't have any NaN or Null value among the dataset we are trying to classify . Let 's check the correlation between features and target .
1417	Logistic Regression
511	Rescaling the Image Most image preprocessing functions want the image as grayscale . So here 's a function that rescales to the normal grayscale range .
1086	Submission
260	SGD Regressor
561	Exploratory Data Analysis ( EDA
487	To train Word2Vec , we use Keras pre-processing library .
803	boosting_typeの種類数据对比
839	Feature Engineering - Cash Data
70	How does the kernel speed up execution ? Does the kernel speed up execution ? As you can see , the kernel speeds up execution if you run out of memory , or if you run out of memory first . If you run out of memory , you 'll see that the kernel did n't speed up execution . If you run out of memory , you 'll get a glimpse at some of the things going on . If you run out of memory , you 'll see a glimpse at some of the things going on here too . Let 's see what we can do with this
174	Time vs. download rate evolution
880	Learning Rate and Estimators
1332	I 'll combine a lot of categories into one .
995	Submission
57	Let 's compute the total error using a weighted mean squared error .
528	d round : Train model with selected hyperparameters
580	Reordered cases by day
1234	Logistic Regression on Lv1 model
15	Padding sequences
509	Zone 1 ? Really Looking at this distribution I 'm left to wonder whether the stage 2 data might be substantially different , and certainly one might think real world data would be different . Zone 1 , the right arm , seems unlikely to be the population 's most likely zone to hide contraband . To begin with , you would have to put your contraband in place with your left hand . So unless most airline passengers that carry contraband are left handed , I am guessing that the real world ( and perhaps stage 2 ) will look different . Its just a guess of
1235	Predictions on LV2 features
1483	Lung Opacity for a random patient
984	Loading the required libraries
1467	The plot above clearly shows that the sales is uniformly distributed across categories .
306	Loading Tokenizer
72	Let 's check the shape of the training and test data .
1263	Pretrain models
652	Remove outliers with extreme values
498	Since we 're only interested in the total count , I 'll just do the same thing .
790	Linear Regression
1513	Let 's split the categorical and numerical features in the test set
16	Create a dataframe with prediction of all the questions
468	XGBOOST
1152	We need a few libraries
537	Use librosa.piptrack to calculate pitches and magnitudes
80	I 'll also convert sex from string to integer .
292	Let 's select a single commit from this dataset .
860	Simple Feature Import
1056	We start with a simple KNN classifier , which will serve as a baseline .
1401	Numeric features
1446	Let 's load some data
1540	Let 's look at the quality of missing data in the encoded data
190	Let 's check which products are shipping depending on the prices .
976	Looks like there 's a lot of metadata in this dicom file . Let 's try to extract the DICOM tag from the first file
1268	Training and Validation
1169	Analysing the occurrence of each catagory
1437	The next_click feature is only present in the train dataset as an int64 type , so we need to encode it with the next_click feature .
993	Vamos analisar alguns dos métodos e ver o que podemos extrair dos dados desta série temporal da competição Atrasos ( lags ) da série temporal está alinhado com sucesso asociados a série temporal está alinhado com sucesso asociados a série temporal está alinhado com seu valor no tempo . Chúng ta série
610	The number of convolutional layers is less than the number of input images . We will use a smaller kernel_size because we have more data .
417	Load the training data
1389	Numeric features
843	Let 's look at the feature importances of the model .
863	Dealing with the test set
325	Get back to building a CNN using Keras . Much better frameworks then others . You will enjoy for sure .
1022	First , we train in the subset , which is completely in English .
1572	Visit by day
865	Running DFS with default parameters
952	Prepare Dataset for Neural Network
542	Calculate maximal probability for each row_id
1448	Convert object data types
52	Logistic Regression
864	We can see that some of the numerical features are highly correlated with each other and we do n't know much about the number of unique values in a feature . Some of them are highly correlated with each other and we do n't know much about the number of unique values . Let 's take a look .
120	FVC Difference
323	Preparing the data
745	Confidence by Fold and Target
403	Find the indices for where the earthquakes occur , then plotting may be performed in the region around failure .
731	Cross Validation F1 Score
725	We 're all there is a lot of columns with high cardinality , so we 'll need to change this one more time .
872	Remove low information features from feature matrix
1122	How does our days distributed like ? Lets apply @ ghostskipper 's visualization ( to out solution .
233	Let 's see what happens if we select a single commit from this dataset .
1420	China
664	Applying one-hot encoding
244	Let 's see what happens if we select a single commit from this dataset .
1214	CNN Model for multiclass classification
551	Gaussian Target Noise
1057	Now we will predict the validation data using the neural network and compare it with the output .
1206	Let 's take a look at the mean price of rooms .
1125	From this plot we can see that addr1 and addr2 are the same . I think that addr1 and addr2 are the same . I think that addr1 and addr2 are the same . But let 's fix it .
513	Masking the Region of Interest Using the slice lists from above , getting a set of masked images for a given threat zone is straight forward . The same note applies here as in the 4x4 visualization above , I used a cv2.resize to get around a size constraint ( see above ) , therefore the images are quite blurry at this resolution . Note that the blurriness only applies to the unit test and visualization . The data returned by this function is at full resolution .
931	Applying CRF seems to have smoothed the model output .
1591	In this section , we will aggregate the news features
1377	Let 's look at the distribution of numeric features .
425	Pretty cool , no Anyway , when you want to use the grayscale library , remember to first convert the tensor into a numpy array .
707	Distribution of heads area1 - area
1439	Now we will read in the data
132	To make this easier to use , I am going to make a function that clean up all the characters we have in the text and do the same thing .
1059	Function for loading image data
1013	Convolution with windowing
899	To remove features from the feature matrix , you can use the selection library to remove features .
114	Shallow copy data
1283	Function for reading data frame
1315	Replace edjefa with float values
999	Session level CV score and user level CV score
167	IP of the IP address
1280	Wikipedia агригированногированногированногированногированногированноги
977	SeriesUIDs
209	Score with Linear Regression
769	Nyc Map Zoom
989	Bkg Color
909	Merging Bureau Data
158	UpVote if this was helpful
1479	Let 's build our model - we will use the Quadratic Weighted Kappa Learner ( also called Learner
593	The most common words in positive_train data frame is
1014	Let 's look at the distribution of game time per installation id .
388	Now , let 's see some of the training data
1502	LOAD PROCESSED TRAINING DATA FROM DISK
319	We 'll create a new column called 'file_name ' to hold the image data .
1195	Most common of the annotators ( toxicity_annotator_count
100	Now we are going to split our data into train and validation sets . Since we are going to use a random forest , we need to create a random subset of the data which we will not use later .
78	Freezing and finding the optimal learning rate
1050	We will load a sample dataset with 4000 images to visualize .
202	Our values currently range from -1024 to around 2000 . Anything above 400 is not interesting to us , as these are simply bones with different radiodensity . A commonly used set of thresholds in the LUNA16 competition to normalize between are -1000 and 400 . Here 's some code you can use
1516	Let 's take a look at the distributions of features ` v2a1 ` and ` v2a11 ` .
400	Load Data and Simple EDA
669	The top n ingredients in the dataset is 100 . Let 's look at the most common ingredients in a list
861	Fit the model on the training set with the different labels and compute the hyperparameters .
1201	Fitting the model
1080	Unfortunately , this will not work . Let 's try to remove images that are no longer blurry .
734	Apply model to test set and labels
530	Loading the data
42	Model Training with Spearman
974	We can see that there are no missing values in the training data . So , to get a better understanding of these values , let 's print them .
418	Get the best number of clusters from the test features
780	We will fit the model on the training data and evaluate it on the test set .
268	Voting Regressor
35	Load packages
541	Common parameters for the model
1115	Fast data loading
965	Shap values and feature importance
1335	Reading in the data
1473	Model
1061	filtered_sub_df ` contains all of the images with at least one mask . ` null_sub_df ` contains all the images with exactly 4 missing masks .
1561	Putting all the preprocessing steps together
1245	Size , Weekly Sales
1480	Quadratic Weighted Kappa
1510	Create video file
1113	A one idea how we can use LV usefull is blending . We probably can find best blending method without LB probing and it 's means we can save our submission .
1260	F1 score for validation set
1036	Again , thank you to [ Xhlulu ] ( notebook [ here ] ( for providing this submission-formatting code
349	Now it 's time to create a generator object
1136	Exploratory Data Analysis
313	Submissions into the competition are [ evaluated on the area under the ROC curve ] ( between the predicted probability and the observed target . Since we have a limited number of submissions per day , implementing a metric for the ROC AUC ( which is non-standard in the fast.ai v1 library ) allows us to calculate the area under the ROC curve .
1533	Well , that does not seem very useful now . Let 's try to see the distribution of winPlacePerc values .
432	Word Cloud for each tag
1402	Load libraries
1290	Baseline XGBoost and LGBM
621	Performing Ridge Regression
369	SVR on train and test
478	Loading the data
126	Now we can plot some of the images and their Hounsfield Units ( HU ) and the frequencies
1504	LOAD DATASET FROM DISK
256	Drop unwanted columns
1373	Let 's look at the distribution of numeric features for the target
1361	Let 's look at the distribution of numeric features .
526	Step.6 Model Training and Evaluation Framework to check Dimentionality Reduction
1528	DBNO - EDA
951	Let 's join the datasets with the new merchant_card_id feature .
151	Train-Test Split
1116	Leak Data loading and concat
1391	Numeric features
414	Computing histogram
405	We can visualize the images returned by stage_1_PIL and stage_1_cv2 on average .
964	Looking at ` returnsClosePrevRaw10 ` and ` returnsOpenPrevMktres10 ` distribution
1578	This is the main metric for the competition
263	Prepare Training and Validation Sets
853	Grid search was successful on the test set .
962	Shap interaction values
1456	Import libraries
1111	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
764	Let 's check the distribution of fare_amount
1078	Data Augmentation
76	CNN with Tensorflow - One hot encoding
842	In this section , we will prepare the data .
378	ExtraTreesRegressor
1093	var_19 ( 0 ) , var ( 1 ) , var ( 2 ) , var ( 3 ) , var ( 4 ) , var ( 5 ) , var ( 6 ) , var ( 7 ) , var ( 8 ) , var ( 9 ) , var ( 11 ) , var ( 12 ) , var ( 12 ) , var ( 13 ) , var ( 14 ) , var ( 15 ) , var ( 15 ) , var ( 16 ) , var ( 17 ) , var ( 18 ) , var ( 17 ) , var ( 18 ) .
1458	Add start and end positions
1398	Let 's look at the distribution of target for numeric features
1284	Let 's see what our model scores looks like .
1203	Logistic Regression
701	Before we look at these values , let 's see how many of these values correspond to each head .
1418	This dataset contains images of individual hand-written [ Bengali characters Bengali characters ( graphemes ) are written by combining three components : a grapheme_root vowel_diacritic , and consonant_diacritic . Your challenge is to classify each image into three components : a grapheme_root vowel_diacritic , and a grapheme_diacritic .
484	Now let 's vectorize our text
1250	Mixing up with batch_mixup
385	Run MpBuild in parallel
1313	Checking for Null values
933	Feature Engineering - Feature Engineering
1568	Now to read our data
823	One hot encoder
789	Define the time features
21	Let 's look at the distribution of muggy-smalt-axolotl-pembus values
956	Let us have a look at a random validation index
286	Let 's look at one of the most interesting feature of our dataset . We start with a simple one : commit_num , dropout_model , FVC_weight .
1334	Cleaning Id 's
1126	Submission Here we will divide the train data by category to get an idea of how much each category is distributed .
1500	Importing the Libraries
104	Detect Face In this frame
396	Let 's fix the missing values in the test set .
1453	Importing the training and testing data
846	Hyperparameters search for optimal hyperparameters
605	Let 's find a solution if we can improve on public LB score
1177	take a look of .dcm extension
1114	Find Best Weight
1557	Let 's tokenize the first text to get a list of words in it
392	Level 2 the most frequent category
377	Bagging Model
37	Let 's now look at the distributions of various `` features
598	We can see that the evaluation metric for this competition ( CV ) is the weighted mean ( AUC ) of the ROC AUC . Since we are performing perfectly on the test set , we want to calculate the Gini coefficient .
41	Loading and preparing data
1305	Converting categorical variables
578	Italy - European countries
678	We see that some particles have high number of hits and other particles have very high momenta . Let 's visualize some of the particles with high number of hits and other particles .
1087	In my way to learning more about OpenCV , I 've tried a couple of ideas on the sample images to extract skin marks and liked to share them . The idea behind this kernel is that we are working with a dataset of pixel values that we can use for training . First , we import some libraries first .
737	ExtraTrees Classifier
1464	Read in the sol order
892	Trends in Credit Sum
488	Hashing the text
223	Let 's see what happens if we select a single commit from this dataset .
1530	killPlace Variable
1444	Let 's do it with chunks of train data
826	SK_ID_CURR has some missing values , let 's try to remove them
904	Analyzing Categorical Data
434	We will split the training data into training and testing sets .
680	Importing necessary libraries
1550	Import Libraries
670	Categories of items less than 10 \u20BD ( top
1135	How does our days distributed like ? Lets apply @ ghostskipper 's visualization ( to out solution .
850	Let 's create some dataframe for results
1032	Taking a look at the tensors
348	Now it 's time to create a generator object
502	Applicatoin train merge
371	SGD Regressor
245	Picking the best commit based on LB score
424	BanglaLekha Confusion Matrix
1044	For each sample , we take the predicted tensors of shape ( 107 , 5 ) or ( 130 , 5 ) , and convert them to the long format ( i.e . $ 629 \times 107 , 5 $ or $ 3005 \times 130 ,
480	Import Libraries and Data
771	Fare Value by Number of passengers
1495	Function to create a description of a program
1406	Loading Necessary Libraries
1252	Label Encoding the Sexo data
217	Importing the Libraries
1414	Checking for Null values
1158	Train logreg model
1326	Create categorical features list
673	Now let 's check the coefficient of variation for prices in different categories .
1344	DAYS_BIRTH - EDA
841	Merging credit_info features
1207	Ploting product category of owner and investment
960	So we have a significant number of features ( public test ) and a significant number of features ( private test ) . Let 's split the data into train and test sets .
365	Almost no difference
161	The idea of Blend is taken from Paulo 's Kernel
43	Understanding the Question Asker intent understanding
1131	Label encode the categorical variables
1357	Numeric features
546	yearbuilt yearbuilt.parcelid : The ID of the parcel that was built for the year
942	Let 's aggregate the features on the Bureau_balance dataset .
914	I recommend initially setting MAX_ROUNDS fairly high and using OPTIMIZE_ROUNDS to get an idea of the appropriate number of rounds ( which , in my judgment , should be close to the maximum value of best_ntree_limit among all folds , maybe even a bit higher if your model is adequately regularized ... or alternatively , you could set verbose=True and look at the details to try to find a number of rounds that works well for all folds ) . Then I would turn off OPTIMIZE_ROUNDS and set MAX_RO
648	Train the model
1107	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross-validation by cross
881	Number of Estimators vs Learning Rate
1142	Training and Evaluating the model
527	Data Preparation
1546	SAVE DATASET TO DISK
754	Nonlimited Random Forest
856	For recording our result of hyperopt
9	Imputations and Data Transformation
492	CNN with Tensorflow - Defined visible layer
762	Submission
1346	We can see the distribution by target = 0 with repay ( 0 ) and not repay ( 1 ) .
39	Let 's now add some non-image features . We can start with sex , and one-hot encode it .
1112	Leak Validation for public kernels ( not used leak data
1241	The unique value of Store and Type is
397	Mark each sample as in_train and in_test .
1041	trials table
692	Combinations of TTA
232	Let 's see what happens if we select one commit from this dataset .
675	Coefficient of variation ( CV ) for prices in different recognized image categories
1063	Histogram of predictions
493	CNN
1	Submissions into the competition are [ evaluated on the ROC AUC score ] ( between the training and test sets . Since the evaluation metric used in the competition is [ LogLoss ] ( and roc_auc_score , we only need the roc_auc_score during training .
757	Loading the data
690	Let 's create a function that will get the patient data from a DICOM file .
382	Let 's get started
891	Time features , feature names and aggregate primitives
221	Let 's see what happens if we select a single commit from this dataset .
1372	Numeric features
533	Reorder Count
1172	Total number of tokens and unique tokens
897	We will use the ft.dfs method to generate the feature matrix , with the keys being the entity name , and the values being the sum of the numerical features . We will use these parameters to calculate the feature sums .
972	DICOM files can be read and processed easily with pydicom package . DICOM files allow to store metadata along with pixel data inside them . Reading a dicom file creates a pydicom.dataset.FileDataset object .
464	Data Section 1.1.2 MData Files
1425	We can now perform prediction for all the countries .
1582	Let 's take a look at the sample data .
309	Let 's check how many images are in train and test folders .
807	For recording our result of hyperopt
499	Exploratory Data Analysis ( AUC
1338	We also have some missing values in ` application_train ` and ` application_object_na_filled ` . Let 's check more .
1451	Let 's visualize the ratio of click hour to is_attributed
326	Get the Padded Data
1419	Preparing the full training data
750	Confusion Matrix
1155	Import Library & Load Data
628	Let 's see the total number of bookings per day .
917	Reading POS_CASH_balance data
410	Test Duplicates
1488	Lung Nodules and Masses
312	Preparing data
184	Top 10 categories of the products
182	To be able to feed the mask into the MaskRCNN , we need to encode them into RLE .
227	Let 's see what happens if we look at one of the most popular feature in our dataset .
554	Factorize the categorical variables
967	Logistic Growth Curve for confirmed and deaths
1163	Now , let 's check which labels are not in the training set .
135	The first thing we can do is to get the state and country from which the data is sold .
1336	I will use a random color generator to get a subset of features
1445	Let 's load the training data
430	Label Encoding the categorical variables
604	So far we are doing exactly the same thing that we have been doing for the public LB score . Let 's make a copy of the submission that we had correct LB score .
677	Scatter plot of full hits table
1194	Spliting the training data into train and validation sets
398	The install is a bit slow - do n't worry .
963	Go to top Dependence plot of ` returnsClosePrevRaw10_lag_3_mean ` and ` returnsClosePrevRaw10_lag_4_mean ` distribution
1543	That 's the same for both plots . Quaketimes and signals are similar , but very close together . We can confirm this by plotting both plots and see the difference between the quaketimes .
1015	Title Mode Analysis
1118	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
573	In the next section , we 'll create a new data frame called 'active ' which is the sum of confirmed , deaths and recoveries .
1474	If we take this into account , we 'll want to select the group we want to predict from the test set .
1276	Preparing data for Neural Network
1282	We will also need a plot function to plot the predictions and the actual values of the model .
45	Let 's plot a histogram of the target values .
440	UNDERSTANDING TARGET FEATURE meter_reading
162	Pushout + Median Stacking
629	Let 's see the total number of bookings per day
1129	UpVote if this was helpful
742	Random Forest Classifier
549	Room Count Vs Log Error
459	a ) Street ( for any thoroughfare b ) Road ( for any thoroughfare c ) Way ( for major roads - also appropriate for pedestrian routes d ) Avenue ( for residential roads e ) Drive ( for residential roads f ) Gardens ( for residential roads g ) Lane ( for residential roads g ) Lane ( for residential roads h ) Gardens ( for residential roads ) subject to there being no confusion with any local open space i ) Place ( for a crescent shaped
1274	Feature Engineering - Bureau Data
524	Precision and Recall
522	Making a function to generate the classification report
834	Merging Bureau_info features
462	Scaling the lat and long
1469	Melting sales of items
406	Okay , now it 's time to do this . We 're going to use a cv2 library with this , but it 's not as easy as to use it , but it 's a good idea to use it .
1560	Vectorizing Raw Text
869	Features Sample
1524	It is very important to keep the same order of ids as in the sample submission The competition metric relies only on the order of recods ignoring IDs .
101	Let 's see the number of samples we have
341	Making a function to calculate the IoU .
1314	Replace edjefe with float
649	Applying CRF seems to have smoothed the model output .
343	Taking a look at the data sizes
756	We 've our image ready , let 's create an array of bounding boxes for all the wheat heads in the above image and the array of labels ( we 've only 2 class here : wheat head and background ) . As all bounding boxes are of same class , labels array will contain only 1 's .
1411	We will now apply one-hot encoding to each column .
507	Reducing the sample
77	Training the Model
1491	Patient 6 - Normal - Unclear Abnormality
548	Bathroom Count Vs Log Error
458	Make a new column Intersection ID + City name
1511	Lets take a look at one of the patients .
1069	The Kaggle competition used the Cohen 's quadratically weighted kappa so I have that here to compare .
1580	I formulate this task as an extractive question answering problem , such as SQuAD . Given a question and context , the model is trained to find the answer spans in the context . Therefore , I use sentiment as question , text as context , selected_text as answer . Question : sentiment Context : text Answer : selected_text
331	Decision Tree
686	And lastly , let 's see the drawing with 9000052667981386 in the test set .
454	Before we go any further , we need to deal with pesky categorical variables . A machine learning model unfortunately can not deal with categorical variables ( except for some models such as [ LightGBM ] ( Encoding Variables ) . Therefore , we have to find a way to encode ( represent ) these variables as numbers before handing them off to the model . There are two main ways to carry out this process You can see [ this Label Encoding Label encoding assigns each unique value to a different integer . This approach assumes an ordering of the categories : `` Never '' ( 0 ) < `` Rarely '' ( 1 ) < ``
1579	Plot the evaluation metrics over epochs
1379	Let 's look at the distribution of numeric features .
1506	The method for training is borrowed from
60	Let 's look into the existstrun graph
150	Create Testing Generator
375	Prepare Training and Validation Sets
503	Exploratory Data Analysis
353	Creating an Entity Set and loading the data
130	The following function counts the number of words in each sentence
666	Concatenate OH_full and retain_full
508	Next I collect the constants . You 'll need to replace the various file name reference constants with a path to your corresponding folder structure .
642	filtering out outliers
1272	Let 's compute the number of repetitions for each class ( if > 0 ) and sort them in descending order
242	Let 's select a single commit from this dataframe .
536	Mel-Frequency Cepstral Coefficients ( MFCCs
747	For recording our result of hyperopt
1548	Two embedding matrices have been used . Glove , and paragram . The mean of the two is used as the final embedding matrix
950	Numerical features , categorical int features and column types
855	Random Search The best model with random search scores is determined from the parameters found in the random_results dataframe .
827	Feature Importance
1037	Training History
1375	Let 's look at the distribution of numeric features .
659	Variable Correlations ( EDA
520	Cross-validation on logreg and SGD
565	Reading in the test data
472	Splitting training data into train and validation sets
697	Now , let 's check if the family members all have the same target .
1277	Create a Random Forest Classifier
455	Predicting Chunks
1124	From this plot we can see that addr1 and addr2 are identical . It 's the same for addr1 and addr2 , but it 's different for addr3 .
1424	We can observe that Kingdom , Russia and Singapore and New Zealand are the most common countries . Let 's check what we can do with these .
1257	Load the data
1534	Dumbest Path : Go in the order of magnitude : 0 , 1/2 , 2/3 , ....
596	We will now explore the class distribution
1154	Let 's sort the trends into a dictionary
283	Here we can see that LB score is much worse than LB score , but it is much worse than FVC_weight . Let 's see if that 's the case
265	Bagging Model
730	I 'm going to use a pipeline consisting of two methods : imputer , scaling and normalizing . The first are the features , the second is the scale .
741	drop high correlation columns
1553	Importing the required libraries
485	Now let 's try to build a vectorizer using TfidfVectorizer . I have used [ sklearn.feature_extraction.text.TfidfVectorizer ] ( here .
144	Undersample categorical variables
941	Reading in data
30	Submission
1494	To make this easier to use , we 'll define a lifted function as a function
1558	To filter out stop words from our tokenized list of words , we can simply use a list comprehension as follows
1287	This kernel is for beginners only . Please upvote this kernel which motivates me to do more .
338	AdaBoost Regressor
282	Let 's see what happens if we select one commit from our dataset .
600	We need a threshold of 0.30 % for our test set to make sure that it matches the public LB .
1092	Feature importance We can use Seaborn to plot the feature importances .
198	How does the structure look like
1470	B keras를 사용한 NN 모델 개발
175	Now we will read in all the data . First we create a dataframe with only the first 5 rows .
1306	Split the data into train , validation and test sets
1588	Assets without assetName in the training set
751	Build UMAP , PCA , FastICA
616	SVR
29	Let 's calculate the AUC and Gini for each image
1390	Let 's look at the percentages of the target for numeric features
1387	Let 's look at the distribution of numeric features .
632	Check the distribution of ` Demanda_uni_equil_sum ` distribution
1251	Batch Grid Mask
627	Let 's see the total number of bookings per year .
954	Setting the paths
1292	The first thing we can do is to get the minimum FVC for each patient in the test set . We then create a new column called Base_FVC for each patient in the test set . We then merge that into the base dataset .
470	Not looking to seriously compete in this competition so figured I 'd at least share the small experiments I was toying with . This kernel serves as just a starter to look at how to handle the various numerical and categorical variables in a NN with Keras .
273	Let 's see what happens if we select a single commit from this dataset .
51	Let 's take a log histogram of the train counts
347	Make a submission file .
866	Running DFS with default parameters
1028	First , we train in the subset , which is completely in English .
1215	Inference on Test Set
540	Bedrooms and bathrooms
169	Let 's check the quantile values by IP .
356	Embeded Random Forest
787	Fare Amount by Day of Week
94	Let 's look at some of the most common words in our dataset
1571	Average of page 's visits
1103	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross-validation by cross
87	Load libraries
679	Due to Kaggle 's disk space restrictions , we will only extract a few images to classify here . Keep in mind that the pretrained models take almost 650 MB disk space .
702	Exploring missing data in v2a
255	Andorra
900	Why do we need To do it , we need to align the train and test labels with the second feature matrix
191	There 're a lot of descripations in the train set . Let 's see how many of the items have no a description .
925	AMT_INCOME_TOTAL - EDA
1359	Let 's look at the distribution of numeric features .
186	First level of categories
1058	logloss on longitude and latitude
253	Germany
831	Apply PCA with imputer
1098	We solve many of the tasks within the training set using our Neural Cellular Automata model ! I did test on the validation set as well , and it correctly solved 17 of the tasks . There are a number of ways this model could be improved . Please let me know if you 'd be interested in collaboration Solved Tasks
685	Target values are numbers . Let 's check the distribution of the target values .
775	Define Linear Regression
714	Now , let 's see how correated are the features .
380	Voting Regressor
1278	Straight away we can see that the predictions are very innacurate , this could be due to the fact that there are so many 0 values and that there is a lot of noise and barely any signal . One thing to note is that even using SARIMA , the model did not detect any weekly seasonality . The very sparse data points can lead to the predictions going so off . Some modifications can be made , such as introduce Fourier Terms , denoising and maybe even using the top down method so as to get overall trends etc will help .
1426	Creating a dataframe for the country stats
379	AdaBoost Regressor
147	Set a learning rate annealer
10	Impute any values will significantly affect the RMSE score for test set . So Imputations have been excluded
246	Load and preprocess data
118	Data exploration
726	Dimension reduction .
74	Ensure determinism in the results
314	BanglaLekha Classification Report
1427	Let 's check the predictions for each province/state .
1331	I 'll add a new category for each category name .
1271	The following function shows the most common examples in the training dataset .
718	Difference between Porrs and scorrs
1351	Group Battery by Internal Battery Type
1440	Let 's load some data
443	Looking at the distribution of meter readings per primary_use
1006	Train the model
435	Multilabeled questions
1465	Before going further it is important that the previous visitStartTime is sorted by fullVisitorId , so let 's add one new feature - visitStartTime . The previous visitStartTime is sorted by fullVisitorId , so let 's group by fullVisitorId and add 1 's .
1045	Model Summary
107	Now we can work with the ` before.pbz ` file , and save it for later use .
1151	var_91 ` で数据对比
138	month_temperature month_temperature
767	The plot above clearly shows that our data is pretty unbalanced . The number of samples in a sample is less than the total number of samples in the dataset . If we take a logarithmic scale of the data , we wo n't see much difference .
384	Define high-pass and low-pass filter functions
1477	Set the seed
1348	Applicatoin train shape after merge
1396	Numeric features
1298	Categorical and numerical
651	Remove rows with -1s
1428	Fulltable Us Counties Data
796	Fare Prediction on Test Set
1408	Id is not unique Let 's check if the data contains any missing values .
1248	Dept & Weekly Sales
1100	Now we iterate over the tasks and predictions , plotting each sample if the input_output_shape is the same .
923	CNT_CHILDREN ` - child count
1537	Now let 's normalize all these features to have the same number of values .
390	Checking for duplicate categories
383	Setting the hyperparameters
615	Sanity Check for missing values
633	Understanding the Data
490	A Fully connected model
329	Modelling with Linear SVR
849	Logistic Regression Let 's see if there are any values between 0.005 and 0.05 ( or 0.5 ) .
1484	Lung Nodules and Masses
134	In the next section we will use train_df , y , test_df for the training process , and the transformed_x for the test set . Let 's free up some memory
1515	Household Type ( 4 = NonVulnerable , 3 = Moderate , 2 = Vulnerable , 1 = Extereme Poverty
1208	feature_3 has 1 when feautre_1 high than
294	LB score
1438	In this Section , I import necessary modules .
79	Submit to Kaggle
518	Let 's create a class to represent the base classifier .
736	KNN with n_neighbors
320	New feature : binary_target
419	Decision Tree Classifier
1218	Compute and Display Validation Results
1097	Comparing the train and test data for a given sample_struc
1237	Logistic Regression
783	The first thing we can do is to predict the fare amount by random forest on the test set .
1547	Let 's have a look at the first 100 lines of the file
744	Macro F1 metric
943	Cred Card Balance Feature Engineering
1226	We can use this function to rank the predictions according to the Kaggle 's evaluation metric
937	Due to memory constraints , I will split the data into train and test part and remove the target column from the data frame . I will also remove the target column from the data frame and create a list of features that is good for training .
587	Preparing the data
65	Let 's prepare the data which will be used for training .
1228	Logistic Regression
1023	Now that we have pretty much saturated our model on english only data , we train it one more epoch on the validation set , which is significantly smaller but contains a mixture of diffferent languages .
1472	Plate Grouping by Sirna
1368	Numeric features
1083	Getting Test Data
740	Let 's repeat the same process for RF .
211	Importing the required libraries
412	d4d34af4f7 - d
1072	Importing important libraries
123	Observation : From the above plot we observe that Pulmonary Condition Progression by Sex is greater than the Smoking Status of the Patient . We observe that Pulmonary Condition Progression by Sex is less than the Smoking Status of the Patient .
219	Let 's see what happens if we select a single commit from this dataset .
1230	In the next sections of code , we will use the cross_validate_xgb function to compare the predictions of xgboost with the original parameters .
763	Let 's load a sample train data
798	Create model and train set
1522	Instead of 0.5 , one can adjust the values of the threshold for each class individually to boost the score . However , it should be done for each model individually .
674	Load image labels
1339	We also see the distribution of percentages for each feature type in application_train and application_object
1587	Highest volumes of the assets
1209	card_id : Card identifier month_lag : month lag to reference date purchase_date : Purchase date authorized_flag : Y ' if approved , 'N ' if denied category_3 : anonymized category installments : number of installments of purchase category_1 : anonymized category merchant_category_id : Merchant category identifier ( anonymized subsector_id : Merchant category group identifier ( anonymized merchant_id : Merchant identifier ( anonymized purchase_amount : Normalized purchase amount city_id : City identifier ( anonymized state_id : State identifier ( an
833	Numeric aggregation and categorical aggregation
500	Correlations of Features
1353	As light GBM outperformed linear regression , let 's set some initial parameters for the algorithm . One note is that , when categorical features are specifically specified , light GBM works better . I am going to treat some of the numeric features as categorical features . Here is how more categorical features work better in light gbm implementation LightGBM offers good accuracy with categorical features .
297	Import Library & Load Data
1529	headshotKills Variable
467	Time taken for the experiment
706	drop high correlation columns
529	Convolutional Neural Network
366	Computing histogram
395	Splitting the data into train and validation set
1055	Loading data
1150	Train vs Test Data
1567	Let 's load the data and get the labels
111	Preparing the data
713	Extracting feature from all the training data
1011	As you can see , the width of the image is equal to the total number of pixels in the training image . If that 's not the case , the image width will be equal to the total number of pixels . To do that , we need to get an image width that is equal to the total number of pixels . To do that , we need to pad the image to be equal to the total number of pixels .
1407	Let 's get our data
835	Previous Application Data
1073	Import libraries and data
1405	Mel-Frequency Cepstral Coefficients ( VMA
788	Train Validation Split
235	Let 's see what happens if we select one of the most popular feature from this dataset .
1117	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross-validation by cross
332	Random Forest
982	Let 's check if we have any improvement in our validation set .
302	Checking Best Feature for Final Model
438	Lets take a look at the first 3 rows of the dataset
429	Let 's plot a few of the parameters and see what happens .
229	Let 's see what happens if we select a single commit from this dataset .
236	Let 's see what happens if we select one of the most important feature from this dataset .
735	Linear Discriminant Analysis
1324	Concating lugar features into train and test sets
213	In this section I will sample a subset of the data with 5000 images from the training set .
911	Let 's identify variables with more than 90 % correlation , which have more than 90 % correlation
421	BanglaLekha Confusion Matrix
1033	detect_scores ( 各个赛季随时间随时间随时间随时监控
238	Let 's see what happens if we select a single commit from this dataset .
393	Importing the training data
1325	Let 's examine which of these variables have only one value .
1514	Data Visualization
583	Reordered Cases in USA
631	Now we can merge the products into a single data frame
1293	Step 1 : Prepare the data analysis
31	Checking for the optimal K in Kmeans Clustering
696	A look at ` dependency ` , ` edjefa ` and ` no
1269	Create the model
922	Let 's visualize this result
207	One more step and it really is time to create the XGBoost model . We need to create an XGBoost model that uses XGBoost .
810	Saving the trials as json file
969	Loading the data
759	Fix -inf , +inf and NaN
602	Private Difference vs Public Difference
1333	Concatenate both train and test data
501	Những biến này không xuất hiện trong tập test đánh giá mức đánh giá mức đánh giá mức đánh giá mức được xác biến .
1434	We split the data into training and testing sets
1244	Type and Weekly Sales
806	Hyperopt 提供了记录结果的工具，但是我们自己记录，可以方便实时监控
372	Decision Tree
1181	A preprocessing step is to preprocess an image and resize it to the desired size
1409	Null values
1193	A preprocessing step is to preprocess an image and resize it to the format that we will be working with .
276	Feature Engineering : EDA and Feature Importance
310	Train Labels
728	Target and Female Head of Household
1070	Then , to identify an object in an image using the ARC solver the input image should be an array with shape ( batch_size , H , W ) . In this case , the function returns an array with shape ( batch_size , H , W ) . In this case , the input image is a numpy array .
1229	Bernoulli Naive Bayes
753	Up to 3 months ' worth of tree analysis
1392	Let 's look at the distribution of numeric features .
1410	We will use groups of features , ps_ind_01 ' , 'ps_ind_03 ' , 'ps_ind_14 ' , 'ps_ind_15 ' , 'ps_calc_03 ' , 'ps_calc_07 ' , 'ps_calc_08 ' , 'ps_calc_12 ' , 'ps_calc_13 ' , 'ps_calc_14 ' , 'ps_calc_13 ' , 'ps_car_13 ' , 'ps_car_15 ' ,
1256	Let 's create an iterator using the JSONL files .
878	Next , we will add the ` set ` to it .
308	Word Cloud plot
179	Exploratory Data Analysis ( EDA
687	Let 's split the ID into two columns and check the shape .
1085	Clear model and GPU memory
469	BanglaLekha Some Prediction
705	Let 's look at the heads of the household
1004	We will load the partition data and save it in the real dataset .
477	Build and re-install LightGBM with GPU support
258	SVR on train and test
1415	Looking at the distribution of hair and bone length for each type
1288	Pearson correlation between the macro columns
275	First of all , there are n't a lot of 'commit_num ' that we need , but we need to know how many 'dropout_model ' and 'FVC_weight ' . Let 's see what happens when we use only one model .
609	Prepare the model
1266	Adam optimizer
1202	As mentioned earlier , the model 's predict method always returns the predicted probabilities of the target . So we can use the scaler.inverse_transform method to do the same for the test data .
452	Wind Speed
898	Running DFS with app_test features
1383	Let 's look at the distribution of numeric features .
822	Merging Bureau and previous features
867	Let 's create a feature matrix and a feature names matrix
216	Linear SVR on features
800	log 均匀分布
1421	Exploratory Data Analysis
611	Embedding Datasetup
817	Baseline Model ( LGBM
1196	Annotators and comments
317	Load the best weights and make predictions on the test set
1233	Modelling with Random Forest
1026	Build datasets objects
1261	Create submission file
56	Percentile of zeros in training set
49	Prepare the columns to use for training
613	Plot the evaluation metrics over epochs
936	Feature engineering
781	Heatmap showing correlation between variables
1042	Save best hyperparameters
1527	How many assists are there in the dataset
1489	Increased Vascular Markings + Enlarged Heart
1106	Leak Data loading and concat
637	Now we need to create a new column called 'lag_1 ' and 'lag_2 ' for each row . We need to create a new column called 'lag_2 ' and 'lag_3 ' when it does n't exist , and create a new column called 'lag_4 ' .
857	Lets evaluate the hyperparameters
1309	Load the model
799	Baseline Model AUC
361	On the right hand , let 's see what we can do with our prediction and see what our prediction looks like .
852	Here we find the best score with the best hyperparameters .
171	Here we can see the download ratio by click and category of clicker .
555	We need to realize the data
110	By looking above graph we can say that Learning Rate changes continuously in Time Based Approach of Schedulling Learning Rate .
1213	I think the way we perform split is important . Just performing random split may n't make sense for two reasons
494	Once connected , we define a Model object and specify the visible layer ( 2 ) and output ( 2 ) . The visible layer will be the second layer of the network .
761	As this is a multi class classification problem . Lets try Random Forest Classifier algorithm .
154	Save the model
545	Correlation between the top features
571	Datasets used in the competition
90	Here we read in the training text file and take a look at the first 5 rows of the training data .
748	Saving the trials as json file
1109	Fast data loading
1053	Create test generator
601	Plot of public vs private score vs samples spoiled
1388	Let 's look at the distribution of numeric features .
137	Unique values and NAN values
428	Train model
143	Fixing random state
360	Let 's prepare our predictions . We define the split with 5 folds .
535	Mel-Frequency Cepstral Coefficients ( MFCCs
1281	Helper function
1012	Pad and Resize Images
155	Clear the output
1364	Numeric features
301	Ahora que tengan menos en vamos a filtrarlas quedandonos con las columnas que tengançamos , por un lado , comportamiento de datos
665	Simple imputer
160	How fraudent transactions is distributed
1249	Train the model
886	Exploratory Data Analysis
581	Reordered Spain Cases by Day
279	First of all , there are a lot of 'n_commits ' that we need to fix . Let 's fix that .
1090	Reduce Validation Set
1330	Missing Values
55	Let 's start by having a look at the mean value of the missing values for the training set .
1067	Simplified NQ Test
805	Hyperopt Tpe Algorithm
1190	Mel-Frequency Cepstral Coefficients ( MFCCs
959	Loading data
1064	Function for loading image data
1457	Ensure determinism in the results
98	Stage 1 Solution
1342	We also see the distribution of percentages for each feature type in application_train and application_object
1227	Drop some columns
525	Lets see the mean squared error on test and prediction .
210	Feature Score visualization
194	VS price vs coms_length
159	The data comes from two separate sites Hot Pepper Gourmet ( hpg ) : similar to Yelp , here users can search restaurants and also make a reservation online AirREGI / Restaurant Board ( air ) : similar to Square , a reservation control and cash register system
344	Plot the training and validation loss over epochs
1294	To use DICOM files , we need to create a directory where the converted images will be stored . If the directory does n't exist it will be created .
1460	Prepare test data
259	Modelling with Linear SVR
1493	In this notebook , I will use the ` abstraction_and_reasoning_challenge ` dataset as the training dataset .
840	Credit Card Balance
1223	Applying binary encoding for categorical features
504	Let 's declare PATH variables
204	Importing Libraries
12	Preparing text for Neural Network
1066	Now we will split our data to train and validation data , so as to train and validate our model before submitting it to the competition . We will define a BATCH_SIZE of 8 to make the model 32 samples per iteration .
614	Let 's load the datasets
269	Part 2 . Feature Engineering
543	Import Necessary Libraries
482	Importing the Librosa libraries
1031	Visualizing the result as an image
979	Let 's get a random patient
342	Load the data
1463	Converting the cities to xy_int format
431	Checking for Duplicates
156	Clear the output
208	MinMax Scaling the data
437	Importing the necessary libraries
597	Perfect submission looks like : it does n't look like a normal distribution . The target vector does n't look like a normal distribution .
1019	Load Train , Validation and Test data
1168	Import modules we would be frequently using . We would import many more along the way as they are needed .
641	Get the Data ( Collect / Obtain Loading all libs and necessary datasets
682	Shape
1583	Extracting image data and labels
1035	Load the data
1157	Make a new DF with just the wins and losses
786	Fare amount by Hour of Day
717	Most correlations
882	Number of Estimators vs Learning Rate
1222	Let 's encode the categorical features using the freq_encoding function
1236	Predictions for XGBoost with LV
399	This kernel features The Killers ] ( The-Killers The Runners ] ( The-Runners The Drivers ] ( The-Drivers The Swimmers ] ( The-Swimmers The Healers ] ( The-Healers Solos , Duos and Squads ] ( Solos , -Duos-and-Squads Correlation ] ( Pearson-correlation Feature Engineering ] ( Feature-Engineering
439	Meter Type
307	As a starter , we 'll use a simple learning rate of 3e-5 . We 'll use a simple learning rate of 3e-5 as the learning rate ( i.e .
1569	Plotting error bars
1541	Feature Matrix Encoding
88	A simple kernel that uses a Keras model trained in my local system .
448	The ` log1p ` transformation was applied to the square_feet variable . We can see the distribution after applying log tranformation .
778	Baseline Model ( baseline
1366	Let 's look at the distribution of numeric features .
96	In this notebook , we will load the dataframes ` train_variants_train.csv ` and ` train_text_train.csv ` into one DataFrame .
1481	Predict the test set
547	Bedroom Count Vs Log Error
1187	Ok , now we just need to do the same thing for the test set .
178	We can see that there are 2 prominent peaks . The count of pixels with intensity values around 0 is extrememly high ( 250000 ) . We would expect this to occur as the nuclei cover a smaller portion of the picture as compared to the nuclei cover a smaller portion of the nuclei . The number of pixels with intensity values around 0 is extrememly high ( 250000 ) . We would expect this to occur as the nuclei cover a smaller portion of the nuclei as the nuclei cover a smaller portion of the nuclei . The nuclei cover a
125	Let 's take a look at one of the patients ' data
1146	From the above mask we can see that there are some nuclei that are present in the dataset . We will create a new ImageSegment with the original data .
945	extract different column types
53	Let 's take a log histogram of the training data and see if it looks like this distribution
63	Let 's group data by isFraud and D1minusday
844	Feature Engineering and RNN
620	Define the function to perform Lasso
181	There are two cells that belong to one label and we have to open a new one . We can use the ndimage.binary_opening method to do the same thing with the mask
188	Top 10 brands by product
1184	Trying Triplet Loss image.png ] ( attachment : image.png Inspired by
146	See sample image
539	Interest level ( low , medium , high
1545	Importing data
241	Let 's see what happens if we select a single commit from this dataset .
1296	Plot the evaluation metrics over epochs
1185	Reading in the data
1340	Lets look at the distribution of features by type of object
949	merchant_id_cat & merchant_id_num aggregate
4	Load train and test data .
38	Let 's take a look at a few images .
935	Using all feature engineering
479	Submission
994	Now let 's have a look at the DICOM files
1001	Load Model into TPU
1054	filtered_sub_df ` contains all of the images with at least one mask . ` null_sub_df ` contains all the images with exactly 4 missing masks .
127	As a final preprocessing step , it is advisory to calculate the Lung volume , which is defined by the Slice Thickness and Pixel Spacing . In this case , we are going to use the patient 's Slice Thickness ( 0 ) to calculate the volume , and the pixel spacing ( 0 ) to calculate the volume . It is defined by the slice thickness and the pixel spacing [ 0 ] to calculate the volume .
727	Final features analysis
932	We initialize the parser and load the data
1394	Numeric features
591	Word Cloud
1212	Make a Baseline model
889	Extracting features from bureau
1430	Importing the necessary libraries
572	First and last day entry and last day reported
820	This kernel is specifically is for Beginners who want 's to experiment building CNN using Keras . It makes building deep neural networks very easy .
197	We 'll use neato to visualize the sound .
22	Data Cleaning
187	Let 's see the prices of the first level of categories
606	Importing Necessary Packages
719	Correlation matrix ( heatmap
656	Import Library & Load Data
990	We use the cylinder mapper , the property of the Actor ( this is the one we use for making the cylinder ) . We use the rotated version of the cylinder actor as follows
1312	Augmented Dataset
407	We can see the differences between the initial image and the final stage_2 image , and we see the differences between the initial image and the final stage_2 image .
957	Test Predictions on Test Set
157	Import libraries Back to Table of Contents ] ( toc
712	Let 's check the distribution of the heads .
220	Let 's see what happens if we select a single commit .
768	Latitude and Longitude Clean-up Looking closer
66	Prepare training data
1105	Fast data loading
801	boosting_type为goss，subsample就只能为1，所以要把两个起设定
243	Let 's see what happens if we select one of the most popular feature from this dataset .
1010	Saving model
457	Intersection Number
1139	To show the augmented images
813	ROC AUC vs Iteration
1021	Model initialization and fitting on train and val
142	Validate the categorical and continuous variables
201	Please note that when you apply this , to save the new spacing ! Due to rounding this may be slightly off from the desired spacing ( above script picks the best possible spacing with rounding ) . Let 's resample our patient 's pixels to an isomorphic resolution of 1 by 1 mm .
1242	A stores file can be found with the following format : { 'Type ' : 'File ' , 'Size ' : 0 } . We 'll first look at the stores file by type , then we 'll see how the sizes are
20	Let 's see the distribution of muggy-smalt-axolotl-pembus values
1300	We can see that we have columns with values between -32767 and 256 . We can also see columns with values between -32768 and 32767 .
218	Create DL Models
300	Checking Best Feature for Final Model
497	checking missing data in bureau_balance
819	Baseline Model ( CV
647	Using previous sucessful run 's model
131	Specail characters and punctuations
1542	Time to failure vs . acoustic data
1127	Model and Feature Importance
918	Credit Card Balance
357	Import Libraries and Data
729	Modelling part We use Random Forest Classifier , metric , make_scorer
222	Let 's see what happens if we select a specific commit .
599	Gini on random submission
1517	let 's see the distributions of age vs meaneduc for different target
483	Now let 's vectorize the text
224	Let 's see what happens if we select one of the most popular feature from this dataset .
1225	Drop calc columns
318	Let 's prepare a submission file .
1081	Display Blurry samples
337	ExtraTreesRegressor
603	Now let 's plot the public-private absolute difference
287	Let 's see what happens if we use all of these features for one prediction . We 'll just use the first 16 models .
1468	store_id , total_sales , cat_id
1039	For each sample , we take the predicted tensors of shape ( 107 , 5 ) or ( 130 , 5 ) , and convert them to the long format ( i.e . $ 629 \times 107 , 5 $ or $ 3005 \times 130 ,
1416	So it seems we do n't have any useful information about color , so we need to remove it from our dataset .
1211	card_id : Card identifier month_lag : month lag to reference date purchase_date : Purchase date authorized_flag : Y ' if approved , 'N ' if denied category_3 : anonymized category installments : number of installments of purchase category_1 : anonymized category merchant_category_id : Merchant category identifier ( anonymized subsector_id : Merchant category group identifier ( anonymized merchant_id : Merchant identifier ( anonymized purchase_amount : Normalized purchase amount city_id : City identifier ( anonymized state_id : State identifier ( an
859	Boosting Type for Random Search
1544	Let us learn on a simple example
2	Let 's build a simple Ftrl model .
250	As you see , the process is really fast . An example of some of the lag/trend columns for Spain
140	Encode the labels
1441	Couting the length of train.csv
804	Train the model
906	Feature Engineering - Bureau Balance
389	Now , let 's look at the most common levels of images per category .
212	Loading data
67	Import Required Libraries
1573	Let 's split the data into predictions and predictions
173	To get the number of clicks over the day , I will resample the time to an hour and look at the number of clicks over the day .
1433	Model Training Strategy
446	What is the distribution of meter reading for each primary_use
1258	Let 's load the pre trained model .
1371	Let 's look at the distribution of numeric features .
1345	We can see the distribution by target = 0 with repay ( 0 ) and not by target
700	Check for missing data
1197	First , let 's see which words are closest to the target = 0 .
122	Pulmonary Condition Progression by Sex
625	ignored_feat1 = [ 'FEATURE_10 ' , 'FEATURE_257 ' , 'FEATURE_245 ' , 'FEATURE_246 ' , 'FEATURE_249 ' , 'FEATURE_256 ' ] .
163	MinMax + Mean Stacking
401	Load the data , this takes a while . There are over 629 million data rows .
1095	SN_filter
760	First of all , we will calculate the distance between the training and validation set and we will use the cross_val model to predict the target .
1454	In this section we will cluster the hits , stds , filters , and phik , and use the score_event_fast method to calculate the score .
791	Let 's plot the feature importance .
505	metadata_trainの第d_1天の画像を計算されています
261	Decision Tree
1134	Loading Dependencies and Dataset
327	Linear Regression
1299	Interesting , all numerical features are numeric ( -1 ) . Let 's check if all numerical features are numeric .
1243	Type and Size
1141	Take a look at the training dataset and try to build a baseline model using Efficient Det
1047	Making sure the folders exist .
662	Sort ordinal feature values
910	Những biến này không xuất hiện trong tập test là do có một số tương quan cho các biến không xảy ra đủ các khả năng .
1254	Importing the Libraries
773	Now let 's calculate the Minkowski Distance for the test set .
1071	Here is an example of how to solve an ARC problem using a Random Forest Regressor . We will solve an ARC problem using a Random Forest Regressor . We then take a look at the input and output pairs , and plot the pairs .
1174	Finally , let 's add PAD to each sequence ...
373	Random Forest
672	Let 's check the distribution of price variance within parent categories and price
168	How many clicks do we have in each category
1025	Load Train , Validation and Test data
575	According to the [ COVID-19 : COVID-19 : COVID-19 was inspired by July 1 ] ( COVID-19 was inspired by the [ COVID-19 : COVID-19 was inspired by July 1 ] ( COVID-19 was inspired by the [ COVID-19 ] ( COVID-19 but not-COVID-19 but-COVID-19 was inspired by the [ COVID-19 : COVID-19 ] ( COVID-19 was inspired by the
99	Load Necessary Packages
386	We 're going to use the build_rnd_idxs ( ) function to split the data into train and test ids and run the build on them .
1403	Mel-Frequency Cepstral Coefficients ( Mel-Frequency Cepstral Coefficients
1221	Loading the data
1532	Let 's check how these variables correlates to each other
948	Lastly , let 's check for the missing values .
1007	Train the model with the training data and validation data
1355	Numeric features
1559	Lemmatization to the rescue
1436	Let 's see the distribution of Minute 's time
885	Relationship between Target and SK_ID_CURR
884	High Correlation Heatmap
1192	Looking at the data
1496	Function to evaluate images with ships .
722	age vs escolari
564	Time for Submission
1291	We see that we have mo_ye ( 可以看到 item_id ) and mo_ye ( 商品与销售的关系是明显的，这也符合业务常识 month
280	Let 's see what happens if we select a single commit from this dataset .
277	Let 's see what happens when we start from one of the most popular LB models .
1337	We also see the distribution of percentages for the application_train and object type 4 .
13	Parameters and load training data
248	XGBoost Model Importance
765	Does n't seem to be a particular feature or a value of nan , but it can be interesting to see if we can deal with it better .
1101	Fast data loading
1176	Let 's plot a heatmap of the link count .
129	Let us check the memory usage of our dataset
1148	Load train and test data
660	Day distribution
1132	Missing value for V319_V320 & V319_V321
1239	Data Exploration and Feature Engineering
95	Word Distribution Over Whole Text
1350	Checking for Null values
567	Preprocessing ( use clean data
517	Converting nans to the log scale
14	Tokenize Text
46	Let 's look at the distribution of log 1+Target values .
1400	Numeric features
661	Nominal variables
1539	Prepare data for Keras
97	Load test data
612	CNN.jpg CNN.jpg
1317	Create a list of new features , one for each family_size features .
887	Ordinal Variable Types
474	GPU Parameters
34	identity_hate
1232	In the next section we will use the cross_validate_lgb to make our predictions .
608	Limit the number of features we need to predict in each sentence
345	Finally , we generate the predictions on the test set .
958	Make a submission
1179	Number of Patients and Images in Test Images Folder
1536	Ok , so it does n't look like there 's a lot of NaN values in the dataset . I 'm going to replace them with ` np.nan ` . This is because there are n't a lot of NaN values in the dataset . I 'll replace them with ` np.nan ` .
821	Let 's load raw data
1431	Age with the gender and hospital death
968	Trend of curve for Hubei vs China
84	Mix of outcome type
868	Variable Correlations
1349	Generate a new column for overdue status
816	Simple Feature Import
738	Train the model with the train and validation labels
6	Target Variable Analysis
1570	Import Necessary Libraries
322	Spliting the train and validation set
103	Median Absolute Deviation
770	Yeah , there is a slight difference between Latitude and Longitude . There is a slight difference between Latitude and Longitude . There is a slight difference between Absolute latitude and Longitude . There is a slight difference between Absolute latitude and Longitude .
907	Bureau Balance Analysis
1566	It turned out that stacking is much worse than blending on LB .
1370	Numeric features
324	Cohen 's Quadratic Weighted Kappa
1182	Spliting the data into train and validation sets
119	Expected FVC distribution
893	Running DFS with interesting features The following function takes an entityset as an input and outputs a list of interesting features . We 'll use the entityset as the input for the dfs function .
1565	These are the functions we will be using to calculate Hilbert and Hann ( or convolution ) .
374	Train a simple XGBoost model
237	Let 's extract some features from this dataset .
915	Top 100 Features from the bureau data
1563	Corpus - Document - Word : Topic Generation In LDA , the modelling process revolves around three things : the text corpus , its collection of documents , D and the words W in the documents . Therefore the algorithm attempts to uncover K topics from this corpus via the following way ( illustrated by the diagram Three_Level Bayesian Model Model each topic , $ \kappa $ via a Dirichlet prior distribution given by $ \beta_ { k Model each document d by another Dirichlet distribution parameterized by $ \beta_ { k Model each document d by another Dirichlet distribution
623	VT-squared threshold is a measure of how much variance the model has on the test set . This can be done using the perform_variance_threshold method from Scikit-Learn .
684	Number of binary features
7	Let 's plot now the distribution of feature_1 values .
715	Most common correlations between ( -19 , 20 ) and ( 0 , 1 ) .
185	Mean price by category distribution
784	Now we will extract some date features from test data
71	Setting the parse_dates and dropoff_datetime
924	Very interesting values . It seems that some of the features are highly correlated . Lets look at the distribution of data .
362	Now it 's time to make our predictions and see what our submission looks like .
566	The test set has 10 audio files
315	The model performs very poorly on uncommon classes . The boxes are imprecise : the input has a very low resolution ( one pixel is 40x40cm in the real world ! ) , and we arbitrarily threshold the predictions and fit boxes around these boxes . As we evaluate with IoUs between 0.4 and 0.75 , we can expect that to hurt the score . The model is barely converged - we could train for longer . We only use LIDAR data , and we only use one lidar sweep . We compress the height dimension into only 3 channels . We assume
195	t-SNE with cervix indicators
456	PandasのdataFrameをきれいに表示する関数
1362	Let 's look at the distribution of numeric features .
92	Class Distribution Over Entry
1367	Let 's look at the distribution of numeric features .
183	Looking at the data
1175	I will now explore the links and nodes count .
133	Let 's free up some memory
966	Confirmed Growth Rate Percentage
1321	Now we 'll multiply each categorical feature by its average value
1552	Only ~10 % of the total comments have some sort of toxicity in them . There are certain comments ( 20 ) that are marked as all of the above Which tags go together Now let 's have a look at how often the tags occur together . A good indicator of that would be a correlation plot .
284	Here we can see that LB score is much worse than LB score . Let 's see what happens if we use all of them .
657	Read the data
68	Importing the initial data
48	Let 's convert the target variable to a log scale .
436	Multilabel Classifier
1538	Feature Matrix Generation and Feature Definitions
809	Running the optimizer
630	We can see that ` hotel_cluster ` is associated with one of the most popular clusterings . Let 's try to aggregate on the day of the week
743	Feature Selection Scores
460	turn direction The cardinal directions can be expressed using the following equation frac { \theta } { \pi Where $ \theta $ is the angle between the we want to encode direction and the north direction measured clockwise
355	Linear SVR on features
333	Train a simple XGBoost model
1422	World COVID-19 Model with China Data
1520	Classification Report
1423	Hong Kong , Hubei ...
590	This notebook uses a Convolutional Neural Network ( CNN ) to classify Kannada digits , from 1 through 9 .
141	Splitting Train and Test
193	Description of the Items
442	Monthly Reading ARE HIGHEST CHANGES BASED ON BUILDING TYPE
854	Let 's generate some random parameters from the grid
785	Fare Amount versus Time Since Start of Records
746	Let 's generate predictions for the baseline model .
5	Let 's look at the distribution of the target variable
336	Bagging Model
278	Let 's see what happens if we select one commit from this dataset .
1486	Consolidations vs Ground-Glass Opacities
44	Generate embeddings for training data
676	Learned how to import trackml from
1167	Model & Training
653	Before going further it is important to predict whether the model is good or not . In this case , we need to predict whether the model is good or not . In this case , we need to predict whether the model is good or not .
296	We 're going to use the following features : lgb_num_leaves_max , lgb_in_leaf , xgb_max_depth , xgb_min_child_weight 75 , w_lgb , w_xgb
476	Merging transaction and identity dataset
980	Let 's take a look at the first DICOM file
594	The most common words in negative_train data is
588	Run SIR with bounds
532	Across Days Of The Week
391	Most of the level3 categories have more than 10 unique values
1316	Create continuous features list
971	We use the same dataframes for both train and validation data
409	Checking for Duplicates
903	Target Correlation
838	Preparing POS_CASH_balance data
1498	Let 's see if a model program exists in the training data
531	Hour of The Day Order
879	Reg Lambda and Alpha
149	Prepare Testing Data
858	Let 's start with altair
538	Bathrooms - Interest level
1352	Now we need to remove the features that we do n't need .
870	Examine and plot spec feature importances
495	Exploratory Data Analysis
862	LGBM Classifier Algorithm
671	Categories of items > 1M \u20BD ( top
240	Let 's see what happens if we select a single commit from this dataset .
815	Boosting Type
50	Let 's plot a histogram of the train data .
1490	Sample Patient 6 - Normal - Unclear Abnormality
668	Top n Labels
444	Distribution of meter reading values across the weeks
851	Now let 's check how many combinations we have in our grid
471	Merging transaction and identity dataset
733	Build Adversarial Classifier
172	We can see that there 's a lot of missing values for ` attributed_time ` and ` click_time ` . Let 's try having a look at the missing values
811	Bayesian Tuning and Random Search
553	Read the data
1589	For feature engineering , I will perform feature engineering on numerical variables .
1246	Weekly Sales vs Store
453	Converting year_built to uint8
1089	This is a collection of scripts which can be useful for this and next competitions , as I think . There is an example of baseline at the end of this notebook .
921	Splitting the data into train and validation datasets
463	Let 's check now the new train and test data files .
33	I will also define a vectorizer for words and characters . I will also define a vectorizer for characters and words like `` gardening '' , `` gardener '' .
1003	Fake data directory
145	Prepare the data
370	Modelling with Linear SVR
1216	Define dataset and model
890	Bureau Balance Value
473	Loading the data
226	Let 's see what happens if we select one commit from this dataset .
1024	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
433	Frequency of top 20 tags
1180	Looking at the data
515	Normalize and Zero Center With the data cropped , we can normalize and zero center . Note that more work needs to be done to confirm a reasonable pixel mean for the zero center .
1267	Results of the Ckpt Exploration
28	Let 's create a histogram of the train counts
139	Split 'ord
1447	Convert categorical data types to category
1507	Add train leak
321	The first 100 values are the binary targets . Let 's print the first 100 values of the dataframes .
102	Now we have a list of real paths and a list of fake paths . We need to randomly sample from them so we can feed them to the model .
812	ROC AUC Score
902	Xây dựng hình
24	Here we will use the CountVectorizer from sklearn
1482	Sample Patient 1 Normal Image
703	Looking at age and rez_esc for missing values
1307	One way to reduce over-fitting is to grow our trees less deeply . We do this by specifying ( with min_samples_leaf ) that we require some minimum number of rows in every leaf node . This has two benefits There are less decision rules for each leaf node ; simpler models should generalize better . The predictions are made by averaging more rows in the leaf node , resulting in less volatility . max_features We can also increase the amount of variation amongst the trees by using a different sample of rows for each leaf node , resulting in less volatility . min_samples_leaf
1000	TPU Strategy and other configs
643	using outliers column as feature
1285	List Squared
3	Data Prepparation
112	Compile and fit model
298	Prepare Training Data
845	Let 's try LightGBM and see how well it works
164	MinMax + Median Stacking
955	Split dataset into train and validation sets
523	y_decision_function_pred
991	For the cylinder , we can add a single Actor to the model . This will set the background color to be the BkgColor and the camera to be reset .
1191	Split into train and validation sets
1471	This notebook uses display_aspect_ratio metadata field as a great fake video predictor .
465	MNCAATourney & MRegular Season Detailed Results
1110	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross-validation by cross
1449	Let 's see the distribution of ip in the train set
1075	Splitting the data into train and test sets
795	Set the number of jobs and fit the model on the training data and evaluate the model on the validation data
1156	First , we 'll simplify the datasets to remove the columns we wo n't be using and convert the seedings to the needed format ( stripping the regional abbreviation in front of the seed ) .
1017	Plotting some random images
1119	SexuponOutcome
877	BanglaLekha Score
1240	Revenue based on month
1188	In the whole training set , we 've got the images of some patients in the train set that we 're going to use . Let 's do the same for the sub data
1133	There are several things that I think are important . In this section , I think it is possible to add more features like android browser , webview , android webview , generic/Android 7.0 and generic/Android
1048	Replace id_code with _s1 and _s
1219	Update learning rate
1077	Let 's create a random permutation
249	Implementing the SIR model
794	Fitting the model to the tune data
584	Initial Loading the data
655	SAVE MODEL TO A FILE AND TRANSFORMER
927	Import the Data
23	Vectorize
829	Remove features with less than 95 % importance
346	Create Prediction dataframe
1450	Device detection
165	Now we will read in all the data . First we create a dataframe with only the first 5 rows .
617	RandomForestRegressor
1270	Predicting on test set
1347	Non-LIVing area ( or mode ) per multi-feature
304	Build Model
411	Let 's split the data into train and test sets .
814	Boosting Type
947	The goal of this kernel is to sort the input files in the same order as the competition 's submission_dir
461	One-hot encode the city name
825	Finally , we can drop the columns we want to keep .
1525	In this notebook , I will show you how to preprocess and preprocess both train and test data .
466	Function for reading in an image file and getting image paths
1082	Submission
1535	We can use the following function to calculate the distance matrix
285	Here we can see that LB score is much worse than LB score , but it looks like LB score is much worse than FVC_weight . Let 's see what happens if we use FVC_weight = 0 .
1358	Let 's look at the distribution of numeric features .
514	Cropping the Images Using the crop lists from above , getting a set of cropped images for a given threat zone is also straight forward . The same note applies here as in the 4x4 visualization above , I used a cv2.resize to get around a size constraint ( see above ) , therefore the images are quite blurry at this resolution . If you do not face this size constraint , drop the resize . Note that the blurriness only applies the unit test and visualization . The data returned by this function is at full resolution .
1374	Let 's look at the distribution of numeric features .
1002	The original fake paths
264	Model Training with RidgeCV
1442	Wow , that does not look stationary . Lets create some random lines .
422	Random Forest
929	A minor detail to note is the difference between the `` += '' and `` append '' when it comes to Python lists . In many applications the two are interchangeable , but here they are not . If you are appending a list of lists to another list of lists , `` append '' will only append the first list ; you need to use `` += '' in order to join all of the lists at once . If you are appending a list of lists to another list of lists , `` append '' will only append the first list ; you need to use `` += '' in order to join all of the lists at once .
998	Leakage analysis is one of the most important features of site1 and site3 . Leakage analysis is one of the most important features of site1 and site3 . All buildings have the same meter reading , but only a few of them have timestamp .
1143	We can see here that we have several columns with numeric values . In the next section we will take a random sample of unique values for each column . We will also see all unique values except for columns with numeric values . As we can see below that there are many unique values , there are also many unique values . For columns with numeric values , there are also many unique values . Let 's check them out .
266	ExtraTreesRegressor
624	Inference and Submission
1475	Cropping with an amount of boundary
1076	CNN with Tensorflow - Converting to categorical
544	Let see what type of data is present in the data set .
1328	Predicting on test and output
1586	Let 's remove data before 2012 ( optional
251	Let 's try to see results when training with a single country Spain
230	Let 's see what happens if we select a single commit from this dataset .
667	Predicting on Test Set
290	First of all , we need to know how many different models will we have in our dataset . I 'm not sure how to calculate this value , but it 's definitely good . Let 's see what happens when we use all of these features . We 'll fill in the missing values with the mean .
1149	Collecting date features from var
836	The features used for this competition are : 'DAYS_ENTRY_PAYMENT ' , 'DAYS_INSTALMENT ' and 'LATE ' .
214	Creating an Entity Set and loading the data
113	calendar.csv - Contains information about the dates on which the products are sold . sales_train_validation.csv - Contains the historical daily unit sales data per product and store [ d_1 - d sample_submission.csv - The correct format for submissions .
367	Helper functions
699	Now , let 's check all the households with the same target .
946	adapted from
589	How many predictions do we have
426	Import libraries and helper functions
985	Now let 's apply log transformation .
166	How many values are in the training data
1253	Let 's check the distribution of data in train/test set
54	Let 's check the distribution of the nonzero counts in the test set .
913	Removing Correlations
420	BanglaLekha Confusion Matrix
27	Data Prepparation
1386	Let 's look at the distribution of numeric features .
381	Part 2 . Feature Engineering
1161	Sample 10,000 samples from the training set
1186	Let 's create an array of images to feed to the model .
1301	Load test data
1311	Let 's empty the data file for training and testing .
1503	SAVE DATASET TO DISK
1577	Univariate Feature Engineering
352	We will sample 10,000 rows from the training set .
920	Inference
721	Education Distribution by Target
252	Italy
351	Loading data
330	SGD Regressor
427	Credits and comments on changes
354	High Correlation Matrix
0	Target Variable
1592	Remove columns with type ` object ` .
704	Let 's see what we got
622	Feature Accuracies
177	Brightness Manipulation
1259	Create valid predictions
1079	Let 's look at a single image
592	Let 's create three separate dataframes for positive , neutral and negative sentiments . This will help in analyzing the text statistics separately for separate polarities .
640	The next step is to fill the missing values of the predicted set with the Quadratic Weighted Kappa score . We will use the permutation of the predicted set
758	Lets check the distribution of surface ( target ) value in the label dataset
206	I recommend initially setting MAX_ROUNDS fairly high and using OPTIMIZE_ROUNDS to get an idea of the appropriate number of rounds ( which , in my judgment , should be close to the maximum value of the optimized set , maybe even a bit higher if your model is adequately regularized
402	Lets check the test files . This verifies that they all contain 150,000 samples as expected .
558	We take a look at the masks file , and read their summary information
940	Aggregate the features
1343	Feature percentages for integer features
654	Sample 100 records from the training set .
895	Late Payment Feature
1318	Feeling columns with missing values : 0 , 1 , 2 , 3 .
916	We are using a typical data science stack : `` numpy `` , `` pandas `` , `` matplotlib `` .
797	I 've added imports that will be used in training too
81	Which animals do we have
271	Let 's see what happens if we select a single commit from this dataset .
481	Fit the Model
1452	Some functions that might be useful .
585	Well , that 's all right . Let 's divide by day and by population
1074	Let 's load the pretrain weights , which will be used later
311	Let 's sample the training data in train and test set ( with SAMPLE_SIZE ) . In this case , we will sample a subset of the training data in ( 0 , 1 ) .
1310	Get the Data ( Collect / Obtain Loading all libs and necessary datasets
136	Number of unique values
557	Lets take a look at the data sizes
239	Let 's see what happens if we select one of the most popular feature from this dataset . I 'm not sure how to use it , but it can be interesting . Let 's look at a few of the most popular feature .
1485	Lung Opacity vs Lung Nodules and Masses
1120	Spayed and Intact Female Outcome
1049	Pad and Resize Images in Train and Test
926	Let 's get started
1501	Ensure determinism in the results
1286	Split the data into train and validation sets
574	China and Mainland China are the most common China countries .
1052	Load the U-Net++ model trained in this kernel .
1170	Total Sentences
61	Time histogram of Product code
1476	How does our days distributed like ? Lets apply @ ghostskipper 's visualization ( to out solution .
408	Image Vizualization
1130	Dropping V1 features
1220	Predictions on Test set
1165	TPU Strategy and other configs
335	Model Training with Ridge
607	Loading and preparing data
582	Reordered Iran data
1159	Make Predictions
1304	NAN Processing
875	Let 's look at the hyperparameters
776	Train Validation Split
350	Importing the required libraries
779	Simple Gradient Boosting
441	Meter Reading Hour
1302	Fill missing values in test set
234	Let 's see what happens if we select a single commit from this dataset .
1128	For class
170	Download by click ratio
793	Now we will make our predictions on the validation set . We will use random forest to make our predictions on the validation set .
116	Price Distribution of whole data
808	Running the optimizer
506	Plotting samples for the target
1247	Analyzing FVC vs Weekly Sales distribution
802	boosting_type ` で一番参数放到
1262	Importing the Libraries
475	Submission
883	High Correlation Heatmap
200	Let 's take a look at one of the patients .
199	We 'll use neato to visualize the sound .
711	Target vs Warning Variable
496	Type features
1380	Let 's look at the distribution of numeric features .
1030	Convert to a submission format
1027	Model initialization and fitting on train and valid sets
939	OOF Submission
874	Table of Contents The Affine Transform Elastic Deformation Flipped images Spatial Padding Experimental Method from APTOS Appendix A : Libraries and Functions
988	Create the Display object
109	Data augmentation
569	We 're going to use the ` DataGenerator ` class for training and a ` DataGenerator ` for validation .
1153	Let 's compute the mean per store value per day of the week .
975	Let 's take a look at the DICOM image data .
106	Loading ` before.pbz ` and ` beforeM ` from file
1224	Drop calc columns
792	Get the list of features
40	The datasets can be distinguished quite good . Let 's take a look at the feature importances .
486	Now we have 6 features , so let 's try a vectorizer with 6 features .
415	Predict on Test Set
1556	Finally plotting the word clouds using Cthulhu-Squidy
17	Load the predictions
1273	Oversampling the training dataset
124	In this competition , you ’ re challenged to build a baseline model that predict the probability of an object . You ’ re challenged to train a model to predict the probability of an object . You ’ re challenged to train a model to predict the probability of an object .
1051	As you can see , the most of the images are of the same type as the first one in the sample dataset . This can be achieved by looking at which image belongs to each label , and looking at which image belongs to each label . To do that , we need to pivot the sample dataframe into two columns ` Label ` , ` filename ` and ` type ` .
894	In the previous dataset the average term of previous credit is equal to the mean of all the features .
1397	Numeric features
772	Let 's read test data
445	Meter Reading Recall from MAY TO OCTOBER
231	Let 's see what happens if we select a single commit from this dataset .
215	High Correlation Matrix
837	Feature Engineering : installments_info
303	Set some parameters
1275	agregating installments into previous_app dataset
26	The datasets can be distinguished quite good . Let 's take a look at the feature importances .
1549	The method for training is borrowed from
83	Outcome Type & Neutered
1413	Data generator
1043	Inference and Submission
1102	Leak Data loading and concat
328	SVR on train and test
689	How do the DICOM files look like
953	Load the data
1443	HOURLY CLICK FREQUENCY
970	load mapping dictionaries
18	Load train and test data .
121	Let 's see how correated the features are
404	Data Preparation
644	Let 's split the label into two parts , we will keep the first 5 parts of the label , and the rest to be labelled again
1108	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
363	Duplicate clicks with different target values in train data
1094	We can now calculate the SNR ratio by taking a sample and calculating the mean .
449	Wow ! The dataset contains buildings that were made in the 1900s . I thought the buildings will be just newer ones .
516	Some missing values in test and train dataset
1204	Run LSTM on train and validation set
1231	In the next sections of code , we will use the cross_validate_xgb function to compare the predictions of xgboost with the previous sections .
691	From the above function we can see that the score is higher than 0.5 , which makes sense .
777	Fitting the model on the training data and checking the coefficients
203	As a final preprocessing step , it is advisory to zero center your data so that your mean value is 0 . To do this you simply subtract the mean pixel value from all pixels . To determine this mean you simply average all images in the whole dataset . If that sounds like a lot of work , we found this to be around 0.25 in the LUNA16 competition . Warning : Do not zero center with the mean per image ( like is done in some kernels on here ) . The CT scanners are calibrated to return accurate HU measurements . There is no such thing as an
295	Average prediction
1303	Null values in the test set
521	Set a threshold to evaluate for classifiers
987	In this notebook , we 'll be working on one of the patients . The first thing we need to do is to open the CT scan and read the CT scan images from the DICOM files . We 'll use the DICOM files directly from these patients to load information .
1295	Plot the accuracy and validation accuracy of the model
1554	Loading dataset and basic visualization
683	One of the most important features have all 0 values . Let 's check how many of them are in the train and test sets .
973	Let 's check the patient name .
1323	Area and Instance Levels
1084	Model initialization
986	Label encode all object columns
180	Let 's check if there are too many objects in our dataset .
1099	We solve many of the tasks in the training set using our Neural Cellular Automata model ! I did test on the validation set as well , and it correctly solved 17 of the tasks . There are a number of ways this model could be improved . Please let me know if you 'd be interested in collaboration Solved Tasks
1104	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
1147	Number of masks per image
450	Air Temperature
695	Integer Columns
1356	Numeric features
368	Linear Regression
1461	Lets fix neutral sentiments
1198	Splitting data into train and test
1016	Predicting with the best parameters
376	Model Training with Ridge
340	Part 2 . Feature Engineering
93	Gene and Varation
1384	Let 's look at the distribution of numeric features .
196	How does the structure look like
638	Importing the necessary modules
89	Let 's use the tokenizer to clean the comments
766	The time series data is pretty unbalanced , so let 's perform some EDA
59	Create new feature
1497	less than function of product ( a , b )
1382	Let 's look at the distribution of numeric features .
519	Cross Validating the model with the acc_logreg , acc_SGD and acc_rfc .
1145	Open the mask with the .rle library
896	Most common feature engineering
901	agregating Bureau features into a single dataframe
934	Predicting on Validation and Test
1238	Create submission file
274	First of all , let 's see what happens when we look at one of the most popular models . We 'll take a look at one of the most popular models , and compute their score .
873	The final step is to create train and test dataframes . We will use pd.get_dummies ( ) to do the same for the test and train datasets .
75	The below code transforms the images by flipping them horizontally and vertically along with rotating , zooming , lighting ...
1369	Numeric features
710	As we can see there are very few comments which can be considered as 'warning ' . These can be achieved by taking a weighted average of these comments .
205	OneHotEncoding
1404	So it does n't seem like there 's a clear difference between close and 12EMA . We will use EDA to estimate the macd score .
1137	Model with Image Augmentation
961	Monthly skewed
272	Let 's see what happens if we select a single commit from this dataset .
908	Feature Engineering - Bureau Balance
1164	class_countの累計回数
716	Correlations in the train set
663	Time features
626	Let 's take a look at the total number of bookings per level
289	Here we see that there are no missing data . It is worth noticing that there are a lot of missing data . It is worth noticing that there are a lot of missing data . It is worth noticing that there are no missing data .
1088	Run the four models and check the output ...
257	Linear Regression
1178	Number of Patients and Images in Training Images Folder
1279	Null analysis of the dataset
305	Construction of the LSTM
64	t-SNE with animation
85	Since we already have the quarter feature , we can just divide the age by 12 to get the normalized age .
115	store_id & item_id price data
47	Target variable ( log ( 1+target.values
117	Let 's get rid of unnecessary rows for Xmas data .
1518	T-SNE embedding
128	Analysis of the segmented data
358	Read in the data
1020	Build datasets objects
225	Let 's see what happens if we look at one of the most popular feature in our dataset .
1505	Two embedding matrices have been used . Glove , and paragram . The mean of the two is used as the final embedding matrix
1341	We also have some missing values in ` application_train ` and ` application_object_na_filled ` . Let 's check more .
1121	Outcome Type , Neutered , Animal Type
1521	Evaluate the score with using 4-fold TTA ( test time augmentation ) .
293	Let 's split the data into three columns ; the first column contains the commit number , the dropout model , the FVC weight , and GaussianNoise_stddev .
595	Top 20 neutral words in selected_text
1123	Feature Slicing in Time Series Data
1385	Let 's look at the distribution of numeric features .
32	Read the data
681	Exploratory Data Analysis ( EDA
1160	Prepare data for Keras
73	Model Training with Fastai Library
720	drop high correlation columns
749	Train Validation Split
635	In order to get a better understanding of the features , we need to transpose it .
416	Unit sales by state
254	Albania
938	LightGBM Classifier Algorithm
550	Vs logerror
992	Show the image
618	KNN Regression
693	This is a collection of scripts which can be useful for this and next competitions , as I think . There is an example of baseline at the end of this notebook .
1183	Data Augmentation to avoid overfitting
774	What about correlation with the Fare amount
888	Replace outliers with np.nan
818	Random Search for Submission
1432	Difference between d1 and h1
1062	Preparing final submission data
1466	Dependencies
1322	Now we 'll multiply all the categorical features by the average value in these features .
387	Now , let 's see some of the columns
645	How many unique labels do we have
19	Let 's look at the distribution of the target variable
491	Compile and visualize model
559	Images with ships and masks
732	Feature Importance
152	Train the model
1265	In this section I 'm going to show how many variables decaying is in the bert_nq model .
1395	Numeric features
534	Order Count
8	Let 's load the data
1189	square of full data
339	Voting Regressor
270	Dropout Model : CNN
423	BanglaLekha Confusion Matrix
1393	Let 's look at the distribution of numeric features .
1327	Load the data
1519	t-SNE visualization in 3 dimensions
1166	Load the ` sample_submission.csv ` and see what we got
413	Predict
646	For validation I 'm going to use 5 different length of labels to validate our model . For this , I 'm going to use 5 different length of labels to validate our model .
847	Boosting type and subsample
69	Next , we calculate the distance between the tour and the target .
636	Preparing the data
1210	merchant_id : Unique merchant identifier merchant_group_id : Merchant group ( anonymized merchant_category_id : Unique identifier for merchant category ( anonymized subsector_id : Merchant category group ( anonymized numerical_1 : anonymized measure numerical_2 : anonymized measure category_1 : anonymized category most_recent_sales_range : Range of revenue ( monetary units ) in last active month -- > A > B > C > D > E most_recent_purchases_range : Range of quantity of transactions in last active month -- >
189	Top 10 categories of items with a price of 0
828	These features have some unique values , so let 's drop them from the dataset .
1462	Saving model and weights
1523	Similar to validation , additional adjustment may be done based on public LB probing results ( to predict approximately the same fraction of images of a particular class as expected from the public LB .
299	Training the LGBM model
586	Run the model if it has any run-length encoding
1319	Let 's multiply all the features .
1378	Let 's look at the distribution of numeric features .
512	Spreading the Spectrum From the histogram , you can see that most pixels are found between a value of 0 and and about 25 . The entire range of grayscale values in the scan is less than ~125 . You can also see a fair amount of ghosting or noise around the core image . Maybe the millimeter wave technology scatters some noise ? Not sure ... Anyway , if someone knows what this is caused by , drop a note in the comments . That said , let 's see what we can do to clean the image up . In the following function , I threshold the background .
576	Looking at the grouped cases by country
912	Let 's find the pairs of above threshold variables to remove .
262	Random Forest
1029	Now that we have pretty much saturated our model on english only data , we train it one more epoch on the validation set , which is significantly smaller but contains a mixture of diffferent languages .
1008	LOAD DATASET FROM DISK
658	Correlation in the number of columns
176	Let us check the memory usage again
871	Featuretools - Exploratory Data Analysis
997	Leakage Episode
1499	Understanding created time
288	Let 's see what happens if we select one commit from our dataset .
364	Type_1 & Type
91	Gene Frequency Plot
1376	Let 's look at the distribution of numeric features for the 23th feature
1200	We 're going to create two datasets that we 'll need later .
1478	Now we will read in the data
1205	Mode by Owners and Investments
688	Transforming an image id to a filepath
1060	Predicting the Test Set
562	Lets get the mask directories for the image .
1381	Numeric features
552	Data Augmentation
86	Add a new feature : AgeCategory
