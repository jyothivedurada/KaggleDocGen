902	Let 's take a look at the correlations of the target variable
239	Let 's take a look at the data .
15	Pad sequences
185	Mean price by category distribution
1192	Loading the data
71	Loading the data
1375	Let 's take a look at numeric features
1419	Let 's take a look at the data .
171	Let 's take a look at the data .
201	Resampling
1569	Let 's check the distribution of id_error .
726	Remove correlated columns
484	Let 's take a look at the vectorizer
35	Import libraries
63	Fraud & D1minusday
32	Loading the data
708	Now let 's see the distribution of walls and targets .
1327	Loading the data
1276	Feature Engineering
991	Cylinder
1261	Let 's take a look at the data
949	Let 's take a look at the prices for each merchant_id .
304	Macro F1 Metric
186	First levels of categories
374	XGBoost Model
1461	Neutral Sentiments
234	Let 's take a look at the data .
29	Let 's take a look at the AUC and Gini scores
1030	Let 's take a look at the results .
996	Let 's take a look at the submission data
511	Convert to grayscale
132	Clean up text with all process
1366	Let 's take a look at the numeric features
1108	Feature Engineering
954	Training and Test data
140	Label Encoding
1231	Feature Engineering
1382	Let 's take a look at numeric features
182	RLE Encoding for the current mask
1054	Let 's take a look at the results .
1185	Loading the data
94	Lemmatization
552	Augmentation
1139	Augmented Images
129	Let 's take a look at the data .
1320	Let 's do the same for public , planpri , noelec and coopele .
619	Linear Regressor
985	Let 's add 1 to the data .
727	Final Feature Engineering
545	Let 's take a look at the correlation matrix .
510	Let 's take a look at a single image
1342	Feature Engineering
409	Let 's see if there are any duplicates
928	Let 's check the comment length .
1085	Let 's clear the model .
1362	Let 's take a look at numeric features
39	Male Sex
1040	Loading the data
144	Let 's take a look at the categorical features
11	Outliers
683	Let 's check the number of features with all zero values
294	LB Score
1454	Let 's take a look at the results .
968	Italy - Curve for Cases
1346	Let 's take a look at the KDE for the target variable
772	Let 's load the test data
342	Loading the data
110	Build LR function
156	Finally , let 's clear the output .
1203	Let 's take a look at the target data .
1016	Simple XGBoost
335	RidgeCV
154	Save the model
1141	Efficient Detection
440	SUNDAYS HAVE THE LOWEST READINGS
295	FVC and Confidence
55	Let 's take a look at the data .
709	Walls + ROOF + Floor
227	Let 's take a look at the data .
1084	Building the model
289	Let 's take a look at the data .
1373	Let 's take a look at the numeric features
625	Let 's take a look at some features
930	Model
81	Let 's take a look at the data .
852	Let 's check the results .
566	Feature Engineering
1356	Let 's take a look at the numeric features
82	OutcomeType and Sex
363	Duplicate clicks with different target values in train data
556	Let 's take a look at text features
316	Preparing the test data
614	Loading the data
1501	Let 's seed everything .
951	Splitting the data into train and test sets
84	OutcomeType : Mix
1153	Let 's take a look at the data .
790	Linear Regression
715	Let 's take a look at the data .
1015	Title Mode
699	There are some households where the family members are not all equal .
1193	Preprocess the image
958	Submission
974	Let 's take a look at the keyword dictionary
92	Class Distribution Over Entries
964	Let 's take a look at the data .
1218	Let 's check the validation metrics
348	Let 's start with a generator .
900	Let 's align the test and train data
1063	Let 's take a look at the data .
896	Most recent data
605	Let 's try a few times to fix the data .
563	Masks over image
253	Germany
380	Voting Regressor
1507	Leak and LogLeak
893	Let 's take a look at the data .
244	Let 's take a look at the data .
1538	Feature Engineering
211	Importing necessary libraries
993	Let 's take a look at the code .
771	Distribution of Fare Amount by Number of Passengers
1344	Let 's check the KDE for the target variable .
523	Let 's take a look at the decision function scores
1312	Load the data
392	Level 2
707	Let 's take a look at the distribution of targets for each area
221	Let 's take a look at the data .
1439	Let 's load the data
1433	Importing the necessary libraries
321	Let 's take a look at the data .
1188	Let 's take a look at the images for each patient
551	GaussianTargetNoise
317	Train the model
662	Let 's take a look at the data .
914	Importing required libraries
1534	Let 's take a look at the eratosthenes .
160	Is Fraud
809	Let 's take a look at the best solution .
438	Preview of Data
97	Read the test data
586	Let 's take a look at the data .
77	Resnet
449	Year Built and Building ID
1037	Training History
179	Let 's take a look at the labels .
1432	H1 and D1 features
649	RLE Encoding
1510	Create a video
943	Feature Engineering
135	Let 's take a look at the data .
1198	Let 's take a look at the data .
527	Let 's take a look at the data
149	Test Data
1410	Feature Engineering
873	Final Training and Testing Data
413	DataGenOsic
207	Training and Validation
812	Let 's take a look at the scores .
398	Importing necessary libraries
941	Loading the data
1025	Loading the data
483	Let 's take a look at the text .
739	Submitting the model
420	Confusion Matrix
1553	Import libraries
862	Fitting the model
357	Importing necessary libraries
1197	First , let 's take a look at the target variable .
416	Let 's take a look at the data .
618	KNN Regressor
123	Pulmonary Condition Progression by Sex
721	Education Distribution by Target
1094	Let 's take a look at the sample data .
313	ROC AUC
1023	Fitting the model
1124	Let 's take a look at the address change .
967	Logistic Growth Curve
1532	Correlation of winPlacePerc
284	Let 's take a look at the data .
268	Voting Regressor
98	Let 's take a look at the test data .
1278	Importing necessary libraries
338	AdaBoost
296	Here we define the parameters for the model .
1118	Feature Engineering
142	Categorical and Continuous Data
666	Let 's encode the full data
350	Importing necessary libraries
3	Let 's check the data .
1368	Let 's plot the category percent of target for numeric features
1381	Let 's plot the category percent of target for numeric features
124	Importing Libraries
1307	Random Forest Regressor
1447	Let 's take a look at the data
303	Let 's take a look at our model .
444	Let 's take a look at the data .
1508	Let 's take a look at the good features
886	Let 's check the number of boolean variables
801	Let 's take a random boosting_type
664	One-Hot Encoding
424	Confusion Matrix
1401	Let 's plot the category percent of target for numeric features
780	Fitting the model
573	Exploratory Data Analysis
1100	Let 's take a look at the test data
1294	Converting the DICOM files
623	Let 's take a look at the model accuracy
864	Aggregation Primitives
653	Random Forest Regressor
1183	Create Data Generator
310	Let 's check the data .
1090	Let 's take a look at the validation data
190	Does shipping depend of prices ?
533	Reorder Count
569	Training and Validation
840	Preparing the credit card balance data
819	Bayesian Optimization
1157	Let 's take a look at the results .
1506	Let 's take a look at the dataset
1076	Let 's reshape the data .
232	Let 's take a look at the data .
768	Let 's take a look at the new data .
561	TCA-G9-6362-01Z-00-DX1
1453	Loading the data
609	Define the model
1523	Let 's take a look at the mean value of th_t
815	Let 's take a look at the distribution of random parameters
107	Let 's take a look at the data .
1494	Lifting the function
1592	Remove object columns
476	Merge the data
1004	Let 's take a look at the data .
1409	Missing Noisy
779	Let 's take a look at the test data .
630	Let 's take a look at the data .
5	Histogram of target counts
150	Let 's take a look at the test data
983	Preparing the test data
808	Let 's take a look at the best solution .
611	Word Embeddings
878	Random Search and Bayesian
982	Let 's take a look at the training data .
1478	Preprocessing
820	Import libraries
1286	Training and Validation KFold
978	Let 's set the _should_scroll method to false .
1462	Yolov3 model
576	Let 's take a look at the country cases
1211	New Merchant Data
756	Let 's take a look at the bounding boxes of the image
1341	Feature Engineering
1229	BernoulliNB
1515	Household Type
422	Random Forest Classifier
346	Let 's take a look at the predictions
1062	Final Submission
1163	Let 's take a look at the missing labels .
936	Feature Engineering
971	Let 's plot the data .
57	Let 's take a look at the total error .
1490	Patient 6 - Normal and Unclear Abnormality
212	Loading the data
1516	Distribution of v2a11
750	Poverty Confusion Matrix
1489	Increased Vascular Markings + Enlarged Heart
133	Let 's take a look at the data .
286	Let 's take a look at the data .
1204	Model with LSTM
679	Extract images from zip
714	Let 's take a look at the data .
593	Most common words
1352	Remove null values
1194	Splitting the data
677	Let 's take a sample of hits .
1152	Importing necessary libraries
601	Let 's take a look at the public and private scores .
1317	Let 's take a look at the new features
1463	Let 's take a look at the data .
1042	Save the best model
571	Let 's load the data
148	Let 's take a look at the data .
1093	Let 's take a look at the data .
1233	Random Forest Classifier
417	Feature Engineering
695	Count of Unique Values in Integer Columns
1045	Build Model
684	Let 's check the number of binary features
657	Loading the data
938	Feature Engineering
547	Bedroom Count Vs Log Error
173	Let 's see the number of clicks over the day .
1079	Visualizing Diagnosis Images
1206	Let 's take a look at the mean price per room
1491	Sample Patient 6 - Normal and Unclear Abnormality
1122	Importing necessary libraries
1475	Importing the necessary libraries
502	Merge Applicatoin data
526	Fitting the model
761	StratifiedKFold
280	Let 's take a look at the data .
58	Loading the data
1353	Categorical Features
76	Let 's check the F1 score .
90	Loading the training data
427	Preparing the data
1459	Let 's take a look at the data
641	Import libraries
1056	KNeighbors Classifier
1544	Let us learn on a example
889	Bureau Credit Data
1055	Loading the data
141	Let 's take a look at the data
624	Preprocessing
535	Import libraries
994	Let 's take a look at the data
1187	Let 's take a look at the test data .
1449	Let 's take a look at the counts of each IP
712	Bonus Variable
1032	Let 's take a look at the data .
169	Let 's take a look at the quantiles
37	Distribution of age_approx values
1441	Let 's check the number of lines in the train.csv file
678	Let 's take a sample of particles
1155	Importing necessary libraries
470	Importing necessary libraries
332	Random Forest
1528	Distribution of DBNOs
1022	Fitting the model
1325	Let 's take a look at the columns with only one value
1380	Let 's take a look at numeric features
139	ord_5_1 , ord_5_2
298	Let 's take a look at the score of each seed .
1088	Let 's take a look at the video ids
498	Let 's take a look at the data .
336	Bagging Regressor
1448	Let 's take a look at the data .
1034	Let 's take a look at the results
805	Hyperopt Tpe
668	Most common label names
264	RidgeCV
1150	Let 's see the test data .
1159	Let 's take a look at the predictions
1095	SN_filter
1227	Drop target column
1457	Let 's seed everything .
83	OutcomeType : Neutered
1293	Import libraries
1557	Tokenize the text
755	C14C1e300 Images
836	Exploration
91	Gene Frequency
1237	Logistic Regression
145	Let 's take a look at the data
643	Outliers and Target
466	Let 's take a look at the images
405	Stage 1
1576	Loading the data
1067	Let 's load the test data
777	Fitting the model
1207	Investment and OwnerOccupier
1009	Training the model
1364	Let 's take a look at the numeric features
61	Product Code
917	Cash Balance
164	MinMax + Median Stacking
600	Let 's take a look at the submission data .
1437	Let 's take a look at the data .
696	Let 's do the same for train and test datasets
302	Let 's define the parameters
995	Submission
495	Loading the data
1340	Feature Engineering
730	Feature Engineering
1343	Let 's take a look at the ` features_dtype_int ` column .
760	LB Distribution
849	Let 's see if there are any values between 0 .
356	Random Forest Regressor
152	CatBoostClassifier
36	Load the data
434	Train and Test Split
402	Let 's check the test data
1121	OutcomeType : Neutered , AnimalType : Animal
1512	Importing required libraries
1027	Building the model
271	Let 's take a look at the data .
521	Sensitivity and Specificity
101	Let 's take a look at the data .
1331	Add new categories
740	Submission
1245	Size , Weekly Sales
590	Importing necessary libraries
1434	Splitting the data
835	Preparing the previous_application data
753	Exploratory Data Analysis
479	Now , let 's take a look at the test data
584	Let 's load the data
17	Load Predictions
640	Let 's take a look at the results .
544	Distribution of Variables Count Across Datatype
246	Loading the data
1114	Let 's take a look at the scores .
72	Let 's check the data .
447	Let 's take a look at the correlation matrix .
1310	Import libraries
1169	Let 's take a look at the data .
1541	Let 's take a look at the data .
1387	Let 's take a look at the numeric features
376	RidgeCV
659	Let 's take a look at the correlation of the target column .
599	Gini on random submission
743	Feature Selection
558	Let 's check the masks
361	Let 's take a look at the sample wts
935	Let 's take a look at the data .
1337	Feature Engineering
6	Let 's check the target values .
1113	Let 's take a look at the mean prediction .
1242	Let 's take a look at the data .
489	Tokenization
1467	Let 's take a look at the total sales
908	Bureau balance by client
559	Let 's see if there are images with masks
1215	Let 's load the test data
724	Let 's take a look at the range of the data .
838	Cash Balance
1474	Select plate group
1222	Feature Engineering
1545	Load the data
1354	Let 's take a look at the numeric features
1053	Create a generator for the test images
911	Let 's take a look at the above threshold .
763	Loading the data
210	Let 's take a look at the feature score
1083	Let 's load the test data
822	Training and Testing
260	SGD Regressor
217	Import libraries
961	Month of the year
999	Session level CV and User level CV
198	Bulge Graph
1407	Loading the data
1142	Wheat Detection
270	Let 's set the model parameters .
524	Let 's take a look at the metrics .
1173	Set the number of features and the number of workers
1239	Let 's check the structure of train and test data .
686	Let 's take a look at the image data
803	Let 's take a sample
1316	Feature Engineering
1069	Linear Weighted Kappa
1581	Loading the data
830	Feature Importance
1297	Let 's take a look at the submission data
385	Let 's run the mp_build .
1529	HeadshotKills Distribution
953	Load the data
654	Random Forest Regressor
904	Let 's take a look at the data .
1074	Pretraining & Submission
220	Let 's take a look at the data .
1104	Feature Engineering
948	Let 's check for NaN values
175	Let 's load the data .
155	Finally , let 's clear the output .
404	Let 's check the data .
360	Feature importance
687	Let 's take a look at the data .
353	Let 's take a look at the data .
773	Minkowski Distance
1502	Loading the data
408	Let 's take a look at the image data
541	Setting up some variables
60	Let 's take a look at the connected components
323	Training and Validation
647	Loading the model
204	Import libraries
1488	Lung Nodules and Masses
114	Let 's copy the data
1378	Let 's take a look at the numeric features
868	Let 's take a look at the data .
661	Let 's check the nominal features
1224	Dropping ps_calc columns
1357	Let 's take a look at the numeric features
888	Let 's do the same for day outliers
340	Let 's take a look at the models .
769	NYC Map Zoom
1290	Train the XGBoost model
18	Loading the data
592	Let 's take a look at the data .
581	Spain
648	Train the model
509	Let 's take a look at the data .
319	Create a new file name
397	Let 's set the ` in_train ` and ` in_test ` features
927	Loading the data
1161	Let 's take a sample of data
855	Let 's take a look at the best model
1208	feature_1 - feature_2
688	Let 's take a look at the data .
1071	Let 's take a look at the results .
550	No of Storeys Vs Log Error
891	Time features
802	Let 's take a look at the subsample parameter
531	Hour of the day
1010	Save the model
645	Let 's take a look at the number of unique labels
507	Reducing the sample data
1451	HOURLY CONVERSION RATIO
1151	Let 's take a look at the distribution of var_91 .
1521	Let 's take a look at the predictions .
841	Merging the credit info
322	Splitting the data
70	Let 's take a look at the data
1361	Let 's take a look at the numeric features
1168	Import libraries
1338	Feature Engineering
880	Score as function of Learning Rate and Estimators
909	Bureau Test
49	Let 's take a look at the test data
828	Let 's drop the zero features
1280	Let 's take a look at the topics
1548	Let 's take a look at the embeddings
1146	Let 's take a look at the mask
500	Pearson Correlation
1389	Let 's plot the category percent of target for numeric features
1277	Random Forest Classifier
1309	Loading the model
486	Feature Extraction
1546	Save the data
1050	Let 's take a random sample
670	Categories of items < 10 \u20BD ( top
925	Exploratory Data Analysis
131	Clean special characters
620	Lasso
53	Let 's take a look at the data .
1026	Training and Validation datasets
369	SVR
292	Let 's take a look at the data .
300	XGBoost Model
1370	Let 's plot the category percent of target for numeric features
877	Let 's take a look at the results .
362	Let 's take a look at the data
1292	Let 's take a look at the number of weeks per patient
617	Random Forest
192	WordCloud
1000	TPU
1125	Let 's take a look at the address change .
273	Let 's take a look at the data .
546	The number of stories for each parcel is equal to the number of stories for each parcel .
882	Number of Estimators vs Learning Rate
966	Confirmed Growth Rate Percentage
570	Importing necessary libraries
646	Let 's take a look at the labels
343	Let 's check the data sizes
1442	Let 's take a look at the data .
899	Remove low information features
471	Merge the data
905	Count categorical variables
1562	Feature Engineering
136	Num of Unique Values
1043	Preprocessing
1099	Let 's check the results .
1369	Let 's plot the category percent of target for numeric features
1408	We do not need to worry about missing values .
807	Let 's take a look at the data
446	Let 's take a look at the meter reading by timestamp .
411	Let 's take a look at the data .
299	Define LGBM parameters
1212	Feature Engineering
1404	Let 's take a look at the mean value of close .
741	Let 's take a look at the correlation matrix
694	Loading the data
632	Log1p of Demanda Uni Equil Sum
423	Confusion Matrix
1399	Let 's plot the category percent of target for numeric features
1	Importing the necessary libraries
530	Loading the data
977	Let 's take a look at the data .
354	Let 's take a look at the correlation matrix .
903	Target Correlation
195	T-SNE
656	Importing necessary libraries
229	Let 's take a look at the data .
863	Add missing values to train and test sets
1511	Create video
16	Toxic Predictions
1131	Label Encoding
898	Feature Engineering
371	SGD Regressor
1174	Adding PAD to each sequence
450	Air Temperature
242	Let 's take a look at the data .
698	Households without head
781	Let 's take a look at the correlation matrix .
1533	Let 's see the average winPlacePerc for each column
1391	Let 's plot the category percent of target for numeric features
432	WordCloud
508	Let 's load the data
431	Remove duplicate questions
339	Voting Regressor
965	Shap importance
1334	Let 's take a look at the data .
1392	Let 's take a look at the numeric features
596	Let 's take a look at the data
1493	Import libraries and data
255	Andorra
775	Linear Regression
1226	Let 's take a look at the rank of the prediction .
1495	Let 's take a look at the program description
1201	Training the model
799	Fitting the baseline model
737	ExtraTreesClassifier
577	China
496	Feature Engineering
312	Training and Validation
480	Importing necessary libraries
604	Spoiled Submission
1195	Most annotators
20	Histogram of muggy smalt axolotl-pembus counts
1024	Word Piece Tokenizer
1322	Let 's take a look at the new features
580	Reorder china cases by day
126	Hounsfield Units ( HU
945	Let 's take a look at the column types
463	Training and Test Data
585	Let 's take a look at the target country .
876	Random Search and Bayesian Opt
1065	Train the model
250	Spain
1127	Pd District
429	Bayesian blocks
214	Let 's take a look at the data .
43	Let 's take a look at the data .
441	Reading HIGHEST DURING THE MIDDLE OF DAY
401	Loading the data
1191	Train and Validation Split
1372	Let 's plot the category percent of target for numeric features
1554	Loading the data
1051	Pivoting the sample data
178	Otsu thresholding
1279	Check the number of records
74	Let 's seed everything .
934	Now let 's train the model with the validation and test data
804	Let 's take a look at the results
738	Random Forest
377	Bagging Regressor
1254	Importing necessary libraries
174	Download rate evolution over the day
765	Fare Binned
776	Splitting the data
235	Let 's take a look at the data .
1374	Let 's take a look at numeric features
64	Feature Engineering
1148	Loading the data
389	Let 's take a look at the images for each category_id
33	TfidfVectorizer
130	Count the words in the series
274	Let 's take a look at the data .
691	Let 's take a look at the results .
513	Let 's take a look at the mask .
1565	Hilbert and Hann
1416	Drop color columns
520	SGD Classifier
1220	Evaluate the model on the training data
1586	Let 's take a look at the data .
281	Let 's take a look at the data .
747	Let 's take a look at the data
987	Let 's take a look at the data
1005	DenseNet
46	Histogram of target values
421	Confusion Matrix
492	Let 's define our Keras layers .
31	KMeans
1077	Let 's take a random permutation of the data
1527	Let 's check the distribution of assists
1517	Let 's take a look at the target variable .
1134	Importing necessary libraries
1582	Let 's check the sample data .
236	Let 's take a look at the data .
872	Remove low information features
637	Create Lags
501	Top Correlation
589	Let 's take a look at the results .
1578	Let 's check the metrics .
301	Feature Engineering
249	Let 's define a function that can be used to calculate the loss function .
456	Preview of Train and Test Data
469	Let 's take a look at the test data
1398	Let 's plot the category percent of target for numeric features
269	Let 's take a look at the models .
165	Let 's load the data .
1287	Importing necessary libraries
1555	Splitting the text into words
759	Let 's fill missing values with 0s
1564	Let 's take a look at the data .
534	Order Count
9	Let 's take a look at the data .
972	Let 's take a look at the data .
854	Let 's take a random sample of the parameters
1061	Let 's take a look at the results .
333	XGBoost Model
1238	Stacking Prediction
1367	Let 's take a look at the numeric features
1250	Batch Mixup
189	Top 10 categories of items with price of 0
717	Most negative and positive Spearman correlations
990	Cylinder
1253	Distribution of cod_prov
314	Let 's see the classification report .
644	Let 's take a look at the labels
1186	Let 's take a look at the images for each patient
1058	KNN logloss on longitude and latitude
1033	First 10 detection scores
923	CNT_CHILDREN
1417	Logistic Regression
166	Let 's check the number of different values
445	May to OCTOBER
1458	Let 's take a look at the start and end positions .
347	Save the submission file
634	Loading Global Data
293	Let 's take a look at the data .
96	Loading the data
1395	Let 's plot the category percent of target for numeric features
853	Fitting the model
1350	Let 's check for missing data
751	UMAP - PCA - FastICA - TSNE
1154	Let 's take a look at the trends
583	Let 's take a look at the cases by day .
1038	Loading the model
540	Bedrooms & Bathrooms
725	Let 's take a look at the data .
616	SVR
38	Let 's take a look at the images
674	Let 's take a look at the image labels
1348	Merge Applicatoin train
418	KMeans
452	Wind Speed
536	XC195200.mp3
187	First level of categories
415	Let 's see the test image
109	Augmentation Pipeline
568	Feature Selection
529	Convolutional Model
992	Let 's visualize the data .
667	Logistic Regression
1143	Let 's take a sample of 5 unique values for each column .
924	Let 's take a look at the distribution of targets .
548	Bathroom Count Vs Log Error
194	Description length VS price
671	Exploratory Data Analysis
845	Feature Engineering
102	Let 's take a look at the real and fake paths
1236	Feature Engineering
1241	Let 's check the data .
325	Import libraries
1274	Bureau
525	Let 's check the mean squared error .
128	Let 's take a look at the data .
652	Let 's take a look at the data .
950	Categorical int and Numerical features
1087	Importing necessary libraries
1036	Preprocessing
1359	Let 's take a look at the numeric features
1181	Preprocess the image
628	Let 's take a look at the total number of bookings .
493	Define Keras layers
1412	Categorize the data
788	Splitting the data
78	Freezing the model
65	Training Data
1048	Let 's build a new dataframe
461	Let 's take a look at the city data .
981	Let 's take a look at the image data
1444	Let 's take a look at the data .
10	Let 's take a look at the numeric columns
1179	Let 's take a look at the data
1271	Preparing the training data
606	Importing necessary libraries
1498	Build the model
1465	Let 's sort the data by visitStartTime .
1525	Importing necessary libraries
680	Import libraries
1255	Pretrain models
1394	Let 's plot the category percent of target for numeric features
564	Submission
183	Let 's take a look at the data
587	Let 's take a look at the data .
42	Spearman Correlation
1044	Now , let 's take a look at the predictions .
514	Cropping the image
689	Let 's read the data .
1228	Logistic Regression
1321	Elimbasu and Sanitario
1540	Missing Ratio
1347	Feature Engineering
1103	Preprocessing
635	Transpose the Data
885	Let 's take a look at the target column .
1269	Data Augmentation
1513	Let 's look at the categorical and numerical features
1140	Load the image
345	Train the model
555	Standard Scaler
1264	Pre-trained model
881	Number of Estimators vs Learning Rate
1268	Let 's check the training data .
89	Tokenize the data
1259	Let 's check the validation data .
906	Bureau Balance
100	Let 's take a look at the data .
306	Loading the data
1575	Let 's take a look at the data .
69	Let 's take a look at the distance of the tour .
86	Age Category
28	Histogram of 0 train counts
1570	Importing necessary libraries
1535	Let 's take a look at the distance matrix .
818	Submission
1509	Let 's take a look at the test data
633	Loading the data
1110	Preprocessing
1086	Let 's take a look at the submission file
1324	We can see that there are a lot of missing values in train and test sets .
813	ROC AUC vs Iteration
1351	Group Battery Type
400	Let 's set the data and test directories
1240	Month , Week , Day
785	Fare Amount versus Time Since Start of Records
223	Let 's take a look at the data .
80	Male , Female , Neutered , Intact
1574	Prophet
663	Let 's take a look at the time columns
1526	WinPlace Perc
816	Load the data
1098	Let 's take a look at the results
1503	Save the data
1002	Let 's take a look at the original fake paths
1452	Let 's take a look at the data .
919	Training and Validation
1561	Lemmatization
1273	Oversampling
857	Let 's look at the hyperparameters
366	Let 's compute the histogram of the image
722	Distribution of escolari/age
834	Bureau Info
1411	One-Hot Encoding
1171	Let 's take a look at the words in the sentences
272	Let 's take a look at the data .
582	Iran
162	Pushout Median Stacking
1577	Feature Engineering
800	Let 's see the distribution of the learning rate
1225	Dropping ps_calc columns
1388	Let 's take a look at numeric features
827	Feature importance
505	Target 0 and Target 1 Data
612	Training the model
1497	Let 's check if the product is less than or equal .
515	Normalize the image
875	Let 's see the hyperparameters
690	Let 's take a look at the patient data
368	Linear Regression
767	ECDF
430	Label Encoding
1080	Let 's check the data .
196	Bulge Graph
554	Let 's factorize the categorical features
1272	Let 's check the number of repetitions for each class
639	Let 's take a look at the data
588	Sir Learning
1566	Preparing the submission file
433	Top 20 tags
1129	Importing necessary libraries
1319	Let 's take a look at the new features
442	HIGHEST CHANGES
879	Score as function of Reg Lambda and Alpha
892	Distribution of Trends in Credit Sum
143	Set the seed
388	Let 's take a look at the data
512	Spike Spectrogram
682	Let 's take a look at the data
487	The quick brown fox jumps over the lazy dog
1133	Feature Engineering
394	Category_count vs Image_count
1014	Let 's take a look at the data .
1384	Let 's take a look at the numeric features
283	Let 's take a look at the data .
265	Bagging Regressor
146	Let 's take a look at the images
749	Training the model
134	Let 's take a look at the data .
1323	We can see that there are a lot of missing values in train and test sets .
267	AdaBoost
631	Producto_ID : Producto_ID : Venta_uni_hoy_sum : Venta_uni_sum : Venta_uni_hoy_sum : Venta_uni_sum : Venta_uni_hoy_sum : Venta_uni_sum : Venta_uni_sum : Venta_uni_hoy_sum : Venta_uni_sum : Venta_uni_sum : Venta_uni_hoy_sum : Venta_uni_sum : Venta_uni
318	Create submission file
254	Albania
1223	Feature Engineering
1584	Let 's take a look at the image names
158	Importing necessary libraries
821	Loading raw data
1117	Preprocessing
594	Negative Words
1443	HOURLY CLICK FREQUENCY
59	Let 's take a look at the data .
1492	Importing necessary libraries
865	Feature Engineering
946	Optimized Rounder
50	Histogram of all train counts
1202	Evaluate the model on test data
833	Let 's take a look at the data .
8	Loading the data
519	Let 's take a look at the accuracy .
859	Boosting Type for Random Search
202	Normalize the image
980	Let 's take a look at the first DICOM file
1418	Importing Libraries
843	Feature Importance
710	Exploratory Data Analysis
757	Loading the data
847	Boosting type and subsample
1209	Y or N
1158	Logistic Regression
574	China and Mainland
393	Importing the data
425	Convert to RGB
279	Let 's take a look at the data .
1530	Distribution of killPlace
784	Let 's take a look at the test data .
1332	Add new categories
1422	World COVID-19 Prediction
1039	Now , let 's take a look at the predictions .
1496	Let 's take a look at the data .
1360	Let 's take a look at the numeric features
263	Splitting the data
199	Let 's take a look at the data .
1283	Read data from folder
754	Random Forest Classifier
412	Let 's take a look at the data
1096	SN_filter = 1
373	Random Forest
711	Target vs Warning Variable
437	Importing necessary libraries
1397	Let 's plot the category percent of target for numeric features
1314	Replace edjefe
856	Let 's take a look at the data .
288	Let 's take a look at the data .
658	Let 's check the correlation between columns
622	Feature Augmentation
1301	Load the test data
465	Let 's load the data
1102	Let 's take a look at the data
367	Let 's check the image data .
205	Let 's take a look at the data .
929	Word2Vec
1112	Load the submission data
745	Confidence by Fold and Target
386	Let 's split the raw data
1285	Let 's take a look at the squared values of the input list .
642	Feature Engineering
1182	Train and Validation Split
1256	Let 's take a look at the data .
1046	Building the model
384	Let 's take a look at the filter functions .
311	Let 's take a look at the data .
1349	Exploratory Data Analysis
1428	Let 's take a look at the data .
1385	Let 's take a look at numeric features
1156	Let 's take a look at the seed values
228	Let 's take a look at the data .
334	Splitting the data
850	Let 's take a look at the results .
1249	Batch Cutmix
1345	Let 's take a look at the KDE for each target .
969	Loading the data
918	Importing the data
947	Let 's take a look at the input files
256	Let 's take a look at the data .
180	Let 's see if there are any separate components / objects .
414	Let 's compute the histogram of the image
460	Let 's take a look at the directions
266	ExtraTreesRegressor
705	Let 's take a look at the heads of the data .
1328	Submission
93	Remove null values
168	Let 's check how many clicks we have in each caterogy
1438	Importing necessary libraries
1393	Let 's take a look at numeric features
1295	Let 's take a look at the data .
48	Let 's take a look at the target variable .
1567	Let 's load the data
308	WordCloud
216	Linear SVR
713	Let 's take a look at the data .
660	Day distribution
921	Splitting the images into train and validation sets
1097	Let 's take a look at the sample_struc
1190	Let 's take a look at the learning rate of the model .
193	Coms Length
1524	Let 's take a look at the data .
1028	Fitting the model
381	Let 's take a look at the models .
1260	Let 's check the validation data
1589	Let 's take a look at the number of columns
1460	Feature Engineering
1284	Let 's take a look at the validation score .
976	Let 's take a look at the dicom data .
1057	Let 's take a look at the test data .
247	Let 's take a look at the final ensemble .
488	The quick brown fox jumped over the lazy dog .
766	Exploratory Data Analysis
746	Baseline Model
742	Feature Selection
1329	Import libraries
117	Let 's take a look at the Xmas data .
99	Import libraries
1355	Let 's take a look at the numeric features
752	Random Forest
127	Lung Volume
482	Importing necessary libraries
379	AdaBoost
1230	Feature Engineering
21	Histogram of muggy smalt axolotl-pembus counts
1246	Weekly Sales and IsHoliday
1559	Lemmatization
138	Month Temperature
1483	Lung Opacity
387	Let 's take a look at the data
262	Random Forest
720	Drop columns with high correlation
907	Bureau Balance
1219	Update the learning rate
459	Road Encoding
1446	Loading the data
940	Let 's take a look at the data .
1007	Train the model
1073	Importing necessary libraries
209	Linear Regression
307	Dropout
1431	Hospital Deaths
1120	Male , Spayed , Intact , Unknown
383	Let 's define some variables
538	Bathrooms
736	KNN
1466	Importing necessary packages
1585	Loading the data
170	Distribution of DL by click ratio
887	Let 's take a copy of the app_types
324	Cohen Kappa
1164	Let 's take a look at the class counts
40	Feature importance
351	Loading the data
1199	Let 's take a look at the dataset
1257	Let 's load the data
1281	Extracting the series
1560	Let 's take a look at the data .
125	Let 's take a look at the data .
1189	Let 's take a look at the full data .
1504	Loading the data
562	Let 's take a look at the masks
1484	Lung Nodules and Masses
989	Bkg Color
108	TPU
331	Decision Tree
1572	Let 's take a look at the data .
1486	Sample Patient 4 - Ground-Glass Opacities
1571	Time Series - Average
826	Let 's take a look at the data
959	Loading the data
1138	Tagging the images
1176	Let 's see the number of links in the dataset .
1423	Hong Kong and Hubei
1487	Sample Patient 6 - Pleural Effusion
1514	Let 's take a look at the data .
1180	Loading the data
939	Submission
591	WordCloud
372	Decision Tree
910	Let 's align the test and train data
41	Loading the data
1539	Label Encoding
0	Histogram of target counts
349	Let 's start with a generator .
597	Let 's take a look at the test data
895	Feature Engineering
797	Import libraries
1386	Let 's take a look at numeric features
344	Training and Validation Loss
112	Fitting the model
1105	Loading the data
1175	Let 's count the number of links and nodes in the dataset .
1135	Importing necessary libraries
1019	Loading the data
1243	Type and Size
197	Let 's take a look at the data .
1406	Importing necessary libraries
259	Linear SVR
1413	Fitting the model
390	Unique categories and level 1 categories
1580	Find all occurrences of search_str in input_str
118	Let 's check the data .
1178	Let 's take a look at the data .
1214	EfficientNet
337	ExtraTreesRegressor
962	Let 's take a look at the data .
837	Let 's take a look at the installments data .
1021	Training the model
700	Let 's check for missing data .
516	Fill missing values for missing values
676	Importing necessary libraries
1464	Let 's take a look at the data .
1031	Let 's take a look at the results
1008	Loading the data
1518	Standard Scaler
719	Distribution of Correlation between Target and dependency
786	Fare Amount by Hour of Day
243	Let 's take a look at the data .
326	Padded Data
19	Histogram of target counts
233	Let 's take a look at the data .
213	Let 's take a sample
276	Let 's take a look at the data .
453	Let 's do the same for the test set .
1573	Let 's take a look at the data .
1426	Let 's take a look at the data .
443	HIGHEST READINGS
1081	Let 's take a look at the images
106	Loading the before matrix
1003	Let 's create a fake directory .
1376	Let 's take a look at numeric features
1440	Load the data
528	Let 's define the model parameters
522	Logreg , SGD and RF
1313	Let 's check for missing values
621	Ridge Regression
1232	Cross Validation with LGBM
328	SVR
1590	Feature Extraction
1591	Let 's take a look at the data .
1403	MA_7MA , MA_15MA , MA_30MA , MA_30MA , MA_60MA .
1282	Let 's plot the predictions and actual data .
1200	Create dataset
375	Splitting the data
7	Feature 1 Counts
448	Distribution after log transformation
894	Average Term of Previous Credit
732	Feature importance
27	Let 's check the data .
305	Let 's define the EPOCHS and LR
846	Let 's take a look at the results .
208	MinMax Scaler
984	Importing necessary libraries
85	Let 's try to calculate the age in years
45	Histogram of target values
774	Correlation with Fare Amount
120	FVC Difference
297	Import libraries
474	Define the model parameters
116	Whole data distribution
557	Let 's take a look at the embedding size
603	Public - Private Absolute Difference
701	Distribution of Value Counts
4	Loading the data
1468	Let 's plot the total sales for each store_id
475	Now , let 's take a look at the test data
1390	Let 's plot the category percent of target for numeric features
1116	Leak Data
1519	t-SNE visualization in 3 dimensions
553	Loading the data
1049	Let 's resize the images
13	Let 's take a look at the data .
870	Feature importance
1216	Let 's load the data
73	Feature Engineering
1001	EfficientNetB
1549	Let 's take a look at the dataset
890	Loan 5001709 over Time
52	Let 's take a look at the data .
1144	Let 's take a look at the categories
932	Let 's load the data .
47	Let 's take a look at the target values
602	Public - Private Difference
231	Let 's take a look at the data .
1006	Train the model
103	Let 's take a look at the model predictions
206	Importing necessary libraries
330	SGD Regressor
203	Zero centering
795	Fitting the model
1469	Melt Sales
1415	Let 's take a look at the data .
933	Splitting the data
607	Loading the data
848	Let 's see the distribution of the learning rate
975	Let 's take a look at the first dicom image
172	Let 's take a look at missing values
931	RLE Encoding
1270	Train the model
960	Let 's take a look at test data
278	Let 's take a look at the data .
789	Feature Engineering
810	Save the trials to json file
399	Importing necessary libraries
956	Let 's take a look at the validation masks
391	Exploratory Data Analysis
796	Predicted Test Fare Distribution
549	Let 's take a look at the logerror .
410	Let 's take a look at the test data
261	Decision Tree
1029	Fitting the model
1339	Feature Engineering
1265	Let 's take a look at the trainable variables
1298	Let 's take a look at the categorical features
871	Create top 100 features
494	Training Model
167	Number of click by IP
352	Let 's take a sample of data .
842	Let 's take a look at the data .
703	Let 's check for missing values in rez_esc
783	Distribution of Random Forest Predicted Fare Amount
56	Histogram of percentage zeros train counts
1302	Fill missing values with missing values
137	Let 's take a look at the data .
251	Let 's take a look at the data .
184	Top 10 categories
454	Label Encoding
915	Top 100 Features
1041	Let 's take a look at the data .
832	PC2 vs PC1
1551	Melt
54	Let 's take a look at the test data .
1244	Weekly Sales
458	Intersection
860	Load the data
473	Import libraries
257	Linear Regression
565	Training the model
1101	Loading the data
370	Linear SVR
1075	Let 's take a look at the data
439	ELECTRICITY THE MOST FREQUENT METER TYPE
285	Let 's take a look at the data .
1414	Let 's check for missing data
282	Let 's take a look at the data .
23	Word Vectorization
1296	Let 's plot the training and validation loss .
901	Bureau_ID_CURR
1068	Let 's take a look at the test data .
457	Most commmon IntersectionID
51	Log Histogram of all train counts
1035	Loading the data
638	Import libraries
874	Importing necessary libraries
798	Preparing the model
537	XC195200.mp3
922	Let 's take a look at the keypoints
1308	Loading the data
426	Import libraries
839	Aggregating the cash data
1383	Let 's take a look at the numeric features
692	TTA Transformations
1165	TPU
1123	Let 's take a look at the start date .
782	Random Forest
942	Bureau Bal
884	Correlation Heatmap
1587	Highest trading volumes
952	Remove features from train and test
1247	Dept , Weekly Sales , Type
497	Bureau Balance
770	Absolute latitude and longitude differences
1196	Toxicity annotators and comments
973	Patient Name
1537	Let 's take a look at the data .
238	Let 's take a look at the data .
543	Importing necessary libraries
477	How to use LightGBM
955	Split the data into train and validation sets
329	Linear SVR
733	Importing necessary libraries
490	Preparing the model
1115	Loading the data
598	Gini on perfect submission
1547	Let 's take a look at the data
219	Let 's take a look at the data .
998	Let 's take a look at the data .
869	Let 's take a look at the data
359	Tanh function
252	Italy
1579	Let 's plot the training and validation loss .
1425	Let 's take a look at the data .
1536	Let 's replace the missing values in the previous app_df
883	Correlation Heatmap
478	Import libraries
358	Load the data
539	Bedrooms
913	Remove Corr Columns
1217	Create supervised trainer and evaluation
963	Let 's take a look at the dependence plot
1471	Importing Libraries
200	Hounsfield Units ( HU
1456	Import libraries
1396	Let 's plot the category percent of target for numeric features
1365	Let 's take a look at numeric features
1235	LV2 Predictions
937	Let 's take a look at the data .
1234	Logistic Regression
704	Let 's take a look at the data .
825	Dropping columns
629	Let 's take a look at the total bookings .
157	Importing required libraries
1450	Count of clicks and proportion of downloads by device
572	First day , last day , total of tracked days
575	Let 's group the data by date .
986	Label Encoding
706	Drop columns with upper correlations
702	v2a1 - missing
1421	World COVID-19 Prediction
1107	Preprocessing
1402	Import libraries
1149	Let 's sort the data by date .
1477	Set the seed
1082	Submission
1119	Sexupon Outcome
1130	Diff V109_V110 , V329_V330 , V316_V331
1210	merchant_id : Merchant ID
395	Let 's take a look at the image IDs
1315	Replace edjefa values
1128	Tree Explainer
1476	Importing necessary libraries
560	Let 's take a look at the data .
787	Pickup Day of Week
655	Preprocess & Classify
979	Let 's take a look at the patients
1358	Let 's take a look at the numeric features
506	Target 1
636	Mortality , Fortality , ConfirmedCases , Population , Land Area
1275	Merge previous_app and installments
675	Coefficient of variation ( CV
926	Importing necessary libraries
1300	Int8 and Int16 columns
1326	Feature Engineering
277	Let 's take a look at the data .
794	Feature Engineering
455	Let 's take a look at the test data .
1251	Batch Grid Mask
159	Importing necessary libraries
75	Let 's take a look at the data .
468	Import libraries
762	Create Submission File
627	Sum of bookings per year
115	Store_id and Item_id
161	Ieee Blend
897	Feature Engineering
1303	Test and Test data
1018	Loading the data
957	Test Predictions
406	Stage 1b
225	Let 's take a look at the data .
111	Let 's take a look at the training data .
1017	Let 's take a random subset of the images
34	Confusion Matrix
240	Let 's take a look at the data .
861	Feature Engineering with LGBM
697	Let 's check if all the family members have the same target .
1445	Load the data
1371	Let 's take a look at numeric features
407	Stage 2
241	Let 's take a look at the data .
734	Model
88	Let 's check the score of each path .
44	Let 's take a look at the embeddings
1291	Let 's encode the mo_ye features
365	Training dataset
341	Let 's calculate the IoU .
66	Let 's take a sample of data
1472	Let 's take a look at the plate groups
1318	Let 's fill missing values for new features
1299	Let 's take a look at the integer columns
1420	China
275	Let 's take a look at the data .
163	MinMax Mean Stacking
595	Most common words
467	Let 's check the time taken .
988	Importing PyVirtualDisplay
866	Feature Engineering
1162	Let 's see the number of classes in the dataset .
320	Binary Target
215	Let 's take a look at the correlation matrix .
1205	Distribution of build_year by product_type
1072	Importing the necessary libraries
464	Loading the data
1429	Province/State
916	Import libraries
436	OneVsRestClassifier
1167	Build the model
518	Training and Predicting
1221	Loading the data
403	Time_to_failure
290	Let 's take a look at the data .
1335	Loading the data
867	Feature Engineering
517	Transaction Revenue Log
1170	Training and Test Sentences
1136	Loading the data
258	SVR
191	No description
615	Check missing values
472	Bayesian Model
748	Save the trials to json file
723	Exploratory Data Analysis
1263	Pretrain models
1078	Augmentation with albu
823	Let 's take a look at the data
1520	Test and Predict
685	Distribution of target transaction values
315	Remove the base directory
997	Let 's take a look at the data .
24	Feature Extraction
1505	Let 's take a look at the embeddings
181	Let 's take a look at the cell mask
1330	Let 's take a look at the data .
1499	Day of the week of the year
1132	V319 - V320 - V321
672	Price variance within parent categories
1089	Import libraries
944	Loading the data
1588	There are some assets without assetName in the training set
435	TfidfVectorizer
218	Dropout Model
87	Importing necessary libraries
731	Random Forest
327	Linear Regression
608	Setting max_features and text_length
1479	Let 's train the model .
1047	Let 's create the folders
119	Expected FVC
287	Let 's take a look at the data .
1480	QWK insample
25	Submission
1481	Submission
504	Data Preprocessing
1473	Create Model
122	Pulmonary Condition Progression by Sex
1011	Let 's take a look at the image data
1252	Label Encoding
1405	VMA_7MA , VMA_15MA , VMA_30MA , VMA_60MA .
1542	Time-to-failure
764	Distribution of Fare
1091	Let 's define the parameters .
1111	Feature Engineering
1531	Distribution of kills
1060	Let 's take a look at the test images
718	Let 's take a look at the correlated features
735	Linear Discriminant Analysis
1064	Load the image
222	Let 's take a look at the data .
1258	Pre-trained model
1436	Let 's take a look at the minute distribution .
716	Most negatively correlated variables
1543	Let 's take a look at the data .
814	Bayes Optimization Boosting Type
485	TfidfVectorizer
1336	Let 's create a random color generator .
1558	Remove stopwords
79	Create submission file
844	Feature Engineering
613	Training and Validation Loss
610	We can see that there are more than 250 images in the dataset .
1556	HP Lovecraft
829	Remove features with less than 0.95 importance
1305	Let 's take a look at the categorical features
1147	With masks
681	Importing necessary libraries
1013	Apply convolution
858	Importing altair
970	Loading the data
1500	Importing necessary libraries
1109	Loading the data
1184	Importing Libraries
1550	Importing necessary libraries
1306	Let 's take a look at the training set
378	ExtraTreesRegressor
817	Feature Engineering
728	Average Education by Target and Female Head
309	Let 's take a look at the data
230	Let 's take a look at the data .
1145	Let 's take a look at the mask
68	Let 's load the initial data
226	Let 's take a look at the data .
1012	Let 's resize the images
1248	Weekly Sales and IsHoliday
12	Let 's load the data
1070	Let 's take a look at the data .
1482	Sample Patient 1 - Normal Image
1563	Latent Dirichlet Allocation
481	Training the model
532	Day of the Week
113	Loading the data
364	Type_1 & Type_2
1311	Loading the data
1126	Let 's take a submission
121	Let 's take a look at the correlation matrix .
245	LB Score
567	Loading the data
579	Brazil cases by day
491	Compile the model
542	Let 's take a look at the results .
462	Standard Scaler
1177	Let 's take a look at the DICOM files
22	Let 's take a look at the target variable
851	Let 's see how many combinations we have in the grid
428	CatBoostRegressor
1400	Let 's take a look at the numeric features
1289	Train and Test Split
1304	Fill missing values for missing values
1059	Load the image
1213	Let 's take a look at the data .
67	Importing necessary libraries
1427	Province/State
693	Importing necessary libraries
104	Let 's check the data .
105	BZ2 Pickle
248	Importing necessary libraries
1377	Let 's take a look at the numeric features
1137	Let 's define the augmenters
1288	Spearman Correlation
95	Word Distribution Over Whole Text
578	Italy
651	Let 's take a look at the data .
1568	Let 's check the data .
1455	Let 's take a look at the results .
1583	Lidar Data
1092	Feature importance
1430	Importing necessary libraries
26	Feature importance
1485	Lung Nodules and Masses
1435	Let 's take a look at the count features .
811	Bayesian and Random Search
396	Let 's take a look at the test data .
1266	AdamW optimizer
62	Frauds and Non-Fraud
177	Convert RGB to grayscale
188	Top 10 brands
1470	Training the model
650	Let 's take a look at the missing values .
499	Let 's take a look at the distribution of the data .
355	Linear SVR
729	Importing the necessary libraries
778	Baseline Training and Validation
1424	Let 's take a look at the data .
669	Most common ingredients
912	Let 's take a look at the above threshold variables
665	Let 's take a look at the data .
1020	Training and Validation datasets
806	Hyperopt Trials
1379	Let 's take a look at numeric features
1267	Let 's take a look at the results
791	Feature Importance
744	Let 's take a look at the F1 score .
1166	Load the submission data
2	Feature Engineering
1160	Feature Engineering : Label Encoding
224	Let 's take a look at the data .
451	Dew Temperature
1052	Unet model
503	Let 's take a look at the distribution of the data .
758	Let 's check the distribution of surface values .
382	Importing necessary libraries
14	Tokenization
1552	Let 's take a look at the correlation matrix .
1172	Let 's check the total number of unique tokens
1333	Let 's take a look at the data .
1522	Let 's take a look at the macro score .
1262	Importing necessary libraries
291	Let 's take a look at the data .
831	PCA
673	Coefficient of variation ( CV
419	Decision Tree
626	Let 's take a look at the total bookings
147	Let 's define the learning rate reduction .
1363	Let 's take a look at the numeric features
793	Distribution of Validation Fares
151	Splitting the data
176	Let 's check the memory usage .
153	Feature Engineering
30	Submission
824	Let 's take a look at the correlation matrix
1106	Let 's take a look at the data
1066	Splitting the data
792	Feature Engineering
920	Load the model
237	Let 's take a look at the data .
