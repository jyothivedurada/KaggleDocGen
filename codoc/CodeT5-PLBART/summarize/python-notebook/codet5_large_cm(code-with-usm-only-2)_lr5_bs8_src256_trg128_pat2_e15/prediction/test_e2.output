525	Mean Squared Error
1470	CNN
1088	Extracting the video id from the frame matrix
1402	Import libraries and data
471	Merge Transaction and Identity Dataframes
216	Linear Regression Model
1147	Let 's take a look at the number of masks per image .
1280	Topic продать стать стать стать стать стать стать стать стать стать прода
1353	Categorical Features
657	Read in the data
747	Let 's create a csv file with the training data
411	Let 's check the same for train and test datasets .
768	Let 's check the new number of observations .
1156	Let 's do the same for the ` Seed ` column .
1559	Lemmatization with nltk
811	Evaluate Bayesian and Random Search
629	Let 's take a look at the total number of bookings per day .
978	Let 's define a function to determine if the output area should scroll .
990	Let 's create a vtkActor for the cylinder .
16	Let 's create a dataframe with the predictions for both the train and test sets .
352	Let 's take a look at some random data
104	Let 's see if we can detect any face in this frame .
434	Let 's split the data into training and testing sets .
1185	Load the data
611	Load word embeddings
786	Fare Amount by Hour of Day
1175	Let 's now look at the number of links and the number of nodes per title .
1376	Let 's look at the distribution of target values for the numeric features
1527	Distribution of assists
176	Let 's take a look at the memory usage of the dataframe
449	Looking at the distribution of year_built and building_id
1001	Model
237	Let 's create a new feature ` commit_num ` and ` dropout_model ` .
131	Let 's clean up the special characters .
445	Meter Reading from MAY TO OCTOBER
3	Let 's check what data files are available .
854	Let 's take a look at some random parameters
1193	Preprocess the image
1207	Let 's check the distribution of investment and owner_occupier .
1294	Convert DICOM images to PNG
88	Let 's check the performance of the model .
919	Split the masks into training and validation sets
1262	Import libraries and data
1251	Let 's take a look at the results
1042	Save the best model
1014	Let 's take a look at the distribution of the ` event_count ` and ` game_time ` columns .
1543	Let 's plot the quaketimes .
1517	Let 's look at the distribution of age , meaneduc for each target .
1347	LIVINGAREA_MEDIUM & MODE
614	Let 's load the data
585	In this competition , I 'm not sure of what 's going on in this competition . I 'm not sure of what 's going on in this competition . I 'm not sure of what 's going on in this competition . I 'm not sure of what 's going on in this competition . I 'm not sure what 's going on in this competition .
1266	Build the model
229	Let 's create a new feature ` commit_num ` and ` hidden_dim_first ` and ` hidden_dim_second ` .
68	Let 's load the initial data
218	Dropout Model
108	TPU Strategy and other configs
282	Let 's create a new feature : commit_num , Dropout_model , FVC_weight , LB_score
1089	Importing Libraries
1079	Visualizing Diagnosis Image
1460	Prepare test data for submission
701	Ploting Value Counts for ` parentesco1 ` .
1100	Let 's plot the predictions for the test set .
399	Import libraries and data
1222	Encoding Categorical Features
1008	LOAD DATASET FROM DISK
450	Air Temperature
316	Load the test data
1209	authorized_flag ` - Y or N
886	Let 's check the number of boolean variables in the dataset .
1521	Evaluate the model on the training data
965	Let 's take a look at the importance of each column
679	Extracting all the images
1029	Fitting the model
1149	Let 's split the data into train/val
1275	Feature Engineering - Previous Applications
369	SVR
259	Linear SVR
261	Decision Tree
1371	Let 's look at the distribution of target values for numeric features
55	Let 's take a look at the percentiles of the training data .
1165	TPU Strategy and other configs
1462	YOLOV
385	Let 's run the build in parallel
977	Let 's look at the SeriesInstanceUIDs in the dicom files
1073	Importing Libraries
1070	Let 's take a look at a random task .
711	Target vs Warning Variable
15	Pad sequences to the maximum length of characters
440	SUNDAYS HAVE THE LOWEST READINGS
1438	Import libraries and data
342	Load the data
1171	Let 's do the same for words in total
651	Replace -1 , -2 , 0 ...
1571	Let 's take a look at the average time series .
865	Let 's take a look at the features of the entity .
506	Target
636	Joining the dataframes for the country to the population and the land area
957	Stacking the predictions
766	Let 's calculate the EDF for the data
734	Model with MLP
952	Prepare the data
728	Target and Female Head of Household
129	Let 's check the memory usage of the dataset .
1331	Add New Category
1069	Linear Weighted Kappa Score
1397	Let 's take a look at the target for the numeric features
1584	Split the filename into host , cam and timestamp
862	Predicting with LGBM
1074	Pre-trained Model
790	Linear Regression
826	One hot encode the data
1095	Let 's take a look at SN_filter .
298	Prepare Training Data
939	Make Submission File
824	Let 's check the correlation matrix
738	Modeling with Random Forest
250	Spain
1509	Add leak to test
1536	Replace NAs with np.nan
1020	Build dataset objects
1344	Let 's take a closer look at the number of days in each target .
179	Let 's take a look at the labels .
148	Let 's take a look at one example
602	Let 's take a look at the difference between public and private scores .
808	Train the model
278	Let 's create a new feature : commit_num , Dropout_model , FVC_weight , LB_score
631	Producto_ID ` : Unique identifier for producto . Venta_uni_hoy_sum Venta_uni_proxima_sum Venta_uni_hoy_count Venta_uni_proxima_sum Venta_uni_hoy_sum Venta_uni_hoy_sum Venta_uni_hoy_sum Venta_uni_proxima_sum Venta_uni_hoy_sum Venta_uni_hoy_sum Venta_uni_hoy_sum Venta_un
262	Random Forest
1212	Make a Baseline model
464	Data loading and overview
1539	Process the data
338	AdaBoost Regressor
296	Let 's set the parameters and train the model .
1304	Fill in missing values
1423	Hong Kong and Hubei
281	Let 's take a look at the last 10 commits .
1106	Leak Data Exploration
1291	Let 's do the same for mo_ye .
812	Let 's take a look at the scores of the training data .
1255	Load the pretrained models
378	ExtraTrees Regressor
873	Final Train and Testing Data
1306	Split the data into training and validation sets
969	Load the data
346	Create Prediction DataFrame
1308	Let 's load the data
1192	Load the data
1104	We will use building_id , year_built , weekend , weekend_hr , cloud_coverage , wind_direction , wind_speed
78	Unfreeze the model and find the best learning rate .
182	RLE Encoding for the current mask
698	Let 's see if there are any households without a head .
590	Let 's take a look at the data
574	Replace mainland_china with China
351	Load the data
1528	DBNOs - EDA
152	Train the model
1361	Let 's look at the distribution of target values for numeric features
926	Import libraries and data
1061	filtered_sub_df ` and ` null_sub_df
7	Let 's look at the distribution of feature_1 values
438	Exploratory Data Analysis
183	Let 's check for missing data .
1113	Leak Score
1311	Load the data
1242	First of all , let 's see the size of the dataset .
876	Bayesian Optimization and Random Search
1060	Predicting for Test Set
396	Let 's check for missing data in test_metadata_csv .
185	mean price by category distribution
27	Let 's check what data files are available .
1555	Let 's split the data into a list of all the words we have in our dataset
356	SelectFromModel
1481	Make Submission File
1514	Visualizing the data
164	MinMax + Median Stacking
1035	Load the data
613	Plot of training and validation data
158	Let 's take a look at the data
1550	Importing Libraries
1123	Feature Engineering - Time Series Analysis
1552	Let 's check the correlations between train and test set .
74	Ensure determinism in the results
828	Let 's drop all the features from the train and test set .
845	Train the model
1036	Preprocess the public and private inputs
834	Merging Bureau Info
597	Perfect Submission
823	Let 's align the data for training and testing .
330	SGD Regressor
803	Let 's take a subsample of the data
604	Spoiled Submission
1188	Let 's do the same for the sub-df
822	Merge Training and Testing Datasets
827	Train the model
979	Let 's take a random patient
1580	I formulate this task as an extractive question answering problem , such as SQuAD .
9	Check for Null values
569	Let 's use the pretrained model for training and validation .
514	Cropping the image
1427	Province/State Prediction
358	Load the data
904	One-hot encode categorical variables
626	Let 's take a look at the total number of bookings per level .
787	Fare Amount by Day of Week
1258	Load the pretrained model
1219	Update learning rate
1523	Let 's take a closer look at the mean of the predicted values .
496	Let 's take a look at the data types .
503	Let 's take a closer look at the other features
260	SGD Regressor
527	Let 's look at the data types
1148	Load the data
1078	Augmentations with albu
1422	World COVID-19 Prediction
409	Let 's check for duplicate ids in the train set .
347	Save the submission file
235	Let 's create a new feature ` commit_num ` and ` dropout_model ` .
1333	Concatenate both train and test data
1324	Let 's multiply all the features by the product of the other features .
482	Loading Librosa Data
364	Type_1 & Type
821	Load raw data
1433	Importing Libraries
105	Pickle and Save
275	Let 's create a new feature : commit_num , Dropout_model , FVC_weight , LB_score
1177	Let 's take a look at the DICOM files
987	Let 's take a look at the data .
181	Opening the cell mask
642	Remove outliers and convert to categorical features
1269	Build the model
146	Let 's take a random image
453	As we can see , there is no clear difference between year_built and year_not_built .
1208	feature_1 - feature_2 , target - target
1356	Let 's plot the histogram for the numeric features
712	Target vs Bonus
851	Let 's take a look at the number of combinations
1541	Split the data into train and test
61	Let 's take a look at the product code .
648	Train the model
485	Feature Extraction ( TF-IDF
1202	Evaluate the model on the test data
1134	Importing Libraries
743	Feature Selection Scores
1143	Let 's take a look at the unique values in each column .
11	Check for outliers
935	Using selected features
169	Let 's take a look at the quantiles of the network .
885	Prepare the data for training and testing .
551	GaussianTargetNoise
937	Let 's split the data into train and test .
1421	World COVID-19 Prediction
1512	Importing the data
1325	Let 's see if there are any columns with only one value .
1166	Load the data
559	There are no null values in the masks .
1018	Load the data
43	Exploring the data
1419	Replace the mainland/china/China with the mainland/china
1354	Let 's look at the distribution of target values for numeric features .
1582	Let 's take a look at the sample_data.json
539	bedrooms - Interest level
238	Let 's create a new feature ` commit_num ` , ` dropout_model ` and ` hidden_dim_first ` .
327	Linear Regression
726	Remove correlated columns
540	Bedrooms and bathrooms
638	Importing Libraries
160	isFraud
1005	DenseNet121
727	Final Feature Selection
890	Loan 5001709 over Time
444	Place of HIGHEST READINGS ON WEEKDAYS
594	Highest common words in selected_text
350	Importing Libraries
761	Let 's split the data into folds .
850	Let 's take a closer look at the results
665	Let 's do the same for the full data set
353	The ` entity_from_dataframe ` method accepts a ` dataframe ` as an argument . The ` entity_from_dataframe ` method accepts a ` dataframe ` as an argument .
22	Let 's look at the target variable
147	Learning Rate Reduction
127	Lung Volume
13	Set embedding size and length
1529	Distribution of headshotKills
1490	Sample Patient 6 - Normal - Abnormality
1090	Let 's split the data into train and validation set .
860	Simple Feature Engineering
561	Let 's take a look at one image
31	Let 's check the inertia of each cluster .
1359	For numeric features
412	Lets take a look at the image and the mask .
1303	Check for Null values
318	Save the submission file
495	Data loading and overview
1535	Let 's take a look at the distance matrix .
1572	Visits by month
907	Bureau - EDA
306	Let 's load the data
1152	Importing the libraries
524	Precision and Recall Score
533	Hour of The Day Reorder
6	Let 's check the distribution of target values
1099	Let 's plot the results
1227	Dropping the target column
47	Lets take a look at the log of the target values
397	Let 's create a new column for ` in_train ` and ` in_test ` .
1579	Plot of model loss and validation loss
1048	Create a new dataframe with the same ` id_code ` and ` filename ` .
382	Import libraries and data
1334	Let 's get rid of sessionId and visitId .
733	Modelling with sklearn
1302	Fill missing values with null values
1398	Let 's take a look at the target for the numeric features
14	Tokenize Text
28	Let 's take a look at the target distribution
1085	Let 's free up some memory
641	Importing the required libraries
1375	Let 's look at the distribution of target values for numeric features
408	Let 's take a look at the ` train_dataset ` .
1560	Vectorize the data
596	Let 's take a look at the class distribution
1457	Ensure determinism in the results
1009	Train the model
1080	Let 's remove images that do n't have any blur .
1181	Preprocess the image
1305	Converting categorical variables to category
625	FEATURE_10 ` and ` FEATURE
1272	Let 's create a function to calculate the number of repetitions for each class ( if > 0 ) .
326	Split the data into train and test sets
922	Let 's take a look at the keypoints
1476	Importing Libraries and Loading Data
1349	Let 's see if it 's overdue .
836	Exploring installments_payments.csv
512	Spreading Spectrum
1138	Tagging the image
5	Lets take a look at the target distribution
523	Let 's check the threshold for the prediction .
1006	Train the model
1590	Vectorize and Tfidf
635	Transposed Data
1332	Add New Category
999	Let 's take a look at the best score .
62	Frauds and Non-Frauds
1524	Let 's load the sample data and save it for submission
63	Let 's take a closer look at the ` TransactionDTday ` feature .
1239	Examine the structure of train and test data
168	How many clicks do we have in each category
391	Most common level
1408	We do not need to worry about missing values . We do not need to worry about missing values .
857	Evaluate the hyperparameters
1378	Let 's take a look at the distribution of target values for numeric features
1506	The method for training is borrowed from
630	Let 's aggregate the ` bookings ` by ` date ` and ` weekday
222	Let 's have a look at more details about the model .
1516	Let 's take a look at the log of v2a1 .
966	Growth Rate Percentage for confirmed vs. confirmed
117	Let 's drop the Xmas date .
901	Add bureau features
310	Exploratory Data Analysis
410	Check for Duplicates
383	Setting up hyper-parameters
581	Spain Cases by Day
616	SVR
1252	Label Encoding for Null values
714	Let 's take a look at the correlation matrix and plot it .
682	Exploring the data
107	Let 's take a look at the before data .
398	Version
392	Level
265	Bagging Model
751	We use UMAP , PCA , FastICA and TSNE for classification .
645	Let 's check how many unique labels we have in our dataset .
1128	Let 's use SHAP library for prediction .
754	Let 's take a look at the non-limited tree .
1399	Let 's take a look at the target for the numeric features
211	Importing Libraries
1464	Let 's take a look at the LK sol file
1520	Predicting with the best ntree limit
646	Let 's take a look at the first 5 labels
623	Let 's take a look at the performance of the model .
1130	Diff V109_V110 to V329_V330 and V329_V331
192	Lets take a look at the item description
209	Linear Regression
186	First levels of categories
1196	Let 's look at the distribution of toxicity and comments .
1416	Remove unwanted columns
1396	Let 's take a look at the target for numeric features
1259	Let 's do the same for the validation set
653	Let 's try the same for the test set .
1437	Now let 's create a new column for the click_time and next_click .
163	MinMax + Mean Stacking
240	Let 's create a new feature ` commit_num ` and ` dropout_model ` .
955	Let 's split the data into training and validation sets .
171	Now let 's plot the download ratio and the category of clicker .
87	Import libraries and data
849	Let 's take a closer look at the learning rate
763	Load the training data
765	Fare - Binned Data
960	Let 's split the test data into public and private test data
321	Let 's take a closer look at the data .
1245	Size and Weekly Sales
17	Load the submission file
964	We can see that ` returnsCloseRaw10_lag_3_mean ` and ` returnsOpenMktres10 ` have the same distribution with ` returnsCloseMktres10 ` and ` returnsOpenMktres10 ` .
220	Let 's create a new feature ` commit_num ` and ` hidden_dim_first ` .
149	Let 's load the test data
236	Let 's create a new feature ` commit_num ` and ` dropout_model ` .
742	Random Forest Feature Selection
931	RLE Encoding
1153	Let 's compute the rolling mean per store .
592	Split the data into positive , negative and neutral ones
313	ROC AUC
1526	Distribution of winPlacePerc
283	Let 's create a new feature ` commit_num ` and ` Dropout_model ` and ` FVC_weight
417	Now let 's create the features for the training data .
871	Create top 100 features
1388	Let 's look at the distribution of target values for numeric features
1585	Let 's load the data
1279	Check the number of records and the null analysis
1381	Let 's take a look at the target for the numeric features
1071	Let 's take a look at the input and output objects
785	Fare Amount versus Time Since Start of Records
866	Let 's take a look at the features in the entity set .
402	Lets take a look at the test data
498	Let 's take a look at the number of unique values per columns .
933	Let 's split the data into training and validation sets
705	Let 's look at the heads of the household
288	Let 's create a new feature : commit_num , Dropout_model , FVC_weight , LB_score
1120	Let 's create a new feature : 'Sex ' , 'Neutered ' and 'Intact ' .
928	Let 's check the length of the comment .
323	Preparing the data
961	Let 's take a closer look at the date features .
1363	Let 's take a look at the distribution of target values for numeric features
1241	The unique value of Store and Type is
251	Let 's create a list of all the dates in this competition .
1317	Let 's create a new features for the family size features .
1367	Let 's look at the distribution of target values for numeric features .
612	Setting up some basic model specs
354	High Correlation Matrix
64	t-SNE for 2D features
1309	Load the pretrained model
381	Let 's create a dataframe with all the models .
492	Input Layer
1479	Now let 's train the model .
343	Examine the data
675	Coefficient of variation ( CV ) for prices in different image categories
632	Semana - Demanda_uni_equil_sum
1483	Lung Opacity of Patient
667	Logistic Regression
1034	Preparing the submission file
669	Top 100 ingredients
755	Let 's take a look at one image
1466	Importing required libraries
793	Let 's check the distribution of Validation Fares .
1160	Label Encoding for train data
287	Let 's create a new feature ` commit_num ` and ` Dropout_model ` and ` FVC_weight
722	age - Escolari
1164	Let 's now look at the class counts .
918	Exploring Credit Card Data
191	Let 's see if the items have no description .
756	Let 's have a look at the bounding boxes and labels of the image .
1413	Let 's use the ImageDataGenerator to train our model .
570	Importing the required libraries
1452	Calculate Extra Data
1238	Make Submission File
891	Let 's explore the time features and their names .
1161	Let 's take a look at the data
85	Here we calculate the age in years .
1010	Save the model
947	Get the list of input files
775	Linear Regression
1456	Import libraries
600	Let 's create a function to evaluate the gini score .
932	Let 's initialize the parser and load the data
976	Let 's take a look at the dicom data .
842	Prepare the data for training and testing .
513	Masking the image
1495	Exploratory Data Analysis
470	Importing Libraries
1336	Let 's create a random color generator
1278	Import libraries and data
142	Split the data into continuous and categorical variables
279	Let 's create a new feature : commit_num , Dropout_model , FVC_weight , LB_score
49	Let 's see if there is any difference between train and test set .
184	Top 10 categories
212	Load the data
1519	t-SNE visualization in 3 dimensions
290	Let 's create a new feature : commit_num , Dropout_model , FVC_weight , LB_score
1236	Cross-validation for XGboost
233	Let 's create a new feature ` commit_num ` , ` dropout_model ` and ` hidden_dim_first ` .
1429	United States COVID-19 Prediction
1184	Import libraries and data
1190	Let 's create a function to calculate the md learning rate
1226	Function to convert probability to rank
42	Spearman correlation
1176	Let 's look at the number of links in the dataset .
1299	Let 's see if all the numerical columns are numeric .
1076	Reshape the data for training and testing .
329	Linear SVR
655	SAVE MODEL TO DISK
1346	We can see that most of the values in the data set are relatively small . Let 's take a closer look .
404	Let 's take a look at the data
544	Let 's check the distribution of data types across the datatype .
121	Pearson correlation
595	Top 20 selected_text words
215	High Correlation Matrix
1230	Cross-validation of XGBoost and LB
538	Bathrooms & interest level
1410	We will use these features to train our model . We will use these features to train our model .
101	Let 's check how many samples we have in our dataset .
986	Label Encoding for ROB
1031	Visualizing the results
335	Ridge CV
123	Pulmonary Condition Progression by Sex
1136	Let 's load the data
1096	Let 's take a closer look at the variance of the data .
1501	Ensure determinism in the results
677	Let 's take a look at a sample of hits .
1021	Model creation and prediction
1075	Split the data into train and test sets
116	Let 's check the distribution of the whole data .
276	Let 's take a look at the last 10 commits .
1455	Create a submission file
560	Let 's create a dataframe of all the bounding boxes in our dataset .
380	Voting Regressor
1050	Let 's take a random sample of the images
90	Let 's load the training text
394	Category_count vs Image_count
532	Order Count Across Days Of The Week
902	Let 's take a closer look at the correlation of target values .
941	Load the data
286	Let 's create a new feature : commit_num , Dropout_model , FVC_weight , LB_score
187	Let 's take a look at the first level of categories
1172	Total number of tokens and unique tokens
1591	Let 's aggregate the news features
672	Let 's check the distribution of price variance within parent categories .
687	Let 's split the ` ID ` by ` _ ` and ` Subtype ` .
1203	Let 's do the same for target_train .
576	Let 's take a look at the cases of each country .
244	Let 's create a new feature ` commit_num ` and ` dropout_model ` .
1019	Load the data
1077	Permute the data
384	Here we define a high-pass filter and a band-pass filter .
483	Let 's see what we got
1191	Train and Validation Split
1282	Let 's plot the actual and forecast data .
526	Let 's add the constant to the data and fit the model .
643	Remove outliers and target column
301	Let 's split the data into train/val
1169	Exploratory Data Analysis
67	Import libraries and data
337	ExtraTrees Regressor
1335	Data loading and overview
930	Train the model
0	Target distribution
106	Load the ` before.pbz ` matrix
564	Submit
946	adapted from
1110	Some basic feature engineering
1046	Model
39	Let 's create a new column for sex , and multiply it by 1 .
1492	Import libraries and data
501	Lets take a closer look at the correlated features
746	Baseline Submission
940	Let 's split the data into two aggs
1286	Split the data into training and validation folds
401	Load the training data
1578	I 'm not sure if this is the best metric for this competition . I 'm not sure if this is the best metric for this competition .
1059	Function to load and validate images
1355	For numeric features
363	NUmber of duplicate clicks with different target values in train data
325	Importing Libraries
1268	Let 's take a look at the training data
83	Outcome Type and Neutered
1542	time_to_failure & acoustic_data
441	HIGHEST DURING THE MIDDLE OF THE DAY
1183	Create a Data Generator
874	Import libraries and data
82	OutcomeType and Sex
1544	Let us learn on a example
1459	Exploring the data
71	Load the data
1124	Let 's do the same for addr2 .
1199	Create X and Y Data
807	Save the results to a csv file
942	Feature aggregator on bureau_balance
593	Most common words in selected_text
758	Distribution of surface Value of label
1330	Check for Null values
1549	The method for training is borrowed from
678	Let 's see the number of hits per particle .
173	Let 's see the number of clicks over the day .
1127	Fitting the model on the training data
376	Ridge CV
1003	Let 's create a fake save_dir
1568	Let 's take a look at the data
911	Let 's remove variables with correlations above threshold .
1276	Create X and y
60	Let 's take a look at the connected components
1218	Evaluate and Display Validation Results
32	Let 's load the data
534	Let 's see the order count by user_id .
1498	Build the model and check what it looks like .
37	Let 's take a look at the data .
414	Computing the histogram
190	Does shipping depend of prices
430	Label Encoding
1369	Let 's take a look at the target for numeric features
456	PandasのdataFrameを出す
1217	Create Train and Evaluator
1407	Load the data
830	Run the model on the training data and test data
695	Count of Unique Values in Integer Columns
1214	Efficientnet
1119	Sexupon Outcome
620	Lasso Regression
1588	There are some assets without assetName in the training set . Let 's check the number of assets without assetName in the training set .
605	Let 's see if we find a better solution .
69	Let 's calculate the distance between the tour and the data
467	Let 's check the time taken .
474	We define the hyperparameters for the model .
1039	Now we can do the same for both train and test sets
253	Germany
1234	Logistic Regression
418	Find the best number of clusters in the test set
1362	Let 's look at the distribution of target values for the numeric features
776	Spliting the data into train and validation sets
274	Let 's take a closer look at the ` commit_num ` .
1041	Let 's take a look at the trials .
1038	Build and load the model
1471	Import libraries and data
713	As we can see , there is a huge difference in the number of ` qmobilephone ` , ` tablets-per-capita ` , ` rooms-per-capita ` and ` rent-per-capita ` .
1339	Let 's take a look at the percentages of the missing values in the object .
863	Now let 's add the missing features to the train and test sets .
1556	HP Lovecraft
345	Predicting on Test Set
122	Pulmonary Condition Progression by Sex
195	TSNE for Feature Engineering
710	Now let 's see if there is any difference between ` sanitario1 ` and ` elec ` .
1458	Add start and end positions
188	Top 10 brands
884	Correlation Heatmap
1032	Let 's look at the result
1081	Visualizing Blurry Images
1468	Let 's take a look at the ` store_id ` and ` total_sales ` .
912	Remove columns that do n't exist in above_threshold_vars
1558	Let 's remove the stopwords
1480	Quadratic Weighted Kappa
1342	Let 's take a look at the percentage of missing values for the object type .
716	Most negatively correlated variables
4	Let 's load the data
130	Count the words in a sentence
603	Public-Private Absolute Difference
1326	Create Categorical Features
1012	Pad and Resize Images
1253	Let 's check the distribution of cod_prov
477	Build and Run LightGBM
773	Let 's check the manhattan and euclidean distance
458	IntersectionId and City
908	Merge Bureau_balance and Bureau_balance_counts
813	ROC AUC vs Iteration
109	Data augmentation
18	Let 's load the data
791	Let 's plot the importance of the feature .
1449	Let 's look at the counts of IPs in the dataset .
1574	Let 's take a look at the forecasts
703	Checking for missing rez_esc and age
699	There are some households where the family members do not all have the same target .
702	Exploring missing data for v2a
178	Otsu Thresholding
1167	Build the model
975	Let 's take a look at the DICOM images
1497	Let 's take a look at the product less than the others .
193	Let 's see the description length
989	Bkg Color
1551	Let 's do the same for ` value ` column .
175	Let 's take a look at the data
606	Importing Libraries
206	Importing Libraries
1040	Load the data
1441	Let 's take a look at the size of the training data
573	The ` confirmed ` - ` deaths ` , ` recovered ` - ` recovered
692	Combinations of TTA
230	Let 's create a new feature ` commit_num ` , ` dropout_model ` and ` lb_score
1548	Glove , and paragram embeddings
1295	Plot of training accuracy and validation accuracy
386	Let 's split the raw data into train and test data
1121	Outcome Type and Neutered
1368	Let 's plot the target for the numeric features
114	Preparing the data
741	Remove features with high correlation
1111	We will use building_id , year_built , weekend , weekend_hr , cloud_coverage , wind_direction , wind_speed
909	Preparing the test data
1412	Categorize the target
143	Ensure determinism in the results
970	load mapping dictionaries
1114	Find Best Weight
899	Let 's remove features from the training and testing set .
553	Load the data
771	Distribution of Fare Amount by Number of passengers
309	Let 's take a look at the data
839	Feature Engineering - Cash
838	Exploratory Data Analysis
299	Let 's define the parameters for the LGBM model .
197	Let 's visualize the data using neato
243	Let 's create a new feature ` commit_num ` and ` dropout_model ` .
518	Let 's create a class that implements the cross_val_score method .
420	Confusion Matrix
292	Let 's create a new feature : commit_num , Dropout_model , FVC_weight , GaussianNoise_stddev , LB_score
718	Difference between Porr and Scorr
53	Let 's take a look at the log value of the training data
671	Categories of items > 1M \u20BD
700	Checking for Null values
1195	Let 's take a look at the most common annotators .
1103	Some basic feature engineering
925	Exploring the data
102	Let 's take a closer look at the real data .
879	Score as function of Reg Lambda and Alpha
1327	Load the data
959	Load the data
1587	Highest trading volumes
1157	Make a dataframe that summarizes wins and losses
1426	Let 's create a dataframe of the country-level statistics .
167	Let 's take a look at the number of click by IP
462	Let 's scale the lat and long
582	Reorder the cases by day
1581	Load the data
1194	Train/validation split
1237	Logistic Regression
992	Show the image
1057	Let 's do the same for the test set .
1567	Let 's load the data
1270	Train the model
1373	Let 's look at the distribution of target values for numeric features .
1197	Let 's do the same for target = 0 .
1379	Let 's take a look at the distribution of target values for numeric features
26	Let 's look at the feature importance .
995	Make Submission File
138	Month Temperature
833	Let 's create a function to aggregate the child features .
731	Cross Validation F1 Score
98	Merge the test and train dataframes
421	Confusion Matrix
1358	For numeric features
1569	Let 's take a look at the distribution of id_error
1321	Let 's multiply all the features by the other features .
1488	Lung Nodules and Masses
515	Normalize & Masks
1461	Selecting the neutral sentiments
59	Let 's split the ` TransactionDT ` into ` day ` and ` D1 ` .
99	Import libraries and data
1	The roc_auc_score is a measure of the roc_auc of the training data . The roc_auc_score is a measure of the roc_auc of the training data .
1296	Plot the training loss and validation loss
419	Decision Tree
740	Make Submission File
20	Distribution of muggy-smalt-axolotl-pembus
1557	Let 's take a look at the first text
1472	Let 's take a look at the rest of the data .
226	Let 's create a new feature ` commit_num ` and ` dropout_model ` .
144	Categorical and Numerical Data
997	Let 's take a look at site1 .
732	Train the model and get the feature importance .
914	Light GBM
691	Let 's take a closer look at the score .
52	Lets take a look at the log of the target values
357	Importing Libraries
50	Let 's take a look at the distribution of the data .
1211	card_id : Card identifier month_lag : month lag to reference date purchase_date : Purchase date authorized_flag : Y ' if approved , 'N ' if denied category_3 : anonymized category installments : number of installments of purchase category_1 : anonymized category merchant_category_id : Merchant category identifier ( anonymized merchant_id : Merchant identifier ( anonymized purchase_amount : Normalized purchase amount city_id : City identifier ( anonymized state_id : State identifier ( anonymized state_id : State identifier ( anonymized state_id
852	Let 's take a look at the best parameters
1064	Function to load and validate images
546	The number of stories for each parcel is equal to the number of stories for each year built .
379	AdaBoost Regressor
1562	Tf-idf vectorization
725	Let 's create a new column for the aggregated data .
373	Random Forest
615	Check for missing data
1340	Let 's take a look at the percentage of the missing values in the object .
272	Let 's create a new feature ` commit_num ` , ` dropout_model ` and ` FVC_weight
119	Expected FVC Distribution
463	Let 's take a look at the train and test data .
1469	The ` melt_sales ` function above drops the ` item_id ` , ` dept_id ` , ` cat_id ` , ` state_id ` and ` demand ` variables .
80	Exploratory Data Analysis
557	Get the size of the embedding
375	Let 's split the data into train and validation sets
362	Let 's check the results
113	Load the data
1221	Load the data
565	Finally , we create our prediction iterator .
910	Let 's align the test and train data
889	Exploring Bureau Data
778	Baseline Training and Validation
782	Random Forest
134	Let 's take a closer look at the data .
664	One-Hot Encoding
898	Let 's take a look at the target entity 'app_test ' .
155	Let 's clear the output .
720	Remove columns with correlation greater than 0.95 .
984	Importing Libraries
1146	Let 's take a look at the masks
361	Let 's take a closer look at the watershed .
796	Predicting the Test Fare
202	Let 's normalize the data to the range of -1000 to 400 .
1473	Create Model
145	Let 's explore the data
962	Let 's see what happens if we use SHAP to make predictions .
1223	Encoding Categorical Columns
46	Let 's take a look at the distribution of target values .
622	Feature Augmentation
1417	Logistic Regression
1314	Replace EDA with Float64
1053	Create test generator
1424	Let 's take a look at the time series for each country .
1573	Let 's take a closer look at the lagged data .
663	Let 's do the same for month and day
96	Load the training data
1233	Cross Validating the Model
1547	Let 's take a look at the data
159	Importing Libraries
133	Let 's free up some memory .
609	Build the model
1546	SAVE DATASET TO DISK
232	Let 's create a new feature ` commit_num ` and ` dropout_model ` .
1030	Create a submission file
24	We will use the CountVectorizer class from sklearn .
1013	Apply convolution
10	Let 's see if there are any numeric columns in the dataset .
577	In China
779	Predicting for Submission
868	Let 's take a look at the correlation matrix
825	Dropping unwanted columns
744	F1-Score
1052	Load the U-net model
1434	Let 's split the data into train and test .
224	Let 's create a new feature ` commit_num ` , ` dropout_model ` and ` lb_score
968	Italy and China w/o Hubei - Curve for Cases
58	Importing the Data
818	Make Submission File
579	Reorder Brazil Cases by day
1482	Sample Patient 1 - Normal Image
967	Logistic Growth Curve for confirmed and deaths
423	Confusion Matrix
1189	square of full data
1489	Let 's take a look at the data .
1084	Build the model
1531	Distribution of kills
548	Bathroom Count Vs Log Error
536	Using librosa.onset.onset_strength
1140	Load Image
724	Let 's take a look at the range of the data .
311	Shuffling the training data
304	F1-Score
880	Score as function of Learning Rate and Estimators
331	Decision Tree
599	Gini on random submission
23	Feature Engineering - Words
322	Train/validation split
1414	Checking for Null values
598	Gini on perfect submission
1017	Let 's take a look at some random images
389	Let 's take a look at the first 25 categories
500	Pearson Correlation of Features
51	Let 's take a look at the log value of the training data .
442	Monthly Readings ARE HIGHEST CHANGES BASED ON BUILDING TYPE
917	Exploratory Data Analysis
972	Let 's take a look at DICOM files
425	Converting Tensor to Image
563	Masks over image
266	ExtraTrees Regressor
256	Let 's drop the ` Id ` column .
1118	We will use building_id , year_built , weekend , weekend_hr , cloud_coverage , wind_direction , wind_speed
547	Bedroom Count Vs Log Error
372	Decision Tree
210	Let 's take a closer look at the feature scores .
988	Let 's start the display process .
562	Let 's take a look at the masks
1447	Convert data type to category
1486	Ground-Glass Opacities & Consolidations
426	Import libraries and data
479	Submission
448	Distribution after log transformation
75	Let 's create a ` DataBunch ` of ` sz ` and ` bs ` for ` tfms
201	Let 's resample our patient 's pixels
491	Compile the model
333	Train the XGBoost model
156	Let 's clear the output .
1391	Let 's plot the category percent of the target for numeric features
1044	Now we can do the same for both train and test sets .
1132	Fill in the missing values
953	Load the data
1247	Dept & Weekly Sales
1385	Let 's look at the distribution of target values for numeric features
1545	Load the data
151	Train/validation split
1170	Let 's take a closer look at the data
1374	Let 's take a look at the distribution of target values for numeric features
895	Let 's take a closer look at installments .
519	Cross-validation with logreg and SGD
400	Preparing the data
858	Let 's enable the notebook
1475	Let 's take a look at the data
1198	Let 's split the data into train and test .
284	Let 's create a new feature : commit_num , Dropout_model , FVC_weight , LB_score
628	Let 's see the total number of bookings per level
633	Let 's load the data
1004	Let 's take a look at the data .
339	Voting Regressor
752	Fitting the model
1577	Converting Inf to NaN
79	Make Submission File
802	boosting_type ` = { 'subsample ' : 1.0 }
680	Import libraries and data
522	LogReg and SGD Report
1116	Leak Data Loading
1319	Let 's create a new column for x_x_y_z and x_y_z .
1206	Let 's take a look at the mean price of the rooms .
1323	Create new features based on area and level
749	Train the model
1564	Let 's extract the first and last topics from the LDA .
489	Tokenization
661	Let 's take a look at nominal features
1406	Importing Libraries
1377	Let 's look at the distribution of target values for numeric features .
869	Let 's take a look at some sample features
1474	Selecting the group of observations from the test set
460	The cardinal directions can be expressed using the equation : $ \frac { \theta } { \pi Where $ \theta $ is the angle between the direction we want to encode and the north pole .
794	Fitting the model with the tuned features
12	Load and Preprocessing Steps
390	Let 's check for duplicate category names .
303	Setting up some basic model specs
1534	Let 's take a closer look at the primes .
1108	We will use building_id , year_built , weekend , weekend_hr , cloud_coverage , wind_direction , wind_speed
1058	KNN logloss on longitude and latitude
1465	Let 's add previous_visitStartTime and visitStartTime as feature .
1267	Let 's check the results .
610	In this competition we are using a convolutional neural network , which is similar to using a convolutional neural network . We are using a convolutional neural network , which is similar to using a convolutional neural network . We are using a convolutional neural network , which is similar to using a convolutional neural network , which is similar to using a convolutional neural network . We are using a convolutional neural network , which is similar to using a convolutional neural network , which is similar to using a convolutional neural network .
843	Feature importance
234	Let 's create a new feature ` commit_num ` and ` dropout_model ` .
84	Mix Outcome Type
558	Exploring the masks
1105	Fast data loading
729	Let 's take a look at the f1 score .
406	In this competition , we are going to use ` box_blur ` , ` flip_code ` and ` flip_cv2 ` .
1364	Let 's plot the kde histogram for the numeric features
1107	Some basic feature engineering
1293	Importing the required libraries
1540	Checking for Null values
110	Define learning rate
696	Let 's replace the data type for train and test .
139	Let 's drop the ` ord_5 ` column from the ` all_df ` .
264	Modeling with Ridge
267	AdaBoost Regressor
847	Boosting type and subsample
360	Let 's take a look at the importance of the features
1150	Load test data
1155	Import libraries and data
1065	Load model and make predictions on test set
36	Load OOF and Submission File
689	Let 's take a look at the DICOM files
1137	Image Augmentation
1352	Dropping null values from train and test
1264	Load the pretrained model
637	Create Lags Data
555	Let 's use the standard scaler for real features .
1002	Let 's look at the original fake paths
317	Train the model
493	CNN
1491	Let 's take a look at the sample patient data .
723	We can see that the ` inst/age ` and ` v18q/mobilephone ` techs do n't seem to be useful .
1448	Converting the date and time columns
832	PCA by Target
624	Let 's do the same for test set .
255	Andorra
537	Let 's take a look at the pitches and the magnitudes .
774	Correlation with Fare Amount
647	Using previous sucessful run 's model
246	Load the data
297	Importing Libraries
1570	Importing Libraries
92	Class Distribution Over Entries
1450	Count of clicks and proportion of downloads by device
535	Import Librosa Packages
118	Let 's check the size of the data .
1418	Import libraries and data
1301	Load test data
73	We will use the fastai v1 metric for this competition .
944	load mapping dictionaries
466	Let 's create a function to get the image paths and the image id
994	Let 's take a look at the DICOM files
649	RLE Encoding
1043	Let 's take a look at the public and private data
1404	EMA & MACD
1117	Some basic feature engineering
30	Create Submission File
1499	week_of_year ( week of year
781	Let 's check the correlations .
1343	Let 's take a look at the percentage of values for each feature type
1400	Let 's take a look at the target for the numeric features
231	Let 's create a new feature ` commit_num ` , ` dropout_model ` and ` lb_score
974	Let 's take a look at the keyword dictionary
497	Checking for Null values
1390	Let 's plot the category percent of the target for the numeric features
607	Load and Preprocessing Steps
1256	Let 's take a look at the training data
676	Importing the required libraries
529	Train the model
572	First day entry , last day reported , total of tracked days
924	CNT_CHILDREN ` - number of children , grouped by ` TARGET ` and unstack it .
112	Compile and fit the model
1522	Let 's take a look at the F1 score of the model .
804	Save the results to a csv file
248	Importing Libraries
949	merchant_id_cat & merchant_id_num aggs_cat_num
1254	Import libraries and data
580	Reorder China Cases by day
1525	Importing Libraries
627	Let 's see the total bookings per year .
1451	Let 's take a look at the ratio of click hour to is_attributed .
263	Let 's split the data into train and validation sets
1403	MA_7MA , M_15MA , M_30MA , M_60MA
617	Random Forest Regressor
715	Sinusoidal Correlation
686	Let 's take a look at the image with the size 256x256 .
841	Merging Credit Info
1163	Let 's check the labels that are not present in the train set .
1125	Let 's do the same for addr2 .
906	Merge Bureau_balance and Bureau_balance_counts
1178	Number of Patients and Images
1260	Compute F1 scores for Valid Data
973	Let 's take a look at the patient name
469	Make Predictions on Test Data
196	Let 's create a BulgeGraph object from the structure and the sequence
697	There are some households where the family members do not all have the same target .
44	Get the embeddings from the training data
659	Let 's check the correlation of the target variable .
137	Let 's check the number of unique values and the number of NAN values
1139	Let 's visualize the augmented images
1224	Drop calc columns
1281	Function to extract a series from a dataframe
97	Load test data
239	Let 's create a new feature ` commit_num ` and ` dropout_model ` .
1126	Make Submission File
1566	Submit to Kaggle
963	We can see that ` returnsCloseRaw10_lag_3 ` and ` returnsCloseRaw10_lag_4 ` do n't seem to be the same .
934	Make Predictions
199	Let 's visualize the data using neato
307	In this competition , we will use a dropout rate of 0.15 and use it for training .
658	Let 's check the correlation of the columns .
1484	Lung Nodules and Masses
213	Let 's take a look at some random data
86	Age Category Calculation
124	Importing Libraries
120	FVC Difference
508	Let 's load the data
228	Let 's create a new feature ` commit_num ` , ` dropout_model ` and ` lb_score ` .
436	One-Vs-Rest Classifier
1026	Build dataset objects
1066	Let 's split the data into train and validation sets .
25	Make Submission File
451	Dew Temperature
867	Let 's take a look at the feature matrix and feature names .
1382	Let 's look at the distribution of target values for the numeric features
1187	Let 's do the same for the test set
1180	Load the data
587	Let 's take a look at the data
814	Bayes Optimization Boosting Type
289	Let 's create a new feature : commit_num , Dropout_model , FVC_weight , LB_score
136	Number of unique values in each column
1285	Function to calculate the squared sum of elements
1292	Let 's take a look at the maximum FVC for each patient .
770	Let 's take a look at the absolute latitude and longitude differences
896	Let 's take a look at the most recent feature .
34	identity_hate
1033	Let 's take a look at the first 10 detection scores
1298	Let 's see if there are any missing values in ` df_train ` .
1086	Submit to Kaggle
1439	Read in the data
1284	Let 's take a look at the best model for the proposed model .
789	Let 's define the features and the time_features
140	Label Encoding for Continuous Features
388	Let 's look at the test data .
846	Let 's take a look at the results
433	Let 's take a look at top 20 tags
1351	Group Internal Battery Type
971	Let 's take a look at the data
48	Let 's take a look at the target .
1087	Exploratory Data Analysis
916	Import Packages
856	Let 's create a csv file to hold the results
1023	Fitting the model
704	Let 's see if we have covered every variable
1420	China
257	Linear Regression
1401	Let 's take a look at the target for the numeric features
66	Let 's split the data into train/val
1037	Training History
870	Let 's take a look at the feature importance
708	Let 's take a closer look at the ` epared1 ` and ` epared2 ` .
829	Let 's remove the features with importance below 0.95 .
416	Let 's take a look at the total units sold .
668	Top 10 label names
543	Importing Libraries
1182	Spliting the data into train and validation sets
1133	Let 's check the rest of the ids_31 .
1011	Let 's take a look at the image .
432	Let 's take a look at the word cloud of each tag .
877	Let 's create a new column for the score .
41	Load the data
905	Let 's create a function to count categorical variables
920	Load the model
1446	Let 's load the data
799	Baseline Model AUC
94	Let 's take a look at the first 100 words
1575	Let 's split the data into train and test
1007	Train the model
1478	Preprocess the data
344	Plot of training and validation loss
1453	Load the train and test data
1062	Make Submission File
805	Hyperopt Tpe
293	Let 's create a new feature ` commit_num ` and ` Dropout_model ` .
312	Preparing the data
223	Let 's have a look at the ` LB_score ` feature .
1131	Label Encoding
748	Save the trials to a json file
530	Data loading and overview
415	Let 's take a look at the test image
1265	Let 's take a closer look at the trainable variables
792	Let 's see the number of features we have in the dataset .
1389	Let 's plot the target for the numeric features
428	Train the model
241	Let 's create a new feature ` commit_num ` and ` dropout_model ` .
982	Let 's take a look at the training data .
150	Create Testing Generator
521	Let 's evaluate the threshold .
227	Feature Engineering : LSTM
1173	Setting up some basic model specs
1240	Let 's take a look at the date .
1216	Setting up some basic model specs
1310	Importing Libraries
820	Importing Libraries
913	We can see that the ` train ` and ` test ` data sets do n't have the same number of unique values . Let 's drop them .
707	Area1 and area
161	Let 's take a look at the available files
413	Create a DataGenOsic object
621	Ridge Regression
162	Pushout + Median Stacking
1093	Let 's take a look at the scatter plots .
861	Let 's do the same for the test set .
673	Coefficient of variation ( CV ) for prices in different categories
706	Remove columns with correlation greater than 0 .
455	Let 's do the same for the test set .
1428	Let 's load the full table of the us counties .
737	ExtraTrees Classifier
1142	Train the model
305	Train the model
1315	Replace edjefa
54	Let 's take a look at the log of the test data
8	Data loading and overview
762	Submission
154	Save the model
681	Importing the Data
332	Random Forest
1532	Let 's check the correlation of winPlacePerc .
320	Let 's define a function to return the binary target .
221	Let 's create a new feature ` commit_num ` , ` dropout_model ` and ` lb_score ` .
242	Let 's create a new feature ` commit_num ` and ` dropout_model ` .
1454	Let 's check the score again .
848	Lets take a look at the learning rate distribution
308	Lets take a look at the word clouds
1228	Logistic Regression
207	Let 's use XGBoost to train the model .
214	The ` entity_from_dataframe ` method accepts a ` dataframe ` as an argument . The ` entity_from_dataframe ` method accepts a ` dataframe ` as an argument .
1274	Merging Bureau Data
831	Principal Component Analysis ( PCA
567	Data Cleaning
1186	Let 's take a look at the images
447	Let 's check the correlations .
204	Importing Libraries
473	Importing Libraries
1533	Exploratory Data Analysis
1243	Size and Type
1502	Load the data
1565	Hilbert & Hann
1432	diff h1 and d1
757	Load the data
443	Let 's take a look at the HIGHEST READINGS
806	Hyperopt
549	Vs Log Error
991	Let 's add a cylinder
2	Feature Engineering
788	Spliting the data into train and validation sets
19	Lets take a look at the target distribution
461	Creating dummies from city column
203	As a final preprocessing step , it is advisable to zero center your data so that your mean value is 0 . To do this you simply subtract the mean pixel value from all pixels . To do this you simply average all images in the whole dataset . To do this you simply average all images in the whole dataset . To do this you simply average all images in the whole dataset . To do this you simply average all images in the whole dataset . To do this you simply average all images in the whole dataset , you simply average all images in the whole dataset . To do this you simply subtract the mean pixel value from the
132	Here is a function to clean up the text
349	Infinity Generator Generator
583	Reorder Cases by Day
1174	Adding PAD to each sequence
654	Let 's drop the timestamp column and use it for prediction .
141	Split the data into train and test
1586	Let 's remove data before 2012 .
760	Let 's take a look at the distribution of train and validation set .
452	Wind Speed
1387	Let 's look at the distribution of target values for numeric features
254	Albania
674	Let 's load the train and test data
1392	Let 's look at the distribution of target values for numeric features
767	Let 's take a look at the percentile of the data
294	Let 's take a closer look at the score of the commit .
619	Linear Regression
1510	Create a video
328	SVR
1246	IsHoliday vs Weekly Sales
1097	We can see that ` train ` and ` test ` have the same structure as ` sample_struc ` . We can see that ` test ` and ` sample_struc ` have the same structure as ` sample_struc ` .
1094	Calculate SNR for each class
475	Submission
38	Let 's take a look at some of the images
1383	Let 's look at the distribution of target values for numeric features .
1554	Load the training data
476	Merge Transaction and Identity Dataframes
1485	Lung Nodules and Masses
745	Distribution of Confidence by Fold and Target
393	Importing the training data
709	walls+roof+floor
371	SGD Regressor
126	Hounsfield Units ( HU
429	Let 's plot the bayesian blocks
881	Number of estimators vs Learning Rate
993	Let 's create a file that contains the code of the slicer .
172	Let 's take a closer look at the missing values
1092	Feature importance
56	Let 's take a look at the percentage of zeros in the training set .
365	Let 's take a look at the complete dataset
494	Model
1287	Importing Libraries
545	Let 's take a look at the correlated features .
1320	Let 's split the data into train and test sets .
872	Remove Low Information Features
427	Preparing the data
817	Let 's check the cross validation score on the full dataset .
894	Average Term of Previous Credit
1162	Let 's now look at the number of unique attributes in the dataset .
719	Let 's check the correlations between these variables .
1158	Train the model
280	Let 's create a new feature ` commit_num ` and ` Dropout_model ` and ` FVC_weight
801	boosting_type ` : type of boosting to use for training
750	Confusion Matrix of Poverty
1592	Remove columns with type ` object ` .
1511	Let 's create a video for the first patient
764	Distribution of Fare Amount
45	Let 's take a look at the target distribution
584	Let 's load the data
1101	Fast data loading
1091	We define the hyperparameters for the model .
1312	Load the test and train data
336	Bagging Model
1072	Import libraries and data
739	Make Submission File
472	Bayesian Feature Engineering
769	NYC Map Zoom
735	Linear Discriminant Analysis
1415	Exploratory Data Analysis ( EDA
454	Label Encoding
81	Mix and not breed
1067	Load the test and submission files
571	Let 's take a look at the covid
662	Let 's do the same for ord_1 .
1201	Train the model
368	Linear Regression
488	Let 's take a look at the hash of the text
1248	Dept & Weekly Sales
1444	Let 's take a closer look at the data .
780	Fitting and Evaluating the model
205	Get dummies of the data
887	Ordinal Variable Types
377	Bagging Model
921	Split the data into training and validation sets
601	Let 's take a look at the score of each sample .
247	Ensembling
1273	Oversampling the training dataset
1318	Replace NaNs with 0s
1307	Train the model
1576	Let 's load the data
1290	Mean Squared Error
480	Import libraries and load data
1225	Drop calc columns
938	LightGBM
736	KNN with nearest neighbors
424	Confusion Matrix
929	Training the model
157	Version
504	Define Train and Test Data Paths
277	Let 's create a new feature ` commit_num ` , ` dropout_model ` , ` FVC_weight ` and ` lb_score
1553	Importing Libraries
575	Deaths by date
407	Let 's take a look at the images and compare them .
878	Random Search and Bayesian
753	Let 's take a look at the ` tree_limited ` .
1289	Let 's split the data into train and test
552	Augmentations with GaussianTargetNoise and TemporalFlip
855	Predicting ROC AUC on the test set
291	Let 's create a new feature ` commit_num ` and ` lb_score
1145	Open the mask
465	MNCAATourney & Season Detailed Results
795	Fitting and Evaluating the model
784	Let 's extract the ` pickup_datetime ` from the ` test ` dataframe .
1249	Let 's take a look at the training data .
1056	KNN for Classification
639	Configure hyper-parameters Back to Table of Contents ] ( toc
103	Let 's take a look at the model performance
936	Let 's take a look at aggs_all_cat and aggs_medium
556	Let 's take a look at the full text
660	Day Distribution
340	Let 's create a dataframe with all the models .
245	Let 's take a look at the best commit score .
1220	Make Predictions
315	Let 's remove the ` base_dir
1231	Cross-validation of XGBoost and LGBM
1443	Let 's take a closer look at the time series .
1015	Title Mode Analysis
1205	Mode of build year by product type
219	Let 's create a new feature : ` commit_num ` , ` dropout_model ` and ` lb_score
439	ELECTRICITY MOST FREQUENT METER TYPE
457	Top 50 most commmon IntersectionID 's
435	Multilabel Feature Engineering
783	Let 's see the distribution of the fare amount from the test set .
319	Function to create filename
954	Preparing the data
174	Download rate evolution over the day
1329	Import libraries and data
1328	Make Submission File
915	Top 100 Features
1115	Fast data loading
302	We define the hyperparameters for the model .
945	extract different column types
815	Let 's take a look at the boosting_type
566	Let 's take a look at the test data
1082	Make Submission File
490	Build the model
507	Reducing the target
983	Create X_test
1563	Latent Dirichilet Allocation
374	Train the XGBoost model
542	Let 's take a look at the highest probability of each class in each row .
387	Let 's take a look at the training data
634	Exploring the data
951	Let 's join the ` new_merchant_card_id_cat ` and ` new_merchant_card_id_num
1028	Train the model
531	Time Series Analysis
1049	Pad and Resize Images
980	Let 's take a look at the patient 's DICOM files
57	Let 's take a look at the mean squared error .
431	Check for Duplicates
800	Let 's take a look at the learning rate distribution
1370	Let 's plot the target for the numeric features
366	Computing the histogram
985	Let 's create a new feature : TransactionAmt , dist1 and dist
520	Logreg on logreg and SGD
1025	Load the data
797	Importing Libraries
882	Number of estimators vs Learning Rate
273	Let 's create a new feature ` commit_num ` , ` dropout_model ` and ` FVC_weight ` .
1232	Cross-validation with LGBM
875	Let 's look at the hyperparameters
1341	Let 's take a look at the percentage of the missing values in the object .
1496	Evaluate the model
355	Linear Regression Model
554	factorize for categorical features
816	Simple Feature Engineering
1257	Load the data
314	Binary Classification Report
1277	Define a Random Forest Classifier
405	Let 's take a look at the results
1283	Lets take a look at the data
568	Selecting top 15 features
217	Importing Libraries
1063	Let 's split the ` EncodedPixels ` into ` ImageId ` and ` ClassId ` .
1271	Let 's load the training dataset and get the most common labels
652	Let 's split the data into two quantiles
578	Italy
437	Importing Libraries
1538	Let 's take a look at application features .
300	Building XGBoost model
1316	Continous Features
1507	Add train leak
128	Let 's take a look at the segmented data .
1168	Importing Libraries
1515	Replace non-vulnerable with Vulnerable
1213	Let 's take a look at the data .
996	Let 's load the submission file for site_id = 0 .
505	Let 's see the shape of the target data .
650	Exploring Missing Values
403	Let 's take a closer look at the time_to_failure column
1300	We can see that there are some columns with values less than 256 and less than 32767 . Let 's take a look at them .
1405	VMA_7MA & VMA_15MA
844	Load the features and labels
1322	Let 's split the data into train and test sets .
809	Find Best Feature
478	Importing Libraries
1518	T-SNE for numerical features
21	Let 's take a look at the distribution of the data .
486	Vectorize the data using HashingVectorizer
100	Let 's take a closer look at the results .
499	Let 's take a closer look at other features
70	Let 's take a closer look at the data
487	Lets take a look at the sequence of words in the text
1000	TPU Strategy and other configs
1386	Let 's look at the distribution of target values for the numeric features
468	Importing Libraries
1487	Pleural Effusion vs Normal
1360	Let 's look at the distribution of target values for the numeric features
269	Let 's create a dataframe with all the models .
1425	Let 's take a closer look at the prediction of COVID-19 .
270	Dropout Model
1477	Ensure determinism in the results
859	Boosting Type for Random Search
730	Let 's create a pipeline for training and testing data .
903	Target Correlation
189	Top 10 categories of items with a price of 0
528	We define the hyperparameters for the model .
370	Linear SVR
1055	Let 's load the data
644	Let 's split the labels into a list of labels
608	Setting up some basic model specs
1384	For numeric features
759	Replace NaNs with 0s
688	Let 's create a function to get the filepath of the image .
864	Let 's take a look at the aggregation type
1288	Spearman Feature Correlation
516	Fill NaNs
1215	Let 's load the test data
684	Let 's take a look at the number of features with all zero values .
77	Let 's start with resnet
819	Cross Validation Score on the full dataset for Bayesian optimization
446	Distribution of meter readings by primary_use
923	CNT_CHILDREN ` - number of children
65	Preparing the data for training .
1589	Next , let 's explore the numerical variables .
225	Let 's have a look at more details about the model .
656	Importing Libraries
509	Let 's take a look at the data .
1467	Let 's take a look at the total sales for each state .
502	Merging Applicatoin Data
1372	Let 's plot the target for the numeric features
1263	Load the pretrained models
1098	Let 's take a closer look at the results
1022	Train the model
694	Let 's load the data
1504	LOAD DATASET FROM DISK
1500	Importing Libraries
1141	Efficient Detection
1109	Fast data loading
153	Let 's check the fbeta score .
981	Let 's take a look at the image .
1435	Let 's define the unique and other features .
588	Runs differential evolution
1366	Let 's look at the distribution of target values for numeric features .
40	Let 's look at the feature importance .
177	Lets take a look at the shape of the image
76	Function to calculate the F1 score
1393	Let 's look at the distribution of target values for numeric features .
586	Let 's see if the model can be run on the fly .
685	Let 's check the distribution of the target values .
1350	Checking for Null values
484	Let 's take a look at the vectorized text
897	Let 's take a look at the features of the entities in the entity set .
840	Exploring Credit Card Balance
1235	Applying XGBoost on LB
693	Import libraries and data
194	Description length VS price
135	Let 's take a look at the data .
618	KNN Regressor
95	Word Distribution Over Whole Text
1159	Make Predictions
1357	For numeric features
93	Dropping Gene andVariation Columns
1348	Merging Applicatoin Data
249	In this competition we are using a simple linear regression model and a simple linear regression model . We are using a simple linear regression model and a simple linear regression model .
285	Let 's create a new feature ` commit_num ` , ` dropout_model ` and ` FVC_weight
1129	Let 's take a look at the data
835	Lets take a look at previous_application.csv
1409	Exploring missing data
893	Let 's take a closer look at the previous entity 's name_CONTRACT_STATUS
1561	Lemmatization with CountVectorizer
1505	Glove , and paragram embeddings
958	Make Submission File
1045	Building and building the model
1345	We can see that most of the values in the data set are relatively small . Let 's take a closer look .
517	Let 's apply log1p transformation to the revenue .
72	Examine the data
1508	Selecting features with RMSLE
1513	Let 's look at the categorical features and the numerical features .
166	Let 's check the length of the different values .
810	Save the trials in json format
1537	Lets take a closer look at the other features .
1436	Let 's check the distribution of the minute .
1395	Let 's take a look at the target for the numeric features
1530	killPlace Variable
252	Italy
1016	Simple XGBOOST
888	Let 's replace missing values with np.nan
89	Let 's clean the data .
683	Let 's check the number of features with all 0 values .
1027	Model building
690	Get Patient Data
481	Train the LGBM model
589	Plot the infection peak for sake of visualization
550	No Of Storeys Vs Log Error
1583	Lidar Data Exploration
1440	Exploratory Data Analysis
1144	Converting categorical features to category
348	Let 's start with a simple example
33	N-Grams
998	Leakage Site
258	SVR
324	Quadratic Weighted Kappa
1430	Importing Libraries
943	Feature Engineering - Credit Card Balance
395	Let 's take a look at the ` img ` column .
115	store_id , item_id unique value counts
1338	Let 's take a look at the percentages of the missing values in the object .
341	Define a function to calculate the IoU .
883	Correlation Heatmap
1047	Let 's create the folders
334	Let 's split the data into train and validation sets
798	Train the model
1297	Number of data per diagnosis
208	Fitting and transforming the data
717	Most negative Spearman correlations
35	Import libraries and data
1083	Preparing the test data
91	Gene Frequency Plot
111	Preparing the data
1151	Let 's see the distribution of var_91 for train and test set .
29	Let 's take a look at AUC and Gini scores
950	Let 's check the column types again .
1210	merchant_id : Unique merchant identifier merchant_group_id : Merchant group ( anonymized merchant_id : Merchant identifier ( anonymized subsector_id : Merchant group ( anonymized purchase_amount : Normalized purchase amount city_id : City identifier ( anonymized state_id : State identifier ( anonymized state_id : Merchant identifier ( anonymized state_id : Merchant identifier ( anonymized state_id : Merchant identifier ( anonymized state_id : Merchant identifier ( anonymized state_id : Merchant identifier ( an
511	Let 's convert the images to grayscale .
541	Setting up some basic model specs
180	Let 's check if there are any separate objects in the dataset .
1463	Let 's load the cities data and save it for later .
956	Show Validation Masks and Predictions
772	Let 's load the test data
670	Categories of items < 10 \u20BD ( top
1445	Let 's load the data
1380	Let 's look at the distribution of target values for the numeric features
837	Feature Engineering : installments_info
1442	Let 's take a look at some random lines
1102	Leak Data Exploration
1493	Exploring the data
1503	SAVE DATASET TO DISK
1365	For numeric features
1068	Compute the text and questions for the test set .
900	Let 's align the test and training data with the feature matrix
666	Concatenate full data
1313	Checking for Null values
1394	Let 's take a look at the target for the numeric features
200	Let 's take a look at one of the patients .
170	Let 's check the download ratio .
1204	Train the model
165	Let 's take a look at the data
1250	Batch Mixup
777	Fitting the model
367	Function to return image data as numpy array
422	Random Forest
1261	Predicting on test set
510	Function for getting a single image
591	Lets take a look at the word clouds
359	tanh function
1054	filtered_sub_df ` and ` null_sub_df
1411	One-Hot Encoding
268	Voting Regressor
1337	Let 's take a look at the percentage of missing values for the object type .
1494	Define lifted function
1244	Type and Weekly Sales
1229	BernoulliNB
853	Fitting and predicting the model
1154	Let 's take a closer look at the trend of each store .
1051	Pivot File and Type
125	Let 's take a look at the DCM files
459	Road Encoding
1024	Create fast tokenizer
721	Education Distribution by Target
271	Let 's create a new feature ` commit_num ` , ` dropout_model ` and ` FVC_weight
1200	Create Train and Test Set
1179	Let 's take a look at the data
892	Distribution of Trends in Credit Sum
948	Check for Null values
1431	age , gender , hospital death , bmi
640	Quadratic Weighted Kappa
198	Let 's create a BulgeGraph object from the structure and the sequence
1112	Leak Validation for public kernels
1122	Importing Libraries
1135	Importing Libraries
927	Let 's load the data
295	FVC & Confidence
