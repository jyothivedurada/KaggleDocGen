220	Highlighting the upper triangle
1700	Evaluate the functions
1568	Pinball loss for multiple quantiles
1507	remove images and labes during training
159	Separate objects and their labels
1676	Lung Opacity
503	scale pixel values to grayscale
294	patch predictions for each image
1009	drop variables with too many missing values
1108	Load image file
1738	Import and Read Data
1364	Address change function
366	Linear SVR model
1391	Draw bounding boxes on the image
1539	Preparing the testing series data
668	Read the Data
1519	Number of repetitions for each class
1438	Create datagen
789	ignore all warnings
468	Loading the data
68	if augment then horizontal flip half the time
354	check if everything is ok
1235	Draw the line on the image
299	Read in image and resize it
1623	Logistic Regression on all data
949	Plot the parameters
10	merge weather data
464	Merge datasets
1601	checking missing data
594	text features to full text
515	Calculating the confusion matrix
353	Here we create a simple model
892	Find the columns with more than 0.75
611	if save to dir
584	Create dataframe with highest probability for each row
1027	get score on best score
322	Voting Regression
1773	for numerical regressors
125	Load and Preparation
321	Fitting our model
1410	Set up some basic model specs
8	merge with building info
1288	Load dataset info
985	Example of data
266	There are some missing values in the dataset
396	Calculate the confusion matrix
698	Applying CRF seems to have smoothed their prediction
779	assume age is in centimeters
1554	Train the model
677	filtering out out outliers
656	COMBINE TRAIN AND TEST DATA
13	Train and Test data
1306	Importing relevant Libraries
773	Correlation of all the heads
545	Select Percentile
333	Read the DICOM files
1171	Save images to a GIF file
79	Resize image to desired size
826	Read the image on how to visualize it
1007	Average of all Repaid targets
820	Non Limitable Classifier
1041	Clean up memory
1455	inverse transform for test data
651	Create submission file
1324	Change namedtuple defaults
1411	Create the layout
344	create my generator
1492	Predict on validation set
1129	Generate train and test paths
189	Shortest and longest coms
202	Determine current pixel spacing
901	credits to for the parameters values
782	change column names
311	Target Predictions
89	Improvement clearly visible
1045	Joining common values with main dataframe
998	Normalized Mode Count
1105	load mapping dictionaries
1414	sorted by count
567	A simple Keras implementation that mimics that of
268	Custom Loss
1435	take a look of .dcm extension
632	Load the data
1072	You can access to the actual image itself like this
337	Train the model
412	Importing necessary librarys
21	Check for missing values in training set
1614	Split the train dataset into development and valid based on time
606	Only the classes that are true for each sample will be filled in
966	Plotting the feature by target
1663	kick off the animation
1707	Find the program that we have
1075	Getting the Predictions
978	There might be a more efficient method to accomplish this
743	pivot to have the same dimensions as the train dataframe
721	nominal columns countplots
710	PRINT CV AUC
769	Get the rest of the heads
447	Encoding the Regions
637	Import and Read Data
1017	Sort the table by percentage of missing descending
870	Write column names
663	Create a date aggregation for
96	Save before to before.pbz
140	Set seed for reproducability
1465	Define dataset and model
1735	set color palette
1538	actual is lagged here
1024	check for encoding
742	The DataFrame has the following format
919	Brand new features
386	Verify that length is correct
1029	returns the dataframe describing our scores
1655	Draw the heatmap using seaborn
1388	Stack the bboxes into a single image
1620	Checking for Null values
57	Seeding everything for reproducible results
1449	create the dataset for regression
1370	Label Encoding of District
1791	Flatten the date columns
256	Voting Regression
1486	Here we are getting the pretrained model
477	Vectorizing the data
1739	distribution of winPlacePerc
1786	load a piece of data from file
1733	Import the necessary libraries
198	display threshold image
1446	Order does not matter since we will be shuffling the data anyway
659	Performing feature agglomeration
1313	Light GBM Results
1774	Shuffling happens when splitting for kfolds
1429	make train features
729	CV with standard deviation
1656	written by MJ Bahman
381	Convert item data to bson representation
403	Train the model
836	Zooming nyc map from
225	Scatter plot of COVID
1265	Create train and validation datagens
326	Initialize patient entry into parsed
312	Patient is lying on their stomach
1540	Fit on all data
1795	Fit the second model
14	visualization of Target values
1713	Building the model
1121	Extract target variable
672	Create list of models to use
1732	Samples which have unique values are real the others are fake
1311	Display current run and time used
759	households without head
64	Distribution of continuous variables among continuous variables
91	Augmentations and Feature Engineering
1038	Previous aggregation function
1014	Bureau memory leak
1212	Applying Quadratic Spline
528	Create the Pipelinees
1593	fill up the missing values
298	Read in image and resize it
274	Strings and Directories
499	handle .ahi files
990	Example of installments due date
1078	Set values for various parameters
1404	Let us encode the input data first day by day
623	Adding the country column
1799	shift test predictions for plotting
937	This is the out file
888	Merge the bureau data with the previous features
1182	in case of large numbers , using tf.function
1665	fill in mean for floats
775	Draw the heatmap
1221	Find the best score
6	eliminate bad rows
1644	Clicks withing the same amount of time
190	Distribution of the length of the description
1190	Event code distribution
1301	load test data
994	What is the most common client type where Contract was approved
480	Tokenize a piece of text
90	fast less accurate
1789	Forceasting with decompasable model
382	Exploratory Data Analysis
760	Histogram of heads only
1482	set the necessary directories
131	Prepare Testing Data
1127	Initialize the data
1191	Predicting the test set
848	Create random Forest Object using the parameters
350	Linear SVR model
795	delete nestimators from the dict
1309	Fetch a batch of videos
1281	Libraries and Configurations
1168	Adding some new features from original data
747	Wrapper function to create a backbone model from its parameters
845	Fixing missing values
1192	fill in missing values with
1238	Create a session and run
816	Getting the predictions in the right format
1008	Correlation between columns
1586	Correlation with the Target
30	The following cell defines the train and test data sets
988	Lets plot the example cash accounts over time
665	date agg dữ liệu
1010	Add the column name
1392	Save the dataframe to the parquet file
1058	Loading the best weights and weights
756	Visualizing distribution of features by level
1453	Drop rows with NaN values
1215	Only load those columns in order to save space
1520	LIST DESTINATION PIXEL VALUES
634	Run Sir and Sir examples
380	Verify that length is correct
1406	Get just the digits from the seeding
445	Extracting informations from street features
356	Read the image from image id
939	Create dataframe with scores
449	Setting the target variable
1434	if unk is True and there is no previous word
1012	Add the column names
548	Fit the best model
1202	Load Model into TPU
547	Run Grid Search
1784	Compute the STA and the LTA
1475	MAKE MIXUP IMAGE
1566	same as above
339	Make predictions on test set
1352	Leak Data loading and concat
1368	Display the map
981	Replace day outliers
1366	Read in the Data
610	Return the absolute path of a file
1749	Create an entity from dataframe
178	Get the mean price for each category
184	Brand name price
1187	Get the sample size for the test set
556	create feature dictionary
1734	Imputing missing values
418	Preview of Data
1756	Relationship between applications and previous applications
687	Get a sample from the dataset
324	Save the data
1671	Reading the csv files
410	OneVs Rest Classifier
530	Fit the best model
1332	Depthwise convolution phase
1423	function for transform sentence to list of words
152	How many times each categories of clickers download the app
879	Iterate through random parameters
1255	Load Model into TPU
980	Place the values into different variables
78	save dictionary as csv file
307	Tokenize the text
671	Joining all the rows with the selected columns
1574	Looking at the data
1403	Compare the encoding intervals
1717	LOAD PROCESSED TRAINING DATA FROM DISK
1600	Diverging color palette
1642	Most IPs in train data
318	Decision Tree Regression
1565	Import the necessary modules
1044	Remove missing columns
573	Exploratory Data Analysis
1262	code and base image path
1595	Pad the sentences
1563	import xgboost as xgb
852	Fare amount versus time since start of records
838	Plot the binary features
1106	Load metadata file
460	Training the random search
247	Apply exponential transf
36	Log target variable
1505	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
395	Matrix of Confusion
1581	coluns with newchoes
1502	Order does not matter since we will be shuffling the data anyway
212	compute the mean value of each feature
1765	PLOT FOR RELEASED TIME
1743	EXTRACT DEVELOPTMENT TEST
232	Double check that there are no informed ConfirmedCases and Fatalities after
1798	shift train predictions for plotting
609	if save to dir
1721	LOAD DATASET FROM DISK
1641	Most IPs in train data
213	creating dummies columns
1808	How many transactions are there
748	Load the trained weights
604	Convert wav data to raw data
812	Write column names
891	Drop the columns from the testing set
1393	Transform categorical features into cateogry
757	Create a bar chart
1259	Using original generator
204	Remove other air pockets insided body
555	Defining the data types
1470	Scale the data
473	Loading the data
250	Linear SVR model
375	Run the build process
520	Train the model
50	What are the most frequent variables in the dataset
1625	New table with necessary information
1491	Returns a dictionary of counts
1082	A single set of features of data
1354	Fast data loading
206	CONVERT DEGREES TO RADIANS
1318	build the train and test dataframes
1390	Use the dataset class and create train dataset
11	Compute the STA and the LTA
245	Filter Andorra , run the Linear Regression workflow
641	For negative words , the top words are most common
387	Convert item data to bson representation
330	Visualizing the images
510	process remaining batch
1654	Exploring the correlation values
200	for later plotting
975	iteration score 两列
424	PLOT FOR METER READING
792	Merge with predictions
940	Sort by score
620	Area of Bounding Boxes
290	cleanup working directory
345	Fully connected generator
1761	Label encode categoricals
1073	The target variable is most common
1360	Leak Data loading and concat
639	Segregating the sentiment data
1543	The SMAPE scores are
1771	text version for squash
1248	Load and preprocess data
600	Get the mask directory
1602	Moving Average of all the values
1649	Lets add extra features from the Time series
1149	Get the preprocessed data
272	configurations and main hyperparammeters
1396	Get the date information with an epoch
894	Light GBM Results
54	Examine the duration of the taxi trip
99	intialize the network
359	Segm RGB image
60	Submit Test Predictions
647	Import the Libraries
1769	SAVE DATASET TO DISK
1211	Here we take all the images and resize them
1183	We need to convert the weightage into a list
862	Standard deviation of best score
719	save preprocessed weights
749	This is the primary method this needs to be defined
1142	Sum up the importance values
1147	Unique IDs from train and test
1338	The first block must take care of stride and filter size increase
1274	Each color is one of the original images
1509	LIST DESTINATION PIXEL VALUES
438	We will look at the dimensions of our data
1081	Fit the model
451	Only the classes that are true for each sample will be filled in
1725	The method for training is borrowed from
454	Run the object detection on the image
746	Get the counts of each series
615	An optimizer for rounding thresholds
1452	sort by visit day
1680	the time spent in the app so far
1252	Load Model Weights
902	Train the model with early stopping
392	Read and resize random images
373	Compute the STA and the LTA
367	SGD model
943	ROC AUC on test data
1622	Print the feature ranking
1472	size and spacing
882	Create a LightGBM Random Search
320	Gradient Boosting
1445	we assume we are less accurate here , boost confidence
335	Looking some informations of all datasets
603	Output class encoding
726	Image of the Training Photo
1116	Returns the counts of each type of rating that a rater made
531	Classification of Test
332	read in the images
1278	Find the objects identified by this object
288	sklearn , Lasso , and auc
1780	The wordcloud of the raven for Edgar Allen Poe
174	Load an image from a file
513	Conference Tourney Games
532	Create the Pipelinees
536	Create the Pipelinees
1667	Stacking of Test Predictions
526	Fit the best model
897	Plot the cumulative importance
1294	warm up model
896	There might be a more efficient method to accomplish this
864	Fitting and predicting the baseline model
1744	FITTING THE MODEL
365	SVR model on train and test
1308	Fetch a batch of videos
1087	Transform the coverage class
1569	Initial Bayesian Optimization
376	Mean absolute error
979	get the categorical variables
1003	dfs to get a matrix of features
187	Descriptions and Items have no
439	Top most commmon IntersectionIDs
1545	Create new column name
739	Compute the ratio of discriminant
830	Lets start with the class labels
248	Getting the probabilities for test data
423	Monthly READINGS ARE HIGHEST RMSE
1242	Run the detector on the image string
1140	dependence plot for returnsClosePrev
1233	Build datasets objects
1757	recuring features based on previous applications
876	iteration score 两列
1591	Load best model and check the performance
1339	Final linear layer
714	Count the missing values in each column
191	Display scatterPlot between description length to price
931	count combinations of parameters
434	Exploring the data
348	Highlighting the upper triangle
1427	Set values for various parameters
217	MinMax scale all feature scores
492	Calculate Correlation Matrix
1277	Find the objects identified by color
1343	Number of steps affected by this
428	all other columns
1779	Generate the Mask for EAP
1395	Draw the graph of molecules
1709	Importing sklearn libraries
334	Looking some informations of all datasets
1299	Embedding function for feature extraction
1065	Model Hyper Parameters
807	Convert to numpy array
878	Create scores dataframe
1510	ROTATION PIXELS ONTO ORIGIN PIXELS
837	Set alpha a little
271	Partial imports
1511	Create the input layer and the augmentation layer
1050	check for encoding
1056	Split into train and validation sets
565	Predict the feature importance with the data
429	Convert year column to uint
1102	Preparing final file
1766	cross validation and metrics
1399	transform to series
1331	Loads pretrained weights , and downloads if loading for the first time
569	Hours of Order Day
653	Read the training data
1076	Average length of the comment
138	Compiling GPU and the previous compiler
182	Top level categories with highest price
1378	Get random labels
77	retrieve x , y , height and width
915	unique values in each column
1805	Lets look at the memory usage of each dataframe
1473	MAKE CUTMIX LABEL
260	Convert the string representation to int
44	Normalize colors based on the image
76	load and shuffle filenames
349	highlight the text
904	Clean up memory
1481	Histogram of continuous variables
12	This block is SPPED UP
1244	Load the data
291	predictions image by image
1440	Process Patient images
819	Save the reduction results
411	OneVsRestClassifier with a Logistic Regression model
1155	Create strategy from tpu
1690	check if all the columns are empty
895	Find the features with zero importance
1071	You can access to the actual face itself like this
1384	convert to RGB array
1745	Here is the unbiased version of the generator
1479	ROTATION PIXELS ONTO ORIGIN PIXELS
572	Number of orders by week
1648	So now we can validate on all data
803	Understanding Confidence by Target
1228	Load Train , Validation and Test data
1398	this is the start of data in the process
1346	show predictions for test
690	Iterate over data
194	display threshold image
1178	take a look of .dcm extension
1448	Scaling the dataset
1498	Decide the layers of the model
529	Run Grid Search
253	Create Validation Sets
737	Test key draw
527	Classification of Test
1226	Plotting some random images to check how cleaning works
1686	Get RGB values
472	Precision and Recall
1137	What about the months after normalization
727	Most frequent items in a category
863	Fitting and predicting the test set
968	Remove low features
1477	batch by batch
1245	Convert floats to integers
397	Random Forest Classifier
612	Playing some audio
1284	Get the data ready for modeling
1551	Average values for all months
1258	so we have to pad the images
537	Run Grid Search
82	Make the submission
1320	convert string columns to int
630	Numba model to be multiplied with I
806	Convert parameters into int
797	Convert to numpy array
1369	Draw the centroids of the districts
209	FIND ORIGIN PIXEL VALUES
810	save best score and standard deviation
258	Predict for testn
278	Word Cloud visualization
973	Get score on random set
48	Normalize colors based on the image
1659	for Neutral tweets keep things same
965	reset index and use floats as labels
922	Plot the normalized importance
442	Latitude and Longitude
818	Add the prediction values to the test dataframe
733	Function that detects and computes the image
538	Fit the best model
1673	Add box if opacity is present
781	change column names
787	Cumulative importance plot
955	EDA with dataframe mapping
683	Creating a DataFrame with the labels sorted in descending order
1759	Feature matrix and other configs
1518	Get the training dataset
566	Keras is only imported for splitting the data
1227	Read the data
735	Classify an image with different models
1136	Spliting in the test data
1092	Applying CRF seems to have smoothed their prediction
783	Import all that we need
554	Add RUC metric to monitor NN
934	Fitting and predicting the test set
927	the score , parameters and iteration
801	Get the predictions in the right format
516	add team conf
1375	Diff Differences
1530	Previous app data
617	Split the train data into two sets , train and test
774	that have not been eliminated yet
141	histogram of Fraud vs
265	Scaling the features
851	Calculate elapsed time
1373	We fit the model
1363	Some place I should change
849	Extract feature importances
1697	Helper function to make a list of images
458	Parameters for LGBM
173	What are the most unique calsses in the dataset
1607	one hot encode the categorical features
1037	Get the size of a dataframe
1764	Copy X features
462	Data processing , modelling
1225	Make a picture format from flat vector
938	Write column names
1557	Creation of the External Marker
592	Read the data
490	plot distribution of the validation features
533	Run Grid Search
1304	Delete to reduce memory usage
1064	Generate data for the BERT model
285	Just labels to identify theissue
842	Plot the correlation between variables
1310	Get feature importances
1416	Detect hardware , return appropriate distribution strategy
1264	Load best model and predict the test set
1091	Create submission dataframe
1806	Reading the Data
1109	Unique IDs from train and test
1387	transforms sample data
86	Raw data analysis
893	Get data ready for modeling
1722	The mean of the two is used as the final embedding matrix
855	separate train and validation sets
1778	Import Train and Test csv data
111	Merge the three dataframes above with the date information
961	We will be using the above created features
875	dict存储的参数转化
166	Analyze number of masks
323	calculate iou score for this competition
858	Show plot of actual validation and predicted
358	skin like mask
825	Set to instance variables to use this later
9	fill test weather data
1166	Parallelization of test data
986	Create the date columns
369	Create Validation Sets
1144	Growth Rate Percentage
695	remove layter activation layer and use losvasz loss
1669	gather input and output parts of the pattern
1130	Create dataloader
784	Now we evaluate the model
948	Boosting Type for Random Search
1141	Plot the dependence of the target
1588	If the score is correct but there is noise in the prediction
701	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1812	extract important features
1646	Retreive convolutional layer
551	Run Grid Search
130	Look at how data generator augment the data
1030	There might be a more efficient method to accomplish this
1167	Import the Libraries
917	Amount loaned relative to salary
1624	Table of Contents
172	Saving the model in JSON format
1489	Read candidates from a .gz file
257	Predict and Submit
1154	Checking for differences between datasets
1635	Splitting the data
802	Get the predictions in the right format
1794	Train the model
1062	Here are the main phases of the encoder
853	Fare Amount by Day of Week
1400	Transform series into binary
22	Impute any values will significantly affect the RMSE score for test set
717	Random Forest Regressor
1159	LIST DESTINATION PIXEL VALUES
1180	assign the confidence value to the confmtx
1636	Brand new features
1048	Credit card balance
1236	Draw the text boxes with the text size
100	code takesn from
1305	Submission with best score
39	Get the next batch
1531	Previous applications categorical features
1266	Getting the Predictions
945	iteration score 两列
923	Cumulative importance plot
871	Run the objective
58	Creating a DataBunch
557	Number of times the user interacted
1152	create validation set
185	Categories of items with a price of
1172	Read in the image
1512	size and spacing
1616	Computes gradient of the Lov chain w.r.t sorted errors
1069	Write the prediction to file for submission
521	Create the model winners dataframe
541	Run Grid Search
1802	Correlations between images
597	Brief check on the data
1295	Prepare the data
1754	create installment Relationship
1283	Get the data ready for modeling
342	Save the predictions for submission
932	Evaluate the model
1276	get the inputs and targets of the task
517	Get the seeds as integers
847	For example lets fit model on full data
1526	Default empty prediction
1544	Get the data frame
1206	Define the model
1218	We can get stats on the group bys
433	Toxic Comment data set
1207	LOAD DATASET FROM DISK
440	Top most commmon Paths
70	add trailing channel dimension
1666	StackNetClassifier with GPU
1611	Specify which attributes will be used as targets
1164	DISPLAY VALIDATION IMAGES
560	Plot Gain importances
1767	Tokenize the sentences
808	Split up with indices
199	inpaint with original image and threshold image
1000	Custom Feature Data
1203	List of Fake Images
465	get bayesian train and validation index
1768	shuffling the data
706	MODEL AND PREDICT WITH QDA
284	Just labels to identify theissue
153	Ratio of Download by Click
1741	HANDLE MISSING VALUES
1442	prepare submission data
1286	Brightness Manipulation with imgaug
909	standardize the data type
631	calculate I and D
115	Defining the parameter grid for the model
682	Creating a DataFrame with the labels sorted in descending order
502	Rescaling the Image Most image preprocessing functions want the image as grayscale
804	Get the subsample parameters
1762	missing data
1224	select proper model parameters
491	Pearson Correlation heatmap
1209	Save model and weights
1143	Dividing the data by date
741	Scatter plot of single line
1724	text version for squash
831	replace NaNs in train and test data with
835	new observations after NORMALIZE
1006	Remove low features from training and testing features
1529	Read candidates with real multiple processes
72	define iou or jaccard loss function
559	Create a dictionary with the values in order
957	Relationship with the previous cash and credit
1456	Number of Rooms and Price
1583	coluns with new features
1497	Here we are getting the pretrained model
295	Binary Target Variable
1382	Stemming and Lemmatization
329	read in the images
576	target and model
958	We can see there are some weird things in dataset
658	Performing different models with different random transforms
92	Save everything for you to use
170	Second component of main path
401	obtain one batch of training images
1165	Ready to start training
148	Clicks by IP
1096	generate predictions on the test data
500	Separate the zone and subject id into a df
1267	Here we take the test data and generate sequences from the test data
201	Function to render neato images
19	Imputations and Data Transformation
1696	Here I write a helper function to evaluate the images
1185	update user samples
1397	Exploring the variable
1500	Print the directory name
1730	Add leak to test
1589	to truncate it
736	Remove zero features
1371	Train the model
1426	Find max length of a positive and negative numbers
898	Get the features with zero importance
793	Random Forest Classifier
1645	Converting the correlation matrix into a numpy array
1612	Split the train dataset into development and valid based on time
534	Fit the best model
811	Create a file and open a connection
417	Preview of Data
673	add order by country
1804	Plotting ROC Curve
52	Normalize colors based on the image
276	sort the validation data
1706	choose a candidate to score from
869	Create a file and open a connection
724	Treating values with Simple Imputer
1204	Create fake folder
1522	FIND ORIGIN PIXEL VALUES
235	Clean Id columns and keep ForecastId as index
1049	one hot encoding
850	Get the data type
1459	checking missing data
924	Draw the threshold lines
996	seed features and their distributions
1605	I think the way we perform split is important
1053	get score on best score
704	MODEL AND PREDICT WITH QDA
619	Area of Bounding Boxes
1584	Checking only one value columns
1652	Load the data
26	visualization of Target values
504	A couple of more interesting features
132	Create Testing Generator
684	Creating a DataFrame for the count of unicode characters
1818	CHECK FOR EACH CATEGORY VARIABLE
1782	Calling our overwritten Count vectorizer
1222	define the parameters for the grid
316	Linear SVR model
1653	Exploratory Data Analysis
1674	Add boxes with random color if present
640	MosT common positive words
1131	PyTorch dataset loader
956	EDA and Feature Engineering
207	LIST DESTINATION PIXEL VALUES
1169	Print dimensions of dataframe
47	Join the month with the date
1547	Get the data frame
1093	salt parameters are from the above mentioned tutorial
249	SVR model on train and test
1474	Cutmix on batches
935	Sort by score
765	Visualize the markers
292	Load and predict
498	read in header and get dimensions
730	extract time features
543	Classification of Test
593	factorize categorical features
34	Loading Train and Test Data
1359	Fast data loading
255	Fitting our model
628	Groping by day
722	Sort ordinal feature values
1251	The function below get the data from the category column
497	creatingDF for binary target
577	Get a sample
1698	test if the program has any empty mask
691	Computes gradient of the Lov chain w.r.t sorted errors
1112	extract different column types
147	IP of the traffic source
648	Load and Preprocessing Steps
394	Decision Tree Classifier
767	drop high correlation columns
7	declare target , categorical and numeric columns
1290	Squeeze and Excitation block
1523	oversampled training dataset
564	cast item descriptions to int
314	Getting the probabilities for test data
1695	Plot the tasks
1604	Nulls in Train and Test Data
1205	Load and Preparation
796	parameter value is copied from
930	Create random results
275	Dropout RandomForest
364	Scaling the data
942	Sort by score
574	bedrooms with respect to bedrooms
928	Get a random sample
121	Checking the current coverage
1496	Some MODELS
81	First fully connected layer
427	first column only
1232	Load Train , Validation and Test data
1409	Set up some basic model specs
1317	TPU and GPU configs
1219	Function to calculate the title mode
1101	Training the model
1122	define lightgbm params
660	Computes and stores the average and current value
821	Non Limitable Classifier
1578	replace NaNs with
1079	vocaublary , add its feature vector
461	Training the random search
713	Predictions for the test set
407	Run the map
305	Load the model and check the performance
581	test if the number of steps is too small
175	Load the image data
523	Remove Confidence Stadium
1723	missing entries in the embedding are set using np.random.normal
952	Get main data
755	Preprocess the data for this fold
1672	Initialize patient entry into parsed
1776	For later use
1777	plot with correlations
1026	Train the model with early stopping
124	pct change of group by columns
907	Get the size of the data
1585	fill all na as
629	Groping by day
279	What is inertia
512	Transform the season data into a pandas DataFrame
144	get the data fields ready for stacking
1110	Extract processed data and format them as DFs
1422	deep copy the sentences
1433	Plot the count of links
75	create train and validation generators
107	count number of stores and item ids
772	Lets plot the cyclic curve for the wavelet
1436	Number of Patients and Images in Training Images Folder
1208	define parameters for model training
1716	FUNCTIONS TAKEN FROM
1054	Clean up memory
360	Here is the sample submission
1462	Create dataset for training and Validation
1157	numpy and matplotlib defaults
38	observation data as pandas DataFrame
95	For getting the indices of the selected columns
1336	Skip connection and drop connect
1340	Remove signal to noise
145	Frame creation and gc.collect
109	Plot rolling statistics
1750	Creating an entity from dataframe
1084	Read the image from the source image
872	Create a file and open a connection
283	How many data are there per label
347	Import Libraries and Data
1705	Return the program that would give the best candidates
1629	Exploratory Data Analysis
203	For every slice we determine the largest solid structure
1156	watch out for overfitting
984	Days with Days
1598	Checking for Null values
1608	Parameters that we are going to tune
1792	The traffic month cross days
1271	check if all pairs are same or not
361	Image data processing
1389	Create Stratified validation split
1457	checking missing data
43	Add new features
1189	Start generate data sets
1501	Detect hardware , return appropriate distribution strategy
686	Use the dataframe to define data generator
1679	get some sessions information
328	Read the DICOM files
1597	checking missing data
269	Merge Dense Players
374	Avoid division by zero by setting zero values to tiny float
1347	iterate through all the columns of a dataframe and modify the data type
1677	draw patients in the correct format
1408	Predict and Submit
317	SGD model
1458	checking missing data
1675	draw patient image
911	Convert categorical data into integer values
87	fake data sampling
1480	batch grid mask
1499	Check if the latest checkpoint exists
1575	Reading the Data
139	from mmcv import mm
156	Frame creation and gc.collect
1201	Detect hardware , return appropriate distribution strategy
1335	Squeeze and Excitation
436	Preview of Train and Test Data
1013	Bureau memory leak
1634	so we can take a look at the differences
815	Training the model
135	Checking for Class Imbalance
969	Getting the train and test data
1241	Load an image
489	Function for group by
61	Get the sex of the patients
336	batch size for training and validation
1287	Function to display blurry samples
1592	some config values
692	Using previous model
1323	Parameters for an individual model block
1134	Stacking the validation masks
1139	Plot the dependence plot
1412	Class Imbalance
925	Fitting and predicting the baseline model
642	neutral word count in selected text
1333	Squeeze and Excitation layer , if desired
390	Set some parameters
508	Here is the custom function that checks and prints the threshold
1793	Plot the traffic months cross days
890	Correlation with kfold
590	Gaussian target noise
123	Process text for RNNs
1561	Macro columns by name
416	Read the dataset
1034	Unique values of each column
1330	Encodes a list of BlockArgs to a list of strings
446	Encoding the Regions
1704	Delete best candidate
169	Fully Connected Layer
1138	SHAP interaction values
219	Import Libraries and Data
899	one hot encoding
854	Investigation of Fare Amount vs
481	Tokenization of text data
420	Distribution of meter type
1485	Use the TFRecords generated by the generator
196	Show original image and grayscale
906	Cumulative variance explained with PCA
1748	Creating an entity from dataframe
165	Here is the unclustered image
183	Brand name and number of item
1175	Add the cylindrical actor to the display
596	bird is a type of regularisation
1688	Check if all the colors are the same
667	Plot the distribution of the highest and the lowest asset
1158	size and spacing
370	Gradient Boosting
868	Sample out data
1031	Suppress warnings due to deprecation of methods used
644	Perfect submission and target vector
1610	Log transformation of target variable
1246	Training History Show
1090	Encodes predicted probabilities with the best threshold
1570	Convert DICOM images to PNG via openCV
1193	Go to actual revenues
768	Check if walls have been properly loaded
539	Classification of Test
1487	Check if the latest checkpoint exists
1322	Read the input and target data
1515	Preparing the data
989	due dates , both the same day counts
467	Precision and Recall
1303	Delete to reduce memory usage
116	Defining the parameter grid for the model
104	Compile and fit model
913	Joining parent variables with the categorical features
1718	Tokenize the sentences
712	ADD PSEUDO LABELED DATA
887	Print the original features and balance features
302	Read in image and resize it
69	add trailing channel dimension
705	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
351	Random Forest Feature Selection
1580	colunms new features
1548	Merge Weather Data
1797	Plot rolling statistics
1503	of the image to the TPU
511	Teams with holes
766	Create a legend and annotate it
233	Create date columns
1197	Prediction of Revenue
661	get different test sets and process each
1060	split dataset into train and validation datasets
146	What are the actual values
28	MODEL WITH SUPPORT VECTOR
960	The default feature names are
770	Bonus Variable
494	Distribution of the new features
1015	Import the datasets
1269	If the inx pairs are in the same color
110	Plotting how sales varies per year
1803	Correlation between images
186	Free shipping fee paid by seller
309	Create an embedding matrix of words in the data
1307	Calculates the vector of the maximum quantized value
976	Setting up the hyperparameters
1300	It is needed for the tokenizer to work correctly
1094	Generate the dimensions of the image
448	Updated Train and Test Data for Modelling
1358	iterate through all the columns of a dataframe and modify the data type
41	Overview of Missing Values
1068	Print CV scores , as well as score on the test data
453	Draw the line on the image
118	Sex Condition Progression by Sex
40	load the data
1577	Building the continuous features list
1321	TPU and GPU configs
1527	Computes official answer key from raw logits
282	Data Prepparation
744	Convert image id to filepath
430	Encode Categorical Data
1001	Return the most recent value for a given column
267	Some Player College Names have missing values
264	Prepare Training Data
1685	Load the data
346	Fully connected generator
363	Getting the probabilities for test data
496	get the data
484	start to building a CNN using Keras
1210	so we have pca with MLP as well as with CNN
1689	Sort by weight
999	Returns the longest possible element
962	create feature matrix and summary
273	get lead and lags features
1579	colunms new features
195	inpaint with original image and threshold image
1257	Build New Data
177	Most common level
1046	Group by loan
1460	checking missing data
236	Filter Spain , run the Linear Regression workflow
699	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
16	To plot pretty figures
1089	Resize test predictions
1162	Order does not matter since we will be shuffling the data anyway
1376	Deep Learning Libraries
83	Read the Text Data
1516	Detect hardware , return appropriate distribution strategy
1405	rolling mean for each store
546	Create the Pipelinees
840	Manhattan Distance by Fare Amount
435	Lets display some nice things about our data
128	Prepare Traning Data
164	Reading the image
1314	Checking the distribution of the variable
1535	Loading the data
1711	Read data from the CSV file
926	Set up the objective function
237	Filter Spain , run the Linear Regression workflow
1755	Relationship between bureau and rureau
693	Using previous model
1573	Prepare the data
1100	Get the best fold AUC
192	Show original image and grayscale
1437	Number of Patients and Images in Training Images Folder
1292	Instantiating the model
1562	Get predicted probabilities for each feature
790	run model on full training set and predict the results
1553	Average day of the week
627	Groping the confirmed cases by day
230	Implementing the SIR model
1807	Here are the members and transactions
1763	Copy X features
697	Precision helper function
388	Helper function for histogram processing
4	Remove Unused Columns
1726	for numerical regressors
1691	lifting function for unlifted parameters
1115	Check if columns between the two DFs are the same
425	One Time Series
1379	If the object is ok to warp
970	align the data
452	import required libraries
552	Fit the best model
404	create some empty vectors
160	Look at the cells
1467	realign the column to be
1493	if validation mode
1385	suppose all instances are not crowd
362	Get image id and type
371	Fitting our model
1421	Converting the comments into lower case
142	get the data fields ready for stacking
1430	make test features
85	Fake data preparation
1775	This enables operations which are only applied during training like dropout
391	convert to float
49	Plot the distribuition of the dataset
419	Examine the shape of our data
944	loop over all hyperparameters
188	Generating the wordcloud with the dataset
1021	First Order the columns by name
466	Plot ROC Curve
1631	Load full table with all data
575	Correlation of bedrooms and bathrooms
1270	Remove duplicate images
822	Read the image on how to visualize it
595	Create the model and train
59	Unfreezing the model and checking the best lr for another cycle
1325	Calculate and round number of filters based on depth multiplier
1521	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
482	Define the model
1471	Order does not matter since we will be shuffling the data anyway
181	Prices of the first level categories
112	Merge the data with the categorical variables
1002	Custom Feature Pipeline
652	matplotlib and seaborn for plotting
1151	Load train set
168	initialize exp avg sqm
485	Now through the second convolutional layer
997	seed features and their distributions
1280	Run the arc solver
562	Parameters from Tilii kernel
1715	Ensure determinism in the results
1781	Calling our overwritten Count vectorizer
1123	Create the LightGBM datasets
1494	Prediction on test data
224	get the best score
1377	Function to plot different images with different color distributions
105	load the data
670	Transposing the lat and long columns
1432	Exploratory Data Analysis
1627	Plot country predictions
738	Test key draw
1419	Import libraries and data
1043	Print some summary information
1729	Add train leak
1746	Replace some missing values
900	check for encoding
824	Applies the cutout augmentation on the given image
734	Classify image and return top matches
1312	reduce validation set
1747	Amount loaned relative to salary
1124	save oof val
84	Class Distribution Over Entries
1790	import the libraries we gon na need
20	Imputations and Data Transformation
1282	get train and test data
1022	Remove all columns except for the identified ones
1104	Training the model
2	Add new Features
605	Pad the audio data
688	draw box over image
884	Loading the data
1374	Drop target , fill in missing values
614	Weight of the class
1243	to truncate it
1638	Check the distribution of min
910	Unique values of each column
1603	Checking for missing values
544	Length of Tourneys
216	MinMax scale all variables
1348	Fast data loading
1424	function for cleaning lower case words
66	load and shuffle filenames
522	Add the model losers to the dataframe
171	Now through the second convolutional layer
456	Load libraries and data
114	Merge the data with the previous state
149	show some images
844	Train model on all data
1254	Run predictions on test set
1263	Using original generator
42	Lets look at the month and year from the official dataset
750	Combinations of TTA
443	Latitude and Longitude
745	build a dict to convert surface names into numbers
1386	convert to tensor
505	Joint plot for the revenue
244	Filter Andorra , run the Linear Regression workflow
1811	Plot the actual values vs predictions
74	cosine learning rate annealing
908	Remove other variables from parent variable
155	Plot the download rate over the day
23	Detect and Correct Outliers
1630	Looking at the best choices for each Province
553	Classification of Test
1632	Seting the df according to the province
1813	summarize history for loss
1796	times series means
158	The function to get the image shape
1670	Setting the Random Seeds
254	Gradient Boosting
108	Sales volume per state group
933	Sort by score
469	Data processing , metrics and modeling
635	Determine the model
1660	Import and convert to integers
972	random search and bayesian optimization
1590	Load the data
1469	splitted features into subfeatures
1402	Extract data from first n samples
645	try random samples
602	Load the data
929	find parameters between 0.005 and 0.0
133	Spliting the data into training and evaluation
1576	checking missing data
778	drop high correlation columns
524	Create the Pipelinees
437	Preview of Train and Test Data
1337	Update block input and output filters based on depth multiplier
798	Split up with indices
1619	Checking for Null values
1683	contrast , etc
251	SGD model
626	Sort by day
1441	Process Patient images
540	Create the Pipelinees
1550	Calculate the average week of each year
780	Range of values in a Series
157	Print final result
1572	How many data are there per diagnosis
1351	Fast data loading
791	Train the model on the test data
1085	If the original image is different from the mask
763	current y value
1692	lifting function to combine multiple results into one list
1174	Visualize the bkg colors
1099	predict on validation set
393	What are the most frequent hot clusters in the test set
716	We now have the data ready for the model
1758	For pos balance we will create a Relationship object
1196	Get fold results
1542	Fix the spurious classes
242	Filter Albania , run the Linear Regression workflow
62	Exploring the categorical variables
167	Only the classes that are true for each sample will be filled in
1097	Check if train and test indices overlap
1249	parse trials and create submission file
583	Sum the probabilities of each model in batch
535	Classification of Test
591	Combining all the augmentations together
3	Reset Index for Fast Update
1365	Download the Kaggle Data
117	Preparing the submission data
1273	Count the number of objects in the object
1582	coluns with new features
1664	import stacknet
5	Encode Categorical Data
696	Exclude background from the analysis
1217	Group by game time
1504	LIST DESTINATION PIXEL VALUES
1177	plot the colors of the object
259	Convert the string representation to int
723	Sort ordinal data by ordinal
1617	remove layter activation layer and use losvasz loss
53	Plot the triplets
303	Preparing the data
1727	Shuffling happens when splitting for kfolds
834	Elastic Scaling is done to enhance accuracy
1153	Let us do the same analysis for the validation set
740	add white noise
1148	some tests for our dataset
319	Create Validation Sets
1508	remove images and labes during training
1682	An optimizer for rounding thresholds
599	Read the image and convert to a grayscale
1694	To plot the images
953	import matplotlib as plt
1564	Now we can label encode the categorical variables
762	Plot the counts
297	Split training and validation sets
1268	Linear Weighted Kappa
301	move image to sub folder
995	What are the most common client types
280	use k means to cluster the labels
252	Decision Tree Regression
306	Combining all the data into one DataFrame
400	convert to image
325	Save the data
1088	Remove padding from images
1135	Load the timestamps
654	function to read the test data
946	altair is a very nice plotting library by the way
1150	Image augmentation and validation augmentation
136	Save the model
889	align the rows of train and test data
1260	Combine the filename column with the variable column
1801	k is camera instrinsic matrix
832	Observatory Survey
1118	Manually adjusted coefficients
1615	show mask class example
1179	The class probability predictions need to be converted to a matrix representation
761	There is missing data in test and train data
883	Fit the model
1240	Form the submission file
856	Get the list of features
675	Run a grid search
231	Merge train and test , exclude overlap
1816	Loading the data
886	Getting data from leak
1004	dfs to get a matrix of features
1113	Subset text features
73	create network and compiler
1451	inverse transform the class values
1095	Predict on validation set
1160	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
558	If you want to know the order of the variables
874	ROC AUC on test data
32	Identity Hate Feature
470	Merge datasets
1223	Predict Test Set
860	We will now train the model
1720	SAVE DATASET TO DISK
1247	to truncate it
222	Linear SVR model
405	calculate the log loss
1195	Make a prediction
1814	Copy predictions to submission file
1800	plot baseline and predictions
649	Create the model
1080	Divide the result by the number of words to get the average
715	Remove the Outliers
1173	convert to HU
718	Preprocess classifiers
1298	Make labels binary
1220	Drop nuisance columns and fill in missing values
1011	Function for creating categorical variable
732	Function that detects and computes the image
1490	Read candidates with real multiple processes
1810	from sklearn.metrics import metrics
1650	Final results
689	functions to show an image
1633	Age distribution with seaborn
1678	convert text into datetime
94	Create a subset of values that are valid
408	tag to count map
1753	Relationship between applications and bureau
239	Filter Italy , run the Linear Regression workflow
1319	Read the target and input data
313	so we label encode categoricals
598	Some of the images without any ship
568	Loading the Data
1042	Sort the table by percentage of missing descending
1107	Load sentiment file
1214	Order does not matter since we will be shuffling the data anyway
27	histogram of the data
805	Add subsample parameters
1350	iterate through all the columns of a dataframe and modify the data type
228	Ensemble final scores
379	RMSE for this competition
479	From Strings to Vectors
406	Exploring the data
1051	Hyperparameters search for LGBM
1444	Square for Train and Test data
210	Strip the integer columns and the categorical columns
29	Loading train and test data
355	Target values that differ a lot
331	Read the DICOM files
1342	Calculates ratios of each message
1712	Since the labels are textual , so we encode them categorically
916	Merge main data with bureau data
1817	Convert dataset to Lidar data format
308	Padding thedocs
1057	Transform image and mask to float
1752	Creating an entity from dataframe
1693	lifting function name
399	Print the confusion matrix
622	Examples for usage and understanding
93	Set the before and after indices to the values
1558	Creation of the Watershed Marker
1785	Avoid division by zero by setting zero values to tiny float
1415	Number of labels for each instance
709	create stratified validation split
1114	Remove missing target column from test
662	date aggregations
1019	Merge with bureau data
1740	Distribution of DBNOs
1658	Tokenizing the selected text
586	bedroom Count Vs Log Error
1188	Calculate the average accuracy of each assessment
1327	Convolutions like TensorFlow , for a fixed image size
1120	function to rename the columns
1341	measured vs unmeasured plots
625	Sort by day
708	ADD PSEUDO LABELED DATA
1117	Compute QWK based on OOF train predictions
1353	iterate through all the columns of a dataframe and modify the data type
1170	Split the data back into its training and testing data
1439	Reading in the Data
992	Relationship with the previous cash and credit
1362	Converting to Total Days , Weekdays and Hours
809	Train the model with early stopping
993	get interesting features
843	separate train and validation sets
37	Lets take the natural log on the column names
1534	This code is copied from here
1289	Get the input shape
1316	convert string columns to int
15	Common data processors
1361	import modules and define models
991	EDA and Feature Engineering
1420	from googletrans import Translator
827	Read the image on how to visualize it
1820	Expand if missing
1506	FIND ORIGIN PIXEL VALUES
873	Write column names
936	Fitting and predicting the test set
343	create my generator
707	QDA with CV
1618	average the predictions from different folds
1546	Get date columns information
1302	Build model into the TFA
215	converting the data into xgboost format
478	Vectorizing the data
1329	Encodes a block to a string
493	Applicatoin train data
618	Area of Bounding Boxes
1643	How many clicks and downloads by device
293	Extract the ID from file names
101	load the image file using cv
1714	cross validation and metrics
1699	convert the sample input and output to array
912	Here we are merging the parent variables and the child variables
1239	Run the model
310	First we split the data into three datasets
352	load the data
1128	Compute salt mask coverage
1176	Save images to a GIF file
1098	Create Validation Sets
1367	Plot the districts of the day
1032	Remove other variables from parent variable
63	Exploring continuous variables
1772	always call this before training for deterministic results
655	There are no missing values in the dataframe
1086	Check that the training set is ready
287	Load the model and check the loss
1450	create the look back datasets
1200	Show RMSE scores
1668	Let us now look at the sales
1070	Convert to RGB
1023	one hot encoding
1033	standardize the data type
624	Groping the Ialy dataset
967	There might be a more efficient method to accomplish this
340	Create predictions dataframe
509	Here is the custom function that checks and prints the threshold
758	Which are not equal in terms of households
1710	Keras Libraries for Neural Networks
1146	load mapping dictionaries
300	Make a folder with all the files
1701	The candidates for each node are inserted into the candidates list
129	See sample image
1514	size and spacing
1145	Curve Fit
1077	Function to remove stopwords from text
1103	Import the datasets
951	Reading the Data
341	change column names
1552	Calculate the average day of the year
764	msizes of sqm values
711	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1198	Create submission file
549	Classification of Test
638	Function to generate a word cloud image
211	label encoding the categorical variables
281	File sizes and specifications
694	remove activation layer and use losvasz loss
921	Sum up the importance
561	There are some weird spikes ..
1549	Merge Weather Data
426	Distribution after log transformation
385	check test files
238	Filter Italy , run the Linear Regression workflow
1059	Visualize the validation images
270	LightGBM with a low learning rate
444	Latitude and Longitude
459	Training the random search
1517	Get the training dataset
1425	total tokens and unique tokens
103	Deleting unnecessary columns
1355	iterate through all the columns of a dataframe and modify the data type
154	Check for missing values
1213	Create strategy from tpu
1372	Creating the submission file
1662	Display some images
214	make sure to convert the training set to lgbm datasets
1125	Apply the final layer
1297	Predict on Test Set
833	Plot distribution of fare
608	Return the absolute path of a file
1537	Custom function for breakdown topics
501	show the graphs
409	Features from TfidfVectorizer
571	Hour of Reorder Count
113	Joining the month with the actual values
126	Prepare continuous features
1637	do cumulative count
277	reorder the input data
1613	Split the train dataset into development and valid based on time
800	Light GBM Results
1532	Create a model
1819	Join market and news
1275	Remove other objects with different color values
880	Reading the Data
963	create feature matrix and summary
1186	count the number of actions per session type
519	Preparing the training data
859	run randomized search
414	Check for Class Imbalance
813	Which we can plot up ..
296	Sample the data
1253	Train the model with a checkpoint
1626	Optimize COVID-19 predictions
525	Run Grid Search
45	quad plot of the dataset
1770	missing entries in the embedding are set using np.random.normal
1731	Function to create video file
1737	The competition metric relies only on the order of recods ignoring IDs
1126	Load the mask from file
974	loop over all hyperparameters
240	Filter Germany , run the Linear Regression workflow
304	Set class weights
703	STRATIFIED K FOLD
841	uclidean Distance by Fare Amount
646	For each type get the mean value
1229	Build datasets objects
1067	split training and validation data
1524	Eval data available for a single example
1020	Lists to keep track of columns to remove
441	Latitude and Longitude
1703	Find the best candidates
1344	Show some examples
1328	Gets a block through a string notation of arguments
788	Draw the threshold lines
664	Date Aggregate for Category
601	And there you have it
106	load the data
163	RLE encoding for the mask
1719	shuffling the data
263	We will use LightGBM for our model
1047	Lets read in the cash data
905	returns the dataframe describing our scores
1587	Create categorical and object features
51	Add columns for each row
1234	Model initialization and fitting on train and valid sets
151	Plot the distribution of users downloads the app
1461	Make a Baseline model
1380	Creating the xy values
1555	Convert training set to lightgbm dataset format
262	Shows parameters and LB score visualization
506	Function to plot boxplot between columns and revenue
681	Creating a DataFrame for the count of unicode characters
1345	The last prediction is the last one
1039	Previous counts categorical
987	Previous Loan Amount
1647	Create the tqdm notebook
384	to reduce memory usage , dtype is specified
1163	Order does not matter since we will be shuffling the data anyway
1357	Find Best Weight
1296	The params I use can be tweaked to your desire
1495	set the necessary directories
1133	Stacking the val predictions and masks
1349	Leak Data loading and concat
246	Set the dataframe where we will update the predictions
920	Credit card balance
1639	Clicks with several clicks
432	Apply each model on test set and output predictions
243	Filter Albania , run the Linear Regression workflow
1488	Eval data available for a single example
205	Importing relevant Libraries
1684	Load the data
463	Data processing , metrics and modeling
679	Split the labels into a few parts
903	get score on best score
1326	Round number of filters based on depth multiplier
700	MODEL AND PREDICT WITH QDA
1132	make a prediction
357	get different image data types
1760	I define a function to preprocess the data
17	Now extract the data from the new transactions
226	Scatter plot of CNN
450	Initial Data Prepparation
223	get the best score
829	Read the image on how to visualize it
570	Days of the Week
1525	Span logits minus the cls logits seems to be better
621	Area of Bounding Boxes
754	Visualizing distribution of features by level
1606	I think the way we perform split is important
752	so we can see the columns
120	Function for calculating word count
1293	Load dataset info
229	Ensemble final scores
1742	SCALE target variable
1651	Final results
372	Voting Regression
35	create an array of embeddings from train text
1285	member of the model
0	DICOM Image Metadata
1484	Create examples from the training dataset
137	clear output after each batch
666	Here is the distribution of the products
914	Joining the aggregated data with the original data
753	Import Train and Test dataset
261	Ploting parameters and score visualization of OSIC PFP solution
1356	meter split based
24	Detect and Correct Outliers
1040	Merge with previous counts
455	Draw bounding boxes on the image
483	Define the model
413	Load Libraries and Data
1315	ATOMIC Numbering
777	Plot the feature plots
1466	Run the model on the test set
55	find number of clusters
657	get the columns after preprocessing
67	split into train and validation filenames
518	Make a new DF with just the wins and losses
977	Converting data into numeric type
1702	Evaluate the candidates
389	Check for empty images
1728	This enables operations which are only applied during training like dropout
982	Conversion of Days for Bureau
97	Befine the before values
1657	Sentiments with POS Tagging
846	baseline train and validation
643	Can I get your attention
1536	Check for Null values
1063	We will use the most basic of all of them
947	random hyp is from grid
1463	CNN Model for multiclass classification
1787	index to vectorize the error
585	Building the year column
1261	Create test generator
1237	Decoding the string and converting to float
1594	Tokenize the sentences
1272	Check for the current object pairs
1184	Assign all variables to their default values
1687	Function that checks if all the masks are the same
119	Pulmonary Condition Progression by Sex
1005	Make predictions on the test data
580	Logmel feature extractor
475	vectorize the text
680	Length of labels
1783	Generating the wordcloud with the values under the category dataframe
1230	Load model into the TFA
338	plot and visualise the losses
46	What are the most frequent variables in the dataset
731	Sample out hits
918	Some new features from the train dataset
857	Predict on validation set
579	Calculate logmel spectrogram using pytorch
471	Plot ROC Curve
1528	Join examples with features and raw results
1788	Peak frequency plot
218	MinMax scale all floats
636	Deterministic model
1559	List of features with the specified methods
983	Adding new features from bureau
1596	Train data set
315	SVR model on train and test
65	save pneumonia location in dictionary
823	Custom Cutout augmentation with handling of bounding boxes
1381	split the dataset in train and test set
1476	MAKE CUTMIX LABEL
1417	Build the model
234	Filter selected features
613	Any results you write to the current directory
179	now we can get the mean price by category
1231	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
1018	Print some summary information
1751	Creating an entity from dataframe
650	These parameters are copied from this colab notebook
865	Create hyperparameters
1028	Clean up memory
1468	Max and min of a feature
814	Train and Validation
1061	Create a list of DownConvolutions
162	Get the indices of the two cell masks
550	Create the Pipelinees
1815	Create data object
127	Get the categorical variables
1025	Hyperparameters search for LGBM
771	calculate the households for each customer
1111	Extract processed data and format them as DFs
421	Plot the meter reading
877	Evaluate Bayesian Results
71	create numpy batch
702	ADD PSEUDO LABELED DATA
785	Sum up the importance
542	Fit the best model
1640	Load the Data
1541	checking the score on the train data
1609	Preparation for XGBoost
971	Get score on random search
1443	Square for Train and Test data
1250	save the best model
616	Change the Standard Deviation
143	Compile and fit model
1035	Convert categorical data into integer values
1681	the accurace is the all time wins divided by the all time attempts
587	There is too many bathrooms , so we will subset the first
563	Descriptive of Items
1256	create train , test and test folders
685	Creating a DataFrame with the labels sorted in descending order
1216	creating a function that aggregates game time stats by installation id
176	Visualize the images
457	Libraries to import
402	Custom data types
1401	Explore the series
327	Add box if opacity is present
1708	Importing all libraries
368	Decision Tree Regression
1066	First dense layer
1334	Expansion and Depthwise Convolution
33	prophet expects the folllwing label names
589	Number of Stores Log error
1036	Average of all Repaid targets
839	Set alpha a little
1	resizing to desired size
1431	Save the data
286	Preparing the data
776	PairGrid between Target and Plot Data
1055	returns the dataframe describing our scores
488	function to get the categorical and numerical features
227	Scatter plot of LB score visualization
161	make a mask from cell
588	Room Count Vs Log Error
431	Extract target variable
1447	Create submission file
98	Function for comparing sets
1560	Correlation between macro features
88	Create and plot the models
514	Summary of NCAATIES
1383	split x , y , and validate
476	the times in the text
828	Read the image on how to visualize it
122	Function for cleaning special chars
678	using outliers column as labels instead of target column
1661	Read in the OOF files
415	Lets display some nice things about our data
208	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
507	split into two sets , train and test based on dates
1418	Import the Libraries
1052	Train the model with early stopping
1016	Bureau balance by loanedness
1571	load best model
1599	Checking for Null values
134	Initializing a CatBoostClassifier
56	Modeling with Fastai Library
474	Save submissions to file
1074	Distribution of initial values in each application
150	Lets see if the categorical variables are present or not
18	impute missing values
378	Mean absolute error
941	Evaluate the objective function
1279	Identify by both
31	Vectorize the data
487	Exploration Road Map
725	Train the model and predict the test
241	Filter Germany , run the Linear Regression workflow
607	Return a normalized weight vector for the contributions of each class
193	Here is the blackhat
1513	Order does not matter since we will be shuffling the data anyway
383	Read the data
495	Read the data
422	Distribution of meter reading over the night
377	RMSE for each fold
1291	Squeeze and Excitation
1628	Some place I should know ..
881	Get main data
728	Read in the labels
197	Here is the blackhat
180	zoom to the second level categories
751	Settings for pretty nice plots
486	LSTM for Time Series Forecasting
964	Features with missing values
25	Loading train and test data
786	Plot the normalized importance
1809	Columns to be consolidated
578	Calculate spectrogram using pytorch
676	Store the data for our model
1199	plot validation loss vs boosting iterations
866	choice of boosting type
633	Running the model
1083	Load the data
794	Create a dataframe with the selected features
669	Load all the data
1454	inverse transform yhat
1161	FIND ORIGIN PIXEL VALUES
1567	Pinball loss for multiple quantiles
867	First we extract the type of prediction
1194	Predict on train data
582	Get the batch probabilities
102	grid mask augmentation
1464	Load test dataset
1119	Distribution inspection of original target and predicted train and test
1483	Some MODELS
959	Aggregated Feature Data Set
817	Applying method on train and test
1556	some config values
720	Toxic Comment data set
1428	Add PAD to each sequence
398	Print the confusion matrix
221	highlight the text
1407	Train the model
1394	With the Masks
885	import matplotlib as plt
1621	What about the general variables
1533	Mean ROC curve
799	Train the model with early stopping
950	Iterate over random hyperparameters
289	Ekush Classification Report
954	Set the target variable
861	Split into training and testing data
1181	conf_mtx and conf
1413	Most common labels
1478	LIST DESTINATION PIXEL VALUES
674	sets the training and validation sets
80	Passes through the convolutional filter set
1736	Visualizing the Target Feature
