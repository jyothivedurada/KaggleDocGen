756	Deal with Poverty Levels
924	Plot the cumulative importance
1200	plot of RMSE score
1616	Computes gradient of the Laplace w.r.t sorted errors
774	spearmanr on feature importance
1277	Find the objects in the image
1454	inverse transform for test data
402	Defining the data types
1064	Generate data for the BERT model
907	Determine the size of the data
1339	Final linear layer
1407	Train the model with all the parameters
521	Merging the model winners
1695	Exploratory data analysis
1335	Squeeze and Excitation
203	For every slice we determine the largest solid structure
1325	Calculate and round number of filters based on depth multiplier
37	histogram of log value for each column
329	load the images
148	Number of clicks by IP
502	Scaling with skimage
273	get lead and lags features
573	Bathrooms per Interest Level
1003	Evaluating Feature Matrix
131	Prepare Testing Data
349	highlight based on threshold
518	Preparing dataframe for modeling
1093	Initialize salt parser
819	Add the reduction to the dataframe
1068	Print CV scores and score
853	Fare amount by Day of the week
589	No of Storeys Vs Log Error
19	Imputations and Data Transformation
1376	Importing Libraries and Loading Dataset
847	Training and Evaliction
149	Print some stats
1711	Load the data
460	Train random search
1061	Depths Depthwise Convolutions
1081	Train the model
1112	extract different column types
500	Separate the zone and subject id into a df
566	Keras Logistic Regression
1000	Feature engineering for custom features
1294	warm up model
1536	Check number of null values and sample
498	read in header and get dimensions
763	set color of bars based on percent
21	Imputations and Data Transformation
524	Standard Deviation
1605	Defining Training and Testing Sets
241	Filter Germany , run the Linear Regression workflow
1693	Unapply the lift function
187	Check for No Descriptions
1815	LightGBM data loading
1613	Split the dataset back into development and test sets
546	Standard Deviation
1446	Order does not matter since we will be shuffling the data anyway
1715	Ensure determinism in the results
50	Distribution of the number of hours during the night
1351	Fast data loading
959	Aggregations for Feature Engineering
134	Initialize CatBoostoost model
248	Extract target from training data
1784	Compute the STA and the LTA
1062	Apply before pooling on the input image
1406	Convert seed to int
165	Otsu segmentation model
360	visualize the image
1461	Make a Baseline model
41	checking missing values
511	WINS , Teams , Teams , etc
216	MinMax scale all numerical features
1543	MODEL AND ARMALES
929	check parameter grid for learning rate
258	Fitting and predicting
559	Create a copy of the dictionary without changing order
750	Combinations of TTA
1702	Evaluate all the candidates
1778	Importing dataset and basic visualization
856	Remove Unhelpful Features
1330	Encodes a list of BlockArgs to a list of strings
1477	Iterate through batches
1411	Create the Layout
112	Merge the output dataframes with the input df
688	draw bounding box
543	Classification and Prediction
1503	flip image to original image
1202	Load model into the TPU
51	Add columns for each image
1051	LightGBM Classifier
532	Standard Deviation
1658	Tokenize the text
1191	argmax for test set
510	process remaining batch
539	Classification and Prediction
1044	Remove missing values from train and test set
1320	convert column names to int
575	Correlation between bedrooms and bathrooms
1192	fill the missing values with zero
610	Adds absolute path to filenames
1488	Eval data available for a single example
429	Converting year column as np.array
517	convert seed column as int
637	Importing Libraries and Loading Dataset
1690	sort the xs list descending on the number of zeros
906	Sum up the explained variance
1053	Score each validation and train scores
576	params we will use for training
1425	Total number of unique tokens
818	Add the reduced values to the dataframe
347	Import Libraries and Data Loading
1018	Print some summary information
996	Feature engineering for Late Payment
1556	configurations for model
768	find the indices of the walls
1727	Create dataloaders and train
471	ROC curve for ROC curve
1510	MODEL DESTINATION PIXELS ONTO ORIGIN PIXELS
946	altair for plotting
884	Importing Libraries
1298	Replace dummy fillna with yhat
239	Filter Italy , run the Linear Regression workflow
1304	Delete to reduce memory usage
1700	Evaluate each image with fitness function
852	Fare amount versus time since
1664	Import Pystacknet
1537	Find breakdown topics
1547	if inplace , copy the data frame
81	Adding custom Layers
504	Exploratory Data Analysis
417	preview of data
1213	Create strategy from tpu
1078	some config values
1159	LIST DESTINATION INDICES
1765	PLOT FOR PLOT TIME
1633	Exploratory Data Analysis
207	LIST DESTINATION INDICES
603	Binary Label Encoding
1035	Create unique index for categorical features
1413	Which attributes are not in train set
1271	check if all init pairs are in the same class
1314	Exploratory data analysis
263	LightGBM modeling
1133	Get the predictions and masks from the stack
444	Latitude and Longitude
386	unpack the next field from the file
595	Train the model
912	Aggregate the data by parent variable
217	MinMax scale all numerical features
687	Get image and label
1333	Squeeze and Excitation layer
1525	Span logits minus the cls logits seems to be close to best
1590	Read Json Files
296	Exploratory Data Analysis
977	Split into train and test datasets
910	Unique values in each group
303	Load the data
1542	Check the model score
1810	import sklearn.metrics as metrics
897	Plot cumulative importance
1266	Load Test and Submit
1655	Draw the heatmap using seaborn
1803	The function is used to convert images from world coordinates to pixel values
1684	Importing Libraries and Loading Dataset
1174	Assigning colors to the grid
1217	group game time by installation id
364	Scaling the data
1198	Create a submission file
1066	First dense layer
1744	Train the model
1210	get image and make sure nothing was missed out
1802	For each image , calculate the camera position and size of the image
1431	save the data file
814	separate train and validation sets
336	define training and validation sets
643	import torchnet as efn
1412	Class Imbalance
1356	meter split based
837	Set alpha for legends
867	Add subsample to our hyperparameters
528	Standard Deviation
192	convert image to grayscale
1807	load the data
583	Calculating probability of each batch
1563	import xgboost as xgb
555	LOAD DATASET FROM DISK
808	Create training and validation sets
1788	peak frequency , i.e
1614	Split the dataset back into development and test sets
324	Pickle the Dataframes
1219	Function for creating title mode by title
563	Length of Items
1135	Load timestamps from file
1617	remove layter layer and use losvasz loss
1102	Preparing submission file
153	Plot by click
1022	remove the columns that are not in the list
1384	convert image to grayscale
1207	load train and test datasets
777	Plot by Target
1255	Load Model into TPU
1792	Visualizing Web Traffic Months cross Weekdays
1643	Exploratory Data Analysis
623	replace the country column by China
1427	Set some global variables
1134	Visualizing the validation images
196	convert image to grayscale
36	Log target feature
320	Gradient Boosting Regressor
123	Process text for RNNs
1352	Leak Data Preparation
1229	Build dataset objects
68	if augment then horizontal flip half the time
773	Most correlated variables
301	load the image and resize the image
1221	Find the best score and parameters
1444	square of full data
494	Distribution of values for each application
667	For the distribuition of the house
1006	removed low features
1248	get comp data
838	Plot the data
1037	Calculate the memory footprint
626	For each day , find the most confirmed cases
830	Plotting surface distribution
1104	Feature aggregation for credit card balance
948	Boosting Type for Random Search
243	Filter Albania , run the Linear Regression workflow
473	Load Libraries and Data
67	split into train and validation filenames
1660	Importing the Data
1676	Lung Opacity Sample
1036	Average number of zeros in each variable
1144	Growth Rate by Percentage
888	Preparing Bureau Data
1734	impute missing values
1509	LIST DESTINATION INDICES
674	define training and validation sets for each country
6	eliminate bad rows
276	sort the validation data
945	Add iteration and score to dataframe
420	Distribution of Meter Type
881	Preparing Data for Modeling
1800	plot the visibilities
1739	Distribution of winPlacePerc
1361	Importing Libraries and Loading Dataset
981	replace the day outliers with the corresponding values
458	heuristic parameters for Bayesian optimization
1619	missing data in training data
499	handle .ahi files
574	Bedrooms per Interest Level
653	Read the training data
1462	Load dataset and labels
1152	convert validation set to categorical
13	Load Train and Test datasets
883	Fit Bayesian Optimization
178	Average Price for each Category
44	Normalize for each color
1663	kick off the animation
330	Iterate through data
1464	load test dataloaders
544	Predict for each tourney
964	Exploratory Data Analysis
1438	Create image data generator and image generator
1322	Get the data for training and validation
160	label the cells with white background
43	Add new columns to make calculations easier
1239	Run the model
932	Add the results to the dataframe
317	SGDRegressor with SGDRegressor
481	Tokenize the text
1670	Fixing random state
938	Write column names
989	Update installments attributes
99	set up params
337	Train the model
135	Fitting the model
695	remove layter layer and use losvasz loss
1100	Get fold AUC
26	histogram of target counts
1146	load mapping dictionaries
1420	Importing Libraries and Loading Dataset
866	choice of boosting type
648	Load Train and Test Data
1143	For each day , keep only those columns
55	find the number of clusters in the database
1593	fill up the missing values
406	Exploratory Data Analysis
121	Function for checking current coverage
459	Train random search
84	Class distribution of Entries
23	Detect and Correct Outliers
195	inpaint with original image and threshold image
1179	Create the confusion matrix
379	Root Mean Squared Error
1652	Importing the Dataframes
435	Lets display some of our data
1758	Relationship for application and POS balance
1666	StackNetClassifier with GPU
175	Load image data
1338	Final block needs to take care of stride and filter size increase
1393	Converting Categorical Features
245	Filter Andorra , run the Linear Regression workflow
513	Conference Tourney Games
660	Computes and stores the average and current value
1118	Manually adjusted coefficients
485	Pass through the first convolutional layer
1249	parse all trials in a DataFrame
1487	restored model from last checkpoint
923	Cumulative importance of each feature
1141	Exploratory Data Analysis
1367	Plot the distribution of inc per day
1654	Remove columns that are not correlated
714	Calculates the number of missing values per column
718	Load the model in train mode
889	align the test set with the train set
654	Read Test Data
1303	Delete to reduce memory usage
875	For each Hyp , create a DataFrame of Hyp Parameters
1005	Evaluating Feature Matrix
136	Save the model to the cbm file
836	Zooming out of NYC
434	Importing Libraries
312	Iterate through all the columns of the data set
1743	EXTRACT DEVELOPTMENT TEST
1270	check if all the colors are in the image
60	Submit to Kaggle
1240	Applying the model on the sample data
327	Add box if opacity is present
963	Build feature matrix and feature names
685	Convert unicode count to pandas dataframe
1273	Add a new object to the histogram
1572	Distribution of number of data per each diagnosis
954	Add target column for each app
1398	process time series
188	wordcloud for each item description
921	Normalization of importance
1465	Define dataset and model
1381	split the dataset in train and test set
781	change column names for aggregate features
635	Deterministic SIR model
1474	loop to iterate over batches
656	Combined Dataframes
562	heuristic parameters for LightGBM
791	Train the model on train and predict the predictions
1546	Add new columns based on the date column
490	Plot the distribution of the validation features
519	Preparing the training data
1710	Importing the Keras Libraries and Loading Models
1583	Expanding Features
759	households without head
335	Looking data format and types
132	create test generator
1311	Display current run and time used
1713	Build the model
1632	Get just the rows that match the criteria
1299	Embedding the data
1609	XGBoost watchlist for XGBoost
1038	Previous applications categorical features
212	Accuracy of the model
1014	Clean up memory
619	Area of each contour
991	Load the data
1293	Load dataset info
75	create train and validation generators
1399	calculate series mean and standard deviation
512	Exploratory Data Analysis
1313	Feature importance for each feature
1785	Avoid division by zero by setting zero values to tiny float
280	cluster means for each image
169	Fully connected layers
1048	Credit card balance
1209	Save model and weights
1469	Binary Features
649	create the model
1769	SAVE DATASET TO DISK
1628	For each COVID , check if they should convert
1513	Order does not matter since we will be shuffling the data anyway
376	Mean absolute error
892	Check if there are any missing values
1433	Plotting the count of links
1362	Converting date and time information
1224	Modeling Parameters
1538	actual , predictor , actual
1789	IMPORTING MODULES
748	Load model from checkpoint path
1295	Prepare Training Data
1552	Average trip duration for each day of the year
922	Most Important Features
117	Submit to Kaggle
1762	missing data in feature matrix
1095	Predict on validation set
1746	replace some outliers
1479	MODEL DESTINATION PIXELS ONTO ORIGIN PIXELS
749	Process Detection Boxes
1594	Tokenize the sentences
1750	Create entity from previous app df
1050	check data type
96	Save before and after normalization
1717	LOAD DATA FROM DISK
232	ConfirmedCases , Fatalities , and Revenue
1472	size and spacing
102	grid mask
1533	Mean ROC curve
1103	Importing the Dataframes
641	For negative words , remove stopwords
1309	Frames x , y , x
1468	Maximum Scaled Features
88	Create the models and summary
904	Clean up memory
400	function to convert a tensor to image
722	Replace the dummy fillna with the ordinal values
414	Importing Libraries and Loading Dataset
289	Classification Report
451	Only the classes that are true for each sample will be filled in
1017	Sort the table by percentage of missing descending
1481	Distribution of Categorical Features
1516	Detect hardware , return appropriate distribution strategy
224	Convert floats to int
1471	Order does not matter since we will be shuffling the data anyway
957	Relationship for Previous Features
58	you can play around
1598	missing data in training data
1130	Create dataloaders for training
66	load and shuffle filenames
201	Exploratory Data Analysis
767	Find the columns that have high correlation
1486	Get the pretrained model
1820	Expansion of news data
1637	do cumulative count for all features
1107	Load sentiment file
1236	draw the text boxes with the display string
1448	Scaling of the data
462	Importing Libraries
970	align the test set with the train set
1274	Determine the unique colors of the true image
315	Accuracy of the model
1757	Add the relationship to the es object
633	Running Models
1723	missing entries in the embedding are set using np.random.normal
410	OneVsRestClassifier with SGD
39	Get next batch
849	Feature importance for each feature
404	initialize zeros for best and last outputs
903	Score each validation and train scores
1023	one hot encoding
226	visualize of LB score of each COVID
919	Balance Payment Features
369	Create LightGBM data containers
167	Only the classes that are true for each sample will be filled in
56	fastai imports for fastai
18	impute missing values
1264	Load the model for the test set
843	separate train and test sets
1353	iterate through all the columns of a dataframe and modify the data type
1414	sort the dict numerically
1728	Sets the model in training mode
1541	Arima model check
1059	Visualize the validation images and mask
1147	Unique IDs from train and test
634	Set up plot
1735	set matplotlib to darkgrid
439	Most commmon IntersectionIDs
1340	Remove those that are not noise
1756	Relationship between Applications and Previous Application Features
1285	member checkpoints for each fold
1766	cross validation and metrics
8	merge with building info
375	Run all processes in parallel
855	separate train and test sets
1391	apply the bboxes to the image
754	Deal with Poverty Levels
503	Scaling with skimage
1321	Initialize tensorflow session
204	Remove other air pockets insided body
968	remove low features
882	Train the model on the dataset
419	Examine the data
1755	Relationship Bureau Bubalance
592	Read the data
236	Filter Spain , run the Linear Regression workflow
1349	Leak Data Preparation
1476	MAKE CUTMIX LABEL
1521	ROTATE DESTINATION PIXELS MALES
469	Preprocessing and modeling
785	Normalization of importance
739	Determine the ratio of discretization
297	Split the data into training and validation set
735	Classify an image with different models
1530	Previous app features
1517	Exploratory Data Analysis
507	Split the data back into train and test sets
421	plot for meter reading
234	Filter selected features
911	Create unique index for categorical features
90	fast less accurate
1237	Decoding the image
266	Check for missing values
1523	checking number of examples in the oversampled training dataset
74	cosine learning rate
1120	function for renaming columns
628	For each day of the year
1113	Subset text features
1561	For macro features
411	OneVsRestClassifier with Logistic Regression
443	Latitude and Longitude
1052	Train the model
1176	save multiple images in a list
351	Random Forest Features
1070	convert image to grayscale
772	Plot the autocorrelation of the signal
1020	Create a list of all the columns to remove
244	Filter Andorra , run the Linear Regression workflow
1737	Exploratory Data Analysis
686	Load image generator
1819	Join market data with news data
736	Find number of zero features
1667	Predict and Submit
1761	label encoding categorical features
1624	Importing Libraries
783	Import the Libraries
627	For each day , get corresponding spain cases
496	Exploratory Data Analysis
1329	Encodes a block to a string
1117	Compute QWK based on OOF train predictions
15	Import Libraries and Data Loading
150	Check if cat is DL or not
1397	Exploratory Data Analysis
85	Prepare data for modeling
1178	Display DICOM for patient ID
251	SGDRegressor with SGDRegressor
172	Save the model in JSON format
1233	Build dataset objects
1357	Find Best Weights
1551	Average trip duration for each day of the month
827	Load image and convert to grayscale
582	Probability of each model in batch
1289	Initialize input layers
1801	k is camera instrinsic matrix
1386	Save image and scale
972	Bayesian Optimization
151	Exploratory data analysis
1696	Evaluate the image by applying the function on the image
890	Bivariate Correlation Matrix
158	Check image shape
1370	Label Encoding for categorical features
762	plot the actual counts for x , y
345	define a generator that iterate forever
1430	make test features
1741	HANDLE MISSING VALUES
1186	update session statistics
535	Classification and Prediction
971	Bayesian Optimization
1508	Iterate over training images
433	Ignore the warnings
1227	Import the Data
1511	input layer and data augmentation layer
1124	save oof val and test
1138	Feature importance for each feature
569	Hours of The Day
684	Creating a dictionary for fast lookup of labels
352	load the data
1154	Diff Two Differences
359	skin segmentation for each image
205	Importing Libraries and Loading Dataset
618	Area of each contour
140	Ensure determinism in results
163	RLE Encoding for the current mask
901	LightGBM Classifier
597	Data loading and overview
1760	Process categorical features
1165	Create mel features
848	Random Forest Regressor
1722	missing entries in embedding are set using np.random.normal
259	convert floats to int
537	GRID Search for GridSearchCV
864	Fitting and predicting
949	Plotting learning rate distribution
488	Most of the numerical features are categorical , i.e
1709	Importing sklearn libraries
933	Sort the results by score
515	Calculating the contributions of each player in the team team group
716	Random Forest Regressor
580	Logmel feature extractor
1332	Depthwise Convolution
1620	missing data in training data
909	For each parent variable , extract their ids
229	Ensembles ensemble by target
1275	Remove abnormal background from image
372	Voting Regressor
706	MODEL AND PREDICT QDA
1752	Create entity from dataframe
1232	Load the data
1470	Scaling the data
975	Add iteration and score to dataframe
1188	Calculate average accuracy of each assessment
1585	fill all zero values
885	Importing Libraries and Loading Dataset
1119	Distribution inspection of original target and predicted train and test
1445	note that we only have two columns , i.e
1131	Create dataloaders
111	Merge the output df with the input df
1105	load mapping dictionaries
1065	Hyperparameters for model training
1252	Load the best weights
614	Weight of each class
1754	Relationship for application and installments
708	ADD PSEUDO LABELED DATA
1317	Initialize tensorflow session
463	preprocessing , metrics and modeling
1151	load train dataset
869	Create output file and open a connection
784	Random Forest Classifier
1267	Here we take the test texts and the questions from the test sequence
1742	Scaling the target
1034	Unique values in each group
1570	convert sample to convert folder
1724	text version of squash , slight different from original one
789	Ignore the warnings
42	Distribution of year , month and day number for the official dataset
527	Classification and Prediction
497	reduce the sample for each column
655	Checking missing values
194	plot image with blackhat
1606	Defining Training and Testing Sets
384	LOAD TRAIN META DATA
839	Set alpha for legends
1653	Exploratory Data Analysis
1435	Display DICOM for patient ID
620	Area of each contour
253	Create LightGBM data containers
1201	Detect hardware , return appropriate distribution strategy
886	Loading the data
293	Extract the first dot from file names
275	Initialize LightGBM model
92	A utility function to load data as a Python object
272	configurations for model training
899	one hot encoding
1203	Obtain the original fake paths
1225	convert flat flat flat flat to gray
536	Standard Deviation
925	Fitting and predicting
1636	Band for HourofDay
606	Only the classes that are true for each sample will be filled in
1403	Get the data in the correct format
657	separate train and test sets
1641	Exploratory Data Analysis
1030	There might be a more efficient method to accomplish this
806	convert parameter names to int
200	Importing Libraries and Loading Dataset
577	Get a sample
962	Build feature matrix and feature names
30	load the data
223	Convert floats to int
880	Load the Data
567	A Keras implementation that mimics that of
1164	DISPLAY IMAGES FOR EACH VALIDATION
265	Scaling the data
1253	Train the model for the given category
425	Distribution of meter reading for each building
937	Create a file and open a connection
1611	Which columns should be categorical
424	Distribution of meter reading for each month
1063	Initialize model and output directories
602	Load the Data
72	Defining iou loss function
371	Extra Tree Model
943	Train ROC model on test data
1345	Check that all the predictions are the same
1483	These are our new heads
1604	Remove nulls from train and test set
1280	Check the shape of the task
1046	First merge by loan
418	preview of data
790	Cross Validating the model
1449	function to create look back dataset
396	Accumulate the confusion matrix
1707	Build the model and check the program
721	Most of the columns are nominal
758	Find households where the family members do not all have the same target
982	Converting date features from numerical features to timedelta features
10	merge with weather data
237	Filter Spain , run the Linear Regression workflow
1168	Log transformation of TransactionAmt
1012	Add column name
1531	Aggregate for Previous Application
585	Year of Release , which year has most of the releases
1389	Stratified Validation
53	Draw the graph
1090	Generate submission file
1243	only making predictions on the first part of each sequence
145	Read the data
1109	Unique IDs from train and test
941	Add the results to the array
668	Load Train and Test Data
1009	For each feature group
698	Applying CRF seems to have smoothed model output
436	Preview of Data
57	for reproducible results
142	get the rest of the columns
632	Load the data
1092	Applying CRF seems to have smoothed model output
157	Print final result
584	Apply the probabilities to the dataframe
719	save model and preprocessed weights
191	Display scatterPlot with description length VS price
1527	Computes official answer key from raw logits
609	save the images to the directory provided
873	Write column names
325	Pickle the Dataframes
1372	LGBM Final Model
551	GRID Search for GridSearchCV
1494	Predict on test set
219	Importing Libraries
202	Determine current pixel spacing
1627	Optimize on all the data
313	Encoding the categorical variables
1452	Sort by visit day , and keep only those columns
1268	Linear Weighted Kappa Scores
815	LightGBM Classifier
38	Set the index of train data
1096	Encodes the test predictions and generate submission
1736	Exploring for numerical features
590	Gaussian target noise
1524	Eval data available for a single example
1272	check the object pairs are in the right state
387	Deal with the given item
1072	You can access the actual image like this
746	fill the label with the most occured series
405	calculate the fit vector
197	blackhat with blackhat
1379	apply the augmentations to the image
960	Total number of features
1780	Edgar Allen Poe
461	Train random search
638	custom function for wordcloud visualization
712	ADD PSEUDO LABELED DATA
571	Hours of the Day
24	Detect and Correct Outliers
1649	Exploratory Data Analysis
1291	Squeeze and add
1419	Importing Libraries
986	Adding some new features from previous days
753	Import Train and Test Data
467	Precision and Recall
181	Price of the first level of categories
1033	For each parent variable , extract their ids
1126	Load image and mask
483	Create the model with all layers
725	Model and Predict
803	Confidence by Target
693	LOAD MODEL FROM DISK
101	load image file
613	Importing Libraries and Loading Dataset
1283	Split the data by label and ids
928	if you have subsample set to
1786	load the parquet file
355	For each target , determine the number of duplicate clicks
1222	construct list of parameters from grid
71	convert images and masks to numpy array
1566	same for test
1635	Exploratory Data Analysis
824	Apply the cutout augmentations on the image
1704	delete the best candidate from the candidates list
1128	Compute coverage for each class
116	grid for parameter selection
79	Scaling with gcv
1578	replace NaN values with
282	checking some of the data
1318	build train and test dataframes
1772	always call this before training for deterministic results
482	Build the model
608	Adds absolute path to filenames
908	Remove columns which are not present in test set
3	Reset Index for Fast Update
711	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1087	convert coviton to class
77	retrieve x , y , height and width
505	Exploratory Data Analysis
83	Read text data
1599	missing data in training data
1002	Evaluating Feature Importance
465	Bayesian Train and Validation
70	add trailing channel dimension
600	Extract mask from image
453	draw image and return bounding box
254	Gradient Boosting Regressor
152	Plotting the cross tab
78	save dictionary as csv file
348	For each feature in matrix , highlight the most correlated values
1242	Run the detector and save the prediction string
1097	Check that train and test indices overlap
1555	Create LightGBM model and train
514	Exploratory Data Analysis
130	Create train generator and generator generator
978	There might be a more efficient method to accomplish this
306	Combining train and test sets
33	Separating features from visits
640	For positive words , remove stop words
281	Checking the directory of the input files
1422	deep copy the sentence list
953	Importing Libraries and Loading Dataset
710	MODEL AND PREDICTIONS
1437	Number of Patients and Images in Test Folder
1160	ROTATE DESTINATION PIXELS MALES
1646	Build model and return top layer
456	Importing Libraries
1497	Get the pretrained model
100	code takesn from
1387	Apply transforms to sample
1091	Create submission dataframe
478	Exploring the data
1506	FIND ORIGIN PIXEL VALUES
894	Feature importance divided by importance
1573	Which columns are categorical
73	Create the model and compile
294	Preparing submission file
353	Create arrays and train model
474	Create the submission file
1512	size and spacing
221	highlight based on threshold
1507	Iterate over training images
291	Preparing test images
1	Resize to desired image format
961	Aggregated feature matrix and feature names
1262	Load an image
958	Aggregations for feature engineering
1137	Same for test and train months
541	GRID Search for GridSearchCV
1671	Preprocess the data
108	Sales volume per year
1669	extract input and output parts of the pattern
1656	Add the data type in the list
318	Decision Tree Model
1777	plot for second image
357	Looking at different types of images for different classes
1307	Scaling happens when the maximum quantized value is high
1804	Plot ROC Curve
723	Replace NA values with unique values
147	Number of clicks by IP
1569	Initialize Bayesian Optimization
1791	Extract year and month from date for each day
14	histogram of target counts
1519	Number of repetitions for each class
761	Exploratory data analysis
980	Most of the features are Ordinal , i.e
801	Add target and confidence for each fold
115	grid for parameter selection
399	Accumulate the confusion matrix
1484	Generator that iterate over all the jsonl files
1428	add PAD to each clean sentence
1597	checking missing data
374	Avoid division by zero by setting zero values to tiny float
395	Accumulate the confusion matrix
1336	Skip connection and drop connect
1071	You can access the actual face itself like this
1550	Average week of year for each year
1169	Clean up the missing values
1300	Process text for RNNs
1718	Tokenize the sentences
1301	Process test data
812	Write column names
1218	Statistics by game time group
97	before before after normalization
587	Bathroom Count Vs Log Error
999	longest element in a sequence of elements
995	Most Common Client Type where Contract was Refused
7	declare target , categorical and numeric columns
1610	Categorize Class Imbalance
242	Filter Albania , run the Linear Regression workflow
1177	set color of particles
1747	Amount loaned relative to credit amounts
1493	compute validation score
440	Exploratory data analysis
1582	Expand Features
143	Compiling the model
871	Fit the model on the sample data
1725	The method for training is borrowed from
95	For each image id , determine the indices of those images
508	Adaptive Thresholding
1016	Bureau balance by each loan
1683	Apply the greycoprops to the image
920	Credit card balance
979	Finding number of boolean variables in train set
408	Wordcloud for each tag and frequencies
817	Apply the method on the train and test set
495	Defining data path
689	function to show an image
1592	some config values
1364	addr can be either of the four methods mentioned above
54	check if the duration is less than
144	get the data fields ready for stacking
1783	wordcloud for First Topic
1645	Cross Validity
9	fill test weather data
1535	Importing the Dataframes
171	Adding custom Layers
209	FIND ORIGIN PIXEL VALUES
984	Difference Bureau Credit Features
1642	Exploratory Data Analysis
862	Standard deviation of best score
1607	one hot encoding
1687	For each image shape , determine the intersections
1694	Plot image and corresponding mask
1056	Split into train and validation sets
94	function to create a subset of the values that appear in the vset
1797	plotting rolling statistics
442	Latitude and Longitude
380	unpack the next field from the file
185	Most of the items have a price of
190	Distribution of the length of the description
441	Latitude and Longitude
127	For categorical features
734	Classify image and return top matches model
161	make sure label is small
362	Load image file
358	skin like mask , skin image , etc
333	Iterate through the test images
29	Load Train and Test datasets
1195	Make a prediction for test set
1480	process each image one by one
438	Check the shape of our data
1576	checking missing data
1060	separate train and test sets
1716	FUNCTIONS TAKEN from
493	Applicatoin train data
1211	Pad the images
1212	Plot the smoothed segments
427	first column
604	Load raw data
798	Split the data into train and validation sets
747	Make all layers following the Keras convention
1149	Preprocess preprocessed images
1121	Set up X and y datasets
1180	Assign the new weights to the training set
164	Load image and convert to grayscale
104	Compiling the model
1122	define lightgbm params
1612	Split the dataset back into development and test sets
422	Distribution of meter reading hours
233	Converting Date column
672	List of models to be engineered
916	Merge with bureau info
231	Merge train , test and train data
705	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
182	There are two categories with highest prices
764	create a list of bounding rectangle points , e.g
936	LightGBM model with random search parameters
1699	iterate through all the files in the task
1634	Diff Diff Diff Diff Diff
1327	Convolutions like TensorFlow , for a fixed image size
1175	Add the cylinder to the graph
1571	select the best loss
1679	get some sessions information
146	Number of different values
213	One Hot Encoding
1677	Study of patients in patient class
605	Pad the audio data
1496	These are our new heads
1787	Extract the error from the xs array
1079	Add the feature for each of the words
665	Aggregate the number of bookings in each month
696	Exclude background from the analysis
726	Lets look at the photos we found
1408	Apply the model on the test sample
1574	Defining data path
407	Apply each feature to the pool
179	Mean price by category distribution
370	Gradient Boosting Regressor
1226	Plotting some random images
579	Calculate logmel spectrogram using pytorch
61	Exploratory Data Analysis
1108	Load image file
1763	Extract missing values from feature matrix
413	Importing Libraries
206	CONVERT DEGREES TO RADIANS
914	Aggregate the data for categorical features
174	Load an image
230	Implementing the SIR model
1775	Sets the model in training mode
1738	Importing Libraries and Loading Dataset
1478	LIST DESTINATION INDICES
80	Convolutional Layer
992	Relationship for Previous Features
1382	Porter Stemmer
1057	apply the mask to the image
1458	checking missing data
969	get the train and test sets
1162	Order does not matter since we will be shuffling the data anyway
1302	Load model into the TPU
682	Convert unicode count to pandas dataframe
1814	Copy predictions to submission file
133	Split the data into train and eval data
1040	Merge the train and test dataframes with the previous counts
1396	Convert Var LB to year of last week
124	pct change for group by columns
891	Remove unwanted columns from train and test set
550	Standard Deviation
935	Sort the results by score
1183	get config dictionary
310	Preparing the data
1424	For lower case words
1732	Samples which have unique values are real the others are fake
284	Defining the label
822	Load image and convert to grayscale
1475	MAKE MIXUP IMAGE
863	Train model and predict probabilities
447	Encoding the Regions
1394	For each image determine the number of masks
486	Importing the Keras Libraries and Loading Models
542	Fitting and Predicting
673	preparing data for modeling
489	Group by time and count number of zeros in groups
1350	iterate through all the columns of a dataframe and modify the data type
299	Read in image and resize
1688	For each image id , determine which color is available
1279	now we can train by both
49	Draw the graph
1501	Detect hardware , return appropriate distribution strategy
1548	Add new columns for weather features
1163	Order does not matter since we will be shuffling the data anyway
780	Range for each feature
730	Extracting time calculation features
1691	Apply the function fct to each of the unlifted features
1115	Check if columns between the two DFs are the same
1813	summarize history for loss
109	plot for raw data
390	Set some parameters
1347	iterate through all the columns of a dataframe and modify the data type
1082	A single set of features
859	run randomized search
671	Preparing data for modeling
1021	iterate through an array and assign the column name to the next column
1482	Initialize input and main directories
728	load all image labels
733	Function for reading and decoding an image
1158	size and spacing
683	Convert unicode count to pandas dataframe
727	Most of the items are charge free
1315	THIS METHORD IS LESS THAN MALES
1544	if inplace , copy the data frame
1354	Fast data loading
1116	Returns the counts of each type of rating that a rater made
1257	Build the new data
561	Number of missing entries in each column
1740	Distribution of DBNOs
1041	Reducing the memory usage
1418	Importing Libraries and Loading Dataset
1247	only making predictions on the first part of each sequence
1564	Now we label encode the missing values
1808	Creating a dataframe counting the number of transactions
1260	preparing submission file
1818	Function to check if index is present or not
533	GRID Search for GridSearchCV
1400	convert series to logits
1324	Change namedtuple defaults
1426	get the minimum and maximum length of each string
1007	Average number of zeros in each variable
1245	Converting inputs and labels from list to array
565	Predict feature importance
1150	Scaling with augmentations
740	update res line
34	Load Train and Test datasets
1499	restored model from last checkpoint
208	ROTATE DESTINATION PIXELS MALES
1781	Fit the model using all the text
1282	Split the data into train and test
1553	Average number of days of the week
1705	Return the program that has the most candidates
1557	Creation of the External Marker
612	Load the Testing Data
1360	Leak Data Preparation
1770	missing entries in the embedding are set using np.random.normal
645	scores and types of fix samples
732	Function for reading and decoding an image
1708	Importing Libraries
1721	LOAD DATASET FROM DISK
821	Random Forest Classifier
771	calculate capitm based on tamviv
484	imports for building the model
1500	Save results to .txt
1591	Load and Evaluate
644	Perfect submission and target vector
63	Distribution of continuous variables
391	convert test image to float
264	Prepare Training Data
955	Load entity data
286	Load the data
663	Aggregating by month for each bookings
1284	Split the data into train and test sets
1182	compute the contributions of each batch
1001	Get the most recent values for each column
1170	Create arrays for train and test sets
389	For each image id , image type , determine empty images
257	Fit and predict
1208	define model and save model
1261	Create test generator
220	For each feature in matrix , highlight the most correlated values
12	Eigenvectors in train and test set
186	Price without outliers
1094	Determine the input dimensions
93	Fit before and after feature engineering
378	Mean absolute error
850	Get the data type
1073	Target and child count
832	inverse transform for LB
1344	Check that all the predictions are the same
1031	Suppress warnings due to deprecation of matplotlib
1206	Create Densenet layer
1490	Read candidates with real multiple processes
1136	Remove test data that were not on
1238	Initiating the Model
292	Load the model and predict the predictions
795	Hyperparameters for Bayesian Optimization
1377	Visualizing Augmented Images
1443	square of full data
82	Submission file
1720	SAVE DATASET TO DISK
1228	Load the data
516	First merge all the confferences
701	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1374	Drop target and fill missing values
810	store scores and standard deviations
1140	Plot for feature importance
538	Fitting and Predicting
1331	Loads pretrained weights , and downloads if loading for first time
1811	plot of actual and predicted
1190	Event code distribution
617	Split the data into train and test sets
87	Generate fake paths and fake images
1456	For each price , find the number of rooms
699	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
255	Extra Tree Model
290	Clean temporary directories
1342	Calculates the mean ratio of each image by averaging the missing values
1745	sieve eratosthenes primes
409	XGBoost multilabel
1581	New Features Features
198	plot image with blackhat
1401	convert series to array
834	Exploratory data analysis
825	Set to instance variables for future use
917	Previous Application Features
2	Add new Features
59	Unfreeze the model and search for a good learning rate
332	load the images
1661	Find the optimal order of lk
468	Load Libraries and Data
278	Wordcloud for text column
1518	Exploratory Data Analysis
331	Iterate through each patient in the dataset
91	Identify any faces not present in this frame
1730	Add leak to test
367	SGDRegressor with SGDRegressor
246	Set the dataframe where we will update our predictions
1251	ONLY TRAIN WITH CATEGORYS
1369	Draw the centroids of each district
931	count number of possible combinations
1187	Get the sample for the test set
998	Normalization of Discrete Features
309	create an embedding matrix
105	Importing the Dataframes
1618	load test weights for each fold
1355	iterate through all the columns of a dataframe and modify the data type
1495	Initialize input and main directories
578	Calculate spectrogram using pytorch
840	Manhattan Distance by Fare Amount
841	Euclidean Distance by Fare Amount
4	Remove Unused Columns
1085	check if image is different from original image
295	Binary Target Features
1088	remove padding from images
168	update learning rate
1025	LightGBM Classifier
373	Compute the STA and the LTA
415	Lets display some of our data
828	Load image and convert to grayscale
27	histogram of whole dataset
1343	Plot the errs
751	Set some matplotlib configs for visualization
802	Add target and confidence for each fold
934	Fitting model with grid search
1647	Iterate the folds
240	Filter Germany , run the Linear Regression workflow
1575	Read in the data
1650	Cluster for all hits
1086	Check that training set is ready
1153	Visualize the images
268	Extract features that are highly correlated
1363	addr can be either of the four digits mentioned above
844	Fit the model
1074	Distribution of number of missing entries in each group
1796	Create index for test stationarity
323	COMPUTE KAGGLE MODEL
321	Extra Tree Model
283	Exploratory Data Analysis
1173	convert to HU
1045	Join to main dataframe
942	Sort the results by score
1812	Importance with RandomForest
183	Brand name , brand number of item
260	convert floats to int
709	MODEL for QDA
1099	Apply the gbm model on the data
993	get features and interesting features
1520	LIST DESTINATION INDICES
308	Padded Dataframes
1601	checking missing data
305	Load the model and evaluate the model
1579	New Features Exploration
872	Create output file and open a connection
218	MinMax scale all numerical features
842	Correlation with Fare Amount
388	Function for computing histograms
1287	Blurry Sample Images
1039	Previous counts categorical
857	Predict on validation data
1296	create data generator and image generator
1047	Converting the data frame to dataframe
1231	Create fast tokenizer using DistilBERT
288	Evaluating model on all data
107	Check number of stores and item id
588	There might be a more efficient method to accomplish this
770	Bonus Variable
1114	Remove missing target from test
1404	DataFrame for each predicted encoding day by day
570	Days of the Week
786	Most importance features
622	Cache for all files in the dataset
1101	Model for LightGBM
423	Monthly readings on buildings
1148	input image size
1216	Computes stats by installation id and event time
1250	save the best model
377	RMSE for each fold
125	Prepare Data for Modeling
939	Create dataframe for future use
466	ROC curve for ROC curve
1776	Importing Libraries
1630	Go though the Province table and see if there are any matches
531	Classification and Prediction
509	Adaptive Thresholding
1651	Cluster for all hits
769	find ROOF heads
1125	Final output layer
1366	Load the Data
631	Computes gradient of the Laplace model
951	Load the Data
452	Importing Libraries
287	Load the model
69	add trailing channel dimension
383	"../input and test directories
878	scores for each ROC
1080	division by the number of words to get the average
354	check if everything seems to be ok
126	label encoding continuous features
334	Looking data format and types
159	Detect connected objects
952	Preparing Data for Modeling
228	Ensembles ensemble by target
1054	Clean up memory
1194	Make a submission file
558	Determine the order of the features
874	Train ROC model on test data
341	Rename columns that are not features
64	Distribution of continuous variables
557	Interactions for each feature
279	Principal Component Analysis
256	Voting Regressor
98	compare two sets of intersections
1375	create dummy columns for all three features
450	heuristic parameters for LightGBM
393	Optimal number of clusters for each signal
860	Fitting model with njobs
448	Updated data for modelling
267	Checking for Player College Name
344	Create a generator that iterate three items
1623	Logistic Regression
607	Return the contributions of each class
1558	Creation of the Watershed Marker
1504	LIST DESTINATION INDICES
690	Iterate through data
1019	merge with bureau data
724	imputer for continuous variables
788	Plot the cumulative importance
741	Deformation of each line
1701	Create a list of candidates with maximum length limit
564	convert item description into int
624	Grouping by country and getting aggregated data
1451	inverse transformation
1640	Load the Data
416	load all the data
1111	Extract processed data and format them as DFs
215	XGBoost Evaluating XGBoost
437	Preview of Data
744	Convert image id to filepath
356	Load an image
1265	Create train and validation datagens
1297	Predict on the test set
1156	using validation set for training
1463	CNN model for multiclass classification
114	Merge the output df with the input df
902	Train the model
865	convert parameter names to int
779	AVERAGE AVERAGE
11	Compute the STA and the LTA
704	MODEL AND PREDICT WITH QDA
1615	Visualize few samples of mask class
1588	Argmax of all the predictions
1256	Create train , test and test folders
1181	normalize conf mtx and output prod
930	Create random results and grid results
184	Brand price per brand name
956	Create entity dataframes from dataframe data
1453	Drop rows with NaN values
118	Pulmonary Condition Progression by Sex
525	GRID Search for GridSearchCV
1554	Fit lightgbm model
526	Fitting and Predicting
1793	Plot Web Traffic Months cross days
522	First merge the model with the training data
966	Plotting the distribution by Target Value
1171	save multiple images in a list
927	Get score and hyperparameters for CV
1459	checking missing data
1748	Create entity from dataframe
692	LOAD MODEL FROM DISK
697	Precision helper function
1013	Clean up memory
1312	reduce validation set for each fold
1129	Create train and test paths
103	Y is the target , X has the rest
556	Helper function for creating feature names
1439	Loading the Data
1286	Exploring Blur Images
647	Importing Libraries and Loading Dataset
328	Iterate through each patient in the dataset
1714	cross validation and metrics
679	Split into a list of labels
913	Aggregate Categorical Features
17	Now extract the data from the new transactions
457	Importing Libraries
1450	create train and test datset
222	Linear SVR model
1644	Click Rnd and is attributed
1441	process test images
1467	fill NaN values with most occured value
833	Plot the distribution of Fare
162	find two cell indices
990	Credit for loaned during
1545	Create new column name
731	Relationship between Volume id and hits
120	Based on this great kernel
472	Precision and Recall
1771	text version of squash , slight different from original one
1629	Exploratory data analysis
1346	Check that all the predictions are the same
846	metrics on train and validation set
1567	Pinball loss for multiple quantiles
1199	plot of validation loss vs boosting iterations
1625	New feature called Active , Confirmed , Recovered
1673	Add box if opacity is present
199	inpaint with original image and threshold image
361	Load image data
1767	Tokenize the sentences
534	Fitting and Predicting
1157	numpy and matplotlib defaults
1621	Exploratory data analysis
0	show image and corresponding mask
477	vectorize the text
1622	Print the feature ranking
271	Importing Libraries
1390	Using tranformations for training
1602	add some features
1193	Store predicted revenues for each Visitor in training set
523	insert confstrength in tourney df
1515	Initialize input and output directories
757	Visualizing label counts
599	Load image and convert to grayscale
646	summarize history for mean
776	Pair grid for plotting
1319	Extract target and input data
173	Calsses value of each class
1674	Add boxes with random color if present
851	Add field elapsed time in seconds
454	Run the model on the input image path
1235	draw image and return bounding box
707	CV for QDA
1083	Load the data
247	Apply exponential transf
703	STRATIFIED K FOLD
1075	Read Train and Test Data
1560	Correlation for macro features
326	Initialize patient entry into parsed
1473	MAKE MIXUP LABEL
1540	check the model score
270	LightGBM Model
987	Previous Loan Amounts
593	factorize categorical features
154	missing data in time
616	Standard Deviation
48	Normalize for each color
225	Visualizing visualization of CNN
1028	Clean up memory
737	draw the image
976	Add hypth to random list
342	save the submission file
470	Merge Dataset
1246	Training History plot
1662	show sample image
45	Draw the graph
1010	Add the column name
918	Converting installments file
1205	Read in all the files
368	Decision Tree Model
28	MODEL AND PREDICT WITH MODEL
52	Normalize for each color
1106	Load metadata file
1084	load the image
302	load the image and resize it
76	load and shuffle filenames
1011	Function for creating categorical features
1682	An optimizer for rounding thresholds
487	Exploration of data
845	calculate mean age per era
713	StratifiedLS label slice
1259	create train and test generators
1029	Create metrics dataframe
412	Importing Libraries
794	select only the selected features
1405	For each store , compute rolling mean
363	Extract target from training data
805	Add subsample and subsample frequency to our dict
1417	EfficientNetB
1077	function to remove stopwords from text
1276	get the inputs and output
501	show the image and resize
664	Aggregate number of bookings per day
1421	convert the comments to lower case
385	read all test data
793	Random Forest Classifier
340	Create dataframe for future use
520	Train the model
106	Importing the Dataframes
316	Linear SVR model
1123	LightGBM data structures
1409	Set X and y variables
1489	Create a dictionary for each answer id
22	Impute any values will significantly affect the score for test set
738	draw the image
277	reorder the input data
1712	Label Encoding for Class Imbalance
1076	Comment Length Check
432	Accuracy of each model using all folds
1055	Create metrics dataframe
1004	Evaluating Feature Matrix
670	Exploratory Data Analysis
1798	shift train predictions for plotting
1466	Run the evaluation batches
792	Merge with predictions
1334	Expansion and Depthwise Convolution
1532	Random Forest Classifier
630	Define Jit Model
621	Area of each contour
1665	fill mean for floats
1698	test if the program has at least one sample
20	Imputations and Data Transformation
639	For positive , negative and neutral sentiments
1410	Set X and y variables
1455	inverse transform for test set
1434	word ids for unknown words
1731	Creating a video from a list of images
1817	Lidar image data
269	Deal with dense players
1142	Feature importance for each feature
1689	Sort the xs by max number of zeros
988	Exploratory check
611	save the images to the directory provided
1703	Iterate over the best candidates
62	Count of missing values in categorical variables
1069	Preparing the submission file
796	Initialize lightgbm model
700	MODEL AND PREDICT WITH QDA
1809	Feature importance via map
381	Deal with the given item
1764	Extract missing values from feature matrix
1795	Fit second AdaBoostRegressor
662	Aggregate the number of bookings per day
594	Concat the text features with the feature values
1779	Generate the Mask for EAP
119	Pulmonary Condition Progression by Sex
1278	find the objects we want to draw
338	summarize history for training and validation loss
403	Train the model
366	Linear SVR model
250	Linear SVR model
1562	List of Train , Test and Train
1161	FIND ORIGIN PIXEL VALUES
1371	LightGBM Model
1359	Fast data loading
1799	shift test predictions for plotting
1447	Create submission file
974	iterate through all the hyperparameters
787	Cumulative importance of each feature
765	Visualize the markers for each image
529	GRID Search for GridSearchCV
1692	lifting function for reducing memory usage
128	Creating a dataframe for train set
170	First component of main path
1751	Load entity from dataframe
86	Check that there are no missing data
1373	LightGBM Classifier
1672	Initialize patient entry into parsed
1223	XGBoost with grid
835	BanglaLekha new observations
1155	Create strategy from tpu
1139	plot for feature importance
1442	process all the patients in the dataset
568	Importing the Dataframes
110	Sales volume per year
40	Load the Data
1197	Preparing dataframe for modeling
854	For each day , pickup frac fare amount
1440	Process the images from the train set
46	For each variable determine the month number
804	Get subsample and drop rate based on type
1549	first merge by date and hour
1234	Load model into the TPU
636	Deterministic model fitting
1681	accurace is the all time wins divided by the all time attempts
1308	Frames x , y , x
720	Load Libraries and Data
1719	shuffling the data
1089	convert test predictions to original image
1759	dfs to remove features that are not features
480	convert text to sequence of words
189	Length of coms
1706	Random selection of best candidates
1305	Weighted toxic score
1580	New Features Exploration
304	Class weights for each class
752	Importing the Data
1184	set all variables to zero
553	Classification and Prediction
25	Load Train and Test datasets
346	define a generator that iterate forever
343	Create a generator that iterate three items
311	Predict on all the columns
491	Pearson Correlation heatmap
1729	Add train leak
65	save pneumonia location in dictionary
1586	Exploring the Model
227	Visualizing visualization of LB score
365	Accuracy of the model
678	Remove outliers and target columns
831	replace inf values with
1595	Pad the sentences
877	Evaluate Bayesian Results and Random Search
743	pivot to create a new dataframe
915	Create index for each feature
642	Common Word frequency for neutral text
16	To plot pretty figures
394	Decision Tree Classifier
426	Distribution after log transformation
319	Create LightGBM data containers
1589	only making predictions on the first part of each sequence
876	iteration score 
350	Linear SVR model
238	Filter Italy , run the Linear Regression workflow
870	Write column names
1675	Normal Image Visualization
879	Iterate through random parameters
285	Defining the label
659	Perform feature agglomeration
745	convert surface names to numbers
1008	Bivariate Correlation
1310	Get feature importance for each fold
615	An optimizer for rounding thresholds
31	vectorizer for vectorizer
1726	for numerical loss
1067	fold number and validation data
5	Encode Categorical Data
475	vectorize the text
545	Select Percentile
1306	Importing Libraries
1348	Fast data loading
455	Draw bounding boxes on the image
887	get the original and bureau features
729	coefficient of variation for different image categories
1773	for numerical loss
572	There are too many orders , so we will subset the data
1423	function for transforming sentence to list of words
1326	Round number of filters based on depth multiplier
492	Checking for Class Imbalance
1492	Predict on validation set
1098	Create LightGBM data containers
1127	Initialize the Data
1816	Read the data
1565	Importing Libraries
816	max probabilities for each class
176	Visualizing some random images
141	Histogram of Fraud class
800	Create dataframe with feature importance
430	Encode Categorical Data
1341	Measured area , and unmeasured area
983	Adding new features from bureau
449	Exploring the data
1292	warm up model
893	Get Train and Test IDs
322	Voting Regressor
829	Load image and convert to grayscale
715	Remove outliers that are not high enough
680	Split the data into a train and test set
1323	Parameters for an individual model block
137	Clean up output
113	Merge the output dataframes with the input df
1457	checking missing data
1678	convert text into datetime
1058	Load the model
1032	Remove columns which are not present in test set
235	Clean Id columns from train and test set
1395	Draw a graph for each MOLECULE
650	You cann choose your configurations here
1337	Update block input and output filters based on depth multiplier
1328	Gets a block through a string notation of arguments
261	Visualizing parameters and LB score
782	change column names for aggregate features
1600	plot of correlations
1368	zoom to desired area
760	Exploratory data analysis
392	load images from train folder
1498	Find the variables that are toxic
1269	Get the colors of the regions in the image
651	Create the submission file
560	Plot Gain importances
1774	Create dataloaders and train
807	LightGBM Classifier
1491	Computes the probabilities for each example and stores them in a dictionary
274	Constants and Folders
1733	Importing Libraries and Loading Dataset
1254	Apply the model on the test set and output the predictions
431	Separating target and row id columns
1358	iterate through all the columns of a dataframe and modify the data type
799	Fitting the model with early stopping
1753	Relationship for Bureau
1587	Create list of features for categorical features
1385	suppose all instances are not crowd
811	Create file and open a connection
1402	Extract the first n samples from the array
586	Bedroom Count Vs Log Error
797	Convert to array
755	replace some columns with float
675	GRID SEARCH for GridSearchCV
1429	make train and test vectors
677	Remove outliers from training set
155	Plot download rate evolution over the day
1220	Preparing data for modeling
625	For each day of the year
823	Wrapper around Cutout augmentations
809	Fitting the model with early stopping
1638	Minute attributions
861	separate train and test sets
1697	Helper function to make a list of images
1024	check data type
1416	Detect hardware , return appropriate distribution strategy
166	Convert labels to dataframe
139	get the CUDA version and the compiler
398	confusion matrix for each target and prediction
1539	testing series for each building
813	Plot the distribution of the predicted labels
1316	convert column names to int
1686	Divide the pixel values into a list
252	Decision Tree Model
1534	Standard Deviation
1584	check which columns have only one value
1749	Create entity from dataframe
479	A simple Keras implementation that mimics that of
985	Credit for Bureau
1631	Total number of Deaths in each State
214	Create LightGBM DataSets
967	There might be a more efficient method to accomplish this
940	Sort the results by score
1502	Order does not matter since we will be shuffling the data anyway
1577	Build the list of continuous features
156	Read the data
1230	Load model into the TFA
997	Get feature names and seed features
249	Accuracy of the model
591	Combining all augmentations together
601	And there you have it
1559	For each feature determine the most correlated features
1794	Fit the model with all the data
1526	Default empty prediction
1790	from sklearn import tree
1388	Store all the boxes in a dictionary
1043	Print some summary information
598	ImageId column contains names of images and masks
548	Fitting and Predicting
820	Random Forest Classifier
89	Check the mean and median absolute deviation
464	Merge Dataset
1668	Sales by Store id
926	Hyperparameters search for LGBM
1258	Pad the images
775	plot the correlation matrix
445	Extracting features from street features
905	Create metrics dataframe
896	There might be a more efficient method to accomplish this
180	zoom by category
1132	Save the prediction and mask for further processing
210	skip columns with different data types
1685	Importing Libraries and Loading Dataset
300	select the validation folders
778	Find columns with correlations greater than
1241	Loading Test Images
973	Set the scoring
1608	heuristic parameters for LightGBM
540	Standard Deviation
1392	Save output path to parquet file
994	Most Common Client Type where was approved
193	blackhat with blackhat
858	plot of ECDF of predicted and actual training
1485	Process the nq lines for training
32	Identity Hate Classification
1626	Optimize for each COVID
1657	Training and Validation Sets
1806	Read the data
1042	Sort the table by percentage of missing descending
177	Most common category
1568	Pinball loss for multiple quantiles
262	Scatter plot of parameters and LB score
947	add search column for random and grid search
1505	ROTATE DESTINATION PIXELS MALES
658	perform model on all data
1110	Extract processed data and format them as DFs
965	Cleaning the data
1639	Click Rnd , by Click Rnd
1522	FIND ORIGIN PIXEL VALUES
1415	Number of labels per each instance
1196	fold results for each model
1015	Read in the data
1263	create train and test generators
446	Encoding the Regions
1244	Read Json Files
1145	Curve for Cases
1383	Train Validation Split
1166	preparing test data
1026	Train the model
307	Tokenize the text
1365	Download and Importance
1167	Importing Libraries
676	Store results in list for future use
1214	Order does not matter since we will be shuffling the data anyway
629	For each country , get their respective dates
681	Convert unicode count to pandas DataFrame
552	Fitting and Predicting
1782	Feature importance via LemmaVectorizer
1432	Find the number of links and nodes
1603	missing data in training set
1215	Only load those columns in order to save space
1172	PROCESSED IMAGE FROM DISK
661	Predict on test set
35	Create a list of embeddings from train text
1436	Number of Images and Patients in Training Folder
1768	shuffling the data
122	Funtion to clean special chars
1659	Preprocess for Neutral Texts
530	Fitting and Predicting
826	Load image and convert to grayscale
766	Draw legends and labels
717	Random Forest Regressor
549	Classification and Prediction
1189	generate train and test data sets
1378	Generate random labels for each image
314	Extract target from training data
944	iterate through all the hyperparameters
1805	Cache for all memory in a DataFrame
298	Read in image and resize
339	Predict on the test set
1596	Checking for missing values in training data
950	Iterate through random hyperparameters
397	Random Forest Classifier
1288	Load dataset info
868	Sample the data with subsample
900	check data type
742	split the ID column by the _ character
666	Sum number of products by product
506	Exploratory Data Analysis
895	Find the features with zero importance
691	Computes gradient of the Laplace w.r.t sorted errors
652	Importing Libraries and Loading Dataset
547	GRID Search for GridSearchCV
1049	one hot encoding
138	Check GPU availability
1290	Excitation Layer
694	add the loss function and optimizer
1460	checking missing data
702	ADD PSEUDO LABELED DATA
1380	You will also need functions from the previous cells
1648	prepare validation set
476	vectorize the text
1528	Join examples with features and raw results
1281	Load Libraries and Data
596	set bird to one
1204	Create fake folder
428	stacked bar chart
1027	Score each validation and train scores
401	Iterate through data
129	Pick a sample image
1529	Read candidates with real multiple processes
382	Exploratory Validation
1185	keeping track of test set features
669	Importing the Data
211	label encoding of categorical variables
581	Round number of steps per epoch
1514	size and spacing
47	Add month and year columns
554	Auc metric for the competition
898	Get a list of zero importance features
1680	the time spent in the app so far
