312	select columns with numerics
224	convert to int
394	Decision Tree Classifier
889	align to align with keras
930	Create random results
1758	same as above
96	This is always a key aspect to review when doing pickle
714	Calculating the miss rate
878	We can see there are some new features
1573	set columns to what you need
8	Merge Building Data
935	sort by score
1441	prepare test data
381	Load item data
872	create a file and open a connection
921	Sum up the importance
303	Loading the data
1355	iterate through all the columns of a dataframe and modify the data type
1111	Extract processed data and format them as DFs
1082	An interface for loading data and processing
1676	Lung Opacity
1167	Loading the Data
67	split into train and validation filenames
910	unique values in each column
1691	lifting function to lift values
1257	Build new data
1022	remove columns that do not exist in the kernel
1456	Numerical and Percentage
1292	warm up model
1689	Not efficient for long lists
1587	create list of features
1081	This loop runs on the dataset
22	Impute any values will significantly affect the RMSE score for test set
749	This is a very performative way to compute scores
1018	Print some summary information
617	Split train and test sets
1130	Pytorch Data Loader
1205	Load and Preparation data
1168	Clean up the data
1409	Set up trainments
1603	Finding the missing values
1718	Tokenize the sentences
14	Show target distribution
182	Performance of Top Features
1145	Curve models
266	There are some missing values in the dataset
526	Training and Prediction
1816	Looking at the data
1313	Light GBM Results
785	Sum up the importance
1805	Lets look at the memory usage of our dataframe
1652	Loading the data
1592	Parameters for preprocessing and algorithms
1792	Web Traffic Months cross Weekdays
525	Run Grid Search Cross Validation
1619	checking missing data
959	Aggregations and Features
1201	Detect hardware , return appropriate distribution strategy
1369	Draw the districts on the map
514	Summary of Wins and Lames
1181	the product of the product of the gradient
1724	text version for squash
965	reset index and reset style
922	Plot normalized importance
1162	Order does not matter since we will be shuffling the data anyway
1017	Sort the table by percentage of missing descending
1345	Show some results
1392	We will need this function later
1204	Create fake folder
1375	calculate the Difference between V32 and V33
659	Perform feature agglomeration
1357	Find Best Weight
920	Credit card information
1782	Calling our overwritten Count vectorizer
1246	one line training history
779	Average Age per Escari
538	Training and Prediction
1217	and reduced to one group of game time
1015	Loading the data
1504	LIST DESTINATION PIXEL INDICES
1539	get different testing series numbers
325	import pickle file
1296	Data image augmentation
191	Description length VS price
1741	HANDLE MISSING VALUES
1407	Train the model
602	Reading the datasets
1055	to update the metrics dataframe
1413	We will look at the labels
774	The competition metric
806	Convert parameters to int
1139	We can see the plot
1725	The method for training is borrowed from
1515	Makes sure that they are aligned
700	MODEL AND PREDICT WITH QDA
450	First we try to identify Defect type
231	Merge train and test , exclude overlap
497	reduce the sample data for reduced distribution
1480	we will run this on multiple images
1496	This is a very performative way to compute
1031	Settings for pretty nice plots
375	Run in parallel
150	Lets see if this is a critical column
641	Now we can take a look at the common words
93	sets the values to the value calculated above
1530	Previous app data
1489	Parse examples from .gz
516	Load team confferences
448	Updated train and test data
1049	one hot encoding
1712	Since the labels are textual , so we encode them categorically
820	RandomForestClassifier with Graphviz
226	Scatter plot of full train set
915	unique values in a pandas DataFrame
1469	build bin features
1638	Minute distribution
918	Now extract installments from csv file
409	Multilabel feature extraction
244	Filter Andorra , run CR
1623	Logistic Regression on balanced dataset
656	Combination of Correlation
6	eliminate bad rows
1075	LOAD TRAIN AND TEST
1177	set color codes
511	Same as above
544	The same as above
1272	Check and Delete Check Pairs
489	Function to group data by time
574	Number of bedrooms
1191	Evaluation of Test Predictions
1651	Clustering for All Features
401	obtain one batch of training images
705	ONLY TRAIN WITH DATA WHEEZY EQUALS I
709	create stratified train folds
1566	same as above
882	Creating a LightGBM Results
1163	Order does not matter since we will be shuffling the data anyway
360	plot the complete images
1236	draw text annotations
1193	fill in missing values with the most probable value
1354	Fast data loading
1509	LIST DESTINATION PIXEL INDICES
1736	Exploring numerical variables
1432	Load data from train.csv
805	Add the results to the dictionary
1554	Create LGB model and train
386	unpacks a file descriptor
1164	we will run this on all training images
1544	Get input data frame
877	Bayesian Results
56	Modeling with Fastai Library
1518	raw training dataset
1324	Change namedtuple defaults
675	Run a grid search
1327	Convolutions like TensorFlow , for a fixed image size
1188	Calculate the average accuracy of each assessment
838	Plot binary Features
1012	Add the column name
200	The most difficult part of this Problem
1776	Loading the Data
912	Agg and Correlation
1011	creating dummy variables for categorical features
1316	convert column names to int
1318	build train , test and train
1747	Correlation between app features
1314	Checking the distribution of the variable
1406	Get just the digits from the seeding
186	Does shipping depend of prices
707	CV with oof
1303	Delete to reduce memory usage
338	plot and visualise the loss
1278	We reset the object detections to their initial state
519	Preparing the training data
1190	Merge train and test events
665	Aggregate the data
750	Combinations of TTA
570	Days of Week
315	Accuracy Model
1563	import xgboost as xgb
170	First component of main path
1688	Takes as input a list of color values
209	FIND ORIGIN PIXEL VALUES
1726	for numerical stability in the loss
1110	Extract processed data and format them as DFs
1351	Fast data loading
1147	Unique IDs from train and test
897	Cumulative importance
787	Cumulative importance
1378	Randomly picked images
1098	Create LightGBM datasets
1070	Convert to RGB image
183	Brand name of item
637	Install and import necessary libraries
1229	Build datasets objects
107	Most common values
411	OneVsRestClassifier with all features
557	Number of times the user clicks
696	Exclude background from the analysis
621	find the areas of each contour
1379	If we have a square we can warp
1376	BUILD BASELINE CNN
412	Importing the necessary libraries
214	separate train and validation sets
1206	Define the model
279	find inertia
893	Get Train and Test data
413	Load Meta Data
37	Lets take a look at the histogram of our model
1696	This is the evaluation function
859	run randomized search
597	Brain segmentation masks
1262	Load an image from a given code
769	we need to predict roof
90	fast less accurate
957	Relationship the previous recordings
267	Find and replace missing values
344	create a generator that iterate over the data
545	Select Percentile
909	Selecting the direct parents
822	Read image and convert to a grayscale image
751	Settings for pretty nice plots
857	Distribution of Validation Fares
1611	Channel Categories
1371	Create and fit model
1063	We will use the most basic of all of them
994	What is the most common client type
59	unfreeze and search appropriate learning rate for full training
390	Set some parameters
793	RFECV feature selection
842	Calculate the correlation coefficient
369	create training and validation dataloader
474	Making submission data
378	MAPE error
1781	Calling our overwritten Count vectorizer
1545	Create new column name
638	generate word cloud
121	Checking the coverage
263	We can test out the individual weights to see the change
18	impute missing values
1014	Clean up memory
1765	PLOT_0 ..
849	Importance for each Feature
103	Deleting to reduce memory usage
1754	we need to associate application with installments
1353	iterate through all the columns of a dataframe and modify the data type
727	Categories of Items
1622	Print the feature ranking
796	parameter value is copied from
316	Linear SVR model
1529	Read candidates with real multiple processes
1194	BanglaLekha Some Prediction
1158	size and spacing
74	cosine learning rate annealing
1690	check if all elements are zero length
1662	Lets plot some of the images
1219	Create title mode
73	create network and compiler
15	Common data processors
30	The number of train and test data sets
77	retrieve x , y , height and width
1751	We will need these functions later
1043	Print some summary information
330	Makes sure that they are aligned
1535	Loading and overview
1746	replace some outliers
884	Loading the Dataset
287	Evaluate the model
193	Blackhat image processing
89	Exploring submission file
651	Making the submission
888	Merge data with previous features
1719	shuffling the data
1711	Read data from file
159	split connected objects
164	Reading the image
1254	Run model on test data
520	Train the model
874	And fit model on test data
1182	we need to calculate the KL score
1307	Calculates the bias we want to use
1208	define model and model name
1716	FUNCTIONS TAKEN FROM
1503	of image and label
887	Get the original features
588	There are too many errors , so we will subset the first
710	PRINT CV AUC
1576	checking missing data
815	Fit best lightgbm model
84	Class Distribution Over Entries
1097	Check if train and test indices overlap
792	Merge with predictions
1460	checking missing data
1212	Applying Quadratic Spline
1438	Create image generator
847	Training model on all data once
1605	We will need these functions later on
794	Select most positive and negative features
119	Pulmonary Condition by sex
929	check parameters between 0.005 and 0.0
1325	Calculate and round number of filters based on depth multiplier
1710	Keras Libraries and Configurations
1039	Previous counts
540	create different transforms
319	create training and validation dataloader
961	We can look into the feature sets
1455	inverse transformation for test data
1166	preparing test data
1315	Atom number converter
1395	Graphs and cool stuff
1753	Bureau Feature
673	prepare data subsets
606	Only the classes that are true for each sample will be filled in
484	import the Keras libraries and packages for keras
1320	convert column names to int
753	Import Train and Test dataset
534	Training and Prediction
843	separate train and validation sets
1789	Forceasting with decompasable model
1408	We clipped predictions
768	find index of walls
1346	Test data prediction
1123	Create LightGBM datasets
890	Set a threshold for the correlation matrix
1547	Get input data frame
979	get the object types
1513	Order does not matter since we will be shuffling the data anyway
990	Amount difference in days
1704	delete best candidate
1437	Looking at the data
225	Scatter plot of LB score
248	Getting Training Data
1048	Credit card balance
867	We need to predict and process the data
458	Set up feature map
1670	Fixing random state
737	Test key drawings
1019	Merge with Bureau data
646	now lets take a look at the average
367	SGD regressor
359	These were mostly used in the past
1078	Set values for various parameters
976	Setting for random search
289	Classification Report
256	Voting model
427	first column
1335	Squeeze and Excitation
1231	Create fast tokenizer
29	Loading train and test data
380	unpacks a file descriptor
1035	Convert categorical variables to integer codes
1655	Draw the heatmap using seaborn
781	change column name
1285	load best model
153	Ratio of Download by click
1199	plot validation loss vs boosting iterations
46	What are the most frequent variables in the dataset
1698	evaluate program and return the result
509	Here is the custom function that we will use to evaluate
647	Importing the Libraries
502	Rescaling the Image Most image preprocessing functions want the image as grayscale
429	Convert year built to uint
1056	Split the dataset into train and validation
829	Load image and convert to a grayscale image
972	random search and bayesian optimization
1558	Creation of the Watershed Marker
1071	retrieve face image
317	SGD regressor
844	Train the model
868	Get sample and subsample
1683	greycoprops of one image
1300	Process text for RNNs
10	Merge Weather Data
64	Distribution of continuous variables
27	Distribution of the data
206	CONVERT DEGREES TO RADIANS
432	Test data AUC
1796	Different Time Series Modelling
1507	now timing for one iteration
766	Create a legend and annotate it
1301	Load test data
1760	Label Encoding Categorical Features
660	Computes and stores the average and current value
327	Add box if opacity is present
1516	Detect hardware , return appropriate distribution strategy
196	Show original image
695	remove layter activation layer
1495	KAGGLE if it is not there already
1211	Here we take the data and resize it
1750	Creating an entity from dataframe
759	households without head
199	inpaint with original image
1598	checking missing data
758	Households where the family members do not all have the same target
1639	Click Rnd
717	Random Forest with Date
1700	Evaluate the program and return the score
212	Accuracy of the model
1331	Loads pretrained weights , and downloads if loading for the first time
875	dict存储数转化为数据的数据
1616	Computes gradient of the Lovas extension w.r.t sorted errors
1569	Create a Bayesian Optimization
1453	drop rows with NaN values
1223	Predict with all parameters
230	Implementing the SIR model
1196	fold results , for each fold
213	one hot encoding
168	compute exp loss
1101	Training Final Model
716	First basic Random Forest Model
1738	Loading and overview
531	BEST TEST AUC
1519	Number of repetitions for each class
777	We can also plot the lower triangle
503	scale pixel values to grayscale
968	Remove Unused Features
1131	PyTorch dataset loader
1685	Loading the data
799	Lets train our model with early stopping
772	To see this at a simple way
130	Look at how data generator augment works
1342	Calculates the ratios of our messages
404	initialize best and last vectors
832	Generating Submission File
634	From now on , we have to predict again
1136	Create test data
1253	Train the model and save the best epoch
1087	convert coverage to class
480	convert text to sequence of words
1032	remove variables that are not in parent var
160	Here we select some random labels to display
334	Looking dimensions of data
984	Bureau Correlation
31	add some new features
435	i found the attached video below best for understanding
840	Manhattan Distance by Fare Amount
1382	removing common words
643	from tqdm import tqdm
1126	Load image and mask
718	Load serialized objects
198	display threshold image
1717	LOAD PROCESSED TRAINING DATA FROM DISK
981	replace day outliers with the values in the previous dataset
146	Number of different values
220	Correlations of the upper triangle
285	Just labels to identify the issue
57	Seeding everything for reproducible results
1733	Loading the Data
151	How many users download the app
1435	take a look of .dcm files
735	Classify an image with different models
609	Save images to disk
205	Libraries and Configurations
456	Import the necessary libraries
950	iterate over all hyperparameters
1492	CreateValidationRecords and Submit
586	Number of bedroom Count Vs Log Error
385	check test files
740	add ratio to res
1522	FIND ORIGIN PIXEL VALUES
1092	Applying CRF seems to have smoothed the model output
335	Looking dimensions of data
1540	Lets check what our model looks like
1584	Get a list of only one value columns
501	show result image
1699	convert sample to numpy array
428	all other columns
1447	Create submission file
964	Features and Target Features
149	We can look at the Quantiles
812	Write column names
1370	Label encode categorical variables
734	Classify image and return top matches
366	Linear SVR model
828	Load image and convert to a grayscale image
202	Determine current pixel spacing
1282	get train and test data
1198	A couple of the new features have been removed from
585	There are a lot of stories ..
227	Scatter plot of full train set
469	Data processing , metrics and modeling
114	Merge Datasets
1329	Encodes a block to a string
388	Helper function for histogram processing
1721	LOAD DATASET FROM DISK
982	Converting dates into timedelta object
1060	split train set to validation set
82	Making the submission
494	Distribution of Amount
323	Wrap Everything Up
1221	find best score
1286	Test the model for a few images
963	create feature matrix specifiers
1783	Word cloud of First Topic
128	Prepare Traning Data
548	Training and Prediction
252	Decision Tree Model
492	find top correlated features
1209	Save model directory
1514	size and spacing
596	set bird to
2	Add new Features
1265	Create Data Generators
1352	Leak Data loading and concat
396	calculate the confusion matrix
78	save dictionary as csv file
977	Convert dataset to numpy array
1116	Returns the counts of each type of rating that a rater made
1589	to truncate it
1697	Helper function to convert list of images to list
1105	load mapping dictionaries
619	find the areas of each contour
1264	Load and predict
277	reorder the input data
1425	total number of tokens
1216	creating a function to aggregate game time stats per customer
1499	Check if checkpoint exists and restore
75	create train and validation generators
690	Iterate over the rows
118	Pulmonary Condition Progression
1742	SCALE target variable
1766	cross validation and metrics
582	Test across small batches
1414	Returns a sorted list of the same class counts
1471	Order does not matter since we will be shuffling the data anyway
748	Load the model
823	Custom Cutout augmentation
1501	Detect hardware , return appropriate distribution strategy
978	There might be a more efficient method to accomplish this
1801	k is camera instrinsic matrix
271	Load modules and libraries
655	Check Missing Values
1602	Moving Average Features
1434	unk is an important parameter in this function
1427	Set values for various parameters
1713	Build the model
1583	coluns with new features
309	create an embedding matrix
350	We can now make a linear model
1124	Get predictions for each fold
1061	Create list of DownConvvs
1319	Read the data
35	get embeddings from train text
914	Join the aggregated data with the original data
1027	get scores from best model
397	Random Forest Classification
1293	Load dataset info
666	We can now merge the data
1793	Web Traffic Months cross days
1028	Clean up memory
928	Randomly choose a subsample
923	Cumulative importance
158	The function to convert images to grayscale
826	Load image and convert to a grayscale image
569	Hours of Day
561	Number of unique values
1067	split training and validation data
1773	for numerical stability in the loss
1643	Count of clicks and downloads by device
165	Here is the unclustered image
1	Resize image to desired size
1003	create a matrix of features
364	normalizing the data
1756	set application tracking features
1757	set the same applications with different balance values
934	Fitting and Tuning
1752	Sentiment Engineering
79	Resize image to random state
1422	deep copy the loaded sentences
328	Read the DICOM files
505	Exploratory Data Analysis
1309	run function
1271	check init pairs
1537	A lot of breakdown topics are
137	Clear output and output
249	Accuracy Model
1490	Read candidates with real multiple processes
667	We can see the distribution of log1p_Demanda unielectil sum
265	We scale the train and test data for visualization
871	Run the objective
1089	Resize predictions to expected size
286	Loading the data
204	Remove other air pockets insided body
522	Get the model losers
120	Get the vocab from a pandas series
268	Split the dataset
848	Create random Forest Object using the mentioned parameters
1185	the next batch
1183	in each round of Kaggle
433	Ignore the warnings
1629	Get the best stats
954	set the label
1487	Check if checkpoint exists and restore
1160	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
550	create different transforms
845	Function to replace NaN values with
44	Normalize for Size
1184	set the variables to zeros
1482	KAGGLE if it is not there already
998	normalizing the numeric values
1475	MAKE MIXUP IMAGE
694	remove activation layer and compiler
1520	LIST DESTINATION PIXEL INDICES
1790	import statsmodels.tsa.tsa
172	Save the json output of the model
407	Run the map
1546	Add new features
298	Read and resize image
1474	Cutmix iteration
66	load and shuffle filenames
438	We will look at the dimensions of our data
1462	Create dataset and create submission
65	save pneonumiaiaiaiaiaiaia data in dictionary
1273	this is an important parameter in this function
1340	Only taking data with signal to noise
1779	Generate the Mask for EAP
32	Identity Hate Feature
594	Subset text features
324	import pickle file
1577	Get continuous features
1570	Copy DICOM files to convert folder
712	ADD PSEUDO TO DATAFRAME
1450	create train and test datasets
1479	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1609	Preparation for XGBoost
1768	shuffling the data
133	Split train and eval data
1261	Create test generator
1289	Initialize the model with the input shape
418	Preview of Building and Weather Data
527	BEST TEST AUC
1284	shape of train and test sets
797	convert to numpy array
1814	Copy predictions to submission file
1013	Clean up memory
1132	Save the prediction and the mask
343	create a generator that iterate over the data
987	Previous Loan Amount
167	Only the classes that are true for each sample will be filled in
640	Now , we can take a look at the common words
1115	Check if columns between two DFs are the same
17	Now extract the data from the new transactions
554	Add RUC metric to monitor NN
1673	Add box if opacity is present
331	Read the DICOM files
1666	Specify parameters and begin training
681	Creating a .csv file
113	Merge Store Lags
1235	draw image on the image boundaries
1740	Distribution of DBNOs
810	find best score
332	Read input DICOM files
725	Create and fit model
477	Bad results overall for TfidfVectorizer
599	Load an image
1428	add PAD to each sequence
1494	create test records
1420	from apex import amp
1088	Remove padding from images
682	Creating a unicode for the labels
1058	Loading and checking the state
116	Build the model
1118	Manually adjusted coefficients
713	Predict on label data
463	Data processing , metrics and modeling
995	most common client type
642	Now we can take a look at the common words
395	Matrix of Confusion Matrix
1669	gather input and output parts of the pattern
1294	warm up model
1385	suppose all instances are not crowd
1493	Show results in JSON format
1538	actual is same as prediction
1302	Load model into the TFA
1214	Order does not matter since we will be shuffling the data anyway
1112	extract different column types
1002	Evaluating Feature Set
685	Creating a unicode for the labels
1064	Generate data for the BERT model
672	Create list of models
786	Most Important Features
374	Avoid division by zero by setting zero values tiny
1349	Leak Data loading and concat
1287	Display a few images
936	Building a model with random parameters
498	read header and get dimensions
48	Normalize for dimension reduction
1810	Precision and Recall
1363	We need to change the data type
371	Extra Tree Regressor
25	Loading train and test data
1083	Load data from csv files
1200	Print Best Score
1224	select proper model parameters
147	Number of click by IP
1021	first scan for a column
543	BEST TEST AUC
321	Extra Tree Regressor
1305	Score of toxic vs
370	We can tune the number of models by providing additional parameters
670	Create a function to transpose the data
1585	fill all na as
559	Create a dictionary with the values in order
946	altair is very nice plotting library by the way
95	k is the index of the array in S
1473	MAKE CUTMIX LABEL
788	Plot the cumulative importance
687	Get image and label
1656	written by Nanashi
1593	fill up with missing values
1004	create a matrix of features
1311	Display current run and time used
937	This is the output file
1484	Creating examples with the same dataset
58	you can play around
1171	Save images to disk
436	Preview of Data
1359	Fast data loading
377	Root Mean Squared Error
297	Prepare Training and Validation
11	Compute the STA and the LTA
439	Let us now look at the intersections
173	create a dictionary of all calsses values
614	Weight of the class
1708	Importing the basic libraries
1005	We can see there are other features which are not
201	We can render the actual image using neato
1330	Encodes a list of BlockArgs to a list of strings
1642	ip count in unique values
615	An optimizer for rounding thresholds
421	Distribution of meter reading
275	Prepare the Dropout
1416	Detect hardware , return appropriate distribution strategy
970	Join the datasets with the original data
1288	Load dataset info
862	Standard deviation of best score
353	create folds
1536	Check number of records and any null analysis
630	MAKE CUTMIX
1066	First dense layer
1613	Split train dataset into development and valid based on time
1159	LIST DESTINATION PIXEL INDICES
475	vectorizing the tweet base texts
778	Select columns with correlations above
1195	Make a submission
589	Number of stories
235	Clean Id columns and keep ForecastId as index
1203	Reading Fake Data
99	intialize the model
1658	Tokenizing the selected text
1820	Expandes first and last nulls
517	Convert seeds to integers
633	Running Models
1426	lowercased words
269	Merge Dense Players
894	Light GBM Results
808	Setting up training and validation sets
1586	Remove unwanted columns
579	Calculate logmel spectrogram
1165	CreateMel features
447	Encoding the Regions
1452	Preparing the train set
1277	We can reset and plot the identified objects
870	Write column names
678	get rid of outliers
654	Read test data
1076	Average length of comment
440	Top Most commmon Paths
211	label encode categoricals
1532	Create a Random Forest model
636	Load model into memory
1606	We will need these functions later on
239	Filter Italy , run the CRF
1210	so we have a look at the image
953	More To Come
566	Keras is for feature extraction
1230	Load model into the TFA
1431	Save the data file
465	Now we split the dataset into train and validation
1628	We can check in the confirmed cases
1308	run function
1381	split the dataset in train and test set
1220	Remove nuisance from X
1645	We can see the correlation between different light curves
1715	Ensure determinism in the results
1483	This is a very performative way to compute
1077	Function to remove stopwords and remove stopwords
143	Compile and fit model
1228	Load Train , Validation and Test data
260	Convert to integer
101	load the image file using cv
899	one hot encoding
1322	Read the data
1348	Fast data loading
1334	Expansion and Depthwise Convolution
1510	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1103	Loading the data
1737	The competition data
752	Import and view data
1506	FIND ORIGIN PIXEL VALUES
988	Now lets plot on our data
697	Precision helper function
1819	Join market and news
604	Load raw data
1312	we need to predict for each fold
1767	Tokenize the sentences
110	Exploring sales per year
290	Clean base dir
372	Voting model
1663	kick off the animation
691	Computes gradient of the Lovas extension w.r.t sorted errors
993	get interesting features
765	Visualize Distributions
1739	Distribution of winPlacePerc
692	Using previous model
798	Split up to train and validation sets
521	Model with winners
722	Sort ordinal feature values
618	find the areas of each contour
530	Training and Prediction
600	We will look at the masks
1234	Load model into the TPU
1449	function to create the dataset
723	Sort ordinal features
166	Analyze single images
1151	initialize train dataset
1653	plotting a pie chart
1813	summarize history for loss
1561	Macro Macro Features
1297	Predict on Test Data
1641	ip count in unique values
635	the fitting model
490	Distribution of values in application train set
452	Importing the necessary libraries
952	Get data ready for modeling
1391	draw bounding boxes on the image
92	Code from Whale Classification
333	Read the DICOM files
471	Evaluate ROC Curve
1791	Create year and month columns
1430	make train features
902	Lets train our model with early stopping
1795	Fit two models
744	Convert image id to filepath
246	Set the dataframe where we update the dataframe
356	Load an image
1807	Lets read all the files
1405	rolling mean per store id
1045	We will now merge the data back into main dataframe
706	MODEL AND PREDICT WITH QDA
837	Handle legends
679	Splitting the labels
536	create different transforms
886	Loading raw data
109	plot rolling statistics
391	convert to float
488	Extract Feature Types
866	choice with hp.choice
1360	Leak Data loading and concat
1749	creating an entity from dataframe of installments
1671	Loading the data
389	Check for empty images
1037	Returns the size of a dataframe
942	sort by score
352	Load the data
825	Set toout position and size to resulting image
181	Prices of the first level of categories
1590	Load the data
272	configurations and main hyperparammeters
499	handle .ahi files
628	Groupping by day
1664	Download the pystacknet repo
354	Check if it all works
1778	Loading dataset and basic visualization
1080	Divide the result by the number of words
956	Bureau balance , index
1675	draw patient image
1667	Get the predictions
851	Add elapsed time in seconds
800	Light curve plot
1630	Each Province has several uniques
661	get different test sets and process each
623	Separating the country variable
1555	Create LightGBM model and train
711	ONLY TRAIN WITH DATA WHEEZY EQUALS I
529	Run Grid Search Cross Validation
1306	Retrieving the Data
1761	Label encode categorical features
437	Preview of Data
1232	Load Train , Validation and Test data
720	Importing the Libraries
940	sort by score
834	Set up the evaluation functions
1543	Checking the SMAPE score
903	get scores from best model
399	calculate the confusion matrix
1572	Distribution of data size
135	sklearn is for feature selection
1298	create labels for submission
1703	find the best candidates
1443	Squares of Full Dataset
1138	SHAP interaction values
1295	Load train data
247	Process data to prepare for submission
924	Plot the importance data
55	Look at the number of clusters
185	Number of products with a price of
576	Set global parameters
974	iterate over all hyperparameters
1109	Unique IDs from train and test
568	Loading and Feature Selection
1549	Merge Weather Data
87	fake data sampling
756	Deaths by Poverty level
1084	Lets read in the image
1389	Creating Stratified Validation
1279	Identify by both objects
1065	Model Hyper Parameters
382	Exploratory Data Analysis
1225	Make a picture format from flat vector
1775	This enables operations which are only applied during training like dropout
1627	We can do some basic stats
276	sort the validation data
1686	Get pixel values for a pixel value
237	Run the CRF
1743	Prepare Training and Validation Sets
1133	Stacking the val predictions and masks
21	Check for missing values in training set
1646	This was copied from it is MIT licensed
3	Reset Index for Fast Update
776	Create the pairgrid
295	Binary Target Features
719	save preprocessed outputs
70	add trailing channel dimension
250	Linear SVR model
989	Split installments to create features
216	MinMax scale all columns
861	Split into training and testing
1620	checking missing data
760	Histogram of Value Counts
1531	Previous applications features
708	ADD PSEUDO TO DATAFRAME
481	Tokenization of the text
1073	Distribution of Target in the application
40	Loading the dataset
207	LIST DESTINATION PIXEL INDICES
985	Example of output from bureau balance
551	Run Grid Search Cross Validation
485	Now , we design the network
1291	Squeeze and Excitation block
1818	Function to check an index
162	find two cell indices
339	Load weights and make predictions
1466	Evaluation and Inference
1155	Create strategy from tpu
1052	Lets train our model with early stopping
980	Some other application types
537	Run Grid Search Cross Validation
605	Pads to make the experiment more reproducible
45	Distribution of the Target Column
680	Creating Labels for Modeling
739	Get the ratio of lasso to the best
1237	Augmentations and Submission
1172	Read in the image
5	Encode Categorical Data
1571	load best model
322	Voting model
1283	Get train and test indices
869	create a file and open a connection
541	Run Grid Search Cross Validation
393	What is it all about
645	try random number for reproducability
728	Read in the labels
1678	convert text into datetime
807	Convert to numpy array
1390	Load dataset and create image and mask
789	Ignore the warnings
1051	Create a LightGBM model
34	Loading Train and Test dataset
770	Bonus Variable
53	Distribution of the Target Column
1714	cross validation and metrics
117	Preparing the submission data
1260	Combine the filename column with the variable column
1072	retrieve the pixel data corresponding to this point
650	Setting up some basic model specs
593	Convert categorical features to continuous features
1358	iterate through all the columns of a dataframe and modify the data type
473	Loading the data
518	Make a submission file
780	Range of X values
400	Convert images to numpy array
1281	Libraries and Configurations
1142	Calculating Shap importances
1463	CNN for multiclass classification
508	Here is the custom function that we will use to evaluate
115	Build the model
510	process remaining batch
19	Imputations and Data Transformation
406	Excut the variables
1400	transform series to binary
1213	Create strategy from tpu
387	Load item data
818	Add the rest to the dataframe
901	Define LightGBM parameters
583	sum over all models in batch
629	Groupping by day
1476	MAKE CUTMIX LABEL
431	Separating the target column
649	Create my own word embedding
1808	Now lets look at the data
938	Write column names
1809	Fill NaNs with
126	Prepare Data for Modeling
208	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1541	Printing the SMAPE score
4	Remove Unused Columns
262	Show parameters and LB score
98	This is just a simple function that compares two sets
863	Fitting and predicting
299	Read and resize image
971	Get score from random search
1274	check if all the colors are the same
1574	Looking at the data
419	Glimpse of Data
1259	Using original generator
1086	Check that the training set is ready
880	Reading and preparing data
1156	watch out for overfitting
1337	Update block input and output filters based on depth multiplier
795	delete hyperparameters from hyp
755	Preparing the data
1440	Prepare Training Data
1244	Load the data
854	Investigation of Fare amount
1338	The first block needs one extra dimension
403	We can train a CatBoostRegressor
36	Logging the target variable
1128	calculate coverage per fold
88	Creating and checking the models
1608	Create out of fold feature
278	word cloud function
677	filtering out out out out out out out outliers
791	Train and predict
1634	Diff for single label
60	Submit to Kaggle
1794	Train all models
1815	Load data from Lyft dataset
703	STRATIFIED K FOLD
1007	Exploring VM features
816	BERT Some Wrong Prediction
743	Pivot to convert old dataframe to new format
1581	update new columns names
373	Compute the STA and the LTA
1523	oversampled training dataset
1042	Sort the table by percentage of missing descending
203	For every slice we determine the largest solid structure
802	Applying to predictions
1008	Find the correlated pairs
472	Precision and Recall
533	Run Grid Search Cross Validation
1525	Span logits minus the cls logits seems to be close to the best
669	Load Data from CSV files
434	Exploratory Data Analysis
1024	just an error check here
1626	Run Length Encodance
1152	create validation set
376	MAPE error
1215	Only load those columns in order to save space
363	Getting Training Data
304	Set class weights
349	highlight if value is less than threshold
457	Loading the Data
804	Get best settings
1588	Evaluate for one epoch
507	separate train and test sets based on dates
1276	get the inputs and outputs
1304	Delete to reduce memory usage
1041	Reducing the memory usage
1241	Loading an image
771	calculate households per capita
1562	Get train and test data
1396	Extracting date information
773	Most correlated variables
155	The download rate evolution over the day
1341	Measured vs
1802	CALCULATE NEW FEATURES
1470	scale and scale features
1800	plot baseline and predictions
148	Number of click by IP
1243	to truncate it
966	Display distribution of feature by Target Value
624	Groping Data for Ialy
228	Finalize all ensembles
1597	checking missing data
1502	Order does not matter since we will be shuffling the data anyway
724	Treating NaN values with Simple Imputer
1804	Plotting the ROC Curve
302	Read and resize image
1174	Setup the colors
1137	Create Years
178	Get the price of each category
140	Make PyTorch deterministic
790	rerun model on full train set
392	Resize images to desired size
1108	Load image file
1398	process time series
839	Handle legends
1255	Load Model into TPU
1534	This is exactly what our code looks like
1402	get data with first n samples
563	Descriptive Text
1149	Load preprocessed functions
284	Just labels to identify the issue
1612	Split train dataset into development and valid based on time
1374	Prepare Data for Modeling
683	Creating a unicode for the labels
532	create different transforms
879	iterate over parameters
1734	Now we fill in the values
907	Get the size of a dataframe
97	not needed but somewhat easier to visualise
1647	create tqdm notebook
699	ONLY TRAIN WITH DATA WHEEZY EQUALS I
308	Padded examples
144	get data fields ready for stacking
1560	Correlation with Macro
627	GIven to Spain Data
1692	lifting function to help with visualizations
577	Get a sample
1100	Get the average fold AUC
1565	Import modules and data
590	Gaussian target noise
1596	The first few rows are having null values
1446	Order does not matter since we will be shuffling the data anyway
1505	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1114	Remove missing target column from test
1681	the accurace is the all time wins divided by the all time attempts
1744	FITTING THE MODEL
104	Compiling the model
102	grid mask augmentation
342	Save predictions to file
1556	Set values for various parameters
733	Read and preprocess an image
222	We can now make a linear model
1707	Run program and report progress
1399	take mean and standard deviation
1091	A function to create a submission
112	Merge the data
215	XGBOOST Sparse Feature Storage
383	Read the data
127	Get the dimensions of categorical features
253	create training and validation dataloader
455	Draw bounding boxes on the image
612	Listen to the test data
7	declare target , categorical and numeric columns
663	Creating a date aggerange
1119	Distribution inspection of original target and predicted train
1680	the time spent in the app so far
179	now to get prices by category
625	We can now find the data about each day
157	Print final result
189	The coms length
1248	get comp data
242	Filter Albania , run CRF
1157	numpy and matplotlib defaults
1774	Shuffling happens when splitting for kfolds
736	Get number of zero features
451	Only the classes that are true for each sample will be filled in
313	so we label encode categoricals
1202	Load Model into TPU
1451	inverse transform from X to Y
1350	iterate through all the columns of a dataframe and modify the data type
1512	size and spacing
1445	we keep only the modified values
100	code takesn from
1170	We need the same columns to process the data
1457	checking missing data
632	Load the population
819	Add the results to the dataframe
658	from RandomForestRegressor import RandomForestRegressor
1154	Diff two datasets
580	Logmel feature extractor
1093	We can tune the Salt parameters
28	MODEL WITH SUPPORT VECTOR
947	SETUP SOME COLUMNS
1607	one hot encode
1270	remove duplicate images
1023	one hot encoding
1238	Start tensorflow session and run
763	Set the annotate function
1333	Squeeze and Excitation layer , if desired
1102	Creating Submission Files
491	Pearson Correlation Heatmap
1129	We will need these functions later
1772	always call this before training for deterministic results
1763	Extract missing values
1730	Add leak to test
1179	calculate the confusion matrix
932	Evaluating the model
184	Brand name price
238	Filter Italy , run the CRF
326	Initialize patient entry into parsed
1068	Print CV scores
232	Double check that there are no informed Confirmed Confirmed Confirmed Confirmance after
891	Drop unwanted columns
931	count number of combinations
229	Finalize all ensembles
1393	CATEGORICAL COLUMNS
194	display threshold image
674	set up training and validation sets
784	Now lets check what our model says
1618	average results from different folds
1127	Initialize the data
1567	Pinball loss
1268	Linear Weighted Kappa
1527	Computation for raw logits
1275	identifying object with given color
72	define iou or jaccard loss function
1321	set up gpu
274	Setting up some basic model specs
764	create a list of bounding box coordinates
240	Filter Germany , run CRF
136	Save model to file
1247	to truncate it
1175	Add a cylinder actor
1817	Lets take a look at the data
1339	Final linear layer
997	Seed features and entity sets
1595	Pad the sentences
555	Defining the data types
1633	Exploratory Data Analysis
337	Train the model
124	We can see that group by columns are all zeros
512	Extract the columns of the season
444	Latitude and Longitude
142	get the data fields ready for stacking
1461	Make a Baseline model
1122	Create out of fold feature
933	sort by score
91	Augmentations and Fatalities
291	Load Test Data
459	Start time search
1771	text version for squash
94	Creating a mask for the null values
443	Latitude and Longitude
831	Now our data sample size is same as target sample size
556	create lgbm feature dict
1429	make train features
973	Get scores for random and option sets
616	Add in data
721	The nominal variables
123	Process text for RNNs
282	Data extraction from train labels
1497	Get pretrained model
398	Matrix of Confusion Matrix
415	i found the attached video below best for understanding
81	Now lets take a look at how it performs
1417	Load Model into TPU
1344	Show some results
549	BEST TEST AUC
1347	iterate through all the columns of a dataframe and modify the data type
595	CreatePUModel and train
668	Load and Explore Data
280	cluster plot
1161	FIND ORIGIN PIXEL VALUES
811	Create a file and open a connection
405	calculate best position
693	Using previous model
192	Show original image
20	Imputations and Data Transformation
1187	getting the last batch as a batch
1472	size and spacing
762	create a scatter plot
1328	Gets a block through a string notation of arguments
422	Distribution of meter reading
236	Filter Spain
1498	check and create list of decay variables
1649	Calculate extra features
1787	Separating the error from the rest
927	Evaluate AUC
803	Confidence by Target
346	Continuous , Submission , etc
652	Importing the necessary libraries
686	Randomly train the image generator
368	Decision Tree Model
1040	Merge with previous counts
336	define training and validation sets
905	to update the metrics dataframe
1222	build list of parameters
841	Euclidean Distance by Fare Amount
1094	Get the dimensions of the image
132	Create Testing Generator
270	LightGBM modeling
449	Setting up Training and Validation datasets
864	Fitting and predicting
546	create different transforms
1454	We need to inverse transform the yhat
470	Merge datasets into full dataset
1107	Load sentiment file
1755	Bureau Balance
1702	Evaluate all candidates
883	Evaluating Bayes
571	Hour of Day
1290	Squeeze and Excitation block
539	BEST TEST AUC
355	Check for Duplicates
1631	Load full table
1411	Create the layout
1481	Distribution of cod Provants
1777	create the heatmap
108	Sales volume per state
578	Calculate spectrogram using pytorch
916	Merge with Bureau info
1694	To be sure that some images have the same color scheme
1053	get scores from best model
273	get lead and lags features
62	Exploring categorical variables
598	Get a list of images without ship information
852	What is it all about
13	Load train and test data
51	Create columns for each image
783	Import required libraries
1582	process new columns
1668	sale per store
1803	The function is to convert the input image to world coordinates
1373	We can test this model using all three methods
1444	Squares of Full Dataset
1146	load mapping dictionaries
261	Show parameters and LB score
1249	parse trials and create submission
141	Number of Fraud vs
1046	Joining with aggregated features
1036	Exploring VM features
1226	Plotting some random images to check how cleaning works
245	Filter Andorra , run
259	Convert to integer
478	It was the best of times
487	Exploratory Data Analysis
1074	Distribution of income buckets
85	Makes Fake Data
1256	create train , test and train folders
809	Lets train our model with early stopping
969	The final training and testing sets
873	Write column names
1233	Build datasets objects
1812	get relevant features
1511	Augmentation layer
1720	SAVE DATASET TO DISK
1054	Clean up memory
197	Blackhat image processing
567	A simple Keras callback for this competition
1706	pick a candidate for the best candidates
1636	Brand new features
1672	Initialize patient entry into parsed
52	Normalize for Size
1780	The wordcloud of Edgar Allen Poe
962	create feature matrix specifiers
1207	LOAD DATASET FROM DISK
476	vectorize the sentences
1062	This is the main prediction steps , i.e
1785	Avoid division by zero by setting zero values tiny
219	Loading the Data
1695	Shows example of a sample
1762	The missing data
885	More To Come
951	Reading and preparing data
1424	task to clean lower case words
493	Data preparation before merge
945	iteration score 两列
761	There are other variables which are missing
1550	we can now merge the data
402	Load values in various formats
1575	Reading the datasets
1654	Correlation with Pearman
1038	Previous aggregation features
855	separate train and validation sets
234	Filter selected features
83	Loading and Cleaning Data
1403	we need to predict during training
919	Balance , Correlation , and more
466	Evaluate ROC Curve
1579	colunms new features
1178	What is Dicom images
426	Distribution after log tranformation
1029	to update the metrics dataframe
1250	save best model
1336	Skip connection and drop connect
564	get item description as an integer
1085	Resize image and mask if needed
1491	Returns a dictionary containing summary counts for each example
288	from sklearn.metrics import auc
300	The same for both training and validation sets
601	And there you have it
38	Get the train data
1465	Define dataset and model
1553	Calculate the average day week for the trip duration
1601	checking missing data
1299	Some functions to embeddings
1679	get some sessions information
177	Most common labels
926	Set up scoring
1057	if we have a mask we can simply return the cropped area
702	ADD PSEUDO TO PCA
71	create numpy batch
1439	Loading and overview
1727	Shuffling happens when splitting for kfolds
1044	removed missing columns
1797	Plot rolling statistics
991	Load data from dataframe
830	Lets take a look at the surface
1731	Creating a video
535	BEST TEST AUC
613	Libraries and Data Loading
898	Get a list of zero importance features
865	Initiate the hyperparameters
1362	Preprocess date column
1394	With Masks
960	Total Features
1364	Address change function
129	Check sample images
1095	Make predictions on validation set
379	Root Mean Squared error
1176	Save images to disk
729	Distribution of variation
500	Separate the zone id into a df
361	Image data processing
996	seed features and entity sets
190	Distribution of the description length
1415	Number of labels for each instance
983	Credit Data
318	Decision Tree Model
917	Load previous data
1624	Exploratory Data Analysis
1709	Importing sklearn libraries
1410	Set up trainments
1693	lifted function definition
1485	Load train data
408	create the wordcloud with the most frequent words
1326	Round number of filters based on depth multiplier
1578	replace NaNs with
1552	Distribution of trip duration
423	Monthly readings
1280	Check the ARC model
1150	Augmentations with augmentations
1079	add the vector of each word to the model
294	Submit to Kaggle
1197	We can see there are some missing values
1542	Extracting and Transform
1635	Split Train Test
293	Get the extracted id
0	if this is a train image
461	Start time search
1784	Compute the STA and the LTA
320	We can tune the number of models by providing additional parameters
1143	Creating new features
591	Combinations with augmentations
1240	Read sample submission
1442	prepare submission files
1729	Add train leak
1423	function to clean the sentence
824	Applies the augmentations on the given image
1397	Distribution of var
496	get data for each target
1788	peak frequency
33	prophet expects the folllwing label names
622	Examples for usage
1644	Click Rnd
1025	Create a LightGBM model
180	zoom to the category
1069	Write the prediction to file for submission
1252	We will need these functions later
1798	shift train predictions for plotting
1173	convert to HU
345	Continuous , Submission , etc
43	Create columns for feature engineering
1786	Loading the data
264	Prepare Training Data
26	Show target distribution
1384	convert to numpy array
467	Precision and Recall
731	Samples of full hits table
257	linreg on train and test
1705	Returns the program with the best candidates
210	check columns with numerics
1251	Split the category variable
523	Add Confidence
111	Merge the data
258	Building and Predicting
310	Preparing the Data
1323	Parameters for an individual model block
827	Load image and convert to a grayscale image
122	We need to clean the special chars
1517	raw training dataset
1242	Run the run function on the session
562	Create out of fold feature
217	MinMax scale all feature
296	Exploratory Data Analysis
188	Generating and selecting a wordcloud image
1113	Subset text features
746	replace the last series id with the number
301	Read and resize image
41	Check for Null values
254	We can tune the number of models by providing additional parameters
1148	Build Model and create submission
105	Lets check the datasets
1412	Number of unique classes
943	And fit model on test data
662	Creating a date aggerange
131	Prepare Testing Data
156	Importing the data
955	EDA with dataframe
775	plot the heatmap
63	Distribution of continuous variables
442	Latitude and Longitude
292	Load and predict
1239	Run the session
698	Applying CRF seems to have smoothed the model output
305	Load and evaluate model
552	Training and Prediction
86	Check the number of fake train and validation samples
1009	drop variables that are not features
565	Predict and Importance
1640	Load data and describe features
495	Merging metadata and training data
1464	Load Augmentations from Test dataset
1674	Add boxes with random color if present
1615	show mask class example
1421	to lower case the sentences
581	ceil and divide by the number of steps
329	Read input DICOM files
608	Loads a given filename from a given location
1632	Keep only useful columns
1117	Compute QWK based on OOF train predictions
1604	Remove Nulls
515	Calculate the distribution of confferences
833	Distribution of Fare
145	Importing the data
631	complex equation , see
1404	we need to predict first day
281	Looking file sizes
1433	visualize the heatmap
430	Encode Categorical Data
1186	update this histogram for each session type
76	load and shuffle filenames
1448	Split dataset into train and test
1365	Download the Shapefile dataset
1141	We can see the dependence plot
1687	Return a list of pixels that are in the image
1180	assign new confmtx
154	We can see a lot of data is missing
1799	shift test predictions for plotting
1020	list to remove columns
967	There might be a more efficient method to accomplish this
671	Joining all the features together
161	make a mask where labels are too small
941	Add the evaluated results to this array
347	Loading the Data
732	Read and preprocess an image
1104	Credit card balance
688	draw box over image
340	df to hold predictions
61	Male , Female , Nuttered
341	change column names
1722	The mean of the two is used as the final embedding matrix
925	Fitting and predicting
1059	Convert inputs to output image
1135	Load the timestamps
1106	Load metadata file
1621	What is Fake News
251	SGD regressor
1735	set color palette
1723	missing entries in the embedding are set using np.random.normal
1317	set up gpu
1153	Let us look at the data
1267	Here we take the same steps to process the test data
1218	We can get stats like world , game time , etc
1488	Eval data available for a single example
860	Training and Evaluating the model
747	Unfreeze backbone layers
1096	We need to convert the test predictions into a submission
134	Initializing a CatBoostClassifier
175	Load image data
1030	There might be a more efficient method to accomplish this
1684	Loading the data
1006	Remove low information features
1436	Number of Patients and Images in Training Data
174	The following was copied from
1034	unique values in each column
1000	Custom Feature Features
358	skin like mask
454	Run a binary image
1192	need to fill in zero values
856	Get the available features
482	Build the model
233	Create date features
1245	Convert annotation string to integer
1010	Add the column name
468	Loading the data
311	Separating the target variables
1600	Diverging palette
365	Accuracy Model
1610	Categorization with log transformation
587	Number of bathroom Count
958	We can see the same for other types
881	Get data ready for modeling
1401	Extracting sequences from series
821	RandomForestClassifier with Graphviz
1526	Default empty prediction
1189	Start generate data sets
417	Preview of Building and Weather Data
464	Merge datasets into full dataset
1169	Check dataframe shape
1458	checking missing data
949	Plot the distribution of parameters
1033	Selecting the direct parents
644	Perfect submission file
1548	Merge Weather Data
558	If we have a list of parameters , we will use them later
1811	and plot the actual and predicted
767	find heads with high correlation rate
1728	This enables operations which are only applied during training like dropout
1120	Rename column names
726	Lets take a look at the photos
1377	Augmentations and Keypoint Detection
1332	Depthwise convolution phase
1599	checking missing data
69	add trailing channel dimension
1387	We need to apply transforms to sample
892	List of columns with more than
715	Remove outliers
584	Apply max to all probs
1806	Load Train and Test Data
1227	Load the data
664	Create a date aggregation for
939	Create dataframe with best score
54	Understanding the schedule
745	build a dict to convert surface names into numbers
171	BUILD MODEL NEW TOP
1745	Find primes from eratosthenes
176	Check some random images
911	Convert categorical variables to integer codes
420	Distribution of meter type
704	MODEL AND PREDICT WITH QDA
1759	create feature matrix
68	if augment then horizontal flip half the time
684	Creating a unicode for the labels
1661	Read in the .sol file
1625	New Table Features
1551	month over day
221	highlight if value is less than threshold
896	There might be a more efficient method to accomplish this
218	MinMax scale all feature
152	How many downloaders clickers download the app
1134	Stacking the validation data
817	Applying the method on the selected data
9	fill test weather data
195	inpaint with original image
47	Create features
999	Returns the longest repeating elements
106	Lets check the datasets
50	Distribution of the Frequency
283	Exploratory Data Analysis
648	Load Train and Test data
1533	This is what our mean looks like
1500	Making the Submission
351	RandomForestRegressor with selected parameters
676	append the data to the list
1568	Pinball loss
24	Detect and Correct Outliers
657	get the columns names
1356	meter split based
1269	the same color pair count
243	Filter Albania , run
125	from base data
49	Create a Quadrogram
524	create different transforms
1637	documcount function for feature generation
1090	Encodes predicted to a submission file
1386	Scaling is necessary for images which are very thin
610	Loads a given filename from a given location
1121	Extract features from train and test dataset
384	Load raw train data
1769	SAVE DATASET TO DISK
1266	Reading Testing and Submission
1617	remove layter activation layer
1677	A couple of more patients and effort
986	Create date features
575	Correlations between bedrooms and bathrooms
547	Run Grid Search Cross Validation
573	Number of bathrooms
424	Distribution of meter reading
462	Loading the data
850	Modified to add option to use datetime
453	draw image on the image boundaries
992	Relationship the previous recordings
306	Combining all sets
814	Now further split the training test into train and validation sets
357	get different images of different types
1310	Get feature importances
754	Deaths by Poverty level
445	Extracting informations from street features
1564	Time based analysis
1508	now timing for one iteration
1659	Preprocess for Neutral tweets
1263	Using original generator
1660	Import and convert to integers
741	and plot the local deform of these points
904	Clean up memory
782	change column name
506	Lets plot some of the columns of the dataset
1343	Data analysis and make submission
1657	Sentiment analysis
1016	Bureau balance by loan
626	China Schedule
1383	split train and validation sets
846	MAE for train and validation
528	create different transforms
1764	Extract missing values
900	just an error check here
1594	Tokenize the sentences
1559	iterate through all the methods and extract the first
1748	Bureau balance data
1026	Lets train our model with early stopping
1770	missing entries in the embedding are set using np.random.normal
479	From Strings to Vectors
446	Encoding the Regions
1591	Load and Evaluate
1701	The end of the candidates list
348	Correlations of the upper triangle
314	Getting Training Data
255	Extra Tree Regressor
1478	LIST DESTINATION PIXEL INDICES
906	Cumulative variance explained with PCA
1557	Creation of the external Marker
908	remove variables that are not in parent var
689	functions to show an image
169	Building the model
138	Here we will check various PyTorch modules
410	OneVsRestClassifier with SGDClassifier
948	Boosting Type Chart
701	ONLY TRAIN WITH WHEEZY EQUALS I
1419	Load libraries and data
1368	display original map
39	Get the next batch
1528	Join examples with features and raw results
1467	realiza the same as above
1468	Max or min of a feature
592	Loading the data
1372	Making the Submission
1099	predict oof train and test
1650	Clustering for All Features
553	BEST TEST AUC
486	import the Keras libraries and packages for LSTM
223	convert to int
639	Go to actual sentiments
542	Training and Prediction
607	Return a normalized weight for the contributions of each class
1140	dependence plot
1125	custom function to calculate final value
513	Conference Tourney Games
1682	An optimizer for rounding thresholds
1418	Load modules and libraries
425	Distribution of Meter Reading
1521	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
742	We need to transform the ID into aSubtype
16	To plot pretty figures
1366	Load Districts of the dataset
835	New observations
975	iteration score 两列
1144	Growth Rate Percentage
944	iterate over all hyperparameters
42	Distribution of the Available Values
187	Descrip of Items
139	check the version of mmcv
572	Number of orders by user
1380	Separating the bounding box coordinates
307	Tokenize the text
836	Zooming nyc map
1614	Split train dataset into development and valid based on time
560	Plot Gain importances
414	Importing the necessary libraries
1258	Here we take the data and resize it
1648	This will validate on all series
730	We can now extract the time features
12	This is a very performative way to compute distance
853	Fare amount by Day of Week
1665	fill in mean and std
653	Loading the data
483	Build the model
913	Joining with parent variables
1001	Returns the most recent dates for the given date
163	RLE Encoding
1050	just an error check here
1367	distance per day
1732	Returns a list of the same size as the index count
504	A couple of more charts ..
1388	the same as above
1524	Eval data available for a single example
416	Import and Preview Data
603	Multi Label Encoding
801	Applying to predictions
441	Map Based Visualizations
460	Start time search
738	Test key drawings
858	making a scatter plot
1361	Table printing large
620	find the areas of each contour
23	Detect and Correct Outliers
813	Show label distributions
1580	colunms new features
1047	Lets check the cash data
1459	checking missing data
757	creating a bar chart
241	Filter Germany , run the CRF
362	Open image , return image and extractor
895	There are a lot of features with zero importance
80	No Penalty Version
611	Save images to disk
1486	Get pretrained model
876	iteration score 两列
1477	we will repeat this for our tests
