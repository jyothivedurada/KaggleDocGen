1601	checking missing data
1471	Order does not matter since we will be shuffling the data anyway
1795	fitting two models with different parameters
1261	Create test generator
201	Function to render neato image
1053	get score on best scores
226	Scatter plot of CNN solutions
919	Now extract the cash variables
837	Set alpha for legend
905	Create metrics dataframe
1655	Draw the heatmap using seaborn
1171	Save images to a GIF file
298	Read and resize image
296	Take Sample Images for Binary Target
316	Linear SVR model
254	Hyperparameters search for logreg with GridSearchCV
1009	Remove columns with missing values Id columns and numeric columns
658	Feature extraction by Random Forest
850	change field type to datetime64
1583	Multiply the new features by the original features
1429	Making the training features
111	Deaths and Deaths
330	Visualizing the images
1401	inverse the order
995	plot of most common client types
1172	Read the image
46	The distribution of the values is highly skewed
1645	LGBM Correlation
443	Latitude and Longitude
391	convert test image to float
280	What is the label color of the clusters
762	scatter plot of raw count
285	Just labels for the target variable
320	Hyperparameters search for logreg with GridSearchCV
804	Get subsample parameters
1695	Plot the sample
701	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
931	count number of combinations
190	Distribution of the length of the description
519	Get the training set
1191	distribution of accuracy group
1148	the size of the image
380	unpacks a field off the end of a file descriptor
1595	Pad the sentences
1525	Span logits minus the cls logits seems to be close to the best
362	Load image file
593	Transforming the categorical variables
209	FIND ORIGIN PIXEL VALUES
88	Create the models after each training fold
1088	Remove padding from images
992	Relationship the Previous Cases
845	Set invalid APEs to
1764	Copy feature matrix
72	define iou or jaccard loss function
378	Mean absolute error
217	MinMax scale all features
1720	SAVE DATASET TO DISK
1434	and predict labels if unk is True
645	try random samples
233	Create date columns
1523	Oversample the oversampled training dataset
612	Listen to the Testing Store
248	Getting the targets for Training
723	Sort ordinal data
598	Images that have no ship
511	Same as above
393	What is the distribution of the test data
1398	Thanks to with the preprocessing part
141	histogram of Fraud vs
743	Create new pivot table
543	Classificationreport and confusion matrix
1014	Running the model
369	Prepare the data for modeling
1620	checking missing data
269	Merge the Dense Players
1444	Square Error
602	Reading the Files
19	Check for missing values in training set
1755	Relationship between Bureau and Aubalance
590	The Gaussian target noise
1519	The number of repetitions for each class
324	Save the inputs and targets
243	Filter Albania , run the Linear Regression workflow
1101	Run the lightgbm model
1475	MAKE MIXUP IMAGE
745	build a dict to convert surface names into numbers
910	unique index from agg
778	drop columns with correlations above
1549	preparing data for plotting
1335	Squeeze and Excitation
517	Get the seeds as integers
1188	Calculate the average accuracy of each assessment
996	Seed feature names
266	There are some missing values in the data
1676	Lung Opacity
14	Visualization is always better
392	Resize Train Images
200	for more please refer this link
1784	Compute the STA and the LTA
491	Pearson Correlation heatmap
554	Add RUC metric to monitor NN
1625	filling the missing values with the number
1136	Which features have reduced in the number of modules that are compatible
990	Amount loaned relative to deposited
85	fake data sampling
735	Classify an image with different models
1368	Plot the map
1125	Get the output
1367	Plot number of places per day
807	array to store each feature and its score
1702	Evaluate the candidates
417	preview of data
544	Building the Tourney
730	We can see the total number of clicks
800	Light curve plot
560	Plot Gain importances
81	First linear Layer
431	Extract target variable
1459	checking missing data
423	Monthly readings
797	Convert to numpy array
1077	Convert to lower case
1323	Parameters for an individual model block
1798	shift train predictions for plotting
128	Prepare Traning Data
585	Building year , building year , and number of stories
477	vectorize the data using TfidfVectorizer
1421	Converting the comments into lower case
522	add the losers of each team to the dataframe
1428	add the PAD to each sequence
1426	Number of unique words
191	Display scatterPlot between Description length to price
1268	Linear Weighted Kappa
1749	Replace the missing values with an entity
541	Run the estimator
1224	select proper model parameters
1797	Plot rolling statistics
279	What is the point of inertia
426	Distribution of the square feet
550	Create our pipelines for later use
268	Find the columns with the Standard Deviation
561	so we can see all the columns
1402	extracting the data for the first sample
1802	This code is for classifcation model
691	Computes gradient of the Lovasz extension minus the sorted errors
198	display the threshold
915	unique position of data frame
997	First some final features
911	Convert categorical variables to integer indices
1160	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
401	obtain one batch of training images
638	Function for generating the wordcloud
375	Run the consumer
1389	Fast AI Data Block API for Regression
848	Create random Forest Object using the mentioned parameters
581	Take one batch and dump if necessary
297	EXTRACT DEVELOPTMENT TEST
959	Aggregations of the features
1560	correlation with macro features
1629	creating a list of all the stats
1462	Create dataset for training and Validation
336	Setup the hyperparameters for the model
1403	Compare the timestamps to get the expected encoding
1449	create the dataset for this look back
1603	Finding the missing values
149	Print some examples
49	Quadrate the bar chart
907	How big data is in the data
1308	run a batch of frames
1041	Reducing the memory usage
823	Custom Cutout augmentation
559	Create a copy of the dictionary respecting order
1584	Checking only the columns with only one value
1344	Visualize some samples
965	reset index and use floats as labels
1086	Check that the training set is ready
1244	Read the data
1744	FITTING THE MODEL
457	Loading the Data
122	Fianlly , we have to clean special chars
1255	Implementing Efficientnet
472	Precision curve
681	Creating a dictionary for later usage
1074	Distribution of number of items in each class
1816	Loading the data
535	Classificationreport and confusion matrix
346	Fully connected generators
750	Combinations of TTA
536	Create our pipelines for later use
227	visualization of LB score visualization of COVID
779	Get the average age from the index data
621	find contours and areas
1324	Change namedtuple defaults
1156	watch out for overfitting
405	Calculate the best position
513	Conference Tourney Games
819	Add the reduction values
1723	missing entries in the embedding are set using np.random.normal
1680	the time spent in the app so far
1700	Evaluate the evaluations
4	Remove Unused Columns
753	Import Train and Test Data
1535	Loading the required files
232	Fixing the missing values
1544	Get the data frame
844	Fit the model
480	converting a sequence of strings to indices
1719	shuffling the data
1109	Unique IDs from train and test
361	Image data processing
1376	Deep Learning Libraries
1262	code is a filename of the image file
1097	Check indices overlap
430	Encode Categorical Data
171	Now through the second convolutional layer
1130	Create the dataloaders with the previous dataset
548	Finding Best Weight
1343	The SNP for each sample is highly skewed
1047	Reading the data
1745	Here is the main prediction steps
927	the score and iteration parameters
383	Setting the Paths
1626	Distribution of probabilities for each Province
158	The original image shape
386	unpacks a field off the end of a file descriptor
673	join the country with the training set
1049	creating dummies for categorical features
789	Ignore the warnings
414	Data Visualization and Feature Engineering
592	Read the data
117	Merge the eval data with the prediction data
323	Wrap the prediction into a function
570	Days of the Week
840	Manhattan Distance by Fare Amount
228	Creation of the final ensemble
376	Mean absolute error
242	Filter Albania , run the Linear Regression workflow
985	Now let us see how it performs on our data
349	highlight if value is above threshold
61	Function to extract the gender , female , neutral , or unknown
527	Classificationreport and confusion matrix
1388	if there is one batch of bounding boxes
838	Plot the Dropoff locations
1725	The method for training is borrowed from
901	Initiate the model
447	Encoding the Regions
1093	salt parameters , for comparison
10	merge weather data
329	Read in the Images
1533	Plot the mean ROC curve
1062	Apply each of the decoder
1406	Get the seeds as integers
877	Evaluate Bayesian Results
646	For each sample we can see the average score
364	We scale the data
545	Select Percentile
1278	Now we can look at the individual objects
1273	Count the number of objects in this set
774	The scorr of each feature
1232	Load text data into memory
589	Plotting the error
1145	Curve Fit Function
163	RLE Encoding
637	so we can see all the columns
902	Train the model with early stopping
155	Plotting download rate evolution over the day
580	Logmel feature extractor
1460	checking missing data
1608	Checking Best Feature for Final Model
1022	remove columns that do not exist in the data
95	Finding the indices of the most common values
760	Histogram of the heads only
725	Predicting and Prediction
1059	Convert inputs to expected mask
385	Lets validate the test files
1178	import visualization packages
830	Surface We can see the distribution of surface
1082	A data generator
1003	Featurize feature matrix
1198	Create submission file
1483	This is a list of all the contributors
382	The number of images can be quite different
1466	Predictions on the test images
856	Get all the features
724	Simple imputer
1779	Generate the Mask for EAP
1463	CNN for multiclass classification
641	For negative sample
1452	Seting the target columns based on visit dates
1553	Average day week
1450	create look back data
1590	Read the data
164	Reading the image
157	Print final result
153	Download by click
1455	inverse transform for test data
26	Visualization is always better
7	declare the features for the model
812	Write column names
653	Read the training data
1469	splitted features into a single dataframe
1794	Train the model
246	Set the dataframe where we will update the predictions
1174	Named Color Visualization
763	Annotating chart
983	Credit Bureau
84	Class Distribution Over Entries
1054	Reducing the memory usage
224	get the best score
203	For every slice we determine the largest solid structure
1634	Subtracting the differences for each feature
926	Function for comparing different hyperparameters
624	Groping the Icel dataframe
301	fname is the destination of the image
909	Selecting the direct parents of a variable
1277	Identify the objects by color
1363	Some functions that modify a single value
1104	Feature aggregation for partial credit card balance
1789	Forceasting of models
930	Create random results
1605	Dictionary for Features
1663	kick off the animation
1482	sets if it is not a working directory
218	MinMax scale all features
1427	Set the hyperparameters for the optimizer
1515	if this is a new fold
617	Splitting the dataset into train and test sets
1243	only making predictions on the first part of each sequence
222	Linear SVR model for this fold
726	Photo file path
1711	Read data from the CSV file
1209	Save model and weights
683	Creating a unicode string
949	Plot the distribution of parameters
846	Average Mean Squared Scores
885	import matplotlib and set font size
1200	Print RMSE
1588	the evaluation metric for this competition
1732	Samples which have unique values are real the others are fake
486	plot LSTM for time series forecasting
858	making a scatterplot
1759	Feature matrix and list of features
591	Combinations of data
1257	Build New Feature
16	To plot pretty figures
367	SGDRegressor and SVC
552	Finding Best Weight
169	Batch Normalization
0	Show the DICOM files and labels
204	Remove other air pockets insided body
91	This frame is empty
159	Here are the labels
311	Predicting the target variables
634	We measure the run time for each day
31	Vectorize the vectors
419	Examine the shape of our data
353	Folds of the original dataset
1211	Here we resizes all the images
968	Remove the low features
460	Calculate start time
1521	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
83	Read the Training Texts
1386	the image scale if it exists
402	Different types of comment and counting
999	Returns the longest element
525	Run the estimator
867	extracting the best parameters
860	Evaluate the model with the provided training set
888	Merge Previous Features
1119	Distribution inspection of original target and predicted samples
1159	LIST DESTINATION PIXEL INDICES
137	Clear output and output lines
707	print CV AUC
1637	do cumulative count
1615	show mask class example
549	Classificationreport and confusion matrix
1063	We will use the most basic of them
669	Load the Data
1714	cross validation and metrics
1632	sets the df containing the date info for a province
933	Show the best scoring features
1681	the accuracy is the all time wins divided by the all time attempts
397	Random Forest Classifier
1698	Test the program by evaluating all the images
1432	Find the number of links per title
58	you can play around tfms and image sizes
611	Save images to a specified location
496	get the data
186	Plotting outliers
105	without any limitations and dtype modification
709	SPIFT LABELS
448	Updated Train and Test Data
640	MosT common positive words
488	function to extract the categorical and numerical features
588	Room Count Vs Log error
1396	Extracting date information
1055	Create metrics dataframe
1354	Fast data loading
883	Hyperparameters of Bayesian optimization
1654	correlation plot for correlation data
138	Checking MFCC version
438	We will look at the dimensions of our data
1184	Assign the variables to the class variables
620	find contours and areas
1333	Squeeze and Excitation layer , if desired
657	get the X and y columns for building model
650	These are some parameters of the network
829	Read the image on which data augmentaion will be performed
302	Read and resize image
1008	Correlation between variables
1575	Reading the datasets
818	Add the prediction values for each class
173	Find unique calsses value
1024	the encoding of the features
489	Function for group by
1362	Converting the datetime field to match localized date and time
103	Reducing the memory usage
682	Creating a unicode string
1365	Download the data
384	to reduce memory usage , dtype is specified
1163	Order does not matter since we will be shuffling the data anyway
1404	compute the sumbmission values for each day
1567	Pinball loss for quantiles
1768	shuffling the data
1817	Lidar data processing
988	Ploting cash accounts over time
1573	creating a list of categorical features
587	There is a box around the error
1505	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
508	Here is the main prediction steps
79	Resize the image
870	Write column names
1068	Print CV scores , as well as on the test data
1228	Load text data into memory
676	Store the targets and the inputs for building
479	From Strings to Vector Vectorization
1728	This enables operations which will take a few hours
1722	The mean of the two is used as the final embedding matrix
1813	summarize history for loss
970	realligning the data ..
459	Calculate start time
352	Read the data
337	Train the model with the callbacks provided
97	Apply before after normalization
1332	Depthwise convolution phase
577	Get a sample
275	variables for the Dropout part
1058	Initialize the model
796	parameter value is copied from
1286	Removing Remaning Images
1237	Read the image and convert to float
1078	Set values for various parameters
932	Add the evaluated results to this array
828	Read the image on which data augmentaion will be performed
1531	Previous app dataframe
799	Train the model with early stopping
584	Create a new dataframe by aggregating how many birds per row
1269	the else return the original image
334	Looking dimensions of our data
1646	Function for retrepping the last convolutional layer
1307	Calculate the biased values
161	if the label is too small
1568	Pinball loss for multiple quantiles
1380	save the coordinates
1572	Distribution of number of data in each training and validation set
1587	Create categorical features list
427	first column only
1328	Gets a block through a string notation of arguments
1079	add the feature vector for each of the words provided
898	Returns the list of zero importance features
975	iteration score 两列
1303	Reducing the memory usage
696	Exclude background from the analysis
1027	get score on best scores
356	Read the image file
29	Loading the data
373	Compute the STA and the LTA
210	identifying columns with numerics
1253	A model is saved every training epoch if the validation error improved
348	highlight columns with correlations above threshold
1769	SAVE DATASET TO DISK
1413	Mostly look very similar
1417	CNN for multiclass classification
1712	Since the labels are textual , so we encode them categorically
1134	Stacked Masks Stacked
187	Items have no description
1662	Display sample image
944	dict存储的数据 df
291	First lets visualise the test images
429	Year built vs
652	matplotlib and seaborn for plotting
523	Manipulation of conf values
908	Remove columns with constant values and their SK ids
507	Flatten the sets to be used later
619	find contours and areas
654	function to read the test data
940	Show the best scoring features
537	Run the estimator
764	Marker for each feature
1371	create a baseline set and make predictions
759	households without head
1346	the prediction of each test
1196	fold results
421	Distribution of meter reading
1699	sample to numpy array
363	Getting the targets for Training
906	Cumulative variance explained with PCA
1673	Add box if opacity is present
150	There is an outlier in the data
843	separate train and validation sets
1000	Evaluating Custom Features
895	Find the zero features
377	Print Root Mean Squared error
1089	Resize test predictions
571	Hours of the Day
1129	Get train and test paths
827	Read the image on which data augmentaion will be performed
1226	Plot some random images to see the changes
1300	Process text for RNNs
396	Calculate the confusion matrix
1686	Get RGB values
976	Setting the hyperparameters for random search
286	defining the paths to train and validation images
1040	Previous Counts
410	OneVsRest Classifier
1116	Returns the counts of each type of rating that a rater made
1685	Load the training data
118	Plot the Progression by Sex
1085	Ensure that the original image is the original image
12	This block is SPPED UP
1140	dependence plot for returnsClosePrevRaw
230	Implementing the SIR model
273	get lead and lags features
87	fake data sampling
912	Function to agg the data of a parent function
40	load the dataset
993	Returns a list of interesting features
603	Splitting the labels into a validation set
1496	This is a list of all the contributors
1606	Dictionary for Features
659	Perform feature agglomeration
193	blackhat image processing
1123	LightGBM dataset
651	Create submission file
894	Extract feature importances
565	Here we make a prediction
1056	Split the dataset into train and validation
1631	Prophet Ups
1706	Select a new candidate for the best score
101	load the image file using the cv library
43	Adding new columns
1747	Some new features
1616	Computes gradient of the Lovasz extension minus the sorted errors
1284	get the data ready for the model
1517	raw training data
1115	Check if columns between the two DFs are the same
690	Iterate through the data
287	Evaluate the model on the validation set
1325	Calculate and round number of filters based on depth multiplier
1733	Loading the Data
655	Now checking the missing values
1785	Avoid division by zero by setting zero values to tiny float
1152	to convert validation set to expected format
1050	the encoding of the features
446	Encoding the Regions
963	do this more efficiently with indices
1659	for Neutral tweets
1746	Replace some missing values
274	Strings and Vectors
1121	Extract features from test set and remove useless features
28	MODEL WITH SUPPORT VECTOR MACHINE
73	create network and compiler
484	imports for building the model
879	Iterate over random hyperparameters
1315	ATOMIC NumberS
1320	add distances from atoms
1039	Previous counts
176	Check some random XS
475	We can now vectorize the text
882	Test the hyperparameters of the random search
693	If this is a new model , load the previous model
1739	distribution of winPlacePerc
1635	Splitting the data into train and test data
1742	SCALE target variable
282	Looking at the data
1392	Save the dataframe path and the list of original transactions
533	Run the estimator
216	MinMax scale all features
1337	Update block input and output filters based on depth multiplier
1545	Change column name
420	Distribution of meter type
1604	Looking at the missing values
102	grid mask augmentation
1312	reduce validation sets
1689	Sort elements by weight
886	Loading raw data
78	save dictionary as csv file
1011	Function for creating categorical features
1498	Making a list of all the variables that are decayed
635	The fitting model
284	Just labels for the target variable
801	Applying the predictions to create submission file
1556	Setting the Hyperparameters for the Masking
551	Run the estimator
1707	Run program
740	update res line
1364	Address needs to be converted to Asia
1760	This function preprocess the dataframe
252	Decision Tree Regression
534	Finding Best Weight
756	Plot distribution of features by target
766	Add a legend and annotate the labels
564	For each item description , extract the digits
454	Run the session
126	Prepare continuous features
247	Transforming the predicted values into numbers
1811	Plot the actual vs predicted
1153	Here we can see the distinctions
1399	Compute series mean and standard deviation
1338	The first block needs to take a stride and filter size increase
1623	Classification
826	Read the image on which data augmentaion will be performed
1169	print the shape of dataframe
772	Correlations of the Target Variable
67	split into train and validation filenames
981	Replace the Outliers
1737	The competition metric relies only on the order of recods
732	Read the image from the dataset path
833	Distribution of Fare
74	cosine learning rate annealing
1491	Returns a dictionary of counts
1218	Processing by the world time stats
458	Checking Best Feature for Final Model
1122	Setting up Hyperparameters for LGBM
749	This is the main processing of boxes and scores
1757	Relationship between applications with different number of lines
374	Avoid division by zero by setting zero values to tiny float
1045	Count number of loans grouped by customer
1701	Generate a list of the candidates made
1361	import pandas and matplotlib for printing
1630	Guessing a reason for ordering by Province
1448	Creating the Validation Vectors
1090	Encode predicted probabilities with the highest confidence threshold
1470	We scale the KF values
1649	Lets calculate the extra features
1033	Selecting the direct parents of a variable
1390	Use the dataframe to define train and test dataloaders
1610	Categorize the labels
1596	Check the missing values
757	plotting a bar chart
47	Group by date
1647	Create the plot with the folds
365	Accuracy of the model
68	if augment then horizontal flip half the time
1447	Create submission file
1636	Brand new features
418	preview of data
1028	Reducing the memory usage
181	prices of the first level of categories
309	Create an embedding matrix
281	Info about dataset
219	Loading the Data
34	Loading Train and Test Data
574	The number of bedrooms per Interest Level
445	Extracting informations from street features
412	Importing Libraries And Loading Datasets
1221	Find the nearest scoring parameter
854	Most Fare Amount
1349	Leak Data loading and concat
1216	The stats change a lot between releases
647	imports for building the network
1442	Prepare submission images
1503	of image , label are flipped
556	Creating a dict for fast feature engineering
1486	Here we are getting the pretrained model
1644	Clicks By Click Rnd
595	Prepare the model
1007	Need to do some quick averaging
1735	set color palette
751	Settings for pretty nice plots
1661	Read in the Sol file
914	Joining the id with the categorical info
865	Initiate the hyperparameters
1422	deep copy the sentences
631	calculate I and D
413	Load libraries and data
782	change column names
1806	Read the data into a list
1192	fill the missing values with
1495	sets if it is not a working directory
1726	for numerical stability in the loss
733	Read the image from the dataset path
961	Aggregated feature engineering
1418	import the Libraries
45	Quadrate the bar chart
1766	cross validation and metrics
853	Fare amount by Day of Week
600	Get the mask directory
1443	Square Error
1158	size and spacing
326	Initialize patient entry into parsed
866	choice of boosting types
327	Add box if opacity is present
1274	Now we determine the unique colors
1339	Final linear layer
1609	Preparation for XGBoost
1780	The wordcloud of the raven for the raven
1778	Import training data
1598	checking missing data
572	There are FAR less ones than zeros
114	Here we merge the three dataframes , day and month
1819	Join market and news frames
1522	FIND ORIGIN PIXEL VALUES
312	Finding the columns with only Numerical columns
1227	Importing the data
1445	note that this below is suboptimal
660	Computes and stores the average and current value
1696	Here I write a function that evaluate an input image
59	Unfreezing the model and checking the best lr for another cycle
1137	Distribution of Resolutions
1513	Order does not matter since we will be shuffling the data anyway
881	Preparing the data for modeling
1781	Calling our overwritten Count vectorizer
1351	Fast data loading
1293	Load dataset info
331	Read in the Images
1326	Round number of filters based on depth multiplier
1378	Generate random labels
1762	checking missing data
1446	Order does not matter since we will be shuffling the data anyway
734	Classify image and return top matches
642	Now , we can see the common words
1372	create submission file
194	display the threshold
92	Save object as a Python module
360	Show the sample
115	Defining the parameter grid for the Random Search
787	Cumulative importance plot
1474	Cutmix iteration
1287	Display a number of examples with and without Clear
1271	Check the neighbors
474	Create submission file
982	Bureau date features
1229	Converting data into Tensordata for TPU processing
411	OneVsRest Classifier
532	Create our pipelines for later use
913	First , we need to agg first
265	We scale the data
562	Checking Best Feature for Final Model
1162	Order does not matter since we will be shuffling the data anyway
810	store the score and standard deviation
1650	Score the track
922	Plot the most important features
100	code takesn from
127	Finding the dimensions of categorical variables
1807	Lets validate the data
1072	y , width , height
50	The number of series in the DataFrame can be quite different
151	Correlation between users Download the app
1799	shift test predictions for plotting
1622	Print the feature ranking
1537	Regex strings for matching breakdown topics
432	That means each prediction has several folds
890	Bivariate Linear Model
494	Distribution of the validation features
1034	unique index from agg
52	Normalize the values
483	hidden layer
680	Function for encoding and decoding
713	Predict for submission file
1619	checking missing data
1057	if the mask is empty return original image
188	The wordcloud of the dataset
1205	List of files to be evaluated
1258	Here we resizes all the images
1411	Create the layout
546	Create our pipelines for later use
139	import library for building
1808	creating a new dataframe counting the number of transactions
722	Sort ordinal feature values
253	Prepare the data for modeling
822	Read the image on which data augmentaion is going
1752	Creating an entity from dataframe
1311	Display current run and time used
1186	count the number of actions processed per session type
321	Hyperparameters search for ExtraTreeRegressor
720	Toxic Comment data set
1210	Pad the image to be visually palatable
953	import matplotlib and set font size
1005	Evaluating the Test Feature
814	Split the training set into training and validation sets
1272	Check for the current object pairs
1667	Get the submission values
142	get the data fields ready for stacking
688	draw box over the image boundaries
1593	fill up the missing values
863	Train the model and evaluate the predictions
290	Clean up working directory
1369	Map the centroids of each district
1675	Patient 1 Test Image
1331	Loads pretrained weights , and downloads if loading for the first time
699	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
943	Train the model on the test data
1017	Sort the table by percentage of missing descending
1581	Updating Columns
1048	Credit card balance
456	Load the packages
893	Get all missing values
322	Voting Regression
1366	Load .shp files
1528	Join examples with features and raw results
1554	Still a very high AUC
748	Load the model from the path directory provided
861	Split into training and testing data
1248	get comp data
1026	Train the model with early stopping
1704	Delete the candidate from the candidates list
462	Load libs and funcs
1259	Using original generator
1359	Fast data loading
1299	Embedding function
1600	visualize the correlations
644	Perfect submission and evaluation
1318	Build the mol outputs
467	Precision curve
1594	Tokenize the sentences
1767	Tokenize the sentences
1305	Score of toxic vs
941	Add the evaluated results to this array
1107	Load sentiment file
971	Get score on random search and reset indexes
824	Applies the cutout augmentation
182	Correlation between categories
805	Add our parameters here
404	Initialize input and output dimensions
38	observation data frame
347	Loading the Data
160	Here we select some random cells
1352	Leak Data loading and concat
174	Load the image from a file
234	Filter selected features
1559	Correlation between features
1067	split training and validation data
716	rf for random forest regresion
1431	Save the files
686	Use the dataframe to define the labels
1788	Plot the peaks
267	Player College Name Missing Values
15	Common data processors
947	random hypothesis search
271	Code from Whale Classification
1611	Features to user level
241	Filter Germany , run the Linear Regression workflow
110	Plotting sales volumes per year
744	Function for path retreival
1414	Print top number keywords
1743	EXTRACT DEVELOPTMENT
1669	gather the parts of the pattern
341	Change column names
1748	Bureau amount per entity
1580	Multiply the new columns by the original columns
1538	actual data frame
605	Pad the audio
874	Train the model on the test data
1555	Convert training set to lightgb model
9	fill test weather data
172	Save the model as a JSON file
1108	Load image file
1127	Initialize the dataframes
107	Most common value
1035	Convert categorical variables to integer indices
509	Here is the main prediction steps
718	if mode is train mode
20	Check for missing values in training set
567	A simple Keras implementation that mimics that of
276	sort the validation data
371	Hyperparameters search for ExtraTreeRegressor
531	Classificationreport and confusion matrix
1756	Relationship between Previous App Features
175	image data loading
37	Histogram of the log value of the feature columns
553	Classificationreport and confusion matrix
542	Finding Best Weight
664	Date Aggregate Features
1571	load best model
244	Filter Andorra , run the Linear Regression workflow
1394	With the Masks
1266	Getting the predictions
1653	Visualization is always better
183	Brand name of item
104	Compiling the model
1002	Use the projection as new features
1801	k is camera instrinsic matrix
702	ADD PSEUDO LABELED DATA
1029	Create metrics dataframe
1405	Searching for rolling mean for a given store
710	PRINT CV AUC
399	Print the confusion matrix
1738	Loading the data
1607	one hot encode
1708	Importing the Libraries
2	Add new Features
1070	Convert image to RGB
547	Run the estimator
1777	plot with correlation matrix
1557	Creation of the External Marker
1479	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1037	Convert types to original memory usage
1267	Here we take the test data and generate sequences from them
1143	This is how our data looks like
1319	Read the target and the inputs
146	The number of different values
1301	Process the test data
1415	Number of labels for each instance
1203	Prepare Fake Files
1539	Different Time Series Modelling
781	change column names
1678	convert text into datetime
294	Submit to Kaggle
1540	Scoring the data
1252	We will retrain the model weights and compile the model
1374	Drop columns that are not features
1245	Convert floats to ints
558	Dictionary comprehension to convert a list of parameters to a list
387	Convert item to length
1076	Average length of comment
1480	Run the iteration batches
1013	Running the model
714	Count the missing values
540	Create our pipelines for later use
1207	load the data
1242	Write result to file in the correct format
1705	Return the program associated with the best candidate
389	Check for empty images
1682	An optimizer for rounding thresholds
148	Click by IP
1105	load mapping dictionaries
1071	y , width , height
1562	Get predictedors
1356	meter split based
675	Run the Grid Search
610	getting the absolute path of a file
704	MODEL AND PREDICT WITH QDA
109	Plot rolling statistics
1275	Identify the object by applying a color
112	This causes that we diverge from the data frame
1015	Loading the data
614	Weight of the class
499	handle .ahi files
1164	See sample of validation matches
409	Now we can build the input vectors for the classifier
428	all other columns
964	Feature Matrix Feature Matrix Features
1792	Visualizing Traffic Months cross Weekdays
1500	Print results in a list
1672	Initialize patient entry into parsed
1639	Click Rnd Ratio
897	Plot the cumulative importance
842	Correlation with Fare Amount
715	Remove the Outliers
1730	Add leak to test
1018	Print some summary information
783	Import the libraries we gon na need
1518	raw training data
179	Filling the missing price by category distribution
899	creating dummies for categorical features
1687	Return a list of the same shape as x
962	do this more efficiently with indices
1514	size and spacing
1481	Histogram of continuous variables
1709	Importing sklearn libraries
616	Change the Standard Deviation
1112	extract different column types
1204	Create fake folder
1385	suppose all instances are not crowd
21	Check for missing values in training set
891	Drop the columns from the two datasets
852	Fare Amount versus Start of Records
1511	Setting the input layers
339	make predictions on the test set and save weights
649	Create embedding matrix
465	Bayesian Trains
739	Calculate the ratio
1381	split the dataset in train and test set
711	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
202	Determine current pixel spacing
1643	Explore the proportion of downloads by device
951	Reading the Data
622	Examples for usage and understanding
1099	predict oof train and test
978	There might be a more efficient method to accomplish this
98	This is the main function of this competition
1310	Get feature importances
1250	Save the best model
530	Finding Best Weight
1750	Create an entity from dataframe
864	Train the baseline model on the test set
1138	Feature importance with SHAP model
1117	Compute QWK based on OOF train predictions
955	Feature Engineering with autofeaturetools
859	This implements the RandomizedSearchCV
1235	draw the image
1185	the next batch
1493	if validating , run prediction file
973	Set the scores
1703	Top most candidates
1487	restore the latest checkpoint
13	Load train and test data
1379	if border is present and M is empty
424	Plotting meter reading
1092	Applying CRF model and checking the changes
1066	First dense layer
398	Print the confusion matrix
516	add team conffences
728	Read in the labels
977	Preparing data for modeling
1197	Create dataframe for later usage
667	Plot the distribution of log1p and other variables
1190	Merge train events with test events
1265	Data Augmentation for Mask Size
1570	save the converted images
44	Normalize the values
1618	average the predictions from different folds
406	The function that we will use to calculate the distribution
754	Plot distribution of features by target
1800	plot baseline and predictions
636	Define the fitting model
258	Building submissions for submission
582	Test batch prediction
937	write out file and open connection
310	Preparing the data for our model
130	Look at how data generator augment the data
934	Training the model with the test set and predicting on the test set
1091	Convert rle to dataframe
1025	Hyperparameters search for LGBM Classifier
1126	Load image and mask
180	zoom to the second level of categories
449	Setting the X and y
3	Reset Index for Fast Update
1187	Get the average accuracy of the splits
249	Accuracy of the model
51	Add columns for bbox coordinates
1692	lifted function to convert a sequence of arguments
1527	Computes official answer key from raw logits
256	Voting Regression
1087	Applying coverage class
1506	FIND ORIGIN PIXEL VALUES
395	Matrix of Confusion
1424	function for cleaning lower case words
165	find the mask where the threshold is high
578	Calculate spectrogram using pytorch
455	Draw bounding boxes on the image
1804	Plotting ROC Curve
121	Checking the coverage of the current words
1697	convert list to list
372	Voting Regression
469	Data processing , metrics and modeling
780	Range Engineering
1741	HANDLE MISSING VALUES
55	look at the number of clusters
1507	Each iteration takes a fewseconds
1660	Read the cities table and convert to integers
435	Brightness Manipulation with matplotlib
359	Extract a segment from an image
328	Read in the Images
1512	size and spacing
1497	Here we are getting the pretrained model
125	Preparing the base data
1407	Train the estimator
1031	Ignore warnings due to deprecation of methods used
1165	Set all variables to None
1543	What is the MAPE score
64	Distribution of continuous variables
921	Sum up the importance values
11	Compute the STA and the LTA
178	Get the mean price of each category
1397	Plot distribution of variance
935	Show the best scoring features
1508	Each iteration takes a fewseconds
1602	Different Time Series Modelling
195	inpaint with original image and threshold image
1456	Numerical and numerical variables
476	vectorize the sentences
338	plot and explain the training and validation losses
1715	Ensure determinism in the results
255	Hyperparameters search for ExtraTreeRegressor
1410	Set the parameters for the model
1297	Predicting the Test Set
687	Get a sample image
1199	plot validation loss vs boosting iterations
1316	create molecule index and distance columns
1467	realign the missing values to be
742	The DataFrame has the following format
1176	Save images to a GIF file
769	Get the ROOF values
357	Getting the Data for the Images
1713	A custom LSTM model
260	get the numerical values of max
48	Normalize the values
1771	text version , slight different from original one
1147	Unique IDs from train and test
738	Test key drawings
1812	select important features
8	merge with building info
305	Evaluate the model on the validation set
1775	This enables operations which will take a few hours
857	Distribution of Validation Fares
892	Remove columns with more than 75% missing values
920	Credit card balance
56	Modeling with Fastai Library
1633	Exploring the data
1803	Correlation between World Correlation
1564	No significant trend is observed in above pair plot
1391	With the bounding boxes
25	Loading the data
1465	Define dataset and model
1484	This creates a generator that processes the examples
875	dict存储的数据 df
939	Create dataframe with scores
461	Calculate start time
1546	Get year , month , day of week
1046	First merge by loan
335	Looking dimensions of our data
99	intialize the hyperparameters
1638	Check the boxplot of the data
1238	Initializing variables and summaries
1139	Plot the dependence plot
464	Merge datasets into full training and test dataframe
1149	Get the preprocessed images
134	Initializing the model
304	Class Weights of the class
403	We are gon na use CatBoostRegressor
604	Convert wavs to numpy array
1786	load the table
1670	fixing random seeds
876	iteration score 两列
1782	Calling our overwritten Count vectorizer
60	get test predictions and submit
633	Running the model
889	We will align the test set with the training set
1064	Generate data for this run
555	Defining the data types
746	Find the most dominant series
1529	Read candidates with real multiple processes
1128	Compute coverage class
1489	Read examples from a .gz file
497	reducing the number of samples for each target
1473	MAKE CUTMIX LABEL
1453	drop rows with NaN values
308	Here are our new features
170	Now through the second convolutional layer
1569	Initialize Bayesian Optimization
945	iteration score 两列
196	Show original image
1288	Load dataset info
1754	Relationship between applications and installments
1189	add title information
358	skin like mask
1718	Tokenize the sentences
1168	Adding the transformed variables
1550	Average week of year
24	Remove the Outliers
768	Example of walls
1103	Loading the data
1710	Keras Libraries
1476	MAKE CUTMIX LABEL
1494	can also do test prediction later
929	There are some interesting observations
583	Get the probabilities of each batch
315	Accuracy of the model
351	Find the most dominant feature
1627	Plot country statistics
289	Classification Report
1787	index to select the indices of the error
573	Bathrooms and Interest Levels
784	Model Training and Validation
1260	Combine the filename column with the variable column
441	Latitude and Longitude
468	Loading the data
124	group by cols , extract their pct change
712	ADD PSEUDO Positivity
368	Decision Tree Regression
184	Brand name price
849	Calculate feature importance
444	Latitude and Longitude
1155	Create strategy from tpu
237	Filter Spain , run the Linear Regression workflow
626	China Cases
1329	Encodes a block to a string
343	create a generator that iterate over the data
145	Creating a dataframe
979	get the categorical variables
1472	size and spacing
1674	Add boxes with random color if present
1763	Copy feature matrix
1384	convert image to numpy array
668	Loading the Data
1614	Split the dataset into development and valid based on time
1776	Processing the data
493	Applicatoin train data
1774	Shuffling happens when splitting for kfolds
317	SGDRegressor and SVC
526	Finding Best Weight
834	The evaluation metric for this competition
154	Remove not available data
1183	returns the configuration dictionary
1576	checking missing data
1536	Check the number of records and any missing values
1412	Most Common Class Imbalance
719	Save the preprocessed weights
1665	fill in mean for floats
1241	Load an image
415	Brightness Manipulation with matplotlib
873	Write column names
1179	Calculate the confusion matrix
1150	Resize cropped area to original image size
300	Get the subfolders corresponding to the validation set
1383	We will build a nested analysis per customer
168	compute the average exp
1454	inverse transform yhat
643	Can I get your attention
263	We will need some weighting parameters later
623	replace the country variable with China
1285	for each model index , determine the path
666	First merge by product
1151	Initialize the dataset with the filenames
495	Read the data
1175	Add the cylinder actor to the display
22	Impute any values will significantly affect the RMSE score for test
1170	Getting the X and y columns for our model
1006	Remove small features from training and testing features
6	eliminate bad rows
1340	Filter for signal to be used later
528	Create our pipelines for later use
133	Splitting the data into training and validation data
340	df to hold predictions
798	Split up with indices
969	Getting the Train and Test Data
1132	Forward pass the prediction to the decoder
206	CONVERT DEGREES TO RADIANS
808	Split up with indices
1641	Imbalanced dataset
1100	Get fold AUC
1256	Creating the subfolders
299	Read and resize image
765	markers for the map
1249	parse trials , create submission file
529	Run the estimator
1023	creating dummies for categorical features
132	Create Testing Generator
966	Display the distribution of the feature by Value
1336	Skip connection and drop connect
207	LIST DESTINATION PIXEL INDICES
563	Descriptive Text
820	Graphviz model for non limitable features
1436	Looking at the data
831	Now our data file sample size is same as target sample size
1283	Drop the unnecessary columns
989	extracting the due and paid dates
225	Scatter visualization of LB Scores
925	Train the baseline model on the test set
452	Load the required libraries
708	ADD PSEUDO Positivity
250	Linear SVR model
1098	Create LightGBM dataset
1180	Assign the result to the TPU
306	Combinations of the data
1	Resize the image to original image size
1721	LOAD DATASET FROM DISK
811	Create a file and open a connection
1509	LIST DESTINATION PIXEL INDICES
215	converting the input data into XGBoost format
1357	Find Best Weight
606	Only the classes that are true for each sample will be filled in
1815	Initialize the Lyft dataset
936	Training the model with random search params
86	Lets look at the fake data
613	IMPORTING REQUIRED LIBRARIES
80	Now , we will merge all the convs
1651	Score the track
71	create numpy batch
601	And the mask over the image
1648	This will be run on the full training set
1677	Pleural Effusion
1478	LIST DESTINATION PIXEL INDICES
1294	warm up model
1295	Prepare the data
1051	Hyperparameters search for LGBM Classifier
515	Calculate the distribution of the team confferencestrength
1425	total number of unique tokens
1206	Define the densenet layers
794	Create a new dataframe with only the columns that match the selector
1683	greycoprops of a color
1247	only making predictions on the first part of each sequence
1657	Sentiments with POS Tagging
1195	Make a prediction
381	Convert item to length
303	Base paths for training and validation
946	altair is a very nice plotting library
1334	Expansion and Depthwise Convolution
569	Hours of the Day
53	Quadrate the bar chart
314	Getting the targets for Training
106	without any limitations and dtype modification
1111	Extract processed data and format them as DFs
503	scale pixel values to grayscale range
259	get the numerical values of max
586	There might be a more efficient method to accomplish this
76	load and shuffle filenames
917	Formatting for Previous Application
520	Train the estimator
229	Creation of the final ensemble
761	There are FAR less ones than zeros
538	Finding Best Weight
1693	lifted function name
717	Random Forest Regressor
1042	Sort the table by percentage of missing descending
1141	dependence plot for returnsClosePrevRaw
791	Train the model with the training labels and make predictions
1373	isolated histogram plot
903	get score on best scores
277	reorder the input data
27	Histogram of the data
1254	Run the model on the test set and output the predictions
506	Box plot of log of transaction revenue
1516	Detect my accelerator
878	Table for scores
1281	Libraries and Configurations
1772	always call this before training for deterministic results
23	Remove the Outliers
504	Plot the lineplot of the original dataset
1353	iterate through all the columns of a dataframe and modify the data type
662	aggregating on the level level
510	process remaining batch
1230	Load the model into the TFA
1289	Get the input shape
987	Previous Loan Amounts
1501	Detect my accelerator
108	Sales volume per year
579	Calculate logmel spectrogram using pytorch
57	just to be sane
1729	Add train leak
466	Plot ROC Curve
524	Create our pipelines for later use
839	Set alpha for legend
512	Sea lion competition
1075	Getting the predictions
1451	inverse transform
439	Plotting the most commmon IntersectionIDs
450	PDs and Targets
1727	Shuffling happens when splitting for kfolds
77	retrieve x , y , height and width
656	Combinations of Correlation
1012	Add the column name
1282	get the indices of train and test
205	Importing relevant Libraries
1239	Run the session
1409	Set the parameters for the model
998	Returns the normalized mode count
30	The path to the train and test data files
1818	Function to check if index is a palatable
69	add trailing channel dimension
956	We can use this data frame as new features
1668	Sales by Store
295	Here is the binary target
478	We can see the shape of the input data
1688	Return a list of the same shape as x
1408	We clipped preds
816	Create column for each row with the highest probability
1524	Eval data available for a single example
1485	Read the nq line and process the tfrecord
366	Linear SVR model
677	filtering only the columns that are not null
1820	Expand rows with missing values
862	Standard deviation of best score
1219	Function for creating title mode
1217	and reduced using summation and other summary stats
270	Hyperparameters for LightGBM
672	list of models to be engineered
1161	FIND ORIGIN PIXEL VALUES
729	Calculate the coefficient of variation
1440	Preparing the training set
251	SGDRegressor and SVC
1330	Encodes a list of BlockArgs to a list of strings
32	Identity Hate Feature
425	The distribution is certainly irregular
841	Plot the distribution of distances by fare amount
96	Save the before and after conversion to be used later
437	Preview of Training and Test Data
33	prophet expects the folllwing label names
1423	function for transforming a sentence to its corresponding words
350	Linear SVR model for this fold
695	remove layter activation layer and use losvasz loss
1420	from googletrans import Translator
1520	LIST DESTINATION PIXEL INDICES
1264	Load the best weights and predict the test set
869	Create output file and open a connection
1652	Loading the data
1617	remove layter activation layer
238	Filter Italy , run the Linear Regression workflow
1314	Plot the variables
482	Add a final sigmoid layer
594	Adding the text for each document
1280	Run the ARC solver
1020	create a list of the columns to remove
1585	fill all na as
1360	Leak Data loading and concat
1177	change color of the particles
63	Distribution of continuous variables
777	Plot the lower triangle
628	Groping theiron cases by day
407	Run the map
806	Finalize the hyperparameters
608	getting the absolute path of a file
1106	Load metadata file
596	Add one bird to the y axis
388	Looking at the histogram of the image
1577	Build continuous features
433	Ignore the warnings
685	Creating a unicode string
1065	Model Hyper Parameters
1081	Unfortunately , this is HUGE
123	Process text for RNNs
1499	restore the latest checkpoint
1044	drop missing columns
501	show the graphs
896	There might be a more efficient method to accomplish this
1102	Preparing the submission file
1118	Manually adjusted coefficients
167	Only the classes that are true for each sample will be filled in
855	separate train and validation sets
354	Check that it all loads without a bug
928	Get a subsample
1157	numpy and matplotlib defaults
539	Classificationreport and confusion matrix
342	Save the submission
1457	checking missing data
1502	Order does not matter since we will be shuffling the data anyway
1578	replace the missing values with
847	Training the model
1021	Store the column name associated with a given key
82	make a submission
1628	Without any limitations , see
960	Running the feature names
65	save pne location in dictionary
1342	Calculates ratios
185	Most of the items with a price of
1731	Creating a video
144	get the data fields ready for stacking
325	Save the inputs and targets
1032	Remove columns with constant values and their SK ids
1490	Read candidates with real multiple processes
453	draw the image
1542	the SMAPE score
1052	Train the model with early stopping
119	Pulmonary Condition by Sex
607	Return a normalized weight vector for each class
703	STRATIFIED K FOLD
741	Scatter plot of local deform lines
1761	Label encode categorical features
706	MODEL AND PREDICT WITH QDA
5	Encode Categorical Data
75	create train and validation generators
235	Clean Id columns to be used later
792	Merge with the predictions
1114	Remove missing target column from test
1154	Looking at the differences between the specifications
473	Loading the data
1441	Preparing the test images
1030	There might be a more efficient method to accomplish this
41	checking missing values
648	Load the dataset
318	Decision Tree Regression
1304	Reducing the memory usage
770	Bonus Variable vs Target Variable
803	plot confidence that will be used as submission
1666	StackNetClassifier with GPU
1758	Relationship between applications and pos balance
958	We can see the object types are more interesting
129	See sample image
177	Most common level
257	Predicting and final submission
1317	Define GPU configuration
487	Reading all the datasets
408	Lets generate a word cloud from the frequencies of each tag
18	impute missing values
980	app types are copied from here
1679	get some sessions information
307	Tokenize the tokens
678	using outliers column instead of target column
292	Load the model and make predictions
1393	CATEGORICAL COLUMNS
162	find two cell indices
1194	We can see that the model has not overfit
442	Latitude and Longitude
1004	Featurize feature matrix
1010	Add the column name
1327	Convolutions like TensorFlow , for a fixed image size
972	plot distribution of confidence that will be used as submission
916	First merge with main dataframe
1461	Make a Baseline model
54	Print the trip duration
737	Test key drawings
156	Creating a dataframe
502	Rescaling the Image Most image preprocessing functions want the image as grayscale
1724	text version , slight different from original one
1740	Distribution of Number of DBNOs
147	IP of the traffic source
1814	Copy predictions to submission file
1592	some config values
731	Sampling of full hits table
1133	Stacking the predictions and masks
1142	Sum up the importance values
505	Joint plot
665	Let us now have a look on the data
166	Analyze a subset of the image
575	Correlation between bedrooms , bathrooms and price
1246	Training History Example
1694	Plot images using matplotlib
986	Previous dates
887	Bureau and Bureau features combined
1534	This code is copied from here
1135	Load the timestamps
1167	Import the required Libraries
994	What is the most common client type that was approved
1094	Calculate the dimensions of the image
1656	written by MJ Bahmani
697	Precision helper function
1166	Parallelization of the test
880	Reading the Data
630	You can skip this and use your own parameters
1796	Same for teststationity
1597	checking missing data
1433	visualize the count of links found
1201	Detect my accelerator
94	function to create a subset of values
790	Evaluate the model on the full training set
671	Concatenate all the features into a single dataframe
705	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1690	check if all the elements are zero
1586	Correlation between features
1613	Split the dataset into development and valid based on time
1458	checking missing data
1621	What about the data types
773	Most correlated variables
795	delete the parameters from the hyp dictionary
400	Here we can just convert the tensor to image
463	Data processing , metrics and modeling
223	get the best score
974	creating a DataFrame of all the hyperparameters
609	Save images to a specified location
821	Graphviz model for non limitable features
231	Merge train , test , exclude overlap
967	There might be a more efficient method to accomplish this
436	Preview of Training and Test Data
871	Running the hyperopt Function
785	Sum up the importance values
152	Visualization is always better
1236	draw text annotations
615	An optimizer for rounding thresholds
1220	Drop missing values
1173	convert to HU
440	Top most commmon Paths
1144	Growth Rate Percentage
514	Summary of WINS and Lleaves
950	Iterate over random hyperparameters
485	Now through the second convolutional layer
379	Define Root Mean Squared Error
835	Brand new columns
1069	Write the prediction to file for submission
135	sklearn is only imported for building the model
954	Set the target variable
345	Fully connected generators
1416	Detect my accelerator
1671	get the data ready for the model
492	Calculate Top Correlation Matrix
948	Here we can see the bars
288	sklearn , LGBM and RUC
333	Read the test images
262	Scatter plot of parameters and LB score
1038	Previous aggregation function
1124	Get predictions for validation set
90	fast less accurate
261	Ploting parameters and LB score
599	Load an image
1358	iterate through all the columns of a dataframe and modify the data type
1182	denominator and product of them
313	so we can label encode categorical variables
1716	FUNCTIONS TAKEN FROM
1110	Extract processed data and format them as DFs
212	Fast metric computation for this competition
727	Here is a list of categories
721	Histogram of nominal variables
239	Filter Italy , run the Linear Regression workflow
481	it was the worst of the words
1574	Reading the Data
293	Get the extracted id
809	Train the model with early stopping
1624	Preliminaries and Setup
1292	Instantiating the Second Model
93	this is faster than using dict
370	Hyperparameters search for logreg with GridSearchCV
1551	Now we can add month data
213	creating a dummies table
1658	Tokenize the selected text
221	highlight if value is above threshold
272	configurations and main hyperparammeters
211	so we can use LabelEncoder
692	If this is a new model , load the previous model
1001	Return the most recent value of a Series
1270	The clique is assigned a max val
1251	Get the indices for the respective category
670	Transposing the dataset
1691	lifted function of encoding
851	Add elapsed time in seconds
1080	Divide the result by the number of words to get the average
793	Random Forest Classifier
355	Check for any overlapping target values
1468	Finding the maximum feature
1532	Choose your parameters
1430	make test features
1548	add time information
1276	get the inputs and targets
776	Pair plot of Target variable
1717	LOAD TRAIN AND TEST
1642	Imbalanced dataset
1437	Looking at the data
89	Downsize to one file and one file per file
1488	An object for storing results
1212	Plot the waveform curves
1793	Web Traffic Months cross days
942	Show the best scoring features
220	highlight columns with correlations above threshold
208	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1558	Creation of the Watershed Marker
1348	Fast data loading
498	read header and get dimensions
1193	Go to actual revenues
1599	checking missing data
1131	Create the dataloaders with the previous dataset
1561	sigmacro columns
568	Loading the Data
597	just to check if it works as intended
236	Filter Spain , run the Regression workflow
422	Distribution of meter reading
1400	Applying log function on series
1120	function to rename column names
868	Sample out data
1579	Multiply the new columns by the original columns
1306	Load the Image Data
1526	Default empty prediction
825	Set to instance variables to use this later
1563	import xgboost as xgb
1736	Visualizing the numerical variables
143	Compile and fit model
518	Make a new dataframe with the wins and losses
1113	Subset text features
758	Which households where the family members do not have the same target
1279	Identify the objects by both
1083	Load csv files
1664	Distilbert for Python
70	add trailing channel dimension
625	Sort by day number
698	Applying CRF model and checking the changes
131	Prepare Testing Data
471	Plot ROC Curve
1296	Add Image augmentation to our generator
1181	the confmtx and outprod tensors
1640	Load the Data
189	Length of coms
319	Prepare the data for modeling
39	Get a batch
1302	Load the model into the TPU
689	functions to show an image
755	correct entries that were not delined as such
1765	Plot the negative waveform
984	Closer Bureau Credit
1582	Replace columns with new epared values
924	Plot the importance curve
245	Filter Andorra , run the Linear Regression workflow
332	Read in the Images
872	Create output file and open a connection
1043	Print some summary information
1096	encoding the test set for submission
113	Joining the month with the actual values
490	Distribution of the features
884	Loading the Data
747	Unfreeze backbone layers
1073	Distribution of the target variable
1345	Show some examples
1084	Read the image source
197	blackhat image processing
1347	iterate through all the columns of a dataframe and modify the data type
1036	Need to do some quick averaging
1231	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
1382	A funtion to remove stopwords
661	get different test sets and process each
140	Make PyTorch deterministic
1350	iterate through all the columns of a dataframe and modify the data type
557	The number of times the user clicks
1263	Using original generator
66	load and shuffle filenames
1565	Import the necessary modules
1291	Add the first convolutional layer
1809	Columns to be consolidated
1019	Reading test data
1435	take a look of .dcm extension
1095	Predict the validation set for our StackNet
1215	Only load those columns , not their contents
283	Exploratory Data Analysis
639	GIven that people sentiment analysis
1566	extract the week base FVC
1492	to the validation set
1225	Make a picture format from flat vector
1395	Draw the graph
1213	Create strategy from tpu
416	load the data
629	Groupping by day
1223	Predicting X test
434	Read data and prepare some stuff
500	Separate the zone into a df
1061	Create a list of all the filters
42	The distribution of the official variables is certainly irregular
192	Show original image
451	Only the classes that are true for each sample will be filled in
684	Creating a dictionary for the label count
1504	LIST DESTINATION PIXEL INDICES
1222	Generate parameters for the Grid
136	Save the model with the given parameters
1233	Converting data into Tensordata for TPU processing
1322	Read the inputs
815	Hyperparameters search for LGBM Classifier
116	Defining the parameter grid for the Random Search
632	Load the data
1370	Label encode categorical variables
700	MODEL AND PREDICT WITH QDA
576	Set global parameters
1477	Mixup of the images
1510	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
566	Classification
674	get the X and y variables for the country
1016	Bureau balance aggregation
1790	import libraries for the model
618	find contours and areas
694	Fully connected layers
786	Plot the most important features
802	Applying the predictions to create submission file
1208	define the parameters for the model
1810	Importing sklearn metrics
775	plot the heatmap
1202	Implementing Efficientnet
278	Word Cloud plot
1377	Function to plot images using matplotlib
1464	Read all test filenames
1783	Generating the wordcloud with the values under the category dataframe
736	Find the number of zero features
832	Observation Looks like orientation features
1773	the loss function and optimizer
1355	iterate through all the columns of a dataframe and modify the data type
900	the encoding of the features
663	Let us now have a look on the data
1547	Get the data frame
1591	Evaluate the models for the test samples
1805	Lets look at the memory usage of a dataframe
264	Prepare Training Data
957	Relationship the Previous Cases
679	Split labels into a list of labels
1552	Average day of year
214	make a set of training and validation sets
1375	Difference Variations and Submission
1753	Relationship between application id and bureau id
36	Log target label
1589	only making predictions on the first part of each sequence
17	Now extract the data from the new transactions
767	drop columns with correlations above
1060	Train Validation Split
62	Exploring the categorical variables
1321	Define GPU configuration
1214	Order does not matter since we will be shuffling the data anyway
904	Reducing the memory usage
952	Preparing the data for modeling
344	create a generator that iterate over the data
1290	Squeeze and Excitation block
813	Show the distribution of the target
1734	Preparing the data for this fold
1419	Import the Libraries
938	Write column names
627	Groupping by day
35	First some embeddings
1439	Loading the data
771	calculate the average age per capita
788	Plot the importance curve
1751	Here we can see the es dataframe
1612	Split the dataset into development and valid based on time
991	Ekush Some Feature Engineering
1541	Argmax of the model
1530	Previous app data
1240	Read in the submission file and make predictions
470	Merge datasets into full training and test dataframe
199	inpaint with original image and threshold image
1309	run a batch of frames
240	Filter Germany , run the Linear Regression workflow
394	Decision Tree Classifier
1146	load mapping dictionaries
390	Set some parameters
521	We merge the model winners into a single dataframe
1770	missing entries in the embedding are set using np.random.normal
1341	Draw the measured and unmeasured segments
918	Now extract the installments payments
1684	Load the training data
1234	Model initialization and fitting on training set
836	Zooming the Bounding Box
1791	extracting year , month from date for flattening
1387	We just need to apply transoforms on sample
120	Function for extracting words from series
1313	plot feature importance
923	Cumulative importance plot
1298	if yhat is a list of labels
817	Applying the method
1438	This is the augmentation configuration we will need for the training
752	Table printing large
