1589	to truncate it
1106	Load metadata file
702	ADD PSEUDO LABELED DATA
91	Augmentations and Feature Engineering
782	change column names
594	text features to full text
1082	A single set of features of data
1505	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1300	It is needed for the tokenizer to work correctly
1073	Distribution of the target variable
484	start to building a CNN using Keras
951	Reading the Data
108	Sales volume per state group
1393	Transform categorical features into cateogry
1395	Draw the graph of molecules
347	Import Libraries and Data
431	Extract target variable
1123	LightGBM dataset objects
1634	Diff Diff for each H
1751	Creating an entity from dataframe
1490	Read candidates with real multiple processes
10	merge weather data
868	Sample out data
398	Print Confusion Matrix
892	Check if there are any missing values
1549	Merge Weather Data
1166	Parallelization of test data
188	Generating the wordcloud with the dataset
475	A fast vectorization is done to transform text
646	For each type get the mean value
114	Merge the data with the previous state
776	PairGrid between Target and Plot Data
1085	resize the image to the desired size
688	draw box over image
1302	Load model into the TFA
28	MODEL WITH SUPPORT VECTOR MACHINE
1484	Use the examples to create a generator
1108	Load image file
1131	define the dataloaders here
395	Table for Confusion Matrix
1621	What about the general variables
438	Glimpse of Data
1434	if unk is True and there is no previous word
112	Merge data with categorical features
265	We use red from sklearn to scale the data
800	Light GBM Results
875	dict存储的参数转化的数据 df
1452	Seting the target variable by date
1545	Create new column name
814	Train and Validation
206	CONVERT DEGREES TO RADIANS
1515	set the necessary directories
643	Can I get your attention
1364	addr to addr
1204	Create fake folder
137	clear output after batching
305	Load the model and check the performance
1488	Eval data available for a single example
1189	Start generate data sets
1158	size and spacing
1138	SHAP interaction values
812	Write column names
615	An optimizer for rounding thresholds
1444	Square of Error
637	Import and Read Data
354	check if everything is ok
388	Helper function for histogram processing
925	Fitting and predicting the test set
684	Creating a DataFrame for the labels
62	Exploring the categorical variables
308	Padding thedocs
1387	Apply transforms to sample
1748	Creating an entity from dataframe
1227	Read the data
1133	Stacking the val predictions and masks
1322	Read the input and target data
1543	Regression of the actual values
1309	Fetch a batch of videos
1710	Keras Libraries for Neural Networks
1473	MAKE CUTMIX LABEL
862	Standard deviation of best score
492	Calculate Correlation Matrix
1437	Number of Patients and Images in Training Images Folder
1192	fill the missing values with
1537	Custom function for breakdown topics
750	Combinations of TTA
744	Convert DNN ids to filepath
726	Photo image and truth
1808	How many transactions are there
249	SVR model on train and test
699	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1491	Returns a dictionary of counts
1184	v is set to all of the variables
469	Data processing , metrics and modeling
1465	Define dataset and model
274	Constants and Directories
506	Plotting the boxplot of the revenue
93	Set the before and after indices to the values
798	Split up with indices
190	Distribution of the description length
572	Number of orders by week
538	Fit the best model
171	Now through the second convolutional layer
1125	Appdend output layers based on our date
1598	Checking for Null values
1480	batch grid mask
1319	Read the target and input data
803	Understanding Confidence by Target
1472	size and spacing
733	Read the image and find appropriate filter settings
1149	Load preprocessed data
106	load the data
1144	Growth Rate Percentage
241	Filter Germany , run the Linear Regression workflow
488	Lets type our categorical and numerical features
766	Add a legend and annotate it
1146	load mapping dictionaries
205	Importing relevant Libraries
1762	missing data
1528	Join examples with features and raw results
105	load the data
1558	Creation of the Watershed Marker
141	histogram of Fraud vs
285	Just labels to identify theissue
661	get different test sets and process each
592	Read the data
1790	import libraries
1290	Squeeze and ResNetBottleneck block
1481	Histogram of continuous variables
557	Number of times the interaction was made
906	Visualizing Cumulative Variance
35	create an array of embeddings from train text
1583	coluns with new features
879	Iterate through random parameters
794	Create a dataframe with the selected features
1727	Shuffling happens when splitting for kfolds
452	import required libraries
1174	Visualize the bkg colors
46	What are the most frequent variables in the dataset
1563	import xgboost as xgb
1572	Plotting the distribution of data for each diagnosis
458	Parameters for LGBM
1254	Run prediction on test set
1407	Train the model
1567	Pinball loss for multiple quantiles
1044	Remove missing columns
530	Fit the best model
200	for later plotting
1412	Class Imbalance
134	Initializing CatBoostClassifier
1139	Plot the dependence plot
401	obtain one batch of training images
411	OneVsRestClassifier with a Logistic Regression model
570	Days of the Week
853	Fare Amount by Day of Week
32	Identity Hate Feature
13	Load train and test data
1003	dfs to get a matrix of features
563	Descriptive of Items
154	Check for missing values
1068	Print CV scores , as well as score on the test data
1245	Convert floats to integers
34	Loading Train and Test Data
843	separate train and validation sets
1244	Load the data
123	Process text for RNNs
1252	Load Model Weights
1708	Importing standard libraries
1016	Bureau balance by loanedness
217	MinMax scale all feature scores
955	EDA with dataframe mapping
642	For Neutral Word count
1713	Building the model
584	Create output dataframe with highest probability for each row
294	patch predictions for each image
139	from mmcv import mm
375	Run the build process
544	Building the Tourney
1368	Plot the map
877	Evaluate Bayesian Results
793	RFECV selector
393	What are the most frequent hot clusters in the test set
416	Read the dataset
1403	we need to predict at least
1017	Sort the table by percentage of missing descending
947	random hypothesis search
1502	Order does not matter since we will be shuffling the data anyway
1048	Credit card balance
1773	for numerical regressors
1064	Generate data for the BERT model
717	Random Forest Regressor
588	Room Count Vs Log Error
344	define my generator
1568	Pinball loss for multiple quantiles
1031	Suppress warnings due to deprecation of methods used
1606	I get rid of some features for best LB score
1474	Cutmix on batches
718	Preprocess classifiers
1074	Distribution of initial values for each application
722	Sort ordinal feature values
551	Run Grid Search
611	if save to dir
1340	Remove signal to noise
933	Sort by score
904	Clean up memory
370	Gradient Boosting
675	Run a grid search
453	Draw the line on the image
1273	Count the number of objects in the object
428	all other columns
1114	Remove missing target column from test
863	Fitting and predicting the test set
850	Get the data type
561	There are some weird spikes ..
405	calculate the log loss
1369	Draw the centroids of the districts
1551	Average values for all months
1386	convert to tensor
417	Preview of Data
696	Exclude background from the analysis
768	Check if walls have been properly loaded
734	Classify image and return top matches
1373	We can now plot the model using pandas
1681	the accurace is the all time wins divided by the all time attempts
1431	Save the data
1040	Merge with previous counts
27	histogram of the data
674	sets the training and validation sets
1151	Load train set
357	get different image data types
1732	Samples which have unique values are real the others are fake
911	Convert categorical data to integer indices
1069	Write the prediction to file for submission
1651	clusters based on event number
1596	Train data set
353	Here we create a simple model
1466	Run the model on the test set
390	Set some parameters
196	Show original image and grayscale
317	SGD model
869	Create a file and open a connection
742	The DataFrame has the following format
1675	draw patient image
870	Write column names
329	read in the images
1365	Download the Kaggle Data
1351	Fast data loading
1461	Make a Baseline model
1801	k is camera instrinsic matrix
889	align the rows of train and test data
997	seed features after each training round
1680	the time spent in the app so far
607	Return a normalized weight vector for the contributions of each class
296	Sample the data
929	find parameters between 0.005 and 0.0
1223	Predict Test Set
783	Import all that we need
1187	Get the average number of samples for the test set
222	Linear SVR model
236	Filter Spain , run the Linear Regression workflow
1562	Get predicted probabilities for each feature
822	Read the image on how to visualize it
1592	some config values
1755	Relationship between bureau and rureau
1058	Load the last checkpoint with the best model
549	Classification of Test
242	Filter Albania , run the Linear Regression workflow
295	Binary Target Variable
7	declare the columns for the model
22	Impute any values will significantly affect the RMSE score for test set
1384	convert to RGB array
1008	Correlation of the target variables
174	Load an image from a file
1164	DISPLAY VALIDATION IMAGES
1672	Initialize patient entry into parsed
619	Area of contours
691	Computes gradient of the Lovasz extension w.r.t sorted errors
1620	Checking for Null values
535	Classification of Test
1331	Loads pretrained weights , and downloads if loading for the first time
723	Sort ordinal data by ordinal
26	visualization of Target values
1147	Unique IDs from train and test
23	Detect and Correct Outliers
246	Set the dataframe where we will update the predictions
1593	fill up the missing values
1253	Train the model
1619	Checking for Null values
769	find the location of the roof
1102	Prepare for Submission
1419	Import libraries and data
1805	Lets look at the memory usage of each dataframe
64	Distribution of continuous variables among continuous variables
763	current y value
86	Raw data analysis
678	using outliers column as labels instead of target column
18	impute missing values
1006	Remove low features from train and testing features
1699	convert sample to its input and output
60	Submit to Kaggle
631	calculate PDE related variables
1072	draw the attribute image and return the coordinates
689	functions to show an image
1079	vocaVec for each word
9	fill test weather data
365	SVR model on train and test
292	Load and predict
21	Check for missing values in training set
1695	Plot the tasks
1479	ROTATION PIXELS ONTO ORIGIN PIXELS
1501	Detect hardware , return appropriate distribution strategy
17	Now extract the data from the new transactions
1243	to truncate it
493	Applicatoin train data
1752	Creating an entity from dataframe
1612	Split the train dataset into development and valid based on time
384	to reduce memory usage
1600	Diverging color palette
1495	set the necessary directories
1194	Predict on train set
1269	If the inx pairs are in the same color
754	Visualizing distribution of features by level
189	Shortest and longest coms
1285	member of the model
1771	text version for squash
180	zoom to the second level
1714	cross validation and metrics
916	Merge main data with bureau data
1560	correlation among the macro features
1707	Run the program
410	Predicting with SGDClassifier
581	test if the number of steps is too small
606	Only the classes that are true for each sample will be filled in
1764	Copy X features
548	Fit the best model
903	get score on best score
132	Create Testing Generator
601	And there you have it
371	Fitting our model
1013	Clean up memory
502	Rescaling the Image Most image preprocessing functions want the image as grayscale
74	cosine learning rate annealing
635	Determine the model
1337	Update block input and output filters based on depth multiplier
1497	Here we are getting the pretrained model
1124	save oof val
1226	Plot some random images to check how cleaning works
335	Looking some informations of all datasets
1001	Return the most recent value for a given column
176	Visualize the images
801	Get the predictions in the right format
854	Investigation of Fare Amount vs
1587	Binary Cat Features
857	Distribution of Validation Fares
524	Create the Pipelinees
943	ROC AUC on test data
1521	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1045	Next , we will filter by loan
1810	For data analysis , model building , evaluating
873	Write column names
1161	FIND ORIGIN PIXEL VALUES
517	Get the seeds as integers
1077	Function to remove stopwords from the question text
1643	Count the number of clicks and downloads by device
1496	Some MODELS
474	Save submissions to file
1456	Number ofrooms and price
1647	Create a tqdm notebook
1111	Extract processed data and format them as DFs
1811	Plot the actual values vs predictions
1718	Tokenize the sentences
1595	Pad the sentences
894	Light GBM Results
1705	Find the program that has the largest number of candidates
50	Distribution of the total occurences of each variable
1667	Stacking of Test Predictions
0	DICOM Image Metadata
1272	Check for the current object pairs
1046	Group by loan
701	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1582	coluns with new features
389	Check for empty images
962	create feature matrix and summary
1288	Load dataset info
1676	Lung Opacity
107	count number of stores and item ids
211	label encode categoricals
1427	Set values for various parameters
1500	Print directory info
1141	Plot the dependence of the target
490	plot distribution of the validation features
133	Spliting the data into training and evaluation
999	longest possible element
983	Adding new features from bureau
1613	Split the train dataset into development and valid based on time
973	Get score for random set
1228	Load Train , Validation and Test data
765	Visualize the markers
860	We will now train the model
1120	function to rename the columns
1518	Get raw training data
97	Apply before to see the effect
731	Sample out hits
1051	Hyperparameters search for LGBM
784	Now we evaluate the model
1623	Logistic Regression without Standardization
1075	Getting the Predictions
333	read in and write out the file
1442	prepare submission data
449	Setting the target variable
1210	Pad the image to be able to resize it
1448	Scaling the dataset
617	Splitting the train data further into train and test sets
1482	set the necessary directories
948	Draw a bar chart
164	Reading the image
902	Train the model with early stopping
1282	get train and test data
1086	Check that the training set is ready
1665	fill in mean for floats
1059	Visualize the validation images
888	Merge the bureau data with the previous features
109	Plot rolling statistics
183	Brand name of brand
1240	Read sample submission file
885	import matplotlib as plt
424	PLOT FOR METER READING
1007	averages of the target variable
1749	Create an entity from dataframe
1035	Convert categorical data to integer indices
1687	Function that checks if all the masks are the same
1494	Prediction on test set
382	And there you have it
1586	Correlation with the Target
852	Fare amount versus time since start of records
1429	make train features
482	Define the model
967	There might be a more efficient method to accomplish this
910	Unique values of each column
649	Building the model
578	Calculate spectrogram using pytorch
466	Plot ROC Curve
1527	Computes official answer key from raw logits
760	Histogram of heads only
95	For getting the indices of the selected columns
455	Draw bounding boxes on the image
1080	Divide the result by the number of words to get the average
1591	Load best model and check the performance
1794	Train the model
781	change column names
1513	Order does not matter since we will be shuffling the data anyway
1731	Function to create video file
1616	Computes gradient of the Lovasz extension w.r.t sorted errors
409	Feature extraction from train and test
749	This is the primary method this needs to be defined
1316	convert string columns to int
998	Calculate the normalized mode values
714	Count the missing values
982	Converting date columns from day to timedelta
1447	Create submission file
1201	Detect hardware , return appropriate distribution strategy
1315	ATOMIC Numbering
168	exp avg sqm
1185	update user samples
67	split into train and validation filenames
1744	FITTING THE MODEL
1573	Prepare the data
764	Marker for each sqm
1717	LOAD PROCESSED TRAINING DATA FROM DISK
256	Voting Regression
518	Make a new DF with just the wins and losses
660	Computes and stores the average and current value
746	take first occurence of each label
1522	FIND ORIGIN PIXEL VALUES
404	create some empty arrays
1682	An optimizer for rounding thresholds
1095	Predict on validation set
773	Correlation of all the heads
1399	Compute series mean and standard deviation
736	Remove zero features
1529	Read candidates with real multiple processes
193	blackhat with blackhat threshold
921	Sum up the importance
681	Creating a DataFrame for the count of unicode characters
286	Preparing the data
981	Replace day outliers
1464	Load test dataset
396	Calculate the confusion matrix
560	Plot Gain importances
1356	meter split based
861	Split into training and testing data
1191	Predicting the test set
1604	Nulls in Train and Test Data
777	Plot the feature plots
1380	Creating the xy values
1241	Load an image
1559	List of features with the specified methods
1814	Copy predictions to submission file
87	fake data sampling
901	credits to for the parameters values
978	There might be a more efficient method to accomplish this
762	Plot the counts
281	How many files are there in the dataset
1034	Unique values of each column
66	load and shuffle filenames
716	We now have the data ready for the model
1394	Number of masks per image
1392	Save the dataframe to the parquet file
310	Preparing the Data
149	Quantiles of the IPs
887	Print the original features and balance features
541	Run Grid Search
1767	Tokenize the sentences
1584	Checking only one value columns
275	Dropout RandomForest
263	We will need some functions to calculate the logreg coefficient
359	Segm RGB image
161	make a mask if it is too small
364	Scaling the data
954	set the target variable
320	Gradient Boosting
1217	Group by game time
1264	Load model and make predictions on test set
115	Defining the parameter grid for the model
1673	Add box if opacity is present
1760	I define a function to preprocess the data
1776	For later use
944	loop over all hyperparameters
508	Here I write a helper function to evaluate the threshold
942	Sort by score
1679	get some sessions information
537	Run Grid Search
842	Plot the correlation between variables
225	Scatter plot of LB score visualization
43	Add new features
1347	iterate through all the columns of a dataframe and modify the data type
1015	Import the datasets
1214	Order does not matter since we will be shuffling the data anyway
1763	Copy X features
1336	Skip connection and drop connect
264	Prepare Training Data
1236	Draw the text boxes with the text size
1274	Each color is one of the original images
1009	drop variables with too many missing values
1576	checking missing data
1746	Replace some missing values
332	read in the images
1314	Plot the variables
583	Sum up the batch probabilities
640	MosT common positive words
1686	Get RGB values
1259	Using original generator
539	Classification of Test
1417	building the model and compiling it
1454	inverse transform yhat
985	Using Bureau balance date over time
937	This is the out file
653	Read the training data
301	move image to sub folder
436	Preview of Train and Test Data
1382	To remove stopwords
926	Set up the objective function
964	Features with sample data
817	Applying method on train and test
1276	get the inputs and targets of the task
1756	Relationship between applications and previous applications
1816	Loading the data
1218	We can get stats on the group bys
1325	Calculate and round number of filters based on depth multiplier
1318	build train and test data
1283	Drop unwanted columns
1603	Checking any missing values ,
759	households without head
845	Set invalid APE to
503	scale pixel values to grayscale
369	Create Validation Sets
72	define iou or jaccard loss function
340	Making predictions dataframe
1511	Creating the input layer and the augmentation layer
1088	Remove padding from images
573	Number of bathrooms per Interest Level
771	calculate the households for each customer
1235	Draw the line on the image
871	Run the objective
56	Modeling with Fastai Library
883	Fit the model
82	make submission for model
1508	now timing for one iteration
1042	Sort the table by percentage of missing descending
690	Iterate over data
970	align the data
975	iteration score 两列
1055	returns the dataframe describing our scores
949	Ploting learning rate distribution
1698	test if the program has any empty mask
900	check for encoding
1142	Sum up the importance values
483	Define the model
596	If only one bird is in the data
1251	The function below create a split from a train and validation set
61	Function to extract the sex , smoking , or unknown
327	Add box if opacity is present
1067	split training and validation data
715	Remove the Outliers
432	Apply each model on all folds and output the results
380	Verify that length is correct
110	Plotting sales volumes per year
756	Visualizing distribution of features by level
1153	Let us do the same analysis for the validation set
1542	Fix the spurious negative trend
1370	Label Encoding of District
991	EDA and Feature Engineering
1723	missing entries in the embedding are set using np.random.normal
558	If you want to know the order of the variables
210	loop over numerical columns
791	Train the model on the test data
334	Looking some informations of all datasets
1789	Forceasting with decompasable model
1581	colunms new features
1363	Some place I should know ..
1344	Show some examples
367	SGD model
360	Here is the sample submission
386	Verify that length is correct
1179	calculate the confusion matrix
976	Setting up the hyperparameters
315	SVR model on train and test
221	highlight the value with a threshold
1614	Split the train dataset into development and valid based on time
934	Fitting and predicting the test set
247	Apply exponential transf
602	Load the data
1530	Previous app data
1719	shuffling the data
1054	Clean up memory
669	Load all the data
610	Loads a file from a directory
634	Define the run methods
996	seed features and their distributions
1242	Run the detector on the image string
1231	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
153	Ploting the download by click
755	does the mapping make sense
266	There are some missing values in the dataset
1540	Fit on all data
1506	FIND ORIGIN PIXEL VALUES
1	Resize to desired size
1733	Importing the required libraries
1795	fit second model
786	Plot the normalized importance
1163	Order does not matter since we will be shuffling the data anyway
1408	Predicting with kfolds
81	First fully connected layer
908	Remove variables with constant values
1297	Predicting the Test Set
804	Get subsample parameters
626	Sort by day
1029	returns the dataframe describing our scores
630	Numba model to calculate seirnas
846	baseline train and validation
268	Custom Loss
1110	Extract processed data and format them as DFs
1349	Leak Data loading and concat
25	Load train and test data
516	add conf fold
695	remove layter activation layer and use losvasz loss
468	Loading the data
554	Add RUC metric to monitor NN
1552	Calculate the average day of the year
589	Number of Stores Log error
99	intialize the network
1367	Plot the districts of the day
928	Get a random sample
1504	LIST DESTINATION PIXEL VALUES
1246	Training History Show
419	Examine the shape of our data
1175	Add the cylindrical actor to the display
1575	Reading the Data
1355	iterate through all the columns of a dataframe and modify the data type
598	Some of the images without any ship
1327	Convolutions like TensorFlow , for a fixed image size
1376	Deep Learning Libraries
130	Look at how data generator augment the data
346	I will create a generator that iterate over all the digits
1206	Define the model
349	highlight the value with a threshold
1062	Here are the main phases of the encoder
1599	Checking for Null values
1569	Initial Bayesian Optimization
707	QDA with CV
213	creating dummies columns
499	handle .ahi files
1129	Define train and test paths
169	Batch normalization and building model
338	plot and save the training and validation losses
312	loop over all numerical columns
1089	Resize test predictions
434	Libraries and Configurations
1266	Getting the Predictions
471	Plot ROC Curve
57	just to be sane
49	Plot the distribuition of the data
1277	Find the objects identified by color
1326	Round number of filters based on depth multiplier
1626	Plot COVID-19 predictions
1476	MAKE CUTMIX LABEL
459	Training the random search
1783	Word cloud of First Topic
239	Filter Italy , run the Linear Regression workflow
15	Common data processors
38	observation data as pandas DataFrame
1209	Save model and weights
757	create a bar chart
838	Plot the binary features
577	Get a sample
143	Compile and fit model
463	Data processing , metrics and modeling
1668	Let us now look at the sales
647	Import the Libraries
719	save preprocessed weights
520	Train the model
1391	Draw bounding boxes on the image
1547	inplace input data
865	Create hyperparameters
511	Group by season
1183	We need to pass the weightage as a dict
992	Relationship with the previous cash and credit
342	Save predictions to file for submission
632	Load the data
300	Make a folder with all the files
1065	Model Hyper Parameters
963	create feature matrix and summary
1577	Prepare the continuous features
1430	make test features
1802	This function is used to calculate the rotation of the image
650	These parameters are copied from this colab notebook
1193	And put it in a dataframe
351	Random Forest Feature Selection
1441	Process Patient images
1132	make a prediction
1571	load best model
79	Resize image to desired size
1693	lifting function name
1115	Check if columns between the two DFs are the same
1036	averages of the target variable
866	choice of boosting type
185	Categories of items with a price of
284	Just labels to identify theissue
1747	Amount loaned relative to salary
68	if augment then horizontal flip half the time
1400	Transform series into binary
270	lgb weight encoding
767	drop high correlation columns
65	save pneumonia location in dictionary
663	date aggregates with a helper function
1677	Draw Patients Based on Age
659	Performing feature agglomeration
792	Merge with predictions
151	Plot distribution of users downloads the app
408	tag to count map
1440	Preparing the training images
856	The Pickup datetime and fare features
1212	Plot the smoothing curves
184	Brand name price
336	batch size for training and validation
966	Plotting the feature by target
839	Attaches legends
945	iteration score 两列
708	ADD PSEUDO LABELED DATA
657	separate the data
84	Frequency of class interactions
126	Prepare Categorical Data
1167	Import required libraries
464	Merge datasets
299	Read in image and resize it
780	Range of values in a Series
1414	sorted by count
1820	Expand if missing
603	Output class encoding
1181	conf_mtx and conf
1378	Get random labels
1536	Check for Null values
162	Opening cells with two neighbours
621	Area of contours
514	Summary of NCAATIES
345	I will create a generator that iterate over all the digits
891	Drop unwanted columns
226	Scatter plot of CNN
245	Filter Andorra , run the Linear Regression workflow
402	Custom data types
175	Load the image data
1818	CHECK FOR EACH CATEGORY VARIABLE
1750	Creating an entity from dataframe
1216	creating a function that aggregates game time stats by installation id
1460	checking missing data
1625	filling missing values
1162	Order does not matter since we will be shuffling the data anyway
54	Examine the duration of the trip
88	Create and plot the models
145	Frame creation and gc.collect
102	grid mask augmentation
243	Filter Albania , run the Linear Regression workflow
979	get the categorical variables
1358	iterate through all the columns of a dataframe and modify the data type
896	There might be a more efficient method to accomplish this
1544	inplace input data
457	Libraries to import
435	Lets display some nice things about our data
250	Linear SVR model
276	sort the validation data
641	For negative words , we need to remove stopwords
255	Fitting our model
1449	create the dataset for regression
1738	Import and Read Data
1154	Checking for differences between datasets
1425	total number of tokens
1032	Remove variables with constant values
1208	define parameters for model training
1060	split dataset into train and validation datasets
526	Fit the best model
1196	Get fold results
1788	Peak frequency plot
1136	Spliting in the test data
1475	MAKE MIXUP IMAGE
1176	Save images to a GIF file
1635	Splitting the data
430	Encode Categorical Data
1799	shift test predictions for plotting
1000	Custom Feature Data
426	Plot the square feet
494	Distribution of the new features
505	Joint plot for the revenue
532	Create the Pipelinees
1396	Get the date for var
956	EDA and Feature Engineering
146	Number of different values
451	Only the classes that are true for each sample will be filled in
1778	Import Train and test csv data
748	Load the trained weights
1730	Add leak to test
550	Create the Pipelinees
1652	Load the data
946	altair is a great plotting library by the way
752	so we can see the data
500	Separate the zone and subject id into a df
423	Monthly READINGS ARE HIGHEST RMSE
1372	Creating the submission file
844	Fit model to train data
1728	This enables operations which are only applied during training like dropout
927	the score , parameters and iteration
917	Replace some missing values with
1207	load train and test data
1255	Load Model into TPU
531	Classification of Test
741	Scatter plot of single line
280	use k means to cluster the labels
1594	Tokenize the sentences
1726	for numerical regressors
939	Create dataframe to display best scores
730	Group by time
585	Building Year , Month , Number of stories
923	Cumulative importance plot
1234	Model initialization and fitting on train and valid sets
567	A simple Keras implementation that mimics that of
1390	Use train dataset
1352	Leak Data loading and concat
1137	What about the month field
1725	The method for training is borrowed from
1785	Avoid division by zero by setting zero values to tiny float
1438	Create datagen
181	Prices of the first level categories
55	find number of clusters
790	run model on full training set and predict the results
834	There is an outlier in the prediction
569	Hours of Order Day
1107	Load sentiment file
1066	First dense layer
326	Initialize patient entry into parsed
1406	Get just the digits from the seeding
478	Vectorizing the data
1768	shuffling the data
935	Sort by score
220	highlight the upper triangle
323	Wrap the prediction in iou or jaccard
1043	Print some summary information
709	create stratified validation split
2	Add new Features
199	inpaint with original image and threshold image
1041	Disable objects to save memory
77	retrieve x , y , height and width
1556	some config values
711	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1165	Ready to compute mel features
227	Scatter plot of LB score visualization
724	Treating values with Simple Imputer
907	Get the size of a dataframe
1310	Get feature importances
559	Create a dictionary with the values in order
1293	Load dataset info
952	Get data ready for modeling
214	make sure to convert validation set to train and validation set
71	create numpy batch
498	read in header and get dimensions
100	code takesn from
1546	Get date columns information
142	get the data fields ready for stacking
721	nominal variables countplots
1697	Helper function to make a list of images
986	Adding some new features from previous days
473	Loading the data
1671	Reading the csv files
1385	suppose all instances are not crowd
1757	recuring features based on previous applications
608	Loads a file from a directory
361	Image data processing
1239	Run the model
350	Linear SVR model
182	Optimal Price
1103	Import the datasets
496	get the data
1170	Split the data into train and test
481	Tokenize text of the training data
872	Create a file and open a connection
1160	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1027	get score on best score
806	Convert parameters into int
271	Partial imports
770	Bonus Variable
1637	do cumulative count
1777	plot with correlations
1653	Exploratory Data Analysis
664	Date Aggregate for Category
1691	lifting function for univariate functions
1296	The params I use can be tweaked to your desire
1105	load mapping dictionaries
1026	Train the model with early stopping
309	Create an embedding matrix of words in the data
1182	in case of large numbers , using tf.function
339	Load Weights and make predictions
1649	Lets add some extra features to the dataframe
568	Loading the Data
1753	Relationship between applications and bureau
163	RLE encoding for the mask
1742	SCALE target variable
1084	Read the image and convert to grayscale
1256	create train , test and test directories
671	Joining all the columns together
489	Function for group by
604	Convert wav data to raw data
571	Hour of Reordering Count
42	There are too many addr , so we will subset the data
1155	Create strategy from tpu
683	Creating a DataFrame with the labels sorted in descending order
30	Load the train and test data sets
391	convert to float
1287	Blurry Samples
156	Frame creation and gc.collect
899	one hot encoding
1213	Create strategy from tpu
98	This is the main plotting function
915	unique values in each column
1602	Moving Average of all the values
905	returns the dataframe describing our scores
1640	Load the Data
705	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1083	Load csv files
1517	Get raw training data
720	Toxic Comment data set
823	Custom Cutout augmentation with handling of bounding boxes
1766	cross validation and metrics
1294	warm up model
1052	Train the model with early stopping
1690	check if all the columns are empty
1405	rolling mean for each store
1492	Prediction on validation set
120	Function for calculating word count
837	Attaches legends
1215	Only load those columns in order to save space
348	highlight the upper triangle
1446	Order does not matter since we will be shuffling the data anyway
739	Compute the ratio of rejected
48	Normalize colors based on the image
880	Reading the Data
987	Previous Loan Amounts
1409	Set up some basic model specs
1229	Build datasets objects
1308	Fetch a batch of videos
1761	Label encode categoricals
1011	count categorical variables
229	Ensemble final scores
1186	count of each type
476	the times in the text
306	Combining all the dataframes into one
1743	EXTRACT DEVELOPTMENT TEST
1238	Start tensorflow session
377	RMSEs for this fold
1772	always call this before training for deterministic results
1499	Check if the latest checkpoint exists
605	Pad the audio data
1221	Find the best score
1416	Detect hardware , return appropriate distribution strategy
1415	Number of labels for each instance
685	Creating a DataFrame with the labels sorted in descending order
51	Add columns for bbox coordinates
1021	Store the columnsseen after each key
574	bedrooms with respect to bedrooms
1432	calculate the count of links and nodes in the dataset
515	Calculating the confusion matrix
387	Load item from file
797	Convert to numpy array
1334	Expansion and Depthwise Convolution
218	MinMax scale all floats
829	Read the image on how to visualize it
694	remove activation layer and use losvasz loss
805	Add subsample parameters
833	Plot distribution of fare
58	you can play around with tfms and image sizes
1030	There might be a more efficient method to accomplish this
1566	same as above
1311	Display current run and time used
1275	Applyes color to the image
1711	Read data from the CSV file
501	show the graphs
1289	Initialize the Neural Network
1023	one hot encoding
425	Distribution of the Primary Use
1038	Previous aggregation function
1143	Dividing the data by date
187	Items have no description
1740	Distribution of DBNOs
555	Defining the data types
1770	missing entries in the embedding are set using np.random.normal
121	Checking the current coverage
418	Preview of Data
1678	convert text into datetime
1433	Plot the count of links
298	Read in image and resize it
1457	checking missing data
450	Initial Data Prepparation
909	standardize the data type
732	Read the image and find appropriate filter settings
1028	Clean up memory
1233	Build datasets objects
1126	Load the mask from file
613	Suppress warnings due to deprecation of methods used
636	Determine the model
1526	Default empty prediction
462	Load libs and funcs
1632	Seting the df according to the province
960	The default feature names are
47	Add month information
1703	Find the best candidates
1561	Macro columns by name
576	target and params
1050	check for encoding
203	For every slice we determine the largest solid structure
1398	this is a very performative way to compute the time block
1483	Some MODELS
160	Labeled Cells
1410	Set up some basic model specs
950	Iterate over random hyperparameters
809	Train the model with early stopping
1458	checking missing data
1281	Libraries and Configurations
922	Plot normalized importance
207	LIST DESTINATION PIXEL VALUES
918	Some new features from installments payments
1100	Calculate GBM AUC
40	load the data
4	Remove Unused Columns
1803	Correlation between images
480	Tokenize a piece of text
1648	Iterate over all tqdm predictions
1087	convert coverage to class
235	Clean Id columns and keep ForecastId as index
167	Only the classes that are true for each sample will be filled in
1670	Set seed for reproducability
467	Precision and Recall
1284	shape of train and test data
223	get the best score
1533	Mean ROC curve
1127	Initialize the data
1758	Adding application level features
378	Mean absolute error
415	Lets display some nice things about our data
599	Load an image
1754	create installment Relationship
994	What is the most common client type where Contract was approved
808	Split up with indices
1232	Load Train , Validation and Test data
735	Classify an image with different models
543	Classification of Test
1689	Sort by weight
855	separate train and validation sets
1171	Save images to a GIF file
297	Spliting the training and validation sets
638	Function to generate a word cloud image
1361	import modules and define models
1486	Here we are getting the pretrained model
1383	split x , y , and validate
597	Brief check on the data
1298	Make labels binary
1135	Load the timestamps
886	Getting data from leak
704	MODEL AND PREDICT WITH QDA
1014	Clean up memory
1012	Add the column names
362	Load image by name
131	Prepare Testing Data
1512	size and spacing
1230	Load model into the TFA
495	Read the data
1784	Compute the STA and the LTA
441	Latitude and Longitude
670	Transposing the lat and long columns
314	Drop nuisance columns
8	merge with building info
1694	Plot images using matplotlib
729	Compute the coefficient of variation for different categories
652	matplotlib and seaborn for plotting
232	Double check that there are no informed ConfirmedCases and Fatalities after
252	Decision Tree Regression
1819	Join market and news
59	Unfreezing the model and checking the best lr for another cycle
1716	FUNCTIONS TAKEN FROM
1250	save best model
534	Fit the best model
654	function to read test data
1199	plot validation loss vs boosting iterations
693	Using previous model
1666	StackNetClassifier with GPU
356	Read the image from image id
820	Non Limitable Classifier
3	Reset Index for Fast Update
1317	set up params
836	Zooming nyc map
1570	Convert DICOM to PNG via openCV
835	BanglaLekha new observations
1248	Load and preprocess data
400	convert to image
575	Correlation of bedrooms and bathrooms
448	Plot the data for modelling
668	Read the Data
1588	If the score is correct but there is noise in the prediction
687	Get a sample from the dataset
1459	checking missing data
821	Non Limitable Classifier
129	See sample image
522	Add the model losers to the dataframe
1342	Calculates ratios of each message
1225	Make a picture format from flat vector
1404	Let us encode the input data first day by day
20	Imputations and Data Transformation
1022	Remove all columns except for the identified ones
1053	get score on best score
1292	Instantiating the model
1644	Clicks with respect to time
37	Lets take the natural log of the target variable
6	eliminate bad rows
307	Tokenize the text
743	pivot to have one row per type
1389	StratifiedKFold On Labels
1004	dfs to get a matrix of features
1734	Fill in missing values
1117	Compute QWK based on OOF train predictions
1782	Calling our overwritten Count vectorizer
195	inpaint with original image and threshold image
1468	max of max features
1450	create the look back datasets
802	Get the predictions in the right format
440	Top most commmon Paths
990	Amount loaned before
198	ploting the threshold
819	Save the reduction results
1263	Using original generator
165	split image with ndimage.label function
1633	Age distribution with seaborn
930	Create random results
1099	predict oof on validation set
1306	Dealing with player tracking data
1321	set up params
322	Voting Regression
1109	Unique IDs from train and test
1197	Prediction of Revenue
1787	index to vectorize the error
117	Preparing the submission data
1268	Linear Weighted Kappa
470	Merge datasets
703	STRATIFIED K FOLD
980	Place the values into different variables
311	Target Columns Go to TOXIC
1590	Load the data
1313	Light GBM Results
1025	Hyperparameters search for LGBM
700	MODEL AND PREDICT WITH QDA
39	Get the next batch
316	Linear SVR model
12	This block is SPPED UP
895	Find the features with zero importance
682	Creating a DataFrame with the labels sorted in descending order
618	Area of contours
1134	Stacked masks and predicted probabilities
1817	Lidar data processing
666	Here is the distribution of the products
1780	The wordcloud of the raven for Edgar Allen Poe
1712	Since the labels are textual , so we encode them categorically
1514	size and spacing
122	Function for cleaning special chars
358	skin like mask
1692	lifting function to combine multiple results into one list
1775	This enables operations which are only applied during training like dropout
1323	Parameters for an individual model block
319	Create Validation Sets
269	Merge Dense Players
616	Change the Standard Deviation
140	Set seed for reproducability
1071	You can access to the actual face itself like this
1237	Decoding the string and converting to float
224	get the best score
984	Days with Days
90	fast less accurate
1424	function for cleaning lower case words
1360	Leak Data loading and concat
138	import torch and mmdet
1793	Plot the traffic months cross days
1721	LOAD DATASET FROM DISK
1354	Fast data loading
1664	import stacknet
73	create network and compiler
1661	Read in OOF Files
313	so we label encode categoricals
815	Optimal hyperparameters
799	Train the model with early stopping
150	Lets see if it is an attributed category
460	Training the random search
504	A couple of more interesting features
1565	Import the necessary modules
304	Set class weights
512	Transform the season data into a Season dataframe
725	Create and fit a model
796	parameter value is copied from
527	Classification of Test
919	Brand new features
208	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
938	Write column names
1122	define lightgbm params
52	Normalize colors based on the browser
1435	take a look of .dcm extension
1737	The competition metric relies only on the order of recods ignoring IDs
968	Remove low features
556	create feature dictionary
1609	Preparation for XGBoost
1180	assign the conf matrix to self
876	iteration score 两列
932	Evaluate the model
710	PRINT CV AUC
1467	realign the column to be of type int
111	Merge the three dataframes above with the date information
1470	Scale the data
989	due dates , installments , and paid days
1335	Squeeze and Excitation
383	Read the data
273	get lead and lags features
1341	measured vs unmeasured plots
1262	Load an image
454	Run the object detection on the image
1722	The mean of the two is used as the final embedding matrix
1445	we assume we are less accurate here , boost confidence
1305	Submission with best score
965	reset index and style
727	Most frequent items in a category
1812	extract important features
1745	Here is the unbiased version of the generator
609	if save to dir
75	create train and validation generators
1049	one hot encoding
639	Segregating the sentiment data
1375	Diff Diff V32 values
533	Run Grid Search
1299	Embedding function for feature extraction
1413	Most common labels
893	Get data ready for modeling
697	Precision helper function
562	Parameters from Tilii kernel
785	Sum up the importance
53	Plot the triplets
667	Ploting the distribuition of the asset
253	Create Validation Sets
303	Load the data
1339	Final linear layer
1145	Curve Fit
1190	Event code distribution
676	Store the data for our model
958	We can see there are some weird things in dataset
1332	Depthwise convolution phase
1428	Add PAD to each sequence
582	Get the batch probabilities
290	cleanup working directory
1645	Converting the correlation matrix into a numpy array
644	Perfect submission and target vector
1270	Remove duplicate images
1564	Now we can label encode the categorical variables
673	prepare data for modeling
1656	written by Nanashi
1739	Plot the distribution of winPlacePerc
1381	split the dataset in train and test set
170	Second component of main path
230	Implementing the SIR model
1610	Log transformation of target variable
913	Joining parent and child df
1729	Add train leak
1779	Generate the Mask for EAP
1279	Identify by both
96	Save before to before.pbz
1534	This code is copied from here
509	Here I write a helper function to evaluate the threshold
1605	I get rid of some features for best LB score
848	Create randomforest object
740	add noise
1628	Some place I should know ..
1411	Create the layout
1093	salt parameters are from the above mentioned tutorial
566	Keras Implementation for Regression
600	load mask directory
1205	Load and Preparation
194	ploting the threshold
831	replace NaNs in train and test data with
1798	shift train predictions for plotting
1557	Creation of the External Marker
318	Decision Tree Regression
623	Adding the country column
197	blackhat with blackhat threshold
1597	checking missing data
385	check test files
1519	Number of repetitions for each class
442	Latitude and Longitude
363	Drop nuisance columns
1402	extract data from first n samples
1800	plot baseline and predictions
629	Groping by day
1312	reduce validation set
1113	Subset text features
1333	Squeeze and Excitation layer , if desired
828	Read the image on how to visualize it
1328	Gets a block through a string notation of arguments
429	Convert year column to uint
972	random search and bayesian optimization
713	SPLIT TRAIN AND TEST
686	Use the dataframe to define data generator
186	Shipping fee paid by seller
277	reorder the input data
957	Relationship with the previous cash and credit
658	perform model on different models and calculate accuracy
662	date aggregates
1601	checking missing data
16	To plot pretty figures
1503	of the TPU while respecting padding
510	process remaining batch
487	Exploration Road Map
1362	Converting to Total Days , Weekdays and Hours
1806	Reading the Data
521	Add the Teams
546	Create the Pipelinees
1130	define training dataloader
78	save dictionary as csv file
788	Draw the threshold lines
177	Most common level
624	Groping the Ialy dataset
324	Save the data
399	Print the confusion matrix
343	define my generator
513	Conference Tourney Games
1617	remove layter filter and use losvasz loss
1538	actual is lagged here
158	The function to convert an image to a grayscale
1550	Calculate the average week of each year
931	count combinations of parameters
1658	Tokenizing the selected text
840	Manhattan Distance by Fare Amount
152	How many times each categories of clickers download the app
234	Filter selected features
633	Running a model
747	Unfreeze backbone layers
127	Get the categorical variables
407	Run the map
851	Calculate elapsed time
447	Encoding the Regions
914	Joining the aggregated data with the original data
920	Credit card balance
818	Add the prediction values to the test dataframe
1485	Use the TFRecords generated by the generator
1630	Looking at the best choices for each Province
1172	Read in the image
593	factorize categorical features
728	Read in the labels
456	Load packages and data
1260	Combine the filename column with the variable column
706	MODEL AND PREDICT WITH QDA
392	Read and resize random images
24	Detect and Correct Outliers
622	Examples for usage and understanding
912	Function to aggregate data from parent and store them in a dataframe
830	We can see the distribution of surface
1091	Create submission dataframe
14	visualization of Target values
69	add trailing channel dimension
1101	Training the model
240	Filter Germany , run the Linear Regression workflow
1224	select proper model parameters
1720	SAVE DATASET TO DISK
1791	Flatten the date columns
1462	Create dataset for training and Validation
1219	Function for creating title mode by title
565	Predict the feature with the data
1578	replace NaNs with
94	Define the function to assignates new values to the dict
1451	inverse transform for test data
988	Amount of loans over time
1104	Credit card balance
1674	Add boxes with random color if present
321	Fitting our model
1097	Check if train and test indices overlap
1509	LIST DESTINATION PIXEL VALUES
1152	create validation set
374	Avoid division by zero by setting zero values to tiny float
191	Display scatterPlot between description length to price
1786	load a table
1638	Check the distribution of min
282	Data Prepparation
372	Voting Regression
1348	Fast data loading
1531	Previous applications categorical features
1696	This is the main prediction steps , along with some cleaning
1195	Make a prediction
376	Mean absolute error
209	FIND ORIGIN PIXEL VALUES
1622	Print the feature ranking
1278	Run object detection
813	Which we can plot up ..
523	Remove Confidence Stadium
1724	text version for squash
1535	Loading the data
293	Extract the ID from file names
352	load the data
238	Filter Italy , run the Linear Regression workflow
1377	Function to plot different images with different color distributions
1303	Delete to reduce memory usage
116	Defining the parameter grid for the model
1024	check for encoding
519	Preparing the training data
545	Select Percentile
1453	Drop rows with NaN values
341	change column names
1112	extract different column types
1487	Check if the latest checkpoint exists
1541	checking the score of the model
233	Create date columns
1706	choose a candidate to iterate from
1804	Plotting ROC Curve
1553	Average day of the week
1688	Check if all the colors are the same
824	Applies the cutout augmentation on the given image
1421	Cache Overview Number of Sentences
70	add trailing channel dimension
1329	Encodes a block to a string
1096	Generate submission and calculate score
881	Get data ready for modeling
810	store performance metrics
655	There are no missing values in the dataframe
507	Split the train dataset into two sets , flat and log based
1422	deep copy the sentences
1683	contrast , etc
1796	Same for test stationity
1660	Read and convert cities into integers
257	Predict and Submit
1769	SAVE DATASET TO DISK
1741	HANDLE MISSING VALUES
1148	define image size and input size
497	reduced target0sample data
651	Storing results to disk
1220	Drop nuisance columns and fill missing values
472	Precision and Recall
1507	now timing for one iteration
1258	so we have to pad the images
1119	Distribution inspection of original target and predicted train and test
445	Extracting informations from street features
63	Exploring continuous variables
977	Converting data into numeric type
698	Applying CRF seems to have smoothed their prediction
961	We can see there is no clear relation between features
1388	Stack the bboxes into a single image
553	Classification of Test
1353	iterate through all the columns of a dataframe and modify the data type
1081	Fit the neural network
1374	Preparing the data
1061	Create a list of DownConvolutions
1169	Print dimensions of dataframe
267	Some Player College Names have missing values
76	load and shuffle filenames
737	Test key draw
586	bedroom Count Vs Log Error
1736	Analyzing the Target Feature
1420	from googletrans import Translator
262	Ploting the parameters and LB score visualization
1631	Load full table with all data
1498	Decide the layers of the model
1140	dependence plot for returnsClosePrev
1005	Make predictions using the test dataset
1615	show mask class example
261	Ploting parameters and score visualization of OSIC PFP solution
1128	Compute salt mask coverage
172	Save the model as a JSON file
19	Imputations and Data Transformation
849	Feature importance with random forest
1265	Create train and validation datagens
811	Create a file and open a connection
80	Passes through the convolutional filter images and merge convs
1657	Building the Sentiments
1418	Load the needed libraries
1076	Average length of the comment
173	What are the most unique calsses in the dataset
1345	The first prediction is easy
355	Target and number of duplicate clicks
1338	The first block needs to take care of filter size increase
864	Fitting and predicting the test set
738	Test key draw
1636	Brand new features
157	Print final result
1150	Image augmentation with augmentations
807	Convert to numpy array
291	predictions image by image
33	prophet expects the folllwing label names
969	Getting the train and test data
1203	The original fake faces are
1700	Evaluate the Genetic Operators
113	Merge the month and store ids with the day lag
753	Import Train and Test dataset
5	Encode Categorical Data
525	Run Grid Search
1641	ip count in train
612	Listen to the test data
1489	Read candidates from a .gz file
775	Draw the heatmap
1291	Squeeze and Excitation
1624	Table of Contents
789	ignore all warnings
1168	Adding some new features from original data
1650	clusters based on event number
394	Decision Tree Classifier
128	Prepare Traning Data
878	Create scores dataframe
625	Sort by day
479	From Strings to Vectors
1765	PLOT FOR RELEASED TIME
302	Read in image and resize it
1704	Delete best candidate
579	Calculate logmel spectrogram using pytorch
1520	LIST DESTINATION PIXEL VALUES
83	Read the Text Data
995	Plot the most common client type
1639	Clicks withing a few clicks
433	Toxic Comment data set
779	escolari diversity in age
1574	Looking at the data
1261	Create test generator
1033	standardize the data type
1063	We will use the most basic of all of them
540	Create the Pipelinees
485	Now through the second convolutional layer
936	Fitting and predicting the test set
542	Fit the best model
1510	ROTATION PIXELS ONTO ORIGIN PIXELS
135	Checking for Class Imbalance
379	RMSE for this competition
1426	Number of characters in the sentence
288	sklearn , Lasso , and auc
1366	Load Data and Data Merging
1307	Calculates the quantized range and bias vector
1295	Prepare the data
1608	Parameters for xgboost
1267	This functions take the test data and generate sequences from the test data
761	There is missing data in test and train data
289	Ekush Classification Report
119	Pulmonary Condition Progression by Sex
1019	Merge with bureau data
136	Save the model
1685	Load the data
1774	Shuffling happens when splitting for kfolds
29	Load train and test data
940	Sort by score
841	What is the distribution of distances by fare amount
1585	fill all na as
953	import matplotlib as plt
614	Weight of the class
1056	Split into train and validation sets
924	Draw the threshold lines
251	SGD model
898	Returns a list of zero features with zero importance
1359	Fast data loading
1524	Eval data available for a single example
816	Getting the predictions in the right format
679	Split the labels into a few parts
31	Vectorize the data
774	that have not been eliminated yet
580	Logmel feature extractor
1684	Load the data
1525	Span logits minus the cls logits seems to be close to the best
228	Ensemble final scores
1701	The final candidates
1157	numpy and matplotlib defaults
219	Import Libraries and Data
1548	add time information
628	Groping the confirmed cases by day
406	Exploring the data
1436	Number of Patients and Images in Training Images Folder
486	LSTM for Time Series Forecasting
712	ADD PSEUDO LABELED DATA
104	Compile and fit model
1807	Let us check this data
491	Pearson Correlation heatmap
1607	one hot encode the categorical features
1173	convert to HU
529	Run Grid Search
215	converting into xgb DMatrix
536	Create the Pipelinees
1020	Remove all the columns that are needed for training the model
1301	load test data
620	Area of contours
124	pct change of group by columns
1118	Manually adjusted coefficients
437	Preview of Train and Test Data
1159	LIST DESTINATION PIXEL VALUES
259	get the numerical features
1715	Ensure determinism in the results
847	For now just a simple model
1439	Getting familiar with tabular data
92	This was copied from
258	Make final predictions
587	There is a high level of bathroom count and lgerror
1618	average the predictions from different folds
1781	Calling our overwritten Count vectorizer
1324	Change namedtuple defaults
758	Which are not equal in terms of households
1759	Feature matrix and other configs
204	Remove other air pockets insided body
897	Plot the cumulative importance
1177	plot the colors of the object
858	Show plot of actual validation and predicted
330	Visualizing the images
1655	Draw the heatmap using seaborn
1094	Generate the dimensions of the image
1662	Example from sample image
1078	Set values for various parameters
1156	watch out for overfitting
1116	Returns the counts of each type of rating that a rater made
890	Correlations with Wins
528	Create the Pipelinees
974	loop over all hyperparameters
1443	Square of Error
1523	oversampled training dataset
745	build a dict to convert surface names into numbers
1627	Plot country predictions
1539	Preparing the testing series data
279	so we have pca with inertia
867	First we extract the type of prediction
118	Sex Condition Progression by Sex
1346	show predictions for each test
325	Save the data
1611	Specify which attributes will be used as targets
1735	set color palette
36	Log target variable
272	configurations and main hyperparammeters
1057	Transforms images to float
1471	Order does not matter since we will be shuffling the data anyway
1371	Train the model
787	Cumulative importance plot
202	Determine current pixel spacing
552	Fit the best model
1286	If you have a GPU put back on CPU
103	Deleting unnecessary columns
971	Get score on random search
283	How many data are there per label
1211	Here we take all the images and resize them
439	Top most commmon IntersectionIDs
1580	calculate new features based on original features
1039	Previous counts categorical
859	run randomized search
1098	Create Validation Sets
1629	Plots by country and time series
260	get the numerical features
1280	Run the ARC solver
590	Gaussian target noise
1423	function for transforming raw sentence to wordlist
795	using hyp dict to reduce memory usage
1493	compute validation results
1047	Lets read in the cash data
826	Read the image on how to visualize it
1343	Number of steps affected by this
1320	convert string columns to int
1469	splitted features into a single dataframe
192	Show original image and grayscale
1809	We can deal great wildlife picures
148	Clicks by IP
85	Fake data preparation
884	Loading the data
1646	Retreive convolutional layer
443	Latitude and Longitude
1010	Add the column name
287	Load the model and check the performance
373	Compute the STA and the LTA
89	Improvement clearly visible
1792	Plot the traffic months cross days
680	Length of labels
446	Encoding the Regions
993	get interesting features
216	MinMax scale all variables
665	Creating a date agg_4
144	get the data fields ready for stacking
1669	gather input and output parts of the pattern
147	IP of the traffic source
1247	to truncate it
45	Quadplot of number of items in each dataset
231	Merge train and test , exclude overlap
1815	Load the data
1532	Create a model
1178	DICOM image and mask
772	Plot the sin function for the wavelet
1663	kick off the animation
1350	iterate through all the columns of a dataframe and modify the data type
1257	Build New Data
564	cast item descriptions to int
461	Training the random search
874	ROC AUC on test data
1397	Plot distribution of variance
237	Filter Spain , run the Linear Regression workflow
254	Gradient Boosting
547	Run Grid Search
1188	Calculate the average accuracy of each assessment
328	read in the images
1018	Print some summary information
692	Using previous model
366	Linear SVR model
1455	inverse transform for test data
413	Load Libraries and Data
1198	Create submission file
1478	LIST DESTINATION PIXEL VALUES
1642	ip count in train
672	list of models to use
1554	Train the model
412	Importing necessary librarys
1555	Convert training set to lightgbm dataset format
751	Settings for pretty nice plots
381	Load item from file
1222	define the grid parameters
368	Decision Tree Regression
166	Analyze number of masks
1037	Get the size of a dataframe
677	filtering outliers
1202	Load Model into TPU
477	Vectorizing the data
465	Bayesian Optimization
1121	Extract target variable
178	Get the mean price for each category
648	Load Train and Test Data
421	Plotting meter reading
41	Overview of Missing Values
1271	check if all pairs are same or not
645	try random samples
403	Train the model
959	Aggregated Feature Data Set
159	Separate objects and their labels
1516	Detect hardware , return appropriate distribution strategy
11	Compute the STA and the LTA
44	Normalize colors based on the image
591	Combined Compose
832	Observation Looks like orientation is cyclic
101	load the image file using cv
595	Create the model and train
825	Set to instance variables to use this later
1709	Importing sklearn libraries
1813	summarize history for loss
427	first column only
331	read in the images
1579	colunms new features
1002	Custom Feature Pipeline
125	Load and Preparation
1330	Encodes a list of BlockArgs to a list of strings
941	Evaluate the objective function
444	Latitude and Longitude
1654	Correlation with Spearman
1379	If the object is ok to warp
155	Plotting download rate evolution over the day
656	COMBINE TRAIN AND TEST DATA
201	Neatmap of current working directory
882	Check for Class Imbalance
1463	CNN Model for multiclass classification
1357	Find Best Weight
1401	Explore the series
1092	Applying CRF seems to have smoothed their prediction
422	Distribution of meter reading for the hour
179	now we can get the mean price by category
1249	parse trials and create submission file
278	Word Cloud visualization
244	Filter Andorra , run the Linear Regression workflow
627	Groping by day
1659	Pass neutral tweets
1090	Encodes predicted probabilities and targets based on confidence threshold
414	Check for Class Imbalance
1304	Delete to reduce memory usage
827	Read the image on how to visualize it
248	Drop nuisance columns
1702	Evaluate the candidates
1070	Convert to RGB
337	Train the model
420	Distribution of meter type
212	compute the mean value of each feature
397	Random Forest Classifier
1200	Plot RMSE
778	drop high correlation columns
1797	Plot rolling statistics
1477	batch by batch
