451	Dew Temperature
1487	Let 's take a look at the patients
313	Calculate ROC AUC
1401	Let 's take a look at the numeric features
1269	Define input layer
513	Let 's take a look at the image
74	Seed everything
1355	Let 's take a look at the numeric features
528	Let 's define the parameters for the model .
1360	Let 's take a look at the numeric features
1133	Let 's take a look at the data
853	Fitting the model on the test set
1327	Load the data
1181	Preprocess the image
321	Let 's take a look at the data
872	Remove low information features
145	Let 's take a look at the data
525	Let 's take a look at the mean squared error of y_test
1208	Let 's take a look at the target variable .
984	Import Libraries
1190	Let 's calculate the learning rate
1017	Let 's take a look at the images
1351	Group Battery
581	Let 's take a look at the spain cases
9	Let 's check for null values
1088	Let 's take a look at the video
904	Let 's take a look at the categorical features
690	Let 's look at the DICOM files
864	Let 's take a look at the types of the object
1280	Let 's take a look at the data
454	Let 's take a look at the data
160	Let 's take a look at the distribution of isFraud
801	Let 's take a look at the hyperparameters
1586	Let 's take a look at market and news data
398	Let 's check the version of the data
188	Top 10 brands
495	Load the data
711	Let 's check the target variable
980	First DICOM
1271	Let 's load the raw training dataset .
1427	Let 's take a look at the data
692	TTA Concatenation
972	Let 's read the first DICOM file
1101	Loading the data
1501	Seed everything
854	Let 's take a look at the parameters
770	Absolute latitude and longitude difference
57	Let 's check the error of the model .
734	Create the model
380	Voting Regressor
809	First , let 's find the best solution to the problem .
1440	Load the data
930	In this competition , I 'll create a new model with 6 hidden layers .
1143	Let 's check the number of unique values for each column
848	Let 's check the distribution of the learning rate .
1273	Let 's check the oversampled training dataset
408	Exploratory Data Analysis
18	Load the data
421	Confusion matrix
382	Import Libraries
1345	Exploratory Data Analysis
884	Let 's take a look at the correlation matrix
86	Let 's calculate age category
192	Word Cloud
953	Load the data
785	Fare Amount Since Start of Records
367	Let 's take a look at the image
237	Let 's take a look at the output of the model .
1122	Importing the necessary libraries
474	Fitting the model
316	Load the test set
1013	Apply convolution to the signal
718	Let 's take a look at the correlation between the two features .
923	CNT_CHILDREN - CNT_CHILDREN - CNT_CHILDREN - CNT_CHILDREN
736	Let 's take a look at the results
19	Let 's take a look at the target distribution
24	Let 's take a look at the data
1040	Load the data
137	Let 's check the unique values of each column .
1224	Dropping columns that start with ps_calc_
1228	Logistic Regression
291	Let 's take a look at the number of fitted models
77	Train the model
1465	Let 's sort the data by visitStartTime .
94	Let 's take a look at the words of the text
531	Hour Of The Day
107	Let 's take a look at the pre-processed data
548	Bathroom Count Vs Log Error
335	Fitting the Ridge model
1502	Load the data
265	Bagging Model
405	Let 's take a look at the two images
911	Let 's take a look at the correlations with the above threshold
512	Let 's take a look at the spectrum of the image
129	Let 's check the memory usage of the dataset .
1216	In this notebook , we will use a batch size of 32 .
337	We can see that there are no missing values in the test set . Let 's take a look at the test set .
108	Let 's try TPU
185	Mean price of each category
1585	Load the data
269	Fitting the model
1535	Let 's calculate the distance between the target and the target .
1223	Let 's split the data into train and test sets
989	Let 's take a look at the colors of the bkg
283	Let 's take a look at the fitted model .
1188	Let 's take a look at the images of the patients
1311	Load the data
938	Running the model on the test set
360	Fold Importance
932	Load the data
1516	Let 's take a look at the v2a1 data
294	Let 's take a look at the LB score
762	Submission
1519	T-SNE visualization in 3 dimensions
1164	Let 's take a look at the class counts .
946	Let 's take a look at the coefficients of the test set
276	Let 's take a look at the number of fitted models
751	UMAP and FastICA
383	Setting up the data
610	Set the number of filters and hidden layers
1244	Weekly Sales
676	Import Libraries
483	Vectorization
103	Let 's check the median absolute deviation of the predictions .
752	Random Forest
602	Let 's check the difference between public and private scores .
629	Let 's take a look at the number of bookings and total
624	Let 's take a look at the public and private data
329	Linear SVR
1056	Let 's split the data into train and test sets .
713	Let 's calculate the number of phone and tablets per capita
1194	Split the data into training and validation sets
743	Feature Selection Scores
342	Load the data
247	Let 's take a look at the ensemble
1282	Let 's plot the prediction and actual data .
1474	Select Plate Group
54	Let 's take a look at the test set
48	Let 's take a look at the target variable
1568	Let 's load the data
162	PushOut Medium
925	Let 's take a look at the number of income in each target
728	Average Education by Target and Female Head of Household
150	Let 's create the test generator
1241	The shape of the data set
1587	Highest trading volumes
594	Let 's find the most common words in selected_text
903	Let 's take a look at the correlations of the target variable .
807	Write CSV file for test data
1242	Let 's take a look at the sizes of the stores
747	Let 's take a look at the results of the model .
608	Set max_text_length
166	Number of unique values
1299	Let 's check if there are any missing values .
1344	Let 's take a look at the age of the patient .
344	Training and Validation Loss
843	Feature Importance
1067	Load the test set
988	Let 's take a look at the data
1367	Let 's take a look at the numeric features
959	Load the data
357	Import Libraries
1015	Let 's take a look at the mode of the title .
325	Import Libraries
1540	Missing Values
859	Boosting Type for Random Search
1052	Load the unet model
469	Prediction on test set
1536	Let 's take a look at the previous days .
389	Let 's take a look at the images
744	Let 's calculate the f1 score of the predictions
266	We can see that there are no missing values in the test set . Let 's take a look at the test set .
1176	Let 's take a look at the count of each link .
1554	Load the data
745	Confidence by fold and target
700	Missing Values
260	SGD Model
679	Exploratory Data Analysis
968	Curve for Cases
288	Let 's take a look at the number of fitted models
1506	Let 's create a new dataset .
1386	Let 's take a look at the numeric features
1030	Now that we have the result , we can convert it into a string .
289	Let 's take a look at the number of fitted models .
1486	Sample 4 - Ground-Glass Opacities
596	Let 's load the data
657	Load the data
1372	Let 's take a look at the numeric values of the target column
794	Let 's take a look at the data
261	Fitting the decision tree model
557	Let 's take a look at the data
1527	Let 's check the distribution of assists
446	Let 's take a look at the meter reading
673	Let 's check the coefficient of variation ( CV ) for different categories
1085	First , let 's load the model
660	Let 's take a look at the day distribution
1544	Let us learn on a example
1123	Let 's take a look at the start of the transaction
1132	Let 's take a look at the differences between V319 and V320 .
1546	Save the data
230	Let 's take a look at the number of hidden layers in the model .
1530	Let 's check the distribution of killPlace
678	Let 's take a look at the particles
635	Let 's take a look at the number of trips in each country .
1126	Let 's take a look at the data
1524	Let 's take a look at the predictions
1570	Import Libraries
203	Let 's calculate the zero center of the image
187	Let 's check the price of the first level categories
1108	Let 's take a look at the categorical features
1529	Distribution of headshotKills
857	Let 's take a look at the results
1102	Let 's take a look at the test data
121	Let 's take a look at the correlations of the data
617	Random Forest
1187	Let 's take a look at the test set
1074	Preparation
964	Let 's take a look at the impact of the model .
810	Let 's take a look at the trials
1080	Let 's take a look at the images
571	Load the complete data
318	Submission
296	Fitting the model
1417	Logistic Regression
445	Let 's take a look at meter reading
1005	Densenet
1373	Let 's take a look at the numeric features
837	Let 's take a look at the installments
1218	Let 's check the validation results .
395	Let 's check the number of images in the training set .
1377	Let 's take a look at the numeric features
25	Submission
720	Let 's take a look at the correlation matrix
694	Load the data
622	Let 's check the accuracy of the model .
1173	Let 's set the number of words in each sentence .
505	Let 's take a look at the target variable
1359	Let 's take a look at the numeric features
1347	Non-LIVINGAREA mode
328	Fit the SVR model
282	Let 's take a look at the fitted models
1350	Missing Values
1014	Let 's take a look at the distribution of event_count and game_time
1110	Preparation
444	Let 's check the distribution of meter readings on weekdays
1217	Create Trainers and Evaluate
404	Let 's load the data
486	Vectorization
299	Fitting the model
731	Random Foreval
459	Road encoding
52	Let 's plot the logarithmic logarithmic logarithmic logarithmic logarithmic logarithmic logarithmic logarithmic sum
183	Let 's check the shape of the data
1462	Load and save the model
1454	Let 's take a look at the results
210	Let 's take a look at the feature score
259	Linear SVR
1092	Feature Importance
746	Submission
1562	Vectorization
634	Importing the data
628	Let 's take a look at the number of bookings and total
191	Number of items have no description
902	Let 's check the correlation between the target and the target column .
427	We can see that there are no missing values in the data . We can see that there are no missing values in the data . We can see that there are no missing values in the data .
123	Pulmonary Condition Progression by sex
490	Define the model
827	Fitting the model
1204	Create a multi-layer model
1460	Let 's look at the test set .
1483	Let 's take a look at the patients
436	OneVsRestClassifier
1154	We can see that ` train_end_date ` , ` ` ` , ` ` ` , ` ` ` , ` ` ` , ` ` ` , ` ` ` , ` ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` ` , ` , ` , ` , ` , ` , ` , ` , ` , `
331	Fitting the decision tree model
1156	Let 's convert the seed into an integer
838	Let 's take a look at the data
1370	Let 's take a look at the numeric features
945	Let 's check the column types
72	Let 's check the number of missing values in the data
1494	Let 's take a look at the lift function
434	Split the data into train and test sets
1121	Let 's take a look at the number of neutered and animal types
647	Load the previous model
1470	Importing the necessary libraries
14	Tokenization
420	Let 's take a look at the confusion matrix
662	Let 's create a new feature called ord_1 .
1577	Let 's take a look at the missing values
1575	Let 's take a look at the data
1592	Remove columns that are of type object
960	Let 's take a look at public and private test data
804	Fit the model on the test set
122	Pulmonary Condition Progression by sex
698	Let 's check the number of households without a head
783	Random Forest Prediction
1139	Let 's look at the augmented images
1539	Let 's encode all the categorical features .
598	Gini on perfect submission
773	Minkowski distance
866	Let 's take a look at the features
1212	Let 's split the data into train and test sets
881	Let 's take a look at the number of estimators
126	Hounsfield Units ( HU
364	Type_1 and Type_2
1115	Loading the data
368	Linear Regression
601	Let 's take a look at public and private scores
29	Let 's take a look at the AUC score
1464	Let 's take a look at the order of the data
1024	Let 's use DistilBERT tokenizer
314	Let 's take a look at the predictions
372	Fitting the decision tree model
631	Let 's take a look at the number of unique values in each product .
239	Let 's take a look at the output of the model .
1382	Let 's take a look at the numeric features
651	Let 's take a look at the data
1361	Let 's take a look at the numeric features
517	Let 's take a look at the transaction revenue
63	Let 's take a look at the data
1256	Create an iterator over the JSON files .
958	Submission
1127	Fitting the model on the test set
686	Let 's take a look at the test set
1196	Number of annotators
867	Feature engineering
1239	Let 's check the structure of the data
1065	Load the model and predict the test set
130	Count the number of words in each sentence
319	Let 's create a new file
782	Random Forest
1526	Let 's check the distribution of winPerc
961	Let 's take a look at the number of months in the year .
205	Let 's take a look at the data
1183	Create Data Generator
1391	Let 's take a look at the numeric features
1047	Create folders for training and test data
1053	Create test generator
1482	Let 's take a look at the normal image
1251	Let 's see how many images we have in our dataset .
1393	Let 's take a look at the numeric features
590	Import Libraries
349	We can see that there are no missing values . We can see that there are no missing values .
824	Let 's take a look at the correlation matrix
37	Let 's take a look at the distribution of the age .
89	Preparation
66	Let 's take a look at the data
670	Category of items < 10 \u20B ( top
1352	Remove null columns
829	Let 's see how many features we have to keep .
1545	Load the data
3	Let 's check the data
1480	Fit the model on the test set
797	Import Libraries
415	Let 's take a look at the test set
970	Load the data
1293	Import Libraries
1380	Let 's take a look at the numeric features
787	Fare Amount by Day of week
58	Load the data
1131	Let 's take a look at the data
1534	Sieve Eatosthenes
818	Let 's take a look at the results
1591	Let 's see if there are any missing values in the data
1301	Load test data
921	Split the data into training and validation sets
1262	Import Libraries
584	Load the data
68	Let 's take a look at the data
83	Neutered and neutered
165	Load the data
509	Let 's take a look at the data
621	Ridge Regression
402	Load test data
240	Let 's take a look at the output of the model .
1576	Load the data
401	Load the data
723	Let 's take a look at the ` v18q ` and ` mobilephone
212	Load the data
994	Let 's take a look at the data
1549	Let 's create a new dataset .
441	Let 's check the meter reading
366	Let 's calculate the histogram of the image .
393	Let 's take a look at the data
180	Let 's take a look at the images
15	Let 's see how many sequences we have in our dataset .
655	Save model to file
287	Let 's take a look at the number of fitted models
217	Import Libraries
392	Top level 2 most frequent category
484	Vectorization
727	Let 's take a look at the final features
1062	Let 's take a look at the test set
1560	Vectorization
597	Let 's take a look at the test set
701	Let 's take a look at the number of values in each column .
1514	Let 's check the distribution of Accent and CMRmap
1302	Check for missing values in test set
1260	Let 's take a look at the validation predictions
206	Import Libraries
550	No Of Storeys Vs Log Error
422	Random Forest
706	Let 's take a look at the correlation matrix
1291	Let 's take a look at the test data
461	Let 's split the data into train and test sets
136	Number of unique values
104	Let 's see if we can detect the face in this frame
109	Augmentation Pipeline
1310	Light GBM
1210	Missing Values
341	Let 's calculate the IoU of the model .
985	Let 's split the data into train and test sets .
51	Let 's take a look at the number of unique values in each column .
246	Load the data
354	Let 's take a look at the correlation matrix
894	Average Term of Previous Credit
1437	Let 's take a look at the next click time .
1027	Build the model
1463	Let 's take a look at the data
4	Load the data
1240	Let 's take a look at the data
1476	Let 's take a look at the data
1365	Let 's take a look at the numeric features
879	Let 's check the score of the Reg Lambda and Alpha
216	Linear SVR
1238	Create Submission file
1077	Let 's take a look at the data
396	Let 's split the test set into train and test sets
1076	Let 's split the data into train and test sets .
609	Embedding
142	Let 's see which columns are categorical or continuous .
1289	Train and Test Split
920	Load the model
69	Let 's calculate the distance between the tour and the target .
577	China and non-China countries
378	We can see that there are no missing values in the test set . Let 's take a look at the test set .
21	Let 's check the distribution of wheezy-copper-turtle-magic .
304	Let 's calculate the f1 score of the model .
1409	Let 's check for missing values .
863	Add missing values to app_train and app_test
1049	Let 's split the data into train and test sets
674	Load the data
1324	Let 's split the data into train and test sets
148	Let 's take a look at the data
1100	Let 's check the output shape of the test set .
1252	Let 's take a look at the sexo features
842	Let 's split the data into train and test sets
1079	Let 's take a look at the image
1028	Fitting the model
1294	Let 's take a look at the images
410	Let 's see if there are any duplicates in the test set .
575	Let 's take a look at the total number of deals per date .
1050	Let 's take a look at the data
803	Let 's take a look at the data
416	Sales evolution
886	Let 's see the number of boolean variables in app_train
1109	Loading the data
681	Import Libraries
689	Reading DICOM files
271	Let 's take a look at the number of fitted models .
1584	Let 's take a look at the images
232	Let 's take a look at the output of the model .
12	Load the data
1557	Let 's take a look at the first words of the text
213	Let 's take a look at the data
1016	Let 's take a look at the test set
1565	First , let 's calculate the convolution of the signal .
749	Fitting the model
423	Let 's take a look at the confusion matrix
1404	Let 's take a look at the EMA of the close .
1011	Let 's take a look at the image
855	Fitting the model
1254	Import Libraries
675	Let 's check the coefficient of variation ( CV ) for different recognized image categories
1068	Let 's see how many words we have in the test set .
919	Let 's split the data into training and validation sets
117	Let 's take a look at the data
20	Let 's check the count of muggy-smalt-axotl-pem
641	Import Libraries
714	Let 's check the correlation between the two data sets .
606	Import Libraries
952	Let 's split the data into train and test sets
1019	Load the data
677	Let 's take a look at the hits
1493	Import Libraries
466	Let 's take a look at the test images
1396	Let 's take a look at the numeric features
888	Replace missing values with NaNs
974	Let 's take a look at the keyword dictionary
1174	Adding PADs to each sentence
719	Let 's take a look at the correlation matrix
860	Load the data
1453	Load the data
1245	Scatter plot
1118	Let 's take a look at the categorical features
315	Let 's clean up the data
1099	Let 's take a look at the results of the model
1172	Let 's check the number of unique tokens
1036	Preprocess public and private data
428	Fitting the model
814	Boosting Type
523	Let 's check the score of the decision function .
1195	Number of records where toxicity_annotator_count is 1
1140	Load image
350	Import Libraries
224	Let 's take a look at the output of the model .
312	Let 's define path for training and validation set .
244	Let 's take a look at the output of the model .
741	Let 's take a look at the correlation matrix
950	Let 's check the feature types of the new_merchant_
1104	Let 's take a look at the categorical features
1255	BERT and DISTILBERT
1141	Let 's load the Efficient Det model
522	Let 's take a look at the results of the model
937	Let 's take a look at the data
1303	Check if there are any missing values in the test set
1490	Let 's take a look at the data
834	Let 's take a look at the bureau_info
1107	Preparation
487	Let 's convert the text into a word sequence
1177	Let 's load the DICOM file
174	Let 's take a look at the download rate over the day
1010	Save model to file
541	In this notebook , we can see that the training and validation set have the same shape as the test set .
708	Let 's take a look at the data
1202	Fit the model on the test data
1272	Number of repetitions for each class
322	Split the data into training and validation sets
494	Define the model
620	Linear OLS
1200	Let 's create the train and test datasets
195	T-SNE - T-SNE - T-SNE
273	Let 's take a look at the number of fitted models
721	Education by target
426	Import Libraries
653	Random Forest
248	Import Libraries
914	Light GBM
223	Let 's take a look at the output of the model .
1058	KNN logloss on longitude and latitude
1435	Let 's see the number of unique values per IP .
947	Let 's check the input files
965	Let 's take a look at the shap importance
1191	Split the data into training and validation sets
1054	Let 's take a look at the null images
317	Load the model and predict the test set
284	Let 's take a look at the number of fitted models .
1478	Preparation
1097	Let 's see the shape of the test set .
390	Unique level 1 and level 2 and level 3
760	Let 's check the accuracy of the model .
399	Import Libraries
1420	China - China - China - China
214	Create EntitySet
1237	Logistic Regression
1374	Let 's take a look at the numeric features
475	Submission
1326	Let 's see how many categorical features we have .
1499	Let 's take a look at the data
1505	Load GloVe and paragram embeddings
1059	Load the image
582	Let 's take a look at the Iran cases
986	Let 's take a look at the cleaned data
171	Let 's check the ratio of download by click .
30	Submission
71	Load the data
176	Let 's check the memory usage of the dataframe .
869	Load the features
1509	Load test leak data
1265	Let 's see how many variables we have in our model .
36	Load the data
627	Let 's take a look at the number of bookings and total
1116	Let 's take a look at the test data
462	Standard Scaling
1426	Let 's take a look at the data
865	Let 's take a look at the features
478	Import Libraries
1201	Define the model
882	Let 's take a look at the number of estimators
178	Let 's take a look at the mask of the image
1276	Let 's take a look at the data
1103	Preparation
118	Number of data points
957	Predictions for test images
88	Let 's check the performance of the model .
949	Let 's take a look at the merchant data
768	Pickup and dropoff locations
908	Bureau balance by loan
267	AdaBoost
1496	This function is used to evaluate the model on the input image .
384	Now , let 's calculate the des_bw_filter_lp and des_bw_filter_bp
704	Let 's see how many variables we have in the data .
940	Let 's take a look at the data
939	Submission
1358	Let 's take a look at the numeric features
1469	Let 's take a look at the melt sales
1338	Let 's take a look at the number of missing values for each object
931	Let 's take a look at the data
1384	Let 's take a look at the numeric features
177	Let 's take a look at the original image
300	XGBoost
779	Submission
1295	Let 's take a look at the accuracy of the model
476	Let 's check the shape of the data .
1558	Remove stop words
154	Save model to cbm file
551	Let 's take a look at the data
1320	Exploratory Data Analysis
412	Let 's take a look at the image and mask
1309	Load the model
830	Fit the model on the test set
1432	Let 's take a look at h1 and d1 columns
1259	Let 's take a look at the validation set
1444	Load the data
1330	Let 's take a look at the number of missing values
1491	Let 's take a look at the patients
682	Let 's check the shape of the data .
258	Fit the SVR model
1423	Let 's take a look at the model
330	SGD Model
910	Let 's align the test set with the train set .
1421	Prediction with China Data
229	Let 's take a look at the output of the model .
1378	Let 's take a look at the numeric features
1189	Let 's take a look at the data
1039	Let 's take a look at the predictions
765	Let 's take a look at the fare amount
1336	Let 's create a random color generator
1371	Let 's take a look at the numeric features
666	Let 's take a look at the data
298	Let 's calculate the difference between the score and the score of the train and test set .
1155	Import Libraries
1389	Let 's take a look at the numeric features
796	Predicted Test Fare
1513	Let 's take a look at the categorical features
482	Import Libraries
1397	Let 's take a look at the numeric features
1182	Split the data into training and validation sets
901	Let 's take a look at the bureau features
1346	Exploratory Data Analysis
1153	Let 's calculate the rolling mean per store .
320	Let 's take a look at the binary target .
468	Import Libraries
186	First level of categories
1498	Build the model
116	Let 's check the distribution of the whole data
696	Let 's take a look at the data
669	Most common ingredients
912	Remove columns from above_threshold_vars
1032	Let 's take a look at the image
1589	Let 's see how many columns we have in this dataset .
897	Let 's take a look at the features
373	Random Forest
559	Let 's take a look at the images with missing values
979	Let 's check the data
1356	Let 's take a look at the numeric features
544	Let 's take a look at the types of the data
120	Let 's check the difference between FVC and expected FVC .
1458	Let 's take a look at the start and end positions .
1	Import the necessary packages
1006	Fitting the model
1018	Load the data
1449	Let 's take a look at the distribution of IPs .
250	Let 's take a look at the data
1263	BERT and DISTILBERT
23	Tf-idf-vectorization
777	Fitting the model
200	Let 's take a look at the first patient
499	Let 's take a look at the distribution of the target variable .
327	Linear Regression
775	Linear Regression
339	Voting Regressor
375	Split the data into train and validation sets
311	Let 's take a look at the data
235	Let 's take a look at the output of the model .
585	Italy cases by day
179	Let 's take a look at the data
111	Let 's take a look at the data
99	Import Libraries
1114	Let 's calculate the score for each meter reading .
941	Load the data
593	Top 20 common words in selected_text
361	Let 's take a look at the data
35	Import Libraries
281	Let 's take a look at the number of fitted models
275	Let 's take a look at the number of fitted models
1553	Import Libraries
579	Let 's take a look at the number of confirmed cases per day
975	Let 's take a look at the first image
1087	Import Libraries
1277	Random Forest
1523	Let 's take a look at the test set
1481	Prediction
944	Load the data
1392	Let 's take a look at the numeric features
748	Let 's take a look at the trials
705	Let 's take a look at the heads of the data
841	Let 's take a look at the data
10	Let 's check the numeric types
1436	Let 's check the distribution of min and max values .
92	Class Distribution Over Entries
1398	Let 's take a look at the numeric features
11	Let 's take a look at the outliers
847	Boosting type : goss or goss
664	One-Hot Encoding
222	Let 's take a look at the output of the model .
1043	Preprocess the test set
146	Let 's take a random image
438	Let 's take a look at the data
556	Let 's take a look at the text features
1332	Add new category
485	It was the best of times
709	Let 's take a look at the ROOF and ROOF + Floor features
254	Albania
2	Feature engineering
790	Linear Regression
379	AdaBoost
515	Normalize the image
991	Let 's add a cylinder
374	Fitting the model
231	Let 's take a look at the output of the model .
1158	Fit a logistic regression model
41	Load the data
1325	Let 's check which columns have only one value
567	Load the data
836	Load the installments
31	Let 's see how many clusters we have in the test set .
1571	Let 's take a look at the time series
194	Description length VS price
929	Word2Vec - Word2Vec
228	Let 's take a look at the output of the model .
1029	Fitting the model
793	Predictions of Validation Fares
1552	Let 's check the correlation between the train and test set .
233	Let 's take a look at the output of the model .
1209	Let 's take a look at the data
376	Fitting the Ridge model
1329	Import Libraries
1178	Let 's take a look at the data
661	Let 's take a look at the nominal features
1471	Import Libraries
878	Random Search and Bayesian Search
1134	Import Libraries
703	Let 's take a look at the missing values
460	Let 's take a look at the direction of the vehicle .
498	Let 's take a look at the number of unique values in each column
1106	Let 's take a look at the test data
643	Exploratory Data Analysis
739	Submission
1363	Let 's take a look at the numeric features
1147	Let 's take a look at the number of masks in each image
1144	CATEGORICAL FEATURES
190	Let 's take a look at the shipping amount
133	Let 's clean up the data .
1213	Let 's take a look at the data
128	Let 's take a look at the statistics of the segmented data
1159	Fit the model on the test set
38	Let 's take a look at the images
532	Let 's check the order count across the days of the week
80	Get sex and neutered
1117	Preparation
1229	Bernoulli predictions
566	Let 's check the test set
204	Import Libraries
1442	Let 's take a look at the skiplines
687	Let 's split the dataset into train and test dataframes
1012	Let 's take a look at the images
501	Let 's take a look at the correlation matrix of the target variable
381	Fitting the model
510	Let 's take a look at the image
27	Let 's check the data
13	Toxic and Severe toxic
663	Let 's take a look at the time columns
963	We can see that there are no missing values . We can see that there are no missing values
987	Let 's load the images
1387	Let 's take a look at the numeric features
1145	Let 's take a look at the mask
835	Preparation
1411	One-Hot Encoding
543	Import Libraries
1007	Train the model
105	Let 's take a look at the data
1479	Define the model
839	Let 's take a look at the data
1362	Let 's take a look at the numeric features
685	Distribution of target transaction values
346	Let 's take a look at the predictions
28	Let 's take a look at the data
1146	Let 's take a look at the mask
913	Let 's check the shape of the data .
1580	Find all occurence of search_str
518	Let 's calculate the cross-validation score of the model .
308	Let 's take a look at the words
44	Embeddings
391	Let 's take a look at the number of categories in the dataset
954	Let 's split the data into train and test datasets
135	Load the data
1219	Exponential Learning Rate
1171	Let 's take a look at the total words
595	Let 's take a look at the neutral words
285	Let 's take a look at the number of fitted models .
795	Fitting the model
303	Fitting the LGBM model
883	Let 's take a look at the correlation matrix
439	ELECTRICITY OF MEASURED
157	Torch and MMDet
769	Let 's take a look at the map
1414	Missing Values
1093	Let 's take a look at the first 10 variables
352	Let 's take a random sample of the data
1048	Let 's build a new dataframe
808	Let 's find the best solution to the problem .
358	Load the data
49	Let 's take a look at the columns we want to use .
1073	Import Libraries
792	Let 's take a look at some of the features
819	Fitting the model on the full dataset
481	Train the model
1468	Let 's check the total sales of the store .
1537	Let 's take a look at the data
90	Load the data
387	Let 's take a look at the data
1203	Let 's take a look at the data
1075	Let 's check the shape of the data .
772	Load test data
432	Let 's take a look at the word counts of each tag .
127	Let 's take a look at the lung volume
424	Confusion matrix
1199	Let 's create the dataset .
87	Import Libraries
363	Let 's check the number of duplicate clicks with different target values .
455	Let 's take a look at the results of the models
257	Linear Regression
691	Let 's take a look at the boxes and scores
1528	Distribution of DBNOs
1225	Dropping columns that start with ps_calc_
243	Let 's take a look at the output of the model .
1095	Let 's take a look at the errors
1287	Importing the necessary libraries
207	Let 's take a look at the data
1376	Let 's take a look at the numeric features
998	Load the site 4 data
425	Let 's take a look at the image
1457	Seed everything
1466	Import Libraries
488	Let 's take a look at the words in the text
1369	Let 's take a look at the numeric features
1522	Let 's check the f1 score of the data
971	Let 's take a look at the data
1055	Load the data
472	Let 's split the data into training and validation sets
852	Let 's take a look at the best parameters
649	Let 's take a look at the data
592	Let 's take a look at the sentiments
1231	Let 's take a look at the output of the cross validation
542	Let 's see how many birds we have in each row
604	Let 's take a look at public and private scores
464	Load the data
326	Toxic and Severe toxic
371	SGD Model
812	Let 's take a look at the ROC AUC
1381	Let 's take a look at the numeric values of the target column
1510	Create video file
1184	Import Libraries
1511	Create video for one patient
1136	Load the data
1090	Let 's take a look at the validation data
1339	Let 's take a look at the number of missing values for each object
1236	Let 's take a look at the parameters
1060	Predictions on test set
1281	Let 's take a look at the data
1439	Load the data
218	Preparation
97	Load test data
255	Andorra
788	Split data into training and validation set
1003	Let 's take a look at the data
840	Let 's take a look at the data
993	Let 's create a new file
297	Import Libraries
1021	Build the model
917	Let 's load the data
115	Let 's check the unique value of store_id and item_id
552	Augmentation
868	Load correlations
639	Let 's load the data .
1385	Let 's take a look at the numeric features
999	Let 's check the best score of the model .
916	Import Libraries
1198	Let 's split the data into train and test sets
1160	Let 's take a look at the data
885	Let 's split the data into train and test sets
1078	Scale Rotation
644	Let 's take a look at the labels
504	Load the data
93	Let 's remove the null values .
1148	Load the data
359	Now we can create a function that converts the input to the output of the function .
823	Let 's take a look at the data
1425	CovID-19 Prediction
1061	Let 's take a look at the null images
633	Load the data
738	Fitting the model
1448	Let 's take a look at the data
292	Let 's take a look at the number of fitted models
1057	Let 's take a look at the test set
630	Let 's take a look at the data
1422	Let 's take a look at the China data
1321	Let 's split the data into train and test sets
534	Let 's take a look at the number of orders per user
722	Let 's take a look at the target variable
50	Let 's take a look at the number of unique values in each column .
619	Linear Regression
983	Preparation
977	Let 's take a look at the DICOM files
182	Let 's take a look at the run length
236	Let 's take a look at the output of the model .
1246	Weekly Sales and IsHoliday
1150	Load test data
896	Most recent value
430	Let 's encode the categorical features .
1538	Feature engineering
1408	Let 's check if there are any missing values .
95	Word Distribution Over Whole Text
84	Let 's take a look at the distribution of the outcome type .
1459	Let 's take a look at the sentiments
263	Split the data into train and validation sets
967	Logistic Growth Curve
81	Let 's take a look at the breeds
511	Rescaling the image
110	Define the learning rate
407	Let 's take a look at the two images
873	Let 's split the data into train and test sets .
1504	Load the data
417	Load the data
1081	Let 's look at the blurry samples
197	Let 's take a look at the output of neato
1333	Let 's split the data into train and test dataframes .
286	Let 's take a look at the number of fitted models
293	Let 's take a look at the fitted model .
1569	Let 's take a look at the id_error column .
301	Let 's take a look at the number of players in the game
789	Feature engineering
774	Correlations with Fare Amount
78	Unfreeze the model
56	Let 's take a look at the number of zeros in the dataset .
147	Training Rate Reduction
973	Let 's take a look at first dicom file
926	Import Libraries
1441	Number of lines in train.csv
502	Applicatoin train shape
978	Let 's take a look at the output area .
277	Let 's take a look at the number of fitted models
1135	Importing the necessary libraries
1357	Let 's check the distribution of numeric features
219	Let 's take a look at the output of the model .
215	Let 's take a look at the correlation matrix
1563	Latent Dirichlet
506	Let 's take a look at the target
46	Let 's take a look at the target distribution
1312	Load the data
386	Split raw data into train and test set
623	Let 's check the accuracy of the model .
1031	Let 's take a look at the bounding boxes of the target image
1270	Train the model on the training dataset
956	Let 's take a look at the validation data
280	Let 's take a look at the number of fitted models .
60	Let 's take a look at the connected components of the graph
1447	Convert categorical variables to categorical variables
527	Let 's take a look at the data
62	Let 's take a look at the distribution of the productCD
53	Let 's check the number of non-zero values in the dataset .
202	Normalize the image
477	Build and run the model
370	Linear SVR
132	Let 's clean up the text .
568	Let 's take a look at the top 15 features
496	Let 's take a look at the features
1400	Let 's take a look at the numeric values of the target column
114	Let 's split the data into train and price dataframes
1283	Load data from folder
898	Let 's take a look at the test set
353	Create EntitySet
227	Let 's take a look at the output of the model .
1220	Evaluate the model
538	Let 's take a look at the bathrooms
489	It was the best of times and worst of times
948	Check for Null values
519	Let 's see the accuracy of the model .
825	Let 's check the shape of the data .
791	Feature Importance
591	Word cloud
1243	Let 's take a look at the data
1235	Let 's take a look at the columns we want to predict
990	Let 's take a look at the cylinder
253	Germany
309	Let 's check the number of images in the dataset
414	Let 's calculate the histogram of the image .
1083	Let 's load the test set
470	Import Libraries
1551	Let 's take a look at the ` value ` column
900	Let 's align the target matrix with the test matrix .
798	Train and Test sets
1561	Lemmatization
76	F1 score
418	Let 's check the number of clusters in the test set .
969	Load the data
724	Let 's take a look at the range of the target
113	Load the data
680	Import Libraries
1590	Let 's take a look at the data
997	Let 's load the site1 .
1234	Logistic Regression
918	Load the credit card data
702	Check for missing values in v2a1
1366	Let 's take a look at the numeric features
1064	Load the image
874	Import Libraries
1151	Let 's take a look at the distribution of the var_91 variable .
262	Random Forest
1456	Import Libraries
569	Preprocessing and Data Generator
816	Load the data
1521	We can see that there are no missing values in the test set . We can see that there are no missing values in the test set .
1165	Running on TPU
1477	Let 's check the results of the model .
805	Let 's try the TPE algorithm .
1405	Let 's take a look at the data
833	Let 's take a look at the values of the parent variable .
1488	Let 's take a look at the patients
64	T-SNE
726	Remove correlated columns
7	Let 's check the distribution of the feature_1 values .
175	Load the data
43	Let 's take a look at the question_asker_intent_understanding
369	Fit the SVR model
1169	Catagories and Occurrence
158	Import Libraries
61	Let 's take a look at the product codes
570	Import Libraries
1331	Let 's add a new category
636	Let 's take a look at the data
1112	Let 's load the data
516	Let 's take a look at the missing values
124	Importing the necessary libraries
800	Let 's check the distribution of the learning rate .
1364	Let 's take a look at the numeric features
962	Let 's take a look at the feature importance .
1009	Define the model
144	Let 's see the number of unique values in each column .
742	Random Forest
1098	Let 's take a look at the data
1455	Now that we have the result , we can convert it into a string .
1086	Let 's calculate the ROC AUC score
1286	Let 's split the data into train and validation sets
1583	Let 's take a look at the data
549	Room Count Vs Log Error
1503	Save the data
725	We can see that there are no missing values . We can see that there are no missing values .
1434	Train and Test split
1071	Let 's take a look at the input and output objects
1261	Get predictions for test set
754	Fitting a Random Forest
1556	Let 's take a look at the word clouds
431	Let 's take a look at the duplicate pairs
1318	Replace NaNs with 0s .
786	Fare Amount by Hour of Day
1250	Let 's take a look at the number of images in the dataset .
646	Let 's split the data into train and test sets .
1000	Running on TPU
996	Load the submission file
849	Let 's check the number of values between 0.005 and 0.05 .
1431	Age , , , , , , , , , , , , , , , ,
1037	Let 's plot the training history
712	Let 's take a look at the target variable
580	China cases by day
1292	Let 's merge the test set with the test set .
1485	Sample 2 - Lung Opacity and Masses
1149	Let 's take a look at the data
245	Let 's take a look at the score of the train set .
290	Let 's take a look at the number of fitted models .
667	Logistic Regression
1403	Let 's take a look at the data
437	Import Libraries
1489	Increased Vascular Markings + Enlarged Heart
652	Let 's take a look at the data
306	Preparation
615	Check for missing values in the dataframe
1424	Let 's take a look at the data
305	EPOCHS and LR
766	Let 's take a look at the data
181	Let 's find the indices of the two cells in the mask .
1452	Let 's take a look at the data
1543	Let 's take a look at the signal .
821	Load the data
156	Clear Output
1341	Let 's take a look at the number of missing values for each object
168	Let 's check the number of clicks needed to download an app
1008	Load the data
70	Let 's take a look at the results of kopt
1451	Now , let 's take a look at the converted ratio .
1495	Let 's create a function that takes in a list of program names and returns a list of the program names .
611	Load word embeddings
688	Let 's get the filepath for the image
1211	Missing Values
1063	Let 's take a look at the data
1419	Let 's take a look at the rest of the data
642	Let 's split the data into train and test sets
1533	Let 's take a look at the distribution of winPerc and count
1532	Let 's check the correlation between winPlacePerc and winPlacePerc
433	Top 20 tags
1042	Save the best model to file
252	Italy
1520	Predict on test set
1288	Let 's take a look at the correlation matrix of the macro columns .
45	Let 's take a look at the target distribution
1319	Exploratory Data Analysis
943	Let 's load the credit card balance
98	Load the test set
733	Import the necessary packages
555	Scaling the data
34	Let 's check the confusion matrix for identity hate
1253	Let 's check the distribution of the cod_prov .
562	Let 's take a look at the masks
1315	Let 's replace the edjefa value with 1 or 0 .
1089	Import Libraries
966	Growth Rate Over Time
572	First day and Last day
1518	T-SNE ( T-SNE
1581	Load the data
845	LGBM Classifier
1588	Let 's take a look at the unknown assets
125	Load the DICOM files
756	Let 's take a look at the bounding boxes of the pascal voc
889	Let 's take a look at the bureau data
491	Compile the model
1179	Let 's look at the test images
463	Let 's check the data for modelling
1113	Let 's take a look at the leak score
740	Submission
1388	Let 's take a look at the numeric features
211	Import Libraries
39	Let 's take a look at the sex column
893	Let 's take a look at the interesting features
159	Import Libraries
497	Let 's take a look at the bureau_balance
1290	Let 's train the model on the test set
1300	Let 's take a look at the int8 and int16 columns
1247	Weekly Sales
365	Let 's take a look at the data
1170	Let 's check the number of words in each sentence .
992	Let 's take a look at the image
102	Let 's take a random sample of the data
1026	Let 's create the dataset .
546	Let 's take a look at the year buildings
707	Let 's take a look at the number of unique values in each area
1129	Import Libraries
573	Let 's take a look at the number of deals and recovered
1500	Import Libraries
1130	Let 's split the data into train and test sets
781	Let 's take a look at the correlation matrix of the data
1274	Let 's take a look at the bureau features
640	Let 's take a look at the results
536	We can use librosa.onset.onset_strength
184	Top 10 categories
1084	Pre-trained Model
1348	Applicatoin train shape
225	Let 's take a look at the output of the model .
554	Let 's take a look at the categorical features
73	First , let 's import the necessary packages
1041	Let 's take a look at the trials
198	Let 's take a look at the bulge graph
876	Random Search and Bayesian Optimization
465	Load the results
1285	Let 's calculate the squared of each element in the input list .
59	Let 's take a look at the data
659	Let 's check the correlation between the target and the target .
456	Let 's take a look at the data
778	Let 's take a look at the results
1305	Let 's take a look at the categorical features
1433	Let 's take a look at the data
323	Let 's split the data into training and validation sets
933	Let 's split the data into training and validation sets
1304	Let 's check for missing values .
1163	Let 's see how many attributes we have in the dataset .
1517	Let 's take a look at the distribution of the target variable
560	Let 's take a look at the bounding boxes
149	Let 's check the test set
295	Submission
1215	Load test images
710	Let 's check the number of missing values in the training set .
563	Masks over image
832	Let 's take a look at the target variable
1410	Let 's take a look at the features we want to predict
55	Let 's take a look at the number of zeros in each column .
935	Let 's take a look at the data
955	Split the data into train and test sets
887	Let 's see what we can do with this dataset .
735	Fit the model on the test set
507	Reducing the target variable
1119	SexuponOutcome
226	Let 's take a look at the output of the model .
589	Let 's see if we have to plot infection peaks .
138	Let 's see the number of days in each month .
199	Let 's take a look at the output of neato
242	Let 's take a look at the output of the model .
172	Let 's take a look at the missing values
654	Random Forest
411	Let 's take a look at the test set
650	Let 's check the number of missing values for each column
1316	Let 's check the continuous features
338	AdaBoost
256	Dropping missing values
143	Set random seed
279	Let 's take a look at the number of fitted models .
1180	Load the data
875	Let 's take a look at the hyperparameters
343	Let 's check the shape of the data .
75	Let 's take a look at the images
1368	Let 's take a look at the numeric features
1166	Load the data
241	Let 's take a look at the output of the model .
1308	Load the data
1567	Let 's take a look at the data
1512	Importing the required libraries
153	Let 's calculate the f-beta score for the validation set .
449	Year built and year buildings
1412	Let 's take a look at the log values of the target variable
520	Calibrated Classifier
140	Let 's take a look at the features
529	Compile and fit the model
500	Pearson Correlation of Features
776	Split data into training and validation set
1343	Let 's take a look at the number of values in each column .
508	Let 's take a look at the data
1473	Create the base model
1467	Let 's check the total sales of the state .
1034	Run for each image
251	Let 's take a look at the data
802	Let 's take a look at the hyperparameters
440	Let 's check meter reading
524	Let 's calculate the precision and recall score of the model .
131	Remove special characters from text
877	Let 's check the score of the test set .
761	StratifiedKFold
1475	Import Libraries
307	Dropout
5	Let 's take a look at the target distribution
927	Load the data
1314	Let 's replace edjefe with 1 or 0 .
716	Most positively correlated variables
79	Final Submission
942	Bureau Bureau Bureau
806	Let 's take a look at the trials
333	Fitting the model
526	Fit the model on the test set
880	Score as function of learning rate and number of estimators
385	Running the model in parallel
858	Let 's take a look at the data
656	Import Libraries
1296	Let 's plot the loss and validation loss .
799	Fit the baseline model on the test set
612	We define the batch size and the number of epochs
1383	Let 's take a look at the numeric features
981	Let 's take a look at the top bottom image
473	Import Libraries
453	Let 's take a look at the year buildings
1205	Mode by owner and invest
890	Let 's take a look at the bureau balance
1298	Let 's see how many columns we have in our dataset .
995	Let 's take a look at the results
152	Fitting the model
540	Let 's check the correlation between bedrooms and bathrooms
332	Random Forest
683	Let 's check the number of features with all 0 values
1284	Let 's calculate the validation score for the proposed model .
6	Let 's take a look at the target variable
732	Fitting the model
780	Fit and Evaluate
1138	Let 's take a look at the images
1484	Let 's take a look at the data
715	Let 's take a look at the data
345	Load and predict the test set
278	Let 's take a look at the number of fitted models
443	Let 's take a look at the meter reading
547	Bedroom Count Vs Log Error
448	Let 's check the distribution after log tranformation
1297	Number of data per each diagnosis
1507	Load leak data
1023	Fitting the model
1111	Let 's take a look at the categorical features
268	Voting Regressor
1406	Import Libraries
1394	Let 's take a look at the numeric features
310	Load the data
625	Feature 2 : Feature 2 : Feature 2 : Feature 3 : Feature 5 : Feature
447	Let 's take a look at the correlation matrix
553	Load the data
1428	Load the full table
861	Fitting the model on the test set
264	Fitting the Ridge model
1278	Import Libraries
348	Generator
400	Let 's load the data
208	Let 's take a look at the data
565	Create an iterator over the test set .
1038	Load the public and private models
1167	Define the model
684	Let 's check the number of binary features .
672	Let 's take a look at the price of the parent category .
493	Here we define the hidden layer and the fully connected layer .
1555	Let 's check the number of words in each sentence
340	Fitting the model
234	Let 's take a look at the output of the model .
82	Let 's check the distribution of Sex and Outcome Type
826	Let 's split the data into train and test sets
1185	Load the data
193	Let 's check the length of the coms
844	Load the data
605	Let 's try a few times .
1547	Let 's take a look at the data
1279	Let 's check the number of records
1337	Let 's take a look at the number of missing values for each object
397	Let 's check if we have any missing values in the test set .
141	Let 's split the data into train and test sets
1161	Let 's take a look at the data
539	Let 's take a look at the number of bedrooms in the dataset
1390	Let 's take a look at the numeric features
1550	Import Libraries
1002	Let 's take a look at the original images
892	Distribution of Trends in Credit Sum
645	Number of unique label ( unicode_trans.csv
65	Let 's take a look at the data
767	Let 's take a look at the logarithm of the data
1069	Linear and Quadratic Kappa
813	ROC AUC vs iteration
1051	Let 's take a look at the data
561	Let 's take a look at the image
189	Top 10 categories of items with a price of 0
586	Let 's see if we have to run on Sir or Seir
1096	Let 's take a look at the error of the SN_filter
347	Prediction
1375	Let 's take a look at the numeric features
119	Let 's take a look at the expected FVC
1334	Let 's take a look at the data
1438	Import Libraries
695	Unique Values in integer columns
828	Let 's check the shape of the data .
758	Let 's check the distribution of the surface .
1429	Prediction of United States
1193	Preprocess the image
1275	SK_ID_PREV and SK_ID_CURR
632	Let 's take a look at the distribution of the target variable .
1531	Distribution of kills
521	Evaluate threshold
471	Let 's check the shape of the data .
458	Intersection
503	Let 's check the distribution of the missing values .
1035	Load the data
757	Load the data
1192	Load the data
1322	Let 's split the data into train and test sets
750	Poverty Confusion matrix
1418	Importing the necessary packages
1264	Pre-trained model
1157	Let 's take a look at the wins and losses
784	Extract date information from test set
697	Let 's take a look at the households that have the same target .
1168	Import Libraries
638	Import Libraries
1232	Let 's take a look at the output of the cross validation
161	Let 's take a look at the data
1317	Let 's calculate new features per family size .
730	Let 's take a look at the data
169	Let 's take a look at the distribution of the IPs
1120	SexuponOutcome and Neutered Male and Intact Female
479	Submission
1354	Let 's take a look at the numeric features
763	Load the data
139	Let 's take a look at the ` ord_5 ` column .
16	Toxic Predictions
492	Now that we have our data , let 's see how it looks like .
1257	Create validation dataset
394	Let 's take a look at the number of available images
626	Let 's take a look at the number of bookings and total
831	Let 's split the data into train and test sets
1267	Let 's take a look at the results
85	Let 's calculate age in year month week day
334	Split the data into train and validation sets
618	We can see that there is a difference between train and test set . We can see that there is a difference between train and test set .
1307	Random Forest
1323	Let 's split the data into train and test sets
982	Let 's take a look at the validation masks
648	Train the model
1162	Let 's check the number of classes in the dataset .
480	Import Libraries
1313	Missing Values
870	Feature Importance
614	Load the data
1046	Compile the model
403	Time-to-failure vs time-to-failure
377	Bagging Model
665	Let 's take a look at the data
1258	Pre-trained model
1066	Split the data into training and validation sets
759	Replace NaNs with 0s .
1152	Import Libraries
1207	Distribution of investment and owner occupier
419	Decision Tree Classifier
1402	Import Libraries
1559	Lemmatization
1033	Let 's check the output of the model .
891	Let 's take a look at the time features
764	Distribution of Fare
1142	Train the model
22	Let 's take a look at the target variable
1399	Let 's take a look at the numeric values of the target column
600	Let 's calculate the gini of the target
155	Clear Output
1249	Let 's take a look at the number of images in the dataset .
1472	Let 's take a look at the number of plates in each sirna .
164	MinMax + Median Stacking
514	Cropping function
587	Let 's take a look at the target country .
1579	Let 's check the loss of the model .
272	Let 's take a look at the number of fitted models .
1222	Frequent Features
1450	The number of clicks and proportion of downloads by device
905	Let 's see the number of unique values in each group .
40	LightGBM Features
815	Let 's take a look at the count of the boosting_type
820	Import Libraries
151	Split the data into training and validation set
1415	Let 's take a look at the data
1492	Import Libraries
1226	Let 's take a look at the output of the model .
1044	Let 's take a look at the predictions
1206	Let 's take a look at the number of rooms and price
1004	Let 's take a look at the data
163	MinMax + Mean Stacking
1094	Let 's calculate the SNR of the data .
47	Let 's check the logarithm of the target variable .
1137	Image augmentation
220	Let 's take a look at the output of the model .
221	Let 's take a look at the output of the model .
537	Pitches and magnitudes
922	Let 's take a look at the keypoints
545	Let 's take a look at the top 20 features
1578	Let 's check the accuracy of the model .
1328	Prediction
1541	Let 's take a look at the target variable
409	Let 's see if there are any duplicates in the dataset .
106	Load before matrix
1574	Now , let 's take a look at the data
196	Let 's take a look at the bulge graph
435	Vectorization
351	Load the data
924	Let 's take a look at the number of missing values in the target column
457	Top 50 most commmon IntersectionID
637	Let 's create a function that calculates the number of non-zero values in each column .
1342	Let 's take a look at the number of missing values for each object
1515	Let 's take a look at the target values
1542	Let 's take a look at the acoustic data
170	Let 's check the ratio of download by click .
564	Let 's take a look at the results
1221	Load the data
851	Let 's see how many combinations we have in param_grid
951	Let 's split the data into train and test sets .
1582	Let 's take a look at the data
613	Cross-Entropy Loss
1353	Feature engineVersion , AppVersion , AppVersion , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version , App version
936	Exploratory Data Analysis
33	Vectorization
822	Let 's take a look at the previous features
934	Predict on test and validation set
846	Fitting the model on the test set
737	Exploratory Model
1497	Let 's see if there is any difference between a and b .
909	Load test data
1566	Submission
729	Let 's take a look at the data
1128	Let 's take a look at the output of the model .
530	Load the data
1349	Let 's see the overdue values .
1407	Load the data
1445	Load the data
1001	Compile the model
91	Let 's take a look at the number of genes in the dataset
558	Let 's load the mask files
26	LightGBM Features
101	Let 's take a look at the data
976	Get DICOM tag
850	Let 's take a look at the results
96	Load the data
1508	Let 's take a look at the features that are less than 0.7955 .
607	Load the data
616	SVR
32	Load the data
1379	Let 's take a look at the numeric features
1335	Load the data
413	Submission
274	Let 's take a look at the number of fitted models .
599	Gini on random data
1197	Let 's take a look at the distance between mys and target
1072	Import Libraries
1022	Fitting the model
209	Linear Regression
583	Let 's take a look at the USA cases
362	Let 's check the result of the model .
588	We can see that there is a difference between the number of parameters to run and the number of parameters to run sir
167	Number of click by IP
895	Let 's take a look at the features of the app_train dataset
42	Spearman correlation
671	Category of items > 1M \u20BD ( top
388	Let 's take a look at the test set
1573	Let 's take a look at the lagged basic data
1413	Image Data Generator
1230	Let 's take a look at the output of the cross validation
1268	Let 's see how many images we have in our dataset .
406	Let 's take a look at the image
1572	Let 's take a look at the number of visits per day .
249	First , let 's calculate the autocorrelations between the two models .
576	Let 's see the number of confirmed and deaths per country .
1248	Weekly Sales and IsHoliday
1214	Define the neural network
1186	Let 's take a look at the data
1266	Ada optimizer
1125	Let 's take a look at the addresses
856	Random Search Trial
67	Import Libraries
1025	Load the data
1306	Let 's split the data into training and validation sets
467	Let 's take a look at the start time
100	Let 's take a random sample of the data
1105	Loading the data
1461	Let 's check the sentiments of the test set .
603	Public-Private Absolute Difference
1124	Let 's take a look at the addresses .
1564	First , Second , Third , fourth , third , fourth
356	Random Forest
1548	Load GloVe and paragram embeddings
771	Fare Amount by Number of Passengers
928	Let 's check the length of the comment
907	Let 's check the number of bureau and bureau_balance
173	Number of clicks over the day
817	The cross validation score on the full dataset
8	Load the data
533	Hour Of The Day
906	Bureau by loan
1340	Let 's take a look at the number of missing values for each object
1446	Load the data
452	Wind Speed
112	Compile the model
574	Let 's replace China with China
578	Italy
1082	Let 's create a submission file .
915	Top 100 Features
1045	Build the model
753	Exploratory Exploratory
717	Most negative and positive correlations
450	Let 's take a look at the air temperature
693	Import Libraries
535	Import Libraries
429	Let 's take a look at the bayesian blocks
755	Let 's take a look at the image
442	Monthly readings
134	Let 's delete the test and train dataframes
270	Dropout Model
1443	Converted Ratio
811	Bayesian Model
0	Let 's take a look at the target distribution
201	Resampling
668	Let 's take a look at the top 10 labels
658	Let 's take a look at the number of columns in the dataset
238	Let 's take a look at the output of the model .
871	Let 's look at the top 100 features .
1227	Drop columns from train and test set
1020	Let 's create the dataset .
699	There are some households where the family members do not all have the same target
862	LGBM Classifier
336	Bagging Model
1525	Import Libraries
1430	Import Libraries
355	Linear SVR
324	Let 's calculate the cohen kappa score .
302	Let 's define the parameters for the model .
1416	Remove unwanted columns
899	Remove low information features
1091	Let 's define the parameters for the model .
1233	Random Forest
1395	Let 's take a look at the numeric features
1175	Let 's check the number of links and node count for each title .
17	Predictions
1070	Let 's take a look at one of the tasks
