0	import pandas as pd import numpy as np import matplotlib import matplotlib . pyplot as plt import seaborn as sns color = sns . color_palette ( ) import plotly . offline as py py . init_notebook_mode ( connected = True ) from plotly . offline import init_notebook_mode , iplot init_notebook_mode ( connected = True ) import plotly . graph_objs as go import plotly . offline as offline offline . init_notebook_mode ( ) import cufflinks as cf cf . go_offline ( )
1	train_labels = application_train_dummies [ 'TARGET' ] application_train_dummies , application_test_dummies = application_train_dummies . align ( application_test_dummies , join = 'inner' , axis = 1 ) application_train_dummies [ 'TARGET' ] = train_labels print ( 'Training Features shape: ' , application_train_dummies . shape ) print ( 'Testing Features shape: ' , application_test_dummies . shape )
2	from sklearn . experimental import enable_iterative_imputer from sklearn . impute import IterativeImputer from sklearn . ensemble import ExtraTreesRegressor from sklearn . linear_model import BayesianRidge import random
3	from sklearn . ensemble import IsolationForest rs = np . random . RandomState ( 0 ) clf = IsolationForest ( max_samples = 100 , random_state = rs , contamination = .1 ) clf . fit ( imputed_total ) if_scores = clf . decision_function ( imputed_total ) pred = clf . predict ( imputed_total ) imputed_total [ 'anomaly' ] = pred outliers = imputed_total . loc [ imputed_total [ 'anomaly' ] == - 1 ] outlier_index = list ( outliers . index ) print ( imputed_total [ 'anomaly' ] . value_counts ( ) )
4	total = X_new . isnull ( ) . sum ( ) . sort_values ( ascending = False ) percent = ( X_new . isnull ( ) . sum ( ) / X_new . isnull ( ) . count ( ) * 100 ) . sort_values ( ascending = False ) missing_application_train_data = pd . concat ( [ total , percent ] , axis = 1 , keys = [ 'Total' , 'Percent' ] ) missing_application_train_data . head ( 20 )
5	columns_without_id = [ col for col in X_new . columns if col != 'SK_ID_CURR' ] X_new [ X_new . duplicated ( subset = columns_without_id , keep = False ) ] print ( 'The no of duplicates in the data:' , X_new [ X_new . duplicated ( subset = columns_without_id , keep = False ) ] . shape [ 0 ] )
6	import seaborn as sns color = sns . color_palette ( ) plt . figure ( figsize = ( 12 , 5 ) ) plt . title ( "Distribution of AMT_INCOME_TOTAL" ) ax = sns . distplot ( X_new [ "AMT_INCOME_TOTAL" ] )
7	from scipy . stats import boxcox from matplotlib import pyplot np . log ( application_train [ 'AMT_INCOME_TOTAL' ] ) . iplot ( kind = 'histogram' , bins = 100 , xTitle = 'log(INCOME_TOTAL)' , yTitle = 'Count corresponding to Incomes' , title = 'Distribution of log(AMT_INCOME_TOTAL)' )
8	import seaborn as sns color = sns . color_palette ( ) plt . figure ( figsize = ( 12 , 5 ) ) plt . title ( "Distribution of AMT_CREDIT" ) ax = sns . distplot ( application_train [ "AMT_CREDIT" ] )
9	original_train_data = pd . read_csv ( '../input/home-credit-default-risk/application_train.csv' ) contract_val = original_train_data [ 'NAME_CONTRACT_TYPE' ] . value_counts ( ) contract_df = pd . DataFrame ( { 'labels' : contract_val . index , 'values' : contract_val . values } ) contract_df . iplot ( kind = 'pie' , labels = 'labels' , values = 'values' , title = 'Types of Loan' )
10	original_train_data [ "NAME_INCOME_TYPE" ] . iplot ( kind = "histogram" , bins = 20 , theme = "white" , title = "Passenger's Income Types" , xTitle = 'Name of Income Types' , yTitle = 'Count' )
11	original_train_data [ "NAME_TYPE_SUITE" ] . iplot ( kind = "histogram" , bins = 20 , theme = "white" , title = "Accompanying Person" , xTitle = 'People accompanying' , yTitle = 'Count' )
12	( original_train_data [ "DAYS_BIRTH" ] / - 365 ) . iplot ( kind = "histogram" , bins = 20 , theme = "white" , title = "Customer's Ages" , xTitle = 'Age of customer' , yTitle = 'Count' )
13	grp = bureau . groupby ( by = [ 'SK_ID_CURR' ] ) [ 'SK_ID_BUREAU' ] . count ( ) . reset_index ( ) . rename ( columns = { 'SK_ID_BUREAU' : 'BUREAU_LOAN_COUNT' } ) application_bureau = application_bureau . merge ( grp , on = 'SK_ID_CURR' , how = 'left' ) application_bureau [ 'BUREAU_LOAN_COUNT' ] = application_bureau [ 'BUREAU_LOAN_COUNT' ] . fillna ( 0 ) application_bureau_test = application_bureau_test . merge ( grp , on = 'SK_ID_CURR' , how = 'left' ) application_bureau_test [ 'BUREAU_LOAN_COUNT' ] = application_bureau_test [ 'BUREAU_LOAN_COUNT' ] . fillna ( 0 )
14	def isOneToOne ( df , col1 , col2 ) : first = df . drop_duplicates ( [ col1 , col2 ] ) . groupby ( col1 ) [ col2 ] . count ( ) . max ( ) second = df . drop_duplicates ( [ col1 , col2 ] ) . groupby ( col2 ) [ col1 ] . count ( ) . max ( ) return first + second == 2 isOneToOne ( previous_application , 'SK_ID_CURR' , 'SK_ID_PREV' )
15	grp = pos_cash . drop ( 'SK_ID_PREV' , axis = 1 ) . groupby ( by = [ 'SK_ID_CURR' ] ) . mean ( ) . reset_index ( ) prev_columns = [ 'POS_' + column if column != 'SK_ID_CURR' else column for column in grp . columns ] grp . columns = prev_columns
16	df_holdout . drop ( labels = 'pred_lgb' , axis = 1 , inplace = True ) df_tst . drop ( labels = 'pred_lgb' , axis = 1 , inplace = True ) df_corr = df_holdout . corr ( ) df_corr . style . background_gradient ( ) . set_precision ( 2 )
17	frames = [ df_holdout , df_tst ] hold_test = pd . concat ( frames ) ranked_hold_test = hold_test . rank ( axis = 0 ) / hold_test . shape [ 0 ] ranked_hold = ranked_hold_test . loc [ df_holdout . index , : ] ranked_test = ranked_hold_test . loc [ df_tst . index , : ]
18	from sklearn . preprocessing import MinMaxScaler , StandardScaler , PolynomialFeatures def preprocessing ( X , degree ) : poly = PolynomialFeatures ( degree ) scaler = MinMaxScaler ( ) lin_scaler = StandardScaler ( ) poly_df = pd . DataFrame ( lin_scaler . fit_transform ( poly . fit_transform ( scaler . fit_transform ( X ) ) ) ) poly_df [ 'SK_ID_CURR' ] = X . index poly_df . set_index ( 'SK_ID_CURR' , inplace = True , drop = True ) return poly_df
19	clf = LogisticRegression ( ) clf . fit ( ranks_pca_hold . values , y_hold . values . ravel ( ) ) ensemble_holdout = clf . predict_proba ( ranks_pca_hold . values ) [ : , 1 ] ensemble_holdout = ( ensemble_holdout - ensemble_holdout . min ( ) ) / ( ensemble_holdout . max ( ) - ensemble_holdout . min ( ) ) roc_auc_score ( y_hold , ensemble_holdout )
20	wf_hold_test = pd . DataFrame ( pca_hold_test . index ) for feature in pca_hold_test . columns : for predictor in hold_test . columns : col_name = str ( predictor ) + str ( feature ) wf_hold_test [ col_name ] = ( pca_hold_test [ feature ] * hold_test [ predictor ] ) . values wf_hold_test . set_index ( 'SK_ID_CURR' , inplace = True , drop = True ) wf_hold_test . head ( )
21	estimators = [ 'lgb' ] estimator = estimators [ 0 ] j = 0 train_x = ranks_pca_hold train_y = y_hold
22	from sklearn . preprocessing import StandardScaler x = X_all . values scaler = StandardScaler ( ) x_scaled = scaler . fit_transform ( x ) X_all = pd . DataFrame ( x_scaled ) . set_index ( X_all . index )
23	from sklearn . linear_model import Ridge import sklearn . linear_model def ridge ( trn_x , trn_y ) : clf = Ridge ( alpha = 20 , copy_X = True , fit_intercept = True , solver = 'auto' , max_iter = 10000 , normalize = False , random_state = 0 , tol = 0.0025 ) clf . fit ( trn_x , trn_y ) return clf
24	def xgb_predict ( X , model ) : xgb_X = xgb . DMatrix ( X . values ) return model . predict ( xgb_X )
25	data_pass = '/kaggle/input/m5-forecasting-accuracy/' sales = pd . read_csv ( data_pass + 'sales_train_validation.csv' ) calendar = pd . read_csv ( data_pass + 'calendar.csv' ) calendar = reduce_mem_usage ( calendar ) sell_prices = pd . read_csv ( data_pass + 'sell_prices.csv' ) sell_prices = reduce_mem_usage ( sell_prices )
26	def get_s ( drop_days = 0 ) : d_name = [ 'd_' + str ( i + 1 ) for i in range ( 1913 - drop_days ) ] sales_train_val = roll_mat_csr * sales [ d_name ] . values no_sales = np . cumsum ( sales_train_val , axis = 1 ) == 0 sales_train_val = np . where ( no_sales , np . nan , sales_train_val ) weight1 = np . nanmean ( np . diff ( sales_train_val , axis = 1 ) ** 2 , axis = 1 ) return weight1
27	def get_w ( sale_usd ) : total_sales_usd = sale_usd . groupby ( [ 'id' ] , sort = False ) [ 'sale_usd' ] . apply ( np . sum ) . values weight2 = roll_mat_csr * total_sales_usd return 12 * weight2 / np . sum ( weight2 )
28	W_df = pd . DataFrame ( W , index = roll_index , columns = [ 'w' ] ) data_pass = '/kaggle/input/original-weights/' W_original_df = pd . read_csv ( data_pass + 'weights_validation.csv' ) W_original_df = W_original_df . set_index ( W_df . index ) W_original_df [ 'Predicted' ] = W_df . w W_original_df [ 'diff' ] = W_original_df . Weight - W_original_df . Predicted m = W_original_df . Weight . values - W_df . w . values > 0.000001 W_original_df [ m ]
29	file_pass = '/kaggle/working/' sw_df = pd . read_pickle ( file_pass + 'sw_df.pkl' ) S = sw_df . s . values W = sw_df . w . values SW = sw_df . sw . values roll_mat_df = pd . read_pickle ( file_pass + 'roll_mat_df.pkl' ) roll_index = roll_mat_df . index roll_mat_csr = csr_matrix ( roll_mat_df . values ) del roll_mat_df
30	sub = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/sample_submission.csv' ) sub = sub [ sub . id . str . endswith ( 'validation' ) ] sub . drop ( [ 'id' ] , axis = 1 , inplace = True ) DAYS_PRED = sub . shape [ 1 ] dayCols = [ "d_{}" . format ( i ) for i in range ( 1914 - DAYS_PRED , 1914 ) ] y_true = sales [ dayCols ]
31	def centroids ( X , lbls ) : centroids = np . zeros ( ( len ( np . unique ( lbls ) ) , 2 ) ) for l in np . unique ( lbls ) : mask = lbls == l centroids [ l ] = np . mean ( X [ mask ] , axis = 0 ) return centroids
32	C = [ 1757 , 3809 , 511 , 3798 , 625 , 3303 , 4095 , 1283 , 4209 , 1696 , 3511 , 816 , 245 , 1383 , 2071 , 3492 , 378 , 2971 , 2366 , 4414 , 2790 , 3979 , 193 , 1189 , 3516 , 810 , 4443 , 3697 , 235 , 1382 , 4384 , 3418 , 4396 , 921 , 3176 , 650 ]
33	base = os . path . abspath ( '/kaggle/input/m5-forecasting-accuracy/' ) sell_prices = pd . read_csv ( os . path . join ( base + '/sell_prices.csv' ) ) calendar = pd . read_csv ( os . path . join ( base + '/calendar.csv' ) ) sales_train_validation = pd . read_csv ( os . path . join ( base + '/sales_train_validation.csv' ) ) submission_file = pd . read_csv ( os . path . join ( base + '/sample_submission.csv' ) )
34	itx = items . T . iloc [ : 5 ] . reset_index ( ) plt . figure ( ) ax = andrews_curves ( itx , class_column = 'index' , colormap = 'cubehelix' ) ax . spines [ 'top' ] . set_visible ( False ) ax . spines [ 'right' ] . set_visible ( False ) ax . grid ( color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) plt . show ( )
35	plt . figure ( ) ax = autocorrelation_plot ( items . T . iloc [ 1 ] ) ax . grid ( color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) plt . show ( )
36	plt . figure ( ) ax = lag_plot ( items . T . iloc [ 1 ] ) ax . grid ( color = 'grey' , linestyle = '-' , linewidth = 0.25 , alpha = 0.5 ) plt . show ( )
37	train_file = '../input/train.csv' test_file = '../input/test.csv' train = pd . read_csv ( train_file , index_col = 'ID_code' ) X_test = pd . read_csv ( test_file , index_col = 'ID_code' )
38	trainImgPath = "/kaggle/input/severstal-steel-defect-detection/train_images/" trainCsv = "/kaggle/input/severstal-steel-defect-detection/train.csv" dfFull = pd . read_csv ( trainCsv ) dfFullEncodedOnly = dfFull [ ~ dfFull [ 'EncodedPixels' ] . isnull ( ) ] print ( dfFullEncodedOnly . shape ) print ( dfFull . shape ) timin ( )
39	not_null_sex = train [ train [ 'sex' ] . notnull ( ) ] . reset_index ( drop = True ) nan_sex = train [ train [ 'sex' ] . isnull ( ) ] . reset_index ( drop = True )
40	def create_dist ( df , title ) : fig = plt . figure ( figsize = ( 15 , 6 ) ) x = df [ "age_approx" ] . value_counts ( normalize = True ) . to_frame ( ) x = x . reset_index ( ) ax = sns . barplot ( data = x , y = 'age_approx' , x = 'index' ) ax . set ( xlabel = 'Age' , ylabel = 'Percentage' ) ax . set ( title = title ) ;
41	train_images_dir = '../input/siim-isic-melanoma-classification/train/' train_images = listdir ( train_images_dir ) test_images_dir = '../input/siim-isic-melanoma-classification/test/' test_images = listdir ( test_images_dir )
42	def plot_diagnosis ( skin_lesion ) : fig = plt . figure ( figsize = ( 12 , 6 ) ) for i in range ( 0 , 6 ) : image = train [ train [ 'diagnosis' ] == skin_lesion ] . reset_index ( drop = True ) [ 'image_name' ] [ i ] ds = pydicom . dcmread ( train_images_dir + image + '.dcm' ) fig . add_subplot ( 2 , 3 , i + 1 ) plt . imshow ( ds . pixel_array ) plt . suptitle ( skin_lesion . upper ( ) )
43	test_x = test [ [ 'image_name' ] ] test_x [ 'image_name' ] = test_x [ 'image_name' ] . apply ( lambda x : x + '.jpg' )
44	train = pd . read_csv ( '/kaggle/input/liverpool-ion-switching/train.csv' ) submission = pd . read_csv ( '/kaggle/input/liverpool-ion-switching/sample_submission.csv' ) test = pd . read_csv ( '/kaggle/input/liverpool-ion-switching/test.csv' )
45	MAX_roll = 6 def movingaverage ( df ) : df [ 'cummax' ] = df [ 'signal' ] . cummax ( ) df [ 'cummin' ] = df [ 'signal' ] . cummin ( ) for i in range ( 2 , MAX_roll ) : df [ 'MA_{}' . format ( i ) ] = df [ 'signal' ] . rolling ( window = i ) . mean ( ) df . fillna ( - 999 , inplace = True ) df . reset_index ( drop = True , inplace = True ) return df
46	import pandas as pd import warnings warnings . filterwarnings ( 'ignore' ) train = pd . read_csv ( "../input/train.csv" ) train . head ( )
47	import seaborn as sns correlations = train . corr ( ) sns . heatmap ( correlations )
48	for store_id in STORES_IDS : print ( 'Training Store ' , store_id ) grid_df , features_columns = get_data_by_store ( store_id , START_TRAIN ) dbunch = create_dbunch ( grid_df ) del grid_df gc . collect ( ) seed_everything ( SEED ) learner = create_learner ( dbunch ) predict_store ( learner )
49	feats = list ( all_preds ) feats . remove ( 'id' ) for feat in feats : all_preds [ feat ] = np . round ( all_preds [ feat ] . values * 1.00 , 4 )
50	plt . figure ( figsize = ( 10 , 8 ) ) sns . distplot ( train . price_doc . values , bins = 60 , kde = True ) plt . xlabel ( 'Price' , fontsize = 12 ) plt . show ( )
51	plt . figure ( figsize = ( 10 , 8 ) ) sns . distplot ( np . log ( train . price_doc . values ) , bins = 60 , kde = True ) plt . xlabel ( 'Price' , fontsize = 12 ) plt . show ( )
52	data_train [ 'num_room' ] . fillna ( data_train [ "num_room" ] . mean ( ) , inplace = True ) data_test [ 'num_room' ] . fillna ( data_train [ "num_room" ] . mean ( ) , inplace = True ) data_train . head ( ) data_test . head ( )
53	C_mat = market_train_df . corr ( ) fig = plt . figure ( figsize = ( 15 , 15 ) ) sb . heatmap ( C_mat , vmax = 0.5 , square = True , annot = True ) plt . show ( )
54	def process_merged_data ( df ) : df = df . dropna ( ) features = [ 'time' , 'returnsClosePrevRaw1' , 'returnsOpenPrevRaw1' , 'returnsClosePrevMktres1' , 'returnsOpenPrevMktres1' , 'returnsClosePrevRaw10' , 'returnsOpenPrevRaw10' , 'returnsClosePrevMktres10' , 'returnsOpenPrevMktres10' ] x = df [ features ] y = df [ [ 'time' , 'returnsOpenNextMktres10' ] ] return x , y market_data_no_outlier , market_data_no_outlier_target = process_merged_data ( market_data_no_outlier )
55	features = [ 'returnsClosePrevRaw1' , 'returnsOpenPrevRaw1' , 'returnsClosePrevMktres1' , 'returnsOpenPrevMktres1' , 'returnsClosePrevRaw10' , 'returnsOpenPrevRaw10' , 'returnsClosePrevMktres10' , 'returnsOpenPrevMktres10' ] temp_show = market_data_no_outlier_scaled [ features ] temp_show [ 'target' ] = market_data_no_outlier_target [ 'returnsOpenNextMktres10' ] C_mat = temp_show . corr ( ) fig = plt . figure ( figsize = ( 15 , 15 ) ) sb . heatmap ( C_mat , vmax = 0.5 , square = True , annot = True ) plt . show ( ) del temp_show
56	from keras . callbacks import ModelCheckpoint , EarlyStopping early_stopping = EarlyStopping ( monitor = 'val_loss' , patience = 3 , verbose = 1 , mode = 'auto' , restore_best_weights = True ) callbacks_list = [ early_stopping ]
57	def make_my_prediction ( x ) : my_pred = ( model . predict ( x ) ) . reshape ( 1 , - 1 ) [ 0 ] my_pred [ my_pred > 0 ] = 1 my_pred [ my_pred < 0 ] = - 1 return my_pred
58	df_train . dropna ( subset = [ "song_length" ] , inplace = True ) df_train . dropna ( subset = [ "language" ] , inplace = True )
59	path = Path ( '../input' ) train_df = pd . read_csv ( path / 'train.csv' ) var_names = [ col for col in train_df if 'var_' in col ]
60	def count_dist_peaks ( series , bins , prominence , width ) : count , division = np . histogram ( series , bins = bins ) peaks , props = find_peaks ( count , prominence = prominence , width = width ) return peaks
61	def make_histogram ( col ) : plt . figure ( figsize = ( 15 , 5 ) ) plt . subplot ( 121 ) ; plt . title ( col + "_histogram" + "_Original" ) sns . distplot ( train_df [ col ] . dropna ( ) , kde = False ) ; plt . subplot ( 122 ) ; plt . title ( col + "_histogram" + "_groupby" ) sns . distplot ( train_df_groupby [ col ] . dropna ( ) , kde = False ) ;
62	train_df_num_list = train_df_num . columns . tolist ( ) for i in train_df_num_list : make_histogram ( i )
63	import json import os import sys import numpy as np import pandas as pd from tqdm import tqdm import matplotlib . pyplot as plt import cv2
64	f , axarr = plt . subplots ( 2 , 2 ) img1 = cv2 . imread ( '/kaggle/input/plant-pathology-2020-fgvc7/images/Train_0.jpg' ) img2 = cv2 . imread ( '/kaggle/input/plant-pathology-2020-fgvc7/images/Train_1.jpg' ) img3 = cv2 . imread ( '/kaggle/input/plant-pathology-2020-fgvc7/images/Train_2.jpg' ) img4 = cv2 . imread ( '/kaggle/input/plant-pathology-2020-fgvc7/images/Train_3.jpg' ) axarr [ 0 , 0 ] . imshow ( img1 ) axarr [ 0 , 1 ] . imshow ( img2 ) axarr [ 1 , 0 ] . imshow ( img3 ) axarr [ 1 , 1 ] . imshow ( img4 )
65	f , axarr = plt . subplots ( 2 , 2 ) img1 = cv2 . imread ( '/kaggle/input/plant-pathology-2020-fgvc7/images/Test_10.jpg' ) img2 = cv2 . imread ( '/kaggle/input/plant-pathology-2020-fgvc7/images/Test_1005.jpg' ) img3 = cv2 . imread ( '/kaggle/input/plant-pathology-2020-fgvc7/images/Test_101.jpg' ) img4 = cv2 . imread ( '/kaggle/input/plant-pathology-2020-fgvc7/images/Test_1.jpg' ) axarr [ 0 , 0 ] . imshow ( img1 ) axarr [ 0 , 1 ] . imshow ( img2 ) axarr [ 1 , 0 ] . imshow ( img3 ) axarr [ 1 , 1 ] . imshow ( img4 )
66	for index , row in data_train . iterrows ( ) : pathname = str ( row [ 'image_id' ] ) + '.jpg' data_train_new . loc [ index , 'image_id' ] = pathname if ( row [ 'healthy' ] == 1 ) : cat = 'healthy' elif ( row [ 'multiple_diseases' ] == 1 ) : cat = 'multiple_diseases' elif ( row [ 'rust' ] == 1 ) : cat = 'rust' else : cat = 'scab' data_train_new . loc [ index , 'Category' ] = cat
67	import os import sys sys . path . append ( "monk_v1/monk/" ) ;
68	gtf . Default ( dataset_path = "/kaggle/input/plant-pathology-2020-fgvc7/images/" , path_to_csv = "trainWithext.csv" , model_name = "resnet18" , freeze_base_network = False , num_epochs = 20 ) ;
69	img_name = "/kaggle/input/plant-pathology-2020-fgvc7/images/Test_0.jpg" ; predictions = gtf . Infer ( img_name = img_name ) ; from IPython . display import Image Image ( filename = img_name )
70	import pandas as pd from tqdm import tqdm_notebook as tqdm from scipy . special import softmax df = pd . read_csv ( "/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv" )
71	model = simple_cnn ( ) model . fit_generator ( gen_flow , validation_data = ( [ X_valid , X_angle_valid ] , y_valid ) , steps_per_epoch = len ( X_train ) / batch_size , epochs = 20 )
72	test_predictions = model . predict ( [ X_test , X_angle_test ] ) pred_df = test [ [ 'id' ] ] . copy ( ) pred_df [ 'is_iceberg' ] = test_predictions pred_df . to_csv ( 'predictions.csv' , index = False ) pred_df . sample ( 3 )
73	fig , axes = create_axes_grid ( 1 , 1 , 20 , 10 ) set_axes ( axes , x_val = [ 0 , len ( df_train ) , 500000 , 100000 ] , y_val = [ - 6 , 12 , 1 , 1 ] ) axes . set_title ( 'Initial Train Signal' ) axes . plot ( df_train [ 'open_channels' ] , color = 'red' , linewidth = 0.8 ) ; axes . plot ( df_train [ 'signal' ] , color = 'darkblue' , linewidth = 0.2 ) ;
74	offset = 2.74 df_train [ 'batch' ] = df_train . index // 500000 df_train [ 'modified_signal' ] = df_train [ 'signal' ] + offset mean_by_channel_per_batch = df_train . groupby ( [ 'batch' , 'open_channels' ] ) [ 'modified_signal' ] . mean ( ) df_train [ 'channel_means' ] = df_train [ [ 'batch' , 'open_channels' ] ] . apply ( lambda x : mean_by_channel_per_batch [ x [ 0 ] , x [ 1 ] ] , axis = 1 )
75	train_dogs_filepaths = [ TRAIN_DIR + dog_file_name for dog_file_name in os . listdir ( TRAIN_DIR ) if 'dog' in dog_file_name ] train_cats_filepaths = [ TRAIN_DIR + cat_file_name for cat_file_name in os . listdir ( TRAIN_DIR ) if 'cat' in cat_file_name ] print ( "Done" )
76	test_img_file_path = train_dogs_filepaths [ 0 ] img_array = cv2 . imread ( test_img_file_path , cv2 . IMREAD_COLOR ) plt . imshow ( img_array ) plt . show ( )
77	img_array_gray = cv2 . imread ( test_img_file_path , cv2 . IMREAD_GRAYSCALE ) plt . imshow ( img_array_gray , cmap = "gray" ) plt . show ( ) print ( img_array_gray . shape )
78	ROW_DIMENSION = 60 COLUMN_DIMENSION = 60 CHANNELS = 3 new_array = cv2 . resize ( img_array_gray , ( ROW_DIMENSION , COLUMN_DIMENSION ) ) plt . imshow ( new_array , cmap = 'gray' ) plt . show ( )
79	print ( "PREPING TRAINING SET" ) train_data = prep_data ( train_images_filepaths ) print ( "\nPREPING TEST SET" ) test_data = prep_data ( test_images_filepaths ) print ( "\nDone" )
80	X_train = np . array ( train_data ) print ( X_train . shape )
81	print ( train_images_filepaths [ : 3 ] ) print ( "\n" ) print ( test_images_filepaths [ : 3 ] )
82	from tensorflow import keras from keras . models import Sequential from keras . layers import Dense , Flatten , Conv2D , Dropout print ( "Import Successful" )
83	dvc_classifier . compile ( loss = keras . losses . binary_crossentropy , optimizer = 'adam' , metrics = [ 'accuracy' ] )
84	dvc_classifier . fit ( X_train , y_train , batch_size = 128 , epochs = 3 , validation_split = 0.2 )
85	for i in range ( 5 , 11 ) : if prediction_probabilities [ i , 0 ] >= 0.5 : print ( 'I am {:.2%} sure this is a Dog' . format ( prediction_probabilities [ i ] [ 0 ] ) ) else : print ( 'I am {:.2%} sure this is a Cat' . format ( 1 - prediction_probabilities [ i ] [ 0 ] ) ) plt . imshow ( arr_test [ i ] ) plt . show ( )
86	import sys import shutil mydir = "/kaggle/working" try : shutil . rmtree ( mydir ) except OSError as e : print ( "Error: %s - %s." % ( e . filename , e . strerror ) )
87	PROJECT_ID = 'kaggle-competitions-project' from google . cloud import bigquery client = bigquery . Client ( project = PROJECT_ID , location = "US" ) dataset = client . create_dataset ( 'bqml_example' , exists_ok = True ) from google . cloud . bigquery import magics from kaggle . gcp import KaggleKernelCredentials magics . context . credentials = KaggleKernelCredentials ( ) magics . context . project = PROJECT_ID table = client . get_table ( "kaggle-competition-datasets.geotab_intersection_congestion.train" ) client . list_rows ( table , max_results = 5 ) . to_dataframe ( )
88	CREATE MODEL IF NOT EXISTS ` bqml_example . model1 ` OPTIONS ( model_type = 'linear_reg' ) AS SELECT TotalTimeStopped_p20 as label , Weekend , Hour , EntryHeading , ExitHeading , City FROM ` kaggle - competition - datasets . geotab_intersection_congestion . train ` WHERE RowId < 2600000
89	SELECT * FROM ML . TRAINING_INFO ( MODEL ` bqml_example . model1 ` ) ORDER BY iteration
90	SELECT * FROM ML . EVALUATE ( MODEL ` bqml_example . model1 ` , ( SELECT TotalTimeStopped_p20 as label , Weekend , Hour , EntryHeading , ExitHeading , City FROM ` kaggle - competition - datasets . geotab_intersection_congestion . train ` WHERE RowId > 2600000 ) )
91	SELECT RowId , predicted_label as TotalTimeStopped_p20 FROM ML . PREDICT ( MODEL ` bqml_example . model1 ` , ( SELECT RowId , Weekend , Hour , EntryHeading , ExitHeading , City FROM ` kaggle - competition - datasets . geotab_intersection_congestion . test ` ) ) ORDER BY RowId ASC
92	df [ 'RowId' ] = df [ 'RowId' ] . apply ( str ) + '_0' df . rename ( columns = { 'RowId' : 'TargetId' , 'TotalTimeStopped_p20' : 'Target' } , inplace = True ) df
93	application_train = pd . merge ( application_train , previous_loan_counts , on = 'SK_ID_CURR' , how = 'left' ) application_train [ 'previous_loan_counts' ] . fillna ( 0 , inplace = True ) application_train . head ( )
94	def plot2x2Array ( image , mask ) : f , axarr = plt . subplots ( 1 , 2 ) axarr [ 0 ] . imshow ( image ) axarr [ 1 ] . imshow ( mask ) axarr [ 0 ] . grid ( ) axarr [ 1 ] . grid ( ) axarr [ 0 ] . set_title ( 'Image' ) axarr [ 1 ] . set_title ( 'Mask' )
95	AUTO = tf . data . experimental . AUTOTUNE try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync ) GCS_DS_PATH = KaggleDatasets ( ) . get_gcs_path ( )
96	IMG_SIZE = 784 BATCH_SIZE = 8 * strategy . num_replicas_in_sync nb_classes = 4
97	path = '../input/plant-pathology-2020-fgvc7/' train = pd . read_csv ( path + 'train.csv' ) train_id = train [ 'image_id' ] train . pop ( 'image_id' ) y_train = train . to_numpy ( ) . astype ( 'float32' ) category_names = [ 'healthy' , 'multiple_diseases' , 'rust' , 'scab' ] root = 'images' images_paths = [ ( os . path . join ( GCS_DS_PATH , root , idee + '.jpg' ) ) for idee in train_id ]
98	train_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) . map ( decode_image , num_parallel_calls = AUTO ) . map ( data_augment , num_parallel_calls = AUTO ) . repeat ( ) . shuffle ( 512 ) . batch ( BATCH_SIZE ) . prefetch ( AUTO ) )
99	def get_model ( ) : base_model = efn . EfficientNetB7 ( weights = 'imagenet' , include_top = False , input_shape = ( IMG_SIZE , IMG_SIZE , 3 ) , pooling = 'avg' ) x = base_model . output predictions = Dense ( nb_classes , activation = "softmax" ) ( x ) return Model ( inputs = base_model . input , outputs = predictions )
100	from tensorflow . keras . preprocessing . image import ImageDataGenerator from tensorflow . keras . callbacks import ModelCheckpoint from tensorflow . keras . callbacks import ReduceLROnPlateau model_name = 'effNetPlants.h5' best_model = ModelCheckpoint ( model_name , monitor = 'val_loss' , verbose = 1 , save_best_only = True , save_weights_only = True , mode = 'min' ) reduce_lr = ReduceLROnPlateau ( monitor = 'val_loss' , factor = 0.5 , verbose = 1 , min_lr = 0.000001 , patience = 6 )
101	print ( "Read in libraries" ) import numpy as np import pandas as pd import matplotlib . pyplot as plt from scipy . optimize import curve_fit from statsmodels . tsa . statespace . sarimax import SARIMAX from statsmodels . tsa . arima_model import ARIMA from random import random
102	print ( "read in train file" ) df = pd . read_csv ( "/kaggle/input/covid19-global-forecasting-week-2/train.csv" , usecols = [ 'Province_State' , 'Country_Region' , 'Date' , 'ConfirmedCases' , 'Fatalities' ] )
103	sns . set_style ( 'whitegrid' ) sns . set ( rc = { 'figure.figsize' : ( 11.7 , 8.27 ) } )
104	cols = train . columns . tolist ( ) print ( "Columns: " , cols ) columns = cols [ 1 : 11 ] + cols [ 56 : ] values = train [ columns ] labels = train [ 'Cover_Type' ] print ( "\nFeatures: " , columns )
105	import lightgbm as lgb params = { 'learning_rate' : 0.05 , 'max_depth' : 13 , 'boosting' : 'gbdt' , 'objective' : 'multiclass' , 'num_class' : 7 , 'metric' : [ 'multi_logloss' ] , 'is_training_metric' : True , 'seed' : 19 , 'num_leaves' : 256 , 'feature_fraction' : 0.8 , 'bagging_fraction' : 0.8 , 'bagging_freq' : 5 , 'lambda_l1' : 4 , 'lambda_l2' : 4 , 'num_threads' : 12 }
106	plt . figure ( figsize = ( 8 , 6 ) ) plt . scatter ( range ( train_df . shape [ 0 ] ) , np . sort ( train_df . trip_duration . values ) ) plt . xlabel ( 'index' , fontsize = 12 ) plt . ylabel ( 'trip duration' , fontsize = 12 ) plt . show ( )
107	plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( train_df . trip_duration . values , bins = 50 , kde = True ) plt . xlabel ( 'trip_duration' , fontsize = 12 ) plt . show ( )
108	plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( np . log ( train_df . trip_duration . values ) , bins = 50 , kde = True ) plt . xlabel ( 'trip_duration' , fontsize = 12 ) plt . show ( )
109	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "pickup_hour" , data = train_df ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'pick up hour' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
110	grouped_df = train_df . groupby ( 'pickup_hour' ) [ 'trip_duration' ] . aggregate ( np . median ) . reset_index ( ) plt . figure ( figsize = ( 12 , 8 ) ) sns . pointplot ( grouped_df . pickup_hour . values , grouped_df . trip_duration . values , alpha = 0.8 , color = color [ 3 ] ) plt . ylabel ( 'median trip duration' , fontsize = 12 ) plt . xlabel ( 'pick up hour' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
111	train_file_path = '../input/train.csv' train_data = pd . read_csv ( train_file_path , sep = ',' )
112	plt . hist ( train_data [ "teacher_number_of_previously_posted_projects" ] , bins = 45 ) plt . xticks ( range ( 0 , 500 , 50 ) ) plt . show ( )
113	plt . hist ( train_data [ "teacher_number_of_previously_posted_projects" ] , bins = [ 0 , 10 , 450 ] ) plt . xticks ( range ( 0 , 500 , 50 ) ) plt . show ( )
114	import tensorflow as tf from tensorflow . python . data import Dataset import numpy as np import sklearn . metrics as metrics
115	import pandas as pd train_file_path = '../input/train.csv' train_data = pd . read_csv ( train_file_path , sep = ',' )
116	my_feature_name = 'teacher_number_of_previously_posted_projects' my_feature = train_data [ [ my_feature_name ] ] my_target_name = 'project_is_approved'
117	N_TRAINING = 160000 N_VALIDATION = 100000 training_examples = train_data . head ( N_TRAINING ) [ [ my_feature_name ] ] . copy ( ) training_targets = train_data . head ( N_TRAINING ) [ [ my_target_name ] ] . copy ( ) validation_examples = train_data . tail ( N_VALIDATION ) [ [ my_feature_name ] ] . copy ( ) validation_targets = train_data . tail ( N_VALIDATION ) [ [ my_target_name ] ] . copy ( )
118	def my_input_fn ( features , targets , batch_size = 1 , shuffle = True , num_epochs = None ) : features = { key : np . array ( value ) for key , value in dict ( features ) . items ( ) } ds = Dataset . from_tensor_slices ( ( features , targets ) ) ds = ds . batch ( batch_size ) . repeat ( num_epochs ) if shuffle : ds = ds . shuffle ( 10000 ) features , labels = ds . make_one_shot_iterator ( ) . get_next ( ) return features , labels
119	learning_rate = 0.00001 def construct_feature_columns ( input_features ) : return set ( [ tf . feature_column . numeric_column ( my_feature ) for my_feature in input_features ] ) my_optimizer = tf . train . GradientDescentOptimizer ( learning_rate = learning_rate ) my_optimizer = tf . contrib . estimator . clip_gradients_by_norm ( my_optimizer , 5.0 ) linear_classifier = tf . estimator . LinearClassifier ( feature_columns = construct_feature_columns ( training_examples ) , optimizer = my_optimizer )
120	batch_size = 10 training_input_fn = lambda : my_input_fn ( training_examples , training_targets [ my_target_name ] , batch_size = batch_size ) predict_training_input_fn = lambda : my_input_fn ( training_examples , training_targets [ my_target_name ] , num_epochs = 1 , shuffle = False ) predict_validation_input_fn = lambda : my_input_fn ( validation_examples , validation_targets [ my_target_name ] , num_epochs = 1 , shuffle = False )
121	training_metrics = linear_classifier . evaluate ( input_fn = predict_training_input_fn ) validation_metrics = linear_classifier . evaluate ( input_fn = predict_validation_input_fn ) print ( "AUC on the training set: %0.2f" % training_metrics [ 'auc' ] ) print ( "AUC on the validation set: %0.2f" % validation_metrics [ 'auc' ] )
122	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) test_ids = test . Id
123	def split_data ( train , y , households , test_percentage = 0.20 , seed = None ) : train2 = train . copy ( ) cv_hhs = np . random . choice ( households , size = int ( len ( households ) * test_percentage ) , replace = False ) cv_idx = np . isin ( households , cv_hhs ) X_test = train2 [ cv_idx ] y_test = y [ cv_idx ] X_train = train2 [ ~ cv_idx ] y_train = y [ ~ cv_idx ] return X_train , y_train , X_test , y_test
124	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) test_ids = test . Id
125	train = "../input/spooky-author-identification/train.csv" test = "../input/spooky-author-identification/test.csv" wv = "../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt" X_train = pd . read_csv ( train , header = 0 , delimiter = "," ) X_test = pd . read_csv ( test , header = 0 , delimiter = "," ) authors = [ 'EAP' , 'MWS' , 'HPL' ] Y_train = LabelEncoder ( ) . fit_transform ( X_train [ 'author' ] )
126	def clean ( X_train , X_test ) : X_train [ 'words' ] = [ re . sub ( "[^a-zA-Z]" , " " , data ) . lower ( ) . split ( ) for data in X_train [ 'text' ] ] X_test [ 'words' ] = [ re . sub ( "[^a-zA-Z]" , " " , data ) . lower ( ) . split ( ) for data in X_test [ 'text' ] ] return X_train , X_test X_train , X_test = clean ( X_train , X_test )
127	images = "Acropolis" if images == "Bridge of Sighs" : IMAGE_1_URL = 'https://upload.wikimedia.org/wikipedia/commons/2/28/Bridge_of_Sighs%2C_Oxford.jpg' IMAGE_2_URL = 'https://upload.wikimedia.org/wikipedia/commons/c/c3/The_Bridge_of_Sighs_and_Sheldonian_Theatre%2C_Oxford.jpg' elif images == "Golden Gate" : IMAGE_1_URL = 'https://upload.wikimedia.org/wikipedia/commons/1/1e/Golden_gate2.jpg' IMAGE_2_URL = 'https://upload.wikimedia.org/wikipedia/commons/3/3e/GoldenGateBridge.jpg' elif images == "Acropolis" : IMAGE_1_URL = 'https://upload.wikimedia.org/wikipedia/commons/c/ce/2006_01_21_Ath%C3%A8nes_Parth%C3%A9non.JPG' IMAGE_2_URL = 'https://upload.wikimedia.org/wikipedia/commons/5/5c/ACROPOLIS_1969_-_panoramio_-_jean_melis.jpg' else : IMAGE_1_URL = 'https://upload.wikimedia.org/wikipedia/commons/d/d8/Eiffel_Tower%2C_November_15%2C_2011.jpg' IMAGE_2_URL = 'https://upload.wikimedia.org/wikipedia/commons/a/a8/Eiffel_Tower_from_immediately_beside_it%2C_Paris_May_2008.jpg'
128	def download_and_resize ( name , url , new_width = 256 , new_height = 256 ) : path = tf . keras . utils . get_file ( url . split ( '/' ) [ - 1 ] , url ) image = Image . open ( path ) image = ImageOps . fit ( image , ( new_width , new_height ) , Image . ANTIALIAS ) return image
129	image1 = download_and_resize ( 'image_1.jpg' , IMAGE_1_URL ) image2 = download_and_resize ( 'image_2.jpg' , IMAGE_2_URL ) plt . subplot ( 1 , 2 , 1 ) plt . imshow ( image1 ) plt . subplot ( 1 , 2 , 2 ) plt . imshow ( image2 )
130	import torch import time import numpy as np import pandas as pd from numba import njit torch . manual_seed ( 20191210 ) N_DAYS = 100 N_FAMILIES = 5000 MAX_OCCUPANCY = 300 MIN_OCCUPANCY = 125 INPUT_PATH = '/kaggle/input/santa-workshop-tour-2019/' OUTPUT_PATH = '' DEFAULT_DEVICE = torch . device ( 'cpu' )
131	MAX_ACTION = 4 SOFT_PENALTY_PER_PERSON = 1000 PENALTY_RAMP_TIME = 2000 BATCH_SIZE = 1000 N_BATCHES = 6000 LR = 0.025 GRADIENT_CLIP = 100.0 MAX_PREFERENCE = 8.5 USE_ADAM = True ADAM_BETA_M = 0.9 ADAM_BETA_V = 0.99 ADAM_EPSILON = 0.000001 MOMENTUM = 0.95
132	main ( ) print ( "DONE" )
133	clf = LGBMClassifier ( n_estimators = 400 , learning_rate = 0.03 , num_leaves = 30 , colsample_bytree = .8 , subsample = .9 , max_depth = 7 , reg_alpha = .1 , reg_lambda = .1 , min_split_gain = .01 , min_child_weight = 2 , silent = - 1 , verbose = - 1 , ) clf . fit ( data_train , y_train , eval_set = [ ( data_train , y_train ) , ( data_valid , y_valid ) ] , eval_metric = 'auc' , verbose = 100 , early_stopping_rounds = 30 )
134	text = " " . join ( train_df . question_text ) wordcloud = WordCloud ( width = 1024 , height = 1024 , margin = 0 ) . generate ( text ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
135	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 8 ) ) ax . set_title ( "Target Status" ) explode = ( 0 , 0.1 ) labels = '0' , '1' ax . pie ( list ( dict ( collections . Counter ( list ( train_df . target ) ) ) . values ( ) ) , explode = explode , labels = labels , autopct = '%1.1f%%' , shadow = True , startangle = 90 )
136	nb_ = MultinomialNB ( ) nb_clf = nb_ . fit ( X = xtrain_tf , y = y_train ) results_df . set_value ( "NB" , "countVectorizer" , accuracy_score ( y_test , nb_clf . predict ( xtest_tf ) ) )
137	rf_clf = RandomForestClassifier ( n_estimators = 25 , max_depth = 15 , random_state = 42 ) rf_clf . fit ( X = xtrain_tf , y = y_train ) results_df . set_value ( "RF" , "countVectorizer" , accuracy_score ( y_test , rf_clf . predict ( xtest_tf ) ) )
138	mlp_clf = MLPClassifier ( solver = 'lbfgs' , alpha = 1e-4 , hidden_layer_sizes = ( 20 , 10 , 2 ) , random_state = 42 ) mlp_clf . fit ( X = xtrain_tf , y = y_train ) results_df . set_value ( "MLP" , "countVectorizer" , accuracy_score ( y_test , mlp_clf . predict ( xtest_tf ) ) )
139	lreg_clf = LogisticRegression ( solver = 'lbfgs' , multi_class = 'multinomial' , random_state = 42 ) lreg_clf . fit ( X = xtrain_tf , y = y_train ) results_df . set_value ( "LREG" , "countVectorizer" , accuracy_score ( y_test , lreg_clf . predict ( xtest_tf ) ) )
140	text_ = train_df . question_text targets_ = train_df . target class GetSentences ( object ) : def __iter__ ( self ) : counter = 0 for sentence_iter in text_ : tmp_sentence = sentence_iter counter += 1 yield tmp_sentence . split ( ) len ( text_ )
141	num_features = 200 min_word_count = 5 num_workers = 4 context = 10 downsampling = 1e-3 get_sentence = GetSentences ( ) model = gensim . models . Word2Vec ( sentences = get_sentence , min_count = min_word_count , size = num_features , workers = 4 ) w2v = dict ( zip ( model . wv . index2word , model . wv . syn0 ) )
142	EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt' def get_coefs ( word , * arr ) : return word , np . asarray ( arr , dtype = 'float32' ) glov = dict ( get_coefs ( * o . split ( " " ) ) for o in open ( EMBEDDING_FILE ) )
143	EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt' def get_coefs ( word , * arr ) : return word , np . asarray ( arr , dtype = 'float32' ) program = dict ( get_coefs ( * o . split ( " " ) ) for o in open ( EMBEDDING_FILE , encoding = "latin1" ) )
144	model = gensim . models . KeyedVectors . load_word2vec_format ( fname = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin' , binary = True ) google = dict ( zip ( model . wv . index2word , model . wv . syn0 ) )
145	fig , axes = plt . subplots ( 1 , 1 , figsize = ( 15 , 8 ) ) plt . ylim ( ( .5 , 1 ) ) axes . set_ylabel ( "Accuracy" ) axes . set_title ( "Traditional Classfieris Results for 67% Training and 23% Testing with Two Types of Embedding" ) results_df [ results_df . index != "RF" ] . plot ( kind = "bar" , ax = axes )
146	for name in dir ( ) : if not name . startswith ( '_' ) : del globals ( ) [ name ] for name in dir ( ) : if not name . startswith ( '_' ) : del locals ( ) [ name ] import gc ; gc . collect ( )
147	num_words = 5000 maxlen = 100 tokenizer = Tokenizer ( num_words = num_words ) tokenizer . fit_on_texts ( list ( X_train ) ) X_train = tokenizer . texts_to_sequences ( X_train ) X_test = tokenizer . texts_to_sequences ( X_test )
148	embedding_size = 300 model = Sequential ( ) model . add ( Embedding ( num_words , embedding_size ) ) model . add ( Bidirectional ( CuDNNGRU ( 64 , return_sequences = True ) ) ) model . add ( GlobalMaxPool1D ( ) ) model . add ( Dense ( 16 , activation = "relu" ) ) model . add ( Dropout ( 0.1 ) ) model . add ( Dense ( 1 , activation = "sigmoid" ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )
149	import pandas as pd import numpy as np import matplotlib . pyplot as plt import collections import warnings from kaggle . competitions import twosigmanews from datetime import datetime from wordcloud import WordCloud warnings . filterwarnings ( 'ignore' ) env = twosigmanews . make_env ( )
150	market_train_full_df = env . get_training_data ( ) [ 0 ] sample_market_df = pd . read_csv ( "../input/marketdata_sample.csv" ) sample_news_df = pd . read_csv ( "../input/news_sample.csv" )
151	fig , axes = plt . subplots ( nrows = 1 , ncols = 1 , figsize = ( 9 , 9 ) ) axes . set_title ( "Daily assetCodes Violin" ) axes . set_ylabel ( "Repetition" ) axes . violinplot ( list ( market_train_full_df . assetCode . value_counts ( ) . values ) , showmeans = False , showmedians = True )
152	from wordcloud import WordCloud wordcloud = WordCloud ( width = 1024 , height = 1024 , margin = 0 ) . generate ( " " . join ( market_train_full_df . assetName ) ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 20 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
153	fig , axes = plt . subplots ( nrows = 1 , ncols = 1 , figsize = ( 9 , 9 ) ) axes . set_title ( "Volume Violin" ) axes . set_ylabel ( "Volume" ) axes . violinplot ( list ( market_train_full_df [ "volume" ] . values ) , showmeans = False , showmedians = True )
154	fig , axes = plt . subplots ( figsize = ( 20 , 10 ) ) axes . set_title ( "Volume" ) axes . set_ylabel ( "volume" ) axes . set_xlabel ( "records" ) axes . plot ( market_train_full_df [ "volume" ] )
155	fig , axes = plt . subplots ( figsize = ( 20 , 10 ) ) axes . set_title ( "Open Price" ) axes . set_ylabel ( "open price" ) axes . set_xlabel ( "records" ) axes . plot ( market_train_full_df [ "open" ] )
156	market_returns_df = pd . concat ( [ market_train_full_df [ "returnsClosePrevRaw1" ] . describe ( ) , market_train_full_df [ "returnsOpenPrevRaw1" ] . describe ( ) , market_train_full_df [ "returnsClosePrevMktres1" ] . describe ( ) , market_train_full_df [ "returnsOpenPrevMktres1" ] . describe ( ) , market_train_full_df [ "returnsClosePrevRaw10" ] . describe ( ) , market_train_full_df [ "returnsOpenPrevRaw10" ] . describe ( ) , market_train_full_df [ "returnsClosePrevMktres10" ] . describe ( ) , market_train_full_df [ "returnsOpenPrevMktres10" ] . describe ( ) , market_train_full_df [ "returnsOpenNextMktres10" ] . describe ( ) ] , axis = 1 ) market_returns_df
157	fig , axes = plt . subplots ( nrows = 1 , ncols = 1 , figsize = ( 15 , 8 ) ) axes . set_title ( "Box Plot" ) axes . set_ylabel ( "Returns" ) market_train_full_df . boxplot ( column = [ 'returnsClosePrevRaw1' , 'returnsOpenPrevRaw1' , "returnsClosePrevRaw10" , "returnsOpenPrevRaw10" , "returnsOpenNextMktres10" ] )
158	fig , axes = plt . subplots ( nrows = 1 , ncols = 1 , figsize = ( 9 , 9 ) , sharey = True ) axes . set_title ( "returnsOpenNextMktres10 violon" ) axes . set_ylabel ( "" ) axes . violinplot ( list ( ( market_train_full_df [ "returnsOpenNextMktres10" ] ) . values ) , showmeans = False , showmedians = True , widths = 0.9 , showextrema = True )
159	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 10 ) ) ax . set_xlabel ( "name" ) ax . set_ylabel ( " news_train_full_df.provider.value_counts().plot(kind=" bar ",legend=" provider ",color=" tan " )
160	text = " " . join ( tmp_list ) . replace ( "'" , "" ) wordcloud = WordCloud ( width = 1024 , height = 1024 , margin = 0 ) . generate ( text ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
161	from collections import Counter tmp_list = [ ] for i in news_train_full_df . head ( 200000 ) . audiences : tmp_list += i . replace ( "{" , "" ) . replace ( "}" , "" ) . replace ( " " , "" ) . split ( "," )
162	text = "" text = " " . join ( list ( news_train_full_df . headlineTag ) ) wordcloud = WordCloud ( width = 1024 , height = 1024 , margin = 0 ) . generate ( text ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 10 ) ) ax . imshow ( wordcloud , interpolation = 'bilinear' ) ax . axis ( "off" ) ax . margins ( x = 0 , y = 0 ) plt . show ( )
163	import os import pandas as pd import numpy as np import json import matplotlib . pyplot as plt import datetime as datetime from datetime import timedelta , date import seaborn as sns import matplotlib . cm as CM import lightgbm as lgb from sklearn import preprocessing from sklearn . metrics import mean_squared_error from sklearn . model_selection import GridSearchCV , train_test_split
164	list_of_devices = train_data . device . apply ( json . loads ) . tolist ( ) keys = [ ] for devices_iter in list_of_devices : for list_element in list ( devices_iter . keys ( ) ) : if list_element not in keys : keys . append ( list_element )
165	train_data . head ( ) train_data [ "revenue" ] = pd . DataFrame ( train_data . totals . apply ( json . loads ) . tolist ( ) ) [ [ "transactionRevenue" ] ]
166	revenue_datetime_df = train_data [ [ "revenue" , "date" ] ] . dropna ( ) revenue_datetime_df [ "revenue" ] = revenue_datetime_df . revenue . astype ( np . int64 ) revenue_datetime_df . head ( )
167	daily_revenue_df = revenue_datetime_df . groupby ( by = [ "date" ] , axis = 0 ) . sum ( ) import matplotlib . pyplot as plt fig , axes = plt . subplots ( figsize = ( 20 , 10 ) ) axes . set_title ( "Daily Revenue" ) axes . set_ylabel ( "Revenue" ) axes . set_xlabel ( "date" ) axes . plot ( daily_revenue_df [ "revenue" ] )
168	import collections tmp_least_10_visitNumbers_list = collections . Counter ( list ( train_data . visitNumber ) ) . most_common ( ) [ : - 10 - 1 : - 1 ] tmp_most_10_visitNumbers_list = collections . Counter ( list ( train_data . visitNumber ) ) . most_common ( 10 ) least_visitNumbers = [ ] most_visitNumbers = [ ] for i in tmp_least_10_visitNumbers_list : least_visitNumbers . append ( i [ 0 ] ) for i in tmp_most_10_visitNumbers_list : most_visitNumbers . append ( i [ 0 ] ) "10 most_common visitNumbers are {} times and 10 least_common visitNumbers are {} times" . format ( most_visitNumbers , least_visitNumbers )
169	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 9 , 5 ) ) ax . set_title ( "Histogram of log(visitNumbers) \n don't forget it is per session" ) ax . set_ylabel ( "Repetition" ) ax . set_xlabel ( "Log(visitNumber)" ) ax . grid ( color = 'b' , linestyle = '-' , linewidth = 0.1 ) ax . hist ( np . log ( train_data . visitNumber ) )
170	traffic_source_df . loc [ traffic_source_df [ "source" ] . str . contains ( "google" ) , "source" ] = "google" fig , axes = plt . subplots ( 1 , 1 , figsize = ( 8 , 8 ) ) traffic_source_df [ "source" ] . value_counts ( ) . head ( 15 ) . plot ( kind = "bar" , ax = axes , title = "source" , rot = 75 , color = "teal" )
171	fig , axes = plt . subplots ( 1 , 2 , figsize = ( 15 , 10 ) ) traffic_source_df [ "keyword" ] . value_counts ( ) . head ( 10 ) . plot ( kind = "bar" , ax = axes [ 0 ] , title = "keywords (total)" , color = "orange" ) traffic_source_df [ traffic_source_df [ "keyword" ] != "(not provided)" ] [ "keyword" ] . value_counts ( ) . head ( 15 ) . plot ( kind = "bar" , ax = axes [ 1 ] , title = "keywords (dropping NA)" , color = "c" )
172	repetitive_users = list ( np . sort ( list ( collections . Counter ( list ( train_data [ "fullVisitorId" ] ) ) . values ( ) ) ) ) "25% percentile: {}, 50% percentile: {}, 75% percentile: {}, 88% percentile: {}, 88% percentile: {}" . format ( np . percentile ( repetitive_users , q = 25 ) , np . percentile ( repetitive_users , q = 50 ) , np . percentile ( repetitive_users , q = 75 ) , np . percentile ( repetitive_users , q = 88 ) , np . percentile ( repetitive_users , q = 89 ) )
173	month = 8 start_date = datetime . date ( 2016 , month , 1 ) end_date = datetime . date ( 2017 , month , 1 ) def daterange ( start_date , end_date ) : for n in range ( int ( ( end_date - start_date ) . days ) ) : yield start_date + timedelta ( n ) dates_month = [ ] for single_date in daterange ( start_date , end_date ) : dates_month . append ( single_date . strftime ( "%Y-%m" ) ) dates_month = list ( set ( dates_month ) ) dates_month
174	tmp_churn_df = pd . DataFrame ( ) tmp_churn_df [ "date" ] = train_data [ "date" ] tmp_churn_df [ "yaer" ] = pd . DatetimeIndex ( tmp_churn_df [ "date" ] ) . year tmp_churn_df [ "month" ] = pd . DatetimeIndex ( tmp_churn_df [ "date" ] ) . month tmp_churn_df [ "fullVisitoId" ] = train_data [ "fullVisitorId" ] tmp_churn_df . head ( )
175	tmp_matrix = np . zeros ( ( 11 , 11 ) ) for i in range ( 0 , 11 ) : k = False tmp_set = [ ] for j in range ( i , 11 ) : if k : tmp_set = tmp_set & set ( intervals_visitors [ j ] ) else : tmp_set = set ( intervals_visitors [ i ] ) & set ( intervals_visitors [ j ] ) tmp_matrix [ i ] [ j ] = len ( list ( tmp_set ) ) k = True
176	A = tmp_matrix mask = np . tri ( A . shape [ 0 ] , k = - 1 ) A = np . ma . array ( A , mask = mask ) fig = plt . figure ( figsize = ( 9 , 9 ) ) ax = fig . add_subplot ( 111 ) ax . set_xlabel ( "interval" ) ax . set_ylabel ( "period" ) cmap = CM . get_cmap ( 'RdBu_r' , 50000 ) cmap . set_bad ( 'w' ) ax . imshow ( A , interpolation = "nearest" , cmap = cmap )
177	revenue_datetime_df = train_data [ [ "revenue" , "date" ] ] . dropna ( ) revenue_datetime_df [ "revenue" ] = revenue_datetime_df . revenue . astype ( np . int64 ) revenue_datetime_df . head ( )
178	df_train [ "transactionRevenue" ] = df_train [ "transactionRevenue" ] . fillna ( 0 ) df_train [ "bounces" ] = df_train [ "bounces" ] . fillna ( 0 ) df_train [ "pageviews" ] = df_train [ "pageviews" ] . fillna ( 0 ) df_train [ "hits" ] = df_train [ "hits" ] . fillna ( 0 ) df_train [ "newVisits" ] = df_train [ "newVisits" ] . fillna ( 0 )
179	df_train , df_test = train_test_split ( df_train , test_size = 0.2 , random_state = 42 ) df_train [ "transactionRevenue" ] = df_train [ "transactionRevenue" ] . astype ( np . float ) df_test [ "transactionRevenue" ] = df_test [ "transactionRevenue" ] . astype ( np . float ) "Finaly, we have these columns for our regression problems: {}" . format ( df_train . columns )
180	predicted_revenue = gbm . predict ( df_test . loc [ : , df_test . columns != "transactionRevenue" ] , num_iteration = gbm . best_iteration ) predicted_revenue [ predicted_revenue < 0 ] = 0 df_test [ "predicted" ] = np . expm1 ( predicted_revenue ) df_test [ [ "transactionRevenue" , "predicted" ] ] . head ( 10 )
181	fig , ax = plt . subplots ( figsize = ( 10 , 16 ) ) lgb . plot_importance ( gbm , max_num_features = 30 , height = 0.8 , ax = ax ) plt . title ( "Feature Importance" , fontsize = 15 ) plt . show ( )
182	d = pydicom . dcmread ( '../input/osic-pulmonary-fibrosis-progression/train/ID00007637202177411956430/19.dcm' ) img = d . pixel_array fig = plt . figure ( figsize = ( 12 , 12 ) ) plt . imshow ( img )
183	from lungmask import mask import SimpleITK as sitk def get_mask ( filename , plot_mask = False , return_val = False ) : input_image = sitk . ReadImage ( filename ) mask_out = mask . apply ( input_image ) [ 0 ] if plot_mask : fig = plt . figure ( figsize = ( 12 , 12 ) ) plt . imshow ( mask_out ) if return_val : return mask_out
184	method = "sklearn_random" start = time . time ( ) train , valid = split ( dataset_path = DATASET_PATH , test_size = TEST_SIZE , stratification = method ) print ( f"Dataset split done for {time.time() - start} seconds" )
185	method = "iterstrat" start = time . time ( ) train , valid = split ( dataset_path = DATASET_PATH , test_size = TEST_SIZE , stratification = method ) print ( f"Dataset split done for {time.time() - start} seconds" )
186	method = "sklearn_stratified" start = time . time ( ) train , valid = split ( dataset_path = DATASET_PATH , test_size = TEST_SIZE , stratification = method ) print ( f"Dataset split done for {time.time() - start} seconds" )
187	print ( "Data Load Stage" ) train_len = training . shape [ 0 ] print ( 'Train shape: {} Rows, {} Columns' . format ( * training . shape ) ) print ( 'Test shape: {} Rows, {} Columns' . format ( * testing . shape ) ) print ( "Combine Train and Test" ) df = pd . concat ( [ training , testing ] , axis = 0 ) del ( training , testing ) ; gc . collect ( ) print ( 'All Data shape: {} Rows, {} Columns' . format ( * df . shape ) ) df . head ( )
188	def get_rmse ( ytest_input = 'ytest' , pred_input = 'pred' ) : n , loss = 0 , 0 reader_ytest = open ( ytest_input , 'r' ) reader_pred = open ( pred_input , 'r' ) for label , pred in tqdm ( zip ( reader_ytest , reader_pred ) ) : n += 1 true_score = float ( label ) pred_score = float ( pred ) loss += np . square ( pred_score - true_score ) reader_ytest . close ( ) reader_pred . close ( ) return np . sqrt ( loss / n )
189	postproc_cols = [ col for col in stage2 . columns if col not in helpers_cols ] for col in postproc_cols : print ( 'Make postprocessing for {}' . format ( col ) ) add_custom_postproc ( stage2 , col , stage2_meta_df , optimal_triplets )
190	import gc import os import librosa import numpy as np import pandas as pd from glob import glob from pathlib import Path from librosa import display import matplotlib . pyplot as plt from scipy . io . wavfile import read from IPython . display import HTML , Audio , display_html from IPython . display import display as display_ipython pd . set_option ( 'display.max_colwidth' , 500 )
191	df = read_sample ( n = 1000 ) doc_text_words = df [ 'document_text' ] . apply ( lambda x : len ( x . split ( ' ' ) ) ) plt . figure ( figsize = ( 12 , 6 ) ) sns . distplot ( doc_text_words . values , kde = True , hist = False ) . set_title ( 'Distribution of text word count of 1000 docs' )
192	short_dist = df [ mask_answer_exists ] . short . apply ( lambda x : "Short answer exists" if len ( x ) > 0 else "Short answer doesn't exist" ) . value_counts ( normalize = True ) plt . figure ( figsize = ( 8 , 6 ) ) sns . barplot ( x = short_dist . index , y = short_dist . values ) . set_title ( "Distribution of short answers in answerable questions" )
193	PROJECT_ID = 'geultto' from google . cloud import bigquery client = bigquery . Client ( project = PROJECT_ID , location = "US" ) dataset = client . create_dataset ( 'bqml_example' , exists_ok = True ) from google . cloud . bigquery import magics from kaggle . gcp import KaggleKernelCredentials magics . context . credentials = KaggleKernelCredentials ( ) magics . context . project = PROJECT_ID
194	table = client . get_table ( "kaggle-competition-datasets.geotab_intersection_congestion.train" ) client . list_rows ( table , max_results = 5 ) . to_dataframe ( )
195	SELECT * FROM ML . TRAINING_INFO ( MODEL ` bqml_example . distance_p20 ` ) ORDER BY iteration
196	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync )
197	with strategy . scope ( ) : model = tf . keras . Sequential ( [ irv . InceptionResNetV2 ( include_top = False , weights = 'imagenet' , input_shape = ( 299 , 299 , 3 ) ) , L . GlobalAveragePooling2D ( ) , L . Dense ( 1 , activation = 'sigmoid' ) ] ) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) model . summary ( )
198	import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns import glob import cv2 import os print ( os . listdir ( "../input" ) )
199	plt . rcParams [ "axes.grid" ] = True f , axarr = plt . subplots ( 6 , 5 , figsize = ( 24 , 22 ) ) curr_row = 0 for i in range ( 30 ) : example = cv2 . imread ( test_list [ i ] ) example = example [ : , : , : : - 1 ] col = i % 6 axarr [ col , curr_row ] . imshow ( example ) if col == 5 : curr_row += 1
200	plt . rcParams [ "axes.grid" ] = True f , axarr = plt . subplots ( 6 , 5 , figsize = ( 24 , 22 ) ) curr_row = 0 for i in range ( 30 ) : example = cv2 . imread ( index_list [ i ] ) example = example [ : , : , : : - 1 ] col = i % 6 axarr [ col , curr_row ] . imshow ( example ) if col == 5 : curr_row += 1
201	plt . rcParams [ "axes.grid" ] = True f , axarr = plt . subplots ( 6 , 5 , figsize = ( 24 , 22 ) ) curr_row = 0 for i in range ( 30 ) : example = cv2 . imread ( train_list [ i ] ) example = example [ : , : , : : - 1 ] col = i % 6 axarr [ col , curr_row ] . imshow ( example ) if col == 5 : curr_row += 1
202	temp = pd . DataFrame ( train_data . landmark_id . value_counts ( ) . head ( 10 ) ) temp . reset_index ( inplace = True ) temp . columns = [ 'landmark_id' , 'count' ] temp
203	temp = pd . DataFrame ( train_data . landmark_id . value_counts ( ) . tail ( 10 ) ) temp . reset_index ( inplace = True ) temp . columns = [ 'landmark_id' , 'count' ] temp
204	zip_path = '../input/train_jpg.zip' with ZipFile ( zip_path ) as myzip : files_in_zip = myzip . namelist ( )
205	with ZipFile ( zip_path ) as myzip : with myzip . open ( files_in_zip [ 3 ] ) as myfile : img = Image . open ( myfile )
206	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
207	from datetime import datetime from os import scandir def convert_date ( timestamp ) : d = datetime . utcfromtimestamp ( timestamp ) formated_date = d . strftime ( '%d %b %Y' ) return formated_date def get_files ( ) : dir_entries = scandir ( 'my_directory/' ) for entry in dir_entries : if entry . is_file ( ) : info = entry . stat ( ) print ( f'{entry.name}\t Last Modified: {convert_date(info.st_mtime)}' )
208	import pandas as pd df_csv = pd . read_csv ( '/kaggle/input/titanic/train.csv' ) df_csv . head ( 2 )
209	from pandas import read_excel my_sheet = 'Sheet1' file_name = 'products_and_categories.xlsx' df = read_excel ( file_name , sheet_name = my_sheet ) print ( df . head ( ) )
210	import sqlite3 connection = sqlite3 . connect ( "myTable.db" )
211	import matplotlib . pyplot as plt import cv2 img = cv2 . imread ( '../input/alaska2-image-steganalysis/Cover/00001.jpg' ) plt . imshow ( img ) plt . show ( )
212	import openslide import skimage . io import random import seaborn as sns import cv2 import pandas as pd import numpy as np import matplotlib import matplotlib . pyplot as plt import PIL import os
213	import matlab . engine eng = matlab . engine . start_matlab ( ) content = eng . load ( "example.mat" , nargout = 1 )
214	import urllib2 from bs4 import BeautifulSoup response = urllib2 . urlopen ( 'http://tutorialspoint.com/python/python_overview.htm' ) html_doc = response . read ( ) soup = BeautifulSoup ( html_doc , 'html.parser' ) strhtm = soup . prettify ( ) print ( strhtm [ : 225 ] )
215	import nibabel as nib import numpy as np img = nib . load ( "../input/trends-assessment-prediction/fMRI_mask.nii" ) img_data = img . get_data ( ) img_data_arr = np . asarray ( img_data )
216	import json f = open ( 'data.json' , ) data = json . load ( f ) for i in data [ 'emp_details' ] : print ( i ) f . close ( )
217	from nilearn import datasets rest_dataset = datasets . fetch_development_fmri ( n_subjects = 20 ) func_filenames = rest_dataset . func confounds = rest_dataset . confounds
218	class DropStringColumns ( TransformerMixin ) : def fit ( self , x , y = None ) : return self def transform ( self , df ) : for col , dtype in zip ( df . columns , df . dtypes ) : if dtype == object : del df [ col ] return df
219	logit_all_features_pipe = Pipeline ( [ ( 'uni' , UnigramPredictions ( ) ) , ( 'nlp' , PartOfSpeechFeatures ( ) ) , ( 'clean' , DropStringColumns ( ) ) , ( 'clf' , LogisticRegression ( ) ) ] ) test_pipeline ( train_df , logit_all_features_pipe )
220	def generate_submission_df ( trained_prediction_pipeline , test_df ) : predictions = pd . DataFrame ( trained_prediction_pipeline . predict_proba ( test_df . text ) , columns = trained_prediction_pipeline . classes_ ) predictions [ 'id' ] = test_df [ 'id' ] predictions . to_csv ( "submission.csv" , index = False ) return predictions
221	import numpy as np import pandas as pd import random import seaborn as sns import matplotlib . pyplot as plt import gc import seaborn as sns sns . set ( style = 'whitegrid' , color_codes = True ) import scipy . stats as st import statsmodels . formula . api as smf from sklearn . ensemble import RandomForestRegressor from sklearn . cross_validation import train_test_split import xgboost as xgb import operator
222	train = pd . merge ( df_train , store , on = "store_nbr" ) train = pd . merge ( train , item , on = "item_nbr" ) train = pd . merge ( train , holiday , on = "date" ) train = pd . merge ( train , oil , on = "date" )
223	train_items = pd . merge ( df_train , item , how = 'inner' ) train_items1 = pd . merge ( df_train , item , how = 'inner' ) train_items2 = pd . merge ( df_train , item , how = 'inner' )
224	train [ 'onpromotion' ] = train [ 'onpromotion' ] . fillna ( 2 ) train [ 'onpromotion' ] = train [ 'onpromotion' ] . replace ( True , 1 ) train [ 'onpromotion' ] = train [ 'onpromotion' ] . replace ( False , 0 )
225	fig , ( axis1 ) = plt . subplots ( 1 , 1 , sharex = True , figsize = ( 15 , 8 ) ) ax1 = oil . plot ( legend = True , ax = axis1 , marker = 'o' , title = "Oil Price" )
226	promo_sales = train [ train [ 'onpromotion' ] == 1.0 ] [ 'unit_sales' ] nopromo_sales = train [ train [ 'onpromotion' ] == 0.0 ] [ 'unit_sales' ] st . ttest_ind ( promo_sales , nopromo_sales , equal_var = False )
227	def R ( n1 , n2 , m , H ) : return ( 1 - .75 ** m ) * H / ( n1 + n2 + m ) H = 500 n1 = 10 n2 = 10 r = [ ] for m in range ( 14 ) : r . append ( R ( n1 , n2 , m , H ) ) plt . title ( 'For total travel = 20 steps' ) plt . xlabel ( 'steps mining' ) plt . ylabel ( 'halite per step' ) plt . plot ( r )
228	opt = [ ] fig = plt . figure ( 1 ) for travel in range ( 30 ) : def h ( mine ) : return - R ( 0 , travel , mine , 500 ) res = scipy . optimize . minimize_scalar ( h , bounds = ( 1 , 15 ) , method = 'Bounded' ) opt . append ( res . x ) plt . plot ( opt ) plt . xlabel ( 'total travel steps' ) plt . ylabel ( 'mining steps' ) plt . title ( 'Optimal steps for mining by total travel' )
229	import os import numpy as np import pandas as pd import warnings from bayes_opt import BayesianOptimization from skopt import BayesSearchCV import matplotlib . pyplot as plt import seaborn as sns import lightgbm as lgb import xgboost as xgb from sklearn . model_selection import train_test_split , StratifiedKFold , cross_val_score import time import sys from sklearn . metrics import roc_auc_score , roc_curve import shap warnings . simplefilter ( action = 'ignore' , category = FutureWarning )
230	explainer = shap . TreeExplainer ( clf ) shap_values = explainer . shap_values ( X ) shap . summary_plot ( shap_values , X )
231	graph = lgb . create_tree_digraph ( clf , tree_index = 3 , name = 'Tree3' ) graph . graph_attr . update ( size = "110,110" ) graph
232	pubs_map = folium . Map ( location = [ 40.742459 , - 73.971765 ] , zoom_start = 12 ) data = [ [ x [ 0 ] , x [ 1 ] , 1 ] for x in np . array ( bars [ [ 'Latitude' , 'Longitude' ] ] ) ] HeatMap ( data , radius = 20 ) . add_to ( pubs_map ) pubs_map
233	city_long_border = ( - 74.03 , - 73.75 ) city_lat_border = ( 40.63 , 40.85 ) ax = plt . scatter ( party [ 'Longitude' ] . values , party [ 'Latitude' ] . values , color = 'blue' , s = 0.5 , label = 'train' , alpha = 0.1 ) ax . axes . set_title ( 'Coordinates of the calls' ) ax . figure . set_size_inches ( 6 , 5 ) plt . ylim ( city_lat_border ) plt . xlim ( city_long_border ) plt . show ( )
234	t4_params = { 'boosting_type' : 'gbdt' , 'objective' : 'multiclass' , 'nthread' : - 1 , 'silent' : True , 'num_leaves' : 2 ** 4 , 'learning_rate' : 0.05 , 'max_depth' : - 1 , 'max_bin' : 255 , 'subsample_for_bin' : 50000 , 'subsample' : 0.8 , 'subsample_freq' : 1 , 'colsample_bytree' : 0.6 , 'reg_alpha' : 1 , 'reg_lambda' : 0 , 'min_split_gain' : 0.5 , 'min_child_weight' : 1 , 'min_child_samples' : 10 , 'scale_pos_weight' : 1 } t4 = lgbm . sklearn . LGBMClassifier ( n_estimators = 1000 , seed = 0 , ** t4_params )
235	plt . figure ( figsize = ( 12 , 6 ) ) plt . title ( 'Number of Team Members' ) tmp = dataset . groupby ( [ 'matchId' , 'groupId' ] ) [ 'Id' ] . agg ( 'count' ) sns . countplot ( tmp )
236	sub = pd . DataFrame ( ) sub [ 'Id' ] = test_df [ 'Id' ] sub [ 'winPlacePerc' ] = lgb_sub_preds sub [ 'winPlacePerc' ] [ sub [ 'winPlacePerc' ] > 1 ] = 1 sub . to_csv ( 'lgb_submission.csv' , index = False )
237	import numpy as np import pandas as pd import os import matplotlib . pyplot as plt from IPython . core . interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = 'all'
238	train_df [ 'hospital_death' ] . dtype test_df [ 'hospital_death' ] . dtype
239	from sklearn . model_selection import train_test_split train = train_df . copy ( ) y = train [ 'hospital_death' ] predictors = train . drop ( [ 'hospital_death' ] , axis = 1 ) X = predictors . select_dtypes ( exclude = [ 'object' ] ) X_train , X_valid , y_train , y_valid = train_test_split ( X , y , train_size = 0.8 , test_size = 0.2 , random_state = 0 ) X_train . shape X_valid . shape
240	from sklearn . impute import SimpleImputer my_imputer = SimpleImputer ( ) imputed_X_train = pd . DataFrame ( my_imputer . fit_transform ( X_train ) ) imputed_X_valid = pd . DataFrame ( my_imputer . transform ( X_valid ) ) imputed_X_train . columns = X_train . columns imputed_X_valid . columns = X_valid . columns
241	threshold = 1 lower = np . percentile ( entropy , threshold ) upper = np . percentile ( entropy , 100 - threshold ) print ( np . min ( entropy ) , np . max ( entropy ) ) print ( lower , upper )
242	import numpy as np import pandas as pd import matplotlib . pyplot as plt import cv2 import seaborn as sns from sklearn . utils import shuffle from sklearn . metrics import confusion_matrix from sklearn . model_selection import train_test_split import itertools import shutil import os print ( os . listdir ( "../input" ) )
243	df = pd . read_csv ( '../input/train_labels.csv' ) print ( 'Shape of DataFrame' , df . shape ) df . head ( )
244	df [ df [ 'id' ] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2' ] df [ df [ 'id' ] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe' ] df . head ( )
245	SAMPLE_SIZE = 80000 df_0 = df [ df [ 'label' ] == 0 ] . sample ( SAMPLE_SIZE , random_state = 0 ) df_1 = df [ df [ 'label' ] == 1 ] . sample ( SAMPLE_SIZE , random_state = 0 ) df_train = pd . concat ( [ df_0 , df_1 ] , axis = 0 ) . reset_index ( drop = True ) df_train = shuffle ( df_train ) df_train [ 'label' ] . value_counts ( )
246	y = df_train [ 'label' ] df_train , df_val = train_test_split ( df_train , test_size = 0.1 , random_state = 0 , stratify = y )
247	import keras from keras . models import Sequential from keras . layers import Conv2D , Dropout , MaxPooling2D , Flatten , Dense from keras . layers . normalization import BatchNormalization from keras . layers . convolutional import SeparableConv2D from keras . layers . core import Activation
248	model = Net . build ( width = 96 , height = 96 , depth = 3 , classes = 2 ) from keras . optimizers import SGD , Adam , Adagrad model . compile ( optimizer = Adam ( lr = 0.0001 ) , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] )
249	model . load_weights ( 'checkpoint.h5' ) val_loss , val_acc = \ model . evaluate_generator ( test_gen , steps = len ( df_val ) ) print ( 'val_loss:' , val_loss ) print ( 'val_acc:' , val_acc )
250	plt . figure ( 1 ) plt . plot ( [ 0 , 1 ] , [ 0 , 1 ] , 'k--' ) plt . plot ( fpr_keras , tpr_keras , label = 'area = {:.2f}' . format ( auc_keras ) ) plt . xlabel ( 'False positive rate' ) plt . ylabel ( 'True positive rate' ) plt . title ( 'ROC curve' ) plt . legend ( loc = 'best' ) plt . show ( )
251	test_dir = 'test_dir' os . mkdir ( test_dir ) test_images = os . path . join ( test_dir , 'test_images' ) os . mkdir ( test_images ) os . listdir ( 'test_dir' )
252	test_filenames = test_gen . filenames df_preds [ 'file_names' ] = test_filenames def extract_id ( x ) : a = x . split ( '/' ) b = a [ 1 ] . split ( '.' ) extracted_id = b [ 0 ] return extracted_id df_preds [ 'id' ] = df_preds [ 'file_names' ] . apply ( extract_id ) df_preds . head ( )
253	submission = pd . DataFrame ( { 'id' : image_id , 'label' : y_pred , } ) . set_index ( 'id' ) submission . to_csv ( 'submission.csv' , columns = [ 'label' ] )
254	f , ax = plt . subplots ( figsize = ( 13 , 13 ) ) corr = numerical_features . corr ( ) sns . heatmap ( corr , mask = np . zeros_like ( corr , dtype = np . bool ) , cmap = sns . diverging_palette ( 220 , 10 , as_cmap = True ) , square = True , ax = ax )
255	fig = plt . figure ( figsize = ( 12 , 6 ) ) sns . countplot ( train [ 'bathrooms' ] , ax = plt . subplot ( 121 ) ) ; plt . xlabel ( 'NB of bathrooms' , fontsize = 13 ) ; plt . ylabel ( 'NB of listings' , fontsize = 13 ) ; sns . countplot ( train [ 'bedrooms' ] , ax = plt . subplot ( 122 ) ) ; plt . xlabel ( 'NB of bedrooms' , fontsize = 13 ) ; plt . ylabel ( 'NB of listings' , fontsize = 13 ) ;
256	def submit ( predictions ) : submit = pd . read_csv ( '../input/sample_submission.csv' ) submit [ "target" ] = predictions submit . to_csv ( "submission.csv" , index = False ) def fallback_auc ( y_true , y_pred ) : try : return metrics . roc_auc_score ( y_true , y_pred ) except : return 0.5 def auc ( y_true , y_pred ) : return tf . py_function ( fallback_auc , ( y_true , y_pred ) , tf . double )
257	len_train = df_tr . shape [ 0 ] df_test [ 'target' ] = - 1 data = pd . concat ( [ df_tr , df_test ] ) data [ 'magic_count' ] = data . groupby ( [ 'wheezy-copper-turtle-magic' ] ) [ 'id' ] . transform ( 'count' ) data = pd . concat ( [ data , pd . get_dummies ( data [ 'wheezy-copper-turtle-magic' ] ) ] , axis = 1 , sort = False ) df_tr = data [ : len_train ] df_test = data [ len_train : ]
258	import numpy as np import pandas as pd import os import time import datetime import gc from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import GroupKFold from sklearn . metrics import mean_absolute_error import matplotlib . pyplot as plt import seaborn as sns from tqdm import tqdm_notebook as tqdm from catboost import CatBoostRegressor , Pool import warnings warnings . filterwarnings ( "ignore" )
259	train = pd . read_csv ( '../input/train.csv' ) structures = pd . read_csv ( '../input/structures.csv' ) print ( 'Train dataset shape is -> rows: {} cols:{}' . format ( train . shape [ 0 ] , train . shape [ 1 ] ) ) print ( 'Structures dataset shape is -> rows: {} cols:{}' . format ( structures . shape [ 0 ] , structures . shape [ 1 ] ) )
260	molecules_fraction = 0.1 molecules_amount = int ( molecules_fraction * len ( unique_molecules ) ) np . random . shuffle ( unique_molecules ) train_molecules = unique_molecules [ : molecules_amount ] train = train [ train [ 'molecule_name' ] . isin ( train_molecules ) ] print ( f'Amount of molecules in the subset of train: {molecules_amount}, samples: {train.shape[0]}' )
261	kf = GroupKFold ( 4 ) for tr_idx , val_idx in kf . split ( X , groups = molecules_id ) : tr_X = X . iloc [ tr_idx ] ; val_X = X . iloc [ val_idx ] tr_y = y . iloc [ tr_idx ] ; val_y = y . iloc [ val_idx ] break
262	def catboost_fit ( model , X_train , y_train , X_val , y_val ) : train_pool = Pool ( X_train , y_train ) val_pool = Pool ( X_val , y_val ) model . fit ( train_pool , eval_set = val_pool ) return model model = CatBoostRegressor ( iterations = 20000 , max_depth = 9 , objective = 'MAE' , task_type = 'GPU' , verbose = False ) model = catboost_fit ( model , tr_X , tr_y , val_X , val_y )
263	from eli5 . permutation_importance import get_score_importances def score ( X , y ) : y_pred = model . predict ( X ) return metric ( y , y_pred ) base_score , score_decreases = get_score_importances ( score , np . array ( val_X ) , val_y , n_iter = 1 ) threshold = 0.001 bad_features = val_X . columns [ score_decreases [ 0 ] > - threshold ]
264	forest [ 'Cover_Type' ] . replace ( { 1 : 'Spruce/Fir' , 2 : 'Lodgepole Pine' , 3 : 'Ponderosa Pine' , 4 : 'Cottonwood/Willow' , 5 : 'Aspen' , 6 : 'Douglas-fir' , 7 : 'Krummholz' } , inplace = True ) forest = forest . rename ( columns = { "Wilderness_Area1" : "Rawah_WA" , "Wilderness_Area2" : "Neota_WA" , "Wilderness_Area3" : "Comanche_Peak_WA" , "Wilderness_Area4" : "Cache_la_Poudre_WA" , "Horizontal_Distance_To_Hydrology" : "HD_Hydrology" , "Vertical_Distance_To_Hydrology" : "VD_Hydrology" , "Horizontal_Distance_To_Roadways" : "HD_Roadways" , "Horizontal_Distance_To_Fire_Points" : "HD_Fire_Points" } )
265	fig = px . pie ( forest , names = "Wild Areas" , height = 300 , width = 800 ) fig . show ( ) fig = px . histogram ( forest , x = "Wild Areas" , color = "Cover_Type" , barmode = "group" , height = 400 , width = 800 ) fig . show ( )
266	fig = px . histogram ( forest , x = "Soil types" , color = "Cover_Type" , height = 400 , width = 850 ) fig . show ( ) fig = px . pie ( forest , names = "Soil types" , height = 400 , width = 850 ) fig . update_traces ( textposition = 'inside' ) fig . show ( )
267	fig = px . box ( forest , x = "Cover_Type" , y = "Elevation" , color = "Cover_Type" , height = 400 , width = 900 ) fig . update_layout ( title = { 'text' : "Elevation Box Plot" } ) fig . show ( )
268	fig = px . histogram ( forest , x = "Aspect" , color = "Cover_Type" , marginal = 'rug' , title = "Aspect Histogram" , height = 500 , width = 900 ) fig . show ( )
269	temp = forest . groupby ( [ 'Wild Areas' , 'Cover_Type' ] , as_index = False ) [ [ 'Slope' ] ] . median ( ) fig = px . bar ( temp , x = "Wild Areas" , y = "Slope" , color = 'Cover_Type' , barmode = 'group' , height = 400 , width = 900 ) fig . show ( )
270	fig = px . histogram ( forest , x = "HD_Hydrology" , color = "Cover_Type" , marginal = 'rug' , title = "HD_Hydrology Histogram" , height = 500 , width = 800 ) fig . show ( )
271	fig = px . box ( forest , x = "Cover_Type" , y = "HD_Hydrology" , color = "Cover_Type" , height = 500 , width = 800 ) fig . update_layout ( title = { 'text' : "Horizontal Dis to Hydrology Box Plot" } ) fig . show ( )
272	temp = forest . groupby ( [ 'Wild Areas' , 'Cover_Type' ] , as_index = False ) [ [ 'HD_Hydrology' ] ] . median ( ) fig = px . bar ( temp , x = "Wild Areas" , y = "HD_Hydrology" , color = 'Cover_Type' , barmode = 'group' , height = 400 , width = 900 ) fig . show ( ) temp . style . background_gradient ( cmap = "Blues" )
273	fig = px . histogram ( forest , x = "VD_Hydrology" , color = "Cover_Type" , marginal = 'rug' , title = "VD_Hydrology Histogram" , height = 500 , width = 800 ) fig . show ( )
274	fig = px . histogram ( forest , x = "HD_Roadways" , color = "Cover_Type" , marginal = 'rug' , title = "HD_Roadways Histogram" , height = 500 , width = 800 ) fig . show ( )
275	temp = forest . groupby ( [ 'Cover_Type' ] , as_index = False ) [ [ 'HD_Roadways' ] ] . median ( ) fig = px . bar ( temp . sort_values ( by = "HD_Roadways" , ascending = False ) , x = "HD_Roadways" , y = "Cover_Type" , color = 'Cover_Type' , orientation = 'h' , height = 300 , width = 900 ) fig . show ( )
276	fig = px . histogram ( forest , x = "HD_Fire_Points" , color = "Cover_Type" , marginal = 'rug' , title = "HD Fire Points Histogram" , height = 500 , width = 800 ) fig . show ( )
277	fig = px . histogram ( forest , x = "Hillshade_9am" , color = "Cover_Type" , marginal = 'box' , title = "Hillshade at 9am Histogram" , height = 500 , width = 800 ) fig . show ( )
278	temp = forest . groupby ( [ 'Wild Areas' , 'Cover_Type' ] , as_index = False ) [ [ 'Hillshade_9am' ] ] . median ( ) fig = px . bar ( temp , x = "Wild Areas" , y = "Hillshade_9am" , color = 'Cover_Type' , barmode = 'group' , height = 400 , width = 900 ) fig . show ( )
279	fig = px . histogram ( forest , x = "Hillshade_Noon" , color = "Cover_Type" , marginal = 'box' , title = "Hillshade at Noon Histogram" , height = 500 , width = 800 ) fig . show ( )
280	fig = px . histogram ( forest , x = "Hillshade_3pm" , color = "Cover_Type" , marginal = 'box' , title = "Hillshade at 3pm Histogram" , height = 500 , width = 800 ) fig . show ( )
281	import numpy as np , pandas as pd from sklearn . model_selection import StratifiedKFold from sklearn . metrics import roc_auc_score from sklearn . preprocessing import StandardScaler from sklearn . feature_selection import VarianceThreshold from sklearn . discriminant_analysis import QuadraticDiscriminantAnalysis from tqdm import tqdm_notebook import warnings warnings . filterwarnings ( 'ignore' )
282	batch_size = 16 lrG = 0.001 lrD = 0.001 beta1 = 0.5 epochs = 300 real_label = 0.5 fake_label = 0 nz = 128 device = torch . device ( "cuda" if torch . cuda . is_available ( ) else "cpu" )
283	x = next ( iter ( train_loader ) ) fig = plt . figure ( figsize = ( 25 , 16 ) ) for ii , img in enumerate ( x ) : ax = fig . add_subplot ( 4 , 8 , ii + 1 , xticks = [ ] , yticks = [ ] ) img = img . numpy ( ) . transpose ( 1 , 2 , 0 ) plt . imshow ( ( img + 1 ) / 2 )
284	MAGIC_N = 42 train_subset = train [ train [ 'wheezy-copper-turtle-magic' ] == MAGIC_N ] test_subset = test [ test [ 'wheezy-copper-turtle-magic' ] == MAGIC_N ] concated = pd . concat ( [ train_subset , test_subset ] ) a = train_subset . std ( ) > 1.2 cols = [ idx for idx in a . index if a [ idx ] ] concated = concated [ cols + [ 'target' ] ]
285	X_embedded = TSNE ( n_components = 2 , perplexity = 25 , random_state = 50 ) . fit_transform ( concated [ cols ] . values ) concated [ 'tsne2_1' ] = X_embedded [ : , 0 ] concated [ 'tsne2_2' ] = X_embedded [ : , 1 ]
286	X_embedded = TSNE ( n_components = 3 , perplexity = 20 , random_state = 42 ) . fit_transform ( concated [ cols ] . values ) concated [ 'tsne3_1' ] = X_embedded [ : , 0 ] concated [ 'tsne3_2' ] = X_embedded [ : , 1 ] concated [ 'tsne3_3' ] = X_embedded [ : , 2 ]
287	from IPython . display import IFrame , YouTubeVideo YouTubeVideo ( 'NSawVyi8aro' , width = 600 , height = 400 )
288	import numpy as np import pandas as pd import os import matplotlib import matplotlib . pyplot as plt import seaborn as sns import PIL from IPython . display import Image , display from plotly import graph_objs as go import plotly . express as px import plotly . figure_factory as ff import openslide
289	print ( "number of unique images : " , len ( train . image_id . unique ( ) ) ) print ( "number of unique data provider : " , len ( train . data_provider . unique ( ) ) ) print ( "number of unique isup_grade : " , len ( train . isup_grade . unique ( ) ) ) print ( "number of unique gleason_score : " , len ( train . gleason_score . unique ( ) ) )
290	fig = plt . figure ( figsize = ( 10 , 6 ) ) ax = sns . countplot ( x = "isup_grade" , hue = "data_provider" , data = train ) for p in ax . patches : height = p . get_height ( ) ax . text ( p . get_x ( ) + p . get_width ( ) / 2 , height + 3 , '{:1.2f}%' . format ( 100 * height / 10616 ) , ha = "center" )
291	fig = plt . figure ( figsize = ( 10 , 6 ) ) ax = sns . countplot ( x = "gleason_score" , hue = "data_provider" , data = train ) for p in ax . patches : height = p . get_height ( ) ax . text ( p . get_x ( ) + p . get_width ( ) / 2 , height + 3 , '{:1.2f}%' . format ( 100 * height / 10616 ) , ha = "center" )
292	example = openslide . OpenSlide ( os . path . join ( BASE_FOLDER + "train_images" , '005e66f06bce9c2e49142536caf2f6ee.tiff' ) ) patch = example . read_region ( ( 17800 , 19500 ) , 0 , ( 256 , 256 ) ) display ( patch ) example . close ( )
293	pen_marked_images = [ 'fd6fe1a3985b17d067f2cb4d5bc1e6e1' , 'ebb6a080d72e09f6481721ef9f88c472' , 'ebb6d5ca45942536f78beb451ee43cc4' , 'ea9d52d65500acc9b9d89eb6b82cdcdf' , 'e726a8eac36c3d91c3c4f9edba8ba713' , 'e90abe191f61b6fed6d6781c8305fe4b' , 'fd0bb45eba479a7f7d953f41d574bf9f' , 'ff10f937c3d52eff6ad4dd733f2bc3ac' , 'feee2e895355a921f2b75b54debad328' , ] overlay_mask_on_slide ( pen_marked_images )
294	import numpy as np import pandas as pd import os import matplotlib . pyplot as plt import torch from torch import nn , optim import torch . nn . functional as F from torchvision import datasets , transforms from torchvision . utils import save_image from torch . utils . data import Dataset , DataLoader from torch . autograd import Variable from PIL import Image from tqdm import tqdm_notebook as tqdm
295	x = next ( iter ( train_loader ) ) fig = plt . figure ( figsize = ( 25 , 16 ) ) for ii , img in enumerate ( x ) : ax = fig . add_subplot ( 4 , 8 , ii + 1 , xticks = [ ] , yticks = [ ] ) img = img . numpy ( ) . transpose ( 1 , 2 , 0 ) plt . imshow ( ( img + 1. ) / 2. )
296	lr = 0.001 epochs = 50 latent_dim = 32 model = VAE ( latent_dim , batch_size = batch_size ) . to ( device ) optimizer = optim . Adam ( model . parameters ( ) , lr = lr )
297	def seed_everything ( seed = 1029 ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True seed_everything ( )
298	df_train = pd . read_csv ( "../input/train.csv" ) df_test = pd . read_csv ( "../input/test.csv" ) df = pd . concat ( [ df_train , df_test ] , sort = True )
299	np . save ( "x_train" , x_train ) np . save ( "x_test" , x_test ) np . save ( "y_train" , y_train ) np . save ( "features" , features ) np . save ( "test_features" , test_features ) np . save ( "word_index.npy" , word_index )
300	x_train = np . load ( "x_train.npy" ) x_test = np . load ( "x_test.npy" ) y_train = np . load ( "y_train.npy" ) features = np . load ( "features.npy" ) test_features = np . load ( "test_features.npy" ) word_index = np . load ( "word_index.npy" ) . item ( )
301	seed_everything ( ) glove_embeddings = load_glove ( word_index ) paragram_embeddings = load_para ( word_index ) embedding_matrix = np . mean ( [ glove_embeddings , paragram_embeddings ] , axis = 0 ) del glove_embeddings , paragram_embeddings gc . collect ( ) np . shape ( embedding_matrix )
302	class MyDataset ( Dataset ) : def __init__ ( self , dataset ) : self . dataset = dataset def __getitem__ ( self , index ) : data , target = self . dataset [ index ] return data , target , index def __len__ ( self ) : return len ( self . dataset )
303	df [ "headshot_rate" ] = df [ "headshotKills" ] / df [ "kills" ] df [ "headshot_rate" ] . fillna ( 0 , inplace = True ) plot_dists ( "headshot_rate" )
304	df [ "roadkills_rate" ] = df [ "roadKills" ] / df [ "kills" ] df [ "roadkills_rate" ] . fillna ( 0 , inplace = True ) plot_dists ( "roadkills_rate" )
305	plt . figure ( figsize = ( 20 , 15 ) ) sns . pointplot ( df [ "heals" ] , df [ "winPlacePerc" ] , linestyles = "-" ) sns . pointplot ( df [ "boosts" ] , df [ "winPlacePerc" ] , color = "green" , linestyles = "--" ) plt . xlabel ( "heals/boost" ) plt . legend ( [ "heals" , "boosts" ] ) plt . show ( )
306	plt . figure ( figsize = ( 20 , 15 ) ) sns . pointplot ( df [ "DBNOs" ] , df [ "assists" ] ) plt . grid ( ) plt . show ( )
307	plt . figure ( figsize = ( 20 , 15 ) ) sns . pointplot ( df [ "heals" ] , df [ "walkDistance" ] , linestyles = "-" ) sns . pointplot ( df [ "boosts" ] , df [ "walkDistance" ] , color = "green" , linestyles = "--" ) plt . xlabel ( "heals/boost" ) plt . legend ( [ "heals" , "boosts" ] ) plt . grid ( ) plt . show ( )
308	import numpy as np import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import os merchants = pd . read_csv ( "../input/merchants.csv" )
309	merchants [ 'category_2' ] = merchants [ 'category_2' ] . fillna ( 0 ) . astype ( int ) merchants . loc [ merchants [ 'city_id' ] == - 1 , 'city_id' ] = 0 merchants . loc [ merchants [ 'state_id' ] == - 1 , 'state_id' ] = 0
310	def rating ( x ) : if np . isfinite ( x ) and x > 0 : x = ( 1 / x ) - 1 if x > 1 : r = 1 elif x <= 1 and x > 0 : r = 2 elif x == 0 : r = 3 elif x < 0 and x >= - 1 : r = 4 else : r = 5 else : r = 5 return r
311	merchants [ 'numerical_2' ] . plot . line ( figsize = ( 20 , 8 ) ) print ( "Data in histogram bins\n" , np . histogram ( merchants [ 'numerical_2' ] ) [ 0 ] ) print ( "Bins\n" , np . histogram ( merchants [ 'numerical_2' ] ) [ 1 ] ) print ( "Mean:" , np . mean ( merchants [ 'numerical_2' ] ) )
312	vc_numerical_1 = merchants [ 'numerical_1' ] . value_counts ( ) vc_numerical_2 = merchants [ 'numerical_2' ] . value_counts ( ) print ( "Value counts of 'numerical_1`" ) vc_numerical_1 . head ( 20 )
313	outliers_numerical_1 = merchants . loc [ merchants [ 'numerical_1' ] > 20 , : ] outliers_numerical_2 = merchants . loc [ merchants [ 'numerical_2' ] > 20 , : ] outliers_numerical_1 . head ( 10 )
314	merchants [ 'most_recent_sales_range' ] = merchants [ 'most_recent_sales_range' ] \ . map ( { 'A' : 1 , 'B' : 2 , 'C' : 3 , 'D' : 4 , 'E' : 5 } ) merchants [ 'most_recent_purchases_range' ] = merchants [ 'most_recent_purchases_range' ] \ . map ( { 'A' : 1 , 'B' : 2 , 'C' : 3 , 'D' : 4 , 'E' : 5 } )
315	def eval_metric ( FVC , FVC_Pred , sigma ) : n = len ( sigma ) a = np . empty ( n ) a . fill ( 70 ) sigma_clipped = np . maximum ( sigma , a ) delta = np . minimum ( np . abs ( FVC , FVC_Pred ) , 1000 ) eval_metric = - np . sqrt ( 2 ) * delta / sigma_clipped - np . log ( np . sqrt ( 2 ) * sigma_clipped ) return eval_metric
316	from sklearn . preprocessing import LabelEncoder cat_features = [ 'Sex' , 'SmokingStatus' ] encoder = LabelEncoder ( ) encoded = data [ cat_features ] . apply ( encoder . fit_transform )
317	import matplotlib . pyplot as plt import seaborn as seabornInstance from sklearn . model_selection import train_test_split from sklearn . linear_model import LinearRegression from sklearn import metrics
318	test = pd . read_csv ( '../input/nomad2018-predict-transparent-conductors/test.csv' ) test_id = test . id train = pd . read_csv ( '../input/nomad2018-predict-transparent-conductors/train.csv' )
319	def get_prop_list ( path_to_element_data ) : return [ f [ : - 4 ] for f in os . listdir ( path_to_element_data ) ] path_to_element_data = '../input/elemental-properties/' properties = get_prop_list ( path_to_element_data ) print ( sorted ( properties ) )
320	for col in [ 'x_Al' , 'x_Ga' , 'x_In' , 'a' , 'b' , 'c' , 'vol' , 'atomic_density' ] : for x in all_data . sg . unique ( ) : sns . distplot ( all_data [ all_data [ 'sg' ] == x ] [ col ] ) plt . title ( col ) plt . show ( )
321	for col in [ 'E' , 'Eg' ] : sns . distplot ( ( train [ col ] ) ) plt . title ( col ) plt . show ( )
322	def rmsle ( h , y ) : return np . sqrt ( np . square ( np . log ( h + 1 ) - np . log ( y + 1 ) ) . mean ( ) )
323	import keras import tensorflow as tf import keras . backend as K from keras . regularizers import l2 from keras . optimizers import Adam from keras . models import Model from keras . models import Sequential from keras . layers import Dense , Dropout , BatchNormalization , Input
324	from sklearn . model_selection import KFold from sklearn . ensemble import GradientBoostingRegressor
325	import numpy as np import pandas as pd from nilearn import plotting , image , input_data , datasets import nibabel as nb import h5py import matplotlib . pyplot as plt import joblib import seaborn as sns from tqdm . auto import tqdm
326	release_year_mean_data = train . groupby ( [ 'release_year' ] ) [ 'budget' , 'popularity' , 'revenue' ] . mean ( ) release_year_mean_data . head ( ) fig = plt . figure ( figsize = ( 10 , 10 ) ) release_year_mean_data [ 'popularity' ] . plot ( kind = 'line' ) plt . ylabel ( 'Mean Popularity value' ) plt . title ( 'Mean Popularity Over Years' )
327	dict_columns = [ 'belongs_to_collection' , 'genres' , 'production_companies' , 'production_countries' , 'spoken_languages' , 'Keywords' , 'cast' , 'crew' ] def text_to_dict ( df ) : for column in dict_columns : df [ column ] = df [ column ] . apply ( lambda x : { } if pd . isna ( x ) else ast . literal_eval ( x ) ) return df train = text_to_dict ( train ) test = text_to_dict ( test )
328	fig = plt . figure ( figsize = ( 30 , 25 ) ) plt . subplot ( 221 ) train [ 'revenue' ] . plot ( kind = 'hist' , bins = 100 ) plt . title ( 'Distribution of Revenue' ) plt . xlabel ( 'Revenue' ) plt . subplot ( 222 ) np . log1p ( train [ 'revenue' ] ) . plot ( kind = 'hist' , bins = 100 ) plt . title ( 'Train Log Revenue Distribution' ) plt . xlabel ( 'Log Revenue' ) print ( 'Skew of revenue attribute: %0.1f' % skew ( train [ 'revenue' ] ) )
329	opt_parameters = { 'random_state' : 501 , 'objective' : 'regression' , 'num_leaves' : 40 , 'min_data_in_leaf' : 15 , 'max_depth' : 4 , 'learning_rate' : 0.01 , 'boosting_type' : 'gbdt' } params [ 'learning_rate' ] = opt_parameters [ 'learning_rate' ] params [ 'max_depth' ] = opt_parameters [ 'max_depth' ] params [ 'num_leaves' ] = opt_parameters [ 'num_leaves' ] params [ 'min_data_in_leaf' ] = opt_parameters [ 'min_data_in_leaf' ]
330	from nilearn import datasets aal = datasets . fetch_atlas_aal ( ) ; try : plotting . plot_roi ( aal [ 'maps' ] , title = 'Example of a Brain Atlas (aal)' ) ; except : print ( "Probably time out" )
331	fnc10_cols = fnc_10 . columns . to_list ( ) [ 1 : ] fnc10_cols_filtered = [ i . split ( '_' ) [ 0 ] for i in fnc10_cols ] print ( np . unique ( fnc10_cols_filtered ) )
332	test_mat1 = h5py . File ( '../input/trends-assessment-prediction/fMRI_test/11000.mat' , mode = 'r' ) print ( test_mat1 . keys ( ) ) test_mat1 = np . array ( test_mat1 . get ( 'SM_feature' ) ) print ( 'Dimensions of ICA feature map' ) print ( test_mat1 . shape ) print ( 'Dimenions of the brain mask' ) print ( brain_mask . shape ) test_mat2 = h5py . File ( '../input/trends-assessment-prediction/fMRI_test/10006.mat' , mode = 'r' ) test_mat2 = np . array ( test_mat2 . get ( 'SM_feature' ) )
333	scores = pd . read_csv ( '../input/trends-assessment-prediction/train_scores.csv' ) scores = scores . set_index ( 'Id' ) scores . head ( )
334	for sc in scores . columns [ : 5 ] : deciles = np . percentile ( scores . loc [ : , sc ] , [ 20 , 40 , 60 , 80 ] ) discr = np . digitize ( scores . loc [ : , sc ] , deciles ) scores . loc [ : , sc + '_discrete' ] = discr . astype ( str ) scores . loc [ : , 'stratify' ] = ( scores [ 'age_discrete' ] + '_' + scores [ 'domain1_var1_discrete' ] + '_' + scores [ 'domain2_var2_discrete' ] )
335	from sklearn . model_selection import train_test_split train_idx , _ = train_test_split ( scores . index , train_size = 0.2 , random_state = 223 , stratify = scores . stratify )
336	from sklearn . decomposition import PCA pca = PCA ( n_components = 0.8 , whiten = True , svd_solver = 'full' ) pca . fit ( fnc_sample . values ) components = pca . transform ( fnc_sample . values ) components = pd . DataFrame ( components , index = scores_stat . index )
337	pca_corr = [ ] for kk in range ( 10 ) : pca_corr . append ( scores_stat [ [ 'age' , 'domain1_var1' , 'domain1_var2' , 'domain2_var1' , 'domain2_var2' ] ] . corrwith ( components . loc [ : , kk ] ) ) pd . concat ( pca_corr , axis = 1 )
338	try : basc_data = datasets . fetch_atlas_basc_multiscale_2015 ( version = 'sym' , data_dir = None , resume = True , verbose = 1 ) except : print ( "Probably time out" )
339	from nilearn import input_data basc197_masker = input_data . NiftiLabelsMasker ( basc_197 , mask_img = brain_mask ) def load_matlab ( participant_id , masker , path = '../input/trends-assessment-prediction/fMRI_train/' ) : mat = np . array ( h5py . File ( f'{path}{participant_id}.mat' , mode = 'r' ) . get ( 'SM_feature' ) ) mat = masker . fit_transform ( nb . Nifti1Image ( mat . transpose ( [ 3 , 2 , 1 , 0 ] ) , affine = masker . mask_img . affine ) ) return mat . flatten ( )
340	from sklearn . decomposition import FastICA pca_2 = PCA ( n_components = 0.6 , whiten = True ) pca_2 . fit ( sm_data ) components2 = pca_2 . fit_transform ( sm_data )
341	components2 = pd . DataFrame ( components2 , index = scores_stat . index ) pca2_corr = [ ] for kk in range ( 20 ) : pca2_corr . append ( scores_stat [ [ 'age' , 'domain1_var1' , 'domain1_var2' , 'domain2_var1' , 'domain2_var2' ] ] . corrwith ( components2 . loc [ : , kk ] ) ) pca2_scorr = pd . concat ( pca2_corr , axis = 1 ) pca2_scorr
342	fig , axes = plt . subplots ( 1 , 5 , figsize = ( 15 , 5 ) , sharex = True , sharey = True ) for n , ax in enumerate ( axes . flatten ( ) ) : ax . plot ( pca2_scorr . iloc [ n , : ] ) ax . set_title ( pca2_scorr . index [ n ] ) ax . axhline ( 0 )
343	import gc try : del fnc del sm_data del pca del pca_2 except : pass gc . collect ( )
344	from sklearn . linear_model import RidgeCV from sklearn . preprocessing import RobustScaler from sklearn . model_selection import StratifiedKFold from sklearn . pipeline import make_pipeline from sklearn . metrics import mean_absolute_error from sklearn . decomposition import PCA
345	model = sm . PHReg ( ys , xs , cs ) result = model . fit ( ) baseline_cum_hazard_func = result . baseline_cumulative_hazard_function [ 0 ] pred_index = np . arange ( - 99 , 100 )
346	df_unique = df_train . groupby ( [ df_train . Patient , df_train . Age , df_train . Sex , df_train . SmokingStatus ] ) [ 'Patient' ] . count ( ) df_unique . index = df_unique . index . set_names ( [ 'Patient Id' , 'Age' , 'Sex' , 'SmokingStatus' ] ) df_unique = df_unique . reset_index ( ) df_unique . rename ( columns = { 'Patient' : 'Frequency' } , inplace = True ) df_unique . head ( )
347	print ( df_unique [ 'Sex' ] . value_counts ( ) ) percent_sexwise = df_unique [ 'Sex' ] . value_counts ( ) / len ( df_unique [ 'Sex' ] ) print ( percent_sexwise )
348	from scipy . stats import kurtosis , skew print ( "The mean of FVC data is: {}" . format ( df_train [ 'FVC' ] . mean ( ) ) ) print ( "The median of FVC data is: {}" . format ( df_train [ 'FVC' ] . median ( ) ) ) print ( "The standard deviation of FVC data is: {}" . format ( df_train [ 'FVC' ] . std ( ) ) ) print ( 'excess kurtosis of normal distribution (should be 0): {}' . format ( kurtosis ( df_train [ 'FVC' ] ) ) ) print ( 'skewness of normal distribution (should be 0): {}' . format ( skew ( df_train [ 'FVC' ] ) ) )
349	fig , ax = plt . subplots ( figsize = ( 20 , 8 ) ) df_weeks = df_train . groupby ( [ 'Weeks' ] ) . count ( ) df_weeks . head ( 20 ) sns . distplot ( df_weeks , color = 'g' )
350	fig , ax = plt . subplots ( figsize = ( 20 , 15 ) ) x = df_train [ 'FVC' ] y = df_train [ 'Percent' ] colors = df_train [ 'FVC' ] plt . scatter ( x , y , c = colors , alpha = 0.5 ) plt . show ( )
351	import numpy as np import os import copy from math import * import matplotlib . pyplot as plt from functools import reduce import pydicom as dicom import glob from skimage import measure , morphology from skimage . morphology import ball , binary_closing from skimage . measure import label , regionprops from scipy . linalg import norm import scipy . ndimage from ipywidgets . widgets import * import ipywidgets as widgets import plotly from plotly . graph_objs import * import chart_studio . plotly as py
352	def image_to_hu ( image_path , image_id ) : dicom = pydicom . read_file ( image_path + 'ID_' + image_id + '.dcm' ) image = dicom . pixel_array . astype ( np . float64 ) intercept = dicom . RescaleIntercept slope = dicom . RescaleSlope if slope != 1 : image = slope * image . astype ( np . float64 ) image = image . astype ( np . float64 ) image += np . float64 ( intercept ) image [ image < - 1024 ] = - 1024 return image , dicom
353	def image_resample ( image , dicom_header , new_spacing = [ 1 , 1 ] ) : spacing = map ( float , dicom_header . PixelSpacing ) spacing = np . array ( list ( spacing ) ) resize_factor = spacing / new_spacing new_real_shape = image . shape * resize_factor new_shape = np . round ( new_real_shape ) real_resize_factor = new_shape / image . shape new_spacing = spacing / real_resize_factor image = scipy . ndimage . interpolation . zoom ( image , real_resize_factor ) return image
354	def image_crop ( image ) : mask = image == 0 coords = np . array ( np . nonzero ( ~ mask ) ) top_left = np . min ( coords , axis = 1 ) bottom_right = np . max ( coords , axis = 1 ) out = image [ top_left [ 0 ] : bottom_right [ 0 ] , top_left [ 1 ] : bottom_right [ 1 ] ] return out
355	def image_pad ( image , new_height , new_width ) : height , width = image . shape im_bg = np . zeros ( ( new_height , new_width ) ) pad_left = int ( ( new_width - width ) / 2 ) pad_top = int ( ( new_height - height ) / 2 ) im_bg [ pad_top : pad_top + height , pad_left : pad_left + width ] = image return im_bg
356	from sklearn . linear_model import LogisticRegressionCV from sklearn . ensemble import RandomForestClassifier from sklearn . model_selection import cross_val_score , StratifiedKFold SKF = StratifiedKFold ( 5 ) logReg_score = cross_val_score ( LogisticRegressionCV ( cv = 5 ) , xTrain , yTrain , cv = SKF ) rfClf_score = cross_val_score ( RandomForestClassifier ( n_estimators = 100 ) , xTrain , yTrain , cv = SKF ) print ( f'Logistic Regression Accuracy: {np.mean(logReg_score):4.3f} +/- {np.std(logReg_score):4.3f}' ) print ( f'Random Forest Accuracy: {np.mean(rfClf_score):4.3f} +/- {np.std(rfClf_score):4.3f}' )
357	import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns from itertools import cycle color_cycle = cycle ( plt . rcParams [ 'axes.prop_cycle' ] . by_key ( ) [ 'color' ] ) import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
358	from sklearn . linear_model import ElasticNet , Lasso , BayesianRidge , LassoLarsIC from sklearn . ensemble import RandomForestRegressor , GradientBoostingRegressor from sklearn . kernel_ridge import KernelRidge from sklearn . pipeline import make_pipeline from sklearn . preprocessing import RobustScaler from sklearn . base import BaseEstimator , TransformerMixin , RegressorMixin , clone from sklearn . model_selection import KFold , cross_val_score , train_test_split from sklearn . metrics import mean_squared_error import xgboost as xgb import lightgbm as lgb
359	def image_ids_in ( root_dir , is_train_data = False ) : ids = [ ] for id in os . listdir ( root_dir ) : if id in TRAIN_ERROR_IDS : print ( 'Skipping ID due to bad training data:' , id ) else : ids . append ( id ) return ids TRAIN_IMAGE_IDS = image_ids_in ( TRAIN_DIR , is_train_data = True ) TEST_IMAGE_IDS = image_ids_in ( TEST_DIR ) print ( 'Examples:' , TRAIN_IMAGE_IDS [ 22 ] , TEST_IMAGE_IDS [ 22 ] )
360	import pandas as pd from sklearn . feature_extraction . text import CountVectorizer from nltk . corpus import stopwords from sklearn import preprocessing from sklearn . naive_bayes import MultinomialNB
361	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) sample = pd . read_csv ( '../input/sample_submission.csv' ) train . head ( 3 )
362	xtrain_ctv_all = ctv . fit_transform ( train . text . values ) xtest_ctv_all = ctv . transform ( test . text . values ) clf = MultinomialNB ( alpha = 1.0 ) clf . fit ( xtrain_ctv_all , y )
363	train_text = train_data [ 'question_text' ] test_text = test_data [ 'question_text' ] train_target = train_data [ 'target' ] all_text = train_text . append ( test_text )
364	import os , gc import datetime import numpy as np import pandas as pd import category_encoders from sklearn . impute import SimpleImputer from sklearn . metrics import mean_squared_error from sklearn . model_selection import KFold from sklearn . preprocessing import LabelEncoder from sklearn . linear_model import Lasso from sklearn . linear_model import Ridge from lightgbm import LGBMRegressor from mlxtend . regressor import StackingRegressor from pandas . api . types import is_categorical_dtype from pandas . api . types import is_datetime64_any_dtype as is_datetime
365	lightgbm = LGBMRegressor ( objective = 'regression' , learning_rate = 0.05 , num_leaves = 1024 , feature_fraction = 0.8 , bagging_fraction = 0.9 , bagging_freq = 5 ) ridge = Ridge ( alpha = 0.3 ) lasso = Lasso ( alpha = 0.3 )
366	predictions = 0 for model in models : predictions += np . expm1 ( model . predict ( np . array ( test ) ) ) / len ( models ) del model ; gc . collect ( ) del test , models ; gc . collect ( )
367	import os import math import cv2 import numpy as np import pandas as pd from pathlib import Path import matplotlib . pyplot as plt import matplotlib . patches as patches import matplotlib as mpl from lyft_dataset_sdk . lyftdataset import LyftDataset from lyft_dataset_sdk . utils . data_classes import LidarPointCloud
368	healthy_noise = train [ healthy ] . signal . values - mean_predict [ train [ healthy ] . open_channels . values ] fixed = mean_predict [ train [ corrupted ] . open_channels . values ] + healthy_noise train . loc [ train [ corrupted ] . index , 'signal' ] = fixed
369	train . signal . plot ( ) plt . title ( 'Train' ) plt . show ( ) test . signal . plot ( ) plt . title ( 'Test' ) plt . show ( ) train . to_csv ( "train_synthetic.csv" , index = False , float_format = '%.4f' ) test . to_csv ( "test_synthetic.csv" , index = False , float_format = '%.4f' )
370	def rescale_noise ( data , means , scale_factor ) : sig = data . signal . values ch = data . open_channels . values noise = sig - means [ ch ] noise *= scale_factor sig_ = noise + means [ ch ] return sig_ def reduce_channels ( data , res , means ) : residual = res . open_channels . values reduced_sig = data . signal . values - ( means - means [ 0 ] ) [ residual ] return reduced_sig , residual
371	batch_size = 64 gen = ImageDataGenerator ( horizontal_flip = True , vertical_flip = True , width_shift_range = 0.1 , height_shift_range = 0.1 , zoom_range = 0.1 , rotation_range = 10 )
372	def plot_sample_size ( tagged_df ) : plt . rcParams [ 'figure.figsize' ] = ( 12 , 5 ) print ( 'There are {} unique tags in this data' . format ( len ( tagged_df . columns ) ) ) colors = cm . rainbow ( np . linspace ( 0 , 1 , len ( tagged_df . columns ) ) ) tagged_df . sum ( ) . sort_values ( ascending = False ) . plot ( title = "Counts of Tags" , color = colors , kind = 'bar' ) plt . show ( ) plot_sample_size ( tagged_df )
373	fig , axes = plt . subplots ( 1 , 3 , figsize = ( 10 , 6 ) ) axes [ 0 ] . imshow ( X_sample [ 1 , : , : , 0 ] , cmap = 'Reds' ) axes [ 1 ] . imshow ( X_sample [ 1 , : , : , 1 ] , cmap = 'Greens' ) axes [ 2 ] . imshow ( X_sample [ 1 , : , : , 2 ] , cmap = 'Blues' )
374	from skimage . color import rgb2gray from skimage import transform , img_as_float , filters X_train_g = rgb2gray ( X_sample ) X_train_sobel = [ ] for i in range ( X_train_g . shape [ 0 ] ) : X_train_sobel . append ( filters . sobel ( X_train_g [ i ] ) ) X_train_sobel = np . asarray ( X_train_sobel ) plot_samples ( X_train_sobel , names_train , tagged_df , 4 , 4 )
375	from sklearn . model_selection import train_test_split X_train , X_validation , y_train , y_validation = train_test_split ( X , y [ 0 : 99 ] , test_size = 0.40 , random_state = 14113 ) print ( 'X_train is a {} object' . format ( type ( X_train ) ) ) print ( 'it has shape {}' . format ( X_train . shape ) ) print ( 'y_train is a {} object' . format ( type ( y_train ) ) ) print ( 'it has {} elements' . format ( len ( y_train ) ) )
376	from sklearn . metrics import fbeta_score np . asarray ( y_validation ) predictions = rf . predict ( X_validation ) fbeta_score ( np . asarray ( y_validation ) , predictions , beta = 2 , average = 'samples' )
377	import warnings warnings . filterwarnings ( "ignore" ) import numpy as np import pandas as pd from datetime import datetime import matplotlib . pyplot as plt import seaborn as sns from scipy import stats import itertools from sklearn import model_selection from sklearn . ensemble import RandomForestRegressor from sklearn import metrics
378	df_train = pd . read_csv ( "../input/train.csv" , sep = ',' , parse_dates = [ 'Date' ] , date_parser = str_to_date , low_memory = False ) df_store = pd . read_csv ( "../input/store.csv" , low_memory = False )
379	df_store . Promo2SinceWeek . fillna ( 0 , inplace = True ) df_store . Promo2SinceYear . fillna ( 0 , inplace = True ) df_store . PromoInterval . fillna ( 0 , inplace = True )
380	df_train_store = pd . merge ( df_train , df_store , how = 'left' , on = 'Store' ) df_train_store . head ( ) print ( "The Train_Store dataset has {} Rows and {} Variables" . format ( str ( df_train_store . shape [ 0 ] ) , str ( df_train_store . shape [ 1 ] ) ) )
381	StoretypeXAssortment = sns . countplot ( x = "StoreType" , hue = "Assortment" , order = [ "a" , "b" , "c" , "d" ] , data = df_store , palette = sns . color_palette ( "Set2" , n_colors = 3 ) ) . set_title ( "Number of Different Assortments per Store Type" ) df_store . groupby ( by = [ "StoreType" , "Assortment" ] ) . Assortment . count ( )
382	sns . factorplot ( data = df_train_store , x = "DayOfWeek" , y = "Sales" , hue = 'Promo' , sharex = False )
383	df_correlation = df_train_store [ [ 'Store' , 'DayOfWeek' , 'Sales' , 'Customers' , 'Open' , 'Promo' , 'SchoolHoliday' , 'CompetitionDistance' , 'CompetitionOpenSinceMonth' , 'CompetitionOpenSinceYear' , 'Promo2' , 'Promo2SinceWeek' , 'Promo2SinceYear' , 'SalesperCustomer' , 'Month' , 'Year' , 'Day' , 'StateHoliday_cat' , 'Assortment_cat' , 'StoreType_cat' , 'PromoInterval_cat' ] ]
384	df_test = pd . read_csv ( "../input/test.csv" , sep = ',' , parse_dates = [ 'Date' ] , date_parser = str_to_date , low_memory = False ) print ( "The Test dataset has {} Rows and {} Variables" . format ( str ( df_test . shape [ 0 ] ) , str ( df_test . shape [ 1 ] ) ) )
385	def rmspe ( y , yhat ) : rmspe = np . sqrt ( np . mean ( ( y - yhat ) ** 2 ) ) return rmspe
386	X_train , X_train_test , y_train , y_train_test = model_selection . train_test_split ( features , targets , test_size = 0.20 , random_state = 15 ) print ( "Training and testing split was successful." )
387	rfr = RandomForestRegressor ( n_estimators = 10 , criterion = 'mse' , max_depth = 5 , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , min_impurity_split = None , bootstrap = True , oob_score = False , n_jobs = 4 , random_state = 31 , verbose = 0 , warm_start = False ) rfr . fit ( X_train , y_train )
388	rfr_val = RandomForestRegressor ( n_estimators = 128 , criterion = 'mse' , max_depth = 20 , min_samples_split = 10 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , min_impurity_split = None , bootstrap = True , oob_score = False , n_jobs = 4 , random_state = 35 , verbose = 0 , warm_start = False ) model_RF_test = rfr_val . fit ( X_train , y_train )
389	df_test_store1 = df_test_store . drop ( [ 'Id' ] , axis = 1 ) kaggle_yhat = model_RF_test . predict ( df_test_store1 ) kaggle_preds = pd . DataFrame ( { 'Id' : df_test_store [ 'Id' ] , 'Sales' : np . exp ( kaggle_yhat ) } ) kaggle_preds . to_csv ( "Stefano_Zakher_RF_Rossman_Kaggle_submission.csv" , index = False )
390	comp_name = "dog_breed" input_path = "../input/" wd = "/kaggle/working/"
391	from keras . preprocessing import sequence from keras . models import Model , Input from keras . layers import Dense , SpatialDropout1D , Dropout from keras . layers import Embedding , GlobalMaxPool1D , BatchNormalization from keras . preprocessing . text import Tokenizer
392	result = np . zeros ( ( 3438 * 22 , ) ) for n , ( simulated_test , sample_prediction ) in enumerate ( simulate_iter_test_on_train_data ( num_plays = 3438 ) ) : simulated_test = process_apply ( simulated_test ) result [ n * 22 : ( n + 1 ) * 22 ] = simulated_test [ "dist_to_back" ] . values result_apply = pd . Series ( result )
393	from scipy import spatial scipy_similarity = np . zeros ( ( test_embeddings . shape [ 0 ] , train_embeddings . shape [ 0 ] ) ) for test_index in range ( test_embeddings . shape [ 0 ] ) : scipy_similarity [ test_index ] = 1 - spatial . distance . cdist ( test_embeddings [ np . newaxis , test_index , : ] , train_embeddings , 'cosine' ) [ 0 ]
394	import mxnet as mx from mxnet import gluon import numpy as np import pandas as pd import matplotlib . pyplot as plt import json import os from tqdm . autonotebook import tqdm from pathlib import Path
395	single_prediction_length = 28 submission_prediction_length = single_prediction_length * 2 m5_input_path = "../input/m5-forecasting-accuracy" submission = True if submission : prediction_length = submission_prediction_length else : prediction_length = single_prediction_length
396	calendar = pd . read_csv ( f'{m5_input_path}/calendar.csv' ) sales_train_validation = pd . read_csv ( f'{m5_input_path}/sales_train_validation.csv' ) sample_submission = pd . read_csv ( f'{m5_input_path}/sample_submission.csv' ) sell_prices = pd . read_csv ( f'{m5_input_path}/sell_prices.csv' )
397	from gluonts . model . deepar import DeepAREstimator from gluonts . distribution . neg_binomial import NegativeBinomialOutput from gluonts . trainer import Trainer estimator = DeepAREstimator ( prediction_length = prediction_length , freq = "D" , distr_output = NegativeBinomialOutput ( ) , use_feat_dynamic_real = True , use_feat_static_cat = True , cardinality = stat_cat_cardinalities , trainer = Trainer ( learning_rate = 1e-3 , epochs = 100 , num_batches_per_epoch = 50 , batch_size = 32 ) ) predictor = estimator . train ( train_ds )
398	from gluonts . evaluation . backtest import make_evaluation_predictions forecast_it , ts_it = make_evaluation_predictions ( dataset = test_ds , predictor = predictor , num_samples = 100 ) print ( "Obtaining time series conditioning values ..." ) tss = list ( tqdm ( ts_it , total = len ( test_ds ) ) ) print ( "Obtaining time series predictions ..." ) forecasts = list ( tqdm ( forecast_it , total = len ( test_ds ) ) )
399	if submission == True : forecasts_acc = np . zeros ( ( len ( forecasts ) , prediction_length ) ) for i in range ( len ( forecasts ) ) : forecasts_acc [ i ] = np . mean ( forecasts [ i ] . samples , axis = 0 )
400	if submission == True : forecasts_acc_sub = np . zeros ( ( len ( forecasts ) * 2 , single_prediction_length ) ) forecasts_acc_sub [ : len ( forecasts ) ] = forecasts_acc [ : , : single_prediction_length ] forecasts_acc_sub [ len ( forecasts ) : ] = forecasts_acc [ : , single_prediction_length : ]
401	if submission == True : import time sample_submission = pd . read_csv ( f'{m5_input_path}/sample_submission.csv' ) sample_submission . iloc [ : , 1 : ] = forecasts_acc_sub submission_id = 'submission_{}.csv' . format ( int ( time . time ( ) ) ) sample_submission . to_csv ( submission_id , index = False )
402	import mxnet as mx from mxnet import gluon import numpy as np import pandas as pd import matplotlib . pyplot as plt import json import os from tqdm . autonotebook import tqdm from pathlib import Path
403	single_prediction_length = 28 submission_prediction_length = single_prediction_length * 2 m5_input_path = "../input/m5-forecasting-uncertainty" submission = True if submission : prediction_length = submission_prediction_length else : prediction_length = single_prediction_length
404	calendar = pd . read_csv ( f'{m5_input_path}/calendar.csv' ) sales_train_validation = pd . read_csv ( f'{m5_input_path}/sales_train_validation.csv' ) sample_submission = pd . read_csv ( f'{m5_input_path}/sample_submission.csv' ) sell_prices = pd . read_csv ( f'{m5_input_path}/sell_prices.csv' )
405	test_cal_features_list = [ test_cal_features ] * len ( agg_series ) test_cal_features_list . extend ( test_cal_features_list_orig ) train_cal_features_list = [ train_cal_features ] * len ( agg_series ) train_cal_features_list . extend ( train_cal_features_list_orig ) stat_cat = np . append ( agg_cat , stat_cat_orig , axis = 0 ) train_df = sales_train_validation . drop ( [ "id" , "item_id" , "dept_id" , "cat_id" , "store_id" , "state_id" ] , axis = 1 ) series_tot = np . append ( agg_series , train_df . values , axis = 0 )
406	from gluonts . model . deepar import DeepAREstimator from gluonts . distribution . neg_binomial import NegativeBinomialOutput from gluonts . trainer import Trainer estimator = DeepAREstimator ( prediction_length = prediction_length , freq = "D" , distr_output = NegativeBinomialOutput ( ) , use_feat_dynamic_real = True , use_feat_static_cat = True , cardinality = stat_cat_cardinalities , trainer = Trainer ( learning_rate = 1e-3 , epochs = 100 , num_batches_per_epoch = 200 , batch_size = 100 ) ) predictor = estimator . train ( train_ds )
407	from gluonts . evaluation . backtest import make_evaluation_predictions forecast_it , ts_it = make_evaluation_predictions ( dataset = test_ds , predictor = predictor , num_samples = 100 ) print ( "Obtaining time series conditioning values ..." ) tss = list ( tqdm ( ts_it , total = len ( test_ds ) ) ) print ( "Obtaining time series predictions ..." ) forecasts = list ( tqdm ( forecast_it , total = len ( test_ds ) ) ) quantiles = [ 0.005 , 0.025 , 0.165 , 0.250 , 0.500 , 0.750 , 0.835 , 0.975 , 0.995 ]
408	if submission == False : from gluonts . evaluation import Evaluator evaluator = Evaluator ( quantiles = quantiles ) agg_metrics , item_metrics = evaluator ( iter ( tss ) , iter ( forecasts ) , num_series = len ( test_ds ) ) print ( json . dumps ( agg_metrics , indent = 4 ) )
409	if submission == True : def quantile_calc ( q ) : res = np . ones ( ( len ( forecasts ) , prediction_length ) ) for i , f in enumerate ( forecasts ) : res [ i ] = f . quantile ( q ) return res allresults = list ( map ( quantile_calc , quantiles ) ) allresults_conc = np . concatenate ( allresults , axis = 0 )
410	if submission == True : forecasts_acc_sub = np . zeros ( ( len ( allresults_conc ) * 2 , single_prediction_length ) ) forecasts_acc_sub [ : len ( allresults_conc ) ] = allresults_conc [ : , : single_prediction_length ] forecasts_acc_sub [ len ( allresults_conc ) : ] = allresults_conc [ : , single_prediction_length : ]
411	if submission == True : import time sample_submission = pd . read_csv ( f'{m5_input_path}/sample_submission.csv' ) sample_submission . iloc [ : , 1 : ] = forecasts_acc_sub submission_id = 'submission_{}.csv' . format ( int ( time . time ( ) ) ) sample_submission . to_csv ( submission_id , index = False )
412	import sys sys . path . insert ( 0 , "../input/transformers/transformers-master/" ) from transformers import * import pandas as pd import numpy as np import re from tqdm . notebook import tqdm from math import floor , ceil import tensorflow as tf print ( tf . __version__ )
413	albert_path = '../input/albertlargev2huggingface/' tokenizer = AlbertTokenizer . from_pretrained ( albert_path , do_lower_case = True ) albert_model = TFAlbertModel . from_pretrained ( albert_path )
414	from tensorflow . keras . layers import Dense , Dropout , Embedding , LSTM , Bidirectional , Input , Dropout , GlobalAveragePooling1D from tensorflow . keras import Sequential from tensorflow . keras . models import Model from tensorflow . keras . preprocessing import sequence from tensorflow . keras . optimizers import Adam from tensorflow . keras . callbacks import EarlyStopping from tensorflow . keras import regularizers import tensorflow . keras . backend as K from sklearn . model_selection import KFold , GroupKFold from scipy . stats import spearmanr import warnings ; warnings . simplefilter ( 'ignore' )
415	def get_iou_vector ( A , B , n ) : intersection = np . logical_and ( A , B ) union = np . logical_or ( A , B ) iou = np . sum ( intersection > 0 ) / np . sum ( union > 0 ) s = pd . Series ( name = n ) for thresh in np . arange ( 0.5 , 1 , 0.05 ) : s [ thresh ] = iou > thresh return s print ( 'Does this IoU hit at each threshold?' ) print ( get_iou_vector ( A , B , 'GT-P' ) )
416	df = pd . DataFrame ( ) for ind , gt_mask in enumerate ( targ_masks ) : s = get_iou_vector ( pred_masks [ 3 ] , gt_mask , 'P3-GT{}' . format ( ind ) ) df = df . append ( s ) print ( 'Performance of Predicted Mask 3 vs. each Ground Truth mask across IoU thresholds' ) print ( df )
417	print ( 'Precision values at each threshold:' ) ps = [ ] for thresh , iou_mat in zip ( np . arange ( 0.5 , 1 , 0.05 ) , iou_vol ) : _ , _ , _ , p = iou_thresh_precision ( iou_mat ) print ( '\tt({:0.2f}) = {:0.3f}' . format ( thresh , p ) ) ps . append ( p ) print ( 'Mean precision for image is: {:0.3f}' . format ( np . mean ( ps ) ) )
418	print ( 'Original image shape: {}' . format ( im . shape ) ) from skimage . color import rgb2gray im_gray = rgb2gray ( im ) print ( 'New image shape: {}' . format ( im_gray . shape ) )
419	from skimage . filters import threshold_otsu thresh_val = threshold_otsu ( im_gray ) mask = np . where ( im_gray > thresh_val , 1 , 0 ) if np . sum ( mask == 0 ) < np . sum ( mask == 1 ) : mask = np . where ( mask , 0 , 1 )
420	from scipy import ndimage labels , nlabels = ndimage . label ( mask ) label_arrays = [ ] for label_num in range ( 1 , nlabels + 1 ) : label_mask = np . where ( labels == label_num , 1 , 0 ) label_arrays . append ( label_mask ) print ( 'There are {} separate components / objects detected.' . format ( nlabels ) )
421	for label_ind , label_coords in enumerate ( ndimage . find_objects ( labels ) ) : cell = im_gray [ label_coords ] if np . product ( cell . shape ) < 10 : print ( 'Label {} is too small! Setting to 0.' . format ( label_ind ) ) mask = np . where ( labels == label_ind + 1 , 0 , mask ) labels , nlabels = ndimage . label ( mask ) print ( 'There are now {} separate components / objects detected.' . format ( nlabels ) )
422	two_cell_indices = ndimage . find_objects ( labels ) [ 1 ] cell_mask = mask [ two_cell_indices ] cell_mask_opened = ndimage . binary_opening ( cell_mask , iterations = 8 )
423	def rle_encoding ( x ) : dots = np . where ( x . T . flatten ( ) == 1 ) [ 0 ] run_lengths = [ ] prev = - 2 for b in dots : if ( b > prev + 1 ) : run_lengths . extend ( ( b + 1 , 0 ) ) run_lengths [ - 1 ] += 1 prev = b return " " . join ( [ str ( i ) for i in run_lengths ] ) print ( 'RLE Encoding for the current mask is: {}' . format ( rle_encoding ( label_mask ) ) )
424	import numpy as np import pandas as pd import matplotlib . pyplot as plt df_train = pd . read_csv ( '../input/train.csv' ) subset = df_train . loc [ ( df_train [ 'crew' ] == 1 ) & ( df_train [ 'experiment' ] == 'CA' ) ] subset . sort_values ( by = 'time' ) plt . plot ( subset [ 'r' ] [ 3000 : 4024 ] )
425	from scipy import signal b , a = signal . butter ( 8 , 0.05 ) y = signal . filtfilt ( b , a , subset [ 'r' ] , padlen = 150 ) plt . plot ( y [ 3000 : 4024 ] )
426	from biosppy . signals import ecg , resp out = resp . resp ( y , sampling_rate = 256 , show = False ) plt . plot ( out [ 'resp_rate_ts' ] , out [ 'resp_rate' ] ) plt . ylabel ( 'Respiratory frequency [Hz]' ) plt . xlabel ( 'Time [s]' ) ;
427	b , a = signal . butter ( 8 , 0.05 ) y = signal . filtfilt ( b , a , subset [ 'ecg' ] , padlen = 150 ) plt . plot ( y [ 3000 : 4024 ] )
428	out = ecg . ecg ( signal = subset [ 'ecg' ] , sampling_rate = 256 , show = False ) plt . plot ( out [ 'heart_rate_ts' ] , out [ 'heart_rate' ] ) plt . ylabel ( 'Heart Rate (BPM)' ) plt . xlabel ( 'Time [s]' ) ;
429	data_neg = processedtext [ : 800000 ] plt . figure ( figsize = ( 20 , 20 ) ) wc = WordCloud ( max_words = 1000 , width = 1600 , height = 800 , collocations = False ) . generate ( " " . join ( data_neg ) ) plt . imshow ( wc )
430	data_pos = processedtext [ 800000 : ] wc = WordCloud ( max_words = 1000 , width = 1600 , height = 800 , collocations = False ) . generate ( " " . join ( data_pos ) ) plt . figure ( figsize = ( 20 , 20 ) ) plt . imshow ( wc )
431	X_train , X_test , y_train , y_test = train_test_split ( processedtext , sentiment , test_size = 0.05 , random_state = 0 ) print ( f'Data Split done.' )
432	vectoriser = TfidfVectorizer ( ngram_range = ( 1 , 2 ) , max_features = 500000 ) vectoriser . fit ( X_train ) print ( f'Vectoriser fitted.' ) print ( 'No. of feature_words: ' , len ( vectoriser . get_feature_names ( ) ) )
433	X_train = vectoriser . transform ( X_train ) X_test = vectoriser . transform ( X_test ) print ( f'Data Transformed.' )
434	BNBmodel = BernoulliNB ( alpha = 2 ) BNBmodel . fit ( X_train , y_train ) model_Evaluate ( BNBmodel )
435	SVCmodel = LinearSVC ( ) SVCmodel . fit ( X_train , y_train ) model_Evaluate ( SVCmodel )
436	LRmodel = LogisticRegression ( C = 2 , max_iter = 1000 , n_jobs = - 1 ) LRmodel . fit ( X_train , y_train ) model_Evaluate ( LRmodel )
437	file = open ( 'vectoriser-ngram-(1,2).pickle' , 'wb' ) pickle . dump ( vectoriser , file ) file . close ( ) file = open ( 'Sentiment-LR.pickle' , 'wb' ) pickle . dump ( LRmodel , file ) file . close ( ) file = open ( 'Sentiment-BNB.pickle' , 'wb' ) pickle . dump ( BNBmodel , file ) file . close ( )
438	import os import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
439	concat_sub [ 'isFraud' ] = np . where ( np . all ( concat_sub . iloc [ : , 1 : ncol ] > cutoff_lo , axis = 1 ) , 1 , np . where ( np . all ( concat_sub . iloc [ : , 1 : ncol ] < cutoff_hi , axis = 1 ) , 0 , concat_sub [ 'ieee_median' ] ) ) concat_sub [ [ 'TransactionID' , 'isFraud' ] ] . to_csv ( 'stack_pushout_median.csv' , index = False , float_format = '%.6f' )
440	concat_sub [ 'isFraud' ] = np . where ( np . all ( concat_sub . iloc [ : , 1 : ncol ] > cutoff_lo , axis = 1 ) , concat_sub [ 'ieee_max' ] , np . where ( np . all ( concat_sub . iloc [ : , 1 : ncol ] < cutoff_hi , axis = 1 ) , concat_sub [ 'ieee_min' ] , concat_sub [ 'ieee_mean' ] ) ) concat_sub [ [ 'TransactionID' , 'isFraud' ] ] . to_csv ( 'stack_minmax_mean.csv' , index = False , float_format = '%.6f' )
441	concat_sub [ 'isFraud' ] = np . where ( np . all ( concat_sub . iloc [ : , 1 : ncol ] > cutoff_lo , axis = 1 ) , concat_sub [ 'ieee_max' ] , np . where ( np . all ( concat_sub . iloc [ : , 1 : ncol ] < cutoff_hi , axis = 1 ) , concat_sub [ 'ieee_min' ] , concat_sub [ 'ieee_median' ] ) ) concat_sub [ [ 'TransactionID' , 'isFraud' ] ] . to_csv ( 'stack_minmax_median.csv' , index = False , float_format = '%.6f' )
442	import numpy as np import pandas as pd import os
443	import librosa import IPython . display as ipd import matplotlib . pyplot as plt import librosa . display from matplotlib import gridspec from PIL import Image import warnings warnings . filterwarnings ( "ignore" )
444	X = librosa . stft ( data ) Xdb = librosa . amplitude_to_db ( abs ( X ) ) plt . figure ( figsize = ( 14 , 5 ) ) librosa . display . specshow ( Xdb , sr = sr , x_axis = 'time' , y_axis = 'hz' ) plt . colorbar ( )
445	n0 = 10000 n1 = 10100 plt . figure ( figsize = ( 14 , 5 ) ) plt . plot ( data [ n0 : n1 ] ) plt . grid ( )
446	plt . figure ( figsize = ( 14 , 5 ) ) spectral_rolloff = librosa . feature . spectral_rolloff ( data , sr = sr ) [ 0 ] librosa . display . waveplot ( data , sr = sr , alpha = 0.4 ) plt . plot ( t , normalize ( spectral_rolloff ) , color = 'r' )
447	mfccs = librosa . feature . mfcc ( data , sr = sr ) print ( mfccs . shape ) plt . figure ( figsize = ( 14 , 5 ) ) librosa . display . specshow ( mfccs , sr = sr , x_axis = 'time' )
448	title_grp = train_labels . groupby ( [ 'title' ] ) [ 'game_session' ] . count ( ) . reset_index ( ) display ( title_grp ) fig = go . Figure ( data = [ go . Pie ( labels = title_grp . title , values = title_grp . game_session ) ] ) fig . show ( )
449	print ( "Qualitative/Categorical Columns:" ) cate_cols = train_data . select_dtypes ( include = [ 'object' ] ) . columns print ( cate_cols ) print ( "\nQuntitative/Numerical Columns:" ) num_cols = train_data . select_dtypes ( exclude = [ 'object' ] ) . columns print ( num_cols )
450	event_count = test_data [ 'title' ] . value_counts ( ) . reset_index ( ) event_count [ 'index' ] = event_count [ 'index' ] . astype ( 'category' ) fig = px . bar ( event_count [ 0 : 10 ] , x = 'index' , y = 'title' , hover_data = [ 'title' ] , color = 'index' , labels = { 'title' : 'Event Count' } , height = 400 ) fig . show ( )
451	fig = px . bar ( event_count [ - 10 : ] , x = 'index' , y = 'title' , hover_data = [ 'title' ] , color = 'index' , labels = { 'title' : 'Event Count' } , height = 400 ) fig . show ( )
452	type_count = train_data [ 'type' ] . value_counts ( ) . reset_index ( ) total = len ( train_data ) type_count [ 'percent' ] = round ( ( type_count [ 'type' ] / total ) * 100 , 2 ) print ( type_count ) fig = px . bar ( type_count , x = 'index' , y = 'type' , hover_data = [ 'index' , 'percent' ] , color = 'type' , labels = { 'type' : 'Type Count' } , height = 400 ) fig . show ( )
453	type_count = test_data [ 'type' ] . value_counts ( ) . reset_index ( ) total = len ( test_data ) type_count [ 'percent' ] = round ( ( type_count [ 'type' ] / total ) * 100 , 2 ) print ( type_count ) fig = px . bar ( type_count , x = 'index' , y = 'type' , hover_data = [ 'index' , 'percent' ] , color = 'type' , labels = { 'type' : 'Type Count' } , height = 400 ) fig . show ( )
454	type_count = train_data [ 'world' ] . value_counts ( ) . reset_index ( ) total = len ( train_data ) type_count [ 'percent' ] = round ( ( type_count [ 'world' ] / total ) * 100 , 2 ) print ( type_count ) fig = px . bar ( type_count , x = 'index' , y = 'world' , hover_data = [ 'index' , 'percent' ] , color = 'world' , labels = { 'world' : 'World Count' } , height = 400 ) fig . show ( )
455	type_count = test_data [ 'world' ] . value_counts ( ) . reset_index ( ) total = len ( test_data ) type_count [ 'percent' ] = round ( ( type_count [ 'world' ] / total ) * 100 , 2 ) print ( type_count ) fig = px . bar ( type_count , x = 'index' , y = 'world' , hover_data = [ 'index' , 'percent' ] , color = 'world' , labels = { 'world' : 'World Count' } , height = 400 ) fig . show ( )
456	date_count = train_data . groupby ( [ 'date' ] ) [ 'installation_id' ] . count ( ) . reset_index ( ) fig = go . Figure ( data = go . Scatter ( x = date_count [ 'date' ] , y = date_count [ 'installation_id' ] ) ) fig . show ( )
457	hour_count = train_data . groupby ( [ 'hour' ] ) [ 'installation_id' ] . count ( ) . reset_index ( ) fig = go . Figure ( data = go . Scatter ( x = hour_count [ 'hour' ] , y = hour_count [ 'installation_id' ] ) ) fig . show ( )
458	week_count = train_data . groupby ( [ 'weekofyear' ] ) [ 'installation_id' ] . count ( ) . reset_index ( ) fig = go . Figure ( data = go . Scatter ( x = week_count [ 'weekofyear' ] , y = week_count [ 'installation_id' ] ) ) fig . show ( )
459	week_type_count = train_data . groupby ( [ 'weekofyear' , 'type' ] ) [ 'installation_id' ] . count ( ) . reset_index ( ) fig = px . line ( week_type_count , x = "weekofyear" , y = "installation_id" , color = 'type' ) fig . show ( )
460	date_title_count = merge_data . groupby ( [ 'hour' , 'title' ] ) [ 'game_session' ] . count ( ) . reset_index ( ) fig = px . line ( date_title_count , x = "hour" , y = "game_session" , color = 'title' ) fig . show ( )
461	date_title_count = merge_data . groupby ( [ 'title' ] ) [ 'game_time' ] . count ( ) . reset_index ( ) print ( date_title_count ) fig = px . line ( date_title_count , x = "title" , y = "game_time" ) fig . show ( )
462	date_title_count = test_data . groupby ( [ 'title' ] ) [ 'game_time' ] . count ( ) . reset_index ( ) date_title_count . sort_values ( by = [ 'game_time' ] , inplace = True , ascending = True ) print ( date_title_count ) fig = px . line ( date_title_count , x = "title" , y = "game_time" ) fig . show ( )
463	data = train_data [ train_data [ 'event_code' ] == 4030 ] data . shape print ( data [ 'title' ] . value_counts ( ) ) print ( data [ 'type' ] . value_counts ( ) ) print ( data [ 'world' ] . value_counts ( ) )
464	game = train_data . groupby ( [ 'title' ] ) [ 'game_time' ] . max ( ) . reset_index ( ) game . sort_values ( by = [ 'game_time' ] , inplace = True , ascending = False ) fig = px . bar ( game [ 0 : 10 ] , x = 'game_time' , y = 'title' , orientation = 'h' , hover_data = [ 'title' ] , color = 'game_time' , labels = { 'game_time' : 'Game Time' } , height = 400 ) fig . show ( )
465	world_type = train_data . groupby ( [ 'world' , 'type' ] ) [ 'game_session' ] . count ( ) . reset_index ( ) world_type plt . figure ( figsize = ( 20 , 7 ) ) ax = sns . barplot ( x = "world" , y = "game_session" , hue = "type" , data = world_type )
466	world_type = train_data . groupby ( [ 'world' , 'type' ] ) [ 'game_time' ] . mean ( ) . reset_index ( ) world_type plt . figure ( figsize = ( 20 , 7 ) ) ax = sns . barplot ( x = "world" , y = "game_time" , hue = "type" , data = world_type )
467	world_type = train_data . groupby ( [ 'world' , 'type' ] ) [ 'event_count' ] . count ( ) . reset_index ( ) world_type plt . figure ( figsize = ( 20 , 7 ) ) ax = sns . barplot ( x = "world" , y = "event_count" , hue = "type" , data = world_type )
468	world_type = merge_data . groupby ( [ 'world' , 'title' ] ) [ 'game_session' ] . count ( ) . reset_index ( ) world_type plt . figure ( figsize = ( 20 , 7 ) ) ax = sns . barplot ( x = "world" , y = "game_session" , hue = "title" , data = world_type )
469	def get_unique ( data , feat ) : return data [ feat ] . nunique ( ) for col in train_labels . columns . values : print ( "unique number of values in " , col ) print ( get_unique ( train_labels , col ) )
470	max_game = train_data . groupby ( [ 'game_session' ] ) [ 'game_time' ] . max ( ) . reset_index ( ) max_game . sort_values ( by = [ 'game_time' ] , inplace = True , ascending = False ) max_game [ 'hrs' ] = pd . to_timedelta ( max_game [ 'game_time' ] , unit = 'ms' ) print ( max_game [ 'hrs' ] [ 0 : 10 ] )
471	evnt_time = train_data . groupby ( 'event_code' ) [ 'game_time' ] . max ( ) . reset_index ( ) evnt_time . sort_values ( by = [ 'game_time' ] , inplace = True , ascending = False ) evnt_time plt . figure ( figsize = ( 20 , 7 ) ) ax = sns . barplot ( x = "event_code" , y = "game_time" , hue = "event_code" , data = evnt_time [ 0 : 10 ] )
472	dd = test_data . groupby ( 'date' ) [ 'world' ] . value_counts ( ) dd = dd . reset_index ( name = 'count' ) dd fig = px . line ( dd , x = "date" , y = "count" , color = 'world' ) fig . show ( )
473	fig , ax = plt . subplots ( 5 , 2 , figsize = ( 30 , 20 ) ) for i in range ( 10 ) : sns . countplot ( data [ f'choice_{i}' ] , ax = ax [ i // 2 ] [ i % 2 ] ) plt . show ( )
474	fig , ax = plt . subplots ( 1 , 1 , figsize = ( 12 , 5 ) ) sns . countplot ( data [ 'n_people' ] , ax = ax ) plt . show ( )
475	fig , ax = plt . subplots ( 5 , 2 , figsize = ( 30 , 20 ) ) data_people = data . groupby ( 'n_people' ) for i in range ( 10 ) : sns . heatmap ( data_people [ f'choice_{i}' ] . value_counts ( ) . unstack ( ) . fillna ( 0 ) , ax = ax [ i // 2 ] [ i % 2 ] ) plt . show ( )
476	fig = go . Figure ( ) fig . add_trace ( go . Line ( x = list ( range ( len ( penalty_list ) ) ) , y = penalty_list , name = 'Preference cost' , marker_color = "forestgreen" ) ) fig . add_trace ( go . Line ( x = list ( range ( len ( cost_list ) ) ) , y = cost_list , name = 'Accounting cost' , marker_color = "salmon" ) ) fig . update_layout ( title = "Compare 2 Cost : Preference & Accounting" ) fig . show ( )
477	train = pd . read_csv ( '/kaggle/input/cat-in-the-dat/train.csv' ) test = pd . read_csv ( '/kaggle/input/cat-in-the-dat/test.csv' ) target = train [ 'target' ] train_id = train [ 'id' ] test_id = test [ 'id' ] train . drop ( [ 'target' , 'id' ] , axis = 1 , inplace = True ) test . drop ( 'id' , axis = 1 , inplace = True )
478	LE_encoder = OrdinalEncoder ( feature_list ) train_le = LE_encoder . fit_transform ( train ) test_le = LE_encoder . transform ( test )
479	TE_encoder = TargetEncoder ( ) train_te = TE_encoder . fit_transform ( train [ feature_list ] , target ) test_te = TE_encoder . transform ( test [ feature_list ] ) train_te . head ( )
480	MEE_encoder = MEstimateEncoder ( ) train_mee = MEE_encoder . fit_transform ( train [ feature_list ] , target ) test_mee = MEE_encoder . transform ( test [ feature_list ] )
481	WOE_encoder = WOEEncoder ( ) train_woe = WOE_encoder . fit_transform ( train [ feature_list ] , target ) test_woe = WOE_encoder . transform ( test [ feature_list ] )
482	JSE_encoder = JamesSteinEncoder ( ) train_jse = JSE_encoder . fit_transform ( train [ feature_list ] , target ) test_jse = JSE_encoder . transform ( test [ feature_list ] )
483	LOOE_encoder = LeaveOneOutEncoder ( ) train_looe = LOOE_encoder . fit_transform ( train [ feature_list ] , target ) test_looe = LOOE_encoder . transform ( test [ feature_list ] )
484	CBE_encoder = CatBoostEncoder ( ) train_cbe = CBE_encoder . fit_transform ( train [ feature_list ] , target ) test_cbe = CBE_encoder . transform ( test [ feature_list ] )
485	if TEST : for idx , label in enumerate ( results [ 'label' ] ) : sub_df = pd . DataFrame ( { 'id' : test_id , 'target' : results . iloc [ idx ] [ 'test' ] } ) sub_df . to_csv ( "LR_{}.csv" . format ( label ) , index = False )
486	fig , ax = plt . subplots ( 1 , 5 , figsize = ( 30 , 8 ) ) for i in range ( 5 ) : sns . countplot ( f'bin_{i}' , data = train , ax = ax [ i ] ) ax [ i ] . set_ylim ( [ 0 , 600000 ] ) ax [ i ] . set_title ( f'bin_{i}' , fontsize = 15 ) fig . suptitle ( "Binary Feature Distribution (Train Data)" , fontsize = 20 ) plt . show ( )
487	fig , ax = plt . subplots ( 1 , 5 , figsize = ( 30 , 8 ) ) for i in range ( 5 ) : sns . countplot ( f'bin_{i}' , hue = 'target' , data = train , ax = ax [ i ] ) ax [ i ] . set_ylim ( [ 0 , 500000 ] ) ax [ i ] . set_title ( f'bin_{i}' , fontsize = 15 ) fig . suptitle ( "Binary Feature Distribution (Train Data)" , fontsize = 20 ) plt . show ( )
488	from category_encoders . target_encoder import TargetEncoder for i in range ( 10 ) : label = TargetEncoder ( ) train [ f'nom_{i}' ] = label . fit_transform ( train [ f'nom_{i}' ] . fillna ( 'NULL' ) , target ) test [ f'nom_{i}' ] = label . transform ( test [ f'nom_{i}' ] . fillna ( 'NULL' ) )
489	import os import pandas as pd for dirname , _ , filenames in os . walk ( '/kaggle/' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
490	fig , ax = plt . subplots ( 1 , 4 , figsize = ( 20 , 5 ) ) for i in range ( 4 ) : sns . countplot ( f'bin_{i}' , hue = 'target' , data = train_df , ax = ax [ i ] ) ax [ i ] . set_title ( f'bin_{i} feature countplot' ) print ( percentage_of_feature_target ( train_df , f'bin_{i}' , 'target' , 1 ) ) plt . show ( )
491	fig , ax = plt . subplots ( 4 , 1 , figsize = ( 40 , 40 ) ) for i in range ( 5 , 9 ) : sns . countplot ( sorted ( train_df [ f'nom_{i}' ] ) , ax = ax [ i - 5 ] ) plt . setp ( ax [ i - 5 ] . get_xticklabels ( ) , rotation = 90 ) plt . show ( ) ;
492	for i in range ( 5 , 9 ) : fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 2 ) ) P_nom = percentage_of_feature_target ( train_df , f'nom_{i}' , 'target' , 1 ) P_nom . plot ( ) plt . show ( ) ;
493	P_ord5 = percentage_of_feature_target ( train_df , 'ord_5' , 'target' , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 20 , 7 ) ) sns . barplot ( P_ord5 . index , P_ord5 , ax = ax ) plt . title ( 'ord_5 : Percentage of target==1 in dictionary order' ) plt . setp ( ax . get_xticklabels ( ) , rotation = 90 , fontsize = 5 ) plt . show ( )
494	import os import numpy as np import pandas as pd from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
495	import os import numpy as np import pandas as pd from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
496	import os import numpy as np import pandas as pd from sklearn . metrics import log_loss from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
497	files = [ 'out1' , 'out2' , 'out3' , 'out4' , 'out5' ] ranking = [ ] for file in files : ranking . append ( log_loss ( labels , eval ( file ) ) )
498	import numpy as np import pandas as pd import matplotlib . pyplot as plt from scipy . ndimage import gaussian_filter from skimage import img_as_float from skimage . morphology import reconstruction train = pd . read_json ( '../input/train.json' ) from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
499	import numpy as np import pandas as pd import matplotlib . pyplot as plt from scipy . ndimage import gaussian_filter from skimage import img_as_float from skimage . morphology import reconstruction train = pd . read_json ( '../input/train.json' ) from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
500	pred_noemb_val_y = model . predict ( [ val_X ] , batch_size = 1024 , verbose = 1 ) for thresh in np . arange ( 0.1 , 0.501 , 0.01 ) : thresh = np . round ( thresh , 2 ) print ( "F1 score at threshold {0} is {1}" . format ( thresh , metrics . f1_score ( val_y , ( pred_noemb_val_y > thresh ) . astype ( int ) ) ) )
501	del model , inp , x import gc ; gc . collect ( ) time . sleep ( 10 )
502	pred_val_y = 0.33 * pred_glove_val_y + 0.33 * pred_fasttext_val_y + 0.34 * pred_paragram_val_y for thresh in np . arange ( 0.1 , 0.501 , 0.01 ) : thresh = np . round ( thresh , 2 ) print ( "F1 score at threshold {0} is {1}" . format ( thresh , metrics . f1_score ( val_y , ( pred_val_y > thresh ) . astype ( int ) ) ) )
503	pred_test_y = 0.33 * pred_glove_test_y + 0.33 * pred_fasttext_test_y + 0.34 * pred_paragram_test_y pred_test_y = ( pred_test_y > 0.35 ) . astype ( int ) out_df = pd . DataFrame ( { "qid" : test_df [ "qid" ] . values } ) out_df [ 'prediction' ] = pred_test_y out_df . to_csv ( "submission.csv" , index = False )
504	train_df [ "null_count" ] = train_df . isnull ( ) . sum ( axis = 1 ) test_df [ "null_count" ] = test_df . isnull ( ) . sum ( axis = 1 ) plt . figure ( figsize = ( 14 , 12 ) ) sns . pointplot ( x = 'null_count' , y = 'price_doc' , data = train_df ) plt . ylabel ( 'price_doc' , fontsize = 12 ) plt . xlabel ( 'null_count' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
505	train_df [ "ratio_floor_max_floor" ] = train_df [ "floor" ] / train_df [ "max_floor" ] . astype ( "float" ) test_df [ "ratio_floor_max_floor" ] = test_df [ "floor" ] / test_df [ "max_floor" ] . astype ( "float" ) train_df [ "floor_from_top" ] = train_df [ "max_floor" ] - train_df [ "floor" ] test_df [ "floor_from_top" ] = test_df [ "max_floor" ] - test_df [ "floor" ]
506	def add_count ( df , group_col ) : grouped_df = df . groupby ( group_col ) [ "id" ] . aggregate ( "count" ) . reset_index ( ) grouped_df . columns = [ group_col , "count_" + group_col ] df = pd . merge ( df , grouped_df , on = group_col , how = "left" ) return df train_df = add_count ( train_df , "yearmonth" ) test_df = add_count ( test_df , "yearmonth" ) train_df = add_count ( train_df , "yearweek" ) test_df = add_count ( test_df , "yearweek" )
507	train_df [ "ratio_preschool" ] = train_df [ "children_preschool" ] / train_df [ "preschool_quota" ] . astype ( "float" ) test_df [ "ratio_preschool" ] = test_df [ "children_preschool" ] / test_df [ "preschool_quota" ] . astype ( "float" ) train_df [ "ratio_school" ] = train_df [ "children_school" ] / train_df [ "school_quota" ] . astype ( "float" ) test_df [ "ratio_school" ] = test_df [ "children_school" ] / test_df [ "school_quota" ] . astype ( "float" )
508	val_time = 201407 dev_indices = np . where ( train_X [ "yearmonth" ] < val_time ) val_indices = np . where ( train_X [ "yearmonth" ] >= val_time ) dev_X = train_X . ix [ dev_indices ] val_X = train_X . ix [ val_indices ] dev_y = train_y [ dev_indices ] val_y = train_y [ val_indices ] print ( dev_X . shape , val_X . shape )
509	fig = plt . figure ( figsize = ( 12 , 6 ) ) plt . plot ( ts_list , r1_overall_reward_list , c = 'blue' ) plt . plot ( ts_list , [ 0 ] * len ( ts_list ) , c = 'red' ) plt . title ( "Cumulative R value change for Univariate Ridge (technical_20)" ) plt . ylim ( [ - 0.04 , 0.04 ] ) plt . xlim ( [ 850 , 1850 ] ) plt . show ( )
510	train [ 'age' ] = train [ 'age' ] . astype ( 'float64' ) age_series = train . age . value_counts ( ) plt . figure ( figsize = ( 12 , 4 ) ) sns . barplot ( age_series . index . astype ( 'int' ) , age_series . values , alpha = 0.8 ) plt . ylabel ( 'Number of Occurrences of the customer' , fontsize = 12 ) plt . xlabel ( 'Age' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
511	col_series = train . antiguedad . value_counts ( ) plt . figure ( figsize = ( 12 , 4 ) ) sns . barplot ( col_series . index . astype ( 'int' ) , col_series . values , alpha = 0.8 ) plt . ylabel ( 'Number of Occurrences of the customer' , fontsize = 12 ) plt . xlabel ( 'Customer Seniority' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
512	train . fillna ( 101850. , inplace = True ) quantile_series = train . renta . quantile ( np . arange ( 0.99 , 1 , 0.001 ) ) plt . figure ( figsize = ( 12 , 4 ) ) sns . barplot ( ( quantile_series . index * 100 ) , quantile_series . values , alpha = 0.8 ) plt . ylabel ( 'Rent value' , fontsize = 12 ) plt . xlabel ( 'Quantile value' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
513	rent_max_cap = train . renta . quantile ( 0.999 ) train [ 'renta' ] [ train [ 'renta' ] > rent_max_cap ] = 101850.0 sns . boxplot ( train . renta . values ) plt . show ( )
514	test = pd . read_csv ( test_file , usecols = [ 'renta' ] ) test [ 'renta' ] = test [ 'renta' ] . replace ( to_replace = [ ' NA' ] , value = np . nan ) . astype ( 'float' ) unique_values = np . sort ( test . renta . unique ( ) ) plt . scatter ( range ( len ( unique_values ) ) , unique_values ) plt . show ( )
515	train_df = pd . read_csv ( '../input/nyc-taxi-trip-duration/train.csv' , parse_dates = [ 'pickup_datetime' ] ) test_df = pd . read_csv ( '../input/nyc-taxi-trip-duration/test.csv' , parse_dates = [ 'pickup_datetime' ] ) print ( "Train dataframe shape : " , train_df . shape ) print ( "Test dataframe shape : " , test_df . shape )
516	train_df [ 'log_trip_duration' ] = np . log1p ( train_df [ 'trip_duration' ] . values ) plt . figure ( figsize = ( 8 , 6 ) ) plt . scatter ( range ( train_df . shape [ 0 ] ) , np . sort ( train_df . log_trip_duration . values ) ) plt . xlabel ( 'index' , fontsize = 12 ) plt . ylabel ( 'log_trip_duration' , fontsize = 12 ) plt . show ( )
517	null_count_df = train_df . isnull ( ) . sum ( axis = 0 ) . reset_index ( ) null_count_df . columns = [ 'col_name' , 'null_count' ] null_count_df
518	train_df [ 'pickup_date' ] = train_df [ 'pickup_datetime' ] . dt . date test_df [ 'pickup_date' ] = test_df [ 'pickup_datetime' ] . dt . date cnt_srs = train_df [ 'pickup_date' ] . value_counts ( ) plt . figure ( figsize = ( 12 , 4 ) ) ax = plt . subplot ( 111 ) ax . bar ( cnt_srs . index , cnt_srs . values , alpha = 0.8 ) ax . xaxis_date ( ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
519	temp_series = train_df [ "project_is_approved" ] . value_counts ( ) labels = ( np . array ( temp_series . index ) ) sizes = ( np . array ( ( temp_series / temp_series . sum ( ) ) * 100 ) ) trace = go . Pie ( labels = labels , values = sizes ) layout = go . Layout ( title = 'Project Proposal is Approved' ) data = [ trace ] fig = go . Figure ( data = data , layout = layout ) py . iplot ( fig , filename = "ProjectApproval" )
520	resource_df = pd . read_csv ( "../input/resources.csv" ) train_df = pd . merge ( train_df , resource_df , on = "id" , how = 'left' ) test_df = pd . merge ( test_df , resource_df , on = "id" , how = 'left' ) resource_df . head ( )
521	trace = go . Histogram ( x = np . log1p ( train_df [ "price" ] ) , nbinsx = 50 , opacity = 0.75 ) data = [ trace ] layout = go . Layout ( title = "Log Histogram of the prices of project proposal" , ) fig = go . Figure ( data = data , layout = layout ) py . iplot ( fig , filename = 'ProjectGradePerc' )
522	< a href = "http://a.video.nfl.com//films/vodzilla/153321/Lechler_55_yd_punt-lG1K51rf-20181119_173634665_5000k.mp4" > ( 2 : 57 ) ( Punt formation ) S . Lechler punts 48 yards to TEN 16 , Center - J . Weeks . A . Jackson pushed ob at TEN 32 for 16 yards ( J . Jenkins ) . < / a > < img src = "https://s3.amazonaws.com/nonwebstorage/headstrong/animation_585_733_3.gif" width = "650" >
523	import pandas as pd import numpy as np import matplotlib . pyplot as plt import seaborn as sns sns . set_palette ( "Set2" ) import os print ( os . listdir ( '../input/nfl-big-data-bowl-2020' ) )
524	fig , ax = plt . subplots ( 1 , 2 , figsize = ( 18 , 7 ) ) sns . distplot ( train_df [ 'X' ] , ax = ax [ 0 ] ) sns . distplot ( train_df [ 'Y' ] , ax = ax [ 1 ] ) plt . show ( )
525	from wordcloud import WordCloud fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 7 ) ) train_df [ 'GameWeather' ] = train_df [ 'GameWeather' ] . apply ( str ) wordcloud = WordCloud ( background_color = 'white' ) . generate ( " " . join ( train_df [ 'GameWeather' ] ) ) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( "off" ) plt . show ( )
526	sns . set_palette ( 'bright' ) fig , ax = plt . subplots ( 2 , 1 , figsize = ( 20 , 10 ) ) for idx , elem in enumerate ( [ "Temperature" , "Humidity" ] ) : sns . distplot ( train_df [ elem ] . dropna ( ) , ax = ax [ idx ] ) plt . show ( )
527	train_df = pd . read_csv ( "../input/train.csv" , parse_dates = [ "activation_date" ] ) test_df = pd . read_csv ( "../input/test.csv" , parse_dates = [ "activation_date" ] ) print ( "Train file rows and columns are : " , train_df . shape ) print ( "Test file rows and columns are : " , test_df . shape )
528	from io import StringIO temp_data = StringIO ( ) region_df = pd . read_csv ( temp_data ) train_df = pd . merge ( train_df , region_df , how = "left" , on = "region" )
529	plt . figure ( figsize = ( 12 , 8 ) ) sns . boxplot ( y = "region_en" , x = "deal_probability" , data = train_df ) plt . xlabel ( 'Deal probability' , fontsize = 12 ) plt . ylabel ( 'Region' , fontsize = 12 ) plt . title ( "Deal probability by region" ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
530	temp_data = StringIO ( ) temp_df = pd . read_csv ( temp_data ) train_df = pd . merge ( train_df , temp_df , on = "parent_category_name" , how = "left" )
531	plt . figure ( figsize = ( 12 , 8 ) ) sns . boxplot ( x = "parent_category_name_en" , y = "deal_probability" , data = train_df ) plt . ylabel ( 'Deal probability' , fontsize = 12 ) plt . xlabel ( 'Parent Category' , fontsize = 12 ) plt . title ( "Deal probability by parent category" , fontsize = 14 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
532	temp_data = StringIO ( ) temp_df = pd . read_csv ( temp_data ) train_df = pd . merge ( train_df , temp_df , on = "category_name" , how = "left" )
533	train_df [ "price_new" ] = train_df [ "price" ] . values train_df [ "price_new" ] . fillna ( np . nanmean ( train_df [ "price" ] . values ) , inplace = True ) plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( np . log1p ( train_df [ "price_new" ] . values ) , bins = 100 , kde = False ) plt . xlabel ( 'Log of price' , fontsize = 12 ) plt . title ( "Log of Price Histogram" , fontsize = 14 ) plt . show ( )
534	from matplotlib_venn import venn2 plt . figure ( figsize = ( 10 , 7 ) ) venn2 ( [ set ( train_df . user_id . unique ( ) ) , set ( test_df . user_id . unique ( ) ) ] , set_labels = ( 'Train set' , 'Test set' ) ) plt . title ( "Number of users in train and test" , fontsize = 15 ) plt . show ( )
535	from matplotlib_venn import venn2 plt . figure ( figsize = ( 10 , 7 ) ) venn2 ( [ set ( train_df . title . unique ( ) ) , set ( test_df . title . unique ( ) ) ] , set_labels = ( 'Train set' , 'Test set' ) ) plt . title ( "Number of titles in train and test" , fontsize = 15 ) plt . show ( )
536	fig , ax = plt . subplots ( figsize = ( 12 , 18 ) ) lgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) ax . grid ( False ) plt . title ( "LightGBM - Feature Importance" , fontsize = 15 ) plt . show ( )
537	nzi = pd . notnull ( train_df [ "totals.transactionRevenue" ] ) . sum ( ) nzr = ( gdf [ "totals.transactionRevenue" ] > 0 ) . sum ( ) print ( "Number of instances in train set with non-zero revenue : " , nzi , " and ratio is : " , nzi / train_df . shape [ 0 ] ) print ( "Number of unique customers with non-zero revenue : " , nzr , "and the ratio is : " , nzr / gdf . shape [ 0 ] )
538	print ( "Number of unique visitors in train set : " , train_df . fullVisitorId . nunique ( ) , " out of rows : " , train_df . shape [ 0 ] ) print ( "Number of unique visitors in test set : " , test_df . fullVisitorId . nunique ( ) , " out of rows : " , test_df . shape [ 0 ] ) print ( "Number of common visitors in train and test set : " , len ( set ( train_df . fullVisitorId . unique ( ) ) . intersection ( set ( test_df . fullVisitorId . unique ( ) ) ) ) )
539	cols_to_drop = const_cols + [ 'sessionId' ] train_df = train_df . drop ( cols_to_drop + [ "trafficSource.campaignCode" ] , axis = 1 ) test_df = test_df . drop ( cols_to_drop , axis = 1 )
540	sub_df = pd . DataFrame ( { "fullVisitorId" : test_id } ) pred_test [ pred_test < 0 ] = 0 sub_df [ "PredictedLogRevenue" ] = np . expm1 ( pred_test ) sub_df = sub_df . groupby ( "fullVisitorId" ) [ "PredictedLogRevenue" ] . sum ( ) . reset_index ( ) sub_df . columns = [ "fullVisitorId" , "PredictedLogRevenue" ] sub_df [ "PredictedLogRevenue" ] = np . log1p ( sub_df [ "PredictedLogRevenue" ] ) sub_df . to_csv ( "baseline_lgb.csv" , index = False )
541	fig , ax = plt . subplots ( figsize = ( 12 , 18 ) ) lgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) ax . grid ( False ) plt . title ( "LightGBM - Feature Importance" , fontsize = 15 ) plt . show ( )
542	train_df = pd . read_csv ( "../input/train.csv" ) test_df = pd . read_csv ( "../input/test.csv" ) print ( "Train rows and columns : " , train_df . shape ) print ( "Test rows and columns : " , test_df . shape )
543	plt . figure ( figsize = ( 8 , 6 ) ) plt . scatter ( range ( train_df . shape [ 0 ] ) , np . sort ( train_df [ 'target' ] . values ) ) plt . xlabel ( 'index' , fontsize = 12 ) plt . ylabel ( 'Target' , fontsize = 12 ) plt . title ( "Target Distribution" , fontsize = 14 ) plt . show ( )
544	plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( train_df [ "target" ] . values , bins = 50 , kde = False ) plt . xlabel ( 'Target' , fontsize = 12 ) plt . title ( "Target Histogram" , fontsize = 14 ) plt . show ( )
545	plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( np . log1p ( train_df [ "target" ] . values ) , bins = 50 , kde = False ) plt . xlabel ( 'Target' , fontsize = 12 ) plt . title ( "Log of Target Histogram" , fontsize = 14 ) plt . show ( )
546	missing_df = train_df . isnull ( ) . sum ( axis = 0 ) . reset_index ( ) missing_df . columns = [ 'column_name' , 'missing_count' ] missing_df = missing_df [ missing_df [ 'missing_count' ] > 0 ] missing_df = missing_df . sort_values ( by = 'missing_count' ) missing_df
547	dtype_df = train_df . dtypes . reset_index ( ) dtype_df . columns = [ "Count" , "Column Type" ] dtype_df . groupby ( "Column Type" ) . aggregate ( 'count' ) . reset_index ( )
548	unique_df = train_df . nunique ( ) . reset_index ( ) unique_df . columns = [ "col_name" , "unique_count" ] constant_df = unique_df [ unique_df [ "unique_count" ] == 1 ] constant_df . shape
549	cols_to_use = corr_df [ ( corr_df [ 'corr_values' ] > 0.11 ) | ( corr_df [ 'corr_values' ] < - 0.11 ) ] . col_labels . tolist ( ) temp_df = train_df [ cols_to_use ] corrmat = temp_df . corr ( method = 'spearman' ) f , ax = plt . subplots ( figsize = ( 20 , 20 ) ) sns . heatmap ( corrmat , vmax = 1. , square = True , cmap = "YlGnBu" , annot = True ) plt . title ( "Important variables correlation map" , fontsize = 15 ) plt . show ( )
550	train_X = train_df . drop ( constant_df . col_name . tolist ( ) + [ "ID" , "target" ] , axis = 1 ) test_X = test_df . drop ( constant_df . col_name . tolist ( ) + [ "ID" ] , axis = 1 ) train_y = np . log1p ( train_df [ "target" ] . values )
551	kf = model_selection . KFold ( n_splits = 5 , shuffle = True , random_state = 2017 ) pred_test_full = 0 for dev_index , val_index in kf . split ( train_X ) : dev_X , val_X = train_X . loc [ dev_index , : ] , train_X . loc [ val_index , : ] dev_y , val_y = train_y [ dev_index ] , train_y [ val_index ] pred_test , model , evals_result = run_lgb ( dev_X , dev_y , val_X , val_y , test_X ) pred_test_full += pred_test pred_test_full /= 5. pred_test_full = np . expm1 ( pred_test_full )
552	sub_df = pd . DataFrame ( { "ID" : test_df [ "ID" ] . values } ) sub_df [ "target" ] = pred_test_full sub_df . to_csv ( "baseline_lgb.csv" , index = False )
553	fig , ax = plt . subplots ( figsize = ( 12 , 18 ) ) lgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) ax . grid ( False ) plt . title ( "LightGBM - Feature Importance" , fontsize = 15 ) plt . show ( )
554	train_df = pd . read_csv ( "../input/train.csv" , parse_dates = [ "first_active_month" ] ) test_df = pd . read_csv ( "../input/test.csv" , parse_dates = [ "first_active_month" ] ) print ( "Number of rows and columns in train set : " , train_df . shape ) print ( "Number of rows and columns in test set : " , test_df . shape )
555	target_col = "target" plt . figure ( figsize = ( 8 , 6 ) ) plt . scatter ( range ( train_df . shape [ 0 ] ) , np . sort ( train_df [ target_col ] . values ) ) plt . xlabel ( 'index' , fontsize = 12 ) plt . ylabel ( 'Loyalty Score' , fontsize = 12 ) plt . show ( )
556	gdf = hist_df . groupby ( "card_id" ) gdf = gdf [ "purchase_amount" ] . size ( ) . reset_index ( ) gdf . columns = [ "card_id" , "num_hist_transactions" ] train_df = pd . merge ( train_df , gdf , on = "card_id" , how = "left" ) test_df = pd . merge ( test_df , gdf , on = "card_id" , how = "left" )
557	gdf = hist_df . groupby ( "card_id" ) gdf = gdf [ "purchase_amount" ] . agg ( [ 'sum' , 'mean' , 'std' , 'min' , 'max' ] ) . reset_index ( ) gdf . columns = [ "card_id" , "sum_hist_trans" , "mean_hist_trans" , "std_hist_trans" , "min_hist_trans" , "max_hist_trans" ] train_df = pd . merge ( train_df , gdf , on = "card_id" , how = "left" ) test_df = pd . merge ( test_df , gdf , on = "card_id" , how = "left" )
558	gdf = new_trans_df . groupby ( "card_id" ) gdf = gdf [ "purchase_amount" ] . agg ( [ 'sum' , 'mean' , 'std' , 'min' , 'max' ] ) . reset_index ( ) gdf . columns = [ "card_id" , "sum_merch_trans" , "mean_merch_trans" , "std_merch_trans" , "min_merch_trans" , "max_merch_trans" ] train_df = pd . merge ( train_df , gdf , on = "card_id" , how = "left" ) test_df = pd . merge ( test_df , gdf , on = "card_id" , how = "left" )
559	features = [ 'cloud_coverage' , 'precip_depth_1_hr' , 'wind_direction' , 'wind_speed' ] weather_train [ features ] = weather_train [ features ] . fillna ( 0 ) weather_test [ features ] = weather_test [ features ] . fillna ( 0 ) weather_train . head ( )
560	data = list ( ) for i in building_metadata [ 'primary_use' ] . unique ( ) : data . append ( go . Bar ( name = i , x = building_metadata [ 'site_id' ] . unique ( ) , y = building_metadata [ building_metadata [ 'primary_use' ] == i ] [ 'site_id' ] . value_counts ( ) . sort_index ( ) ) ) fig = go . Figure ( data = data ) fig . update_layout ( barmode = 'stack' ) fig . show ( )
561	fig , ax = plt . subplots ( 1 , 2 , figsize = ( 17 , 5 ) ) sns . distplot ( weather_train [ 'wind_direction' ] , ax = ax [ 0 ] ) sns . distplot ( weather_train [ 'wind_speed' ] , ax = ax [ 1 ] ) plt . show ( )
562	def cloud_graph ( df ) : df = df . sort_values ( 'timestamp' ) fig = go . Figure ( ) fig . add_trace ( go . Scatter ( x = df [ 'timestamp' ] , y = df [ 'cloud_coverage' ] , name = "Cloud Coverage" , line_color = 'lightskyblue' , opacity = 0.7 ) ) fig . update_layout ( template = 'plotly_dark' , title_text = 'Cloud' , xaxis_rangeslider_visible = True ) fig . show ( )
563	test_df = pd . read_json ( "../input/test.json" ) print ( "Train Rows : " , train_df . shape [ 0 ] ) print ( "Test Rows : " , test_df . shape [ 0 ] )
564	int_level = train_df [ 'interest_level' ] . value_counts ( ) plt . figure ( figsize = ( 8 , 4 ) ) sns . barplot ( int_level . index , int_level . values , alpha = 0.8 , color = color [ 1 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'Interest level' , fontsize = 12 ) plt . show ( )
565	cnt_srs = train_df [ 'bathrooms' ] . value_counts ( ) plt . figure ( figsize = ( 8 , 4 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 0 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'bathrooms' , fontsize = 12 ) plt . show ( )
566	cnt_srs = train_df [ 'bedrooms' ] . value_counts ( ) plt . figure ( figsize = ( 8 , 4 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 2 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'bedrooms' , fontsize = 12 ) plt . show ( )
567	plt . figure ( figsize = ( 8 , 6 ) ) plt . scatter ( range ( train_df . shape [ 0 ] ) , np . sort ( train_df . price . values ) ) plt . xlabel ( 'index' , fontsize = 12 ) plt . ylabel ( 'price' , fontsize = 12 ) plt . show ( )
568	ulimit = np . percentile ( train_df . price . values , 99 ) train_df [ 'price' ] . ix [ train_df [ 'price' ] > ulimit ] = ulimit plt . figure ( figsize = ( 8 , 6 ) ) sns . distplot ( train_df . price . values , bins = 50 , kde = True ) plt . xlabel ( 'price' , fontsize = 12 ) plt . show ( )
569	train_df [ "created" ] = pd . to_datetime ( train_df [ "created" ] ) train_df [ "date_created" ] = train_df [ "created" ] . dt . date cnt_srs = train_df [ 'date_created' ] . value_counts ( ) plt . figure ( figsize = ( 12 , 4 ) ) ax = plt . subplot ( 111 ) ax . bar ( cnt_srs . index , cnt_srs . values , alpha = 0.8 ) ax . xaxis_date ( ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
570	test_df [ "created" ] = pd . to_datetime ( test_df [ "created" ] ) test_df [ "date_created" ] = test_df [ "created" ] . dt . date cnt_srs = test_df [ 'date_created' ] . value_counts ( ) plt . figure ( figsize = ( 12 , 4 ) ) ax = plt . subplot ( 111 ) ax . bar ( cnt_srs . index , cnt_srs . values , alpha = 0.8 ) ax . xaxis_date ( ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
571	train_df [ "hour_created" ] = train_df [ "created" ] . dt . hour cnt_srs = train_df [ 'hour_created' ] . value_counts ( ) plt . figure ( figsize = ( 12 , 6 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 3 ] ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
572	train_df [ "num_photos" ] = train_df [ "photos" ] . apply ( len ) cnt_srs = train_df [ 'num_photos' ] . value_counts ( ) plt . figure ( figsize = ( 12 , 6 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 ) plt . xlabel ( 'Number of Photos' , fontsize = 12 ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . show ( )
573	train_df [ "num_features" ] = train_df [ "features" ] . apply ( len ) cnt_srs = train_df [ 'num_features' ] . value_counts ( ) plt . figure ( figsize = ( 12 , 6 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'Number of features' , fontsize = 12 ) plt . show ( )
574	labels = [ ] values = [ ] for col in df . columns : labels . append ( col ) values . append ( df [ col ] . isnull ( ) . sum ( ) ) print ( col , values [ - 1 ] )
575	cols_to_use = [ 'technical_30' , 'technical_20' , 'fundamental_11' , 'technical_19' ] fig = plt . figure ( figsize = ( 8 , 20 ) ) plot_count = 0 for col in cols_to_use : plot_count += 1 plt . subplot ( 4 , 1 , plot_count ) plt . scatter ( range ( df . shape [ 0 ] ) , df [ col ] . values ) plt . title ( "Distribution of " + col ) plt . show ( )
576	plt . figure ( figsize = ( 8 , 5 ) ) plt . scatter ( range ( df . shape [ 0 ] ) , df . y . values ) plt . show ( )
577	fig = plt . figure ( figsize = ( 12 , 6 ) ) sns . countplot ( x = 'timestamp' , data = df ) plt . show ( )
578	plt . figure ( figsize = ( 8 , 6 ) ) plt . scatter ( range ( train_df . shape [ 0 ] ) , np . sort ( train_df . y . values ) ) plt . xlabel ( 'index' , fontsize = 12 ) plt . ylabel ( 'y' , fontsize = 12 ) plt . show ( )
579	ulimit = 180 train_df [ 'y' ] . ix [ train_df [ 'y' ] > ulimit ] = ulimit plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( train_df . y . values , bins = 50 , kde = False ) plt . xlabel ( 'y value' , fontsize = 12 ) plt . show ( )
580	dtype_df = train_df . dtypes . reset_index ( ) dtype_df . columns = [ "Count" , "Column Type" ] dtype_df . groupby ( "Column Type" ) . aggregate ( 'count' ) . reset_index ( )
581	missing_df = train_df . isnull ( ) . sum ( axis = 0 ) . reset_index ( ) missing_df . columns = [ 'column_name' , 'missing_count' ] missing_df = missing_df . ix [ missing_df [ 'missing_count' ] > 0 ] missing_df = missing_df . sort_values ( by = 'missing_count' ) missing_df
582	var_name = "X0" col_order = np . sort ( train_df [ var_name ] . unique ( ) ) . tolist ( ) plt . figure ( figsize = ( 12 , 6 ) ) sns . stripplot ( x = var_name , y = 'y' , data = train_df , order = col_order ) plt . xlabel ( var_name , fontsize = 12 ) plt . ylabel ( 'y' , fontsize = 12 ) plt . title ( "Distribution of y variable with " + var_name , fontsize = 15 ) plt . show ( )
583	var_name = "ID" plt . figure ( figsize = ( 12 , 6 ) ) sns . regplot ( x = var_name , y = 'y' , data = train_df , scatter_kws = { 'alpha' : 0.5 , 's' : 30 } ) plt . xlabel ( var_name , fontsize = 12 ) plt . ylabel ( 'y' , fontsize = 12 ) plt . title ( "Distribution of y variable with " + var_name , fontsize = 15 ) plt . show ( )
584	train_df = pd . read_csv ( "../input/train.csv" ) test_df = pd . read_csv ( "../input/test.csv" ) print ( "Train shape : " , train_df . shape ) print ( "Test shape : " , test_df . shape )
585	tfidf_vec = TfidfVectorizer ( stop_words = 'english' , ngram_range = ( 1 , 3 ) ) tfidf_vec . fit_transform ( train_df [ 'question_text' ] . values . tolist ( ) + test_df [ 'question_text' ] . values . tolist ( ) ) train_tfidf = tfidf_vec . transform ( train_df [ 'question_text' ] . values . tolist ( ) ) test_tfidf = tfidf_vec . transform ( test_df [ 'question_text' ] . values . tolist ( ) )
586	for thresh in np . arange ( 0.1 , 0.201 , 0.01 ) : thresh = np . round ( thresh , 2 ) print ( "F1 score at threshold {0} is {1}" . format ( thresh , metrics . f1_score ( val_y , ( pred_val_y > thresh ) . astype ( int ) ) ) )
587	order_products_train_df = pd . read_csv ( "../input/order_products__train.csv" ) order_products_prior_df = pd . read_csv ( "../input/order_products__prior.csv" ) orders_df = pd . read_csv ( "../input/orders.csv" ) products_df = pd . read_csv ( "../input/products.csv" ) aisles_df = pd . read_csv ( "../input/aisles.csv" ) departments_df = pd . read_csv ( "../input/departments.csv" )
588	cnt_srs = orders_df . eval_set . value_counts ( ) plt . figure ( figsize = ( 12 , 8 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 1 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'Eval set type' , fontsize = 12 ) plt . title ( 'Count of rows in each dataset' , fontsize = 15 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
589	cnt_srs = orders_df . groupby ( "user_id" ) [ "order_number" ] . aggregate ( np . max ) . reset_index ( ) cnt_srs = cnt_srs . order_number . value_counts ( ) plt . figure ( figsize = ( 12 , 8 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 2 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'Maximum order number' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
590	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "order_dow" , data = orders_df , color = color [ 0 ] ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'Day of week' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . title ( "Frequency of order by week day" , fontsize = 15 ) plt . show ( )
591	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "order_hour_of_day" , data = orders_df , color = color [ 1 ] ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'Hour of day' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . title ( "Frequency of order by hour of day" , fontsize = 15 ) plt . show ( )
592	grouped_df = orders_df . groupby ( [ "order_dow" , "order_hour_of_day" ] ) [ "order_number" ] . aggregate ( "count" ) . reset_index ( ) grouped_df = grouped_df . pivot ( 'order_dow' , 'order_hour_of_day' , 'order_number' ) plt . figure ( figsize = ( 12 , 6 ) ) sns . heatmap ( grouped_df ) plt . title ( "Frequency of Day of week Vs Hour of day" ) plt . show ( )
593	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "days_since_prior_order" , data = orders_df , color = color [ 3 ] ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'Days since prior order' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . title ( "Frequency distribution by days since prior order" , fontsize = 15 ) plt . show ( )
594	grouped_df = order_products_prior_df . groupby ( "order_id" ) [ "reordered" ] . aggregate ( "sum" ) . reset_index ( ) grouped_df [ "reordered" ] . ix [ grouped_df [ "reordered" ] > 1 ] = 1 grouped_df . reordered . value_counts ( ) / grouped_df . shape [ 0 ]
595	grouped_df = order_products_train_df . groupby ( "order_id" ) [ "add_to_cart_order" ] . aggregate ( "max" ) . reset_index ( ) cnt_srs = grouped_df . add_to_cart_order . value_counts ( ) plt . figure ( figsize = ( 12 , 8 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'Number of products in the given order' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
596	order_products_prior_df = pd . merge ( order_products_prior_df , products_df , on = 'product_id' , how = 'left' ) order_products_prior_df = pd . merge ( order_products_prior_df , aisles_df , on = 'aisle_id' , how = 'left' ) order_products_prior_df = pd . merge ( order_products_prior_df , departments_df , on = 'department_id' , how = 'left' ) order_products_prior_df . head ( )
597	cnt_srs = order_products_prior_df [ 'aisle' ] . value_counts ( ) . head ( 20 ) plt . figure ( figsize = ( 12 , 8 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 5 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'Aisle' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
598	plt . figure ( figsize = ( 10 , 10 ) ) temp_series = order_products_prior_df [ 'department' ] . value_counts ( ) labels = ( np . array ( temp_series . index ) ) sizes = ( np . array ( ( temp_series / temp_series . sum ( ) ) * 100 ) ) plt . pie ( sizes , labels = labels , autopct = '%1.1f%%' , startangle = 200 ) plt . title ( "Departments distribution" , fontsize = 15 ) plt . show ( )
599	from subprocess import check_output print ( check_output ( [ "ls" , "../input" ] ) . decode ( "utf8" ) )
600	counter = 0 with open ( "../input/training_text" ) as infile : while True : counter += 1 line = infile . readline ( ) print ( line ) if counter == 2 : break
601	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "Class" , data = train_variants_df ) plt . ylabel ( 'Frequency' , fontsize = 12 ) plt . xlabel ( 'Class Count' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . title ( "Frequency of Classes" , fontsize = 15 ) plt . show ( )
602	plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( train_text_df . Text_num_words . values , bins = 50 , kde = False , color = 'red' ) plt . xlabel ( 'Number of words in text' , fontsize = 12 ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . title ( "Frequency of number of words" , fontsize = 15 ) plt . show ( )
603	plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( train_text_df . Text_num_chars . values , bins = 50 , kde = False , color = 'brown' ) plt . xlabel ( 'Number of characters in text' , fontsize = 12 ) plt . ylabel ( 'log of Count' , fontsize = 12 ) plt . title ( "Frequency of Number of characters" , fontsize = 15 ) plt . show ( )
604	train_df = pd . merge ( train_variants_df , train_text_df , on = 'ID' ) plt . figure ( figsize = ( 12 , 8 ) ) sns . boxplot ( x = 'Class' , y = 'Text_num_words' , data = train_df ) plt . xlabel ( 'Class' , fontsize = 12 ) plt . ylabel ( 'Text - Number of words' , fontsize = 12 ) plt . show ( )
605	print ( f"The total number of games in the training data is {train_df['GameId'].nunique()}" ) print ( f"The total number of plays in the training data is {train_df['PlayId'].nunique()}" ) print ( f"The NFL seasons in the training data are {train_df['Season'].unique().tolist()}" )
606	temp_df = train_df . groupby ( "PlayId" ) . first ( ) temp_df = temp_df [ temp_df [ "Yards" ] == 0 ] . reset_index ( ) . head ( 5 ) for play_id in temp_df [ "PlayId" ] . values : plt = get_plot ( play_id ) plt . show ( )
607	temp_df = train_df . groupby ( "PlayId" ) . first ( ) temp_df = temp_df [ temp_df [ "Yards" ] > 10 ] . reset_index ( ) . head ( 5 ) for play_id in temp_df [ "PlayId" ] . values : plt = get_plot ( play_id ) plt . show ( )
608	plt . figure ( figsize = ( 16 , 12 ) ) temp_df = train_df . query ( "NflIdRusher == NflId" ) sns . scatterplot ( temp_df [ "Dis" ] , temp_df [ "Yards" ] ) plt . xlabel ( 'Distance covered' , fontsize = 12 ) plt . ylabel ( 'Yards (Target)' , fontsize = 12 ) plt . title ( "Distance covered by Rusher Vs Yards (target)" , fontsize = 20 ) plt . show ( )
609	plt . figure ( figsize = ( 16 , 12 ) ) temp_df = train_df . query ( "NflIdRusher == NflId" ) sns . scatterplot ( temp_df [ "S" ] , temp_df [ "Yards" ] ) plt . xlabel ( 'Rusher Speed' , fontsize = 12 ) plt . ylabel ( 'Yards (Target)' , fontsize = 12 ) plt . title ( "Rusher Speed Vs Yards (target)" , fontsize = 20 ) plt . show ( )
610	plt . figure ( figsize = ( 16 , 12 ) ) temp_df = train_df . query ( "NflIdRusher == NflId" ) sns . scatterplot ( temp_df [ "A" ] , temp_df [ "Yards" ] ) plt . xlabel ( 'Rusher Acceleration' , fontsize = 12 ) plt . ylabel ( 'Yards (Target)' , fontsize = 12 ) plt . title ( "Rusher Acceleration Vs Yards (target)" , fontsize = 20 ) plt . show ( )
611	plt . figure ( figsize = ( 16 , 12 ) ) temp_df = train_df . query ( "NflIdRusher == NflId" ) sns . boxplot ( data = temp_df , x = "Position" , y = "Yards" , showfliers = False , whis = 3.0 ) plt . xlabel ( 'Rusher position' , fontsize = 12 ) plt . ylabel ( 'Yards (Target)' , fontsize = 12 ) plt . title ( "Rusher Position Vs Yards (target)" , fontsize = 20 ) plt . show ( )
612	plt . figure ( figsize = ( 16 , 12 ) ) temp_df = train_df . query ( "NflIdRusher == NflId" ) sns . boxplot ( data = temp_df , x = "DefendersInTheBox" , y = "Yards" , showfliers = False , whis = 3.0 ) plt . xlabel ( 'Number of defenders in the box' , fontsize = 12 ) plt . ylabel ( 'Yards (Target)' , fontsize = 12 ) plt . title ( "Count of defenders in the box Vs Yards (target)" , fontsize = 20 ) plt . show ( )
613	plt . figure ( figsize = ( 16 , 12 ) ) temp_df = train_df . query ( "NflIdRusher == NflId" ) sns . boxplot ( data = temp_df , x = "Down" , y = "Yards" , showfliers = False ) plt . xlabel ( 'Down Number' , fontsize = 12 ) plt . ylabel ( 'Yards (Target)' , fontsize = 12 ) plt . title ( "Down Number Vs Yards (target)" , fontsize = 20 ) plt . show ( )
614	plt . figure ( figsize = ( 12 , 10 ) ) temp_df = train_df . query ( "NflIdRusher == NflId" ) sns . boxplot ( data = temp_df , y = "PossessionTeam" , x = "Yards" , showfliers = False , whis = 3.0 ) plt . ylabel ( 'PossessionTeam' , fontsize = 12 ) plt . xlabel ( 'Yards (Target)' , fontsize = 12 ) plt . title ( "Possession team Vs Yards (target)" , fontsize = 20 ) plt . show ( )
615	plt . figure ( figsize = ( 16 , 12 ) ) temp_df = train_df . query ( "NflIdRusher == NflId" ) sns . catplot ( data = temp_df , x = "Quarter" , y = "Yards" , kind = "boxen" ) plt . xlabel ( 'Quarter' , fontsize = 12 ) plt . ylabel ( 'Yards (Target)' , fontsize = 12 ) plt . title ( "Quarter Vs Yards (target)" , fontsize = 20 ) plt . show ( )
616	import random
617	plt . figure ( figsize = ( 8 , 6 ) ) plt . scatter ( range ( train_df . shape [ 0 ] ) , np . sort ( train_df . price_doc . values ) ) plt . xlabel ( 'index' , fontsize = 12 ) plt . ylabel ( 'price' , fontsize = 12 ) plt . show ( )
618	plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( train_df . price_doc . values , bins = 50 , kde = True ) plt . xlabel ( 'price' , fontsize = 12 ) plt . show ( )
619	plt . figure ( figsize = ( 12 , 8 ) ) sns . distplot ( np . log ( train_df . price_doc . values ) , bins = 50 , kde = True ) plt . xlabel ( 'price' , fontsize = 12 ) plt . show ( )
620	train_df = pd . read_csv ( "../input/train.csv" , parse_dates = [ 'timestamp' ] ) dtype_df = train_df . dtypes . reset_index ( ) dtype_df . columns = [ "Count" , "Column Type" ] dtype_df . groupby ( "Column Type" ) . aggregate ( 'count' ) . reset_index ( )
621	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "floor" , data = train_df ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'floor number' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
622	grouped_df = train_df . groupby ( 'floor' ) [ 'price_doc' ] . aggregate ( np . median ) . reset_index ( ) plt . figure ( figsize = ( 12 , 8 ) ) sns . pointplot ( grouped_df . floor . values , grouped_df . price_doc . values , alpha = 0.8 , color = color [ 2 ] ) plt . ylabel ( 'Median Price' , fontsize = 12 ) plt . xlabel ( 'Floor number' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
623	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "max_floor" , data = train_df ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'Max floor number' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
624	plt . figure ( figsize = ( 12 , 8 ) ) sns . boxplot ( x = "max_floor" , y = "price_doc" , data = train_df ) plt . ylabel ( 'Median Price' , fontsize = 12 ) plt . xlabel ( 'Max Floor number' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
625	train = pd . read_csv ( data_path + train_file , usecols = [ 'ncodpers' ] ) test = pd . read_csv ( data_path + test_file , usecols = [ 'ncodpers' ] ) print ( "Number of rows in train : " , train . shape [ 0 ] ) print ( "Number of rows in test : " , test . shape [ 0 ] )
626	train_unique_customers = set ( train . ncodpers . unique ( ) ) test_unique_customers = set ( test . ncodpers . unique ( ) ) print ( "Number of customers in train : " , len ( train_unique_customers ) ) print ( "Number of customers in test : " , len ( test_unique_customers ) ) print ( "Number of common customers : " , len ( train_unique_customers . intersection ( test_unique_customers ) ) )
627	num_occur = train . groupby ( 'ncodpers' ) . agg ( 'size' ) . value_counts ( ) plt . figure ( figsize = ( 8 , 4 ) ) sns . barplot ( num_occur . index , num_occur . values , alpha = 0.8 , color = color [ 0 ] ) plt . xlabel ( 'Number of Occurrences of the customer' , fontsize = 12 ) plt . ylabel ( 'Number of customers' , fontsize = 12 ) plt . show ( )
628	train = pd . read_csv ( data_path + "train_ver2.csv" , dtype = 'float16' , usecols = [ 'ind_ahor_fin_ult1' , 'ind_aval_fin_ult1' , 'ind_cco_fin_ult1' , 'ind_cder_fin_ult1' , 'ind_cno_fin_ult1' , 'ind_ctju_fin_ult1' , 'ind_ctma_fin_ult1' , 'ind_ctop_fin_ult1' , 'ind_ctpp_fin_ult1' , 'ind_deco_fin_ult1' , 'ind_deme_fin_ult1' , 'ind_dela_fin_ult1' , 'ind_ecue_fin_ult1' , 'ind_fond_fin_ult1' , 'ind_hip_fin_ult1' , 'ind_plan_fin_ult1' , 'ind_pres_fin_ult1' , 'ind_reca_fin_ult1' , 'ind_tjcr_fin_ult1' , 'ind_valo_fin_ult1' , 'ind_viv_fin_ult1' , 'ind_nomina_ult1' , 'ind_nom_pens_ult1' , 'ind_recibo_ult1' ] )
629	train [ 'age' ] = train [ 'age' ] . astype ( 'float64' ) age_series = train . age . value_counts ( ) plt . figure ( figsize = ( 12 , 4 ) ) sns . barplot ( age_series . index . astype ( 'int' ) , age_series . values , alpha = 0.8 ) plt . ylabel ( 'Number of Occurrences of the customer' , fontsize = 12 ) plt . xlabel ( 'Age' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
630	col_series = train . antiguedad . value_counts ( ) plt . figure ( figsize = ( 12 , 4 ) ) sns . barplot ( col_series . index . astype ( 'int' ) , col_series . values , alpha = 0.8 ) plt . ylabel ( 'Number of Occurrences of the customer' , fontsize = 12 ) plt . xlabel ( 'Customer Seniority' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
631	train . fillna ( 101850. , inplace = True ) quantile_series = train . renta . quantile ( np . arange ( 0.99 , 1 , 0.001 ) ) plt . figure ( figsize = ( 12 , 4 ) ) sns . barplot ( ( quantile_series . index * 100 ) , quantile_series . values , alpha = 0.8 ) plt . ylabel ( 'Rent value' , fontsize = 12 ) plt . xlabel ( 'Quantile value' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
632	rent_max_cap = train . renta . quantile ( 0.999 ) train [ 'renta' ] [ train [ 'renta' ] > rent_max_cap ] = 101850.0 sns . boxplot ( train . renta . values ) plt . show ( )
633	test = pd . read_csv ( test_file , usecols = [ 'renta' ] ) test [ 'renta' ] = test [ 'renta' ] . replace ( to_replace = [ ' NA' ] , value = np . nan ) . astype ( 'float' ) unique_values = np . sort ( test . renta . unique ( ) ) plt . scatter ( range ( len ( unique_values ) ) , unique_values ) plt . show ( )
634	plt . figure ( figsize = ( 8 , 6 ) ) plt . scatter ( range ( train_df . shape [ 0 ] ) , np . sort ( train_df . logerror . values ) ) plt . xlabel ( 'index' , fontsize = 12 ) plt . ylabel ( 'logerror' , fontsize = 12 ) plt . show ( )
635	train_df [ 'transaction_month' ] = train_df [ 'transactiondate' ] . dt . month cnt_srs = train_df [ 'transaction_month' ] . value_counts ( ) plt . figure ( figsize = ( 12 , 6 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 , color = color [ 3 ] ) plt . xticks ( rotation = 'vertical' ) plt . xlabel ( 'Month of transaction' , fontsize = 12 ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . show ( )
636	plt . figure ( figsize = ( 12 , 12 ) ) sns . jointplot ( x = prop_df . latitude . values , y = prop_df . longitude . values , size = 10 ) plt . ylabel ( 'Longitude' , fontsize = 12 ) plt . xlabel ( 'Latitude' , fontsize = 12 ) plt . show ( )
637	pd . options . display . max_rows = 65 dtype_df = train_df . dtypes . reset_index ( ) dtype_df . columns = [ "Count" , "Column Type" ] dtype_df
638	missing_df = train_df . isnull ( ) . sum ( axis = 0 ) . reset_index ( ) missing_df . columns = [ 'column_name' , 'missing_count' ] missing_df [ 'missing_ratio' ] = missing_df [ 'missing_count' ] / train_df . shape [ 0 ] missing_df . ix [ missing_df [ 'missing_ratio' ] > 0.999 ]
639	corr_zero_cols = [ 'assessmentyear' , 'storytypeid' , 'pooltypeid2' , 'pooltypeid7' , 'pooltypeid10' , 'poolcnt' , 'decktypeid' , 'buildingclasstypeid' ] for col in corr_zero_cols : print ( col , len ( train_df_new [ col ] . unique ( ) ) )
640	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "bathroomcnt" , data = train_df ) plt . ylabel ( 'Count' , fontsize = 12 ) plt . xlabel ( 'Bathroom' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . title ( "Frequency of Bathroom count" , fontsize = 15 ) plt . show ( )
641	plt . figure ( figsize = ( 12 , 8 ) ) sns . boxplot ( x = "bathroomcnt" , y = "logerror" , data = train_df ) plt . ylabel ( 'Log error' , fontsize = 12 ) plt . xlabel ( 'Bathroom Count' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . title ( "How log error changes with bathroom count?" , fontsize = 15 ) plt . show ( )
642	plt . figure ( figsize = ( 12 , 8 ) ) sns . countplot ( x = "bedroomcnt" , data = train_df ) plt . ylabel ( 'Frequency' , fontsize = 12 ) plt . xlabel ( 'Bedroom Count' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . title ( "Frequency of Bedroom count" , fontsize = 15 ) plt . show ( )
643	train_df [ 'bedroomcnt' ] . ix [ train_df [ 'bedroomcnt' ] > 7 ] = 7 plt . figure ( figsize = ( 12 , 8 ) ) sns . violinplot ( x = 'bedroomcnt' , y = 'logerror' , data = train_df ) plt . xlabel ( 'Bedroom count' , fontsize = 12 ) plt . ylabel ( 'Log Error' , fontsize = 12 ) plt . show ( )
644	from ggplot import * ggplot ( aes ( x = 'yearbuilt' , y = 'logerror' ) , data = train_df ) + \ geom_point ( color = 'steelblue' , size = 1 ) + \ stat_smooth ( )
645	ggplot ( aes ( x = 'latitude' , y = 'longitude' , color = 'logerror' ) , data = train_df ) + \ geom_point ( ) + \ scale_color_gradient ( low = 'red' , high = 'blue' )
646	ggplot ( aes ( x = 'finishedsquarefeet12' , y = 'taxamount' , color = 'logerror' ) , data = train_df ) + \ geom_point ( alpha = 0.7 ) + \ scale_color_gradient ( low = 'pink' , high = 'blue' )
647	cnt_srs = train_df [ 'author' ] . value_counts ( ) plt . figure ( figsize = ( 8 , 4 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'Author Name' , fontsize = 12 ) plt . show ( )
648	grouped_df = train_df . groupby ( 'author' ) for name , group in grouped_df : print ( "Author name : " , name ) cnt = 0 for ind , row in group . iterrows ( ) : print ( row [ "text" ] ) cnt += 1 if cnt == 5 : break print ( "\n" )
649	train_df [ 'num_words' ] . loc [ train_df [ 'num_words' ] > 80 ] = 80 plt . figure ( figsize = ( 12 , 8 ) ) sns . violinplot ( x = 'author' , y = 'num_words' , data = train_df ) plt . xlabel ( 'Author Name' , fontsize = 12 ) plt . ylabel ( 'Number of words in text' , fontsize = 12 ) plt . title ( "Number of words by author" , fontsize = 15 ) plt . show ( )
650	train_df [ 'num_punctuations' ] . loc [ train_df [ 'num_punctuations' ] > 10 ] = 10 plt . figure ( figsize = ( 12 , 8 ) ) sns . violinplot ( x = 'author' , y = 'num_punctuations' , data = train_df ) plt . xlabel ( 'Author Name' , fontsize = 12 ) plt . ylabel ( 'Number of puntuations in text' , fontsize = 12 ) plt . title ( "Number of punctuations by author" , fontsize = 15 ) plt . show ( )
651	fig , ax = plt . subplots ( figsize = ( 12 , 12 ) ) xgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) plt . show ( )
652	tfidf_vec = TfidfVectorizer ( stop_words = 'english' , ngram_range = ( 1 , 3 ) ) full_tfidf = tfidf_vec . fit_transform ( train_df [ 'text' ] . values . tolist ( ) + test_df [ 'text' ] . values . tolist ( ) ) train_tfidf = tfidf_vec . transform ( train_df [ 'text' ] . values . tolist ( ) ) test_tfidf = tfidf_vec . transform ( test_df [ 'text' ] . values . tolist ( ) )
653	def runMNB ( train_X , train_y , test_X , test_y , test_X2 ) : model = naive_bayes . MultinomialNB ( ) model . fit ( train_X , train_y ) pred_test_y = model . predict_proba ( test_X ) pred_test_y2 = model . predict_proba ( test_X2 ) return pred_test_y , pred_test_y2 , model
654	tfidf_vec = CountVectorizer ( stop_words = 'english' , ngram_range = ( 1 , 3 ) ) tfidf_vec . fit ( train_df [ 'text' ] . values . tolist ( ) + test_df [ 'text' ] . values . tolist ( ) ) train_tfidf = tfidf_vec . transform ( train_df [ 'text' ] . values . tolist ( ) ) test_tfidf = tfidf_vec . transform ( test_df [ 'text' ] . values . tolist ( ) )
655	fig , ax = plt . subplots ( figsize = ( 12 , 12 ) ) xgb . plot_importance ( model , max_num_features = 50 , height = 0.8 , ax = ax ) plt . show ( )
656	cnf_matrix = confusion_matrix ( val_y , np . argmax ( pred_val_y , axis = 1 ) ) np . set_printoptions ( precision = 2 ) plt . figure ( figsize = ( 8 , 8 ) ) plot_confusion_matrix ( cnf_matrix , classes = [ 'EAP' , 'HPL' , 'MWS' ] , title = 'Confusion matrix of XGB, without normalization' ) plt . show ( )
657	env = kagglegym . make ( ) observation = env . reset ( ) train = observation . train
658	cols_to_use = [ 'technical_30' , 'technical_20' , 'fundamental_11' , 'technical_19' ] temp_df = train [ cols_to_use ] corrmat = temp_df . corr ( method = 'spearman' ) f , ax = plt . subplots ( figsize = ( 8 , 8 ) ) sns . heatmap ( corrmat , vmax = .8 , square = True ) plt . show ( )
659	models_dict = { } for col in cols_to_use : model = lm . LinearRegression ( ) model . fit ( np . array ( train [ col ] . values ) . reshape ( - 1 , 1 ) , train . y . values ) models_dict [ col ] = model
660	print ( "Max y value in train : " , train . y . max ( ) ) print ( "Min y value in train : " , train . y . min ( ) )
661	low_y_cut = - 0.086093 high_y_cut = 0.093497 y_is_above_cut = ( train . y > high_y_cut ) y_is_below_cut = ( train . y < low_y_cut ) y_is_within_cut = ( ~ y_is_above_cut & ~ y_is_below_cut ) y_is_within_cut . value_counts ( )
662	train_df = pd . read_csv ( "../input/train.csv" ) test_df = pd . read_csv ( "../input/test.csv" ) print ( train_df . shape ) print ( test_df . shape )
663	is_dup = train_df [ 'is_duplicate' ] . value_counts ( ) plt . figure ( figsize = ( 8 , 4 ) ) sns . barplot ( is_dup . index , is_dup . values , alpha = 0.8 , color = color [ 1 ] ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xlabel ( 'Is Duplicate' , fontsize = 12 ) plt . show ( )
664	all_ques_df = pd . DataFrame ( pd . concat ( [ train_df [ 'question1' ] , train_df [ 'question2' ] ] ) ) all_ques_df . columns = [ "questions" ] all_ques_df [ "num_of_words" ] = all_ques_df [ "questions" ] . apply ( lambda x : len ( str ( x ) . split ( ) ) )
665	plt . figure ( figsize = ( 12 , 6 ) ) sns . boxplot ( x = "is_duplicate" , y = "unigrams_common_count" , data = train_df ) plt . xlabel ( 'Is duplicate' , fontsize = 12 ) plt . ylabel ( 'Common unigrams count' , fontsize = 12 ) plt . show ( )
666	plt . figure ( figsize = ( 12 , 6 ) ) sns . boxplot ( x = "is_duplicate" , y = "unigrams_common_ratio" , data = train_df ) plt . xlabel ( 'Is duplicate' , fontsize = 12 ) plt . ylabel ( 'Common unigrams ratio' , fontsize = 12 ) plt . show ( )
667	ques = pd . concat ( [ train_df [ [ 'question1' , 'question2' ] ] , \ test_df [ [ 'question1' , 'question2' ] ] ] , axis = 0 ) . reset_index ( drop = 'index' ) ques . shape
668	cnt_srs = train_df [ 'q1_q2_intersect' ] . value_counts ( ) plt . figure ( figsize = ( 12 , 6 ) ) sns . barplot ( cnt_srs . index , np . log1p ( cnt_srs . values ) , alpha = 0.8 ) plt . xlabel ( 'Q1-Q2 neighbor intersection count' , fontsize = 12 ) plt . ylabel ( 'Log of Number of Occurrences' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
669	cnt_srs = train_df [ 'q1_freq' ] . value_counts ( ) plt . figure ( figsize = ( 12 , 8 ) ) sns . barplot ( cnt_srs . index , cnt_srs . values , alpha = 0.8 ) plt . xlabel ( 'Q1 frequency' , fontsize = 12 ) plt . ylabel ( 'Number of Occurrences' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
670	plt . figure ( figsize = ( 12 , 8 ) ) grouped_df = train_df . groupby ( 'q1_freq' ) [ 'is_duplicate' ] . aggregate ( np . mean ) . reset_index ( ) sns . barplot ( grouped_df [ "q1_freq" ] . values , grouped_df [ "is_duplicate" ] . values , alpha = 0.8 , color = color [ 4 ] ) plt . ylabel ( 'Mean is_duplicate' , fontsize = 12 ) plt . xlabel ( 'Q1 frequency' , fontsize = 12 ) plt . xticks ( rotation = 'vertical' ) plt . show ( )
671	pvt_df = train_df . pivot_table ( index = "q1_freq" , columns = "q2_freq" , values = "is_duplicate" ) plt . figure ( figsize = ( 12 , 12 ) ) sns . heatmap ( pvt_df ) plt . title ( "Mean is_duplicate value distribution across q1 and q2 frequency" ) plt . show ( )
672	cols_to_use = [ 'q1_q2_intersect' , 'q1_freq' , 'q2_freq' ] temp_df = train_df [ cols_to_use ] corrmat = temp_df . corr ( method = 'spearman' ) f , ax = plt . subplots ( figsize = ( 8 , 8 ) ) sns . heatmap ( corrmat , vmax = 1. , square = True ) plt . title ( "Leaky variables correlation map" , fontsize = 15 ) plt . show ( )
673	data_path = "../input/" train_file = data_path + "train.json" test_file = data_path + "test.json" train_df = pd . read_json ( train_file ) test_df = pd . read_json ( test_file ) print ( train_df . shape ) print ( test_df . shape )
674	train_X = sparse . hstack ( [ train_df [ features_to_use ] , tr_sparse ] ) . tocsr ( ) test_X = sparse . hstack ( [ test_df [ features_to_use ] , te_sparse ] ) . tocsr ( ) target_num_map = { 'high' : 0 , 'medium' : 1 , 'low' : 2 } train_y = np . array ( train_df [ 'interest_level' ] . apply ( lambda x : target_num_map [ x ] ) ) print ( train_X . shape , test_X . shape )
675	preds , model = runXGB ( train_X , train_y , test_X , num_rounds = 400 ) out_df = pd . DataFrame ( preds ) out_df . columns = [ "high" , "medium" , "low" ] out_df [ "listing_id" ] = test_df . listing_id . values out_df . to_csv ( "xgb_starter2.csv" , index = False )
676	path = '../input/' train = pd . read_csv ( path + 'train.csv' ) test = pd . read_csv ( path + 'test.csv' ) print ( 'Number of rows and columns in the train data set:' , train . shape ) print ( 'Number of rows and columns in the test data set:' , test . shape )
677	vect_word = TfidfVectorizer ( max_features = 20000 , lowercase = True , analyzer = 'word' , stop_words = 'english' , ngram_range = ( 1 , 3 ) , dtype = np . float32 ) vect_char = TfidfVectorizer ( max_features = 40000 , lowercase = True , analyzer = 'char' , stop_words = 'english' , ngram_range = ( 3 , 6 ) , dtype = np . float32 )
678	col = 'identity_hate' print ( "Column:" , col ) pred = lr . predict ( X ) print ( '\nConfusion matrix\n' , confusion_matrix ( y [ col ] , pred ) ) print ( classification_report ( y [ col ] , pred ) )
679	path = '../input/' train = pd . read_csv ( path + 'train.csv' ) test = pd . read_csv ( path + 'test.csv' ) print ( 'Number of rows and columns in train data set:' , train . shape ) print ( 'Number of rows and columns in test data set:' , test . shape )
680	fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 4 ) ) ax1 , ax2 = ax . flatten ( ) sns . distplot ( train [ 'formation_energy_ev_natom' ] , bins = 50 , ax = ax1 , color = 'b' ) sns . distplot ( train [ 'bandgap_energy_ev' ] , bins = 50 , ax = ax2 , color = 'r' )
681	cor = train . corr ( ) plt . figure ( figsize = ( 12 , 8 ) ) sns . heatmap ( cor , cmap = 'Set1' , annot = True )
682	def mean_median_feature ( df ) : print ( ' dmean = df.mean() dmedian = df.median() q1 = df.quantile(0.25) d2 = df.quantile(0.5) q3 = df.quantile(0.75) col = df.columns del_col = [' id ',' formation_energy_ev_natom ',' bandgap_energy_ev '] col = [w for w in col if w not in del_col] for c in col: df[' mean_ '+c] = (df[c] > dmean[c]).astype(np.uint8) df[' median_ '+c] = (df[c] > dmedian[c]).astype(np.uint8) df[' q1_ '+c] = (df[c] < q1[c]).astype(np.uint8) df[' q2_ '+c] = (df[c] < q1[c]).astype(np.uint8) df[' q3_ '+c] = (df[c] > q3[c]).astype(np.uint8) print(' Shape ' , df . shape ) mean_median_feature ( train ) mean_median_feature ( test )
683	print ( 'Original text:\n' , train [ 'text' ] [ 0 ] ) review = re . sub ( '[^A-Za-z0-9]' , " " , train [ 'text' ] [ 0 ] ) print ( '\nAfter removal of punctuation:\n' , review )
684	cv = CountVectorizer ( max_features = 2000 , ngram_range = ( 1 , 3 ) , dtype = np . int8 , stop_words = 'english' ) X_cv = cv . fit_transform ( train [ 'clean_text' ] ) . toarray ( ) X_test_cv = cv . fit_transform ( test [ 'clean_text' ] ) . toarray ( )
685	y_pred = pred_test_full / 10 submit = pd . DataFrame ( test [ 'id' ] ) submit = submit . join ( pd . DataFrame ( y_pred ) ) submit . columns = [ 'id' , 'EAP' , 'HPL' , 'MWS' ] submit . to_csv ( 'spooky_pred1.csv' , index = False )
686	tfidf = TfidfVectorizer ( max_features = 2000 , dtype = np . float32 , analyzer = 'word' , ngram_range = ( 1 , 3 ) , use_idf = True , smooth_idf = True , sublinear_tf = True ) X_tf = tfidf . fit_transform ( train [ 'clean_text' ] ) . toarray ( ) X_test_tf = tfidf . fit_transform ( test [ 'clean_text' ] ) . toarray ( )
687	y_pred = pred_test_full / 10 submit = pd . DataFrame ( test [ 'id' ] ) submit = submit . join ( pd . DataFrame ( y_pred ) ) submit . columns = [ 'id' , 'EAP' , 'HPL' , 'MWS' ] submit . to_csv ( 'spooky_pred2.csv' , index = False )
688	y_pred = pred_test_full / 2 submit = pd . DataFrame ( test [ 'id' ] ) submit = submit . join ( pd . DataFrame ( y_pred ) ) submit . columns = [ 'id' , 'EAP' , 'HPL' , 'MWS' ] submit . to_csv ( 'spooky_pred3.csv' , index = False )
689	import numpy as np import pandas as pd from fbprophet import Prophet
690	train = pd . read_csv ( "../input/train_1.csv" ) keys = pd . read_csv ( "../input/key_1.csv" ) ss = pd . read_csv ( "../input/sample_submission_1.csv" )
691	m = Prophet ( ) m . fit ( df ) future = m . make_future_dataframe ( periods = 10 ) forecast = m . predict ( future ) m . plot ( forecast ) ;
692	y = X_train . dropna ( 0 ) . as_matrix ( ) [ 0 ] y = [ None if i >= np . percentile ( y , 95 ) or i <= np . percentile ( y , 5 ) else i for i in y ] df_na = pd . DataFrame ( { 'ds' : X_train . T . index . values , 'y' : y } )
693	path = '../input/' train = pd . read_csv ( path + 'train.csv' , na_values = - 1 ) test = pd . read_csv ( path + 'test.csv' , na_values = - 1 ) print ( 'Number rows and columns:' , train . shape ) print ( 'Number rows and columns:' , test . shape )
694	cor = train . drop ( 'id' , axis = 1 ) . corr ( ) plt . figure ( figsize = ( 16 , 16 ) ) sns . heatmap ( cor , cmap = 'Set1' )
695	ps_cal = train . columns [ train . columns . str . startswith ( 'ps_calc' ) ] train = train . drop ( ps_cal , axis = 1 ) test = test . drop ( ps_cal , axis = 1 ) train . shape
696	k = pd . DataFrame ( ) k [ 'train' ] = train . isnull ( ) . sum ( ) k [ 'test' ] = test . isnull ( ) . sum ( ) fig , ax = plt . subplots ( figsize = ( 16 , 5 ) ) k . plot ( kind = 'bar' , ax = ax )
697	def missing_value ( df ) : col = df . columns for i in col : if df [ i ] . isnull ( ) . sum ( ) > 0 : df [ i ] . fillna ( df [ i ] . mode ( ) [ 0 ] , inplace = True )
698	def basic_details ( df ) : b = pd . DataFrame ( ) b [ 'Missing value' ] = df . isnull ( ) . sum ( ) b [ 'N unique value' ] = df . nunique ( ) b [ 'dtype' ] = df . dtypes return b basic_details ( train )
699	def category_type ( df ) : col = df . columns for i in col : if df [ i ] . nunique ( ) <= 104 : df [ i ] = df [ i ] . astype ( 'category' ) category_type ( train ) category_type ( test )
700	X = train1 . drop ( [ 'target' , 'id' ] , axis = 1 ) y = train1 [ 'target' ] . astype ( 'category' ) x_test = test1 . drop ( [ 'target' , 'id' ] , axis = 1 ) del train1 , test1
701	y_pred = pred_test_full / 5 submit = pd . DataFrame ( { 'id' : test [ 'id' ] , 'target' : y_pred } ) submit . to_csv ( 'lr_porto.csv' , index = False )
702	path = '../input/' train = pd . read_csv ( path + 'train.csv' , na_values = - 1 ) test = pd . read_csv ( path + 'test.csv' , na_values = - 1 ) print ( 'Number rows and columns:' , train . shape ) print ( 'Number rows and columns:' , test . shape )
703	cor = train . corr ( ) plt . figure ( figsize = ( 16 , 10 ) ) sns . heatmap ( cor , cmap = 'plasma' ) ;
704	ps_cal = train . columns [ train . columns . str . startswith ( 'ps_calc' ) ] train = train . drop ( ps_cal , axis = 1 ) test = test . drop ( ps_cal , axis = 1 ) train . shape
705	k = pd . DataFrame ( ) k [ 'train' ] = train . isnull ( ) . sum ( ) k [ 'test' ] = test . isnull ( ) . sum ( ) k
706	def missing_value ( df ) : col = df . columns for i in col : if df [ i ] . isnull ( ) . sum ( ) > 0 : df [ i ] . fillna ( df [ i ] . mode ( ) [ 0 ] , inplace = True )
707	X = train . drop ( [ 'target' , 'id' ] , axis = 1 ) y = train [ 'target' ] . astype ( 'category' ) x_test = test . drop ( 'id' , axis = 1 ) test_id = test [ 'id' ]
708	y_pred = pred_xgb submit = pd . DataFrame ( { 'id' : test_id , 'target' : y_pred } ) submit . to_csv ( 'xgb_porto.csv' , index = False )
709	import matplotlib . pyplot as plt from skimage . transform import resize
710	import numpy as np import pandas as pd import seaborn as sns import matplotlib . pyplot as plt from sklearn . model_selection import StratifiedKFold import xgboost as xgb
711	X = train . drop ( [ 'id' , 'target' ] , axis = 1 ) . values y = train . target . values test_id = test . id . values test = test . drop ( 'id' , axis = 1 )
712	sub = pd . DataFrame ( ) sub [ 'id' ] = test_id sub [ 'target' ] = np . zeros_like ( test_id )
713	import numpy as np import pandas as pd import os import math import seaborn as sns import matplotlib . pyplot as plt print ( os . listdir ( "../input" ) )
714	probs = sub_df . is_turkey . values n , bins , _ = plt . hist ( probs , bins = 100 ) print ( n , bins ) pos_threshold = 0.99 neg_threshold = 0.01 pseudo_index = np . argwhere ( np . logical_or ( probs > pos_threshold , probs < neg_threshold ) ) [ : , 0 ]
715	import numpy as np import pandas as pd from tqdm import tqdm import os print ( os . listdir ( "../input" ) )
716	train_X , val_X , test_X , train_y , val_y , word_index = load_and_prec ( ) embedding_matrix_1 = load_glove ( word_index ) embedding_matrix_2 = load_fasttext ( word_index ) embedding_matrix_3 = load_para ( word_index )
717	if isfile ( P2SIZE ) : print ( "P2SIZE exists." ) with open ( P2SIZE , 'rb' ) as f : p2size = pickle . load ( f ) else : p2size = { } for p in tqdm ( join ) : size = pil_image . open ( expand_path ( p ) ) . size p2size [ p ] = size
718	from fastai . conv_learner import * from fastai . dataset import * from tqdm import tqdm import pandas as pd import numpy as np import os from sklearn . model_selection import train_test_split , StratifiedShuffleSplit import matplotlib . pyplot as plt import math
719	labels_count = train_df . Id . value_counts ( ) _ , _ , _ = plt . hist ( labels_count , bins = 100 ) labels_count
720	avg_width = 0 avg_height = 0 for fn in os . listdir ( TRAIN ) [ : 1000 ] : img = cv2 . imread ( os . path . join ( TRAIN , fn ) ) avg_width += img . shape [ 1 ] avg_height += img . shape [ 0 ] avg_width //= 1000 avg_height //= 1000 print ( avg_width , avg_height )
721	learn . fit ( lr , 2 , cycle_len = 3 ) learn . unfreeze ( ) lrs = np . array ( [ lr / 10 , lr / 20 , lr / 40 ] ) learn . fit ( lrs , 4 , cycle_len = 4 , use_clr = ( 20 , 16 ) ) learn . save ( MODEL_PATH )
722	preds_v , y_v = learn . TTA ( is_test = False , n_aug = 2 ) preds_v = np . stack ( preds_v , axis = - 1 ) preds_v = np . exp ( preds_v ) preds_v = preds_v . mean ( axis = - 1 ) y_v += 1
723	TEST = TRAIN total_new_whale = len ( new_whale_df . index . values ) md = get_data ( 384 , batch_size , test_names = new_whale_df . index . values [ : int ( total_new_whale * 0.2 ) ] , test_dir = TRAIN ) learn . set_data ( md ) preds_w , y_w = learn . TTA ( is_test = True , n_aug = 2 ) preds_w = np . stack ( preds_w , axis = - 1 ) preds_w = np . exp ( preds_w ) preds_w = preds_w . mean ( axis = - 1 )
724	learn . fit ( lr , 1 , cycle_len = 2 ) learn . unfreeze ( ) lrs = np . array ( [ 1e-4 , 5e-4 , 1.2e-3 ] ) learn . fit ( lrs , 1 , cycle_len = 5 , use_clr = ( 20 , 16 ) ) learn . fit ( lrs / 4 , 1 , cycle_len = 5 , use_clr = ( 10 , 8 ) ) learn . fit ( lrs / 16 , 1 , cycle_len = 5 , use_clr = ( 10 , 8 ) )
725	preds_t , y_t = learn . TTA ( is_test = True , n_aug = 8 ) preds_t = np . stack ( preds_t , axis = - 1 ) preds_t = np . exp ( preds_t ) preds_t = preds_t . mean ( axis = - 1 ) [ : , 1 ]
726	nulls = df_train . isnull ( ) . sum ( ) print ( nulls )
727	x = percentage . values y = np . array ( percentage . index ) plt . figure ( figsize = ( 16 , 5 ) ) sns . set ( font_scale = 1.2 ) ax = sns . barplot ( y , x , palette = 'hls' , log = False ) ax . set ( xlabel = 'Feature' , ylabel = '(Percentage of Nulls)' , title = 'Number of Nulls' )
728	df = pd . concat ( [ df_train . drop ( "Survived" , axis = 1 ) , df_test ] , ignore_index = True ) label = df_train [ "Survived" ] index = df_train . shape [ 0 ]
729	import re codes = [ i for i in df [ "Ticket_code" ] . unique ( ) if i != "No Code" ] def split_codes ( code ) : return re . split ( '[^a-zA-Z0-9]+' , code ) new_codes = [ ] for i in codes : for j in split_codes ( i ) : new_codes . append ( j ) pd . Series ( new_codes ) . value_counts ( )
730	from sklearn import preprocessing df = df . drop ( [ "Name" , "Ticket" ] , axis = 1 ) categorical = [ "Sex" , "Embarked" , "Ticket_code" , "Ticket_code_HEAD" , "Ticket_code_TAIL" , "Initial" ] lbl = preprocessing . LabelEncoder ( ) for col in categorical : df [ col ] . fillna ( 'Unknown' ) df [ col ] = lbl . fit_transform ( df [ col ] . astype ( str ) ) df . head ( )
731	df [ 'FamilySize' ] = df [ 'SibSp' ] + df [ 'Parch' ] + 1 df [ "Isalone" ] = df [ "FamilySize" ] . apply ( lambda x : 0 if x != 1 else 1 ) df = df . drop ( "Ticket_number" , axis = 1 ) df [ 'Fare' ] . fillna ( ( df [ 'Fare' ] . median ( ) ) , inplace = True ) df . head ( )
732	ntrain = length ntest = len ( df ) - length y_train = label . ravel ( ) x_train = df [ : length ] . values x_test = df [ length : ] . values SEED = 0 NFOLDS = 5 kf = KFold ( n_splits = NFOLDS )
733	train_identity = pd . read_csv ( '../input/ieee-fraud-detection/train_identity.csv' ) train_transaction = pd . read_csv ( '../input/ieee-fraud-detection/train_transaction.csv' ) test_identity = pd . read_csv ( '../input/ieee-fraud-detection/test_identity.csv' ) test_transaction = pd . read_csv ( '../input/ieee-fraud-detection/test_transaction.csv' ) sub = pd . read_csv ( '../input/ieee-fraud-detection/sample_submission.csv' ) train = pd . merge ( train_transaction , train_identity , on = 'TransactionID' , how = 'left' ) test = pd . merge ( test_transaction , test_identity , on = 'TransactionID' , how = 'left' )
734	from sklearn . utils import resample not_fraud = train [ train . isFraud == 0 ] fraud = train [ train . isFraud == 1 ] not_fraud_downsampled = resample ( not_fraud , replace = False , n_samples = 400000 , random_state = 27 ) downsampled = pd . concat ( [ not_fraud_downsampled , fraud ] ) downsampled . isFraud . value_counts ( )
735	params = { 'num_leaves' : 493 , 'min_child_weight' : 0.03454472573214212 , 'feature_fraction' : 0.3797454081646243 , 'bagging_fraction' : 0.4181193142567742 , 'min_data_in_leaf' : 106 , 'objective' : 'binary' , 'max_depth' : - 1 , 'learning_rate' : 0.006883242363721497 , "boosting_type" : "gbdt" , "bagging_seed" : 11 , "metric" : 'auc' , "verbosity" : - 1 , 'reg_alpha' : 0.3899927210061127 , 'reg_lambda' : 0.6485237330340494 , 'random_state' : 47 }
736	train_df = train [ train [ 'store' ] == 1 ] train_df = train_df [ train [ 'item' ] == 1 ] train_df [ 'year' ] = train [ 'date' ] . dt . year train_df [ 'month' ] = train [ 'date' ] . dt . month train_df [ 'day' ] = train [ 'date' ] . dt . dayofyear train_df [ 'weekday' ] = train [ 'date' ] . dt . weekday train_df . head ( )
737	train_df = train_df . set_index ( 'date' ) train_df [ 'sales' ] = train_df [ 'sales' ] . astype ( float ) train_df . head ( )
738	first_diff = train_df . sales - train_df . sales . shift ( 1 ) first_diff = first_diff . dropna ( inplace = False ) test_stationarity ( first_diff , window = 12 )
739	import statsmodels . api as sm fig = plt . figure ( figsize = ( 12 , 8 ) ) ax1 = fig . add_subplot ( 211 ) fig = sm . graphics . tsa . plot_acf ( train_df . sales , lags = 40 , ax = ax1 ) ax2 = fig . add_subplot ( 212 ) fig = sm . graphics . tsa . plot_pacf ( train_df . sales , lags = 40 , ax = ax2 )
740	start_index = 1730 end_index = 1826 train_df [ 'forecast' ] = sarima_mod6 . predict ( start = start_index , end = end_index , dynamic = True ) train_df [ start_index : end_index ] [ [ 'sales' , 'forecast' ] ] . plot ( figsize = ( 12 , 8 ) )
741	storeid = 1 itemid = 1 train_df = train [ train [ 'store' ] == storeid ] train_df = train_df [ train_df [ 'item' ] == itemid ] train_df [ 'year' ] = train_df [ 'date' ] . dt . year - 2012 train_df [ 'month' ] = train_df [ 'date' ] . dt . month train_df [ 'day' ] = train_df [ 'date' ] . dt . dayofyear train_df [ 'weekday' ] = train_df [ 'date' ] . dt . weekday start_index = 1730 end_index = 1826
742	start_index = '2017-10-01' end_index = '2017-12-30' end_index1 = '2017-12-31'
743	shapefile = '/kaggle/input/110m-cultural/ne_110m_admin_0_countries.shp' gdf = gpd . read_file ( shapefile ) [ [ 'ADMIN' , 'ADM0_A3' , 'geometry' ] ] gdf . columns = [ 'country' , 'code' , 'geometry' ]
744	times_series_df . plot ( figsize = ( 20 , 10 ) , title = "The Cumulative total of Confirmed cases" ) plt . legend ( loc = 2 , prop = { 'size' : 20 } ) plt . show ( )
745	from pylab import rcParams import statsmodels . api as sm times_series_df . index = pd . to_datetime ( times_series_df . index , format = '%Y-%m-%d' ) rcParams [ 'figure.figsize' ] = 18 , 8 decomposition = sm . tsa . seasonal_decompose ( times_series_df . diff ( ) . fillna ( 0 ) , model = 'additive' ) fig = decomposition . plot ( ) plt . show ( )
746	import numpy as np import pandas as pd import matplotlib . pyplot as plt import cv2 import os
747	model = Sequential ( ) model . add ( ResNet50 ( include_top = False , pooling = RESNET50_POOLING_AVERAGE , weights = resnet_weights_path ) ) model . add ( Dense ( NUM_CLASSES , activation = DENSE_LAYER_ACTIVATION ) ) model . layers [ 0 ] . trainable = False
748	from tensorflow . python . keras import optimizers sgd = optimizers . SGD ( lr = 0.01 , decay = 1e-6 , momentum = 0.9 , nesterov = True ) model . compile ( optimizer = sgd , loss = OBJECTIVE_FUNCTION , metrics = LOSS_METRICS )
749	from keras . applications . resnet50 import preprocess_input from keras . preprocessing . image import ImageDataGenerator image_size = IMAGE_RESIZE data_generator = ImageDataGenerator ( preprocessing_function = preprocess_input ) train_generator = data_generator . flow_from_directory ( '../input/catsdogs-trainvalid-80pc-prepd/trainvalidfull4keras/trainvalidfull4keras/train' , target_size = ( image_size , image_size ) , batch_size = BATCH_SIZE_TRAINING , class_mode = 'categorical' ) validation_generator = data_generator . flow_from_directory ( '../input/catsdogs-trainvalid-80pc-prepd/trainvalidfull4keras/trainvalidfull4keras/valid' , target_size = ( image_size , image_size ) , batch_size = BATCH_SIZE_VALIDATION , class_mode = 'categorical' )
750	from tensorflow . python . keras . callbacks import EarlyStopping , ModelCheckpoint cb_early_stopper = EarlyStopping ( monitor = 'val_loss' , patience = EARLY_STOP_PATIENCE ) cb_checkpointer = ModelCheckpoint ( filepath = '../working/best.hdf5' , monitor = 'val_loss' , save_best_only = True , mode = 'auto' )
751	test_generator . reset ( ) pred = model . predict_generator ( test_generator , steps = len ( test_generator ) , verbose = 1 ) predicted_class_indices = np . argmax ( pred , axis = 1 )
752	def latex_tag_in_text ( text ) : x = text . lower ( ) return ' [ math ] ' in x train [ 'latex_tag_in_text' ] = train [ 'question_text' ] . apply ( lambda x : latex_tag_in_text ( x ) )
753	EMBED_SIZE = 300 MAX_WORDS_LEN = 70 MAX_VOCAB_FEATURES = 200000
754	def clean_latex_tag ( text ) : corr_t = [ ] for t in text . split ( " " ) : t = t . strip ( ) if t != '' : corr_t . append ( t ) text = ' ' . join ( corr_t ) text = re . sub ( '(\[ math \]).+(\[ / math \])' , 'mathematical formula' , text ) return text
755	train_df = pd . read_csv ( "../input/train.csv" , encoding = 'utf8' ) test_df = pd . read_csv ( "../input/test.csv" , encoding = 'utf8' ) all_test_texts = '' . join ( test_df . question_text . values . tolist ( ) ) print ( 'Train:' , train_df . shape ) print ( 'Test:' , test_df . shape )
756	model = models . resnet18 ( pretrained = True ) fc_in_features = model . fc . in_features model . fc = nn . Linear ( fc_in_features , 2 ) model = model . to ( device ) loss_fn = nn . CrossEntropyLoss ( ) optimizer = optim . SGD ( model . parameters ( ) , lr = 0.001 , momentum = 0.9 )
757	test_pids = test_df . PetID . values input_tensor = torch . zeros ( 1 , 3 , 224 , 224 ) test_image_features = { } for petid in tqdm ( test_pids ) : test_img = f"../input/test_images/{petid}-1.jpg" if not os . path . exists ( test_img ) : continue test_img = Image . open ( test_img ) test_img = extract_transform ( test_img ) input_tensor [ 0 , : , : , : ] = test_img input_tensor = input_tensor . cuda ( ) model ( input_tensor ) test_image_features [ petid ] = image_features [ 0 ] image_features . clear ( )
758	generator = Generator ( ) . cuda ( ) discriminator = Discriminator ( ) . cuda ( ) generator . apply ( weights_init ) discriminator . apply ( weights_init ) adversarial_loss = nn . BCELoss ( ) optimizer_G = torch . optim . Adam ( generator . parameters ( ) , lr = lr , betas = ( 0.5 , 0.999 ) ) optimizer_D = torch . optim . Adam ( discriminator . parameters ( ) , lr = lr , betas = ( 0.5 , 0.999 ) )
759	import matplotlib . animation as animation fig = plt . figure ( ) ims = [ ] for j in range ( len ( ims_animation ) ) : im = plt . imshow ( ims_animation [ j ] [ 0 ] , animated = True ) ims . append ( [ im ] ) anim = animation . ArtistAnimation ( fig , ims , interval = 50 , blit = True , repeat_delay = 1000 , repeat = True ) anim . save ( 'generate_dog.gif' , writer = 'ffmpeg' )
760	print ( 'Number of row in transaction:' , len ( train_transaction ) ) print ( 'Number of row in identity:' , len ( train_identity ) )
761	train_full_num = train_full . filter ( regex = 'isFraud|TransactionDT|TransactionAmt|dist|C|D' ) plt . figure ( figsize = ( 18 , 9 ) ) sns . heatmap ( train_full_num . isnull ( ) , cbar = False )
762	train_full_Vesta = train_full . filter ( regex = 'V' ) plt . figure ( figsize = ( 18 , 9 ) ) sns . heatmap ( train_full_Vesta . isnull ( ) , cbar = False )
763	plt . hist ( train_transaction [ 'TransactionDT' ] , label = 'train' ) plt . hist ( test_transaction [ 'TransactionDT' ] , label = 'test' ) plt . legend ( ) plt . title ( 'Distribution of TransactionDT' )
764	plt . figure ( figsize = ( 12 , 6 ) ) g = sns . countplot ( x = 'P_emaildomain' , data = train_full ) g . set_title ( 'Purchaser Email Domain Distribution' , fontsize = 15 ) g . set_xlabel ( "Email Domain" , fontsize = 15 ) g . set_ylabel ( "Count" , fontsize = 15 ) plt . xticks ( rotation = 'vertical' )
765	protonmail_fraud = len ( train_full [ ( train_full [ 'P_parent_emaildomain' ] == "protonmail" ) & ( train_full [ 'isFraud' ] == 1 ) ] ) protonmail_non_fraud = len ( train_full [ ( train_full [ 'P_parent_emaildomain' ] == "protonmail" ) & ( train_full [ 'isFraud' ] == 0 ) ] ) protonmail_fraud_rate = protonmail_fraud / ( protonmail_fraud + protonmail_non_fraud ) print ( "Number of protonmail fraud transactions:" , protonmail_fraud ) print ( "Number of protonmail non-fraud transactions:" , protonmail_non_fraud ) print ( "Protonmail fraud rate:" , protonmail_fraud_rate )
766	train_full [ 'major_os' ] = train_full [ "id_30" ] . str . split ( ' ' , expand = True ) [ [ 0 ] ] visualize_cat_cariable ( 'major_os' )
767	train_full [ 'browser' ] = train_full [ "id_31" ] . str . split ( ' ' , expand = True ) [ [ 0 ] ] visualize_cat_cariable ( 'browser' )
768	n_splits = 5 splits = list ( StratifiedKFold ( n_splits = n_splits , shuffle = True ) . split ( train_features , train_target ) ) splits [ : 3 ]
769	train_features = train_df . drop ( [ 'target' , 'ID_code' ] , axis = 1 ) test_features = test_df . drop ( [ 'ID_code' ] , axis = 1 ) train_target = train_df [ 'target' ]
770	esemble = 0.6 * oof + 0.4 * train_preds print ( 'NN auc = {:<8.5f}' . format ( auc ) ) print ( 'LightBGM auc = {:<8.5f}' . format ( roc_auc_score ( train_target , oof ) ) ) print ( 'NN+LightBGM auc = {:<8.5f}' . format ( roc_auc_score ( train_target , esemble ) ) )
771	my_submission_nn = pd . DataFrame ( { "ID_code" : id_code_test , "target" : test_preds } ) my_submission_lbgm = pd . DataFrame ( { "ID_code" : id_code_test , "target" : predictions } ) my_submission_esemble = pd . DataFrame ( { "ID_code" : id_code_test , "target" : esemble_pred } )
772	import sys package_path = '../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master' sys . path . append ( package_path ) from efficientnet_pytorch import EfficientNet
773	data_dir = '/kaggle/input/prostate-cancer-grade-assessment/train_images' mask_dir = '/kaggle/input/prostate-cancer-grade-assessment/train_label_masks' train_labels = pd . read_csv ( '/kaggle/input/prostate-cancer-grade-assessment/train.csv' )
774	import os import gc print ( os . listdir ( "../input" ) ) import numpy as np import pandas as pd import time
775	train [ 'exist_ship' ] = train [ 'EncodedPixels' ] . fillna ( 0 ) train . loc [ train [ 'exist_ship' ] != 0 , 'exist_ship' ] = 1 del train [ 'EncodedPixels' ]
776	print ( len ( train [ 'ImageId' ] ) ) print ( train [ 'ImageId' ] . value_counts ( ) . shape [ 0 ] ) train_gp = train . groupby ( 'ImageId' ) . sum ( ) . reset_index ( ) train_gp . loc [ train_gp [ 'exist_ship' ] > 0 , 'exist_ship' ] = 1
777	print ( train_gp [ 'exist_ship' ] . value_counts ( ) ) train_gp = train_gp . sort_values ( by = 'exist_ship' ) train_gp = train_gp . drop ( train_gp . index [ 0 : 100000 ] )
778	print ( train_gp [ 'exist_ship' ] . value_counts ( ) ) train_sample = train_gp . sample ( 5000 ) print ( train_sample [ 'exist_ship' ] . value_counts ( ) ) print ( train_sample . shape )
779	from sklearn . preprocessing import OneHotEncoder targets = data_target . reshape ( len ( data_target ) , - 1 ) enc = OneHotEncoder ( ) enc . fit ( targets ) targets = enc . transform ( targets ) . toarray ( ) print ( targets . shape )
780	from sklearn . model_selection import train_test_split x_train , x_val , y_train , y_val = train_test_split ( data , targets , test_size = 0.2 ) x_train . shape , x_val . shape , y_train . shape , y_val . shape
781	from keras . preprocessing . image import ImageDataGenerator img_gen = ImageDataGenerator ( rescale = 1. / 255 , zca_whitening = False , rotation_range = 90 , width_shift_range = 0.2 , height_shift_range = 0.2 , brightness_range = [ 0.5 , 1.5 ] , shear_range = 0.2 , zoom_range = 0.2 , horizontal_flip = True , vertical_flip = True )
782	from keras . applications . resnet50 import ResNet50 as ResModel img_width , img_height = 256 , 256 model = ResModel ( weights = 'imagenet' , include_top = False , input_shape = ( img_width , img_height , 3 ) )
783	from keras import optimizers epochs = 10 lrate = 0.001 decay = lrate / epochs sgd = optimizers . SGD ( lr = lrate , momentum = 0.9 , decay = decay , nesterov = False ) model_final . compile ( loss = 'categorical_crossentropy' , optimizer = sgd , metrics = [ 'accuracy' ] ) model_final . summary ( )
784	from sklearn . preprocessing import OneHotEncoder targets_predict = data_target_predict . reshape ( len ( data_target_predict ) , - 1 ) enc = OneHotEncoder ( ) enc . fit ( targets_predict ) targets_predict = enc . transform ( targets_predict ) . toarray ( ) print ( targets_predict . shape )
785	drop_cols = [ "bin_0" ] ddall [ "ord_5a" ] = ddall [ "ord_5" ] . str [ 0 ] ddall [ "ord_5b" ] = ddall [ "ord_5" ] . str [ 1 ] drop_cols . append ( "ord_5" )
786	for col in [ "nom_5" , "nom_6" , "nom_7" , "nom_8" , "nom_9" ] : train_vals = set ( dd0 [ col ] . unique ( ) ) test_vals = set ( ddtest0 [ col ] . unique ( ) ) xor_cat_vals = train_vals ^ test_vals if xor_cat_vals : ddall . loc [ ddall [ col ] . isin ( xor_cat_vals ) , col ] = "xor"
787	ohc = scipy . sparse . hstack ( [ ohc1 ] + thermos ) . tocsr ( ) display ( ohc ) X_train = ohc [ : num_train ] X_test = ohc [ num_train : ] y_train = dd0 [ "target" ] . values
788	import numpy as np import pandas as pd import os print ( os . listdir ( "../input" ) )
789	drop_cols = [ "bin_0" ] ddall [ "ord_5a" ] = ddall [ "ord_5" ] . str [ 0 ] ddall [ "ord_5b" ] = ddall [ "ord_5" ] . str [ 1 ] drop_cols . append ( "ord_5" )
790	for col in [ "nom_5" , "nom_6" , "nom_7" , "nom_8" , "nom_9" ] : train_vals = set ( dd0 [ col ] . unique ( ) ) test_vals = set ( ddtest0 [ col ] . unique ( ) ) xor_cat_vals = train_vals ^ test_vals if xor_cat_vals : ddall . loc [ ddall [ col ] . isin ( xor_cat_vals ) , col ] = "xor"
791	ohc = scipy . sparse . hstack ( [ ohc1 ] + thermos ) . tocsr ( ) display ( ohc ) X_train = ohc [ : num_train ] X_test = ohc [ num_train : ] y_train = dd0 [ "target" ] . values
792	import numpy as np from sklearn . metrics import log_loss from sklearn . base import BaseEstimator from scipy . optimize import minimize
793	from sklearn . datasets import make_classification from sklearn . linear_model import LogisticRegression from sklearn . ensemble import RandomForestClassifier , GradientBoostingClassifier , ExtraTreesClassifier from sklearn . svm import SVC from sklearn . neighbors import KNeighborsClassifier from sklearn . calibration import CalibratedClassifierCV from sklearn . cross_validation import train_test_split from sklearn . linear_model import LogisticRegressionCV from xgboost . sklearn import XGBClassifier random_state = 1
794	n_classes = 12 data , labels = make_classification ( n_samples = 2000 , n_features = 100 , n_informative = 50 , n_classes = n_classes , random_state = random_state ) X , X_test , y , y_test = train_test_split ( data , labels , test_size = 0.2 , random_state = random_state ) X_train , X_valid , y_train , y_valid = train_test_split ( X , y , test_size = 0.25 , random_state = random_state ) print ( 'Data shape:' ) print ( 'X_train: %s, X_valid: %s, X_test: %s \n' % ( X_train . shape , X_valid . shape , X_test . shape ) )
795	lr = LogisticRegressionCV ( Cs = 10 , dual = False , fit_intercept = True , intercept_scaling = 1.0 , max_iter = 100 , multi_class = 'ovr' , n_jobs = 1 , penalty = 'l2' , random_state = random_state , solver = 'lbfgs' , tol = 0.0001 ) lr . fit ( XV , y_valid ) y_lr = lr . predict_proba ( XT ) print ( '{:20s} {:2s} {:1.7f}' . format ( 'Log_Reg:' , 'logloss =>' , log_loss ( y_test , y_lr ) ) )
796	print ( len ( memory ) ) augmentData ( memory ) print ( len ( memory ) )
797	TRAIN_PATH = '../input/understanding_cloud_organization/train_images/' TEST_PATH = '../input/understanding_cloud_organization/test_images/' train_df = pd . read_csv ( '../input/understanding_cloud_organization/train.csv' ) train_fns = sorted ( glob ( TRAIN_PATH + '*.jpg' ) ) train_image_path = os . path . join ( '/kaggle/input/understanding_cloud_organization' , 'train_images' ) print ( 'There are {} images in the train set.' . format ( len ( train_fns ) ) )
798	labels = 'Train' , 'Test' sizes = [ len ( train_fns ) , len ( test_fns ) ] explode = ( 0 , 0.1 ) fig , ax = plt . subplots ( figsize = ( 6 , 6 ) ) ax . pie ( sizes , explode = explode , labels = labels , autopct = '%1.1f%%' , shadow = True , startangle = 90 ) ax . axis ( 'equal' ) ax . set_title ( 'Train and Test Sets' ) plt . show ( )
799	split_df = train_df [ "Image_Label" ] . str . split ( "_" , n = 1 , expand = True ) train_df [ 'Image' ] = split_df [ 0 ] train_df [ 'Label' ] = split_df [ 1 ] train_df . head ( )
800	print ( 'Total number of images: %s' % len ( train_df [ 'Image' ] . unique ( ) ) ) print ( 'Images with at least one label: %s' % len ( train_df [ train_df [ 'EncodedPixels' ] != 'NaN' ] [ 'Image' ] . unique ( ) ) )
801	corrs = np . corrcoef ( corr_df . values . T ) sns . set ( font_scale = 1 ) sns . set ( rc = { 'figure.figsize' : ( 7 , 7 ) } ) hm = sns . heatmap ( corrs , cbar = True , annot = True , square = True , fmt = '.2f' , yticklabels = [ 'Fish' , 'Flower' , 'Gravel' , 'Sugar' ] , xticklabels = [ 'Fish' , 'Flower' , 'Gravel' , 'Sugar' ] ) . set_title ( 'Cloud type correlation heatmap' ) fig = hm . get_figure ( )
802	train_imgs , val_imgs = train_test_split ( train_df [ 'Image' ] . values , test_size = 0.1 , stratify = train_df [ 'Class' ] . map ( lambda x : str ( sorted ( list ( x ) ) ) ) , random_state = 43 )
803	data_generator_train = DataGenenerator ( train_imgs , augmentation = albumentations_train ) data_generator_train_eval = DataGenenerator ( train_imgs , shuffle = False ) data_generator_val = DataGenenerator ( val_imgs , shuffle = False )
804	from tensorflow . keras . applications . inception_resnet_v2 import InceptionResNetV2 def get_model ( ) : base_model = model = ResNeXt101 ( ... , backend = tf . keras . backend , layers = tf . keras . layers , weights = 'imagenet' , models = tf . keras . models , utils = tf . keras . utils ) x = base_model . output y_pred = Dense ( 4 , activation = 'sigmoid' ) ( x ) return Model ( inputs = base_model . input , outputs = y_pred ) model = get_model ( )
805	for base_layer in model . layers [ : - 1 ] : base_layer . trainable = False model . compile ( optimizer = Adam ( lr = 0.0001 ) , loss = 'categorical_crossentropy' ) history_0 = model . fit_generator ( generator = data_generator_train , validation_data = data_generator_val , epochs = 1 , callbacks = [ train_metric_callback , val_callback ] , workers = num_cores , verbose = 1 )
806	def plot_with_dots ( ax , np_array ) : ax . scatter ( list ( range ( 1 , len ( np_array ) + 1 ) ) , np_array , s = 50 ) ax . plot ( list ( range ( 1 , len ( np_array ) + 1 ) ) , np_array )
807	store [ 'Promo2SinceWeek' ] . fillna ( 0 , inplace = True ) store [ 'Promo2SinceYear' ] . fillna ( store [ 'Promo2SinceYear' ] . mode ( ) [ 0 ] , inplace = True ) store [ 'PromoInterval' ] . fillna ( store [ 'PromoInterval' ] . mode ( ) [ 0 ] , inplace = True )
808	size = 1024 for img in hair_images : image = cv2 . imread ( BASE_PATH + '/jpeg/train/' + img + '.jpg' ) image_resize = cv2 . resize ( image , ( size , size ) ) image_resize = cv2 . cvtColor ( image_resize , cv2 . COLOR_BGR2RGB ) plt . imshow ( image_resize ) plt . show ( )
809	lower_limit = 20 grayScale = cv2 . cvtColor ( image_resize , cv2 . COLOR_RGB2GRAY ) kernel = cv2 . getStructuringElement ( 1 , ( 17 , 17 ) ) blackhat = cv2 . morphologyEx ( grayScale , cv2 . MORPH_BLACKHAT , kernel ) _ , threshold = cv2 . threshold ( blackhat , 20 , 255 , cv2 . THRESH_BINARY ) threshold = cv2 . bitwise_not ( threshold ) plt . imshow ( threshold , cmap = 'gray' )
810	for i , img_name in enumerate ( hair_images ) : _ , hair_mask , _ = img ( img_name ) plt . title ( f'{i},{img_name}' ) plt . imshow ( cv2 . bitwise_and ( image_1 , image_1 , mask = hair_mask ) ) plt . show ( )
811	import matplotlib . pyplot as plt import numpy as np import pandas as pd import seaborn as sns import math from math import * from plotly . offline import init_notebook_mode , iplot import plotly . graph_objects as go from ipywidgets import widgets from ipywidgets import * init_notebook_mode ( connected = True )
812	rate = train [ "species" ] . value_counts ( ) . sort_values ( ) / 264 print ( f'{"Target" :-<40} {"rate":-<20}' ) for n in range ( len ( rate ) ) : print ( f'{rate.index[n] :-<40} {rate[n]}' )
813	longitude = pd . to_numeric ( train [ 'longitude' ] , errors = 'coerce' ) latitude = pd . to_numeric ( train [ 'latitude' ] , errors = 'coerce' ) df = pd . concat ( [ longitude , latitude ] , axis = 1 )
814	from IPython . display import YouTubeVideo YouTubeVideo ( 'MhOdbtPhbLU' , width = 800 , height = 300 )
815	N = 5 ebird_code_simple = sample ( list ( train [ "ebird_code" ] . unique ( ) ) , N ) AudioProcessing ( ) . PlotSampleWave ( nrows = N , captions = ebird_code_simple , df = train )
816	application_sample1 = application_train . loc [ application_train . TARGET == 1 ] . sample ( frac = 0.1 , replace = False ) print ( 'label 1 sample size:' , str ( application_sample1 . shape [ 0 ] ) ) application_sample0 = application_train . loc [ application_train . TARGET == 0 ] . sample ( frac = 0.1 , replace = False ) print ( 'label 0 sample size:' , str ( application_sample0 . shape [ 0 ] ) ) application = pd . concat ( [ application_sample1 , application_sample0 ] , axis = 0 ) . sort_values ( 'SK_ID_CURR' )
817	categorical_list = [ ] numerical_list = [ ] for i in application . columns . tolist ( ) : if application [ i ] . dtype == 'object' : categorical_list . append ( i ) else : numerical_list . append ( i ) print ( 'Number of categorical features:' , str ( len ( categorical_list ) ) ) print ( 'Number of numerical features:' , str ( len ( numerical_list ) ) )
818	del application_train ; gc . collect ( ) application = pd . get_dummies ( application , drop_first = True ) print ( application . shape )
819	X = application . drop ( [ 'SK_ID_CURR' , 'TARGET' ] , axis = 1 ) y = application . TARGET feature_name = X . columns . tolist ( )
820	from sklearn . feature_selection import SelectKBest from sklearn . feature_selection import chi2 from sklearn . preprocessing import MinMaxScaler X_norm = MinMaxScaler ( ) . fit_transform ( X ) chi_selector = SelectKBest ( chi2 , k = 100 ) chi_selector . fit ( X_norm , y )
821	from sklearn . feature_selection import RFE from sklearn . linear_model import LogisticRegression rfe_selector = RFE ( estimator = LogisticRegression ( ) , n_features_to_select = 100 , step = 10 , verbose = 5 ) rfe_selector . fit ( X_norm , y )
822	from sklearn . feature_selection import SelectFromModel from sklearn . linear_model import LogisticRegression embeded_lr_selector = SelectFromModel ( LogisticRegression ( penalty = "l1" ) , '1.25*median' ) embeded_lr_selector . fit ( X_norm , y )
823	from sklearn . feature_selection import SelectFromModel from sklearn . ensemble import RandomForestClassifier embeded_rf_selector = SelectFromModel ( RandomForestClassifier ( n_estimators = 100 ) , threshold = '1.25*median' ) embeded_rf_selector . fit ( X , y )
824	from sklearn . feature_selection import SelectFromModel from lightgbm import LGBMClassifier lgbc = LGBMClassifier ( n_estimators = 500 , learning_rate = 0.05 , num_leaves = 32 , colsample_bytree = 0.2 , reg_alpha = 3 , reg_lambda = 1 , min_split_gain = 0.01 , min_child_weight = 40 ) embeded_lgb_selector = SelectFromModel ( lgbc , threshold = '1.25*median' ) embeded_lgb_selector . fit ( X , y )
825	PATH = '/kaggle/input/covid19-global-forecasting-week-4/' train_df = pd . read_csv ( PATH + 'train.csv' , parse_dates = [ 'Date' ] ) test_df = pd . read_csv ( PATH + 'test.csv' , parse_dates = [ 'Date' ] ) add_datepart ( train_df , 'Date' , drop = False ) add_datepart ( test_df , 'Date' , drop = False )
826	cat_vars = [ 'Country_Region' , 'Province_State' , 'continent' ] cont_vars = [ 'DaysSinceFirst' , 'DaysSince50' , 'Dayofyear' , 'DaysQua' , 'DaysSql' , 'latitude' , 'longitude' , 'TRUE POPULATION' , 'TFR' , 'Personality_uai' , 'testper1m' , 'positiveper1m' , 'casediv1m' , 'deathdiv1m' , 'FatalityRate' , 'smokers' , 'lung' , ] dep_var = [ 'ConfirmedCases' , 'Fatalities' ] df = train_df [ cont_vars + cat_vars + dep_var + [ 'Date' ] ] . copy ( ) . sort_values ( 'Date' )
827	df1 = df . copy ( ) df1 [ 'ConfirmedCases' ] = np . log ( df1 [ 'ConfirmedCases' ] ) df1 [ 'Fatalities' ] = np . log ( df1 [ 'Fatalities' ] + 1 )
828	to_tst = to . new ( test_df ) to_tst . process ( ) to_tst . all_cols . head ( )
829	dls . c = 2 model = TabNetModel ( emb_szs , len ( to . cont_names ) , dls . c , n_d = 16 , n_a = 8 , n_steps = 3 ) opt_func = partial ( Adam , wd = 0.01 , eps = 1e-5 ) learn = Learner ( dls , model , MSELossFlat ( ) , opt_func = opt_func , lr = 3e-2 , metrics = [ rmse ] )
830	import pandas as pd pd . set_option ( 'display.max_columns' , None ) import numpy as np import matplotlib . pyplot as plt import seaborn as sns color = sns . color_palette ( ) import gc import warnings import time warnings . filterwarnings ( "ignore" )
831	def plot_numerical ( data , col , size = [ 8 , 4 ] , bins = 50 ) : plt . figure ( figsize = size ) plt . title ( "Distribution of %s" % col ) sns . distplot ( data [ col ] . dropna ( ) , kde = True , bins = bins ) plt . show ( ) plot_numerical ( application_train , 'AMT_CREDIT' )
832	corr_mat = application_train . corr ( ) plt . figure ( figsize = [ 15 , 15 ] ) sns . heatmap ( corr_mat . values , annot = False ) plt . show ( )
833	from sklearn . impute import SimpleImputer , MICEImputer application_train = pd . read_csv ( '../input/application_train.csv' ) application_test = pd . read_csv ( '../input/application_test.csv' )
834	import lightgbm as lgb from lightgbm import LGBMClassifier from sklearn . metrics import roc_auc_score from sklearn . model_selection import StratifiedKFold
835	application_test = pd . read_csv ( '../input/application_test.csv' ) output = pd . DataFrame ( { 'SK_ID_CURR' : application_test . SK_ID_CURR , 'TARGET' : sub_preds } ) output . to_csv ( 'only_application_pred.csv' , index = False )
836	import os import time import gc import warnings warnings . filterwarnings ( "ignore" ) import json from pandas . io . json import json_normalize import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns color = sns . color_palette ( ) from sklearn . preprocessing import LabelEncoder from sklearn . model_selection import KFold from sklearn . metrics import mean_squared_error import lightgbm as lgb
837	def find_missing ( data ) : count_missing = data . isnull ( ) . sum ( ) . values total = data . shape [ 0 ] ratio_missing = count_missing / total return pd . DataFrame ( data = { 'missing_count' : count_missing , 'missing_ratio' : ratio_missing } , index = data . columns . values ) train_missing = find_missing ( train ) test_missing = find_missing ( test )
838	if test . fullVisitorId . nunique ( ) == len ( sub ) : print ( 'Till now, the number of fullVisitorId is equal to the rows in submission. Everything goes well!' ) else : print ( 'Check it again' )
839	train [ "totals.transactionRevenue" ] = train [ "totals.transactionRevenue" ] . astype ( 'float' ) target = np . log1p ( train . groupby ( "fullVisitorId" ) [ "totals.transactionRevenue" ] . sum ( ) ) print ( 'The ratio of customers with transaction revenue is' , str ( ( target != 0 ) . mean ( ) ) )
840	a = train . groupby ( "fullVisitorId" ) [ "visitNumber" ] . max ( ) plt . figure ( figsize = [ 12 , 6 ] ) sns . distplot ( a ) plt . xlabel ( 'VisitNumber' ) plt . title ( 'Visit Number' ) plt . show ( )
841	plt . figure ( figsize = [ 12 , 6 ] ) sns . distplot ( train [ 'date' ] ) plt . xlabel ( 'Date' ) plt . title ( 'Date' ) plt . show ( )
842	train_idx = train . fullVisitorId test_idx = test . fullVisitorId train [ "totals.transactionRevenue" ] = train [ "totals.transactionRevenue" ] . astype ( 'float' ) . fillna ( 0 ) train_y = train [ "totals.transactionRevenue" ] train_target = np . log1p ( train . groupby ( "fullVisitorId" ) [ "totals.transactionRevenue" ] . sum ( ) )
843	y_train = np . log1p ( train [ "totals.transactionRevenue" ] ) x_train = train . drop ( [ "totals.transactionRevenue" ] , axis = 1 ) x_test = test . copy ( ) print ( x_train . shape ) print ( x_test . shape )
844	lgbBO = BayesianOptimization ( lgb_eval , { 'num_leaves' : ( 24 , 45 ) , 'feature_fraction' : ( 0.1 , 0.9 ) , 'bagging_fraction' : ( 0.8 , 1 ) , 'max_depth' : ( 5 , 8.99 ) , 'lambda_l1' : ( 0 , 5 ) , 'lambda_l2' : ( 0 , 3 ) , 'min_split_gain' : ( 0.001 , 0.1 ) , 'min_child_weight' : ( 5 , 50 ) } , random_state = 0 )
845	import numpy as np import pandas as pd import os import json from pathlib import Path import matplotlib . pyplot as plt from matplotlib import colors import numpy as np from pathlib import Path for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : print ( dirname )
846	data_path = Path ( '/kaggle/input/abstraction-and-reasoning-challenge/' ) training_path = data_path / 'training' evaluation_path = data_path / 'evaluation' test_path = data_path / 'test' training_tasks = sorted ( os . listdir ( training_path ) ) eval_tasks = sorted ( os . listdir ( evaluation_path ) )
847	evaluation_examples = [ ] for i in range ( 400 ) : task = Evals [ i ] basic_task = Create ( task , 0 ) a = Function ( basic_task ) if a != - 1 and task [ 'test' ] [ 0 ] [ 'output' ] == a : plot_picture ( a ) plot_task ( task ) print ( i ) evaluation_examples . append ( i )
848	import pandas as pd import dask . dataframe as dd import os from tqdm import tqdm TRAIN_PATH = '../input/train.csv'
849	traintypes = { 'fare_amount' : 'float32' , 'pickup_datetime' : 'str' , 'pickup_longitude' : 'float32' , 'pickup_latitude' : 'float32' , 'dropoff_longitude' : 'float32' , 'dropoff_latitude' : 'float32' , 'passenger_count' : 'uint8' } cols = list ( traintypes . keys ( ) )
850	SIZE_X : int = 15 SIZE_Y : int = 15 NUM_SHIPS : int = 7 MIN_NUM_GATHERER : int = 3 NUM_HUNTER : int = 1 HALITE_MINE_FACTOR : float = 0.25 TIME_FACTOR : float = 0.99
851	import numpy as np mask = torch . stack ( [ mask , mask , mask ] , dim = 2 ) mask = mask . cpu ( ) . numpy ( ) . astype ( "uint8" ) instances = cv2 . multiply ( image , mask ) plt . imshow ( instances ) plt . show ( )
852	import os import cv2 import pdb import glob import argparse import numpy as np
853	import torch def residual_add ( lhs , rhs ) : lhs_ch , rhs_ch = lhs . shape [ 1 ] , rhs . shape [ 1 ] if lhs_ch < rhs_ch : out = lhs + rhs [ : , : lhs_ch ] elif lhs_ch > rhs_ch : out = torch . cat ( [ lhs [ : , : rhs_ch ] + rhs , lhs [ : , rhs_ch : ] ] , dim = 1 ) else : out = lhs + rhs return out
854	from tensorflow . keras import Sequential from tensorflow . keras . layers import Flatten , Dense , Embedding , LSTM , Dropout from tensorflow . keras . preprocessing . text import one_hot from tensorflow . keras . preprocessing . sequence import pad_sequences
855	import nltk import re from nltk . corpus import stopwords from nltk . stem . porter import PorterStemmer
856	ps = PorterStemmer ( ) corpus = [ ] for i in range ( 0 , len ( messages ) ) : result = re . sub ( '[^a-zA-Z]' , ' ' , messages [ 'title' ] [ i ] ) result = result . lower ( ) result = result . split ( ) result = [ ps . stem ( word ) for word in result if not word in stopwords . words ( "english" ) ] result = " " . join ( result ) corpus . append ( result )
857	voc_size = 5000 onehot_repr = [ one_hot ( words , voc_size ) for words in corpus ] onehot_repr
858	sent_length = 20 embeded_docs = pad_sequences ( onehot_repr , padding = 'pre' , maxlen = sent_length ) embeded_docs
859	embedding_vector_features = 40 model = Sequential ( ) model . add ( Embedding ( voc_size , embedding_vector_features , input_length = sent_length ) ) model . add ( Dropout ( 0.3 ) ) model . add ( LSTM ( 100 ) ) model . add ( Dropout ( 0.3 ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] )
860	params = { 'booster' : 'gbtree' , 'objective' : 'multi:softprob' , 'eval_metric' : 'mlogloss' , 'eta' : 0.005 , 'max_depth' : 10 , 'subsample' : 1.0 , 'colsample_bytree' : 1.0 , 'tree_method' : 'gpu_hist' , 'num_class' : 5 }
861	dtest = xgb . DMatrix ( test_results_df . drop ( columns = [ 'diagnosis' ] ) ) test_pred = pd . DataFrame ( np . argmax ( bst . predict ( dtest ) , axis = 1 ) ) submission = pd . concat ( [ test [ 'id_code' ] , test_pred ] , axis = 1 ) submission . columns = [ 'id_code' , 'diagnosis' ] submission . to_csv ( 'submission.csv' , index = False ) submission . head ( )
862	stepsize = np . diff ( train . quaketime ) train = train . drop ( train . index [ len ( train ) - 1 ] ) train [ "stepsize" ] = stepsize train . head ( 5 )
863	from sklearn . model_selection import TimeSeriesSplit cv = TimeSeriesSplit ( n_splits = 5 )
864	window_sizes = [ 10 , 50 , 100 , 1000 ] for window in window_sizes : train [ "rolling_mean_" + str ( window ) ] = train . signal . rolling ( window = window ) . mean ( ) train [ "rolling_std_" + str ( window ) ] = train . signal . rolling ( window = window ) . std ( )
865	oof_qda1 = oof_qda1 . reshape ( - 1 , 1 ) oof_qda2 = oof_qda2 . reshape ( - 1 , 1 ) oof_gmm = oof_gmm . reshape ( - 1 , 1 ) oof_lr = oof_lr . reshape ( - 1 , 1 ) oof_ls = oof_ls . reshape ( - 1 , 1 ) oof_knn = oof_knn . reshape ( - 1 , 1 ) oof_nn = oof_nn . reshape ( - 1 , 1 ) oof_qda3 = oof_qda3 . reshape ( - 1 , 1 )
866	train = pd . read_csv ( "../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv" ) test = pd . read_csv ( "../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv" ) print ( "Train shape : " , train . shape ) print ( "Test shape : " , test . shape ) train . head ( )
867	df = pd . concat ( [ train [ [ 'id' , 'comment_text' ] ] , test ] , axis = 0 ) del ( train , test ) gc . collect ( )
868	def build_vocab ( texts ) : sentences = texts . apply ( lambda x : x . split ( ) ) . values vocab = { } for sentence in sentences : for word in sentence : try : vocab [ word ] += 1 except KeyError : vocab [ word ] = 1 return vocab
869	train = df . iloc [ : 1804874 , : ] test = df . iloc [ 1804874 : , : ] train . head ( )
870	MAX_NUM_WORDS = 100000 TOXICITY_COLUMN = 'target' TEXT_COLUMN = 'comment_text' tokenizer = Tokenizer ( num_words = MAX_NUM_WORDS ) tokenizer . fit_on_texts ( train_df [ TEXT_COLUMN ] ) MAX_SEQUENCE_LENGTH = 256 def pad_text ( texts , tokenizer ) : return pad_sequences ( tokenizer . texts_to_sequences ( texts ) , maxlen = MAX_SEQUENCE_LENGTH )
871	submission = pd . read_csv ( '../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv' , index_col = 'id' ) submission [ 'prediction' ] = model . predict ( pad_text ( test [ TEXT_COLUMN ] , tokenizer ) ) submission . reset_index ( drop = False , inplace = True ) submission . head ( )
872	import os import matplotlib import matplotlib . pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn . metrics import log_loss from tqdm import tqdm import warnings
873	data = pd . concat ( ( train , test ) ) np . random . seed ( 42 ) data = data . iloc [ np . random . permutation ( len ( data ) ) ] data . reset_index ( drop = True , inplace = True ) x = data . drop ( [ 'target' , 'ID_code' , 'train_test' ] , axis = 1 ) y = data . train_test
874	train_examples = len ( train ) x_train = x [ : train_examples ] x_test = x [ train_examples : ] y_train = y [ : train_examples ] y_test = y [ train_examples : ]
875	clf = LogisticRegression ( penalty = "l1" , C = 0.1 , solver = "liblinear" , random_state = 42 ) clf . fit ( x_train , y_train ) y_pred = clf . predict_proba ( x_test ) [ : , 1 ] roc_auc_score ( y_test , y_pred )
876	import pandas as pd import numpy as np import matplotlib . pylab as plt import seaborn as sns from itertools import cycle pd . set_option ( 'max_columns' , 50 ) plt . style . use ( 'bmh' ) color_cycle = cycle ( plt . rcParams [ 'axes.prop_cycle' ] . by_key ( ) [ 'color' ] )
877	d_cols = [ c for c in stv . columns if 'd_' in c ] stv . loc [ stv [ 'id' ] == 'FOODS_3_090_CA_3_validation' ] \ . set_index ( 'id' ) [ d_cols ] \ . T \ . plot ( figsize = ( 15 , 5 ) , title = 'FOODS_3_090_CA_3 sales by "d" number' , color = next ( color_cycle ) ) plt . legend ( '' ) plt . show ( )
878	twenty_examples = stv . sample ( 20 , random_state = 529 ) \ . set_index ( 'id' ) [ d_cols ] \ . T \ . merge ( cal . set_index ( 'd' ) [ 'date' ] , left_index = True , right_index = True , validate = '1:1' ) \ . set_index ( 'date' )
879	store_list = sellp [ 'store_id' ] . unique ( ) for s in store_list : store_items = [ c for c in past_sales . columns if s in c ] past_sales [ store_items ] \ . sum ( axis = 1 ) \ . rolling ( 90 ) . mean ( ) \ . plot ( figsize = ( 15 , 5 ) , alpha = 0.8 , title = 'Rolling 90 Day Average Total Sales (10 stores)' ) plt . legend ( store_list ) plt . show ( )
880	from sklearn import preprocessing , metrics from keras . models import Sequential from keras . layers import Dense from keras . layers import LSTM , Dropout from keras . layers import RepeatVector , TimeDistributed from numpy import array from keras . models import Sequential , load_model import re from tqdm import tqdm import os
881	for i in train_sales . dept_id . unique ( ) : fig = plt . figure ( figsize = ( 20 , 5 ) ) for j in range ( 10 ) : ax = fig . add_subplot ( 1 , 1 , 1 ) ax . plot ( train_sales [ train_sales . dept_id == i ] . iloc [ j , : ] [ time_series_columns ] . values ) ax . set_title ( str ( i ) ) ax . set_xlabel ( 'Time' ) ax . set_ylabel ( 'Sales' )
882	npt = nlplot . NLPlot ( train , target_col = 'text' ) npt_negative = nlplot . NLPlot ( train . query ( 'sentiment == "negative"' ) , target_col = 'text' ) npt_neutral = nlplot . NLPlot ( train . query ( 'sentiment == "neutral"' ) , target_col = 'text' ) npt_positive = nlplot . NLPlot ( train . query ( 'sentiment == "positive"' ) , target_col = 'text' )
883	npt . bar_ngram ( title = 'uni-gram' , xaxis_label = 'word_count' , yaxis_label = 'word' , ngram = 1 , top_n = 50 , width = 800 , height = 1100 , stopwords = stopwords , )
884	npt . word_distribution ( title = 'number of words distribution' )
885	import lightgbm as lgb import catboost as cat import pandas as pd import xgboost as xgb import numpy as np import matplotlib . pyplot as plt from mpl_toolkits . mplot3d import Axes3D import seaborn as sns from tqdm import tqdm from datetime import datetime from sklearn . model_selection import GroupKFold from sklearn . metrics import mean_squared_error from sklearn . linear_model import LinearRegression from sklearn . preprocessing import LabelEncoder import warnings import os print ( os . listdir ( "../input" ) ) warnings . filterwarnings ( 'ignore' ) random_seed = 2018
886	release_dates = pd . read_csv ( '../input/release-dates/release_dates_per_country.csv' ) release_dates [ 'id' ] = range ( 1 , 7399 ) release_dates . drop ( [ 'original_title' , 'title' ] , axis = 1 , inplace = True ) release_dates . index = release_dates [ 'id' ] train = pd . merge ( train , release_dates , how = 'left' , on = [ 'id' ] ) test = pd . merge ( test , release_dates , how = 'left' , on = [ 'id' ] )
887	trainAdditionalFeatures = pd . read_csv ( '../input/additionnal-features/TrainAdditionalFeatures.csv' ) [ [ 'imdb_id' , 'popularity2' , 'rating' , 'totalVotes' ] ] testAdditionalFeatures = pd . read_csv ( '../input/additionnal-features/TestAdditionalFeatures.csv' ) [ [ 'imdb_id' , 'popularity2' , 'rating' , 'totalVotes' ] ] train = pd . merge ( train , trainAdditionalFeatures , how = 'left' , on = [ 'imdb_id' ] ) test = pd . merge ( test , testAdditionalFeatures , how = 'left' , on = [ 'imdb_id' ] )
888	Train = train . copy ( ) Train . sort_values ( 'revenue' , ascending = False , inplace = True ) Train = Train . head ( 20 ) Train [ [ 'title' , 'popularity' , 'budget' , 'genres' , 'revenue' , 'release_date' , 'production_companies' ] ]
889	cols = [ 'revenue' , 'budget' , 'popularity' , 'theatrical' , 'runtime' , 'release_year' ] sns . heatmap ( train [ cols ] . corr ( ) ) plt . show ( )
890	sns . set ( ) x = np . array ( train [ "budget" ] ) y = np . array ( train [ "revenue" ] ) fig = plt . figure ( 1 , figsize = ( 10 , 8 ) ) sns . regplot ( x , y ) plt . xlabel ( "budget" , fontsize = 10 ) plt . ylabel ( "revenue" , fontsize = 10 ) plt . title ( "Link between revenue and budget" , fontsize = 10 )
891	sns . set ( ) x = train [ 'revenue' ] y = train [ 'popularity' ] plt . figure ( figsize = ( 15 , 8 ) ) sns . regplot ( x , y ) plt . xlabel ( 'popularity' ) plt . ylabel ( 'revenue' ) plt . title ( 'Relationship between popularity and revenue of a movie' )
892	def score ( data , y ) : validation_res = pd . DataFrame ( { "id" : data [ "id" ] . values , "transactionrevenue" : data [ "revenue" ] . values , "predictedrevenue" : np . expm1 ( y ) } ) validation_res = validation_res . groupby ( "id" ) [ "transactionrevenue" , "predictedrevenue" ] . sum ( ) . reset_index ( ) return np . sqrt ( mean_squared_error ( np . log1p ( validation_res [ "transactionrevenue" ] . values ) , np . log1p ( validation_res [ "predictedrevenue" ] . values ) ) )
893	lgbmodel = lgb . LGBMRegressor ( n_estimators = 10000 , objective = 'regression' , metric = 'rmse' , max_depth = 5 , num_leaves = 30 , min_child_samples = 100 , learning_rate = 0.01 , boosting = 'gbdt' , min_data_in_leaf = 10 , feature_fraction = 0.9 , bagging_freq = 1 , bagging_fraction = 0.9 , importance_type = 'gain' , lambda_l1 = 0.2 , bagging_seed = random_seed , subsample = .8 , colsample_bytree = .9 , use_best_model = True )
894	features_train , feature_names_train = ft . dfs ( entityset = es , target_entity = 'train' , max_depth = 1 ) del es gc . collect ( )
895	columns_categorical = [ 'MODE(new_merchant_transactions.authorized_flag)' , 'MODE(new_merchant_transactions.category_1)' , 'MODE(new_merchant_transactions.category_3)' , 'MODE(new_merchant_transactions.merchant_id)' , 'MODE(historical_transactions.authorized_flag)' , 'MODE(historical_transactions.category_1)' , 'MODE(historical_transactions.category_3)' , 'MODE(historical_transactions.merchant_id)' ]
896	from sklearn . metrics import mean_squared_error import lightgbm as lgb model = lgb . LGBMRegressor ( ) model . fit ( xtrain , ytrain ) print ( "RMSE of Validation Data using Light GBM: %.2f" % math . sqrt ( mean_squared_error ( yval , model . predict ( xval ) ) ) )
897	import eli5 from eli5 . sklearn import PermutationImportance perm = PermutationImportance ( model ) . fit ( xval , yval ) eli5 . show_weights ( perm , feature_names = xval . columns . tolist ( ) )
898	from sklearn . feature_selection import SelectFromModel submission = pd . read_csv ( '../input/sample_submission.csv' ) features_test = features_test . fillna ( 0 ) features_test = features_test . reindex ( index = submission [ 'card_id' ] ) sel = SelectFromModel ( perm , threshold = 0.002 , prefit = True ) X = sel . transform ( X ) features_test = sel . transform ( features_test ) print ( "Modified shape:" , X . shape )
899	start_time = time . time ( ) hf = h5py . File ( os . path . join ( output_dir , "train.h5" ) ) segs = np . array ( hf [ "sound" ] [ : 100 ] ) seg_ttf = np . array ( hf [ "ttf" ] [ : 100 ] ) hf . close ( ) end_time = time . time ( ) print ( "{} seconds" . format ( end_time - start_time ) )
900	start_time = time . time ( ) df = pd . read_csv ( os . path . join ( input_dir , "train.csv" ) , nrows = chunk_size * 100 , dtype = { "acoustic_data" : np . int16 , "time_to_failure" : np . float32 } ) end_time = time . time ( ) print ( "{} seconds" . format ( end_time - start_time ) )
901	summ = pd . DataFrame ( { 'data' : [ 'train.csv' , 'test.csv' , 'sample_submission.csv' ] , 'rows' : [ len ( trainset ) , len ( testset ) , len ( sample_sub ) ] , 'patient' : [ trainset [ 'Patient' ] . nunique ( ) , testset [ 'Patient' ] . nunique ( ) , sample_sub [ 'Patient_Week' ] . nunique ( ) ] } ) summ . set_index ( 'data' , inplace = True ) display ( summ )
902	fig = px . histogram ( trainset , x = 'Age' , color = 'Sex' , marginal = 'box' , histnorm = 'probability density' , opacity = 0.7 ) fig . update_layout ( title = 'Distribution of Age between Male and Female' , width = 800 , height = 500 ) fig . show ( )
903	parti_patient = trainset . drop_duplicates ( subset = 'Patient' ) fig = px . histogram ( parti_patient , x = 'Age' , facet_row = 'SmokingStatus' , facet_col = 'Sex' , ) fig . for_each_annotation ( lambda a : a . update ( text = a . text . replace ( "SmokingStatus=" , "" ) ) ) fig . update_layout ( title = 'Distribution of Age sperated by Sex (col) and Smoking Status (row)' , autosize = True , width = 800 , height = 600 , font_size = 14 ) fig . show ( )
904	pivot_smkstat_sex = pd . pivot_table ( trainset , index = [ 'Sex' , 'SmokingStatus' ] , aggfunc = { 'Age' : [ 'max' , 'min' , np . mean , np . std ] , 'FVC' : [ 'max' , 'min' , np . mean , np . std ] , 'Percent' : [ 'max' , 'min' , np . mean , np . std ] } ) display ( pivot_smkstat_sex )
905	fig = px . density_contour ( trainset , x = 'Percent' , y = 'FVC' , marginal_x = "histogram" , marginal_y = "histogram" , color = 'SmokingStatus' , ) fig . update_layout ( title = 'Relationship between Percent and FVC' , width = 800 , height = 400 ) fig . show ( )
906	below_100 = trainset . query ( 'Percent < 100' ) more_100 = trainset . query ( 'Percent > 100' ) between_5 = trainset . query ( '97.5 <= Percent <= 102.5' ) x_bar = below_100 . groupby ( 'SmokingStatus' ) . size ( ) . index . to_list ( )
907	DICOM_DIR = '/kaggle/input/osic-pulmonary-fibrosis-progression/train' dicom_dict = defaultdict ( list ) for dirname in os . listdir ( DICOM_DIR ) : path = os . path . join ( DICOM_DIR , dirname ) dicom_dict [ dirname ] . append ( path ) p_id = sorted ( trainset [ 'Patient' ] . unique ( ) )
908	interp = ClassificationInterpretation . from_learner ( learn ) losses , idxs = interp . top_losses ( ) len ( data . valid_ds ) == len ( losses ) == len ( idxs )
909	from PIL import Image im = Image . open ( df [ 'path' ] [ 1 ] ) width , height = im . size print ( width , height ) im . show ( )
910	from sklearn . metrics import cohen_kappa_score def quadratic_kappa ( y_hat , y ) : return torch . tensor ( cohen_kappa_score ( torch . round ( y_hat ) , y , weights = 'quadratic' ) , device = 'cuda:0' )
911	interp = ClassificationInterpretation . from_learner ( learn ) losses , idxs = interp . top_losses ( ) len ( data . valid_ds ) == len ( losses ) == len ( idxs )
912	import os import numpy as np import pandas as pd import lightgbm as lgb from sklearn . model_selection import train_test_split from sklearn . linear_model import LogisticRegression from sklearn . metrics import classification_report
913	plt . rcParams [ "font.size" ] = "12" ax = df . ffill ( ) \ . count ( axis = 1 ) \ . plot ( figsize = ( 20 , 8 ) , title = 'Number of Teams in the Competition by Date' , color = color_pal [ 5 ] , lw = 5 ) ax . set_ylabel ( 'Number of Teams' ) plt . show ( )
914	plt . rcParams [ "font.size" ] = "12" TOP_TEAMS = df . max ( ) . loc [ df . max ( ) > FIFTYTH_SCORE ] . index . values df [ TOP_TEAMS ] . max ( ) . sort_values ( ascending = True ) . plot ( kind = 'barh' , xlim = ( TOP_SCORE - 0.1 , FIFTYTH_SCORE + 0.1 ) , title = 'Top 50 Public LB Teams' , figsize = ( 12 , 15 ) , color = color_pal [ 3 ] ) plt . show ( )
915	plt . rcParams [ "font.size" ] = "12" df [ TOP_TEAMS ] . nunique ( ) . sort_values ( ) . plot ( kind = 'barh' , figsize = ( 12 , 15 ) , color = color_pal [ 1 ] , title = 'Count of Submissions improving LB score by Team' ) plt . show ( )
916	import cv2 img = cv2 . imread ( "../input/train/HEPG2-01/Plate1/B03_s1_w2.png" ) plt . imshow ( img ) gray_img = cv2 . cvtColor ( img , cv2 . COLOR_RGB2GRAY ) plt . imshow ( gray_img ) gray_img . shape
917	def image2np ( image : Tensor ) -> np . ndarray : "Convert from torch style `image` to numpy/matplotlib style." res = image . cpu ( ) . permute ( 1 , 2 , 0 ) . numpy ( ) if res . shape [ 2 ] == 1 : return res [ ... , 0 ] elif res . shape [ 2 ] > 3 : return res [ ... , : 3 ] else : return res vision . image . image2np = image2np
918	from sklearn . model_selection import StratifiedKFold from sklearn . model_selection import train_test_split train_df , val_df = train_test_split ( proc_train_df , test_size = 0.035 , stratify = proc_train_df . sirna , random_state = 42 ) _proc_train_df = pd . concat ( [ train_df , val_df ] )
919	data = ( MultiChannelImageList . from_df ( df = _proc_train_df , path = '../input/train/' ) . split_by_idx ( list ( range ( len ( train_df ) , len ( _proc_train_df ) ) ) ) . label_from_df ( ) . transform ( get_transforms ( ) , size = 256 ) . databunch ( bs = 128 , num_workers = 4 ) . normalize ( ) )
920	from fastai . metrics import * learn = Learner ( data , efficientnetb0 ( ) , metrics = [ accuracy ] ) . to_fp16 ( ) learn . path = Path ( '../' )
921	k = 'var_44' v = engineering_feats [ k ] T = df [ v ] . copy ( ) for i , fe in enumerate ( v ) : T [ '%s_%s' % ( k . split ( '_' ) [ 1 ] , fe . split ( '_' ) [ 1 ] ) ] = df [ k ] + df [ fe ] T = T . drop ( v , axis = 1 ) T . corr ( )
922	k = 'var_44' v = df . columns [ 2 : 14 ] T = df [ v ] . copy ( ) for i , fe in enumerate ( v ) : T [ '%s_%s' % ( k . split ( '_' ) [ 1 ] , fe . split ( '_' ) [ 1 ] ) ] = df [ k ] + df [ fe ] T = T . drop ( v , axis = 1 ) T . corr ( )
923	X = df_e . values [ : , 2 : ] . astype ( np . float32 ) Y = df_e . values [ : , 1 ] . astype ( np . float32 ) print ( X . shape )
924	clz_attr = np . zeros ( ( max_clz + 1 , max_attr + 1 ) ) clz_attrid2idx = [ [ ] for _ in range ( max_clz + 1 ) ] clz_attr . shape
925	for clzid in range ( len ( clz_attr_num ) ) : if clz_attr_num [ clzid ] > 0 : if not os . path . isfile ( MODEL_FILE_DIR + "attrmodel_%d-%d.model" % ( attr_image_size [ 0 ] , clzid ) ) : model = train_attr_net ( clzid , 32 ) torch . save ( model . state_dict ( ) , MODEL_FILE_DIR + "attrmodel_%d-%d.model" % ( attr_image_size [ 0 ] , clzid ) )
926	class MaskDataset ( object ) : def __init__ ( self , keys ) : self . keys = keys def __getitem__ ( self , idx ) : k = self . keys [ idx ] return ztop ( data_mask [ k ] [ 0 ] ) , ztop ( data_mask [ k ] [ 1 ] ) def __len__ ( self ) : return len ( self . keys )
927	predict_imgeid = [ predict_imgeid [ i ] for i in set ( uses_index ) ] predict_mask = [ predict_mask [ i ] for i in set ( uses_index ) ] predict_rle = [ predict_rle [ i ] for i in set ( uses_index ) ] predict_classid = [ predict_classid [ i ] for i in set ( uses_index ) ] predict_attr = [ predict_attr [ i ] for i in set ( uses_index ) ] predict_attri_str = [ predict_attri_str [ i ] for i in set ( uses_index ) ]
928	def seed_everything ( seed ) : random . seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True
929	from sklearn . metrics import cohen_kappa_score def quadratic_kappa ( y_hat , y ) : return torch . tensor ( cohen_kappa_score ( y_hat . argmax ( dim = - 1 ) , y , weights = 'quadratic' ) , device = 'cuda:0' )
930	interp = ClassificationInterpretation . from_learner ( learn ) losses , idxs = interp . top_losses ( ) len ( data . valid_ds ) == len ( losses ) == len ( idxs )
931	idx = 1 im , cl = learn . data . dl ( DatasetType . Valid ) . dataset [ idx ] cl = int ( cl ) im . show ( title = f"pred. class: {interp.pred_class[idx]}, actual class: {learn.data.classes[cl]}" )
932	import os import random import pandas as pd import numpy as np import glob import matplotlib . pyplot as plt import cv2 import IPython . display as ipd import librosa from albumentations . core . transforms_interface import DualTransform , BasicTransform
933	class AudioTransform ( BasicTransform ) : @ property def targets ( self ) : return { "data" : self . apply } def update_params ( self , params , ** kwargs ) : if hasattr ( self , "interpolation" ) : params [ "interpolation" ] = self . interpolation if hasattr ( self , "fill_value" ) : params [ "fill_value" ] = self . fill_value return params
934	audio_path = '../input/birdsong-recognition/train_audio/aldfly/XC181484.mp3' y , sr = librosa . load ( audio_path , sr = 22050 ) print ( 'Audio Intially' ) ipd . Audio ( y , rate = sr )
935	audio_path = '../input/birdsong-recognition/train_audio/ameavo/XC133080.mp3' y , sr = librosa . load ( audio_path , sr = 22050 ) print ( 'Audio Intially' ) ipd . Audio ( y , rate = sr )
936	audio_path = '../input/birdsong-recognition/train_audio/ameavo/XC292919.mp3' y , sr = librosa . load ( audio_path , sr = 22050 ) print ( 'Audio Intially' ) ipd . Audio ( y , rate = sr )
937	class PitchShift ( AudioTransform ) : def __init__ ( self , always_apply = False , p = 0.5 , n_steps = None ) : super ( PitchShift , self ) . __init__ ( always_apply , p ) self . n_steps = n_steps def apply ( self , data , ** params ) : return librosa . effects . pitch_shift ( data , sr = 22050 , n_steps = self . n_steps )
938	audio_path = '../input/birdsong-recognition/train_audio/ameavo/XC292919.mp3' y , sr = librosa . load ( audio_path , sr = 22050 ) print ( 'Audio Intially' ) ipd . Audio ( y , rate = sr )
939	class AddGaussianNoise ( AudioTransform ) : def __init__ ( self , always_apply = False , p = 0.5 ) : super ( AddGaussianNoise , self ) . __init__ ( always_apply , p ) def apply ( self , data , ** params ) : noise = np . random . randn ( len ( data ) ) data_wn = data + 0.005 * noise return data_wn
940	audio_path = '../input/birdsong-recognition/train_audio/ameavo/XC292919.mp3' y , sr = librosa . load ( audio_path , sr = 22050 ) print ( 'Audio Intially' ) ipd . Audio ( y , rate = sr )
941	audio_path = '../input/birdsong-recognition/train_audio/ameavo/XC292919.mp3' y , sr = librosa . load ( audio_path , sr = 22050 ) print ( 'Audio Intially' ) ipd . Audio ( y , rate = sr )
942	audio_path = '../input/birdsong-recognition/train_audio/ameavo/XC292919.mp3' y , sr = librosa . load ( audio_path , sr = 22050 ) print ( 'Audio Intially' ) ipd . Audio ( y , rate = sr )
943	class CutOut ( AudioTransform ) : def __init__ ( self , always_apply = False , p = 0.5 ) : super ( CutOut , self ) . __init__ ( always_apply , p ) def apply ( self , data , ** params ) : start_ = np . random . randint ( 0 , len ( data ) ) end_ = np . random . randint ( start_ , len ( data ) ) data [ start_ : end_ ] = 0 return data
944	import albumentations def get_train_transforms ( ) : return albumentations . Compose ( [ TimeShifting ( p = 0.9 ) , albumentations . OneOf ( [ AddCustomNoise ( file_dir = '../input/freesound-audio-tagging/audio_train' , p = 0.8 ) , SpeedTuning ( p = 0.8 ) , ] ) , AddGaussianNoise ( p = 0.8 ) , PitchShift ( p = 0.5 , n_steps = 4 ) , Gain ( p = 0.9 ) , PolarityInversion ( p = 0.9 ) , StretchAudio ( p = 0.1 ) , ] )
945	try : tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) print ( 'Running on TPU ' , tpu . master ( ) ) except ValueError : tpu = None if tpu : tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) else : strategy = tf . distribute . get_strategy ( ) print ( "REPLICAS: " , strategy . num_replicas_in_sync )
946	def roc_auc ( predictions , target ) : fpr , tpr , thresholds = metrics . roc_curve ( target , predictions ) roc_auc = metrics . auc ( fpr , tpr ) return roc_auc
947	token = text . Tokenizer ( num_words = None ) max_len = 1500 token . fit_on_texts ( list ( xtrain ) + list ( xvalid ) ) xtrain_seq = token . texts_to_sequences ( xtrain ) xvalid_seq = token . texts_to_sequences ( xvalid ) xtrain_pad = sequence . pad_sequences ( xtrain_seq , maxlen = max_len ) xvalid_pad = sequence . pad_sequences ( xvalid_seq , maxlen = max_len ) word_index = token . word_index
948	embeddings_index = { } f = open ( '/kaggle/input/glove840b300dtxt/glove.840B.300d.txt' , 'r' , encoding = 'utf-8' ) for line in tqdm ( f ) : values = line . split ( ' ' ) word = values [ 0 ] coefs = np . asarray ( [ float ( val ) for val in values [ 1 : ] ] ) embeddings_index [ word ] = coefs f . close ( ) print ( 'Found %s word vectors.' % len ( embeddings_index ) )
949	embedding_matrix = np . zeros ( ( len ( word_index ) + 1 , 300 ) ) for word , i in tqdm ( word_index . items ( ) ) : embedding_vector = embeddings_index . get ( word ) if embedding_vector is not None : embedding_matrix [ i ] = embedding_vector
950	with strategy . scope ( ) : model = Sequential ( ) model . add ( Embedding ( len ( word_index ) + 1 , 300 , weights = [ embedding_matrix ] , input_length = max_len , trainable = False ) ) model . add ( SpatialDropout1D ( 0.3 ) ) model . add ( GRU ( 300 ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) model . summary ( )
951	with strategy . scope ( ) : model = Sequential ( ) model . add ( Embedding ( len ( word_index ) + 1 , 300 , weights = [ embedding_matrix ] , input_length = max_len , trainable = False ) ) model . add ( Bidirectional ( LSTM ( 300 , dropout = 0.3 , recurrent_dropout = 0.3 ) ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) model . summary ( )
952	import os import tensorflow as tf from tensorflow . keras . layers import Dense , Input from tensorflow . keras . optimizers import Adam from tensorflow . keras . models import Model from tensorflow . keras . callbacks import ModelCheckpoint from kaggle_datasets import KaggleDatasets import transformers from tokenizers import BertWordPieceTokenizer
953	def fast_encode ( texts , tokenizer , chunk_size = 256 , maxlen = 512 ) : tokenizer . enable_truncation ( max_length = maxlen ) tokenizer . enable_padding ( max_length = maxlen ) all_ids = [ ] for i in tqdm ( range ( 0 , len ( texts ) , chunk_size ) ) : text_chunk = texts [ i : i + chunk_size ] . tolist ( ) encs = tokenizer . encode_batch ( text_chunk ) all_ids . extend ( [ enc . ids for enc in encs ] ) return np . array ( all_ids )
954	tokenizer = transformers . DistilBertTokenizer . from_pretrained ( 'distilbert-base-multilingual-cased' ) tokenizer . save_pretrained ( '.' ) fast_tokenizer = BertWordPieceTokenizer ( 'vocab.txt' , lowercase = False ) fast_tokenizer
955	with strategy . scope ( ) : transformer_layer = ( transformers . TFDistilBertModel . from_pretrained ( 'distilbert-base-multilingual-cased' ) ) model = build_model ( transformer_layer , max_len = MAX_LEN ) model . summary ( )
956	import numpy as np import pandas as pd import os import matplotlib . pyplot as plt import seaborn as sns import PIL from IPython . display import Image , display from plotly import graph_objs as go import plotly . express as px import plotly . figure_factory as ff import openslide
957	print ( "unique ids : " , len ( train . image_id . unique ( ) ) ) print ( "unique data provider : " , len ( train . data_provider . unique ( ) ) ) print ( "unique isup_grade(target) : " , len ( train . isup_grade . unique ( ) ) ) print ( "unique gleason_score : " , len ( train . gleason_score . unique ( ) ) )
958	print ( train [ ( train [ 'gleason_score' ] == '3+4' ) | ( train [ 'gleason_score' ] == '4+3' ) ] [ 'isup_grade' ] . unique ( ) ) print ( train [ ( train [ 'gleason_score' ] == '3+5' ) | ( train [ 'gleason_score' ] == '5+3' ) ] [ 'isup_grade' ] . unique ( ) ) print ( train [ ( train [ 'gleason_score' ] == '5+4' ) | ( train [ 'gleason_score' ] == '4+5' ) ] [ 'isup_grade' ] . unique ( ) )
959	print ( "shape : " , test . shape ) print ( "unique ids : " , len ( test . image_id . unique ( ) ) ) print ( "unique data provider : " , len ( test . data_provider . unique ( ) ) )
960	fig = plt . figure ( figsize = ( 10 , 6 ) ) ax = sns . countplot ( x = "isup_grade" , hue = "data_provider" , data = train ) for p in ax . patches : height = p . get_height ( ) ax . text ( p . get_x ( ) + p . get_width ( ) / 2 , height + 3 , '{:1.2f}%' . format ( 100 * height / 10616 ) , ha = "center" )
961	fig = plt . figure ( figsize = ( 10 , 6 ) ) ax = sns . countplot ( x = "gleason_score" , hue = "data_provider" , data = train ) for p in ax . patches : height = p . get_height ( ) ax . text ( p . get_x ( ) + p . get_width ( ) / 2. , height + 3 , '{:1.2f}%' . format ( 100 * height / 10616 ) , ha = "center" )
962	example = openslide . OpenSlide ( os . path . join ( BASE_FOLDER + "train_images" , '005e66f06bce9c2e49142536caf2f6ee.tiff' ) ) patch = example . read_region ( ( 17800 , 19500 ) , 0 , ( 256 , 256 ) ) display ( patch ) example . close ( )
963	images1 = [ '08ab45297bfe652cc0397f4b37719ba1' , '090a77c517a7a2caa23e443a77a78bc7' ] for image in images1 : mask_img ( image )
964	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
965	import os import time import torch import torch . nn as nn import torch . nn . functional as F from sklearn import metrics from sklearn . model_selection import train_test_split import transformers from transformers import AdamW import torch_xla . core . xla_model as xm import torch_xla . distributed . parallel_loader as pl import torch_xla . distributed . xla_multiprocessing as xmp import warnings warnings . filterwarnings ( "ignore" )
966	t0 = torch . randn ( 2 , 2 , device = xm . xla_device ( ) ) t1 = torch . randn ( 2 , 2 , device = xm . xla_device ( ) ) print ( t0 + t1 ) print ( t0 . mm ( t1 ) )
967	l_in = torch . randn ( 10 , device = xm . xla_device ( ) ) linear = torch . nn . Linear ( 10 , 20 ) . to ( xm . xla_device ( ) ) l_out = linear ( l_in ) print ( l_out )
968	class config : MAX_LEN = 224 TRAIN_BATCH_SIZE = 32 VALID_BATCH_SIZE = 8 EPOCHS = 1 MODEL_PATH = "model.bin" TRAINING_FILE = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv' TOKENIZER = transformers . BertTokenizer . from_pretrained ( 'bert-base-uncased' , do_lower_case = True )
969	import keras from keras . models import Sequential from keras . layers import Dense , Dropout , Flatten from keras . layers import Conv2D , MaxPooling2D from keras . layers . normalization import BatchNormalization
970	BATCH_SIZE = 1024 EPOCHS = 150 LR = 0.02 seed = 2020 patience = 50 device = torch . device ( 'cuda' ) FOLDS = 5
971	img1 = sk . imread ( "/kaggle/input/png-for-steg/bald-eagle-png-transparent-image-pngpix-7.png" ) img2 = sk . imread ( "/kaggle/working/encoded.png" ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 18 , 8 ) ) ax [ 0 ] . imshow ( img1 ) ax [ 1 ] . imshow ( img2 )
972	BASE_PATH = "/kaggle/input/alaska2-image-steganalysis" train_imageids = pd . Series ( os . listdir ( BASE_PATH + '/Cover' ) ) . sort_values ( ascending = True ) . reset_index ( drop = True ) test_imageids = pd . Series ( os . listdir ( BASE_PATH + '/Test' ) ) . sort_values ( ascending = True ) . reset_index ( drop = True )
973	fig , ax = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 30 , 15 ) ) k = 0 for i , row in enumerate ( ax ) : for j , col in enumerate ( row ) : img = sk . imread ( cover_images_path [ k ] ) col . imshow ( img ) col . set_title ( cover_images_path [ k ] ) k = k + 1 plt . suptitle ( 'Samples from Cover Images' , fontsize = 14 ) plt . show ( )
974	coverDCT = np . zeros ( [ 512 , 512 , 3 ] ) stegoDCT = np . zeros ( [ 512 , 512 , 3 ] ) jpeg = jio . read ( cover_images_path [ 0 ] ) stego_juni = jio . read ( JUNIWARD_images_path [ 0 ] )
975	fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 12 ) ) ax [ 0 ] . imshow ( abs ( DCT_diff ) ) ax [ 1 ] . imshow ( abs ( pixelsDiff ) )
976	keywords = [ 'american' , 'greek' , 'filipino' , 'indian' , 'jamaican' , 'spanish' , 'italian' , 'mexican' , 'chinese' , 'thai' , 'vietnamese' , 'cajun' , 'creole' , 'french' , 'japanese' , 'irish' , 'korean' , 'moroccan' , 'russian' , ] d = { } for k in keywords : temp = [ ingredient for ingredient in raw_ingredients if k in ingredient ] d [ k ] = temp
977	for ingredient , expected in [ ( 'Eggs' , 'egg' ) , ( 'all-purpose flour' , 'all purpose flour' ) , ( 'purée' , 'puree' ) , ( '1% low-fat milk' , 'low fat milk' ) , ( 'half & half' , 'half half' ) , ( 'safetida (powder)' , 'safetida (powder)' ) ] : actual = preprocess ( [ ingredient ] ) assert actual == expected , f'"{expected}" is excpected but got "{actual}"'
978	classifier = SVC ( C = 100 , kernel = 'rbf' , degree = 3 , gamma = 1 , coef0 = 1 , shrinking = True , tol = 0.001 , probability = False , cache_size = 200 , class_weight = None , verbose = False , max_iter = - 1 , decision_function_shape = None , random_state = None )
979	merge = gp . merge ( gk , on = [ feature ] , how = 'left' ) sns . lmplot ( x = "mean_download_delay_time" , y = "download_rate" , data = merge ) plt . title ( 'Download-rate vs. Download_delay_time' ) plt . ylim ( 0 , 1 ) plt . xlim ( 0 , 24 )
980	train = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/train.csv' ) test = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/test.csv' ) ss = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/sample_submission.csv' )
981	fig = go . Figure ( go . Funnelarea ( text = temp . sentiment , values = temp . text , title = { "position" : "top center" , "text" : "Funnel-Chart of Sentiment Distribution" } ) ) fig . show ( )
982	def jaccard ( str1 , str2 ) : a = set ( str1 . lower ( ) . split ( ) ) b = set ( str2 . lower ( ) . split ( ) ) c = a . intersection ( b ) return float ( len ( c ) ) / ( len ( a ) + len ( b ) - len ( c ) )
983	hist_data = [ train [ 'Num_words_ST' ] , train [ 'Num_word_text' ] ] group_labels = [ 'Selected_Text' , 'Text' ] fig = ff . create_distplot ( hist_data , group_labels , show_curve = False ) fig . update_layout ( title_text = 'Distribution of Number Of words' ) fig . update_layout ( autosize = False , width = 900 , height = 700 , paper_bgcolor = "LightSteelBlue" , ) fig . show ( )
984	plt . figure ( figsize = ( 12 , 6 ) ) p1 = sns . kdeplot ( train [ 'Num_words_ST' ] , shade = True , color = "r" ) . set_title ( 'Kernel Distribution of Number Of words' ) p1 = sns . kdeplot ( train [ 'Num_word_text' ] , shade = True , color = "b" )
985	plt . figure ( figsize = ( 12 , 6 ) ) p1 = sns . kdeplot ( train [ train [ 'sentiment' ] == 'positive' ] [ 'difference_in_words' ] , shade = True , color = "b" ) . set_title ( 'Kernel Distribution of Difference in Number Of words' ) p2 = sns . kdeplot ( train [ train [ 'sentiment' ] == 'negative' ] [ 'difference_in_words' ] , shade = True , color = "r" )
986	plt . figure ( figsize = ( 12 , 6 ) ) p1 = sns . kdeplot ( train [ train [ 'sentiment' ] == 'positive' ] [ 'jaccard_score' ] , shade = True , color = "b" ) . set_title ( 'KDE of Jaccard Scores across different Sentiments' ) p2 = sns . kdeplot ( train [ train [ 'sentiment' ] == 'negative' ] [ 'jaccard_score' ] , shade = True , color = "r" ) plt . legend ( labels = [ 'positive' , 'negative' ] )
987	def clean_text ( text ) : text = str ( text ) . lower ( ) text = re . sub ( '\[.*?\]' , '' , text ) text = re . sub ( 'https?://\S+|www\.\S+' , '' , text ) text = re . sub ( '<.*?>+' , '' , text ) text = re . sub ( '[%s]' % re . escape ( string . punctuation ) , '' , text ) text = re . sub ( '\n' , '' , text ) text = re . sub ( '\w*\d\w*' , '' , text ) return text
988	train [ 'temp_list' ] = train [ 'selected_text' ] . apply ( lambda x : str ( x ) . split ( ) ) top = Counter ( [ item for sublist in train [ 'temp_list' ] for item in sublist ] ) temp = pd . DataFrame ( top . most_common ( 20 ) ) temp . columns = [ 'Common_words' , 'count' ] temp . style . background_gradient ( cmap = 'Blues' )
989	def remove_stopword ( x ) : return [ y for y in x if y not in stopwords . words ( 'english' ) ] train [ 'temp_list' ] = train [ 'temp_list' ] . apply ( lambda x : remove_stopword ( x ) )
990	fig = px . bar ( temp , x = "count" , y = "Common_words" , title = 'Commmon Words in Text' , orientation = 'h' , width = 700 , height = 700 , color = 'Common_words' ) fig . show ( )
991	Positive_sent = train [ train [ 'sentiment' ] == 'positive' ] Negative_sent = train [ train [ 'sentiment' ] == 'negative' ] Neutral_sent = train [ train [ 'sentiment' ] == 'neutral' ]
992	df_train = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/train.csv' ) df_test = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/test.csv' ) df_submission = pd . read_csv ( '/kaggle/input/tweet-sentiment-extraction/sample_submission.csv' )
993	def save_model ( output_dir , nlp , new_model_name ) : output_dir = f'../working/{output_dir}' if output_dir is not None : if not os . path . exists ( output_dir ) : os . makedirs ( output_dir ) nlp . meta [ "name" ] = new_model_name nlp . to_disk ( output_dir ) print ( "Saved model to" , output_dir )
994	sentiment = 'positive' train_data = get_training_data ( sentiment ) model_path = get_model_out_path ( sentiment ) train ( train_data , model_path , n_iter = 3 , model = None )
995	train = pd . read_csv ( '../input/train.csv' , index_col = 'plaintext_id' ) test = pd . read_csv ( '../input/test.csv' , index_col = 'ciphertext_id' ) sub = pd . read_csv ( '../input/sample_submission.csv' , index_col = 'ciphertext_id' )
996	plain_dict = { } for p_id , row in train . iterrows ( ) : text = row [ 'text' ] plain_dict [ text ] = p_id print ( len ( plain_dict ) )
997	level12_train_index = list ( sub [ sub [ "index" ] > 0 ] [ "index" ] ) print ( len ( level12_train_index ) ) train34 = train [ ~ train [ "index" ] . isin ( level12_train_index ) ] . copy ( ) test3 = test [ test [ 'difficulty' ] == 3 ] . copy ( ) test4 = test [ test [ 'difficulty' ] == 4 ] . copy ( ) print ( train34 . shape , test3 . shape [ 0 ] + test4 . shape [ 0 ] )
998	c_id = 'ID_f0989e1c5' index = 34509 sub . loc [ c_id ] = index
999	c_id = 'ID_0414884b0' index = 42677 sub . loc [ c_id ] = index
1000	dict_level3 = { } for ciphertext_id , row in test3 [ test3 [ "nb" ] >= 200 ] . iterrows ( ) : ct = row [ "ciphertext" ] index , key_index = find_mapping ( ciphertext_id , ct , train34 ) if index > 0 : print ( ciphertext_id , index , key_index , "(length: {})" . format ( row [ "nb" ] ) ) dict_level3 [ ciphertext_id ] = ( index , key_index )
1001	from collections import Counter import matplotlib . pyplot as plt plt . rcParams [ "figure.figsize" ] = ( 20 , 10 )
1002	fullcipher3 = " " . join ( ( test3 [ "ciphertext" ] . values ) ) dict_fullcipher3 = Counter ( fullcipher3 . split ( " " ) ) df_fullcipher3 = pd . DataFrame . from_dict ( dict_fullcipher3 , orient = 'index' ) df_fullcipher3 = df_fullcipher3 . reset_index ( ) df_fullcipher3 . columns = [ "num" , "nb" ] df_fullcipher3 . sort_values ( "nb" , ascending = False , inplace = True ) print ( df_fullcipher3 . shape ) df_fullcipher3 . head ( )
1003	for df in df_fragments ( '../input/train.csv' , 150000 ) : mfcc = librosa . feature . mfcc ( df [ 'acoustic_data' ] . values . astype ( 'float32' ) ) plt . figure ( figsize = ( 25 , 5 ) ) librosa . display . specshow ( mfcc , x_axis = 'time' ) plt . colorbar ( ) break
1004	print ( 'counting total...' ) total = count_rows ( '../input/train.csv' ) print ( 'total: {}' . format ( total ) )
1005	fig = plt . figure ( figsize = [ 20 , 5 ] ) ax1 = fig . add_subplot ( 111 ) ax2 = ax1 . twinx ( ) mfcc_ttf_df [ 'time_to_failure' ] . plot ( ax = ax1 , y = 'time_to_failure' , legend = True , color = 'black' ) ax1 . legend ( loc = 'upper left' ) mfcc_ttf_df . drop ( [ 'time_to_failure' ] , axis = 1 ) . plot ( ax = ax2 , legend = True ) plt . show ( )
1006	from sklearn . linear_model import LinearRegression from sklearn . model_selection import cross_val_score def report_cv ( model ) : X = mfcc_ttf_df . drop ( [ 'time_to_failure' ] , axis = 1 ) . values y = mfcc_ttf_df [ 'time_to_failure' ] . values scores = cross_val_score ( model , X , y , scoring = 'neg_mean_absolute_error' , cv = 10 ) print ( 'Cross Validation scores: {}' . format ( abs ( scores ) ) ) print ( 'Average score: {}' . format ( abs ( scores . mean ( ) ) ) ) report_cv ( LinearRegression ( ) )
1007	from xgboost import XGBRegressor report_cv ( XGBRegressor ( random_state = random_seed ) )
1008	import os import gc import numpy as np import pandas as pd import seaborn as sns import matplotlib . pyplot as plt import warnings warnings . filterwarnings ( 'ignore' )
1009	plt . figure ( figsize = ( 26 , 24 ) ) for i , col in enumerate ( typelist ) : plt . subplot ( 4 , 2 , i + 1 ) sns . distplot ( train [ train [ 'type' ] == col ] [ 'scalar_coupling_constant' ] , color = 'indigo' ) plt . title ( col )
1010	sns . distplot ( dipole_moments . X , color = 'mediumseagreen' ) plt . title ( 'Dipole moment along X-axis' ) plt . show ( ) sns . distplot ( dipole_moments . Y , color = 'seagreen' ) plt . title ( 'Dipole moment along Y-axis' ) plt . show ( ) sns . distplot ( dipole_moments . Z , color = 'green' ) plt . title ( 'Dipole moment along Z-axis' ) plt . show ( )
1011	plt . figure ( figsize = ( 26 , 24 ) ) for i , col in enumerate ( typelist ) : plt . subplot ( 4 , 2 , i + 1 ) sns . distplot ( potential_energy [ train [ 'type' ] == col ] [ 'potential_energy' ] , color = 'orangered' ) plt . title ( col )
1012	def is_outlier ( points , thresh = 3.5 ) : if len ( points . shape ) == 1 : points = points [ : , None ] median = np . median ( points , axis = 0 ) diff = np . sum ( ( points - median ) ** 2 , axis = - 1 ) diff = np . sqrt ( diff ) med_abs_deviation = np . median ( diff ) modified_z_score = 0.6745 * diff / med_abs_deviation return modified_z_score > thresh
1013	import os import gc import cv2 import json import time import numpy as np import pandas as pd from pathlib import Path from keras . utils import to_categorical import seaborn as sns import plotly . express as px from matplotlib import colors import matplotlib . pyplot as plt import plotly . figure_factory as ff import torch T = torch . Tensor import torch . nn as nn from torch . optim import Adam from torch . utils . data import Dataset , DataLoader
1014	test_task_files = sorted ( os . listdir ( TEST_PATH ) ) test_tasks = [ ] for task_file in test_task_files : with open ( str ( TEST_PATH / task_file ) , 'r' ) as f : task = json . load ( f ) test_tasks . append ( task )
1015	Xs_test , Xs_train , ys_train = [ ] , [ ] , [ ] for task in test_tasks : X_test , X_train , y_train = [ ] , [ ] , [ ] for pair in task [ "test" ] : X_test . append ( pair [ "input" ] ) for pair in task [ "train" ] : X_train . append ( pair [ "input" ] ) y_train . append ( pair [ "output" ] ) Xs_test . append ( X_test ) Xs_train . append ( X_train ) ys_train . append ( y_train )
1016	means = [ np . mean ( X ) for X in matrices ] fig = ff . create_distplot ( [ means ] , group_labels = [ "Means" ] , colors = [ "green" ] ) fig . update_layout ( title_text = "Distribution of matrix mean values" )
1017	plot = sns . jointplot ( widths , heights , kind = "kde" , color = "blueviolet" ) plot . set_axis_labels ( "Width" , "Height" , fontsize = 14 ) plt . show ( plot )
1018	def flattener ( pred ) : str_pred = str ( [ row for row in pred ] ) str_pred = str_pred . replace ( ', ' , '' ) str_pred = str_pred . replace ( '[[' , '|' ) str_pred = str_pred . replace ( '][' , '|' ) str_pred = str_pred . replace ( ']]' , '|' ) return str_pred
1019	test_predictions = [ [ list ( pred ) for pred in test_pred ] for test_pred in test_predictions ] for idx , pred in enumerate ( test_predictions ) : test_predictions [ idx ] = flattener ( pred ) submission = pd . read_csv ( SUBMISSION_PATH ) submission [ "output" ] = test_predictions
1020	import os import gc import numpy as np import pandas as pd from tqdm import tqdm_notebook as tqdm import seaborn as sns from collections import Counter import matplotlib . pyplot as plt from IPython . display import SVG import warnings warnings . filterwarnings ( 'ignore' ) import lightgbm import xgboost import catboost import keras from keras . models import Model from keras . utils . vis_utils import model_to_dot from keras . layers import Input , Dense , Dropout , BatchNormalization from sklearn . preprocessing import MinMaxScaler
1021	DATA_PATH = '../input/ieee-fraud-detection/' TRAIN_PATH = DATA_PATH + 'train_transaction.csv' TEST_PATH = DATA_PATH + 'test_transaction.csv'
1022	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "ProductCD" , data = train_df , palette = reversed ( [ 'aquamarine' , 'mediumaquamarine' , 'mediumseagreen' , 'seagreen' , 'darkgreen' ] ) ) . set_title ( 'ProductCD' , fontsize = 16 ) plt . show ( plot )
1023	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "TransactionAmt < 500" ) \ . groupby ( "ProductCD" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'lightgreen' , 'green' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
1024	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . violinplot ( x = "ProductCD" , y = "TransactionAmt" , hue = "isFraud" , data = train_df . query ( 'TransactionAmt < 500' ) , palette = [ 'lightgreen' , 'green' ] , split = True , ax = ax ) . set_title ( 'ProductCD vs. TransactionAmt' , fontsize = 16 ) plt . show ( plot )
1025	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . boxplot ( x = "ProductCD" , y = "TransactionAmt" , hue = "isFraud" , data = train_df . query ( 'TransactionAmt < 500' ) , palette = [ 'lightgreen' , 'green' ] , ax = ax ) . set_title ( 'ProductCD vs. TransactionAmt' , fontsize = 16 ) plt . show ( plot )
1026	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "P_emaildomain" , data = train_df . query ( "P_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']" ) . query ( "TransactionAmt < 500" ) , palette = [ 'navy' , 'darkblue' , 'blue' , 'dodgerblue' , 'skyblue' ] ) . set_title ( 'P_emaildomain vs. TransactionAmt' , fontsize = 16 ) plt . show ( plot )
1027	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "P_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']" ) . query ( "TransactionAmt < 500" ) \ . groupby ( "P_emaildomain" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'lightblue' , 'darkblue' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
1028	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . violinplot ( x = "P_emaildomain" , y = "TransactionAmt" , hue = "isFraud" , data = train_df . query ( "P_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']" ) . query ( "TransactionAmt < 500" ) , palette = [ 'lightblue' , 'darkblue' ] , split = True , ax = ax ) . set_title ( 'TransactionAmt vs. P_emaildomain' , fontsize = 16 ) plt . show ( plot )
1029	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . boxplot ( x = "P_emaildomain" , y = "TransactionAmt" , hue = "isFraud" , data = train_df . query ( "P_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']" ) . query ( "TransactionAmt < 500" ) , palette = [ 'lightblue' , 'darkblue' ] , ax = ax ) . set_title ( 'TransactionAmt vs. P_emaildomain' , fontsize = 16 ) plt . show ( plot )
1030	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "R_emaildomain" , data = train_df . query ( "R_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']" ) . query ( "TransactionAmt < 500" ) , palette = [ 'red' , 'crimson' , 'mediumvioletred' , 'darkmagenta' , 'indigo' ] ) . set_title ( 'R_emaildomain' , fontsize = 16 ) plt . show ( plot )
1031	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "R_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']" ) . query ( "TransactionAmt < 500" ) \ . groupby ( "R_emaildomain" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'pink' , 'crimson' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
1032	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . violinplot ( x = "R_emaildomain" , y = "TransactionAmt" , hue = "isFraud" , data = train_df . query ( "R_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']" ) . query ( "TransactionAmt < 500" ) , palette = [ 'pink' , 'crimson' ] , split = True , ax = ax ) . set_title ( 'TransactionAmt vs. R_emaildomain' , fontsize = 16 ) plt . show ( plot )
1033	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . boxplot ( x = "R_emaildomain" , y = "TransactionAmt" , hue = "isFraud" , data = train_df . query ( "R_emaildomain in ['gmail.com', 'yahoo.com', 0.0, 'hotmail.com', 'anonymous.com']" ) . query ( "TransactionAmt < 500" ) , palette = [ 'pink' , 'crimson' ] , ax = ax ) . set_title ( 'TransactionAmt vs. R_emaildomain' , fontsize = 16 ) plt . show ( plot )
1034	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "card4" , data = train_df . query ( "TransactionAmt < 500" ) , palette = reversed ( [ 'orangered' , 'darkorange' , 'orange' , 'peachpuff' , 'navajowhite' ] ) ) . set_title ( 'card4' , fontsize = 16 ) plt . show ( plot )
1035	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "TransactionAmt < 500" ) \ . groupby ( "card4" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'peachpuff' , 'darkorange' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
1036	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . violinplot ( x = "card4" , y = "TransactionAmt" , hue = "isFraud" , data = train_df . query ( "TransactionAmt < 500" ) , palette = [ 'peachpuff' , 'darkorange' ] , split = True , ax = ax ) . set_title ( 'TransactionAmt vs. card4' , fontsize = 16 ) plt . show ( plot )
1037	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . boxplot ( x = "card4" , y = "TransactionAmt" , hue = "isFraud" , data = train_df . query ( "TransactionAmt < 500" ) , palette = [ 'peachpuff' , 'darkorange' ] , ax = ax ) . set_title ( 'TransactionAmt vs. card4' , fontsize = 16 ) plt . show ( plot )
1038	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "card6" , data = train_df . query ( "TransactionAmt < 500" ) . query ( "card6 == 'credit' or card6 == 'debit'" ) , palette = reversed ( [ 'red' , 'crimson' , 'mediumvioletred' , 'darkmagenta' , 'indigo' ] ) ) . set_title ( 'card6' , fontsize = 16 ) plt . show ( plot )
1039	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) props = train_df . query ( "TransactionAmt < 500" ) . query ( "card6 == 'credit' or card6 == 'debit'" ) \ . groupby ( "card6" ) [ 'isFraud' ] . value_counts ( normalize = True ) . unstack ( ) sns . set_palette ( [ 'plum' , 'purple' ] ) props . plot ( kind = 'bar' , stacked = 'True' , ax = ax ) . set_ylabel ( 'Proportion' ) plt . show ( plot )
1040	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . violinplot ( x = "card6" , y = "TransactionAmt" , hue = "isFraud" , data = train_df . query ( "TransactionAmt < 500" ) , palette = [ 'plum' , 'purple' ] , split = True , ax = ax ) . set_title ( 'TransactionAmt vs. card6' , fontsize = 16 ) plt . show ( plot )
1041	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . boxplot ( x = "card6" , y = "TransactionAmt" , hue = "isFraud" , data = train_df . query ( "TransactionAmt < 500" ) , palette = [ 'plum' , 'purple' ] , ax = ax ) . set_title ( 'TransactionAmt vs. card6' , fontsize = 16 ) plt . show ( plot )
1042	def prepare_data ( df , cat_cols = cat_cols ) : cat_cols = [ col for col in cat_cols if col in df . columns ] for col in tqdm ( cat_cols ) : \ df [ col ] = pd . factorize ( df [ col ] ) [ 0 ] return df
1043	X = train_data . sort_values ( 'TransactionDT' ) . drop ( [ 'isFraud' , 'TransactionDT' , 'TransactionID' ] , axis = 1 ) y = train_data . sort_values ( 'TransactionDT' ) [ 'isFraud' ] del train_data
1044	parameters = { 'application' : 'binary' , 'objective' : 'binary' , 'metric' : 'auc' , 'is_unbalance' : 'true' , 'boosting' : 'gbdt' , 'num_leaves' : 31 , 'feature_fraction' : 0.5 , 'bagging_fraction' : 0.5 , 'bagging_freq' : 20 , 'learning_rate' : 0.05 , 'verbose' : 0 } train_data = lightgbm . Dataset ( X_train , label = y_train , categorical_feature = cat_cols ) val_data = lightgbm . Dataset ( X_val , label = y_val ) model = lightgbm . train ( parameters , train_data , valid_sets = val_data , num_boost_round = 5000 , early_stopping_rounds = 100 )
1045	plt . rcParams [ "axes.titlesize" ] = 16 plt . rcParams [ "axes.labelsize" ] = 15 plt . rcParams [ "xtick.labelsize" ] = 13 plt . rcParams [ "ytick.labelsize" ] = 13 plot = lightgbm . plot_importance ( model , max_num_features = 10 , figsize = ( 20 , 20 ) , grid = False , color = sns . color_palette ( "husl" , 20 ) ) plt . show ( plot )
1046	def get_neural_network ( ) : inputs = Input ( shape = ( X . shape [ 1 ] , ) ) dense_1 = Dense ( 10 , activation = 'relu' ) ( inputs ) dense_2 = Dense ( 10 , activation = 'relu' ) ( dense_1 ) outputs = Dense ( 1 , activation = 'sigmoid' ) ( dense_2 ) model = Model ( inputs = inputs , outputs = outputs ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ] ) return model
1047	fig , ax = plt . subplots ( figsize = ( 7 , 7 ) ) plt . plot ( history . history [ 'acc' ] , color = 'blue' ) plt . plot ( history . history [ 'val_acc' ] , color = 'orangered' ) plt . title ( 'Model accuracy' ) plt . ylabel ( 'Accuracy' ) plt . xlabel ( 'Epoch' ) plt . legend ( [ 'Train' , 'Validation' ] , loc = 'upper left' ) plt . show ( )
1048	fig , ax = plt . subplots ( figsize = ( 7 , 7 ) ) plt . plot ( history . history [ 'loss' ] , color = 'blue' ) plt . plot ( history . history [ 'val_loss' ] , color = 'orangered' ) plt . title ( 'Model loss' ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'Epoch' ) plt . legend ( [ 'Train' , 'Validation' ] , loc = 'upper left' ) plt . show ( )
1049	H = 512 W = 512 VF = 0.5 HF = 0.5 DELAY = 30 FRAC = 0.1 DROP = 0.225 FOLDS = 8 EPOCHS = 3 LR = 1e-3 , 1e-3 BATCH_SIZE = 32 VAL_BATCH_SIZE = 32 MODEL_NAME = 'efficientnet_b3' MODEL = 'rwightman/gen-efficientnet-pytorch'
1050	kfolds = KFold ( n_splits = FOLDS ) image_id = os . listdir ( COVER_PATH ) split_indices = kfolds . split ( image_id ) val_ids , train_ids = [ ] , [ ] for index in split_indices : val_ids . append ( np . array ( image_id ) [ index [ 1 ] ] ) train_ids . append ( np . array ( image_id ) [ index [ 0 ] ] )
1051	def bce ( inp , targ ) : return nn . BCELoss ( ) ( nn . Sigmoid ( ) ( inp ) , targ ) def acc ( inp , targ ) : targ_idx = targ . squeeze ( ) inp_idx = torch . round ( nn . Sigmoid ( ) ( inp ) ) . squeeze ( ) return ( inp_idx == targ_idx ) . float ( ) . sum ( axis = 0 ) / len ( inp_idx )
1052	TEXT_COL = 'comment_text' EMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec' MAXLEN = 128 ENDLEN = 32 MAX_FEATURES = 100000 EMBED_SIZE = 300 BATCH_SIZE = 2048 NUM_EPOCHS = 100
1053	lengths = train_df [ TEXT_COL ] . apply ( len ) train_df [ 'lengths' ] = lengths lengths = train_df . loc [ train_df [ 'lengths' ] < 1125 ] [ 'lengths' ] sns . distplot ( lengths , color = 'r' ) plt . show ( )
1054	words = train_df [ TEXT_COL ] . apply ( lambda x : len ( x ) - len ( '' . join ( x . split ( ) ) ) + 1 ) train_df [ 'words' ] = words words = train_df . loc [ train_df [ 'words' ] < 200 ] [ 'words' ] sns . distplot ( words , color = 'g' ) plt . show ( )
1055	avg_word_len = train_df [ TEXT_COL ] . apply ( lambda x : 1.0 * len ( '' . join ( x . split ( ) ) ) / ( len ( x ) - len ( '' . join ( x . split ( ) ) ) + 1 ) ) train_df [ 'avg_word_len' ] = avg_word_len avg_word_len = train_df . loc [ train_df [ 'avg_word_len' ] < 10 ] [ 'avg_word_len' ] sns . distplot ( avg_word_len , color = 'b' ) plt . show ( )
1056	tokenizer = Tokenizer ( num_words = MAX_FEATURES , lower = True ) tokenizer . fit_on_texts ( list ( train_df [ TEXT_COL ] ) + list ( test_df [ TEXT_COL ] ) ) word_index = tokenizer . word_index
1057	def squash ( x , axis = - 1 ) : s_squared_norm = K . sum ( K . square ( x ) , axis , keepdims = True ) + K . epsilon ( ) scale = K . sqrt ( s_squared_norm ) / ( 0.5 + s_squared_norm ) return scale * x
1058	with open ( 'word_index.json' , 'w' ) as f : json . dump ( word_index , f )
1059	import os import gc import pandas as pd import numpy as np from sklearn . metrics import accuracy_score , mean_absolute_error , mean_squared_error import matplotlib . pyplot as plt import seaborn as sns from langdetect import detect import markdown import json import requests import warnings import time from colorama import Fore , Back , Style , init
1060	train_df = pd . read_csv ( '../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv' ) comments = train_df [ 'comment_text' ] targets = train_df [ 'target' ] severe_toxicities = train_df [ 'severe_toxicity' ] obscenities = train_df [ 'obscene' ] del train_df gc . collect ( )
1061	with open ( '../input/google-api-information/Google API Key.txt' ) as f : google_api_key = f . readline ( ) [ : - 1 ] client = Perspective ( google_api_key )
1062	print ( "Toxicity Mean Absolute Error : " + \ str ( mean_absolute_error ( targets [ : len ( toxicity_scores ) ] , toxicity_scores [ : len ( toxicity_scores ) ] ) ) ) print ( "Obscneity Mean Absolute Error : " + \ str ( mean_absolute_error ( obscenities [ : len ( toxicity_scores ) ] , obscenity_scores [ : len ( toxicity_scores ) ] ) ) ) print ( "Severe Toxicity Mean Absolute Error : " + \ str ( mean_absolute_error ( severe_toxicities [ : len ( toxicity_scores ) ] , severe_toxicity_scores [ : len ( toxicity_scores ) ] ) ) )
1063	print ( "Toxicity Squared Absolute Error : " + \ str ( mean_squared_error ( targets [ : len ( toxicity_scores ) ] , toxicity_scores [ : len ( toxicity_scores ) ] ) ) ) print ( "Obscneity Squared Absolute Error : " + \ str ( mean_squared_error ( obscenities [ : len ( toxicity_scores ) ] , obscenity_scores [ : len ( toxicity_scores ) ] ) ) ) print ( "Severe Toxicity Squared Absolute Error : " + \ str ( mean_squared_error ( severe_toxicities [ : len ( toxicity_scores ) ] , severe_toxicity_scores [ : len ( toxicity_scores ) ] ) ) )
1064	F = 512 DROP = 0.2 EPOCHS = 100 LR = 1e-3 , 1e-2 VAL_BATCH_SIZE = 64 TRAIN_BATCH_SIZE = 64 MAX_OUTPUTS = 3 THRESHOLD = 6e-3 MIN_THRESHOLD = 5e-3 SR = 44100 CHUNKS = 1 TSR = 32000 SPLIT = 0.8 N_MELS = 256 MEL_LEN = 313 POP_FRAC = 0.25 MAXLEN = 2000000 AMPLITUDE = 1000 CHUNK_SIZE = 160000
1065	TEST_DATA_PATH = '../input/birdsong-recognition/test.csv' TRAIN_DATA_PATH = '../input/birdsong-recognition/train.csv' TEST_AUDIO_PATH = '../input/birdsong-recognition/test_audio/' TRAIN_AUDIO_PATH = '../input/birdsong-recognition/train_audio/' CHECKING_PATH = '../input/prepare-check-dataset/birdcall-check/' IMG_PATHS = [ 'train_1' , 'train_2' , 'train_3' , 'train_4' , 'train_5' ]
1066	keys = set ( train_df . ebird_code ) values = np . arange ( 0 , len ( keys ) ) code_dict = dict ( zip ( sorted ( keys ) , values ) )
1067	def get_idx ( length ) : length = get_len ( length ) max_idx = MAXLEN - CHUNK_SIZE idx = np . random . randint ( length + 1 ) chunk_range = idx , idx + CHUNK_SIZE chunk_idx = max ( [ 0 , chunk_range [ 0 ] ] ) chunk_idx = min ( [ chunk_range [ 1 ] , max_idx ] ) return ( chunk_idx , chunk_idx + CHUNK_SIZE ) def get_len ( length ) : if length > MAXLEN : return MAXLEN if length <= MAXLEN : return int ( length * POP_FRAC )
1068	train_df = shuffle ( train_df ) split = int ( SPLIT * len ( train_df ) ) train_df = train_df . reset_index ( drop = True ) valid_df = train_df [ split : ] . reset_index ( drop = True ) train_df = train_df [ : split ] . reset_index ( drop = True ) train_set = MelDataset ( train_df ) valid_set = MelDataset ( valid_df ) valid_loader = tqdm ( DataLoader ( valid_set , batch_size = VAL_BATCH_SIZE ) ) train_loader = DataLoader ( train_set , batch_size = TRAIN_BATCH_SIZE , shuffle = True )
1069	O = len ( code_dict ) network = BirdNet ( f = F , o = O ) optimizer = Adam ( [ { 'params' : network . resnet . parameters ( ) , 'lr' : LR [ 0 ] } , { 'params' : network . dense_output . parameters ( ) , 'lr' : LR [ 1 ] } ] )
1070	def cel ( y_true , y_pred ) : y_true = torch . argmax ( y_true , axis = - 1 ) return nn . CrossEntropyLoss ( ) ( y_pred , y_true . squeeze ( ) ) def accuracy ( y_true , y_pred ) : y_true = torch . argmax ( y_true , axis = - 1 ) . squeeze ( ) y_pred = torch . argmax ( y_pred , axis = - 1 ) . squeeze ( ) return ( y_true == y_pred ) . float ( ) . sum ( ) / len ( y_true )
1071	network . eval ( ) test_preds = [ ] test_set = BirdTestDataset ( test_df , TEST_AUDIO_PATH ) test_loader = DataLoader ( test_set , batch_size = VAL_BATCH_SIZE ) if os . path . exists ( TEST_AUDIO_PATH ) : for test_X in tqdm ( test_loader ) : test_pred = network . forward ( test_X . view ( - 1 , * D ) . to ( device ) ) test_preds . extend ( softmax ( test_pred . detach ( ) . cpu ( ) . numpy ( ) ) . flatten ( ) )
1072	import os import gc import re import numpy as np import pandas as pd import nltk from nltk . corpus import wordnet , stopwords from nltk . stem import WordNetLemmatizer from nltk . stem . porter import PorterStemmer from colorama import Fore , Back , Style
1073	def remove_numbers ( text ) : text = '' . join ( [ i for i in text if not i . isdigit ( ) ] ) return text
1074	def replace_multi_exclamation_mark ( text ) : text = re . sub ( r"(\!)\1+" , ' multiExclamation ' , text ) return text def replace_multi_question_mark ( text ) : text = re . sub ( r"(\?)\1+" , ' multiQuestion ' , text ) return text def replace_multi_stop_mark ( text ) : text = re . sub ( r"(\.)\1+" , ' multiStop ' , text ) return text
1075	stoplist = stopwords . words ( 'english' ) def remove_stop_words ( text ) : finalTokens = [ ] tokens = nltk . word_tokenize ( text ) for w in tokens : if ( w not in stoplist ) : finalTokens . append ( w ) text = " " . join ( finalTokens ) return text
1076	def replace_elongated ( word ) : repeat_regexp = re . compile ( r'(\w*)(\w)\2(\w*)' ) repl = r'\1\2\3' if wordnet . synsets ( word ) : return word repl_word = repeat_regexp . sub ( repl , word ) if repl_word != word : return replace_elongated ( repl_word ) else : return repl_word def replace_elongated_words ( text ) : finalTokens = [ ] tokens = nltk . word_tokenize ( text ) for w in tokens : finalTokens . append ( replace_elongated ( w ) ) text = " " . join ( finalTokens ) return text
1077	stemmer = PorterStemmer ( ) def stem_words ( text ) : finalTokens = [ ] tokens = nltk . word_tokenize ( text ) for w in tokens : finalTokens . append ( stemmer . stem ( w ) ) text = " " . join ( finalTokens ) return text
1078	lemmatizer = WordNetLemmatizer ( ) def lemmatize_words ( text ) : finalTokens = [ ] tokens = nltk . word_tokenize ( text ) for w in tokens : finalTokens . append ( lemmatizer . lemmatize ( w ) ) text = " " . join ( finalTokens ) return text
1079	def get_neural_network ( ) : inputs = Input ( shape = ( X . shape [ 1 ] , ) ) dense_1 = Dense ( 10 , activation = 'relu' ) ( inputs ) dense_2 = Dense ( 10 , activation = 'relu' ) ( dense_1 ) outputs = Dense ( 1 , activation = 'sigmoid' ) ( dense_2 ) model = Model ( inputs = inputs , outputs = outputs ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ] ) return model model = get_neural_network ( )
1080	split = np . int32 ( 0.8 * len ( X ) ) X_train = X [ : split ] y_train = np . int32 ( y ) [ : split ] X_val = X [ split : ] y_val = np . int32 ( y ) [ split : ]
1081	def get_model_one ( ) : inputs = Input ( shape = ( X . shape [ 1 ] , ) ) dense_1 = Dense ( 100 , activation = 'relu' ) ( inputs ) dense_2 = Dense ( 100 , activation = 'relu' ) ( dense_1 ) outputs = Dense ( 1 , activation = 'sigmoid' ) ( dense_2 ) model = Model ( inputs = inputs , outputs = outputs ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ] ) return model model_one = get_model_one ( )
1082	def get_model_two ( ) : inputs = Input ( shape = ( X . shape [ 1 ] , ) ) dense_1 = Dense ( 10 , activation = 'relu' ) ( inputs ) dense_2 = Dense ( 10 , activation = 'relu' ) ( dense_1 ) outputs = Dense ( 1 , activation = 'sigmoid' ) ( dense_2 ) model = Model ( inputs = inputs , outputs = outputs ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ] ) return model model_two = get_model_two ( )
1083	preds_one_val = model_one . predict ( X_val ) preds_two_val = model_two . predict ( X_val ) preds_one_train = model_one . predict ( X_train ) preds_two_train = model_two . predict ( X_train )
1084	def get_model_one ( ) : inputs = Input ( shape = ( X . shape [ 1 ] , ) ) dense_1 = Dense ( 20 , activation = 'relu' ) ( inputs ) dense_2 = Dense ( 20 , activation = 'relu' ) ( dense_1 ) outputs = Dense ( 1 , activation = 'sigmoid' ) ( dense_2 ) model = Model ( inputs = inputs , outputs = outputs ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ] ) return model model_one = get_model_one ( )
1085	def get_model_two ( ) : inputs = Input ( shape = ( X . shape [ 1 ] , ) ) dense_1 = Dense ( 25 , activation = 'relu' ) ( inputs ) dense_2 = Dense ( 25 , activation = 'relu' ) ( dense_1 ) outputs = Dense ( 1 , activation = 'sigmoid' ) ( dense_2 ) model = Model ( inputs = inputs , outputs = outputs ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ] ) return model model_two = get_model_two ( )
1086	print ( "Accuracy of the first model : " + str ( accuracy_score ( y_val , np . round ( preds_one ) ) * 100 ) + " %" ) print ( "Accuracy of the second model : " + str ( accuracy_score ( y_val , np . round ( preds_two ) ) * 100 ) + " %" ) print ( "Accuracy of the average ensemble : " + str ( accuracy_score ( y_val , np . round ( ensemble ) ) * 100 ) + " %" )
1087	acc_1 = accuracy_score ( y_val , np . round ( preds_one ) ) * 100 acc_2 = accuracy_score ( y_val , np . round ( preds_two ) ) * 100 acc_ensemble = accuracy_score ( y_val , np . round ( ensemble ) ) * 100
1088	labels = [ 'Model 1' , 'Model 2' , 'Ensemble' ] fig = go . Figure ( data = [ go . Bar ( x = labels , y = [ acc_1 - 96.9 , acc_2 - 96.9 , acc_ensemble - 96.9 ] , marker = { 'color' : 'crimson' } ) ] ) fig . update_layout ( title = "Accuracy for different models (above 96.9)" , yaxis = dict ( title = "Accuracy (above 96.9)" ) ) fig . show ( )
1089	def get_model_one ( ) : inputs = Input ( shape = ( X . shape [ 1 ] , ) ) dense_1 = Dense ( 200 , activation = 'relu' ) ( inputs ) dense_2 = Dense ( 200 , activation = 'relu' ) ( dense_1 ) outputs = Dense ( 1 , activation = 'sigmoid' ) ( dense_2 ) model = Model ( inputs = inputs , outputs = outputs ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'acc' ] ) return model model_one = get_model_one ( )
1090	preds_one_val = model_one . predict ( X_val ) preds_two_val = model_two . predict ( X_val ) preds_one_train = model_one . predict ( X_train ) preds_two_train = model_two . predict ( X_train )
1091	import os import numpy as np import pandas as pd from tqdm import tqdm tqdm . pandas ( ) from nltk import word_tokenize , pos_tag from collections import Counter import matplotlib . pyplot as plt import seaborn as sns import warnings warnings . filterwarnings ( 'ignore' )
1092	tags = [ ] for i , tag in enumerate ( pos_tags ) : pos_tags [ i ] = list ( map ( list , tag ) ) tags . append ( np . array ( pos_tags [ i ] ) [ : , 1 ] ) flat_tags = np . concatenate ( [ tag for tag in tags ] )
1093	def count_pos ( tag_dict , tag_name ) : if tag_name in tag_dict : return tag_dict [ tag_name ] else : return 0
1094	all_tags = set ( flat_tags ) df = pd . DataFrame ( np . zeros ( ( SAMPLE_SIZE , 3 ) ) ) df . columns = [ 'count_dict' , 'pos_feature' , 'target' ] df . count_dict = counts df . target = targets
1095	visualize_count_feature ( 'VBD' ) visualize_count_feature ( 'VBG' ) visualize_count_feature ( 'VBZ' ) visualize_count_feature ( 'VBP' )
1096	SIGNAL_LEN = 150000 MIN_NUM = - 27 MAX_NUM = 28
1097	acoustic_data = seismic_signals . acoustic_data time_to_failure = seismic_signals . time_to_failure data_len = len ( seismic_signals ) del seismic_signals gc . collect ( )
1098	signals = [ ] targets = [ ] for i in range ( data_len // SIGNAL_LEN ) : min_lim = SIGNAL_LEN * i max_lim = min ( [ SIGNAL_LEN * ( i + 1 ) , data_len ] ) signals . append ( list ( acoustic_data [ min_lim : max_lim ] ) ) targets . append ( time_to_failure [ max_lim ] ) del acoustic_data del time_to_failure gc . collect ( ) signals = np . array ( signals ) targets = np . array ( targets )
1099	def min_max_transfer ( ts , min_value , max_value , range_needed = ( - 1 , 1 ) ) : ts_std = ( ts - min_value ) / ( max_value - min_value ) if range_needed [ 0 ] < 0 : return ts_std * ( range_needed [ 1 ] + abs ( range_needed [ 0 ] ) ) + range_needed [ 0 ] else : return ts_std * ( range_needed [ 1 ] - range_needed [ 0 ] ) + range_needed [ 0 ]
1100	def prepare_data ( start , end ) : train = pd . DataFrame ( np . transpose ( signals [ int ( start ) : int ( end ) ] ) ) X = [ ] for id_measurement in tqdm ( train . index [ int ( start ) : int ( end ) ] ) : X_signal = transform_ts ( train [ id_measurement ] ) X . append ( X_signal ) X = np . asarray ( X ) return X
1101	X = [ ] def load_all ( ) : total_size = len ( signals ) for start , end in [ ( 0 , int ( total_size ) ) ] : X_temp = prepare_data ( start , end ) X . append ( X_temp ) load_all ( ) X = np . concatenate ( X )
1102	plot = sns . jointplot ( x = perm_entropies , y = targets , kind = 'kde' , color = 'orangered' ) plot . set_axis_labels ( 'perm_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1103	plot = sns . jointplot ( x = perm_entropies , y = targets , kind = 'reg' , color = 'orangered' ) plot . set_axis_labels ( 'perm_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1104	plot = sns . jointplot ( x = app_entropies , y = targets , kind = 'kde' , color = 'magenta' ) plot . set_axis_labels ( 'app_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1105	plot = sns . jointplot ( x = app_entropies , y = targets , kind = 'reg' , color = 'magenta' ) plot . set_axis_labels ( 'app_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1106	plot = sns . jointplot ( x = higuchi_fds , y = targets , kind = 'kde' , color = 'crimson' ) plot . set_axis_labels ( 'higuchi_fd' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1107	plot = sns . jointplot ( x = higuchi_fds , y = targets , kind = 'reg' , color = 'crimson' ) plot . set_axis_labels ( 'higuchi_fd' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1108	plot = sns . jointplot ( x = katz_fds , y = targets , kind = 'hex' , color = 'forestgreen' ) plot . set_axis_labels ( 'katz_fd' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1109	plot = sns . jointplot ( x = katz_fds , y = targets , kind = 'reg' , color = 'forestgreen' ) plot . set_axis_labels ( 'katz_fd' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1110	import os import gc import numpy as np from numpy . fft import * import pandas as pd import matplotlib . pyplot as plt import seaborn as sns import pywt from statsmodels . robust import mad import scipy from scipy import signal from scipy . signal import butter , deconvolve import warnings warnings . filterwarnings ( 'ignore' )
1111	acoustic_data = seismic_signals . acoustic_data time_to_failure = seismic_signals . time_to_failure data_len = len ( seismic_signals ) del seismic_signals gc . collect ( )
1112	signals = [ ] targets = [ ] for i in range ( data_len // SIGNAL_LEN ) : min_lim = SIGNAL_LEN * i max_lim = min ( [ SIGNAL_LEN * ( i + 1 ) , data_len ] ) signals . append ( list ( acoustic_data [ min_lim : max_lim ] ) ) targets . append ( time_to_failure [ max_lim ] ) del acoustic_data del time_to_failure gc . collect ( ) signals = np . array ( signals ) targets = np . array ( targets )
1113	def maddest ( d , axis = None ) : return np . mean ( np . absolute ( d - np . mean ( d , axis ) ) , axis )
1114	def high_pass_filter ( x , low_cutoff = 1000 , SAMPLE_RATE = SAMPLE_RATE ) : nyquist = 0.5 * SAMPLE_RATE norm_low_cutoff = low_cutoff / nyquist sos = butter ( 10 , Wn = [ norm_low_cutoff ] , btype = 'highpass' , output = 'sos' ) filtered_sig = signal . sosfilt ( sos , x ) return filtered_sig
1115	def average_smoothing ( signal , kernel_size , stride ) : sample = [ ] start = 0 end = kernel_size while end <= len ( signal ) : start = start + stride end = end + stride sample . append ( np . mean ( signal [ start : end ] ) ) return np . array ( sample )
1116	SIGNAL_LEN = 150000 MIN_NUM = - 27 MAX_NUM = 28
1117	acoustic_data = seismic_signals . acoustic_data time_to_failure = seismic_signals . time_to_failure data_len = len ( seismic_signals ) del seismic_signals gc . collect ( )
1118	signals = [ ] targets = [ ] for i in range ( data_len // SIGNAL_LEN ) : min_lim = SIGNAL_LEN * i max_lim = min ( [ SIGNAL_LEN * ( i + 1 ) , data_len ] ) signals . append ( list ( acoustic_data [ min_lim : max_lim ] ) ) targets . append ( time_to_failure [ max_lim ] ) del acoustic_data del time_to_failure gc . collect ( ) signals = np . array ( signals ) targets = np . array ( targets )
1119	def min_max_transfer ( ts , min_value , max_value , range_needed = ( - 1 , 1 ) ) : ts_std = ( ts - min_value ) / ( max_value - min_value ) if range_needed [ 0 ] < 0 : return ts_std * ( range_needed [ 1 ] + abs ( range_needed [ 0 ] ) ) + range_needed [ 0 ] else : return ts_std * ( range_needed [ 1 ] - range_needed [ 0 ] ) + range_needed [ 0 ]
1120	def prepare_data ( start , end ) : train = pd . DataFrame ( np . transpose ( signals [ int ( start ) : int ( end ) ] ) ) X = [ ] for id_measurement in tqdm ( train . index [ int ( start ) : int ( end ) ] ) : X_signal = transform_ts ( train [ id_measurement ] ) X . append ( X_signal ) X = np . asarray ( X ) return X
1121	X = [ ] def load_all ( ) : total_size = len ( signals ) for start , end in [ ( 0 , int ( total_size ) ) ] : X_temp = prepare_data ( start , end ) X . append ( X_temp ) load_all ( ) X = np . concatenate ( X )
1122	plot = sns . jointplot ( x = spectral_entropies , y = targets , kind = 'kde' , color = 'blueviolet' ) plot . set_axis_labels ( 'spectral_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1123	plot = sns . jointplot ( x = spectral_entropies , y = targets , kind = 'reg' , color = 'blueviolet' ) plot . set_axis_labels ( 'spectral_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1124	plot = sns . jointplot ( x = sample_entropies , y = targets , kind = 'kde' , color = 'mediumvioletred' ) plot . set_axis_labels ( 'sample_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1125	plot = sns . jointplot ( x = sample_entropies , y = targets , kind = 'reg' , color = 'mediumvioletred' ) plot . set_axis_labels ( 'sample_entropy' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1126	plot = sns . jointplot ( x = detrended_fluctuations , y = targets , kind = 'kde' , color = 'mediumblue' ) plot . set_axis_labels ( 'detrended_fluctuation' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1127	plot = sns . jointplot ( x = detrended_fluctuations , y = targets , kind = 'reg' , color = 'mediumblue' ) plot . set_axis_labels ( 'detrended_fluctuation' , 'time_to_failure' , fontsize = 16 ) plt . show ( )
1128	INPUT_DIR = '../input/m5-forecasting-accuracy' calendar = pd . read_csv ( f'{INPUT_DIR}/calendar.csv' ) selling_prices = pd . read_csv ( f'{INPUT_DIR}/sell_prices.csv' ) sample_submission = pd . read_csv ( f'{INPUT_DIR}/sample_submission.csv' ) sales_train_val = pd . read_csv ( f'{INPUT_DIR}/sales_train_validation.csv' )
1129	def average_smoothing ( signal , kernel_size = 3 , stride = 1 ) : sample = [ ] start = 0 end = kernel_size while end <= len ( signal ) : start = start + stride end = end + stride sample . extend ( np . ones ( end - start ) * np . mean ( signal [ start : end ] ) ) return np . array ( sample )
1130	df = pd . DataFrame ( np . transpose ( [ means , store_list ] ) ) df . columns = [ "Mean sales" , "Store name" ] px . bar ( df , y = "Mean sales" , x = "Store name" , color = "Store name" , title = "Mean sales vs. Store name" )
1131	error = [ error_naive , error_avg , error_holt , error_exponential , error_arima , error_prophet ] names = [ "Naive approach" , "Moving average" , "Holt linear" , "Exponential smoothing" , "ARIMA" , "Prophet" ] df = pd . DataFrame ( np . transpose ( [ error , names ] ) ) df . columns = [ "RMSE Loss" , "Model" ] px . bar ( df , y = "RMSE Loss" , x = "Model" , color = "Model" , title = "RMSE Loss vs. Model" )
1132	import os import gc import numpy as np import pandas as pd from tqdm import tqdm tqdm . pandas ( ) from collections import Counter from operator import itemgetter import scipy import cv2 from cv2 import imread import matplotlib import matplotlib . pyplot as plt import seaborn as sns
1133	train_images = [ ] image_dirs = np . take ( os . listdir ( '../input/train' ) , select_rows ) for image_dir in tqdm ( sorted ( image_dirs ) ) : image = imread ( '../input/train/' + image_dir ) train_images . append ( image ) del image gc . collect ( ) train_images = np . array ( train_images )
1134	labels_df = pd . read_csv ( '../input/labels.csv' ) label_dict = dict ( zip ( labels_df . attribute_id , labels_df . attribute_name ) ) for key in label_dict : if 'culture' in label_dict [ key ] : label_dict [ key ] = label_dict [ key ] [ 9 : ] if 'tag' in label_dict [ key ] : label_dict [ key ] = label_dict [ key ] [ 5 : ]
1135	train_targets = [ ] for targets in targets_df . attribute_ids : target = targets . split ( ) target = list ( map ( lambda x : label_dict [ int ( x ) ] , target ) ) train_targets . append ( target ) train_targets = np . array ( train_targets )
1136	fig , ax = plt . subplots ( nrows = 4 , ncols = 4 , figsize = ( 50 , 50 ) ) count = 0 for i in range ( 4 ) : for j in range ( 4 ) : ax [ i , j ] . imshow ( cv2 . cvtColor ( train_images [ count ] , cv2 . COLOR_BGR2RGB ) ) ax [ i , j ] . set_title ( str ( train_targets [ count ] ) , fontsize = 24 ) count = count + 1
1137	FOLDS = 8 EPOCHS = 4 RRC = 1.0 FLIP = 1.0 NORM = 1.0 ROTATE = 1.0 LR = ( 1e-4 , 1e-3 ) MODEL_SAVE_PATH = "resnet_model" WIDTH = 512 HEIGHT = 512 BATCH_SIZE = 128 VAL_BATCH_SIZE = 128 DATA_PATH = '../input/prostate-cancer-grade-assessment/' RESIZED_PATH = '../input/panda-resized-train-data-512x512/train_images/'
1138	test_df = pd . read_csv ( TEST_DATA_PATH ) train_df = pd . read_csv ( TRAIN_DATA_PATH ) sample_submission = pd . read_csv ( SAMPLE_SUB_PATH )
1139	gleason_replace_dict = { 0 : 0 , 1 : 1 , 3 : 2 , 4 : 3 , 5 : 4 } def process_gleason ( gleason ) : if gleason == 'negative' : gs = ( 1 , 1 ) else : gs = tuple ( gleason . split ( '+' ) ) return [ gleason_replace_dict [ int ( g ) ] for g in gs ] train_df . gleason_score = train_df . gleason_score . apply ( process_gleason )
1140	model = ResNetDetector ( ) x = torch . randn ( 2 , 3 , 32 , 32 ) . requires_grad_ ( True ) y = model ( x ) make_dot ( y , params = dict ( list ( model . named_parameters ( ) ) + [ ( 'x' , x ) ] ) )
1141	def cel ( inp , targ ) : _ , labels = targ . max ( dim = 1 ) return nn . CrossEntropyLoss ( ) ( inp , labels ) def acc ( inp , targ ) : inp_idx = inp . max ( axis = 1 ) . indices targ_idx = targ . max ( axis = 1 ) . indices return ( inp_idx == targ_idx ) . float ( ) . sum ( axis = 0 ) / len ( inp_idx )
1142	EPOCHS = 8 BATCH_SIZE = 128 DATA_PATH = '../input/nfl-big-data-bowl-2020/'
1143	data = train_df . sample ( frac = 0.025 ) quantile = data [ "Yards" ] . quantile ( 0.95 ) data = data . loc [ data [ "Yards" ] < quantile ] plot = sns . jointplot ( x = data [ "X" ] , y = data [ "Yards" ] , kind = 'kde' , color = 'forestgreen' , height = 7 ) plot . set_axis_labels ( 'X coordinate' , 'Yards' , fontsize = 16 ) plt . show ( plot )
1144	data = train_df . sample ( frac = 0.025 ) quantile = data [ "Yards" ] . quantile ( 0.95 ) data = data . loc [ data [ "Yards" ] < quantile ] plot = sns . jointplot ( x = data [ "Y" ] , y = data [ "Yards" ] , kind = 'kde' , color = ( 179 / 255 , 0 , 30 / 255 ) , height = 7 ) plot . set_axis_labels ( 'Y coordinate' , 'Yards' , fontsize = 16 ) plt . show ( plot )
1145	data = train_df . sample ( frac = 0.025 ) plot = sns . jointplot ( x = data [ "X" ] , y = data [ "Y" ] , kind = 'kde' , color = 'mediumvioletred' , height = 7 ) plot . set_axis_labels ( 'X coordinate' , 'Y coordinate' , fontsize = 16 ) plt . show ( plot )
1146	data = train_df . sample ( frac = 0.025 ) quantile = data [ "Yards" ] . quantile ( 0.95 ) data = data . loc [ data [ "Yards" ] < quantile ] plot = sns . jointplot ( x = data [ "Dir" ] , y = data [ "Yards" ] , kind = 'kde' , color = ( 255 / 255 , 102 / 255 , 25 / 255 ) , height = 7 ) plot . set_axis_labels ( 'Dir' , 'Yards' , fontsize = 16 ) plt . show ( plot )
1147	data = train_df . sample ( frac = 0.025 ) quantile = data [ "Yards" ] . quantile ( 0.95 ) data = data . loc [ data [ "Yards" ] < quantile ] plot = sns . jointplot ( x = data [ "A" ] , y = data [ "Yards" ] , kind = 'kde' , color = ( 0 , 0 , 230 / 255 ) , height = 7 ) plot . set_axis_labels ( 'A' , 'Yards' , fontsize = 16 ) plt . show ( plot )
1148	fig = ff . create_distplot ( hist_data = [ train_df . sample ( frac = 0.025 ) [ "S" ] ] , group_labels = "S" , colors = [ 'rgb(230, 0, 191)' ] ) fig . update_layout ( title = "S" , yaxis = dict ( title = "Probability Density" ) , xaxis = dict ( title = "S" ) ) fig . show ( )
1149	data = train_df . sample ( frac = 0.025 ) quantile = data [ "Yards" ] . quantile ( 0.95 ) data = data . loc [ data [ "Yards" ] < quantile ] plot = sns . jointplot ( x = data [ "S" ] , y = data [ "Yards" ] , kind = 'kde' , color = ( 230 / 255 , 0 , 191 / 255 ) , height = 7 ) plot . set_axis_labels ( 'S' , 'Yards' , fontsize = 16 ) plt . show ( plot )
1150	data = train_df . sample ( frac = 0.025 ) quantile = data [ "Yards" ] . quantile ( 0.95 ) data = data . loc [ data [ "Yards" ] < quantile ] plot = sns . jointplot ( x = data [ "Humidity" ] , y = data [ "Yards" ] , kind = 'kde' , color = ( 0 / 255 , 77 / 255 , 77 / 255 ) , height = 7 ) plot . set_axis_labels ( 'Humidity' , 'Yards' , fontsize = 16 ) plt . show ( plot )
1151	data = train_df . sample ( frac = 0.025 ) [ "Temperature" ] fig = ff . create_distplot ( hist_data = [ data . fillna ( data . mean ( ) ) ] , group_labels = [ "Temperature" ] , colors = [ 'rgb(51, 34, 0)' ] ) fig . update_layout ( title = "Temperature" , yaxis = dict ( title = "Probability Density" ) , xaxis = dict ( title = "Temperature" ) )
1152	data = train_df . sample ( frac = 0.025 ) quantile = data [ "Yards" ] . quantile ( 0.95 ) data = data . loc [ data [ "Yards" ] < quantile ] plot = sns . jointplot ( x = data [ "Temperature" ] , y = data [ "Yards" ] , kind = 'kde' , color = ( 51 / 255 , 34 / 255 , 0 ) , height = 7 ) plot . set_axis_labels ( 'Temperature' , 'Yards' , fontsize = 16 ) plt . show ( plot )
1153	fig = go . Figure ( ) for index , category in enumerate ( hist_data ) : fig . add_trace ( go . Violin ( y = category , name = tags [ index ] ) ) fig . update_layout ( title = "VisitorTeamAbbr vs. Yards" , yaxis = dict ( title = "Yards" ) , xaxis = dict ( title = "VisitorTeamAbbr" ) ) fig . show ( )
1154	cat_cols = [ 'Team' , 'FieldPosition' , 'OffenseFormation' ] value_dicts = [ ] for feature in cat_cols : values = set ( train_df [ feature ] ) value_dicts . append ( dict ( zip ( values , np . arange ( len ( values ) ) ) ) )
1155	def indices ( data , feat_index ) : value_dict = value_dicts [ feat_index ] return data [ cat_cols [ feat_index ] ] . apply ( lambda x : value_dict [ x ] ) def one_hot ( indices , feat_index ) : return to_categorical ( indices , num_classes = len ( value_dicts [ feat_index ] ) )
1156	num_cols = [ 'X' , 'S' , 'A' , 'Dis' , 'Orientation' , 'Dir' , 'YardLine' , 'Quarter' , 'Down' , 'Distance' , 'HomeScoreBeforePlay' , 'VisitorScoreBeforePlay' , 'DefendersInTheBox' , 'PlayerWeight' , 'Week' , 'Temperature' , 'Humidity' ] def get_numerical_features ( sample ) : return sample [ num_cols ] . values
1157	train_df = train_df . sample ( frac = 1 ) . reset_index ( drop = True ) split = np . int32 ( 0.8 * len ( train_df ) ) train_set = NFLCompetitionDataset ( data = train_df . iloc [ : split ] , stage = 'train' ) val_set = NFLCompetitionDataset ( data = train_df . iloc [ split : ] , stage = 'val' )
1158	hl_graph = hl . build_graph ( CNN1DNetwork ( ) , torch . zeros ( [ 1 , 25 , 17 ] ) ) hl_graph . theme = hl . graph . THEMES [ "blue" ] . copy ( ) hl_graph
1159	mean = 0. std = 0. nb_samples = 0. for data , _ in tqdm ( train_loader ) : batch_samples = data . size ( 0 ) data = data . view ( batch_samples , data . size ( 1 ) , - 1 ) mean += data . mean ( ( 0 , 1 ) ) std += data . std ( ( 0 , 1 ) ) nb_samples += batch_samples mean /= nb_samples std /= nb_samples
1160	from keras . models import Sequential from keras . layers import Dense , Dropout , BatchNormalization from keras . regularizers import L1L2
1161	def nonan ( x ) : if type ( x ) == str : return x . replace ( "\n" , "" ) else : return "" text = ' ' . join ( [ nonan ( abstract ) for abstract in train_data [ "comment_text" ] ] ) wordcloud = WordCloud ( max_font_size = None , background_color = 'black' , collocations = False , width = 1200 , height = 1000 ) . generate ( text ) fig = px . imshow ( wordcloud ) fig . update_layout ( title_text = 'Common words in comments' )
1162	def get_language ( text ) : return Detector ( "" . join ( x for x in text if x . isprintable ( ) ) , quiet = True ) . languages [ 0 ] . name train_data [ "lang" ] = train_data [ "comment_text" ] . progress_apply ( get_language )
1163	fig = px . bar ( df . query ( "Language != 'English' and Language != 'un'" ) . query ( "Count >= 50" ) , y = "Language" , x = "Count" , title = "Language of non-English comments" , template = "plotly_white" , color = "Language" , text = "Count" , orientation = "h" ) fig . update_traces ( marker = dict ( line = dict ( width = 0.75 , color = 'black' ) ) , textposition = "outside" ) fig . update_layout ( showlegend = False ) fig
1164	fig = px . choropleth ( df . query ( "Language != 'English' and Language != 'un' and country != 'None'" ) . query ( "Count >= 5" ) , locations = "country" , hover_name = "country" , projection = "natural earth" , locationmode = "country names" , title = "Countries of non-English languages" , color = "Count" , template = "plotly" , color_continuous_scale = "agsunset" ) fig . show ( )
1165	fig = px . choropleth ( df . query ( "Language != 'English' and Language != 'un' and country != 'None'" ) , locations = "country" , hover_name = "country" , projection = "natural earth" , locationmode = "country names" , title = "Asian countries" , color = "Count" , template = "plotly" , color_continuous_scale = "spectral" , scope = "asia" ) fig . show ( )
1166	fig = px . choropleth ( df . query ( "Language != 'English' and Language != 'un' and country != 'None'" ) . query ( "Count >= 5" ) , locations = "country" , hover_name = "country" , projection = "natural earth" , locationmode = "country names" , title = "African countries" , color = "Count" , template = "plotly" , color_continuous_scale = "agsunset" , scope = "africa" ) fig . show ( )
1167	df [ "country" ] = df [ "Language" ] . apply ( get_country ) df = df . query ( "country != 'None'" ) fig = px . choropleth ( df , locations = "country" , hover_name = "country" , projection = "natural earth" , locationmode = "country names" , title = "Average comment length vs. Country" , color = "Average_comment_words" , template = "plotly" , color_continuous_scale = "aggrnyl" ) fig
1168	fig = go . Figure ( go . Histogram ( x = [ pols [ "neg" ] for pols in train_data [ "polarity" ] if pols [ "neg" ] != 0 ] , marker = dict ( color = 'seagreen' ) ) ) fig . update_layout ( xaxis_title = "Negativity sentiment" , title_text = "Negativity sentiment" , template = "simple_white" ) fig . show ( )
1169	nums_1 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 1" ) [ "negativity" ] nums_2 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 0" ) [ "negativity" ] fig = ff . create_distplot ( hist_data = [ nums_1 , nums_2 ] , group_labels = [ "Toxic" , "Non-toxic" ] , colors = [ "darkorange" , "dodgerblue" ] , show_hist = False ) fig . update_layout ( title_text = "Negativity vs. Toxicity" , xaxis_title = "Negativity" , template = "simple_white" ) fig . show ( )
1170	fig = go . Figure ( go . Histogram ( x = [ pols [ "pos" ] for pols in train_data [ "polarity" ] if pols [ "pos" ] != 0 ] , marker = dict ( color = 'indianred' ) ) ) fig . update_layout ( xaxis_title = "Positivity sentiment" , title_text = "Positivity sentiment" , template = "simple_white" ) fig . show ( )
1171	nums_1 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 1" ) [ "positivity" ] nums_2 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 0" ) [ "positivity" ] fig = ff . create_distplot ( hist_data = [ nums_1 , nums_2 ] , group_labels = [ "Toxic" , "Non-toxic" ] , colors = [ "darkorange" , "dodgerblue" ] , show_hist = False ) fig . update_layout ( title_text = "Positivity vs. Toxicity" , xaxis_title = "Positivity" , template = "simple_white" ) fig . show ( )
1172	fig = go . Figure ( go . Histogram ( x = [ pols [ "neu" ] for pols in train_data [ "polarity" ] if pols [ "neu" ] != 1 ] , marker = dict ( color = 'dodgerblue' ) ) ) fig . update_layout ( xaxis_title = "Neutrality sentiment" , title_text = "Neutrality sentiment" , template = "simple_white" ) fig . show ( )
1173	nums_1 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 1" ) [ "neutrality" ] nums_2 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 0" ) [ "neutrality" ] fig = ff . create_distplot ( hist_data = [ nums_1 , nums_2 ] , group_labels = [ "Toxic" , "Non-toxic" ] , colors = [ "darkorange" , "dodgerblue" ] , show_hist = False ) fig . update_layout ( title_text = "Neutrality vs. Toxicity" , xaxis_title = "Neutrality" , template = "simple_white" ) fig . show ( )
1174	fig = go . Figure ( go . Histogram ( x = [ pols [ "compound" ] for pols in train_data [ "polarity" ] if pols [ "compound" ] != 0 ] , marker = dict ( color = 'orchid' ) ) ) fig . update_layout ( xaxis_title = "Compound sentiment" , title_text = "Compound sentiment" , template = "simple_white" ) fig . show ( )
1175	nums_1 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 1" ) [ "compound" ] nums_2 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 0" ) [ "compound" ] fig = ff . create_distplot ( hist_data = [ nums_1 , nums_2 ] , group_labels = [ "Toxic" , "Non-toxic" ] , colors = [ "darkorange" , "dodgerblue" ] , show_hist = False ) fig . update_layout ( title_text = "Compound vs. Toxicity" , xaxis_title = "Compound" , template = "simple_white" ) fig . show ( )
1176	train_data [ "flesch_reading_ease" ] = train_data [ "comment_text" ] . progress_apply ( textstat . flesch_reading_ease ) train_data [ "automated_readability" ] = train_data [ "comment_text" ] . progress_apply ( textstat . automated_readability_index ) train_data [ "dale_chall_readability" ] = train_data [ "comment_text" ] . progress_apply ( textstat . dale_chall_readability_score )
1177	fig = go . Figure ( go . Histogram ( x = train_data . query ( "flesch_reading_ease > 0" ) [ "flesch_reading_ease" ] , marker = dict ( color = 'darkorange' ) ) ) fig . update_layout ( xaxis_title = "Flesch reading ease" , title_text = "Flesch reading ease" , template = "simple_white" ) fig . show ( )
1178	nums_1 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 1" ) [ "flesch_reading_ease" ] nums_2 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 0" ) [ "flesch_reading_ease" ] fig = ff . create_distplot ( hist_data = [ nums_1 , nums_2 ] , group_labels = [ "Toxic" , "Non-toxic" ] , colors = [ "darkorange" , "dodgerblue" ] , show_hist = False ) fig . update_layout ( title_text = "Flesch reading ease vs. Toxicity" , xaxis_title = "Flesch reading ease" , template = "simple_white" ) fig . show ( )
1179	fig = go . Figure ( go . Histogram ( x = train_data . query ( "automated_readability < 100" ) [ "automated_readability" ] , marker = dict ( color = 'mediumaquamarine' ) ) ) fig . update_layout ( xaxis_title = "Automated readability" , title_text = "Automated readability" , template = "simple_white" ) fig . show ( )
1180	nums_1 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 1" ) [ "automated_readability" ] nums_2 = train_data . sample ( frac = 0.1 ) . query ( "toxic == 0" ) [ "automated_readability" ] fig = ff . create_distplot ( hist_data = [ nums_1 , nums_2 ] , group_labels = [ "Toxic" , "Non-toxic" ] , colors = [ "darkorange" , "dodgerblue" ] , show_hist = False ) fig . update_layout ( title_text = "Automated readability vs. Toxicity" , xaxis_title = "Automated readability" , template = "simple_white" ) fig . show ( )
1181	fig = go . Figure ( go . Histogram ( x = train_data . query ( "dale_chall_readability < 20" ) [ "dale_chall_readability" ] , marker = dict ( color = 'deeppink' ) ) ) fig . update_layout ( xaxis_title = "Dale-Chall readability" , title_text = "Dale-Chall readability" , template = "simple_white" ) fig . show ( )
1182	fig = go . Figure ( data = [ go . Pie ( labels = train_data . columns [ 2 : 7 ] , values = train_data . iloc [ : , 2 : 7 ] . sum ( ) . values , marker = dict ( colors = px . colors . qualitative . Plotly ) ) ] ) fig . update_traces ( textposition = 'outside' , textfont = dict ( color = "black" ) ) fig . update_layout ( title_text = "Pie chart of labels" ) fig . show ( )
1183	def fast_encode ( texts , tokenizer , chunk_size = 240 , maxlen = 512 ) : tokenizer . enable_truncation ( max_length = maxlen ) tokenizer . enable_padding ( max_length = maxlen ) all_ids = [ ] for i in range ( 0 , len ( texts ) , chunk_size ) : text_chunk = texts [ i : i + chunk_size ] . tolist ( ) encs = tokenizer . encode_batch ( text_chunk ) all_ids . extend ( [ enc . ids for enc in encs ] ) return np . array ( all_ids )
1184	AUTO = tf . data . experimental . AUTOTUNE tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) GCS_DS_PATH = KaggleDatasets ( ) . get_gcs_path ( 'jigsaw-multilingual-toxic-comment-classification' ) EPOCHS = 2 BATCH_SIZE = 32 * strategy . num_replicas_in_sync
1185	tokenizer = transformers . DistilBertTokenizer . from_pretrained ( 'distilbert-base-multilingual-cased' ) save_path = '/kaggle/working/distilbert_base_uncased/' if not os . path . exists ( save_path ) : os . makedirs ( save_path ) tokenizer . save_pretrained ( save_path ) fast_tokenizer = BertWordPieceTokenizer ( 'distilbert_base_uncased/vocab.txt' , lowercase = True )
1186	x_train = fast_encode ( train . comment_text . astype ( str ) , fast_tokenizer , maxlen = 512 ) x_valid = fast_encode ( val_data . comment_text . astype ( str ) . values , fast_tokenizer , maxlen = 512 ) x_test = fast_encode ( test_data . content . astype ( str ) . values , fast_tokenizer , maxlen = 512 ) y_valid = val . toxic . values y_train = train . toxic . values
1187	train_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_train , y_train ) ) . repeat ( ) . shuffle ( 2048 ) . batch ( BATCH_SIZE ) . prefetch ( AUTO ) ) valid_dataset = ( tf . data . Dataset . from_tensor_slices ( ( x_valid , y_valid ) ) . batch ( BATCH_SIZE ) . cache ( ) . prefetch ( AUTO ) ) test_dataset = ( tf . data . Dataset . from_tensor_slices ( x_test ) . batch ( BATCH_SIZE ) )
1188	with strategy . scope ( ) : transformer_layer = transformers . TFDistilBertModel . \ from_pretrained ( 'distilbert-base-multilingual-cased' ) model_vnn = build_vnn_model ( transformer_layer , max_len = 512 ) model_vnn . summary ( )
1189	def callback ( ) : cb = [ ] reduceLROnPlat = ReduceLROnPlateau ( monitor = 'val_loss' , factor = 0.3 , patience = 3 , verbose = 1 , mode = 'auto' , epsilon = 0.0001 , cooldown = 1 , min_lr = 0.000001 ) cb . append ( reduceLROnPlat ) log = CSVLogger ( 'log.csv' ) cb . append ( log ) RocAuc = RocAucEvaluation ( validation_data = ( x_valid , y_valid ) , interval = 1 ) cb . append ( RocAuc ) return cb
1190	N_STEPS = x_train . shape [ 0 ] // BATCH_SIZE calls = callback ( ) train_history = model_vnn . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
1191	with strategy . scope ( ) : model_cnn = build_cnn_model ( transformer_layer , max_len = 512 ) model_cnn . summary ( )
1192	train_history = model_cnn . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
1193	with strategy . scope ( ) : model_lstm = build_lstm_model ( transformer_layer , max_len = 512 ) model_lstm . summary ( )
1194	train_history = model_lstm . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
1195	with strategy . scope ( ) : model_capsule = build_capsule_model ( transformer_layer , max_len = 512 ) model_capsule . summary ( )
1196	train_history = model_capsule . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
1197	with strategy . scope ( ) : model_distilbert = build_distilbert_model ( transformer_layer , max_len = 512 ) model_distilbert . summary ( )
1198	train_history = model_distilbert . fit ( train_dataset , steps_per_epoch = N_STEPS , validation_data = valid_dataset , callbacks = calls , epochs = EPOCHS )
1199	EPOCHS = 5 MAXLEN = 64 SPLIT = 0.8 DROP_RATE = 0.3 LR = ( 4e-5 , 1e-2 ) BATCH_SIZE = 256 VAL_BATCH_SIZE = 8192 MODEL_SAVE_PATH = 'insincerity_model.pt'
1200	EPOCHS = 20 SAMPLE_LEN = 100 IMAGE_PATH = "../input/plant-pathology-2020-fgvc7/images/" TEST_PATH = "../input/plant-pathology-2020-fgvc7/test.csv" TRAIN_PATH = "../input/plant-pathology-2020-fgvc7/train.csv" SUB_PATH = "../input/plant-pathology-2020-fgvc7/sample_submission.csv" sub = pd . read_csv ( SUB_PATH ) test_data = pd . read_csv ( TEST_PATH ) train_data = pd . read_csv ( TRAIN_PATH )
1201	def load_image ( image_id ) : file_path = image_id + ".jpg" image = cv2 . imread ( IMAGE_PATH + file_path ) return cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) train_images = train_data [ "image_id" ] [ : SAMPLE_LEN ] . progress_apply ( load_image )
1202	fig = ff . create_distplot ( [ values ] , group_labels = [ "Channels" ] , colors = [ "purple" ] ) fig . update_layout ( showlegend = False , template = "simple_white" ) fig . update_layout ( title_text = "Distribution of channel values" ) fig . data [ 0 ] . marker . line . color = 'rgb(0, 0, 0)' fig . data [ 0 ] . marker . line . width = 0.5 fig
1203	fig = ff . create_distplot ( [ red_values ] , group_labels = [ "R" ] , colors = [ "red" ] ) fig . update_layout ( showlegend = False , template = "simple_white" ) fig . update_layout ( title_text = "Distribution of red channel values" ) fig . data [ 0 ] . marker . line . color = 'rgb(0, 0, 0)' fig . data [ 0 ] . marker . line . width = 0.5 fig
1204	fig = ff . create_distplot ( [ green_values ] , group_labels = [ "G" ] , colors = [ "green" ] ) fig . update_layout ( showlegend = False , template = "simple_white" ) fig . update_layout ( title_text = "Distribution of green channel values" ) fig . data [ 0 ] . marker . line . color = 'rgb(0, 0, 0)' fig . data [ 0 ] . marker . line . width = 0.5 fig
1205	fig = ff . create_distplot ( [ blue_values ] , group_labels = [ "B" ] , colors = [ "blue" ] ) fig . update_layout ( showlegend = False , template = "simple_white" ) fig . update_layout ( title_text = "Distribution of blue channel values" ) fig . data [ 0 ] . marker . line . color = 'rgb(0, 0, 0)' fig . data [ 0 ] . marker . line . width = 0.5 fig
1206	fig = px . parallel_categories ( train_data [ [ "healthy" , "scab" , "rust" , "multiple_diseases" ] ] , color = "healthy" , color_continuous_scale = "sunset" , \ title = "Parallel categories plot of targets" ) fig
1207	def blur ( img ) : fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 ) ) ax [ 0 ] . imshow ( img ) ax [ 0 ] . set_title ( 'Original Image' , fontsize = 24 ) ax [ 1 ] . imshow ( cv2 . blur ( img , ( 100 , 100 ) ) ) ax [ 1 ] . set_title ( 'Blurred Image' , fontsize = 24 ) plt . show ( )
1208	AUTO = tf . data . experimental . AUTOTUNE tpu = tf . distribute . cluster_resolver . TPUClusterResolver ( ) tf . config . experimental_connect_to_cluster ( tpu ) tf . tpu . experimental . initialize_tpu_system ( tpu ) strategy = tf . distribute . experimental . TPUStrategy ( tpu ) BATCH_SIZE = 16 * strategy . num_replicas_in_sync GCS_DS_PATH = KaggleDatasets ( ) . get_gcs_path ( )
1209	def format_path ( st ) : return GCS_DS_PATH + '/images/' + st + '.jpg' test_paths = test_data . image_id . apply ( format_path ) . values train_paths = train_data . image_id . apply ( format_path ) . values train_labels = np . float32 ( train_data . loc [ : , 'healthy' : 'scab' ] . values ) train_paths , valid_paths , train_labels , valid_labels = \ train_test_split ( train_paths , train_labels , test_size = 0.15 , random_state = 2020 )
1210	lrfn = build_lrfn ( ) STEPS_PER_EPOCH = train_labels . shape [ 0 ] // BATCH_SIZE lr_schedule = tf . keras . callbacks . LearningRateScheduler ( lrfn , verbose = 1 )
1211	with strategy . scope ( ) : model = tf . keras . Sequential ( [ DenseNet121 ( input_shape = ( 512 , 512 , 3 ) , weights = 'imagenet' , include_top = False ) , L . GlobalAveragePooling2D ( ) , L . Dense ( train_labels . shape [ 1 ] , activation = 'softmax' ) ] ) model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'categorical_accuracy' ] ) model . summary ( )
1212	with strategy . scope ( ) : model = tf . keras . Sequential ( [ efn . EfficientNetB7 ( input_shape = ( 512 , 512 , 3 ) , weights = 'imagenet' , include_top = False ) , L . GlobalAveragePooling2D ( ) , L . Dense ( train_labels . shape [ 1 ] , activation = 'softmax' ) ] ) model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'categorical_accuracy' ] ) model . summary ( )
1213	with strategy . scope ( ) : model = tf . keras . Sequential ( [ efn . EfficientNetB7 ( input_shape = ( 512 , 512 , 3 ) , weights = 'noisy-student' , include_top = False ) , L . GlobalAveragePooling2D ( ) , L . Dense ( train_labels . shape [ 1 ] , activation = 'softmax' ) ] ) model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'categorical_accuracy' ] ) model . summary ( )
1214	ensemble_1 , ensemble_2 , ensemble_3 = [ sub ] * 3 ensemble_1 . loc [ : , 'healthy' : ] = 0.50 * probs_dnn + 0.50 * probs_efn ensemble_2 . loc [ : , 'healthy' : ] = 0.25 * probs_dnn + 0.75 * probs_efn ensemble_3 . loc [ : , 'healthy' : ] = 0.75 * probs_dnn + 0.25 * probs_efn ensemble_1 . to_csv ( 'submission_ensemble_1.csv' , index = False ) ensemble_2 . to_csv ( 'submission_ensemble_2.csv' , index = False ) ensemble_3 . to_csv ( 'submission_ensemble_3.csv' , index = False )
1215	columns = features . columns for column in columns [ : - 10 ] : sns . pairplot ( x_vars = column , y_vars = column , hue = 'target' , diag_kind = 'kde' , data = features )
1216	EPOCHS = 2 SPLIT = 0.8 LR = ( 1e-4 , 1e-3 ) MODEL_SAVE_PATH = "resnet_model" W = 64 H = 64 BATCH_SIZE = 32 VAL_BATCH_SIZE = 32 DATA_PATH = '../input/trends-assessment-prediction/'
1217	TEST_MAP_PATH = DATA_PATH + 'fMRI_test/' TRAIN_MAP_PATH = DATA_PATH + 'fMRI_train/' FEAT_PATH = DATA_PATH + 'fnc.csv' TARG_PATH = DATA_PATH + 'train_scores.csv' SAMPLE_SUB_PATH = DATA_PATH + 'sample_submission.csv' TEST_IDS = [ map_id [ : - 4 ] for map_id in sorted ( os . listdir ( TEST_MAP_PATH ) ) ] TRAIN_IDS = [ map_id [ : - 4 ] for map_id in sorted ( os . listdir ( TRAIN_MAP_PATH ) ) ]
1218	model = ResNetModel ( ) x = torch . randn ( 1 , 3 , 64 , 64 ) . requires_grad_ ( True ) y = model ( x ) make_dot ( y , params = dict ( list ( model . named_parameters ( ) ) + [ ( 'x' , x ) ] ) )
1219	def weighted_nae ( inp , targ ) : W = torch . FloatTensor ( [ 0.3 , 0.175 , 0.175 , 0.175 , 0.175 ] ) return torch . mean ( torch . matmul ( torch . abs ( inp - targ ) , W . cuda ( ) / torch . mean ( targ , axis = 0 ) ) )
1220	val_out_shape = - 1 , 5 train_out_shape = - 1 , 5 split = int ( SPLIT * len ( train_df ) ) val = train_df [ split : ] . reset_index ( drop = True ) train = train_df [ : split ] . reset_index ( drop = True ) test_set = TReNDSDataset ( test_df , None , TEST_MAP_PATH , False ) test_loader = DataLoader ( test_set , batch_size = VAL_BATCH_SIZE )
1221	EPOCHS = 20 SPLIT = 0.8 MAXLEN = 48 DROP_RATE = 0.3 np . random . seed ( 42 ) OUTPUT_UNITS = 3 BATCH_SIZE = 384 LR = ( 4e-5 , 1e-2 ) ROBERTA_UNITS = 768 VAL_BATCH_SIZE = 384 MODEL_SAVE_PATH = 'sentiment_model.pt'
1222	def cel ( inp , target ) : _ , labels = target . max ( dim = 1 ) return nn . CrossEntropyLoss ( ) ( inp , labels ) * len ( inp ) def accuracy ( inp , target ) : inp_ind = inp . max ( axis = 1 ) . indices target_ind = target . max ( axis = 1 ) . indices return ( inp_ind == target_ind ) . float ( ) . sum ( axis = 0 )
1223	val_losses = [ torch . load ( 'val_loss_{}.pt' . format ( i ) ) for i in range ( EPOCHS ) ] train_losses = [ torch . load ( 'train_loss_{}.pt' . format ( i ) ) for i in range ( EPOCHS ) ] val_accuracies = [ torch . load ( 'val_acc_{}.pt' . format ( i ) ) for i in range ( EPOCHS ) ] train_accuracies = [ torch . load ( 'train_acc_{}.pt' . format ( i ) ) for i in range ( EPOCHS ) ]
1224	W = 512 H = 512 B = 0.5 SPLIT = 0.8 SAMPLE = True MU = [ 0.485 , 0.456 , 0.406 ] SIGMA = [ 0.229 , 0.224 , 0.225 ] EPOCHS = 5 LR = 1e-3 , 1e-3 BATCH_SIZE = 32 VAL_BATCH_SIZE = 32 MODEL = 'efficientnet-b3' IMG_PATHS = [ '../working/test' , '../working/train_1' , '../working/train_2' ]
1225	PATH_DICT = { } for folder_path in tqdm ( IMG_PATHS ) : for img_path in os . listdir ( folder_path ) : PATH_DICT [ img_path ] = folder_path + '/'
1226	def bce ( y_true , y_pred ) : return nn . BCEWithLogitsLoss ( ) ( y_pred , y_true ) def acc ( y_true , y_pred ) : y_true = y_true . squeeze ( ) y_pred = nn . Sigmoid ( ) ( y_pred ) . squeeze ( ) return ( y_true == torch . round ( y_pred ) ) . float ( ) . sum ( ) / len ( y_true )
1227	split = int ( SPLIT * len ( train_df ) ) train_df , val_df = train_df . loc [ : split ] , train_df . loc [ split : ] train_df , val_df = train_df . reset_index ( drop = True ) , val_df . reset_index ( drop = True )
1228	C = np . array ( [ B , ( 1 - B ) ] ) * 2 ones = len ( train_df . query ( 'target == 1' ) ) zeros = len ( train_df . query ( 'target == 0' ) ) weightage_fn = { 0 : C [ 1 ] / zeros , 1 : C [ 0 ] / ones } weights = [ weightage_fn [ target ] for target in train_df . target ]
1229	length = len ( train_df ) val_ids = val_df . image_name . apply ( lambda x : x + '.jpg' ) train_ids = train_df . image_name . apply ( lambda x : x + '.jpg' ) val_set = SIIMDataset ( val_df , False , True , ids = val_ids ) train_set = SIIMDataset ( train_df , True , True , ids = train_ids )
1230	train_sampler = WeightedRandomSampler ( weights , length ) if_sample , if_shuffle = ( train_sampler , False ) , ( None , True ) sample_fn = lambda is_sample , sampler : if_sample if is_sample else if_shuffle sampler , shuffler = sample_fn ( SAMPLE , train_sampler ) val_loader = DataLoader ( val_set , VAL_BATCH_SIZE , shuffle = False ) train_loader = DataLoader ( train_set , BATCH_SIZE , sampler = sampler , shuffle = shuffler )
1231	device = xm . xla_device ( ) network = CancerNet ( features = 1536 ) . to ( device ) optimizer = Adam ( [ { 'params' : network . efn . parameters ( ) , 'lr' : LR [ 0 ] } , { 'params' : network . dense_output . parameters ( ) , 'lr' : LR [ 1 ] } ] )
1232	import pandas as pd import numpy as np import matplotlib . pylab as plt import json import ast import seaborn as sns import os from itertools import cycle pd . set_option ( 'max_columns' , None )
1233	bpps_files = os . listdir ( '../input/stanford-covid-vaccine/bpps/' ) example_bpps = np . load ( f'../input/stanford-covid-vaccine/bpps/{bpps_files[0]}' ) print ( 'bpps file shape:' , example_bpps . shape )
1234	import numpy as np import pandas as pd import json from pathlib import Path import matplotlib . pyplot as plt from matplotlib import colors import os
1235	import random index = random . sample ( raw_data . index [ raw_data . target == 1 ] . tolist ( ) , 5 ) for i in index : print ( raw_data . iloc [ i , 1 ] )
1236	src = ( ImageItemList . from_df ( df , path , folder = train ) . random_split_by_pct ( 0.2 , seed = 2019 ) . label_from_df ( 'has_oilpalm' ) . add_test ( test_names + holdout_names ) )
1237	data = ( src . transform ( get_transforms ( ) , size = 128 ) . databunch ( bs = 64 ) . normalize ( imagenet_stats ) )
1238	from sklearn . metrics import roc_auc_score def auc_score ( preds , targets ) : return torch . tensor ( roc_auc_score ( targets , preds [ : , 1 ] ) )
1239	learn = create_cnn ( data , models . resnet18 , metrics = [ accuracy ] , model_dir = '/kaggle/working/models' )
1240	labels_df = pd . read_json ( os . path . join ( path , 'train_sample_videos/metadata.json' ) ) labels_df = labels_df . T print ( labels_df . shape ) labels_df . head ( )
1241	import numpy as np import pandas as pd import matplotlib . pyplot as plt import plotly . offline as py py . init_notebook_mode ( connected = True ) import plotly . tools as tls import warnings import seaborn as sns plt . style . use ( 'fivethirtyeight' ) from collections import Counter warnings . filterwarnings ( 'ignore' ) import plotly . graph_objs as go import plotly . tools as tls import plotly . plotly as plpl
1242	train = pd . read_csv ( "../input/train.csv" ) test = pd . read_csv ( "../input/test.csv" ) train . head ( 20 )
1243	train_cp = train train_cp = train_cp . replace ( - 1 , np . NaN ) data = train
1244	colwithnan = train_cp . columns [ train_cp . isnull ( ) . any ( ) ] . tolist ( ) print ( "Just a reminder this dataset has %s Rows. \n" % ( train_cp . shape [ 0 ] ) ) for col in colwithnan : print ( "Column: %s has %s NaN" % ( col , train_cp [ col ] . isnull ( ) . sum ( ) ) )
1245	train_float = train . select_dtypes ( include = [ 'float64' ] ) train_int = train . select_dtypes ( include = [ 'int64' ] ) Counter ( train . dtypes . values )
1246	colormap = plt . cm . jet plt . figure ( figsize = ( 16 , 12 ) ) plt . title ( 'Pearson correlation of continuous features' , y = 1.05 , size = 15 ) sns . heatmap ( train_float . corr ( ) , linewidths = 0.1 , vmax = 1.0 , square = True , cmap = colormap , linecolor = 'white' , annot = True )
1247	colormap = plt . cm . jet cotrain_float = train_float . drop ( [ 'ps_calc_03' , 'ps_calc_02' , 'ps_calc_01' ] , axis = 1 ) plt . figure ( figsize = ( 16 , 12 ) ) plt . title ( 'Pearson correlation of continuous features' , y = 1.05 , size = 15 ) sns . heatmap ( cotrain_float . corr ( ) , linewidths = 0.1 , vmax = 1.0 , square = True , cmap = colormap , linecolor = 'white' , annot = True )
1248	colormap = plt . cm . jet plt . figure ( figsize = ( 21 , 16 ) ) plt . title ( 'Pearson correlation of categorical features' , y = 1.05 , size = 15 ) sns . heatmap ( train_int . corr ( ) , linewidths = 0.1 , vmax = 1.0 , square = True , cmap = colormap , linecolor = 'white' , annot = False )
1249	colormap = plt . cm . jet plt . figure ( figsize = ( 25 , 25 ) ) plt . title ( 'Pearson correlation of All the features' , y = 1.05 , size = 15 ) sns . heatmap ( train . corr ( ) , linewidths = 0.1 , vmax = 1.0 , square = True , cmap = colormap , linecolor = 'white' , annot = False )
1250	bin_col = [ col for col in train . columns if '_bin' in col ] zeros = [ ] ones = [ ] for col in bin_col : zeros . append ( ( train [ col ] == 0 ) . sum ( ) ) ones . append ( ( train [ col ] == 1 ) . sum ( ) )
1251	train_int = train_int . drop ( [ 'id' , 'target' ] , axis = 1 ) train_int = train_int . drop ( bin_col , axis = 1 ) some_bin = train_int . describe ( ) some_bin
1252	colormap = plt . cm . jet cotrainnb = cotrain . drop ( [ 'ps_ind_10_bin' , 'ps_ind_11_bin' , 'ps_ind_12_bin' , 'ps_ind_13_bin' ] , axis = 1 ) plt . figure ( figsize = ( 21 , 16 ) ) plt . title ( 'Taking away some binary data' , y = 1.05 , size = 12 ) sns . heatmap ( cotrainnb . corr ( ) , linewidths = 0.1 , vmax = 1.0 , square = True , cmap = colormap , linecolor = 'white' , annot = False )
1253	models = [ ( 'LR' , LogisticRegression ( ) ) , ( 'LDA' , LinearDiscriminantAnalysis ( ) ) , ( 'CART' , DecisionTreeClassifier ( ) ) , ( 'NB' , GaussianNB ( ) ) ] results = [ ] names = [ ] for name , model in models : print ( "Training model %s" % ( name ) ) model . fit ( X_train , Y_train ) result = model . score ( X_validation , Y_validation ) msg = "Classifier score %s: %f" % ( name , result ) print ( msg ) print ( "----- Training Done -----" )
1254	toplot = [ ] for name , model in ensembles : trace = go . Bar ( x = model . feature_importances_ , y = X_validation . columns , orientation = 'h' , textposition = 'auto' , name = name ) toplot . append ( trace ) layout = dict ( title = 'Barplot of features importance' , width = 900 , height = 2000 , barmode = 'group' ) fig = go . Figure ( data = toplot , layout = layout ) py . iplot ( fig , filename = 'features-figure' )
1255	num_epochs = 1000 optimizer = torch . optim . Adam ( gol_model . parameters ( ) , lr = 0.01 ) criterion = nn . CrossEntropyLoss ( ) losses = np . zeros ( num_epochs ) for e in range ( num_epochs ) : optimizer . zero_grad ( ) y_pred = gol_model ( x_train ) loss = criterion ( y_pred , y_train ) losses [ e ] = loss . item ( ) loss . backward ( ) optimizer . step ( ) plt . plot ( losses ) print ( f"Last loss: {losses[-1]:.5f}" )
1256	task = train_tasks [ "db3e9e38" ] [ "train" ] for sample in task : plot_sample ( sample )
1257	predictions = predict ( model , task ) for i in range ( len ( task ) ) : plot_sample ( task [ i ] , predictions [ i ] )
1258	test = train_tasks [ "db3e9e38" ] [ "test" ] predictions = predict ( model , test ) for i in range ( len ( test ) ) : plot_sample ( test [ i ] , predictions [ i ] )
1259	for task , prediction , solved in tqdm ( zip ( train_tasks . values ( ) , train_predictions , train_solved ) ) : if solved : for i in range ( len ( task [ 'train' ] ) ) : plot_sample ( task [ 'train' ] [ i ] ) for i in range ( len ( task [ 'test' ] ) ) : plot_sample ( task [ 'test' ] [ i ] , prediction [ i ] )
1260	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . distplot ( train_objects [ 'center_x' ] , color = 'darkorange' , ax = ax ) . set_title ( 'center_x and center_y' , fontsize = 16 ) sns . distplot ( train_objects [ 'center_y' ] , color = 'purple' , ax = ax ) . set_title ( 'center_x and center_y' , fontsize = 16 ) plt . xlabel ( 'center_x and center_y' , fontsize = 15 ) plt . show ( )
1261	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . distplot ( train_objects [ 'center_z' ] , color = 'navy' , ax = ax ) . set_title ( 'center_z' , fontsize = 16 ) plt . xlabel ( 'center_z' , fontsize = 15 ) plt . show ( )
1262	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . distplot ( train_objects [ 'yaw' ] , color = 'darkgreen' , ax = ax ) . set_title ( 'yaw' , fontsize = 16 ) plt . xlabel ( 'yaw' , fontsize = 15 ) plt . show ( )
1263	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . distplot ( train_objects [ 'width' ] , color = 'magenta' , ax = ax ) . set_title ( 'width' , fontsize = 16 ) plt . xlabel ( 'width' , fontsize = 15 ) plt . show ( )
1264	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . distplot ( train_objects [ 'length' ] , color = 'crimson' , ax = ax ) . set_title ( 'length' , fontsize = 16 ) plt . xlabel ( 'length' , fontsize = 15 ) plt . show ( )
1265	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . distplot ( train_objects [ 'height' ] , color = 'indigo' , ax = ax ) . set_title ( 'height' , fontsize = 16 ) plt . xlabel ( 'height' , fontsize = 15 ) plt . show ( )
1266	fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) plot = sns . countplot ( y = "class_name" , data = train_objects . query ( 'class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"' ) , palette = [ 'navy' , 'darkblue' , 'blue' , 'dodgerblue' , 'skyblue' , 'lightblue' ] ) . set_title ( 'Object Frequencies' , fontsize = 16 ) plt . yticks ( fontsize = 14 ) plt . xlabel ( "Count" , fontsize = 15 ) plt . ylabel ( "Class Name" , fontsize = 15 ) plt . show ( plot )
1267	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . boxplot ( x = "class_name" , y = "center_x" , data = train_objects . query ( 'class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"' ) , palette = 'YlGnBu' , ax = ax ) . set_title ( 'center_x (for different objects)' , fontsize = 16 ) plt . yticks ( fontsize = 14 ) plt . xticks ( fontsize = 14 ) plt . xlabel ( "Class Name" , fontsize = 15 ) plt . ylabel ( "center_x" , fontsize = 15 ) plt . show ( plot )
1268	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . boxplot ( x = "class_name" , y = "center_y" , data = train_objects . query ( 'class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"' ) , palette = 'YlOrRd' , ax = ax ) . set_title ( 'center_y (for different objects)' , fontsize = 16 ) plt . yticks ( fontsize = 14 ) plt . xticks ( fontsize = 14 ) plt . xlabel ( "Class Name" , fontsize = 15 ) plt . ylabel ( "center_y" , fontsize = 15 ) plt . show ( plot )
1269	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . boxplot ( x = "class_name" , y = "width" , data = train_objects . query ( 'class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"' ) , palette = 'YlGn' , ax = ax ) . set_title ( 'width (for different objects)' , fontsize = 16 ) plt . yticks ( fontsize = 14 ) plt . xticks ( fontsize = 14 ) plt . xlabel ( "Class Name" , fontsize = 15 ) plt . ylabel ( "width" , fontsize = 15 ) plt . show ( plot )
1270	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . boxplot ( x = "class_name" , y = "length" , data = train_objects . query ( 'class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal" and length < 15' ) , palette = 'Purples' , ax = ax ) . set_title ( 'length (for different objects)' , fontsize = 16 ) plt . yticks ( fontsize = 14 ) plt . xticks ( fontsize = 14 ) plt . xlabel ( "Class Name" , fontsize = 15 ) plt . ylabel ( "length" , fontsize = 15 ) plt . show ( plot )
1271	fig , ax = plt . subplots ( figsize = ( 15 , 15 ) ) plot = sns . boxplot ( x = "class_name" , y = "height" , data = train_objects . query ( 'class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal" and height < 6' ) , palette = 'Reds' , ax = ax ) . set_title ( 'height (for different objects)' , fontsize = 16 ) plt . yticks ( fontsize = 14 ) plt . xticks ( fontsize = 14 ) plt . xlabel ( "Class Name" , fontsize = 15 ) plt . ylabel ( "height" , fontsize = 15 ) plt . show ( plot )
1272	def render_scene ( index ) : my_scene = lyft_dataset . scene [ index ] my_sample_token = my_scene [ "first_sample_token" ] lyft_dataset . render_sample ( my_sample_token )
1273	lyft_dataset . render_pointcloud_in_image ( sample_token = my_sample [ "token" ] , dot_size = 1 , camera_channel = 'CAM_FRONT' )
1274	sensor_channel = 'CAM_BACK' my_sample_data = lyft_dataset . get ( 'sample_data' , my_sample [ 'data' ] [ sensor_channel ] ) lyft_dataset . render_sample_data ( my_sample_data [ 'token' ] )
1275	sensor_channel = 'CAM_FRONT_LEFT' my_sample_data = lyft_dataset . get ( 'sample_data' , my_sample [ 'data' ] [ sensor_channel ] ) lyft_dataset . render_sample_data ( my_sample_data [ 'token' ] )
1276	sensor_channel = 'CAM_FRONT_RIGHT' my_sample_data = lyft_dataset . get ( 'sample_data' , my_sample [ 'data' ] [ sensor_channel ] ) lyft_dataset . render_sample_data ( my_sample_data [ 'token' ] )
1277	sensor_channel = 'CAM_BACK_LEFT' my_sample_data = lyft_dataset . get ( 'sample_data' , my_sample [ 'data' ] [ sensor_channel ] ) lyft_dataset . render_sample_data ( my_sample_data [ 'token' ] )
1278	sensor_channel = 'CAM_BACK_RIGHT' my_sample_data = lyft_dataset . get ( 'sample_data' , my_sample [ 'data' ] [ sensor_channel ] ) lyft_dataset . render_sample_data ( my_sample_data [ 'token' ] )
1279	my_scene = lyft_dataset . scene [ 0 ] my_sample_token = my_scene [ "first_sample_token" ] my_sample = lyft_dataset . get ( 'sample' , my_sample_token ) lyft_dataset . render_sample_data ( my_sample [ 'data' ] [ 'LIDAR_TOP' ] , nsweeps = 5 )
1280	my_scene = lyft_dataset . scene [ 0 ] my_sample_token = my_scene [ "first_sample_token" ] my_sample = lyft_dataset . get ( 'sample' , my_sample_token ) lyft_dataset . render_sample_data ( my_sample [ 'data' ] [ 'LIDAR_FRONT_LEFT' ] , nsweeps = 5 )
1281	my_scene = lyft_dataset . scene [ 0 ] my_sample_token = my_scene [ "first_sample_token" ] my_sample = lyft_dataset . get ( 'sample' , my_sample_token ) lyft_dataset . render_sample_data ( my_sample [ 'data' ] [ 'LIDAR_FRONT_RIGHT' ] , nsweeps = 5 )
1282	import numpy as np import pandas as pd from sklearn import * from sklearn . metrics import f1_score import lightgbm as lgb import xgboost as xgb from catboost import Pool , CatBoostRegressor import matplotlib . pyplot as plt import seaborn as sns import time import datetime sns . set_style ( "whitegrid" ) from sklearn . model_selection import KFold ROW_PER_BATCH = 500000
1283	train [ 'batch' ] = 0 for i in range ( 0 , train . shape [ 0 ] // ROW_PER_BATCH ) : train . iloc [ i * ROW_PER_BATCH : ( i + 1 ) * ROW_PER_BATCH , 3 ] = i
1284	test [ 'batch' ] = 0 for i in range ( 0 , test . shape [ 0 ] // ROW_PER_BATCH ) : test . iloc [ i * ROW_PER_BATCH : ( i + 1 ) * ROW_PER_BATCH , 2 ] = i
1285	plt . figure ( figsize = ( 20 , 5 ) ) plt . plot ( train . signal [ 500000 : 1000000 ] [ : : 100 ] ) plt . show ( )
1286	plt . figure ( figsize = ( 20 , 6 ) ) sns . distplot ( train . signal [ 500000 : 1000000 ] , color = 'r' ) sns . distplot ( train . signal_undrifted [ 500000 : 1000000 ] , color = 'g' ) . set ( xlabel = "Signal" ) plt . legend ( labels = [ 'Original Signal' , 'Undrifted Signal' ] )
1287	booster = result_dict_lgb [ 'model' ] fi = pd . DataFrame ( ) fi [ 'importance' ] = booster . feature_importance ( importance_type = 'gain' ) fi [ 'feature' ] = booster . feature_name ( ) best_features = fi . sort_values ( by = 'importance' , ascending = False ) [ : 20 ] plt . figure ( figsize = ( 16 , 12 ) ) ; sns . barplot ( x = "importance" , y = "feature" , data = best_features ) ; plt . title ( 'LGB Features (avg over folds)' ) ;
1288	import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns from kmodes . kmodes import KModes from sklearn import preprocessing from sklearn . decomposition import PCA pd . set_option ( 'mode.chained_assignment' , None )
1289	bin_features = [ 'bin_0' , 'bin_1' , 'bin_2' , 'bin_3' , 'bin_4' ] nom_features = [ 'nom_0' , 'nom_1' , 'nom_2' , 'nom_3' , 'nom_4' ] ord_features = [ 'ord_0' , 'ord_1' , 'ord_2' , 'ord_3' , 'ord_4' ] time_features = [ 'day' , 'month' ] target = [ 'target' ] all_features = bin_features + nom_features + ord_features + time_features + target
1290	for col in all_features : print ( f'Filling in missing catagorical data in {col} with mode of {col}...' ) train [ col ] . fillna ( train [ col ] . mode ( ) [ 0 ] , inplace = True ) print ( f'Number of unique catagories in {col} = {train[col].nunique()}\n' )
1291	train_bin = train [ bin_features ] train_bin [ 'bin_3' ] = train_bin [ 'bin_3' ] . apply ( transform_true_false ) train_bin [ 'bin_4' ] = train_bin [ 'bin_4' ] . apply ( transform_yes_no ) train_bin = train_bin . astype ( 'int64' ) train_bin . head ( )
1292	train_nom = train [ nom_features ] for col in nom_features : le = preprocessing . LabelEncoder ( ) train_nom [ col ] = le . fit_transform ( train_nom [ col ] ) train_nom . head ( )
1293	train_ord = train [ ord_features ] train_ord [ 'ord_0' ] = train_ord [ 'ord_0' ] . apply ( transform_ord_0 ) train_ord [ 'ord_1' ] = train_ord [ 'ord_1' ] . apply ( transform_ord_1 ) train_ord [ 'ord_2' ] = train_ord [ 'ord_2' ] . apply ( transform_ord_2 ) train_ord [ 'ord_3' ] = train_ord [ 'ord_3' ] . map ( ord_3_dict ) train_ord [ 'ord_4' ] = train_ord [ 'ord_4' ] . map ( ord_4_dict ) train_ord . head ( )
1294	train_time = train [ time_features ] train_time [ 'day' ] = train_time [ 'day' ] . apply ( lambda x : x - 1 ) train_time [ 'month' ] = train_time [ 'month' ] . apply ( lambda x : x - 1 ) train_time = train_time . astype ( 'int64' ) train_time . head ( )
1295	corr = train_final . corr ( method = 'spearman' ) mask = np . zeros_like ( corr , dtype = np . bool ) mask [ np . triu_indices_from ( mask ) ] = True f , ax = plt . subplots ( figsize = ( 20 , 18 ) ) sns . heatmap ( corr , mask = mask , cmap = "YlGnBu" , vmax = .30 , center = 0 , square = True , linewidths = .5 , cbar_kws = { "shrink" : .5 } )
1296	cost = [ ] K = range ( 1 , 5 ) for num_clusters in list ( K ) : kmode = KModes ( n_clusters = num_clusters , init = "Cao" , n_init = 1 , verbose = 1 ) kmode . fit_predict ( train_final ) cost . append ( kmode . cost_ ) plt . plot ( K , cost , 'bx-' ) plt . xlabel ( 'k clusters' ) plt . ylabel ( 'Cost' ) plt . title ( 'Elbow Method For Optimal k' ) plt . show ( )
1297	km = KModes ( n_clusters = 2 , init = "Cao" , n_init = 1 , verbose = 1 ) cluster_labels = km . fit_predict ( train_final ) train [ 'Cluster' ] = cluster_labels
1298	for col in all_features : plt . subplots ( figsize = ( 15 , 5 ) ) sns . countplot ( x = 'Cluster' , hue = col , data = train ) plt . show ( )
1299	b , a = butter ( order , lpf_cutoff / nyq , btype = 'low' , analog = False ) w , h = freqz ( b , a , fs = fs ) plt . figure ( figsize = ( 16 , 8 ) ) ; plt . plot ( w , 20 * np . log10 ( abs ( h ) ) , 'b' ) plt . ylabel ( 'Amplitude [dB]' , color = 'b' ) plt . xlabel ( 'Frequency [Hz]' ) plt . title ( 'Low-pass Butterworth Filter, cutoff @ 600Hz' )
1300	batch = 8 train [ 'signal' ] [ batch_size * ( batch - 1 ) : batch_size * batch ] = signal_lpf_batch_8 train [ 'signal_undrifted' ] = train [ 'signal' ] test [ 'signal_undrifted' ] = test [ 'signal' ]
1301	pd . concat ( [ data_normal . feature_1 . value_counts ( normalize = True ) . sort_index ( ) , data_less . feature_1 . value_counts ( normalize = True ) . sort_index ( ) ] , axis = 1 ) . plot ( kind = 'bar' ) plt . legend ( [ 'normal' , '-33' ] )
1302	pd . concat ( [ data_normal . feature_2 . value_counts ( normalize = True ) . sort_index ( ) , data_less . feature_2 . value_counts ( normalize = True ) . sort_index ( ) ] , axis = 1 ) . plot ( kind = 'bar' ) plt . legend ( [ 'normal' , '-33' ] )
1303	pd . concat ( [ data_normal . feature_3 . value_counts ( normalize = True ) . sort_index ( ) , data_less . feature_3 . value_counts ( normalize = True ) . sort_index ( ) ] , axis = 1 ) . plot ( kind = 'bar' ) plt . legend ( [ 'normal' , '-33' ] )
1304	card_id = list ( data_train . card_id . unique ( ) ) + list ( data_test . card_id . unique ( ) ) merchant_id = list ( pd . read_csv ( '../input/elo-merchant-category-recommendation/merchants.csv' , usecols = [ 'merchant_id' ] ) . merchant_id . unique ( ) ) card_id_dict = { value : key for key , value in enumerate ( card_id ) } merchant_id_dict = { value : key for key , value in enumerate ( merchant_id ) }
1305	df . ingredients = df . ingredients . str . replace ( "[" , " " ) df . ingredients = df . ingredients . str . replace ( "]" , " " ) df . ingredients = df . ingredients . str . replace ( "'" , " " ) df . ingredients = df . ingredients . str . replace ( "," , " " )
1306	df . ingredients = df . ingredients . astype ( 'str' ) df . ingredients = df . ingredients . str . replace ( "[" , " " ) df . ingredients = df . ingredients . str . replace ( "]" , " " ) df . ingredients = df . ingredients . str . replace ( "'" , " " ) df . ingredients = df . ingredients . str . replace ( "," , " " )
1307	import numpy as np import pandas as pd import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) ) from time import time from tqdm import tqdm_notebook as tqdm from collections import Counter from scipy import stats import lightgbm as lgb from sklearn . metrics import cohen_kappa_score from sklearn . model_selection import KFold , StratifiedKFold import gc import json pd . set_option ( 'display.max_columns' , 1000 )
1308	params = { 'n_estimators' : 100 , "num_rounds" : 10 , 'boosting_type' : 'gbdt' , 'objective' : 'regression' , 'metric' : 'rmse' , 'subsample' : 0.75 , 'subsample_freq' : 1 , 'learning_rate' : 0.1 , 'feature_fraction' : 0.9 , 'max_depth' : 15 , 'lambda_l1' : 1 , 'lambda_l2' : 1 , 'verbose' : 100 , 'eval_metric' : 'cappa' }
1309	from sklearn . model_selection import KFold from lofo import LOFOImportance , Dataset , plot_importance import lightgbm as lgb NUM_FOLDS = 2 kfold = KFold ( NUM_FOLDS , shuffle = False , random_state = 0 ) model = lgb . LGBMRegressor ( ** params ) dataset = Dataset ( df = reduce_train , target = "accuracy_group" , features = features ) lofo_imp = LOFOImportance ( dataset , model = model , cv = kfold , scoring = "neg_mean_absolute_error" , fit_params = { "categorical_feature" : categoricals } ) importance_df = lofo_imp . get_importance ( )
1310	cities = pd . read_csv ( '../input/WCities.csv' ) gamecities = pd . read_csv ( '../input/WGameCities.csv' ) tourneycompactresults = pd . read_csv ( '../input/WNCAATourneyCompactResults.csv' ) tourneyseeds = pd . read_csv ( '../input/WNCAATourneySeeds.csv' ) tourneyslots = pd . read_csv ( '../input/WNCAATourneySlots.csv' ) regseasoncompactresults = pd . read_csv ( '../input/WRegularSeasonCompactResults.csv' ) seasons = pd . read_csv ( '../input/WSeasons.csv' ) teamspellings = pd . read_csv ( '../input/WTeamSpellings.csv' , engine = 'python' ) teams = pd . read_csv ( '../input/WTeams.csv' )
1311	fig , ( ax1 , ax2 ) = plt . subplots ( ncols = 2 , figsize = ( 12 , 4 ) ) ax1 = sns . countplot ( x = tourneycompactresults [ 'WSeed' ] , ax = ax1 ) ax1 . set_title ( "Seed of Winners - Tourney" ) ax2 = sns . countplot ( x = tourneycompactresults [ 'LSeed' ] , ax = ax2 ) ; ax2 . set_title ( "Seed of Losers - Tourney" ) plt . legend ( ) ;
1312	fig , ( ax1 , ax2 ) = plt . subplots ( ncols = 2 , figsize = ( 12 , 4 ) ) ax1 = sns . countplot ( x = regseasoncompactresults [ 'WLoc' ] , ax = ax1 ) ax1 . set_title ( "Reg Season" ) ax1 . set_xlabel ( 'Winning location' ) ax2 = sns . countplot ( x = tourneycompactresults [ 'WLoc' ] , ax = ax2 ) ax2 . set_title ( "Tourneys" ) ax2 . set_xlabel ( 'Winning location' ) plt . legend ( ) ;
1313	averageseed = tourneyseeds . groupby ( [ 'TeamID' ] ) . agg ( np . mean ) . sort_values ( 'SeedNumber' ) averageseed = averageseed . merge ( teams , left_index = True , right_on = 'TeamID' ) averageseed . head ( 20 ) . plot ( x = 'TeamName' , y = 'SeedNumber' , kind = 'bar' , figsize = ( 15 , 5 ) , title = 'Top 20 Average Tournament Seed' ) ;
1314	import numpy as np import xml . etree . ElementTree as ET import matplotlib . pyplot as plt from PIL import Image import os import glob root_images = "../input/all-dogs/all-dogs/" root_annots = "../input/annotation/Annotation/"
1315	binary_corr_data = [ ] r = 0 for i in binary_columns : binary_corr_data . append ( [ ] ) for j in binary_columns : s = sum ( train_master [ i ] ^ train_master [ j ] ) / float ( len ( train_master [ i ] ) ) binary_corr_data [ r ] . append ( s ) r += 1
1316	binary_target_corr_data = [ ] for i in binary_columns : s = sum ( train_master [ i ] ^ train_master [ target_column ] ) / float ( len ( train_master [ i ] ) ) binary_target_corr_data . append ( s )
1317	from sklearn . feature_selection import chi2 , mutual_info_classif minfo_target_to_continuous_features = mutual_info_classif ( train_master [ continuous_columns ] , train_master [ target_column ] ) minfo_target_to_continuous_chart = [ go . Bar ( x = continuous_columns , y = minfo_target_to_continuous_features ) ] iplot ( minfo_target_to_continuous_chart )
1318	continuous_corr_data = train_master [ continuous_columns ] . corr ( method = 'pearson' ) . as_matrix ( ) trace = go . Heatmap ( z = continuous_corr_data , x = continuous_columns , y = continuous_columns , colorscale = 'Greys' ) data = [ trace ] iplot ( data )
1319	def gini ( y , pred ) : fpr , tpr , thr = metrics . roc_curve ( y , pred , pos_label = 1 ) g = 2 * metrics . auc ( fpr , tpr ) - 1 return g
1320	from sklearn . model_selection import StratifiedKFold n_splits = 10 folds = StratifiedKFold ( n_splits = n_splits , shuffle = True , random_state = 7 )
1321	plt . figure ( figsize = ( 20 , 10 ) ) train , test = plt . hist ( np . ceil ( train_trans [ 'TransactionDT' ] / 86400 ) , bins = 182 ) , plt . hist ( np . ceil ( test_trans [ 'TransactionDT' ] / 86400 ) , bins = 182 )
1322	import numpy as np import pandas as pd train = pd . read_csv ( '/kaggle/input/covid19-global-forecasting-week-1/train.csv' ) test = pd . read_csv ( '/kaggle/input/covid19-global-forecasting-week-1/test.csv' )
1323	import plotly . graph_objs as go import plotly . express as px import plotly . io as pio from plotly . subplots import make_subplots from plotly . offline import iplot , init_notebook_mode , plot import cufflinks cufflinks . go_offline ( connected = True ) init_notebook_mode ( connected = True )
1324	train_agg = train [ [ 'Country_Region' , 'Date' , 'ConfirmedCases' , 'Fatalities' ] ] . groupby ( [ 'Country_Region' , 'Date' ] , as_index = False ) . agg ( { 'ConfirmedCases' : 'sum' , 'Fatalities' : 'sum' } ) train_agg [ 'Date' ] = pd . to_datetime ( train_agg [ 'Date' ] )
1325	fig = px . line ( train_agg , x = 'Date' , y = 'ConfirmedCases' , color = "Country_Region" , hover_name = "Country_Region" ) fig . update_layout ( autosize = False , width = 1000 , height = 500 , title = 'Confirmed Cases Over Time for Each Country' ) fig . show ( )
1326	import geopandas as gpd shapefile = '/kaggle/input/natural-earth-maps/ne_110m_admin_0_countries.shp' gdf = gpd . read_file ( shapefile ) gdf = gdf . drop ( gdf . index [ 159 ] )
1327	cv = pd . read_csv ( '/kaggle/input/covid19-googletrends/coronavirus.csv' , encoding = 'ISO-8859-1' ) covid = pd . read_csv ( '/kaggle/input/covid19-googletrends/covid19.csv' , encoding = 'ISO-8859-1' )
1328	import seaborn as sns sns . regplot ( x = 'hits' , y = 'ConfirmedCases_log10' , data = cc_google , scatter_kws = { 's' : 25 } , fit_reg = True , line_kws = { "color" : "black" } )
1329	ir = cc_google [ cc_google [ 'Country_Region' ] == 'Iran' ] . reset_index ( ) ir = ir [ [ 'Date' , 'ConfirmedCases' , 'Fatalities' , 'hits' ] ] ir . Date = pd . to_datetime ( ir . Date ) ir . index = ir . Date
1330	cols = [ 'f190486d6' , '58e2e02e6' , 'eeb9cd3aa' , '9fd594eec' , '6eef030c1' , '15ace8c9f' , 'fb0f5dbfe' , '58e056e12' , '20aa07010' , '024c577b9' , 'd6bb78916' , 'b43a7cfd5' , '58232a6fb' , '1702b5bf0' , '324921c7b' , '62e59a501' , '2ec5b290f' , '241f0f867' , 'fb49e4212' , '66ace2992' , 'f74e8f13d' , '5c6487af1' , '963a49cdc' , '26fc93eb7' , '1931ccfdd' , '703885424' , '70feb1494' , '491b9ee45' , '23310aa6f' , 'e176a204a' , '6619d81fc' , '1db387535' , 'fc99f9426' , '91f701ba2' , '0572565c2' , '190db8488' , 'adb64ff71' , 'c47340d97' , 'c5a231d81' , '0ff32eb98' ]
1331	def load_images ( train = True , batch_size = 16 ) : while True : for data , _ in create_data_loader ( batch_size ) : yield data def create_data_loader ( batch_size ) : ds = torch . utils . data . TensorDataset ( torch . Tensor ( imagesIntorch ) , torch . Tensor ( np . zeros ( 22125 ) ) ) return torch . utils . data . DataLoader ( ds , batch_size = batch_size , shuffle = True )
1332	import pandas as pd import numpy as np import matplotlib . pyplot as plt import seaborn as sns from pathlib import Path
1333	DATA_PATH = "../input/santander-customer-transaction-prediction/" train = pd . read_csv ( str ( Path ( DATA_PATH ) / "train.csv" ) ) test = pd . read_csv ( str ( Path ( DATA_PATH ) / "test.csv" ) ) print ( "Train and test shapes" , train . shape , test . shape )
1334	from sklearn . linear_model import LogisticRegression from sklearn . model_selection import StratifiedKFold , cross_val_predict from sklearn . metrics import roc_auc_score from sklearn . preprocessing import StandardScaler
1335	import lightgbm as lgb
1336	weighted_preds = y_preds_lr * 0.05 + y_preds_lgb * 0.95 weighted_test_preds = y_test_preds_lr * 0.05 + y_test_preds_lgb * 0.95 roc_auc_score ( y , weighted_preds )
1337	from sklearn import metrics def gini_xgb ( preds , dtrain ) : labels = dtrain . get_label ( ) gini_score = gini_normalizedc ( labels , preds ) return [ ( 'gini' , gini_score ) ] def gini_lgb ( actuals , preds ) : return 'gini' , gini_normalizedc ( actuals , preds ) , True gini_sklearn = metrics . make_scorer ( gini_normalizedc , True , True )
1338	train = pd . read_csv ( "../input/train.csv" ) feats = [ col for col in train . columns if col not in [ 'id' , 'target' ] ] X = train [ feats ] y = train [ 'target' ]
1339	import numpy as np import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import os print ( os . listdir ( "../input" ) ) from PIL import Image import random from tqdm import tqdm_notebook
1340	test_images = os . listdir ( TEST_BASE ) test = np . array ( [ load_img ( os . path . join ( TEST_BASE , i ) ) for i in test_images ] ) test_emb = vgg_facenet . predict ( test . transpose ( 0 , 3 , 1 , 2 ) ) print ( test . shape , test_emb . shape )
1341	probs = [ ] for dist in vector_distances : prob = np . sum ( vector_distances [ np . where ( vector_distances <= dist ) [ 0 ] ] ) / total_sum probs . append ( 1 - prob )
1342	vals = plt . hist ( train [ 'TransactionDT' ] / ( 3600 * 24 ) , bins = 182 * 24 ) plt . xlim ( 0 , 3 ) plt . xlabel ( 'Days' ) plt . ylabel ( 'Number of transactions' ) plt . ylim ( 0 , 500 )
1343	fraud_fracHr = train . groupby ( 'TransactionHour' ) [ 'isFraud' ] . mean ( ) plt . plot ( fraud_fracHr ) plt . ylim ( 0.02 , 0.12 )
1344	syllable_sincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 0 ] . progress_apply ( textstat . syllable_count ) ) syllable_insincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 1 ] . progress_apply ( textstat . syllable_count ) ) plot_readability ( syllable_sincere , syllable_insincere , "Syllable Analysis" , 5 )
1345	fre_sincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 0 ] . progress_apply ( textstat . flesch_reading_ease ) ) fre_insincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 1 ] . progress_apply ( textstat . flesch_reading_ease ) ) plot_readability ( fre_sincere , fre_insincere , "Flesch Reading Ease" , 20 )
1346	def consensus_all ( text ) : return textstat . text_standard ( text , float_output = True ) con_sincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 0 ] . progress_apply ( consensus_all ) ) con_insincere = np . array ( quora_train [ "question_text" ] [ quora_train [ "target" ] == 1 ] . progress_apply ( consensus_all ) ) plot_readability ( con_sincere , con_insincere , "Readability Consensus based upon all the above tests" , 2 )
1347	vectorizer_sincere = CountVectorizer ( min_df = 5 , max_df = 0.9 , stop_words = 'english' , lowercase = True , token_pattern = '[a-zA-Z\-][a-zA-Z\-]{2,}' ) sincere_questions_vectorized = vectorizer_sincere . fit_transform ( sincere_questions ) vectorizer_insincere = CountVectorizer ( min_df = 5 , max_df = 0.9 , stop_words = 'english' , lowercase = True , token_pattern = '[a-zA-Z\-][a-zA-Z\-]{2,}' ) insincere_questions_vectorized = vectorizer_insincere . fit_transform ( insincere_questions )
1348	lda_sincere = LatentDirichletAllocation ( n_components = 10 , max_iter = 5 , learning_method = 'online' , verbose = True ) sincere_lda = lda_sincere . fit_transform ( sincere_questions_vectorized ) lda_insincere = LatentDirichletAllocation ( n_components = 10 , max_iter = 5 , learning_method = 'online' , verbose = True ) insincere_lda = lda_insincere . fit_transform ( insincere_questions_vectorized )
1349	pyLDAvis . enable_notebook ( ) dash = pyLDAvis . sklearn . prepare ( lda_sincere , sincere_questions_vectorized , vectorizer_sincere , mds = 'tsne' ) dash
1350	pyLDAvis . enable_notebook ( ) dash = pyLDAvis . sklearn . prepare ( lda_insincere , insincere_questions_vectorized , vectorizer_insincere , mds = 'tsne' ) dash
1351	print ( 'Number of Unique Birds in the the Dataset is: ' + str ( train_df [ 'ebird_code' ] . nunique ( ) ) ) train_df [ 'ebird_code' ] . value_counts ( ) . plot . bar ( )
1352	aldfly = '../input/birdsong-recognition/train_audio/aldfly/XC134874.mp3' y , sr = librosa . load ( aldfly , sr = None ) ipd . Audio ( aldfly )
1353	Y = librosa . stft ( y ) Xdb = librosa . amplitude_to_db ( abs ( Y ) ) plt . figure ( figsize = ( 14 , 5 ) ) librosa . display . specshow ( Xdb , sr = sr , x_axis = 'time' , y_axis = 'hz' ) plt . colorbar ( )
1354	sample_df = pd . read_csv ( PATH / 'sample_submission.csv' ) learn . data . add_test ( ImageList . from_df ( sample_df , PATH , folder = 'test_images' , suffix = '.png' ) ) preds , y = learn . get_preds ( DatasetType . Test ) sample_df . diagnosis = preds . argmax ( 1 ) sample_df . head ( ) sample_df . to_csv ( 'submission.csv' , index = False )
1355	fig = plt . figure ( figsize = ( 15 , 10 ) ) columns = 5 ; rows = 4 for i in range ( 1 , columns * rows + 1 ) : ds = pydicom . dcmread ( train_images_dir + train_images [ i ] ) fig . add_subplot ( rows , columns , i ) plt . imshow ( - ds . pixel_array , cmap = plt . cm . bone ) fig . add_subplot
1356	df = pd . DataFrame ( train . benign_malignant . value_counts ( ) ) df [ 'name' ] = df . index alt . Chart ( df ) . mark_bar ( ) . encode ( x = 'name' , y = 'benign_malignant' , tooltip = [ "name" , "benign_malignant" ] ) . interactive ( )
1357	fig = plt . figure ( figsize = ( 15 , 10 ) ) columns = 4 ; rows = 5 for i in range ( 1 , columns * rows + 1 ) : ds = pydicom . dcmread ( train_images_dir + train [ train [ 'benign_malignant' ] == 'benign' ] [ 'image_name' ] [ i ] + '.dcm' ) fig . add_subplot ( rows , columns , i ) plt . imshow ( ds . pixel_array , cmap = plt . cm . bone ) fig . add_subplot
1358	df = pd . DataFrame ( train . anatom_site_general_challenge . value_counts ( ) ) df [ 'name' ] = df . index alt . Chart ( df ) . mark_bar ( ) . encode ( x = 'name' , y = 'anatom_site_general_challenge' , tooltip = [ "name" , "anatom_site_general_challenge" ] ) . interactive ( )
1359	df = pd . DataFrame ( train . age_approx . value_counts ( ) ) df [ 'name' ] = df . index alt . Chart ( df ) . mark_bar ( ) . encode ( x = 'name' , y = 'age_approx' , tooltip = [ "name" , "age_approx" ] ) . interactive ( )
1360	fig = plt . figure ( figsize = ( 22 , 6 ) ) test [ "age_approx" ] . value_counts ( normalize = True ) . to_frame ( ) . iplot ( kind = 'bar' , yTitle = 'Percentage' , linecolor = 'black' , opacity = 0.7 , color = 'red' , theme = 'pearl' , bargap = 0.8 , gridcolor = 'white' , title = 'It does not exactly follow the same distribution in test though.' ) plt . show ( )
1361	partitions = [ ] for train_idx , test_idx in splitter . split ( train . index . values ) : partition = { } partition [ "train" ] = train . image_name . values [ train_idx ] partition [ "validation" ] = train . image_name . values [ test_idx ] partitions . append ( partition ) print ( "TRAIN:" , train_idx , "TEST:" , test_idx ) print ( "TRAIN:" , len ( train_idx ) , "TEST:" , len ( test_idx ) )
1362	class Config : BATCH_SIZE = 8 EPOCHS = 40 WARMUP_EPOCHS = 2 LEARNING_RATE = 1e-4 WARMUP_LEARNING_RATE = 1e-3 HEIGHT = 224 WIDTH = 224 CANAL = 3 N_CLASSES = train [ 'target' ] . nunique ( ) ES_PATIENCE = 5 RLROP_PATIENCE = 3 DECAY_DROP = 0.5
1363	import numpy as np import pandas as pd import matplotlib . pyplot as plt import seaborn as sns from scipy . sparse import csr_matrix from collections import Counter
1364	test = pd . merge ( test , prior [ [ 'user_id' , 'order_id' , 'order_number' ] ] , how = 'left' , on = 'user_id' ) print ( test . shape ) test . head ( )
1365	test = pd . merge ( test , order_prior , left_on = 'order_id_y' , right_on = 'order_id' ) test [ 'new_order_id' ] = test [ 'order_id_x' ] test [ 'prior_order_id' ] = test [ 'order_id_y' ] test = test . drop ( [ 'order_id_x' , 'order_id_y' ] , axis = 1 ) del [ orders , order_prior , train ]
1366	product_list = test [ test [ 'reordered' ] == 1 ] . groupby ( [ 'user_id' , 'order_number_x' , 'new_order_id' ] ) . agg ( { 'product_id' : lambda x : tuple ( x ) , 'aisles' : lambda x : tuple ( x ) } ) product_list = pd . DataFrame ( product_list . reset_index ( ) ) product_list [ 'num_products_reordered' ] = product_list . product_id . apply ( len ) product_list . head ( 15 )
1367	indptr = [ 0 ] indices = [ ] data = [ ] column_position = { } for order in product_list [ 'product_id' ] : for product in order : index = column_position . setdefault ( product , len ( column_position ) ) indices . append ( index ) data . append ( 1 ) indptr . append ( len ( indices ) ) prod_matrix = csr_matrix ( ( data , indices , indptr ) , dtype = int )
1368	from sklearn . decomposition import NMF from sklearn . preprocessing import normalize nmf = NMF ( n_components = 50 , random_state = 42 ) model = nmf . fit ( prod_matrix ) H = model . components_ model . components_ . shape
1369	W = model . transform ( prod_matrix ) user_data = pd . DataFrame ( normalize ( W ) , index = product_list [ 'user_id' ] ) idx = user_data . dot ( user_data . iloc [ 0 ] ) . nlargest ( 5 ) . index user_data . dot ( user_data . iloc [ 0 ] ) . nlargest ( 5 )
1370	def id_values ( row , overlap ) : for key , value in row . items ( ) : if key in overlap : print ( key , value )
1371	def filter_signal ( signal , threshold = 1e8 ) : fourier = rfft ( signal ) frequencies = rfftfreq ( signal . size , d = 1e-5 ) fourier [ frequencies > threshold ] = 0 return irfft ( fourier )
1372	for threshold in [ 1e3 , 5e3 , 1e4 , 5e4 ] : filtered = filter_signal ( signal , threshold = threshold ) plt . figure ( figsize = ( 15 , 10 ) ) plt . plot ( signal , label = 'Raw' ) plt . plot ( filtered , label = 'Filtered' ) plt . legend ( ) plt . title ( f"FFT Denoising with threshold = {threshold :.0e}" , size = 15 ) plt . show ( )
1373	import pandas as pd import numpy as np import matplotlib . pyplot as plt import seaborn as sns plt . style . use ( 'dark_background' ) from IPython . display import display from IPython . core . interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = "all" import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
1374	print ( "Shape of data is " ) train . shape print ( 'The total number of movies are' , train . shape [ 0 ] )
1375	plt . figure ( figsize = ( 20 , 12 ) ) edgecolor = ( 0 , 0 , 0 ) , sns . countplot ( train [ 'release_year' ] . sort_values ( ) , palette = "Dark2" , edgecolor = ( 0 , 0 , 0 ) ) plt . title ( "Movie Release count by Year" , fontsize = 20 ) plt . xlabel ( 'Release Year' ) plt . ylabel ( 'Number of Movies Release' ) plt . xticks ( fontsize = 12 , rotation = 90 ) plt . show ( )
1376	plt . figure ( figsize = ( 20 , 12 ) ) edgecolor = ( 0 , 0 , 0 ) , sns . distplot ( train [ 'popularity' ] , kde = False ) plt . title ( "Movie Popularity Count" , fontsize = 20 ) plt . xlabel ( 'Popularity' ) plt . ylabel ( 'Count' ) plt . xticks ( fontsize = 12 , rotation = 90 ) plt . show ( )
1377	plt . figure ( figsize = ( 20 , 12 ) ) edgecolor = ( 0 , 0 , 0 ) , sns . countplot ( train [ 'release_month' ] . sort_values ( ) , palette = "Dark2" , edgecolor = ( 0 , 0 , 0 ) ) plt . title ( "Movie Release count by Month" , fontsize = 20 ) plt . xlabel ( 'Release Month' ) plt . ylabel ( 'Number of Movies Release' ) plt . xticks ( fontsize = 12 ) plt . show ( )
1378	plt . figure ( figsize = ( 20 , 12 ) ) edgecolor = ( 0 , 0 , 0 ) , sns . countplot ( train [ 'release_day' ] . sort_values ( ) , palette = "Dark2" , edgecolor = ( 0 , 0 , 0 ) ) plt . title ( "Movie Release count by Day of Month" , fontsize = 20 ) plt . xlabel ( 'Release Day' ) plt . ylabel ( 'Number of Movies Release' ) plt . xticks ( fontsize = 12 ) plt . show ( )
1379	plt . figure ( figsize = ( 20 , 12 ) ) sns . countplot ( train [ 'release_weekday' ] . sort_values ( ) , palette = 'Dark2' ) loc = np . array ( range ( len ( train [ 'release_weekday' ] . unique ( ) ) ) ) day_labels = [ 'Mon' , 'Tue' , 'Wed' , 'Thu' , 'Fri' , 'Sat' , 'Sun' ] plt . xlabel ( 'Release Day of Week' ) plt . ylabel ( 'Number of Movies Release' ) plt . xticks ( loc , day_labels , fontsize = 12 ) plt . show ( )
1380	def sieve_eratosthenes ( n ) : primes = [ False , False ] + [ True for i in range ( n - 1 ) ] p = 2 while ( p * p <= n ) : if ( primes [ p ] == True ) : for i in range ( p * 2 , n + 1 , p ) : primes [ i ] = False p += 1 return primes
1381	def dist_matrix ( coords , i , penalize = False ) : begin = np . array ( [ df . X [ i ] , df . Y [ i ] ] ) [ : , np . newaxis ] mat = coords - begin if penalize : return np . linalg . norm ( mat , ord = 2 , axis = 0 ) * penalization else : return np . linalg . norm ( mat , ord = 2 , axis = 0 )
1382	def build_vocab ( texts ) : sentences = texts . apply ( lambda x : x . split ( ) ) . values vocab = { } for sentence in sentences : for word in sentence : try : vocab [ word ] += 1 except KeyError : vocab [ word ] = 1 return vocab
1383	def add_lower ( embedding , vocab ) : count = 0 for word in vocab : if word in embedding and word . lower ( ) not in embedding : embedding [ word . lower ( ) ] = embedding [ word ] count += 1 print ( f"Added {count} words to embedding" )
1384	def clean_contractions ( text , mapping ) : specials = [ "’" , "‘" , "´" , "`" ] for s in specials : text = text . replace ( s , "'" ) text = ' ' . join ( [ mapping [ t ] if t in mapping else t for t in text . split ( " " ) ] ) return text
1385	def load_embed ( file ) : def get_coefs ( word , * arr ) : return word , np . asarray ( arr , dtype = 'float32' ) if file == '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec' : embeddings_index = dict ( get_coefs ( * o . split ( " " ) ) for o in open ( file ) if len ( o ) > 100 ) else : embeddings_index = dict ( get_coefs ( * o . split ( " " ) ) for o in open ( file , encoding = 'latin' ) ) return embeddings_index
1386	def build_vocab ( texts ) : sentences = texts . apply ( lambda x : x . split ( ) ) . values vocab = { } for sentence in sentences : for word in sentence : try : vocab [ word ] += 1 except KeyError : vocab [ word ] = 1 return vocab
1387	def add_lower ( embedding , vocab ) : count = 0 for word in vocab : if word in embedding and word . lower ( ) not in embedding : embedding [ word . lower ( ) ] = embedding [ word ] count += 1 print ( f"Added {count} words to embedding" )
1388	def clean_contractions ( text , mapping ) : specials = [ "’" , "‘" , "´" , "`" ] for s in specials : text = text . replace ( s , "'" ) text = ' ' . join ( [ mapping [ t ] if t in mapping else t for t in text . split ( " " ) ] ) return text
1389	train [ 'treated_question' ] = train [ 'question_text' ] . apply ( lambda x : x . lower ( ) ) train [ 'treated_question' ] = train [ 'treated_question' ] . apply ( lambda x : clean_contractions ( x , contraction_mapping ) ) train [ 'treated_question' ] = train [ 'treated_question' ] . apply ( lambda x : clean_special_chars ( x , punct , punct_mapping ) ) train [ 'treated_question' ] = train [ 'treated_question' ] . apply ( lambda x : correct_spelling ( x , mispell_dict ) )
1390	def make_treated_data ( X ) : t = Tokenizer ( num_words = len_voc , filters = '' ) t . fit_on_texts ( X ) X = t . texts_to_sequences ( X ) X = pad_sequences ( X , maxlen = max_len ) return X , t . word_index
1391	def madev ( d , axis = None ) : return np . mean ( np . absolute ( d - np . mean ( d , axis ) ) , axis )
1392	for wav in pywt . wavelist ( ) : try : filtered = wavelet_denoising ( signal , wavelet = wav , level = 1 ) except : pass plt . figure ( figsize = ( 10 , 6 ) ) plt . plot ( signal , label = 'Raw' ) plt . plot ( filtered , label = 'Filtered' ) plt . legend ( ) plt . title ( f"DWT Denoising with {wav} Wavelet" , size = 15 ) plt . show ( )
1393	plt . figure ( figsize = ( 15 , 10 ) ) sns . countplot ( train_df [ 'event' ] ) plt . xlabel ( "State of the pilot" , fontsize = 12 ) plt . ylabel ( "Count" , fontsize = 12 ) plt . title ( "Target repartition" , fontsize = 15 ) plt . show ( )
1394	plt . figure ( figsize = ( 15 , 10 ) ) sns . countplot ( 'event' , hue = 'seat' , data = train_df ) plt . xlabel ( "Seat and state of the pilot" , fontsize = 12 ) plt . ylabel ( "Count (log)" , fontsize = 12 ) plt . yscale ( 'log' ) plt . title ( "Left seat or right seat ?" , fontsize = 15 ) plt . show ( )
1395	plt . figure ( figsize = ( 15 , 10 ) ) sns . violinplot ( x = 'event' , y = 'time' , data = train_df . sample ( 50000 ) ) plt . ylabel ( "Time (s)" , fontsize = 12 ) plt . xlabel ( "Event" , fontsize = 12 ) plt . title ( "Which time do events occur at ?" , fontsize = 15 ) plt . show ( )
1396	plt . figure ( figsize = ( 15 , 10 ) ) sns . violinplot ( x = 'event' , y = 'ecg' , data = train_df . sample ( 50000 ) ) plt . ylabel ( "Electrocardiogram Signal (µV)" , fontsize = 12 ) plt . xlabel ( "Event" , fontsize = 12 ) plt . title ( "Electrocardiogram signal influence" , fontsize = 15 ) plt . show ( )
1397	plt . figure ( figsize = ( 15 , 10 ) ) sns . violinplot ( x = 'event' , y = 'r' , data = train_df . sample ( 50000 ) ) plt . ylabel ( "Respiration Signal (µV)" , fontsize = 12 ) plt . xlabel ( "Event" , fontsize = 12 ) plt . title ( "Respiration influence" , fontsize = 15 ) plt . show ( )
1398	plt . figure ( figsize = ( 15 , 10 ) ) sns . violinplot ( x = 'event' , y = 'gsr' , data = train_df . sample ( 50000 ) ) plt . ylabel ( "Electrodermal activity measure (µV)" , fontsize = 12 ) plt . xlabel ( "Event" , fontsize = 12 ) plt . title ( "Electrodermal activity influence" , fontsize = 15 ) plt . show ( )
1399	gc . collect ( ) df = df . assign ( actor = lambda x : actor ( x , maxima ) ) . drop ( original_features , axis = 1 ) df . head ( )
1400	duplicates_dist = ( duplicates . groupby ( [ 'click_time' , 'actor' ] ) . is_attributed . agg ( [ 'count' , 'mean' ] ) . groupby ( 'count' ) . agg ( [ 'count' , 'mean' ] ) ) duplicates_dist = ( duplicates_dist . rename ( columns = { 'mean' : 'training' } , level = 0 ) . rename ( columns = { 'mean' : 'avg attr rate' } , level = 1 ) . rename_axis ( '' ) ) del ( df , duplicates ) gc . collect ( ) duplicates_dist
1401	df = load_data ( 'test' ) maxima = df . describe ( ) . loc [ 'max' , : ] . astype ( np . uint32 ) df . info ( )
1402	analysis . plot_overlap ( 'ip' ) analysis . plot_overlap ( 'app' ) analysis . plot_overlap ( 'channel' ) analysis . plot_overlap ( 'device' ) analysis . plot_overlap ( 'os' )
1403	import warnings warnings . filterwarnings ( "ignore" ) import numpy as np import random as rd import pandas as pd import datetime from operator import add import matplotlib . pyplot as plt import seaborn import folium from sklearn . cluster import AgglomerativeClustering as AggClust from scipy . cluster . hierarchy import ward , dendrogram from statsmodels . tsa . arima_model import ARIMA from statsmodels . tsa . statespace . sarimax import SARIMAX from pandas . plotting import autocorrelation_plot from statsmodels . tsa . stattools import adfuller , acf , pacf , arma_order_select_ic
1404	ts = sales . loc [ sales [ 'store_nbr' ] == 47 , [ 'date' , 'transactions' ] ] . set_index ( 'date' ) ts = ts . transactions . astype ( 'float' ) plt . figure ( figsize = ( 12 , 12 ) ) plt . title ( 'Daily transactions in store plt.xlabel(' time ') plt.ylabel(' Number of transactions ' ) plt . plot ( ts ) ;
1405	plt . figure ( figsize = ( 12 , 12 ) ) plt . plot ( ts . rolling ( window = 30 , center = False ) . mean ( ) , label = 'Rolling Mean' ) ; plt . plot ( ts . rolling ( window = 30 , center = False ) . std ( ) , label = 'Rolling sd' ) ; plt . legend ( ) ;
1406	plt . figure ( figsize = ( 12 , 6 ) ) autocorrelation_plot ( ts ) ; plt . figure ( figsize = ( 12 , 6 ) ) autocorrelation_plot ( ts ) ; plt . xlim ( xmax = 100 ) ; plt . figure ( figsize = ( 12 , 6 ) ) autocorrelation_plot ( ts ) ; plt . xlim ( xmax = 10 ) ;
1407	plt . figure ( figsize = ( 12 , 6 ) ) plt . subplot plt . plot ( ts ) ; plt . plot ( model_fit . fittedvalues , alpha = .7 ) ;
1408	plt . figure ( figsize = ( 12 , 12 ) ) x = abs ( ts [ - forecast_len : ] - predictions_series ) seaborn . distplot ( x , norm_hist = False , rug = True , kde = False ) ;
1409	plt . figure ( figsize = ( 12 , 6 ) ) plt . subplot ( 1 , 2 , 1 ) seaborn . heatmap ( Means2_norm . iloc [ : , 0 : 7 ] , cmap = 'Oranges' ) ; plt . subplot ( 1 , 2 , 2 ) seaborn . heatmap ( Means2_norm . iloc [ : , 7 : 14 ] , cmap = 'Oranges' ) ;
1410	fig = plt . figure ( figsize = ( 15 , 15 ) ) ax = fig . add_subplot ( 1 , 1 , 1 ) dendrogram ( ward ( Means2_norm ) , ax = ax ) ax . tick_params ( axis = 'x' , which = 'major' , labelsize = 15 ) ax . tick_params ( axis = 'y' , which = 'major' , labelsize = 8 ) plt . show ( )
1411	map_Ecuador = folium . Map ( location = [ - 1.233333 , - 78.516667 ] , zoom_start = 7 ) [ add_city_map ( x , y ) for x , y in zip ( stores . city , stores . new_cluster ) ] map_Ecuador
1412	cities = pd . read_csv ( '../input/cities.csv' ) cities [ 'isPrime' ] = cities . CityId . apply ( isprime ) prime_cities = cities . loc [ cities . isPrime ]
1413	def concorde_tsp ( seed = 42 ) : cities = pd . read_csv ( '../input/cities.csv' ) solver = TSPSolver . from_data ( cities . X , cities . Y , norm = "EUC_2D" ) tour_data = solver . solve ( time_bound = 60.0 , verbose = True , random_seed = seed ) if tour_data . found_tour : path = np . append ( tour_data . tour , [ 0 ] ) make_submission ( 'concorde' , path ) return path else : return None path_cc = concorde_tsp ( )
1414	cities = pd . read_csv ( '../input/cities.csv' ) cities [ 'isPrime' ] = cities . CityId . apply ( isprime ) prime_cities = cities . loc [ ( cities . CityId == 0 ) | ( cities . isPrime ) ] solver = TSPSolver . from_data ( prime_cities . X , prime_cities . Y , norm = "EUC_2D" ) tour_data = solver . solve ( time_bound = 5.0 , verbose = True , random_seed = 42 ) prime_path = np . append ( tour_data . tour , [ 0 ] )
1415	import pandas as pd import numpy as np dataset = pd . read_csv ( '../input/train.csv' ) testset = pd . read_csv ( '../input/test.csv' ) X = dataset . iloc [ : , 2 : ] . values y = dataset . iloc [ : , 1 ] . values X_Test = testset . iloc [ : , 1 : ] . values y = np . log ( y )
1416	from sklearn . feature_selection import VarianceThreshold feature_selector = VarianceThreshold ( ) X = feature_selector . fit_transform ( X ) X_Test = feature_selector . transform ( X_Test )
1417	from xgboost import XGBRegressor regressor = XGBRegressor ( n_estimators = 300 ) regressor . fit ( X , y )
1418	submission = pd . DataFrame ( ) submission [ 'ID' ] = testset [ 'ID' ] submission [ 'target' ] = results submission . to_csv ( 'submission.csv' , index = False )
1419	import glob import matplotlib . pyplot as plt import seaborn as sns import pandas as pd import pydicom import numpy as np import warnings import multiprocessing import os from skimage import morphology from skimage import feature from skimage import measure from skimage import util from skimage import transform warnings . filterwarnings ( 'ignore' )
1420	boxes_per_patient = tr . groupby ( 'patientId' ) [ 'Target' ] . sum ( ) ax = ( boxes_per_patient > 0 ) . value_counts ( ) . plot . bar ( ) _ = ax . set_title ( 'Are the classes imbalanced?' ) _ = ax . set_xlabel ( 'Has Pneumonia' ) _ = ax . set_ylabel ( 'Count' ) _ = ax . xaxis . set_tick_params ( rotation = 0 )
1421	ax = boxes_per_patient . value_counts ( ) . plot . bar ( ) _ = ax . set_title ( 'How many cases are there per image?' ) _ = ax . set_xlabel ( 'Number of cases' ) _ = ax . xaxis . set_tick_params ( rotation = 0 )
1422	centers = ( tr . dropna ( subset = [ 'x' ] ) . assign ( center_x = tr . x + tr . width / 2 , center_y = tr . y + tr . height / 2 ) ) ax = sns . jointplot ( "center_x" , "center_y" , data = centers , height = 9 , alpha = 0.1 ) _ = ax . fig . suptitle ( "Where is Pneumonia located?" , y = 1.01 )
1423	g = sns . FacetGrid ( col = 'Target' , hue = 'gender' , data = tr . drop_duplicates ( subset = [ 'patientId' ] ) , height = 9 , palette = dict ( F = "red" , M = "blue" ) ) _ = g . map ( sns . distplot , 'age' , hist_kws = { 'alpha' : 0.3 } ) . add_legend ( ) _ = g . fig . suptitle ( "What is the age distribution by gender and target?" , y = 1.02 , fontsize = 20 )
1424	areas = tr . dropna ( subset = [ 'area' ] ) g = sns . FacetGrid ( hue = 'gender' , data = areas , height = 9 , palette = dict ( F = "red" , M = "blue" ) , aspect = 1.4 ) _ = g . map ( sns . distplot , 'area' , hist_kws = { 'alpha' : 0.3 } ) . add_legend ( ) _ = g . fig . suptitle ( 'What are the areas of the bounding boxes by gender?' , y = 1.01 )
1425	pixel_vc = tr . drop_duplicates ( 'patientId' ) [ 'pixel_spacing' ] . value_counts ( ) ax = pixel_vc . iloc [ : 6 ] . plot . bar ( ) _ = ax . set_xticklabels ( [ f'{ps:.4f}' for ps in pixel_vc . index [ : 6 ] ] ) _ = ax . set_xlabel ( 'Pixel Spacing' ) _ = ax . set_ylabel ( 'Count' ) _ = ax . set_title ( 'How is the pixel spacing distributed?' , fontsize = 20 )
1426	areas_with_count = areas . merge ( pd . DataFrame ( boxes_per_patient ) . rename ( columns = { 'Target' : 'bbox_count' } ) , on = 'patientId' ) g = sns . FacetGrid ( hue = 'bbox_count' , data = areas_with_count , height = 8 , aspect = 1.4 ) _ = g . map ( sns . distplot , 'area' ) . add_legend ( ) _ = g . fig . suptitle ( "How are the bounding box areas distributed by the number of boxes?" , y = 1.01 )
1427	ax = sns . boxplot ( tr . mean_black_pixels ) _ = ax . set_xlabel ( 'Percentage of black pixels' ) _ = ax . set_title ( 'Are there images with mostly black pixels?' )
1428	ax = sns . distplot ( tr [ 'aspect_ratio' ] . dropna ( ) , norm_hist = True ) _ = ax . set_title ( "What does the distribution of bounding aspect ratios look like?" ) _ = ax . set_xlabel ( "Aspect Ratio" )
1429	g = sns . relplot ( x = 'area' , y = 'aspect_ratio' , data = tr . dropna ( subset = [ 'area' , 'aspect_ratio' ] ) , height = 8 , alpha = 0.8 , aspect = 1.4 , ) _ = g . fig . suptitle ( "Is there a relationship between the bounding box's aspect ratio and area?" , y = 1.005 )
1430	import pandas as pd import numpy as np trainset = pd . read_csv ( '../input/train.csv' ) testset = pd . read_csv ( '../input/test.csv' ) y = trainset . iloc [ : , 1 ] . values trainset = trainset . iloc [ : , 2 : ] testset = testset . iloc [ : , 1 : ]
1431	from sklearn . discriminant_analysis import LinearDiscriminantAnalysis as LDA lda = LDA ( ) X = lda . fit_transform ( X , y . astype ( int ) ) X_Test = lda . transform ( X_Test )
1432	from sklearn . model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 42 ) import lightgbm as lgb train = lgb . Dataset ( X_train , label = np . log1p ( y_train ) ) test = lgb . Dataset ( X_test , label = np . log1p ( y_test ) )
1433	params = { 'boosting_type' : 'gbdt' , 'metric' : 'rmse' , 'zero_as_missing' : True } regressor = lgb . train ( params , train , 3000 , valid_sets = [ test ] , early_stopping_rounds = 100 , verbose_eval = 100 )
1434	class Accuracy ( nn . Module ) : def __init__ ( self , threshold = 0.5 ) : super ( ) . __init__ ( ) self . threshold = threshold def forward ( self , y_true , y_pred ) : y_pred = ( y_pred > self . threshold ) . int ( ) y_true = y_true . int ( ) return ( y_pred == y_true ) . float ( ) . mean ( )
1435	train [ 'nouns_vs_length' ] = train [ 'nouns' ] / train [ 'total_length' ] train [ 'adjectives_vs_length' ] = train [ 'adjectives' ] / train [ 'total_length' ] train [ 'verbs_vs_length' ] = train [ 'verbs' ] / train [ 'total_length' ] train [ 'nouns_vs_words' ] = train [ 'nouns' ] / train [ 'words' ] train [ 'adjectives_vs_words' ] = train [ 'adjectives' ] / train [ 'words' ] train [ 'verbs_vs_words' ] = train [ 'verbs' ] / train [ 'words' ]
1436	import seaborn as sns fig , ax = plt . subplots ( figsize = ( 10 , 10 ) ) sns . heatmap ( train_correlations , annot = True , vmin = - 0.23 , vmax = 0.23 , center = 0.0 , ax = ax )
1437	import pandas as pd import matplotlib . pyplot as plt import seaborn as sns import numpy as np from scipy . stats import norm from sklearn . preprocessing import StandardScaler from scipy import stats import warnings warnings . filterwarnings ( 'ignore' ) import gc from sklearn . model_selection import KFold from sklearn . metrics import mean_squared_error import lightgbm as lgb import xgboost as xgb from scipy . optimize import minimize
1438	def scale_data ( X , scaler = None ) : if not scaler : scaler = MinMaxScaler ( feature_range = ( - 1 , 1 ) ) scaler . fit ( X ) X = scaler . transform ( X ) return X , scaler
1439	sss = StratifiedShuffleSplit ( target , random_state = 1001 , test_size = 0.75 ) for train_index , test_index in sss : break X_train , y_train = train [ train_index ] , target [ train_index ] del train , target gc . collect ( ) dtrain = xgb . DMatrix ( X_train , label = y_train )
1440	XGB_BO = BayesianOptimization ( XGB_CV , { 'max_depth' : ( 2 , 12 ) , 'gamma' : ( 0.001 , 10.0 ) , 'min_child_weight' : ( 0 , 20 ) , 'max_delta_step' : ( 0 , 10 ) , 'subsample' : ( 0.4 , 1.0 ) , 'colsample_bytree' : ( 0.4 , 1.0 ) } )
1441	print ( '-' * 130 ) print ( '-' * 130 , file = log_file ) log_file . flush ( ) with warnings . catch_warnings ( ) : warnings . filterwarnings ( 'ignore' ) XGB_BO . maximize ( init_points = 2 , n_iter = 5 , acq = 'ei' , xi = 0.0 )
1442	print ( '\nLoading files ...' ) train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) X = train . drop ( [ 'id' , 'target' ] , axis = 1 ) . values y = train [ 'target' ] . values . astype ( np . int8 ) target_names = np . unique ( y ) print ( '\nThere are %d unique target valuess in this dataset:' % ( len ( target_names ) ) , target_names )
1443	n_comp = 20 print ( '\nRunning PCA ...' ) pca = PCA ( n_components = n_comp , svd_solver = 'full' , random_state = 1001 ) X_pca = pca . fit_transform ( X ) print ( 'Explained variance: %.4f' % pca . explained_variance_ratio_ . sum ( ) ) print ( 'Individual variance contributions:' ) for j in range ( n_comp ) : print ( pca . explained_variance_ratio_ [ j ] )
1444	rfc = RandomForestClassifier ( n_estimators = 200 , n_jobs = 4 , class_weight = 'balanced' , max_depth = 6 ) boruta_selector = BorutaPy ( rfc , n_estimators = 'auto' , verbose = 2 ) start_time = timer ( None ) boruta_selector . fit ( X , y ) timer ( start_time )
1445	folds = 5 step = 2 rfc = RandomForestClassifier ( n_estimators = 100 , max_features = 'sqrt' , max_depth = 10 , n_jobs = 4 ) rfecv = RFECV ( estimator = rfc , step = step , cv = StratifiedKFold ( n_splits = folds , shuffle = False , random_state = 1001 ) . split ( X , y ) , scoring = 'roc_auc' , n_jobs = 1 , verbose = 2 )
1446	starttime = timer ( None ) start_time = timer ( None ) rfecv . fit ( X , y ) timer ( start_time )
1447	print ( '\n Optimal number of features: %d' % rfecv . n_features_ ) sel_features = [ f for f , s in zip ( all_features , rfecv . support_ ) if s ] print ( '\n The selected features are {}:' . format ( sel_features ) )
1448	plt . figure ( figsize = ( 12 , 9 ) ) plt . xlabel ( 'Number of features tested x 2' ) plt . ylabel ( 'Cross-validation score (AUC)' ) plt . plot ( range ( 1 , len ( rfecv . grid_scores_ ) + 1 ) , rfecv . grid_scores_ ) plt . savefig ( 'Porto-RFECV-01.png' , dpi = 150 ) plt . show ( )
1449	ranking = pd . DataFrame ( { 'Features' : all_features } ) ranking [ 'Rank' ] = np . asarray ( rfecv . ranking_ ) ranking . sort_values ( 'Rank' , inplace = True ) ranking . to_csv ( 'Porto-RFECV-ranking-01.csv' , index = False )
1450	score = round ( ( np . max ( rfecv . grid_scores_ ) * 2 - 1 ) , 5 ) test [ 'target' ] = rfecv . predict_proba ( X_test ) [ : , 1 ] test = test [ [ 'id' , 'target' ] ] now = datetime . now ( ) sub_file = 'submission_5fold-RFECV-RandomForest-01_' + str ( score ) + '_' + str ( now . strftime ( "%Y-%m-%d-%H-%M" ) ) + '.csv' print ( "\n Writing submission file: %s" % sub_file ) test . to_csv ( sub_file , index = False ) timer ( starttime )
1451	params = { 'min_child_weight' : [ 1 , 5 , 10 ] , 'gamma' : [ 0.5 , 1 , 1.5 , 2 , 5 ] , 'subsample' : [ 0.6 , 0.8 , 1.0 ] , 'colsample_bytree' : [ 0.6 , 0.8 , 1.0 ] , 'max_depth' : [ 3 , 4 , 5 ] }
1452	xgb = XGBClassifier ( learning_rate = 0.02 , n_estimators = 600 , objective = 'binary:logistic' , silent = True , nthread = 1 )
1453	folds = 3 param_comb = 5 skf = StratifiedKFold ( n_splits = folds , shuffle = True , random_state = 1001 ) random_search = RandomizedSearchCV ( xgb , param_distributions = params , n_iter = param_comb , scoring = 'roc_auc' , n_jobs = 4 , cv = skf . split ( X , Y ) , verbose = 3 , random_state = 1001 ) start_time = timer ( None ) random_search . fit ( X , Y ) timer ( start_time )
1454	print ( '\n All results:' ) print ( random_search . cv_results_ ) print ( '\n Best estimator:' ) print ( random_search . best_estimator_ ) print ( '\n Best normalized gini score for %d-fold search with %d parameter combinations:' % ( folds , param_comb ) ) print ( random_search . best_score_ * 2 - 1 ) print ( '\n Best hyperparameters:' ) print ( random_search . best_params_ ) results = pd . DataFrame ( random_search . cv_results_ ) results . to_csv ( 'xgb-random-grid-search-results-01.csv' , index = False )
1455	y_test = random_search . predict_proba ( test ) results_df = pd . DataFrame ( data = { 'id' : test_df [ 'id' ] , 'target' : y_test [ : , 1 ] } ) results_df . to_csv ( 'submission-random-grid-search-xgb-porto-01.csv' , index = False )
1456	folds = 4 runs = 2 cv_LL = 0 cv_AUC = 0 cv_gini = 0 fpred = [ ] avpred = [ ] avreal = [ ] avids = [ ]
1457	LL_oof = log_loss ( avreal , avpred ) print ( '\n Average Log-loss: %.5f' % ( cv_LL / folds ) ) print ( ' Out-of-fold Log-loss: %.5f' % LL_oof ) AUC_oof = roc_auc_score ( avreal , avpred ) print ( '\n Average AUC: %.5f' % ( cv_AUC / folds ) ) print ( ' Out-of-fold AUC: %.5f' % AUC_oof ) print ( '\n Average normalized gini: %.5f' % ( cv_gini / folds ) ) print ( ' Out-of-fold normalized gini: %.5f' % ( AUC_oof * 2 - 1 ) ) score = str ( round ( ( AUC_oof * 2 - 1 ) , 5 ) ) timer ( starttime ) mpred = pred / folds
1458	print ( ' now = datetime.now() oof_result = pd.DataFrame(avreal, columns=[' target ']) oof_result[' prediction '] = avpred oof_result[' id '] = avids oof_result.sort_values(' id ', ascending=True, inplace=True) oof_result = oof_result.set_index(' id ') sub_file = ' train_5fold - keras - run - 0 1 - v1 - oof_ ' + str(score) + ' _ ' + str(now.strftime(' % Y - % m - % d - % H - % M ')) + ' . csv ' print(' \ n Writing out - of - fold file : % s ' % sub_file) oof_result.to_csv(sub_file, index=True, index_label=' id ' )
1459	result = pd . DataFrame ( mpred , columns = [ 'target' ] ) result [ 'id' ] = te_ids result = result . set_index ( 'id' ) print ( '\n First 10 lines of your 5-fold average prediction:\n' ) print ( result . head ( 10 ) ) sub_file = 'submission_5fold-average-keras-run-01-v1_' + str ( score ) + '_' + str ( now . strftime ( '%Y-%m-%d-%H-%M' ) ) + '.csv' print ( '\n Writing submission: %s' % sub_file ) result . to_csv ( sub_file , index = True , index_label = 'id' )
1460	XGB_BO = BayesianOptimization ( XGB_CV , { 'max_depth' : ( 2 , 6.99 ) , 'gamma' : ( 0.1 , 5 ) , 'min_child_weight' : ( 0 , 5 ) , 'scale_pos_weight' : ( 1 , 5 ) , 'reg_alpha' : ( 0 , 10 ) , 'reg_lambda' : ( 1 , 10 ) , } )
1461	print ( '-' * 126 ) start_time = timer ( None ) with warnings . catch_warnings ( ) : warnings . filterwarnings ( 'ignore' ) XGB_BO . maximize ( init_points = 1 , n_iter = 2 , acq = 'ei' , xi = 0.0 ) timer ( start_time )
1462	print ( '-' * 126 ) print ( '\n Final Results' ) print ( ' Maximum XGBOOST value: %f' % XGB_BO . res [ 'max' ] [ 'max_val' ] ) print ( ' Best XGBOOST parameters: ' , XGB_BO . res [ 'max' ] [ 'max_params' ] ) grid_file = 'Bayes-gini-5fold-XGB-target-enc-run-04-v1-grid.csv' print ( ' Saving grid search parameters to %s' % grid_file ) XGB_BO . points_to_csv ( grid_file )
1463	features = [ 'X118' , 'X127' , 'X47' , 'X315' , 'X311' , 'X179' , 'X314' , 'X232' , 'X29' , 'X263' , 'X261' ]
1464	train = pd . read_csv ( '../input/train.csv' ) test = pd . read_csv ( '../input/test.csv' ) y_clip = np . clip ( train [ 'y' ] . values , a_min = None , a_max = 130 )
1465	mtcnn = MTCNN ( margin = 14 , keep_all = True , factor = 0.5 , device = device ) . eval ( ) resnet = InceptionResnetV1 ( pretrained = 'vggface2' , device = device ) . eval ( )
1466	bias = - 0.2942 weight = 0.68235746 submission = [ ] for filename , x_i in zip ( filenames , X ) : if x_i is not None : prob = 1 / ( 1 + np . exp ( - ( bias + ( weight * x_i ) . mean ( ) ) ) ) else : prob = 0.5 submission . append ( [ os . path . basename ( filename ) , prob ] )
1467	fast_mtcnn = FastMTCNN ( stride = 4 , resize = 1 , margin = 14 , factor = 0.6 , keep_all = True , device = device )
1468	fast_mtcnn = FastMTCNN ( stride = 4 , resize = 0.5 , margin = 14 , factor = 0.5 , keep_all = True , device = device )
1469	from facenet_pytorch import MTCNN detector = MTCNN ( device = device , post_process = False ) def detect_facenet_pytorch ( detector , images , batch_size ) : faces = [ ] for lb in np . arange ( 0 , len ( images ) , batch_size ) : imgs = [ img for img in images [ lb : lb + batch_size ] ] faces . extend ( detector ( imgs ) ) return faces times_facenet_pytorch = [ ] times_facenet_pytorch_nb = [ ]
1470	print ( 'Detecting faces in 540x960 frames' , end = '' ) _ , elapsed = timer ( detector , detect_facenet_pytorch , images_540_960 , 1 ) times_facenet_pytorch_nb . append ( elapsed ) print ( 'Detecting faces in 720x1280 frames' , end = '' ) _ , elapsed = timer ( detector , detect_facenet_pytorch , images_720_1280 , 1 ) times_facenet_pytorch_nb . append ( elapsed ) print ( 'Detecting faces in 1080x1920 frames' , end = '' ) faces , elapsed = timer ( detector , detect_facenet_pytorch , images_1080_1920 , 1 ) times_facenet_pytorch_nb . append ( elapsed )
1471	from dlib import get_frontal_face_detector detector = get_frontal_face_detector ( ) def detect_dlib ( detector , images ) : faces = [ ] for image in images : image_gray = cv2 . cvtColor ( image , cv2 . COLOR_BGR2GRAY ) boxes = detector ( image_gray ) box = boxes [ 0 ] face = image [ box . top ( ) : box . bottom ( ) , box . left ( ) : box . right ( ) ] faces . append ( face ) return faces times_dlib = [ ]
1472	from mtcnn import MTCNN detector = MTCNN ( ) def detect_mtcnn ( detector , images ) : faces = [ ] for image in images : boxes = detector . detect_faces ( image ) box = boxes [ 0 ] [ 'box' ] face = image [ box [ 1 ] : box [ 3 ] + box [ 1 ] , box [ 0 ] : box [ 2 ] + box [ 0 ] ] faces . append ( face ) return faces times_mtcnn = [ ]
1473	device = torch . device ( "cuda" if torch . cuda . is_available ( ) else "cpu" ) TRAIN = "../input/stanford-covid-vaccine/train.json" TEST = "../input/stanford-covid-vaccine/test.json" SS = "../input/stanford-covid-vaccine/sample_submission.csv" train = pd . read_json ( TRAIN , lines = True ) test = pd . read_json ( TEST , lines = True ) sample_sub = pd . read_csv ( SS ) print ( f"Using {device}" )
1474	config = CFILE = "config.json" with open ( CFILE , 'w' ) as handle : handle . write ( config ) cfg = Config ( ) . from_json ( CFILE )
1475	lr = 0.005 epochs = 360 netD = Discriminator ( ) . to ( device ) optimizerD = optim . Adam ( netD . parameters ( ) , lr = lr ) criteria = nn . BCELoss ( ) netD . conv1 . weight = nn . Parameter ( torch . Tensor ( [ [ [ [ - 1.0 ] , [ 1.0 ] ] ] ] ) . to ( device ) ) for param in netD . conv1 . parameters ( ) : param . requires_grad = False
1476	z = zipfile . PyZipFile ( 'images.zip' , mode = 'w' ) d = DogGenerator ( ) for k in range ( 10000 ) : img = d . getDog ( np . random . normal ( 0 , 1 , 100 ) ) f = str ( k ) + '.png' img . save ( f , 'PNG' ) ; z . write ( f ) ; os . remove ( f ) z . close ( )
1477	sales . loc [ sales . item_id == 'HOBBIES_1_001' ] \ . sort_values ( "id" ) \ . head ( )
1478	_cols = list ( sales . columns ) sales . columns = pd . Index ( _cols [ : 6 ] + [ int ( c . replace ( "d_" , "" ) ) for c in _cols [ 6 : ] ] ) del _cols
1479	fig , axes = plt . subplots ( nrows = 5 , figsize = ( 12 , 20 ) ) _ids = sales [ "id" ] . sample ( n = 5 , random_state = 1 ) for i in range ( len ( _ids ) ) : series_from_id ( _ids . iloc [ i ] ) . plot ( ax = axes [ i ] ) del _ids
1480	fig , axes = plt . subplots ( nrows = 5 , figsize = ( 12 , 20 ) ) _ids = sales [ "id" ] . sample ( n = 5 , random_state = 1 ) for i in range ( len ( _ids ) ) : series_from_id_binned ( _ids . iloc [ i ] , bin_every = 7 ) . plot ( ax = axes [ i ] )
1481	fig , axes = plt . subplots ( nrows = 5 , figsize = ( 12 , 20 ) ) random . seed ( 2 ) _ids = sample ( list ( sales [ "item_id" ] . unique ( ) ) , 5 ) for i in range ( len ( _ids ) ) : series_from_item_binned ( _ids [ i ] , bin_every = 7 ) . plot ( ax = axes [ i ] ) axes [ i ] . set_title ( "Item: %s" % _ids [ i ] )
1482	fig , axes = plt . subplots ( nrows = 5 , figsize = ( 12 , 20 ) ) random . seed ( 3 ) _ids = sample ( list ( sales [ "dept_id" ] . unique ( ) ) , 5 ) for i in range ( len ( _ids ) ) : series_from_dept_binned ( _ids [ i ] , bin_every = 7 ) . plot ( ax = axes [ i ] ) axes [ i ] . set_title ( "Department: %s" % _ids [ i ] )
1483	daily_sales_item_lookup . pivot_table ( index = "variable" , columns = "item_id" , values = "value" ) \ . iloc [ : , : 5 ] \ . plot ( figsize = ( 12 , 6 ) )
1484	random . seed ( 1 ) daily_sales_item_lookup_scaled_clustered . loc [ 1 ] \ . T \ . iloc [ : , random . sample ( range ( daily_sales_item_lookup_scaled_clustered . loc [ 1 ] . shape [ 0 ] ) , 10 ) ] \ . plot ( figsize = ( 12 , 6 ) )
1485	random . seed ( 1 ) daily_sales_item_lookup_scaled_clustered . loc [ 2 ] \ . T \ . iloc [ : , random . sample ( range ( daily_sales_item_lookup_scaled_clustered . loc [ 2 ] . shape [ 0 ] ) , 10 ) ] \ . plot ( figsize = ( 12 , 6 ) )
1486	random . seed ( 1 ) daily_sales_item_lookup_scaled_clustered . loc [ 3 ] \ . T \ . iloc [ : , random . sample ( range ( daily_sales_item_lookup_scaled_clustered . loc [ 3 ] . shape [ 0 ] ) , 10 ) ] \ . plot ( figsize = ( 12 , 6 ) )
1487	random . seed ( 1 ) daily_sales_item_lookup_scaled_clustered . loc [ 7 ] \ . T \ . iloc [ : , random . sample ( range ( daily_sales_item_lookup_scaled_clustered . loc [ 7 ] . shape [ 0 ] ) , 10 ) ] \ . plot ( figsize = ( 12 , 6 ) )
1488	fig , [ ax1 , ax2 ] = plt . subplots ( nrows = 2 , figsize = ( 12 , 6 ) ) daily_sales_item_lookup_scaled_weekly [ "HOBBIES_1_062" ] . plot ( ax = ax1 , color = "C0" ) daily_sales_item_lookup_scaled_weekly [ "HOUSEHOLD_2_040" ] . plot ( ax = ax2 , color = "C1" ) ax1 . set_title ( "HOBBIES_1_062" , fontsize = 14 ) ax2 . set_title ( "HOUSEHOLD_2_040" , fontsize = 14 ) ax1 . set_xlabel ( "" ) ax2 . set_xlabel ( "Days since start" )
1489	dtw ( daily_sales_item_lookup_scaled_weekly [ "HOUSEHOLD_2_040" ] , \ daily_sales_item_lookup_scaled_weekly [ "HOBBIES_1_062" ] , \ keep_internals = True , step_pattern = rabinerJuangStepPattern ( 3 , "c" ) ) \ . plot ( type = "twoway" , offset = 10 )
1490	def get_dtw_diff_matrix ( cols : list ) : diff_matrix = { } cross = itertools . product ( cols , cols ) for ( col1 , col2 ) in cross : series1 = daily_sales_item_lookup_scaled_weekly [ col1 ] series2 = daily_sales_item_lookup_scaled_weekly [ col2 ] diff = dtw ( series1 , series2 , keep_internals = True , step_pattern = rabinerJuangStepPattern ( 2 , "c" ) ) \ . normalizedDistance diff_matrix [ ( col1 , col2 ) ] = [ diff ] return diff_matrix
1491	daily_sales_item_lookup_scaled_weekly . T . merge ( dtw_clusters . loc [ dtw_clusters . cluster == 1 ] , left_index = True , right_index = True ) \ . T \ . plot ( figsize = ( 12 , 4 ) )
1492	def plot_dtw ( series1 : str , series2 : str ) -> None : dtw ( daily_sales_item_lookup_scaled_weekly [ series1 ] , \ daily_sales_item_lookup_scaled_weekly [ series2 ] , \ keep_internals = True , step_pattern = rabinerJuangStepPattern ( 2 , "c" ) ) \ . plot ( type = "twoway" , offset = 5 ) plot_dtw ( "FOODS_1_119" , "HOUSEHOLD_2_423" ) plot_dtw ( "FOODS_2_043" , "HOUSEHOLD_2_423" ) plot_dtw ( "HOBBIES_1_300" , "HOUSEHOLD_2_423" )
1493	daily_sales_item_lookup_scaled_weekly . T . merge ( dtw_clusters . loc [ dtw_clusters . cluster == 5 ] , left_index = True , right_index = True ) \ . T \ . plot ( figsize = ( 12 , 4 ) )
1494	plot_dtw ( "FOODS_3_247" , "FOODS_3_284" ) plot_dtw ( "FOODS_3_247" , "HOBBIES_1_122" ) plot_dtw ( "FOODS_3_247" , "HOUSEHOLD_1_164" ) plot_dtw ( "FOODS_3_247" , "HOUSEHOLD_1_429" ) plot_dtw ( "FOODS_3_247" , "HOUSEHOLD_2_318" )
1495	import matplotlib . pyplot as plt from matplotlib . backend_bases import RendererBase from scipy import signal from scipy . io import wavfile import os import numpy as np from PIL import Image from scipy . fftpack import fft
1496	audio_path = '../input/train/audio/' pict_Path = '../input/picts/train/' test_pict_Path = '../input/picts/test/' test_audio_path = '../input/test/audio/' samples = [ ]
1497	subFolderList = [ ] for x in os . listdir ( audio_path ) : if os . path . isdir ( audio_path + '/' + x ) : subFolderList . append ( x )
1498	sample_audio = [ ] total = 0 for x in subFolderList : all_files = [ y for y in os . listdir ( audio_path + x ) if '.wav' in y ] total += len ( all_files ) sample_audio . append ( audio_path + x + '/' + all_files [ 0 ] ) print ( 'count: %d : %s' % ( len ( all_files ) , x ) ) print ( total )
1499	def log_specgram ( audio , sample_rate , window_size = 20 , step_size = 10 , eps = 1e-10 ) : nperseg = int ( round ( window_size * sample_rate / 1e3 ) ) noverlap = int ( round ( step_size * sample_rate / 1e3 ) ) freqs , _ , spec = signal . spectrogram ( audio , fs = sample_rate , window = 'hann' , nperseg = nperseg , noverlap = noverlap , detrend = False ) return freqs , np . log ( spec . T . astype ( np . float32 ) + eps )
1500	fig = plt . figure ( figsize = ( 8 , 20 ) ) for i , filepath in enumerate ( sample_audio [ : 6 ] ) : plt . subplot ( 9 , 1 , i + 1 ) samplerate , test_sound = wavfile . read ( filepath ) plt . title ( filepath . split ( '/' ) [ - 2 ] ) plt . axis ( 'off' ) plt . plot ( test_sound )
1501	fig = plt . figure ( figsize = ( 8 , 20 ) ) for i , filepath in enumerate ( five_samples ) : plt . subplot ( 9 , 1 , i + 1 ) samplerate , test_sound = wavfile . read ( filepath ) plt . title ( filepath . split ( '/' ) [ - 2 ] ) plt . axis ( 'off' ) plt . plot ( test_sound )
1502	def wav2img ( wav_path , targetdir = '' , figsize = ( 4 , 4 ) ) : fig = plt . figure ( figsize = figsize ) samplerate , test_sound = wavfile . read ( filepath ) _ , spectrogram = log_specgram ( test_sound , samplerate ) output_file = wav_path . split ( '/' ) [ - 1 ] . split ( '.wav' ) [ 0 ] output_file = targetdir + '/' + output_file plt . imsave ( '%s.png' % output_file , spectrogram ) plt . close ( )
1503	def wav2img_waveform ( wav_path , targetdir = '' , figsize = ( 4 , 4 ) ) : samplerate , test_sound = wavfile . read ( sample_audio [ 0 ] ) fig = plt . figure ( figsize = figsize ) plt . plot ( test_sound ) plt . axis ( 'off' ) output_file = wav_path . split ( '/' ) [ - 1 ] . split ( '.wav' ) [ 0 ] output_file = targetdir + '/' + output_file plt . savefig ( '%s.png' % output_file ) plt . close ( )
1504	import shutil shutil . make_archive ( 'train_zipped' , 'zip' , '/kaggle/working/train' ) shutil . make_archive ( 'test_zipped' , 'zip' , '/kaggle/working/test' ) shutil . make_archive ( 'exa_test_zipped' , 'zip' , '/kaggle/working/exa_test' )
1505	train_meta_df [ "signal_mean" ] = train_df . agg ( np . mean ) . values train_meta_df [ "signal_sum" ] = train_df . agg ( np . sum ) . values train_meta_df [ "signal_std" ] = train_df . agg ( np . std ) . values
1506	import numpy as np import pandas as pd df_train = pd . read_csv ( '../input/application_train.csv' ) target_count = df_train . TARGET . value_counts ( ) print ( 'Class 0:' , target_count [ 0 ] ) print ( 'Class 1:' , target_count [ 1 ] ) print ( 'Proportion:' , round ( target_count [ 0 ] / target_count [ 1 ] , 2 ) , ': 1' ) target_count . plot ( kind = 'bar' , title = 'Count (target)' ) ;
1507	count_class_0 , count_class_1 = df_train . TARGET . value_counts ( ) df_class_0 = df_train [ df_train [ 'TARGET' ] == 0 ] df_class_1 = df_train [ df_train [ 'TARGET' ] == 1 ]
1508	df_class_0_under = df_class_0 . sample ( count_class_1 ) df_test_under = pd . concat ( [ df_class_0_under , df_class_1 ] , axis = 0 ) print ( 'Random under-sampling:' ) print ( df_test_under . TARGET . value_counts ( ) ) df_test_under . TARGET . value_counts ( ) . plot ( kind = 'bar' , title = 'Count (TARGET)' ) ;
1509	df_class_1_over = df_class_1 . sample ( count_class_0 , replace = True ) df_test_over = pd . concat ( [ df_class_0 , df_class_1_over ] , axis = 0 ) print ( 'Random over-sampling:' ) print ( df_test_over . TARGET . value_counts ( ) ) df_test_over . TARGET . value_counts ( ) . plot ( kind = 'bar' , title = 'Count (TARGET)' ) ;
1510	from sklearn . datasets import make_classification X , y = make_classification ( n_classes = 2 , class_sep = 1.5 , weights = [ 0.9 , 0.1 ] , n_informative = 3 , n_redundant = 1 , flip_y = 0 , n_features = 20 , n_clusters_per_class = 1 , n_samples = 100 , random_state = 10 ) df = pd . DataFrame ( X ) df [ 'TARGET' ] = y df . TARGET . value_counts ( ) . plot ( kind = 'bar' , title = 'Count (TARGET)' ) ;
1511	def plot_2d_space ( X , y , label = 'Classes' ) : colors = [ ' markers = [' o ', ' s '] for l, c, m in zip(np.unique(y), colors, markers): plt.scatter( X[y==l, 0], X[y==l, 1], c=c, label=l, marker=m ) plt.title(label) plt.legend(loc=' upper right ' ) plt . show ( )
1512	from sklearn . decomposition import PCA import matplotlib . pyplot as plt pca = PCA ( n_components = 2 ) X = pca . fit_transform ( X ) plot_2d_space ( X , y , 'Imbalanced dataset (2 PCA components)' )
1513	from imblearn . under_sampling import RandomUnderSampler rus = RandomUnderSampler ( return_indices = True ) X_rus , y_rus , id_rus = rus . fit_sample ( X , y ) print ( 'Removed indexes:' , id_rus ) plot_2d_space ( X_rus , y_rus , 'Random under-sampling' )
1514	from imblearn . under_sampling import TomekLinks tl = TomekLinks ( return_indices = True , ratio = 'majority' ) X_tl , y_tl , id_tl = tl . fit_sample ( X , y ) print ( 'Removed indexes:' , id_tl ) plot_2d_space ( X_tl , y_tl , 'Tomek links under-sampling' )
1515	from imblearn . under_sampling import ClusterCentroids cc = ClusterCentroids ( ratio = { 0 : 10 } ) X_cc , y_cc = cc . fit_sample ( X , y ) plot_2d_space ( X_cc , y_cc , 'Cluster Centroids under-sampling' )
1516	from imblearn . over_sampling import SMOTE smote = SMOTE ( ratio = 'minority' ) X_sm , y_sm = smote . fit_sample ( X , y ) plot_2d_space ( X_sm , y_sm , 'SMOTE over-sampling' )
1517	from imblearn . combine import SMOTETomek smt = SMOTETomek ( ratio = 'auto' ) X_smt , y_smt = smt . fit_sample ( X , y ) plot_2d_space ( X_smt , y_smt , 'SMOTE + Tomek links' )
1518	from sklearn import preprocessing import matplotlib . pyplot as plt plt . rc ( "font" , size = 14 ) from sklearn . linear_model import LogisticRegression from sklearn . cross_validation import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X_smt , y_smt , test_size = 0.2 , random_state = 0 ) from sklearn import metrics logreg = LogisticRegression ( ) logreg . fit ( X_train , y_train )
1519	import numpy as np import pandas as pd import os print ( os . listdir ( "../input" ) )
1520	import numpy as np import pandas as pd import matplotlib . pyplot as plt import pickle import re from IPython . core . display import display from tqdm import tqdm_notebook as tqdm pd . options . mode . chained_assignment = None
1521	fig , ax = plt . subplots ( nrows = 2 ) fig . set_size_inches ( 22 , 8.27 ) sns . lineplot ( x = 'Weeks' , y = 'Percent' , data = df , ax = ax [ 0 ] ) sns . lineplot ( x = 'Weeks' , y = 'FVC' , data = df , ax = ax [ 1 ] ) fig . savefig ( "weeksvsfvc.jpeg" )
1522	smoker = df [ df [ "SmokingStatus" ] == "Ex-smoker" ] never_smoked = df [ df [ "SmokingStatus" ] == "Never smoked" ] current_smoker = df [ df [ "SmokingStatus" ] == "Currently smokes" ]
1523	files = [ ] for dirname , _ , filenames in os . walk ( '../input/osic-pulmonary-fibrosis-progression/train' ) : for filename in filenames : files . append ( os . path . join ( dirname , filename ) )
1524	def decode_image ( image_path ) : image_bytes = tf . io . read_file ( image_path ) image = tfio . image . decode_dicom_image ( image_bytes , dtype = tf . uint16 ) image = np . squeeze ( image . numpy ( ) ) return image
1525	imp = pd . DataFrame ( index = feature_names ) imp [ 'train' ] = pd . Series ( bst . get_score ( importance_type = 'gain' ) , index = feature_names ) imp [ 'OOB' ] = pd . Series ( bst_after . get_score ( importance_type = 'gain' ) , index = feature_names ) imp = imp . fillna ( 0 )
1526	import plotly . offline as pyo import plotly . plotly as py from plotly . graph_objs import * import pandas as pd import plotly plotly . offline . init_notebook_mode ( ) from scipy import signal pyo . offline . init_notebook_mode ( ) import plotly . plotly as py from plotly . graph_objs import * import plotly . plotly as py from plotly . graph_objs import *
1527	def get_edge ( nb , data , threshold ) : edge = [ ] for i in range ( len ( data ) ) : if data [ 'count' ] [ i ] >= threshold : edge . append ( ( data [ 'neighborhood1' ] [ i ] , data [ 'neighborhood2' ] [ i ] ) ) return edge
1528	def get_numbers_of_adjcs ( edge , nb ) : n = len ( nb ) num_of_adjacencies = [ ] for i in range ( n ) : num_of_adjacencies . append ( 0 ) for d in edge : num_of_adjacencies [ d [ 0 ] - 1 ] += 1 num_of_adjacencies [ d [ 1 ] - 1 ] += 1 return num_of_adjacencies
1529	edge = get_edge ( nb , data , 500 ) num_of_adjacencies = get_numbers_of_adjcs ( edge , nb ) text = [ ] for i in range ( len ( nb ) ) : t = 'neighborhood:' + '<b>' + str ( nb [ 'neighborhood_name' ] [ i ] ) + '</b>' + '<br>' + 'boro:' + '<b>' + str ( nb [ 'boro' ] [ i ] ) + '</b>' + '<br>' + ' text . append ( t ) fig = prep ( edge , num_of_adjacencies , text , nb ) pyo . iplot ( fig )
1530	edge = get_edge ( nb , data , 2000 ) num_of_adjacencies = get_numbers_of_adjcs ( edge , nb ) text = [ ] for i in range ( len ( nb ) ) : t = 'neighborhood:' + '<b>' + str ( nb [ 'neighborhood_name' ] [ i ] ) + '</b>' + '<br>' + 'boro:' + '<b>' + str ( nb [ 'boro' ] [ i ] ) + '</b>' + '<br>' + ' text . append ( t ) fig = prep ( edge , num_of_adjacencies , text , nb ) pyo . iplot ( fig )
1531	from scipy import ndimage import operator import cv2 import numpy as np import os from tqdm . notebook import tqdm import matplotlib . pyplot as plt
1532	def generate_images ( imagelist ) : for x in imagelist : img_name = x . split ( sep = '/' ) [ - 1 ] img = plt . imread ( x ) img = crop_and_zoom ( img ) plt . imsave ( img_name , img )
1533	df_train = pd . read_csv ( 'train.csv' ) df_test = pd . read_csv ( 'test.csv' ) df_struct = pd . read_csv ( 'structures.csv' ) df_train_sub_charge = pd . read_csv ( 'mulliken_charges.csv' ) df_train_sub_tensor = pd . read_csv ( 'magnetic_shielding_tensors.csv' )
1534	def plot_history ( history , label ) : plt . plot ( history . history [ 'loss' ] ) plt . plot ( history . history [ 'val_loss' ] ) plt . title ( 'Loss for %s' % label ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'Epoch' ) _ = plt . legend ( [ 'Train' , 'Validation' ] , loc = 'upper left' ) plt . show ( )
1535	def submit ( predictions ) : submit = pd . read_csv ( 'sample_submission.csv' ) print ( len ( submit ) , len ( predictions ) ) submit [ "scalar_coupling_constant" ] = predictions submit . to_csv ( "/kaggle/working/workingsubmission-test.csv" , index = False ) submit ( test_prediction ) print ( 'Total training time: ' , datetime . now ( ) - start_time ) i = 0 for mol_type in mol_types : print ( mol_type , ": cv score is " , cv_score [ i ] ) i += 1 print ( "total cv score is" , cv_score_total )
1536	training_curated_df = pd . read_csv ( "../input/train_curated.csv" ) training_noisy_df = pd . read_csv ( "../input/train_noisy.csv" ) training_df = [ training_curated_df , training_noisy_df ] testing_df = pd . read_csv ( '../input/sample_submission.csv' )
1537	arch = models . resnet18 learn = cnn_learner ( data , arch , pretrained = False , metrics = [ lwlrap ] , wd = 0.1 , ps = 0.5 ) learn . lr_find ( ) learn . recorder . plot ( )
1538	size = 256 data = src . transform ( tfms , size = size ) . databunch ( bs = bs ) . normalize ( imagenet_stats )
1539	import numpy as np import pandas as pd import os import json from pathlib import Path import matplotlib . pyplot as plt from matplotlib import colors import numpy as np for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : print ( dirname ) from pathlib import Path
1540	import numpy as np import pandas as pd import matplotlib . pyplot as plt import itertools import string import re import os plt . style . use ( 'Solarize_Light2' ) pd . set_option ( "display.max_columns" , 2 ** 10 )
1541	date_cols = [ 'project_submitted_datetime' ] train = pd . read_csv ( os . path . join ( r'../input' , 'train.csv' ) , low_memory = False , parse_dates = date_cols ) test = pd . read_csv ( os . path . join ( r'../input' , 'test.csv' ) , low_memory = False , parse_dates = date_cols )
1542	train [ 'source' ] = 'train' test [ 'source' ] = 'test' test_train = pd . concat ( ( test , train ) ) print ( f'The shape of the data is {test_train.shape}.' )
1543	numerical_cols = [ ] dummy_categorical_cols = [ ] text_cols = [ ]
1544	mask_four_essays = ~ ( test_train . project_essay_3 . isnull ( ) & test_train . project_essay_4 . isnull ( ) ) test_train [ mask_four_essays ] = ( test_train [ mask_four_essays ] . assign ( project_essay_1 = lambda df : df . project_essay_1 + df . project_essay_2 ) . assign ( project_essay_2 = lambda df : df . project_essay_3 + df . project_essay_4 ) ) test_train = test_train . drop ( columns = [ 'project_essay_3' , 'project_essay_4' ] )
1545	print ( "=" * 30 ) print ( "Detecting NaN values in data:" ) print ( "=" * 30 ) print ( resource_stats . isnull ( ) . sum ( axis = 0 ) [ resource_stats . isnull ( ) . sum ( axis = 0 ) > 0 ] )
1546	test_train [ 'month' ] = test_train . project_submitted_datetime . dt . month . apply ( str ) test_train [ 'daytime' ] = pd . Series ( np . where ( ( ( 7 <= test_train . project_submitted_datetime . dt . hour ) & ( test_train . project_submitted_datetime . dt . hour <= 10 ) ) , 1 , 0 ) ) . apply ( str )
1547	import reprlib def set_of_categories ( col_name ) : list_train = test_train [ col_name ] . tolist ( ) list_test = test_train [ col_name ] . tolist ( ) return set ( ', ' . join ( list_train + list_test ) . split ( ', ' ) ) unique_categories = set_of_categories ( 'project_subject_categories' ) unique_subcategories = set_of_categories ( 'project_subject_subcategories' ) unique_cats_total = list ( unique_categories . union ( unique_subcategories ) ) dummy_categorical_cols += unique_cats_total print ( 'Categories:' , reprlib . repr ( unique_cats_total ) )
1548	project_cat_colnames = [ 'project_subject_categories' , 'project_subject_subcategories' ] df_cats = test_train . loc [ : , project_cat_colnames ] for category in unique_categories : df_cats [ category ] = np . where ( df_cats . project_subject_categories . str . contains ( category ) , 1 , 0 ) for category in unique_subcategories : df_cats [ category ] = np . where ( df_cats . project_subject_subcategories . str . contains ( category ) , 1 , 0 ) df_cats = df_cats . drop ( columns = project_cat_colnames ) df_cats . head ( 1 ) . T . head ( 5 )
1549	test_train = pd . concat ( ( test_train , df_cats ) , axis = 1 ) print ( f'The dataset now has ~{len(test_train.columns)} features.' ) test_train . head ( 1 )
1550	from csv import QUOTE_ALL for text_col in text_cols : test_train [ text_col ] = test_train [ text_col ] . str . replace ( '"' , ' ' )
1551	from tempfile import mkdtemp from sklearn . pipeline import Pipeline , FeatureUnion from sklearn . feature_extraction . text import TfidfVectorizer from sklearn . linear_model import LogisticRegression from sklearn . base import BaseEstimator , TransformerMixin from sklearn . decomposition import TruncatedSVD from sklearn . metrics import roc_auc_score , roc_curve , auc from scipy . sparse import coo_matrix , hstack from sklearn . preprocessing import RobustScaler
1552	from sklearn . model_selection import GridSearchCV param_grid = { "estimator__C" : np . linspace ( 0.24285 - 0.1 , 0.24285 + 0.1 , num = 6 ) } grid_search = GridSearchCV ( pipeline_logreg , param_grid = param_grid , scoring = 'roc_auc' , n_jobs = 1 , verbose = 1 , cv = 3 )
1553	n = 25000 subset_A = test_train . loc [ lambda df : ( df . project_is_approved == 1 ) ] . sample ( n ) subset_B = test_train . loc [ lambda df : ( df . project_is_approved == 0 ) ] . sample ( n ) test_train_subset = pd . concat ( ( subset_A , subset_B ) ) test_X = test_train_subset test_y = test_X . project_is_approved
1554	def soft_AUC_theano ( y_true , y_pred ) : pos_pred_vr = y_pred [ y_true . nonzero ( ) ] neg_pred_vr = y_pred [ theano . tensor . eq ( y_true , 0 ) . nonzero ( ) ] pred_diffs_vr = pos_pred_vr . dimshuffle ( 0 , 'x' ) - neg_pred_vr . dimshuffle ( 'x' , 0 ) stats = theano . tensor . nnet . sigmoid ( pred_diffs_vr * 2 ) return 1 - theano . tensor . mean ( stats )
1555	import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
1556	params = { 'num_leaves' : 546 , 'min_child_weight' : 0.03454472573214212 , 'feature_fraction' : 0.1797454081646243 , 'bagging_fraction' : 0.2181193142567742 , 'min_data_in_leaf' : 106 , 'objective' : 'binary' , 'max_depth' : - 1 , 'learning_rate' : 0.005883242363721497 , "boosting_type" : "gbdt" , "bagging_seed" : 11 , "metric" : 'auc' , "verbosity" : - 1 , 'reg_alpha' : 0.3299927210061127 , 'reg_lambda' : 0.3885237330340494 , 'random_state' : 42 , }
1557	import numpy as np import pandas as pd import matplotlib . pyplot as plt import warnings warnings . filterwarnings ( "ignore" )
1558	A = np . transpose ( crystal_lat ) R = crystal_xyz [ 0 ] [ 0 ] print ( "The lattice vectors:" ) print ( A ) print ( "The position vector:" ) print ( R )
1559	def get_optimal_lmn ( bmat , R_max = 20.0 ) : lmn = dict ( ) lmn [ "l_max" ] = int ( length ( bmat [ 0 ] ) * R_max ) + 1 lmn [ "m_max" ] = int ( length ( bmat [ 1 ] ) * R_max ) + 1 lmn [ "n_max" ] = int ( length ( bmat [ 2 ] ) * R_max ) + 1 lmn [ "R_max" ] = R_max return lmn
1560	def get_factor ( spacegroup , gamma ) : if spacegroup == 12 : return 1.4 elif spacegroup == 33 : return 1.4 elif spacegroup == 167 : return 1.5 elif spacegroup == 194 : return 1.3 elif spacegroup == 206 : return 1.5 elif spacegroup == 227 : if gamma < 60 : return 1.4 else : return 1.5 else : raise NameError ( 'get_factor does not support the spacegroup: {}' . format ( spacegroup ) )
1561	import pandas as pd import torch import torch . nn . functional as F from torch . optim import Adam import schnetpack as spk import schnetpack . atomistic as atm import schnetpack . representation as rep from schnetpack . datasets import * device = torch . device ( "cuda" )
1562	import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ) : for filename in filenames : print ( os . path . join ( dirname , filename ) )
1563	bin_dict = { 'T' : 1 , 'F' : 0 , 'Y' : 1 , 'N' : 0 } train [ 'bin_3' ] = train [ 'bin_3' ] . map ( bin_dict ) train [ 'bin_4' ] = train [ 'bin_4' ] . map ( bin_dict ) test [ 'bin_3' ] = test [ 'bin_3' ] . map ( bin_dict ) test [ 'bin_4' ] = test [ 'bin_4' ] . map ( bin_dict )
1564	test [ 'target' ] = 'test' df = pd . concat ( [ train , test ] , axis = 0 , sort = False ) print ( "Data shape:" , df . shape )
1565	print ( f'Shape before dummy transformation: {df.shape}' ) df = pd . get_dummies ( df , columns = [ 'nom_0' , 'nom_1' , 'nom_2' , 'nom_3' , 'nom_4' ] , \ prefix = [ 'nom_0' , 'nom_1' , 'nom_2' , 'nom_3' , 'nom_4' ] , drop_first = True ) print ( f'Shape after dummy transformation: {df.shape}' )
1566	X = train . drop ( [ 'id' , 'target' ] , axis = 1 ) y = train [ 'target' ] X_test = test . drop ( 'id' , axis = 1 )
1567	feature_importances [ 'average' ] = feature_importances [ [ f'fold_{fold_n + 1}' for fold_n in range ( folds . n_splits ) ] ] . mean ( axis = 1 ) feature_importances . to_csv ( 'feature_importances.csv' ) plt . figure ( figsize = ( 16 , 16 ) ) sns . barplot ( data = feature_importances . sort_values ( by = 'average' , ascending = False ) . head ( 50 ) , x = 'average' , y = 'feature' ) ; plt . title ( '50 TOP feature importance over {} folds average' . format ( folds . n_splits ) ) ;
1568	df_smk = train . query ( 'SmokingStatus == "Currently smokes"' ) df_smk . corr ( ) [ 'Age' ] [ 'FVC' ]
1569	plt . pie ( train [ "SmokingStatus" ] . value_counts ( ) , labels = [ "Ex-smoker" , "Never smoked" , "Currently smokes" ] , autopct = "%.1f%%" ) plt . title ( "SmokingStatus" ) plt . show ( )
1570	def extract_num ( s , p , ret = 0 ) : search = p . search ( s ) if search : return int ( search . groups ( ) [ 0 ] ) else : return ret
1571	import numpy as np import pandas as pd molecules = pd . read_csv ( '../input/structures.csv' ) molecules = molecules . groupby ( 'molecule_name' ) magnetic_shielding_tensors = pd . read_csv ( '../input/magnetic_shielding_tensors.csv' )
1572	x = magnetic_shielding_tensors . columns . values [ 2 : ] x = magnetic_shielding_tensors [ x ] . values x = x . reshape ( - 1 , 3 , 3 ) x = x + np . transpose ( x , ( 0 , 2 , 1 ) ) x = 0.5 * x w , v = np . linalg . eigh ( x )
1573	import pandas as pd import torch import torch . nn . functional as F from torch . optim import Adam import schnetpack as spk import schnetpack . atomistic as atm import schnetpack . representation as rep from schnetpack . datasets import * device = torch . device ( "cuda" )
1574	def get_size_list ( targets , dir_target ) : result = list ( ) for target in tqdm ( targets ) : img = np . array ( Image . open ( os . path . join ( dir_target , target ) ) ) result . append ( str ( img . shape ) ) return result
1575	data = pd . read_csv ( '../input/train.csv' ) data [ 'size_info' ] = get_size_list ( data . Image . tolist ( ) , dir_target = '../input/train' ) data . to_csv ( './size_train.csv' , index = False )
1576	counts = data . size_info . value_counts ( ) agg = data . groupby ( 'size_info' ) . Id . agg ( { 'number_sample' : len , 'rate_new_whale' : lambda g : np . mean ( g == 'new_whale' ) } ) agg = agg . sort_values ( 'number_sample' , ascending = False ) agg . to_csv ( 'result.csv' ) print ( agg . head ( 20 ) )
1577	new_img = skimage . exposure . adjust_gamma ( original_img , gamma = 0.8 ) plt . figure ( figsize = ( 6 , 6 ) ) plt . imshow ( new_img ) plt . title ( "gamma 0.8" ) plt . show ( )
1578	new_img = skimage . exposure . adjust_gamma ( original_img , gamma = 1.2 ) plt . figure ( figsize = ( 6 , 6 ) ) plt . imshow ( new_img ) plt . title ( "gamma 1.2" ) plt . show ( )
1579	imageio . imwrite ( 'quality-70.jpg' , original_img , quality = 70 ) new_img = skimage . io . imread ( 'quality-70.jpg' ) plt . figure ( figsize = ( 6 , 6 ) ) plt . imshow ( new_img ) plt . title ( "quality-70" ) plt . show ( )
1580	imageio . imwrite ( 'quality-90.jpg' , original_img , quality = 90 ) new_img = skimage . io . imread ( 'quality-90.jpg' ) plt . figure ( figsize = ( 6 , 6 ) ) plt . imshow ( new_img ) plt . title ( "quality-90" ) plt . show ( )
1581	imageio . imwrite ( 'quality-5.jpg' , original_img , quality = 5 ) new_img = skimage . io . imread ( 'quality-5.jpg' ) plt . figure ( figsize = ( 6 , 6 ) ) plt . imshow ( new_img ) plt . title ( "quality-5" ) plt . show ( )
1582	import random import numpy as np import pandas as pd import chainer import chainer_chemistry from IPython . display import display
1583	from chainer . datasets . dict_dataset import DictDataset train_dataset = DictDataset ( graphs = train_graphs , targets = train_targets ) valid_dataset = DictDataset ( graphs = valid_graphs , targets = valid_targets ) test_dataset = DictDataset ( graphs = test_graphs , targets = test_targets )
1584	train_iter = chainer . iterators . SerialIterator ( train_dataset , batch_size , order_sampler = train_sampler ) valid_iter = chainer . iterators . SerialIterator ( valid_dataset , batch_size , repeat = False , order_sampler = valid_sampler ) test_iter = chainer . iterators . SerialIterator ( test_dataset , batch_size , repeat = False , order_sampler = test_sampler )
1585	from chainer import optimizers optimizer = optimizers . Adam ( alpha = 1e-3 ) optimizer . setup ( model )
1586	from subprocess import check_output images = [ int ( x ) for x in check_output ( [ "ls" , "../input/images_sample" ] ) . decode ( "utf8" ) . strip ( ) . split ( '\n' ) ] df = df [ df . listing_id . isin ( images ) ] print ( df . shape ) df [ 'n_images' ] = df . apply ( lambda x : len ( x [ 'photos' ] ) , axis = 1 )
1587	from scipy . stats import pearsonr print ( "Trees vs Buildings: {:5.4f}" . format ( pearsonr ( pvt [ 1 ] , pvt [ 5 ] ) [ 0 ] ) ) print ( "Trees vs Buildings and Structures: {:5.4f}" . format ( pearsonr ( pvt [ 1 ] + pvt [ 2 ] , pvt [ 5 ] ) [ 0 ] ) )
1588	from PIL import Image import os os . listdir ( '../input/three_band' ) with open ( '../input/three_band/6120_2_2.tif' , encoding = 'utf-8' , errors = 'ignore' ) as f : print ( f . readlines ( ) )
1589	train_model = Sampler ( train ) train_model . compute_bounds ( ) train_y = train_model . compute_samples ( )
1590	model = Sampler ( train , test ) model . compute_bounds ( ) y = model . compute_samples ( ) sample_sub = pd . read_csv ( '/kaggle/input/liverpool-ion-switching/sample_submission.csv' ) sample_sub [ 'open_channels' ] = np . array ( y ) . astype ( 'int64' ) sample_sub . to_csv ( 'submission_0.csv' , index = False , float_format = '%.4f' )
1591	train_sales = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv' ) sell_prices = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/sell_prices.csv' ) calendar = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/calendar.csv' ) submission_file = pd . read_csv ( '/kaggle/input/m5-forecasting-accuracy/sample_submission.csv' )
1592	if CREATE_TIDY_DF : def categorically_encode_col ( df , col ) : encoded_df = pd . get_dummies ( df [ col ] , prefix = str ( col ) , drop_first = False ) return encoded_df total_tidy_df . columns if CREATE_TIDY_DF : cols_to_encode = [ 'cat_id' , 'store_id' , 'weekday' , 'event_type_1' , 'event_type_2' ] for col in cols_to_encode : new_cols = pd . DataFrame ( categorically_encode_col ( total_tidy_df , col ) ) total_tidy_df = pd . concat ( [ total_tidy_df , new_cols ] , axis = 1 )
1593	dept_storeloc_cross = pd . crosstab ( train_sales [ 'dept_id' ] , train_sales [ 'store_id' ] ) print ( dept_storeloc_cross ) ax = sns . heatmap ( dept_storeloc_cross , linewidths = 0.4 , cmap = "BuGn" ) ax . set ( title = 'Number of items in each category per store - Uniform' )
1594	n_items_dept = train_sales [ 'dept_id' ] . value_counts ( ) mean_of_total_sales_per_dept = dept_sum . mean ( axis = 0 ) ax = sns . regplot ( n_items_dept , mean_of_total_sales_per_dept ) ax . set ( title = 'Do departments with more items sell more? - No' , xlabel = 'Number of items Per Department' , ylabel = 'Mean total sales per department.' ) plt . show ( )
1595	cat_sum = train_sales . groupby ( [ 'cat_id' ] ) . sum ( ) . T . reset_index ( drop = True ) disp_boxplot ( data = cat_sum , title = 'Total Sales by Category' , xlabel = "Category" , ylabel = "Total Sales" )
1596	state_sum = train_sales . groupby ( [ 'state_id' ] ) . sum ( ) . T . reset_index ( drop = True ) state_mean = train_sales . groupby ( [ 'state_id' ] ) . mean ( ) . T . reset_index ( drop = True ) disp_boxplot ( data = state_sum , title = 'Total Sales by State ID' , xlabel = "State ID" , ylabel = "Total Sales" ) disp_boxplot ( data = state_mean , title = 'Mean Sales by State ID' , xlabel = "State ID" , ylabel = "Mean Sales" )
1597	store_sum = train_sales . groupby ( [ 'store_id' ] ) . sum ( ) . T . reset_index ( drop = True ) store_mean = train_sales . groupby ( [ 'store_id' ] ) . mean ( ) . T . reset_index ( drop = True ) disp_boxplot ( data = store_sum , title = 'Total Sales by Store ID' , xlabel = "Store ID" , ylabel = "Total Sales" ) disp_boxplot ( data = store_mean , title = 'Mean Sales Per Day by Store ID' , xlabel = "Store ID" , ylabel = "Total Sales" )
1598	ax = sns . regplot ( x = np . arange ( dept_sales . shape [ 0 ] ) , y = dept_sales , scatter_kws = { 'color' : 'blue' , 'alpha' : 0.1 } , order = 3 , line_kws = { 'color' : 'green' } , ) ax . set ( title = "Mean Total Sales Per Item Per Day Over Time" , xlabel = 'Day ID' , ylabel = 'Total sale per item per day' ) plt . show ( )
1599	from statsmodels . tsa . seasonal import seasonal_decompose weeks_per_year = 365 time_series = store_sum [ "CA_1" ] sj_sc = seasonal_decompose ( time_series , period = weeks_per_year ) sj_sc . plot ( ) plt . show ( )
1600	from statsmodels . tsa . seasonal import seasonal_decompose days_per_week = 7 time_series = store_sum [ "CA_1" ] sj_sc = seasonal_decompose ( time_series , period = days_per_week ) sj_sc . plot ( ) plt . show ( )
1601	submission_df . to_csv ( 'submission.csv' , index = False ) print ( submission_df . shape ) print ( "Submission file created" )
1602	columns = train . columns for cc in tqdm_notebook ( columns ) : train [ cc ] = train [ cc ] . fillna ( train [ cc ] . mode ( ) [ 0 ] ) test [ cc ] = test [ cc ] . fillna ( test [ cc ] . mode ( ) [ 0 ] )
1603	X_train = train . copy ( ) X_test = test . copy ( ) for cc in tqdm_notebook ( columns ) : le = LabelEncoder ( ) le . fit ( list ( train [ cc ] . values ) + list ( test [ cc ] . values ) ) X_train [ cc ] = le . transform ( train [ cc ] . values ) X_test [ cc ] = le . transform ( test [ cc ] . values )
1604	X_train = train . copy ( ) X_test = test . copy ( ) ohe = OneHotEncoder ( dtype = 'uint16' , handle_unknown = "ignore" ) ohe . fit ( train ) X_train = ohe . transform ( train ) X_test = ohe . transform ( test )
1605	import os import numpy as np from sklearn . model_selection import KFold from sklearn . linear_model import LinearRegression from sklearn . metrics import mean_absolute_error import matplotlib . pyplot as plt from tqdm import tqdm_notebook import pandas as pd from numba import njit import gc
1606	@ njit def entropy_fast ( vec , bins ) : h = np . histogram ( vec , bins = bins ) [ 0 ] + 1E-15 h = h / np . sum ( h ) return - np . sum ( h * np . log ( h ) )
1607	ent = np . zeros ( X . shape [ 0 ] ) n = 2000 ent_temp = np . zeros ( n ) cv = KFold ( n , shuffle = False ) for idx in tqdm_notebook ( range ( X . shape [ 0 ] ) ) : for idx2 , ( train_idx , test_idx ) in enumerate ( cv . split ( X [ idx ] ) ) : ent_temp [ idx2 ] = entropy_fast ( X [ idx , test_idx ] , 300 ) ent [ idx ] = np . mean ( ent_temp )
1608	plt . figure ( figsize = ( 15 , 5 ) ) plt . plot ( ent ) plt . xlabel ( 'time' ) plt . ylabel ( 'entropy' ) ;
1609	feature = np . clip ( ent , a_min = 2.37 , a_max = 2.66 ) model = LinearRegression ( n_jobs = - 1 ) model . fit ( feature . reshape ( - 1 , 1 ) , y . reshape ( - 1 , 1 ) ) feature = model . predict ( feature . reshape ( - 1 , 1 ) ) print ( 'MAE: ' , mean_absolute_error ( feature , y ) )
1610	plt . figure ( figsize = ( 15 , 5 ) ) plt . plot ( feature , 'r' ) plt . plot ( y , 'k' ) plt . ylabel ( 'TTF' ) plt . xlabel ( 'time' ) plt . grid ( )
1611	test_set_TTF_peaks = [ 11 , 11 , 11 , 8 , 11 , 16 , 9 , 11 , 16 ] test_set_TTF_mean_peak = np . mean ( test_set_TTF_peaks ) scaling_factor = test_set_TTF_mean_peak / np . max ( feature )
1612	import matplotlib . pyplot as plt import numpy as np import pandas as pd from math import floor , log from scipy . stats import skew , kurtosis from scipy . io import loadmat
1613	def openfile_dialog ( ) : return '../input/train_1/1_25_1.mat'
1614	def convertMatToDictionary ( path ) : try : mat = loadmat ( path ) names = mat [ 'dataStruct' ] . dtype . names ndata = { n : mat [ 'dataStruct' ] [ n ] [ 0 , 0 ] for n in names } except ValueError : print ( 'File ' + path + ' is corrupted. Will skip this file in the analysis.' ) ndata = None return ndata
1615	def calcNormalizedFFT ( epoch , lvl , nt , fs ) : lseg = np . round ( nt / fs * lvl ) . astype ( 'int' ) D = np . absolute ( np . fft . fft ( epoch , n = lseg [ - 1 ] , axis = 0 ) ) D [ 0 , : ] = 0 D /= D . sum ( ) return D
1616	def defineEEGFreqs ( ) : return ( np . array ( [ 0.1 , 4 , 8 , 14 , 30 , 45 , 70 , 180 ] ) )
1617	def calcShannonEntropy ( epoch , lvl , nt , nc , fs ) : dspect = calcDSpect ( epoch , lvl , nt , nc , fs ) spentropy = - 1 * np . sum ( np . multiply ( dspect , np . log ( dspect ) ) , axis = 0 ) return spentropy
1618	def corr ( data , type_corr ) : C = np . array ( data . corr ( type_corr ) ) C [ np . isnan ( C ) ] = 0 C [ np . isinf ( C ) ] = 0 w , v = np . linalg . eig ( C ) x = np . sort ( w ) x = np . real ( x ) return x
1619	def calcActivity ( epoch ) : activity = np . nanvar ( epoch , axis = 0 ) return activity
1620	def calcMobility ( epoch ) : mobility = np . divide ( np . nanstd ( np . diff ( epoch , axis = 0 ) ) , np . nanstd ( epoch , axis = 0 ) ) return mobility
1621	def calcComplexity ( epoch ) : complexity = np . divide ( calcMobility ( np . diff ( epoch , axis = 0 ) ) , calcMobility ( epoch ) ) return complexity
1622	def petrosianFD ( X , D = None ) : if D is None : D = np . diff ( X ) N_delta = 0 ; for i in range ( 1 , len ( D ) ) : if D [ i ] * D [ i - 1 ] < 0 : N_delta += 1 n = len ( X ) return np . log10 ( n ) / ( np . log10 ( n ) + np . log10 ( n / n + 0.4 * N_delta ) )
1623	def katzFD ( epoch ) : L = np . abs ( epoch - epoch [ 0 ] ) . max ( ) d = len ( epoch ) return ( np . log ( L ) / np . log ( d ) )
1624	def hurstFD ( epoch ) : lags = range ( 2 , 100 ) tau = [ np . sqrt ( np . nanstd ( np . subtract ( epoch [ lag : ] , epoch [ : - lag ] ) ) ) for lag in lags ] poly = np . polyfit ( np . log ( lags ) , np . log ( tau ) , 1 ) return poly [ 0 ] * 2.0
1625	def logarithmic_n ( min_n , max_n , factor ) : assert max_n > min_n assert factor > 1 max_i = int ( np . floor ( np . log ( 1.0 * max_n / min_n ) / np . log ( factor ) ) ) ns = [ min_n ] for i in range ( max_i + 1 ) : n = int ( np . floor ( min_n * ( factor ** i ) ) ) if n > ns [ - 1 ] : ns . append ( n ) return ns
1626	def calcSkewness ( epoch ) : sk = skew ( epoch ) return sk
1627	def calcKurtosis ( epoch ) : kurt = kurtosis ( epoch ) return kurt
1628	def calcShannonEntropyDyad ( epoch , lvl , nt , nc , fs ) : dspect = calcDSpectDyad ( epoch , lvl , nt , nc , fs ) spentropyDyd = - 1 * np . sum ( np . multiply ( dspect , np . log ( dspect ) ) , axis = 0 ) return spentropyDyd
1629	def replaceZeroRuns ( df ) : return ( df . replace ( 0 , np . nan ) . fillna ( ) )
1630	from sklearn import preprocessing def normalizeFeatures ( df ) : min_max_scaler = preprocessing . MinMaxScaler ( ) x_scaled = min_max_scaler . fit_transform ( df ) df_normalized = pd . DataFrame ( x_scaled , columns = df . columns ) return df_normalized def normalizePanel ( pf ) : pf2 = { } for i in range ( pf . shape [ 2 ] ) : pf2 [ i ] = normalizeFeatures ( pf . ix [ : , : , i ] ) return pd . Panel ( pf2 )
1631	from os import listdir def ieegGetFilePaths ( directory , extension = '.mat' ) : filenames = sorted ( listdir ( directory ) ) files_with_extension = [ directory + '/' + f for f in filenames if f . endswith ( extension ) and not f . startswith ( '.' ) ] return files_with_extension
1632	train = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/train.csv' ) test = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/test.csv' ) submission = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv' )
1633	all_data [ 'FirstWeek' ] = all_data [ 'Weeks' ] all_data . loc [ all_data . Dataset == 'submission' , 'FirstWeek' ] = np . nan all_data [ 'FirstWeek' ] = all_data . groupby ( 'Patient' ) [ 'FirstWeek' ] . transform ( 'min' )
1634	mse = mean_squared_error ( train [ 'FVC' ] , predictions , squared = False ) mae = mean_absolute_error ( train [ 'FVC' ] , predictions ) print ( 'MSE Loss: {0:.2f}' . format ( mse ) ) print ( 'MAE Loss: {0:.2f}' . format ( mae ) )
1635	fig = px . histogram ( new_df , x = 'Age' , nbins = 42 ) fig . update_traces ( marker_color = 'rgb(158,202,225)' , marker_line_color = 'rgb(8,48,107)' , marker_line_width = 1.5 , opacity = 0.6 ) fig . update_layout ( title = 'Distribution of Age' ) fig . show ( )
1636	fig = px . histogram ( train_x , x = 'Age' , color = 'SmokingStatus' , color_discrete_map = { 'Never smoked' : 'yellow' , 'Currently smokes' : 'cyan' , 'Ex-smoker' : 'green' , } , hover_data = train_x . columns ) fig . update_layout ( title = 'Distribution of Age w.r.t. SmokingStatus for unique patients' ) fig . update_traces ( marker_line_color = 'black' , marker_line_width = 1.5 , opacity = 0.85 ) fig . show ( )
1637	fig = px . histogram ( train_x , x = 'Age' , color = 'Sex' , color_discrete_map = { 'Male' : 'blue' , 'Female' : 'mediumturquoise' } , hover_data = train_x . columns ) fig . update_layout ( title = 'Distribution of Age w.r.t. sex for unique patients' ) fig . update_traces ( marker_line_color = 'black' , marker_line_width = 1.5 , opacity = 0.85 ) fig . show ( ) 50 55 60 65 70 75 80 85 0 20 40 60 80 100 120 140
1638	a = sns . distplot ( train_x [ 'FVC' ] , color = 'r' , ) a . set_title ( 'Distribution plot of SVC ' , color = 'g' , fontsize = 18 )
1639	def eval_metric ( FVC , FVC_Pred , sigma ) : n = len ( sigma ) a = np . empty ( n ) a . fill ( 70 ) sigma_clipped = np . maximum ( sigma , a ) delta = np . minimum ( np . abs ( FVC , FVC_Pred ) , 1000 ) eval_metric = - np . sqrt ( 2 ) * delta / sigma_clipped - np . log ( np . sqrt ( 2 ) * sigma_clipped ) return eval_metric
1640	sub_df = pd . read_csv ( '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv' ) print ( f"The sample submission contains: {sub_df.shape[0]} rows and {sub_df.shape[1]} columns." )
1641	def get_baseline_week ( df ) : _df = df . copy ( ) _df [ 'Weeks' ] = _df [ 'Weeks' ] . astype ( int ) _df . loc [ _df . Source == 'test' , 'min_week' ] = np . nan _df [ "min_week" ] = _df . groupby ( 'Patient' ) [ 'Weeks' ] . transform ( 'min' ) _df [ 'baselined_week' ] = _df [ 'Weeks' ] - _df [ 'min_week' ] return _df
1642	def old_baseline_FVC ( ) : return get_baseline_FVC_old ( data_df ) pass def new_baseline_FVC ( ) : return get_baseline_FVC ( data_df ) duration_old = timeit ( old_baseline_FVC , number = 3 ) duration_new = timeit ( new_baseline_FVC , number = 3 ) print ( f"Taking the old, non-vectorized version took {duration_old / 3:.2f} sec, while the vectorized version only took {duration_new / 3:.3f} sec. That's {duration_old/duration_new:.0f} times faster!" )
1643	from sklearn . preprocessing import OneHotEncoder , LabelEncoder from sklearn . preprocessing import StandardScaler , MinMaxScaler , RobustScaler from sklearn . compose import ColumnTransformer no_transform_attribs = [ 'Patient' , 'Weeks' , 'min_week' ] num_attribs = [ 'FVC' , 'Percent' , 'Age' , 'baselined_week' , 'base_FVC' ] cat_attribs = [ 'Sex' , 'SmokingStatus' ]
1644	sigma_opt = mean_absolute_error ( y , train_preds [ : , 1 ] ) sigma_uncertain = train_preds [ : , 2 ] - train_preds [ : , 0 ] sigma_mean = np . mean ( sigma_uncertain ) print ( sigma_opt , sigma_mean )
1645	import numpy as np import pandas as pd import os
1646	import random from tqdm . notebook import tqdm from sklearn . model_selection import train_test_split , KFold from sklearn . metrics import mean_absolute_error from tensorflow_addons . optimizers import RectifiedAdam from tensorflow . keras import Model import tensorflow . keras . backend as K import tensorflow . keras . layers as L import tensorflow . keras . models as M from tensorflow . keras . optimizers import Nadam import seaborn as sns import plotly . express as px import plotly . graph_objects as go from PIL import Image import tensorflow as tf
1647	binary = [ ] ordinal = [ ] numeric = [ ] for col in v_cols : if train_df [ col ] . value_counts ( ) . shape [ 0 ] == 2 : binary . append ( col ) elif train_df [ col ] . sum ( ) - train_df [ col ] . sum ( ) . astype ( 'int' ) == 0 : ordinal . append ( col ) else : numeric . append ( col ) print ( f'Binary features {len(binary)}: {binary}\n' ) print ( f'Ordinal features {len(ordinal)}: {ordinal}\n' ) print ( f'Numeric features {len(numeric)}: {numeric}\n' )
1648	fig = plt . figure ( figsize = ( 11 , 15 ) ) for i , col in enumerate ( binary ) : plt . subplot ( f'42{i}' ) plot_cat ( col , annot = True , fillna = 'Null' ) plt . tight_layout ( )
1649	short_ordinal = [ ] long_ordinal = [ ] for col in ordinal : if train_df [ col ] . value_counts ( ) . shape [ 0 ] > 20 : long_ordinal . append ( col ) else : short_ordinal . append ( col ) print ( f'Short: {len(short_ordinal)}' , short_ordinal , '\n' ) print ( f'Long: {len(long_ordinal)}' , long_ordinal )
1650	denoise = True import shutil import glob import numpy as np import pandas as pd import gc import os import matplotlib . pyplot as plt import tensorflow as tf print ( tf . __version__ ) strategy = tf . distribute . get_strategy ( ) print ( 'Number of devices: {}' . format ( strategy . num_replicas_in_sync ) )
1651	import numpy as np import pandas as pd import matplotlib . colors as colors import matplotlib . pyplot as plt import scipy . signal as signal import os import gc
1652	def region_plot ( df ) : data = df . copy ( ) data [ 'time_to_failure' ] = data [ 'time_to_failure' ] * 100 data [ 'time' ] = data . index data [ 'time' ] = data [ 'time' ] * ( 1 / 4e6 ) data [ 'Time [sec]' ] = data [ 'time' ] - data [ 'time' ] . min ( ) data [ [ 'acoustic_data' , 'time_to_failure' , 'Time [sec]' ] ] . plot ( x = 'Time [sec]' , figsize = ( 8 , 5 ) ) return
1653	import numpy as np import pandas as pd import random import scipy . ndimage as scipyImg import scipy . misc as misc import matplotlib . pyplot as plt import seaborn as sns import os
1654	file_imgs = os . listdir ( path = '../input/train/images/' ) file_masks = os . listdir ( path = '../input/train/masks/' ) print ( 'Images found: {0}\nCorresponding masks: {1}' . format ( len ( file_imgs ) , len ( file_masks ) ) )
1655	df1 = depths . set_index ( 'id' ) df2 = train_masks . set_index ( 'id' ) dataset = pd . concat ( [ df1 , df2 ] , axis = 1 , join = 'inner' ) dataset = dataset . reset_index ( )
1656	from kaggle_datasets import KaggleDatasets GCS_PATH = KaggleDatasets ( ) . get_gcs_path ( f'melanoma-{IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}' ) GCS_PATH2 = KaggleDatasets ( ) . get_gcs_path ( f'malignant-v2-{IMAGE_SIZE[0]}x{IMAGE_SIZE[1]}' )
1657	AugParams = { 'd1' : 100 , 'd2' : 160 , 'rotate' : 45 , 'ratio' : 0.4 }
1658	from tensorflow . keras . applications import DenseNet201 , Xception , InceptionV3 , InceptionResNetV2 import efficientnet . tfkeras as efn
1659	import numpy as np , matplotlib . pyplot as plt , pandas as pd , seaborn as sns import math , os , time , cv2 print ( 'Installing fastai2...' ) print ( 'Installation complete' )
1660	from fastai2 . basics import * from fastai2 . vision . all import * from fastai2 . medical . imaging import *
1661	pixel_dist = dcm . scaled_px . flatten ( ) fig , ax = plt . subplots ( figsize = ( 20 , 7 ) ) sns . kdeplot ( np . asarray ( pixel_dist ) , shade = True ) plt . show ( )
1662	bins = pixel_dist . freqhist_bins ( 20 ) print ( bins ) fig , ax = plt . subplots ( figsize = ( 20 , 7 ) ) plt . hist ( pixel_dist , bins = bins ) plt . show ( )
1663	fig , ax = plt . subplots ( figsize = ( 20 , 7 ) ) plt . plot ( bins , torch . linspace ( 0 , 1 , len ( bins ) ) ) plt . show ( )
1664	scales = False , dicom_windows . lungs , True titles = 'Raw' , 'Windowed - Lung' , 'Scaled' for s , a , t in zip ( scales , subplots ( 1 , 3 , imsize = 7 ) [ 1 ] . flat , titles ) : dcm . show ( scale = s , ax = a , title = t )
1665	mask = dcm_to_crop . mask_from_blur ( dicom_windows . brain ) wind = dcm_to_crop . windowed ( * dicom_windows . brain ) _ , ax = subplots ( 1 , 1 , figsize = ( 7 , 7 ) ) show_image ( wind , ax = ax [ 0 ] ) show_image ( mask , alpha = 0.5 , ax = ax [ 0 ] ) ;
1666	file_dir = Path ( '../input/osic-pulmonary-fibrosis-progression/train/ID00007637202177411956430/' ) dicom_meta = pd . DataFrame . from_dicoms ( file_dir . ls ( ) ) print ( f"Extracted DICOM data is of dimension: {dicom_meta.shape}" ) dicom_meta . head ( )
1667	index = [ 'BitsStored' , 'PixelRepresentation' ] dicom_meta . pivot_table ( values = [ 'img_mean' , 'img_max' , 'img_min' , 'PatientID' ] , index = index , aggfunc = { 'img_mean' : 'mean' , 'img_max' : 'max' , 'img_min' : 'min' , 'PatientID' : 'count' } )
1668	train = pd . read_json ( '../input/stanford-covid-vaccine/train.json' , lines = True ) test = pd . read_json ( '../input/stanford-covid-vaccine/test.json' , lines = True ) sample_sub = pd . read_csv ( '../input/stanford-covid-vaccine/sample_submission.csv' )
1669	print ( train . shape ) if ~ train . isnull ( ) . values . any ( ) : print ( 'No missing values' ) train . head ( )
1670	fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 ) ) sns . kdeplot ( train [ 'signal_to_noise' ] , shade = True , ax = ax [ 0 ] ) sns . countplot ( train [ 'SN_filter' ] , ax = ax [ 1 ] ) ax [ 0 ] . set_title ( 'Signal/Noise Distribution' ) ax [ 1 ] . set_title ( 'Signal/Noise Filter Distribution' ) ;
1671	feature_imp = pd . DataFrame ( sorted ( zip ( clf . feature_importance ( ) , FEAT_COLS ) ) , columns = [ 'Value' , 'Feature' ] ) plt . figure ( figsize = ( 20 , 10 ) ) sns . barplot ( x = "Value" , y = "Feature" , data = feature_imp . sort_values ( by = "Value" , ascending = False ) . head ( 20 ) ) plt . title ( 'LightGBM Features' ) plt . tight_layout ( ) plt . show ( ) plt . savefig ( 'lgbm_importances-01.png' )
1672	feature_imp = pd . DataFrame ( sorted ( zip ( clf . feature_importance ( ) , columns ) ) , columns = [ 'Value' , 'Feature' ] ) plt . figure ( figsize = ( 20 , 10 ) ) sns . barplot ( x = "Value" , y = "Feature" , data = feature_imp . sort_values ( by = "Value" , ascending = False ) . head ( 20 ) ) plt . title ( 'LightGBM Features' ) plt . tight_layout ( ) plt . show ( ) plt . savefig ( 'lgbm_importances-01.png' )
1673	feature_imp = pd . DataFrame ( sorted ( zip ( clf . feature_importance ( ) , features ) ) , columns = [ 'Value' , 'Feature' ] ) plt . figure ( figsize = ( 20 , 10 ) ) sns . barplot ( x = "Value" , y = "Feature" , data = feature_imp . sort_values ( by = "Value" , ascending = False ) . head ( 20 ) ) plt . title ( 'LightGBM Features' ) plt . tight_layout ( ) plt . show ( ) plt . savefig ( 'lgbm_importances-01.png' )
1674	feature_imp = pd . DataFrame ( sorted ( zip ( clf . feature_importance ( ) , features ) ) , columns = [ 'Value' , 'Feature' ] ) plt . figure ( figsize = ( 20 , 10 ) ) sns . barplot ( x = "Value" , y = "Feature" , data = feature_imp . sort_values ( by = "Value" , ascending = False ) . head ( 20 ) ) plt . title ( 'LightGBM Features' ) plt . tight_layout ( ) plt . show ( ) plt . savefig ( 'lgbm_importances-01.png' )
1675	feature_imp = pd . DataFrame ( sorted ( zip ( clf . feature_importance ( ) , features ) ) , columns = [ 'Value' , 'Feature' ] ) plt . figure ( figsize = ( 20 , 10 ) ) sns . barplot ( x = "Value" , y = "Feature" , data = feature_imp . sort_values ( by = "Value" , ascending = False ) . head ( 20 ) ) plt . title ( 'LightGBM Features' ) plt . tight_layout ( ) plt . show ( ) plt . savefig ( 'lgbm_importances-01.png' )
1676	feature_imp = pd . DataFrame ( sorted ( zip ( clf . feature_importance ( ) , features ) ) , columns = [ 'Value' , 'Feature' ] ) plt . figure ( figsize = ( 20 , 20 ) ) sns . barplot ( x = "Value" , y = "Feature" , data = feature_imp . sort_values ( by = "Value" , ascending = False ) . head ( 5 ) ) plt . title ( 'LightGBM Features' ) plt . tight_layout ( ) plt . show ( ) plt . savefig ( 'lgbm_importances-01.png' )
1677	feature_imp = pd . DataFrame ( sorted ( zip ( clf . feature_importance ( ) , features ) ) , columns = [ 'Value' , 'Feature' ] ) plt . figure ( figsize = ( 20 , 20 ) ) sns . barplot ( x = "Value" , y = "Feature" , data = feature_imp . sort_values ( by = "Value" , ascending = False ) . head ( 5 ) ) plt . title ( 'LightGBM Features' ) plt . tight_layout ( ) plt . show ( ) plt . savefig ( 'lgbm_importances-02.png' )
1678	feature_imp = pd . DataFrame ( sorted ( zip ( clf . feature_importance ( ) , columns ) ) , columns = [ 'Value' , 'Feature' ] ) plt . figure ( figsize = ( 20 , 20 ) ) sns . barplot ( x = "Value" , y = "Feature" , data = feature_imp . sort_values ( by = "Value" , ascending = False ) . head ( 100 ) ) plt . title ( 'LightGBM Features' ) plt . tight_layout ( ) plt . show ( ) plt . savefig ( 'lgbm_importances-01.png' )
