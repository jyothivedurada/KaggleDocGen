1436	Let 's see the distribution of Minute distribution
837	Let 's take a look at the installments
124	Import modules
20	Muggy-SMalt-axotl-pembus
577	Let 's take a look at the China
78	Unfreeze the learning rate
71	Read the data
1311	Load the data
1247	Dept and Weekly Sales
1507	Load train leak data
1528	Distribution of DBNOs
892	Distribution of Trends in Credit Sum
437	Importing necessary libraries
1266	AdaBoost optimizer
277	Let 's start with the first 6 commits .
1475	Import necessary libraries
125	Let 's take a look at the DICOM files
141	Split the data into train and test
893	Let 's see the interesting features and their names .
1250	Let 's do a batch_mixup .
63	Feature Engineering
1012	Let 's resize the images to make sure they are all the same size
654	Random Forest
1355	Let 's see the distribution of the numeric features
307	Define Dropout and lr
1135	Importing necessary libraries
237	Let 's create a new column called `` commit_num `` and `` dropout_model `` and `` hidden_dim_third `` .
821	Load the data
423	Confusion Matrix
1401	Let 's take a look at the numeric features
360	Let 's create a KFold with 5 folds
405	Let 's take a look at one of the images
1307	Random Forest Regressor
897	Let 's take a look at the feature matrix .
775	Linear Regression
1567	Let 's get the labels for the training and testing datasets .
1434	Train and Test Split
610	Let 's define the filters and hidden layers .
859	Boosting Type for Random Search
1025	Load the data
1282	Let 's plot the prediction and actual data .
1351	Group Battery Type
889	Bureau_credit_application_date - Bureau_credit_end_date - Bureau_credit_close_date - Bureau_enddate_
614	Load the data
465	Load the data
1116	Load and Preprocessing Steps
1039	Let 's take a look at the public and private predictions .
1271	Get the raw training dataset
398	We will import the necessary modules .
248	Import necessary libraries
1107	Preprocess the data
1236	Light GBM
542	Concatenate the probabilities and birds
1096	SN_filter = 1 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter = 2 means SN_filter
581	Let 's take a look at the Spain cases by day
1560	Vectorize a sentence
1406	Import necessary libraries
181	Let 's find the indices of the cells that are connected to each other . We need to find the indices of the cells that are connected to each other .
1520	Classification Report
896	Let 's create a function to get the most recent value of y .
1214	Instantiate the EfficientNet
1248	Concatenate the data
1192	Load the data
51	Let 's take a look at the distribution of the training data .
238	Let 's create a new column called `` commit_num `` and `` dropout_model `` and `` hidden_dim_third `` .
59	Create Key
288	Let 's create a new column `` commit_num `` , `` Dropout_model `` , `` FVC_weight `` , `` lb_score `` , `` - 6.8092 `` ] .
1366	Let 's see the distribution of the numeric features
1566	Submission
381	Let 's create a simple model .
35	Import necessary libraries
1302	Fill missing values in the test data
583	Let 's take a look at the US cases
105	Let 's load the data and save it in pickle format .
1563	Latent Dirichilet Allocation
1336	Let 's define a function to generate random colors .
1514	Let 's see the distribution of Accent and Acc_d
909	Load test data
520	Calibrated Classification
1530	Distribution of killPlace
404	Let 's load the data
569	We will use the ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_preprocessing ` , ` get_
1089	Import necessary libraries
883	Correlation Heatmap
607	Load the data
364	Type
233	Let 's create a new column called 'hidden_dim_first ' and 'hidden_dim_second ' and 'hidden_dim_third ' .
100	Let 's generate a random sample of the data .
219	Let 's take a look at the hidden dim first and hidden dim second and hidden dim third .
214	Create Entity Set
1102	Let 's load the UCF file and load the test labels
626	Let 's take a look at the sum of bookings and total
1546	SAVE DATASET
575	Let 's take a look at the number of confirmed and deaths for each COVID
1522	Let 's see the f1 score
92	Class Distribution Over Entries
389	Let 's take a look at the images of the item .
1387	Let 's see the distribution of the numeric features
1500	Importing necessary libraries
690	Let 's get the DICOM data
912	Remove unwanted columns from above_threshold_vars
362	Ok
1575	Train and Test
1439	Let 's load the data
1061	Filter Submission
425	Convert image to numpy array
513	Let 's create a function that returns the ROI as a numpy array .
1051	Let 's see the shape of the sample data .
1398	Let 's take a look at the numeric features
1287	Import necessary libraries
449	Year Built and Building_ID
332	Gradient Boosting with Grid Search
325	Import necessary libraries
1493	Load necessary libraries
54	Let 's look at the distribution of the test data
537	Let 's load the librosa library and print the pitches and magnitudes
516	Fill missing values in train and test data
772	Let 's load the test data
1165	TPU Setup
661	Number of nominal variables
1290	Let 's train the model on the test data .
1023	Train the model
846	Let 's create an objective function .
797	Import necessary libraries
1238	Stacking
442	Monthly Readings ARE HIGHEST CHANGES BASED ON BUILDING TYPE
1297	Number of data per each diagnosis
719	Let 's look at the correlation matrix .
576	Let 's create a function to get the number of confirmed and deaths for each country .
162	Pushout + Medium Stacking
203	Zero Center
506	Let 's take a look at the target 1 data
1038	Build Model
149	Test Data Augmentation
687	Let 's split the ID 's string into a list of ID 's components .
26	LightGBM Features
1373	Let 's see the distribution of the numeric features
1017	Let 's take a look at some random images
527	Let 's see the data types
418	KMeans on test data
1564	Let 's take a look at the topic distributions .
390	Let 's take a look at the category names
1340	Let 's see the number of missing values in each column .
827	LGBM Classification
1154	Convert the training data into a dictionary
595	Let 's see the most common words in the temp list .
345	Predict on Test Set
370	Linear SVr
1047	Create folder for training and test data
631	Merging the data into a pandas dataframe
1425	Let 's see if we can predict the COVID-19 .
854	Let 's set some random parameters .
891	Let 's start with the time features .
445	Meter Reading
1415	Let 's see the distribution of the target variable .
1230	Let 's validate the output of the cross_validate function .
1489	Increased Vascular Markings + Enlarged Heart
803	Create a sample for each boosting_type
102	Let 's generate a random path and a fake path .
468	Import necessary libraries
900	Let 's see the shape of the feature matrix .
518	Baseline Classifier
1149	Let 's take a look at the var_68 values .
18	Load the data
1451	Converted Ratio
374	Gradient Boosting
252	Italy
963	SHAP returnsCloseRaw10_lag_3_mean
1147	Let 's look at the number of masks for each image .
386	Split raw data into train and test
1261	Make predictions on test data
160	Let 's see the distribution of isFraud vs. Fraud
337	ExtraTrees Regressor
768	Let 's see the distribution of pickup and dropoff coordinates
1219	Let 's define the learning rate .
387	Let 's see the number of images in the training set .
5	Let 's take a look at the target distribution
589	Let 's see the distribution of the infection peaks .
910	Let 's align the train and test data .
874	Importing necessary libraries
1301	Load the test data
918	Read in the credit_card_balance file
435	Let 's start with the TfidfVectorizer
1080	Let 's take a look at some of the images
735	Fit Model
1423	Let 's take a look at the COVID-19 Prediction
1239	Let 's see the structure of train and test data .
766	Let 's define a function to calculate the EDA
1357	Let 's see the distribution of the numeric features
1040	Load the data
1132	Diff V319 V320 vs V32
651	Let 's create a new column called 'cate0 ' , which is a list of all the values of cate0 .
903	Let 's check the correlation between the target and the target features .
692	Combinations of TTA
1397	Let 's take a look at the numeric features
841	Merge credit_info
212	Load the data
1288	Let 's see the correlation between the features and the macro features .
385	Run on multiprocessing
814	Boosting Type
166	Number of different values
12	Load the data
1143	Let 's see the number of unique values for each column .
422	Random Forest Classifier
653	Random Forest
410	Test Duplication
310	Let 's load the training data .
906	Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau
1350	checking missing data in train data
535	Importing necessary libraries
1289	Train and Test Split
204	Import necessary libraries
812	Let 's look at the scores .
216	Let 's start with a simple linear SVr .
1460	Test Data Augmentation
992	Show a 3D image
871	Let 's look at the top 100 features .
138	Month Temperature
1035	Load the data
615	Check for missing values
1155	Import necessary libraries
667	Logistic Regression
1376	Let 's see the distribution of the numeric features
536	librosa.onset_strength ( librosa.onset.strength
112	Compile and fit the model
347	Submission
1179	Let 's look at the test data
179	Let 's take a look at the number of separate components / objects detected .
1408	Let 's check if id is unique or not .
1118	Let 's have a look at the data .
246	Read the data
357	Import necessary libraries
1380	Let 's see the distribution of the numeric features
1020	Create Train and Validation Dataset
798	LightGBM Classifier
929	Word2Vec
1341	Let 's see the distribution of the number of missing values in each column .
1264	Get the pretrained model
391	Let 's see the distribution of category level
1447	Convert categorical variables
549	Let 's see the distribution of the log error of the filtered data .
1542	Time to failure and acoustic data
184	Top 10 categories
1487	Sample Patient 6 - Normal , Pleural Efficient
511	Convert to grayscale
1065	Predict on Test Set
517	Let 's take a look at the transaction revenue
414	Let 's compute the histogram of the image .
15	We can see that most of the words are longer than max_len . We can see that most of the words are longer than max_len . We can see that most of the words are longer than max_len .
1317	Calculate the number of new features per family size
241	Let 's create a new column called 'hidden_dim_first ' and 'hidden_dim_second ' and 'hidden_dim_third ' .
868	Read the correlations file
640	Let 's take a random permutation of the predicted data and compare it to the predicted data
36	Load the data
1233	Random Forest Classification
438	Let 's see the distribution of the data .
584	Load the data
159	Importing necessary libraries
925	Let 's take a look at the number of income bins in the application .
716	Most correlated variables
365	Let 's take a look at the training data
278	Let 's create a new column called `` commit_num '' .
224	Let 's create a new column called 'dropout_model ' and 'lb_score
819	Bayesian Optimization
143	Set the seeds
98	Load Test Data
720	Let 's look at the correlation matrix .
658	Let 's see the correlation between the features .
1079	Let 's take a look at one of the training images
538	Bathrooms and interest level
1548	Load Glove , and paragram and fasttext embeddings
1399	Let 's take a look at the numeric features
993	Make a file
23	Let 's create a simple word vectorizer
761	Let 's create a StratifiedKFold
983	Prepare test data
1334	Let 's drop the fullVisitorId , sessionId and visitId columns
148	Let 's start with a simple example .
534	Let 's take a look at the number of prior orders
434	Train and Test Split
571	Let 's load the complete data
700	Checking for missing data
1323	Let 's create a new column called 'new_area1 ' and 'new_area2 ' .
590	Let 's import necessary libraries
863	Appulate app_train with app_test
1365	Let 's see the distribution of the numeric features
314	Classification Report
1003	Let 's create a fake directory
1068	Let 's compute text and questions from the test data .
223	Let 's create a new column called 'dropout_model ' and 'lb_score
243	Let 's create a new column called 'dropout_model ' and 'lb_score ' .
1411	One-Hot Encoding
787	Fare Amount by Day of Week
957	Test Predictions
839	Let 's take a look at the cash information
1326	Let 's look at the binary and object categorical features
1298	Let 's see the number of columns we have .
66	Let 's start with the data .
1591	Let 's see the distribution of the news data
1174	Adding PAD to each sequence
904	One-hot encode categorical features
220	Let 's create a new column called 'dropout_model ' and 'lb_score ' .
1294	Let 's take a look at the sample images
1163	Let 's look at the class counts of the labels .
1094	Calculate SNR
475	Submission
172	Let 's check the missing values .
433	Let 's look at the top 20 tags
1075	Let 's see the shape of the data .
1404	Let 's see the difference between close and MACD .
1220	Evaluate the model
1139	Let 's plot some of the augmented images
696	Let 's replace ` dependency ` , ` edjefa ` , ` edjefe ` with ` numpy.float64 ` .
487	Convert text to word sequence
572	Let 's take a look at the first day of the month .
500	Let 's see the correlation between the features
1349	Let 's look at some of the features we are going to use .
655	Save Preprocessing and Classification
560	Convert bboxes_dict to pandas DataFrame
296	Define parameters Back to Table of Contents ] ( toc
1382	Let 's see the distribution of the numeric features
1119	SexuponOutcome and SexuponOutcome
122	Pulmonary Condition Progression by Sex
788	Split data into train and validation
397	Let 's see the distribution of in_train and in_test
1234	Logistic Regression
458	Intersection + City + IntersectionId
182	RLE Encoding
994	Let 's take a look at the DICOM
473	Import necessary libraries
190	Let 's see the price of the products .
1049	Let 's take a look at the size of the images .
1573	Let 's take a look at the lagged data
1405	Let 's take a look at the volume
456	Preview of Train and Test Data
1315	Replace edjefa
1496	Evaluate
856	Random Search Trip
1009	Define the hyperparameters and save the model .
448	Let 's see the distribution after applying log tranformation .
39	Let 's add a new column 'sex ' .
533	Hour Of The Day
206	Import necessary libraries
1539	Label encoding for categorical features
478	Import necessary libraries
888	Replace Day Outliers
660	Day Distribution
709	Let 's take a look at the number of walls and the number of rofs and floors .
525	Mean Squared Error
1093	Scatter plot of var_ + shap_values
344	Training and Validation Loss
617	Let 's perform the Ridge regression model on the test data .
1443	HHOURLY CONVERSION RATIO
428	CatBoost Regressor
1381	Let 's take a look at the numeric features
881	Number of Estimators vs Learning Rate
885	Split the data into train and test
890	Bureau_balance
539	Bedrooms
61	Let 's take a look at the products
1469	Melt Sales
905	Let 's create a function that counts the number of unique values in each group .
1369	Let 's take a look at the numeric features
770	Absolute latitude and longitude difference
1379	Let 's see the distribution of the numeric features
1257	Get the validation and test datasets
420	Confusion Matrix
308	Let 's plot the word clouds
462	Normalize Latitude and Longitude
1169	Catagories and Occurrence
524	Let 's calculate the precision and recall scores .
1540	checking missing data in feature matrix
1421	World COVID-19 Prediction
1162	Let 's look at the number of unique classes in the training set .
1268	Average timing for 1 iteration
483	Transform text into a vector
1347	Non LIVINGAREA_MEDI and non_LIVINGAREA_MODE
1338	Let 's see the distribution of the number of missing values in each column .
282	Let 's create a new column called `` commit_num '' .
659	Let 's take a look at the correlation matrix of the target variable .
1088	Let 's take a look at the first frame of the video
875	Let 's look at the hyperparameters
1001	Compile the model
718	Let 's see the correlation between the two features .
117	Let 's create a list of Xmas date
1279	Check the number of records
211	Import necessary libraries
1087	Import modules
1133	Let 's see what kind of browser we 're using .
1402	Import necessary libraries
1055	Load the data
729	Import necessary libraries
686	Let 's take a look at the test data .
226	Let 's create a new column called 'dropout_model ' and 'lb_score ' .
1386	Let 's see the distribution of the numeric features
305	Let 's define some hyperparameters .
1033	Let 's check the shape and type of the output image .
17	Predictions
1263	BERT and DISTILBERT
1199	Let 's create a function to create the data
792	Let 's see the number of features we have .
1111	Let 's have a look at the data .
1278	Import necessary libraries
76	F1 score
245	Let 's create a new feature called ``LB_score '' .
1325	Let 's see the number of columns that have only one value .
77	Let 's start with resnet50 .
973	Let 's get the name of the patient .
464	Load the data
873	One-hot encoding
1570	Import necessary libraries
806	Hyperopt Trials
813	Let 's see the ROC AUC vs Iteration
1210	checking missing data in merchant table
1393	Let 's see the distribution of the numeric features
791	Feature Importance
1173	Let 's start with 200 features .
1558	Remove Stopwords
880	Score as Function of Learning Rate and Estimators
948	Checking for Null values
942	Bureau_balance - Basic
578	Italy
552	Combining augmentation
1254	Importing necessary libraries
657	Load the data
200	Let 's take a look at one of the patients .
1389	Let 's take a look at the numeric features
861	Let 's start with the training data .
749	Train and Validation Split
227	Let 's create a new column called 'dropout_model ' and 'lb_score
250	Let 's take a look at Spain
486	Let 's see the shape of the text
915	Top 100 Features
340	Let 's create a simple model .
949	Let 's see the distribution of merchant_id and merchant_num_feats
302	Let 's define some params .
715	Let 's see the correlation between the signals .
955	Train Test Split
1569	Let 's see the distribution of id_error values
11	Outliers
907	Bureau_balance , bureau_agg , bureau_agg_new , bureau_balance_count
1584	Extract host and timestamp from image filename
1203	Let 's take a look at the number of visits per air store .
497	Bureau_balance
188	Let 's see the distribution of the brand name .
1117	Preprocess the data
1474	Select the plate group for a given experiment
1291	Let 's encode the year and month .
201	Let 's resample first_patient_pixels to get the same spacing as before .
703	Check for missing values
1356	Let 's see the distribution of the numeric features
894	Average Term of Previous Credit
31	Let 's take a look at the KMeans
109	Augmentation Pipeline
937	Let 's see the number of features
947	List of input files
1477	Set the seeds
920	Load the model
413	Create a Data Generator
1483	Sample Patient 2 - Lungacity
677	Let 's take a look at a sample of hits
1280	Let 's look at the top of the article
1008	Let 's load the data .
202	Normalize the Image
1392	Let 's see the distribution of the numeric features
1300	Let 's take a look at the size of the data .
1531	Distribution of kills
1310	Import necessary libraries
886	Number of boolean variables
933	Train and Test Split
488	Hashing with keras
921	Train Test Split
941	Load the data
1581	Read the data
1332	Add a new category
161	Let 's get the list of all files
205	One-hot encoding
1145	Let 's open a mask to see the shape of the image
713	Let 's calculate the number of phone and number of number of number of number of number of phone and number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number of number
600	Let 's calculate the Gini
24	Feature Extraction
43	Let 's see the distribution of question_asker_intent_understanding
1188	Process the patient images
459	Road ,Street , Avenue , Drive , Road ,Street , Avenue , Drive , Road ,Street , Avenue , Drive , Drive , Road , Drive , Road , Road ,Street , Avenue , Drive , Road , Drive , Road , Drive , Road , Drive , Road , Drive , Road , Road , Road , Road , Road , Road , Road , Road , Road , Road , Road , Road , Road , Road , Road , Road , Road , Road
453	Let 's convert year_built to int
4	Load the data
1481	Submission
944	Load the data
747	Write Optimization File
1516	Let 's take a look at the age of v2a
1335	Read the data
663	Let 's define the time columns
1112	Let 's take a look
168	Let 's see the number of clicks needed to download an app .
1245	Scatter plot
1515	Convert Household Type
1431	Let 's take a look at the distribution of age , gender , hospital_death , 'bmi
601	Let 's take a look at the public and private scores .
968	Curve for Cases
62	Let 's see the distribution of Fraud and Non-Fraud
546	Let 's see the number of stories in each year .
185	Let 's look at the price of each category .
666	Encode Full Data
115	Let 's see the number of unique values in each store and item .
321	Concatenate the data into a single data frame
553	Read the data
90	Read the training data
736	KNN with 20 neighbors
64	T-SNE
1030	Function to format the prediction string
45	Let 's take a look at the target distribution
1391	Let 's take a look at the numeric features
270	Dropout Model
1249	Let 's take a look at batch_cutmix
120	Let 's see the difference between FVC and expected FVC .
2	Let 's start with the training data .
706	Let 's look at the correlation matrix .
279	Let 's create a new column called `` commit_num '' .
230	Let 's create a new column called 'dropout_model ' and 'lb_score ' .
128	Let 's create a function that calculates the histogram of the values .
1190	Let 's define the learning rate .
1237	Logistic Regression
930	Let 's create a simple MLPClassifier
773	Minkowski Distance
292	Let 's create a new column called `` commit_num `` , which is the number of times the commit was made .
131	Let 's clean special characters .
1103	Preprocess the data
1517	Let 's see the distribution of age and meaneduc
985	Let 's see the distribution of TransactionAmt .
1004	List of Evaluitions
25	Submission
728	Average Education by Target and Female Head of Household
356	Random Forest
1494	Let 's define the lift function .
1377	Let 's see the distribution of the numeric features
412	D4D34af4f7 - D4D
705	Let 's take a look at the heads
701	Let 's see the distribution of the ` parentesco1 ` values .
313	Compute ROC AUC Score
1176	Let 's take a look at the number of links we have .
234	Let 's create a new column called 'dropout_model ' and 'lb_score ' .
1060	Predict on test data
156	Clear output and wait for it to finish
872	Remove low information features
286	Let 's start with the first 15 commit numbers .
1450	Distribution of number of clicks and proportion of downloads by device
1283	Read the data into a pandas DataFrame
107	We save before.pbz and sets.pbz
94	Let 's see the distribution of the keywords in the text .
1044	Let 's take a look at the public and private predictions .
1383	Let 's see the distribution of the numeric features
430	Encode categorical features
725	Let 's create a new column for each level .
1538	Let 's define the feature matrix and the feature definitions .
382	Import necessary libraries
463	Modelling
253	Germany
1504	Load the data
1046	Compile the model
377	Bagging Regressor
1497	Let 's create a function that returns the product of a and b
1151	Let 's see the distribution of var_91 for train and test data .
879	Score as Function of Reg Lambda and Alpha
1211	checking missing data in new_merchant
1059	Let 's load the image
916	Importing necessary libraries
688	Let 's create a function to get the DICOM filepath
820	Import necessary libraries
81	Let 's see the distribution of the animals ' Mix ' .
196	Create a Bulge Graph
1144	Convert categorical columns
1526	Let 's see the distribution of winPlacePerc
274	Let 's create a new column called `` commit_num '' .
1101	Let 's load the data
174	Let 's plot the download rate over the day .
110	Build the learning rate
1549	Let 's define a function that takes in a dataset as input and returns the data and target .
1513	Convert categorical variables to numerical variables
838	Let 's load the data into a pandas DataFrame
1321	Let 's create a new column 'sanitario1 ' , 'sanitario2 ' , 'sanitario3 ' , 'sanitario4 ' , 'sanitario5 ' , 'sanitario6 ' , 'sanitario7 ' , 'sanitario8 ' , 'sanitario9 ' , 'sanitario7 ' , 'sanitario8 ' , 'sanitario9 ' , 'sanitario7 ' , 'sanitario8 ' , 'sanitario9 ' , 'sanitario8 ' , 'sanitario9 ' ,
1243	Concatenate the types and sizes
1571	Average of the time series
80	Function to get the sex and neutered
1375	Let 's see the distribution of the numeric features
862	LGBM Classifier
982	Let 's see if there are any matches in the training set .
1082	Submission
1492	Importing necessary libraries
1104	Let 's have a look at the data .
934	Predict on test data
471	Merging the data
588	Sir Optimization
177	Let 's see the shape of the image .
472	Train Test Split
683	Number of training and test features with all 0 values
1270	Let 's see the performance of the model .
1063	Let 's create a new column `` isNan '' .
988	Let 's import the necessary modules
523	Let 's take a look at the first 6 decision functions
1172	Let 's see the number of unique and unk tokens .
1459	Prepare Training and Test Data
951	Join Train and Test
1299	Let 's check if there are any integer columns in the data .
708	Let 's look at the top 5 walls .
217	Import necessary libraries
1041	Write the Trial Table
1441	Let 's see the length of the train.csv file .
562	Let 's take a look at the first 10 masks for an image .
403	Let 's see the difference between the test and the training data .
697	There are some households where the family members do n't all have the same target .
1314	Replace edjefe
169	Let 's take a look at the distribution of IPs .
1533	Let 's see the distribution of winPlacePer and mean
22	Let 's take a look at the target variable .
922	Keypoints
297	Import necessary libraries
1099	Let 's take a look at the evaluation
1286	Let 's split the data into train and val .
195	T-SNE
636	Confirmed Cases by Population ( 2020
733	Import necessary libraries
671	Categories of items > 1M \u20 ( BDTop
970	Load the data
765	Let 's take a look at the Fare amount
498	Let 's create a function to group the data into two groups .
56	Let 's take a look at the number of zeros in the training data .
866	Let 's take a look at the feature matrix .
964	Let 's see the dependence of the returns
1267	Read the results file
1072	Importing necessary libraries
914	Import necessary libraries
492	Input layer
1556	HP Lovecraft
721	Education Distribution by Target
1014	Let 's compute the game time stats for each installation .
260	SGDRegressor ( SGDRegressor
672	Let 's see the price variance of the parent category .
1313	checking missing data in train data
1463	Let 's take a look at the first 1000 cities .
785	Fare Amount vs Time Since Start of Records
1006	Train the model
508	Let 's define constants .
1476	Importing necessary libraries
596	Let 's take a look at the class distribution
1225	Dropping ps_calc
822	Merge Train and Test Data
790	Linear Regression
58	Load the data
255	Andorra
847	Let 's take a look at the param_grid
1445	Let 's load the data .
79	Submission
265	Bagging Regressor
528	Let 's define some params
68	Initial function
1547	GloVe tweet data
563	Masks Over Image
505	Let 's take a look at the shape of the data
1000	TPU Setup
111	Let 's start with the data .
512	Spreading Spectrum
650	Let 's check the number of missing values in each column .
72	Checking for missing values in the training and testing data
953	Initialize the data
37	Let 's take a look at the distribution of age approx
1043	Preprocess inputs for seq_length
924	Let 's take a look at the distribution of CNT_CHILDREN and TARGET .
352	Let 's take a random sample from the training data .
1424	Let 's see the best model for each country .
1521	sigmoid_np ( sigmoid_np ( sigmoid_np ( )
378	ExtraTrees Regressor
1565	We need to import the necessary modules
823	One-hot encoding
530	Load the data
1527	Let 's see the distribution of assists
358	Load the data
756	Let 's take a look at the bounding boxes of each image .
411	Let 's take a look at the md5 hashes .
567	Load the data without drift
171	Let 's see the distribution of download by click .
1131	Let 's transform the data .
1512	Load necessary libraries
103	Let 's see the distribution of the model predictions .
218	Dropout Model
1457	Seed everything for reproducibility
939	Submission File
267	AdaBoost Regressor
1128	Let 's take a look at the SHAP model
49	Let 's create a list of all the columns we want to use .
178	Threshold Otsu
1206	Let 's take a look at the number of rooms and price
745	Confidence by Fold and Target
632	Let 's take a look at the distribution of the demand
1124	Let 's change the ` addr2 ` to ` addr
1136	Load the data
244	Let 's create a new column called 'dropout_model ' and 'lb_score ' .
972	First DICOM
485	It was the best of times , it was the age of foolness
73	Import necessary libraries
1070	Let 's create a simple ARC solver .
1342	Let 's see the distribution of the number of missing values in each column .
1353	Categorical Features
249	Let 's define the fa function .
1449	Distribution of IP Address
643	Remove outliers and target
1435	Let 's define the number of unique features and the number of other unique features
1241	Let 's see the shape of the data set .
1265	Let 's create a list of all trainable variables .
1092	Feature importance
793	Let 's see the distribution of Validation Fares
1159	Predict on Test Data
1585	Import twosigmanews
776	Split data into train and validation
1042	Pickle the best model
87	Import necessary libraries
587	Let 's calculate the number of infected and deaths for each country .
1013	Apply convolution
864	Let 's take a look at some of the features
1502	Load the data
145	Let 's look at the files .
366	Let 's compute the histogram of the image .
327	Linear Regression
353	Create Entity Set
235	Let 's create a new column called 'dropout_model ' and 'lb_score ' .
183	Let 's check the shape of the data .
173	Number of clicks over the day
1309	Load the pretrained model
21	Distribution of Wheezy-copper-turtle-magic
1473	Create Model
548	Bathroom Count Vs Log Error
134	Let 's free up some memory
1293	Import necessary libraries
679	Extract images from zip file
681	Load the necessary libraries
612	Let 's define some hyperparameters .
829	Let 's keep only the features with cumulative importance < 0.95 .
280	Let 's create a new column called `` commit_num
326	Let 's define X_identityx and y_severe_toxic and y_obscene
193	Let 's see the length of coms length
984	Import necessary libraries
662	Map ord_1 to full_data.ord_1
1002	Let 's create a list of original fake images paths
834	Merge bureau_info
144	Let 's see the number of unique values in each column .
1485	Sample Patient 2 - Lung Opacity and Masses
760	Let 's define a function that calculates the lb_dist between the training and validation set .
1458	Let 's add the start and end positions to train and test data .
931	Convert to RLE
329	Linear SVr
801	Let 's define some hyperparams
1454	Let 's do the clustering .
669	Let 's take a look at the top 100 ingredients .
213	Let 's take a random sample from the training data .
330	SGDRegressor ( SGDRegressor
758	Let 's take a look at the distribution of the surface
153	Let 's calculate the f-beta score .
605	Let 's fix some of the public submissions
336	Bagging Regressor
421	Confusion Matrix
622	Perform_feature_agglomeration
256	Let 's drop the ID column .
1501	Let 's seed everything .
1235	Let 's take a look at the LB score .
551	Let 's create a Gaussian target noise .
763	Load the data
474	Let 's define some params
1544	Tokenize Text
1413	Data Augmentation
522	Classification Report
764	Let 's see the distribution of Fare
312	Define the path and the training and validation directories .
1224	Dropping ps_calc
194	Description length VS price
1021	Build the model
1242	Let 's see the sizes of the stores .
328	acc_model ( acc_model
1274	SK_ID_CURR - BUREAU
613	CNN sentiment
1228	Logistic Regression
1106	Let 's load the UCF file and load the test labels
197	We can use neato to render the image .
545	Let 's see the correlation between the features
928	Let 's see the length of the comment length .
1221	Load the data
1134	Import necessary libraries
1396	Let 's take a look at the numeric features
855	Train the best model from random search scores
154	SAVE MODEL TO FILE
877	Let 's create a new column called 'set ' .
555	Standard Scaling
1067	Build and Submit
408	Let 's import the necessary libraries .
228	Let 's create a new column called 'dropout_model ' and 'lb_score ' .
568	VarianceThreshold is a threshold for the number of open channels . It is a threshold for the number of open channels .
1204	Build the multi-layer model
1535	Let 's define a function that calculates the distance between two points .
1285	This function computes the square matrix for each element in the input list .
704	Let 's see if we covered every variable .
301	Let 's split the data into dense and categorical features .
1420	China
645	Number of unique label
494	Define the Model
771	Distribution of Fare Amount by Number of Passengers
1098	Let 's see the distribution of train and test samples .
748	Let 's save the trials
1495	Let 's define a function to print the description of the program .
976	Extract DICOM tags from dicom
1232	Let 's take a look at the cross_validate_lgb
439	MOST FREQUENT METER TYPE
34	Identity Hate
1453	Load the data
354	Let 's look at the correlation matrix .
1455	Function to format the prediction string
1115	Let 's load the data
1437	Convert click time to int64 format .
731	Random Forest Classification
1085	Let 's import the necessary libraries
1511	Create video
258	acc_model ( acc_model
858	Let 's import the altair package .
664	One-Hot Encoding
999	Let 's check the performance of the model .
1506	Let 's define a function that takes in a dataset as input and returns the data and target .
469	Predict on Test Data
1218	Let 's start with the validation results .
804	Let 's train the model and save the results .
1324	Let 's add some new features to the train and test data .
1448	Convert time to category
499	Let 's take a look at the distribution of the application data .
714	Let 's see the correlation between the features .
1578	Let 's import the necessary modules .
242	Let 's create a new column called 'dropout_model ' and 'lb_score ' .
1100	Let 's check the shape of the output shapes .
207	Let 's start with the training data and validation data .
853	Train the best model from Grid search score
1045	Build the model
1048	Build the new dataframe
998	Load the site 4 data
746	Submission
1164	Let 's look at the number of unique values for each class .
266	ExtraTrees Regressor
106	Load before matrix
1572	Let 's take a look at the number of visits per day .
1157	Concatenate Wins and Loss
1586	Let 's take a look at the first day of the week .
331	Let 's start with the decision tree .
50	Let 's take a look at the distribution of the training data .
680	Import necessary libraries
276	Let 's start with the first 5 commit numbers .
579	Let 's see the number of confirmed cases by day .
1054	Filter Submission
1171	Let 's take a look at the word length of each sentence .
1519	t-SNE visualization in 3 dimensions
490	Build the model
1372	Let 's take a look at the numeric features
1316	Let 's look at the continuous features .
738	Random Forest Classification
1158	Fit with Grid Search
158	Let 's import the necessary libraries
1223	Let 's encode the ps_ind_02 , ps_ps_03 , ps_ps_09 , ps_ps_ind_02 , ps_ps_03 , ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps_ps
1471	Import necessary libraries
851	Let 's take a look at param_grid
556	Concatenate full text and featurefull_text
1433	Import necessary libraries
902	Let 's see the correlation between the target and the features
531	Hour of the Day
19	Let 's take a look at the target distribution
95	Let 's look at the frequency of each word in the corpus .
495	Load the data
91	Gene Frequency
32	Load the data
712	Bonus Variable
167	Number of clicks by IP
1559	Lemmatization
849	Let 's take a look at the learning rate
519	Let 's take a look at cross val
1246	Concatenate Store and Weekly Sales
1212	Create Train and Test and Feature Engineering
946	Adoption Speed
532	Day Of The Week
93	Dropping Gene and Varation
1553	Import necessary libraries
917	Let 's take a look at the cash balance
334	Train and Validation Split
444	Let 's see the distribution of meter readings across the weekdays .
96	Read the training data
938	Run the Light GBM model
346	Predictions
240	Let 's create a new column called 'hidden_dim_first ' and 'hidden_dim_second ' and 'hidden_dim_third
752	Random Forest Classification
7	Let 's see the distribution of the feature_1 values .
116	Let 's see the distribution of the whole data .
8	Load the data
1069	Cohen Kappa Score
251	Let 's create a list of all the dates .
1296	Let 's plot the loss and validation loss
380	Voting Regressor
911	Let 's take a look at the above threshold variables
691	Let 's create a function to process the outputs .
1177	Let 's read the DICOM file .
592	Let 's see the distribution of the sentiments
127	Let 's define the lung volume
521	Evaluate Threshold
304	MacroF1 metric
895	Let 's take a look at the number of installments paid and due dates .
165	Read the data
1534	Sieve of Eratosthenes
1523	Let 's take a look at the validation set
395	Let 's take a look at the size of the training images .
624	Let 's take a look at the inputs
83	Distribution of the number of neutered animals
557	Let 's take a look at the shape of the data
962	SHAP Interactions
515	Normalize the image
1370	Let 's take a look at the numeric features
13	Define parameters Back to Table of Contents ] ( toc
777	Let 's see the intercept
88	Let 's see the performance of our model .
369	acc_model ( acc_model
137	Let 's check the number of unique values and the number of NaN values .
104	Blaze face detection
342	Let 's load the data .
1462	Make the model and save it to file
151	Train Test Split
10	Let 's check the numeric data types .
263	Train and Validation Split
876	Random Search and Bayesian Optimization
257	Linear Regression
830	Feature Importance and Submission
454	Label Encode the primary_use column
318	Submission File
1071	Let 's create a simple ARC solver .
759	Replace missing values with 0 .
375	Train and Validation Split
824	Let 's see the correlation matrix
481	Train the LightGBM
415	Let 's look at the test images
675	Coefficient of variation ( CV ) for prices in different recognized image categories
727	Merge the final features
1322	Let 's create a new column 'abastaguadentro ' and 'abastaguafuera ' .
1498	Build Model
467	Let 's define a function to calculate the time taken to run the model .
1194	Train and Test Split
757	Load the data
956	Let 's take a look at the validation data .
637	Let 's create a function to create Lags .
638	Import necessary libraries
711	Target vs Warning Variable
470	Import necessary libraries
945	Convert the column types
619	Linear Regression
1363	Let 's see the distribution of the numeric features
350	Import necessary libraries
967	Logistic Growth Curve
1306	Split the training data into training and validation sets
1319	Let 's do the same for all three columns .
1185	Read the data
1330	Let 's take a look at the number of missing values in the training data .
1589	Let 's define the number of columns we need to predict .
817	Let 's see the cross validation score on the full dataset
1509	Add leak to test data
809	First of all let 's try to find the optimal fmin function .
1358	Let 's see the distribution of the numeric features
409	Let 's see if there are any duplicates in the training set .
1510	Create a function to create video
163	MinMax + Mean Stacking
1337	Let 's see the distribution of the number of missing values in each column .
1126	Let 's create a submission .
1120	Let 's take a look at the SexuponOutcome
269	Let 's create a simple model .
1320	Let 's see the distribution of public , planpri , noelec , coopele
1168	Import necessary libraries
831	Let 's import the necessary libraries .
320	Let 's see the binary value of the target variable .
980	Let 's take a look at the DICOM
543	Import necessary libraries
811	Bayesian and Random Search Tuning
1184	Importing necessary libraries
231	Let 's create a new column called 'dropout_model ' and 'hidden_dim_first ' .
1384	Let 's see the distribution of the numeric features
647	Load previous sucessful model
835	Load previous application data
558	Trainship_segmentation_v
348	Generator
479	Submission
1108	Let 's have a look at the data .
805	Hyperopt TPE
1097	Concatenate the sample_struc and id columns
724	Let 's take a look at the range of idhogar
1152	Import necessary libraries
1150	Load the test data
436	One-vs-rest classifier
1275	SK_ID_PREV & SK_ID_CURR
371	SGDRegressor ( SGDRegressor
417	Load the meta data
285	Let 's create a new column called `` commit_num `` , which is the number of times the commit was made .
60	Liste existstrun graph
1272	Number of repetitions for each class
1083	Let 's load the test data .
867	Let 's start with the feature matrix .
1427	Prediction of COVID-19 Province/State
611	Load word embeddings
808	First of all we need to find the optimal hyperparameters . First we need to find the optimal hyperparameters
451	Dew Temperature
699	There are some households where the family members do n't all have the same target .
1359	Let 's see the distribution of the numeric features
1467	Let 's see the number of sales per state .
372	Let 's start with the decision tree .
315	Let 's delete the base directory .
489	Tokenization
689	Let 's read the DICOM file .
926	Import necessary libraries
698	Let 's see the number of households with a head .
1183	Create a Data Generator
1339	Let 's see the number of missing values in each column .
816	Load the data
901	Bureau_ID_CURR and bureau_ID_CURR
1256	Let 's create an iterator that yields examples from the JSON files .
1367	Let 's see the distribution of the numeric features
99	Import necessary libraries
832	Principal Component Analysis
155	Clear output and wait for it to finish
750	Poverty Confusion Matrix
84	Let 's see the distribution of the outcomes of the animals .
1488	Sample Patient 6 - Normal Lung Nodules and Masses
1215	Test Data Augmentation
1015	Let 's create a function to create a title mode
1186	Process the patient images
1018	Load the data
676	Import necessary libraries
529	Compile and fit model
33	Tf-Idf
1191	Train and Validation Split
1193	Preprocess the image
695	Number of Unique Values
1073	Import necessary libraries
936	Bureau Bureau Bureau Bureau Bureau Bureau
1262	Importing necessary libraries
338	AdaBoost Regressor
303	Define parameters Back to Table of Contents ] ( toc
132	Let 's clean up the text .
1166	Load the data
620	Linear OLS
594	Let 's see the most common words in the temp list .
349	Let 's define a generator function .
1303	Check for missing values in test data
739	Submission
502	Applicatoin train shape
1058	KNN logloss on longitude and latitude
1403	Let 's take a look at the autocorrelation
618	Function to perform KNN on the test data
989	Let 's take a look at the Bkg Color
1312	Data Augmentation
208	MinMaxScaler ( MinMaxScaler
1354	Let 's see the distribution of the numeric features
1200	Create Train and Test
113	Read the data
287	Let 's calculate the weight of each commit .
742	Random Forest Classifier
359	We 're going to make a function that turns the input into a function that turns the output into a function that turns the output into a function that turns the output into a function that turns it into a function
493	Build the network
954	Load the data
466	Get the path of the test image
126	Hounsfield Units ( HU
1240	Let 's start with the year , month , week and day .
129	Let 's check the memory usage .
783	Random Forest Prediction
1525	Load the necessary libraries
668	Let 's take a look at the top 10 labels
82	Let 's see the distribution of Sex and Outcome Type
215	Let 's look at the correlation matrix .
646	Let 's split the training data into a list of labels
633	Load the data
919	Train and Validation Split
1394	Let 's take a look at the numeric features
1344	KDE for days_BIRTH
1395	Let 's take a look at the numeric features
509	Let 's get the labels for each subject and zone .
1259	Let 's create a function to get the predictions for the validation set .
1167	Compile the model
164	MinMax + Median Stacking
627	Let 's look at the number of bookings and total
367	Let 's create a function that reads the image and returns the data as a numpy array .
1576	Let 's load the data .
1114	Let 's take a look at the LB score
393	Let 's take a look at the training data .
1028	Train the model
1050	Let 's check if the sample images are in the training set .
802	Set the hyperparams
565	Prediction
1057	Predict on Test Data
1217	Create Trainers and Evaluators
1027	Build the model
1422	The World COVID-19 Prediction ( without China Data
477	Build LightGBM
974	Let 's look at the keyword dictionary .
484	Vectorize the text
1148	Load the data
609	Embedding
101	Let 's check the number of train and val samples
1062	Concatenate the test and submission data
1410	Let 's create a list of all the features we want to predict .
443	Let 's see the distribution of meter reading
291	Let 's create a new column called `` commit_num '' .
1037	Train History
457	Most commmon IntersectionID 's
1036	Preprocess inputs for test data
796	Submission
898	Let 's start with the test data .
135	Let 's load the data
1295	Let 's see the accuracy of the model .
416	Sales evolution - 2017
826	Let 's see the shape of the data .
1031	Let 's draw the bounding boxes of the output image .
870	Feature Importance
794	Let 's take a look at the data
1056	KNN Classification
40	Light GBM Features
384	We 'll use the des_bw_filter_lp and des_bw_filter_hp functions .
730	Let 's create a pipeline .
1537	Feature Engineering
991	Let 's add a cylinder
192	Word Cloud
446	Let 's see the distribution of meter reading for each primary_use
392	Let 's look at the most frequent category
1554	Load the data
755	Let 's take a look at some images
570	Import necessary libraries
1552	Correlation
175	Read the data
644	Let 's split the labels into a list of labels
1464	Let 's take a look at the order
1251	We are going to use batch_grid_mask for this task . We are going to use batch_grid_mask for this task . We will use batch_grid_mask for this .
1142	Pytorch Lightning
975	Let 's take a look at the DICOM
222	Let 's create a new column called 'dropout_model ' and 'lb_score ' .
440	Meter Reading
825	Drop some columns from the training and testing data
1178	Let 's see the folder structure of the data .
429	Step filled vs. Step
1331	Add new category
969	Load the data
1480	Let 's fit the model and get the predictions .
1202	Predict on test data
987	Let 's take a look at one of the patients
283	Let 's create a new column called `` commit_num '' .
1590	Let 's import the necessary libraries
1076	Split the data into train and test
299	Light GBM
236	Let 's create a new column called 'dropout_model ' and 'hidden_dim_first ' .
1010	SAVE MODEL TO DISK
958	Submission
1446	Read the data
604	Let 's create a submission .
1470	Import necessary libraries
319	Create a function to create a filename
1442	Let 's generate a random sample of skiplines .
97	Load Test Data
450	Let 's see the air temperature distribution
670	Categories of items < 10 \u20 ( TOP
1016	Simple XGBoost
1468	Let 's take a look at the total sales per store .
723	Let 's see the distribution of age and v18q
1138	Let 's create a function to create a jpg tag for each image
977	Let 's take a look at the DICOM files
273	Let 's start with the first 6 commit numbers .
1541	Let 's take a look at the target variable .
6	Let 's see the distribution of the target variable
950	Let 's see the types of the categorical features .
978	Let 's define some functions that we want to apply to our output area .
807	Bayes Test
848	Let 's take a look at the learning rate
114	Make a copy of the data
1140	Load the image
1208	Let 's see the target distribution of feature_3 .
995	Submission
694	Load the data
351	Load the data
1428	Let 's take a look at the US Counties
316	Test Data Augmentation
1440	Load the data
574	Let 's replace the country with China .
732	Feature Importance
544	Let 's look at the data types of the data .
852	Let 's take a look at the best hyperparameters
865	Let 's see the distribution of the features
1419	Now that we 've created the full table , we 'll create a new column `` Active '' .
965	Feature Engineering
1034	Run the model on sample submission
1141	Let 's import necessary libraries .
734	Create MLP Classifier
264	Ridge CV
482	Load necessary libraries
1561	Lemmatize
1490	Sample Patient 6 - Normal and Unclear Abnormality
623	Let 's perform the variance threshold .
1156	Convert seeds to integers
70	Let 's run the kopt function .
1276	Feature Engineering
1524	Protein Classification
187	Let 's see the price of the first level categories .
639	Generate Train Validation Splits
1197	Let 's sort the comments by distance .
30	Submission File
496	Convert categorical and numerical features
762	Submission
779	Submission
427	Let 's define some constants that we will use later .
1328	Submission
323	Define Train and Validation Paths
923	CNT_CHILDREN - The number of childRENs in the application
139	Let 's create a new column called ord_5_1 , ord_5_2 and ord_5
685	Let 's see the distribution of target transaction values
504	Load the data
1213	Let 's define the folder names and the class names .
693	Importing necessary libraries
14	Tokenize Text
355	Let 's start with a simple linear SVr .
630	Let 's take a look at the hotel cluster
402	Let 's load the test data .
461	Concatenate Train and Test Data
1551	Let 's convert the ` train_df ` to numeric format
943	Let 's take a look at the credit card balance
1175	Let 's take a look at the number of links between previous_title and title
1574	Time Series Analysis
1	Import necessary libraries
1281	Let 's create a function to extract the series we want from the training data
1258	Get the pretrained model
289	Let 's create a new column `` commit_num `` , `` dropout_model `` , `` FVC_weight `` , `` lb_score `` , `` dropout_model `` , `` FVC_weight
593	Most common words in selected text
1478	Preprocess the data
247	Finalize Ensembles
780	Fit and Evaluate
828	Let 's drop the features with zero values .
1196	Number of annotators and comments
541	Set global parameters Back To Table
1579	Let 's plot the training and validation loss .
1091	Light GBM
1277	Random Forest Classification
1032	Let 's take a look at the shape of the image
952	Let 's drop the first_active_month and drop other features .
1550	Importing necessary libraries
1417	Logistic Regression
3	Let 's check the files .
1360	Let 's see the distribution of the numeric features
1562	Lemma CountVectorizer
507	Reducing the target variable
1432	Difference between h1_ and d1_
1181	Preprocess the image
635	Let 's create a function to convert the data into a pandas DataFrame .
1090	Reducing
1269	Define input layer
210	Let 's see the feature importance .
1482	Sample Patient 1 - Normal Image
281	Let 's create a new column called `` commit_num `` , which is the number of times the commit was made .
547	Bedroom Count Vs Log Error
1352	Let 's drop the columns that we do n't need .
1503	SAVE DATASET
940	aggs1 - aggs_num_basic aggs_cat_basic aggs_num_basic aggs_num_basic aggs_cat_basic aggs_num
311	Let 's take a random sample of the training data .
142	Let 's drop the categorical and continuous columns .
0	Let 's look at the target distribution
16	Convert toxic predictions
795	Evaluate
1187	Process the test data
1077	Let 's generate a random permutation of the data .
1195	Number of records where toxicity_annotator_count is 1
42	Spearman correlation
1456	Import necessary libraries
118	Number of data points
1121	Let 's take a look at the distribution of outcomes and neutered animals
147	Let 's define the learning rate and the number of epochs
1414	checking missing data in train data
1260	Let 's compute the f1 scores
1400	Let 's take a look at the numeric features
1086	Submission
85	Let 's calculate the age in years
559	Let 's check if there are any missing values in the images .
1222	Concatenate with frequency encoding
383	Configure parameters Back to Table of Contents ] ( toc
737	Let 's start with the ExtraTreesClassifier .
376	Ridge CV
295	Submission
419	Decision Tree Classifier
580	China Cases by Day
961	Let 's take a look at the number of months in each year .
1588	Let 's look at the number of unknown assets .
399	Import necessary libraries
608	Setting the max_text_length and max_text_length
1127	LGBM Classification
564	Submission File
1205	Let 's look at the mode of the build year .
642	Feature Engineering
591	Generate Word Cloud
1252	Let 's encode the sex .
966	Growth Rate
799	Baseline AUC on the test set
136	Let 's see the number of unique values in each column
726	Let 's see the correlation between the upper and lower correlated features
887	Let 's look at the app types .
29	Let 's take a look at AUC score
554	Let 's factorize the categorical features .
960	Let 's see the shape of the test data .
1182	Train and Validation Split
146	Let 's take a random image .
400	Setting up the data
268	Voting Regressor
1292	Let 's see the FVC of the test data .
1130	Dropping V109_V1 and V
1011	Let 's read the image and resize it .
27	Let 's check the files .
850	Let 's create a DataFrame with the results we want to predict .
986	Let 's transform the cleaned data .
254	Albania
309	Let 's check the size of the files
300	Define XGB Parameters
717	Most negative Spear correlations
74	Seed everything for reproducibility
1407	Load the data
935	Let 's see the distribution of the data .
157	Let 's import the required modules .
1081	Display Blry Samples
869	Load the feature matrix
510	Let 's create a function to get a single image from a file
1461	Let 's see the sentiment of the test set .
373	Gradient Boosting with Grid Search
1486	Sample Patient 4 - Ground-Glass Opacities 5 - Consolidation
740	Submission
1078	Augmentation with albu
836	Extract installments from installments.csv
682	Let 's see the shape of the data .
860	Load the data
1388	Let 's see the distribution of the numeric features
335	Ridge CV
180	Let 's check the number of separate components / objects detected .
1484	Sample Patient 3 - Lung Nodules and Masses
396	Check for missing values in test_metadata_csv
656	Import necessary libraries
108	TPU Setup
1518	T-SNE
1022	Train the model
1348	Applicatoin Train
343	Let 's check the shape of the data .
48	Let 's see the distribution of the target variable .
1253	Let 's see the distribution of cod_prov
52	Let 's take a look at the distribution of the target variable .
67	Import necessary libraries
550	No Of Storeys Vs Log Error
298	Prepare Training Data
1160	Let 's encode the category_id .
1438	Importing necessary libraries
1378	Let 's see the distribution of the numeric features
1304	Fill missing values
1499	Let 's take a look at the day of the week of the year .
133	We need to free up space .
271	Let 's see the weight of each commit .
743	Feature Selection Scores
1374	Let 's see the distribution of the numeric features
1129	Let 's import the necessary libraries
1284	Let 's calculate the validation score for the proposed model .
1333	Concatenate Train and Test Data
990	Cylinder
152	CatBoost Classifier
1412	Let 's take a look at the distribution of the features
1123	Let 's take a look at the start date .
341	Calculate IoU
503	Let 's take a look at the distribution of the application data .
1308	Load the data
1361	Let 's see the distribution of the numeric features
460	Let 's define the north south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south south
121	Let 's see the correlation between the features .
1161	Let 's define X_col , Y_col , Z_COL , HUE_COL , X_col , Y_col , Z_COL , HUE_COL
1409	Let 's take a look at missingno
1153	Let 's compute the rolling mean for each store .
41	Read the data
540	Correlation between bedrooms and bathrooms and price
857	Let 's take a look at the results
47	Let 's see the distribution of the target variable .
566	Get the list of test audio files
789	Let 's define time features .
1329	Import necessary libraries
616	SVr
598	Gini on perfect submission
491	Compile the model
363	Checking for duplicate clicks with different target values in train data
1557	Let 's take a look at the first sentence of the training data .
1364	Let 's see the distribution of the numeric features
229	Let 's create a new column called 'dropout_model ' and 'lb_score ' .
1587	Highest trading volumes
1491	Sample Patient 6 - Normal and Unclear Abnormality
1466	Install necessary libraries
628	Let 's take a look at the number of bookings and total
710	Let 's create a new column called 'warning ' .
1430	Importing necessary libraries
1209	YYYYMMZ ( YYYYMMZ ( YYYY
1201	Compile and fit model
89	Tokenize Train Comments
191	No description
625	Let 's look at some of the features .
1064	Let 's load the image
599	Random Submission
844	Train and Test Split
46	Let 's take a look at the distribution of target values
665	Let 's apply imputer to the full data
842	We reset the index and reset the garbage collector .
1273	Let 's see the number of examples in the oversampled training dataset .
1568	Load the data into a pandas DataFrame
913	Let 's see the shape of the data .
1532	Let 's see the correlation between winPlacePerc and winPlacePerc
1074	Configure parameters Back to Table of Contents ] ( toc
1109	Let 's load the data
684	Let 's see the number of binary features in the training set .
815	Boosting Type
290	Let 's create a new column called `` commit_num
1545	Load the data
778	Baseline Model
1066	Train and Validation Split
452	Wind Speed
786	Fare Amount by Hour of Day
702	Checking for missing values
394	Let 's take a look at the number of available images and the number of classes
379	AdaBoost Regressor
1536	Let 's replace 365243 with np.nan .
582	Let 's take a look at the number of confirmed cases by day .
774	Correlation with Fare Amount
648	Train
1029	Train the model
501	Correlation between application_train and application_train.csv
959	Load the data
1007	Train the model
476	Merging the data
884	Correlation Heatmap
561	TTCGA-G9-6362-01Z-00-DX
597	Perfect Submission
1053	Create a function to create the test generator
1084	Build TFAuto Model
38	Let 's take a look at some of the images .
1227	Drop the id and target columns
1125	Let 's create a function to change the addresses .
86	Let 's calculate the age category .
586	Let 's see if it 's time to run the model .
1583	Let 's check the format of the image data
123	Pulmonary Condition Progression by Sex
1095	SN_filter
294	Let 's create a new feature called ``LB_score '' .
1529	Distribution of headshotKills
1113	L1 Peak
221	Let 's create a new column called 'dropout_model ' and 'hidden_dim_first ' and 'hidden_dim_second
1327	Load the data
1216	Setting up the data
769	Let 's take a look at some of the images
845	LightGBM Classifier
1122	Importing necessary libraries
1555	All Words
1508	Let 's look at the features and their RMSs .
603	Public Absolute Difference
1479	Tabular Model
744	The metric used for this competition is macro_f1 .
140	Convert categorical features to continuous features
800	Let 's see the distribution of the learning rate
1137	Data Augmentation
767	ECDF
840	Let 's look at the credit balance
1582	Let 's take a look at the sample data
1189	Let 's take a look at the shape of the data
1026	Create Train and Validation Dataset
1452	Calculate Extra Time Series
1198	Let 's take a look at the size of the data .
1244	Concatenate the data
1180	Load the data
908	Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau Bureau
602	Let 's see the distribution of public and private difference .
971	Let 's take a look at the data .
1231	Let 's cross validate the output of XGBoost .
754	Random Forest Classification
649	Convert to RLE
782	Random Forest
878	Random Search & Bayesian Search
833	Let 's create a function that aggreates the values of the parent variable with respect to the child variable .
673	Coefficient of variation ( CV ) for prices in different categories
225	Let 's create a new column called 'dropout_model ' and 'lb_score
997	Let 's take a look at the site we are looking at
1465	Let 's create a new column 'visitStartTime ' and 'previous_visitStartTime ' .
1543	Let 's see the correlation between the two plots .
1577	Add missing values to train and test data
441	Meter Reading
629	Let 's take a look at the number of bookings and total
1345	KDE for EXT_SOURCE_2
981	Let 's take a look at the top bottom corner of the image
979	Get a list of random patients
388	Let 's take a look at the test data .
641	Import necessary libraries
150	Let 's start with the test data
1305	Convert categorical variables to categorical variables
1170	Let 's take a look at the number of comments in the training set .
306	Load the data
1207	Distribution of investment and owner occupier
199	We can use neato to render the image .
339	Voting Regressor
1005	Define DenseNet
186	Let 's extract the first level of categories .
401	Load the data
996	Let 's take a look at the site 0 .
634	Load the data
652	Let 's take a look at the high and low values
1110	Preprocess the data
621	Ridge regression
1426	Let 's take a look at some stats
1362	Let 's see the distribution of the numeric features
275	Let 's create a new column called `` commit_num '' .
424	Confusion Matrix
606	Import necessary libraries
284	Let 's create a new column `` commit_num `` and `` dropout_model `` and `` F_weight `` .
1368	Let 's take a look at the numeric features
1226	Let 's convert the probability to rank .
843	Feature Importance
259	Linear SVr
573	Let 's look at the number of confirmed deaths recovered
1146	Let 's import fastai.visionion
1505	Load Glove , and paragram and fasttext embeddings
198	Create a Bulge Graph
1429	Let 's look at the COVID-19 Prediction
585	Let 's see the distribution of the target country .
119	Let 's see the FVC distribution .
189	The top 10 categories of items with a price of 0 .
57	Calculate Mean Squared Error
1416	Drop the columns that match the regular expression
1229	Bernoulli NB
1024	Let 's load Distilbert tokenizer and save it .
317	Predict on Test Set
707	Let 's take a look at the ` area1 ` and ` area2 ` .
1343	Let 's see the distribution of the number of unique values in application_train .
678	Let 's take a look at the distribution of particles
324	Cohen kappa
882	Number of Estimators vs Learning Rate
69	Let 's calculate the distance between the tour and the data .
1385	Let 's see the distribution of the numeric features
899	Remove low information features
431	Remove duplicate questions
1019	Load the data
55	Let 's take a look at the percent of the missing values .
1592	Remove the columns that are of type 'float ' .
1105	Let 's load the data
1390	Let 's take a look at the numeric features
1052	Load the unet model
751	Import modules Back to Table of Contents ] ( toc
28	Let 's take a look at the distribution of the target variable .
1255	BERT and DISTILBERT
65	Let 's look at the training data .
322	Train and Validation Split
1371	Let 's see the distribution of the numeric features
447	Let 's take a look at the correlation matrix .
130	Count the number of words in each sentence
361	Let 's take a look at the sample wts
44	Embeddings
406	Stage 1b
722	Let 's see the distribution of age and escolari
170	Let 's see the download by click ratio
1346	KDE for EXT_SOURCE_3
53	Let 's take a look at the distribution of the missing values in the training data .
455	Predict on test data
810	Let 's save the trials as json
526	Let 's see the OLS of the model .
1318	Replace missing values with 0 .
514	Cropping function
209	Linear Regression
262	Gradient Boosting with Grid Search
293	Let 's create a new column called `` commit_num '' .
674	Load and Preprocessing Steps
932	Let 's initialize and load the data .
333	Gradient Boosting
426	Import necessary libraries
9	Let 's see the number of null columns and the number of data types
1418	Import necessary libraries
818	Submission
781	Let 's see the correlation matrix of the data .
261	Let 's start with the decision tree .
75	Let 's define the size of the image .
753	Exploring with GraphViz
741	Let 's see the correlation between the upper and lower features .
1580	Function to find all occurrences of search_str in input_str
239	Let 's create a new column called 'dropout_model ' and 'lb_score
1444	Load the data into a pandas dataframe
272	Let 's create a new column called `` commit_num '' which is the number of times the commit was made .
176	Let 's see the memory usage of the dataframe .
368	Linear Regression
927	Load the data
480	Import necessary libraries
432	Let 's take a look at the word clouds .
232	Let 's create a new column called 'hidden_dim_first ' and 'hidden_dim_second ' and 'hidden_dim_third ' .
1472	Let 's take a look at the number of plates that are present in the training set .
407	Let 's take a look at one of the images
784	Extract date information from test data
