10/31/2022 07:21:30 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=10, config_name='microsoft/graphcodebert-base', dev_filename='/raid/cs21mtech12001/Research/Notebooks_Dataset/splitted_data/competition_notebooks_with_atleast_1_medal_and_10_votes/with_spacy_summarization/english-code-tokens-with-sm/valid_dataset.jsonl', do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/raid/cs21mtech12001/Research/CodeBERT/Repository/GraphCodeBERT/code-summarization/output/notebooks/with_spacy_summarization/english-code-tokens-only-2/checkpoint-best-bleu/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=320, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3, output_dir='/raid/cs21mtech12001/Research/CodeBERT/Repository/GraphCodeBERT/code-summarization/output/notebooks/with_spacy_summarization/english-code-tokens-only-2', seed=42, test_filename='/raid/cs21mtech12001/Research/Notebooks_Dataset/splitted_data/competition_notebooks_with_atleast_1_medal_and_10_votes/with_spacy_summarization/english-code-tokens-with-sm/test_dataset.jsonl', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/31/2022 07:21:41 - INFO - __main__ -   reload model from /raid/cs21mtech12001/Research/CodeBERT/Repository/GraphCodeBERT/code-summarization/output/notebooks/with_spacy_summarization/english-code-tokens-only-2/checkpoint-best-bleu/pytorch_model.bin
10/31/2022 07:21:48 - INFO - __main__ -   Test file: /raid/cs21mtech12001/Research/Notebooks_Dataset/splitted_data/competition_notebooks_with_atleast_1_medal_and_10_votes/with_spacy_summarization/english-code-tokens-with-sm/valid_dataset.jsonl
  0%|          | 0/885 [00:00<?, ?it/s]  2%|▏         | 22/885 [00:00<00:03, 216.95it/s]  5%|▌         | 46/885 [00:00<00:03, 229.51it/s]  8%|▊         | 69/885 [00:00<00:04, 198.67it/s] 10%|█         | 90/885 [00:00<00:04, 198.25it/s] 13%|█▎        | 111/885 [00:00<00:04, 175.19it/s] 15%|█▌        | 133/885 [00:00<00:04, 180.08it/s] 19%|█▊        | 165/885 [00:00<00:03, 219.24it/s] 22%|██▏       | 192/885 [00:00<00:03, 226.72it/s] 24%|██▍       | 216/885 [00:01<00:03, 198.12it/s] 27%|██▋       | 237/885 [00:01<00:03, 199.23it/s] 29%|██▉       | 258/885 [00:01<00:03, 187.85it/s] 32%|███▏      | 285/885 [00:01<00:02, 209.34it/s] 35%|███▍      | 307/885 [00:01<00:02, 205.59it/s] 38%|███▊      | 339/885 [00:01<00:02, 229.14it/s] 42%|████▏     | 371/885 [00:01<00:02, 254.03it/s] 47%|████▋     | 416/885 [00:01<00:01, 309.09it/s] 52%|█████▏    | 458/885 [00:01<00:01, 340.02it/s] 56%|█████▌    | 493/885 [00:02<00:01, 334.85it/s] 60%|█████▉    | 527/885 [00:02<00:01, 327.90it/s] 63%|██████▎   | 561/885 [00:02<00:01, 314.69it/s] 68%|██████▊   | 604/885 [00:02<00:00, 342.86it/s] 72%|███████▏  | 639/885 [00:02<00:00, 288.74it/s] 76%|███████▌  | 670/885 [00:02<00:00, 281.76it/s] 80%|███████▉  | 707/885 [00:02<00:00, 298.95it/s] 83%|████████▎ | 738/885 [00:02<00:00, 301.02it/s] 87%|████████▋ | 769/885 [00:02<00:00, 294.37it/s] 91%|█████████ | 804/885 [00:03<00:00, 306.03it/s] 94%|█████████▍| 836/885 [00:03<00:00, 307.97it/s] 99%|█████████▊| 872/885 [00:03<00:00, 322.22it/s]100%|██████████| 885/885 [00:03<00:00, 266.11it/s]
  0%|          | 0/28 [00:00<?, ?it/s]/raid/cs21mtech12001/Research/CodeBERT/Repository/GraphCodeBERT/code-summarization/model.py:175: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  prevK = bestScoresId // numWords
  4%|▎         | 1/28 [00:08<03:40,  8.16s/it]  7%|▋         | 2/28 [00:10<01:59,  4.58s/it] 11%|█         | 3/28 [00:12<01:28,  3.53s/it] 14%|█▍        | 4/28 [00:14<01:11,  2.99s/it] 18%|█▊        | 5/28 [00:16<00:59,  2.59s/it] 21%|██▏       | 6/28 [00:18<00:50,  2.32s/it] 25%|██▌       | 7/28 [00:20<00:46,  2.22s/it] 29%|██▊       | 8/28 [00:22<00:41,  2.09s/it] 32%|███▏      | 9/28 [00:24<00:42,  2.26s/it] 36%|███▌      | 10/28 [00:26<00:38,  2.15s/it] 39%|███▉      | 11/28 [00:29<00:37,  2.23s/it] 43%|████▎     | 12/28 [00:32<00:42,  2.67s/it] 46%|████▋     | 13/28 [00:36<00:43,  2.90s/it] 50%|█████     | 14/28 [00:38<00:39,  2.79s/it] 54%|█████▎    | 15/28 [00:40<00:33,  2.55s/it] 57%|█████▋    | 16/28 [00:42<00:28,  2.40s/it] 61%|██████    | 17/28 [00:44<00:25,  2.30s/it] 64%|██████▍   | 18/28 [00:46<00:21,  2.14s/it] 68%|██████▊   | 19/28 [00:48<00:19,  2.13s/it] 71%|███████▏  | 20/28 [00:50<00:16,  2.10s/it] 75%|███████▌  | 21/28 [00:52<00:14,  2.05s/it] 79%|███████▊  | 22/28 [00:54<00:12,  2.05s/it] 82%|████████▏ | 23/28 [00:56<00:10,  2.01s/it] 86%|████████▌ | 24/28 [00:58<00:07,  1.96s/it] 89%|████████▉ | 25/28 [01:00<00:05,  1.90s/it] 93%|█████████▎| 26/28 [01:02<00:03,  1.92s/it] 96%|█████████▋| 27/28 [01:04<00:01,  1.99s/it]100%|██████████| 28/28 [01:06<00:00,  2.02s/it]100%|██████████| 28/28 [01:06<00:00,  2.38s/it]
Total: 885
10/31/2022 07:22:58 - INFO - __main__ -     bleu-4 = 20.12 
10/31/2022 07:22:58 - INFO - __main__ -     ********************
10/31/2022 07:22:58 - INFO - __main__ -   Test file: /raid/cs21mtech12001/Research/Notebooks_Dataset/splitted_data/competition_notebooks_with_atleast_1_medal_and_10_votes/with_spacy_summarization/english-code-tokens-with-sm/test_dataset.jsonl
  0%|          | 0/988 [00:00<?, ?it/s]  3%|▎         | 33/988 [00:00<00:02, 328.49it/s]  8%|▊         | 77/988 [00:00<00:02, 386.33it/s] 12%|█▏        | 116/988 [00:00<00:02, 348.43it/s] 15%|█▌        | 152/988 [00:00<00:02, 279.46it/s] 18%|█▊        | 182/988 [00:00<00:04, 167.16it/s] 21%|██        | 205/988 [00:01<00:05, 154.63it/s] 23%|██▎       | 224/988 [00:01<00:05, 133.66it/s] 24%|██▍       | 240/988 [00:01<00:06, 116.36it/s] 26%|██▌       | 254/988 [00:01<00:06, 109.09it/s] 27%|██▋       | 267/988 [00:01<00:06, 112.50it/s] 29%|██▊       | 283/988 [00:01<00:05, 117.67it/s] 30%|██▉       | 296/988 [00:01<00:05, 117.18it/s] 31%|███▏      | 309/988 [00:02<00:06, 108.41it/s] 33%|███▎      | 328/988 [00:02<00:05, 127.53it/s] 35%|███▍      | 344/988 [00:02<00:04, 131.18it/s] 36%|███▌      | 358/988 [00:02<00:10, 60.04it/s]  37%|███▋      | 369/988 [00:02<00:09, 66.26it/s] 39%|███▉      | 383/988 [00:03<00:08, 75.53it/s] 41%|████      | 401/988 [00:03<00:06, 94.45it/s] 43%|████▎     | 423/988 [00:03<00:04, 116.50it/s] 45%|████▍     | 443/988 [00:03<00:04, 133.56it/s] 46%|████▋     | 459/988 [00:03<00:03, 133.60it/s] 48%|████▊     | 479/988 [00:03<00:03, 144.43it/s] 52%|█████▏    | 509/988 [00:03<00:02, 182.73it/s] 54%|█████▎    | 529/988 [00:03<00:02, 185.51it/s] 56%|█████▌    | 549/988 [00:03<00:02, 177.37it/s] 57%|█████▋    | 568/988 [00:04<00:02, 166.97it/s] 62%|██████▏   | 609/988 [00:04<00:01, 228.65it/s] 65%|██████▍   | 639/988 [00:04<00:01, 247.87it/s] 69%|██████▊   | 679/988 [00:04<00:01, 289.88it/s] 72%|███████▏  | 716/988 [00:04<00:00, 310.38it/s] 77%|███████▋  | 756/988 [00:04<00:00, 336.09it/s] 80%|████████  | 791/988 [00:04<00:00, 333.33it/s] 84%|████████▎ | 825/988 [00:04<00:00, 334.30it/s] 88%|████████▊ | 869/988 [00:04<00:00, 360.83it/s] 92%|█████████▏| 912/988 [00:05<00:00, 380.31it/s] 96%|█████████▋| 951/988 [00:05<00:00, 382.41it/s]100%|██████████| 988/988 [00:05<00:00, 189.06it/s]
  0%|          | 0/31 [00:00<?, ?it/s]  3%|▎         | 1/31 [00:03<01:52,  3.74s/it]  6%|▋         | 2/31 [00:05<01:15,  2.59s/it] 10%|▉         | 3/31 [00:07<01:01,  2.20s/it] 13%|█▎        | 4/31 [00:09<00:56,  2.08s/it] 16%|█▌        | 5/31 [00:10<00:49,  1.90s/it] 19%|█▉        | 6/31 [00:13<00:50,  2.02s/it] 23%|██▎       | 7/31 [00:15<00:50,  2.10s/it] 26%|██▌       | 8/31 [00:17<00:46,  2.04s/it] 29%|██▉       | 9/31 [00:19<00:46,  2.10s/it] 32%|███▏      | 10/31 [00:21<00:44,  2.13s/it] 35%|███▌      | 11/31 [00:23<00:42,  2.11s/it] 39%|███▊      | 12/31 [00:25<00:39,  2.07s/it] 42%|████▏     | 13/31 [00:28<00:42,  2.36s/it] 45%|████▌     | 14/31 [00:32<00:48,  2.86s/it] 48%|████▊     | 15/31 [00:36<00:48,  3.02s/it] 52%|█████▏    | 16/31 [00:38<00:43,  2.91s/it] 55%|█████▍    | 17/31 [00:40<00:36,  2.63s/it] 58%|█████▊    | 18/31 [00:42<00:31,  2.46s/it] 61%|██████▏   | 19/31 [00:44<00:27,  2.29s/it] 65%|██████▍   | 20/31 [00:46<00:23,  2.17s/it] 68%|██████▊   | 21/31 [00:48<00:20,  2.06s/it] 71%|███████   | 22/31 [00:50<00:17,  1.98s/it] 74%|███████▍  | 23/31 [00:51<00:15,  1.90s/it] 77%|███████▋  | 24/31 [00:53<00:13,  1.90s/it] 81%|████████  | 25/31 [00:55<00:11,  1.86s/it] 84%|████████▍ | 26/31 [00:57<00:09,  1.90s/it] 87%|████████▋ | 27/31 [00:59<00:08,  2.01s/it] 90%|█████████ | 28/31 [01:02<00:06,  2.30s/it] 94%|█████████▎| 29/31 [01:06<00:05,  2.84s/it] 97%|█████████▋| 30/31 [01:09<00:02,  2.90s/it]100%|██████████| 31/31 [01:11<00:00,  2.59s/it]100%|██████████| 31/31 [01:11<00:00,  2.32s/it]
Total: 988
10/31/2022 07:24:16 - INFO - __main__ -     bleu-4 = 17.28 
10/31/2022 07:24:16 - INFO - __main__ -     ********************
