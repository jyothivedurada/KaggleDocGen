10/22/2022 12:26:09 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=10, config_name='microsoft/graphcodebert-base', dev_filename='/raid/cs21mtech12001/Research/Notebooks_Dataset/splitted_data/competition_notebooks_with_atleast_1_medal_and_10_votes/with_spacy_summarization/code-with-sm-only/valid_dataset.jsonl', do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='/raid/cs21mtech12001/Research/CodeBERT/Repository/GraphCodeBERT/code-summarization/output/notebooks/with_spacy_summarization/code-with-sm-only/checkpoint-best-bleu/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=320, max_steps=-1, max_target_length=256, model_name_or_path='microsoft/graphcodebert-base', model_type='roberta', no_cuda=False, num_train_epochs=3, output_dir='/raid/cs21mtech12001/Research/CodeBERT/Repository/GraphCodeBERT/code-summarization/output/notebooks/with_spacy_summarization/code-with-sm-only', seed=42, test_filename='/raid/cs21mtech12001/Research/Notebooks_Dataset/splitted_data/competition_notebooks_with_atleast_1_medal_and_10_votes/with_spacy_summarization/code-with-sm-only/test_dataset.jsonl', tokenizer_name='microsoft/graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/22/2022 12:26:18 - INFO - __main__ -   reload model from /raid/cs21mtech12001/Research/CodeBERT/Repository/GraphCodeBERT/code-summarization/output/notebooks/with_spacy_summarization/code-with-sm-only/checkpoint-best-bleu/pytorch_model.bin
10/22/2022 12:26:24 - INFO - __main__ -   Test file: /raid/cs21mtech12001/Research/Notebooks_Dataset/splitted_data/competition_notebooks_with_atleast_1_medal_and_10_votes/with_spacy_summarization/code-with-sm-only/valid_dataset.jsonl
  0%|          | 0/594 [00:00<?, ?it/s]  4%|▍         | 25/594 [00:00<00:02, 245.22it/s]  9%|▉         | 52/594 [00:00<00:02, 254.22it/s] 13%|█▎        | 78/594 [00:00<00:02, 237.52it/s] 18%|█▊        | 104/594 [00:00<00:02, 244.04it/s] 22%|██▏       | 129/594 [00:00<00:02, 201.22it/s] 26%|██▌       | 155/594 [00:00<00:02, 217.01it/s] 31%|███       | 183/594 [00:00<00:01, 235.18it/s] 35%|███▌      | 208/594 [00:00<00:01, 232.97it/s] 39%|███▉      | 233/594 [00:01<00:01, 236.11it/s] 44%|████▍     | 260/594 [00:01<00:01, 243.83it/s] 48%|████▊     | 287/594 [00:01<00:01, 250.90it/s] 53%|█████▎    | 316/594 [00:01<00:01, 261.09it/s] 58%|█████▊    | 343/594 [00:01<00:01, 234.42it/s] 62%|██████▏   | 368/594 [00:01<00:01, 215.59it/s] 66%|██████▌   | 393/594 [00:01<00:00, 222.23it/s] 71%|███████   | 419/594 [00:01<00:00, 230.11it/s] 75%|███████▍  | 443/594 [00:01<00:00, 221.79it/s] 79%|███████▊  | 467/594 [00:02<00:00, 224.67it/s] 83%|████████▎ | 494/594 [00:02<00:00, 232.92it/s] 87%|████████▋ | 518/594 [00:02<00:00, 227.15it/s] 91%|█████████▏| 543/594 [00:02<00:00, 230.20it/s] 95%|█████████▌| 567/594 [00:02<00:00, 230.69it/s]100%|█████████▉| 592/594 [00:02<00:00, 234.51it/s]100%|██████████| 594/594 [00:02<00:00, 232.32it/s]
  0%|          | 0/19 [00:00<?, ?it/s]/raid/cs21mtech12001/Research/CodeBERT/Repository/GraphCodeBERT/code-summarization/model.py:175: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  prevK = bestScoresId // numWords
  5%|▌         | 1/19 [00:15<04:31, 15.07s/it] 11%|█         | 2/19 [00:16<02:01,  7.14s/it] 16%|█▌        | 3/19 [00:18<01:12,  4.55s/it] 21%|██        | 4/19 [00:19<00:49,  3.31s/it] 26%|██▋       | 5/19 [00:21<00:37,  2.66s/it] 32%|███▏      | 6/19 [00:23<00:32,  2.53s/it] 37%|███▋      | 7/19 [00:25<00:28,  2.41s/it] 42%|████▏     | 8/19 [00:27<00:23,  2.17s/it] 47%|████▋     | 9/19 [00:28<00:19,  1.97s/it] 53%|█████▎    | 10/19 [00:30<00:16,  1.85s/it] 58%|█████▊    | 11/19 [00:31<00:13,  1.70s/it] 63%|██████▎   | 12/19 [00:33<00:11,  1.61s/it] 68%|██████▊   | 13/19 [00:34<00:09,  1.61s/it] 74%|███████▎  | 14/19 [00:35<00:07,  1.53s/it] 79%|███████▉  | 15/19 [00:37<00:06,  1.54s/it] 84%|████████▍ | 16/19 [00:38<00:04,  1.49s/it] 89%|████████▉ | 17/19 [00:40<00:02,  1.49s/it] 95%|█████████▍| 18/19 [00:42<00:01,  1.52s/it]100%|██████████| 19/19 [00:42<00:00,  1.31s/it]100%|██████████| 19/19 [00:42<00:00,  2.26s/it]
Total: 594
10/22/2022 12:27:09 - INFO - __main__ -     bleu-4 = 20.42 
10/22/2022 12:27:09 - INFO - __main__ -     ********************
10/22/2022 12:27:09 - INFO - __main__ -   Test file: /raid/cs21mtech12001/Research/Notebooks_Dataset/splitted_data/competition_notebooks_with_atleast_1_medal_and_10_votes/with_spacy_summarization/code-with-sm-only/test_dataset.jsonl
  0%|          | 0/675 [00:00<?, ?it/s]  4%|▎         | 24/675 [00:00<00:02, 234.72it/s]  8%|▊         | 55/675 [00:00<00:02, 274.19it/s] 12%|█▏        | 83/675 [00:00<00:02, 252.95it/s] 16%|█▋        | 111/675 [00:00<00:02, 258.54it/s] 21%|██        | 139/675 [00:00<00:02, 263.64it/s] 25%|██▍       | 166/675 [00:00<00:02, 254.33it/s] 28%|██▊       | 192/675 [00:00<00:01, 250.02it/s] 32%|███▏      | 218/675 [00:00<00:01, 248.72it/s] 36%|███▋      | 245/675 [00:00<00:01, 254.73it/s] 41%|████      | 274/675 [00:01<00:01, 264.09it/s] 45%|████▌     | 304/675 [00:01<00:01, 271.24it/s] 49%|████▉     | 333/675 [00:01<00:01, 276.12it/s] 53%|█████▎    | 361/675 [00:01<00:01, 260.50it/s] 57%|█████▋    | 388/675 [00:01<00:01, 262.52it/s] 61%|██████▏   | 415/675 [00:01<00:01, 253.09it/s] 66%|██████▌   | 444/675 [00:01<00:00, 262.64it/s] 70%|██████▉   | 471/675 [00:01<00:00, 258.53it/s] 74%|███████▎  | 497/675 [00:01<00:00, 250.76it/s] 77%|███████▋  | 523/675 [00:02<00:00, 252.72it/s] 81%|████████▏ | 549/675 [00:02<00:00, 247.79it/s] 85%|████████▌ | 574/675 [00:02<00:00, 226.41it/s] 89%|████████▊ | 599/675 [00:02<00:00, 232.78it/s] 92%|█████████▏| 623/675 [00:02<00:00, 232.91it/s] 97%|█████████▋| 654/675 [00:02<00:00, 254.18it/s]100%|██████████| 675/675 [00:02<00:00, 245.10it/s]
  0%|          | 0/22 [00:00<?, ?it/s]  5%|▍         | 1/22 [00:02<00:50,  2.42s/it]  9%|▉         | 2/22 [00:03<00:37,  1.89s/it] 14%|█▎        | 3/22 [00:05<00:33,  1.76s/it] 18%|█▊        | 4/22 [00:07<00:30,  1.69s/it] 23%|██▎       | 5/22 [00:08<00:28,  1.70s/it] 27%|██▋       | 6/22 [00:10<00:26,  1.68s/it] 32%|███▏      | 7/22 [00:13<00:30,  2.00s/it] 36%|███▋      | 8/22 [00:14<00:26,  1.87s/it] 41%|████      | 9/22 [00:16<00:23,  1.79s/it] 45%|████▌     | 10/22 [00:18<00:21,  1.76s/it] 50%|█████     | 11/22 [00:19<00:18,  1.68s/it] 55%|█████▍    | 12/22 [00:21<00:16,  1.68s/it] 59%|█████▉    | 13/22 [00:23<00:16,  1.82s/it] 64%|██████▎   | 14/22 [00:25<00:14,  1.79s/it] 68%|██████▊   | 15/22 [00:26<00:12,  1.73s/it] 73%|███████▎  | 16/22 [00:28<00:09,  1.64s/it] 77%|███████▋  | 17/22 [00:29<00:08,  1.65s/it] 82%|████████▏ | 18/22 [00:31<00:06,  1.67s/it] 86%|████████▋ | 19/22 [00:33<00:05,  1.69s/it] 91%|█████████ | 20/22 [00:34<00:03,  1.68s/it] 95%|█████████▌| 21/22 [00:36<00:01,  1.71s/it]100%|██████████| 22/22 [00:37<00:00,  1.33s/it]100%|██████████| 22/22 [00:37<00:00,  1.69s/it]
Total: 675
10/22/2022 12:27:49 - INFO - __main__ -     bleu-4 = 18.11 
10/22/2022 12:27:49 - INFO - __main__ -     ********************
