0	Retrieving the Data
1	Detecting the inputs
2	Create the dictionary
3	Import the libraries
4	checking missing data
5	Remove the number of duplicated columns
6	Distribution of Amount Distribution
7	As you can see the distribution
8	Distribution of Amount Credit
9	Contractoring the dates
10	Plotting the result
11	Plot the heatmap
12	Bureau for the missing values
13	Bureau for the missing values
14	Checking only one row
15	get the dataframes
16	Previous applications categorical features
17	one hot encoding
18	Read out the holdout data
19	Add to the dataframe
20	Rank test set
21	FIND NORMAL TO HYPERPLANE
22	make a submission
23	Fitting a simple linear model
24	Create Training and Validation Sets
25	make a score
26	Get the model
27	Predict for test
28	Get the prediction
29	Write predictions to csv
30	Scale and flip
31	CALCULATE MEANS AND STANDARD DEVIATIONS
32	SMOOTH A DISCRETE FUNCTION
33	STORE PROBABILITIES IN PR
34	DISPLAY PROBABILITY FUNCTION
35	Extract the holdout data frame
36	End for overlap
37	iterate through folds
38	Building Ridge model
39	make train data
40	make train data
41	iterate through train and validation sets
42	Prepare the data
43	Read data and merge
44	Previous applications numeric features
45	Previous applications categorical features
46	Count pos cash accounts
47	Count installments accounts
48	Count credit card lines
49	Setting the data
50	Examine the values
51	Pick the corresponding weights
52	We set the original values
53	Prediction on the new DataFrame
54	Import swkl data
55	Round the submission
56	Load the submission
57	Get the centroids
58	Get the centroids
59	Exploring the cluster
60	Finding the centroids
61	first sorted average
62	It can be ambiguous
63	Remove other ts
64	Plot the seaborn heatmap
65	Plot the autocorrelation
66	Plot the lag
67	Show distribution of parameters
68	distance between normal distribution
69	Load the Data
70	CALCULATE MEANS AND STANDARD DEVIATIONS
71	SMOOTH A DISCRETE FUNCTION
72	STORE PROBABILITIES IN PR
73	DISPLAY PROBABILITY FUNCTION
74	take most correlated features
75	visualize the products
76	the difference above
77	Checking for missing values
78	insert missing values
79	Checking for missing values
80	Remove missing values
81	Age Distribution
82	Exploring the data
83	Plot the DICOM files
84	Benign image visualizations
85	Visualising images
86	Taking mean of the image name
87	Reading the csv files
88	CALCULATE MEANS AND SKLE
89	Now lets prepare our data
90	Reading the datasets
91	We will need these functions later
92	Plot the distribution of the price
93	UpVote if this was helpful
94	Correlations of the feature
95	Compile and visualize model
96	Train NN model
97	convert a prediction for simple model
98	fill in the predictions
99	First , we import the data
100	Lets look at the third trace for each continent
101	Exploratory Data Analysis
102	Answers Sentiment
103	Importing the data
104	load the data
105	Creating the Data
106	Define the image transformations here
107	Lets fit our model
108	Pull test image
109	Downsize the image
110	Build a model
111	Compile the loss
112	save the model
113	load json and create model
114	Example of prediction
115	Kaggle test images
116	Plot the images
117	idea from this kernel
118	Creating tf.data objects
119	Load libraries and read in data
120	Exploring the data
121	Lets prepare some results
122	Setting Class Imbalance
123	Get the columns format
124	Train model and predict
125	Plot the distribution of trip duration
126	Plot the distribution of trip duration
127	Calculate the great circle distance between two points
128	Same for pickup datetime
129	Engineer feature engineering
130	plot the important features
131	Most of the houses have ATLEAST ONE of hour
132	Plot trip duration
133	Plotting the result
134	background library for learning
135	Train and validation data
136	Return the training inputs
137	Prediction on training dataset
138	Prediction on the validation set
139	Clear the results
140	for placeholder labels
141	save pneumonia location in dictionary
142	load and shuffle filenames
143	split into train and validation filenames
144	if augment then horizontal flip half the time
145	add trailing channel dimension
146	add trailing channel dimension
147	create numpy batch
148	define iou or jaccard loss function
149	create network and compiler
150	cosine learning rate annealing
151	create train and validation generators
152	load and shuffle filenames
153	retrieve x , y , height and width
154	save dictionary as csv file
155	Light GBM Results
156	Print the feature ranking
157	aggregation rules over household
158	deal with those OHE , where there is a sum over columns
159	Load the data
160	do feature engineering and drop useless columns
161	converting these object type columns to floats
162	Needed columns with redundant columns
163	Function for generating different data
164	get train and test data
165	Add train and test data
166	fit the estimator
167	Light GBM Results
168	Print the ranking ranking
169	aggregation rules over household
170	deal with those OHE , where there is a sum over columns
171	Load the data
172	do feature engineering and drop useless columns
173	converting these object type columns to floats
174	Needed columns with redundant columns
175	get relevant samples
176	Add train and test data
177	Split the train data into a training and the test data
178	the default value
179	Find useless features
180	Create a list of the public forest model
181	Find useless features
182	Set up the voting classifier
183	Function for cleaning the text
184	Compile the model
185	Importing Libraries and Loading Dataset
186	Select feature locations for putative matches
187	Perform geometric verification using RANSAC
188	Select feature locations for putative matches
189	Perform geometric verification using RANSAC
190	Naming the paths
191	Define some helper functions
192	Train the model
193	The size of the image
194	apply the weights
195	Assign the weights to the output image
196	the matplotlib way
197	Define the train and test for the question text
198	Plot the results
199	The basic Model
200	Kaggle test data
201	Word Cloud for all assets
202	the matplotlib way
203	Plot the order
204	the matplotlib way
205	Lets generate a wordcloud
206	the matplotlib way
207	the matplotlib way
208	the matplotlib way
209	the matplotlib way
210	Libraries and Data Loading
211	Get all zero features
212	Resize the revenue data
213	Daily revenues
214	Plotting the number of keywords
215	Drop target , fill in NaNs
216	Generate final dataframe
217	Loop over swim info
218	lihood of the patient
219	Collage of each borrower
220	Splitting the train data
221	Function for comparing different thresholds
222	Add custom preprocessing functions
223	TurnOff You can not use the internet in this competition
224	set some global variables
225	byte strings , so we need to decode it
226	find the intersection between text and selected text
227	get the start difference
228	predict validation set and compute jaccardian distances
229	decode test set and add to submission file
230	Introduction to BigQuery Model
231	Training the model
232	Get training statistics
233	Detect my accelerator
234	Load Model into TPU
235	What Makes LIME excellent
236	Visualizing a row
237	Visualizing a row
238	Visualizing a row
239	We can see there is no missing data
240	Lets see least frequent landmarks
241	Function to load a original image
242	Peek of the input data folder
243	Convert to datetime
244	Load the needed libraries
245	Read videos from Random Forest Model
246	print basic information on the dataset
247	print basic information on the dataset
248	Brain Development Functional Datasets
249	Initialize DictLearning object
250	Show networks using plotting utilities
251	Mean of all correlations
252	Then find the center of the regions and plot a connectome
253	Add as an overlay all the regions of index
254	print basic information
255	Plot the labels
256	First , we will plot
257	Fit and predict
258	Deleting unigram predictions
259	Get the number of words in the question
260	Calculate trend vector
261	Get label descriptions
262	Convert a single image
263	make the dataset
264	This cell is empty
265	Load the data
266	Importing necessary libraries
267	First , we merge train and test data
268	Plot the oil price
269	This needs to be tuned , perhaps based on amount of halite left
270	initialize the global turn data for this turn
271	filled in by shipid as a ship takes up a square
272	Do initalization things
273	we are called in competition , quiet output
274	return new Position from pos when action is applied
275	we wrap around
276	we wrap around
277	Manhattan distance of the Point difference a to b , considering wrap around
278	return distance , position of nearest shipyard to pos
279	global ship targets should already exist
280	in the direct to shipyard section
281	Not efficient for long lists
282	Now check the rest to see if they should convert
283	CHECK if in danger without escape , convert if h
284	Libraries For Fun
285	max of up model
286	example of a model
287	Heatmap for each location
288	change column
289	Number of features
290	add variables from bedrooms
291	Add to DataFrame
292	Encoding the categorical features
293	Define LightGBM parameters
294	The majority of match
295	Create arrays with features
296	Write Submission File
297	Import the libraries
298	Reading the data
299	Split train and validation sets
300	Write the submission
301	Data loading , we check
302	Remove any duplicate images
303	Splitting the train and test
304	Defining the label
305	Defining the label
306	Build the model
307	Evaluate the model
308	Evaluate the model
309	Extracting test predictions from original test set
310	Get the id column
311	Save the submission
312	Most common building
313	Plot the feature ranking
314	from import import importlib import reload
315	Get the accuracy of the model on validation data
316	Number of bedrooms
317	Support Vector Model
318	Helper Function for Submission File
319	Libraries and Configurations
320	Train and Test Data
321	Train the model
322	Replace the prefix
323	prepare target data
324	Plotting the variance
325	visualize the histogram of the single variable
326	Visualizing the forest
327	Plot the heatmap
328	Plot the forest
329	Dropping the number of tumors
330	Dropping the trend
331	plot the forest
332	Plot the FVC curve
333	Load libs and load data
334	Setting up some basic model specs
335	Visualizing Train Data
336	Preparing data for further analysis
337	load the data
338	Bone Scan for Diagnosis of Metastatic Disease
339	credits to Rohit Singh
340	Import Required Libraries
341	Visualizing Train Data
342	cross validation and metrics
343	Ensure determinism in the results
344	FUNCTIONS TAKEN FROM
345	LOAD PROCESSED TRAINING DATA FROM DISK
346	Tokenize the sentences
347	shuffling the data
348	SAVE DATASET TO DISK
349	LOAD DATASET FROM DISK
350	The mean of the two is used as the final embedding matrix
351	missing entries in the embedding are set using np.random.normal
352	text version of squash , slight different from original one
353	The method for training is borrowed from
354	for numerical stability in the loss
355	Shuffling happens when splitting for kfolds
356	This enables operations which are only applied during training like dropout
357	Computes and stores the average and current value
358	Process to prepare the data
359	Looking at the data
360	Redundant when combined with my Preprocessing function
361	Display the generated image
362	load the GloVe vectors in a dictionary
363	Convert values to embeddings
364	Define a basic model
365	to train the model
366	More To Come
367	the missing category
368	rate of type of float
369	get the most recent loss
370	get some merge
371	Let us merge both train and test datasets
372	Create the parameters for LGBM
373	Prediction for test
374	Prepare test data
375	splitting train and test
376	Train model on test data
377	And finally , create the submission
378	Let us encode the categorical features
379	Importing all libraries
380	Training and Predictions
381	Extract all the features
382	add averaged properties to DataFrame
383	convert lattice angles from degrees to radians for volume calculation
384	For each group
385	Exploring the data
386	Plot the distribution of the target variable
387	list of features we need
388	Extract Principal Component Analysis
389	Define RMSL Error Function
390	Deep Learning Begins ..
391	Find RMSL Error Function
392	Instantiate gradient Boosting model
393	credits to for the parameters
394	Initializing the CatBoostRegressor
395	Final Random Forest Model
396	We again fit the data on clones of the original models
397	from sklearn import tree
398	We again fit the data on clones of the original models
399	Find the stacked models
400	fillna with mode
401	Plotting the date
402	Generate a mask for the upper triangle
403	from this kernel
404	Join the word cloud
405	Plotting distribution of revenue
406	Check the distribution of the train and test data
407	This revenue is different
408	ADD the log
409	split the train and test data
410	Set global parameters
411	to be included
412	create a matrix of the arguments
413	create the concatenate features
414	plot the preprocessed data
415	Sumarizing the predicted scores
416	use aggregated features
417	Split train train and test
418	plot the preprocessed data
419	Importing the inputs
420	Correlation between unique features
421	Test if there are overlap
422	Exploring absolute error
423	get test set
424	The competition model
425	Importing skimage image
426	Change the data
427	Fitting an image
428	Determine current pixel spacing
429	Shifting the hue
430	Return out the mask
431	Pad the cropped area into a new image
432	Importing important libraries
433	Find out correlation
434	Lets look at the distribution of a logistic regression
435	Train cloned base models
436	This function make predictions using all three models conveniently wrapped
437	We again fit the data on clones of the original models
438	one of several error
439	Get the number of images
440	All stolen from
441	Import Train and Test dataset
442	fit a random forest on all features
443	Split the data
444	Import libraries and data
445	Random Forest Pipeline
446	Logarithmic transform of target values
447	Most common values
448	Fill the missing values
449	Merge with the weather data
450	Tokenize the test data
451	Blend predictions ..
452	Create the submission
453	Saving the train and test dataframe
454	Setting up some basic image
455	Preparing the data
456	looking at the primary data
457	Test image sizes
458	Filter out the mean
459	Filter out the mean
460	look at the prior steps
461	Split train data into train and validation set
462	Importing the libraries
463	Things to use
464	OSIC Pulmonary Fibrosis Progression Analysis
465	Loading the data
466	Load train data
467	Reading test dataset
468	RF regressor model
469	Show the ranked data
470	Plot the feature importances of the forest
471	FROM EARLY STOP
472	This Testing Set
473	Get the back to the original image
474	Importing the libraries
475	Prepare the estimator
476	Creating submission
477	Importing the libraries
478	Prepare the estimator
479	pad the sentences
480	Create a submission
481	Set the xlabels
482	Train simple convolutional layer
483	Fit the model
484	Plotting original image
485	Predict and Visualise bboxes
486	plot the picture
487	check if label is small
488	Create a mask of the original cell
489	RLE encoding , as suggested by Tadeusz Hupa≈Ço
490	Reading the image
491	for original image
492	Function for getting dataframe
493	Below we can test data set
494	Reading for training data
495	Plot the distribution of the sentence
496	Regex strings for different images
497	lemmatize the question text
498	Split train data into train and test data
499	Transform the data
500	Evaluation of Regression model
501	Save the LGB model
502	Pick the vectoriser
503	Pick the LR model
504	MAKE CUTMIX LABEL
505	wrap the data into one ,
506	Lets predict the original text
507	Upvote if this was helpful
508	get the data fields ready for stacking
509	We can also display a spectrogram using librosa.display.specshow
510	Plot the log spectrogram
511	Zero Crossing Rate
512	Zero Crossing Rate
513	Plot the waveform
514	For every slice we determine the largest solid structure
515	Remove other air pockets insided body
516	isolate lung from chest
517	Standardize the pixel values
518	to renormalize washed out images
519	Plotting the group
520	Getting only necessary columns
521	Teams with the game ID
522	Calculate the event counts
523	Plotting the distribution of the test data
524	What are the dataframe
525	How many users are there different
526	Plot the date count
527	Week of year
528	Plotting the title
529	Lets look at the day
530	Plotting the result
531	Plot the distribution of the world
532	What is the distribution of the World
533	Evaluate each column
534	Evaluate each column
535	What is the same
536	Using our lookup dictionaries to make simpler variable names
537	using soft constraints instead of hard constraints
538	Loop over the rest of the days , keeping track of previous count
539	Start with the sample submission values
540	loop over each family choice
541	Get the current train and test data
542	We encode the words
543	Show the no axis
544	use a linear Color map
545	feature distribution of the target
546	Lets look at the distribution of the word
547	Read in the submission file
548	get the data fields ready for stacking
549	get the data fields ready for stacking
550	Loading our data
551	Plotting outlier
552	some config values
553	fill up the missing values
554	Tokenize the sentences
555	Pad the sentences
556	plot the important features
557	Return the score
558	Get the train dataframe
559	Find out the label range
560	Prepare target variables
561	Load the data
562	store the feature
563	Get the train and test dataframes
564	Create submission file
565	Create submission file
566	Get the train and test dataframes
567	Create prediction file
568	Create submission file
569	Get the train and test dataframes
570	Create prediction file
571	Create submission file
572	LOAD TRAIN AND TEST
573	Merge the data
574	Looking at the distribution
575	Loading the data
576	Exploring the data
577	Deal Probability Distribution
578	Deal Probability Distribution
579	Deal Probability Distribution
580	Deal Probability Distribution
581	Deal Probability Distribution
582	Deal Probability Distribution
583	Splitting the data for model training
584	Making a submission file
585	plot feature importances
586	Split the train dataset into development and valid based on time
587	Draw the heatmap using seaborn
588	Build Train and Test Data for Modeling
589	We submit the solution
590	Lets look at the target
591	Merge Purchase Data
592	Looking at the distribution
593	Custom visualization
594	Plot the most correlated variables
595	Plot the price
596	FROM EARLY STOP
597	Wordcloud for display comments
598	Using outliers
599	Train Set Missing Values
600	defining a simple XGBoost model
601	plot the important features
602	custom function for ngram generation
603	custom function for horizontal bar chart
604	Get the bar chart from sincere questions
605	Get the bar chart from sincere questions
606	Creating two subplots
607	Creating two subplots
608	Creating two subplots
609	Get the tfidf vectors
610	find out the validation threshold
611	Days since the revenue
612	Target Variable Exploration
613	visualize department distribution
614	Please upvote this kernel which motivates me to do more
615	Looking at the training data
616	Running Vs Yards
617	I hope it will help for you to create more accurate data
618	Load the data
619	plot the important features
620	Floor We will see the count plot of floor variable
621	Now let us see how the price changes with respect to floors
622	Are there seasonal patterns to the number of transactions
623	Latitude and Longitude
624	Downcasting the object type columns
625	Train Set Missing Values
626	Draw the heatmap using seaborn
627	Plot the distribution of Bathroom count
628	Remove bins from bedrooms
629	Plot the results
630	Plot the rest of locations
631	plot the important features
632	Wordcloud on submissions
633	Load the train and test data file
634	The number of punctuations by author
635	Prepare the data for modeling
636	plot the important features
637	Get the tfidf vectors
638	Get the tfidf vectors
639	Get the tfidf vectors
640	add the predictions as new features
641	add the predictions as new features
642	add the new columns as new features
643	plot the important features
644	BanglaLekha Confusion Matrix
645	Kagglegym import ..
646	Get the test data
647	Get the environment data
648	Create the environment
649	Target Variable Exploration
650	Plot the frequency
651	Find out correlation between columns
652	Draw the heatmap using seaborn
653	set the data path
654	Load the data
655	Get label information
656	Plotting the original image
657	Read the train and columns
658	define tfidf vectorizer
659	fit the latent semantic analysis
660	Training the model
661	Get the train and test data set
662	Plot the distribution of the wavelet
663	We fit the correlation matrix
664	Get the degrees
665	Remove the original text
666	Examine the search
667	Creating Submission File
668	Creating Submission File
669	Get rid of calc features
670	Creating Submission File
671	Read the data
672	Examine the number of samples
673	Read the train and test data
674	identifying missing values
675	function to replace missing values in format
676	Defining some useful functions
677	Drop unused and target columns
678	Submitting the prediction
679	Read the train and test data
680	Checking missing values
681	Drop unused and target columns
682	Submission for XGBoost model
683	Draw the heatmap with the fare aspect ratio
684	define the parameters
685	Drop unused and target columns
686	Create submission DataFrame
687	Preparation for XGBoost
688	Get accuracy of model on validation data
689	Convert values to embeddings
690	change the graph
691	upper triangle of equations
692	some config values
693	Tokenize the sentences
694	Pad the sentences
695	shuffling the data
696	Duplicate image identification
697	Compute phash for each image in the training and test set
698	For each image id , determine the list of pictures
699	For each image id , determine the list of pictures
700	If an image id was given , convert to filename
701	Apply affine transformation
702	Normalize to zero mean and unit variance
703	For each whale , find the unambiguous images ids
704	Compute a derangement for matching whales
705	Construct unmatched whale pairs from the LAP solution
706	Force a different choice for an eventual next epoch
707	Map whale id to the list of associated training picture hash value
708	Collect history data
709	Evaluate the model
710	Resize the image to desired size
711	Test the predictions
712	get the square image
713	LOAD ALL new data
714	Preprocessing data to transform object data types
715	CALCULATE NEW FEATURES
716	Loading the data
717	Diff Sample Images
718	Plot rolling statistics
719	Plot the distribution of the numercial features
720	Plot the frequencies of the model
721	It means that We were able to make a good model
722	Plot the frequencies of the model
723	It means that We were able to make a good model
724	Get forecast and predict
725	Top most common value
726	show the plot
727	plotting the figure
728	Time Series Prophet Forecast for Prophet
729	Plot the cumulative total of Confirmed cases
730	Plot the cumulative total of confirmed cases
731	New Balance Cases
732	Cases for test data
733	Checking for missing data
734	Find the columns
735	Looking at the data
736	for a list of features
737	Preprocessing , categorical and parameters
738	Filling NaNs with mean
739	from sklearn import tree
740	use cached rdkit mol object to save memory
741	this is faster than using dict
742	Compile and visualize model
743	Import the datasets
744	Apply the predicted examples
745	Apply the predicted Test set and output predictions
746	Plot the actual class
747	Print current column type
748	make variables for Int , max and min
749	Integer does not support NA , therefore , NA needs to be filled
750	test if column can be converted to an integer
751	Print final result
752	function to clean the labels
753	Checking for missing data
754	Find the columns
755	Looking at the data
756	for a list of features
757	Preprocessing , categorical and parameters
758	Filling NaNs with mean
759	to set up scoring parameters
760	Evaluation of Meta Data
761	some config values
762	Exploratory Data Analysis
763	Load the data
764	remove extra spaces and ending space if any
765	add space before and after punctuation and symbols
766	preprocess text main steps
767	go over training data
768	Convert acc to RGB image
769	Answers Sentiment
770	Now , our model
771	copy the best model weights
772	Get the train and test images
773	speech a grid
774	Preparing the test data
775	Time Series Prophet Forecast for Prophet
776	Top most common line
777	nb preds to train and test data
778	define adv layer
779	split train , val
780	Distribution of Transaction Type
781	Rate of Fraud by product
782	Rate of Fraud by Card
783	Rate of Fraud by Card Type
784	How fraudent transactions is distributed
785	Remove the lead
786	Check if any variance
787	Create arrays with zeros
788	model , criterion , optimizer
789	model , criterion , optimizer
790	Drop useless features
791	Drop useless features
792	Build the model
793	Create the submission dataframe
794	Exploring the data
795	UpVote if this was helpful
796	How well does
797	Same number of Images in each image
798	You can see there is no same ship
799	create a new column
800	One hot encode the categorical features
801	Split train data into train and validation set
802	Build SGD model
803	One hot encode
804	Drop ordinal feature values
805	OneHot Encoding Labels
806	Energy Consumption
807	Import your environment
808	Reading the dataset
809	Lets take the test data from the submission file
810	reading all submission files
811	Drop ordinal feature values
812	OneHot Encoding Labels
813	Energy Consumption
814	GCP Project Id
815	try the same kernel
816	Create strategy from tpu
817	Split the data
818	Get the label
819	Importing the libraries
820	Minimize the loss function
821	Blend predictions ..
822	Build the prediction
823	Example of data
824	Split train and validation sets
825	Add a list of the different parameters
826	Perform a sanity check on the given classifiers
827	Fitting a model
828	Fitting a model
829	separate train and test sets
830	Encode the Categorical features
831	fill the previous data
832	Get predicted actions
833	Print the action of the model
834	read the data
835	Generate the loop
836	need to update generator
837	plotly offline imports
838	load dataframe with train labels
839	Plot the pie chart for the train and test datasets
840	plotting a pie chart which demonstrates train and test sets
841	There are one row
842	plotting a pie chart
843	Get dummy fillna
844	fill dummy columns
845	plotting a pie chart
846	Find out correlation between columns
847	get sizes of images from test and train sets
848	Function to get the labels for the image by name
849	open image with a random index
850	plot the image
851	convert rle to mask
852	visualize the image and map
853	get segmentation masks
854	plot images and masks
855	plot images and masks
856	plot images and masks
857	plot images and masks
858	get masks for different classes
859	create a segmantation map
860	Function to add labels to the image
861	get masks for different classes
862	draw the map on image
863	visualize the image and map
864	draw segmentation maps and labels on image
865	plot the image
866	reduce for train data
867	Import our model
868	Plot image examples
869	Plotting the date
870	Get the test dataframe
871	fill the corpus
872	Create the intermediate dataset
873	for each label
874	iterate through each class
875	splitting the data
876	plot the hair image
877	Loading the image
878	inpaint with original image and threshold image
879	Associate albuels
880	Look at the source
881	Convert lat and latitude
882	Plot the audio
883	Get the number of categorical features
884	Remove the features from the data
885	feature correlation with target
886	Sort feature Selection
887	Adding in the dataframe
888	add mapping values
889	merge train and test dataframes
890	We can logtransform for there
891	Process to process Data
892	import packages and data
893	use this for ploting the count of categorical features
894	use this for ploting the distribution of numercial features
895	Average of the target
896	import data packages
897	Label encode categoricals
898	Libraries For Fun
899	Function to find missing data
900	check the number of new entries in the test set
901	Label encode categoricals
902	Loading ARC paths
903	Evaluation of Test
904	make sure types to ints
905	Analysing helper functions
906	update scores in the order
907	Get the player info
908	Plot the mask
909	TurnOff You can not use the internet in this competition
910	for future Affine transformation
911	Need to send lazy defined parameter to device ..
912	Depends on train configuration
913	Preprocessing with nltk
914	Creation of the vocabulary
915	Setting up a validation strategy
916	converting values to numpy array
917	Transform training set
918	Transform training set
919	Grab one batch of training images
920	Grab the batch of the test data
921	Transform training set
922	Transform training set
923	Grab one batch of training images
924	Grab the batch of the test data
925	Merge Train and Test Data
926	What are the maximum length of text
927	All comments must be truncated or padded to be the same length
928	Factorize categorical columns
929	Merge news on unstacked assets
930	Standard Deviation of asset
931	Drop columns that are not features
932	Merge market data
933	we are going to average
934	Flatten the dictionary
935	Factorize categorical columns
936	Merge news on unstacked assets
937	Drop columns that are not features
938	Merge market data
939	Merge Merge
940	What is the data
941	Read in the data
942	Clearing first and last day from the data
943	Converting the calendar data from calendar format
944	Lets look at the number of words
945	So the revenue
946	Lets convert to format required libraries
947	parameter value is copied from
948	Light GBM Model
949	What is the dataset
950	Distribution of Age between Male and Female Gender
951	Smoking Status Viz
952	store label values
953	FVC between Percent and FVC
954	Determine current pixel spacing
955	For every slice we determine the largest solid structure
956	Load train data
957	Getting the Predictions
958	Importing the libraries
959	Only load those columns in order to save space
960	and reduced using summation and other summary stats
961	Number of teams by Date
962	Top LB Scores
963	Create Top Teams List
964	Count of LB Submissions that improved score
965	Get the data
966	Train the model
967	Here is the trick
968	extract the predict id of the train dataset
969	Ensure determinism in the results
970	Example of predictions
971	Putting the heatmap
972	Importing the libraries
973	Shifting time axis
974	Shifting time axis
975	This augmentation is a wrapper of librosa function
976	Add Gaussian noise
977	Add Gaussian noise
978	Defining tranformations
979	Initiate AUC
980	Fitting auc
981	using keras tokenizer here
982	zero pad the sequences
983	Run final model with the right number of iteration
984	load the GloVe vectors in a dictionary
985	create an embedding matrix for the words we have in the dataset
986	Create an embedding matrix
987	building the model
988	Define a model
989	Define a model
990	Import the Libraries
991	Exploring the data
992	App padded with padding
993	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
994	Get the tokenizer output
995	Adding percents over bars
996	define the model
997	The hidden cell above define the dataframe
998	Get the model
999	separate train predictions for each model
1000	basic training configuration
1001	Here we define the model
1002	TAKEN PARAMS HERE
1003	set learning rate scheduler
1004	history of loss values in each epoch
1005	Visualizing Some Images from Cover Section
1006	Examine the actual values
1007	Examine the actual values
1008	Train the model
1009	Data loaded to the kernel
1010	Function for cleaning the text
1011	Segregating positive , negative , neutral sentiment data
1012	MosT common positive words
1013	MosT common positive words
1014	MosT common positive words
1015	Exploring the data
1016	This Function Saves model to
1017	Load the model , set up the pipeline and train the entity recognizer
1018	Returns Model output path
1019	Returns Trainong data in the format needed to train spacy NER
1020	Training for Positive and Negative tweets
1021	Read the train , test and sub files
1022	Make a dictionary for fast lookup of plaintext
1023	Combine all the key
1024	Any results you write to the current directory are saved as output
1025	Lets look at the distribution of the model
1026	Creating a new key
1027	HANDLE MISSING VALUES
1028	SCALE target variable
1029	EXTRACT DEVELOPTMENT TEST
1030	FITTING THE MODEL
1031	stem the corpus
1032	Also we have XGBRegressor
1033	Here I load DELG model and the previous DNN model
1034	What is the difference between X and y
1035	Plot the distribution of the current type
1036	Function to remove outliers
1037	Importing the necessary libraries
1038	First , we import test set
1039	Prepare the data
1040	Plot the distribution of the mean values
1041	flatten the prediction function
1042	flatten the submission
1043	Load libraries and data
1044	Defining the paths
1045	Let us see the product
1046	Looking at the sales data
1047	Looking at the sales data
1048	This is the Card Features
1049	Distribution of Charge Other
1050	Is there the device
1051	General Feature Preprocessing Problem
1052	Transform the categorical features
1053	Sort X , y
1054	Train the lightgbm model
1055	plot the important features
1056	summarize history for accuracy
1057	summarize history for loss
1058	configure text feature
1059	Number of characters in the sentence
1060	plot average word length
1061	Average Word Length
1062	Actual tokenizer of text
1063	text version of squash , slight different from original one
1064	Lets save the word index to JSON API
1065	Load the documentation
1066	Separate features by CA
1067	Load the data
1068	Import the Google API
1069	Find Average Absolute Error
1070	See absolute error
1071	convert dictionary to unique values
1072	specific for our model
1073	The competition metric
1074	perform predictions on test set
1075	Importing the libraries
1076	Function to remove these text
1077	Replace repetitions of exlamation marks
1078	Replaces repetitions of exlamation marks
1079	Replaces repetitions of question marks
1080	Define Learner
1081	Neural Network with Keras
1082	Split between train and validation sets
1083	Change the model
1084	Change the model
1085	Change the model
1086	Change the model
1087	Change the model
1088	Change the model
1089	Predict on train , val and test
1090	Change the model
1091	Change the model
1092	Change the model
1093	Change the model
1094	Change the model
1095	Change the model
1096	Change the model
1097	Change the model
1098	Predict on train , val and test
1099	Load the data
1100	The main pipeline
1101	get the data
1102	Preparing the data
1103	The problem move
1104	Transform training data
1105	Loading the data
1106	Plot the distribution of the target variable
1107	Plot the distribution of the given permICE
1108	Postprocess the memory
1109	Plot the distribution
1110	Plot the distribution
1111	Compute the loss
1112	Limits the dimension
1113	Plot the difference
1114	Plot the distribution of the kills
1115	Load libraries and data
1116	get the data
1117	The mean of the axis
1118	Get the filter coefficients
1119	The main pipeline
1120	get the data
1121	Preparing the data
1122	The problem move
1123	Transform training data
1124	Loading the data
1125	get the meanogram
1126	Plot the distribution of the Spectrogram
1127	Plot the distribution of the signal
1128	Postprocess the memory
1129	Return the sampler
1130	Plot the distribution of the target variable
1131	Plot the distribution of the target variable
1132	Convert vector to linear models
1133	Plot the results
1134	Plot the results
1135	Read in the data
1136	Checking the error values
1137	Load libraries and data
1138	Get the train images
1139	Prepare label data
1140	Build Train set
1141	Plot image examples
1142	Training the Model
1143	Loading the data
1144	lets see the gleason score
1145	example of model
1146	The competition loss
1147	Variables for the model
1148	Look at the distribution
1149	Look at the distribution
1150	Visualizing the dataset
1151	distribution of the target variable
1152	Training the model
1153	Converting the given data
1154	Get the columns
1155	build graph features
1156	Prepare the train and test data
1157	Wordcloud for all comments
1158	Average comment length
1159	histogram of length of question titles
1160	Distribution of the toxic vs skin
1161	 Histogram plot for each feature
1162	Distribution of Fills
1163	Plot the automrelation
1164	Distribution of the toxic vs
1165	Plot pie chart of pie chart
1166	Define helper functions and useful vars
1167	Create fast tokenizer
1168	Create fast tokenizer
1169	Build datasets objects
1170	Model initialization and fitting on train and valid sets
1171	Training the model
1172	Fitting the model
1173	Training the model
1174	fit train model
1175	Model initialization and fitting on selected model
1176	Fit train model
1177	Training the model
1178	Training the model
1179	Model initialization and fitting model
1180	Fit train model
1181	Global training settings
1182	Constants and Folders
1183	Loading the training images refer
1184	Distribution of channel values
1185	Red Channel Values
1186	Green Channel Values
1187	Blue Channel Values
1188	Define helper functions and useful vars
1189	Return the path
1190	Define learning rate scheduler
1191	configurations and main hyperparammeters
1192	Function for cross entropy loss
1193	Load the train
1194	Get the slide
1195	Set some parameters
1196	build the path
1197	Defining the loss function
1198	Calculate weights for sampler
1199	Initiate the MAI dataset
1200	Get train sample
1201	Model Train AND VALIDATION
1202	Look at Numpy Data
1203	Setting figure parameters
1204	Drop unused and target columns
1205	CV score
1206	Get the CV scores
1207	Setting figure parameters
1208	Plot the distribution of the target variable
1209	Get sample target class
1210	Use the NLP
1211	count the sentences
1212	Finding the insincere questions
1213	Creating the sincere text
1214	Defining AUC metric
1215	Preprocessed Hits Data
1216	Convert at mean
1217	Load the labels
1218	Load the labels
1219	Get the frame size of the video
1220	Function for video length
1221	Importing Libraries and Loading Dataset
1222	Importing librarys to use on interactive graphs
1223	Binary features inspection
1224	Convert training and validation sets
1225	Move the prediction
1226	Plot the distribution of yaw
1227	Frequency of Objects
1228	Estimate the point of the model
1229	Calculate the model
1230	Get the predictor
1231	Getting the track of our sample
1232	Get the moves
1233	Get the activations
1234	remove lag and change in this round
1235	if there is last example
1236	Remove the points
1237	Calculate the model
1238	Calculate the model
1239	Defining the tuple
1240	Iterate over points
1241	Take only one sample
1242	Let us check the features
1243	Returns the corners
1244	Draw the sides
1245	Draw the sides
1246	Loads database and creates reverse indexes and shortcuts
1247	Initialize map mask for each map record
1248	Loads database from table
1249	Store the mapping from token to table index for each table
1250	Get the index of the table in each table
1251	Get the boxes
1252	List of boxes in the form of data
1253	rotation of directions
1254	Move the model
1255	Get the map
1256	Calculate the box on our sample
1257	Wrapper around Torch Dataset
1258	Get the number of categories of each image
1259	Get the number of each feature
1260	preparing the data
1261	Display the sample
1262	Cropping the image
1263	Get the sample submission
1264	Get the surface data
1265	Get aggregated point cloud in lidar frame
1266	Limit the axes
1267	Get samples for lidar frame
1268	Format for aggregated points
1269	Limit the axes
1270	get train data
1271	Get prediction for test image
1272	Get the scene records
1273	Pad the scene
1274	Get prediction for test image
1275	Find the image
1276	Get the scene record from the lid record
1277	Get the spectrum
1278	Put points into the map
1279	create the map
1280	set the scene
1281	Get the sample
1282	Get thescene
1283	Test Data Analisys
1284	Remove Drift from Training Data
1285	to set up scoring parameters
1286	Libraries and Configurations
1287	Training the nominal variables
1288	Get the filter coefficients
1289	Get the filter coefficients
1290	Get the filter coefficients
1291	Setting up the filter
1292	to set up scoring parameters
1293	Get the prediction
1294	to set up scoring parameters
1295	Get the prediction
1296	Get the data
1297	Data Preprocessing Libraries
1298	convert text into datetime
1299	get some sessions information
1300	the time spent in the app so far
1301	the accurace is the all time wins divided by the all time attempts
1302	Common data description
1303	Load all the data as pandas Dataframes
1304	Calculate the Average Team Seed
1305	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1306	PRINT CV AUC
1307	Retreive test set AUC
1308	Looking at the distribution of transaction amount
1309	Perform the groupby
1310	Exploratory Data Analysis
1311	Remove train , valid data
1312	For Date column
1313	Matching function between the ISO code and country names
1314	Make the figure
1315	Make the figure
1316	Line plot with the Notebook
1317	Sea lion way
1318	Fatching function between the ISO code and country names
1319	Looking at the distribution
1320	populate the pickup cases from the country
1321	replace some missing values with
1322	Plot for hits
1323	get valid data
1324	fit the series of first dimension
1325	Prepare DNN model
1326	Preprocess the embeddings
1327	padding if necessary
1328	Flatten Forward Prop
1329	A discriminator for each encoder
1330	 revive the encoder
1331	A residual connection followed by a layer
1332	Building the head
1333	LSTM and Simple Model
1334	A residual connection followed by a layer
1335	Get the outputs
1336	from sklearn import tree
1337	Load the data
1338	Implementing the Gini metric
1339	encode the string
1340	An optimizer for rounding thresholds
1341	declare invalid data
1342	Train the loss
1343	We can take the weights
1344	Save the next epoch
1345	What is Fake News
1346	separate stop words
1347	Plot the most frequent sentence
1348	Visualizing the sentence
1349	Vectorizing the sincere questions
1350	Latent Dirichilet All
1351	Get list of selected topics
1352	Create the plot
1353	Create the plot
1354	Let us plot the audio file
1355	Get the sample
1356	and there is one example file
1357	function from EDA kernel
1358	more functions from LightGBM baseline
1359	Standard plotly imports
1360	Importing the spacy libraries
1361	import libraries for building the neural network
1362	Create the tokenizer
1363	import the document length
1364	Train the model
1365	Example of Submission
1366	Sort the table by percentage of missing descending
1367	Print some summary information
1368	Embedding the html string
1369	Benign image viewing
1370	added the grid lines for pixel purposes
1371	BY SERGEI ISSAEV
1372	added the grid lines for pixel purposes
1373	Finding unknown region
1374	Finding unknown region
1375	The basic structure of model
1376	combine correlation matrix
1377	The function for our top row pairs
1378	Filter the signal
1379	Importing the necessary libraries
1380	several prints in one cell
1381	Convert the date of movies
1382	Plotting the result
1383	Ploting the movie
1384	Movie Release count by Day of the Month
1385	Most of the releases
1386	using eratosthenes
1387	Building Vocabulary and calculating coverage
1388	Adding lower case words to embeddings if missing
1389	Function to Clean the sentences
1390	Building Vocabulary and calculating coverage
1391	Function to Clean the sentences
1392	Tokenizing the sentences
1393	Display ALL wavelet
1394	left seat right seat
1395	Time of the experiment
1396	Galvanic Skin Response
1397	Parameters of load
1398	Loading the data
1399	plotting rolling statistics
1400	Evaluation of residuals
1401	Loading the tour
1402	Reading the tour
1403	fit the XGBRegressor
1404	Load the needed libraries
1405	Average values for each class
1406	Plot image examples
1407	Grab the center of the image
1408	Let us now look at the age distribution by gender
1409	Let us see the number of restaurants in the gender
1410	Setting the pixel spacing
1411	Here are the bounding box distributed by the number of boxes
1412	We can see the distribution of black pixels
1413	Look at the distribution of the Resent aspect ratio
1414	Linear Discriminant Analysis
1415	Linear Discriminant Analysis
1416	specify your configurations as a dict
1417	Return the value of the next channel
1418	define next model
1419	loop over the val
1420	Get the best criterion
1421	Sort for the last feature
1422	update the gradient of the gradient
1423	most recent feature values
1424	Get the node
1425	get the left node and right edges for the right node
1426	get the node id
1427	Generate the inputs
1428	get the node id
1429	gather input graph
1430	Return the value of the next channel
1431	Initialize node with this dataset
1432	loop over the val
1433	Calculate the gradients CV AUC
1434	Update best weights
1435	most recent feature values
1436	Get the node
1437	get the left node and right edges for the right node
1438	get the node id
1439	Generate the inputs
1440	get the node id
1441	gather input graph
1442	Get image from solution
1443	Get image id
1444	Visualizing the correlation matrix
1445	Load libs and funcs
1446	Calculate the final score
1447	Convert the data to transform
1448	example of environment
1449	Computes the trained model
1450	Train the model
1451	reset index to conform to how kagglegym works
1452	Log the AUC
1453	Show the number of features
1454	split the dataset by class
1455	Get the features
1456	Return filter points
1457	Create the partition
1458	Find Best Position
1459	Remove Cutpoints with the RMSE value
1460	Store the cutpoints
1461	Write bins to log format
1462	fit the model
1463	Plotting the result
1464	Save the ranking
1465	Checking Best Feature for Final Model
1466	define list of best parameter
1467	Train the model
1468	We add up predictions on the test data for each fold
1469	Here we average all the predictions and provide the final summary
1470	Save the final prediction
1471	Compute the mean
1472	Final result
1473	Looking at the data
1474	Create MTCNN and Inception Resnet models
1475	Loop through frames
1476	Resize frame to desired size
1477	When batch is full , detect faces and reset frame list
1478	load the train videos
1479	create fastM CNN
1480	create fastM CNN
1481	Remove the face
1482	MTCNN face detector
1483	Linear Output Layer
1484	Train the model
1485	Iterate over folds
1486	Generate to zip file
1487	Return the data
1488	Table of products
1489	One item lookup data
1490	Plot number of samples
1491	how does the look upitems
1492	look up data
1493	Calculate the linkage between two classes and see the score
1494	make the diagram
1495	look at data
1496	Look at the items
1497	Plot the results
1498	Returns the matrix
1499	Put the two columns in order
1500	items in China
1501	define DIFng model
1502	create card ID
1503	Import required libraries
1504	Set the paths
1505	Listen to the files
1506	Comparing Spectrograms for different birds
1507	Plot the sample sound
1508	Plot the sample
1509	Looking at the test data
1510	Extract the directory
1511	Start with a simple Logistic Regression Model
1512	show the scatter plot
1513	Fit the model
1514	Display the contours
1515	Define some Global Variables
1516	replace the question text by line
1517	Tokenize the sentences
1518	Define the objective functions
1519	Compile your model
1520	Reading the train , test and train files
1521	Build the model
1522	from tqdm import tqdm
1523	Pad the signal
1524	Only the classes that are true for each sample will be filled in
1525	Return a normalized weight vector for the contributions of each class
1526	Histogram of the missing values
1527	Calculate the variance of each feature
1528	Apply the mean of the regions of each image
1529	Plot the FVC
1530	Listing the available files
1531	Custom GAP Dataset class
1532	Import feature importances
1533	Importing librarys to use on interactive graphs
1534	take the edge
1535	get the edge features
1536	get the edge features
1537	return the cropped area
1538	compute the mean of the folds
1539	Get the impact
1540	Importing standard libraries
1541	Importing sklearn libraries
1542	Deep Learning Begins ..
1543	Read data from the CSV file
1544	Since the labels are textual , so we encode them categorically
1545	Tokenize the data
1546	make sure train set
1547	Fit the model
1548	summarize history for loss
1549	summarize history for accuracy
1550	load the additional data as well
1551	add some features
1552	Set up GPU
1553	Shape the train data
1554	Set up the parameters
1555	Split the train data
1556	Formatting final predictions
1557	Only the classes that are true for each sample will be filled in
1558	Wrapper for fast.ai library
1559	Special thanks to
1560	The most difficult part of this Problem ..
1561	Things to use
1562	Import packages and data
1563	Load the data
1564	Exploring NaN values
1565	Exploring some data
1566	Exploring some data
1567	Exploring some data
1568	Same for day
1569	Creating dummy columns
1570	Turn rid of unique categories in train and test set
1571	Plotting the result
1572	visualize feature comparision
1573	evaluate the date
1574	Plot the kurtosis
1575	make unique words
1576	MAKE CUTMIX LABEL
1577	from sklearn import tree
1578	Class for extracting outliers
1579	Define the grid search
1580	preparing the dataset
1581	Reading the data
1582	Things to use
1583	Dataset and dataloader
1584	get the dataset
1585	Load the best model
1586	Get the pred
1587	Reading the data
1588	Maping the category values in our dict
1589	Concating train and test
1590	Split the train and test
1591	Transforming ordinal Features
1592	from util import
1593	What is the training data
1594	Defining the paths
1595	Prepare the data
1596	preparing test data
1597	Pearson correlation matrix
1598	Plot the percentage of Sex
1599	Smoker status vs
1600	Smoker status vs
1601	Training Data Set
1602	Dataset and dataloader
1603	create a generator for the train and test dataset
1604	Load the best model
1605	Get the prediction
1606	Create spk , val
1607	the func is from
1608	Looking at the data
1609	Overall distribution of the number of nuclei annotated objects
1610	Save the original image
1611	Save the original image
1612	Import the libraries
1613	Define the iterator
1614	Set up the best optimizer
1615	Load the image
1616	Listing the polygons of each image
1617	Examine the plot
1618	get rid of polygons
1619	visualize the polygons
1620	Lets view some predictions
1621	Now , we will start our model
1622	Train simple model
1623	load the data
1624	Finally merge prices
1625	Categorize the values of the column
1626	Encode the Categorical columns in the dataframe
1627	For each Department
1628	Total Sales by Category
1629	Plotting sales by State ID
1630	Mean Sales Vs
1631	Plotting the sales ratio
1632	Importing relevant predictions
1633	Creating final training dataframe
1634	evaluate test dataset
1635	merge the average of the predictions
1636	Save to csv
1637	Filling NA values with mode
1638	Preprocess data
1639	Separating target and ids
1640	Exploring the data
1641	Plot the combined entity
1642	Plot the autocorrelation
1643	Load the data
1644	Things to use
1645	Import and load data
1646	Importing your dataset
1647	function to convert matrices
1648	set up the FFT curve
1649	This Feature Engineering
1650	Train the model
1651	Calculate NDVI score
1652	Lets calculate the optimal score
1653	Calculate the loss
1654	Fit the poly curves
1655	normalize the gaps
1656	Filling the values with zero
1657	Find the final result
1658	Make ground truth data
1659	We can see the correlation between a X and y
1660	Dealing with zero
1661	Normalize the panel
1662	Importing the required libraries
1663	Reading in the data
1664	mean squared error and mean absolute error
1665	Age Distribution for the revenue
1666	Smoker status vs
1667	Pixels and Sex
1668	Predict the FVC for SmokingStatus
1669	evaluation metric for the comp
1670	Example of Submission
1671	as test data is containing all weeks ,
1672	fill the df with the baseline FVC values
1673	same as above
1674	Prepare the data
1675	define which attributes shall not be transformed , are numeric or categorical
1676	OneHot Encodes categorical features
1677	APPLY DEFINED TRANSFORMATIONS
1678	Combine Score and qloss
1679	extract Patient IDs for ensuring
1680	Build and fit model
1681	Predicting on validation set
1682	Exclude most common value
1683	define data space
1684	from sklearn import tree
1685	Set the denominator
1686	Plot the graph
1687	Lets look at the long encoded values
1688	What is Fake News
1689	fit the three models
1690	Function to remove these values
1691	cretae the index
1692	Any results you write to the current directory are saved as output
1693	Region of length
1694	Read in the image
1695	Plotting the image
1696	Merge the dataset between its training and test set
1697	An optimizer for Neural Network
1698	Train MLP model
1699	An optimizer for rounding thresholds
1700	Fetch the actual values
1701	Add train the model
1702	Use the mean of folds
1703	Create data loader and get ready for training
1704	load pretrained weights
1705	Save model to AUC
1706	number of epochs to train the model
1707	Calculate average classifier
1708	print the model
1709	Reading the dataset
1710	RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR
1711	get total number of Train , Validation and Test images
1712	numpy and matplotlib defaults
1713	size and spacing
1714	Make the plot
1715	Peek at training data
1716	peer at test data
1717	remove grid mask
1718	to reduce memory usage
1719	CUSTOM LEARNING SCHEUDLE
1720	Load learning rate
1721	Learning Rate schedule
1722	Now , we define the model
1723	Instantiate Xception bottleneck features
1724	from util import
1725	Define estimator
1726	Getting the dataset
1727	SAVE BEST MODEL EACH FOLD
1728	Submit to Kaggle
1729	Plot the Kurt graph
1730	Load the dependancies
1731	Display sample DICOM files
1732	Plot the histogram of the pixel value
1733	Plot the examples
1734	Plot the raw waveform
1735	Extract DICOM data
1736	Generate the row
1737	reload the mAP
1738	Define the convolutional layer
1739	MAKE CUTMIX LABEL
1740	create train , dev data
1741	Build sentence graph
1742	Create some memory
1743	Make a new checkpoint
1744	Build sentence graph
1745	Train the model
1746	HANDLE MISSING VALUES
1747	SCALE target variable
1748	EXTRACT DEVELOPTMENT TEST
1749	FITTING THE MODEL
1750	Read training , test and sample submission data
1751	get different test sets and process each
1752	Get Training Data
1753	Get the validation set
1754	load best model weights
