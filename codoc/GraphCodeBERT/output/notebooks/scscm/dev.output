1309	Run the model
228	Ensembling all the columns
51	legend placement needs more work
1518	Load raw training data
563	Convert item descriptions to string description
501	show the graphs
457	Importing the libraries
285	Defining a batch of targets
1508	Obtain the training data
209	FIND ORIGIN PIXEL VALUES
1385	suppose all instances are not crowd
1516	Detect hardware , return appropriate distribution strategy
1116	Returns the counts of each type of rating that a rater made
178	Get the mean price of the category
1209	Save model and weights
864	Fitting model on train and test data
65	save pneumonia location in dictionary
61	functions to find some information
191	Display the scatterPlot between description
447	Encoding the Regions
476	Fitting the timestamps
1034	Get the number of rows with
1232	Load Train , Validation and Test data
54	Summary of the three minutes
1149	Prepare the data
407	Generate the Pool of a variable to train the model
1466	Turn off gradients
1330	Encodes a list of BlockArgs to a list of strings
1436	Number of Patients and Images in Training Images Folder
1808	Creating transaction counts
859	define searching object
451	Only the classes that are true for each sample will be filled in
919	Dealing with player tracking data
1206	Define the model
569	Age spread evenly
1657	Analysis of the Sentiment
1780	The wordcloud of the raven for Edgar Allen Poe
13	Load train and test data
1554	Train the model
1650	Get the score data
326	Initialize patient entry into parsed
1429	Make a list of training features
865	Convert the hyperparameters to the model
696	Exclude background from the analysis
1786	load a piece of data from a file
318	Decision Tree Regression
440	Most commom letters in the dataset
1563	BanglaLekha Error
689	functions to show an image
1811	Find the crosstab with actual values
189	Computing column description
778	drop high correlation columns
198	thresholding the threshold using Thresholding
735	Classify an image with different models
1735	plot the general variables
704	MODEL AND PREDICT WITH QDA
1236	draw a display text labels
541	GRID SEARCH for BEST TARGETS
1652	Loading the data
88	Train our model
1494	make predictions on test set
940	Show the best score
1098	The LGBM Dataset Class
255	train random forest regressor
775	Clusters of correlation matrix
161	check if label is small
1130	define data loader and define model
600	Lets create masks based on the image name
1698	 Evaluating the model
1287	Function to display multiple images
1266	Build test dataset
740	rescale box coordinates
1182	MAKE CUTMIX LABEL
393	What are the best model
1442	prepare submission data
142	get the data fields ready for stacking
93	Have to fix this
1354	Fast data loading
466	ROC curve and AUC
1583	Difference between columns and labels
592	Load the data
163	RLE encoding , as suggested by Tadeusz Hupało
1800	summarize loaded dataset
206	CONVERT DEGREES TO RADIANS
1769	SAVE DATASET TO DISK
1776	In this Section , I import necessary modules
928	Distribution of interest
1301	Test data preparation
1708	Importing all libraries
747	Make a Baseline model with the parameters
333	Read a DICOM files and medical images
758	There are two indexes of the family member
727	Leak categories of item
429	Convert year
1372	Submit to Kaggle
546	Now we can add the along each feature
1437	Number of Patients and Images in Training Images Folder
1399	Logarithmic transform of logits
1327	Convolutions like TensorFlow , for a fixed image size
146	Number of unique values
1247	to truncate it
1300	Comments data processors
350	Linear Output Layer
1093	Initialize the heatmap with some parameters
1493	Score trained model
1815	Instantiate the model
334	Check dimensions of data
946	altair is a very nice plotting library by the way
777	Features by Target
552	Evaluate the model
1310	Get feature importances
1409	Set some columns
1140	Does the relationship between raw data and other features
449	splitting the data
1402	Put missing values
664	date count features
1573	Prepare the data and model
1589	to truncate it
114	Join data frames with what state level
469	Data processing , metrics and modeling
1683	contrast , etc
1804	Plotting ROC Curve
1648	reorder the data
646	summarize history for error
821	Export our model with these methods
548	Evaluate the model
135	Hitting the model
432	Predict the output
1161	FIND ORIGIN PIXEL VALUES
1470	Scale the data
644	Check for test submission
435	Display the missing values
1342	Simple Mean Model
1022	check the columns missing values
810	store predictions for each validation set
1316	add extra features
939	CREATE the dataframe
292	Predicting and Test Set
542	Evaluate the model
1813	summarize history for loss
505	Create a plot of categorical features
1525	Span logits minus the cls logits seems to be close to the best
1796	 index of date column
1103	Import the datasets
538	Evaluate the model
1529	Read candidates with real multiple processes
1197	Create submission dataframe
877	Evaluation and Test
1195	Prepare submission file
817	calculate the associated model
741	simple way to do it faster
1707	Build a model
283	Sample out data
1043	Print some summary information
1010	Add a column to the aggregate the heatmap
186	Does the price depend on sales
1547	Read in the dataframe
96	Save to list
224	convert best score to int
313	label encoding the categorical columns
1285	Read all the members path
327	Add box if opacity is present
1622	Print the feature ranking
1393	Converting categorical columns to categorical features
1805	highlight the memory usage
1221	extract best score
130	Look at how data generator augment the data
788	Check for remaining importance
781	Change columns names
1220	Drop in and fill in NaNs
958	Some numerical features
1083	Load the data
514	count the number of words in each week
1133	padding up the validation set
23	Detect and Correct Outliers
1785	Avoid division by zero by setting zero values to tiny float
1476	MAKE CUTMIX LABEL
234	Filter selected features
1396	create date column
1099	predict OOF on validation set
1537	Based on strange things
1725	The method for training is borrowed from
1574	Read the data
1312	Prepare data for our model
1777	plot the heatmap
1819	Join to market
601	Perform the mask wise
890	Pearson correlation matrix
323	Numba function to calculate the iou score
929	Training the model
6	eliminate bad rows
1478	LIST DESTINATION PIXEL INDICES
1473	MAKE CUTMIX LABEL
539	Ekush Classification Report
1025	parameter value is copied from
1560	Correlation with macro features
365	Train the model
1039	Previous counts features
217	Average feature score
1280	Plot of ARC number
611	Save image to disk
1308	Run the model
1623	NumtaDB Classification Report
1720	SAVE DATASET TO DISK
1795	Train model and predict
1661	Read the solvable solution
765	array to plot
1561	Link for macro data
330	Storing the output
1104	Preparing the data
1594	Tokenize the sentences
1086	Loading the data
1	Resize a image to the shape of original image
1226	Plotting some random images to check how cleaning works
663	date time features
1000	define custom feature names
39	Checking data batch
229	add the ensemble columns to the average ensemble
743	Create one column for each type
629	Normalize the USA Data
490	history of missing values
118	Line plot by Female
493	format the data
1692	Generates functions that diverge functions
1755	Bureau AUC
175	simple image data
1498	Finding the weight columns
995	Where do the histogram of the predictions
141	look at target distribution
1557	Creation of the external Marker
1090	Write output prediction to file
1568	Pinball loss for multiple quantiles
257	Predict Potential Energy with test data
262	create a scatter plot
1351	Fast data loading
973	Table for top feature selection
1125	Submit to binary
338	plot and visualise the training and validation losses
1682	An optimizer for rounding thresholds
1080	Divide the result by the number of words to get the average
1242	Run prediction string with the second model
866	integer and string parameters , used with hp.choice
433	sklearn two moons generator makes lots of these ..
1611	target column names
1546	extract the columns
1760	Encode categorical features
1412	Duplicate image identification
411	Predict on logistic regression
1460	checking missing data
638	Word Cloud for positively classified movie reviews
1671	Inference and Submission
1375	Replace null values with three columns
1793	 Web View in day
764	iterate over chunks to train the image
897	Check for the cumulative importance
1059	Load the mask
924	Check for remaining importance
247	Apply exponential transf
507	Difference between train and valid sets
460	train random search
131	Prepare Testing Data
692	Load previous model
43	group data by column
1204	Save up folder
1134	Check if validation index is possible
471	ROC curve and AUC
1205	Read in the training data
1789	Forceasting with decompasable model
14	Target variable exploration
145	Reading the dataframe with pandas dataframe
1449	Transform training data
1292	Initialize the model
120	Function to count the words in each sentence
468	Loading the data
138	Get the model
64	Histogram of continuous variables
676	store the features
1551	Average values for each month
1052	Train the model with early stopping
487	load the data
570	Order does not look at the week days
1370	Label Encoding the categorical variables
994	Does the outlier impact of the previous patient
438	change the data
1577	create the continuous features
270	numeric column
1481	Histogram of correlation between train and test sets
1169	Print dimensions of data
1180	Cast to conf
968	Feature importance with random forest
497	Preparing the data
1531	Previous applications categorical features
833	Distribution of fare
389	Check for empty images
193	Blackhat with blackhat
1768	shuffling the data
1349	Leak Data loading and concat
882	Cross validation using Kaggle
725	Building a final model
867	Turn the hyperparams
841	Evaluation of fare
956	creating entity dataframe
1716	FUNCTIONS TAKEN FROM
110	Unit sales for each year
1379	Should be removed
1338	The first block needs to take care of stride and filter size increase
1323	Parameters for an individual model block
201	function to plot thene image
124	Preprocess the columns
824	Applies the cutout augmentation on the given image
1491	Compute summary of examples
694	remove layter activation layer and use losvasz loss
223	convert best score to int
509	Evaluate the threshold
392	resize train images
1527	Computes official answer key from raw logits
1758	Add other features
918	OneHot Encoding
287	load Model Weights
1656	written by MJ Bahmani
375	This function is going to run
1540	check the SMAPE score
947	Add random hypotes
511	Categorical features
154	Patient time features
907	Normalize to the objects
1127	Import .csv files
200	Importing necessary libraries
103	Split the data
1335	Squeeze and Excitation
1107	Load sentiment file
30	Load the data and columns
1802	Correlation between net and world
484	make a prediction model
340	Save predictions to csv
832	We submit the solution
1538	Check the actual values
985	Visualizing the credit balance
437	Preprocess the data
1696	Set up the evaluate function
1548	Merge with weather data
337	model check point
776	creating the pair matrix
4	Remove Unused Columns
799	Train the model
543	Ekush Classification Report
931	count possible combinations
584	Transform dataframe into a Pandas dataframe
1579	Convert x and y
1426	Number of missing values
1138	Training the model
1355	iterate through all the columns of a dataframe and modify the data type
996	Load Feature Names
317	Train the model
388	add the histogram in image
607	Return a normalized weight vector for the contributions of each class
445	Extracting informations from street features
119	Predict and Submit
1186	accumulate the model to the session
1110	Extract processed data and format them as DFs
1512	size and spacing
642	MosT common positive words
117	submit test data
102	grid mask augmentation
1196	save the best performance in a fold
976	DONT LOAD THESE
1029	to dataframe with column names
1087	one hot encoding of class
322	Fitting our model
116	Set global parameters
1040	Merge data with previous count
164	Reading the image
380	Is the item length of the data
140	Set seed for reproducability
1218	Total number of events
139	Get the model
1382	Example of corpus
481	Use the Keras tokenizer
826	Load the image on which data augmentaion is to be performed
245	Filter Andorra , run the Linear Regression workflow
1166	Test Data Preparation
504	A simple line plot of individual models
1185	checking if user has competition
1217	and reduced using summation and other summary stats
81	Adding linear layer
1268	Quadratic Weighted Kappa
167	Only the classes that are true for each sample will be filled in
858	make a scatter plot of actual and actual validation
1346	plot each task using the examples
1672	Initialize patient entry into parsed
1157	numpy and matplotlib defaults
1070	Converting image to LAB Color model
647	importing the libraries
534	Evaluate the model
418	Preview of the data
1371	Train our model
643	import the torch
488	Extract target features
1475	MAKE MIXUP IMAGE
1686	FIND ORIGIN PIXEL VALUES
268	Some useful features
1569	Build Bayesian Optimization
1321	Initialize the model
614	Weight of the class is inversely proportional to the population of the class
936	Fitting a model
1428	Add a column to each sequence
148	Number of clicks by IP
19	Imputations and Data Transformation
938	Write column names
1272	find unique values
1153	Checking the cropped train and validation sets
204	Remove other air pockets insided body
150	Analyze the data for a batch of values
1101	Run model with tuned parameters
436	Preprocess the data
1036	average the distribution of missing values
1422	collapse to train and test set
271	import necessary libraries
714	Count the missing values
1447	Create submission file
500	Separate the zone and subject id into a df
756	important dataset exploration
583	Get predicted probabilities for each model
1632	Create new rows with the missing values
1566	same for test
1112	extract different column types
619	Create the areas for the contours
1252	Load Model Weights
1339	Final linear layer
1649	Add extra timeseries
16	To plot pretty figures
1367	DISPLAY PROBABILITY FUNCTION
1135	load train data
613	Import required libraries
1358	iterate through all the columns of a dataframe and modify the data type
212	note that score is copied from
275	some basic dropout
1763	set the target matrix
236	Filter Spain , run the Linear Regression workflow
219	Import required libraries
1647	number of folds
1775	This enables operations which are only applied during training like dropout
557	Plot number of time features
577	Get the sample
1238	Sequence operations and Tensorflow
431	Separating target and features
702	ADD PSEUDO LABELED DATA
416	load train data
1298	Convert annotation to CSV
540	Now we can add the along each feature
1035	unique values to one column
1605	Preparing the data
1752	Creating new entity dataframe
104	Compile this model
1770	missing entries in the embedding are set using np.random.normal
1299	Function to get language embeddings for classification
1521	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
566	sklearn is only imported for splitting the data
90	fast less accurate
7	declare target , categorical and numeric columns
683	transforming the data into a dataframe
267	Remove player clusters
1304	Clear GPU memory
536	Now we can add the along each feature
1612	Split train and validation sets
904	Clean up memory
1129	Loading the data
875	dict存储的参数转化为 结构化的数据 df
1148	Lets view some predictions and detected objects
1413	Make label dict features
1603	Checking any missing values ,
1496	Defining a list of features
305	load Model as Categorical
1117	Compute QWK based on OOF train predictions
73	create network and compiler
1723	missing entries in the embedding are set using np.random.normal
1192	error in case
1131	define the dataloaders with the previous dataset
303	Specify the configuration
880	Loading the data
261	create a scatter plot
85	Sample real data
631	shuffling the equation
746	Read in the train set and the target series
1792	Plot the Web View
732	Load an image
430	Preprocessing data to transform object data types
1497	get the pretrained model
210	Make categorical columns
724	Force the full CNN
1146	load mapping dictionaries
1485	process feature importance
1271	Check if the object is needed
316	Linear Output Layer
1487	check the last checkpoint
332	Read a DICOM files and medical images
362	Get an image from a file
844	Train the model
50	One Hot Encoding
367	Train the model
680	Any results you write to the current directory are saved as output
843	Splitting train and test
508	Evaluate the threshold
1639	Compute ratio for plot
1784	Compute the STA and the LTA
221	highlight the highlight
783	Original code from
79	resize the image to desired size
963	define tf.rix with features
455	Draw bounding boxes with the predicted masks
408	words wrong datatype
942	Show the best score
716	Train a random forest on the test data
625	Bounding Boxes per day
1742	SCALE target variable
456	Import necessary libraries
48	use colors based on histogram
395	Checking the confusion matrix
816	Cast to desired
672	Stratified Models
1499	check the last checkpoint
1745	using sieve of eratosthenes
571	Revenue based on the day of the day
719	SAVE MODEL TO A FILE AND RESTORE FROM FILE
1667	Inference on the test data
818	Adding the predictions column
1504	LIST DESTINATION PIXEL INDICES
678	using outliers column as labels instead of target column
56	Importing required fastai modules and packages
1444	Create square matrix
1427	Set values for various parameters
1624	Read the data
1189	collect data sets
1404	Format the submission
78	save dictionary as csv file
222	Linear Output Layer
1655	Draw the heatmap using seaborn
889	realligning two datasets based on the features selected in training
707	Compute QDA and Submit
1459	checking missing data
893	Loading the data
1241	load test image
1047	Simple data manipulation
1291	Wrapper for numerical block
1653	Target Variable Analysis
1532	Choose and initialize a model
1505	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
521	Add these results to the map
1362	Preprocess date column
1621	Data Visualization by variable Length
3	Reset Index for Fast Update
1064	Generate data for the BERT model
1102	Store predictions for Submission
403	Train a CatBoostModel
745	build a dict to convert surface names into numbers
883	Encode Hyperparameters
143	fit the model
1544	Read in the dataframe
1281	Any results you write to the current directory are saved as output
1757	What are the other features
615	An optimizer for rounding thresholds
1038	Previous applications categorical features
633	Run basic model
836	Run the model
668	Load train and test data
1511	Build the data
605	Pad the audio
1388	predict the bounding boxes with the required boxes
260	convert this weight column to int
1506	FIND ORIGIN PIXEL VALUES
861	Split into training and testing data
1629	Time series transformation
356	Load an image from a file and modify it
1165	Create part of data
616	USE the signal
831	replace NaNs in train and test data with
1122	create a baseline model with all features
0	Load Images for DICOM
622	Examples for usage and understanding
587	lets see how many bbox count in Log Error count
1334	Expansion and Depthwise Convolution
1341	create the baseline model
1187	limit target column
659	Fitting the model
952	Preparing the data
1469	Fitting the numerical columns
905	to dataframe with column names
1482	create the new features
1046	mean to groupby
969	One hot encoding
347	Import required libraries
173	making the unique values
581	make sure multiple steps into batch
1055	to dataframe with column names
686	Make train and test images
1488	A wrapper for a single example
1443	Create square matrix
635	Create the model
1613	Split train and validation sets
1616	Computes gradient of the Lovasz extension w.r.t sorted errors
301	Read the image
1322	Split input data
94	Lets take a look at the value of any columns
1715	Ensure determinism in the results
1585	get the features
149	Looking at the dataset
932	Get the prediction
848	Create random ForestRegressor
1178	DICOM files allow to store metadata along with pixel data inside them
398	Checking the confusion matrix
786	Plot the most important features
1012	Add column names for categorical columns
1295	Load Train Data
499	handle .ahi files
302	Read the image
11	Compute the STA and the LTA
218	MinMax scale all importances
870	Write column names
448	Show different train data for modelling
360	train and test data
1060	Splitting the data into train and validation sets
951	Loading the data
1457	checking missing data
1141	Does the correlation between previous softmax
510	process remaining batch
248	Drop rows with target and target
934	Fitting a model
273	get lead and lags features
1199	Plot over folds
1453	drop rows with NaN values
1144	Use a map of China
649	Any results you write to the current directory are saved as output
906	Cumulative importance plot
1033	Separate the heads with the numerical values
873	Write column names
1244	Load the data
913	collate agg categorical features
325	Making the train and test data
972	Random Forest Results
921	normalize the data
530	Evaluate the model
506	functions for plotting results
567	A simple Keras implementation that mimics that of
1067	split training and validation data
992	Getting the relationships associated with an important features
489	Number of entries by column
562	define lgbm hyperparammeters
900	check if type of encoding
158	What is the original image shape
585	There are two rows with same fare
480	words wrong datatype
556	Create feature dictionary
687	Get the image and mask
654	Function to load test data
1106	Load metadata file
165	just Thresholding .
1668	Lets start with the target type
308	Creating a larger dataset
473	Loading the data
784	fold Cross validation FVC model
312	select numerical columns
1640	Set up data types
1625	Prepare the full training data
849	get feature importances
834	Custom Loss Function
677	filtering out outliers
1111	Extract processed data and format them as DFs
954	created a dummy column
851	Time series computation
127	In case you want to train and test categorical features
423	OLD per month
860	Evaluating the model
797	Convert data to numpy array
40	Load the data
779	Target and age
1455	Convert to NN
12	This block is SPPED UP
720	Import required libraries
1620	Checking for Null values
798	split into training and validation sets
1433	Lets start by the chart
1435	take a look of DICOM files and medical images
1118	Manually adjusted coefficients
1553	Average the week vs
999	Construct an additional parameter
1669	gather input and output parts of the pattern
558	change column names
892	Select the columns with very missing values
1484	iterate through each file and generate the predictions
59	Unfreezing the model and checking the best lr for another cycle
796	Define a model
688	draw a number of bounding box
828	Load the image on which data augmentaion is to be performed
1741	HANDLE MISSING VALUES
957	Getting the relationships associated with an important features
1340	Which features are correlated
1717	LOAD PROCESSED TRAINING DATA FROM DISK
55	Find the clusters
806	cast floats of hyperparameters to int
1123	LGBM Dataset Formatting
171	A Fully connected model
1673	Add box if opacity is working
277	reorder the input data
945	iteration score 两列
372	Fitting our model
1198	We create the submission file
532	Now we can add the along each feature
1249	extract Training data
670	Transforming the Data
1578	Fill NaNs with mean
1474	Taken from batches
669	Exploratory Data Analysis
691	Computes gradient of the Lovasz extension w.r.t sorted errors
1113	Subset text features
1766	cross validation and metrics
863	TARGET AUC
516	add the team confusion data
1434	function to extract unknown word length
1637	check distribution of data
1604	checking missing columns
106	Reading the dataset
1619	Checking for Null values
459	train random search
1401	Convert series to linear models
82	Make the submission file
63	plot the distribution of continuous variables
1179	Create a confusion matrix
1710	Keras Libraries for Neural Networks
41	Check for missing values
1160	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1423	function to convert sentence to wordlist
258	Predict on test data
1231	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
1643	Distribution of the device
446	Encoding the Regions
1764	blending feature matrix with the expected value
524	Now we can add the along each feature
755	Normalize the data
343	Create a generator that iterate over the generator
1191	Estimate the submission group
335	Looking data format and types
637	Import Libraries and Data Input
1315	Public LB score
52	use colors based on histogram
1571	load best model
768	Computing target variable
812	Write column names
406	Preparing the data
155	Download the distribution of the day
1530	Merge the data with the previous dataset
208	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
617	reduce training and testing sets
1773	for numerical stability in the loss
84	Class distribution of class distribution
711	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1119	Distribution inspection of original target and predicted train and test
1729	Add train leak
1592	some config values
1405	rolling mean for same store
698	Applying CRF seems to have smoothed the model output
25	Load train and test data
1678	convert text into datetime
1003	define the matrix of features
216	MinMax scale all importances
887	Previous columns identified with previous features
1670	Set seed for all
941	Get the results
1615	show mask class example
891	Drop unused columns
1201	TPU Strategy and other configs
553	Ekush Classification Report
990	Look at the credit card
1085	make the predicted image
1380	concatenate some features
549	Ekush Classification Report
660	Computes and stores the average and current value
502	Rescaling the Image Most image preprocessing functions want the image as grayscale
177	Most common level
1726	for numerical stability in the loss
923	Cumulative importance plot
1207	read the dataset
1344	Test a transformed prediction
1490	Read candidates with real multiple processes
1743	EXTRACT DEVELOPTMENT TEST
58	you can play around with tfms and image sizes
1806	Read Train and Test data
665	Plot date agg features
1791	Get year , month and day from the date column
998	Normalize mode and counts
434	OSIC Pulmonary Fibrosis Progression Analysis
726	 photos from the photo
529	GRID SEARCH for BEST TARGETS
697	Precision helper function
572	Sales by importance
565	make a prediction
20	Imputations and Data Transformation
391	select only the test data
1595	Pad the sentences
1068	Print CV scores , as well as score on the test data
737	draw a prediction
1373	Train the LGBM model
1400	Neural Network with numerical values
568	Load the data
1259	Using original generator
246	Set the dataframe where we will update the predictions
1282	Split the train and test data
1541	Evaluate the model
661	Inference and Submission
728	Merge and Labels
1044	drop needed columns
458	define list of hyperparameters
811	Create a file and open a connection
17	Now extract the data from the new transactions
95	Returns indices for this fold
1651	Get the score data
226	create a scatter plot of basic dimensions
414	check for plot
708	ADD PSEUDO LABELED DATA
249	Train the model
1053	save best scores
679	Split the data into a train and validation sets
595	Create a model to run
377	Print mean of error and save results
1326	Round number of filters based on depth multiplier
1363	Change this to a map
1751	Creating a entity dataframe
352	Read test data
1689	Return list of all classes
763	current pixel labels
1816	load train data
1575	Reading Train and Test Data
1051	parameter value is copied from
1297	make test predictions
464	Merging transaction and identity dataset
1691	Lets convert data for this fold
1001	limit target columns
1588	Fit a matrix
1277	Plot the image
123	Cleaning up text
738	draw a prediction
197	Blackhat with blackhat
1397	I will look at the distribution of var days
122	F cleaning and cleaning them
760	How many heads of household
1695	Plot each task
1216	Evaluating the model time
780	For each feature
706	MODEL AND PREDICT WITH QDA
1366	Load the pandas data
196	To plot how many images are very different
1078	Set values for various parameters
1345	Test each evaluation
495	Setting the Paths
1164	M Missing Values
1227	Read the data
603	Encoding Categorical Features
1736	Exploring the data
537	GRID SEARCH for BEST TARGETS
1142	Get the importance of the column
289	NumtaDB Classification Report
1115	Check if columns between the two DFs are the same
852	Fit the model
1679	get some sessions information
1688	Check if all shapes match
232	Double check that there are no informed ConfirmedCases and Fatalities after
369	split to train and validation sets
183	Most common brand features
309	Create an embedding matrix of words in the data
1552	Average values for every day of year
1522	FIND ORIGIN PIXEL VALUES
1317	Initialize the model
129	See sample image
280	create a scatter plot
46	Most common values
1121	Remove missing target and columns
1468	Update the global features
299	Read the image from the folder
1706	Print a random candidate length
949	Plot the densities of learning rate
653	function to load training data
1283	Split the data
770	COMPUTE LB SCORE
1011	Lets check categorical features
105	Reading the dataset
1328	Gets a block through a string notation of arguments
1190	concatenate event codes per batch
1279	Plot the image
291	Prepare test data
1170	Prepare the data
1600	Detect the heatmap with these correlations
1137	Same for month
348	display correlation matrix
188	Lets generate a wordcloud
1077	Split stop words
1089	test if the same size is equal to the test set
1523	Explore the results
1175	Add the map
1451	Inference on test data
1520	LIST DESTINATION PIXEL INDICES
66	load and shuffle filenames
410	START WITH BEST TRAINING DATA MODEL
503	scale pixel values to grayscale
75	create train and validation generators
590	Gaussian Target Feature
1690	Removing list of outliers
1374	Drop target , fill in NaNs
1797	Plot rolling statistics
1072	Region of interest
152	Show some categories of dropers
830	Segmented area distribution
576	Global training settings
311	Prepare target columns
87	RANDOMLY SELECT TRAIN AND VALIDATE
254	Find out the optimizer
121	Check the coverage of the question data
1042	Sort the table by percentage of missing descending
782	Change columns names
426	Distribution of log tran transformation
620	Create the areas for the contours
610	returns filename instance
809	Train the model
1258	padding image and resizing
231	Merge train and test , exclude overlap
794	drop the feature selection
535	Ekush Classification Report
1274	loop to see colors
461	train random search
453	Draw the draw bounding boxes on the image
304	class weights to holdout set
1239	Run the prediction
602	Reading the dataset
439	total number of IntersectionIds
1091	serialized results to a dictionary
582	Get predicted probabilities for each model
1223	Predicting with the best parameters
1803	draw the face
624	Gaussian Bounding Boxes
757	first label counts
101	load the image file using cv
1762	checking missing data
1810	Importing the librarys to metrics
640	Histogram of positive examples
1057	apply transforms to image
1535	read the data
1263	Using original generator
83	Load the data
160	make a scatter plot
1441	prepare test data
1296	Pixel Normalization and Image Augmentation
1026	Train the model with early stopping
76	load and shuffle filenames
874	Fitting a model
2	Add new Features
1471	Order does not matter since we will be shuffling the data anyway
894	calculate the average feature importance score for each feature
705	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
608	returns filename instance
1224	select proper model parameters
298	Read the image from the folder
33	prophet expects the folllwing label names
237	Filter Spain , run the Linear Regression workflow
295	I will be used as submission
723	Maping the ordinal Features
961	define the matrix of features
988	Visualizing the credit card
987	Previous Balance History
72	define iou or jaccard loss function
703	STRATIFIED K FOLD
239	Filter Italy , run the Linear Regression workflow
1065	Model Hyper Parameters
1438	Data image augmentation
1421	Divide the sentences into dataframe
879	run all numerical parameters
965	reset index to conform to how kagglegym works
1155	detect and init the TPU
823	Custom Cutout augmentation with handling of bounding boxes
202	Determine current pixel spacing
993	Find Feature Names
1458	checking missing data
1378	Generate labels for the image
230	Implementing the SIR model
1606	Categorical features
912	For each borrower
272	configurations and main hyperparammeters
1543	Check the SMAPE score
1432	Create a new link of the title
885	English is not my first language , so sorry for any error
169	Binary convolutional layer
1009	separate the column names
314	Drop rows with target and target
609	Save image to disk
766	Draw the legend and informative columns
1056	Train Validation Split
1790	Load libs
1253	A model is saved every training epoch if the validation error improved
1151	Prepare data for training
479	Where doing the right suspects
1778	Load data and describe it a bit
909	Separate the heads with the numerical values
1641	Most frequent labels in the correct column
