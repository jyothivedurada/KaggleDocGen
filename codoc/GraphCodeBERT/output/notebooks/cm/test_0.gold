0	The test set is almost 2.5 times larger than the train set . It also has 5 features with missing data . Now , let 's take a look at the target . We want to see the distribution of target values in the train set .
1	Now let 's import datatable
2	Now , let 's fit the model
3	Let 's see what files we have in the input directory
4	We see that in addition to the usual , ` train ` , ` test ` and ` sample_submission ` files , we also have ` merchants ` , ` historical_transactions ` , ` new_merchant_transactions ` , and even one ( HORROR ! ! ! ) excel file - ` Data_Dictionary ` . The names of the files are pretty self-explanatory , but we 'll take a look at them and explore them . First , let 's look at the ` train ` and ` test ` files .
5	Seems like a very wide range of values , relatively spaking . Let 's take a look at the graph of the distribution
6	Seems like a pretty nice normal-looking distribution , except for the few anomalous elements at teh far left . They will have to be dealt with separately . Let 's look at the `` violin '' version of the same plot .
7	Let 's now look at the distributions of various `` features
8	Now we add another set of features from [ this kernel
9	Imputations and Data Transformation
10	Impute any values will significantly affect the RMSE score for test set . So Imputations have been excluded
11	Detect and Correct Outliers
12	Now we 'll load the Quora datasets
13	We will embed words from these tweets into a word-vector space using one of the previously trained word embeddings . Here we use a 300-dimensional vector space that comes curtesy of FastText . Unfortunately , this embedding is not available for the Quora compatition , but as we are using this kernel just for the educational purposes , that will be fine . We will also limit the length of text to 220 words . This is an overkill for questions , but for general purpose it is rather small text length . The original was aimed at much longer text sizes , and this was a reasonable length for those purposes . The best embedding that we used in Toxic limited length to 900 words .
14	In order for our pretrained models to work , we need to transform the text here into the appropriate vectorized format .
15	We also need to pad the tweets that are less than 220 words , which is essentially all of them .
16	In other words , at nearly 1.0 probability the model seems pretty confident about the `` toxicity '' of some of the tweets . Now let 's put the predictions into a dataframe , so we can have a better view of them and how they relate to the actual tweets .
17	Let 's load up all the individual predictions
18	So this is a pretty standard fare of Kaggle compatition files : ` train ` , ` test ` and ` sample_submission ` . However , this competition also contains a hidden ` test ` file , that is only accessible to Kaggle . All the code is supposed to run in parallel on this file , but we ca n't really `` probe '' it .
19	Wow , this is a very balanced dataset . No surprises , since this is all presumably artificial data .
20	Let 's now look at the distributions of various `` features
21	Now there appears to be one feature that is not gaussian
22	Seems farily straightforward - just ID , text and target firlds . In addition , the train set is very decently sized - 1.3 million records is probably enough for a decent text classifier . Let 's take a look at the targetvariable
23	For our second model we 'll use TF-IDF with a logistic regression . The next couple of secontions are based on my [ LR with n-grams notebook ] ( Firtst , let 's embed all the text vectors
24	The following Logistic Regression is based on Premvardhan 's [ Count Vectorizer notebook
25	Now we 'll train on the full set and make predictions based on that
26	Whoa , that 's a pretty significant AUC ! At AUC of 0.85 there is a very significant difference between the train and test sets , and a very very good chance of a major shakeup ... Let 's look now at the top 20 `` adversarial '' features .
27	Now let 's look at the data
28	Seems that all of the features are numerical , with approximately 0 mean and 1.0 standard deviation . That 's very interesting . Let 's now look at the distributions of various features in the train set
29	Next , we 'll do some unviariate estimations of the target . We 'll try to find the features that best predict the target by themselves .
30	Not bad ! AUC of 0.903 is pretty good for any predictive model . But is it overfitting ? There is only one way to find out ! Let 's submit it and see how it performs on public LB .
31	So PCA does n't seem to help much here . Let 's take a look at clustering . We 'll try to fit the KMeans clustering on the entire dataset , and try to see what the optimal number of clusters is .
32	Read data set
33	Text preprosesing source
34	Model Validation on train data set
35	It turns out that just the image metadata and and the image size contains a lot of useful information We 'll start by creating feature-engineered datasets with just that information . The approach here follows the one in this notebook
36	We 'll now add metafeatures from Chris Deotte 's TF kernel
37	Let 's now look at the distributions of various `` features
38	Let 's take a look at a few images .
39	Let 's now add some non-image features . We can start with sex , and one-hot encode it .
40	Wow , AUC of 0.9999 is as large as it gets ! Let 's see which columns are the most responsible for this discrepancy .
41	Let 's now load the datasets .
42	The metric for this competitiomn is Spearman Correlation , and we will define it here for later use
43	Let 's take a look at some of these
44	And now we embed each chunk individually . This takes about 15 minutes .
45	We see a similar distribution of various statistical aggregates , but by no means the same : seems like there soem substantial distribution shifts between the train and test sets . This will probably be another major concern when it comes to feature selection/engineering . Now let 's do some plotting . We 'll take a look at , naturally , the `` ` target `` ` variable . First , let 's make a histogram of its raw value .
46	This is a highly skewed distribution , so let 's try to re-plot it with with log transform of the target .
47	As expected , this distribution looks much more , ahem , normal . This is probably one of the main reasons why the metric that we are trying to optimize for this competition is RMSLE - root mean square logarithmic error . Another way of looking at the same distribution is with the help of violinplot .
48	That 's ... revealing . And it looks like a fairly nice distribution , albeit still fairly asymetrical . Let 's take a look at the statistics of the Log ( 1+target
49	So this is interesting : there are 256 constant columns in the train set , but none in the test set . These constant columns are thus most likely an artifact of the way that the train and test sets were constructed , and not necessarily irrelevant in their own right . This is yet another byproduct of having a very small dataset . For most problems it would be useful to take a look at the description of these columns , but in this competition they are anonymized , and thus would not yield any useful information . So let 's subset the colums that we 'd use to just those that are not constant .
50	If we treat all the train matrix values as if they belonged to a single row vector , we see a huge amount of varience , far exceeding the similar variance for the target variable . Now let 's plot it to see how diverse the numerical values are .
51	Wow , not very diverse at all ! Most of the values are heavily concentrated around 0 . Maybe if we used the log plot things would be better .
52	Only marginal improvement - there is a verly small bump close to 15 . Can the violin plot help
53	Not really - the plot looks nicer , but the overall shape is pretty much the same . OK , let 's take a look at the distribution of non-zero values .
54	OK , that 's much more interesting . Let 's do the same thing with the test data .
55	So as we suspected , almost 97 % of all values in the train dataframe are zeros . That looks pretty sparse to me , but let 's see how much variation is there between different columns .
56	So it seems that the vast majority of columns have 95+ percent of zeros in them . Let 's see how would that look on a plot .
57	Meke some plots Build a few models Do feature importance analysis To be continued ...
58	Have a look at this kernel , which is a starting point to the present one Basicaly , it says that D1 is the number of days elapsed since the first transaction of a card . So that in order to identify a unique card , we could aggregate card1 to card6 ( which we assume is stable by card ) , and D1 minus day .
59	On the cell below , I create the cardID resulting of the starting point notebook .
60	We get the following couples of TransactionID that match based on these transformations of V307 and V307plus . Let 's group them
61	Let 's take the example of D1 ( but this approach works with other D features ) . In the following cell , I plot the D1 feature by product . If we take look at the orange dots ( is Fraud=1 ) , we can clearly see that some are forming lines .
62	The intercept of this line is 78 . Does it mean that 78 days before the start date , more cards were used than the other days , and more Frauds appear on this day ( proportions being equals ) ( Frauds are plotted on top of non Fraud ) , or did we identity a fraudulent user ? Let 's check
63	It seems that the spike around day 78 , corresponds to relatively more fraudulent cases I would not recommend to use this Feature for training , as it becomes a proxy of the Transaction day . Also , the original D1 feature is cliped , and many groups would not appear in the testing set , overfitting risk is increased ( especially if you are using Kfolds as the future can easily predicts the past with this variable ) . But i would use it as a grouping feature ( in our example : day 78 can help to identify a specific card ) . For instance , two cards with the same numbers ( card1 to card6 ) that correspond to different cards can be distinguished thanks to this variable .
64	There is some structure there , with `` central '' denser reagion , and more dispersed periphery , but other than that it 's hard to see any distinct groupings .
65	Index the data by id and timestamp . Look at the data types and some basic info about the different columns .
66	Plot feature importance ( from [ here Interesting , it looks like by reindexing the data , the feature importance has changed .
67	IMPORTS
68	LOADING CITIES & SUBMISSION While loading the data , I create a third dimension ' Z ' to apply the penalty as fast as posible . As input I 'm going to use a LKH solver solution provided by [ Kostya Atarik ] ( ( Thanks man
69	COST FUNCTION Can be used to compute the whole tour or just tour chunks . I keep the result as an array because I will be changing the values of it during the process like the tour array .
70	Some runs
71	Getting to Know the Data
72	The Shape of the Data
73	Importing all Libraries
74	Seed everything for reproducibility
75	Defining DataBunch for FastAI
76	Define F1 metric
77	Defining FastAI 's Learner
78	Unfreeze all layers and find best learning rate
79	Predicting for test data
80	Actually , it contains two types of information : if animal was male or female and if it was neutered/spayed or intact . I hope it is a good idea to divided this column into two
81	Also we have information about breed , and some animals has pure or mixed breed . I wonder if breed purity has some impact on the fate of animal .
82	Now , let 's see how different parameters influence on the outcome .
83	Interesting notice : dogs tend to be returned to owner more often than cats . And cats are transferred more often than dogs .
84	Neutered animals have high chances to be adopted , while intact animals are more likely to be transferred .
85	Another one parameter is age , but we have it in different units : years , months , weeks and days . Let 's calculate every age in years and see if there is something interesting inside .
86	Most of the animals in the shelter are 0-1 years old . Let 's see if age has some effect on outcome .
87	Our idea is to run simultaneously many parallel linear regressions for every id ( the 42k series ) and use the resids to estimate quantiles . Let imagine that we have $ N $ series of length $ T $ in a vector $ Y $ with shape $ ( N , T ) $ and covariates of temporal features $ Z $ with shape $ ( T , F ) $ . For id $ i $ we will run the following regression Y_i^T = Z \beta_i^T + \epsilon_i So if we compact all the individual OLS parameters $ \beta_i $ in a vector $ \beta $ . The global model will be written as Y = \beta Z^T + \epsilon STEP 1 : PARAMETER OPTIMIZATION Therefore we can solve the following problem including regularization hat { \beta } _ { \lambda } = \arg\min \mid \mid Y - \beta Z^T \mid \mid_F^2 + \lambda \mid \mid \beta \mid \mid_F It is easy to see that hat { \beta } _ { \lambda } = YZ \left ( Z^T Z + \lambda I_F \right STEP 2 : QUANTILE PREDICTION we have compute then the resid hat { Y } ^ { \lambda } = \hat { \beta } _ { \lambda } Z^T , \quad \hat { \epsilon } = Y - \hat { Y } ^ { \lambda And we finally compute the quantile $ q $ by hat { Y } _ { it } ^q =\hat { Y } ^ { \lambda } _ { it } + quantile ( \hat { \epsilon } _ { it } , q lambda $ can be learned by cross-validation . Now let 's code it out
88	Performance
89	Tokenizing Text
90	Let 's take a look at the text data . Data is still small enough for memory so read to memory using pandas .
91	Gene column is a bit more complicated , values seems to be heavly skewed . Data can still be valuable if normalized and balanced by weights .
92	And finally lets look at the class distribution .
93	Distribution looks skewed towards some classes , there are not enough examples for classes 8 and 9 . During training , this can be solved using bias weights , careful sampling in batches or simply removing some of the dominant data to equalize the field . Finally , lets drop the columns we do n't need and be done with the initial cleaning .
94	Now let 's look at the remaining data in more detail . Text is too long and detailed and technical , so I 've decided to summarize it using gensim 's TextRank algorithm . Still did n't understand anything
95	Mutation and cell seems to be commonly dominating in all classes , not very informative . But the graph is still helpful . And would give more insight if we were to ignore most common words . Let 's plot how many times 25 most common words appear in the whole corpus .
96	Read train_variants , train_text and join them .
97	Read test_variants , test_text and join them .
98	Read stage_1_solutions and join the class values with test files .
99	Import Libraries
100	The data is not balanced . We are going to use the undersampling technique .
101	Apply Underbalancing Techinique
102	The data is not balanced . We are going to use the undersampling technique .
103	Take a look at predictions
104	Ability to Detect Face
105	Loading data etc .
106	Find out matrix ` before
107	Show and save column comparision matrix and save row sets
108	Credit : The code has been adapted from @ mgornergooglestater kernel on Flower classification on TPUs
109	Image Source : medium.com
110	Image Source
111	Preparing the training data
112	Training on the complete Dataset now
113	Data loading and checking
114	Entity % 20Relationship % 20Diagram , % 20M5 % 20forecast.jpg ] ( attachment : Entity % 20Relationship % 20Diagram , % 20M5 % 20forecast.jpg
115	Unique value counts
116	price distribution
117	Before decomposition , looking at each data , we can see that the sales of one day fell sharply and cut . It is speculated that this is because it is taking a break at Christmas . This data will be a strong noise , so this time it will be deleted .
118	Let 's start by checking how many data points there are , and if anything is missing .
119	The following two variables represents the actual FVC values represented in the dataset by the `` Percent '' variable and their difference . Interestingly , there are a couple patients where Percent is over 100 % , meaning the measured FVC is higher than what it 's supposed to be .
120	FVC Difference
121	There does n't seem to be any particular correlation between the categorical variables . The continous variables exhibit a clear positive correlation , between `` FVC '' and `` Percent '' , while the new engineered feature FVC Difference has an almost perfect negative correlation with Percent .
122	FVC Progression by Sex
123	FVC Progression by SmokingStatus
124	In this section I 'll explore the DICOM files representing the CT Lung Scans for the patients , how to load and process them , and how to extract some additional features . For an even more informative explanation , you can visit the Radiology Data Quest blog ( from where I learned the tools used in the rest of this notebook and which I thank for their public explanation . It 's also necessary to thank Kaggle user Dr.Sàndor Kónya ( @ sandorkonya ) for his wonderful Domain Expert Insights , which can be found here ( Part 1 : Part
125	Let 's check the scans for the first patient .
126	Let 's have a look at the distribution of pixels in the images converted to Hounsfield Units .
127	Lung Volume/Total Lung Capacity Estimation
128	Histogram Analysis
129	I 'll select only the columns that we need to reduce some memory usage
130	Count occurance of words
131	All contraction are known
132	Convert to lower case Clean contractions Clean special charactor Convert small caps
133	Let free up some memory before to other hard job . I 'll clean `` ` vocab `` ` and `` ` coverage `` ` up in order for us to have enough memory to continue
134	Let clean memory again , I 'll clean `` ` word_index `` ` and `` ` embedding_index `` ` up in order for us to have enough memory for training
135	Predict submission dates
136	Let 's check number of unique values in every column of our dataset . It will help us in finding out , which type of encoding we can use .
137	Let 's print all features with the unique values and amount of NAN values .
138	So , it is really hard to understand something from this data . Maybe there is a connection between ord_2 ( it can be temperature ) and month . Let 's try to check .
139	Feature ord_5 consist of two letters , so we can divide it on two features .
140	Using LabelEncoding we just change string values to numbers .
141	Let 's get train and test again .
142	To create dataset we use numpy arrays , and our model needs to understand , which features are categorical , which are continuous . So we need to find their indexes .
143	If we want [ reproducible ] ( results , we should fix seeds .
144	Using embedding in NN we can change dimensionality of categorical features . So , we 'll choose new dimensionality for every categorical feature .
145	Prepare Traning Data
146	See sample image
147	Learning Rate Reduction We will reduce the learning rate when then accuracy not increase for 2 steps
148	See how our generator work
149	Prepare Testing Data
150	Create Testing Generator
151	Split the data into train and validation parts
152	Create and set up the model
153	Counting the metric score
154	Saving the model
155	Check out your own CUDA version , version below is Google Colab 's version .
156	You can state below link to your copy of this MMDetection repo . Add .git in the end .
157	Make a simple restart of runtime at this point .
158	More Blending inspired from Giba 's Kernel other used models This blend is to show blending diffrent models make a huge diffrences .
159	phase 1 [ Ensemble
160	Hist Graph of scores
161	phase 2 [ Stacking
162	Pushout + Median Stacking
163	MinMax + Mean Stacking
164	MinMax + Median Stacking
165	I updated importation for a faster version
166	How many different values does our categorial variables take
167	Zoom on this IP
168	Relation between number of click by IP and downloading the app Great ! We 've our number of clicks variable , then we can do our test .
169	Does bots download the app
170	Download by click ratio I think this indicator could help us to understand what kind of clickers download the app the most , in poportion of there amount of clicks .
171	Unsurprised , most of IPs have a low ratio ( 50 % lower than 0.0025 and 75 % lower than 0.2 ) . I guess it would be more interesting to check by category of clicker .
172	Attributed time analysis
173	Back to the click time We 've currently analyze the amount of clicks by IP adress but not the click time . Maybe we could find some interesting information here .
174	Download rate by hour
175	Importation of a entire day data
176	We can now print the results
177	Dealing with color
178	Perhaps the simplest approach for this problem is to assume that there are two classes in the image : objects of interest and the background . Under this assumption , we would expect the data to fall into a bimodal distribution of intensities . If we found the best separation value , we could `` mask '' out the background data , then simply count the objects we 're left with . The `` dumbest '' way we could find the threshold value would be to use a simple descriptive statistic , such as the mean or median . But there are other methods : the `` Otsu '' method is useful because it models the image as a bimodal distribution and finds the optimal separation value .
179	Deriving individual masks for each object
180	A quick glance reveals two problems ( in this very simple image There are a few individual pixels that stand alone ( e.g . top-right Some cells are combined into a single mask ( e.g. , top-middle Using ` ndimage.find_objects ` , we can iterate through our masks , zooming in on the individual nuclei found to apply additional processing steps . ` find_objects ` returns a list of the coordinate range for each labeled object in your image .
181	Label 2 has the `` adjacent cell '' problem : the two cells are being considered part of the same object . One thing we can do here is to see whether we can shrink the mask to `` open up '' the differences between the cells . This is called mask erosion . We can then re-dilate it to to recover the original proportions .
182	Convert each labeled object to Run Line Encoding
183	Firstly , we 'll check some general informations about our dataset
184	Top 10 categories by number of product
185	So , some categories are expensive , but most are cheap
186	Can we split those categories by level Items seems to be classify with an tree structure of 3 levels .
187	Prices of the first level of categories
188	Top 10 brands by number of products
189	What are their top categories
190	Does shipping depends of price
191	Can we get some informations out of the item description
192	What words do people use
193	Can the length of the description give us some informations
194	Is there a correlation between description length and price
195	Method for get TSNE 2D axis
196	Visualize RNA-2D Structure
197	In 2D-Visualization it is difficult to picture which section is which part of RNA . To make it easier , we will generate a Graph Structure . The neato method can take that as input and create a nice visualization of the graph
198	Visualize RNA-2D Structure
199	Graph Representation of RNA structure
200	Let 's take a look at one of the patients .
201	Please note that when you apply this , to save the new spacing ! Due to rounding this may be slightly off from the desired spacing ( above script picks the best possible spacing with rounding ) . Let 's resample our patient 's pixels to an isomorphic resolution of 1 by 1 by 1 mm .
202	Pretty cool , no Anyway , when you want to use this mask , remember to first apply a dilation morphological operation on it ( i.e . with a circular kernel ) . This expands the mask in all directions . The air + structures in the lung alone will not contain all nodules , in particular it will miss those that are stuck to the side of the lung , where they often appear ! So expand the mask a little This segmentation may fail for some edge cases . It relies on the fact that the air outside the patient is not connected to the air in the lungs . If the patient has a [ tracheostomy ] ( this will not be the case , I do not know whether this is present in the dataset . Also , particulary noisy images ( for instance due to a pacemaker in the image below ) this method may also fail . Instead , the second largest air pocket in the body will be segmented . You can recognize this by checking the fraction of image that the mask corresponds to , which will be very small for this case . You can then first apply a morphological closing operation with a kernel a few mm in size to close these holes , after which it should work ( or more simply , do not use the mask for this image ) . pacemaker example Normalization Our values currently range from -1024 to around 2000 . Anything above 400 is not interesting to us , as these are simply bones with different radiodensity . A commonly used set of thresholds in the LUNA16 competition to normalize between are -1000 and 400 . Here 's some code you can use
203	As a final preprocessing step , it is advisory to zero center your data so that your mean value is 0 . To do this you simply subtract the mean pixel value from all pixels . To determine this mean you simply average all images in the whole dataset . If that sounds like a lot of work , we found this to be around 0.25 in the LUNA16 competition . Warning : Do not zero center with the mean per image ( like is done in some kernels on here ) . The CT scanners are calibrated to return accurate HU measurements . There is no such thing as an image with lower contrast or brightness like in normal pictures .
204	Import Required Libraries
205	Apply Logistic Regression
206	Import libraries
207	XGB
208	Logistic Regression
209	Linear Regression
210	Comparison of the all feature importance diagrams
211	Thanks to Automatic FE The main code for basic FE
212	Download datasets
213	Selection part of data for automatic FE - 10000 meters and it 's preprocessing
214	Thanks to
215	FS with the Pearson correlation
216	FS with SelectFromModel and LinearSVR
217	Import libraries
218	Commit now
219	Commit 0 ( parameters from commit
220	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
221	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
222	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
223	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
224	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
225	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
226	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
227	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
228	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
229	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
230	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
231	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
232	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
233	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
234	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
235	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
236	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
237	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
238	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
239	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
240	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
241	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
242	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
243	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
244	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
245	Parameters and LB score visualization
246	From notebook My upgrade : structure of model
247	Ensembling the solutions and submission
248	TABLE OF CONTENTS Exploratory data analysis ( EDA ) ] ( section COVID-19 global tendency excluding China ] ( section COVID-19 tendency in China ] ( section Italy , Spain , UK and Singapore ] ( section SIR model ] ( section Implementing the SIR model ] ( section Fit SIR parameters to real data ] ( section Data enrichment ] ( section Join data , filter dates and clean missings ] ( section Compute lags and trends ] ( section Add country details ] ( section Predictions with machine learning ] ( section Ridge Regression for one country ] ( section Ridge Regression for all countries ( method 1 ) ] ( section Ridge Regression for all countries ( method 2 ) ] ( section Ridge regression with lags ] ( section
249	Implementing the SIR model
250	As you see , the process is really fast . An example of some of the lag/trend columns for Spain
251	Let 's try to see results when training with a single country Spain
252	Italy
253	Germany
254	Albania
255	Andorra
256	The analysis showed that many values ​​are only available in stations 1 and 2 , while others have much less data . We propose that at the start code , the BOD5 prediction should be carried out only for data from the first two stations
257	Linear Regression is a linear approach to modeling the relationship between a scalar response ( or dependent variable ) and one or more explanatory variables ( or independent variables ) . The case of one explanatory variable is called simple linear regression . For more than one explanatory variable , the process is called multiple linear regression . Reference [ Wikipedia Note the confidence score generated by the model based on our training dataset .
258	Support Vector Machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis . Given a set of training samples , each marked as belonging to one or the other of two categories , an SVM training algorithm builds a model that assigns new test samples to one category or the other , making it a non-probabilistic binary linear classifier . Reference [ Wikipedia
259	Linear SVR is a similar to SVM method . Its also builds on kernel functions but is appropriate for unsupervised learning . Reference [ Wikipedia
260	Stochastic gradient descent ( often abbreviated SGD ) is an iterative method for optimizing an objective function with suitable smoothness properties ( e.g . differentiable or subdifferentiable ) . It can be regarded as a stochastic approximation of gradient descent optimization , since it replaces the actual gradient ( calculated from the entire data set ) by an estimate thereof ( calculated from a randomly selected subset of the data ) . Especially in big data applications this reduces the computational burden , achieving faster iterations in trade for a slightly lower convergence rate . Reference [ Wikipedia
261	This model uses a Decision Tree as a predictive model which maps features ( tree branches ) to conclusions about the target value ( tree leaves ) . Tree models where the target variable can take a finite set of values are called classification trees ; in these tree structures , leaves represent class labels and branches represent conjunctions of features that lead to those class labels . Decision trees where the target variable can take continuous values ( typically real numbers ) are called regression trees . Reference [ Wikipedia
262	Random Forest is one of the most popular model . Random forests or random decision forests are an ensemble learning method for classification , regression and other tasks , that operate by constructing a multitude of decision trees ( n_estimators= [ 100 , 300 ] ) at training time and outputting the class that is the mode of the classes ( classification ) or mean prediction ( regression ) of the individual trees . Reference [ Wikipedia
263	Light GBM is a fast , distributed , high-performance gradient boosting framework based on decision tree algorithms . It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise . So when growing on the same leaf in Light GBM , the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms . Also , it is surprisingly very fast , hence the word ‘ Light ’ . Reference [ Analytics Vidhya
264	Tikhonov Regularization , colloquially known as Ridge Regression , is the most commonly used regression algorithm to approximate an answer for an equation with no unique solution . This type of problem is very common in machine learning tasks , where the `` best '' solution must be chosen using limited data . If a unique solution exists , algorithm will return the optimal value . However , if multiple solutions exist , it may choose any of them . Reference [ Brilliant.org
265	Bootstrap aggregating , also called Bagging , is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression . It also reduces variance and helps to avoid overfitting . Although it is usually applied to decision tree methods , it can be used with any type of method . Bagging is a special case of the model averaging approach . Bagging leads to `` improvements for unstable procedures '' , which include , for example , artificial neural networks , classification and regression trees , and subset selection in linear regression . On the other hand , it can mildly degrade the performance of stable methods such as K-nearest neighbors . Reference [ Wikipedia
266	ExtraTreesRegressor implements a meta estimator that fits a number of randomized decision trees ( a.k.a . extra-trees ) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting . The default values for the parameters controlling the size of the trees ( e.g . max_depth , min_samples_leaf , etc . ) lead to fully grown and unpruned trees which can potentially be very large on some data sets . To reduce memory consumption , the complexity and size of the trees should be controlled by setting those parameter values . Reference [ sklearn documentation In extremely randomized trees , randomness goes one step further in the way splits are computed . As in random forests , a random subset of candidate features is used , but instead of looking for the most discriminative thresholds , thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule . This usually allows to reduce the variance of the model a bit more , at the expense of a slightly greater increase in bias . Reference [ sklearn documentation
267	The core principle of AdaBoost is to fit a sequence of weak learners ( i.e. , models that are only slightly better than random guessing , such as small decision trees ) on repeatedly modified versions of the data . The predictions from all of them are then combined through a weighted majority vote ( or sum ) to produce the final prediction . The data modifications at each so-called boosting iteration consist of applying N weights to each of the training samples . Initially , those weights are all set to 1/N , so that the first step simply trains a weak learner on the original data . For each successive iteration , the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data . At a given step , those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased , whereas the weights are decreased for those that were predicted correctly . As iterations proceed , examples that are difficult to predict receive ever-increasing influence . Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence . Reference [ sklearn documentation
268	Thanks for the example of ensemling different models from
269	We can now compare our models and to choose the best one for our problem .
270	Commit now
271	Commit 1 ( parameters from
272	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
273	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
274	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
275	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
276	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
277	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
278	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
279	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
280	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
281	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
282	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
283	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
284	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
285	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
286	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
287	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
288	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
289	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
290	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . GaussianNoise_stddev = 0 . LB = -6 .
291	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . GaussianNoise_stddev = 0 . LB = -6 .
292	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . GaussianNoise_stddev = 0 . LB = -6 .
293	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . GaussianNoise_stddev = 0 . LB = -6 .
294	Parameters and LB score visualization
295	Average prediction
296	Commit now
297	Import libraries
298	Prepare Training Data
299	Thanks to March Madness 2020 NCAAM EDA and baseline March Madness 2020 NCAAM : Simple Lightgbm on KFold
300	XGB
301	Divide features into groups
302	My experience ( for example [ Titanic ( 0.83253 ) - Comparison 20 popular models ] ( has shown that simulation using LGBMClassifier is better if you set both parameter num_leaves and parameter max_depth
303	Commit now
304	LGB
305	Wavenet with SHIFTED-RFC Proba and CBR
306	Code from notebook
307	My upgrade of parameters
308	Using my notebook
309	How many images are in each folder
310	Create a Dataframe containing all images
311	Balance the target distribution
312	Set Up the Generators
313	What is the AUC Score
314	Create a Classification Report
315	MAKE A TEST SET PREDICTION
316	Set up the generator
317	Make a prediction on the test images
318	Create a submission file
319	Create a new column called file_name
320	Create Binary Targets
321	Balance the target distribution
322	Train Test Split
323	Set Up the Generators
324	Cohen Kappa Score ( Quadratic
325	public LB score was 0.048 . I found that this score could be improved to 0.046 by averaging the predictions of this cnn model with the predictions generated by another logistic regression model that I created . The following kernel explains how to easily do this Using a weighted average ( 0.6 x cnn_model + 0.4 x logistic_regression_model ) further improved the public LB score to 0.045 .
326	Define X and y
327	Linear Regression is a linear approach to modeling the relationship between a scalar response ( or dependent variable ) and one or more explanatory variables ( or independent variables ) . The case of one explanatory variable is called simple linear regression . For more than one explanatory variable , the process is called multiple linear regression . Reference [ Wikipedia Note the confidence score generated by the model based on our training dataset .
328	Support Vector Machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis . Given a set of training samples , each marked as belonging to one or the other of two categories , an SVM training algorithm builds a model that assigns new test samples to one category or the other , making it a non-probabilistic binary linear classifier . Reference [ Wikipedia
329	Linear SVR is a similar to SVM method . Its also builds on kernel functions but is appropriate for unsupervised learning . Reference [ Wikipedia
330	Stochastic gradient descent ( often abbreviated SGD ) is an iterative method for optimizing an objective function with suitable smoothness properties ( e.g . differentiable or subdifferentiable ) . It can be regarded as a stochastic approximation of gradient descent optimization , since it replaces the actual gradient ( calculated from the entire data set ) by an estimate thereof ( calculated from a randomly selected subset of the data ) . Especially in big data applications this reduces the computational burden , achieving faster iterations in trade for a slightly lower convergence rate . Reference [ Wikipedia
331	This model uses a Decision Tree as a predictive model which maps features ( tree branches ) to conclusions about the target value ( tree leaves ) . Tree models where the target variable can take a finite set of values are called classification trees ; in these tree structures , leaves represent class labels and branches represent conjunctions of features that lead to those class labels . Decision trees where the target variable can take continuous values ( typically real numbers ) are called regression trees . Reference [ Wikipedia
332	Random Forest is one of the most popular model . Random forests or random decision forests are an ensemble learning method for classification , regression and other tasks , that operate by constructing a multitude of decision trees ( n_estimators= [ 100 , 300 ] ) at training time and outputting the class that is the mode of the classes ( classification ) or mean prediction ( regression ) of the individual trees . Reference [ Wikipedia
333	XGBoost is an ensemble tree method that apply the principle of boosting weak learners ( CARTs generally ) using the gradient descent architecture . XGBoost improves upon the base Gradient Boosting Machines ( GBM ) framework through systems optimization and algorithmic enhancements . Reference [ Towards Data Science .
334	Light GBM is a fast , distributed , high-performance gradient boosting framework based on decision tree algorithms . It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise . So when growing on the same leaf in Light GBM , the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms . Also , it is surprisingly very fast , hence the word ‘ Light ’ . Reference [ Analytics Vidhya
335	Tikhonov Regularization , colloquially known as Ridge Regression , is the most commonly used regression algorithm to approximate an answer for an equation with no unique solution . This type of problem is very common in machine learning tasks , where the `` best '' solution must be chosen using limited data . If a unique solution exists , algorithm will return the optimal value . However , if multiple solutions exist , it may choose any of them . Reference [ Brilliant.org
336	Bootstrap aggregating , also called Bagging , is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression . It also reduces variance and helps to avoid overfitting . Although it is usually applied to decision tree methods , it can be used with any type of method . Bagging is a special case of the model averaging approach . Bagging leads to `` improvements for unstable procedures '' , which include , for example , artificial neural networks , classification and regression trees , and subset selection in linear regression . On the other hand , it can mildly degrade the performance of stable methods such as K-nearest neighbors . Reference [ Wikipedia
337	ExtraTreesRegressor implements a meta estimator that fits a number of randomized decision trees ( a.k.a . extra-trees ) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting . The default values for the parameters controlling the size of the trees ( e.g . max_depth , min_samples_leaf , etc . ) lead to fully grown and unpruned trees which can potentially be very large on some data sets . To reduce memory consumption , the complexity and size of the trees should be controlled by setting those parameter values . Reference [ sklearn documentation In extremely randomized trees , randomness goes one step further in the way splits are computed . As in random forests , a random subset of candidate features is used , but instead of looking for the most discriminative thresholds , thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule . This usually allows to reduce the variance of the model a bit more , at the expense of a slightly greater increase in bias . Reference [ sklearn documentation
338	The core principle of AdaBoost is to fit a sequence of weak learners ( i.e. , models that are only slightly better than random guessing , such as small decision trees ) on repeatedly modified versions of the data . The predictions from all of them are then combined through a weighted majority vote ( or sum ) to produce the final prediction . The data modifications at each so-called boosting iteration consist of applying N weights to each of the training samples . Initially , those weights are all set to 1/N , so that the first step simply trains a weak learner on the original data . For each successive iteration , the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data . At a given step , those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased , whereas the weights are decreased for those that were predicted correctly . As iterations proceed , examples that are difficult to predict receive ever-increasing influence . Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence . Reference [ sklearn documentation
339	Thanks for the example of ensemling different models from
340	We can now compare our models and to choose the best one for our problem .
341	The input arguments will be y_true , y_pred and the `` calculate_iou '' function we created above .
342	Load the pre processed data
343	Initialize the generators
344	Plot the Loss Curves
345	Make a Prediction
346	Process the Predictions
347	Create the submission csv file
348	What is a python generator
349	How to make a generator run infinitely
350	Thanks to Automatic FE The main code for basic FE
351	Download datasets
352	Selection part of data for automatic FE - 10000 meters and it 's preprocessing
353	Thanks to
354	FS with the Pearson correlation
355	FS with SelectFromModel and LinearSVR
356	FS with SelectFromModel and RandomForestRegressor
357	Imports .
358	Load data files .
359	Make a 'tanh ' function for gplearn . I suspect it needs to be in a top level module to pickle properly in the notebook , so create as a separate file ( gplearn_tanh.py ) and import . Here is code for the import file .
360	Set up the folds for cross validation .
361	Optional sample weighting . I tried to up-weight the samples above 10 seconds because they are uncommon , in the hope the model would fit them better .
362	Genetic program model , main code loop .
363	Get the dupplicate clicks with different target values . Can not simply drop duplicate from 'dup ' because there are some clicks with more than 2 duplicates .
364	Basic skin detection
365	Apply skin segmentation on all training data and visualize the result
366	Take a number of images from all classified images and test images Compute histogram on hue channel Perform 5 classes clustering on the data
367	Some stats using jpg exif
368	Linear Regression is a linear approach to modeling the relationship between a scalar response ( or dependent variable ) and one or more explanatory variables ( or independent variables ) . The case of one explanatory variable is called simple linear regression . For more than one explanatory variable , the process is called multiple linear regression . Reference [ Wikipedia Note the confidence score generated by the model based on our training dataset .
369	Support Vector Machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis . Given a set of training samples , each marked as belonging to one or the other of two categories , an SVM training algorithm builds a model that assigns new test samples to one category or the other , making it a non-probabilistic binary linear classifier . Reference [ Wikipedia
370	Linear SVR is a similar to SVM method . Its also builds on kernel functions but is appropriate for unsupervised learning . Reference [ Wikipedia
371	Stochastic gradient descent ( often abbreviated SGD ) is an iterative method for optimizing an objective function with suitable smoothness properties ( e.g . differentiable or subdifferentiable ) . It can be regarded as a stochastic approximation of gradient descent optimization , since it replaces the actual gradient ( calculated from the entire data set ) by an estimate thereof ( calculated from a randomly selected subset of the data ) . Especially in big data applications this reduces the computational burden , achieving faster iterations in trade for a slightly lower convergence rate . Reference [ Wikipedia
372	This model uses a Decision Tree as a predictive model which maps features ( tree branches ) to conclusions about the target value ( tree leaves ) . Tree models where the target variable can take a finite set of values are called classification trees ; in these tree structures , leaves represent class labels and branches represent conjunctions of features that lead to those class labels . Decision trees where the target variable can take continuous values ( typically real numbers ) are called regression trees . Reference [ Wikipedia
373	Random Forest is one of the most popular model . Random forests or random decision forests are an ensemble learning method for classification , regression and other tasks , that operate by constructing a multitude of decision trees ( n_estimators= [ 100 , 300 ] ) at training time and outputting the class that is the mode of the classes ( classification ) or mean prediction ( regression ) of the individual trees . Reference [ Wikipedia
374	XGBoost is an ensemble tree method that apply the principle of boosting weak learners ( CARTs generally ) using the gradient descent architecture . XGBoost improves upon the base Gradient Boosting Machines ( GBM ) framework through systems optimization and algorithmic enhancements . Reference [ Towards Data Science .
375	Light GBM is a fast , distributed , high-performance gradient boosting framework based on decision tree algorithms . It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise . So when growing on the same leaf in Light GBM , the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms . Also , it is surprisingly very fast , hence the word ‘ Light ’ . Reference [ Analytics Vidhya
376	Tikhonov Regularization , colloquially known as Ridge Regression , is the most commonly used regression algorithm to approximate an answer for an equation with no unique solution . This type of problem is very common in machine learning tasks , where the `` best '' solution must be chosen using limited data . If a unique solution exists , algorithm will return the optimal value . However , if multiple solutions exist , it may choose any of them . Reference [ Brilliant.org
377	Bootstrap aggregating , also called Bagging , is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression . It also reduces variance and helps to avoid overfitting . Although it is usually applied to decision tree methods , it can be used with any type of method . Bagging is a special case of the model averaging approach . Bagging leads to `` improvements for unstable procedures '' , which include , for example , artificial neural networks , classification and regression trees , and subset selection in linear regression . On the other hand , it can mildly degrade the performance of stable methods such as K-nearest neighbors . Reference [ Wikipedia
378	ExtraTreesRegressor implements a meta estimator that fits a number of randomized decision trees ( a.k.a . extra-trees ) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting . The default values for the parameters controlling the size of the trees ( e.g . max_depth , min_samples_leaf , etc . ) lead to fully grown and unpruned trees which can potentially be very large on some data sets . To reduce memory consumption , the complexity and size of the trees should be controlled by setting those parameter values . Reference [ sklearn documentation In extremely randomized trees , randomness goes one step further in the way splits are computed . As in random forests , a random subset of candidate features is used , but instead of looking for the most discriminative thresholds , thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule . This usually allows to reduce the variance of the model a bit more , at the expense of a slightly greater increase in bias . Reference [ sklearn documentation
379	The core principle of AdaBoost is to fit a sequence of weak learners ( i.e. , models that are only slightly better than random guessing , such as small decision trees ) on repeatedly modified versions of the data . The predictions from all of them are then combined through a weighted majority vote ( or sum ) to produce the final prediction . The data modifications at each so-called boosting iteration consist of applying N weights to each of the training samples . Initially , those weights are all set to 1/N , so that the first step simply trains a weak learner on the original data . For each successive iteration , the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data . At a given step , those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased , whereas the weights are decreased for those that were predicted correctly . As iterations proceed , examples that are difficult to predict receive ever-increasing influence . Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence . Reference [ sklearn documentation
380	Thanks for the example of ensemling different models from
381	We can now compare our models and to choose the best one for our problem .
382	Below are the imports needed to run the code . The code has been written and run in Python 3.6 and 3.7 Anaconda environments . Many of these libraries request a citation when used in an academic paper . Note the use of the Scikit-Learn ( Pedregosa et al . ( 2011 ) , XGBoost ( Chen & Guestrin , 2016 ) and LightGBM ( Ke , et al. , 2017 ) libraries for machine learning and support . Numpy is utilized to provide many numerical functions for feature creation ( van der Walt , Colbert & Varoquaux , 2011 ) . Pandas is very helpful for its ability to support data manipulation and feature creation ( McKinney , 2010 ) . SciPy is utilized to provide signal processing functions , especially filtering and for Pearson 's correlation metrics ( Jones E. , et al , 2001 ) . The Jupyter environment in which this project is presented is a descendant of the IPython environment originated by Pérez & Granger ( 2007 ) .
383	Define some constants . The signal constants define how the signal and Fourier transforms will be filtered to produce bandwidth limited features .
384	Filter design helper functions . These were added to allow for obtaining statistics on the signal in a bandwidth limited manner . Butterworth 4 pole IIR filters are utilized to obtain the signal split into frequency bands . EDA showed that most , if not all , of the signal above the 20,000 frequency line was likely to be noise , so the frequency bands will concentrate on the region below that . Note that the signal is 150k lines long , hence by the Nyquist criteria there are 75k valid frequency lines before aliasing .
385	Manager function to call the create features functions in multiple processes .
386	Put the feature creation functions together and create the features . Some of these functions can take a long time to run , so it is recommended that it be done from an IDE and one function at a time . If it fails part way down due to a path name being wrong then it is not necessary to re-run every function .
387	As it is said in data description page , ` TRAIN_DB ` contains a list of 7,069,896 dictionaries , one per product . Each dictionary contains product id ( key : _id the category id of the product ( key : category_id ) , images , stored in a list ( key : imgs ) . Let 's look at the first item
388	So , in train dataset we have products indexed by ` _id ` , belong to a ` category_id ` and described by 1-4 images . Now , let 's quickly take a look to test products
389	Display for example a item with ` _id
390	Let 's inspect categories and their relationship to images . We have unique categories unique level 1 categories unique level 2 categories unique level 3 categories
391	So as it was asked in comments ( by @ microland ) we can observe that there are items with different 'category_id ' but the same 'category_level
392	Level 2 and 3 categories are distributed as follows
393	Now , let 's create training data table ` _id ` , ` category_id
394	Category_count vs Image_count
395	How many different cars in train dataset
396	Search for similar cars that have same year , make , model and trim
397	Are there 'same ' cars in train and test
398	Designed and run in a Python 3 Anaconda environment on a Windows 10 computer .
399	These are the needed library imports for problem setup . Many of these libraries request a citation when used in an academic paper . Numpy is utilized to provide many numerical functions needed in the EDA ( van der Walt , Colbert & Varoquaux , 2011 ) . Pandas is very helpful for its ability to support data manipulation ( McKinney , 2010 ) . SciPy is utilized to provide signal processing functions ( Jones E. , et al , 2001 ) . Matplotlib is used for plotting ( Hunter , 2007 ) . The Jupyter environment in which this code is presented and was run is a descendant of the IPython environment originated by Pérez & Granger ( 2007 ) .
400	Define some constants for data location .
401	Code from here and below is commented out because the kernel dies
402	Lets validate the test files . This verifies that they all contain 150,000 samples as expected .
403	Find the indices for where the earthquakes occur , then plotting may be performed in the region around failure .
404	Now let 's setup the input data
405	Let 's compare briefly results of transformations on the first image . Results are not perfectly the same , but it is not important for the benchmark
406	stage : 100 images , blur + flip
407	Again let 's compare briefly results of transformations on the first image
408	Now let 's 'export ' whole train dataset in a few images . So that you can download images and explore them using your favorite image viewer .
409	Let 's check duplicates in the training dataset
410	Let 's check duplicates in the test dataset
411	Let 's check duplicates between training and test datasets
412	However as it stated in [ the comment Salt is very difficult to locate accurately , and if faced with ambiguity , a geophysicist will draw the simplest possible polygon to define it . Since salt can play havoc on time to depth conversions ( velocity models ) , you can get some really weird vertical features in the seismic We should carefully select the vertical masks
413	NOTE : Parameter ` masked=True ` slows down prediction generator and causes ` Notebook timeout ` error .
414	Idea is to use clustering on images of one type to group data
415	Test on the data that is not seen by the network during training
416	To give us a more intuitive feeling about the sales behavior , I 'm comparing the sales behavior of the corporation per state . Two interesting insights come out The corporation is n't growing their market share or improving sales force in 2017 . The total sales units have a static behavior all over the year ( for the engineers , the sales are in their steady state Visually one might realize that sales have a very interesing seasonality .
417	Let 's load the features for the training and test datasets ( s2 joblib
418	Good ! We 've got five cluster again . The number of samples in each cluster is the following
419	Using DecisionTree Classifier
420	Confusion Matrix for Train Data Predictions
421	Confusion Matrix for Test Data Predictions
422	Using RandomForest Classifier
423	Confusion Matrix for Train Data Predictions
424	Confusion Matrix for Test Data Predictions
425	Converting the Input images to plot using plt
426	This notebook is highly inspired by this work [ M5 First Public Notebook Under 0.50 ] ( but with [ CatBoost
427	Data load and process functions
428	CatBoost is RAM expensive so I prefer to utilize GPU
429	The details of the step from $ K $ to $ K + 1 $ may be a bit confusing from this implementation : it boils down to the fact that Scargle et al . were able to show that given an optimal configuration of $ K $ points , the $ ( K + 1 ) $ ^th configuration is limited to one of $ K $ possibilities . The function as written above takes a sequence of points , and returns the edges of the optimal bins . We 'll visualize the result on top of the histogram we saw earlier
430	Please use label encoder to encode the new features generated using the above method as the columns coming out of get_new_feature_train function are of type : category . Small snippet on how to use label encoder
431	Data preprocessing
432	Word map for most frequent Tags
433	Bar plot of top 20 tags
434	Splitting into train and test set with 80:20 ratio
435	Featurization of Training Data
436	Fitting Logistic Regression with OneVsRest Classifier
437	Importing Packages and Collecting Data
438	Variable Description and Identification
439	Meter Reading and Meter Type
440	Weekday and Meter Reading
441	Time of Day and Meter Reading
442	Primary Use and Meter Reading
443	There are considerable differences between building types as to when meter readings are highest . Almost all the building peak in the end of the year due to winter season . The trend holds for most of the different building types , with a few notable exceptions ; Manufacturing dips during that peak period outlined above , while Services , Technology , Utility and Warehouse remained fairly constant over the year .
444	We can see that Utility and Healthcare places tend to have the highest readings , while Religious Worship places the least - they ’ re no doubt frequented less often than the higher energy users .
445	Meter Readings over time
446	Meter Readings over time And Primary Use
447	Correlation between meter_reading And Numeric Variable
448	Square feet size is positively Skewed .
449	Year Built
450	Air Temperature
451	Dew Temperature
452	Wind Speed
453	Imputing Missing variable
454	Encoding Categorical Variable
455	Prediction and Submission
456	Variable Description , Identification , and Correction
457	Intersection ID
458	Intersection ID
459	Encoding Street Names
460	Encoding Cordinal Direction
461	Encoding City + Temperature ( °F ) + Rainfall ( inches
462	standardizing of lat-long
463	Seting X and Y
464	Data Section 1 - The Basics
465	Data Section 2 - Team Box Scores
466	Helper functions
467	TIMER + CONFUSION MATRIX
468	Loading the data
469	Submission
470	Librairies and data
471	MERGE , MISSING VALUE , FILL NA
472	Bayesian Optimisation
473	Loading the data
474	We now train the model with a K80 GPU available in Kaggle . Xgboost provides out of the box support for single GPU training . On a local workstation , a GPU-ready xgboost docker image can be obtained from All we need to change is to set : ` TREE_METHOD = 'gpu_hist
475	Submission
476	MERGE , MISSING VALUE , FILL NA
477	The next step is to build and re-install lightGBM with GPU support .
478	Loading the data
479	Submission
480	The kernel is going to fail as I am using python 2.7 with pandas 0.19 installed . To install lightgbm in python , one can follow the steps in this link
481	As feature engineering is completed with the above step and we also have our validation and train dataset , we can model our data using LightGBM . Before moving to LightGBM , we need to understand how other boosting models work and why LGBM is a good boosting model to start with . I will be starting with few questions discussing about gradient boosting and XGBoost . What is Gradient Boosting It is a boosting algorithm in which the loss is minimised using Gradient Descent method What is XGBoost XGBoost is a regularised boosting model and hence reduces overfitting when compared to other boosting algorithms . It also implements parallel processing and is faster compared to other boosting algorithms Now that we have a basic understanding about Gradient Boosting Models and XGBoost , let 's move on to LightGBM . What is LightGBM ? Why did I use LGBM instead of other boosting algorithms ( Ex : XGBoost Light GBM is a fast , distributed , high-performance gradient boosting framework . Unlike other boosting algorithms it splits the trees leafwise and not level wise . LGBM runs very fast , hence the word 'light ' . It trains faster ( on larger datasets ) compared to other boosting algorithms like XGBoost . It uses leaf wise splitting instead of level wise splitting . Leaf wise splitting may lead to overfitting . This can be avoided by specifying tree-specific hyper parameters like max depth . In my case , I have used num_leaves hyper-parameter to avoid overfitting . Before moving on to discussing about hyper-parameters in LightGBM , let 's discuss about different types of parameters in a boosting model .. What are the parameters in a boosting model Generally boosting algorithms consists of large number of hyperparameters that are to be tuned to perform better than baseline model . These parameters may tune the trees in the model ( Ex : min_samples_leaf ) or are specific to boosting ( Ex : learning rate ) . Above , we have discussed about the types of parameters in a model . Let 's move on to parameters specific to LightGBM Hyper-parameters to tune in LGBM For best fit and better accuracy num_leaves : Number of leaves to form a complete tree . Either this or max_depth can be set . As setting max_depth leads to limiting the number of leaf nodes in tree which equals to 2^max_depth . One can either set this or max_depth to avoid overfitting . Setting both of the hyper parameters may result in dampening one of them and underfitting the tree . min_data_in_leaf : Minimum number of samples required in a leaf node . Too low a value results in overfitting whereas a very high value may result in underfitting . This value results on size of underlying dataset and needs to be carefully tuned For faster speed Bagging_fraction : Fraction of data to be used in each iteration . Default is 1 . Can use a smaller value ( Typically ranging from 0.8 to 1.0 ) to improve the speed of model and reduce overfitting . Feature_fraction : Fraction of features to be used in each iteration . Default is 1 i.e all features are used . Similar to bagging_fraction , we can use a smaller value to improve the speed of training . Typical values are between 0.8 and 1 . Other useful tuning hyper_parameters learning_rate : learning rate of boosting algorithm . Default is 0.1 . Typical values are from 0.01 to 0.2 and may extend upto 0.3 . Higher the learning rate , faster the algorithm runs . Lower learning rates help the algorithm in generalising well but take a lot more training time . n_estimators : Number of trees to fit . I have n't found many resources on how this parameter behaves with respect to LGBM but what I have observed it is generally results in a higher score . It also generalises the model better with more trees . Training time is directly proportial to number of trees initially and then tends to increase more than linear as it gets harder and harder to increase the accuracy of models . Extra information : If we dig into lgbm code ( and sklearn code for lgbm ( we can see that n_estimators is same as num_boost_round which is equivalent to any on of these `` num_iterations '' , `` num_iteration '' , `` num_tree '' , `` num_trees '' , `` num_round '' , `` num_rounds '' . max_bin : maximum number of buckets used . Higher values results in better accuracy whereas lower value results in faster computation . Other parameters such as objective , metric and boosting are specific to each data set . In our case , metric is going to be auc . Objective ' : 'binary ' refers to binary classification , 'Boosting ' : 'gbdt ' ( Gradient Boosted Decision Trees ) refers to the boosting type we are using , 'Verbose ' refers to the level of details we want to be printed and 'metric ' : auc refers to our evaluation metric on our validation set . Please note that , I am using 500 rounds and as such , this will take anywhere from 30 minutes to 2 hours ( approximately ) to run depending on your system . I would like to recieve feedback on this kernel . Please correct me if I have wrongly interpreted anything that I have posted .
482	Tensorboard
483	Create Document Vectors
484	The same vectorizer can be used on documents that contain words not included in the vocabulary . These words are ignored and no count is given in the resulting vector .
485	Document Vectors with TfidfVectorizer
486	Document Vectors with HashingVectorizer
487	Bag-of-Words Model in Keras
488	Document Vectors with hashing trick
489	Keras Tokenizer API
490	The first layer in the network must define the number of inputs to expect . For a Multilayer Perceptron model this is specified by `` input_dim '' attribute .
491	Step 2 . Compile Network
492	Defining Input
493	Connecting Layers
494	Creating the Model
495	Importing The Dataset
496	Function for find out Numerical and categeical Variables
497	identifying the missing value in bureau_balance
498	Types Of Features
499	Analysis based Averages values
500	Checking the Correlation Between The Features for Application Train Dataset
501	If look at the above given plot is clear that all AVG featuers are high correleted values by seeing this plot we can easy find out the coorelated features
502	Merging the bureau dataset along with application train dataset to do more analysis
503	analyzing the numerical features disturbion in previous application dataset
504	Paths to data and metadata
505	Group signals metadata accroding to target
506	Load some target 1 ( fault ) signals and visualize
507	Apply reduction on some samples and visualize the results
508	Next I collect the constants . You 'll need to replace the various file name reference constants with a path to your corresponding folder structure .
509	Zone 1 ? Really Looking at this distribution I 'm left to wonder whether the stage 2 data might be substantially different , and certainly one might think real world data would be different . Zone 1 , the right arm , seems unlikely to be the population 's most likely zone to hide contraband . To begin with , you would have to put your contraband in place with your left hand . So unless most airline passengers that carry contraband are left handed , I am guessing that the real world ( and perhaps stage 2 ) will look different . Its just a guess of course ... Anyway , I am convinced it will be important to figure out how to get as many examples by threat zone as possible . In any event , it will also be handy to easily get a list of zones and probabilities from the labels file , so I added this in here . Note that the subject has contraband in zone 14 ( left leg ) . We 'll keep an eye out for that
510	Digging into the Images When we get to running a pipeline , we will want to pull the scans out one at a time so that they can be processed . So here 's a function to return the nth image . In the unit test , I added a histogram of the values in the image .
511	Rescaling the Image Most image preprocessing functions want the image as grayscale . So here 's a function that rescales to the normal grayscale range .
512	Spreading the Spectrum From the histogram , you can see that most pixels are found between a value of 0 and and about 25 . The entire range of grayscale values in the scan is less than ~125 . You can also see a fair amount of ghosting or noise around the core image . Maybe the millimeter wave technology scatters some noise ? Not sure ... Anyway , if someone knows what this is caused by , drop a note in the coments . That said , let 's see what we can do to clean the image up . In the following function , I first threshold the background . I 've played quite a bit with the threshmin setting ( 12 has worked best so far ) , but this is obviously a parameter to play with . Next we equalize the distribution of the grayscale spectrum in this image . See this tutorial if you want to learn more about this technique . But in the main , it redistributes pixel values to a the full grayscale spectrum in order to increase contrast .
513	Masking the Region of Interest Using the slice lists from above , getting a set of masked images for a given threat zone is straight forward . The same note applies here as in the 4x4 visualization above , I used a cv2.resize to get around a size constraint ( see above ) , therefore the images are quite blurry at this resolution . Note that the blurriness only applies the unit test and visualization . The data returned by this function is at full resolution .
514	Cropping the Images Using the crop lists from above , getting a set of cropped images for a given threat zone is also straight forward . The same note applies here as in the 4x4 visualization above , I used a cv2.resize to get around a size constraint ( see above ) , therefore the images are quite blurry at this resolution . If you do not face this size constraint , drop the resize . Note that the blurriness only applies the unit test and visualization . The data returned by this function is at full resolution .
515	Normalize and Zero Center With the data cropped , we can normalize and zero center . Note that more work needs to be done to confirm a reasonable pixel mean for the zero center .
516	Step 1 - Handle NANs and missing values Manage NANs for columns totals.transactionRevenue , trafficSource.isTrueDirect , totals.newVisits , totals.bounces , trafficSource.adwordsClickInfo.isVideoAd ( not sure abiout this , NAN may be = TRUE ) , trafficSource.isTrueDirect , etc .
517	Step 2 - Visualization LOG of the transaction revenue sums by continent , subcontinent , & operatingSystem , with a hue of deviceCategory . Upon analysis , trafficSource.adwordsClickInfo.isVideoAd ' has no impact on transaction values neither does device.deviceCategory even though its varied . device.operatingSystem ' seems imbalance not to mention additional Operating systems like 'Tizen ' 'Playstation Vita ' 'OS/2 ' , different values between the test and train . Country , Subcontinent and continent seeme to give a better picture of revenue distribution .
518	Accuracy that could be achieved by always predicting the most frequent class . This means that a dumb model that always predicts 0/1 would be right `` null_accuracy '' % of the time .
519	Classification Accuracy
520	Logarithmic Loss / Log Loss / Logistic Loss / Cross-Entropy Loss
521	Interpreting ROC Plot
522	Classification Report
523	Classifier uses a threshold equal to 0 , so the previous code returns the same result as the predict ( ) method ( i.e. , True ) . Let ’ s raise the threshold
524	Now we can simply select the threshold value that gives us the best precision/recall tradeoff for our task . let ’ s suppose you decide to aim for 80 % recall . You look up the first plot ( zooming in a bit ) and find that you need to use a threshold of about 0.32 . To make predictions ( on the training set for now ) , instead of calling the classifier ’ s predict ( ) method , you can just run this code
525	Because the MSE is squared , its units do not match that of the original output . RMSE is the square root of MSE . Since the MSE and RMSE both square the residual , they are similarly affected by outliers . The RMSE is analogous to the standard deviation and is a measure of how large the residuals are spread out . Generally , RMSE will be higher than or equal to MAE . Root\ Mean\ Squared\ Error =\sqrt { \frac { 1 } { N } \sum_ { i=1 } ^ { N } ( y_ { i } - \hat { y_ { i
526	Adjusted R-Squared
527	Load data and fit some models
528	Bounded region of parameter space
529	x_val = np.reshape ( x_val , ( x_val.shape [ 0 ] , x_val.shape [ 1 ] , x_val.shape [ 2 ] ,
530	Lets Read In Data Files
531	When Do People Generally Order
532	At What Day Of The Week People Order
533	When Do People Generally Reorder
534	How many orders users generally made
535	I am trying to share some knowledge that I acquired while researching for this competition . Librosa is a really powerful python library that can be used for retreving lot of meaningful information from audio files . I am showing few for those here . Hope you find it helpful .
536	Spectral flux Spectral flux is a measure of how quickly the power spectrum of a signal is changing , calculated by comparing the power spectrum for one frame against the power spectrum from the previous frame .
537	Pitch Pitch is a perceptual property of sounds that allows their ordering on a frequency-related scale , or more commonly , pitch is the quality that makes it possible to judge sounds as `` higher '' and `` lower '' in the sense associated with musical melodies . Pitch can be determined only in sounds that have a frequency that is clear and stable enough to distinguish from noise . Pitch is a major auditory attribute of musical tones , along with duration , loudness , and timbre . reference
538	Visuallizing Interest Level Vs Bathroom
539	Visualizing Interest Level Vs Bedrooms
540	Correlation Between Price and Other Features
541	Main Config Variables
542	Create final submission DF
543	Global Imports
544	Visualizing Datatypes
545	Correlation Analysis
546	No Of Storey Over The Years
547	Bedroom Count Vs Log Error
548	Bathroom Count Vs Log Error
549	Room Count Vs Log Error
550	No Of Storeys Vs Log Error
551	Gaussian Noise on Target
552	Composition of Augmentations
553	Loading and preprocessing data
554	Preprocessing of features
555	Real Do n't forget about _Scalling
556	Text Processing of text data easily
557	Parameters
558	Let us read the masks
559	and keep only those that contain ships . Keep in mind that image files can be repeated many times in the csv file . So a unique operator will give us the unique filenames that contain ships .
560	The final touch .. I export these bounding boxes for everyone to use in a Pandas dataframe form .
561	Let us load one image and its masks
562	Now we can read the masks for the specific image . We have stored them as png files ` .
563	For the same window we superimpose the masks above the image .
564	So we can see the out-of-fold metric scores around 0.65 . We expect as submission that scores at least 0.45 on the public leaderboard .
565	Get predictions
566	Prediction on test set
567	IMPORTANT : While the time series appears continuous , the data is from discrete batches of 50 seconds long 10 kHz samples ( 500,000 rows per batch ) . In other words , the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000 , and thus discontinuous between 50.0000 and 50.0001 .
568	using VarianceThreshold to remove all low-variance features
569	We define the default preprocessing for resnet architectures and create train and validation generators ( ` keras.utils.Sequence
570	Importing libs
571	Loading data
572	Data cleaning Let 's do a data cleaning based in this [ amazing notebook
573	Active Case = confirmed - deaths - recovered
574	Replacing Mainland china with just China
575	Worldwide scenario
576	Now , let 's take a look at Brazil
577	Let 's have a look at China
578	Now a look at Italy
579	Comparison between Brazil and Italy
580	China scenario since first entry
581	Spain since first recorded case
582	Iran since first case
583	USA since first case
584	Getting population for each country
585	I 'll assume that the whole Brazil 's population is suscetible . So I can define the following initial conditions
586	Select the models to run setting bool variables below
587	Setting fitting domain ( given time for each observation ) and the observations ( observed population at given time
588	To calibrate the model , we define an objective function , which is a Least-Squares function in the present case , and minimize it . To ( try to ) avoid local minima , we use Differential Evolution ( DE ) method ( see this [ nice presentation ] ( to get yourself introduced to this great subject ) . In summary , DE is a family of Evolutionary Algorithms that aims to solve Global Optimization problems . Moreover , DE is derivative-free and population-based method . Below , calibration is performed for selected models
589	Calculating the day when the number of infected individuals is max
590	Lets gets started
591	Full data Analysis
592	Seperating the data into different data frame based on the labels
593	most important or common positive words
594	Most important or common negative words
595	Most important or common words in neutral data
596	We assume the next things Imbalance of positive and negative classes in the test set is the same as in the training set . Our model outputs uniformly distributed probabilities . Public and private leaderboard scores are independent from each other .
597	Appoximately 96.4 % of the data are negative class labels . Let 's create perfect submission ( oh , here we can ! ) and target vector that correspond to the test data size .
598	Almost the same imbalance we have in the training set . Now let 's define Normalized Gini Score . [ Sklearn ] ( package has [ roc_auc_score ] ( and we are going to compute Normalized Gini with AUC using the next formula begin { align Gini & = 2 AUC end { align Let 's see how our perfect submission is evaluated by Gini . Hold your breath
599	Amazing ! This is the highest score we can get from Gini and probably will never see on the leaderboard . Let 's also define completely random submission and test it .
600	Almost zero . That 's what we expect from Gini when we have no idea about the data and use guessing . We know that the public leaderboard is calculated on approximately 30 % of the test data . So we should create private and public sets . Our generated answers are randomly distributed and we can just subset first 30 % for public dataset and the rest 70 % for private dataset . We also create function that evaluates public and private score .
601	The chart showing dependence between number of spoiled samples and Gini is the following
602	Let 's plot the distribution of difference between public score and private score .
603	Public score seems to be usually higher than a private score ( slightly left-skewed distribution ) . Let 's see inverse cumulative plot of absolute difference .
604	It says the absolute difference more than 0.005 appears in every third case . It is a lot But this has nothing to do with guessing . It just tells us we should expect difference in public-private outcome . I manually found the submission where the public Gini score is close to 0.286 . It required predicting more that 170000 samples like in the perfect submission .
605	We should steadily add correct samples to our submission286 in order to get 0.287 . Sounds easy , does n't it We will steadily increase number of correct answers from 5 to infinity with step 5 and give 30 tries for each addition because our correct samples are selected in a random order . We will stop when we reach 0.287 .
606	Import Packages and Functions
607	Load and Explore Data
608	Data Prep — Tokenize and Pad Text Data
609	Create the embedding layer
610	Build the Model
611	Let 's load the data and the embeddings ...
612	Let 's define our training and model parameters
613	Looking at training and validation loss / accuracy figures below , we can see there is no sign of over-fitting .
614	Loading data
615	Imputing missing values
616	Model Functions
617	Random Forest Regressor
618	K-Nearest Neighbors Regressor
619	Linear Regression
620	Linear Model ( Lasso
621	Ridge Regression
622	Feature Agglomeration Results
623	Variance Threshold
624	Predict on test set
625	model_train.get_feature_importance ( ) на одном из этапов моделирования ) . Конечное количество ( 31 посление признаки в спике ) было определено нахождением f1 на валидационной выборке ( обучающий датасет делился на train/val ) последовательным отбросом признаков ( начиная с последнего в списке ) .
626	Bookings per day of week
627	Bookings by year
628	Bookings by month
629	Interactive booking , click , and percentage of booking trends with Bokeh
630	Building interactive charts for hotel clusters , 5 clusters per chart
631	expanding the aggregate
632	a quick check if demand distribution changes week to week
633	Read the csv files from kaggle .
634	Read the csv files on the Johns Hopkins CSSE database on github .
635	Transpose the dataframes
636	Join all dataframes .
637	Let 's now model the temporal evolution of daily new cases for all countries . The approach is the following we first create an order of countries where the epidemic appears ( defined as when 10 days are discovered for the first country ( China ) , a simple auto-regressive model is used for other countries , we model the evolution of daily new cases by also taking into account the impact of total cases in countries where the epidemic appears first the impact of the epidemic of other countries is weighted by their distance between countries and the yearly number of flight passengers to include the `` interaction '' between two countries other variables , like lockdown starting day , will be used . For now , a simple linear regression model is used to model 1 ) directly the total number of confirmed cases , 2 ) the number daily new cases , i.e . confirmed.diff ( ) since this variable should be more stationary and therefore more easily modelled , 3 ) the number of daily new cases averaged within a year ( with rolling ( 7 ) .mean ( ) ) to smooth the data since some data are not updated every day .
638	Baseline Format @ Cam Askew Previous winning solutions from @ Sanyam Bhutani Submission Guidleines by Andre Araujo
639	Prepare the Data for Training Preparing the data for training consists of creating TFRecord files from the raw GLDv2 images grouped into TRAIN and VALIDATION splits . The training set produced contains only the clean subset of the GLDv2 dataset . The CVPR'20 paper introducing the GLDv2 dataset contains a detailed description of the clean subset . For testing purpose , I am using a subset of the cleaned data . In order to use the full dataset , use train_clean_csv_path= .. /input/cleaned-subsets-of-google-landmarks-v2/GLDv2_train_cleaned.csv
640	Shuffle part of predictions to simulate a non-overfit model with realistic accuracy
641	Assuming that you have already finished your feature engineering and you have two dataset train_clean.csv test_clean.csv In train_clean.csv , there 's an 'outlier ' column with values 1/0 . Besides , you have your best LB submission csv ( thanks Ashish Patel ( 阿希什 ) My original model ca n't rich this score , so I try to use the idea to improve your submission to get better LB socre . The flows of this pipline is as follows Training a model using a training set without outliers . ( we get : Model Training a model to classify outliers . ( we get : Model Using Model_2 to predict whether an card_id in test set is an outliers . ( we get : Outlier_Likelyhood Spliting out the card_id from Outlier_Likelyhood with top 10 % ( or some other ratio ) score . ( we get : Outlier_ID Combining your submission using your best submission ( that is , your best model ) to predict Outlier_ID in test set and using Model_1 to predict the rest of the test set . The basic idea behind this pipline is Training model without outliers make the model more accurate for non-outliers . A great proportion of the error is caused by outliers , so we need to use a model training with outliers to predict them . How to find them out ? build a classifier
642	filtering out outliers
643	using outliers column as labels instead of target column
644	Check Unique Label
645	unicode_translation.csvには存在するがtrain.csvに存在しないラベルが569個ある
646	The string should be read as space separated series of values where Unicode character , X , Y , Width , and Height are repeated as many times as necessary .
647	Non physical data augmentation
648	input_layer = Input ( ( img_size_target , img_size_target , output_layer = build_model ( input_layer , 16,0. model1 = Model ( input_layer , output_layer c = optimizers.adam ( lr = 0. model1.compile ( loss= '' binary_crossentropy '' , optimizer=c , metrics= [ my_iou_metric model1.summary
649	Check if valid data looks all right
650	missing value statistics
651	cate0特征下的异常值 processing exception value in cate
652	y的异常值 drop samples which have exception value in y
653	Over-fitting ? Try to use less features or use more samples to fit modle .
654	y hist with defferent timestamps are similar .
655	Save objects for next step
656	Import necessary libraries
657	Let 's load the data and take some observations
658	Let 's plot now the train data ( all the data ) using a heatmap
659	Take a deeper look on different target samples , I will plot separate heatmap for target values 0 and 1 .
660	First , consider these figures as categorical features . I will plot some simple bar charts using seaborn 's countplot
661	High cardinality features
662	Ordinal features mapping
663	I tried to encode cyclic feature with trigonometric functions and expect this technique can be useful to understand and extract insights from samples based upon the patterns and behaviors of the data points over a specific time period
664	One-hot encoding is a process of binarizing the categorical variable . This is done by transforming a categorical variable with n unique values into n unique columns in the datasets while keeping the number of rows the same
665	This produces output as a pandas dataframe.Alternatively we can use OneHotEncoder ( ) method available in sklearn to convert out data to on-hot encoded data . But this method produces a sparse metrix.The advantage of this methos is that is uses very less memory/cpu resourses , if one try to encode all of these features with sklearn 's OneHotEncoder ( ) , it pottentially cause a crash session
666	Combine these encoded dataframe and notice that I 've added a term ( retain_full $ ^2 $ ) which will help me to archive higher score ( but also reduce the training speed
667	Predict test set and make submission
668	now let 's explore a little on the labels
669	Explore the data a little : what are the most common ingredients
670	Remove Extreme Prices
671	Also , about 6 % of the ads have price tage above 1 mllion & 8381 ; ( ~ 16,000 USD ) . These are mosly real estate and cars with high price variance . We will ignore these ads as well . Here is the top 10 category distribution for these items and their percentage among all listed ads .
672	And then let 's see how the price ( $ Log ( price ) $ ) varies within the ` parent_category_name ` in the violin plot below . Again , the variation looks relatively high for almost all the parent categories .
673	For an objective measure , let 's calculate the [ coefficient of variation ( CV ) ] [ cv ] for our 47 categories . A CV value greater than 1 suggests high relative variability in our variable . As you can see blow , almost all categories have CV value greater than 1. cv
674	Use Ad Image to Identify Item Category
675	Price Variance Within Identified Items
676	Import ` trackml-library The easiest and best way to load the data is with the [ trackml-library ] that was built for this purpose . Under your kernel 's Settings tab - > Add a custom package - > GitHub user/repo ( LAL/trackml-library Restart your kernel s session and you will be good to go . trackml-library
677	Affected Surface Object
678	Let 's now take a look at the relationship between different pair combinations of the particle variables . Again , the colors represent the number of hits . There is no large skew in the distribution of the number of hits over other variables . It looks like the particles are targetted towards the global origin $ ( x , y ) = ( 0,0 ) $ and are evenly distributed around it .
679	Due to Kaggle 's disk space restrictions , we will only extract a few images to classify here . Keep in mind that the pretrained models take almost 650 MB disk space .
680	ResNet50 vs InceptionV3 vs Xception
681	The dataset is made of one traning ( ` train.csv ` ) and one test ( ` test.csv ` ) files . Here is what the training data looks like
682	In fact , this is what ~ 2 % of the columns look like . The tranining ( and test ) data set has a massive 4,990 numeric features + ID and target columns . The ` ID ` column is the bank customer 's identification value . The test data is about 10 times the size of the training data . Here is their shape
683	The test set data features have a higher low mean value ( left plot ) , and higher low variance ( right plot ) .
684	All Zero Features
685	We know from the competition description that the target feature is a transaction value . Therefore , it 's distribution should not be much different from the rest of the features , asuming they represent bank transactions as well . The max feature transaction showed earlier may make us question that assumption though . Here is what the target feature distribution looks like .
686	Show Original Image
687	Start by pivoting the DataFrame to explore the label distribution over slices
688	As a Neuroradiologist , this distribution looks pretty true to daily practice .
689	Study Instance UID ` , ` Series Instance UID ` and ` Patient ID ` will be helpful in organizing our data later . Rows ` and ` Columns ` give us the image resolution ( 512 x 512 in this case Window Center ` and ` Window Width ` tell us the window settings applied to the image at acquisition Rescale Intercept ` and ` Rescale Slope ` tell us how to rescale the pixel values to match the standard Hounsfield Unit ( HU ) scale
690	Unique patients , studies & series
691	Demonstration how it works
692	Combinations of TTA
693	We 'll use a familiar stack of data science libraries : ` Pandas ` , ` numpy ` , ` matplotlib ` , ` seaborn ` , and eventually ` sklearn ` for modeling .
694	Read in Data and Look at Summary Information
695	Let 's look at the distribution of unique values in the integer columns . For each column , we 'll count the number of unique values and show the result in a bar plot .
696	The ` Id ` and ` idhogar ` object types make sense because these are identifying variables . However , the other columns seem to be a mix of strings and numbers which we 'll need to address before doing any machine learning . According to the documentation for these columns dependency ` : Dependency rate , calculated = ( number of members of the household younger than 19 or older than 64 ) / ( number of member of household between 19 and edjefe ` : years of education of male head of household , based on the interaction of escolari ( years of education ) , head of household and gender , yes=1 and no edjefa ` : years of education of female head of household , based on the interaction of escolari ( years of education ) , head of household and gender , yes=1 and no These explanations clear up the issue . For these three variables , __ '' yes '' = 1__ and __ '' no '' = 0__ . We can correct the variables using a mapping and convert to floats .
697	Addressing Wrong Labels
698	Families without Heads of Household
699	Well that 's a relief ! This means that we do n't have to worry about a household both where there is no head __AND__ the members have different values of the label ! For this problem , according to the organizers , __if a household does not have a head , then there is no true label . Therefore , we actually wo n't use any of the households without a head for training__ Nonetheless , it 's still a good exercise to go through this process of investigating the data Correct Errors Now we can correct labels for the households that do have a head __AND__ the members have different poverty levels .
700	One of the most important steps of exploratory data analysis is finding missing values in the data and determining how to handle them . Missing values have to be filled in before we use a machine learning model and we need to think of the best strategy for filling them in based on the feature : this is where we 'll have to start digging into the data definitions . First we can look at the percentage of missing values in each column .
701	We do n't have to worry about the ` Target ` becuase we made that ` NaN ` for the test data . However , we do need to address the other 3 columns with a high percentage of missing values . v18q1__ : Number of tablets Let 's start with ` v18q1 ` which indicates the number of tablets owned by a family . We can look at the value counts of this variable . Since this is a household variable , it only makes sense to look at it on a household level , so we 'll only select the rows for the head of household . Function to Plot Value Counts Since we might want to plot value counts for different columns , we can write a simple function that will do it for us
702	The meaning of the home ownership variables is below tipovivi1 , =1 own and fully paid house tipovivi2 , `` =1 own , paying in installments tipovivi3 , =1 rented tipovivi4 , =1 precarious tipovivi5 , `` =1 other ( assigned , borrowed We 've solved the issue ! Well , mostly : the households that do not have a monthly rent payment generally own their own home . In a few other situations , we are not sure of the reason for the missing information . For the houses that are owned and have a missing monthly rent payment , we can set the value of the rent payment to zero . For the other homes , we can leave the missing values to be imputed but we 'll add a flag ( Boolean ) column indicating that these households had missing values .
703	If we read through some of the [ discussions for this competition ] ( we learn that this variable is only defined for individuals between 7 and 19 . Anyone younger or older than this range presumably has no years behind and therefore the value should be set to 0 . For this variable , if the individual is over 19 and they have a missing value , or if they are younger than 7 and have a missing value we can set it to zero . For anyone else , we 'll leave the value to be imputed and add a boolean flag .
704	Let 's make sure we covered all of the variables and did n't repeat any .
705	These are pretty simple : they will be kept as is in the data since we need them for identification . Household Level Variables First let 's subset to the heads of household and then to the household level variables .
706	Redundant Household Variables
707	The final redundant column is ` area2 ` . This means the house is in a rural zone , but it 's redundant because we have a column indicating if the house is in a urban zone . Therefore , we can drop this column .
708	Creating Ordinal Variables
709	In addition to mapping variables to ordinal features , we can also create entirely new features from the existing data , known as feature construction . For example , we can add up the previous three features we just created to get an overall measure of the quality of the house 's structure .
710	The next variable will be a ` warning ` about the quality of the house . It will be a negative value , with -1 point each for no toilet , electricity , floor , water service , and ceiling .
711	We can keep using our ` plot_categoricals ` function to visualize these relationships , but ` seaborn ` also has a number of plotting options that can work with categoricals . One is the ` violinplot ` which shows the distribution of a variable on the y axis with the width of each plot showing the number of observations in that category .
712	The final household feature we can make for now is a ` bonus ` where a family gets a point for having a refrigerator , computer , tablet , or television .
713	Per Capita Features
714	The Spearman correlation is often considered to be better for ordinal variables such as the Target or the years of education . Most relationshisp in the real world are n't linear , and although the Pearson correlation can be an approximation of how related two variables are , it 's inexact and not the best method of comparison .
715	In most cases , the values are very similar .
716	First , we 'll calculate the Pearson correlation of every variable with the Target .
717	The Spearman correlation coefficient calculation also comes with a ` pvalue ` indicating the significance level of the relationship . Any ` pvalue ` less than 0.05 is genearally regarded as significant , although since we are doing multiple comparisons , we want to divide the p-value by the number of comparisons , a process known as the Bonferroni correction .
718	For the most part , the two methods of calculating correlations are in agreement . Just out of curiousity , we can look for the values that are furthest apart .
719	One of my favorite plots is the correlation heatmap because it shows a ton of info in one image . For the heatmap , we 'll pick 7 variables and show the correlations between themselves and with the target .
720	Redundant Individual Variables
721	Higher levels of education seem to correspond to less extreme levels of poverty . We do need to keep in mind this is on an individual level though and we eventually will have to aggregate this data at the household level .
722	We can make a few features using the existing data . For example , we can divide the years of schooling by the age .
723	We can also take our new variable , ` inst ` , and divide this by the age . The final variable we 'll name ` tech ` : this represents the combination of tablet and mobile phones .
724	Feature Engineering through Aggregations
725	With just that one line , we go from 30 features to 180 . Next we can rename the columns to make it easier to keep track .
726	As a first round of feature selection , we can remove one out of every pair of variables with a correlation greater than 0.95 .
727	We 'll drop the columns and then merge with the ` heads ` data to create a final dataframe .
728	We can also look at the difference in average education by whether or not the family has a female head of household .
729	Machine Learning Modeling
730	Because we are going to be comparing different models , we want to scale the features ( limit the range of each column to between 0 and 1 ) . For many ensemble models this is not necessary , but when we use models that depend on a distance metric , such as KNearest Neighbors or the Support Vector Machine , feature scaling is an absolute necessity . When comparing different models , it 's always safest to scale the features . We also impute the missing values with the median of the feature . For imputing missing values and scaling the features in one step , we can make a pipeline . This will be fit on the training data and used to transform the training and testing data .
731	The data has no missing values and is scaled between zero and one . This means it can be directly used in any Scikit-Learn model .
732	With a tree-based model , we can look at the feature importances which show a relative ranking of the usefulness of features in the model . These represent the sum of the reduction in impurity at nodes that used the variable for splitting , but we do n't have to pay much attention to the absolute value . Instead we 'll focus on relative scores . If we want to view the feature importances , we 'll have to train a model on the whole training set . Cross validation does not return the feature importances .
733	Now that we have a good set of features , it 's time to get into the modeling . We already tried one basic model , the Random Forest Classifier which delivered a best macro F1 of 0.35 . However , in machine learning , there is no way to know ahead of time which model will work best for a given dataset . The following plot shows that __there are some problems where even Gaussian Naive Bayes will outperform a gradient boosting machine__ . This is from [ an excellent paper by Randal Olson that discusses many points of machine learning algorithm_comparison What this plot tells us is that we have to try out a number of different models to see which is optimal . Most people eventually settle on the __gradient boosting machine__ and we will try that out , but for now we 'll take a look at some of the other options . There are literally dozens ( maybe hundreds ) of multi-class machine learning models if we look at the [ Scikit-Learn documentation ] ( We do n't have to try them all , but we should sample from the options . What we want to do is write a function that can evaluate a model . This will be pretty simple since we already wrote most of the code . In addition to the Random Forest Classifier , we 'll try eight other Scikit-Learn models . Luckily , this dataset is relatively small and we can rapidly iterate through the models . We will make a dataframe to hold the results and the function will add a row to the dataframe for each model .
734	That performance is very poor . I do n't think we need to revisit the Gaussian Naive Bayes method ( although there are problems on which it can outperform the Gradient Boosting Machine ) .
735	The multi-layer perceptron ( a deep neural network ) has decent performance . This might be an option if we are able to hyperparameter tune the network . However , the limited amount of data could be an issue with a neural network as these generally require hundreds of thousands of examples to learn effectively .
736	The linear model ( with ridge regularization ) does surprisingly well . This might indicate that a simple model can go a long way in this problem ( although we 'll probably end up using a more powerful method ) .
737	As one more attempt , we 'll consider the ExtraTreesClassifier , a variant on the random forest using ensembles of decision trees as well .
738	Comparing Model Performance
739	The function below takes in a model , a training set , the training labels , and a testing set and performs the following operations Trains the model on the training data using ` fit Makes predictions on the test data using ` predict Creates a ` submission ` dataframe that can be saved and uploaded to the competition
740	Let 's make a submission with the Random Forest .
741	One potential method for improving model performance is feature selection . This is the process where we try to keep only the most useful features for our model . `` Most useful '' can mean many different things , and there are numerous heuristics for selecting the most important features . For feature selection in this notebook , we 'll first remove any columns with greater than 0.95 correlation ( we already did some of this during feature engineering ) and then we 'll apply recursive feature elimination with the Scikit-Learn library . First up are the correlations . 0.95 is an arbitrary threshold - feel free to change the values and see how the performance changes
742	Recursive Feature Elimination with Random Forest
743	We can investigate the object to see the training scores for each iteration . The following code will plot the validation scores versus the number of features for the training .
744	Upgrading Our Model : Gradient Boosting Machine
745	For each fold , the ` 1 , 2 , 3 , 4 ` columns represent the probability for each ` Target ` . The ` Target ` is the maximum of these with the ` confidence ` the probability . We have the predictions for all 5 folds , so we can plot the confidence in each ` Target ` for the different folds .
746	We can have the function instead return the actual submission file . This takes the average predictions across the five folds , in effectm combining 5 different models , each one trained on a slghtly different subset of the data .
747	Results History
748	To resume training , we can pass in the same trials object and increase the max number of iterations . For later use , the trials can be saved as json .
749	For the test predictions , we can only compare the distribution with that found on the training data . If we want to compare predictions to actual answers , we 'll have to split the training data into a separate validation set . We 'll use 1000 examples for testing and then we can do operations like make the confusion matrix because we have the right answer .
750	Here 's how to read the confusion matrix : any of the values on the diagonal , the model got correct because the predicted value matches the true value . Anything not on the diagonal our model got wrong which we can assess by looking at the predicted value versus the actual value . For example , our model correctly predicted 25 observations where the poverty was extreme . On the other hand , for 26 cases where the poverty was extreme , our model predicted it was moderate . For 95 cases where the poverty was Non-Vulnerable , our model predicted the poverty was Vulnerable . Overall , we see that our model is only very accurate at idenifying the non-vulnerable households . To look at the percentage of each true label predicted in each class , we can normalize the confusion matrix for the true labels .
751	As a final exploration of the problem , we can apply a few different dimension reductions methods to the selected data set . These methods can be used for visualization or as a preprocessing method for machine learning . We 'll look at four different methods PCA : Principal Components Analysis ] ( Finds the dimensions of greatest variation in the data ICA : Independent Components Analysis ] ( Attempts to separate a mutltivariate signal into independent signals . TSNE : T-distributed Stochastic Neighbor Embedding ] ( Maps high-dimensional data to a low-dimensional manifold attempting to maintain the local structure within the data . It is a non-linear technique and generally only used for visualization . UMAP : Uniform Manifold Approximation and Projection ] ( A relatively new technique that also maps data to a low-dimensional manifold but tries to preserve more global structure than TSNE . All four of these methods are relatively simple to implement in Python . We 'll map the selected features down to 3 dimensions for visualization and then also use PCA , ICA , and UMAP as features for modeling ( TSNE has no ` transform ` method and hence can not be used for preprocessing ) .
752	As a final step , we can look at one decision tree in the random forest . First we 'll limited the max depth for visability , and then we 'll expand the tree all the way . The first step is simply to train a random forest and extract one tree ( we could also train a single decision tree ) .
753	We take the trained tree and export it as a ` .dot ` file using ` export_graphviz ` .
754	Visualize Tree with No Maximum Depth
755	There are two major formats of bounding boxes pascal_voc , which is [ x_min , y_min , x_max , y_max COCO , which is [ x_min , y_min , width , height We 'll see how to perform image augmentations for both the formats . Let 's first start with pascal_voc format .
756	We 've our image ready , let 's create an array of bounding boxes for all the wheat heads in the above image and the array of labels ( we 've only 2 class here : wheat head and background ) . As all bounding boxes are of same class , labels array will contain only 1 's .
757	Reading Data
758	Lets first check the Train Target Distribution
759	Filling missing and infinite data by zeroes
760	The function below check the Train , Test , and CV Scores of the trained model . Lets Check .
761	We can see that the Classifier is clearly overfitting by getting 96 % in local CV and check the matrix That is one clean and bright diagonal . ( Just if I had that for the Leaderboard
762	Submitting our Predictions
763	Read in 5 million rows and examine data
764	Data Exploration and Data Cleaning
765	For visualization purposes , I 'll create a binned version of the fare . This divides the variable into a number of bins , turning a continuous variable into a discrete , categorical variable .
766	Empirical Cumulative Distribution Function Plot
767	Below is an example of the ecdf . This plot is good for viewing outliers and also the percentiles of a distribution .
768	Based on these values , we can remove outliers . This is somewhat based on intuition and there might be a more accurate process for carrying out this operation ! Here is another potential point for improvement Potential improvement 2 : experiment with different methods to remove outliers.__ This could be through domain knowledge ( such as using a map ) or it could be using more rigorous statistical methods ( such as z-scores ) .
769	Rides on Map of NYC
770	It looks like there are 51,000 rides where the absolute latitude and longitude does not change ! That seems a little strange . This might be a point worth following up Let 's remake the plot above colored by the fare bin .
771	Another plot we can make is the passenger count distribution colored by the fare bin .
772	For the test data , we need to save the ` key ` column for making submissions .
773	No fare information here ! It 's our job to predict the fare for each test ride .
774	The test distribution seems to be similar to the training distribution . As a final step , we can find the correlations between distances and fares .
775	Now that we have built a few potentially useful features , we can use them for machine learning : training an algorithm to predict the target from the features . We 'll start off with a basic model - Linear Regression - only using a few features and then move on to a more complex models and more features . There is reason to believe that for this problem , even a simple linear model will perform well because of the strong linear correlation of the distances with the fare . We generally want to use the simplest - and hence most interpretable - model that is above an accuracy threshold ( dependent on the application ) so if a linear model does the job , there 's no need to use a highly complex ensemble model . It 's a best practice to start out with a simple model for just this reason First Model : Linear Regression The first model we 'll make is a simple linear regression using 3 features : the ` abs_lat_diff ` , ` abs_lon_diff ` , and ` passenger_count ` . This is meant to serve as a baseline for us to beat . It 's good to start with a simple model because it will give you a baseline . Also , if a simple model works well enough , then there may be no need for more complex models . If a linear regression will get the job done , then you do n't need a neural network
776	Create Training and Validation Set
777	Train with Simple Features
778	To make sure that machine learning is even applicable to the task , we should compare these predictions to a naive guess . For a regression task , this can be as simple as the average value of the target in the training data .
779	According to the naive baseline , our machine learning solution is effective ! We are able to reduce the percentage error by about half and generate much better predictions than using no machine learning . This should give us confidence we are on the right track . Make a submission In order to make a submission to Kaggle , we have to make predictions on the test data . Below we make the predictions and save them to a csv file in the format specified by the competition
780	Use More Features
781	Using this one more feature improved our score slightly . Here 's another chance for improvement using the same model Potential Improvement 3 : find an optimal set of features or construct more features__ . This can involve [ feature selection ] ( or trying different combinations of features and evaluating them on the validation data . You can build additional features by looking at others ' work or researching the problem . Collinear Features One thing we do want to be careful about is highly correlated , known as [ collinear ] ( features . These can decrease the generalization performance of the model and lead to less interpretable models . Many of our features are already highly correlated as shown in the heatmap below . This plots the Pearson Correlation Coefficient for each pair of variables .
782	When we want to improve performance , we generally have a few options Get more data - either more observations or more variables Engineer more / better features Perform feature selection to remove irrelevant features Try a more complex model Perform hyperparameter tuning of the selected model We already saw that including another feature could improve perfomance . For now let 's move past the features and focus on the model ( we 'll come back to features later ) . The simple linear regression has no hyperparameters to optimize ( no settings to tune ) so we 'll try approach 4 . If the more complex model does well , we can use it for testing additional features or performing feature selection Non-Linear Model : Random Forest For a first non-linear model , we 'll use the [ Random Forest ] ( regressor . This is a powerful ensemble of regression trees that has good performance and generalization ability because of its low variance . We 'll use most of the default hyperparameters but change the ` n_estimators ` and the ` max_depth ` of each tree in the forest . For the features , we 'll use the four features which delivered good performance in the linear regression .
783	The random forest does much better than the simple linear regression . This indicates that the problem is not completely linear , or at least is not linear in terms of the features we have constructed . From here going forward , we 'll use the same random forest model because of the increased performance . Overfitting Given the gap between the training and the validation score , we can see that our model is __overfitting__ to the training data . This is one of the most common problems in machine learning and is usually addressed either by training with more data , or adjusting the hyperparameters of the model . This leads to another recommendation for improvement Potential Improvement 4 : Try searching for better random forest model hyperparameters__ . You may find Scikit-Learn 's ` RandomizedSearchCV ` a useful tool . I 'll provide some starter code for hyperparameter optimization later in the notebook . Next we can make predictions with the random forest for uploading to the competition .
784	For a reference time , we can use the start of the training data . This means that the ` Elapsed ` measure will be the time since the beginning of the observations .
785	Explore Time Variables
786	There appears to be a minor increase in prices over time which might be expected taking into account inflation . Let 's look at the average fare amount by the hour of day .
787	We can make the same plot by day of the week .
788	Test Time Features
789	For the time features , we 'll use the fractional measurements for the day , week , and year , as well as the time elapsed since the beginning of the records . We 'll keep the other same features as the previous training run . ( This gives us a total of 12 features ) .
790	The random forest does considerably better once we use the time features ! As with the distance features , this should give us confidence that the new predictor variables we built are useful . Just for comparison , we can go back to the linear regression and look at the performance .
791	It seems that the new features helped both the random forest and the linear regression . Let 's take a look at the random forest feature importances .
792	Try with All Time Variables
793	Visualize Validation Predicted Target
794	We 'll use a very limited sample of the data since random search is computationally expensive . Random search uses K Fold cross validation to assess the model which means that for each combination of hyperparameters , we are training and testing the model K times , in this example 3 . This is another option that can be adjusted to determine if performance is affected .
795	Evaluate Best Model from Random Search
796	The best model from random search exhibits less overfitting , but also does not do as well on the validation data . There are probably further benefits from more hyperparameter tuning using additional data .
797	With the background details out of the way , let 's get started with Bayesian optimization applied to automated hyperparameter tuning
798	First we can create a model with the default value of hyperparameters and score it using cross validation with early stopping . Using the ` cv ` LightGBM function requires creating a ` Dataset ` .
799	Now we can evaluate the baseline model on the testing data .
800	We can visualize the learning rate by drawing 10000 samples from the distribution .
801	In Hyperopt , we can use nested conditional statements to indicate hyperparameters that depend on other hyperparameters . For example , the `` goss '' ` boosting_type ` can not use subsampling , so when we set up the ` boosting_type ` categorical variable , we have to set the subsample to 1.0 while for the other boosting types it 's a float between 0.5 and 1.0 .
802	We need to set both the boosting_type and subsample as top-level keys in the parameter dictionary . We can use the Python dict.get method with a default value of 1.0 . This means that if the key is not present in the dictionary , the value returned will be the default ( 1.0 ) .
803	Example of Sampling from the Domain
804	Let 's test the objective function with the domain to make sure it works . ( Every time the ` of_connection ` line is run , the ` outfile ` will be overwritten , so use a different name for each trial to save the results .
805	The optimization algorithm is the method for constructing the surrogate function ( probability model ) and selecting the next set of hyperparameters to evaluate in the objective function . Hyperopt has two choices : random search and Tree Parzen Estimator . The technical details of TPE can be found in [ this article ] ( and a conceptual explanation is in [ this article ] ( Although this is the most technical part of Bayesian hyperparameter optimization , defining the algorithm in Hyperopt is simple .
806	The final part is the history of objective function evaluations . Although Hyperopt internally keeps track of the results for the algorithm to use , if we want to monitor the results and have a saved copy of the search , we need to store the results ourselves . Here , we are using two methods to make sure we capture all the results A ` Trials ` object that stores the dictionary returned from the objective function Adding a line to a csv file every iteration . The csv file option also lets us monitor the results of an on-going experiment . Although do not use Excel to open the file while training is on-going . Instead check the results using ` tail results/out_file.csv ` from bash or open the file in Sublime Text or Notepad .
807	The ` Trials ` object will hold everything returned from the objective function in the ` .results ` attribute . We can use this after the search is complete to inspect the results , but an easier method is to read in the ` csv ` file because that will already be in a dataframe .
808	fmin ` takes the four parts defined above as well as the maximum number of iterations ` max_evals ` .
809	Hyperopt can continue searching where a previous search left off if we pass in a ` Trials ` object that already has results . The algorithms used in Bayesian optimization are black-box optimizers because they have no internal state . All they need is the previous results of objective function evaluations ( the input values and loss ) and they can build up the surrogate function and select the next values to evaluate in the objective function . This means that any search can be continued as long as we have the history in a ` Trials ` object .
810	To save the ` Trials ` object so it can be read in later for more training , we can use the ` json ` format .
811	Learning Rate Distribution
812	We can see that the Bayesian search did worse in cross validation but then found hyperparameter values that did better on the test set ! We will have to see if these results translate to the acutal competition data . First though , we can get all the scores in a dataframe in order to plot them over the course of training .
813	Sure enough , we see that the Bayesian hyperparameter optimization scores increase as the search continues . This shows that more promising values ( at least on the cross validation reduced dataset ) were tried as the search progressed . Random search does record a better score , but the results do not improve over the course of the search . In this case , it looks like if we were to continue searching with Bayesian optimization , we would eventually reach higher scores on the cross vadidation data . For fun , we can make the same plot in Altair .
814	The final plot is just a bar chart of the ` boosting_type ` .
815	The Bayes optimization spent many more iterations using the ` dart ` boosting type than would be expected from a uniform distribution . We can use information such as this in further hyperparameter tuning . For example , we could use the distributions from Bayesian hyperparameter optimization to make a more focused hyperparameter grid for grid or even random search . this chart , we can also make it in Altair for the practice .
816	Applied to Full Dataset
817	Random Search on the Full Dataset
818	Then we can make predictions on the test data . The predictions are saved to a csv file that can be submitted to the competition .
819	Bayesian Optimization on the Full Dataset
820	Standard imports for data science work . The LightGBM library is used for the gradient boosting machine .
821	train_bureau ` is the training features built manually using the ` bureau ` and ` bureau_balance ` data train_previous ` is the training features built manually using the ` previous ` , ` cash ` , ` credit ` , and ` installments ` data We first will see how many features we built over the manual engineering process . Here we use a couple of set operations to find the columns that are only in the ` bureau ` , only in the ` previous ` , and in both dataframes , indicating that there are ` original ` features from the ` application ` dataframe . Here we are working with a small subset of the data in order to not overwhelm the kernel . This code has also been run on the full dataset ( we will take a look at some of the results ) .
822	That gives us the number of features in each dataframe . Now we want to combine the data without creating any duplicate rows .
823	Next we want to one-hot encode the dataframes . This does n't give the full features since we are only working with a sample of the data and this will not create as many columns as one-hot encoding the entire dataset would . Doing this to the full dataset results in 1465 features . An important note in the code cell is where we __align the dataframes by the columns.__ This ensures we have the same columns in the training and testing datasets .
824	Identify Correlated Variables
825	Drop Correlated Variables
826	Let 's drop the columns , one-hot encode the dataframes , and then align the columns of the dataframes .
827	Since the LightGBM model does not need missing values to be imputed , we can directly ` fit ` on the training data . We will use Early Stopping to determine the optimal number of iterations and run the model twice , averaging the feature importances to try and avoid overfitting to a certain set of features .
828	Let 's remove the features that have zero importance .
829	We can keep only the features needed for 95 % importance . This step seems to me to have the greatest chance of harming the model 's learning ability , so rather than changing the original dataset , we will make smaller copies . Then , we can test both versions of the data to see if the extra feature removal step is worthwhile .
830	Test `` Full '' Dataset
831	We can go through a quick example to show how PCA is implemented . Without going through too many details , PCA finds a new set of axis ( the principal components ) that maximize the amount of variance captured in the data . The original data is then projected down onto these principal components . The idea is that we can use fewer principal components than the original number of features while still capturing most of the variance . PCA is implemented in Scikit-Learn in the same way as preprocessing methods . We can either select the number of new components , or the fraction of variance we want explained in the data . If we pass in no argument , the number of principal components will be the same as the number of original features . We can then use the ` variance_explained_ratio_ ` to determine the number of components needed for different threshold of variance retained .
832	We only need a few prinicipal components to account for the majority of variance in the data . We can use the first two principal components to visualize the entire dataset . We will color the datapoints by the value of the target to see if using two principal components clearly separates the classes .
833	Combined Aggregation Function
834	Merge with the main dataframe
835	Aggregate previous loans at Home Credit
836	Aggregate Installments Data
837	LOW_PAYMENT ` represents a payment that was less than the prescribed amount .
838	Aggregate Cash previous loans
839	INSTALLMENTS_PAID ` is meant to represent the number of already paid ( or I guess missed ) installments by subtracting the future installments from the total installments .
840	Aggregate Credit previous loans
841	This is usually the point at which the kernel fails.__ To try and alleviate the problem , I have added a pause of 10 minutes .
842	After all the hard work , now we get to test our features ! We will use a model with the hyperparameters from random search that are documented in another notebook . The final model scores __0.792__ when uploaded to the competition .
843	Now we can see if all that time was worth it ! In the code below , we find the most important features and show them in a plot and dataframe .
844	Below we read in the data and separate into a training set of 10000 observations and a `` testing set '' of 6000 observations . After creating the testing set , we can not do any hyperparameter tuning with it
845	We have to pass in a set of hyperparameters to the cross validation , so we will use the default hyperparameters in LightGBM . In the ` cv ` call , the ` num_boost_round ` is set to 10,000 ( ` num_boost_round ` is the same as ` n_estimators ` ) , but this number wo n't actually be reached because we are using early stopping . As a reminder , the metric we are using is Receiver Operating Characteristic Area Under the Curve ( ROC AUC ) . The code below carries out both cross validation with 5 folds and early stopping with 100 early stopping rounds .
846	Hyperparameter Tuning Implementation
847	One aspect to note is that if ` boosting_type ` is ` goss ` , then we can not use ` subsample ` ( which refers to training on only a fraction of the rows in the training data , a technique known as [ stochastic gradient boosting ] ( Therefore , we will need a line of logic in our algorithm that sets the ` subsample ` to 1.0 ( which means use all the rows ) if ` boosting_type=goss ` . As an example below , if we randomly select a set of hyperparameters , and the boosting type is `` goss '' , then we set the ` subsample ` to 1.0 .
848	The ` boosting_type ` and ` is_unbalance ` domains are pretty simple because these are categorical variables . For the hyperparameters that must be integers ( ` num_leaves ` , ` min_child_samples ` ) , we use ` range ( start , stop , [ step ] ) ` which returns a range of numbers from start to stop spaced by step ( or 1 if not specified ) . ` range ` always returns integers , which means that if we want evenly spaced values that can be fractions , we need to use ` np.linspace ( start , stop , [ num ] ) ` . This works the same way except the third argument is the number of values ( by default 100 ) . Finally , ` np.logspace ( start , stop , [ num = 100 ] , [ base = 10.0 ] ) ` returns values evenly spaced on a logarithmic scale . According to the [ the docs ] ( `` In linear space , the sequence starts at $ base^ { start } $ ( base to the power of start ) and ends with $ base ^ { stop } $ `` This is useful for values that differ over several orders of magnitude such as the learning rate .
849	Learning Rate Domain
850	The results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function . When we get to Bayesian Optimization , the model actually _uses the past results to decide on the next hyperparmeters_ to evaluate . Random and grid search are _uninformed_ methods that do not use the past history , but we still need the history so we can find out which hyperparameters worked the best A dataframe is a useful data structure to hold the results .
851	Grid Search Implementation
852	Normally , in grid search , we do not limit the number of evaluations . The number of evaluations is set by the total combinations in the hyperparameter grid ( or the number of years we are willing to wait ! ) . So the lines if i > MAX_EVALS break would not be used in actual grid search . Here we will run grid search for 5 iterations just as an example . The results returned will show us the validation score ( ROC AUC ) , the hyperparameters , and the iteration sorted by best performing combination of hyperparameter values .
853	Now , since we have the best hyperparameters , we can evaluate them on our `` test '' data ( remember not the real test data
854	Random search is surprisingly efficient compared to grid search . Although grid search will find the optimal value of hyperparameters ( assuming they are in your grid ) eventually , random search will usually find a `` close-enough '' value in far fewer iterations . [ This great paper explains why this is so ] ( grid search spends too much time evaluating unpromising regions of the hyperparameter search space because it has to evaluate every single combination in the grid . Random search in contrast , does a better job of exploring the search space and therefore can usually find a good combination of hyperparameters in far fewer iterations . As [ this article ] ( lays out , random search should probably be the first hyperparameter optimization method tried because of its effectiveness . Even though it 's an _uninformed_ method ( meaning it does not rely on past evaluation results ) , random search can still usually find better values than the default and is simple to run . Random search can also be thought of as an algorithm : randomly select the next set of hyperparameters from the grid ! We can build a dictionary of hyperparameters by selecting one random value for each hyperparameter as follows ( again accounting for subsampling
855	We can also evaluate the best random search model on the `` test '' data .
856	Below is the code we need to run before the search . This creates the csv file , opens a connection , writes the header ( column names ) , and then closes the connection . This will overwrite any information currently in the ` out_file ` , so change to a new file name every time you want to start a new search .
857	When we save the results to a csv , for some reason the dictionaries are saved as strings . Therefore we need to convert them back to dictionaries after reading in the results using the ` ast.literal_eval ` function .
858	First we can plot the validation scores versus the iteration . Here we will use the [ Altair ] ( visualization library to make some plots ! First , we need to put our data into a long format dataframe .
859	Distribution of Search Values
860	Testing Results on Full Data
861	First we will test the cross validation score using the best model hyperparameter values from random search . This can give us an idea of the generalization error on the test set . We will delete the number of estimators found from the smaller dataset and use early stopping to find the best number of decision trees to train .
862	The public leaderboard score is only calculated on 10 % of the test data , so the cross validation score might actually give us a better idea of how the model will perform on the full test set . Usually we expect the cross validation score to be higher than on the testing data , but because of the small size of the testing data , this might be reversed for this problem . Next , we will make predictions on the test data that can be submitted to the competition .
863	We 'll join the train and test set together but add a separate column identifying the set . This is important because we are going to want to apply the same exact procedures to each dataset . It 's safest to just join them together and treat them as a single dataframe . I 'm not sure if this is allowing data leakage into the train set and if these feature creation operations should be applied separately . Any thoughts would be much appreciated
864	A [ feature primitive ] ( is an operation applied to a table or a set of tables to create a feature . These represent simple calculations , many of which we already use in manual feature engineering , that can be stacked on top of each other to create complex features . Feature primitives fall into two categories Aggregation__ : function that groups together child datapoints for each parent and then calculates a statistic such as mean , min , max , or standard deviation . An example is calculating the maximum previous loan amount for each client . An aggregation works across multiple tables using relationships between tables . Transformation__ : an operation applied to one or more columns in a single table . An example would be taking the absolute value of a column , or finding the difference between two columns in one table . A list of the available features primitives in featuretools can be viewed below .
865	DFS with Default Primitives
866	If you are interested in running this call on the entire dataset and making the features , I wrote a script [ for that here ] ( Unfortunately , this will not run in a Kaggle kernel due to the computational expense of the operation . Using a computer with 64GB of ram , this function call took around 24 hours ( I do n't think I 'm technically breaking the rules of my university 's high powered computing center ) . I have made the entire dataset available [ here ] ( in the file called ` feature_matrix.csv ` . To generate a subset of the features , run the code cell below .
867	DFS with Selected Aggregation Primitives
868	Next we can look at correlations within the data . When we look at correlations with the target , we need to be careful about the [ multiple comparisons problem ] ( if we make a ton of features , some are likely to be correlated with the target simply because of random noise . Using correlations is fine as a first approximation for identifying `` good features '' , but it is not a rigorous feature selection method . Also , based on examining some of the features , it seems there might be issues with [ collinearity between features ] ( made by featuretools . Features that are highly correlated with one another can diminish interpretability and generalization performance on the test set . In an ideal scenario , we would have a set of independent features , but that rarely occurs in practice . If there are very highly correlated varibables , we might want to think about removing some of them . For the correlations , we will focus on the ` feature_matrix_spec ` , the features we made by specifying the primitives . The same analysis could be applied to the default feature set . These correlations were calculated using the entire training section of the feature matrix .
869	Visualize Distribution of Correlated Variables
870	The feature importances returned by a tree-based model [ represent the reduction in impurity ] ( from including the feature in the model . While the absolute value of the importances can be difficult to interpret , looking at the relative value of the importances allows us to compare the relevance of features . Although we want to be careful about placing too much value on the feature importances , they can be a useful method for dimensionality reduction and understanding the model .
871	We can calculate the number of top 100 features that were made by featuretools .
872	Remove Low Importance Features
873	Align Train and Test Sets
874	Take these with some skepticism because they were performed on a very small subset of the data For more rigorous results , we will turn to the evaluation metrics from running __500 iterations ( with random search ) __ and __400+ iterations ( with Bayesian Optimization ) __ on a full training dataset with about 700 features ( the features are from [ this notebook ] ( by [ Aguiar ] ( These iterations took around 6 days on a machine with 128 GB of RAM so they will not run in a kernel ! The Bayesian Optimization method is still running and I will update the results as they finish . In this notebook we will focus only on the results and building the best model , so for the explanations of the methods , refer to the previous notebooks Overall Results First , let 's start with the most basic question : which model produced the highest cross validation ROC AUC score ( using 5 folds ) on the training dataset
875	Well , there you go ! __Random search slightly outperformed Bayesian optimization and found a higher cross validation model in far fewer iterations.__ However , as we will shortly see , this does not mean random search is the better hyperparameter optimization method . When submitted to the competition ( at the end of this notebook Random search results scored 0 . Bayesian optimization results scored 0 . What were the best model hyperparameters from both methods Random Search best Hyperparameters
876	If we compare the individual values , we actually see that they are fairly close together when we consider the entire search grid Distribution of Scores Let 's plot the distribution of scores for both models in a kernel density estimate plot .
877	Distribution of Scores
878	Plots of Hyperparameters vs Score
879	First up is ` reg_alpha ` and ` reg_lambda ` . These control the amount of regularization on each decision tree and help to prevent overfitting to the training data .
880	The next plot is learning rate and number of estimators versus the score . __Remember that the number of estimators was selected using early stopping for 100 rounds with 5-fold cross validation__ . The number of estimators __was not__ a hyperparameter in the grid that we searched over . Early stopping is a more efficient method of finding the best number of estimators than including it in a search ( based on my limited experience
881	Here there appears to be a clear trend : a lower learning rate leads to higher values ! What does the plot of just learning rate versus number of estimators look like
882	The bayesian optimization results are close in trend to those from random search : lower learning rate leads to higher cross validation scores .
883	Now we can make a heatmap of the correlations . I enjoy heatmaps and thankfully , they are not very difficult to make in ` seaborn ` .
884	That 's a lot of plot for not very much code ! We can see that the number of estimators and the learning rate have the greatest magnitude correlation ( ignoring subsample which is influenced by the boosting type ) .
885	First we need to format the data and extract the labels .
886	Properly Representing Variable Types
887	There are also two ordinal variables in the ` app ` data : the rating of the region with and without the city .
888	There are a number of day offsets that are recorded as 365243 . Reading through discussions , others replaced this number with ` np.nan ` . If we do n't do this , Pandas will not be able to convert into a timedelta and throws an error that the number is too large . The following code has been adapted from a script on [ GitHub
889	These four columns represent different offsets DAYS_CREDIT ` : Number of days before current application at Home Credit client applied for loan at other financial institution . We will call this the application date , ` bureau_credit_application_date ` and make it the ` time_index ` of the entity . DAYS_CREDIT_ENDDATE ` : Number of days of credit remaining at time of client 's application at Home Credit . We will call this the ending date , ` bureau_credit_end_date DAYS_ENDDATE_FACT ` : For closed credits , the number of days before current application at Home Credit that credit at other financial institution ended . We will call this the closing date , ` bureau_credit_close_date ` . DAYS_CREDIT_UPDATE ` : Number of days before current application at Home Credit that the most recent information about the previous credit arrived . We will call this the update date , ` bureau_credit_update_date ` . If we were doing manual feature engineering , we might want to create new columns such as by subtracting ` DAYS_CREDIT_ENDDATE ` from ` DAYS_CREDIT ` to get the planned length of the loan in days , or subtracting ` DAYS_CREDIT_ENDDATE ` from ` DAYS_ENDDATE_FACT ` to find the number of days the client paid off the loan early . However , in this notebook we will not make any features by hand , but rather let featuretools develop useful features for us . To make date columns from the ` timedelta ` , we simply add the offset to the start date .
890	The bureau balance dataframe has a ` MONTHS_BALANCE ` column that we can use as a months offset . The resulting column of dates can be used as a ` time_index ` .
891	Let 's look at some of the time features we can make from the new time variables . Because these times are relative and not absolute , we are only interested in values that show change over time , such as trend or cumulative sum . We would not want to calculate values like the year or month since we choose an arbitrary starting date . Throughout this notebook , we will pass in a ` chunk_size ` to the ` dfs ` call which specifies the number of rows ( if an integer ) or the fraction or rows to use in each chunk ( if a float ) . This can help to optimize the ` dfs ` procedure , and the ` chunk_size ` can have a [ significant effect on the run time ] ( Here we will use a chunk size equal to the number of rows in the data so all the results will be calculated in one pass . We also want to avoid making any features with the testing data , so we pass in ` ignore_entities = [ app_test ] ` .
892	Let 's visualize one of these new variables . We can look at the trend in credit size over time . A positive value indicates that the loan size for the client is increasing over time .
893	To use interesting values , we assign them to the variable and then specify the ` where_primitives ` in the ` dfs ` call .
894	One of the features is ` MEAN ( previous.CNT_PAYMENT WHERE NAME_CONTRACT_STATUS = Approved ) ` . This shows the average `` term of previous credit '' on previous loans conditioned on the previous loan being approved . We can compare the distribution of this feature to the ` MEAN ( previous.CNT_PAYMENT WHERE NAME_CONTRACT_STATUS = Canceled ) ` to see how these loans differ .
895	An additional extension to the default aggregations and transformations is to use [ seed features ] ( These are user defined features that we provide to deep feature synthesis that can then be built on top of where possible . As an example , we can create a seed feature that determines whether or not a payment was late . This time when we make the ` dfs ` function call , we need to pass in the ` seed_features ` argument .
896	These features could be completely useless , or they may be helpful . Only building a model and training it with the features will help us determine the answer . MostRecent The final custom feature will be ` MOSTRECENT ` . This simply returns the most recent value of a discrete variable with respect to time columns in a dataframe . When we create an entity , featuretools will [ sort the entity ] ( by the ` time_index ` . Therefore , the built-in aggregation primitive ` LAST ` calculates the most recent value based on the time index . However , in cases where there are multiple different time columns , it might be useful to know the most recent value with respect to all of the times . To build the custom feature primitive , I adapted the existing ` TREND ` primitive ( [ code here
897	Putting it all Together
898	We will now do the same operation applied to the test set . Doing the calculations separately should prevent leakage from the testing data into the training data .
899	Feature selection ] ( is an entire topic to itself . However , one thing we can do is use the built-in featuretools [ selection function to remove ] ( columns that only have one unique value or have all null values .
900	When we 're done , we probably want to save the results to a csv . We want to be careful because the index of the dataframe is the identifying column , so we should keep the index . We also should align the training and testing dataframes to make sure they have the same columns .
901	We need to create new names for each of these columns . The following code makes new names by appending the stat to the name . Here we have to deal with the fact that the dataframe has a multi-level index . I find these confusing and hard to work with , so I try to reduce to a single level index as quickly as possible .
902	Correlations of Aggregated Values with Target
903	If we go through and inspect the values , we do find that they are equivalent . We will be able to reuse this function for calculating numeric stats for other dataframes . Using functions allows for consistent results and decreases the amount of work we have to do in the future Correlation Function Before we move on , we can also make the code to calculate correlations with the target into a function .
904	First we one-hot encode a dataframe with only the categorical columns ( ` dtype == 'object ' ` ) .
905	Function to Handle Categorical Variables
906	The above dataframes have the calculations done on each _loan_ . Now we need to aggregate these for each _client_ . We can do this by merging the dataframes together first and then since all the variables are numeric , we just need to aggregate the statistics again , this time grouping by the ` SK_ID_CURR ` .
907	Putting the Functions Together
908	Aggregated Stats of Bureau Balance by Client
909	Calculate Information for Testing Data
910	We need to align the testing and training dataframes , which means matching up the columns so they have the exact same columns . This should n't be an issue here , but when we one-hot encode variables , we need to align the dataframes to make sure they have the same columns .
911	We can calculate not only the correlations of the variables with the target , but also the correlation of each variable with every other variable . This will allow us to see if there are highly collinear variables that should perhaps be removed from the data . Let 's look for any variables that have a greather than 0.8 correlation with other variables .
912	For each of these pairs of highly correlated variables , we only want to remove one of the variables . The following code creates a set of variables to remove by only adding one of each pair .
913	We can remove these columns from both the training and the testing datasets . We will have to compare performance after removing these variables with performance keeping these variables ( the raw csv files we saved earlier ) .
914	To actually test the performance of these new datasets , we will try using them for machine learning ! Here we will use a function I developed in another notebook to compare the features ( the raw version with the highly correlated variables removed ) . We can run this kind of like an experiment , and the control will be the performance of just the ` application ` data in this function when submitted to the competition . I 've already recorded that performance , so we can list out our control and our two test conditions For all datasets , use the model shown below ( with the exact hyperparameters ) . control : only the data in the ` application ` files . test one : the data in the ` application ` files with all of the data recorded from the ` bureau ` and ` bureau_balance ` files test two : the data in the ` application ` files with all of the data recorded from the ` bureau ` and ` bureau_balance ` files with highly correlated variables removed .
915	Examining the feature improtances , it looks as if a few of the feature we constructed are among the most important . Let 's find the percentage of the top 100 most important features that we made in this notebook . However , rather than just compare to the original features , we need to compare to the _one-hot encoded_ original features . These are already recorded for us in ` fi ` ( from the original data ) .
916	We spent quite a bit of time developing two functions in the previous notebook agg_numeric ` : calculate aggregation statistics ( ` mean ` , ` count ` , ` max ` , ` min ` ) for numeric variables . agg_categorical ` : compute counts and normalized counts of each category in a categorical variable . Together , these two functions can extract information about both the numeric and categorical data in a dataframe . Our general approach will be to apply both of these functions to the dataframes , grouping by the client id , ` SK_ID_CURR ` . For the ` POS_CASH_balance ` , ` credit_card_balance ` , and ` installment_payments ` , we can first group by the ` SK_ID_PREV ` , the unique id for the previous loan . Then we will group the resulting dataframe by the ` SK_ID_CURR ` to calculate the aggregation statistics for each client across all of their previous loans . If that 's a little confusing , I 'd suggest heading back to the [ first feature engineering notebook
917	Monthly Cash Data
918	Monthly Credit Data
919	Split into training and validation groups
920	Prediction for one image
921	Train and validate
922	Let 's declare variables with those attributes and visualize
923	Let 's see the children count per application
924	Granted applications per number of children
925	Income distribution and target value
926	Import libraries
927	Read the data
928	Check the typical length of a comment .
929	Train a word2vec model
930	We create 6 models , one for each toxic level each .
931	Define helper functions
932	Initialize and load data - call SaltParser functions
933	Perform stratified training/validation split based on coverage .
934	Predict validation and test set masks
935	Set parameters
936	Sample aggregations
937	Make train/test ready to train model
938	Train the LGBM model
939	Output submission
940	Group features , aggregates , are one of the most powerful way to capture relationships between variables in the dataset . Sometimes it is possible to group by target variable and thus provide model with direct information about it ( although one should be careful when doing that in order not to introduce a leak , only training data subset can be grouped this way ) . For grouping of other variables , whole dataset can be used , as you are given both train and test data . Important ! - this concerns Kaggle competitions , one should not do this in real-life ML , as you never know what exactly will the distribution of variables in test data be . In this kernel I try to create an end-to-end feature engineering solution based on groupby features . Whole process can be divided into a few steps Selection of columns for each type categorical ` - categorical features which must be encoded ( standard label encoding is used ) . categorical_int ` - categorical features which are alredy in integer dtype , encoding is not needed . numerical ` - numerical features Factorization of ` categorical ` columns , if needed . Creation of aggregates with columns renaming for each type of columns . Resulting DataFrame is saved ( if possible , will not work in Kaggle Kernels ) . There are two ways of aggregated features creation Batch aggregation : all columns from DataFrame are processed , each column is appended to one of three types , ` categorical ` columns are factorized is there 's a need and selected aggregates are applied to each column type . There is a distinction between aggregates for categorical columns and those for numerical , as each type requires a different approach . Selected aggregation : combination of aggregates/columns should be provided in form of a dictionary , where for each column aggregates are specified in a list . II . Setup First , we need to choose aggregates , which will be used for grouping categorical and numerical variables . For numerical variables ` aggs_num_basic ` will be used , for categoricals - ` aggs_cat_basic ` . Those types of aggregations can be extended further , as is shown in ` aggs1 ` list . python aggs_num_basic = [ 'mean ' , 'min ' , 'max ' , 'std ' , 'sem ' , 'sum aggs_cat_basic = [ 'mean ' , 'std ' , 'sum aggs1 = [ 'mean ' , 'median ' , 'min ' , 'max ' , 'count ' , 'std ' , 'sem ' , 'sum ' , 'mad
941	Loading the data
942	IIIa . Bureau balance & Bureau , batch aggregation
943	and batch aggregations examples for the rest of the tables ...
944	load mapping dictionaries
945	extract different column types
946	adapted from
947	For this competition , dictionary containing information about dataset is available . That 's helpful for feature engineering , as it provides a possible direction of engineering for each feature . One thing to keep in mind is that this set was created _artificially All data is simulated and fictitious , and is not real customer data Kernel environment has it 's memory and speed constraints , therefore ` historical_transactions.csv ` file will not be used , as it 's the biggest one . We will base our workflow on remaining set of files .
948	no NaN in train and test , that 's good some NaNs in both merchants DF , especially ` category_2 ` feature .
949	Group merchant data by merchant_id
950	Group new_merchant data by card_id
951	join new_merchant with train/test by card_id
952	Prepare for training
953	Initialize train and test DataFrames to access IDs and depth information .
954	Set data loading parameters
955	Perform stratified train/valid split based on coverage class
956	Perform check on randomly chosen mask and prediction
957	Test prediction
958	Submission output
959	Load data & reduce memory usage
960	Create DFs imitating public and private test subsets
961	Distribution of months in train and test
962	SHAP Interaction Values
963	Interaction - dependence plots
964	Vertical disperion for interaction plot , which captures main effects , is lower than in the original plot , which was to be expected .
965	Get important features according to SHAP
966	Exponential Growth Curves
967	Based on the above curves , it seems that South Korea 's growth rate has reached an inflection point . The sigmoid function does now fit without error , so it is included in the next section . Logistic Growth Curves China and the subset of China outside Hubei now have resonable sigmoid growth curves . Here are their plots .
968	Gaussian Approximation of Active Cases
969	load core DFs ( train and test
970	load mapping dictionaries
971	inspect datagen output
972	Source pydicom is a pure Python package for working with DICOM files . It lets you read , modify and write DICOM data in an easy `` pythonic '' way . In the example below , we import the first dcm file in the training set , and then display its contents
973	Note that , you ca n't access this field via name ( ValueError raised
974	These keywords are attached to pydicom package . These keywords can be also found on this page First 5 keys in this dictionary are presented below
975	In pydicom package we can refer to pixel_array to get an array for example
976	Due to this problem , we will move on with some function wrapper for this
977	These files should contain CT 3D image , connected by SeriesInstanceUID . Let 's check it out , if it holds for the first patient
978	A beautiful collection ! Let 's find out what the particular features mean . Each feature has a group ( module ) assigned , we will start with a patient module
979	WARNING : many of the threads presented below are not directly related to the OSIC competition , but more to the DICOM standard . Using the documentation , we will try to decipher the data for a sample , randomly selected patient
980	Let 's go ahead and see how it looks in our file . By default pydicom reads in pixel data as the raw bytes found in the file
981	Top - down animation
982	This images from validation data seem to be really strange labeled ....
983	Process test data in parallel
984	Load Packages and Data
985	Outlier Analysis and Feature Scaling
986	Modeling : Xgboost
987	DICOM-CT data 3D visualizations
988	In order to use pyvirt you just need to install it via pip and then turn it on ( disp.start ( ) ) and off ( disp.stop ( ) ) x-server
989	and background color definition
990	The actor is a grouping mechanism : besides the geometry ( mapper ) , it also has a property , transformation matrix , and/or texture map . Here we set its color and rotate it
991	Add the actors to the renderer , set the background and size
992	Finally , gather results , stop dummy x-server and display image
993	From a certain point of view , it may seem senseless to run Slicer from the command line without a real x-server . However , this was my starting point to use beautiful VTK visualizations . The code below shows the loading and visualization of several sample data sets . MRHead Example Generally , it comes down to saving the python code in a separate file and running Slicer with this script Source
994	OSIC training data Example
995	Predict Test Set and Submit Result
996	Site Thanks to [ @ yamsam ] ( and his great kernel .
997	Site Thanks to [ @ mpware ] ( and his great kernel .
998	Site Thanks to [ @ serengil ] ( and his great kernel .
999	Session level CV score : 1 . User level CV score : 1 .
1000	TPU Strategy and other configs
1001	Load Model into TPU
1002	Create fake filepaths dataframe
1003	First downsize all the images
1004	Create real file paths dataframe
1005	Load and freeze DenseNet
1006	Training Phase 1 - Only train top layers
1007	Training Phase 2 - Unfreeze and train all
1008	Loading the 32x32 dataset
1009	Creating and Training the Model
1010	Save model and weights
1011	Padding process and resizing with OpenCV
1012	Pad and resize all the images
1013	Applying Convolutions
1014	Group and Reduce
1015	Adding mode as feature
1016	Refit and Submit
1017	Plotting some random images to check how cleaning works
1018	Load and process data
1019	Load text data into memory
1020	Build datasets objects
1021	Load model into the TPU
1022	First , we train on the subset of the training set , which is completely in English .
1023	Now that we have pretty much saturated the learning potential of the model on english only data , we train it for one more epoch on the ` validation ` set , which is significantly smaller but contains a mixture of different languages .
1024	Create fast tokenizer
1025	Load text data into memory
1026	Build datasets objects
1027	Load model into the TPU
1028	First , we train on the subset of the training set , which is completely in English .
1029	Now that we have pretty much saturated the learning potential of the model on english only data , we train it for one more epoch on the ` validation ` set , which is significantly smaller but contains a mixture of different languages .
1030	In this section , we define a few functions that will be used for processing images and formatting the output prediction . You can safely skip this section and use the following functions as is format_prediction_string ( image_id , result ) ` : ` image_id ` is the ID of the test image you are trying to label . ` result ` is the dictionary created from running a ` tf.Session ` . The output is a formatted output row ( i.e . ` { Label Confidence XMin YMin XMax YMax } , { ... } ` ) , so we need to modify the order from Tensorflow , which is by default ` YMin XMin YMax XMax ` ( Thanks to [ Nicolas for discovering this draw_boxes ( image , boxes , class_names , scores , max_boxes=10 , min_score=0.1 ) ` : ` image ` is a numpy array representing an image , ` boxes ` , ` class_names ` , and ` scores ` are directly retrieved from the model predictions . display_image ( image ) ` : Display a numpy array representing an ` image ` .
1031	Let 's see what it looks like
1032	Step 1 : Building a TF Graph
1033	Step 3 : Running the session
1034	Inference on Test Set
1035	Load and preprocess data
1036	Public and private sets have different sequence lengths , so we will preprocess them separately and load models of different tensor shapes .
1037	Evaluate training history
1038	Public and private sets have different sequence lengths , so we will preprocess them separately and load models of different tensor shapes . This is possible because RNN models can accept sequences of varying lengths as inputs .
1039	For each sample , we take the predicted tensors of shape ( 107 , 5 ) or ( 130 , 5 ) , and convert them to the long format ( i.e . $ 629 \times 107 , 5 $ or $ 3005 \times 130 ,
1040	Load and preprocess data
1041	Unhide below to see all trials results
1042	Save model and best hyperparams
1043	Public and private sets have different sequence lengths , so we will preprocess them separately and load models of different tensor shapes .
1044	For each sample , we take the predicted tensors of shape ( 107 , 5 ) or ( 130 , 5 ) , and convert them to the long format ( i.e . $ 629 \times 107 , 5 $ or $ 3005 \times 130 ,
1045	Phase 1 : Train on all data
1046	Load Model into TPU
1047	Will need those folders later for storing our jpegs .
1048	Create new labels
1049	Pad and resize all the images
1050	We only select a sample of the total training dataset ( we randomly choose 400k files ) , and call it ` sample_df ` .
1051	pivot_df ` is simply ` sample_df ` reformatted so that each column is a label ( this way , we can use multi-label in our data generator later ) .
1052	Load the U-Net++ model trained in the previous kernel .
1053	Create test generator
1054	filtered_sub_df ` contains all of the images with at least one mask . ` null_sub_df ` contains all the images with exactly 4 missing masks .
1055	We 'll load in the data and take a look at its shape to get an idea of how many features and how many samples we have .
1056	First we 'll split the training data into testing and training sets
1057	I want to check what is the % accuracy in predicting the right interest level
1058	Now let 's plot this against every K to see the decrease
1059	Step 1 : Remove test images without defects
1060	Beware : Messy code below
1061	filtered_sub_df ` contains all of the images with at least one mask . ` null_sub_df ` contains all the images with exactly 4 missing masks .
1062	Now , we combine results from the predicted masks with the rest of images that our first CNN classified as having all 4 masks missing .
1063	This EDA will mainly focus on detecting how the null masks are distributed . We will group all the ` ImageId_ClassId ` by their respective ImageId , and keep track of the number of missing masks for each image .
1064	Reducing Image Size
1065	Save results as CSV files
1066	Training the model
1067	Load Test dataframe
1068	Infer using trained model
1069	Comparing various kappa scoring
1070	Relevant attributes of identified objects are stored
1071	Extract input-output pairs if any
1072	Hello everyone , in this kernel , I will share some information I discovered in the training process of kannadamnist dataset . And some tips to get the stable result with 0.99+ accuracy . This kernel will cover The training results before I got 0.99 accuracy and some thoughts I would like to share regarding to previous result Some useful techiques , which I heavily use in other competitions or projects How I improve the kernel to get better result What else might be helpful to get better result than my current result At the begining , I will show you the results from my previous training and what I discovered . I already trained 3 models with different split of data in colab and save their valid accuracy for 30 epochs . And the valid accuracy had been save every 6 epochs . I found some interesting things in the result . And I change some places in the kernel upon these information to get my current result . So after showing the results of previous training , I will show the new results after I adjusted the kernel .
1073	Import the modules
1074	Note Here I read the csv files with pandas api . There are 3 datasets train.csv - > training dataset Dig-MNIST.csv - > validation dataset test.csv - > testing dataset I will merge training dataset with validation dataset first then use train_test_split or KFold function to produce training & validation dataset . Since I 'm not sure whether there will be difference between original training dataset and validation dataset . So I merge them together and trying to make the data more random .
1075	Note From the print out data above . We will know The first column is the label of dataset . Other columns are the digital value of pixels of kannada mnist . And there are 784 columns of digital value in 1 row . The dataset is the same as regular mnist dataset .
1076	Note First we turn the original data columns into square dimension , which is 28x28x1 ( height x width x channel Since we will use the convolution neural network to train the data , so we need to transform the data into image dimension . And I do one-hot encoding on the original labels
1077	Note KFold is a very common cross-validation method . It will split your original training data into several pieces ( depends on the n_splits parameters ) . And use 1/n_splits portion of data to be the validation data and rest of data to be training data . But this validation data will keep change until it go through all your original training data , for instance original data = [ 1,2 , n_splits training validatng data training data : [ 2 , training validating data training data : [ 1 , training validating data training data : [ 1 , But we also know , it is not a good idea to blend the training data with validating data . So We can use several models to train on each different spliting situation . After training , we will have n_splits models , which all of them are training on ( 1-1/n_splits ) of original training data and validating on 1/n_splits of original training data . So all of these models were training on different ( partially different ) datasets . Then we can use these models to predict the data we want to predict , thus we get n_splits different prediction results . In the end , we can blend these results by voting or averaging them and get a much stable result than use only one model .
1078	Note Here I still use albumentations library to do the TTA . And I created a simple tta_wrapper class . It will wrap the original model you just trained then use it to perform tta . The concept is quite simple , it took the model then predict the results of original test image and images after some multiplications . Since the output of the model is the probability of each class in one-hot-encoded form , which already normalized by softmax activation . So I simply add up the probabilities then average them . Thus I got a new result by averaging the prediction results of several different view of original test image .
1079	In this competition , not all images have good quality . Ths blurry image ( such as index 8 ) is difficult to extract features from it . Therefore , we may need to drop the blur images before we train the model .
1080	Drop the blurry image
1081	Display the dropped images
1082	Creating submission file
1083	Build the original and translated test data .
1084	Predict with pure text models .
1085	Predict with mixed language models .
1086	Ensemble with my historical best .
1087	This is a simple modify from
1088	Pick some frames to display
1089	Hope some one could improve the speed of the kernel , and also share more feature engineering idea . Glad to hear advice and bug report
1090	Split Trian and Valid
1091	First , Let 's try a simple lightgbm classifier
1092	There are no clear difference in importance among features If the random seed changes , the feature importance will also change ( The top importance feature always change when I change the random seed
1093	This plot shows summarized information about feature impact against shap output .
1094	I 'm going to use a custom function to caculate the ratio with respect to each row and add it as a new column
1095	Plotting errors for one sample .
1096	text { MSE } = E\big [ ( y - \hat { f } ( x ) ) ^2\big text { Var } ( y ) + \text { Var } ( \hat { f } ( x ) ) + ( y - E [ \hat { f } ( x varepsilon^2 + \text { Var } [ \hat { f } ( x ) ] + \text { Bias } [ \hat { f } ( x by putting $ $ \hat { f } ( x ) = f ( x ) $ $ the bias and variance terms cancel out and we get : $ $ \text { MSE } = \varepsilon for more details check this [ notebook
1097	On this sample the algorithm ouputed the same structure from the dataset , let 's see how accurate the algorithm is with all the structures in the instances
1098	train solved tasks
1099	evaluation solved tasks
1100	All train tasks predictions
1101	Fast data loading
1102	Leak Data loading and concat
1103	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags wind_speed + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross validation . I go not using this data in this kernel for robust modeling .
1104	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so it is better to consider time-splitting . However this notebook is for simple tutorial , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
1105	Fast data loading
1106	Leak Data loading and concat
1107	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags wind_speed + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross validation . I go not using this data in this kernel for robust modeling .
1108	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so it is better to consider time-splitting . However this notebook is for simple tutorial , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
1109	Fast data loading
1110	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags wind_speed + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross validation . I go not using this data in this kernel for robust modeling .
1111	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so it is better to consider time-splitting . However this notebook is for simple tutorial , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
1112	Leak Validation for public kernels ( not used leak data
1113	A one idea how we can use LV usefull is blending . We probably can find best blending method without LB probing and it 's means we can save our submission .
1114	Find Best Weight
1115	Fast data loading
1116	Leak Data loading and concat
1117	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags wind_speed + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross validation . I go not using this data in this kernel for robust modeling .
1118	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so it is better to consider time-splitting . However this notebook is for simple tutorial , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
1119	We can find out all types of sex in SexuponOutcome , and we will calssify them
1120	Now we create the dict and add the data to DataFrame-animals
1121	As you see : The Neutered pets are accepted mostly.The points should be on the Neutereds
1122	what already is known
1123	make hour column from transactionDT
1124	At first , I made Europe future . Soon , I 'll increase it .
1125	There is a gap between them .
1126	SGD scored the best initial result , but after a lengthy hyperparameter tuning , it was not able to pass a 2.54503 threshold . Finally , from the algorithms that scored under 3.0 , we decided to work with LightGBM due to its efficiency and versatility in the hyperparameters tuning . LightGBM is a decision tree boosting algorithm uses histogram-based algorithms which bucket continuous feature ( attribute ) values into discrete bins . This technique speeds up training and reduces memory usage . In layman terms the algorithm works like this Fit a decision tree to the data Evaluate the model Increase the weight to the incorrect samples . Choose the leaf with max delta loss to grow . Grow the tree . Go to step 2 ! [ lightgbm Benchmark There are two types of benchmarks we need to set . The first will be a naive prediction . This prediction will be a baseline score to compare with our model ’ s score to evaluate if we have any significant progress . In a Multiclass Classification , the best way to calculate the baseline is by assuming that the probability of each category equals its average frequency in the train set . The frequency can be calculated easily by dividing the sum of incidents of each category by the number of rows of the training set .
1127	Model Evaluation and Validation
1128	This incident has taken place in 03:30 in the night in a block . As we saw in the Partial Dependencies graphs before , BURGLARY has a higher probability and burglaries happen by definition in blocks . Let ’ s see if our model aligns with our intuition .
1129	Introduction - Do they match
1130	There seem to be difference , but the gap of `` diff_V109_V110 '' is small . In Version7 , I found that `` diff_V109_V110 '' is not meaningful . I deleted . In Version13 , I found that `` diff_V329_V330 '' is not meaningful . I deleted . In Version20 , I found that `` diff_V4_V5 '' is not meaningful . I deleted . I feel that when the gap is big , the column is meaningful .
1131	We can not use literal features for XGB , so these features are changes . For example , [ H , G , W , A ] → [ 0,1,2 , The number of the words is often related to the numeral ( [ 0,1,2,3 ] ) .
1132	They are very similar to each other .
1133	when it comes to Android browser , I will use different way because chrome has `` chrome for android
1134	Importing Library Files
1135	I focused on browser I am a very lazy person , so I rarely update my browser . How swindlers are
1136	This kernel is based on Image Augmentation and the following is applied Horizontal Flip Width Shift Random 45 degree rotation Filling Random Zoom
1137	It 's Augment time .
1138	We need to add the format of the images in the end or Use glob for flexibility
1139	OUTPUT OF AUGMENTATED IMAGES
1140	These augmentations are inspired by [ CutMix ] ( and [ MixMatch Source
1141	Everything is set , time to create the EfficientDet model using the code snippet presented above ( the ` get_train_efficientdet ` function ) .
1142	That 's it , our ` WheatModel ` is ready now . Let 's train it . For that , we will create a ` Trainer ` and set it to ` fast_dev_run=True ` for a quicker demo . Also , since it is in this mode , the ` Trainer does n't automatically save the weights at the end ( correct me if I am wrong of course ) so we need to add a ` torch.save ` call at the end .
1143	Before casting to the appropriate type , we need to explore the columns to find out the best one . In what follows , I will display few values of each column , count the number of unique values and compare it to the length of the column .
1144	Next , any column with `` textual '' information and having more than 3 unique values and less than , say , 60 % of the column length , should be transformed into the [ categorical type . Some background information about this type : it is a fairly new addition to pandas since version 0.21.0 ) and is inspired from the R one . To do so , will use the ` astype ( `` category '' ) ` method .
1145	Great ! So we have one mask ( represented using rle ) with ` class_id ` 1 for the given image . Let 's plot the mask
1146	Hum , that looks like a mask but wrongly shaped . What went wrong Well , this is a quirk of the rle format . I wo n't delve into too much details but just remember that you need to rotate the mask 90 degrees ( counter-clockwise ) . To do so , one can use the transpose operation .
1147	Awesome , it worked ! Alright , now that we have both images and masks saved as files , let 's load a bunch of them . Before we dive deep , here is the plan to get the data pipeline contruct a DataFrame that contains only images having at least one mask ( this will be called with_masks_df Load images using the ` from_df ` method Extract labels using the ` label_from_func ` method : this takes a function that reads one image input path and outputs a mask path . Since we have multiple classes ( 0 for background and 1 through 4 for the different defect types ) , these should be passed as well . Transform the images and masks . Notice that this step is optional but I am adding it to get resized images ( since I pass the ` size ` variable Finally , use the ` databunch ` method to create a [ DataBunch ] ( Again , this is n't necessary but will comes handy in the next notebook so I want you to get used to the concept .
1148	Load the data and add L2 distance computation between atoms
1149	So how to extract a date Well , first , get ride of the decimal values . Then transform to a datetime object supposing that it is an ordinal datetime . Try different offsets until you get a meaningful date range . That 's it . Let 's see this in action .
1150	Let 's see if our observation transfers well to the test dataset .
1151	Most of the dates overlap .
1152	Some imports ( as usual
1153	Rolling monthly and yearly store means
1154	More on this next time ( maybe ) ...
1155	This Notebook uses such great kernels Logistic Regression on Tournament seeds March Madness Data - First Look EDA NCAAM20no-leak starter Basic Starter Kernel - NCAA Women 's Dataset Before we get started I want to thank Kaggle and each member here for sharing such huge concepts , without their efforts i ca n't write this notebook with such ease So , without wasting any time , let 's start with importing some important python modules that I 'll be using .
1156	then , Let 's drop the columns we are not planning on using
1157	Next we 'll create a new dataframe that summarizes wins & losses along with their corresponding seed differences . This is the meat of what we 'll be creating our model on .
1158	Train Our Linear Regression Model
1159	Next , we will make prediction with our LR Model
1160	imbalance targets
1161	Sampling the train data since too much data
1162	targets in labels.csv
1163	targets in train.csv
1164	These labels are not in train
1165	TPU Strategy and other configs
1166	Prepare Data & Loader
1167	Get Model into TPU
1168	Loading and Visualization of Data
1169	We see how often the different catagories occur , thus giving us insight on how to pick up the data for training . As we can see that None of the catagories occur the most and thus we will sample ~10,000 sentences from it .
1170	Making Vocabulary and Text Conversion
1171	Trying something else What if we take only the words that are lower and convert capitalised words to lower , how many tokens to we get then . This gives us ~1000 less tokens compared to when we were also using caps , which is not a difference of lot but will still help us .
1172	Thus we see we have 322849 sentences in total , now we see the lengths of each of them , to decide our designing of the model . So some of the features of the data are as follows
1173	For converting them into features we train a word2vec model which converts each word into it 's corresponding vector . Word2Vec models are extremely efficient in finding temporal relations as they themselves are shallow neural networks . We train the word2vec model on the entire corpus so that it learns the similarities in the text and can give us vectors for all the words , not just those that occur in training dataset . You can perform the following operations and learn about word2vec model .
1174	Making Feature Matrices
1175	Count game trainsition
1176	Visualize by heatmap
1177	View Single Image
1178	Number of Patients and Images in Training Images Folder
1179	Number of Patients and Images in Test Images Folder
1180	Target & ID Loading
1181	Image Loading & Pre-processing
1182	Train & Validation Split
1183	Create Image Augmentation Generator
1184	Imports & Settings
1185	Get Tabular Data
1186	process training images
1187	process test images
1188	process submission images
1189	we add some squared features for some model flexability
1190	This is a crude implementation of gradient descent without the gradients ( so instead of a gradient telling us which direction to go , we try increasing and decreasing the value , we see which direction give us a better score , and we take the value that gives us the better score ) . Random Search is faster , but if we try to implement some kind of adaptive 'learning rate ' ( i.e . making big changes in the beginning and decreasing the changes as we go ) this approach moves a bit faster . I was able to use this in an actual submission .
1191	Here we want to model confidence . I tried adding Laplace Log Likelihood as an additional metric , but eventually removed it due to issues . The metric works , but the model does n't really solve for fvc and confidence at the same time , making the metric useless . In hindsight I think I needed to change the way I was defining my model output , but I just dropped the idea for the time being . I included the code in case anyone wanted to take a look .
1192	Target & ID Loading
1193	Image Loading & Pre-processing
1194	Train & Validation Split
1195	So according to the documentation `` We recognize that toxicity and identity can be subjective , so we collect labels from up to 10 people per comment , to capture a range of opinions . '' Again , they say `` up to 10 people per comment '' . I interpret this to mean that they might not always take 10 , in some cases they might take 9 or 8 or 7 or 6 or 5 , etc . In this case it seems there were 6 annotators and only 1 of them found the comment to be toxic . Let 's learn a bit more about of annotators ...
1196	So what we want to do is take a toxic comment , and find the most similar non-toxic comment We found an article that might be able to help us out here Let 's pick a toxic string we want to use ...
1197	Let 's try this one ..
1198	Split into train and test sets
1199	Convert an array of values into a dataset matrix
1200	Create dataset with look back
1201	Train LSTM with 3 epochs
1202	Make prediction and apply invert scaling
1203	Using all features for model training
1204	Train LSTM with 3 epochs
1205	Lets have a look at the year difference between the year of transaction and the year built
1206	These have their kitchen area larger than the total area of the house
1207	It looks like full_all is not the exact sum of male and female population . We can eliminate this column and use the ratio of male and female population as a feature
1208	feature_3 has 1 when feautre_1 high than
1209	card_id : Card identifier month_lag : month lag to reference date purchase_date : Purchase date authorized_flag : Y ' if approved , 'N ' if denied category_3 : anonymized category installments : number of installments of purchase category_1 : anonymized category merchant_category_id : Merchant category identifier ( anonymized subsector_id : Merchant category group identifier ( anonymized merchant_id : Merchant identifier ( anonymized purchase_amount : Normalized purchase amount city_id : City identifier ( anonymized state_id : State identifier ( anonymized category_2 : anonymized category
1210	merchant_id : Unique merchant identifier merchant_group_id : Merchant group ( anonymized merchant_category_id : Unique identifier for merchant category ( anonymized subsector_id : Merchant category group ( anonymized numerical_1 : anonymized measure numerical_2 : anonymized measure category_1 : anonymized category most_recent_sales_range : Range of revenue ( monetary units ) in last active month -- > A > B > C > D > E most_recent_purchases_range : Range of quantity of transactions in last active month -- > A > B > C > D > E avg_sales_lag3 : Monthly average of revenue in last 3 months divided by revenue in last active month avg_purchases_lag3 : Monthly average of transactions in last 3 months divided by transactions in last active month active_months_lag3 : Quantity of active months within last 3 months avg_sales_lag6 : Monthly average of revenue in last 6 months divided by revenue in last active month avg_purchases_lag6 : Monthly average of transactions in last 6 months divided by transactions in last active month active_months_lag6 : Quantity of active months within last 6 months avg_sales_lag12 : Monthly average of revenue in last 12 months divided by revenue in last active month avg_purchases_lag12 : Monthly average of transactions in last 12 months divided by transactions in last active month active_months_lag12 : Quantity of active months within last 12 months category_4 : anonymized category city_id : City identifier ( anonymized state_id : State identifier ( anonymized category_2 : anonymized category
1211	card_id : Card identifier month_lag : month lag to reference date purchase_date : Purchase date authorized_flag : Y ' if approved , 'N ' if denied category_3 : anonymized category installments : number of installments of purchase category_1 : anonymized category merchant_category_id : Merchant category identifier ( anonymized subsector_id : Merchant category group identifier ( anonymized merchant_id : Merchant identifier ( anonymized purchase_amount : Normalized purchase amount city_id : City identifier ( anonymized state_id : State identifier ( anonymized category_2 : anonymized category
1212	Make a Baseline model
1213	Create dataset for training and Validation
1214	CNN Model for multiclass classification
1215	Create Inference Dataset
1216	Define dataset and model
1217	Let 's define which metrics we will use and create magic objects to train and validate our model
1218	Attach to our trainer a function to run a validator at the end of each epoch
1219	In this problem I think it 's better not to use the same learning rate during all the training , so let 's make it decrease after each epoch
1220	Prediction for test
1221	Porto Seguo - End-to-end Ensemble
1222	let 's run the frequency encoding function
1223	let 's run the binary encoding function
1224	optionally , you can also choose to drop the original categorical features . Shoud you do it ? I say trust your CV let 's do this here jut for demonstration purpose
1225	Feature Reduction
1226	Xgboost K-fold & OOF function
1227	Generate level 1 OOF predictions
1228	Logistic Regression
1229	A little bit of diversity from Naive Bayes never heard , this one of those algorithms that normally do n't generate sigle output that rival XGB/LGB , but nevertheless help to improve the overal stacking performance due the diversity it bring to the party
1230	Level 2 XGB
1231	So what just happened there ? Our CV score for each training fold is pretty descent , but our overall training CV score just fell through the crack ! Well , it turns out since we are using AUC/Gini as metric which is ranking dependent , and it turns out that if you apply xgb and lgb at level 2 stacking , the ranking get messed up when each fold 's prediction scores are put together . And this goes back to why we implemented that function to convert probability into ranks earlier . Now , let 's use the use_rank option , and see what happens
1232	Level 2 LightGBM
1233	Level 2 Random Forest
1234	Level 2 Logistic Regression
1235	Level 3 ensemble
1236	Level 3 XGB
1237	Level 3 Logistic Regression
1238	Well , for training score , we manage to arravie at 0.28443 . We can now try to apply the same weight distribution to generate our submission .
1239	Perspective of analysis Sales can be reponsive to time factor and space factor Store 's sales records are the aggregation of each department Date variable can be split into y/m/w/d variables Day variable can provide much information on sales Outside data such as national holiday of US will be combined to add information
1240	Year / Month / Week / Days / passed days are extracted from 'Date ' column
1241	Stores table consists of three columns 1. store ID 2. store type 3. store size . A , B , and C are the types of stores and total 45 Walmart stores are selling goods to customers
1242	Let 's make a pie chart to show the ratio of A , B , and C types of total 45 Walmart stores . First , let 's group data by type of stores and see the descriptive figures
1243	Imgur
1244	Assumption 1 is correct . By boxplot , we can infer that type A store is the largest store and C is the smallest Even more , there is no overlapped area in size among A , B , and C. Type is the best predictor for Size To check assumption 2 , boxplot showing relation between sales and type is made
1245	The median of A is the highest and C is the lowest That means stores with more sizes have higher sales record ( The order of median of size and median of sales is the same
1246	Store can be the variable giving information on sales But store is including much intrinsic information of type , size , and department
1247	Holiday and Store do not show significant relations but just small higher sales soaring when hoiliday
1248	Each department shows the different level of sales Department may be the powerful variable to predict sales When department and type of store are considered together , generally department in A type shows the highest sales record Assumption 4 : Type and department may have the interaction effect
1249	Compare timing for CutMix
1250	Compare timing for MixUp
1251	Compare timing for GridMask
1252	Perform label encoding on Sexo feature , this is for producing the countplot later on
1253	Cod_prov ( Province Code ) distribution
1254	This inference kernel is the continuation of my TF2 training kerenl [ Use Hugging Face 's Tensorflow 2 transformer models for NQ ] ( These demonstrate how to use Hugging Face 's [ transformers ] ( package , more precisely , theier ` Tensorflow 2 ` models , for this competition . Disclamation I am not a part of Hugging Face . I choose to use ` transformers ` package because I found it 's easier to use and to extend , so I can focus on other parts of this notebook . I take no responsibility for any ( potential ) error in this kernel and in the dataset ` nq-competition ` . ( I would appreciate any feedback . I take no credit of any file ( with/without my own modifications ) containing in ` nq-compeittion ` .
1255	Hugging Face pretrained Bert model names
1256	Make TF record file for test dataset
1257	Get the actual validation / test datasets from TF Records
1258	Choose the model to use
1259	Run on validation dataset
1260	Get metrics for validation dataset
1261	Run on test dataset
1262	This kernel demonstrates how to use Hugging Face 's [ transformers ] ( package , more precisely , theier ` Tensorflow 2 ` models , for this competition . Motivations Give my first contribution on Kaggle . To learn and work on some things new . Provide an easy to read/use training kernel for this competition . The most important ) I want to have your feedbacks , and solutions to some of my questions , so we can improve this kernel Features I tried my efforts to make the following things work batch accumulation . Adam optimizer with weight decay and make it work with learning rate decay . ( I take [ AdamW ] ( class from ` Tensorflow Addons ` with some modification . See ` adamw_optimizer.py Custom learning rate schedule as close as to the original google Bert code . Easy to choose different model configurations . Important Notes I did n't use the pre-trained model from starter kernel . I did n't check ( yet ) the LB scores for the checkpoints I provided . No code for inference/submission in this kernel . ( I will work on a separate kernel . The flag ` model_name ` is default to ` distilbert-base-uncased-distilled-squad ` . The ` batch_size ` and ` batch_accumulation_size ` is however defaut to ` 10 ` . It 's your own responsibility to make sure there is no OOM when you change these parameters . In the training loop , if the training is on ` Kaggle ` , then it stops after ` 8 hours 45 minutes ` so we can still have a saved checkpoint . This kerenl is not optimalized for training . ( See next section . The checkpoints provided in [ nq-competition ] ( ( as the value of ` NQ_DIR ` below ) are trained by myself without any effort on choosing the hyperparameters . After you have your new trained checkpoints ( or if you create your own .tfrecord file ) , when you want to use them for resuming training , you have to set your own value for ` MY_OWN_NQ_DIR ` , which should contain your own checkpoints/.tfrecord file . This kernel is not written for using TPU . Checkpoints , Batch size and Training time In the following table , the model configurations are distilled bert ` : distilbert-base-uncased-distilled-squad bert base ` : bert-base-uncased bert large ` : bert-large-uncased-whole-word-masking-finetuned-squad Here is the summary of the checkpoints I provided ( no ` bert large ` , it will eat up my GUP quota . Probably I will provide one after training on GCP ) . model | batch size | accumulation size | effective batch size | num . updates | training time | epochs | time / epoch distilled bert | 25 | 4 | 100 | 9893 | 30823 s | 2 | 4.28 h bert base | 10 | 10 | 100 | 4500 | 32226 s | 0.909 | 9.84 h bert large | 2 | 50 | 100 | n/a | n/a | n/a | 37.78 h Remark For ` distilled bert ` , I did n't test with ` train_batch_size=10 ` and ` batch_accumulation_size=10 ` for ` epochs=2 ` . It 's possible that this will take more time than the time indicated above . A screenshot of training progress In training loop , I print the losses and accuracies every ` 100 ` effective batch , here ` 1 effective batch = batch_size accumulation_size ` . It includes those for ` start_pos ( loss_S , acc_S ) ` , ` end_pos ( loss_E , acc_E ) ` and ` answer_type ( loss_A , acc_A ) ` . The following screenshot is however taken from a training where the result is printed every effective batch . training.png ] ( attachment : training.png Remark In the trainin progress , ` batch n ` means ` effective batch n ` . TODO - Some contribution wanted This kernel is only for training . I will add the prediction/submission parts as soon as possible ( In a separate kernel . The batch accumulation code seems work fine , but not memory/speed efficient . In particular , I ca n't get rid of the following warning . UserWarning : Converting sparse IndexedSlices to a dense Tensor of unknown shape . This may consume a large amount of memory . Maybe TPU support , not very sure . However , see the 3rd link in the ` Reference ` section . Things tried but not working - Solution wanted I tried to use mixed precision for training , but I could n't make it work . Using [ tf.keras.mixed_precision.experimental.Policy ] ( for mixed precision training causes error when loading pretrained models . ( ` input 1 ( zero-based ) was expected to be a half tensor but is a float tensor [ Op : AddV2 ] name : tf_bert_model_1/bert/embeddings/add/ ` . Using [ tf.config.optimizer.set_experimental_options ] ( does n't make tranining faster . Probably this is designed only for compiled model Disclamation I am not a part of Hugging Face . I choose to use ` transformers ` package because I found it 's easier to use and to extend , so I can focus on other parts of this notebook . I take no responsibility for any ( potential ) error in this kernel and in the dataset ` nq-competition ` . ( I would appreciate any feedback . I take no credit of any file ( with/without my own modifications ) containing in ` nq-compeittion ` . References Transformer model for language understanding ( colab tutorial ) ] ( - Official Tensorflow tutorial on transformer model Transformer 與 TensorFlow 2 英翻中 ] ( - An annotated chinese tutorial based on the above one Sequence classification with Transformers & Strategy ] ( - Hugging Face 's tutorial on how to use their TF 2.0 models on TPU Hugging Face : State-of-the-Art Natural Language Processing in ten lines of TensorFlow 2.0 ] ( - Hugging Face 's post on Tensorflow blog
1263	Hugging Face pretrained Bert model names
1264	Choose the model to use
1265	Get variables to apply weight decay in AdamW optimizer
1266	AdamW Optimizer ( Weight decay
1267	Check saved checkpoints
1268	Timing with advanced ( Chris Deotte 's ) data augumentation only
1269	An example usage
1270	Timing with advanced data augumentation run in GPU / TPU
1271	Get labels and their countings
1272	Define the number of repetitions for each class
1273	Check oversampled dataset
1274	The feature selection has been done using the most important features resulting of Will Koehrsen [ automated features generation ] ( Thanks to him for his awesome notebook and introduction to [ Featuretools ] ( and [ Deep Feature Synthesis ] ( method . First , let 's aggregate the features coming from the credit bureau file in the main loans table .
1275	Next , let 's generate the features coming from the installments data . To do so We compute first the minimum of installment payments in the previous applications table . The rational is to catch the loans where one payment has been missed or was very low . Then we merge the aggregates of all minimum installment payments across all previous loans .
1276	Finally , let 's add all the base features from the main loan table which do n't need aggregation . We fill the missing values with their average .
1277	Random Forest model
1278	B . Load libraries and data files , file structure and content
1279	C. Missing values
1280	Data transformation and helper functions
1281	Forecast methods
1282	E. Facebook prophet library
1283	Selected model performance ( validation score ) over train dataset
1284	E. mixed model
1285	This design can is independent of pandas and can be ported to other data processing frameworks such as spark . Any logical code written using pandas can then be ported over to run on a spark based parallel processing system using the ` Koalas ` API , yet following a similar design pattern . Pure Functions Pure functions have two important properties If given the same arguments , the function must return the same value When the function is evaluated , there are no side effects ( no I/O streams , no mutation of static and non-local variables Fig . 4 A visual representation of pure functions Source When writing functions for data transformations , we can not always write pure functions , especially considering the limitations of system memory , given that to mutate a non-local variable would require to create an entire new copy of the dataframe . Therefore , it is important to have a boolean argument ` inplace ` which can help the developer decide whether or not to mutate the dataframe as per the requirements of the situation . Type Hinting Type hinting , was introduced in ` PEP 484 ` and ` Python 3.5 ` . Therefore I donot recommend using it completely as of now , unless you are sure that all of the libraries you use in your workflow are compatible for ` Python 3.5 ` and above . The basic structure of type hinting in python is as follows def fn_name ( arg_name : arg_type ) - > return_type pass Once a function definition is complete , use the ` - > ` symbol to indicate the return type , it could be ` int ` , ` dict ` or any other python data type . Every argument in the function is followd by a ` : ` and the data type of the argument You can also use more complex ways of representing nested data types , optional data types , etc . using the ` typing ` module in Python Below , you can find an example where I use the ` typing ` module and use type hinting in python
1286	For this particular problem , we can just use one time period of 48 days
1287	Empirical studies indicate that changes in real estate sector mirrors the wider changes taking place in the economy at any point in time . Most of these studies put emphasis in explaining how macroeconomic variables are responsible for short and long run variations in residential property prices . According to Schmitz and Brett ( 2001 ) the economic strength of a place can be demonstrated by its macroeconomic conditions , which includes interest rates , inflation , job security , industrial productivity and stock market stability . In another study in Hong Kong , Ervi ( 2002 ) , found out that the rate of return in property markets is linked to economic activities while demand for retail space is sensitive to changes in employment and local output . The author also recognizes that macroeconomic variables include unemployment , inflation rates , GDP , interest rates , balances of payments and foreign exchange rates . American economistSimon Kuznets believes real estate development has a close relationship with economicgrowth after analyzing a large amount of data of different countries . В экономической литературе приводят различные классификации факторов , влияющих на развитие рынка недвижимости : внутренние и внешние , макроэкономические и микроэкономические и т. д. В исследовании были отобраны показатели , характеризующие факторы спроса и предложения и , по мнению авторов , потенциально влияющие на удорожание жилой недвижимости : 1 ) цена на нефть марки Urals ; 2 ) валовой внутренний продукт ( ВВП ) ; 3 ) уровень доходов населения ; 4 ) инфляция ; 5 ) себестоимость строительства ; 6 ) денежная масса ; 7 ) процентная ставка по ипотечным кредитам ; 8 ) количество ипотечных сделок .
1288	Show influence of economical factors on housing prices
1289	Best parameters are searched by GridSearchCV on my Laptop
1290	Set Model for prediction
1291	Test data preparation
1292	Data preparation for test
1293	If there are any problems , I appreciate it if you inform me of it . Thanks
1294	Convert DCM to PNG
1295	Accuracy
1296	Loss
1297	Predictions class distribution
1298	As discussed in [ my other kernel ] ( parsing the training dataset with default settings could take up to 2GBs of memory unnecessarily . With a few techniques ( also discussed in the linked kernel ) we can cut this down by about a gigabyte . In this kernel , we 'll use similar techniques , if you want to see my reasoning for this please refer to the other kernel . First we need to determine which numeric columns we have so that we can downcast them ( cast from float64 to another type that requires less memory ) .
1299	Now that we 've removed all the ` NaN ` values , we can downcast the columns to the lowest precision . First we 'll need to know which columns are integers , though ! The following snippet does just that .
1300	So we can see here that there are some very small ranges there -- not all of them will need to be ` float64 ` . Let 's downcast them .
1301	Looks like we shaved a whole gig . We 're not done yet , we need to make sure we do the same thing on the test set . Let 's read it in now , merge the tables , and impute it the same way .
1302	We added some columns in our training set and replaced missing values with the medians . We need to add those same columns , and also add the median from the training set for those missing values ( the same ones ) .
1303	Unfortunately , we 're not done yet . We might have some missing values on other numeric columns we did n't anticipate . In practice , we do n't always have a `` test set '' . The test set might be new observations that come in the future , could be a list of observations or a single one , so we ca n't really use statistics from the test set to impute the missing values . So we need to use values from the training set . We 'll use the median as well .
1304	First , we 'll replace missing values with the string ` `` missing '' ` ( we actually do n't need to do this since pandas does it automatically , but I like to give it an explicit label , makes it easier to see ) .
1305	Next we 'll convert the columns in the training set to categorical .
1306	Using the whole training set is too time-consuming for quick iteration , so we can use a sample . We could use a random sample , but since this is time-ordered , I 'm guessing the more recent rows would give us better predictive value . So let 's just grab the bottom rows .
1307	Now that we have a decent model , we can actually train on the whole dataset , including the validation set .
1308	Check the dataset
1309	Load pre-trained model
1310	In this competition , diversity of prediction datasets is really important to improve LB score by blending or stacking . Most of all notebooks are using neural networks including gru , lstm and graph . To increase diversity of prediction dataset , I tried to generate good predictions with lightgbm . As a result , I got LB score of 0.27652 . This notebook is mainly based on both by tito , and by T88 . Thank you very much Version I corrected some mistakes . I did parameter tuning for each target by optuna lightgbm tuner . Local CV was improved from 0.23835 to 0.23392 .
1311	preprocess data 1 - generate shift features
1312	preprocess data
1313	Check null data
1314	edjefe , years of education of male head of household , based on the interaction of escolari ( years of education ) , head of household and gender , yes=1 and no replace yes - > 1 and no
1315	edjefa , years of education of female head of household , based on the interaction of escolari ( years of education ) , head of household and gender , yes=1 and no replace yes - > 1 and no
1316	Make new features using continuous feature
1317	I will reduce the number of features using shap , so let 's generate many features ! ! Hope catch some fortune features
1318	Ratio feature can have infinite values . So Let them be filled with
1319	combination using three features
1320	I want to mix electricity and energy features - > energy features
1321	I want to mix toilet and rubbish disposal features - > other_infra features
1322	I want to mix toilet and water provision features - > water features
1323	I want mix education and area features - > education_zone_features
1324	Mix region and education
1325	Remove feature with only one value
1326	Feature selection using shap
1327	Load and preprocess data
1328	The result seems to better than individual pre-trained models and so we let us create a submission file using this model blend .
1329	Read dataset
1330	Check null data
1331	After looking the feature , simply I think that the category feature can be divided into youtube , google , other categories .
1332	google , baidu , facebook , reddit , yahoo , bing , yandex ,
1333	Time feature
1334	Drop features
1335	Read dataset
1336	Checking the imbalance of dataset is important . If imbalanced , we need to select more technical strategy to make a model .
1337	Suite type
1338	Occupation type
1339	FONDKAPREMONT
1340	House type
1341	Wall material
1342	Emergency
1343	Flag document
1344	DAYS_BIRTH is some high correlation with target . With dividing 365 ( year ) and applying abs ( ) , we can see DAYS_BIRTH in the unit of year ( AGE ) .
1345	The simple kde plot ( kernel density estimation plot ) shows that the distribution of repay and not-repay is different for EXT_SOURCE_1 . EXT_SOURCE_1 can be good feature .
1346	Not as much as EXT_SOURCE_1 do , EXT_SOURCE_2 shows different distribution for each repay and not-repay .
1347	The mutivariate kde plot of not-repay is broader than one of repay . For both CNT_60_SOCIAL_CIRCLE and OBS_30_CNT_SOCIAL_CIRCLE , the distribution of each repay and not-repay is a bit different . Log-operation helps us to see them easily .
1348	A client can have several loans so that merge with bureau data can explode the row of application train .
1349	As you can see , repay have a litter more right-skewed distribution . To see more deeply , Let 's divide the overdue feature into several groups .
1350	Check the dataset
1351	I think this feature means the type of batteries of each machine . Oh , no .... These days , most batteries are lithum-ion battery . So , Let 's group them into lithum-batter group and non0-lithum-battery group
1352	The difference is quite small . Do you think that some malwares recognize and select machine based on the type of battery Battery is very important part for life of machine . I think that malware will focus on other hardware and software parts of machine . remove this .
1353	Categorical features
1354	RtpStateBitfield - NA
1355	IsSxsPassiveMode - NA
1356	DefaultBrowsersIdentifier - ID for the machine 's default browser
1357	AVProductStatesIdentifier - ID for the specific configuration of a user 's antivirus software
1358	AVProductsInstalled - NA
1359	AVProductsEnabled - NA
1360	HasTpm - True if machine has tpm
1361	CountryIdentifier - ID for the country the machine is located in
1362	CityIdentifier - ID for the city the machine is located in
1363	OrganizationIdentifier - ID for the organization the machine belongs in , organization ID is mapped to both specific companies and broad industries
1364	GeoNameIdentifier - ID for the geographic region a machine is located in
1365	LocaleEnglishNameIdentifier - English name of Locale ID of the current user
1366	OsBuild - Build of the current operating system
1367	OsSuite - Product suite mask for the current operating system .
1368	IsProtected - This is a calculated field derived from the Spynet Report 's AV Products field .
1369	AutoSampleOptIn - This is the SubmitSamplesConsent value passed in from the service , available on CAMP
1370	SMode - This field is set to true when the device is known to be in 'S Mode ' , as in , Windows 10 S mode , where only Microsoft Store apps can be installed
1371	IeVerIdentifier - NA
1372	Firewall - This attribute is true ( 1 ) for Windows 8.1 and above if windows firewall is enabled , as reported by the service .
1373	UacLuaenable - This attribute reports whether or not the `` administrator in Admin Approval Mode '' user type is disabled or enabled in UAC . The value reported is obtained by reading the regkey HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\System\EnableLUA .
1374	Census_OEMNameIdentifier - NA
1375	Census_OEMModelIdentifier - NA
1376	Census_ProcessorCoreCount - Number of logical cores in the processor
1377	Census_ProcessorManufacturerIdentifier - NA
1378	Census_ProcessorModelIdentifier - NA
1379	Census_PrimaryDiskTotalCapacity - Amount of disk space on primary disk of the machine in MB
1380	Census_SystemVolumeTotalCapacity - The size of the partition that the System volume is installed on in MB
1381	Census_HasOpticalDiskDrive - True indicates that the machine has an optical disk drive ( CD/DVD
1382	Census_TotalPhysicalRAM - Retrieves the physical RAM in MB
1383	Census_InternalPrimaryDiagonalDisplaySizeInInches - Retrieves the physical diagonal length in inches of the primary display
1384	Census_InternalPrimaryDisplayResolutionHorizontal - Retrieves the number of pixels in the horizontal direction of the internal display .
1385	Census_InternalPrimaryDisplayResolutionVertical - Retrieves the number of pixels in the vertical direction of the internal display
1386	Census_OSBuildNumber - OS Build number extracted from the OsVersionFull . Example - OsBuildNumber = 10512 or
1387	Census_OSInstallLanguageIdentifier - NA
1388	Census_OSUILocaleIdentifier - NA
1389	Census_IsPortableOperatingSystem - Indicates whether OS is booted up and running via Windows-To-Go on a USB stick .
1390	Census_IsFlightsDisabled - Indicates if the machine is participating in flighting .
1391	Census_ThresholdOptIn - NA
1392	Census_FirmwareManufacturerIdentifier - NA
1393	Census_FirmwareVersionIdentifier - NA
1394	Census_IsSecureBootEnabled - Indicates if Secure Boot mode is enabled .
1395	Census_IsWIMBootEnabled - NA
1396	Census_IsVirtualDevice - Identifies a Virtual Machine ( machine learning model
1397	Census_IsTouchEnabled - Is this a touch device
1398	Census_IsPenCapable - Is the device capable of pen input
1399	Census_IsAlwaysOnAlwaysConnectedCapable - Retreives information about whether the battery enables the device to be AlwaysOnAlwaysConnected .
1400	Wdft_IsGamer - Indicates whether the device is a gamer device or not based on its hardware combination .
1401	Wdft_RegionIdentifier - NA
1402	I 'm not quant trader . But , I know some simple indexes to analyze the charts . Ta-lib is very good and very helpful library for calculating various indexes , but kernel does n't support . So , I introduce some indexes and short scripts to obtain them .
1403	Moving average is so simple
1404	Moving average convergence divergence ( MACD ) is a trend-following momentum indicator that shows the relationship between two moving averages of prices . The MACD is calculated by subtracting the 26-day exponential moving average ( EMA ) from the 12-day EMA ref .
1405	A Volume Moving Average is the simplest volume-based technical indicator . Similar to a price moving average , a VMA is an average volume of a security ( stock ) , commodity , index or exchange over a selected period of time . Volume Moving Averages are used in charts and in technical analysis to smooth and describe a volume trend by filtering short term spikes and gaps . ref .
1406	Reference
1407	Data understanding
1408	Data check
1409	Find Null data
1410	Predict null data based on statistical method
1411	Onehot encoding for categorical data
1412	Target , prediction process
1413	As you can see , classes are 5005 but data contain 25361 images .
1414	Null data check
1415	It seems Goblins are a little similar to Ghouls .
1416	The colour of monsters does n't seem to be essential . Maybe it disturb the classification . Let 's classify without colour ...
1417	Logistic Regression seems to be a good classification algorithm for this dataset .
1418	You only have two areas to work on .
1419	Add active column
1420	data looks a bit dirty , we might get an overly optimistic prediction because the last number is not the final one for instance . The model is quite sensitive to this as it has only a handful of points to infer the dynamics from .
1421	Predict World ( With China Data
1422	Predict World ( Without China Data Because china nearly cleared COVID-19 , and data is ahead of the world , so maybe exclude china data sounds resonable .
1423	Predict by Specify Province
1424	Predict by Specify Country
1425	Predict all country greater than
1426	let 's see if we can make some sense from the parameters
1427	Predict all province greater than
1428	Finally there is data with United States by Province , let 's make models
1429	Looks no problem ! Let 's predict all provinces of United States which confirm cases greater than
1430	Hello World ! Have you ever think about how can data science predict our life , or even , health ? This WiDS Datathon definitely inspired me on its power . How can we use patient data to understand inviidual health condition , and predict hospital death Let 's look at the data first
1431	Age distribution of male and female patients .
1432	We also want to capture how the patient 's records change during the first hour and first day . Difference varialbes were created to describe the difference beween maximum and minimum value .
1433	We still have a lot of missing value , so we will use lightgbm regressor to impute two important features : apache_4a_icu_death_prob and apache_4a_hospital_death_prob
1434	Here is a base model without parameter tuning ..
1435	Now can visualize our features by class . Total counts tend to have high range of values in this set , so I group them separately to plot them with log of counts , otherwise the patterns get too squished on the lower end of the spectrum .
1436	Most of the features above tend to have different distribution , and are good candidates for experimenting , though 'uq_device_per_ip ' does n't look as separated to me , so I would put it lower on testing priority . Now let 's say you wanted to use minutes as a predictor based on given dataset
1437	There really is n't any difference in distribution of attributions by minute . So does n't really make sense to dig into that one by itself for consistent results . On the other hand a popular feature in the kernels and forums is time deltas . I 'm going to use the method from kernel to generate it , and fill in the missing values with some large value out of range .
1438	i do intend to make this a linked heading at some point ... TIP 1 - Deleting unused variables and gc.collect TIP 2 - Presetting the datatypes TIP 3 - Importing selected rows of the a file ( including generating your own subsamples TIP 4 - Importing in batches and processing each individually TIP 5 - Importing just selected columns TIP 6 - Creative data processing TIP 7 - Using Dask
1439	TIP 1 Deleting unused variables and gc.collect
1440	TIP 2 Presetting the datatypes
1441	This is how you can do your own random sampling Since 'skiprows ' can take in a list of rows you want to skip , you can make a list of random rows you want to input . I.e . you can sample your data anyway you like Recall how many rows the train set in TalkingData has
1442	Let 's say you want to pull a random sample of 1 million lines out of the total dataset . That means that you want a list of ` lines - 1 - 1000000 ` random numbers ranging from 1 to 184903891 . Note : generating such long list also takes a lot of space and some time . Be patient and make sure to use del and gc.collect ( ) when done
1443	In my previous notebook ( we found that the data is organized by click time . Therefore if our random sampling went according to plan , the resulting set should roughly span the full time period and mimick the click pattern . We see from above that first and last click span the 4 day period . Let 's try a chart to see if the pattern looks consistent
1444	We know that the proportion of clicks that was attributed is very low . So let 's say we want to look at all of them at the same time . We do n't know what rows they are , and we ca n't load the whole data and filter . But we can load in chuncks , extract from each chunk what we need and get rid of everything else The idea is simple . You specify size of chunk ( number of lines ) you want pandas to import at a time . Then you do some kind of processing on it . Then pandas imports the next chunk , untill there are no more lines left . So below I import one million rows , extract only rows that have 'is_attributed'==1 ( i.e . app was downloaded ) and then merge these results into common dataframe for further inspection .
1445	TIP 5 Importing just selected columns
1446	There are different sections to Dask , but for this case you 'll likely just use Dask DataFrames . Here are some basics from the developers A Dask DataFrame is a large parallel dataframe composed of many smaller Pandas dataframes , split along the index . These pandas dataframes may live on disk for larger-than-memory computing on a single machine , or on many different machines in a cluster . One Dask dataframe operation triggers many operations on the constituent Pandas dataframes . For convenience and Dask.dataframe copies the Pandas API . Thus commands look and feel familiar . What DaskDataframes can do they are very fast on most commonly used set of Pandas API below is taken directly from Trivially parallelizable operations ( fast Elementwise operations : ` df.x + df.y , df df Row-wise selections : ` df [ df.x Loc : ` df.loc [ 4.0:10 . Common aggregations : ` df.x.max ( ) , df.max Is in : ` df [ df.x.isin ( [ 1 , 2 , Datetime/string accessors : ` df.timestamp.month Cleverly parallelizable operations ( fast groupby-aggregate ( with common aggregations ) : ` df.groupby ( df.x ) .y.max ( ) , df.groupby ( ' x ' ) .max groupby-apply on index : ` df.groupby ( [ 'idx ' , ' x ' ] ) .apply ( myfunc ) ` , where ` idx ` is the index level name value_counts : ` df.x.value_counts Drop duplicates : ` df.x.drop_duplicates Join on index : ` dd.merge ( df1 , df2 , left_index=True , right_index=True ` ) or ` dd.merge ( df1 , df2 , on= [ 'idx ' , ' x ' ] ) ` where ` idx ` is the index name for both ` df1 ` and ` df Join with Pandas DataFrames : ` dd.merge ( df1 , df2 , on='id Elementwise operations with different partitions / divisions : ` df1.x + df2.y Datetime resampling : ` df.resample ( ... Rolling averages : ` df.rolling ( ... Pearson Correlations : ` df [ [ 'col1 ' , 'col2 ' ] ] .corr Notes/observations To actually get results of many of the above functions you have to add ` .compute ( ) ` at the end . eg , for value_counts would be : ` df.x.value_counts ( ) .compute ( ) ` . This hikes up RAM use a lot . I believe it 's because ` .compute ( ) ` gets the data into pandas format , with all the accompanying overhead . ( Please correct me if wrong ) . I 've been playing with dask for the past little while here on Kaggle Kernels , and while they can load full data and do some nice filtering , many actual operations do hike up RAM to extreme and even crush the system . For example , after loading 'train ' dataframe , just getting ` len ( train ) ` hiked RAM up to 9GB . So be careful ... Use a lot of ` gc.collect ( ) ` and other techniques for making data smaller . So far I find dask most useful for filtering ( selecting rows with specified features ) . Now let 's see some examples . First , let 's load the big train data
1447	ip , app , device , os and channel are actually categorical variables encoded as integers . Set them as categories for analysis .
1448	Convert date stamps to date/time type .
1449	Explore ip counts . Check if multiple ips have any downloads .
1450	Conversions by Device
1451	The proportions may be more reliable if estimated on full data . With the random sample it 's hard too tell because the variability is too high , especially for the hours with low click counts . i.e . the fewer clicks/conversions , the wider margin of the estimated conversion ratio . ( see below
1452	Preparing the data
1453	Chapter 4 : Employ Machine Learning
1454	Use machine learning model
1455	generate pred_str for submission workflow ( ) is an arbitrary function representing all other steps
1456	Import modules
1457	Ensure determinism
1458	For test data , set default start & end positions to dummy integer ( -1 ) .
1459	This notebook will deal with positive , negative and neutral samples independently .
1460	Apply predicsion results onto df_test for positive & negative samples .
1461	For neutral samples , use original texts as they are .
1462	yolov3.weights file can be found in The weight file is also ( supposedly ) in Kaggle dataset ( which I have not found ) .
1463	Write a problem file
1464	Plot the obtained tour
1465	I found that some of mismatches are caused by setting previous ` visitStartTime ` as ` visitId ` . of 898 duplicates are caused by the mismatches .
1466	Import libraries and data , reduce memory usage
1467	observe sales at the scale of state California generally has better salls than the other two states . Apart from foods Texas is better than Wisconsin The total sales of the category is : Foods > household > hobbies
1468	At the scale of stores
1469	Frome the EDA we saw that there is obvious relationship between time ( week day , month , year ) and te sales . So , the training data attributes contains time . At the moment , these are the only attributre . Perhaps in the features there will be more attributes .
1470	Reference
1471	This notebook uses display_aspect_ratio metadata field as a great fake video predictor .
1472	Later we will see that the 4th combination , missing from the training data , does in fact appear in the test data . My conlusion here is that Recursion used some kind of rotation of plates only , therefore only 4 combinations . Let 's calculate which sirna belongs to which plate in every of the 4 assignments
1473	In the code below we load the model , make predictions to get the full probabilites matrix , and set 3 out of 4 plates for every sirna to zero , according to the assignment that we previously selected .
1474	this is the function that sets 75 % of the sirnas to zero according to the selected assignment
1475	I had several spare submissions so I tried to hunt down the public test errors that were plaguing the village . The conclusion so far is that there are 27 corrupted videos in the public test , and the failures for me are specifically at ` Image.fromarray ` call . Some more details below .
1476	The dataset for this notebook is at The train data for this competition is big , almost 500Gb , so I hope it can be useful to have all the json files and the metadata in one dataframe . The dataset includes , for each video file Info from the json files : filename , folder , label , original split : train ( 118346 videos ) , public validation test ( 400 videos ) or train sample ( 400 videos ) . 119146 videos in total . Note that the public validation and the train sample are subsets of the full train , so it is enough to mark them in this dataframe . Full file md5 column Hash on audio file sequence wav.hash and on subset of pixels pxl.hash The rest are metadata fields from the files , obtained with ffprobe . Note that I removed many columns , which did n't give new information .
1477	Disable fastai randomness
1478	Load train and test dataframes and add length columns for Description and Name
1479	Standard fastai model definition with embeddings for categorical variables and two fully connected layers of default size . No regularization applied , to keep everything simple .
1480	Learning and giving a score on the training dataset . I used high learning rate value here in order to fight overfitting to some degree . This kind of configuration without any regularization , without even validation set to keep an eye on it , has a high potential for overfitting . But using only 10 epochs and high learning rate gives decent results out of sample .
1481	Submitting results
1482	What Does a Normal Image Look Like
1483	What are Lung Opacities
1484	A Closer Look Into `` No Lung Opacity / Not Normal '' Images
1485	There are obvious opacities in this image , so what 's going on This patient has a `` Target '' value of 0 in the stage_1_train_labels.csv file , which means he does not have a diagnosis of peumonia . `` There is also a binary target column , Target , indicating pneumonia or non-pneumonia '' - from the [ Data description ] ( page . Why are there multiple opacities in No Lung Opacity / Not Normal images I 'm afraid this competition is misleading in many ways . There are different kinds of opacities ( see a general explanation about opacities above ) . Some are related to pneumonia and some are not . What we see in the image of Patient 3 are Lung Nodules and Masses , which are defined as `` a rounded opacity , well or poorly defined '' - [ Felson 's Principles of Chest Roentgenology ( Fourth Edition ) ] ( The difference between a nodule and a mass is the size of the opacity . Unfortunately , it seems like Patient 3 he has multiple lung tumors , probably metastases from an invasive cancer in a different location of the body . What makes pneumonia associated opacities unique Let 's compare `` Sample Patient 2 - Lung Opacity '' , with `` Sample Patient 3 - Lung Nodules and Masses
1486	The main difference in the types of opacities between these two patients is the borders and the shape of the opacity , Patient 3 has multiple round and clearly defined opacities . Patient 2 has this poorly defined haziness which obscures the margins of the lungs and heart . This haziness is termed consolidation . Lung opacity annotation instructions to radiologists for this dataset Back to top ] ( Table-of-Contents Anouk Stein , MD ] ( from MD.ai posted the instructions given to the radiologists who annotated the competition dataset images in [ Pneumonia Dataset Annotation Methods Lung Opacity ( bounding box ) - a finding on chest radiograph that in a patient with cough and fever has a high likelihood of being pneumonia With the understanding that in the absence of clinical information , lateral radiograph , and serial exams , we have to make assumptions Include any area that is more opaque than the surrounding area ( Fleischner definition Exclude : obvious mass ( es ) , nodule ( s ) , lobar collapse , linear atelectasis In the cases labeled Not Normal/No Lung Opacity , no lung opacity refers to no opacity suspicious for pneumonia . Other non-pneumonia opacities may be present . Also , some of the not normal cases have subtle abnormalities which require a trained eye to discern . ( Which , for now , keeps radiologists around . In the next section I 'll try to put it all together and give a clear definition of the opacities annotated in this data set . A Clear and Detailed Definition of Pneumonia Associated Lung Opacities Back to top ] ( Table-of-Contents Why does the chest radiograph change when a person has pneumonia ? To answer this question we have to ask what is pneumonia first . Pneumonia Credit : Thompson AE . [ Pneumonia . JAMA . ] ( 2016 ; 315 ( 6 ) :626 . Pneumonia is a lung infection that can be caused by bacteria , viruses , or fungi . Because of the infection and the body 's immune response , the sacks in the lungs ( termed alveoli ) are filled with fluids instead of air . The reason that pneumonia associated lung opacities look diffuse on the chest radiograph is because the infection and fluid that accumulate spread within the normal tree of airways in the lung . There is no clear border where the infection stops . That is different from other diseases like tumors , which are totally different from the normal lung , and do not maintain the normal structure of the airways inside the lung . Consolidation vs. Ground-Glass Opacity Let 's compare two patients
1487	Opacities That Are Not Related to Pneumonia
1488	The lower part of the right lung of Patient 7 ( the right lung is at the left side of the picture ) is higher than in a normal image . This is a called a pleural effusion . It is caused by an accumulation of fluid in the chest outside of the lung . This causes the lung to look smaller on the chest radiograph . Lung Masses and Nodules Back to top ] ( Table-of-Contents
1489	This is the same image I used in the A Closer Look Into `` No Lung Opacity / Not Normal '' Images section where I explained the difference between a nodule and a pneumonia associated lung opacity . It 's a striking example of lung masses and nodules , I actually do n't remember seeing an image with so many clear masses in my work as a physician . Lung Nodules and Masses are defined as `` a rounded opacity , well or poorly defined '' - [ Felson 's Principles of Chest Roentgenology ( Fourth Edition ) ] ( There are a lot of articles about automated detection of nodules detection in the recent years , you can look at [ `` Learning to detect chest radiographs containing lung nodules using visual attention networks '' ] ( as an example . Increased Vascular Markings + Enlarged Heart Back to top ] ( Table-of-Contents
1490	Weiteng007 asked an interesting question - Can a lung opacity occur if the lung is removed ? . The answer is yes and no . An opacity can occur where the lung was once , but it can not be a lung opacity . This white lung phenomena in chest radiographs is termed `` hemithorax white-out '' or `` hemithorax opacity '' . Seeing a `` white lung '' on a chest radiograph leads to the question - if we see only an opacity where the lung is supposed to be , what happened to lung ? . These are some possible answers The lung was removed in a surgery called pneumonectomy . The lung is filled with fluid from pneumonia and what we see is a [ pneumonia associated lung opacity ] ( A-Clear-and-Detailed-Definition-of-Pneumonia-Associated-Lung-Opacities ) . The lung is there but it is surrounded by fluids inside the chest cavity ( termed [ pleural effusion ] ( Pleural-effusion ) ) . It 's hard to tell just by the chest radiograph what is the cause of the `` white lung '' . Since this competition is about penumonia , and these patients have a `` Target '' value of 1 , the cause for their big opacity is most probably pneumonia . Meaning - they have a pneumonia associated lung opacity over their entire lung . You can see more examples of `` white lungs '' in [ this radiopedia artice Unclear Abnormality Back to top ] ( Table-of-Contents
1491	Patient 12 - I ca n't see a clear reason for this image to be abnormal . Maybe there are signs of increased vascular markings like Patient 8 ? I 'm not sure . Going through the No Lung Opacity / Not Normal class makes me guess that most of the images in this class are with an unclear abnormality , not something defined . Here is another example
1492	forked from The difference with the original kernel is the switched data . The data on Kaggle is not updated with the recent fixes to some tasks , the up-to-date version is maintaned on the ARC github page This kernel uses the github data , as downloaded on 2020-04-20 , visit the corresponding dataset
1493	If you like the content of this notebook , please consider upvoting it .
1494	Composition of functions
1495	Let 's first write an utilitary function to describe a program as a human readable string .
1496	The evaluation method
1497	The fitness score ( less is better ) of our function will be a 4-dimensional tuple containing the result of each of the fitness functions . We want to be able to compare two score . Unfortunately , the lixocographical order is not adapted , as there is no reason than having a small ` width score ` is better than having a small ` height score ` . We are going to define a partial order that give the same weight to any fitness function . When we compare two tuple with this partial order , ` ( 3 , 2 , 4 , 0 ) < ( 3 , 2 , 5 , 0 ) ` and ` ( 3 , 2 , 4 , 0 ) < ( 4 , 2 , 4 , 0 ) ` . But there is no way to compare ` ( 3 , 2 , 5 , 0 ) ` and ` ( 4 , 2 , 4 , 0 ) ` . We say this two values are incomparable . If two score are incomparable , it means that we can not say that one program is better than the over .
1498	Solve the task
1499	As we can see above , below a certain listing_id value , the interest drops off sharply , and very high listing_id values have no interest at all . Looking at this across time , we see data is even more interesting
1500	lets make it better ! strong text
1501	Ensure determinism in the results
1502	LOAD PROCESSED TRAINING DATA FROM DISK
1503	SAVE DATASET TO DISK
1504	LOAD DATASET FROM DISK
1505	Two embedding matrices have been used . Glove , and paragram . The mean of the two is used as the final embedding matrix
1506	The method for training is borrowed from
1507	Add train leak
1508	Select some features ( threshold is not optimized
1509	Add leak to test
1510	Function which creates final video from list of images
1511	Body of script - change number of patient to create other video
1512	Loading libraries and data
1513	We can conclude , that most of the variables have types of float and integers . Only 5 columns are objects , let 's explore which ones
1514	Seaborn is a great library for visualization . You can choose many palettes , which makes the graphs visually nice . For instance , some of them .
1515	Let 's map index
1516	As a result , we might probably delete some columns without decreasing ROC as they are collinear ( for instance , public , `` =1 electricity from CNFL , ICE , ESPH/JASEC '' and coopele , =1 electricity from cooperative , correlation between these two is - .
1517	Most of train data is allocated around the age of 20 and mean education of 10 years . But we did n't separate data by Target . Let 's do that too .
1518	Multi-Dimensional Reduction and Visualisation with t-SNE
1519	Although t-SNE is a bit unstable ( changing random state may change the pic . ) , from the first glance we can claim that there are some clusters , which can help us to separate class 4 from others . Let 's look at 3d representation .
1520	According to the results , the main feature xgboost extracts is mean education . Then , we observe age , years of education of male head of household squared , overcrowding .
1521	Evaluate the score with using 4-fold TTA ( test time augmentation ) .
1522	Instead of 0.5 , one can adjust the values of the threshold for each class individually to boost the score . However , it should be done for each model individually .
1523	Similar to validation , additional adjustment may be done based on public LB probing results ( to predict approximately the same fraction of images of a particular class as expected from the public LB .
1524	It is very important to keep the same order of ids as in the sample submission The competition metric relies only on the order of recods ignoring IDs .
1525	Lib and Load data
1526	Explore distribution of single variable
1527	winPlacePerc is the target we are going to predict on testing set . distribution on training set is not kind of a 'normal distribution ' but the opposite -- values close to 0 and 1 are apparently more than the middle values .
1528	Here we choose those whose damageDealt is more than 500 to show . We can see above the counts of higher damageDealt smoothly decrease .
1529	PS : DBNO means 'down but not out ' in BUPG , it 's known in experienced BUPG players that many times you may not be able to kill a encounterd enemy but only beat down them , they can still be saved by their teammates .
1530	PS : heals means 'number of healing teammates'.We may naively refer that the more you heal your teammate , the more likely you are going to get a higher rank .
1531	I reaffirm the numbers of matches above to show that the most of killPlace is equal to the number of matches . And the value of killPlace distribute platly from 1 to 95 and slowly decrease when to 100 , which indicates killPlace is the place of gameboard in one match which vary from 1 to 100 . The decrease of 90-100 is caused by the players in one match is not always 100 , 90s is enough to begin a game .
1532	See the variables ' correlation with target
1533	See the non-sparse variables in format way
1534	Getting Prime Cities
1535	This first function gets the distance of all the cities to the current city . If we decide to penalize , we multiply the distance of non-prime cities by $ 1 .
1536	NOTE This NaN handling is just for the sake of it . It is by no-means complete and there are lot of them underneath ( function is built that shows us percentage ) . But there is a specific way that GBM ( light and xBGM ) handle missing values . So even tough it would be better we want to focus on algortihm and automatic feature engineering
1537	NOTE Even tough it is automatic , we can incorporate some manual features . IF we know some domain specific information .
1538	Feature primitives Basically which functions are we going to use to create features . Since we did not specify it we will be using standard ones ( check doc ) There is a option to define own ones or to just select some of the standards .
1539	Label encoding Making it machine readable
1540	NaN imputation will be skipped in this tutorial .
1541	Let us split the variables one more time .
1542	EDA Already some great EDA analysis was made , I will just show the most interesting ( in my opinion ) relationships that can be noticed .
1543	Why 150 000 ? That is the size of one test example that we ought to predict . ANd why negative bvalues of time to failure ? In order to `` approach '' the zero ...
1544	We initialise the Tokenizer ( which is a class from keras package ) , and we stop at the desired number of words that we want to ( en ) -tokenize . `` fit_on_texts '' than makes the representation based on the text we pass ( in our case the Training set ) , than we call a method `` texts_to_sequences '' which performs `` label-encoding '' of our vocabulary ( output of the function below is exactly `` tokenizer.word_index '' ) . In other words we have just represented the desired number of words in a machine readable form . But it is still not good in enough . We can do better that . Thats were the transition to word2vec and other embeddings comes to place . Remember from to theory , sparcity is really bad when our dictionary is large ! One natural question that arises is after we used texts_to_sequences on train_X we used on test and validation data also . But we trained it on the train data set , so what happens if we do not come across a word that was in our train set ? Let us test it
1545	For example glove embeddings -- -not all of the words in the universe will be embedded , hence we will surely not be able to have all of the words of our dictionary represented . But we can reduce the number of such words with some corpus specific analysis . As in [ here ] ( for this competition , and built on top of that is [ this ] ( kernel . In other words there it was searched for OOV-out of vocabulary ( words ) . And we are trying to have less of those . Hence with some text specific analysis ( like the fact that & is in google embedinngs and ? is not we can get rid of such symbols and increase the coverage of our embedding matrix ! But not only that in the following piece of code following things ( among others ) will be conducted Build a vocabulary ( all of the words Than clean it up Clean it of ( out of the place ) numbers , special characters , contractions and incorrect words ...
1546	Save some memory
1547	COOL THING Now the training is conducted with the embedded matrix . Number of rows V is the size of the dictionary and the number of columns D is the desired size . We can learn these representation from scratch OR we can already used pre-trained embeddings were words these values were already optimized to a certain CORPUS ( underneath is the twitter one ) . Below we would see that the word `` out '' already has representation which we can build upon and use to optimise it regarding our own corpus . As we shall do in this kernel .
1548	So which embedding do we use ? Well there are a couple of ways to combine different embeddings , one that comes naturally is taking the mean across all of them
1549	Model is ready , now only thing left to do is to train and make predictions One peculiar thing is that with Pytorch is that it wont go as smoothly as with keras . WE have some manual work do to . Idea [ from
1550	What should good EDA be capable of
1551	Another way to look at the outliers but also in the same time get some more information about distribution ( IQR , median , mean etc ... ) is with the box-plot . But we need to do it efficiently
1552	Correlation map- after throwing the outliers and missing values away ( since it is neccessary before calculating pearson correlation coefficient
1553	As always let us start with EDA , but before that we should load the necessary packages , and connect to the interactive plots plotly .
1554	Reading in the data , as usual .
1555	Another visualization that is useful is value counts of the most frequent words across all of the text . In order to accomplish that first of all we need to extract all of our words and count the number of occurances . ( Note that NLTK library underneath deals with that ) but here we are going to do it `` manually '' and the give it as arguments to our go part of the plotting .
1556	Now for the worldcloud , notice that hpl is for example our text from above ( one of the 3 authors ) . It is one-liner to implement it , and mask changes the image .
1557	Now that we our natural language toolkit , let us go step by step ( even tough we will eventually `` pipe '' it all in the end , i.e . include all of the steps in one Python-class that will do all of the pre-processing at once . But it is important to understand the basics and not look at it as a black-box ! ) . Tokenization We can make the analog code to the str.split ( ) part above but more efficiently with word_tokenize ( ) command from nltk
1558	Stopwords As we can see there is a lot of stopwords to be removed , list-comprehension along with nltk.corpus.stopwords does the trick .
1559	Stemming or Lemming Next pre-processing step . As I mentioned we want to reduce words with the same conotation to the same root . Lemming is not as crude as Stemming hence giving us better results .
1560	Turning words into numbers - Vectorization Now there are is a number of techniques that can be used to make this transition . WE are going to be talking about bag of words approach . ( Name comes from the fact that we just count appearances and nothing else , i.e . just drawing words from the bag and counting ) Steps are as follows Count the words that are to be found in the entire text , build our dictionary in a sense . Record the occurance of that specific word . For every element of the list we will be making a vector that checks the occurance Best way is to look at an Example : ( NOTE that CountVectorizer is from sklearn library
1561	Extending
1562	Applying it on text
1563	LDA Modelling . Finally we are ready to fit the model . There are many parameters that need to be tuned for LDA . As always in order to do it properly fit it we need to have some deeper understanding . Since this tutorial is concerned with implementation , we are going to state hwo we got to the chosen parameters but the reader should explore the Algorithm a bit deeper on tis own . Implementation of LDA can be done with sklearn or gensim . We are going to sitck with skicit learn library . Great source is [ video lecture at Princeton , also offical documentation [ Also to understand what hyper-parameters should be choosen we can consult some of the novel approaches [ or use KMeans + Latent Semantic Analysis Scheme to find out n_components ( number of topics ) whereby the number of Kmeans clusters and number of LSA dimensions were iterated through and the best silhouette mean score . Let us take the `` cooked '' values .
1564	Let us now extract individual topics and build world cloud for them ( for the most occuring words in these topics ) in order to get a better picture what happens in our model .
1565	Features Hilbert transform Hann Window classic_sta_lta Clarification [ consult Various variations of moving averages Regarding MA values and their derivation . WE can gauge the lookback horizont and combinations of MA variables with feature importance . So I did have some baseline , than I saw what variables are the most potent and I played around a bit until I found some indicators . Same logic can be applied to quantiles . After all x is just an series of integer values , and we know from eda that right before earthquake ( but not exactly next milisecond ) accustic values will be huge -- - > in the top of the quantiles . 99 quantile is too much as we can see from graphs but around 95 is the sweetspot -- -- > modify the values of q IMPLICATION Implication of the same thinking that went into modifying quantiles . If we know what we know about values of signal right before earthquake than modifying the values `` std_last_10000 '' and other similiar variables should make an impact . Why ? Simple hypothesis : For example in the last 5000 values of accoustic_signal we will find huge standard deviation . That can be powerful predicator GOAL : I think an avic reader can find a systemic way to gauge these values , and not only trial & error
1566	Submission
1567	Process the training , testing and 'other ' datasets , and then check to ensure the arrays look reasonable .
1568	Read and Explore
1569	Examine how many phases with one ID were labelled problematic . For 80.4 % faulty lines , three phases were all labelled faulty , while one-faulty-phase and two-faulty-phase lines contribute about 10 % and 10 % , respectively .
1570	In this first part we will choose the Time Series to work in the others parts . The idea is to find a Time Serie who could be interesting to work with . So in the data we can find 145K Time Series . We will Find a good Time Series to introduce four approaches ! So the first step is to import few libraries and the data . The four approaches are Basic Approach / ML Approach / GAM Approach / ARIMA Approach .
1571	II . Aggregation & Visualisation
1572	This heatmap show us in average the web traffic by weekdays cross the months . In our data we can see there are less activity in Friday and Saturday for December and November . And the biggest traffic is on the period Monday - Wednesday . It is possible to do Statistics Test to check if our intuition is ok . But You have a lot of works
1573	For this model We will use a simple model with the average of the activity by weekdays . In general rules the simplest things give good results
1574	Prophet is a forecasting tool availaible in python and R. This tool was created by Facebook . More information on the library here Compared to the two methods this one will be faster . We can forecast a time series with few lines . In our case we will do a forecast and a display the trend of activity on the period and for a week .
1575	In this part we will use Keras without optimisation to forecast . It is just a very simple code to begin with Keras and a Time Series . For our example we will try just with one layer and 8 Neurons .
1576	Example
1577	Replace infs and imputing missing values by mean
1578	RF for feature selection
1579	Evaluate the Model
1580	I formulate this task as an extractive question answering problem , such as SQuAD . Given a question and context , the model is trained to find the answer spans in the context . Therefore , I use sentiment as question , text as context , selected_text as answer . Question : sentiment Context : text Answer : selected_text
1581	Load CSV files
1582	Let 's load ` sample_data.json ` , since it contains information about our training data .
1583	We can safely store the two types into separate dataframes
1584	Finally , we will augment the image_df with some information about its host and camera
1585	There is a specific way to load the data in this competition ( give that we have certain constraints and goals that we want to accomplish later on ) and that is
1586	Assumption Data before 2010 will be droped . There is simply to much noise concerning the 2008 crisis . ALSO Since this kernel can not submit anymore and it has a learning character , in order to have sufficient memory , we will take an even smaller subsample of data .
1587	What are the most frequent assets traded ( remember not all are present at all times , bankruptcy , IPO etc
1588	But that does not say us much , we want to know exactly what asset codes are not to be found in the news data set
1589	MICE MICE has become an industry standard way of dealing with null values while preprocessing data . It is argued that by simply using fill values such as the mean or mode we are throwing information away that is present in other variables that might give insight into what the null values might be . With that thought in mind , we predict the null values from the other features present in the data . Thus preserving as much information as possible . If the data is not missing at random ( MAR ) then this method is inappropriate .
1590	TF_IDF Score . In short the most important words ( not too common not to rare ) That could make a difference
1591	Following dictionary will be used for aggregations ( after we merged the datasets further down below
1592	Now lets drop all non-numeric and non-usable features
