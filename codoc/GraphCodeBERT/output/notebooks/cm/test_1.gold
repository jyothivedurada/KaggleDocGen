0	Importing necessary libraries and packages and reading files
1	Aligning Training and Testing Data
2	Handling missing values ( using Iterative Imputer ) prior to outlier detection
3	OUTLIERS DETECTION
4	Missing data in application_train
5	Duplicate data in application_train
6	Distribution of income
7	People with high income tend to not default
8	Distribution of credit
9	Distribution of loan types
10	Distribution of NAME_INCOME_TYPE
11	Who accompanied the person while taking the loan
12	Distribution of AGE
13	Feature Engineering of Bureau Data
14	Using Previous Application Data
15	Using POS_CASH_balance data
16	Let 's drop lightGBM
17	Once again we 'll snoop the test , to get a better score on LB . Clean solution would rank holdout set , train on it . Then match values from test set to ranks in holdout and use them to predict target .
18	Quadratic linear stacking
19	Lorgistic regression with linear stretch on pca + rank features
20	Feature-Weighted Linear Stacking
21	Let ’ s say you want to do 2-fold stacking Split the train set in 2 parts : train_a and train_b Fit a first-stage model on train_a and create predictions for train_b Fit the same model on train_b and create predictions for train_a Finally fit the model on the entire train set and create predictions for the test set . Now train a second-stage stacker model on the probabilities from the first-stage model ( s ) ( using CV ) .
22	Scale and flip
23	Estimators Ridge regression
24	has a nasty feature that it takes only DMatrix as arguments therefore predict method has to be wrapend into a function .
25	All three datasets needed because we need to calculate sales in USD .
26	S - sequence length weights
27	W - USD sales weights
28	Comparison to the Original weights
29	Load wieghts for WRMSSE calculations
30	Create fake predictions
31	Clustering with DBSCAN
32	To check direction of the gcoups , export to excel , sort all rows on last column 'labels ' .
33	calendar.csv ` - Contains information about the dates on which the products are sold . sales_train_validation.csv ` - Contains the historical daily unit sales data per product and store [ d_1 - d sample_submission.csv ` - The correct format for submissions . Reference the Evaluation tab for more info . sell_prices.csv ` - Contains information about the price of the products sold per store and date . sales_train_evaluation.csv ` - Available one month before competition deadline . Will include sales [ d_1 - d
34	andrews curves for six random Items
35	autocorrelation plot for a single random item
36	lag plot for a single random item
37	Load our data
38	On analysis we find that 7095 entries for RLE based encoded strings with roughly highest precentage of type 3 defect
39	Now as sex has two unique values , male and female.It becomes difficult to impute missing values for this feature . One method is to use mode . i.e male in this feature . Another is to relate it with other variables.We 'll try this method and see if we can find something .
40	The risk of melanoma increases as people age . The average age of people when it is diagnosed is 65 .
41	Let 's look at some images
42	By looking at the above image , we se that Nevus , Keratosis , lentigo .These all are non=cancerous . Melanoma is seen in red colour and malignant . We 'll study some of these lesions below for better understanding by looking at the images .
43	Evaluation on test dataset
44	Loading the data ..
45	What happens if we see moving average
46	Importing the Dataset
47	Visualisations There are alot of ways to visualize but lets use the simple ones .
48	Train Models + Predict by Store ( nlag
49	magic number 1.00 ( None
50	Let 's look at our target variable
51	The distribution is right skewed . Let 's log transform the variable .
52	Let 's just build a model using the mean num of rooms for the missing values
53	View Correlation Heatmap Pre-processing
54	Process Market Data - Drop Rows with NaN Values
55	View Final Correlation Heatmap before Training
56	Early stopping callback
57	Simple ConfidenceValue Creation Function from Prediction Values
58	Dropping Null values for song_length and language
59	Import training data
60	Define peak finding function on histogram of a series
61	Yards - the yardage gained on the play ( you are predicting this
62	Make histogram for Numeric Varaiable
63	MONK ] ( MONK Exploratory Data Analysis/ Data Visualization ] ( dv Installing Monk ] ( installingmonk Importing Pytorch Backend ] ( pyb Creating and Managing experiments ] ( cme Quick Mode Training - Load the data and the model ] ( train EDA Using Monk ] ( edaM See what other models Monk 's backend supports ] ( mod Train the classifier ] ( tc Running inference on test images ] ( inf
64	Viewing 4 Train images
65	Viewing 4 Test images
66	Adding extension .jpg to image_id and adding corresponding label to Category
67	Imports
68	Quick mode training Using Default Function dataset_path model_name num_epochs
69	Select image and Run inference
70	Running Inference on all test images
71	Finally fit the model
72	Create predictions.csv
73	In below plot , we can see visually that there appears to be a pattern between open_channels and corresponding signal
74	Apply offset = 2.74 and check mean value of signal per channel
75	We have training data containing both cat as well as dog image . Let us make a list of cat and dog images from the train data and store them separately . Getting the dogs and cats data sorted from the training dataset using list comprehension will do the trick for now .
76	Seeing a sample image
77	The 3 at the end signifies that the image has 3 channels , each for red green blue Incase of grayscale images , there is no need for such three channels . Below is a quick implementation of it ,
78	Resizing the photos
79	The two variables , ` train_data ` and ` test_data ` will be used for storing the modified prep data generated from ` train_images ` and ` test_images
80	Since train_data is a list , let us convert it into a numpy array . I will name it X_train for convenience .
81	Unlike the MNIST dataset , which has a separate column ` label ` depicting the outcome , we have no such column in this case . However , in the filepath names , each file in train.zip folder has 'dog ' or 'cat ' being written in it 's filename . The same is not true for testing images for obvious reasons . You can confirm this with the code below . Let 's start making the ` y_train
82	Choosing a model
83	Compile the model .
84	Fit the model . Since train_data has no need of splitting into X_train and y_train ( due to there being no labels in train_data ) , we can safely conclude ` X_train ` would be = ` train_data ` . For readability , we 'll copy the elements of it anyway in a new varible ` X_train ` and use it for fitting .
85	Let us visualize some of the predictions the model made .
86	We will first remove the zipped files , as they fill up the output section and we are able to see no .csv file . We later make the .csv file 'submissions
87	In order to create BigQuery ML models , you 'll have to link a Google Cloud Platform account . Linkacct.png ] ( attachment : Linkacct.png After you 've connected an account , you 'll need to [ create a project in Google Cloud Platform ] ( and replace the placeholder below with your project ID . At which point , you can create a BigQuery dataset to store your ML model . You do n't actually have to upload any data ; the only table in it will be the one with your trained model .
88	load_ext ` is one of the many Jupyter built-in magic commands . See the Jupyter documentation ] ( for more information about ` % load_ext ` and other magic commands . The BigQuery client library provides a cell magic , bigquery ` , which runs a SQL query and returns the results as a Pandas DataFrame . Once you use this command the rest of your cell will be treated as a SQL command . ( Note that tab complete wo n't work for SQL code written in this way . Here 's the the query that will train our model
89	Step three : Get training statistics
90	Step four : Evaluate your model
91	Step five : Use your model to predict outcomes
92	Step six : Output as CSV
93	application_trainにSK_ID_CURRをキーにして結合するだけです how='left ' ` という引数が与えられている点に注目です。SQLを使ったことのある方はイメージしやすいかと思いますが、引数として与えられているデータセットのうち、左側のファイルを軸にデータセットを結合していくという意味合いです。右側のデータセットに該当する値が含まれていない場合は、欠損値になります SK_ID_CURRのデータセットのみが返ります。ここでprevious_loan_countsには申込が0回のSK_ID_CURRは含まれていないので、データセットの欠落が発生する可能性があります application_trainの行数が減るのは学習用データセットの量を保つ観点から望ましくありません。application_test.csvを扱う場合は、予測すべきデータセットが欠落する事態を招いてしまいます
94	Let us visualize a few samples from the dataset .
95	TPU preparation
96	Important constants
97	Loading data
98	Preparing train and validation sets
99	Model architecture
100	callbacks
101	Read in Libraries
102	Read and Clean Data
103	Exploratory data analysis
104	ExtraTreesClassifier
105	LightGradientBoostMachine
106	It can be observed that test data have fewer columns . It does not contain dropoff datetime and obviously the target variable ( trip_duration ) : ) .
107	Now it looks better . We can perform some additional analysis on it
108	It looks like a right skewed distribution . One can apply logarithm to make it normally distributed .
109	Visualize distribution of pick up hour
110	The distribution shows the car demand with pick up hour time . After mid night less number 's of trips are taken . Now let us see how the trip duration changes with respect to trip time .
111	Next , read the DonorsChoose training data into a ` DataFrame
112	We can see that the minumum number of previously posted projects for a teacher is 0 , the maximum number is 451 , and the mean ( average ) number is 11.23 . Let 's visualize the distribution using a histogram , to get a better sense of the spread .
113	We can see that the vast majority of examples have a ` teacher_number_previously_posted_projects ` value between 0 and 10 , with a sharp dropoff thereafter . However , if we rebucket our data into two bins ( & lt ; 10 and & ge ; 10 ) , we can see that there 's a substantial long tail of examples with previously-posted-project values greater than
114	Build an Initial Linear Classification Model
115	If you did n't import the DonorsChoose training data above , do so now
116	Next , define the feature ( ` teacher_number_of_previously_posted_projects ` ) and label ( ` project_is_approved
117	Then split the data into training and validation sets
118	Then set up the input function to feed data into the model using the [ Datasets API
119	Next , construct the ` LinearClassifier
120	Create input functions for training the model , predicting on the prediction data , and predicting on the validation data
121	Next , let 's calculate the [ AUC ( area under the curve ) ] ( which is the metric this competition uses to assess the accuracy of prediction . This may take a few minutes . When calculation is complete , the training and validation AUC values will be output
122	Read in the data and clean it up
123	Split the data
124	Read in the data and clean it up
125	Read data
126	Clean Data This is to extract the pure words from the texts
127	Replace the value of ` images ` with any of the other possible options in the next code block
128	The following function downloads the image dfrom the given link ` url ` and resizes ( standardizes ) it to a square of 256x
129	Lets download the image of Acropolis-Paratheon and then plot it using ` matplotlib
130	Imports and problem constants
131	Config and hyperparameters
132	The output is once every 100 batches . Scores for mean and `` best '' result are shown according to how the GB sees it - with soft constraints and accounting costs ramping up slowly ( so they are not relevant to the full problem until after the ramp up batches ) . The score for the greedy solution is always evaluated using the full penalties and accounting . It starts at around 27 billion
133	Train the LightGBM model
134	A . TEST WORDCLOUD
135	B . TARGET DISTRIBUTION
136	D. Multinominal NB
137	E. RF
138	F. MLP
139	G. LoggicReg
140	EMBEDDING METHODS_WORD2VEC A . INTRODUCTION Now you know how to deal with texts . At first step , we need to extract features from text ; Then there is a need to investigate classifiers and tune them for getting better accuracy . In previous sections you trained a classifier based on the count vectorizer . Now , lets move on to newer vectorizing method . The name is Word2Vec . As its name is simply representing , by using this manner any word will be replaced by a correspondence vectors . In this manner , training parameters are playing important role . lets try word2vec on our dataset to understand what does it means . As these processes are time consuming , I have simulated these classifiers on my local machine . Committing these codes ( In each commit waiting until finishing whole the classifiers was really annoying for me ) so I have commented the lines which are representing training process . You can easily uncomment them and try to train classifier by yourself ; - ) . B. PYTHON TIP . USE GENERATORS TO AVOID MEMORY ERROR To train a model on text , we need pass following steps Reading text and keeping in memory , Tokenizing the text Iterating on specified window In use cases such as texts ( especially large texts ) keeping whole the text in memory is memory consumable . Using generators , can reduce the overhead of iterating . Python generators are a simple way of creating iterators . All the overhead we mentioned above are automatically handled by generators in Python . Simply speaking , a generator is a function that returns an object ( iterator ) which we can iterate over ( one value at a time ) .
141	Training Word2Vec . We will ignore words which have repetition less than 5 times and scrolling default window size equal to 5 . Changing these parameters can change our model accuracy . We will check it in next steps .
142	C. GLOVE INITIATIONS .
143	EMBEDDING METHODS_PROGRAM
144	EMBEDDING METHODS_GOOGLE_NEWS_VECTORS
145	CONCLUSION ON TRADITIONAL METHODS
146	Now , Lets remove variables for having clean workspace for rest of the kernel .
147	Same to the process we had done in countVectorizer , we need to tokenize the text and convert them to list of integeres . But how Keras provides the tokenizer method for doing it easily .
148	Now , you can see the issule has been solved . Now , whole the X_train has the same dimention . Now , it can be passed to the classfieirs . We have used CuDNNGRU which is Fast GRU implementation backed by cuDNN . So , first of all turn on GPU for your kernel in the your kernel setting . If you are dealing with a system which is does not contain GPU , replace the CuDNNGRU with LSTM method . more information could be found [ here .
149	INTRODUCTION AND ROADMAP In this challenge we will deal with the stock data . There are two datasets . Marketdata_sample and News_sample . contains financial market information . Features like opening price and closing price and this sort of things are existed in dataset . News dataset contains information about news articles/alerts published about assets . Attributes like asset details are located in this dataset . The main goal of the competition is how we can use the content of news analytics to predict stock price performance . Now , we will try to analyze both of datasets feature by feature . In first step we will do study on market data and try to visualize features and extract information from data . In the second step we will concentrate on news data and it 's features . In the third section relation between two dataframes will be validated . Features correlations will be discussed there . Finally in forth step we will try to address the competition challenge goal and do submission based on the EDA we will have in this kernel . Any comment , idea or hint will be appreciated . Your upvote will be motivation for me for continuing the kernel
150	Reading the data and understanding the data .
151	There are 3780 assetCodes . the relation between these codes and the correspondence assetCode which is in news_df can bring considerable information . we will check it in next sections . As it can be seen , there are assetsCode which repeated in 2498 records . The violion chart maybe represents more clear information about assetsCode distributions .
152	WordCloud can represnets more detailed information .
153	There is considerable std in the volume feature . On the other hand , the 75 % percentile represents that the gap between the last quater of volume is very bigger than first quater . As a result , if we plot the violin diagram , It has considerable mass in first quater . Lets validate it ...
154	Our assumption has been approved .
155	G. OPEN the open price for the day ( not adjusted for splits or dividends
156	F. RETURNS Returns are calculated based on different timespans . creating
157	Is there any relation between returnsClosePrevRaw1 and returnsOpenPrevRaw
158	Doing the similar analyze on returnsOpenNextMktres10 , we have
159	H. PROVIDER dentifier for the organization which provided the news item ( e.g . RTRS for Reuters News , BSW for Business Wire
160	Another userful visualization for this feature can be wordcloud .
161	Similar visualization like previous feature can be done on the 'audiences ' feature .
162	O. HEADLINE_TAG The Thomson Reuters headline tag for the news item
163	DATA ANALYSIS A . IMPORTS Importing packages and libraries .
164	F. DEVICE device is stored in json format . There is a need to extract its fields and analyze them . Using json library to deserializing json values .
165	I . TOTALS
166	Extracting all the revenues can bring us an overview about the total revenue .
167	Aggregation on days and plotting daily revenue .
168	Lets find most_common and least_common visitNumbers for being familiar with collections module and its powrefull tools
169	It is clear that the dispersion of the 'visitNumber ' per session is huge . for this sort of features , we can use Log and map the feature space to new lower space . As a result of this mapping , visualization the data will be easier .
170	As it is completely obvious in source diagram , google is the most repetitive source . It would be interesting if we replace all google subdomains with exact 'google ' and do the same analyze again . let 's do it .
171	Google dependent redirects are more than twice the youtube sources . Combination of this feature with revenue and visits may have important result . We will do it in next step ( when we are analyzing feature correlations ) . Now let 's move on keywords feature . A glance to keyword featre represnets lot of missing values ' ( not provided ) ' . Drawing a bar chart for both of them ...
172	L. FULL_VISITOR_ID Now , lets see how many of users are repetitive ? ! This feature can represent important information answering this question ? ( Is more repeation proportional to more buy The response will be discussed in next section ( Where we are analyzing compound features ) but now , lets move on calculation of repetitive visits percentiles .
173	So , we have 366 days ( 12 month = 1 year ) from August 2016 to August 2017 data for churn rate calculations . The bes period for churn maybe is the monthly churn rate . Now , lets list all the months existed in library for checking having no missing months .
174	Whole the period exist in data . So , Lets define new empty dataframe and do this calculations on it and copy the our requirements ot it . for churn rate calculations , we need to check which users visited have visited the website monthly . this information is located in fullVisitorId . we will copy it to new df .
175	So , we have 12 list and each elemets contains the users who visited the website on the correspondence period . Now its time to do some matrix calculation for filling the churn-rate matrix . It is very probable that you calculate the matrix with more efficient ways . I used this manner for more simplicity .
176	A churn-rate heat map is the one the important keys of business . The more repetitive users in continues time periods , the more success in user loyalty . Generaly it is better to drop the zeors below the main diagonal for better visualization and more clearer representaion . I could n't find the sns pleasant visualization for half churn rate matrix . If you find it , it will be appreciated to ping me . Ill replace the below diagram with your recommendation as soon as possible ) .
177	B. REVENUE & DATETIME Now , it is time to move on to analysing compound features . The main target of this section is undestanding the features correlation . At the first point , lets analyze this probable assumption Is more visitNumber proportional to more Revenue
178	Replacing NaN variables with 0 ( It may have positive/negative effect . We will check it later ) . Another point we must touch on is we need to convert
179	Using train_test_split for splitting data to train and evaluate sets . Converting revenue ( our target variable ) to float for performing regression .
180	Congrate You get reasonable rmse in first step . Now , lets predict the revenues for our evaluation dataset to be familar with transformation needed . Point that the revenue of user cant be negative ; - ) so , remove them ) .
181	A . INVESTIGATION OF FEATURE IMPORTANCE LightGBM have a method for representation of feature importance .
182	Original Lung CT-Scan
183	Mask using U-net ( R231 ) Lung CT-Scan
184	Let 's try the most famous random train_test_split and look what are the partitions for every class triplet .
185	As you can see , some validation triplets represent from 3 % to 20 % of the whole set , which seems to me not very stratified Let 's see how other method works .
186	And last method which stratify dataset which as for me suits much better .
187	Preprocessing like However , instead of Tf-idf encoding of text columns , I tried to convert to vw-format . After that I can use default tricks from LinearModelsWorld like momentum/adaptive learning rate/hashing trick etc . Moreover , the addition of various interactions between the features will become available simply by specifying the necessary parameters . I. Preprocessing
188	Function for calculation rmse without loading to memory
189	Apply postproc procedure to second stage data
190	Utils and imports
191	Let 's check the distribution of document size
192	Essentially most of the answerable questions are n't a yes/no type . Distribution of short answers
193	Using BigQuery Dataset
194	Create BigQueryML Model
195	Check models in bigquery console
196	TPU Strategy and other configs
197	Load Model into TPU
198	Exploration of the Dataset
199	Test Images Display
200	Index Images Display
201	Train Images Display
202	Most frequent landmark ID
203	Least frequent landmark ID
204	First , we 'll need a way to check what files are in the archive . The zipfile module let 's us look into the archive without loading the whole archive into memory or unpacking the entire archive ..
205	Now that we have the list of files , we can access individual files by name without loading the whole archive into memory or unpacking the entire archive ..
206	Read all the files available in this kernel
207	If you want to know when the files were last modified
208	csv File
209	xlsx File
210	sqlite File ( .db
211	image Files ( jpeg , jpg , png , etc ..
212	this piece of code is taken from
213	Having MATLAB 2014b or newer installed , the [ MATLAB engine for Python ] ( could be used
214	html File
215	nii File
216	json File
217	Fetch brain development functional datasets
218	Finally , sklearn models generally do n't accept strings as inputs , so we 'll need to drop all string columns . This includes the original text ' column that we read from the csv
219	If you want to experiment with different combinations of features , try writing your own transformers and adding them to the pipeline . If you 're running this at home , expect this next step to take ~30 seconds or so as we 're retraining the model several times during the cross validation .
220	This pipeline is better ... but only just barely . I 'll leave it as an exercise for you to add better features and more powerful models . However , if we did want to submit this , we 'd just feed ` logit_all_features_pipe ` into the ` generate_submission_df ` function .
221	Importing all the libraries that we will need
222	Join Data
223	Join Train and item data
224	On promotion NAN values are UNKNOWN - if item is on promotion So replacing Nan of `` on promotion '' with 2 to indicate the items have unknown status on promotion
225	Plotting Oil Price
226	Null Hypothesis H0 = Promotion and Sales are independent from each other . Alternative Hypothesis HA = Promotion and Sales are not independent of each other . There is a relationship between them . Promotion - categorical variable - Independent variable Sales - continuous variable - Dependent variable Now , to determine if there is a statistically significant correlation between the variables , we use a student t test sample t-test : testing for difference across populations
227	ANSWER The amount of halite mined per step R is therefore R ( n_1 , n_2 , m , H ) =\frac { ( 1-.75^m ) H } { n_1+n_2+m
228	We can use scipy.optimize to calculate the maximum point on this graph for various travel distances . A couple of things to note Only the total travel distance $ n_1+n_2 $ matters , not the individual distances . The amount of halite in the cell does not affect the optimal number of steps , only the resulting average halite per step Therefore , for each total number of travel steps , we can compute the optimal number of steps , using the scipy.optimize function
229	Loading Library and Dataset
230	Understanding the Model Better To get an overview of which features are most important for a model , we can plot the SHAP values of every feature for every sample . The plot below sorts features by the sum of SHAP value magnitudes over all samples , and uses SHAP values to show the distribution of the impacts each feature has on the model output . [ ( source ) ] ( The color represents the feature value ( red high , blue low ) . This reveals for example that a high var_139 lowers the probability of being a customer who will make a specific transaction in the future .
231	We can also plot a tree from the model and see each tree
232	The heatmap shows the zones of high concentration of popular drinking establishments .
233	No surprises here , the majority of parties occur on the weekends and during night hours . And the last plot shows the cyclic nature of the data , peaks represent the weekends . The beginning of the summer and beginning of the fall have the highest number of complaints , surprisingly the number of calls drops in August .
234	Define Hyperparameters for LightGBMClassifier
235	Number of Team Members
236	create submission file
237	Getting to know the data
238	For supervised learning classification problem target variable is hospital_death . Solution_df has encounter_id and target variable ( NaN ) . It has same no of observations as test_df . test_df shows NaN for hospital_death for top 5 rows . There are 39308 observations and 186 features . Out of 186 features , 170 are float and 8 are int . Rest 8 are object type . train_df shows 0 for hospital_death for first 5 rows . It has 91713 observations and 186 features . Out of 186 features , 171 are float and 7 are int . Rest 8 are object type . From 2 and 3 above , it looks like 1 int of train has been changed to 1 float of test because of values of hospital_death target variable . Let us confirm the same .
239	We need to split data into Training set ( 80 % ) and Validation set ( 20 % ) . We will use Validation set for prediction and deciding from it score which of numerous models/approaches work better .
240	IF we use Machine Learning on data having missing values , we will get Errors . To handle that we have 2 approaches Drop all columns with missing values . But it leads to data loss especially if there are a lot of missing values . Imputation - Fill some values . For categorical , we can fill with lost frequent value for that column . For numerical , we can fill with mean or median value for that column . New values filled may be far away from what actual values should be . It is of two types . Simple Imputation - fills some value and does not remember which all positions had been missing . Imputation with extension - fills some value and remembers which all values aere missing . New columns are created to store which positions had missing values .
241	Keep 98 % of samples
242	The images are labeled as 0 or 1 , where 0 = No Tumor Tissue and 1 = Has Tumor Tissue ( s
243	Create a DataFrame of all Train Image Labels
244	See the distribution of Train Labels
245	Take 80K images from both categories
246	Split into Train and Validation Sets
247	Create our Model ( CancerNet
248	Specify optimizer and loss function
249	Load the saved weights
250	Let 's plot our ROC Curve
251	Move the Test images into a directory 'test_dir
252	Extract ID field from Test Image file names
253	Make Submission File
254	Correlation Graph
255	Attribute : Bathrooms , Bedrooms
256	Some functions to make life easier
257	One-hot encodings and basic statistic based on categorical column 'wheezy-copper-turtle-magic
258	Imports and utils
259	Load train data
260	Let 's use subset of training data due kaggle kernel power constraints
261	Let 's make group hold-out validation with 25 % validation size
262	Fit model on all generated features
263	Function returns base_score ` : score on original features score_decreases ` : list of length ` n_iter ` with feature importance arrays
264	The target variable of our data i.e Cover_Type is one hot encoded . We replace the foresteric values back to their original values for better analysis .
265	The dataset consists of four wilderness areas located in the Roosevelt National Forest of northern Colorado .
266	There are a total of 40 Soil Types in our data .
267	Elevation Histogram All the Forest cover types are highly distributed . The above plot shows us that Cottonwood tress grow at a lower elevated regions compared to Krummholz , Spruce and Lodgepole tress grow at higher elevation . And by looking at the rug plot above the histogram , we can see similarities in Cottonwood , Douglas fir and Ponderosa pine trees . Same with Aspen , Lodgepole Pine and Spruce forest covers .
268	According to the information found on the website , aspect identifies the direction of the downhill slope faces . It is measured clockwise in degrees from 0 ( due north ) to 360 ( again due north ) , coming full circle . Flat areas having no downslope direction are given a value of -1 .
269	All the trees have thier average slope in range 10 - 20 as seen in below bar plot .
270	Horizontal Distance to Hydrology
271	The best way to check outliers and compare the medians is a box pot .
272	With this boxplot , we can see a lot of outliers in Aspen , Lodgepole Pine , Spruce and other trees . Many of the Logdepole , Krummholz are located closer to surface waters , some are located at a distance more than 1500 . But we are not able to compare the medians . We 'll look at the bar plot that uses median as estimator .
273	Vertical Distance to Hydrology
274	Horizontal Distance to Roadways
275	And by looking at the below barplot , we justify the above statement .
276	Horizontal distance to fire points ..
277	Hillshade at 9am ..
278	Here we see a left skewed histogram with all the trees having almost highest shade index at 9am .
279	Hillshade at Noon
280	Hillshade at 3pm
281	Why it works so good
282	Parameters of GAN
283	Examples of dogs
284	Choose one of datasets and reduce amount of columns
285	Non-linear dimensionality reduction to 2 dims
286	Non-linear dimensionality reduction to 3 dims
287	Upon seeing any abnormality in the PSA and DRE test results , doctor recommends Prostate biopsy . Q ( 8 ) So , what is Prostat biopsy and how it is done The decision to have a biopsy is based on PSA and DRE results . Your doctor will also consider your family history of prostate cancer , ethnicity , biopsy history and other health factors . A biopsy is a type of minor surgery . For a prostate biopsy , tiny pieces of tissue are removed from the prostate and looked at under a microscope . The pathologist is the doctor who will look carefully at the tissue samples to look for cancer cells . This is the only way to know for sure if you have prostate cancer . It looks scary ! ! ! do n't be scared watch this video
288	Now , let 's deep dive into the Data
289	Now , let 's check the number of images , types of data provider , ISUP Grades and Gleason Scores .
290	In Training set , Biopsys are of more ISUP Grade 0 and ISUP Grade 1 .
291	Gleason Score 0+0 and 3+3 contiribute more to the Training dataset
292	Displaying few images
293	Exploring images with pen markers
294	Time to plunge into the code
295	Examples of data
296	Input dim is ` 64 x 64 x Latent dim is ` 4 x 4 x Bottleneck is 24 times smaller than input image ! Autoencoder should keep most important information
297	Ensure determinism in the results
298	LOAD PROCESSED TRAINING DATA FROM DISK
299	SAVE DATASET TO DISK
300	LOAD DATASET FROM DISK
301	Two embedding matrices have been used . Glove , and paragram . The mean of the two is used as the final embedding matrix
302	The method for training is borrowed from
303	Headshots Rate
304	Roadkills Rate
305	Heals/Boost and WinPer
306	Number of enemy players knocked by Number of enemy players this player damaged that were killed by teammates .
307	Heals/Boost and walk distance
308	In this notebook
309	Create a Merchant Address variable
310	How to use Sales and Purchase Lags
311	What is numerical_1 & numerical
312	Observation 1 . Majority of points in the first bin Observation 2 . Outliers with very high values compared to the mean
313	Let 's investigate the outliers : Threshold for outliers is rounded to the first bin interval Let 's check how the other variables beheave in case of outliers .
314	Rating for Merchants
315	image.png ] ( attachment : image.png
316	Prep categorical variables
317	Simple Linear Regression Model
318	Loading the data
319	Retrieve list of elemental Properties
320	Using Catigorical Features
321	Correcting the distribution of the target variables
322	The performance metric for this competition
323	Standard Dense Nerual Network Implimentation
324	To find values for sklearn `` like '' packages , I used sklearn-optimization for Baysian Optimization
325	I thought it might help to make the 'SM ' features more acessible to some people by reducing the amount of data ( I 'm also casting the values down to floats32 to reduce data - hopefully not losing too much information ) . In neuroimaging parcellations of the brain are often used for this and average values per parcellation are extracted . This way we can get to lower resolutions ( i.e . 53 x 400 in this case ) . Using nilearn masker objects it is also possible to back-project the data into 3D / 4D space , but at the much lower resolution . For simplicity - and to add the participant Id , I am also flattening the data you can recover the structure by reshaping to ( 53 , 400 ) . Using joblib , we get a datasize of around 1GB - which is nice .
326	The popularity over years plot implies that the mean popularity of movies is increasing over year . Year 2017 has a high mean popularity in the train dataset
327	Json Format Columns to Dictionary Format
328	Feature Engineering
329	While waiting for Random Search to do the job , watching Panda is a very productive thing to do ... ! Got ta love pandas
330	static FNC correlation features for both train and test samples First up , for people who do not know a lot about brain imaging studies . What is resting state In resting state measures participants lie in the MRI scanner with their eyes opened or closed ( there is/was a huge debate on what is best way ... ) and are told to do nothing in particular . Sometimes with the addition not to think about anything specific or meditate . This usually goes on for several minutes - it can be really hard not to fall asleep - trust me . While participants lie in the scanner , typically the [ Blood Oxygen Level Dependent ] ( signal is measured , which serves as a proxy of brain activity . This the `` task '' sounds really error-prone and unspecific , this kind of measuring has been the key for many insights in human neuroscience and the results are surprisingly stable . In resting state you have 4D data , a 3D image ( made up of voxels ) measured over time . What has been done The second set are static functional network connectivity ( FNC ) matrices . These are the subject-level cross-correlation values among 53 component timecourses estimated from GIG-ICA of resting state functional MRI ( fMRI ) . This is different - but not uncommon - to typical resting state function connectivity . In classical Functional Connectivity you would use a brain atlas , and average all voxels in a brain region . Then you calculate the correlation ( or other measures ) between the different brain regions and get a brain connectivity matrix . Here an example of an atlas ( well one you would n't typically use , but it serves the purpose
331	For every feature we get a description of the connections ( i.e . which component with which component ) . Let 's extract the different names .
332	fMRI_train - a folder containing 53 3D spatial maps for train samples in .mat format We are doing a few leaps and jumps here , but all these aspects of the data are connected . If I understand the description correctly this data is again something different from the functional network connectivity above . While the networks in ` fnc.csv ` has been estimated on a different dataset , the ICA maps in the different mat files seem to be estimated from resting state of each participant The third set of features are the component spatial maps ( SM ) . These are the subject-level 3D images of 53 spatial networks estimated from GIG-ICA of resting state functional MRI ( fMRI ) . Let 's load some data , and maybe we can fill the gaps in the numerated list above .
333	Target variables - age and assessments
334	Correlations in domain1 is quite high , so dropping one might be quite safe . For domain2 the two variables are not too much correlated , which is unfortunated . For a better split I will , however , drop domain2_var1 from the stratification , as domain2_var2 has less correlation with age .
335	Not great , but could be sufficient .
336	Applying dimensionality might help us find multi-variate patterns
337	Using an elbowplot on the explained variance scores , doesnt really provide us with too much hope , to really find interesting patterns in the PCA analyis . Explained variance of the different components seems to taper off really quickly . Only the first one explainin a reasonable amount .
338	I 'm trying to make this approach a bit easier . Nilearn does not only have great plotting capabilites , but I can also be used to extract great amounts of data . I mentioned atlasses and parcellations in the FNC section before . To reduce the huge amounts of data in the SM files , I am using a relatively low parcellation to extract for each of the networks average values per region . In an ADHD prediction challenge , I had the feeling that too high resolutions are not really helpful , so I am going with the middle way . I am not too familiar with the new trend in atlasses - so I took one I heard about at one time .
339	There are 197 regions in the data . We have 53 components , so we get to 10441 different features . Which is a lot less than using all pixels , but still a lot . To extract the data , we can use a masker function from nilearn
340	So now we have the data ... But I am not really sure what do to with this . Let 's try feature reduction and look at some correlations .
341	The correlation game
342	Based on the elbow plots the explained variance ratios are decreasing quite quickly , which is expected based on the input-data .
343	Well for version 11 I said this is the final version . But sometimes I also like to run some basic models Also to see if the partition we are using helps us in terms of getting a good estimate of the prediction error . I am not going to use the ` SM ` maps , only loadings and FNC data - it 's easier I guess .
344	As a very baseline model let 's use a RidgeRegression - I am not the biggest fan of SVMs ( do n't really know why
345	Estimate parameters of Cox proportional model
346	So , we have 176 unique patients ids . Rest of them are essentially the week-by-week record of these patients . Now we can analyze the data in two ways We can use all the data We can use the data with uniqe ids . Let us first make a new dataframe with unique ids and also make a columns expressing frequency of each patient
347	We can see that on an average , patients had around 9 appointments to get their FVC tested . Some had fewer than that , but most of them had close to 9 appointments . Let us explore other features in the dataset . Data distribution by Sex ( uniques values
348	Since this is real medical data , you will notice the relative timing of FVC measurements varies widely . The timing of the initial measurement relative to the CT scan and the duration to the forecasted time points may be different for each patient . This is considered part of the challenge of the competition . Let us undestand more on how our data is distributed
349	Citing this [ research paper ] ( women have 20-25 % lower capacity than men , owing to their smaller lungs . However , I have one concern about the data shown above . As seen from the boxplot , men and women who currently smoke have higher FVC than ex-smokers and non-smokers which is contradictory to this [ research paper ] ( which states that Some previous studies have demonstrated the effect of smoking on the pulmonary function of adults8,9,10 ) . They showed that smoking decreased pulmonary function including forced vital capacity ( FVC ) , forced expiratory volume in one second ( FEV1 ) , FEV1/FVC , and forced expiratory flow at 25–75 % ( FEF25–75 % ) 9 ) . Cigarette smoking causes deficits in both FEV1/FVC and FEF25–75 which indicate airway obstruction and small airway disease in adult smokers . These works suggest otherwise , which should be a cause of concern . I will read more and try to find the caauses , etc behind this . Data distribution by Weeks This will give us an information on weekly visits by patients
350	The distribution shows that most of the people have had their measurements done in 0-10 weeks . This indicates two things Either patients are reluctant to visit hospitals to get their FVC mesured . Or , the health of patients deteriorated over the course of weeks and gradually death would have occured . FVC vs percent Let us see what is the relationship between FVC and Percent
351	DICOM® — Digital Imaging and Communications in Medicine — is the international standard for medical images and related information . It defines the formats for medical images that can be exchanged with the data and quality necessary for clinical use . DICOM® is implemented in almost every radiology , cardiology imaging , and radiotherapy device ( X-ray , CT , MRI , ultrasound , etc . ) , and increasingly in devices in other medical domains such as ophthalmology and dentistry . With hundreds of thousands of medical imaging devices in use , DICOM® is one of the most widely deployed healthcare messaging Standards in the world . What is the data related to DICOM A DICOM data object consists of a number of attributes , including items such as name , ID , etc. , and also one special attribute containing the image pixel data . A single DICOM object can have only one attribute containing pixel data . For many modalities , this corresponds to a single image . However , the attribute may contain multiple `` frames '' , allowing storage of cine loops or other multi-frame data . Let us start by importing all the dependencies
352	Rescaling to Hounsfield Units
353	Usually , in neuroimaging we want to make sure that the images are in the same space . The code here provides a nice code snipped to resample the images . Again this requires some information which is stored in the DICOM files . I adapted the code from the tutorial so that it works for 2D . Normaly these approaches are done in 3D . However , in this challenge we are tasked to classify slices , so using a whole volume of scans might not be possible . The information is stored in ` dicom.PixelSpacing ` . I decided to go for isotopic pixels of 1 by 1 mm . Note : I am now drawing random images for visualization . I am also windowing the image , so that we can see something in the image .
354	Cropping the images can now help us to create better or nicer images for a later deep neural network ( or what ever
355	Bring images back to equal spacing
356	Let 's have a look at the different distributions . Looking at the t-values ( of course almost everything is significant ~.~ ) , we find the highest difference in distributions around HU 24 - 32 , and 44 to 56 . These values roughly correspond to older hematomas and blood . Which we would expect , when looking for hemorrhage . Maybe the image histograms can already help us to classify the presence of an hemorrhage Table from above Substance | | HU Subdural hematoma | First hours | +75 to After 3 days | +65 to After 10-14 days | +35 to Other blood | Unclotted | +13 to Clotted | +50 to Source
357	Lets first import some modules .....
358	Lets import
359	Data input routines
360	First we import the packages .
361	Next we read in the data .
362	We are now ready to run the model . It takes about a second and is just a few lines of code .
363	Basic Logistic Regression
364	Train data import and processing
365	As a simple example we will create meta-regressor by stacking together regularized Ridge and Lasso regression models with LightGBM regressor . In this example there is minimum hyperparameters tuning and just three regressors are stacked together for simplicity .
366	Make predictions and create submission file
367	This code shows how to get points from .bin files in Dataset to train your own neural network .
368	Healing batch
369	The output files of this kernel are two .csv with augmented dataset .
370	Inverting synthesis . Sealing opened channels . Channel reduction
371	I found only slight data augmentation most helpful .
372	The bar plot shows the sample size for each tag in the training dataset . Notice the large class unbalance . The tags that do not occur very frequently will be difficult to train a model to identify , because there are not many cases in the training data .
373	The images contain numeric pixel values on the red , green , and blue scale . The statistical distributions of the red , green , and blue , pixels differ for different types of tags , indicating that this may be a useful feature for classification . The patterns in these pixels will likely have useful trends for classifying the objects in the images and the image types . The image below shows the red , green , and blue image layers .
374	Features are also created from the images using sobel and canny transforms in the skimage library . The sobel and canny transformations from skimage perform edge detection of the images . An example of the Sobel transformation is plotted below .
375	Random Forest Modeling
376	F2-score and other metrics
377	Libraries to import
378	Load the datasets
379	Findings This case is pretty straighforward , all the missing values comes from fields where ` Promo2 ` =0 which means there are no continuous promotional activities for those stores . Having no promotion means those fields have to be 0 as well since they are linked to Promo2 .
380	Now that we are done with clearing missing values , let 's merge the two datasets .
381	As we cited in the description , assortments have three types and each store has a defined type and assortment type a ` means basic things b ` means extra things c ` means extended things so the highest variety of products . What could be interesting is to see the relationship between a store type and its respective assortment type .
382	What if we go more granular to look at days in a week for promotion impact
383	Correlation Analysis
384	Test Set Adaptation
385	Developing The Model : Define a Performance Metric
386	A crucial Step in Machine Learning is to make sure your model is robust by testing it on a small part of your dataset we call here train_test set which is usually divided 80 % training and 20 % validation .
387	Instead of doing a function , i decided to loop over different combination of hyperparameters and see what would be the optimal combination and at least over the loop i could be monitor how is the training performing . But First let 's setup the RandomForestRegressor object .
388	Test our RF on the validation set
389	Kaggle Submission
390	Helper functions to deal with Kaggle 's file system limitations
391	Use simple fasttext-like model
392	Running over simulated test data ( 3.438 play by play
393	This is the slow `` for loop '' way . For each test query we calculate the cosine similarity for all the elements of the index - with a for loop - using the scipy function ` spatial.distance.cdist ` which calculates the cosine distance between vectors . The advantage of this methow is the low footprint memory usage : it uses 2048 ( 1 + len ( train_embeddings ) ) float32 of memory for each cycle .
394	First we import standard data manipulation libraries .
395	We also define globally accessible variables , such as the prediction length and the input path for the M5 data . Note that ` single_prediction_length ` corresponds to the length of the validation/evaluation periods , while ` submission_prediction_length ` corresponds to the length of both these periods combined . By default the notebook is configured to run in submission mode ( ` submission ` will be ` True ` ) , which means that we use all of the data for training and predict new values for a total length of ` submission_prediction_length ` for which we do n't have ground truth values available ( performance can be assessed by submitting prediction results to Kaggle ) . In contrast , setting ` submission ` to ` False ` will instead use the last ` single_prediction_length ` -many values of our training set as validation points ( and hence these values will not be used for training ) , which enables us to validate our model 's performance offline .
396	Reading the M5 data into GluonTS
397	Define the estimator
398	Once the estimator is fully trained , we can generate predictions from it for the test values .
399	Converting forecasts back to M5 submission format ( if ` submission ` is ` True
400	We then reshape the forecasts into the correct data shape for submission ...
401	Then , we save our submission into a timestamped CSV file which can subsequently be uploaded to Kaggle .
402	First we import standard data manipulation libraries .
403	We also define globally accessible variables , such as the prediction length and the input path for the M5 data . Note that ` single_prediction_length ` corresponds to the length of the validation/evaluation periods , while ` submission_prediction_length ` corresponds to the length of both these periods combined . By default the notebook is configured to run in submission mode ( ` submission ` will be ` True ` ) , which means that we use all of the data for training and predict new values for a total length of ` submission_prediction_length ` for which we do n't have ground truth values available ( performance can be assessed by submitting prediction results to Kaggle ) . In contrast , setting ` submission ` to ` False ` will instead use the last ` single_prediction_length ` -many values of our training set as validation points ( and hence these values will not be used for training ) , which enables us to validate our model 's performance offline .
404	Reading the M5 data into GluonTS
405	Once we have calculated all aggreated series , we still need to ensure that both static and dynamic features are also available for these series and that the aggreagted and the original series are concatenated into the same array .
406	Define the estimator
407	Once the estimator is fully trained , we can generate predictions from it for the test values .
408	Local performance validation ( if ` submission ` is ` False
409	Converting forecasts back to M5 submission format ( if ` submission ` is ` True
410	Once we have obtained these predictions , we still need to reshape them to the correct submission format .
411	Then , we save our submission into a timestamped CSV file which can subsequently be uploaded to Kaggle .
412	Actual Colab Notebook with an extended version on how to implement using tfhub too Colab version implementing DistilBert Kaggle Internet Off Version using Huggingface for Albert/DistilRoberta/Robertalarge Thanks to @ returnofsputnik for pointing out huggingface . Article explaining my understanding BERT and the common issues with using tf2 . Please consider upvoting if you find this useful . This notebook to some extent is an adaptation of akensert ] ( Plus , [ urvishp80 ] ( Please consider upvoting that post too . Thanks to @ abhishek for figuring how to use transformers offline .
413	Go to the github page of huggingface transformers - source - link Open the relevant config , modeling or tokenization . In this case , for example it will be configuration_albert.py Under Config_Archive_Map list , you will find aws source its downloading the required files You will require , one config.json , spiece.model ( if sentence piece is being used ) or vocab.txt/vocab and tf_model.h5 ( for keras Voila ! download file and add it as a dataset to your kernel . You can use the dataset
414	To avoid overfitting and promote generalization - Implementing KFold Validation & Early Stopping & Dropout & L2 Regualizer ( Ridge : pushes weights closer to zero not zero I am not going expand on the concepts here but , will write a separate article and share the link at later point
415	Thresholding the IoU value ( for a single GroundTruth-Prediction comparison
416	Now , for each prediction mask ( P ) , we 'll get a comparison with every ground truth mask ( GT ) . In most cases , this will be zero since nuclei should n't overlap , but this also allows flexibility in matching up each mask to each potential nucleus .
417	Multi-threshold precision for a single image
418	Dealing with color
419	Perhaps the simplest approach for this problem is to assume that there are two classes in the image : objects of interest and the background . Under this assumption , we would expect the data to fall into a bimodal distribution of intensities . If we found the best separation value , we could `` mask '' out the background data , then simply count the objects we 're left with . The `` dumbest '' way we could find the threshold value would be to use a simple descriptive statistic , such as the mean or median . But there are other methods : the `` Otsu '' method is useful because it models the image as a bimodal distribution and finds the optimal separation value .
420	Deriving individual masks for each object
421	A quick glance reveals two problems ( in this very simple image There are a few individual pixels that stand alone ( e.g . top-right Some cells are combined into a single mask ( e.g. , top-middle Using ` ndimage.find_objects ` , we can iterate through our masks , zooming in on the individual nuclei found to apply additional processing steps . ` find_objects ` returns a list of the coordinate range for each labeled object in your image .
422	Label 2 has the `` adjacent cell '' problem : the two cells are being considered part of the same object . One thing we can do here is to see whether we can shrink the mask to `` open up '' the differences between the cells . This is called mask erosion . We can then re-dilate it to to recover the original proportions .
423	Convert each labeled object to Run Line Encoding
424	Introduction to physiological data
425	Oh dear , that 's definitely not a normal respiration- you 're looking at 5 seconds of data , which should show 1 or 2 breaths in a nice sinusoidal pattern . I think there 's just too much high frequency noise for this to be useful . Let 's try some filtering to remove the high frequency signals
426	That 's much better . So we should filter our data to get much more useful insights into it . We can then use clever libraries such as Biosppy to count the respiration rate , which is a more useful metric than the raw waveform .
427	This measures the electrical activity in the heart . This is a single lead ECG , useful for analysing the rhythm and rate . If someone had a heart condition , they 'd be more likely to have a 12-lead ECG to look at the structural picture of any change in heart activity . The basic structure of the ECG What 's interesting about the ECG is while it 's shape might vary between individuals or recordings ( changes in lead position for instance ) , beat to beat , it 's shape does n't change much at all . The shape of the ECG can 'squash ' down slightly as heart rate increases , but the amplitude is fixed , and really the only useful information for this experiment is the heart rate ( which intuitively might be valuable as it increases when you 're stressed ) . Here 's the filtered ECG using the same settings as above
428	If you want to convert this into heart rate data , we can use the template matching tool in Biosppy to detect the R waves , calculate their intervals , and work out the heart rate across the experiment . You might want to do some filtering on this to smooth out the heart rate , but the moment-to-moment heart rate is useful too .
429	Word-Cloud for Negative tweets .
430	Word-Cloud for Positive tweets .
431	Splitting the Data
432	TF-IDF Vectoriser
433	Tranforming the dataset
434	BernoulliNB Model
435	LinearSVC Model
436	Logistic Regression Model
437	Saving the Models
438	Vote early and vote often
439	Pushout + Median Stacking
440	MinMax + Mean Stacking
441	MinMax + Median Stacking
442	image.png ] ( attachment : image.png
443	Sample files kaggle/input/birdsong-recognition/train_audio/brdowl/XC413729.mp kaggle/input/birdsong-recognition/train_audio/pilwoo/XC451058.mp kaggle/input/birdsong-recognition/train_audio/semplo/XC337236.mp kaggle/input/birdsong-recognition/train_audio/ruckin/XC131957.mp kaggle/input/birdsong-recognition/train_audio/linspa/XC475289.mp kaggle/input/birdsong-recognition/train_audio/ribgul/XC151285.mp
444	Amplitude vs Time
445	Zero Crossing Rate
446	its a specified percentage of the total spectral energy in the Frequency of an Audio
447	MFCC - Mel-Frequency Cepstral Coefficients
448	of kids achieved in the first attempt itself .
449	Looks like Chest Sorter is toughest and Bird Measurer is tougher . Most of the kids didnt clear the assessment in the first attempt
450	Training : Top 10 games played most . Looks like super intesting games with mostly animals , especially dinosaur .
451	Test : Top 10 games played most
452	Training : Top 10 least played games .
453	kids mostly interested in interactive things like Game and Activity . Wathcing videos is boring
454	Both Training and Test dataset has similiar range of game types
455	kids are interested in playing games related to hills . so many games provided in that segment .
456	definitly there should be an offer or an event must happend .
457	Peak installation was September 27th . Because of this : PBS KIDS Family Event At O'Neill Public Library .
458	Peak time was 7 in the evening . Most of the kids started playing at 10 in the morning and gradually increased upto night 11. even few kids played the game at 1AM . It may be because of timezone change or that game not played by the parent not by kid .
459	September last week . Already we knew the reason .
460	most assessment happened in 5th day of a week . Weekend happiness
461	In a day most of the assessment happened for 3PM ot 12PM .
462	Except Cart Balancer All other game have high time . because it is easiest . High number of first attempt winners are belonged to this Title .
463	two event codes having highest count . let explore further .
464	Highest game time took by TREETOPCITY
465	Event Count and Game Time is highly correlated ( Common one
466	Good number of games and activities in each world . Introduction world is having only videos .
467	In all 3 world , the Assessment time is almost same . then Game and Activity . It depends on how many sub categories inside game and activity .
468	Based on the previous chart , even its correlated ( game time and event count ) few cases are having little high game time .
469	In terms of assessment , each world is having its own Title . title is not common to all world
470	Based on the above two table , kids who entered the game , minimum 2 times attended the assesment by average .
471	Top 10 games session active for 3 days .
472	Almost similar trend in type from Train and Test
473	Choice & Distribution
474	of People
475	of People & Choice
476	Preference Cost & Accounting Cost
477	read csv and doing some preprocessing
478	Label Encoder ( LE ) , Ordinary Encoder ( OE
479	Target Encoder
480	M-Estimate Encoder
481	Weight of Evidence Encoder
482	James-Stein Encoder
483	Leave-one-out Encoder ( LOO or LOOE
484	Catboost Encoder
485	Even CVs did not solve the target based encoder 's overfit problem .
486	Let 's start with the binary feature .
487	It can be seen that as $ i $ of $ { bin } _i $ increases , the distribution approaches 50 % .
488	I 'll go ahead and target based encoding to believe the relationship between nom_7 and nom_8 .
489	You can use the Python code provided by the Kaggle kernel to get the path .
490	you can also use Facetgrid too
491	It maybe some encoding about hexadecimal . but length 9 is uncomfortable ...
492	How about percentage grapth .. ? It seems there are no relation between nom 's ..
493	And I find out ord3 , ord4 , ord5 feature target percentage is linear ( in dictionary order
494	This is the best public score kernel in the competition until now . I hpoe it be useful for those than pre-train and make knowledge tranfer to the model . If it waas useful for you , please VOTE me UP .
495	This is EVEN better the public score kernel in the competition until now . I wanted strongly dedicate this kernel to that scavenger colleagues that spend the time tracking back the work of other members just to take chance and get any good idea only for his own benefit , but , however they do n't share anything , just providing bad comments for those who share with the best will and without breaking the rules . If you do not feel alluded , please VOTE me UP .
496	Dear Colleagues , as you know , for this competition and others , we are allowed to make only two submission per day . That is a little frustrating , especially when you have a good idea and you have not the possibility of test it . I have spent a lot of time on understanding how log_loss behave over different models and kind of submissions . The main purpose of this kernel is provide a workbench in order to make you able to test different submissions accuracy before be submitted . I would like to take into consideration your experience and your input . Please comment if you have something to add . I highly recommed this related article I hope this kernel be useful for you . Please , Vote up .
497	Then , now can be estimate the ranking of all the files and test the accuary based on labels , and its error . I am going to do it for every file ( 'score already known ' ) available on the kernel .
498	The main purpose of this Notebook is to apply image processing technics in order to provide some additional engineering features to help on the improvement of the classifier accuracy . I highly recommend to read and see some examples about image processing And my Notebook 'Submarineering.Size matters What can you learn An easy way to compare graphically the influency of differents attributes . Undertanding that the cleaning of data is fundamental for the classifier , as the learning process is automatic , unnecessary data will confuse to the algorithm . Does n't matter which classifier or different algorithm you are going to use . This is always important . In this case I am focusing on the isolation of the object . The info provides by the water is irrelevant . As a bonus , at the end , I explain how to generate useful features as result of the morphological analysis .
499	The main purpose of this Notebook is to apply image processing technics in order to provide some additional engineering features to help on the improvement of the classifier accuracy . I highly recommend to read and see some examples about image processing And my Notebooks 'Submarineering.Size matters ' , 'Submarineering.Objects solation What can you learn An easy way to compare graphically the influency of differents attributes , and plot a 3d surface from the images using Plotly . Undertanding that the cleaning of data is fundamental for the classifier , as the learning process is automatic , unnecessary data will confuse to the algorithm . Does n't matter which classifier or different algorithm you are going to use . This is always important . In this case I am focusing on the isolation of the object and calculate his volume . The info provides by the water is irrelevant . As a bonus , at the end , I explain how to generate useful features as result of the morphological analysis .
500	Now let us get the validation sample predictions and also get the best threshold for F1 score .
501	Now that our model building is done , it might be a good idea to clean up some memory before we go to the next step .
502	Observations Overall pretrained embeddings seem to give better results comapred to non-pretrained model . The performance of the different pretrained embeddings are almost similar . Final Blend Though the results of the models with different pre-trained embeddings are similar , there is a good chance that they might capture different type of information from the data . So let us do a blend of these three models by averaging their predictions .
503	The result seems to better than individual pre-trained models and so we let us create a submission file using this model blend .
504	We also have some null values in the dataset . So one feature idea could be to use the count of nulls in the row .
505	Also the next important variables from EDA are floor and max_floor . So let us create two variables Floor number of the house to the total number of floors Number of floor from the top
506	Price of the house could also be affected by the availability of other houses at the same time period . So creating a count variable on the number of houses at the given time period might help .
507	Since schools generally play an important role in house hunting , let us create some variables around school .
508	Validation Methodology But before building our models , let us do some local validation by splitting the train dataset . In this competition , the train and test set are from different time periods and so let us use the last 1 year as validation set for building our models and rest as model development set .
509	We are getting a final R score of 0.01751 using this model . Now let us look at how the cumulative R value changes over time by plotting it along with zero-line .
510	We can now convert the field to dtype 'float ' and then get the counts
511	We have 38 special values . If we use a tree based model , we could probably leave it as such or if we use a linear model , we need to map it to mean or some value in the range of 0 to 256 . Now we can see the distribution plot of this variable .
512	There are quite a few number of missing values present in this field. ! We can do some form of imputation for the same . One very good idea is given by Alan in this [ script ] [ 1 ] . We can check the quantile distribution to see how the value changes in the last percentile .
513	As we can see there is a sudden increase in the rent value from 99.9 % to 100 % . So let us max cap the rent values at 99.9 % and then get a box plot .
514	From the box plot , we can see that most of the rent values fall between 0 and 300,000 . Now we can see the distribution of rent in test data as well .
515	Reading the dataset into pandas dataframe and looking at the top few rows .
516	The columns are self-explanatory and two columns 'dropoff_datetime ' and 'trip_duration ' are not present in the test set . trip_duration ' is the column to predict and Root Mean Square Logarithmic Error is our error metric . So let us look at the log distribution of the target variable .
517	I think 4 is a smaller value and let us not worry about it now ( but probably need to check later if needed ) . Next step is to check whether there are any null values in the data .
518	There are no missing values Validation Strategy Validation strategy is very important because without a proper validation starategy , it will be very hard to evaluate the models against each other . Since dates are given as part of the dataset , it is essential to check whether the train and test datasets are from the same time period or different time period .
519	We have around 182K rows in the dataset with 16 columns . Let us first look into the distribution of the target variable `` project_is_approved '' to understand more about the class imbalance .
520	Hours 03 to 06 has the least number of proposals and acceptance rate is also marginally on the lower side . Now let us combine the resource dataset to get some more insights .
521	Let us get the histogram of the price to understand the price requested for project proposals .
522	A new contest has begun How about seeing all the features before entering the competition
523	image from
524	without Outlier , it seams very normal or uniform distribution
525	It 's hard to see .. How about WordCloud
526	Temperature - temperature ( deg F Humidity - humidity WindSpeed - wind speed in miles/hour WindDirection - wind direction
527	The description of the data files from the data page train.csv - Train data . test.csv - Test data . Same schema as the train data , minus deal_probability . train_active.csv - Supplemental data from ads that were displayed during the same period as train.csv . Same schema as the train data , minus deal_probability . test_active.csv - Supplemental data from ads that were displayed during the same period as test.csv . Same schema as the train data , minus deal_probability . periods_train.csv - Supplemental data showing the dates when the ads from train_active.csv were activated and when they where displayed . periods_test.csv - Supplemental data showing the dates when the ads from test_active.csv were activated and when they where displayed . Same schema as periods_train.csv , except that the item ids map to an ad in test_active.csv . train_jpg.zip - Images from the ads in train.csv . test_jpg.zip - Images from the ads in test.csv . sample_submission.csv - A sample submission in the correct format . Let us start with the train file .
528	So almost 100K Ads has 0 probaility ( which means it did not sell anything ) and few ads have a probability of 1 . Rest of the deal probabilities have values in between . Region wise distribution of Ads Let us look at the region wise distribution of ads .
529	The regions have percentage of ads between 1.71 % to 9.41 % . So the top regions are Krasnodar region - 9 . Sverdlovsk region - 6 . Rostov region - 5 .
530	Parent Category Name Now let us look at the distribution of parent cateory names .
531	o 46.4 % of the ads are for Personal belongings , 11.9 % are for home and garden and 11.5 % for consumer electronics .
532	Category of Ads Now let us look at the category of ads .
533	Private users constitute 72 % of the data followed by company and shop . Price This is the price shown in the Ad .
534	Inferences So the dates are not different between train and test sets . So we need to be careful while doing our validation . May be time based validation is a good option . We are given two weeks data for training ( March 15 to March 28 ) and one week data for testing ( April 12 to April 18 , 2017 ) . There is a gap of two weeks in between training and testing data . We can probably use weekday as a feature since all the days are present in both train and test sets . User id Now we can have a look at the number of unique users in train & test and also the number of common users if any .
535	So out of the 306K users in test , about 68K users are there in train and the rest are new . Title First let us look at the number of common titles between train and test set
536	Feature Importance Now let us look at the top features from the model .
537	Wow , This confirms the first two lines of the competition overview . The 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue . As such , marketing teams are challenged to make appropriate investments in promotional strategies . Infact in this case , the ratio is even less .
538	So the ratio of revenue generating customers to customers with no revenue is in the ratio os 1 . Since most of the rows have non-zero revenues , in the following plots let us have a look at the count of each category of the variable along with the number of instances where the revenue is not zero . Number of visitors and common visitors Now let us look at the number of unique visitors in the train and test set and also the number of common visitors .
539	So apart from target variable , there is one more variable `` trafficSource.campaignCode '' not present in test dataset . So we need to remove this variable while building models . Also we can drop the constant variables which we got earlier . Also we can remove the `` sessionId '' as it is a unique identifier of the visit .
540	So we are getting a validation score of 1.70 using this method against the public leaderboard score of 1.44 . So please be cautious while dealing with this . Now let us prepare the submission file similar to validation set .
541	Feature Importance Now let us have a look at the important features of the light gbm model .
542	This follows the standard format of train , test and sample submission files . Now let us read the train and test file and check the number of rows and columns .
543	Observations The column names are anonymized and so we do not know what they mean There are many zero values present in the data From this [ discussion post ] ( the dataset is a sparse tabular one . Target Variable Let us first do a scatter plot of the target variable to see if there are any visible outliers .
544	Looks like there are not any visible outliers in the data but the range is quite high . We can now do a histogram plot of the target variable .
545	This is a right ( Thanks to Wesam for pointing out my mistake ) skewed distribution with majority of the data points having low value . Our competition admins are aware of this one and so they have chosen the evaluation metric as RMSLE ( Root Mean Squared Logarithmic Error. ) . So let us do a histogram plot on the log of target variables and recheck again .
546	This looks much better than the old one . Missing values Now let us check if there are missing values in the dataset .
547	There are no missing values in the dataset Data Type of Columns Now let us also check the data type of the columns .
548	Majority of the columns are of integer type and the rest are float type . There is only one string column which is nothing but 'ID ' column . Columns with constant values Generally when we get problems with many columns , there might be few columns with constant value in train set . So we can check that one as well .
549	There are quite a few variables with absolute correlation greater than 0 . Correlation Heat Map Now let us take these variables whose absolute value of correlation with the target is greater than 0.11 ( just to reduce the number of features fuether ) and do a correlation heat map . This is just done to identify if there are any strong monotonic relationships between these important features . If the values are high , then probably we can choose to keep one of those variables in the model building process . Please note that we are doing this only for the very few features and feel free to add more features to explore more .
550	Seems like none of the selected variables have spearman correlation more than 0.7 with each other . The above plots helped us in identifying the important individual variables which are correlated with target . However we generally build many non-linear models in Kaggle competitions . So let us build some non-linear models and get variable importance from them . In this notebook , we will build two models to get the feature importances - Extra trees and Light GBM . It could also help us to see if the important features coming out from both of them are consistent . Let us first start with ET model . Feature Importance - Extra trees model Our Evaluation metric for the competition is RMSLE . So let us use log of the target variable to build our models . Also please note that we are removing those variables with constant values ( that we identified earlier ) .
551	Let us do KFold cross validation and average the predictions of the test set .
552	So the validation set RMSLE of the folds range from 1.40 to 1.46 . Let us write the predictions of the model and write it to a file
553	This model scored 1.47 RMSLE on the public LB . We did not do any feature selection ( apart from removing the constant variables ) , feature engineering and parameter tuning . So doing that will further imporve the score . We can use this as our baseline model for any further modeling . Now let us look at the feature importance of this model .
554	So we are given the above files . The description of the files are train.csv - the training set test.csv - the test set sample_submission.csv - a sample submission file in the correct format - contains all card_ids you are expected to predict for . historical_transactions.csv - up to 3 months ' worth of historical transactions for each card_id merchants.csv - additional information about all merchants / merchant_ids in the dataset . new_merchant_transactions.csv - two months ' worth of data for each card_id containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data . First let us explore the train and test set .
555	Target Column Exploration
556	The field descriptions are as follows card_id- Card identifier month_lag- month lag to reference date purchase_date- Purchase date authorized_flag - ' Y ' if approved , 'N ' if denied category_3 - anonymized category installments -number of installments of purchase category_1 -anonymized category merchant_category_id -Merchant category identifier ( anonymized subsector_id -Merchant category group identifier ( anonymized merchant_id -Merchant identifier ( anonymized purchase_amount -Normalized purchase amount city_id -City identifier ( anonymized state_id -State identifier ( anonymized category_2 -anonymized category Now let us make some features based on the historical transactions and merge them with train and test set . Number of Historical Transactions for the card
557	Value of Historical Transactions
558	Loyalty score seem to decrease as the number of new merchant transactions increases except for the last bin .
559	Fill Null Data as cloud_coverage previp_depth_1_hr wind_direction wind_speed Fill Null Data as Mean Value sea_level_pressure
560	Site Id & Primary Use
561	Wind Direction and Wind Speed
562	Cloud and Pressure
563	Wow . This dataset looks interesting . It has numerical features , categorical features , date feature , text features and image features . Let us load the test data as well and check the number of rows in train and test to start with .
564	Target Variable Before delving more into the features , let us first have a look at the target variable 'interest level
565	Interest level is low for most of the cases followed by medium and then high which makes sense . Now let us start looking into the numerical features present in the dataset . Numerical features are bathrooms bedrooms price latitude longitude The last two are actually not numerical variables , but for now let us just consider it to be numerical . Bathrooms Let us first start with bathrooms .
566	Looks like evenly distributed across the interest levels . Now let us look at the next feature 'bedrooms ' . Bedrooms
567	Price Now let us look at the price variable distribution .
568	Looks like there are some outliers in this feature . So let us remove them and then plot again .
569	Created Now let us look at the date column 'created
570	So we have data from April to June 2016 in our train set . Now let us look at the test set as well and see if they are also from the same date range .
571	Looks very similar to the train set dates and so we are good to go . We shall also look at the hour-wise listing trend ( Just for fun
572	Most of the display addresses occur less than 100 times in the given dataset . None of the display address occur more than 500 times . Number of Photos This competition also has a huge database of photos of the listings . To start with , let us look at the number of photos given for listings .
573	Let us now look at the number of features variable and see its distribution . Number of features
574	It seems NaN values are present in all input columns but for two ( technical_22 and technical_34 ) . So let us count the number of missing values in each of the columns .
575	Fundamental_5 has the most number of missing values followed by fundamental_38 . Distribution plot Now let us look at the distribution plot of some of the numeric variables . Univariate analysis from [ this notebook ] [ 1 ] reveals some important variables . So let us look at the plots of top 4 variables . technical technical fundamental technical
576	Some of the observations from the distribution plot are The top two variables ( technical_30 and technical_20 ) range between 0 and 0.8 and there are no major outliers Fundamental_11 has few outliers at the beginning and then looks more or less fine with two small peaks Technical_19 has few high values towards the end Target Distribution Now let us scatter plot the target variable .
577	Target values range between -0.086 to 0.093 . As we can see the target graph is more darker at the middle , suggesting more values are concentrated in those region . Also there seems to be some hard stop at both the ends ( probably capping the target to remain within the limits ? ! ) - this could be inferred from the two dark lines at the top and bottom . Also there seems to be some change in the target distribution with respect to time . As we move from left to right , initially the target is evenly distributed in the given range ( -0.08 to 0.09 ) and then in the middle it is not so . Timestamp Now let us look at the counts for each of the timestamps present in the data .
578	Target Variable y '' is the variable we need to predict . So let us do some analysis on this variable first .
579	Seems like a single data point is well above the rest . Now let us plot the distribution graph .
580	Now let us have a look at the data type of all the variables present in the dataset .
581	X0 to X8 are the categorical columns . Missing values Let us now check for the missing values .
582	So all the integer columns are binary with some columns have only one unique value 0 . Possibly we could exclude those columns in our modeling activity . Now let us explore the categorical columns present in the dataset .
583	Binary variables which shows a good color difference in the above graphs between 0 and 1 are likely to be more predictive given the the count distribution is also good between both the classes ( can be seen from the previous graph ) . We will dive more into the important variables in the later part of the notebook . ID variable One more important thing we need to look at it is ID variable . This will give an idea of how the splits are done across train and test ( random or id based ) and also to help see if ID has some potential prediction capability ( probably not so useful for business Let us first see how the ' y ' variable changes with ID variable .
584	GoogleNews-vectors-negative glove.840B.300d paragram_300_sl wiki-news-300d-1M
585	Inference We can see that the insincere questions have more number of words as well as characters compared to sincere questions . So this might be a useful feature in our model . Baseline Model To start with , let us just build a baseline model ( Logistic Regression ) with TFIDF vectors .
586	Getting the best threshold based on validation sample .
587	Before we dive deep into the exploratory analysis , let us know a little more about the files given . To understand it better , let us first read all the files as dataframe objects and then look at the top few rows .
588	As we could see , orders.csv has all the information about the given order id like the user who has purchased the order , when was it purchased , days since prior order and so on . The columns present in order_products_train and order_products_prior are same . Then what is the difference between these files . As mentioned earlier , in this dataset , 4 to 100 orders of a customer are given ( we will look at this later ) and we need to predict the products that will be re-ordered . So the last order of the user has been taken out and divided into train and test sets . All the prior order informations of the customer are present in order_products_prior file . We can also note that there is a column in orders.csv file called eval_set which tells us as to which of the three datasets ( prior , train or test ) the given row goes to . Order_productscsv file has more detailed information about the products that been bought in the given order along with the re-ordered status . Let us first get the count of rows in each of the three sets .
589	So there are 206,209 customers in total . Out of which , the last purchase of 131,209 customers are given as train set and we need to predict for the rest 75,000 customers . Now let us validate the claim that 4 to 100 orders of a customer are given .
590	So there are no orders less than 4 and is max capped at 100 as given in the data page . Now let us see how the ordering habit changes with day of week .
591	Seems like 0 and 1 is Saturday and Sunday when the orders are high and low during Wednesday . Now we shall see how the distribution is with respect to time of the day .
592	So majority of the orders are made during day time . Now let us combine the day of week and hour of day to see the distribution .
593	Seems Satuday evenings and Sunday mornings are the prime time for orders . Now let us check the time interval between the orders .
594	On an average , about 59 % of the products in an order are re-ordered products . No re-ordered products Now that we have seen 59 % of the products are re-ordered , there will also be situations when none of the products are re-ordered . Let us check that now .
595	About 12 % of the orders in prior set has no re-ordered items while in the train set it is 6.5 % . Now let us see the number of products bought in each order .
596	Now let us merge these product details with the order_prior details .
597	Wow . Most of them are organic products. ! Also majority of them are fruits . Now let us look at the important aisles .
598	The top two aisles are fresh fruits and fresh vegetables . Department Distribution Let us now check the department wise distribution .
599	Let us list the files present in the input folder .
600	Looks like the text field is long . So let us take a single row and check the text column for understanding .
601	Wow . This is huge. ! Before we analyze more about the text , let us first check the class distribution . Class Distribution
602	Let us look at the distribution of number of words in the text column .
603	The peak is around 4000 words . Now let us look at character level .
604	The distribution is similar to the previous one . Let us now check if we could use the number of words in the text has predictive power .
605	Let us first get to understand some basic information about the data .
606	We could clearly see that most of the times the ball carrier moved in the opposite direction of the game and so the yards are negative . Direction of Ball Carrier during Zero Yards Now let us take some plays where the distance covered is zero yards and plot them .
607	Hmmm . Most of the times , the ball carrier is moving towards the center of the opposition . Direction of Ball Carrier during High Positive Yards Coverage
608	It looks like the ball carrier tries to move away from the opposition team towards a gap so as to move forward gaining more yards . Distance Covered by the Rusher at TimeHandoff
609	We can see a nice increasing trend in the yards gained by the rusher as the distance travelled by the rusher increases from zero . Then it starts decreasing which indicates that too much distance covered before the handoff of the ball also affects the yards gained . Speed of the Rusher at TimeHandoff
610	Speed also shows a trend similar to distance covered . 2 to 6 yards per second seem to be a better speed for yards covered . Rusher Acceleration at TimeHandOff
611	Acceleration also shows a similar trend as that of distance and speed . Position of the Rusher / Ball Carrier Let us now see how the position of the rusher affects the yards gained
612	Position CB has the highest median of yards gained followed by Wide Receiver Number of Defenders in the Box Now let us see how the number of defenders in the box affects the target
613	We can see a nice decrease in the median value of the yards gained by the rusher with the increase in the number of defenders in the box . Down Number Vs Yards Let us now check how the yards gained vary with respect to down number .
614	We can see a decrease in the median value of the target with an increase in the down number from 1 to 4 . Possession Team Vs Yards Now let us see the distribution of yards based on the team
615	Quarter Vs Yards
616	Test set also has a very similar distribution . Animation Let us try to have some animation on the available images . Not able to embed the video in the notebook . Please check the output tab for the animation
617	There are quite a few variables in this dataset . Let us start with target variable exploration - 'price_doc ' . First let us do a scatter plot to see if there are any outliers in the data .
618	Looks okay to me . Also since the metric is RMSLE , I think it is okay to have it as such . However if needed , one can truncate the high values . We can now bin the 'price_doc ' and plot it .
619	Certainly a very long right tail . Since our metric is Root Mean Square Logarithmic error , let us plot the log of price_doc variable .
620	There are some variations in the median price with respect to time . Towards the end , there seems to be some linear increase in the price values . Now let us dive into other variables and see . Let us first start with getting the count of different data types .
621	Floor We will see the count plot of floor variable .
622	The distribution is right skewed . There are some good drops in between ( 5 to 6 , 9 to 10 , 12 to 13 , 17 to 18 ) . Now let us see how the price changes with respect to floors .
623	This shows an overall increasing trend ( individual houses seems to be costlier as well - check price of 0 floor houses ) . A sudden increase in the house price is also observed at floor 18 . Max floor Total number of floors in the building is one another important variable . So let us plot that one and see .
624	We could see that there are few tall bars in between ( at 5,9,12,17 - similar to drop in floors in the previous graph ) . May be there are some norms / restrictions on the number of maximum floors present ( ? ) . Now let us see how the median prices vary with the max floors .
625	Dataset Size First let us check the number of rows in train and test file
626	No of Customers Now let us look at the number of unique customers in train data and test data and also the number of customers common between both
627	Let us see the count of occurrences of each of the customers in train set
628	Target Variables distribution There are 24 target variables present in this dataset are as follows ind_ahor_fin_ult1 - Saving Account ind_aval_fin_ult1 - Guarantees ind_cco_fin_ult1 - Current Accounts ind_cder_fin_ult1 - Derivada Account ind_cno_fin_ult1 - Payroll Account ind_ctju_fin_ult1 - Junior Account ind_ctma_fin_ult1 - Más particular Account ind_ctop_fin_ult1 - particular Account ind_ctpp_fin_ult1 - particular Plus Account ind_deco_fin_ult1 - Short-term deposits ind_deme_fin_ult1 - Medium-term deposits ind_dela_fin_ult1 - Long-term deposits ind_ecue_fin_ult1 - e-account ind_fond_fin_ult1 - Funds ind_hip_fin_ult1 - Mortgage ind_plan_fin_ult1 - Pensions ind_pres_fin_ult1 - Loans ind_reca_fin_ult1 - Taxes ind_tjcr_fin_ult1 - Credit Card ind_valo_fin_ult1 - Securities ind_viv_fin_ult1 - Home Account ind_nomina_ult1 - Payroll ind_nom_pens_ult1 - Pensions ind_recibo_ult1 - Direct Debit Let us check the number of times the given product has been bought in the train dataset
629	We can now convert the field to dtype 'float ' and then get the counts
630	We have 38 special values . If we use a tree based model , we could probably leave it as such or if we use a linear model , we need to map it to mean or some value in the range of 0 to 256 . Now we can see the distribution plot of this variable .
631	There are quite a few number of missing values present in this field. ! We can do some form of imputation for the same . One very good idea is given by Alan in this [ script ] [ 1 ] . We can check the quantile distribution to see how the value changes in the last percentile .
632	As we can see there is a sudden increase in the rent value from 99.9 % to 100 % . So let us max cap the rent values at 99.9 % and then get a box plot .
633	From the box plot , we can see that most of the rent values fall between 0 and 300,000 . Now we can see the distribution of rent in test data as well .
634	Logerror Target variable for this competition is `` logerror '' field . So let us do some analysis on this field first .
635	Wow . nice normal distribution on the log error . Transaction Date Now let us explore the date field . Let us first check the number of transactions in each month .
636	Let us explore the latitude and longitude variable to begin with .
637	Now let us check the dtypes of different types of variable .
638	Now let us check the number of Nulls in this new merged dataset .
639	The correlation of the target variable with the given set of variables are low overall . There are few variables at the top of this graph without any correlation values . I guess they have only one unique value and hence no correlation value . Let us confirm the same .
640	Here as well the distribution is very similar to the previous one . No wonder the correlation between the two variables are also high . Bathroom Count
641	There is an interesting 2.279 value in the bathroom count . Edit : As MihwaHan pointed in the comments , this is the mean value Now let us check how the log error changes based on this .
642	Bedroom count
643	is the mean value with which we replaced the Null values .
644	YearBuilt Let us explore how the error varies with the yearbuilt variable .
645	There is a minor incremental trend seen with respect to built year . Now let us see how the logerror varies with respect to latitude and longitude .
646	There are no visible pockets as such with respect to latitude or longitude atleast with the naked eye . Let us take the variables with highest positive correlation and highest negative correlation to see if we can see some visible patterns .
647	We can check the number of occurrence of each of the author to see if the classes are balanced .
648	This looks good . There is not much class imbalance . Let us print some lines of each of the authors to try and understand their writing style if possible .
649	Let us now plot some of our new variables to see of they will be helpful in predictions .
650	EAP seems slightly lesser number of words than MWS and HPL .
651	We are getting a mlogloss of ' 0.987 ' using just the meta features . Not a bad score . Now let us see which of these features are important .
652	Number of characters , mean word length and number of unique words turn out to be the top 3 variables . Now let us focus on creating some text based features . Text Based Features One of the basic features which we could create is tf-idf values of the words present in the text . So we can start with that one .
653	Now that we have got the tfidf vector , here is the tricky part . The tfidf output is a sparse matrix and so if we have to use it with other dense features , we have couple of choices . We can choose to get the top 'n ' features ( depending on the system config ) from the tfidf vectorizer , convert it into dense format and concat with other features . Build a model using just the sparse features and then use the predictions as one of the features along with other dense features . Based on the dataset , one might perform better than the other . Here we can use the second approach since there are some very [ good scoring kernels ] ( using all the features of tfidf . Also it seems that , [ Naive Bayes is performing better ] ( in this dataset . So we could build a naive bayes model using tfidf features as it is faster to train .
654	Naive Bayes on Word Count Vectorizer
655	This has a val score of 0.3055 and LB score of 0.32xx Running it on all the folds might give a better score . Now let us check the important variables again .
656	Naive bayes features are the top features as expected . Now let us get the confusion matrix to see the misclassification errors .
657	Read the train file from Kaggle gym .
658	As expected , the correlation coefficient values are very low and the maximum value is around 0.016 ( in both positive and negative ) as seen from the plot above . Let us take the top 4 variables from the plot above and do some more analysis on them alone . technical technical fundamental technical As a first step , let us get the correlation coefficient in between these variables .
659	There is some negative correlation between 'technical_30 ' and 'technical_20 ' . As the next step , let us build simple linear regression models using these variables alone and see how they perform . Let us first build our models .
660	This multiple regression gave a score of 0.019 which is better than all univariate models . So probably submitting this model might give a better LB score . Model with Clipping As we can see from this [ script ] [ 1 ] which gives the best public LB score of 0.00911 , clipping the ' y ' values help . So let us dig a little deeper to see why the public LB score increased from 0.006 to 0.009 when we clip the ' y ' values .
661	Let us now do the clipping and see the number of rows that will be discarded from the training .
662	Let us read both the train and test dataset and check the number of rows .
663	Target Variable Exploration First let us look at the target variable distribution .
664	Questions Exploration Now let us explore the question fields present in the train data . First let us check the number of words distribution in the questions .
665	It is interesting to see that there are very few question pairs with no common words .
666	There is some good difference between 0 and 1 class using the common unigram count variable . Let us look at the same graph using common unigrams ratio .
667	Leaky Features Exploration Now let us get into the leaky data exploration part . We have a couple of leaky features which seem to improve the score significantly . Frequency based feature by Jared Turkewitz Intersection of common neighbors by Krzysztof Dziedzic implemented by tour1st
668	Q1-Q2 neighbor intersection count Let us first do simple count plots and see the distribution .
669	Wow . This explains why this variable is super predictive . Question1 Frequency
670	We could see a long tail here as well . Now let us check the target variable distribution .
671	Here as well , we can see an increase in the mean target rate as the frequency increases. ! Hopefully this is the case with question 2 as well . Let us now do a heat map between q1_freq and q2_freq to see the target variable distribution .
672	Let us also check the correlation between the three fields .
673	Let us read the train and test files and store it .
674	Now let us stack both the dense and sparse features into a single dataset and also get the target variable .
675	Now let us build the final model and get the predictions on the test set .
676	Read data set
677	source Term Frequency Inverse Document Frequency Vectorizer
678	Model Validation on train data set
679	Read data set
680	Dependant variable distribution
681	Co relation plot
682	Mean & Median range
683	Remove unwanted punctuation mark
684	Bag of words
685	Submit prediction for unseen dataset
686	TfIdf ( Term frequency Inverse document frequency
687	Submit prediction for unseen dataset
688	Submit prediction for unseen dataset
689	Introduction This is a guide for forecasting future web traffic values on previous web traffic dataset provided by Google . I chose to build it with Prophet from facebook because it gives faster result than LSTM and get predictions from patterns of the previous data . First , I will prepare the data ( previous web traffic time-series ) then I will focus on prediction and processing exceptions ( outliers ) . For more information on Prophet , click this link . Prophet This Notebook follows three main parts Data preparation Processing data and forecasting Results prediction and submission
690	Data preparation
691	Outliers can distort predicting from data points with similar magnitudes . To remove them , I supposed data points less than 5 percentile and bigger than 95 percentile as outliers and removed them replacing with None value . Outliers
692	The best way to block outlier is to remove them .
693	Read data set
694	Correlation is a measure bivariate analysis that measure the strength of assciation between variable and direction of relationship.In terms of strength of relationship , the value of the correlation coefficient varies between +1 and
695	The correlation coefficient for ps_calc is 0 , so we will drop these from our dataset .
696	Missing value is data set
697	Replace missing value with mode
698	Convert variables into category type
699	The unique value of `` ps_car_11_cat '' is maximum in the data set is
700	Split data set
701	Predict for unseen data set
702	Read data set
703	Correlation is a measure bivariate analysis that measure the strength of assciation between variable and direction of relationship.In terms of strength of relationship , the value of the correlation coefficient varies between +1 and
704	The correlation coefficient for ps_calc is 0 , so we will drop these from our dataset .
705	Check and fill missing value is data set
706	Missing value in test train data set are in same propotion and same column
707	Split data set
708	Predict for unsen data set
709	Resize training / valid / test data
710	This is a full walkthrough for building the machine learning model for Porto Seguro ’ s Safe Driver Prediction dataset provided by Porto Seguro . Stratified KFold is used due to inbalance of the output variable . XGBoost is used because it is like the winning ticket for classification problem with formatted data . You can check its success on this link . ( [ XGBoost winning solutions ] ( First , I will prepare the data ( driver 's information and whether the driver initiated auto insurance or not ) then I will focus on prediction . For more information on XGBoost , click this link . XGBoost
711	Define X and y
712	Create a submission file
713	Update Just realized that by replacing LSTM with GRU , the score went up a bit more .
714	Since the model seems to be doing quite well , we tried to experiment with pseudo-labeling ( using test data with high confidences as training data ) , however there 's is currently no improvements .
715	I was trying to clean some of my code so I can add more models . However , this can never happen without the awesome kernels from other talented Kagglers . Forgive me if I missed any . Based on SRK 's kernel Vladimir Demidov 's 2DCNN textClassifier Attention layer from Khoi Ngyuen LSTM model from Strideradu Some new things here Take average of embeddings ( Unweighted DME ) instead of blending predictions The original paper of this idea comes from : Frustratingly Easy Meta-Embedding – Computing Meta-Embeddings by Averaging Source Word Embeddings Modified the code to choose best threshold Robust method for blending weights : sort the val score and give the final weight Some thoughts Although I pulished a kernel on Transformer , I will not use it Too much randomness in CuDNN . You may get different results by just rerunning this kernel Blending rocks
716	Main part : load , train , pred and blend
717	Duplicate image identification
718	Let 's start by importing our libararies .
719	Let 's draw a simple histogram to see the sample-per-class distribution .
720	The image sizes seem to vary , so we 'll try to see what the average width and height are
721	We start by training only the newly initialized weights , then unfreeze the model and finetune the pretrained weights with reduced learning rate .
722	Choosing a threshold
723	And then on the all-whale set that we left out at the beginning
724	We start by training only the newly initialized weights , then unfreeze the model and finetune the pretrained weights with reduced learning rate .
725	Let 's add some TTA ( Test-Time-Augmentation ) , this usually increases the accuracy a bit , you may need to do some tests here to compare the result with normal prediction .
726	Nans ( Null Data ) 찾아보기
727	Null Value가 Cabin의 경우 약 77프로로 상당한 양의 데이터가 유실 ( 혹은 없음 ) 되었음을 알 수 있고 , age의 경우 약 20프로임 . Embarked의 경우 0.22프로로 미미한 수준임을 알 수 있으며 나머지 column엔 모든 데이터가 존재 .
728	Ticket
729	Ex : SC/A.3 - > SC와 A.3 , SC와 A 와 3 , 등
730	Categorical Data 를 다듬고 Label encoding 하기
731	Atribute 생성
732	Modeling
733	Reading and Merging Identity and Transaction Datasets
734	The data that we are working on is imbalanced . There are many ways to deal with imbalanced data . Me and my teammate decided to use undersampling the majority class . And , we would like to thank Shahules786 for this very informative kernel . You can check his kernel [ here ] ( And , do n't forget to upvote . We decided do downsample majority class to 400k . We decided it by trial and error .
735	While we were testing our new features with the given parameters below , we decided to play with max_depth a bit to see if we are overfitting . We changed it as 3 , and the difference between validation and training were decreased . It might be a good idea to optimize it . But in my opinion , optimizing it at the last day of the competition was our biggest mistake . Changing max depth parameter caused underfitting and huge shakeup .
736	Forecast modeling - ARIMA
737	Monday=0 , Sunday=6 . Here we can find the weekends ( 5,6 ) has a larger sales , weekdays ( 0-4 ) are smaller . There 's a few outliers on Monday , Wed .
738	the smaller p-value , the more likely it 's stationary . Here our p-value is 0.036 . It 's actually not bad , if we use a 5 % Critical Value ( CV ) , this series would be considered stationary . But as we just visually found an upward trend , we want to be more strict , we use 1 % CV . To get a stationary data , there 's many techiniques . We can use log , differencing etc ...
739	After differencing , the p-value is extremely small . Thus this series is very likely to be stationary . ACF and PACF The partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags . Autoregression Intuition Consider a time series that was generated by an autoregression ( AR ) process with a lag of k. We know that the ACF describes the autocorrelation between an observation and another observation at a prior time step that includes direct and indirect dependence information . This means we would expect the ACF for the AR ( k ) time series to be strong to a lag of k and the inertia of that relationship would carry on to subsequent lag values , trailing off at some point as the effect was weakened . We know that the PACF only describes the direct relationship between an observation and its lag . This would suggest that there would be no correlation for lag values beyond k. This is exactly the expectation of the ACF and PACF plots for an AR ( k ) process . Moving Average Intuition Consider a time series that was generated by a moving average ( MA ) process with a lag of k. Remember that the moving average process is an autoregression model of the time series of residual errors from prior predictions . Another way to think about the moving average model is that it corrects future forecasts based on errors made on recent forecasts . We would expect the ACF for the MA ( k ) process to show a strong correlation with recent values up to the lag of k , then a sharp decline to low or no correlation . By definition , this is how the process was generated . For the PACF , we would expect the plot to show a strong relationship to the lag and a trailing off of correlation from the lag onwards . Again , this is exactly the expectation of the ACF and PACF plots for an MA ( k ) process . Summary From the autocorrelation plot we can tell whether or not we need to add MA terms . From the partial autocorrelation plot we know we need to add AR terms . References
740	Make prediction and evaluation
741	SARIMAX : adding external variables
742	These model coefficients are not very reliable as most of them are not significant . This would imply a high collinearity between the data .
743	To create a static map we will utilize geospatial small scale data at 1:110m resolution . To render a world map we need a shapefile with world coordinates from Natural Earth domain
744	Visualization of time series data
745	We can also visualize our data using a time-series decomposition that allows us to decompose our time series into three distinct components : trend , seasonality , and noise .
746	This kernel is intended to be a tutorial on Keras around image files handling for Transfer Learning using pre-trained weights from ResNet50 convnet . Though loading all train & test images resized ( 224 x 224 x 3 ) in memory would have incurred ~4.9GB of memory , the plan was to batch source image data during the training , validation & testing pipeline . Keras ImageDataGenerator supports batch sourcing image data for all training , validation and testing . Actually , it is quite clean and easy to use Keras ImageDataGenerator except few limitations ( listed at the end ) . Keras ImageDataGenerator expects labeled training images to be available in certain folder heirarchy , 'train ' data was manually split into 10k for training & 2.5k for validation and re-arranged into the desired folder hierarchy . Even 'test ' images had to rearranged due to a known issue in flow_from_directory .
747	Define Our Transfer Learning Network Model Consisting of 2 Layers
748	Compile Our Transfer Learning Model
749	Prepare Keras Data Generators
750	Train Our Model With Cats & Dogs Train ( splitted ) Data Set
751	Observe Prediction Time With Different Batch Size
752	Check LaTeX tags
753	Simple math tag cleaning
754	simple cleaning the math tags
755	Load dataset and Embeddings
756	Finetuning the pretrained model
757	Extract Test Image Features
758	Initial G & D , Loss function , Optimizers
759	Let 's see the computer generation ! ( Animation
760	However , it comes to my attention that the number of rows are different for each table , despite having unique TransactionID
761	Yes , they are indeed complete , except for card 2 . If I were to guess , card1 could be first name and card2 could be last name . Now let 's check out missing data for numerical variables
762	Observation Basic information about transaction such as ID , DT , amount and type of product is complete Dist1 and dist2 is very sparse . C columns are complete Most D columns are sparse except D Lastly , we want to check for data completeness of Vesta 's engineered features
763	Replacement or drop the missings
764	Observation : Not much difference in fraud rate between credit card and debit card Examine Email Domain Purchaser Email
765	Protonmail returns an exemely high fraud rate . Almost 80 % of transactions from purchaser using protonmail.com were label fraud . Let 's double check this result
766	We can aggregate the operating system into a few major OSs .
767	Same as previous plot , we need to reduce the number of categories using aggregation
768	Split K- fold validation
769	reference this kernel
770	Ensemble two model ( NN+ LGBM
771	Create submit file
772	For training , inference kernel ca n't turn on internet
773	Load train data
774	Load segmentation file
775	Tranfer EncodedPixels to target
776	We found there are some duplicate image in training data
777	Balance have chip and no chip data
778	Set Training set count
779	Doing One hot on target
780	Split Training data to training data and validate data to detect overfit
781	Using ImageDataGenerator
782	Load ResNet50 model with Keras
783	Set Hyperparameter and Start training
784	Do one hot for predict target
785	Crazy feature engineering
786	Map cat vals which are not in both sets to single values
787	Combine sparse matrices
788	More Blending inspired from Giba 's Kernel I grabbed output submissions files from following public kernels
789	Crazy feature engineering
790	Map cat vals which are not in both sets to single values
791	Combine sparse matrices
792	Ensemble techniques based on scipy.optimize package .
793	How to use EN_optA and EN_optB in a 3-layers classification architecture .
794	Parameters can be changed to explore different types of synthetic data .
795	Comparing our ensemble results with sklearn LogisticRegression based stacking of classifiers .
796	If looks correct , apply it on all samples
797	also let 's look at the train and test folders along with defining the paths respectively
798	Plot the pie chart for the train and test datasets
799	t observation for each image in the training set there are 4 lines for each type of cloud Image_Label '' contains the image filename along with a cloud type seperated by If a certain type of clouds in present on the image , the EncodedPixels column is non-null and contains the segmentation map for the corresponding cloud type .
800	now that we have our modified csv , we shall explore some more
801	now we can explore the correlation between Label_Fish , Label_Flower , Label_Gravel , Label_Sugar columns
802	Splitting into train/val
803	generator instances
804	Defining a model
805	Initial tuning of the added fully-connected layer
806	Visualizing train and val PR AUC
807	Missing values treatment
808	viewing the images
809	Experiment with ` lower_limit ` to get hair with removed noise Note that ` lower_limit ` is the sensitivity of the threshold of obtaining the mask .. too low value can lead to addition of noise and too high value can lead to loss of info of hair
810	Let 's look for the best quality hair ( s ) for later use
811	Contents
812	Rate of each specie
813	Create a heat map to present of records
814	Audio information / exploration
815	Comparing wave curve for different birds
816	Stratified Sampling ( ratio = 0 .
817	Impute missing values
818	Deal with Categorical features : OneHotEncoding
819	Feature matrix and target
820	Chi
821	Wrapper
822	Embeded
823	Random Forest
824	LightGBM
825	Load input data
826	In fast.ai we can easily select categorical and continuous variables for training . I decided not to choose any external data in baseline model . Adding numerical values from country data provided in this notebook does n't seem to improve the validation score much .
827	Preparing training set
828	Processed test set
829	The model provided here is a simple baseline from [ fast_tabnet ] ( documentation , without any fine tuning . Here ] ( you can find how to tune hyperparameters . In contrary to fast.ai tabular learner , this network accepts multivariable output like here - ( Cases , Fatalities is what we are going to predict .
830	Get the data
831	Numerical features
832	Correlation Matrix
833	A further exploration on application table
834	Train model
835	Prediction
836	Get the Data
837	Check missing values
838	Explore the Data
839	The distribution of 'Target
840	Visit Number
841	Date
842	Baseline Model
843	Modeling
844	Step 2 : Set the range for each parameter
845	Training Evaluation Test This is the simplest idea I tried so far , and was quite surprised it worked . It seems you just need 99 other ideas ....
846	Getting the data
847	Results on the evaluation set
848	Updates August 2018 - Using a neat trick from this [ kernel ] ( by @ btyuhas , the CSV loading time has improved from about 5 minutes to 2 minutes . August 2018 - @ danlester suggested a much faster way ( 10x ) to count the number of lines of a CSV file using the ` wc ` ( word count ) unix command . August 2018 - @ jpmiller suggested using dask to read the csv faster . Added Using Dask at the bottom section .
849	We might not need float64 ( 16 decimal places ) for the longitude and latitude values . float32 ( 7 decimal places ) might be just enough . See
850	Some configuration
851	Only need instances
852	Make mask predictions
853	pytorch model & define classifier
854	Now let 's import Necessary libraries for LSTM
855	Import necessary libraries for data preprocessing
856	ps.stem Stemming is the process of producing morphological variants of a root/base word . Stemming programs are commonly referred to as stemming algorithms or stemmers . A stemming algorithm reduces the words chocolates ” , “ chocolatey ” , “ choco ” to the root word , “ chocolate ” and “ retrieval ” , “ retrieved ” , “ retrieves reduce to the stem “ retrieve
857	One Hot representation
858	Pad_Sequences The pad_sequences ( ) function in the Keras deep learning library can be used to pad variable length sequences . The default padding value is 0.0 , which is suitable for most applications , although this can be changed by specifying the preferred value via the “ value ” argument . By this we are going to make all the sentances in same length.There are 2 types of Padding `` Pre '' and Post '' , pre means it 's going to add 0 in front and post means it 's goint add 0 in back
859	Creating Model
860	Let 's see what kind of score we get using just meta features of the images . The higher the score , the more worried we need to be about our models picking up information irrelevant for the purpose of diagnosis .
861	We clearly see here that the model tends in part to focus on what is going on around the edge of the eyeball , and who can blame it ? It 's a great way to make predictions in the training data . An important thing to note here is that this weird relationship between meta-features and target does NOT extend to the test data . Let 's generate predictions to demonstrate this
862	By computing the difference between to quaketimes we obtain some kind of stepsize that is probably equal within blocks and can show us the jump strength between different quaketimes . The following code computes differences first and drops the last row of train such that we can add the stepsize to the data . I think we wo n't loose fruitful information this way .
863	Setting up a validation strategy
864	I sometimes get stuck in too much details that may not be neccessary or are more fun to explore by yourself . As this is just a starter , I like to continue with some ideas and visualisations that have been used in initial work of Bertrand Rouet-Leduc and the LANL-group . One of the ideas was to use features extracted by a rolling window approach . Let 's do the same and make some visualisations what goes on with these features until the first lab earthquake occurs . Window size I do n't know in adcance which kind of window size would be an appropriate choice and I think it 's an hyperparameter we should try to optimize . But to start , let 's try out some different sizes and the mean and standard deviation to select one that may be sufficient to play around
865	Logistic Regression Stacking
866	Let 's have a little look at the data .
867	The extra features in the training set must be for analysing the bias . This is going to be an interesting competition with such a metric
868	As with most NLP tasks , we will start by using some pre-trained embeddings for our words . This provides us with a numerical representation of our input that we can use for modelling . Mapping words to embeddings is n't always straight forward , however : the data may not be very tidy . The first step , then , is to ensure we get as many words mapped to a suitable embedding as possible . To do this , we 'll make use of two excellent kernels
869	Let 's split the data back into train and test
870	Tokenize the text
871	Let 's submit this as our first submission . Once we have a reasonable pipeline setup , we can move on to looking at the competition metric in more detail .
872	This notebook predicts the probability of the occurrence of the upset , which means that the low seed rank team beats the high seed rank team , aggregating past game results
873	Then we concatenate both frames and shuffle the examples
874	Finally we create a new train/test split
875	Now we ’ re ready to train and evaluate . Here are the scores
876	notebookの目的は参加者にM5コンペの概要をざっと理解してもらう事にあります descriptionを要約した物を箇条書きしておきます Accuracyでもう片方はUncertaintyです accuracyコンペはWeighted Root Mean Squared Scaled Error ( RMSSE ) を指標にしています uncertaintyコンペはWeighted Scaled Pinball Loss ( WSPL ) を指標にしています wal-martの売り上げを予測することが目標です
877	FOODS_3_090_CA_3_validationがよく売れているようなので、これにする
878	super bowl ( アメリカのビッグイベント ) のある日曜日 ? ) スパイクの起きている日については、calendar.csvのevent dataを見れば良い知見が得られそう
879	CA_2には2015年に大きな変化があった事が想像できる。 ( 店舗の改装、増築など
880	TODO facebookのprophetを用いたモデル lgbm/xgbでの予想
881	min-maxで [ 1 , 0 ] にすると似た形なる可能性もある
882	Create instances with all data and positive/neutral/negative
883	N-gram bar chart
884	Histogram of the word count
885	I 'll first trying to understand data and not to rush up into the plots since it 's my first knowledge competition . Let 's see how the data looks like , from a closer view
886	External Data ( release dates , popularity and votes Contains releases dates for every country .
887	Additional features from Kamal Chhirang which contains more informations about movies ( Votes , popularity
888	Let 's take a peak on the top 20 movies
889	My sixth sense smells big correlation between the revenue and the initial budget lets check it closely
890	Guess i 'm right , a big budget can guarantee a good revenue .
891	What about the relation between popularity and revenue
892	MODEL
893	MODEls testing NVM
894	Comes the final part of feature engineering- DFS or Deep Feature Synthesis where different features get stacked up ( that 's mean the term deep comes from ) . I have kept the maximum depth as 1 . With more depth , you get transformation of transformations that can be useful at times but are very hard to interpret , so avoiding that .
895	These are some features that contained string data , so label encoding them below . A better approach could be to label encode all the features before performing , will make that optimization going forward .
896	Using Light GBM , generally I prefer XGBoost but they provide similar accuracy .
897	To select features , I am using the 3 methods In-built feature importance of Light GBM Permutation Importance SHAP values I prefer to perform feature selection based on Permutation Importance simply because that 's the best I understand . Possible , that Feature importances and SHAP values can give better results but I do not understand their maths very well ( especially the SHAP values ) . I will try learning them better . Permutation Importance is easy to comprehend and a natural way to remove useless features- if you randomly shuffle a feature and it does n't reduce your accuracy , then that feature is not a good indicator . If someone had to do all this manually , this is the way to go about it .
898	This is simply selecting the features based on a threshold from Permutation Importance . One problem that I am facing ( not able to resolve completely ) - due to the fact that automtaed feature engineering has generated so many junk features that any single feature is having very less impact on overall accuracy . So , even with a low threshold of 0.001 , I remove almost 70 % of the features generated from featuretools .
899	What about the time to read in the data from disk ? Lets load the first 100 segments from our hdf5 file and compare that to using pandas.read_csv on the raw data .
900	now compare this with the time to read in the data from the csv using pandas .
901	Summary of given data
902	Distribution of Sex and Age
903	We are going to explore further with Smoking Status feature involved .
904	Pivot table below has shown each general its statistics such as maximum value , minimum vale , mean , standard deviation Each value was categorized by Sex followed by SmokingStatus
905	Relationship between FVC and Percent
906	It is pretty obvious that the higher FVC , the higher Percent . As data desciption said FVC ( Forced Vital Capacity ) - the recorded lung capacity in ml Percent- a computed field which approximates the patient 's FVC as a percent of the typical FVC for a person of similar characteristics
907	I am going to use DefalutDict to store all train dicom file path dicom dict = { `` { Patient_id } '' : [ '/kaggle/input/osic-pulmonary-fibrosis-progression/train/ { Patient_id It helps us to easily access all different patient id dicom filepath Note : You might be familiar with some part of code down below Since I am new DICOM file format , I 've searched for any related of similar appoarch . I found this extremely helpful notebook from Data Science bowl 2017 ( written by Guido Zuidhof If you are new to dicom like me and looking for an informative and demonstrative notebook , I encourage you to have a look at mentioned notebook and give him an vote as well .
908	The fastai library also provides some functions for interpreting the models , such as displaying images with the top losses , displaying confusion matrices , and [ more
909	Let 's look at an example image
910	The Kaggle competition used the Cohen 's quadratically weighted kappa so I have that here to compare . This is a better metric when dealing with imbalanced datasets like this one , and for measuring inter-rater agreement for categorical classification ( the raters being the human-labeled dataset and the neural network predictions ) . Here is an implementation based on the scikit-learn 's implementation , but converted to a pytorch tensor , as that is what fastai uses .
911	Let 's evaluate our model
912	About this notebook
913	Number of teams by Date
914	Top LB Scores
915	Count of LB Submissions that improved score
916	Let 's look at an example image . These images are 6-channel images , but the each of the six channels are saved as separate files . Here , I open just one channel of the image .
917	We have to redefine the following function to be able to view the image in the notebook . I view just the first 3 channels .
918	With the multi-channel ` ImageList ` defined , we can now create a DataBunch of the train images . Let 's first create a stratified split of dataset and get the indices .
919	Now we create the ` DataBunch
920	Let 's create our Learner
921	fmm ...
922	It 's Dummy .
923	The number of Feature has not changed
924	Flag of whether or not there is an attribute ID for each class ID is a two-dimensional array
925	Training Attribute Classification Models
926	Next , train the mask image
927	Make Submission File
928	Seeding Everything for Reproducible Results
929	Training ( Transfer learning
930	Let 's evaluate our model
931	Let 's first get a single image and pu [ ] ( it into a minibatch . We put it into a minibatch as that is what our model expects .
932	About this Notebook
933	Note that the transforms are build in such a way that you can apply them on the audio data after reading it as a raw time series numpy array , In case you want the transform classes to read the audio file and then transform you can easily add a function in the main class below to read audio data from file path and then call this function in every other transform class to read data . This implementation is done keeping in mind that you would want to apply these transform after cropping 5 min clips from audio which seem to work best till now as per the results shared in discussion forums from people who are the top of lb righ now
934	General Usage
935	General Usage
936	General Usage
937	Shift the pitch of any audio file by number of semitones
938	General Usage
939	Add Gaussian Noise to the audio
940	General Usage
941	General Usage
942	General Usage
943	Cut-out is famous augmentation for images where it is used to make the model generalize better , in this , a random portion of image pixels are given a value zero
944	Getting It Together
945	For this version of Notebook we will be using TPU 's as we have to built a BERT Model
946	Writing a function for getting auc score for validation
947	Basic Overview What is a RNN Recurrent Neural Network ( RNN ) are a type of Neural Network where the output from previous step are fed as input to the current step . In traditional neural networks , all the inputs and outputs are independent of each other , but in cases like when it is required to predict the next word of a sentence , the previous words are required and hence there is a need to remember the previous words . Thus RNN came into existence , which solved this issue with the help of a Hidden Layer . Why RNN 's In-Depth Understanding Code Implementation So first I will implement the and then I will explain the code step by step
948	While building our simple RNN models we talked about using word-embeddings , So what is word-embeddings and how do we get word-embeddings Here is the answer The latest approach to getting word Embeddings is using pretained GLoVe or using Fasttext . Without going into too much details , I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it and since I am a fan of GloVe vectors , word2vec and fasttext . In this Notebook , I 'll be using the GloVe vectors . You can download the GloVe vectors from here or you can search for GloVe in datasets on Kaggle and add the file
949	Basic Overview Simple RNN 's were certainly better than classical ML algorithms and gave state of the art results , but it failed to capture long term dependencies that is present in sentences . So in 1998-99 LSTM 's were introduced to counter to these drawbacks . In Depth Understanding Why LSTM 's What are LSTM 's Code Implementation We have already tokenized and paded our text for input to LSTM 's
950	Basic Overview Introduced by Cho , et al . in 2014 , GRU ( Gated Recurrent Unit ) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network . GRU 's are a variation on the LSTM because both are designed similarly and , in some cases , produce equally excellent results . GRU 's were designed to be simpler and faster than LSTM 's and in most cases produce equally good results and thus there is no clear winner . In Depth Explanation Code Implementation
951	In Depth Explanation Code Implementation
952	BERT and Its Implementation on this Competition
953	Encoder FOr DATA for understanding waht encode batch does read documentation of hugging face tokenizer here
954	For understanding please refer to hugging face documentation again
955	If you want to use any another model just replace the model name in transformers._____ and use accordingly
956	Now Let 's Begin by Importing the data
957	Now that we know everything about ISUP_grade and Gleason_score , let 's look at how many unique values they have got , as per our guesses , ISUP must have 6 unique values ( 1-5 grades and 0 for non cancer ) and Gleason_score should have 8 values in total ( 2-10 for scores
958	We have seem from the figure showing mappings of ISUP from gleason that 3+4 and 4+3 map to different ISUP scores while other pairs like 3-5 and 5-3 , 4-5 and 5-4 map to same ISUP , let 's verify it
959	LET 's now Look at the test file
960	We see that the isup_grade 0 and 1 i.e no cancer , has the most number of values and that 's what expected in case of most medical datasets , the target class will always be underrepresented and that 's also the most important challenge when performing machine learning tasks on Medical DATA Now let 's Look at how much data is provided by which data-provider
961	From this graph it is also clear that the data will be baised towards non-cancer examples
962	Now we can finally move on to Image EDA . BUT since we are complete beginners , let 's first+ understand the format of image that is provided to us and all the image related jargons that we will be using further Q1 ) What is .tff format and Why it is used Tagged Image File Format ( TIFF ) is a variable-resolution bitmapped image format developed by Aldus ( now part of Adobe ) in 1986 . TIFF is very common for transporting color or gray-scale images into page layout applications , but is less suited to delivering web content . Reasons for Usage IFF files are large and of very high quality . Baseline TIFF images are highly portable ; most graphics , desktop publishing , and word processing applications understand them . The TIFF specification is readily extensible , though this comes at the price of some of its portability . Many applications incorporate their own extensions , but a number of application-independent extensions are recognized by most programs . Four types of baseline TIFF images are available : bilevel ( black and white ) , gray scale , palette ( i.e. , indexed ) , and RGB ( i.e. , true color ) . RGB images may store up to 16.7 million colors . Palette and gray-scale images are limited to 256 colors or shades . A common extension of TIFF also allows for CMYK images . TIFF files may or may not be compressed . A number of methods may be used to compress TIFF files , including the Huffman and LZW algorithms . Even compressed , TIFF files are usually much larger than similar GIF or JPEG files . Because the files are so large and because there are so many possible variations of each TIFF file type , few web browsers can display them without plug-ins . Q2 ) What are image levels In some image formats the image data has a fixed amount of possible intensities . For instance an image may be defined as uint8 ( unsigned integer 8-bit ) which means that each pixel can have a value ( intensity ) between 0-255 , and each intensity is a whole number ( integer ) in that range . So that gives 256 possible intensity levels . Another way to interpret this would be layers . An RGB ( red green blue ) type image uses three layers to define colour ( a single layer would define a large-scale image , some image types contain more than 3 layers ) . For each pixel there are 3 intensity levels , 1 for each colour , are defined and together ( using a kind of mixing of the colours ) they define the colour of that pixels . Similarly for a grayscale there can be two levels i.e black and white Q3 ) What is Down-sampling and Up-sampling in Image processing Downsampling and upsampling are two fundamental and widely used image operations , with applications in image display , compression , and progressive transmission . Downsampling is the reduction in spatial resolution while keeping the same two-dimensional ( 2D ) representation . It is typically used to reduce the storage and/or transmission requirements of images . Upsampling is the increasing of the spatial resolution while keeping the 2D representation of an image . It is typically used for zooming in on a small region of an image , and for eliminating the pixelation effect that arises when a low-resolution image is displayed on a relatively large frame Now that we know all this we are good to go . I will be using openslide to display images as I learned it in this competition from a very informative kernel The benefit of OpenSlide is that we can load arbitrary regions of the slide , without loading the whole image in memory . Want to interactively view a slide ? We have added an interactive viewer to this notebook in the last section . You can read more about the OpenSlide python bindings in the documentation
963	So we are successful , we can easily use this function to visualize on more than example by doing something like this
964	This notebook assumes that you are familiar with Pytorch and have used it before to build models . If you have not , here are some useful links to learn pytorch from zero Using Pytorch is fairly easy , it 's just like using python only in a more advanced kind of way . Here is the link to the repository which contains Pytorch implementations on different architectures
965	PS : Above code only works when TPU is on
966	Now if you have used Pytorch with GPU 's before , you know that running your code and models on GPU 's is this simple device = `` cuda tensors.to ( device model.to ( device Thus While building Pytorch-XLA developers had the same thing in mind and they wanted to create something similar , so that the end users have the same feel and structure while using TPU 's as they had while using GPU 's Pytorch-XLA treats each TPU core as an individual XLA device and thus using a TPU core is as easy as device = xm.xla_device tensors.to ( device model.to ( device
967	Sending a model On TPU
968	Training on a Single TPU Core
969	I am using 5 Conv2D layers with max pooling and batch norm for my baseline
970	We define all the configuration needed elsewhere in the notebook here
971	We just hid a message in our image with two lines of code , Cool huh Let 's try looking at the image to observe if there is any difference
972	Now that we are familiar to the techniques of steganograhy , we can now move to exploration of data and steganalysis part
973	So the organizers have used four algorithms for encoding data into the cover images , they are [ JUNIWARD , JMiPOD , UERD UNIWARD - Universal Wavelet Relative Distortion Paper describing CNN for stegnalysis of [ JUNIWARD images JMiPOD Paper describing CNN for stegnalysis of [ JMiPOD images UERD - Uniform Embedding Revisited Distortion No resources found yet , I will update as soon as I find something For now just assume they are certain algorithms that are used for encoding data into image we will understand everything in a bit
974	Up untill now we were using difference in pixel values and normal pixel values for analysis and it has been argued that 's it is not the right way Lets now look at difference in DCT values and difference in pixels and see the differences , this will help us to make better decision
975	From the images it is hard to conclude which one works better , as images of both DCT and Pixel pattern give us the same underlying pattern . But one can argue that pixel values contain a lot of noise whereas DCT values do not and can get you a better model But from the number of non-zero difference values of both DCT and pixels we can see that Pixels have more non-zero values than DCT and hence are more distintive for differentiation between cover and stego images Let 's also look the images side by side to get a better unders
976	We will explore whether or not the name of the region is present in the name of ingredients , For Eg : - Greek Yogurt . If found this will help the model and an unigram model will perform better
977	Information about unidecode can be read from here
978	I tried LogisticRegression , GaussianProcessClassifier , GradientBoostingClassifier , MLPClassifier , LGBMClassifier , SGDClassifier , Keras but SVC was working the best , So I choose SVC to build our final model
979	Does long download delay time afftect download rate
980	Reading the Data
981	Let 's draw a Funnel-Chart for better visualization
982	In the previous versions of this notebook , I used Number of words in selected text and main text , Length of words in text and selected as main meta features , but in the context of this competition where we have to predict selected_text which is a subset of text , more useful features to generate would be Difference In Number Of words of Selected_text and Text Jaccard Similarity Scores between text and Selected_text Thus it will not be useful for us to generate features we used before as they are of no importance here For what who do n't know what Jaccard Similarity is
983	Let 's look at the distribution of Meta-Features
984	The number of words plot is really interesting , the tweets having number of words greater than 25 are very less and thus the number of words distribution plot is right skewed
985	Now It will be more interesting to see the differnce in number of words and jaccard_scores across different Sentiments
986	I was not able to plot kde plot for neutral tweets because most of the values for difference in number of words were zero . We can see it clearly now , if we had used the feature in the starting we would have known that text and selected text are mostly the same for neutral tweets , thus its always important to keep the end goal in mind while performing EDA
987	Cleaning the Corpus
988	Most Common words in our Target-Selected Text
989	OOPS ! While we cleaned our dataset we didnt remove the stop words and hence we can see the most coomon word is 'to ' . Let 's try again after removing the stopwords
990	So the first two common word was I 'm so I removed it and took data from second row
991	Most common words Sentiments Wise
992	Modelling the Problem as NER
993	For Full Understanding of the how to train spacy NER with custom inputs , please read the spacy documentation along with the code presentation in this notebook : Follow along from Updating Spacy NER
994	Training models for Positive and Negative tweets
995	Read the train , test and sub files
996	Make a dictionary for fast lookup of plaintext
997	Level 3 and level 4 - exploration
998	Cool , we found an exact match for level 3 and 2 possible matches .
999	It makes sense now . We found a match for level
1000	kaggleuser58 : Have you tried finding the corresponding plaintext letters for each group and see if you can see something if you sort the groups in the correct order - it might help in finding the solution .
1001	Frequency analysis on Level
1002	Frequency analysis on Level
1003	Let 's see MFCC of train data ( first 150,000 records
1004	The shape of MFCC is ( \ [ No . of features ( 20 by default ) \ ] , \ [ time\ ] ) . I tentatively create train data by calculating mean values along time axis for each 150000 train records ( same size as test data fragments ) .
1005	Let 's visualize train data .
1006	Some of mean MFCC feature values seem to have linear relationship with time_to_failure . Let 's try linear regression ( cross validation fold=10 ) .
1007	Also try XGBoost
1008	Import necessary libraries
1009	From the above graph , we can see that the distribution of scalar_coupling_coefficient is skewed to the left , but the distribution is not perfectly unimodal . There is a significantly smaller peak close to 100 making it bimodal . The mode and mean is approximately 0 .
1010	Visualize the distribution of dipole moments in X , Y and Z directions
1011	Visualize the distribution of potential energy for each molecule type
1012	Define helper function to remove outliers
1013	Import libraries and define hyperparameters
1014	Get testing tasks
1015	Extract training and testing data
1016	Matrix mean values
1017	Height vs. Width
1018	Define function to flatten submission matrices
1019	Prepare submission dataframe
1020	Import necessary libraries
1021	Define the paths for the train and test data
1022	Frequencies of the different product categories
1023	From the above plot , we can clearly see that the most common ProductCD value is W. The other four product categories , H , C , S , and R are very rare compared to W .
1024	Distributions of transaction amounts for different ProductCD values ( for non-fraudulent and fraudulent cases
1025	In the above violin plots , the light green sections represent the distribution for non-fraudulent cases and the dark green sections represent the distribution for fraudulent cases . We can see the same trend as before . Both the distributions types have strong positive ( leftward ) skews . But , the light green ( non-fraudulent ) distributions have a greater probability density concentrated around the lower values of transaction amounts , once again , because there are several peaks ( unlike the dark green distributions ) . But , there are a few exceptions to this trend in the above visualization . For example , the non-fraudulent and fraudulent distributions are very similar for the C product category . Both the distributions have only one peak and they look very similar in almost every way . Interestingly , the C product category also has the highest fraudulence rate , and this is probably the reason why the correlation between the transaction amount and the target is very low for this category . But , for the rest of the product categories , the trend is roughly followed . The distributions for the S product category seem to have the lowest means and the strongest skews . These distributions have a very high concentration of probability density around the lower values of TransactionAmt . Transactions of type S tend to have low transaction amounts . On the other side of the spectrum , the distributions for the R product category seem to have the highest means and the weakest skews . These distributions have a very even spread and almost no skew . Transactions of type R tend to have high transaction amounts . Maybe , one can create a model that changes the weightage of the TransactionAmt feature based on the product category . For example , the weightage of the TransactionAmt feature can be reduced for the C product category , because the fraudulent and non-fraudulent distributions are very similar for this product category .
1026	Frequencies of the different email domains ( P
1027	Fraudulence Proportion Plot
1028	TransactionAmt vs. P_emaildomain Violin Plot
1029	TransactionAmt vs. P_emaildomain Box Plot
1030	Frequencies of the different email domains ( R
1031	Fraudulence Proportion Plot
1032	TransactionAmt vs. R_emaildomain Violin Plot
1033	TransactionAmt vs. R_emaildomain Box Plot
1034	Frequencies of the different card brands
1035	Fraudulence Proportion Plot
1036	TransactionAmt vs. card4 Violin Plot
1037	TransactionAmt vs. card4 Box Plot
1038	Frequencies of the different card types
1039	Fraudulence Proportion Plot
1040	TransactionAmt vs. card6 Violin Plot
1041	TransactionAmt vs. card6 Box Plot
1042	Convert categorical string data into numerical format
1043	Create final train and validation arrays
1044	Build and train LightGBM model
1045	Visualize feature importances
1046	This is a simple neural network model that consists of two hidden layers with 10 neurons each . The activation function used is ReLU and the activation function for the last layer is sigmoid because a probability between 0 and 1 needs to given as output . Steps Pass the input vector through a dense layer with 10 neurons Pass the output of that layer through another dense layer with 10 neurons Finally , pass the output of the previous layer through a dense layer with one neuron with a sigmoid activation function ( to output a probability between 0 and
1047	Visualize change in accuracy
1048	Visualize change in loss
1049	Set hyperparamerters and paths ( adjust these to improve CV and LB : D
1050	Split 300,000 images into 8 folds ( for cross-validation
1051	Define binary cross entropy and accuracy ( for backpropagation
1052	Initialize constants for data extraction and training
1053	Number of characters in the sentence
1054	Number of words in the sentence
1055	Average Word Length
1056	Tokenize and pad the sentences
1057	The squash activation function to use with the Capsule layer
1058	Save model weights and architecture
1059	Import necessary libraries
1060	Download training data and extract necessary data
1061	Create Perspective API Client with Google Cloud API key
1062	Mean Absolute Error
1063	Mean Squared Error
1064	The hyperparameters used are documented below Data processing related N_MELS is the number of melspectrogram features per time step . MEL_LEN is the total number of time steps in each melspectrogram . AMPLITUDE represents the default signal amplitude applied to unreadable files . SR is the sampling rate at which the audio is loaded ( readings per second ) . It defaults to 44100 Hz . TSR is the sampling rate at which the test audio clips are loaded . It defaults to 32000 Hz . MAXLEN is the maximum number of readings . Longer clips will be trimmed and shorter ones will be padded . SPLIT represents the fraction of data to be used for training . The rest of the data is used for validation . CHUNKS represents the number of chunks that will be extracted from each signal . It defaults to 1 per signal . CHUNK_SIZE represents the sequence length of each audio chunk to be fed into the melspectrogram function . POP_FRAC is the maximum proportion of signal information to be ignored per chunk ( defaults to 0.25 ) . Modelling related F represents the number of total number of features output by the ResNet model ( defaults to 512 ) . DROP represents the dropout rate between the feature layer and final 264D output layer ( 0.2 ) . LEARNING_RATE represents the learning rate of the model . It defaults to a low value of 1e-3 . BATCH_SIZE is the size of each minibatch to be fed into the model while training ( defaults to 64 ) . VAL_BATCH_SIZE is the size of each minibatch to be fed into the model while validating ( defaults to 64 ) . EPOCHS represents the number of times the training dataset is fed into the model ( defaults to 100 ) . Submission related MAX_OUTPUTS is the maximum number of bird species to be output per audio clip ( defaults to 3 ) . THRESHOLD is the minimum probability required for a bird species to be selected ( defaults to 6e-3 ) . MIN_THRESHOLD is the minimum probability required in a list of probabilities to assign any bird species . Note : If the maximum probability falls below MIN_THESHOLD , nocall will be predicted .
1065	The paths used are documented below Metadata related input/birdsong-recognition/test.csv contains the test metadata used for submission . input/birdsong-recognition/train.csv contains the train metadata used for submission . input/prepare-check-dataset/test.csv contains the test metadata used for committing . Audio data related input/birdsong-recognition/test_audio contains the test audio used for submission . input/birdsong-recognition/train_audio contains the train audio used for submission . input/prepare-check-dataset/test_audio contains the test audio used for committing . Spectrogram related IMG_PATHS contains a list of folders with the already generated training spectrograms .
1066	Prepare the label dictionary
1067	Define functions to process audio signals ( unhide code
1068	Split train/val ( 80/20 ) and declare PyTorch datasets and dataloders
1069	Declare model and optimizer
1070	Define categorical cross entropy and accuracy
1071	Run the inference loop
1072	Import necessary libraries
1073	Remove the numbers
1074	Remove the exclamation , question and full stop marks
1075	This function removes the most common words used in English ( stop words ) like ' a ' , 'is ' , 'are ' etc . Eg . He is a very humorous person . -- > He very humorous person .
1076	Replace elongated words with the basic form
1077	This function `` stems '' the words in the comments . It only keeps the stem of the word , which need not be an actual word . Eg . I love swimming and driving happily -- > I love swimm and driv happi
1078	The function lemmatizes the words in the comments . It only keeps the lemma of the actual words , which needs to be an actual word . Eg . I love swimming and driving happily -- > I love swim and drive happy
1079	Build neural network
1080	Split the data into training and validation sets
1081	First , we will build a model with two hidden layers ( 100 neurons each ) and a single-neuron layer with a sigmoid activation at the end to output the probability .
1082	Now , we build another model . The architecture is the same as last time , but this time , there are 10 neurons in each hidden layer instead of 100 .
1083	Make predictions on training and validation data from the models
1084	First , we will build a model with two hidden layers ( 20 neurons each ) and a single-neuron layer with a sigmoid activation at the end to output the probability .
1085	Now , we will build a new model with the same architecture , but with 25 ( instead of 20 ) neurons per hidden layer , and a single-neuron layer with a sigmoid activation at the end to output the probability .
1086	Check the accuracy values for model 1 , model 2 , and their ensemble
1087	Visualize the accuracy for model 1 , model 2 , and their ensemble
1088	Difference of the accuracies of the models above 96 .
1089	First , we will build a model with two hidden layers ( 200 neurons each ) and a single-neuron layer with a sigmoid activation at the end to output the probability .
1090	Make predictions on training and validation data from the models
1091	Import necessary libraries for data manipulation , tokenization and PoS Tagging
1092	Convert 2D tuples in pos_tags to lists
1093	Define helper function to deal with the case of a tag not being there in a sentence and avoid a KeyError .
1094	Create a pandas dataframe containing the PoS features dictionary and leave a dummy column for the tags .
1095	VBD '' , `` VBG '' and `` VBZ '' : Simple Past , Present-Participle and 3rd Person Verb Tag
1096	Initialize necessay constants
1097	Extract the acoustic data and targets from the dataframe
1098	Break the data down into parts
1099	Scaling the signals
1100	Prepare the final signal features
1101	Implement the feature generation process
1102	Bivariate KDE distribution plot
1103	Scatterplot with line of best fit
1104	Bivariate KDE distribution plot
1105	Scatterplot with line of best fit
1106	Bivariate KDE distribution plot
1107	The hexplot is also darkest around a negatively-sloped line .
1108	The KDE plot has highest density ( darkness ) around an almost vertical line .
1109	The hexplot also has highest density around an almost vertical line .
1110	Import necessary libraries
1111	Extract seismic data and targets and delete the original dataframe
1112	Cut out segments of size 150k from the signal
1113	The mean absolute deviation
1114	The butterworth high-pass filter with SOS filter
1115	In this denoising method , we take a `` window '' with a fixed size ( like 10 ) . We first place the window at the beginning of the time series ( first ten elements ) and calculate the mean of that section . We now move the window across the time series in the forward direction by a particular `` stride '' , calculate the mean of the new window and repeat the process , until we reach the end of the time series . All the means we calculated are concatenated into a new time series , which is the denoised signal .
1116	Initialize necessay constants
1117	Extract the acoustic data and targets from the dataframe
1118	Break the data down into parts
1119	Scaling the signals
1120	Prepare the final signal features
1121	Implement the feature generation process
1122	Bivariate KDE distribution plot
1123	Scatterplot with line of best fit
1124	Bivariate KDE distribution plot
1125	Scatterplot with line of best fit
1126	Bivariate KDE distribution plot
1127	Scatterplot with line of best fit
1128	Load the data
1129	Average smooting is a relatively simple way to denoise time series data . In this method , we take a `` window '' with a fixed size ( like 10 ) . We first place the window at the beginning of the time series ( first ten elements ) and calculate the mean of that section . We now move the window across the time series in the forward direction by a particular `` stride '' , calculate the mean of the new window and repeat the process , until we reach the end of the time series . All the mean values we calculated are then concatenated into a new time series , which forms the denoised sales data .
1130	The above plot compares the sales distribution for each store in the dataset . The stores in California seem to have the highest variance in sales , which might indicate that some places in California grow significantly faster than others , i.e . there is development disparity . On the other hand , the Wisconsin and Texas sales seem to be quite consistent among themselves , without much variance . This indicates that development might be more uniform in these states . Moreover , the California stores also seem to have the highest overall mean sales .
1131	Loss for each model
1132	Import necessary libraries
1133	Load images from the selected rows
1134	Create dictionary for cultures and tags
1135	Structure the labels into a list of lists
1136	Visualize some images from the data
1137	Set hyperparamerters and paths
1138	Load .csv data
1139	Convert Gleason scores to list format
1140	Visualize ResNet architecture
1141	Define cross entropy and accuracy
1142	Declare the necessary constants
1143	X coordinate vs. Yards
1144	Y coordinate vs. Yards
1145	X coordinate vs. Y coordinate
1146	Dir vs. Yards
1147	In the plot above , we can see that the distribution of A is asymmetrical , unimodal , and heavily skewed to the right . The probability density of the acceleration peaks at around A = 1 yard per second per second .
1148	S is the speed of the player in yards per second .
1149	In the plot above , we can see that the distribution of S is asymmetrical , unimodal , and heavily skewed to the right . The probability density of the speed peaks at around S = 2 yard per second .
1150	In the plot above , we can see that the distribution of humidity in the dataset has a slight leftward skew and is bimodal in nature . The distribution has two peaks at around Humidity = 0 and 70 . The first peak , at Humidity = 0 is very sudden and goes against the gentle leftward skew of the data .
1151	Temperature is the temperature ( in degrees Fahrenheit ) during the game .
1152	In the plot above , we can see that the distribution of humidity in the dataset has a slight leftward skew and is roughly unimodal in nature . The distribution has one peak at around Temperature = 60 degrees Fahrenheit .
1153	In the above plot , we can see that most distributions are similar . But , the few teams that have the highest average yards gained are the Oakland Raiders , New England Patriots , and Minnesota Vikings . On the other hand , the teams that have the lowest average yards gained are the Jacksonville Jaguars and Carolina Panthers .
1154	Get categorical value sets
1155	Define helper functions to generate categorical features
1156	Define helper functions to generate numerical features
1157	Generate training and validation sets ( 80 % split
1158	Visualize neural network architecture
1159	Calculate the data mean and standard deviation for normalization
1160	Build a simple Deep Neural Network using Dense , Dropout and BatchNormalization layers in Keras
1161	Wordcloud of all comments
1162	Languages
1163	Bar chart of non-English languages
1164	World plot of non-English languages
1165	We can see that German and English are the most common European languages to feature in the dataset , although Spanish and Greek are not far behind .
1166	This plot shows that middle-eastern languages such as Arabic and Persian are represented more than languages from the Indian subcontinent or south-east Asia , such as Hindi , Vietnamese , and Indonesian . There is not a single comment in Mandarin , Korean , or Japanese
1167	Average comment length vs. Country
1168	Negative sentiment refers to negative or pessimistic emotions . It is a score between 0 and 1 ; the greater the score , the more negative the abstract is .
1169	Negativity vs . Toxicity
1170	Positive sentiment refers to positive or optimistic emotions . It is a score between 0 and 1 ; the greater the score , the more positive the abstract is .
1171	Positivity vs . Toxicity
1172	Neutrality sentiment refers to the level of bias or opinion in the text . It is a score between 0 and 1 ; the greater the score , the more neutral/unbiased the abstract is .
1173	Neutrality vs . Toxicity
1174	Compoundness sentiment refers to the total level of sentiment in the sentence . It is a score between -1 and 1 ; the greater the score , the more emotional the abstract is .
1175	Compound sentiment vs . Toxicity
1176	Readability
1177	Distribution of Flesch reading ease
1178	Flesch reading ease vs . Toxicity
1179	Distribution of automated readability
1180	Automated readability vs . Toxicity
1181	Distribution of Dale-Chall readability
1182	Pie chart of targets
1183	Define function to tokenize ( encode ) comments
1184	Setup TPU configuration
1185	Load BERT tokenizer
1186	Encode comments and get targets
1187	Define training , validation , and testing datasets
1188	Build model and check summary
1189	Define ReduceLROnPlateau callback
1190	Train the model
1191	Build model and check summary
1192	Train the model
1193	Build the model and check summary
1194	Train the model
1195	Build the model and check summary
1196	Train the model
1197	Build the model and check summary
1198	Train the model
1199	Define hyperparameters and load data
1200	Load the data and define hyperparameters
1201	Load sample images
1202	All channel values
1203	Red channel values
1204	Green channel values
1205	Blue channel values
1206	All labels together ( parallel plot
1207	Blurring
1208	Setup TPU Config
1209	Load labels and paths
1210	Define hyperparameters and callbacks
1211	DenseNet
1212	EfficientNet
1213	EfficientNet NoisyStudent
1214	Ensembling
1215	Original features
1216	Set hyperparameters and paths ( adjust these to improve LB scores : D
1217	Load .csv data ( to access 3D fMRI maps for training and validation
1218	Visualize ResNet architecture ( with pytorchviz
1219	Define custom weighted absolute error loss ( for backpropagation
1220	Split data into training and validation sets ( to validate performance properly
1221	Define hyperparameters and load data
1222	Define cross entropy and accuracy
1223	Visualize loss and accuracy over time
1224	Define hyperparameters and paths
1225	Get image path dictionary
1226	Define binary cross entropy and accuracy
1227	Split train/val
1228	Define sampling weights
1229	Define PyTorch datasets
1230	Define sampling procedure and DataLoader
1231	Define model and optimizer
1232	DNA）とリボ核酸（RNA）がある DNAはアデニン ( A ) ・グアニン ( G ) ・シトシン ( C ) ・チミン ( T ) から構成され、RNAはアデニン ( A ) ・グアニン ( G ) ・シトシン ( C ) ・ウラシル ( U ) から構成される DNAは二重らせん構造と呼ばれる安定的な構造を持っており、AとT、CとGがそれぞれ水素結合をする ( ワトソン・クリック塩基対 ) ことで二本鎖の構造を持つ DNAは以下の働きを持つ RNAの転写を指令する RNAでは様々な役割と種類を持つが、今回のコンペで主題となっているRNAはメッセンジャーRNA（mRNA）である RNAはDNAに結合したポリメラーゼによって合成される分子である DNAから合成された分子の扱いが異なり、原核生物であれば合成された分子をそのままmRNAとして扱われるが真核生物であれば合成された分子はmRNA前駆体として扱われ、塩基配列の一部が切り出されることでmRNAが完成となる（スプライシング）。このとき、切り出される部分はイントロンと呼ばれ、残される部分はエキソンと呼ばれる mRNAはタンパク質の合成に用いられる mRNAの3塩基ごとにアミノ酸が１つ合成される。（トリプレット overview COVID-19へのワクチンとしてmRNAワクチンは大きな期待が寄せられている mRNAは非常に安定性に欠けるため、ワクチンとしての機能を担保するためには超低温で保存する必要があり、このままでは世界中にワクチンを配布することは難しいと思われる RNA分子には自然に分解される傾向があることが確認されているが、どのRNA構造が分解されやすいのかは知見が少ない RNA分子の各塩基における分解率の予測を行う Eternaデータセットが与えられている RNA分子の配列・構造・各位置における分解率が与えられている Dataset mRNA分子3029個について、すでに主催者が分解率のラベリングを終えている mRNAの末端については取得が難しいため、全長107塩基の場合、最初の68塩基についての情報が与えられている Public LBでスコアリングが行われる Private LBに使用されるデータは全長130塩基のmRNAが使用される seq_scored列に記載されている train data 各列の解説 seq_scored ：IDのRNA分子がどの塩基までスコアリングに使用されているのか（全長107：68 , 全長 seq_length ：IDのRNA分子の延期の長さ（107 , 130の2種類 sequence ：塩基配列（A/U/G/Cの並び方 structure ： ( ) と.で表している。 ( ) はペアになっている ( 結合している ) 塩基同士を表す reactivity ：各塩基における反応性について。RNAの二次構造を決定している deg_pH10 ：pH10における各塩基の反応性をみる ( 予測提出が求められるがスコアリングには使用されない deg_Mg_pH10 ：pH10かつマグネシウムの入った溶媒における各塩基の反応性をみる deg_50C ：50℃における各塩基の反応性をみる ( 予測提出が求められるがスコアリングには使用されない deg_ng_50C ：50℃かつマグネシウムの入った溶媒における各塩基の反応性をみる error_ ：reactivity , deg_列について実験上のエラー率 predicted_loop_type：各アルファベットはそれぞれ構造を表すS : paired `` Stem '' M : Multiloop I : Internal loop B : Bulge H : Hairpin loop E : dangling End X : eXternal loop public data setに使用されるtest.jsonファイルのデータ（RNA分子629個）は信頼性の高いデータを使用している target colsとして使用されている5列がいずれも‐0.5以上 target colsとして使用されている5列のsignal to noise平均が1を上回る（signal_to_noise列 train.jsonとtest.jsonの合計3029分子は塩基配列の相似度でクラスタリングを行われており、test.jsonのRNA分子はクラスターの分子が3種類以下のクラスターを用いている private LBのデータには用いられない private LBにも適用されるようになった。3005RNA分子中、上記フィルタリングを通過した信頼性の高いデータのみをスコアリングに用いる。なお、コンペ期間中にはどれがフィルタリングを通過しているかは不明 bppsファイルについて RNA分子内で結合し構造を構成する確率を表している RNA分子構造のビジュアライズについて Evaluation MCRMSEを使用して評価が行われる MSE（平均二乗誤差 reactivity , deg_Mg_pH10 , deg_pH10 , deg_Mg_50C , deg_50Cの5列 Additional resources mRNA：メッセンジャーRNA。通常DNAの塩基対として構成され、リボソームにてタンパク質を合成する役割を持つ RNAワクチン：ｍRNAがコードするタンパク質を抗原タンパク質の構造の一部とすることで免疫機構に抗原タンパク質の構造を記憶させる mRNAはDNAと異なり一本鎖で構成される（DNAは二本鎖）ため、安定性が劣り、すぐに分解されてしまう RNA分子内で結合構造を樹状に持つように設計することが考えられた image.png ] ( attachment : image.png
1233	Read Numpy File
1234	image.png ] ( attachment : image.png
1235	Imbalanced classes are a common problem in machine learning classification where there are a disproportionate ratio of observations in each class . With just 6.6 % of our dataset belonging to the target class , we can definitely have an imbalanced class This is a problem because many machine learning models are designed to maximize overall accuracy , which especially with imbalanced classes may not be the best metric to use . Classification accuracy is defined as the number of correct predictions divided by total predictions times 100 . For example , if we simply predicted that all questions are sincere , we would get a classification acuracy score of This competition uses the F1 score which balances precision and recall . Precision is the number of true positives divided by all positive predictions . Precision is also called Positive Predictive Value . It is a measure of a classifier 's exactness . Low precision indicates a high number of false positives . Recall is the number of true positives divided by the number of positive values in the test data . Recall is also called Sensitivity or the True Positive Rate . It is a measure of a classifier 's completeness . Low recall indicates a high number of false negatives .
1236	We will use the [ datablock ] ( API from fastai , it is so elegant ! We create an ImageItemList to hold our data , using the ` .from_df ( ) ` method . Then we split the data in train/valid sets , I will use 0.2 ( 20 % of data for validation ) and a seed=2019 , to be able to reproduce my results . Finally we add the test set , nice trick we just sum the lists to get the whole .
1237	We have to add some data augmentation , ` get_transforms ( ) ` get us a basic set of data augmentations . And we set size=128 to train faster , you can test with 256 , but have to reduce the batch size ( bs=16 ) . We have to normalize ( substract the mean , divide by std ) data for the GPU to work better , as I have not computed the actual value , I will just use ImageNet stats .
1238	Let 's impement the competition metric , luckyly it is already implemented in sklearn . We have to modify it a little bit , First , fastai expects a pair ( preds , targets ) and sklearn expects ( targets , preds Secondly , sklearn needs to vectors of equal shape . For our case , ` preds ` has shape ( bs , 2 ) , so we take the second column , the one that contains the probabilities of palmoil
1239	For some extrange reason thie metric does not always work as a callback for the learner .
1240	Equal number of train and test samples
1241	Importing the useful functions , packages and others .
1242	Some data visualization , first see what we got and then we can start cleaning up the dataset .
1243	If there is any -1 , according to the data description , it indicates the feature was missing from the observation . So let 's change it for NaN in a copy of our train .
1244	The following code will allow us to see how many NaN we have in the dataset , I 'm taking a paranohic approach but , this should be useful , I guess this should at least give me the name of the colums for further pre-processing .
1245	You can see a big inbalance in the target , there are only a few amount of people the claim was filed .
1246	I would like to see some correlation plots about the different features , I will start with the float features we captured in the last step from previous sections
1247	I chossed this colors because you have right away good insights of the data , if you see green , the features are no correlated , but if you see something not green , you will identify some correlation . They are somehow correlated ps_reg_01 with 02 and ps_reg_01 , ps_reg_03 = 0. ps_reg_02 , ps_reg_03 = 0. ps_reg_01 , ps_reg_02 = 0 . You will think since , 12 and 15 are related with 13 you will see some form of correlation between them , but is close to zero as you can see in the chart . ps_car_13 , ps_car_12 = 0. ps_car_13 and ps_car_15 = 0 . Since the features ps_calc_03 , 02 and 01 are not correlated to anything , I will retire them to have a more condensed graphic .
1248	Now this is a really good condensed chart where you can see right away all the features correlation
1249	We can see with more detail the features that are correlated and negative correlated ps_ind_06_bin and 07 are ngative corrleated -0. ps_ind_11_bin ps_ind_14 = 0. ps_ind_12_bin with ps_ind_14 = 0. ps_ind_13_bin with ps_ind_14 = 0.43 ( just because 14 is related to the others ps_ind_17_bin with ps_ind_16_bin -0. ps_ind_18_bin with ps_ind_16_bin -0. ps_ind_18_bin with ps_ind_15 -0 . And finally just as to have the check in the final feature map and correlations ( taking away id and target
1250	Binary Features
1251	I tried to produce the binary plot with plot.ly and with matplot lib , you can see right away the difference between both packages . I did n't want to copy that portion but , they are better graphics ! I will transform all the graphics untl now , at some point . This graph is basically telling , features ps_ind_10_bin , 11 , 12 and 13 are basically useless . I will remove them from the feature correlaton map and redraw it , to see if I can spot something .
1252	ps_car_07_cat is totally unbalanced as well as 08 , I think 03 and 05 will be more balanced if we took away the NaN . Remember Column : ps_car_02_cat has 5 NaN Column : ps_car_03_cat has 411231 NaN Column : ps_car_05_cat has 266551 NaN Column : ps_car_07_cat has 11489 NaN I would also exclude 07 and even 02 and 08 from some of the algorithms , in order to save some features . we will see what 's next .
1253	Generating a first impression with the Linear and Non linear algorithms ( now it 's time to get some coffee , even 3 or 4 cups ) . Most of them are relatively fast to train , but the KNN it takes forever to do their job .
1254	I wanted to see in a single plot all the feature importance , so take a look to this piece of code ! : ) most of the ideas I took it from this [ link
1255	We 'll train the model using Cross Entropy Loss for 1000 epochs . We can see that the model converges nicely .
1256	First Task : db3e9e
1257	Now lets see if it at least correctly outputs the training set . To be save we 'll give the model $ n=100 $ steps
1258	It works ! Now lets see if it generalized to the test question
1259	We solve many of the tasks within the training set using our Neural Cellular Automata model ! I did test on the validation set as well , and it correctly solved 17 of the tasks . There are a number of ways this model could be improved . Please let me know if you 'd be interested in collaboration Solved Tasks
1260	Distributions of center_x and center_y
1261	Distribution of center_z
1262	Distribution of yaw
1263	width is simply the width of the bounding volume in which the object lies .
1264	length is simply the length of the bounding volume in which the object lies .
1265	height is simply the height of the bounding volume in which the object lies .
1266	Frequency of object classes
1267	In the violin plots above , we can see that the distributions of center_x for large vehicles including trucks , buses , and other vehicles are well spread . They barely have any skew and have greater means than the distributions for pedestrians and bicycles . This is probably because these large vehicles tend to keep greater distances from the other vehicles , and the smaller vehicles do not stay too close to these large vehicles in order to avoid accidents . Therefore , the mean center_x is clearly greater for larger vehicles like buses and trucks . Contrastingly , the smaller objects like pedestrians and bicycles have center_x distributions with strong positive ( rightward ) skews . These distributions also have clearly lower means than the distributions for the larger vehicles . This is probably because pedestrians ( road-crossers ) and bicyclists do not need to maintain large distances with cars and trucks to avoid accidents . They usually cross the road during a red traffic signal , when the traffic halts .
1268	In the violin plots above , we can see that the distributions of center_y for small objects including pedestrians and bicycles have a greater mean value than large objects like trucks and buses . The distributions for the small objects have much greater probability density concentrated at higher values of center_y as compared to large objects . This signifies that small objects , in general , have greater center_y values than large objects . This is probably because the large vehicles tend to be within the field of view of the camera due to their large size . But , smaller objects like bicycles and pedestrians can not remain in the field of view of the camera when they are too close . Therefore , most pedestrains and bicycles that are detected tend to be far away . This causes the center_y to be greater ( on average ) for small objects as compared to large objects .
1269	In the violin plots , we can clearly see that the width distributions for large vehicles like cars , buses , and trucks have much larger means as compared to small objects like pedestrians and bicycles . This is not surprising because trucks , buses , and cars almost always have much greater width than pedestrians and bicycles .
1270	In the violin plots , we can clearly see that the length distributions for large vehicles like cars , buses , and trucks have much larger means as compared to small objects like pedestrians and bicycles . This is not surprising because trucks , buses , and cars almost always have much greater length than pedestrians and bicycles .
1271	In the violin plots , we can clearly see that the length distributions for large vehicles like buses and trucks have much larger means as compared to small objects like pedestrians and bicycles . This is not surprising because trucks and buses almost always have much greater length than pedestrians and bicycles . The only exception to this trend are the cars . They tend to have a similar height to that of pedestrians .
1272	Create a function to render scences in the dataset
1273	Next , let us render a pointcloud for a sample image in the dataset . The pointcloud is basically a set of contours that represent the distance of various objects as measured by the LiDAR . Basically , the LiDAR uses light beams to measure the distance of various objects ( as discussed earlier ) and this distance information can be visualized as a set of 3D contours . The colours of these contour lines represent the distance . The darker purple and blue contour lines represent the closer objects and the lighter green and yellow lines represent the far away objects . Basically , the higher the wavelength of the color of the contour line , the greater the distance of the object from the camera .
1274	Images from the back camera
1275	Images from the front-left camera
1276	Images from the front-right camera
1277	Images from the back-left camera
1278	Images from the back-right camera
1279	LiDAR data from the top sensor
1280	LiDAR data from the front-left sensor
1281	LiDAR data from the front-right sensor
1282	Get the Data ( Collect / Obtain Loading all libs and necessary datasets
1283	While the time series appears continuous , the data is from discrete batches of 50 seconds long 10 kHz samples ( 500,000 rows per batch ) . In other words , the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000 , and thus discontinuous between 50.0000 and 50.0001 .
1284	Test Data Analisys
1285	Remove Drift from Training Data
1286	As we can see in figure below , the drift removal makes signal closer to a normal distribution .
1287	The model can score 0.938 on LB without further optimization . Maybe with some GridSearch for parameters tunning or more features engineering , it 's possible to get to 0 .
1288	Import Packages and Define Encoder Methods
1289	I decided to limit my k-modes clustering assessment to variables that contain a `` reasonable '' number of catagories . I removed nom_5 thru nom_9 as well as ord_5 since they were too large to properly visualize in the end .
1290	We need to properly fill missing data . We will do this by filling NaNs with the mode for each column . We will also print the number of unique catagories per column to help us understand the data .
1291	The only thing that needed to be done to the binary features was to encode bin_3 and bin_4 .
1292	For the nominal features , we will use a simple Label Encoder .
1293	Since these are ordinal features , we would like to maintain the positional information within . Therefore , we will apply our own transformation functions and dictionaries to the features .
1294	The time features are starting at an index of 1 currently . We will use a simple lambda function to start the time features at index of 0 in order to align with the other features .
1295	As an additional visualization tool , lets take at a correlation heatmap for these features . Note that I am not doing any sort of normalization to the catagorical dataFrame I have created , which is typically a good practice . I am however looking at Spearman instead of Pearson correlation as Spearman is typically better for ordinal data . Based on the heatmap below , I 'm not seeing much correlation between variables other than the 'target ' variable . Perhaps there is some interdependence in the variables that we decided to remove from the analysis , but for the variables we chose to analyze we do not see that . The largest correlation is between target and ord_3 , and the ordinal features in general look to have a larger correlation than the the binary or nominal features .
1296	Now we can begin our cluster analysis . First , let 's do a sweep of the number of clusters and look at how cost changes as we increase the cluster number . I will be using the elbow method to determine the optimal number of clusters . The elbow of the cost plot represents the point where we stop seeing significant improvement in our clustering cost function as we continue to increase the number of clusters .
1297	We will go with 2 Clusters for our analysis , since it looks like our optimal number of clusters based on the elbow in the above cost plot AND it aligns with the fact we have a binary target . Once we have our data labeled with their appropriate cluster we can append this data to our original train dataFrame for further visualization .
1298	In order to determine any trends or insights from the clustered data , we will look at count plots for each feature . Please let me know if you have any other interesting ways to visualize this type of cluster analysis .
1299	Low Pass Filtering By Batch
1300	Now just apply LPF to train and test data . We will start by applying our filter to one batch at a time , however we could see improvement by only applying the filter to batches we know were helped .
1301	We see that there is some differences in the distribution of first_active_month .
1302	There is a higher chance of feature_1 == 5 , given that the target is -33 .
1303	There is a higher chance of feature_2 == 3 , given that the target is -33 .
1304	There is a higher chance of feature_3 == 1 , given that the target is -33 .
1305	Lets remove those unnecessary symbols , which might be problem when tokenizing and lemmatizing
1306	Lemmatization converted it back to list , so change to str again and remove the unncessary words .
1307	Import Necessary Packages
1308	We will be using a LightGBM regressor model to perform our lofo importance analysis
1309	This is where the LOFO importance analysis occurs . The main LOFOImportance method takes the dataset ( train data + target ) , specified model , kfold validation setup , and scoring metric as inputs . It will the output a sorted dataframe from best to worse performing features . The importance dataframe will include feature importance mean and standard deviation details
1310	Load all the data as pandas Dataframes
1311	We have merged all the tables now , we can start working on exploring the data without having to worry about joining multiple table . The data is present in regseasoncompactresults and tourneycompactresults tables .
1312	Based on the visualizations , the top seeds won most of the matches and lost fewer matches relatively . So its safe to assume that this is a key feature for our model both in tourneys and regular seasons .
1313	Based on the distribution , the score tends to be higher in Tourney 's than in regular seasons . Clearly , the score of winners is higher than the score of losers both in tourneys and Regular seasons .
1314	The code below is based on [ the amazing script .
1315	We see that variables `` ` ps_ind_10_bin `` ` , `` ` ps_ind_11_bin `` ` , `` ` ps_ind_12_bin `` ` and `` ` ps_ind_13_bin `` ` have almost all 0s and therefore may not be of much use in prediction . A feature selection step in pipeline will determine whether to include them or not . It is important to note that feature selection should be performed DURING cross validation and not before . In short , feature selection should not use the data from validation set in CV . For more information check out the upsampling section in ensemble and CV notebook . Still , to get a feel for data , lets check for similarity between features . To check similarity between 2 binary variables , we will XOR each 's row element and count the percentage of 0s and 1s
1316	The heatmap gives us some insights into the most important variables . For example , lightly colored columns are most uncorrelated fromall other variables . These are features like - `` ` ps_ind_06_bin `` ` , `` ` ps_ind_16_bin `` ` , `` ` ps_calc_16_bin `` ` , `` ` ps_calc_17_bin `` ` , `` ` ps_calc_19_bin In the same way , lets check similarity of each feature with the target variable to visualise each features prediction power .
1317	We see that only `` ` ps_reg_03 `` ` and `` ` ps_car_14 `` ` have significant number of missing values . Apart from that , `` ` ps_car_11 `` ` has 5 and `` ` ps_car_12 `` ` has 1 missing value . Lets evaluate the chi squared test between each of continuous variables and the target variable
1318	It seems like `` ` ps_reg_03 `` ` and `` ` ps_car_14 `` ` are fairly independant of the target variable . Again , a more formal feature selection will be performed during cross validation Lets evaluate the pearson correlation between between each of continuous variables to see if two features are highly correlated and therefore present redundant information .
1319	Evaluation Criteria for Predictions
1320	To evaluate performance of our model and to ensure that our it generalizes well over new data , we do a 10 fold cross-validation . Folds will be stratified to ensure equal proportions of target variable in each . In each fold We upsample the positive target data from training fold Choose best features using training fold Train the model over training fold and evaluate it over the hold out validation fold .
1321	How about plotting the TransactionDT day wise
1322	Data Loading and Cleaning
1323	Confirmed Cases and Deaths Across Countries/Cities
1324	Time Series Plots Per Continent and Country
1325	Time Series Bar Chart of Cases per Country
1326	Interactive Time Series Map
1327	Google Trends Exploration Google Trends presents a good opportunity to track the public 's interest in a topic in real time and across time . It has been used in academic research to predict Zika virus outbreak , Influenza and Dengue fever . Here I will be using Google search queries of the keywords : 'coronavirus ' and 'COVID-19 ' to explore the relationship between popularity of the virus in search queries and the actual confirmed cases for a few countries . NOTE : The google trends data is normalized by Google and I do not think there is a way to look at the absolute counts of the search queries . They are on a scale of 0-100 , larger representing a higher proportion search or popularity of the keyword in the country . That means that for a country ( e.g. , San Marino ) with very little search queries but a high proportion of those are related to the virus , the score will be higher than a country ( e.g. , Singapore ) , which has a high amounts of search qeuries but a lower proportion of coronavirus related searches . So although San Marino has a higher score than Singapore , it does not mean that it has a higher coronavirus-related search frequency compared to Singapore . Therefore , when comparing between countries , we are limited to looking at the correlation of Google search queries proportion and confirmed cases/fatalities rather than a direct comparison of the search queries counts .
1328	Relationship betwen Google search queries and Confirmed Cases
1329	Auto-ARIMA on Iran Example
1330	We take time series columns from [ here
1331	Dog Image
1332	Why do Kaggle Learning new things strenghtnen intuition for ml algorithms and techniques like competing with fellow kagglers
1333	Problem statement
1334	Pointers Check out existing kernels Check distributions Compare train and test distributions Identify important features ( Most of the times feature engineering is going to be around features with high predictive power Attach a logic to why featurea are important ( Note : data is anonymised here so hard to do this Check previous solutions to similar problems Observations Data normalization and imputation Weak corelations between features and target IV values Most variables have distribution close to normal Almost no corelation between differnt variable - What does it mean No NA values ( already imputed Some features seem to have been clipped at one end Spikes in distributions ( imputed values less unique
1335	Method -1 : train on full and predict on test
1336	Pointers Validation strategy -- Random KFold , holdout or temporal split What to trust validation score or LB socre ? ? trust score from more data ; if test data is more we should treat LB as additional fold Hyperparamter tuning -- Combination of manual tuning and bayesian optimization libraries like ` hyperopt ` and ` scikit-optimize ` . Initial tuninng on single fold and then move to 5 folds . Always check validation and test set prediction distributions Read forums and participate in discussions Disussions Sometimes using geometric mean of probabilities is better than using simple mean When metric is ROC_AUC , even rank average can be used Blending -- blend of your solution and public solution can be used to improve LB score . But , better approach is to understand what is working for other people and integrate in your models .
1337	Wrappers for different algorithms
1338	Update : sklearn example
1339	Understanding the dataset
1340	Now finally reading the testset from the test_folder and predicting on the values from sample submission .
1341	Now from the vector_distances we have calulated above , we are now going to transform these distances into probablity of being same .
1342	If we look at a seven day period we can see that there is a cyclical movement within each day , but there is n't much difference between days . We would expect that the transaction volumns are lowest near midnight . We can calculate the hour of each day when the transaction occure by dividing the timedelta by 24 and taking the remainder and setting an offset .
1343	The day 's 0 , 1 , 2 and 6 all have fraud rates higher than the average . If we were to guess we could consider day 6 to be friday , 0 as saturday , 1 as sunday and 2 as monday .
1344	Syllable Analysis
1345	The Flesch Reading Ease formula
1346	Readability Consensus based upon all the above tests
1347	Count Vectorizers for the data
1348	Applying Latent Dirichlet Allocation ( LDA ) models
1349	Visualizing LDA results of sincere questions with pyLDAvis
1350	Visualizing LDA results of insincere questions
1351	Unlike the test data , where we have multiple additional columns available , test column has only the audio clip , hence this is purely a Ornithological Language Processing Task
1352	From Trigger Words Notebook ( Sequence Model -Coursera What really is an audio recording A microphone records little variations in air pressure over time , and it is these little variations in air pressure that your ear also perceives as sound . You can think of an audio recording is a long list of numbers measuring the little air pressure changes detected by the microphone . We will use audio sampled at 44100 Hz ( or 44100 Hertz ) . This means the microphone gives us 44,100 numbers per second . Thus , a 10 second audio clip is represented by 441,000 numbers ( = $ 10 \times 44,100 $ ) . Spectrogram It is quite difficult to figure out from this `` raw '' representation of audio whether the word `` activate '' was said . In order to help your sequence model more easily learn to detect trigger words , we will compute a spectrogram of the audio . The spectrogram tells us how much different frequencies are present in an audio clip at any moment in time . If you 've ever taken an advanced class on signal processing or on Fourier transforms A spectrogram is computed by sliding a window over the raw audio signal , and calculating the most active frequencies in each window using a Fourier transform . If you do n't understand the previous sentence , do n't worry about it . Let 's look at an example .
1353	Spectogram
1354	prediction on test set
1355	Let 's look at first 20 files
1356	What is a benign tumor A benign tumor put simply is one that will not cause any cancerous growth . It will not damage anythin , it 's just a small blot on the landscape of your skin . What is a malignant tumor A malignant tumor is the evil twin of the benign tumor : it causes cancerous growth .
1357	Benign image viewing
1358	Let 's see where the most frequent amounts of cancerous growth occur
1359	Age is an important factor in carciongenous growth , because it helps you to understand who is more vulnerable at an early age and who is more vulnerable at later stages of their life .
1360	So we have a bell ( Gaussian or normal distribution ) of train data . What about test
1361	Now the splitter sort of splits the data into chunks by adding a certain `` feature '' to the data which determines which batch/fold the data should go in . Here we have 3 batches / 3 folds where the data can be separated to .
1362	The basic structure of model
1363	I have been using matrix factorization and did not see a kernel running it , thought adding this would provide some interesting ideas . This kernel joins the data provided to get the order history of different users , creates an order counts matrix from that , and factors it into two lower dimension matrices . The new matrix could be used as additional feature columns and potentially improve LB scores .
1364	Match user_id of the prior part of the same orders table . That will give you the order history by user .
1365	Merging with order history from order_prior table , ran on 16gb memory , but timed out here on 8gb so I am running a subset of the data below .
1366	Create the product list for each order with filtering in pandas . product_list represents reorders while all represents all items from an order .
1367	To create the users x products count table loop through the prodcut ids data to as a sparse matrix ( much more memory efficient ) , column position contains the product ids with position listed in a dict .
1368	The problem with using the sparse matrix is that it is gigantic , there are a lot of users and a lot of products . Non-negative matrix decomposition is implemented in sklearn , use that to decompose the count matrix into two new matrices with considerably reduced dimensions .
1369	What can I do with the results ? For one , I could pick a random user and find users that have similar purchasing behavior .
1370	Here are there order counts
1371	Fast Fourier Transform denoising
1372	The challenge is to pick a threshold correctly in order to keep useful frequencies , but still remove noise .
1373	Import required libraries
1374	Lets have some statastics of data .
1375	In which year most movies were released
1376	Lets create popularity distribution plot .
1377	In which month most movies are released from 1921 to
1378	On which date of month most movies are released
1379	On which day of week most movies are released
1380	Getting Prime Cities
1381	This first function gets the distance of all the cities to the current city . If we decide to penalize , we multiply the distance of non-prime cities by $ 1 .
1382	Vocabulary and Coverage functions
1383	Better , but we lost a bit of information on the other embeddings .
1384	FastText does not understand contractions
1385	In this v2 , I am using GloVe only , for memory usage purposes mostly .
1386	Vocabulary and Coverage functions
1387	If you apply lowerization , you lost a bit of informations on other embeddings
1388	Not a lot of contractions are known . ( FastText knows none
1389	Part 2 : Checking the improvement
1390	Same thing , but with no filters .
1391	Discrete Wavelet Transform ( dwt ) denoising
1392	Which wavelet to use
1393	Target & Experiment
1394	Which seat the pilot is sitting in . left seat right seat This probably has nothing to do with the outcome of the experiment though .
1395	Time of the experiment
1396	point Electrocardiogram signal . The sensor had a resolution/bit of .012215 µV and a range of -100mV to +100mV . The data are provided in microvolts .
1397	A measure of the rise and fall of the chest . The sensor had a resolution/bit of .2384186 µV and a range of -2.0V to +2.0V . The data are provided in microvolts .
1398	Galvanic Skin Response
1399	For convenience , I replaced the 5 original features by a unique identifier named 'actor ' for each quintuple . Here are the first five lines of the training data , in the Shanghai timezone .
1400	So duplicates are half as likely to be positives within duplicates than within the whole set . Moreover they are also half as likely to be positives if they are first than if they are last of the duplicates . That is interesting . Duplicates might come in more than two though and we need to investigate a little further . The table below shows the number of cases by number of duplicates and the mean of their respective attribution rates
1401	Let us now look at the statistics and distribution of duplicates in the test data .
1402	This class has a funtion plot_overlap which takes a sequence of series and returns a long dataframe with the following columns val ' - all values present in the series serie ' - series in which they are present intersection ' - a boolean indicating the presence of the value in the intersection of all series . We use it to see plot the differences betweein the unique values present in the fake test set from the 9th and the real test set . Note that the IP values were already truncated . In the graphs below , the red denotes the unique values that are not shared between the sets , the blue ones are the common ones .
1403	Kaggle allows for the data to be used from their server directly and has most libraries available , it is therefore a great place to work on your project instead of using you own machine . Combined with Jupyter , it becomes an extremely intuitive tool for exploratory work , doing things step-by-step and tuning our approach as we go . Moreover it allows readers to learn , fork the project and get their hands dirty . We are really looking forward to publishing Let 's then start by loading the libraries and methods we need
1404	The chain established itselft in Quito in 1952 ( [ Wikipedia ] ( so let 's pick a shop in Quito as a starting point , as the brand is well established there . Let us pick 47 and plot the corresponding transactions time series . With a well established store , we can predict that the time series will be almost stationary . High seasonality is expected too , as people consume more during celebration periods . Let 's see what we get
1405	Seasonality and Outliers
1406	The ARIMA models have three parameters , corresponging to its AR , I and MA components The number of autoregressive ( AR ) terms ( p ) : AR terms are just lags of the dependent variable . The number of differences ( d ) : These are the number of non-seasonal differences . The number of moving average ( MA ) terms ( q ) : MA terms are lagged forecast errors in the prediction equation . We will set d to zero and not use the Integrated part of the model , effectivefly using an ARMA model . Let 's start with checking for the auto-correlation of our time series , so as to understand it better and gauge what the parameters of our model should be .
1407	We can check on the residuals that the model found , which will for sure contain our outlier of mid-2015 . We can plot the residuals and their distribution . We can see that we are getting a Gaussian , centered at 0 with as tandard deviation of about 380 transactions .
1408	The results seem rather good . Below is a histogram of the difference in predictions to see how good a forecast we made . We see that 2/3 of errors are below 200 transactions , which is about a 5 % error . Not bad for a frist try and a month long window .
1409	Note that the fact that the means are linearly correlated should be obvious the slope should correspond to the ratio of days off / working days . It gives us a very simple clustering of the stores , there is the one in the bottom right and the rest . We expect that lonely store to be store 1 . Let us check the heatmaps now .
1410	Some real patterns appear now , such as the very distinct standard deviation pattern of stores 25 and 26 , as well as the highlighted Saturday shopping frenzy at store 19 and 25 . This is a good starting point to train a ML algorithm for clustering . We will run a hierachical clustering machine based on the AgglomerativeClustering algorithm from the sklearn library . But that means we need to decide how many clusters we want to see . Instead of counting by hand the number if patterns we can see in the 54 stores , we can use the ward clustering and dendogram features of the scipy library to get an idea of how many clusters we may want
1411	Let 's plot the new clusters to see if we can quickly make georgraphical sense of the classification
1412	Setting up dataframes with an additional ` isPrime ` tag
1413	Concorde TSP Solver
1414	Concorde Solver for only Prime Cities
1415	Load some packages and the data . Create a numpy array called X with the training set features and a numpy array called y with the target values . Create a numpy array with the test set features . Let 's log ( base e ) transform the target variable since it is vast in scale .
1416	Let 's forget all about EDA . Other people showed that there are no missing values , everything is numeric , and no wild outliers . Let 's just get rid of any features with no variance . This will include variables with all the same value .
1417	Now let 's get to making a regressor . A good place to start is XGBoost . Let 's just make most of the paramters at their defaults . Maybe use 300 estimators . Instantiate regressor , fit model , bada boom , bada bing .
1418	Boooooooya ! All thats left to do is write the results to a file and submit it . Do n't ya ' just love pandas
1419	Load and Prepare data
1420	Are the classes imbalanced
1421	How many cases are there per image
1422	Where is Pneumonia located
1423	What is the age distribution by gender and target
1424	What are the areas of the bounding boxes by gender
1425	How is the pixel spacing distributed
1426	How are the bounding box areas distributed by the number of boxes
1427	Are there images with mostly black pixels
1428	How are the bounding box aspect ratios distributed
1429	Is there a relationship between the bounding box 's aspect ratio and area
1430	First , lets load some libraries , the data , and do the typical reformatting .
1431	Notice that this is where the data leakage occurs
1432	Lets continue as normal . Create our train/test split and prepare for LightGBM model training .
1433	Lets create a simple LightGBM regressor using basically the default parameters and see what we get .
1434	Let 's setup an accuracy metric ( just for vanity
1435	Ok , even here , let 's apply normalization ..
1436	Pretty impressive , but still not useful without a proper visualization . Let 's see in heatmap form
1437	Import libs and Load data
1438	Scaling is really not needed for XGBoost , but I leave it here in case if you do the optimization using ML approaches that need it .
1439	I am doing a stratified split and using only 25 % of the data . Obviously , this is done to make sure that this notebook can run to completion on Kaggle . In a production version , you should uncomment the first line in the section below , and comment out or delete everything else .
1440	These are the parameters and their ranges that will be used during optimization . They must match the parameters that are passed above to the XGB_CV function . If you commented out any of them above , you should do the same here . Note that these are pretty wide ranges for most parameters .
1441	In my version of sklearn there are many warning thrown out by the GP portion of this code . This is set to prevent them from showing on screen . If you have a special relationship with your computer and want to know everything it is saying back , you 'd probably want to remove the two `` warnings '' lines and slide the XGB_BO line all the way left . I am doing only 2 initial points , which along with 8 exploratory points above makes it 10 `` random '' parameter combinations . I 'd say that 15-20 is usually adequate . For n_iter 25-50 is usually enough . There are several commented out maximize lines that could be worth exploring . The exact combination of parameters determines [ exploitation vs. exploration ] ( It is tough to know which would work better without actually trying , though in my hands exploitation with `` expected improvement '' usually works the best . That 's what the XGB_BO.maximize line below is specifying .
1442	Simply loading the files without any transformation . If you wish to manipulate the data in any way , it should be done here before doing dimensionality reduction in subsequent steps .
1443	Principal Component Analysis ( [ PCA ] ( identifies the combination of components ( directions in the feature space ) that account for the most variance in the data .
1444	It is worth playing with [ RFC parameters ] ( Initially , I had n_estimators=100 and max_depth=10 which was not selecting enough features . Boruta parameters are explained [ here
1445	Here we define Random Forest classifier and RFECV parameters . To test the features properly , it is probably a good idea to change n_estimators to 200 and max_depth=20 ( or remove max_depth ) . It will take longer , on the order of 2 hours , if you choose to do so . Yet another important parameter is step , which specifies how many features are removed at a time . Setting it to 2-5 usually works well , but set it to 1 if you want to be thorough . Note that I am specifying n_jobs=4 because Kaggle provides 4 CPUs per job . You may wish to set that to -1 so that all CPUs on your system are used . Also , the whole countdown will go 5 times because we are doing 5-fold cross-validation .
1446	We estimate the feature importance and time the whole process .
1447	Let 's summarize the output .
1448	Plot number of features vs. CV scores .
1449	Save sorted feature rankings .
1450	Make a prediction . This is only a proof-of-principle as the prediction will likely be poor until more optimal parameters are used above .
1451	Let 's set up a parameter grid that will be explored during the search . Note that you can use fewer parameters and fewer options for each parameter . Same goes for more parameter and more options if you want to be very thorough . Also , you can plug in any other ML method instead of XGBoost and search for its optimal parameters .
1452	A total number of combinations for the set of parameters above is a product of options for each parameter ( 3 x 5 x 3 x 3 x 3 = 405 ) . It also needs to be multiplied by 5 to calculate a total number of data-fitting runs as we will be doing 5-fold cross-validation . That gets to be a large number in a hurry if you are using many parameters and lots of options , which is why brute-force grid search takes a long time . Next we set up our classifier . We use sklearn 's API of XGBoost as that is a requirement for grid search ( another reason why Bayesian optimization may be preferable , as it does not need to be sklearn-wrapped ) . You should consider setting a learning rate to smaller value ( at least 0.01 , if not even lower ) , or make it a hyperparameter for grid searching . I am not using very small value here to save on running time . Even though we have 4 threads available per job on Kaggle , I think it is more efficient to do XGBoost runs on single threads , but instead run 4 parallel jobs in the grid search . It 's up to you whether you want to change this .
1453	Next we set up our stratified folds and grid search parameters . I am using AUC as a scoring function , but you can plug in a custom scoring function here if you wish . Grid search wil spawn 4 jobs running a single thread each . The param_comb parameter declares how many different combinations should be picked randomly out of our total ( 405 , see above ) . I am doing only 5 here , knowing that it will not properly sample the parameter space . Definitely use a bigger number for param_comb . You may want to increase/decrease verbosity depending on your preference . Note that I have set the number of splits/folds to 3 in order to save time . You should probably put 5 there to get a more reliable result .
1454	You can actually follow along as the search goes on . To convert to normalized gini , multiply the obtained AUC values by 2 and subtract 1 . Let 's print the grid-search results and save them in a file .
1455	Not surprisingly , this search does not produce a great score because of 3-fold validation and limited parameter sampling . Lastly , let 's make a prediction based on best parameters found during the search .
1456	You can ignore most of the parameters below other than the top two . Obviously , more folds means longer running time , but I can tell you from experience that 10 folds with Keras will usually do better than 4 . The number of `` runs '' should be in the 3-5 range . At a minimum , I suggest 5 folds and 3 independent runs per fold ( which will eventually get averaged ) . This is because of stochastic nature of neural networks , so one run per fold may or may not produce the best possible result . If you can afford it , 10 folds and 5 runs per fold would be my recommendation . Be warned that it may take a day or two , even if you have a GPU .
1457	Here we average all the predictions and provide the final summary .
1458	Save the file with out-of-fold predictions . For easier book-keeping , file names have the out-of-fold gini score and are are tagged by date and time .
1459	Save the final prediction . This is the one to submit .
1460	We are doing a little trick here . Since it is highly unlikely that 5-6 parameter search runs would be able to identify anything remotely close to optimal parameters , I am giving us a head-start by providing two parameter combinations that are known to give good scores . Note that these are specifically for random projection encoding . If you go with entity embedding , you 'll want to delete this section and uncomment the whole paragraph underneath it .
1461	We are doing only one random guess of parameters , which makes a total of 3 when combined with two exploratory groups above . Afterwards , only 2 optimization runs are done . A total number of random points ( from .explore section + init_points ) should be at least 10-15 . I would consider 20 if you decide to include more than 6 parameters . n_iter should be in 30+ range to do proper parameter optimization .
1462	Here we print the summary and create a CSV file with grid results .
1463	Five new features were added : X29 , X48 , X232 , X236 and X263 . All of them were found by genetic programming , just like the original set of Scirpus ' features . I think this is justified as the scores will be better in the end . __X48 and X236 were subsequently removed .
1464	In Scirpus ' [ __original script__ ] [ 1 ] the whole y-range is used , so the color-coding gets stretched because of the > 250 outlier . Therefore , most of the y-values end up in the bottom half of the color range and the whole plot is just various shades of blue that are difficult to tell apart . In this script I clip y-values so that everything above 130 will be the same shade of BLUE . original script
1465	Create MTCNN and Inception Resnet models
1466	The below weights were selected by following the same process as above for the train sample videos and then using a logistic regression model to fit to the labels . Note that , intuitively , this is not a very good approach as it does nothing to take into account the progression of feature vectors throughout a video , just combines them together using the weights below . This step is provided as a placeholder only ; it should be replaced with a more thoughtful mapping from a sequence of feature vectors to a single prediction .
1467	Full resolution detection
1468	Half resolution detection
1469	The facenet-pytorch package
1470	The facenet-pytorch package ( non-batched
1471	The dlib package
1472	The mtcnn package
1473	Read and pre-process the data
1474	Following is the original BERT-BASE config dim '' : 768 , dim_ff '' : 3072 , n_layers '' : 12 , p_drop_attn '' : 0.1 , n_heads '' : 12 , p_drop_hidden '' : 0.1 , max_len '' : 512 , n_segments '' : 2 , vocab_size However , as you can see in the following code section , I am using a different configuration .
1475	We are training the discriminator ahead of generator here . It can be done together with the generator but I am imitating what Chris did .
1476	Creat , zip and submit the images
1477	look at a single item 's rows to get a feel for the data structure
1478	Get a feel for some time series ' shapes Before we jump into pre-processing , it 's useful to look at a sample of series to see if anything jumps out at us that will motivate how to move forward .
1479	Plotting 5 unique items ( ` id ` 's
1480	We see some clear differences in the series shapes - some have traffic all throughout while others do n't start until later on Plus the scale is different . The first series gets 1-2 sales a day , while the second gets 2-6 ( after a period of relative quiet Action Items To avoid dealing with very noisy series , I 'll bin sales by week , which will remove weekly seasonality and lead the number of sales per time unit to be less volitile .
1481	Example ` items
1482	example departments Finally , plotting 5 randomly selected departments
1483	Some items are sold at a way higher rate than others . When we cluster , we do n't want the result of our clustering to split popular vs unpopular items . Rather , we 're interested in distinguishing the shape between items . Thus , I 'll rescale each item according to it 's global mean .
1484	Plot samples of series from clusters
1485	The series in cluster 1 seem to be similar in that they start with very few sales up until day 300-500 , and then have relatively steady sales past that point . The series in this cluster might be pre-processed in a similar way , by cutting off the first 300 days of data , for example .
1486	The series in cluster two generally show a period of decreasing trend , and then they stabalize around 700-1000 days into the time frame .
1487	I 'll skip the breakdown of clusters 4-6 and jump straight to cluster 7 , as this gets quite repetitive .
1488	Dynamic Time Warping You might notice that according to the clustering method above , two series that might look `` similar '' to the human eye might not be in the same clusters if they 're not aligned . For example , consider the following two series
1489	Dynamic Time Warping distance : example
1490	Clustering using DTW Now I 'll use the same hierarchical clustering method , but instead of using Euclidean distance to compare series , I 'll use normalized DTW distance . The DTW algorithm time complexity is quadratic in the size of each vector . If there are $ m $ items in the dataset , each of them with $ n $ elements , then the time to compute a full distance matrix between the series in this dataset is $ O ( m^2n^2 ) $ , which would take a long time for a dataset of this size . To speed up this demonstration , I 'll only use a sample of the items in this dataset .
1491	Showing cluster
1492	We can see that what the series in this cluster have in common is that they have intermittent of no sales , followed by periods of stable sales . Showing the series pair-wise makes this clearer
1493	We can repeat this for another cluster
1494	These series are similar in that they all start with few sales , and then experience a period of increasing sales . Seeing the DTW alignment
1495	Load libraries
1496	Set your file path
1497	Kaggle Version : Identify all the subdirectories in the training directory
1498	Pull an audio sample from each word
1499	Preview of Spectograms across different words
1500	Waveforms across different Words
1501	Waveforms within the Same Word
1502	Save Figures as images
1503	Function : convert audio to waveform images
1504	ZIP the Image Files
1505	Note signals 0 , 1 , and 2 are not faulty and signals 3 , 4 , and 5 are faulty . They 're messy , noisy , and not obviously periodic , oh boy . However , there are quite a few signal processing techniques that can be used anyways . Speaking of which , it 's time for some feature engineering . Starting with some basic aggregations .
1506	Here I will work on some techniques to handle highly unbalanced datasets , with a focus on resampling . The Home Credit Risk Prediction competition , is a classic problem of unbalanced classes , since Credit Loan in risk can be considered unusual cases when considering all clients . Other classic examples of unbalanced classes are the detection of financial fraud and attacks on computer networks . Let 's see how unbalanced the dataset is
1507	A widely adopted technique for dealing with highly unbalanced datasets is called resampling . It consists of removing samples from the majority class ( under-sampling ) and / or adding more examples from the minority class ( over-sampling ) . Despite the advantage of balancing classes , these techniques also have their weaknesses ( there is no free lunch ) . The simplest implementation of over-sampling is to duplicate random records from the minority class , which can cause overfitting . In under-sampling , the simplest technique involves removing random records from the majority class , which can cause loss of information . Let 's implement a basic example , which uses the DataFrame.sample method to get random samples each class
1508	Random under-sampling
1509	Random over-sampling
1510	For ease of visualization , let 's create a small unbalanced sample dataset using the make_classification method
1511	We will also create a 2-dimensional plot function , plot_2d_space , to see the data distribution
1512	Because the dataset has many dimensions ( features ) and our graphs will be 2D , we will reduce the size of the dataset using Principal Component Analysis ( PCA
1513	Random under-sampling and over-sampling with imbalanced-learn
1514	Tomek links are pairs of very close instances , but of opposite classes . Removing the instances of the majority class of each pair increases the space between the two classes , facilitating the classification process . In the code below , we 'll use ratio='majority ' to resample the majority class .
1515	This technique performs under-sampling by generating centroids based on clustering methods . The data will be previously grouped by similarity , in order to preserve information . In this example we will pass the { 0 : 10 } dict for the parameter ratio , to preserve 10 elements from the majority class ( 0 ) , and all minority class ( 1 ) .
1516	SMOTE ( Synthetic Minority Oversampling TEchnique ) consists of synthesizing elements for the minority class , based on those that already exist . It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point . The synthetic points are added between the chosen point and its neighbors . We 'll use ratio='minority ' to resample the minority class .
1517	Now , we will do a combination of over-sampling and under-sampling , using the SMOTE and Tomek links techniques
1518	Deploying Machine Learning Model over Resampled Dataset
1519	Blending is the best way to explore diversity from models . Taking that into account , why not to blend a LightGBM and Keras based models I grab output submissions files from that kernels
1520	Making my datamart ( joinining all data into one table
1521	FVC and Percent Trend For All Patients
1522	FVC And Percent Tread Of All Patients Smoker Vs Non-Smoker
1523	Visualising Dicom Files
1524	Using tensorflow_io to decode Dicom files
1525	It only updated gain and cover values . The leaf values remain fixed .
1526	Load the packages
1527	to get the edge : Well I 'm not sure if I get the concept right but here I will get all the pairs of neighborhood which have very high interaction level , which means , the numbers of trips between those neighborhood should surpass a certain level ( which is threshold in the function
1528	number of adjacencies : Which neighborhoods have a lot of interaction with neighborhood A
1529	Set the threshold as
1530	change the threshold to
1531	You can pass the and process images directly with help of ImageDataGenerator of tensorflow utilizing Argument preprocessing_function tf.keras.preprocessing.image.ImageDataGenerator ( preprocessing_function=crop_and_zoom
1532	You can run the follwing function if you want to generate new images with the given croping and zoom
1533	First grab the data .
1534	I rely a lot on loss plots to detect when learning has stopped as well as when overfitting begins .
1535	Prepare results for Submission
1536	II . Gather the data
1537	V. Model data
1538	VI . New learner with increased size of images
1539	forked from ref .
1540	Library imports and settings
1541	We start by importing the data as [ pd.DataFrame ] ( objects .
1542	We want to apply a set of identical functions to both . The most efficient way is to make a new column in each data set indicating the source and concatenating the DataFrames with [ pd.concat
1543	Algorithms are n't very useful without good data . Going from a raw data set to a set of usable features for an algorithm is is process of [ feature selection ] ( There are three types of available features in this competition Numerical : E.g . the total price of the request Categorical : E.g . the gender of the requester Text : E.g . the written text of the second essay We will build a data processing pipeline which heeds these types of variables .
1544	Since essays 3 and 4 were dropped , we will apply the following mapping to the essays text { Essay } _1 : = \text { Essay } _1 \oplus \text { Essay text { Essay } _2 : = \text { Essay } _3 \oplus \text { Essay Where $ \oplus $ denotes concatenation . This is easily accomplished using a mask in pandas , along with the clever [ DataFrame.assign ] ( method for assigning new columns as part of a [ pandas method chain ] ( For more resources related to pandas , see [ awesome-pandas ] ( Also note the use of ` lambda ` keyword . ` lambda ` is used to create [ anonymous functions ] ( and is very useful .
1545	Again , we check for missing values
1546	At this point we have some numerical features . Now it 's time to look at the categorical ones . We 'll try adding the following The month of the request , added using the [ pandas.dt accessor ] ( Pandas has some pretty powerful datetime ( ` dt ` ) functionality . Check out the [ documentation ] ( for details . The time of day . Using [ np.where ] ( for a blazingly fast , vectorized ` IF-THEN-ELSE ` condition , we will find out if the application was sent during the morning hours or not . The gender of the applicant and the school state , using [ pandas.get_dummies ] ( which converts a categorical variable to a dummy variable representation .
1547	The following columns are special project_subject_categories project_subject_subcategories Each entry may contain several categories - clearly a violation of the priniples of [ tidy data ] ( if you consider a single category as a variable . There are $ 51 $ different combinations of categories , and $ 416 $ different combinations of subcategories . In reality , there are way less categories to choose from : $ 9 $ categories and $ 30 $ subcategories in total . In the following we will define a function that creates a list of unique categories and sub-categories . Then we will create dummy variables from the resulting variables
1548	Let 's create the dummy encoding
1549	Again we 'll use [ pandas.concat ] ( to concatenate the results back into the DataFrame .
1550	Save and load the data
1551	We 'll import most of the estimators RobustScaler ] ( ( computes robust statistics by scaling the data according to some given quantile range ) , TfidfVectorizer ] ( ( a technique to re-weight words so that more meaningful words are given more weight ) , and LogisticRegression ] ( ( used when we want to model a the probability of a binary outcome ) . We 'll create some custom transformers ColSplitter to split the columns of the dataset to send data to parallel paths in the main pipeline , LogTransform which computes $ f ( x ; \alpha ) = \alpha \log ( 1 + x ) + ( 1 - \alpha ) x $ on the numerical data , where we can search for optimal $ \alpha $ using hyperparameter search , and ParallelPipe , which will let us apply Tfidf to each text feature individually . We 'll make liberal use of [ Pipeline ] ( and [ FeatureUnion ] ( objects to control data flow .
1552	The [ ROC ] ( ( Receiver operating characteristic ) curve is a useful model validation tool when dealing with binary classifiers . We will use the [ area under the curve ] ( ( AUC ) of the ROC curve to determine the best hyperparameters
1553	Run grid search
1554	The secret sauce This is where the magic happens - the soft_AUC function . This Takes the predictions Splits them into groups according to whether the true values are one/zero Takes each pair of predictions from the one/zero groups , and subtracts the zeroes from the ones . Takes the mean of the sigmoid of the result If AUC is perfect , an ( actual ) one in the CV data will always have a higher pred than a zero in the CV data . Each time the prediction is wrong , and a one has a lower pred than a zero , the output loss is increased . Hence this is an suitable loss function to substitute for genuine AUC in that it decreases as AUC decreases and vice versa . Like AUC , we only care about relative ordering of predictions between the classes . We do n't care about the absolute values of the predictions . This means that your final output values from your NN will also only care about ordering , and hence you use them to blend you will need to use ranking or similar to blend . It 's important to note that you need a large enough batch size , as if you have no examples of one class you 'll get no data , and ideally you want several positive cases in each batch . I used a batch size of 4096 , which with a 3 % approx positive class rate , gives around 100 positives per batch - enough to give a useful result but not so many as to make calculations take forever . I did try calculating on the whole training batch , but convergence was not as good . Some useful references my code based heavily on this code .
1555	Importing Libraries and Reading the Dataset
1556	LightGBM
1557	Reading geometry files
1558	When the reduced coordinate is donated as $ \textbf { r } = ( x , y , z ) ^t $ , the position vector $ \textbf { R } $ is expressed using the lattice vectors $ \textbf { a } _i $ as following textbf { R } = ( X , Y , Z ) ^t = x \textbf { a } _1 + y \textbf { a } _2 + z \textbf { a } _3 = A \textbf { r where $ A = ( \textbf { a } _1 , \textbf { a } _2 , \textbf { a } _3 ) $ . The reduced coordinate is obtained from textbf { r } = A^ { -1 } \textbf { R } $ . B = A^ { -1 } = ( \textbf { b } _1 , \textbf { b } _2 , \textbf { b } _3 ) ^t $ where $ \textbf { b } _i $ is the $ i $ -th reciprocal lattice vector .
1559	Note that optimal values of ` l_max ` , ` m_max ` , and ` n_max ` depend on both the maximum radius ` R_max ` and the lattice matrix ` amat ` . A larger ` R_max ` or a smaller cell requires larger ` l_max ` , ` m_max ` , and ` n_max ` . You can simply estimate them from $ l_ { \rm max } > R_ { \rm max } / a $ where $ a $ is the lattice constant . In general , $ l_ { \rm max } > R_ { \rm max } \cdot \mid { \bf b } _1\mid $ , $ m_ { \rm max } > R_ { \rm max } \cdot \mid { \bf b } _2\mid $ , and $ n_ { \rm max } > R_ { \rm max } \cdot \mid { \bf b } _3\mid $ where $ { \bf b } _i $ is the $ i $ -th reciprocal lattice vector .
1560	If you find lack or excess of connections , you shoud adjust ` factor ` . Maybe , close-packed lattices require a lower value than the defalut value . For a hcp-like lattice , I used ` factor=1.2 ` . I recommend the factor that depends on the spacegroup and the gamma
1561	Please see the SchNetPack API document
1562	Importing Libraries and Reading the Dataset
1563	Binary Features ( Mapping
1564	Low and Medium Cardinality Features ( Dummy Encoding
1565	We are converting categorical variables into dummy variables . Recap these nominal features nom_0 : 3 uniques nom_1 : 6 uniques nom_2 : 6 uniques nom_3 : 6 uniques nom_4 : 4 uniques
1566	LightGBM
1567	Feature Importance
1568	From the output , we see that there is no correlation between age and fvc .
1569	From the output results , we can see that we are overwhelmingly male .
1570	Let 's take a look at the images of patients in the bottom 10 % of FVC .
1571	We need ` ASE 3.17 ` for ` SchNetPack 0.2.1 ` .
1572	The magnetic shielding tensor Magnetic shielding tensor The symmetric portion of the shielding tensor can be diagonalized into its own principal axis system ( PAS PAS
1573	Shielding Parameter Prediction
1574	Make function to get image shapes
1575	Get image shape for each train image
1576	Group by shape and summerize
1577	gamma correction using gamma = 0 .
1578	gamma correction using gamma = 1 .
1579	JPEG compression with quality factor
1580	JPEG compression with quality factor
1581	FYI - JPEG compression with quality factor
1582	Next , I import main packages . Other sub-modules are imported later .
1583	Convert into chainer 's dataset
1584	Make iterators , oprimizer
1585	Adam is used as an optimizer .
1586	Let 's also explore the possible correlations between features and simple high-level properties of images without going into NN . Let 's look at the number of images , image size , brightness , hue and saturation .
1587	Numbers-wise trees are leading by far , being the only major object type on some of the images . They are followed by buildings and other man structures , which are actually not present on every image , as well as vehicles . Waterways are present on only one image , which actually was added in v2 of WKT file , added after the first version of this kernel was published . Visually there also appears to be a correlation between the number of trees and buildings/structures .
1588	Reading tif Files
1589	Let 's see how it works on train data .
1590	Predict for test data and submit
1591	Read in the data for analysis
1592	Price should be a good proxy for item_id and gives it a numerical value instead of a categorical value Once price is merged , we can drop item_id . We can also categorically encode store ID and cat_id .
1593	Do stores sell different kinds of items ? Nope - All stores have the same kind of items
1594	Do total sales correlate with the number of items in a department
1595	Do Sales Differ by Category
1596	How do stores differ by State
1597	How do sales differ by store
1598	Is there seasonality to the sales
1599	Yearly Cycle Decompose of CA_1 store
1600	Weekly cycle decompose of CA_1 store
1601	Visualize sample rows of the submission predictions
1602	Handling missing values
1603	Label encoding
1604	One Hot Encoding
1605	This kernel demonstrates the ability to achieve a high score ( private leaderboard : 2.33037 ; 9th place ) in the LANL Earthquake Prediction competition with only _one_ well-crafted feature and information from the test set data leak . This kernel was inspired by my [ Three Keys to This Competition ] ( discussion topic .
1606	numba ` tized version of information entropy . This is faster than the version in ` scipy ` .
1607	Split the segment into _n_ parts . Calculate the entropy on each part and take the mean to reduce noise .
1608	Plot the entropy .
1609	Do some clipping and linearly transform the feature to best match the TTF . The optimal values for clipping were determined through some iterative hand tuning ( not shown here ) to arrive at an MAE of 2.112 . Some people may say that linear regression is machine learning . However , I would claim that with one variable , it 's a simple univariate linear transformation .
1610	View the feature and TTF together .
1611	The peaks of the TTF in the test set were determined from the figures in [ this discussion post
1612	Import the necessary Python libraries
1613	Identify which MATLAB data file you want to analyze
1614	Load the MATLAB data file
1615	The following code calculates a [ Fast Fourier Transform ] ( of the signal . It also removes the DC component and then divides the Fourier amplitudes by the sum of the amplitudes . I think the DC removal and dividing by the sum are just data normalization ( remove mean and make values go from -1 to +1 ) . This prevents features from being considered more important in the analysis solely because they are at a larger scale . Fourier Transforms are widely used in signals analysis . Usually , signals in the real world have complex shapes and are , therefore , difficult to describe mathematically . If it ca n't be described with an equation , then it is hard to analyze on a computer . Think about clouds in the sky . They are -- by definition -- nebulous . In order to point them out to another person , we typically find shapes and patterns within a completely random collection of water molecules . That one looks like a duck ; this one looks like a rabbit ; that one is a sailboat . The clouds of course are not these objects , but by approximating them with a well-known object we have any easier time with the analysis . We can say , `` Is n't the rabbit 's left ear a little mishapen ? '' And , everyone instantly knows where in the cloud to look . alt-text ] ( `` Clouds : Making shape out of form Similarly , any signal -- no matter how complex -- can be thought of as the addition a collection of simpler ( i.e . easier to describe mathematically ) signals . You can pick any ensemble of signals ( sine waves , traingular waves , polynomials , wavelets ) to approximate a complex signal . Fourier Transforms are the most commonly used because the mathematics of sine waves has been well studied and has some nice convenient properties that help process them more easily on a computer . Sine waves also give an idea of how quickly things are changing over time within a signal -- a concept gives us an intuitive insight into what information or processing is `` going on '' in the system being studied . We may even use a Fourier Transform to hypothesize there are separate components within the system . Filtering ( i.e . separating ) these components in the Fourier domain is easy and can even be done in realtime with hardware . FFTs are important in EEG pre-processing because it is believed that different frequency bands are correlated with different observable behaviors . For example , some frequency bands seem to distinguish awake versus sleeping . Others seem to be correlated with concentration and attention . Brain disorders , such as schizophrenia , are correlated with disruptions in the FFT .
1616	Important EEG frequency bands
1617	Entropy is a non-linear measure quantifying the degree of complexity in a time series . It measures how well you can predict one epoch of the time series from other epochs in the series . The formula for Shannon Entropy is large H ( X ) =\sum_ { i=1 } ^nP ( x_ { i } ) I ( x_ { i } ) = -\sum_ { i=1 } ^nP ( x_ { i } ) log_ { b } P ( x_ { i References Paper for Shannon Entropy in EEG classification Another paper on Shannon Entropy in Biomedical Signals
1618	Cross-correlation ] ( is the measure of similarity between two signals . It is a number between +1 and -1 . A +1 means that when signal A increases , then signal B increases and when signal A decreases , signal B decreases . A -1 means that when signal A increases , signal B decreases and vice versa . A 0 means that when signal A increases , you ca n't say anything about what signal B will do ( i.e . no correlation ) .
1619	Hjorth Activity is the variance in the amplitude of the signal . In the frequency domain , this is the envelope of the power spectral density ( ~ mean power ) . Activity = \large var ( y ( t
1620	Hjorth Mobility is the mean frequency or the proportion of standard deviation of the power spectrum . ( RMS frequency Mobility = \huge\sqrt { \frac { var ( \frac { \text { d } y ( t ) } { \text { dt } } ) } { var ( y ( t
1621	Hjorth Complexity is the ratio of the mobility of the change in signal amplitude to the mobility of the signal itself ( RMS frequency spread ) . In the frequency domain it represents the change in frequency ( or the bandwidth ) . A value close to one indicates that the signal is a pure sinusoid . Complexity = \huge { \frac { Mobility ( \frac { \text { d } y } { \text { d } t } y ( t ) ) } { Mobility ( y ( t
1622	Petrosian fractal dimension
1623	Katz fractal dimension
1624	Taken from Markus ' [ StackOverflow post ] ( There is also a Python library called [ nolds ] ( which has several of these more advanced descriptors coded . Hurst ] ( is a measure of long-term memory in a time series that is not due to periodicity . It 's often used as a measure how how stationary a time series is . Financial analysts use it to determine if there are truly long-term trends in data . ( Is the stock decreasing in value or is this the normal , random fluctuations in stock price The exponent is the rate at which the autocorrelation in a time series decreases as the lags increase . If the series $ x ( t ) $ is a self-similar fractal , then $ x ( bt ) $ is statistically equivalent to $ b^Hx ( t ) $ , where $ H $ is the Hurst exponent . So if we were to take every other datapoint ( $ b=2 $ ) then the mean of that would be the mean of the original signal multiplied by $ 2^H $ . ( Seems analogous to an Eigen Hurst first used his exponent to describe how the Nile river 's size fluctuated over long periods of time . ( He was a hydrologist consulting on the building of a dam . ) The width of the Nile changes with time , but Hurst tried to answer : `` Are these changes ( a ) bouncing around a mean size or ( b ) steadily increasing ( or decreasing ) ? '' That 's a critical point if you want the dam to be useful over decades ( and probably centuries ) of change . The exponent is directly related to the Fractal Dimension of the time series and is basically an objective measure of whether the randomness in a time series is `` mild '' or `` wild '' . In other words , how random is the randomness ? It is usually one of 3 cases : mean reverting , [ random walking ] ( or trending . A random walk is often compared with the walk of a drunk person . With each step there is a 50-50 chance of the drunk moving foward/left or forward/right . In the short term , the walk looks `` random '' , but over the long term the drunk stays along the mean path because of the 50-50 process . H is a number between 0 and 1 . Meanings for H exponents H = 0.5 : $ Geometric random walk ( Brownian motion ) , no correlation H < 0.5 : $ Mean-reverting series or antipersistent ( it 's a random walk with smaller than normal steps . Therefore , an increase in step size is likely followed by a decrease in step size ( or vice-versa H > 0.5 : $ Trending Series - The random walk seems to be persistently going in a direction . So a large step size is likely followed by a large step size .
1625	Detrended Fluctuation Analysis ( DFA
1626	Skewness ] ( measures how asymmetric a probability density function is around its mean value . skewness ] ( `` Skewness : Left or right shift around mean
1627	Kurtosis ] ( is how long the tail is of a probability density function . So longer tails mean more kurtosis . Short , `` tight '' distrubtions centered around the mean have little kurtosis . kurtosis ] ( `` Kurtosis : Tails get longer
1628	Thanks to [ Jason McNeill ] ( for helping with this section We 're also calculating the FFT using dyadic frequency bands . I think this is similar to a [ Wavelet ] ( approach whereby you are looking for frequency bands of successive scales ( in reality they are `` scales '' of the mother wavelet ) . So Fourier is approximating the signal using a combination of sine waves whereas Wavelet is approximating the signal using a combination of non-stationary basis functions . With FT you get a 2D plot ( frequency versus amplitude ) . With Wavelets you are going for a 3D plot ( time versus scale versus amplitude ) . sine verus wavelet ] ( `` Sine wave versus Wavelet Remember , the problem with the Fourier Transform is that we lose temporal information when we go to the frequency domain . So it only works well if the signal is stationary . Wavelets ( or more generally time-frequency domain methods ) can localize to both frequency and time . They give a more robust snapshot of how the signal component frequencies evolve over time . For example , in the figures below , the x axis is time , the y axis is scale ( of the wavelet ) , and the greyscale value is the amplitude . wavelet versus dyad ] ( `` Discrete versus Continuous Wavelet Transform of Chirp Essentially , dyads are a `` poor man 's wavelet analysis '' ( compare how coarse the DWT is compared to the CWT in the figure above . ) I think typically they are defined as scales of 2 ( hence dyad ) . So the second scale is twice the time length of the first and the third is quadruple the first . This lends nicely to computer analysis ( which is base-2 math ) and even works well with acoustical analysis ( which uses octaves for musical notes -- i.e . base-8 ) . Since the power of 2 scale is the smallest regular scale possible , it should be our best hope at tracking changes over time .
1629	Preprocessing the features
1630	Normalize the features
1631	Loading the datafiles
1632	loading data sets
1633	feature engineering
1634	MAE and MSE
1635	distribution of age
1636	here we can see that no of ex smoker is way higher
1637	as we can clearly see that male is dominating in case of ex smoker
1638	as we can see that the percent and FVC are having a good relationship
1639	osic laplace function
1640	data wrangling and processing for tabular data
1641	The first big challenge is data wrangling : We could see that some patients take FVE measurements only after their baseline CT-Images , and some took measurements before that . So let 's first find out what the actual baseline-week and baseline-FVC for each Patient is . We start with the baseline week
1642	The second apporach is using transform , which is not as known as apply , but faster for basic-operations not involving multiple columns of a dataframe . Here is an interesting post about it for those , who want to learn more : Apply vs transform . I wanted to know how much this speeds up the processing , you can find the results in the following
1643	Preparing the data for the Neural Network
1644	In the next section we are going to use the train_preds to calculate the optimized sigma , which is a measure for certainty or rather uncertainty . We can do that , as we have both : the model 's estimate and the real data . We subtract the lower quartile from the upper quartile ( defined in the loss function ) and average it .
1645	Data from with max score : 0 . I take 11 top submission files for this notebook and apply median : 0. mean : 0. minmax_mean : 0. pushout_median : 0 . Data from with max score : 0 . I take 10 top submission files for this notebook and apply median : 0. mean : 0. minmax_mean : 0 . Data from with max score : 0 . I take 5 submission files for this notebook and apply median : 0 .
1646	libararies required for qunatile regression
1647	We can see something interesting here - our features have very small numbers in first , second and third quartiles , but maximum value on contrary is very high . Also it seems like some features are binary ( V1 for example ) , some looks like ordinal ( V2 ) and some looks like numeric ( V126 ) , let 's try to divide features by groups .
1648	We can see that values for our binary features have very similar fraud rates . Let 's return to plotting
1649	Next step - Ordinal features . We have 257 ordinal features in dataset , i 'll plot them by small groups and make some preparations for aesthetic purposes . First , I want to divide them by number of values , if feature have more than 20 unique values - it goes to long_ordinal list , else - to short_ordinal list .
1650	fork . [ covid ] AE pretrain + GNN + Attn + CNN @ mrkmakr
1651	In this notebook I seek to apply a simplified EDA on the available training data . Going further , and since earthquake data are strongly related to cyclic events , I intend to explore this data on the Fourier domain .
1652	Analyzing the failure regions
1653	A . Initial statements
1654	Regarding the training data , for each subsurface image there is its corresponding mask , listed below
1655	C. Exploratory Data Analysis C.1 . Creating a feature dataset Putting together all the training information we have .
1656	TPUs read data directly from Google Cloud Storage ( GCS ) , so we actually need to copy our dataset to a GCS 'bucket ' that is near or 'co-located ' with the TPU . The below chunk of code accomplishes this using the handy kaggle_datasets . We will be using [ Chris Deotte ] ( TFRecords for both this year 's images and also for upsampling images
1657	Now we can explore GridMask , which is essentially a less randomized Course Dropout The below code is taken from [ this notebook
1658	If you decide to use an EfficientNet model for your final model , you need to install something as it is not yet supported by ` keras.applications ` . There is another weight option for EffNets to consider that outperforms Imagenet weights called 'Noisy Student ' that you can read about [ here ] ( For more on EffNets in general , read [ this
1659	DICOM stands for Digital Imaging and COmmunications in Medicine and it is the standard format that allows information to be shared between different medical imaging equipment like X-Ray , MRI , and CT. DICOM files have a ` .dcm ` extension that allows one to store meta-data about the image inside the file , like patient ID or patient gender In general , CT images use 12-bit DICOM files with pixel values ranging from -1024 to 3071 , whereas a normal greyscale image has pixels values between 0 and 225 . The pixel ranges in DICOM files are such that they align with the ' [ Hounsfield Scale ] ( which arbitrarily defined such that the radiodensity of water at STP = 0 and the radiodensity of air at STP = -1000 on the HU scale References : most of this notebook is inspired by [ Jeremy Howard ] ( and his below notebooks on ` fastai2 ` and its implementation for DICOM images . He is a huge inspiration of mine and I recommend you study not only the below notebooks , but all of his notebooks I also did not see this notebook until now , but [ this notebook ] ( by [ Trigram ] ( covers the same contents of this kernel and much more . It is excellent work and I highly suggest you read it
1660	FastAI Medical Imaging
1661	Now there is a follow up question : humans might need windowing to view DICOM images , but do computers need windowing for training ? No , because a neural network accepts data in the form of floating points , which use 32 bits . If we decide to use floats , we ca n't use PIL or save them as JPEGs , but we can use ` fastai.vision We ca n't just forget scaling altogether , however . It is still good practice to scale any type of input when feeding it to a machine learning model , and this is still true for images . Ideally , we want something Gaussian-ish to describe our pixel value distribution . Let 's see if this is the case
1662	Let 's put this in a function and check the other files in our ID directory
1663	We wo n't be able to get a perfectly uniform distribution here simple because some values occur many times and some rarely occur at all . To create a function that connects these bins with a line , we can do this
1664	Yep , those are the same ! Now we can also use the ` show ` method in combination with the scale parameter to compare the raw , windowed , and scaled images
1665	Even though the above sequence of steps was not too involved , we can make our lives even easier by using ` mask_from_blur ` combined with ` mask2bbox ` like so The below code is taken from [ Jeremy Howard ] ( notebook [ here
1666	Get DICOM Metadata
1667	For Next Time
1668	In this [ new competition ] ( we are helping to fight against the worldwide pandemic COVID-19 . mRNA vaccines are the fastest vaccine candidates to treat COVID-19 but they currently facing several limitations . In particular , it is a challenge to design stable messenger RNA molecules . Typical vaccines are packaged in syringes and shipped under refrigeration around the world , but that is not possible for mRNA vaccines ( currently ) . Researches have noticed that RNA molecules tend to spontaneously degrade , which is highly problematic because a single cut can render mRNA vaccines useless . Not much is known about which part of the backbone of a particular RNA is most susceptible to being damaged . Without this knowledge , the current mRNA vaccines are shopped under intense refrigeration and are unlikely to reach enough humans unless they can be stabilized . This is our task as Kagglers : we must create a model to predict the most likely degradation rates at each base of an RNA molecule . We are given a subset of an Eterna dataset comprised of over 3000 RNA molecules and their degradation rates at each position . Our models are then tested on the new generation of RNA sequences that were just created by Eterna players for COVID-19 mRNA vaccines Before we get started , please check out [ Xhlulu ] ( notebook [ here ] ( as this one is based on it : I just added comments , made minor code changes , an LSTM , and fold training
1669	It seems we also have a ` signal_to_noise ` and a ` SN_filter ` column . These columns control the 'quality ' of samples , and as such are important training hyperparameters . We will explore them shortly
1670	Now we explore ` signal_to_noise ` and ` SN_filter ` distributions . As per the data tab of this competition the samples in ` test.json ` have been filtered in the following way Minimum value across all 5 conditions must be greater than -0.5 . Mean signal/noise across all 5 conditions must be greater than 1.0 . [ Signal/noise is defined as mean ( measurement value over 68 nts ) /mean ( statistical error in measurement value over 68 nts To help ensure sequence diversity , the resulting sequences were clustered into clusters with less than 50 % sequence similarity , and the 629 test set sequences were chosen from clusters with 3 or fewer members . That is , any sequence in the test set should be sequence similar to at most 2 other sequences .
1671	So with AUC of about 0.74 , there seems to be a farily significant difference in distribution of features that we have used between the train and test sets . So let 's take a look at what features are most responsible for this difference
1672	AUC of 0.56 reveals some dicrepancy between the train and test sets . Let 's take a look at the feature importances
1673	Whoa , that 's a pretty significant AUC ! 0.999996 adverserial AUC is the biggest one I 've ever come across . I first thought I might be making a mistake , but re-run this script several times , and do n't seem to find any bugs in it . But I am open to criticims/suggestions . Let 's look now at the top 20 `` adversarial '' features .
1674	Well , folks , does n't get any better than the AUC of 1.0 ! Let 's see which ones are the most responsibel columns .
1675	OK , AUC of 0.999261 is not too shabby eaither . Let 's see what 's going on here .
1676	For raw image data we were able to get an AUC of 0.65 , while here we get 0.70 . Seems that the image size and image metafeatrue data has more discrepancy between the train and test sets than the raw rescaled images . Let 's look at the top features and theri relative importances .
1677	So there is a little bit of an `` improvement '' , but overall it is still a fairly significant distinction .
1678	The AUC of 0.63 is well beyond `` randon '' , and could be pretty significant in terms of distinguishing between train and test sets . It 's certainly well byond almost perfect 0.5 AUC of the fist Categorical Encoding competition . Let 's take a look at what features are the most responsible for the discepancy .
