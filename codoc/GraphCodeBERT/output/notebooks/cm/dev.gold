1309	Load pre-trained model
228	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
51	Wow , not very diverse at all ! Most of the values are heavily concentrated around 0 . Maybe if we used the log plot things would be better .
1518	Multi-Dimensional Reduction and Visualisation with t-SNE
563	For the same window we superimpose the masks above the image .
501	If look at the above given plot is clear that all AVG featuers are high correleted values by seeing this plot we can easy find out the coorelated features
457	Intersection ID
285	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
1508	Select some features ( threshold is not optimized
209	Linear Regression
1385	Census_InternalPrimaryDisplayResolutionVertical - Retrieves the number of pixels in the vertical direction of the internal display
1516	As a result , we might probably delete some columns without decreasing ROC as they are collinear ( for instance , public , `` =1 electricity from CNFL , ICE , ESPH/JASEC '' and coopele , =1 electricity from cooperative , correlation between these two is - .
1116	Leak Data loading and concat
178	Perhaps the simplest approach for this problem is to assume that there are two classes in the image : objects of interest and the background . Under this assumption , we would expect the data to fall into a bimodal distribution of intensities . If we found the best separation value , we could `` mask '' out the background data , then simply count the objects we 're left with . The `` dumbest '' way we could find the threshold value would be to use a simple descriptive statistic , such as the mean or median . But there are other methods : the `` Otsu '' method is useful because it models the image as a bimodal distribution and finds the optimal separation value .
1209	card_id : Card identifier month_lag : month lag to reference date purchase_date : Purchase date authorized_flag : Y ' if approved , 'N ' if denied category_3 : anonymized category installments : number of installments of purchase category_1 : anonymized category merchant_category_id : Merchant category identifier ( anonymized subsector_id : Merchant category group identifier ( anonymized merchant_id : Merchant identifier ( anonymized purchase_amount : Normalized purchase amount city_id : City identifier ( anonymized state_id : State identifier ( anonymized category_2 : anonymized category
864	A [ feature primitive ] ( is an operation applied to a table or a set of tables to create a feature . These represent simple calculations , many of which we already use in manual feature engineering , that can be stacked on top of each other to create complex features . Feature primitives fall into two categories Aggregation__ : function that groups together child datapoints for each parent and then calculates a statistic such as mean , min , max , or standard deviation . An example is calculating the maximum previous loan amount for each client . An aggregation works across multiple tables using relationships between tables . Transformation__ : an operation applied to one or more columns in a single table . An example would be taking the absolute value of a column , or finding the difference between two columns in one table . A list of the available features primitives in featuretools can be viewed below .
65	Index the data by id and timestamp . Look at the data types and some basic info about the different columns .
61	Let 's take the example of D1 ( but this approach works with other D features ) . In the following cell , I plot the D1 feature by product . If we take look at the orange dots ( is Fraud=1 ) , we can clearly see that some are forming lines .
191	Can we get some informations out of the item description
447	Correlation between meter_reading And Numeric Variable
476	MERGE , MISSING VALUE , FILL NA
1034	Inference on Test Set
1232	Level 2 LightGBM
54	OK , that 's much more interesting . Let 's do the same thing with the test data .
1149	So how to extract a date Well , first , get ride of the decimal values . Then transform to a datetime object supposing that it is an ordinal datetime . Try different offsets until you get a meaningful date range . That 's it . Let 's see this in action .
407	Again let 's compare briefly results of transformations on the first image
1466	Import libraries and data , reduce memory usage
1330	Check null data
1436	Most of the features above tend to have different distribution , and are good candidates for experimenting , though 'uq_device_per_ip ' does n't look as separated to me , so I would put it lower on testing priority . Now let 's say you wanted to use minutes as a predictor based on given dataset
1580	I formulate this task as an extractive question answering problem , such as SQuAD . Given a question and context , the model is trained to find the answer spans in the context . Therefore , I use sentiment as question , text as context , selected_text as answer . Question : sentiment Context : text Answer : selected_text
859	Distribution of Search Values
451	Dew Temperature
919	Split into training and validation groups
1206	These have their kitchen area larger than the total area of the house
569	We define the default preprocessing for resnet architectures and create train and validation generators ( ` keras.utils.Sequence
13	We will embed words from these tweets into a word-vector space using one of the previously trained word embeddings . Here we use a 300-dimensional vector space that comes curtesy of FastText . Unfortunately , this embedding is not available for the Quora compatition , but as we are using this kernel just for the educational purposes , that will be fine . We will also limit the length of text to 220 words . This is an overkill for questions , but for general purpose it is rather small text length . The original was aimed at much longer text sizes , and this was a reasonable length for those purposes . The best embedding that we used in Toxic limited length to 900 words .
1554	Reading in the data , as usual .
326	Define X and y
1429	Looks no problem ! Let 's predict all provinces of United States which confirm cases greater than
865	DFS with Default Primitives
696	The ` Id ` and ` idhogar ` object types make sense because these are identifying variables . However , the other columns seem to be a mix of strings and numbers which we 'll need to address before doing any machine learning . According to the documentation for these columns dependency ` : Dependency rate , calculated = ( number of members of the household younger than 19 or older than 64 ) / ( number of member of household between 19 and edjefe ` : years of education of male head of household , based on the interaction of escolari ( years of education ) , head of household and gender , yes=1 and no edjefa ` : years of education of female head of household , based on the interaction of escolari ( years of education ) , head of household and gender , yes=1 and no These explanations clear up the issue . For these three variables , __ '' yes '' = 1__ and __ '' no '' = 0__ . We can correct the variables using a mapping and convert to floats .
1558	Stopwords As we can see there is a lot of stopwords to be removed , list-comprehension along with nltk.corpus.stopwords does the trick .
318	Create a submission file
440	Weekday and Meter Reading
689	Study Instance UID ` , ` Series Instance UID ` and ` Patient ID ` will be helpful in organizing our data later . Rows ` and ` Columns ` give us the image resolution ( 512 x 512 in this case Window Center ` and ` Window Width ` tell us the window settings applied to the image at acquisition Rescale Intercept ` and ` Rescale Slope ` tell us how to rescale the pixel values to match the standard Hounsfield Unit ( HU ) scale
1583	We can safely store the two types into separate dataframes
189	What are their top categories
778	To make sure that machine learning is even applicable to the task , we should compare these predictions to a naive guess . For a regression task , this can be as simple as the average value of the target in the training data .
198	Visualize RNA-2D Structure
735	The multi-layer perceptron ( a deep neural network ) has decent performance . This might be an option if we are able to hyperparameter tune the network . However , the limited amount of data could be an issue with a neural network as these generally require hundreds of thousands of examples to learn effectively .
704	Let 's make sure we covered all of the variables and did n't repeat any .
1236	Level 3 XGB
541	Main Config Variables
88	Performance
1494	Composition of functions
940	Group features , aggregates , are one of the most powerful way to capture relationships between variables in the dataset . Sometimes it is possible to group by target variable and thus provide model with direct information about it ( although one should be careful when doing that in order not to introduce a leak , only training data subset can be grouped this way ) . For grouping of other variables , whole dataset can be used , as you are given both train and test data . Important ! - this concerns Kaggle competitions , one should not do this in real-life ML , as you never know what exactly will the distribution of variables in test data be . In this kernel I try to create an end-to-end feature engineering solution based on groupby features . Whole process can be divided into a few steps Selection of columns for each type categorical ` - categorical features which must be encoded ( standard label encoding is used ) . categorical_int ` - categorical features which are alredy in integer dtype , encoding is not needed . numerical ` - numerical features Factorization of ` categorical ` columns , if needed . Creation of aggregates with columns renaming for each type of columns . Resulting DataFrame is saved ( if possible , will not work in Kaggle Kernels ) . There are two ways of aggregated features creation Batch aggregation : all columns from DataFrame are processed , each column is appended to one of three types , ` categorical ` columns are factorized is there 's a need and selected aggregates are applied to each column type . There is a distinction between aggregates for categorical columns and those for numerical , as each type requires a different approach . Selected aggregation : combination of aggregates/columns should be provided in form of a dictionary , where for each column aggregates are specified in a list . II . Setup First , we need to choose aggregates , which will be used for grouping categorical and numerical variables . For numerical variables ` aggs_num_basic ` will be used , for categoricals - ` aggs_cat_basic ` . Those types of aggregations can be extended further , as is shown in ` aggs1 ` list . python aggs_num_basic = [ 'mean ' , 'min ' , 'max ' , 'std ' , 'sem ' , 'sum aggs_cat_basic = [ 'mean ' , 'std ' , 'sum aggs1 = [ 'mean ' , 'median ' , 'min ' , 'max ' , 'count ' , 'std ' , 'sem ' , 'sum ' , 'mad
1098	train solved tasks
255	Andorra
775	Now that we have built a few potentially useful features , we can use them for machine learning : training an algorithm to predict the target from the features . We 'll start off with a basic model - Linear Regression - only using a few features and then move on to a more complex models and more features . There is reason to believe that for this problem , even a simple linear model will perform well because of the strong linear correlation of the distances with the fare . We generally want to use the simplest - and hence most interpretable - model that is above an accuracy threshold ( dependent on the application ) so if a linear model does the job , there 's no need to use a highly complex ensemble model . It 's a best practice to start out with a simple model for just this reason First Model : Linear Regression The first model we 'll make is a simple linear regression using 3 features : the ` abs_lat_diff ` , ` abs_lon_diff ` , and ` passenger_count ` . This is meant to serve as a baseline for us to beat . It 's good to start with a simple model because it will give you a baseline . Also , if a simple model works well enough , then there may be no need for more complex models . If a linear regression will get the job done , then you do n't need a neural network
161	phase 2 [ Stacking
1130	There seem to be difference , but the gap of `` diff_V109_V110 '' is small . In Version7 , I found that `` diff_V109_V110 '' is not meaningful . I deleted . In Version13 , I found that `` diff_V329_V330 '' is not meaningful . I deleted . In Version20 , I found that `` diff_V4_V5 '' is not meaningful . I deleted . I feel that when the gap is big , the column is meaningful .
600	Almost zero . That 's what we expect from Gini when we have no idea about the data and use guessing . We know that the public leaderboard is calculated on approximately 30 % of the test data . So we should create private and public sets . Our generated answers are randomly distributed and we can just subset first 30 % for public dataset and the rest 70 % for private dataset . We also create function that evaluates public and private score .
1287	Empirical studies indicate that changes in real estate sector mirrors the wider changes taking place in the economy at any point in time . Most of these studies put emphasis in explaining how macroeconomic variables are responsible for short and long run variations in residential property prices . According to Schmitz and Brett ( 2001 ) the economic strength of a place can be demonstrated by its macroeconomic conditions , which includes interest rates , inflation , job security , industrial productivity and stock market stability . In another study in Hong Kong , Ervi ( 2002 ) , found out that the rate of return in property markets is linked to economic activities while demand for retail space is sensitive to changes in employment and local output . The author also recognizes that macroeconomic variables include unemployment , inflation rates , GDP , interest rates , balances of payments and foreign exchange rates . American economistSimon Kuznets believes real estate development has a close relationship with economicgrowth after analyzing a large amount of data of different countries . В экономической литературе приводят различные классификации факторов , влияющих на развитие рынка недвижимости : внутренние и внешние , макроэкономические и микроэкономические и т. д. В исследовании были отобраны показатели , характеризующие факторы спроса и предложения и , по мнению авторов , потенциально влияющие на удорожание жилой недвижимости : 1 ) цена на нефть марки Urals ; 2 ) валовой внутренний продукт ( ВВП ) ; 3 ) уровень доходов населения ; 4 ) инфляция ; 5 ) себестоимость строительства ; 6 ) денежная масса ; 7 ) процентная ставка по ипотечным кредитам ; 8 ) количество ипотечных сделок .
1266	AdamW Optimizer ( Weight decay
740	Let 's make a submission with the Random Forest .
1182	Train & Validation Split
393	Now , let 's create training data table ` _id ` , ` category_id
1442	Let 's say you want to pull a random sample of 1 million lines out of the total dataset . That means that you want a list of ` lines - 1 - 1000000 ` random numbers ranging from 1 to 184903891 . Note : generating such long list also takes a lot of space and some time . Be patient and make sure to use del and gc.collect ( ) when done
142	To create dataset we use numpy arrays , and our model needs to understand , which features are categorical , which are continuous . So we need to find their indexes .
93	Distribution looks skewed towards some classes , there are not enough examples for classes 8 and 9 . During training , this can be solved using bias weights , careful sampling in batches or simply removing some of the dominant data to equalize the field . Finally , lets drop the columns we do n't need and be done with the initial cleaning .
1354	RtpStateBitfield - NA
466	Helper functions
592	Seperating the data into different data frame based on the labels
163	MinMax + Mean Stacking
1572	This heatmap show us in average the web traffic by weekdays cross the months . In our data we can see there are less activity in Friday and Saturday for December and November . And the biggest traffic is on the period Monday - Wednesday . It is possible to do Statistics Test to check if our intuition is ok . But You have a lot of works
206	Import libraries
1545	For example glove embeddings -- -not all of the words in the universe will be embedded , hence we will surely not be able to have all of the words of our dictionary represented . But we can reduce the number of such words with some corpus specific analysis . As in [ here ] ( for this competition , and built on top of that is [ this ] ( kernel . In other words there it was searched for OOV-out of vocabulary ( words ) . And we are trying to have less of those . Hence with some text specific analysis ( like the fact that & is in google embedinngs and ? is not we can get rid of such symbols and increase the coverage of our embedding matrix ! But not only that in the following piece of code following things ( among others ) will be conducted Build a vocabulary ( all of the words Than clean it up Clean it of ( out of the place ) numbers , special characters , contractions and incorrect words ...
1551	Another way to look at the outliers but also in the same time get some more information about distribution ( IQR , median , mean etc ... ) is with the box-plot . But we need to do it efficiently
928	Check the typical length of a comment .
1301	Looks like we shaved a whole gig . We 're not done yet , we need to make sure we do the same thing on the test set . Let 's read it in now , merge the tables , and impute it the same way .
747	Results History
333	XGBoost is an ensemble tree method that apply the principle of boosting weak learners ( CARTs generally ) using the gradient descent architecture . XGBoost improves upon the base Gradient Boosting Machines ( GBM ) framework through systems optimization and algorithmic enhancements . Reference [ Towards Data Science .
758	Lets first check the Train Target Distribution
727	We 'll drop the columns and then merge with the ` heads ` data to create a final dataframe .
429	The details of the step from $ K $ to $ K + 1 $ may be a bit confusing from this implementation : it boils down to the fact that Scargle et al . were able to show that given an optimal configuration of $ K $ points , the $ ( K + 1 ) $ ^th configuration is limited to one of $ K $ possibilities . The function as written above takes a sequence of points , and returns the edges of the optimal bins . We 'll visualize the result on top of the histogram we saw earlier
1372	Firewall - This attribute is true ( 1 ) for Windows 8.1 and above if windows firewall is enabled , as reported by the service .
546	No Of Storey Over The Years
1437	There really is n't any difference in distribution of attributions by minute . So does n't really make sense to dig into that one by itself for consistent results . On the other hand a popular feature in the kernels and forums is time deltas . I 'm going to use the method from kernel to generate it , and fill in the missing values with some large value out of range .
1399	Census_IsAlwaysOnAlwaysConnectedCapable - Retreives information about whether the battery enables the device to be AlwaysOnAlwaysConnected .
1327	Load and preprocess data
146	See sample image
1247	Holiday and Store do not show significant relations but just small higher sales soaring when hoiliday
1300	So we can see here that there are some very small ranges there -- not all of them will need to be ` float64 ` . Let 's downcast them .
350	Thanks to Automatic FE The main code for basic FE
1093	This plot shows summarized information about feature impact against shap output .
1493	If you like the content of this notebook , please consider upvoting it .
1587	What are the most frequent assets traded ( remember not all are present at all times , bankruptcy , IPO etc
334	Light GBM is a fast , distributed , high-performance gradient boosting framework based on decision tree algorithms . It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise . So when growing on the same leaf in Light GBM , the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms . Also , it is surprisingly very fast , hence the word ‘ Light ’ . Reference [ Analytics Vidhya
946	adapted from
777	Train with Simple Features
552	Composition of Augmentations
1310	In this competition , diversity of prediction datasets is really important to improve LB score by blending or stacking . Most of all notebooks are using neural networks including gru , lstm and graph . To increase diversity of prediction dataset , I tried to generate good predictions with lightgbm . As a result , I got LB score of 0.27652 . This notebook is mainly based on both by tito , and by T88 . Thank you very much Version I corrected some mistakes . I did parameter tuning for each target by optuna lightgbm tuner . Local CV was improved from 0.23835 to 0.23392 .
1409	Find Null data
1140	These augmentations are inspired by [ CutMix ] ( and [ MixMatch Source
449	Year Built
1402	I 'm not quant trader . But , I know some simple indexes to analyze the charts . Ta-lib is very good and very helpful library for calculating various indexes , but kernel does n't support . So , I introduce some indexes and short scripts to obtain them .
664	One-hot encoding is a process of binarizing the categorical variable . This is done by transforming a categorical variable with n unique values into n unique columns in the datasets while keeping the number of rows the same
114	Entity % 20Relationship % 20Diagram , % 20M5 % 20forecast.jpg ] ( attachment : Entity % 20Relationship % 20Diagram , % 20M5 % 20forecast.jpg
469	Submission
1576	Example
646	The string should be read as space separated series of values where Unicode character , X , Y , Width , and Height are repeated as many times as necessary .
821	train_bureau ` is the training features built manually using the ` bureau ` and ` bureau_balance ` data train_previous ` is the training features built manually using the ` previous ` , ` cash ` , ` credit ` , and ` installments ` data We first will see how many features we built over the manual engineering process . Here we use a couple of set operations to find the columns that are only in the ` bureau ` , only in the ` previous ` , and in both dataframes , indicating that there are ` original ` features from the ` application ` dataframe . Here we are working with a small subset of the data in order to not overwhelm the kernel . This code has also been run on the full dataset ( we will take a look at some of the results ) .
548	Bathroom Count Vs Log Error
135	Predict submission dates
432	Word map for most frequent Tags
1161	Sampling the train data since too much data
1470	Reference
644	Check Unique Label
435	Featurization of Training Data
1342	Emergency
1022	First , we train on the subset of the training set , which is completely in English .
810	To save the ` Trials ` object so it can be read in later for more training , we can use the ` json ` format .
1316	Make new features using continuous feature
939	Output submission
292	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . GaussianNoise_stddev = 0 . LB = -6 .
542	Create final submission DF
1585	There is a specific way to load the data in this competition ( give that we have certain constraints and goals that we want to accomplish later on ) and that is
505	Group signals metadata accroding to target
1568	Read and Explore
1103	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags wind_speed + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross validation . I go not using this data in this kernel for robust modeling .
538	Visuallizing Interest Level Vs Bathroom
1197	Let 's try this one ..
877	Distribution of Scores
1195	So according to the documentation `` We recognize that toxicity and identity can be subjective , so we collect labels from up to 10 people per comment , to capture a range of opinions . '' Again , they say `` up to 10 people per comment '' . I interpret this to mean that they might not always take 10 , in some cases they might take 9 or 8 or 7 or 6 or 5 , etc . In this case it seems there were 6 annotators and only 1 of them found the comment to be toxic . Let 's learn a bit more about of annotators ...
817	Random Search on the Full Dataset
741	One potential method for improving model performance is feature selection . This is the process where we try to keep only the most useful features for our model . `` Most useful '' can mean many different things , and there are numerous heuristics for selecting the most important features . For feature selection in this notebook , we 'll first remove any columns with greater than 0.95 correlation ( we already did some of this during feature engineering ) and then we 'll apply recursive feature elimination with the Scikit-Learn library . First up are the correlations . 0.95 is an arbitrary threshold - feel free to change the values and see how the performance changes
1488	The lower part of the right lung of Patient 7 ( the right lung is at the left side of the picture ) is higher than in a normal image . This is a called a pleural effusion . It is caused by an accumulation of fluid in the chest outside of the lung . This causes the lung to look smaller on the chest radiograph . Lung Masses and Nodules Back to top ] ( Table-of-Contents
283	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
1043	Public and private sets have different sequence lengths , so we will preprocess them separately and load models of different tensor shapes .
1010	Save model and weights
186	Can we split those categories by level Items seems to be classify with an tree structure of 3 levels .
96	Read train_variants , train_text and join them .
224	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
313	What is the AUC Score
1285	This design can is independent of pandas and can be ported to other data processing frameworks such as spark . Any logical code written using pandas can then be ported over to run on a spark based parallel processing system using the ` Koalas ` API , yet following a similar design pattern . Pure Functions Pure functions have two important properties If given the same arguments , the function must return the same value When the function is evaluated , there are no side effects ( no I/O streams , no mutation of static and non-local variables Fig . 4 A visual representation of pure functions Source When writing functions for data transformations , we can not always write pure functions , especially considering the limitations of system memory , given that to mutate a non-local variable would require to create an entire new copy of the dataframe . Therefore , it is important to have a boolean argument ` inplace ` which can help the developer decide whether or not to mutate the dataframe as per the requirements of the situation . Type Hinting Type hinting , was introduced in ` PEP 484 ` and ` Python 3.5 ` . Therefore I donot recommend using it completely as of now , unless you are sure that all of the libraries you use in your workflow are compatible for ` Python 3.5 ` and above . The basic structure of type hinting in python is as follows def fn_name ( arg_name : arg_type ) - > return_type pass Once a function definition is complete , use the ` - > ` symbol to indicate the return type , it could be ` int ` , ` dict ` or any other python data type . Every argument in the function is followd by a ` : ` and the data type of the argument You can also use more complex ways of representing nested data types , optional data types , etc . using the ` typing ` module in Python Below , you can find an example where I use the ` typing ` module and use type hinting in python
327	Linear Regression is a linear approach to modeling the relationship between a scalar response ( or dependent variable ) and one or more explanatory variables ( or independent variables ) . The case of one explanatory variable is called simple linear regression . For more than one explanatory variable , the process is called multiple linear regression . Reference [ Wikipedia Note the confidence score generated by the model based on our training dataset .
1393	Census_FirmwareVersionIdentifier - NA
1577	Replace infs and imputing missing values by mean
1221	Porto Seguo - End-to-end Ensemble
130	Count occurance of words
788	Test Time Features
781	Using this one more feature improved our score slightly . Here 's another chance for improvement using the same model Potential Improvement 3 : find an optimal set of features or construct more features__ . This can involve [ feature selection ] ( or trying different combinations of features and evaluating them on the validation data . You can build additional features by looking at others ' work or researching the problem . Collinear Features One thing we do want to be careful about is highly correlated , known as [ collinear ] ( features . These can decrease the generalization performance of the model and lead to less interpretable models . Many of our features are already highly correlated as shown in the heatmap below . This plots the Pearson Correlation Coefficient for each pair of variables .
1220	Prediction for test
958	Submission output
1083	Build the original and translated test data .
514	Cropping the Images Using the crop lists from above , getting a set of cropped images for a given threat zone is also straight forward . The same note applies here as in the 4x4 visualization above , I used a cv2.resize to get around a size constraint ( see above ) , therefore the images are quite blurry at this resolution . If you do not face this size constraint , drop the resize . Note that the blurriness only applies the unit test and visualization . The data returned by this function is at full resolution .
1133	when it comes to Android browser , I will use different way because chrome has `` chrome for android
23	For our second model we 'll use TF-IDF with a logistic regression . The next couple of secontions are based on my [ LR with n-grams notebook ] ( Firtst , let 's embed all the text vectors
1446	There are different sections to Dask , but for this case you 'll likely just use Dask DataFrames . Here are some basics from the developers A Dask DataFrame is a large parallel dataframe composed of many smaller Pandas dataframes , split along the index . These pandas dataframes may live on disk for larger-than-memory computing on a single machine , or on many different machines in a cluster . One Dask dataframe operation triggers many operations on the constituent Pandas dataframes . For convenience and Dask.dataframe copies the Pandas API . Thus commands look and feel familiar . What DaskDataframes can do they are very fast on most commonly used set of Pandas API below is taken directly from Trivially parallelizable operations ( fast Elementwise operations : ` df.x + df.y , df df Row-wise selections : ` df [ df.x Loc : ` df.loc [ 4.0:10 . Common aggregations : ` df.x.max ( ) , df.max Is in : ` df [ df.x.isin ( [ 1 , 2 , Datetime/string accessors : ` df.timestamp.month Cleverly parallelizable operations ( fast groupby-aggregate ( with common aggregations ) : ` df.groupby ( df.x ) .y.max ( ) , df.groupby ( ' x ' ) .max groupby-apply on index : ` df.groupby ( [ 'idx ' , ' x ' ] ) .apply ( myfunc ) ` , where ` idx ` is the index level name value_counts : ` df.x.value_counts Drop duplicates : ` df.x.drop_duplicates Join on index : ` dd.merge ( df1 , df2 , left_index=True , right_index=True ` ) or ` dd.merge ( df1 , df2 , on= [ 'idx ' , ' x ' ] ) ` where ` idx ` is the index name for both ` df1 ` and ` df Join with Pandas DataFrames : ` dd.merge ( df1 , df2 , on='id Elementwise operations with different partitions / divisions : ` df1.x + df2.y Datetime resampling : ` df.resample ( ... Rolling averages : ` df.rolling ( ... Pearson Correlations : ` df [ [ 'col1 ' , 'col2 ' ] ] .corr Notes/observations To actually get results of many of the above functions you have to add ` .compute ( ) ` at the end . eg , for value_counts would be : ` df.x.value_counts ( ) .compute ( ) ` . This hikes up RAM use a lot . I believe it 's because ` .compute ( ) ` gets the data into pandas format , with all the accompanying overhead . ( Please correct me if wrong ) . I 've been playing with dask for the past little while here on Kaggle Kernels , and while they can load full data and do some nice filtering , many actual operations do hike up RAM to extreme and even crush the system . For example , after loading 'train ' dataframe , just getting ` len ( train ) ` hiked RAM up to 9GB . So be careful ... Use a lot of ` gc.collect ( ) ` and other techniques for making data smaller . So far I find dask most useful for filtering ( selecting rows with specified features ) . Now let 's see some examples . First , let 's load the big train data
234	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
1396	Census_IsVirtualDevice - Identifies a Virtual Machine ( machine learning model
1099	evaluation solved tasks
1506	The method for training is borrowed from
1312	preprocess data
1552	Correlation map- after throwing the outliers and missing values away ( since it is neccessary before calculating pearson correlation coefficient
1591	Following dictionary will be used for aggregations ( after we merged the datasets further down below
601	The chart showing dependence between number of spoiled samples and Gini is the following
890	The bureau balance dataframe has a ` MONTHS_BALANCE ` column that we can use as a months offset . The resulting column of dates can be used as a ` time_index ` .
323	Set Up the Generators
929	Train a word2vec model
6	Seems like a pretty nice normal-looking distribution , except for the few anomalous elements at teh far left . They will have to be dealt with separately . Let 's look at the `` violin '' version of the same plot .
539	Visualizing Interest Level Vs Bedrooms
1025	Load text data into memory
365	Apply skin segmentation on all training data and visualize the result
1039	For each sample , we take the predicted tensors of shape ( 107 , 5 ) or ( 130 , 5 ) , and convert them to the long format ( i.e . $ 629 \times 107 , 5 $ or $ 3005 \times 130 ,
217	Import libraries
1280	Data transformation and helper functions
611	Let 's load the data and the embeddings ...
1308	Check the dataset
1418	You only have two areas to work on .
1501	Ensure determinism in the results
1567	Process the training , testing and 'other ' datasets , and then check to ensure the arrays look reasonable .
1449	Explore ip counts . Check if multiple ips have any downloads .
765	For visualization purposes , I 'll create a binned version of the fare . This divides the variable into a number of bins , turning a continuous variable into a discrete , categorical variable .
330	Stochastic gradient descent ( often abbreviated SGD ) is an iterative method for optimizing an objective function with suitable smoothness properties ( e.g . differentiable or subdifferentiable ) . It can be regarded as a stochastic approximation of gradient descent optimization , since it replaces the actual gradient ( calculated from the entire data set ) by an estimate thereof ( calculated from a randomly selected subset of the data ) . Especially in big data applications this reduces the computational burden , achieving faster iterations in trade for a slightly lower convergence rate . Reference [ Wikipedia
1104	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so it is better to consider time-splitting . However this notebook is for simple tutorial , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
1086	Ensemble with my historical best .
1	Now let 's import datatable
1226	Xgboost K-fold & OOF function
663	I tried to encode cyclic feature with trigonometric functions and expect this technique can be useful to understand and extract insights from samples based upon the patterns and behaviors of the data points over a specific time period
1000	TPU Strategy and other configs
39	Let 's now add some non-image features . We can start with sex , and one-hot encode it .
229	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
743	We can investigate the object to see the training scores for each iteration . The following code will plot the validation scores versus the number of features for the training .
629	Interactive booking , click , and percentage of booking trends with Bokeh
490	The first layer in the network must define the number of inputs to expect . For a Multilayer Perceptron model this is specified by `` input_dim '' attribute .
118	Let 's start by checking how many data points there are , and if anything is missing .
493	Connecting Layers
1477	Disable fastai randomness
1533	See the non-sparse variables in format way
175	Importation of a entire day data
995	Predict Test Set and Submit Result
141	Let 's get train and test again .
1090	Split Trian and Valid
257	Linear Regression is a linear approach to modeling the relationship between a scalar response ( or dependent variable ) and one or more explanatory variables ( or independent variables ) . The case of one explanatory variable is called simple linear regression . For more than one explanatory variable , the process is called multiple linear regression . Reference [ Wikipedia Note the confidence score generated by the model based on our training dataset .
262	Random Forest is one of the most popular model . Random forests or random decision forests are an ensemble learning method for classification , regression and other tasks , that operate by constructing a multitude of decision trees ( n_estimators= [ 100 , 300 ] ) at training time and outputting the class that is the mode of the classes ( classification ) or mean prediction ( regression ) of the individual trees . Reference [ Wikipedia
1351	I think this feature means the type of batteries of each machine . Oh , no .... These days , most batteries are lithum-ion battery . So , Let 's group them into lithum-batter group and non0-lithum-battery group
973	Note that , you ca n't access this field via name ( ValueError raised
1125	There is a gap between them .
338	The core principle of AdaBoost is to fit a sequence of weak learners ( i.e. , models that are only slightly better than random guessing , such as small decision trees ) on repeatedly modified versions of the data . The predictions from all of them are then combined through a weighted majority vote ( or sum ) to produce the final prediction . The data modifications at each so-called boosting iteration consist of applying N weights to each of the training samples . Initially , those weights are all set to 1/N , so that the first step simply trains a weak learner on the original data . For each successive iteration , the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data . At a given step , those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased , whereas the weights are decreased for those that were predicted correctly . As iterations proceed , examples that are difficult to predict receive ever-increasing influence . Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence . Reference [ sklearn documentation
1467	observe sales at the scale of state California generally has better salls than the other two states . Apart from foods Texas is better than Wisconsin The total sales of the category is : Foods > household > hobbies
1080	Drop the blurry image
1242	Let 's make a pie chart to show the ratio of A , B , and C types of total 45 Walmart stores . First , let 's group data by type of stores and see the descriptive figures
866	If you are interested in running this call on the entire dataset and making the features , I wrote a script [ for that here ] ( Unfortunately , this will not run in a Kaggle kernel due to the computational expense of the operation . Using a computer with 64GB of ram , this function call took around 24 hours ( I do n't think I 'm technically breaking the rules of my university 's high powered computing center ) . I have made the entire dataset available [ here ] ( in the file called ` feature_matrix.csv ` . To generate a subset of the features , run the code cell below .
433	Bar plot of top 20 tags
1407	Data understanding
411	Let 's check duplicates between training and test datasets
638	Baseline Format @ Cam Askew Previous winning solutions from @ Sanyam Bhutani Submission Guidleines by Andre Araujo
1458	For test data , set default start & end positions to dummy integer ( -1 ) .
1565	Features Hilbert transform Hann Window classic_sta_lta Clarification [ consult Various variations of moving averages Regarding MA values and their derivation . WE can gauge the lookback horizont and combinations of MA variables with feature importance . So I did have some baseline , than I saw what variables are the most potent and I played around a bit until I found some indicators . Same logic can be applied to quantiles . After all x is just an series of integer values , and we know from eda that right before earthquake ( but not exactly next milisecond ) accustic values will be huge -- - > in the top of the quantiles . 99 quantile is too much as we can see from graphs but around 95 is the sweetspot -- -- > modify the values of q IMPLICATION Implication of the same thinking that went into modifying quantiles . If we know what we know about values of signal right before earthquake than modifying the values `` std_last_10000 '' and other similiar variables should make an impact . Why ? Simple hypothesis : For example in the last 5000 values of accoustic_signal we will find huge standard deviation . That can be powerful predicator GOAL : I think an avic reader can find a systemic way to gauge these values , and not only trial & error
764	Data Exploration and Data Cleaning
897	Putting it all Together
1059	Step 1 : Remove test images without defects
924	Granted applications per number of children
247	Ensembling the solutions and submission
507	Apply reduction on some samples and visualize the results
460	Encoding Cordinal Direction
131	All contraction are known
692	Combinations of TTA
43	Let 's take a look at some of these
1204	Train LSTM with 3 epochs
1134	Importing Library Files
471	MERGE , MISSING VALUE , FILL NA
1205	Lets have a look at the year difference between the year of transaction and the year built
1561	Extending
14	In order for our pretrained models to work , we need to transform the text here into the appropriate vectorized format .
145	Prepare Traning Data
1292	Data preparation for test
120	FVC Difference
468	Loading the data
138	So , it is really hard to understand something from this data . Maybe there is a connection between ord_2 ( it can be temperature ) and month . Let 's try to check .
64	There is some structure there , with `` central '' denser reagion , and more dispersed periphery , but other than that it 's hard to see any distinct groupings .
676	Import ` trackml-library The easiest and best way to load the data is with the [ trackml-library ] that was built for this purpose . Under your kernel 's Settings tab - > Add a custom package - > GitHub user/repo ( LAL/trackml-library Restart your kernel s session and you will be good to go . trackml-library
1356	DefaultBrowsersIdentifier - ID for the machine 's default browser
1052	Load the U-Net++ model trained in the previous kernel .
487	Bag-of-Words Model in Keras
570	Importing libs
994	OSIC training data Example
438	Variable Description and Identification
1377	Census_ProcessorManufacturerIdentifier - NA
270	Commit now
1169	We see how often the different catagories occur , thus giving us insight on how to pick up the data for training . As we can see that None of the catagories occur the most and thus we will sample ~10,000 sentences from it .
1180	Target & ID Loading
968	Gaussian Approximation of Active Cases
497	identifying the missing value in bureau_balance
1339	FONDKAPREMONT
833	Combined Aggregation Function
389	Display for example a item with ` _id
193	Can the length of the description give us some informations
1544	We initialise the Tokenizer ( which is a class from keras package ) , and we stop at the desired number of words that we want to ( en ) -tokenize . `` fit_on_texts '' than makes the representation based on the text we pass ( in our case the Training set ) , than we call a method `` texts_to_sequences '' which performs `` label-encoding '' of our vocabulary ( output of the function below is exactly `` tokenizer.word_index '' ) . In other words we have just represented the desired number of words in a machine readable form . But it is still not good in enough . We can do better that . Thats were the transition to word2vec and other embeddings comes to place . Remember from to theory , sparcity is really bad when our dictionary is large ! One natural question that arises is after we used texts_to_sequences on train_X we used on test and validation data also . But we trained it on the train data set , so what happens if we do not come across a word that was in our train set ? Let us test it
882	The bayesian optimization results are close in trend to those from random search : lower learning rate leads to higher cross validation scores .
725	With just that one line , we go from 30 features to 180 . Next we can rename the columns to make it easier to keep track .
867	DFS with Selected Aggregation Primitives
841	This is usually the point at which the kernel fails.__ To try and alleviate the problem , I have added a pause of 10 minutes .
956	Perform check on randomly chosen mask and prediction
110	Image Source
1323	I want mix education and area features - > education_zone_features
201	Please note that when you apply this , to save the new spacing ! Due to rounding this may be slightly off from the desired spacing ( above script picks the best possible spacing with rounding ) . Let 's resample our patient 's pixels to an isomorphic resolution of 1 by 1 by 1 mm .
124	In this section I 'll explore the DICOM files representing the CT Lung Scans for the patients , how to load and process them , and how to extract some additional features . For an even more informative explanation , you can visit the Radiology Data Quest blog ( from where I learned the tools used in the rest of this notebook and which I thank for their public explanation . It 's also necessary to thank Kaggle user Dr.Sàndor Kónya ( @ sandorkonya ) for his wonderful Domain Expert Insights , which can be found here ( Part 1 : Part
824	Identify Correlated Variables
694	Read in Data and Look at Summary Information
223	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
509	Zone 1 ? Really Looking at this distribution I 'm left to wonder whether the stage 2 data might be substantially different , and certainly one might think real world data would be different . Zone 1 , the right arm , seems unlikely to be the population 's most likely zone to hide contraband . To begin with , you would have to put your contraband in place with your left hand . So unless most airline passengers that carry contraband are left handed , I am guessing that the real world ( and perhaps stage 2 ) will look different . Its just a guess of course ... Anyway , I am convinced it will be important to figure out how to get as many examples by threat zone as possible . In any event , it will also be handy to easily get a list of zones and probabilities from the labels file , so I added this in here . Note that the subject has contraband in zone 14 ( left leg ) . We 'll keep an eye out for that
392	Level 2 and 3 categories are distributed as follows
1335	Read dataset
1536	NOTE This NaN handling is just for the sake of it . It is by no-means complete and there are lot of them underneath ( function is built that shows us percentage ) . But there is a specific way that GBM ( light and xBGM ) handle missing values . So even tough it would be better we want to focus on algortihm and automatic feature engineering
918	Monthly Credit Data
287	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
1445	TIP 5 Importing just selected columns
375	Light GBM is a fast , distributed , high-performance gradient boosting framework based on decision tree algorithms . It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise . So when growing on the same leaf in Light GBM , the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms . Also , it is surprisingly very fast , hence the word ‘ Light ’ . Reference [ Analytics Vidhya
1346	Not as much as EXT_SOURCE_1 do , EXT_SOURCE_2 shows different distribution for each repay and not-repay .
947	For this competition , dictionary containing information about dataset is available . That 's helpful for feature engineering , as it provides a possible direction of engineering for each feature . One thing to keep in mind is that this set was created _artificially All data is simulated and fictitious , and is not real customer data Kernel environment has it 's memory and speed constraints , therefore ` historical_transactions.csv ` file will not be used , as it 's the biggest one . We will base our workflow on remaining set of files .
511	Rescaling the Image Most image preprocessing functions want the image as grayscale . So here 's a function that rescales to the normal grayscale range .
154	Saving the model
907	Putting the Functions Together
1127	Model Evaluation and Validation
200	Let 's take a look at one of the patients .
103	Take a look at predictions
1107	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags wind_speed + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross validation . I go not using this data in this kernel for robust modeling .
30	Not bad ! AUC of 0.903 is pretty good for any predictive model . But is it overfitting ? There is only one way to find out ! Let 's submit it and see how it performs on public LB .
1574	Prophet is a forecasting tool availaible in python and R. This tool was created by Facebook . More information on the library here Compared to the two methods this one will be faster . We can forecast a time series with few lines . In our case we will do a forecast and a display the trend of activity on the period and for a week .
484	The same vectorizer can be used on documents that contain words not included in the vocabulary . These words are ignored and no count is given in the resulting vector .
340	We can now compare our models and to choose the best one for our problem .
832	We only need a few prinicipal components to account for the majority of variance in the data . We can use the first two principal components to visualize the entire dataset . We will color the datapoints by the value of the target to see if using two principal components clearly separates the classes .
1345	The simple kde plot ( kernel density estimation plot ) shows that the distribution of repay and not-repay is different for EXT_SOURCE_1 . EXT_SOURCE_1 can be good feature .
985	Outlier Analysis and Feature Scaling
437	Importing Packages and Collecting Data
1481	Submitting results
1522	Instead of 0.5 , one can adjust the values of the threshold for each class individually to boost the score . However , it should be done for each model individually .
337	ExtraTreesRegressor implements a meta estimator that fits a number of randomized decision trees ( a.k.a . extra-trees ) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting . The default values for the parameters controlling the size of the trees ( e.g . max_depth , min_samples_leaf , etc . ) lead to fully grown and unpruned trees which can potentially be very large on some data sets . To reduce memory consumption , the complexity and size of the trees should be controlled by setting those parameter values . Reference [ sklearn documentation In extremely randomized trees , randomness goes one step further in the way splits are computed . As in random forests , a random subset of candidate features is used , but instead of looking for the most discriminative thresholds , thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule . This usually allows to reduce the variance of the model a bit more , at the expense of a slightly greater increase in bias . Reference [ sklearn documentation
776	Create Training and Validation Set
4	We see that in addition to the usual , ` train ` , ` test ` and ` sample_submission ` files , we also have ` merchants ` , ` historical_transactions ` , ` new_merchant_transactions ` , and even one ( HORROR ! ! ! ) excel file - ` Data_Dictionary ` . The names of the files are pretty self-explanatory , but we 'll take a look at them and explore them . First , let 's look at the ` train ` and ` test ` files .
799	Now we can evaluate the baseline model on the testing data .
543	Global Imports
931	Define helper functions
584	Getting population for each country
1379	Census_PrimaryDiskTotalCapacity - Amount of disk space on primary disk of the machine in MB
1138	We need to add the format of the images in the end or Use glob for flexibility
996	Site Thanks to [ @ yamsam ] ( and his great kernel .
317	Make a prediction on the test images
388	So , in train dataset we have products indexed by ` _id ` , belong to a ` category_id ` and described by 1-4 images . Now , let 's quickly take a look to test products
607	Load and Explore Data
445	Meter Readings over time
119	The following two variables represents the actual FVC values represented in the dataset by the `` Percent '' variable and their difference . Interestingly , there are a couple patients where Percent is over 100 % , meaning the measured FVC is higher than what it 's supposed to be .
1186	process training images
1110	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags wind_speed + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross validation . I go not using this data in this kernel for robust modeling .
1324	Mix region and education
642	filtering out outliers
117	Before decomposition , looking at each data , we can see that the sales of one day fell sharply and cut . It is speculated that this is because it is taking a break at Christmas . This data will be a strong noise , so this time it will be deleted .
102	The data is not balanced . We are going to use the undersampling technique .
1196	So what we want to do is take a toxic comment , and find the most similar non-toxic comment We found an article that might be able to help us out here Let 's pick a toxic string we want to use ...
976	Due to this problem , we will move on with some function wrapper for this
1029	Now that we have pretty much saturated the learning potential of the model on english only data , we train it for one more epoch on the ` validation ` set , which is significantly smaller but contains a mixture of different languages .
1087	This is a simple modify from
322	Train Test Split
116	price distribution
1040	Load and preprocess data
164	MinMax + Median Stacking
380	Thanks for the example of ensemling different models from
140	Using LabelEncoding we just change string values to numbers .
1218	Attach to our trainer a function to run a validator at the end of each epoch
139	Feature ord_5 consist of two letters , so we can divide it on two features .
481	As feature engineering is completed with the above step and we also have our validation and train dataset , we can model our data using LightGBM . Before moving to LightGBM , we need to understand how other boosting models work and why LGBM is a good boosting model to start with . I will be starting with few questions discussing about gradient boosting and XGBoost . What is Gradient Boosting It is a boosting algorithm in which the loss is minimised using Gradient Descent method What is XGBoost XGBoost is a regularised boosting model and hence reduces overfitting when compared to other boosting algorithms . It also implements parallel processing and is faster compared to other boosting algorithms Now that we have a basic understanding about Gradient Boosting Models and XGBoost , let 's move on to LightGBM . What is LightGBM ? Why did I use LGBM instead of other boosting algorithms ( Ex : XGBoost Light GBM is a fast , distributed , high-performance gradient boosting framework . Unlike other boosting algorithms it splits the trees leafwise and not level wise . LGBM runs very fast , hence the word 'light ' . It trains faster ( on larger datasets ) compared to other boosting algorithms like XGBoost . It uses leaf wise splitting instead of level wise splitting . Leaf wise splitting may lead to overfitting . This can be avoided by specifying tree-specific hyper parameters like max depth . In my case , I have used num_leaves hyper-parameter to avoid overfitting . Before moving on to discussing about hyper-parameters in LightGBM , let 's discuss about different types of parameters in a boosting model .. What are the parameters in a boosting model Generally boosting algorithms consists of large number of hyperparameters that are to be tuned to perform better than baseline model . These parameters may tune the trees in the model ( Ex : min_samples_leaf ) or are specific to boosting ( Ex : learning rate ) . Above , we have discussed about the types of parameters in a model . Let 's move on to parameters specific to LightGBM Hyper-parameters to tune in LGBM For best fit and better accuracy num_leaves : Number of leaves to form a complete tree . Either this or max_depth can be set . As setting max_depth leads to limiting the number of leaf nodes in tree which equals to 2^max_depth . One can either set this or max_depth to avoid overfitting . Setting both of the hyper parameters may result in dampening one of them and underfitting the tree . min_data_in_leaf : Minimum number of samples required in a leaf node . Too low a value results in overfitting whereas a very high value may result in underfitting . This value results on size of underlying dataset and needs to be carefully tuned For faster speed Bagging_fraction : Fraction of data to be used in each iteration . Default is 1 . Can use a smaller value ( Typically ranging from 0.8 to 1.0 ) to improve the speed of model and reduce overfitting . Feature_fraction : Fraction of features to be used in each iteration . Default is 1 i.e all features are used . Similar to bagging_fraction , we can use a smaller value to improve the speed of training . Typical values are between 0.8 and 1 . Other useful tuning hyper_parameters learning_rate : learning rate of boosting algorithm . Default is 0.1 . Typical values are from 0.01 to 0.2 and may extend upto 0.3 . Higher the learning rate , faster the algorithm runs . Lower learning rates help the algorithm in generalising well but take a lot more training time . n_estimators : Number of trees to fit . I have n't found many resources on how this parameter behaves with respect to LGBM but what I have observed it is generally results in a higher score . It also generalises the model better with more trees . Training time is directly proportial to number of trees initially and then tends to increase more than linear as it gets harder and harder to increase the accuracy of models . Extra information : If we dig into lgbm code ( and sklearn code for lgbm ( we can see that n_estimators is same as num_boost_round which is equivalent to any on of these `` num_iterations '' , `` num_iteration '' , `` num_tree '' , `` num_trees '' , `` num_round '' , `` num_rounds '' . max_bin : maximum number of buckets used . Higher values results in better accuracy whereas lower value results in faster computation . Other parameters such as objective , metric and boosting are specific to each data set . In our case , metric is going to be auc . Objective ' : 'binary ' refers to binary classification , 'Boosting ' : 'gbdt ' ( Gradient Boosted Decision Trees ) refers to the boosting type we are using , 'Verbose ' refers to the level of details we want to be printed and 'metric ' : auc refers to our evaluation metric on our validation set . Please note that , I am using 500 rounds and as such , this will take anywhere from 30 minutes to 2 hours ( approximately ) to run depending on your system . I would like to recieve feedback on this kernel . Please correct me if I have wrongly interpreted anything that I have posted .
826	Let 's drop the columns , one-hot encode the dataframes , and then align the columns of the dataframes .
245	Parameters and LB score visualization
1166	Prepare Data & Loader
504	Paths to data and metadata
1185	Get Tabular Data
1217	Let 's define which metrics we will use and create magic objects to train and validate our model
81	Also we have information about breed , and some animals has pure or mixed breed . I wonder if breed purity has some impact on the fate of animal .
167	Zoom on this IP
858	First we can plot the validation scores versus the iteration . Here we will use the [ Altair ] ( visualization library to make some plots ! First , we need to put our data into a long format dataframe .
1459	This notebook will deal with positive , negative and neutral samples independently .
1157	Next we 'll create a new dataframe that summarizes wins & losses along with their corresponding seed differences . This is the meat of what we 'll be creating our model on .
1070	Relevant attributes of identified objects are stored
647	Non physical data augmentation
534	How many orders users generally made
418	Good ! We 've got five cluster again . The number of samples in each cluster is the following
643	using outliers column as labels instead of target column
488	Document Vectors with hashing trick
1289	Best parameters are searched by GridSearchCV on my Laptop
1471	This notebook uses display_aspect_ratio metadata field as a great fake video predictor .
268	Thanks for the example of ensemling different models from
614	Loading data
936	Sample aggregations
1412	Target , prediction process
148	See how our generator work
19	Wow , this is a very balanced dataset . No surprises , since this is all presumably artificial data .
938	Train the LGBM model
1153	Rolling monthly and yearly store means
204	Import Required Libraries
150	Create Testing Generator
1101	Fast data loading
436	Fitting Logistic Regression with OneVsRest Classifier
1036	Public and private sets have different sequence lengths , so we will preprocess them separately and load models of different tensor shapes .
1380	Census_SystemVolumeTotalCapacity - The size of the partition that the System volume is installed on in MB
271	Commit 1 ( parameters from
714	The Spearman correlation is often considered to be better for ordinal variables such as the Target or the years of education . Most relationshisp in the real world are n't linear , and although the Pearson correlation can be an approximation of how related two variables are , it 's inexact and not the best method of comparison .
1263	Hugging Face pretrained Bert model names
500	Checking the Correlation Between The Features for Application Train Dataset
756	We 've our image ready , let 's create an array of bounding boxes for all the wheat heads in the above image and the array of labels ( we 've only 2 class here : wheat head and background ) . As all bounding boxes are of same class , labels array will contain only 1 's .
583	USA since first case
1424	Predict by Specify Country
1371	IeVerIdentifier - NA
1112	Leak Validation for public kernels ( not used leak data
619	Linear Regression
1438	i do intend to make this a linked heading at some point ... TIP 1 - Deleting unused variables and gc.collect TIP 2 - Presetting the datatypes TIP 3 - Importing selected rows of the a file ( including generating your own subsamples TIP 4 - Importing in batches and processing each individually TIP 5 - Importing just selected columns TIP 6 - Creative data processing TIP 7 - Using Dask
16	In other words , at nearly 1.0 probability the model seems pretty confident about the `` toxicity '' of some of the tweets . Now let 's put the predictions into a dataframe , so we can have a better view of them and how they relate to the actual tweets .
1135	I focused on browser I am a very lazy person , so I rarely update my browser . How swindlers are
613	Looking at training and validation loss / accuracy figures below , we can see there is no sign of over-fitting .
212	Download datasets
275	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
1540	NaN imputation will be skipped in this tutorial .
236	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
219	Commit 0 ( parameters from commit
1564	Let us now extract individual topics and build world cloud for them ( for the most occuring words in these topics ) in order to get a better picture what happens in our model .
1550	What should good EDA be capable of
557	Parameters
577	Let 's have a look at China
431	Data preprocessing
702	The meaning of the home ownership variables is below tipovivi1 , =1 own and fully paid house tipovivi2 , `` =1 own , paying in installments tipovivi3 , =1 rented tipovivi4 , =1 precarious tipovivi5 , `` =1 other ( assigned , borrowed We 've solved the issue ! Well , mostly : the households that do not have a monthly rent payment generally own their own home . In a few other situations , we are not sure of the reason for the missing information . For the houses that are owned and have a missing monthly rent payment , we can set the value of the rent payment to zero . For the other homes , we can leave the missing values to be imputed but we 'll add a flag ( Boolean ) column indicating that these households had missing values .
416	To give us a more intuitive feeling about the sales behavior , I 'm comparing the sales behavior of the corporation per state . Two interesting insights come out The corporation is n't growing their market share or improving sales force in 2017 . The total sales units have a static behavior all over the year ( for the engineers , the sales are in their steady state Visually one might realize that sales have a very interesing seasonality .
540	Correlation Between Price and Other Features
1035	Load and preprocess data
1487	Opacities That Are Not Related to Pneumonia
1505	Two embedding matrices have been used . Glove , and paragram . The mean of the two is used as the final embedding matrix
104	Ability to Detect Face
1546	Save some memory
1373	UacLuaenable - This attribute reports whether or not the `` administrator in Admin Approval Mode '' user type is disabled or enabled in UAC . The value reported is obtained by reading the regkey HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\System\EnableLUA .
566	Prediction on test set
90	Let 's take a look at the text data . Data is still small enough for memory so read to memory using pandas .
7	Let 's now look at the distributions of various `` features
683	The test set data features have a higher low mean value ( left plot ) , and higher low variance ( right plot ) .
267	The core principle of AdaBoost is to fit a sequence of weak learners ( i.e. , models that are only slightly better than random guessing , such as small decision trees ) on repeatedly modified versions of the data . The predictions from all of them are then combined through a weighted majority vote ( or sum ) to produce the final prediction . The data modifications at each so-called boosting iteration consist of applying N weights to each of the training samples . Initially , those weights are all set to 1/N , so that the first step simply trains a weak learner on the original data . For each successive iteration , the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data . At a given step , those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased , whereas the weights are decreased for those that were predicted correctly . As iterations proceed , examples that are difficult to predict receive ever-increasing influence . Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence . Reference [ sklearn documentation
536	Spectral flux Spectral flux is a measure of how quickly the power spectrum of a signal is changing , calculated by comparing the power spectrum for one frame against the power spectrum from the previous frame .
1408	Data check
904	First we one-hot encode a dataframe with only the categorical columns ( ` dtype == 'object ' ` ) .
1129	Introduction - Do they match
875	Well , there you go ! __Random search slightly outperformed Bayesian optimization and found a higher cross validation model in far fewer iterations.__ However , as we will shortly see , this does not mean random search is the better hyperparameter optimization method . When submitted to the competition ( at the end of this notebook Random search results scored 0 . Bayesian optimization results scored 0 . What were the best model hyperparameters from both methods Random Search best Hyperparameters
1148	Load the data and add L2 distance computation between atoms
1235	Level 3 ensemble
1400	Wdft_IsGamer - Indicates whether the device is a gamer device or not based on its hardware combination .
1592	Now lets drop all non-numeric and non-usable features
305	Wavenet with SHIFTED-RFC Proba and CBR
1117	Some features introduced in by @ ryches Features that are likely predictive Weather time of day holiday weekend cloud_coverage + lags dew_temperature + lags precip_depth + lags sea_level_pressure + lags wind_direction + lags wind_speed + lags Train max , mean , min , std of the specific building historically However we should be careful of putting time feature , since we have only 1 year data in training , including ` date ` makes overfiting to training data . How about ` month ` ? It may be better to check performance by cross validation . I go not using this data in this kernel for robust modeling .
73	Importing all Libraries
1222	let 's run the frequency encoding function
1131	We can not use literal features for XGB , so these features are changes . For example , [ H , G , W , A ] → [ 0,1,2 , The number of the words is often related to the numeral ( [ 0,1,2,3 ] ) .
303	Commit now
880	The next plot is learning rate and number of estimators versus the score . __Remember that the number of estimators was selected using early stopping for 100 rounds with 5-fold cross validation__ . The number of estimators __was not__ a hyperparameter in the grid that we searched over . Early stopping is a more efficient method of finding the best number of estimators than including it in a search ( based on my limited experience
261	This model uses a Decision Tree as a predictive model which maps features ( tree branches ) to conclusions about the target value ( tree leaves ) . Tree models where the target variable can take a finite set of values are called classification trees ; in these tree structures , leaves represent class labels and branches represent conjunctions of features that lead to those class labels . Decision trees where the target variable can take continuous values ( typically real numbers ) are called regression trees . Reference [ Wikipedia
85	Another one parameter is age , but we have it in different units : years , months , weeks and days . Let 's calculate every age in years and see if there is something interesting inside .
631	expanding the aggregate
746	We can have the function instead return the actual submission file . This takes the average predictions across the five folds , in effectm combining 5 different models , each one trained on a slghtly different subset of the data .
1253	Cod_prov ( Province Code ) distribution
732	With a tree-based model , we can look at the feature importances which show a relative ranking of the usefulness of features in the model . These represent the sum of the reduction in impurity at nodes that used the variable for splitting , but we do n't have to pay much attention to the absolute value . Instead we 'll focus on relative scores . If we want to view the feature importances , we 'll have to train a model on the whole training set . Cross validation does not return the feature importances .
430	Please use label encoder to encode the new features generated using the above method as the columns coming out of get_new_feature_train function are of type : category . Small snippet on how to use label encoder
1491	Patient 12 - I ca n't see a clear reason for this image to be abnormal . Maybe there are signs of increased vascular markings like Patient 8 ? I 'm not sure . Going through the No Lung Opacity / Not Normal class makes me guess that most of the images in this class are with an unclear abnormality , not something defined . Here is another example
210	Comparison of the all feature importance diagrams
724	Feature Engineering through Aggregations
1146	Hum , that looks like a mask but wrongly shaped . What went wrong Well , this is a quirk of the rle format . I wo n't delve into too much details but just remember that you need to rotate the mask 90 degrees ( counter-clockwise ) . To do so , one can use the transpose operation .
1299	Now that we 've removed all the ` NaN ` values , we can downcast the columns to the lowest precision . First we 'll need to know which columns are integers , though ! The following snippet does just that .
316	Set up the generator
1513	We can conclude , that most of the variables have types of float and integers . Only 5 columns are objects , let 's explore which ones
332	Random Forest is one of the most popular model . Random forests or random decision forests are an ensemble learning method for classification , regression and other tasks , that operate by constructing a multitude of decision trees ( n_estimators= [ 100 , 300 ] ) at training time and outputting the class that is the mode of the classes ( classification ) or mean prediction ( regression ) of the individual trees . Reference [ Wikipedia
362	Genetic program model , main code loop .
844	Below we read in the data and separate into a training set of 10000 observations and a `` testing set '' of 6000 observations . After creating the testing set , we can not do any hyperparameter tuning with it
50	If we treat all the train matrix values as if they belonged to a single row vector , we see a huge amount of varience , far exceeding the similar variance for the target variable . Now let 's plot it to see how diverse the numerical values are .
367	Some stats using jpg exif
680	ResNet50 vs InceptionV3 vs Xception
843	Now we can see if all that time was worth it ! In the code below , we find the most important features and show them in a plot and dataframe .
508	Next I collect the constants . You 'll need to replace the various file name reference constants with a path to your corresponding folder structure .
1430	Hello World ! Have you ever think about how can data science predict our life , or even , health ? This WiDS Datathon definitely inspired me on its power . How can we use patient data to understand inviidual health condition , and predict hospital death Let 's look at the data first
1555	Another visualization that is useful is value counts of the most frequent words across all of the text . In order to accomplish that first of all we need to extract all of our words and count the number of occurances . ( Note that NLTK library underneath deals with that ) but here we are going to do it `` manually '' and the give it as arguments to our go part of the plotting .
221	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
783	The random forest does much better than the simple linear regression . This indicates that the problem is not completely linear , or at least is not linear in terms of the features we have constructed . From here going forward , we 'll use the same random forest model because of the increased performance . Overfitting Given the gap between the training and the validation score , we can see that our model is __overfitting__ to the training data . This is one of the most common problems in machine learning and is usually addressed either by training with more data , or adjusting the hyperparameters of the model . This leads to another recommendation for improvement Potential Improvement 4 : Try searching for better random forest model hyperparameters__ . You may find Scikit-Learn 's ` RandomizedSearchCV ` a useful tool . I 'll provide some starter code for hyperparameter optimization later in the notebook . Next we can make predictions with the random forest for uploading to the competition .
79	Predicting for test data
963	Interaction - dependence plots
455	Prediction and Submission
408	Now let 's 'export ' whole train dataset in a few images . So that you can download images and explore them using your favorite image viewer .
942	IIIa . Bureau balance & Bureau , batch aggregation
716	First , we 'll calculate the Pearson correlation of every variable with the Target .
625	model_train.get_feature_importance ( ) на одном из этапов моделирования ) . Конечное количество ( 31 посление признаки в спике ) было определено нахождением f1 на валидационной выборке ( обучающий датасет делился на train/val ) последовательным отбросом признаков ( начиная с последнего в списке ) .
1521	Evaluate the score with using 4-fold TTA ( test time augmentation ) .
456	Variable Description , Identification , and Correction
48	That 's ... revealing . And it looks like a fairly nice distribution , albeit still fairly asymetrical . Let 's take a look at the statistics of the Log ( 1+target
395	How many different cars in train dataset
816	Applied to Full Dataset
672	And then let 's see how the price ( $ Log ( price ) $ ) varies within the ` parent_category_name ` in the violin plot below . Again , the variation looks relatively high for almost all the parent categories .
1556	Now for the worldcloud , notice that hpl is for example our text from above ( one of the 3 authors ) . It is one-liner to implement it , and mask changes the image .
1524	It is very important to keep the same order of ids as in the sample submission The competition metric relies only on the order of recods ignoring IDs .
571	Loading data
719	One of my favorite plots is the correlation heatmap because it shows a ton of info in one image . For the heatmap , we 'll pick 7 variables and show the correlations between themselves and with the target .
1454	Use machine learning model
818	Then we can make predictions on the test data . The predictions are saved to a csv file that can be submitted to the competition .
1317	I will reduce the number of features using shap , so let 's generate many features ! ! Hope catch some fortune features
678	Let 's now take a look at the relationship between different pair combinations of the particle variables . Again , the colors represent the number of hits . There is no large skew in the distribution of the number of hits over other variables . It looks like the particles are targetted towards the global origin $ ( x , y ) = ( 0,0 ) $ and are evenly distributed around it .
56	So it seems that the vast majority of columns have 95+ percent of zeros in them . Let 's see how would that look on a plot .
1578	RF for feature selection
1246	Store can be the variable giving information on sales But store is including much intrinsic information of type , size , and department
1419	Add active column
1227	Generate level 1 OOF predictions
78	Unfreeze all layers and find best learning rate
222	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
889	These four columns represent different offsets DAYS_CREDIT ` : Number of days before current application at Home Credit client applied for loan at other financial institution . We will call this the application date , ` bureau_credit_application_date ` and make it the ` time_index ` of the entity . DAYS_CREDIT_ENDDATE ` : Number of days of credit remaining at time of client 's application at Home Credit . We will call this the ending date , ` bureau_credit_end_date DAYS_ENDDATE_FACT ` : For closed credits , the number of days before current application at Home Credit that credit at other financial institution ended . We will call this the closing date , ` bureau_credit_close_date ` . DAYS_CREDIT_UPDATE ` : Number of days before current application at Home Credit that the most recent information about the previous credit arrived . We will call this the update date , ` bureau_credit_update_date ` . If we were doing manual feature engineering , we might want to create new columns such as by subtracting ` DAYS_CREDIT_ENDDATE ` from ` DAYS_CREDIT ` to get the planned length of the loan in days , or subtracting ` DAYS_CREDIT_ENDDATE ` from ` DAYS_ENDDATE_FACT ` to find the number of days the client paid off the loan early . However , in this notebook we will not make any features by hand , but rather let featuretools develop useful features for us . To make date columns from the ` timedelta ` , we simply add the offset to the start date .
707	The final redundant column is ` area2 ` . This means the house is in a rural zone , but it 's redundant because we have a column indicating if the house is in a urban zone . Therefore , we can drop this column .
1275	Next , let 's generate the features coming from the installments data . To do so We compute first the minimum of installment payments in the previous applications table . The rational is to catch the loans where one payment has been missed or was very low . Then we merge the aggregates of all minimum installment payments across all previous loans .
893	To use interesting values , we assign them to the variable and then specify the ` where_primitives ` in the ` dfs ` call .
1047	Will need those folders later for storing our jpegs .
1124	At first , I made Europe future . Soon , I 'll increase it .
1525	Lib and Load data
1318	Ratio feature can have infinite values . So Let them be filled with
521	Interpreting ROC Plot
1192	Target & ID Loading
1115	Fast data loading
3	Let 's see what files we have in the input directory
1064	Reducing Image Size
1102	Leak Data loading and concat
403	Find the indices for where the earthquakes occur , then plotting may be performed in the region around failure .
745	For each fold , the ` 1 , 2 , 3 , 4 ` columns represent the probability for each ` Target ` . The ` Target ` is the maximum of these with the ` confidence ` the probability . We have the predictions for all 5 folds , so we can plot the confidence in each ` Target ` for the different folds .
883	Now we can make a heatmap of the correlations . I enjoy heatmaps and thankfully , they are not very difficult to make in ` seaborn ` .
143	If we want [ reproducible ] ( results , we should fix seeds .
1350	Check the dataset
1563	LDA Modelling . Finally we are ready to fit the model . There are many parameters that need to be tuned for LDA . As always in order to do it properly fit it we need to have some deeper understanding . Since this tutorial is concerned with implementation , we are going to state hwo we got to the chosen parameters but the reader should explore the Algorithm a bit deeper on tis own . Implementation of LDA can be done with sklearn or gensim . We are going to sitck with skicit learn library . Great source is [ video lecture at Princeton , also offical documentation [ Also to understand what hyper-parameters should be choosen we can consult some of the novel approaches [ or use KMeans + Latent Semantic Analysis Scheme to find out n_components ( number of topics ) whereby the number of Kmeans clusters and number of LSA dimensions were iterated through and the best silhouette mean score . Let us take the `` cooked '' values .
1535	This first function gets the distance of all the cities to the current city . If we decide to penalize , we multiply the distance of non-prime cities by $ 1 .
615	Imputing missing values
1038	Public and private sets have different sequence lengths , so we will preprocess them separately and load models of different tensor shapes . This is possible because RNN models can accept sequences of varying lengths as inputs .
633	Read the csv files from kaggle .
836	Aggregate Installments Data
668	now let 's explore a little on the labels
1326	Feature selection using shap
605	We should steadily add correct samples to our submission286 in order to get 0.287 . Sounds easy , does n't it We will steadily increase number of correct answers from 5 to infinity with step 5 and give 30 tries for each addition because our correct samples are selected in a random order . We will stop when we reach 0.287 .
260	Stochastic gradient descent ( often abbreviated SGD ) is an iterative method for optimizing an objective function with suitable smoothness properties ( e.g . differentiable or subdifferentiable ) . It can be regarded as a stochastic approximation of gradient descent optimization , since it replaces the actual gradient ( calculated from the entire data set ) by an estimate thereof ( calculated from a randomly selected subset of the data ) . Especially in big data applications this reduces the computational burden , achieving faster iterations in trade for a slightly lower convergence rate . Reference [ Wikipedia
1319	combination using three features
861	First we will test the cross validation score using the best model hyperparameter values from random search . This can give us an idea of the generalization error on the test set . We will delete the number of estimators found from the smaller dataset and use early stopping to find the best number of decision trees to train .
1355	IsSxsPassiveMode - NA
356	FS with SelectFromModel and RandomForestRegressor
616	Model Functions
831	We can go through a quick example to show how PCA is implemented . Without going through too many details , PCA finds a new set of axis ( the principal components ) that maximize the amount of variance captured in the data . The original data is then projected down onto these principal components . The idea is that we can use fewer principal components than the original number of features while still capturing most of the variance . PCA is implemented in Scikit-Learn in the same way as preprocessing methods . We can either select the number of new components , or the fraction of variance we want explained in the data . If we pass in no argument , the number of principal components will be the same as the number of original features . We can then use the ` variance_explained_ratio_ ` to determine the number of components needed for different threshold of variance retained .
0	The test set is almost 2.5 times larger than the train set . It also has 5 features with missing data . Now , let 's take a look at the target . We want to see the distribution of target values in the train set .
622	Feature Agglomeration Results
587	Setting fitting domain ( given time for each observation ) and the observations ( observed population at given time
1257	Get the actual validation / test datasets from TF Records
1173	For converting them into features we train a word2vec model which converts each word into it 's corresponding vector . Word2Vec models are extremely efficient in finding temporal relations as they themselves are shallow neural networks . We train the word2vec model on the entire corpus so that it learns the similarities in the text and can give us vectors for all the words , not just those that occur in training dataset . You can perform the following operations and learn about word2vec model .
659	Take a deeper look on different target samples , I will plot separate heatmap for target values 0 and 1 .
952	Prepare for training
1278	B . Load libraries and data files , file structure and content
905	Function to Handle Categorical Variables
1296	Loss
1046	Load Model into TPU
969	load core DFs ( train and test
347	Create the submission csv file
173	Back to the click time We 've currently analyze the amount of clicks by IP adress but not the click time . Maybe we could find some interesting information here .
581	Spain since first recorded case
1055	We 'll load in the data and take a look at its shape to get an idea of how many features and how many samples we have .
686	Show Original Image
1302	We added some columns in our training set and replaced missing values with the medians . We need to add those same columns , and also add the median from the training set for those missing values ( the same ones ) .
1260	Get metrics for validation dataset
635	Transpose the dataframes
1366	OsBuild - Build of the current operating system
1411	Onehot encoding for categorical data
301	Divide features into groups
1155	This Notebook uses such great kernels Logistic Regression on Tournament seeds March Madness Data - First Look EDA NCAAM20no-leak starter Basic Starter Kernel - NCAA Women 's Dataset Before we get started I want to thank Kaggle and each member here for sharing such huge concepts , without their efforts i ca n't write this notebook with such ease So , without wasting any time , let 's start with importing some important python modules that I 'll be using .
94	Now let 's look at the remaining data in more detail . Text is too long and detailed and technical , so I 've decided to summarize it using gensim 's TextRank algorithm . Still did n't understand anything
1496	The evaluation method
1582	Let 's load ` sample_data.json ` , since it contains information about our training data .
149	Prepare Testing Data
932	Initialize and load data - call SaltParser functions
848	The ` boosting_type ` and ` is_unbalance ` domains are pretty simple because these are categorical variables . For the hyperparameters that must be integers ( ` num_leaves ` , ` min_child_samples ` ) , we use ` range ( start , stop , [ step ] ) ` which returns a range of numbers from start to stop spaced by step ( or 1 if not specified ) . ` range ` always returns integers , which means that if we want evenly spaced values that can be fractions , we need to use ` np.linspace ( start , stop , [ num ] ) ` . This works the same way except the third argument is the number of values ( by default 100 ) . Finally , ` np.logspace ( start , stop , [ num = 100 ] , [ base = 10.0 ] ) ` returns values evenly spaced on a logarithmic scale . According to the [ the docs ] ( `` In linear space , the sequence starts at $ base^ { start } $ ( base to the power of start ) and ends with $ base ^ { stop } $ `` This is useful for values that differ over several orders of magnitude such as the learning rate .
398	Designed and run in a Python 3 Anaconda environment on a Windows 10 computer .
786	There appears to be a minor increase in prices over time which might be expected taking into account inflation . Let 's look at the average fare amount by the hour of day .
1012	Pad and resize all the images
1128	This incident has taken place in 03:30 in the night in a block . As we saw in the Partial Dependencies graphs before , BURGLARY has a higher probability and burglaries happen by definition in blocks . Let ’ s see if our model aligns with our intuition .
499	Analysis based Averages values
302	My experience ( for example [ Titanic ( 0.83253 ) - Comparison 20 popular models ] ( has shown that simulation using LGBMClassifier is better if you set both parameter num_leaves and parameter max_depth
11	Detect and Correct Outliers
218	Commit now
870	The feature importances returned by a tree-based model [ represent the reduction in impurity ] ( from including the feature in the model . While the absolute value of the importances can be difficult to interpret , looking at the relative value of the importances allows us to compare the relevance of features . Although we want to be careful about placing too much value on the feature importances , they can be a useful method for dimensionality reduction and understanding the model .
448	Square feet size is positively Skewed .
360	Set up the folds for cross validation .
951	join new_merchant with train/test by card_id
1273	Check oversampled dataset
510	Digging into the Images When we get to running a pipeline , we will want to pull the scans out one at a time so that they can be processed . So here 's a function to return the nth image . In the unit test , I added a histogram of the values in the image .
248	TABLE OF CONTENTS Exploratory data analysis ( EDA ) ] ( section COVID-19 global tendency excluding China ] ( section COVID-19 tendency in China ] ( section Italy , Spain , UK and Singapore ] ( section SIR model ] ( section Implementing the SIR model ] ( section Fit SIR parameters to real data ] ( section Data enrichment ] ( section Join data , filter dates and clean missings ] ( section Compute lags and trends ] ( section Add country details ] ( section Predictions with machine learning ] ( section Ridge Regression for one country ] ( section Ridge Regression for all countries ( method 1 ) ] ( section Ridge Regression for all countries ( method 2 ) ] ( section Ridge regression with lags ] ( section
934	Predict validation and test set masks
273	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
1042	Save model and best hyperparams
649	Check if valid data looks all right
906	The above dataframes have the calculations done on each _loan_ . Now we need to aggregate these for each _client_ . We can do this by merging the dataframes together first and then since all the variables are numeric , we just need to aggregate the statistics again , this time grouping by the ` SK_ID_CURR ` .
1033	Step 3 : Running the session
873	Align Train and Test Sets
913	We can remove these columns from both the training and the testing datasets . We will have to compare performance after removing these variables with performance keeping these variables ( the raw csv files we saved earlier ) .
325	public LB score was 0.048 . I found that this score could be improved to 0.046 by averaging the predictions of this cnn model with the predictions generated by another logistic regression model that I created . The following kernel explains how to easily do this Using a weighted average ( 0.6 x cnn_model + 0.4 x logistic_regression_model ) further improved the public LB score to 0.045 .
972	Source pydicom is a pure Python package for working with DICOM files . It lets you read , modify and write DICOM data in an easy `` pythonic '' way . In the example below , we import the first dcm file in the training set , and then display its contents
921	Train and validate
530	Lets Read In Data Files
506	Load some target 1 ( fault ) signals and visualize
567	IMPORTANT : While the time series appears continuous , the data is from discrete batches of 50 seconds long 10 kHz samples ( 500,000 rows per batch ) . In other words , the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000 , and thus discontinuous between 50.0000 and 50.0001 .
992	Finally , gather results , stop dummy x-server and display image
489	Keras Tokenizer API
281	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
450	Air Temperature
1162	targets in labels.csv
730	Because we are going to be comparing different models , we want to scale the features ( limit the range of each column to between 0 and 1 ) . For many ensemble models this is not necessary , but when we use models that depend on a distance metric , such as KNearest Neighbors or the Support Vector Machine , feature scaling is an absolute necessity . When comparing different models , it 's always safest to scale the features . We also impute the missing values with the median of the feature . For imputing missing values and scaling the features in one step , we can make a pipeline . This will be fit on the training data and used to transform the training and testing data .
1468	At the scale of stores
240	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
278	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
343	Initialize the generators
1447	ip , app , device , os and channel are actually categorical variables encoded as integers . Set them as categories for analysis .
914	To actually test the performance of these new datasets , we will try using them for machine learning ! Here we will use a function I developed in another notebook to compare the features ( the raw version with the highly correlated variables removed ) . We can run this kind of like an experiment , and the control will be the performance of just the ` application ` data in this function when submitted to the competition . I 've already recorded that performance , so we can list out our control and our two test conditions For all datasets , use the model shown below ( with the exact hyperparameters ) . control : only the data in the ` application ` files . test one : the data in the ` application ` files with all of the data recorded from the ` bureau ` and ` bureau_balance ` files test two : the data in the ` application ` files with all of the data recorded from the ` bureau ` and ` bureau_balance ` files with highly correlated variables removed .
553	Loading and preprocessing data
82	Now , let 's see how different parameters influence on the outcome .
1390	Census_IsFlightsDisabled - Indicates if the machine is participating in flighting .
1340	House type
1113	A one idea how we can use LV usefull is blending . We probably can find best blending method without LB probing and it 's means we can save our submission .
1089	Hope some one could improve the speed of the kernel , and also share more feature engineering idea . Glad to hear advice and bug report
710	The next variable will be a ` warning ` about the quality of the house . It will be a negative value , with -1 point each for no toilet , electricity , floor , water service , and ceiling .
156	You can state below link to your copy of this MMDetection repo . Add .git in the end .
723	We can also take our new variable , ` inst ` , and divide this by the age . The final variable we 'll name ` tech ` : this represents the combination of tablet and mobile phones .
1208	feature_3 has 1 when feautre_1 high than
1483	What are Lung Opacities
424	Confusion Matrix for Test Data Predictions
417	Let 's load the features for the training and test datasets ( s2 joblib
1383	Census_InternalPrimaryDiagonalDisplaySizeInInches - Retrieves the physical diagonal length in inches of the primary display
555	Real Do n't forget about _Scalling
477	The next step is to build and re-install lightGBM with GPU support .
425	Converting the Input images to plot using plt
63	It seems that the spike around day 78 , corresponds to relatively more fraudulent cases I would not recommend to use this Feature for training , as it becomes a proxy of the Transaction day . Also , the original D1 feature is cliped , and many groups would not appear in the testing set , overfitting risk is increased ( especially if you are using Kfolds as the future can easily predicts the past with this variable ) . But i would use it as a grouping feature ( in our example : day 78 can help to identify a specific card ) . For instance , two cards with the same numbers ( card1 to card6 ) that correspond to different cards can be distinguished thanks to this variable .
211	Thanks to Automatic FE The main code for basic FE
852	Normally , in grid search , we do not limit the number of evaluations . The number of evaluations is set by the total combinations in the hyperparameter grid ( or the number of years we are willing to wait ! ) . So the lines if i > MAX_EVALS break would not be used in actual grid search . Here we will run grid search for 5 iterations just as an example . The results returned will show us the validation score ( ROC AUC ) , the hyperparameters , and the iteration sorted by best performing combination of hyperparameter values .
1381	Census_HasOpticalDiskDrive - True indicates that the machine has an optical disk drive ( CD/DVD
1053	Create test generator
926	Import libraries
1262	This kernel demonstrates how to use Hugging Face 's [ transformers ] ( package , more precisely , theier ` Tensorflow 2 ` models , for this competition . Motivations Give my first contribution on Kaggle . To learn and work on some things new . Provide an easy to read/use training kernel for this competition . The most important ) I want to have your feedbacks , and solutions to some of my questions , so we can improve this kernel Features I tried my efforts to make the following things work batch accumulation . Adam optimizer with weight decay and make it work with learning rate decay . ( I take [ AdamW ] ( class from ` Tensorflow Addons ` with some modification . See ` adamw_optimizer.py Custom learning rate schedule as close as to the original google Bert code . Easy to choose different model configurations . Important Notes I did n't use the pre-trained model from starter kernel . I did n't check ( yet ) the LB scores for the checkpoints I provided . No code for inference/submission in this kernel . ( I will work on a separate kernel . The flag ` model_name ` is default to ` distilbert-base-uncased-distilled-squad ` . The ` batch_size ` and ` batch_accumulation_size ` is however defaut to ` 10 ` . It 's your own responsibility to make sure there is no OOM when you change these parameters . In the training loop , if the training is on ` Kaggle ` , then it stops after ` 8 hours 45 minutes ` so we can still have a saved checkpoint . This kerenl is not optimalized for training . ( See next section . The checkpoints provided in [ nq-competition ] ( ( as the value of ` NQ_DIR ` below ) are trained by myself without any effort on choosing the hyperparameters . After you have your new trained checkpoints ( or if you create your own .tfrecord file ) , when you want to use them for resuming training , you have to set your own value for ` MY_OWN_NQ_DIR ` , which should contain your own checkpoints/.tfrecord file . This kernel is not written for using TPU . Checkpoints , Batch size and Training time In the following table , the model configurations are distilled bert ` : distilbert-base-uncased-distilled-squad bert base ` : bert-base-uncased bert large ` : bert-large-uncased-whole-word-masking-finetuned-squad Here is the summary of the checkpoints I provided ( no ` bert large ` , it will eat up my GUP quota . Probably I will provide one after training on GCP ) . model | batch size | accumulation size | effective batch size | num . updates | training time | epochs | time / epoch distilled bert | 25 | 4 | 100 | 9893 | 30823 s | 2 | 4.28 h bert base | 10 | 10 | 100 | 4500 | 32226 s | 0.909 | 9.84 h bert large | 2 | 50 | 100 | n/a | n/a | n/a | 37.78 h Remark For ` distilled bert ` , I did n't test with ` train_batch_size=10 ` and ` batch_accumulation_size=10 ` for ` epochs=2 ` . It 's possible that this will take more time than the time indicated above . A screenshot of training progress In training loop , I print the losses and accuracies every ` 100 ` effective batch , here ` 1 effective batch = batch_size accumulation_size ` . It includes those for ` start_pos ( loss_S , acc_S ) ` , ` end_pos ( loss_E , acc_E ) ` and ` answer_type ( loss_A , acc_A ) ` . The following screenshot is however taken from a training where the result is printed every effective batch . training.png ] ( attachment : training.png Remark In the trainin progress , ` batch n ` means ` effective batch n ` . TODO - Some contribution wanted This kernel is only for training . I will add the prediction/submission parts as soon as possible ( In a separate kernel . The batch accumulation code seems work fine , but not memory/speed efficient . In particular , I ca n't get rid of the following warning . UserWarning : Converting sparse IndexedSlices to a dense Tensor of unknown shape . This may consume a large amount of memory . Maybe TPU support , not very sure . However , see the 3rd link in the ` Reference ` section . Things tried but not working - Solution wanted I tried to use mixed precision for training , but I could n't make it work . Using [ tf.keras.mixed_precision.experimental.Policy ] ( for mixed precision training causes error when loading pretrained models . ( ` input 1 ( zero-based ) was expected to be a half tensor but is a float tensor [ Op : AddV2 ] name : tf_bert_model_1/bert/embeddings/add/ ` . Using [ tf.config.optimizer.set_experimental_options ] ( does n't make tranining faster . Probably this is designed only for compiled model Disclamation I am not a part of Hugging Face . I choose to use ` transformers ` package because I found it 's easier to use and to extend , so I can focus on other parts of this notebook . I take no responsibility for any ( potential ) error in this kernel and in the dataset ` nq-competition ` . ( I would appreciate any feedback . I take no credit of any file ( with/without my own modifications ) containing in ` nq-compeittion ` . References Transformer model for language understanding ( colab tutorial ) ] ( - Official Tensorflow tutorial on transformer model Transformer 與 TensorFlow 2 英翻中 ] ( - An annotated chinese tutorial based on the above one Sequence classification with Transformers & Strategy ] ( - Hugging Face 's tutorial on how to use their TF 2.0 models on TPU Hugging Face : State-of-the-Art Natural Language Processing in ten lines of TensorFlow 2.0 ] ( - Hugging Face 's post on Tensorflow blog
598	Almost the same imbalance we have in the training set . Now let 's define Normalized Gini Score . [ Sklearn ] ( package has [ roc_auc_score ] ( and we are going to compute Normalized Gini with AUC using the next formula begin { align Gini & = 2 AUC end { align Let 's see how our perfect submission is evaluated by Gini . Hold your breath
1337	Suite type
712	The final household feature we can make for now is a ` bonus ` where a family gets a point for having a refrigerator , computer , tablet , or television .
20	Let 's now look at the distributions of various `` features
1460	Apply predicsion results onto df_test for positive & negative samples .
901	We need to create new names for each of these columns . The following code makes new names by appending the stat to the name . Here we have to deal with the fact that the dataframe has a multi-level index . I find these confusing and hard to work with , so I try to reduce to a single level index as quickly as possible .
1147	Awesome , it worked ! Alright , now that we have both images and masks saved as files , let 's load a bunch of them . Before we dive deep , here is the plan to get the data pipeline contruct a DataFrame that contains only images having at least one mask ( this will be called with_masks_df Load images using the ` from_df ` method Extract labels using the ` label_from_func ` method : this takes a function that reads one image input path and outputs a mask path . Since we have multiple classes ( 0 for background and 1 through 4 for the different defect types ) , these should be passed as well . Transform the images and masks . Notice that this step is optional but I am adding it to get resized images ( since I pass the ` size ` variable Finally , use the ` databunch ` method to create a [ DataBunch ] ( Again , this is n't necessary but will comes handy in the next notebook so I want you to get used to the concept .
589	Calculating the day when the number of infected individuals is max
1111	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so it is better to consider time-splitting . However this notebook is for simple tutorial , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
1243	Imgur
1422	Predict World ( Without China Data Because china nearly cleared COVID-19 , and data is ahead of the world , so maybe exclude china data sounds resonable .
965	Get important features according to SHAP
1255	Hugging Face pretrained Bert model names
1179	Number of Patients and Images in Test Images Folder
771	Another plot we can make is the passenger count distribution colored by the fare bin .
399	These are the needed library imports for problem setup . Many of these libraries request a citation when used in an academic paper . Numpy is utilized to provide many numerical functions needed in the EDA ( van der Walt , Colbert & Varoquaux , 2011 ) . Pandas is very helpful for its ability to support data manipulation ( McKinney , 2010 ) . SciPy is utilized to provide signal processing functions ( Jones E. , et al , 2001 ) . Matplotlib is used for plotting ( Hunter , 2007 ) . The Jupyter environment in which this code is presented and was run is a descendant of the IPython environment originated by Pérez & Granger ( 2007 ) .
1032	Step 1 : Building a TF Graph
1031	Let 's see what it looks like
855	We can also evaluate the best random search model on the `` test '' data .
1584	Finally , we will augment the image_df with some information about its host and camera
551	Gaussian Noise on Target
1490	Weiteng007 asked an interesting question - Can a lung opacity occur if the lung is removed ? . The answer is yes and no . An opacity can occur where the lung was once , but it can not be a lung opacity . This white lung phenomena in chest radiographs is termed `` hemithorax white-out '' or `` hemithorax opacity '' . Seeing a `` white lung '' on a chest radiograph leads to the question - if we see only an opacity where the lung is supposed to be , what happened to lung ? . These are some possible answers The lung was removed in a surgery called pneumonectomy . The lung is filled with fluid from pneumonia and what we see is a [ pneumonia associated lung opacity ] ( A-Clear-and-Detailed-Definition-of-Pneumonia-Associated-Lung-Opacities ) . The lung is there but it is surrounded by fluids inside the chest cavity ( termed [ pleural effusion ] ( Pleural-effusion ) ) . It 's hard to tell just by the chest radiograph what is the cause of the `` white lung '' . Since this competition is about penumonia , and these patients have a `` Target '' value of 1 , the cause for their big opacity is most probably pneumonia . Meaning - they have a pneumonia associated lung opacity over their entire lung . You can see more examples of `` white lungs '' in [ this radiopedia artice Unclear Abnormality Back to top ] ( Table-of-Contents
752	As a final step , we can look at one decision tree in the random forest . First we 'll limited the max depth for visability , and then we 'll expand the tree all the way . The first step is simply to train a random forest and extract one tree ( we could also train a single decision tree ) .
559	and keep only those that contain ships . Keep in mind that image files can be repeated many times in the csv file . So a unique operator will give us the unique filenames that contain ships .
819	Bayesian Optimization on the Full Dataset
617	Random Forest Regressor
1560	Turning words into numbers - Vectorization Now there are is a number of techniques that can be used to make this transition . WE are going to be talking about bag of words approach . ( Name comes from the fact that we just count appearances and nothing else , i.e . just drawing words from the bag and counting ) Steps are as follows Count the words that are to be found in the entire text , build our dictionary in a sense . Record the occurance of that specific word . For every element of the list we will be making a vector that checks the occurance Best way is to look at an Example : ( NOTE that CountVectorizer is from sklearn library
225	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
1049	Pad and resize all the images
1450	Conversions by Device
279	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
446	Meter Readings over time And Primary Use
1338	Occupation type
29	Next , we 'll do some unviariate estimations of the target . We 'll try to find the features that best predict the target by themselves .
991	Add the actors to the renderer , set the background and size
344	Plot the Loss Curves
684	All Zero Features
695	Let 's look at the distribution of unique values in the integer columns . For each column , we 'll count the number of unique values and show the result in a bar plot .
1374	Census_OEMNameIdentifier - NA
414	Idea is to use clustering on images of one type to group data
1457	Ensure determinism
169	Does bots download the app
860	Testing Results on Full Data
478	Loading the data
941	Loading the data
1443	In my previous notebook ( we found that the data is organized by click time . Therefore if our random sampling went according to plan , the resulting set should roughly span the full time period and mimick the click pattern . We see from above that first and last click span the 4 day period . Let 's try a chart to see if the pattern looks consistent
637	Let 's now model the temporal evolution of daily new cases for all countries . The approach is the following we first create an order of countries where the epidemic appears ( defined as when 10 days are discovered for the first country ( China ) , a simple auto-regressive model is used for other countries , we model the evolution of daily new cases by also taking into account the impact of total cases in countries where the epidemic appears first the impact of the epidemic of other countries is weighted by their distance between countries and the yearly number of flight passengers to include the `` interaction '' between two countries other variables , like lockdown starting day , will be used . For now , a simple linear regression model is used to model 1 ) directly the total number of confirmed cases , 2 ) the number daily new cases , i.e . confirmed.diff ( ) since this variable should be more stationary and therefore more easily modelled , 3 ) the number of daily new cases averaged within a year ( with rolling ( 7 ) .mean ( ) ) to smooth the data since some data are not updated every day .
1150	Let 's see if our observation transfers well to the test dataset .
27	Now let 's look at the data
1514	Seaborn is a great library for visualization . You can choose many palettes , which makes the graphs visually nice . For instance , some of them .
1343	Flag document
606	Import Packages and Functions
1362	CityIdentifier - ID for the city the machine is located in
1126	SGD scored the best initial result , but after a lengthy hyperparameter tuning , it was not able to pass a 2.54503 threshold . Finally , from the algorithms that scored under 3.0 , we decided to work with LightGBM due to its efficiency and versatility in the hyperparameters tuning . LightGBM is a decision tree boosting algorithm uses histogram-based algorithms which bucket continuous feature ( attribute ) values into discrete bins . This technique speeds up training and reduces memory usage . In layman terms the algorithm works like this Fit a decision tree to the data Evaluate the model Increase the weight to the incorrect samples . Choose the leaf with max delta loss to grow . Grow the tree . Go to step 2 ! [ lightgbm Benchmark There are two types of benchmarks we need to set . The first will be a naive prediction . This prediction will be a baseline score to compare with our model ’ s score to evaluate if we have any significant progress . In a Multiclass Classification , the best way to calculate the baseline is by assuming that the probability of each category equals its average frequency in the train set . The frequency can be calculated easily by dividing the sum of incidents of each category by the number of rows of the training set .
1537	NOTE Even tough it is automatic , we can incorporate some manual features . IF we know some domain specific information .
1171	Trying something else What if we take only the words that are lower and convert capitalised words to lower , how many tokens to we get then . This gives us ~1000 less tokens compared to when we were also using caps , which is not a difference of lot but will still help us .
658	Let 's plot now the train data ( all the data ) using a heatmap
1344	DAYS_BIRTH is some high correlation with target . With dividing 365 ( year ) and applying abs ( ) , we can see DAYS_BIRTH in the unit of year ( AGE ) .
1352	The difference is quite small . Do you think that some malwares recognize and select machine based on the type of battery Battery is very important part for life of machine . I think that malware will focus on other hardware and software parts of machine . remove this .
887	There are also two ordinal variables in the ` app ` data : the rating of the region with and without the city .
472	Bayesian Optimisation
1452	Preparing the data
1590	TF_IDF Score . In short the most important words ( not too common not to rare ) That could make a difference
266	ExtraTreesRegressor implements a meta estimator that fits a number of randomized decision trees ( a.k.a . extra-trees ) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting . The default values for the parameters controlling the size of the trees ( e.g . max_depth , min_samples_leaf , etc . ) lead to fully grown and unpruned trees which can potentially be very large on some data sets . To reduce memory consumption , the complexity and size of the trees should be controlled by setting those parameter values . Reference [ sklearn documentation In extremely randomized trees , randomness goes one step further in the way splits are computed . As in random forests , a random subset of candidate features is used , but instead of looking for the most discriminative thresholds , thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule . This usually allows to reduce the variance of the model a bit more , at the expense of a slightly greater increase in bias . Reference [ sklearn documentation
1282	E. Facebook prophet library
335	Tikhonov Regularization , colloquially known as Ridge Regression , is the most commonly used regression algorithm to approximate an answer for an equation with no unique solution . This type of problem is very common in machine learning tasks , where the `` best '' solution must be chosen using limited data . If a unique solution exists , algorithm will return the optimal value . However , if multiple solutions exist , it may choose any of them . Reference [ Brilliant.org
216	FS with SelectFromModel and LinearSVR
465	Data Section 2 - Team Box Scores
1495	Let 's first write an utilitary function to describe a program as a human readable string .
345	Make a Prediction
779	According to the naive baseline , our machine learning solution is effective ! We are able to reduce the percentage error by about half and generate much better predictions than using no machine learning . This should give us confidence we are on the right track . Make a submission In order to make a submission to Kaggle , we have to make predictions on the test data . Below we make the predictions and save them to a csv file in the format specified by the competition
900	When we 're done , we probably want to save the results to a csv . We want to be careful because the index of the dataframe is the identifying column , so we should keep the index . We also should align the training and testing dataframes to make sure they have the same columns .
1369	AutoSampleOptIn - This is the SubmitSamplesConsent value passed in from the service , available on CAMP
284	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
770	It looks like there are 51,000 rides where the absolute latitude and longitude does not change ! That seems a little strange . This might be a point worth following up Let 's remake the plot above colored by the fare bin .
851	Grid Search Implementation
1203	Using all features for model training
258	Support Vector Machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis . Given a set of training samples , each marked as belonging to one or the other of two categories , an SVM training algorithm builds a model that assigns new test samples to one category or the other , making it a non-probabilistic binary linear classifier . Reference [ Wikipedia
854	Random search is surprisingly efficient compared to grid search . Although grid search will find the optimal value of hyperparameters ( assuming they are in your grid ) eventually , random search will usually find a `` close-enough '' value in far fewer iterations . [ This great paper explains why this is so ] ( grid search spends too much time evaluating unpromising regions of the hyperparameter search space because it has to evaluate every single combination in the grid . Random search in contrast , does a better job of exploring the search space and therefore can usually find a good combination of hyperparameters in far fewer iterations . As [ this article ] ( lays out , random search should probably be the first hyperparameter optimization method tried because of its effectiveness . Even though it 's an _uninformed_ method ( meaning it does not rely on past evaluation results ) , random search can still usually find better values than the default and is simple to run . Random search can also be thought of as an algorithm : randomly select the next set of hyperparameters from the grid ! We can build a dictionary of hyperparameters by selecting one random value for each hyperparameter as follows ( again accounting for subsampling
83	Interesting notice : dogs tend to be returned to owner more often than cats . And cats are transferred more often than dogs .
1065	Save results as CSV files
1527	winPlacePerc is the target we are going to predict on testing set . distribution on training set is not kind of a 'normal distribution ' but the opposite -- values close to 0 and 1 are apparently more than the middle values .
767	Below is an example of the ecdf . This plot is good for viewing outliers and also the percentiles of a distribution .
1492	forked from The difference with the original kernel is the switched data . The data on Kaggle is not updated with the recent fixes to some tasks , the up-to-date version is maintaned on the ARC github page This kernel uses the github data , as downloaded on 2020-04-20 , visit the corresponding dataset
53	Not really - the plot looks nicer , but the overall shape is pretty much the same . OK , let 's take a look at the distribution of non-zero values .
358	Load data files .
1181	Image Loading & Pre-processing
665	This produces output as a pandas dataframe.Alternatively we can use OneHotEncoder ( ) method available in sklearn to convert out data to on-hot encoded data . But this method produces a sparse metrix.The advantage of this methos is that is uses very less memory/cpu resourses , if one try to encode all of these features with sklearn 's OneHotEncoder ( ) , it pottentially cause a crash session
70	Some runs
1290	Set Model for prediction
667	Predict test set and make submission
41	Let 's now load the datasets .
772	For the test data , we need to save the ` key ` column for making submissions .
31	So PCA does n't seem to help much here . Let 's take a look at clustering . We 'll try to fit the KMeans clustering on the entire dataset , and try to see what the optimal number of clusters is .
253	Germany
1570	In this first part we will choose the Time Series to work in the others parts . The idea is to find a Time Serie who could be interesting to work with . So in the data we can find 145K Time Series . We will Find a good Time Series to introduce four approaches ! So the first step is to import few libraries and the data . The four approaches are Basic Approach / ML Approach / GAM Approach / ARIMA Approach .
1562	Applying it on text
1297	Predictions class distribution
636	Join all dataframes .
1005	Load and freeze DenseNet
244	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
129	I 'll select only the columns that we need to reduce some memory usage
1159	Next , we will make prediction with our LR Model
685	We know from the competition description that the target feature is a transaction value . Therefore , it 's distribution should not be much different from the rest of the features , asuming they represent bank transactions as well . The max feature transaction showed earlier may make us question that assumption though . Here is what the target feature distribution looks like .
1274	The feature selection has been done using the most important features resulting of Will Koehrsen [ automated features generation ] ( Thanks to him for his awesome notebook and introduction to [ Featuretools ] ( and [ Deep Feature Synthesis ] ( method . First , let 's aggregate the features coming from the credit bureau file in the main loans table .
1238	Well , for training score , we manage to arravie at 0.28443 . We can now try to apply the same weight distribution to generate our submission .
1321	I want to mix toilet and rubbish disposal features - > other_infra features
1589	MICE MICE has become an industry standard way of dealing with null values while preprocessing data . It is argued that by simply using fill values such as the mean or mode we are throwing information away that is present in other variables that might give insight into what the null values might be . With that thought in mind , we predict the null values from the other features present in the data . Thus preserving as much information as possible . If the data is not missing at random ( MAR ) then this method is inappropriate .
1141	Everything is set , time to create the EfficientDet model using the code snippet presented above ( the ` get_train_efficientdet ` function ) .
1387	Census_OSInstallLanguageIdentifier - NA
785	Explore Time Variables
377	Bootstrap aggregating , also called Bagging , is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression . It also reduces variance and helps to avoid overfitting . Although it is usually applied to decision tree methods , it can be used with any type of method . Bagging is a special case of the model averaging approach . Bagging leads to `` improvements for unstable procedures '' , which include , for example , artificial neural networks , classification and regression trees , and subset selection in linear regression . On the other hand , it can mildly degrade the performance of stable methods such as K-nearest neighbors . Reference [ Wikipedia
171	Unsurprised , most of IPs have a low ratio ( 50 % lower than 0.0025 and 75 % lower than 0.2 ) . I guess it would be more interesting to check by category of clicker .
620	Linear Model ( Lasso
621	Ridge Regression
967	Based on the above curves , it seems that South Korea 's growth rate has reached an inflection point . The sigmoid function does now fit without error , so it is included in the next section . Logistic Growth Curves China and the subset of China outside Hubei now have resonable sigmoid growth curves . Here are their plots .
1543	Why 150 000 ? That is the size of one test example that we ought to predict . ANd why negative bvalues of time to failure ? In order to `` approach '' the zero ...
884	That 's a lot of plot for not very much code ! We can see that the number of estimators and the learning rate have the greatest magnitude correlation ( ignoring subsample which is influenced by the boosting type ) .
796	The best model from random search exhibits less overfitting , but also does not do as well on the validation data . There are probably further benefits from more hyperparameter tuning using additional data .
838	Aggregate Cash previous loans
1252	Perform label encoding on Sexo feature , this is for producing the countplot later on
1559	Stemming or Lemming Next pre-processing step . As I mentioned we want to reduce words with the same conotation to the same root . Lemming is not as crude as Stemming hence giving us better results .
1503	SAVE DATASET TO DISK
1520	According to the results , the main feature xgboost extracts is mean education . Then , we observe age , years of education of male head of household squared , overcrowding .
26	Whoa , that 's a pretty significant AUC ! At AUC of 0.85 there is a very significant difference between the train and test sets , and a very very good chance of a major shakeup ... Let 's look now at the top 20 `` adversarial '' features .
319	Create a new column called file_name
981	Top - down animation
693	We 'll use a familiar stack of data science libraries : ` Pandas ` , ` numpy ` , ` matplotlib ` , ` seaborn ` , and eventually ` sklearn ` for modeling .
384	Filter design helper functions . These were added to allow for obtaining statistics on the signal in a bandwidth limited manner . Butterworth 4 pole IIR filters are utilized to obtain the signal split into frequency bands . EDA showed that most , if not all , of the signal above the 20,000 frequency line was likely to be noise , so the frequency bands will concentrate on the region below that . Note that the signal is 150k lines long , hence by the Nyquist criteria there are 75k valid frequency lines before aliasing .
406	stage : 100 images , blur + flip
1167	Get Model into TPU
203	As a final preprocessing step , it is advisory to zero center your data so that your mean value is 0 . To do this you simply subtract the mean pixel value from all pixels . To determine this mean you simply average all images in the whole dataset . If that sounds like a lot of work , we found this to be around 0.25 in the LUNA16 competition . Warning : Do not zero center with the mean per image ( like is done in some kernels on here ) . The CT scanners are calibrated to return accurate HU measurements . There is no such thing as an image with lower contrast or brightness like in normal pictures .
77	Defining FastAI 's Learner
937	Make train/test ready to train model
1178	Number of Patients and Images in Training Images Folder
850	The results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function . When we get to Bayesian Optimization , the model actually _uses the past results to decide on the next hyperparmeters_ to evaluate . Random and grid search are _uninformed_ methods that do not use the past history , but we still need the history so we can find out which hyperparameters worked the best A dataframe is a useful data structure to hold the results .
1431	Age distribution of male and female patients .
1413	As you can see , classes are 5005 but data contain 25361 images .
1272	Define the number of repetitions for each class
713	Per Capita Features
791	It seems that the new features helped both the random forest and the linear regression . Let 's take a look at the random forest feature importances .
308	Using my notebook
700	One of the most important steps of exploratory data analysis is finding missing values in the data and determining how to handle them . Missing values have to be filled in before we use a machine learning model and we need to think of the best strategy for filling them in based on the feature : this is where we 'll have to start digging into the data definitions . First we can look at the percentage of missing values in each column .
1239	Perspective of analysis Sales can be reponsive to time factor and space factor Store 's sales records are the aggregation of each department Date variable can be split into y/m/w/d variables Day variable can provide much information on sales Outside data such as national holiday of US will be combined to add information
1092	There are no clear difference in importance among features If the random seed changes , the feature importance will also change ( The top importance feature always change when I change the random seed
123	FVC Progression by SmokingStatus
815	The Bayes optimization spent many more iterations using the ` dart ` boosting type than would be expected from a uniform distribution . We can use information such as this in further hyperparameter tuning . For example , we could use the distributions from Bayesian hyperparameter optimization to make a more focused hyperparameter grid for grid or even random search . this chart , we can also make it in Altair for the practice .
579	Comparison between Brazil and Italy
801	In Hyperopt , we can use nested conditional statements to indicate hyperparameters that depend on other hyperparameters . For example , the `` goss '' ` boosting_type ` can not use subsampling , so when we set up the ` boosting_type ` categorical variable , we have to set the subsample to 1.0 while for the other boosting types it 's a float between 0.5 and 1.0 .
42	The metric for this competitiomn is Spearman Correlation , and we will define it here for later use
355	FS with SelectFromModel and LinearSVR
545	Correlation Analysis
1288	Show influence of economical factors on housing prices
677	Affected Surface Object
379	The core principle of AdaBoost is to fit a sequence of weak learners ( i.e. , models that are only slightly better than random guessing , such as small decision trees ) on repeatedly modified versions of the data . The predictions from all of them are then combined through a weighted majority vote ( or sum ) to produce the final prediction . The data modifications at each so-called boosting iteration consist of applying N weights to each of the training samples . Initially , those weights are all set to 1/N , so that the first step simply trains a weak learner on the original data . For each successive iteration , the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data . At a given step , those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased , whereas the weights are decreased for those that were predicted correctly . As iterations proceed , examples that are difficult to predict receive ever-increasing influence . Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence . Reference [ sklearn documentation
917	Monthly Cash Data
518	Accuracy that could be achieved by always predicting the most frequent class . This means that a dumb model that always predicts 0/1 would be right `` null_accuracy '' % of the time .
1403	Moving average is so simple
349	How to make a generator run infinitely
12	Now we 'll load the Quora datasets
1024	Create fast tokenizer
1058	Now let 's plot this against every K to see the decrease
108	Credit : The code has been adapted from @ mgornergooglestater kernel on Flower classification on TPUs
443	There are considerable differences between building types as to when meter readings are highest . Almost all the building peak in the end of the year due to winter season . The trend holds for most of the different building types , with a few notable exceptions ; Manufacturing dips during that peak period outlined above , while Services , Technology , Utility and Warehouse remained fairly constant over the year .
370	Linear SVR is a similar to SVM method . Its also builds on kernel functions but is appropriate for unsupervised learning . Reference [ Wikipedia
650	missing value statistics
470	Librairies and data
1163	targets in train.csv
1118	To win in kaggle competition , how to evaluate your model is important . What kind of cross validation strategy is suitable for this competition ? This is time series data , so it is better to consider time-splitting . However this notebook is for simple tutorial , so I will proceed with KFold splitting without shuffling , so that at least near-term data is not included in validation .
1416	The colour of monsters does n't seem to be essential . Maybe it disturb the classification . Let 's classify without colour ...
180	A quick glance reveals two problems ( in this very simple image There are a few individual pixels that stand alone ( e.g . top-right Some cells are combined into a single mask ( e.g. , top-middle Using ` ndimage.find_objects ` , we can iterate through our masks , zooming in on the individual nuclei found to apply additional processing steps . ` find_objects ` returns a list of the coordinate range for each labeled object in your image .
751	As a final exploration of the problem , we can apply a few different dimension reductions methods to the selected data set . These methods can be used for visualization or as a preprocessing method for machine learning . We 'll look at four different methods PCA : Principal Components Analysis ] ( Finds the dimensions of greatest variation in the data ICA : Independent Components Analysis ] ( Attempts to separate a mutltivariate signal into independent signals . TSNE : T-distributed Stochastic Neighbor Embedding ] ( Maps high-dimensional data to a low-dimensional manifold attempting to maintain the local structure within the data . It is a non-linear technique and generally only used for visualization . UMAP : Uniform Manifold Approximation and Projection ] ( A relatively new technique that also maps data to a low-dimensional manifold but tries to preserve more global structure than TSNE . All four of these methods are relatively simple to implement in Python . We 'll map the selected features down to 3 dimensions for visualization and then also use PCA , ICA , and UMAP as features for modeling ( TSNE has no ` transform ` method and hence can not be used for preprocessing ) .
1123	make hour column from transactionDT
666	Combine these encoded dataframe and notice that I 've added a term ( retain_full $ ^2 $ ) which will help me to archive higher score ( but also reduce the training speed
276	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
630	Building interactive charts for hotel clusters , 5 clusters per chart
987	DICOM-CT data 3D visualizations
793	Visualize Validation Predicted Target
495	Importing The Dataset
881	Here there appears to be a clear trend : a lower learning rate leads to higher values ! What does the plot of just learning rate versus number of estimators look like
957	Test prediction
748	To resume training , we can pass in the same trials object and increase the max number of iterations . For later use , the trials can be saved as json .
970	load mapping dictionaries
274	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
1187	process test images
251	Let 's try to see results when training with a single country Spain
1539	Label encoding Making it machine readable
1566	Submission
461	Encoding City + Temperature ( °F ) + Rainfall ( inches
249	Implementing the SIR model
768	Based on these values , we can remove outliers . This is somewhat based on intuition and there might be a more accurate process for carrying out this operation ! Here is another potential point for improvement Potential improvement 2 : experiment with different methods to remove outliers.__ This could be through domain knowledge ( such as using a map ) or it could be using more rigorous statistical methods ( such as z-scores ) .
475	Submission
1444	We know that the proportion of clicks that was attributed is very low . So let 's say we want to look at all of them at the same time . We do n't know what rows they are , and we ca n't load the whole data and filter . But we can load in chuncks , extract from each chunk what we need and get rid of everything else The idea is simple . You specify size of chunk ( number of lines ) you want pandas to import at a time . Then you do some kind of processing on it . Then pandas imports the next chunk , untill there are no more lines left . So below I import one million rows , extract only rows that have 'is_attributed'==1 ( i.e . app was downloaded ) and then merge these results into common dataframe for further inspection .
624	Predict on test set
1077	Note KFold is a very common cross-validation method . It will split your original training data into several pieces ( depends on the n_splits parameters ) . And use 1/n_splits portion of data to be the validation data and rest of data to be training data . But this validation data will keep change until it go through all your original training data , for instance original data = [ 1,2 , n_splits training validatng data training data : [ 2 , training validating data training data : [ 1 , training validating data training data : [ 1 , But we also know , it is not a good idea to blend the training data with validating data . So We can use several models to train on each different spliting situation . After training , we will have n_splits models , which all of them are training on ( 1-1/n_splits ) of original training data and validating on 1/n_splits of original training data . So all of these models were training on different ( partially different ) datasets . Then we can use these models to predict the data we want to predict , thus we get n_splits different prediction results . In the end , we can blend these results by voting or averaging them and get a much stable result than use only one model .
916	We spent quite a bit of time developing two functions in the previous notebook agg_numeric ` : calculate aggregation statistics ( ` mean ` , ` count ` , ` max ` , ` min ` ) for numeric variables . agg_categorical ` : compute counts and normalized counts of each category in a categorical variable . Together , these two functions can extract information about both the numeric and categorical data in a dataframe . Our general approach will be to apply both of these functions to the dataframes , grouping by the client id , ` SK_ID_CURR ` . For the ` POS_CASH_balance ` , ` credit_card_balance ` , and ` installment_payments ` , we can first group by the ` SK_ID_PREV ` , the unique id for the previous loan . Then we will group the resulting dataframe by the ` SK_ID_CURR ` to calculate the aggregation statistics for each client across all of their previous loans . If that 's a little confusing , I 'd suggest heading back to the [ first feature engineering notebook
953	Initialize train and test DataFrames to access IDs and depth information .
955	Perform stratified train/valid split based on coverage class
1026	Build datasets objects
1158	Train Our Linear Regression Model
927	Read the data
960	Create DFs imitating public and private test subsets
1417	Logistic Regression seems to be a good classification algorithm for this dataset .
363	Get the dupplicate clicks with different target values . Can not simply drop duplicate from 'dup ' because there are some clicks with more than 2 duplicates .
264	Tikhonov Regularization , colloquially known as Ridge Regression , is the most commonly used regression algorithm to approximate an answer for an equation with no unique solution . This type of problem is very common in machine learning tasks , where the `` best '' solution must be chosen using limited data . If a unique solution exists , algorithm will return the optimal value . However , if multiple solutions exist , it may choose any of them . Reference [ Brilliant.org
348	What is a python generator
286	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
610	Build the Model
718	For the most part , the two methods of calculating correlations are in agreement . Just out of curiousity , we can look for the values that are furthest apart .
282	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
1515	Let 's map index
10	Impute any values will significantly affect the RMSE score for test set . So Imputations have been excluded
529	x_val = np.reshape ( x_val , ( x_val.shape [ 0 ] , x_val.shape [ 1 ] , x_val.shape [ 2 ] ,
195	Method for get TSNE 2D axis
87	Our idea is to run simultaneously many parallel linear regressions for every id ( the 42k series ) and use the resids to estimate quantiles . Let imagine that we have $ N $ series of length $ T $ in a vector $ Y $ with shape $ ( N , T ) $ and covariates of temporal features $ Z $ with shape $ ( T , F ) $ . For id $ i $ we will run the following regression Y_i^T = Z \beta_i^T + \epsilon_i So if we compact all the individual OLS parameters $ \beta_i $ in a vector $ \beta $ . The global model will be written as Y = \beta Z^T + \epsilon STEP 1 : PARAMETER OPTIMIZATION Therefore we can solve the following problem including regularization hat { \beta } _ { \lambda } = \arg\min \mid \mid Y - \beta Z^T \mid \mid_F^2 + \lambda \mid \mid \beta \mid \mid_F It is easy to see that hat { \beta } _ { \lambda } = YZ \left ( Z^T Z + \lambda I_F \right STEP 2 : QUANTILE PREDICTION we have compute then the resid hat { Y } ^ { \lambda } = \hat { \beta } _ { \lambda } Z^T , \quad \hat { \epsilon } = Y - \hat { Y } ^ { \lambda And we finally compute the quantile $ q $ by hat { Y } _ { it } ^q =\hat { Y } ^ { \lambda } _ { it } + quantile ( \hat { \epsilon } _ { it } , q lambda $ can be learned by cross-validation . Now let 's code it out
1368	IsProtected - This is a calculated field derived from the Spynet Report 's AV Products field .
737	As one more attempt , we 'll consider the ExtraTreesClassifier , a variant on the random forest using ensembles of decision trees as well .
1201	Train LSTM with 3 epochs
1223	let 's run the binary encoding function
568	using VarianceThreshold to remove all low-variance features
246	From notebook My upgrade : structure of model
910	We need to align the testing and training dataframes , which means matching up the columns so they have the exact same columns . This should n't be an issue here , but when we one-hot encode variables , we need to align the dataframes to make sure they have the same columns .
1347	The mutivariate kde plot of not-repay is broader than one of repay . For both CNT_60_SOCIAL_CIRCLE and OBS_30_CNT_SOCIAL_CIRCLE , the distribution of each repay and not-repay is a bit different . Log-operation helps us to see them easily .
661	High cardinality features
728	We can also look at the difference in average education by whether or not the family has a female head of household .
502	Merging the bureau dataset along with application train dataset to do more analysis
458	Intersection ID
17	Let 's load up all the individual predictions
95	Mutation and cell seems to be commonly dominating in all classes , not very informative . But the graph is still helpful . And would give more insight if we were to ignore most common words . Let 's plot how many times 25 most common words appear in the whole corpus .
1061	filtered_sub_df ` contains all of the images with at least one mask . ` null_sub_df ` contains all the images with exactly 4 missing masks .
226	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
949	Group merchant data by merchant_id
708	Creating Ordinal Variables
899	Feature selection ] ( is an entire topic to itself . However , one thing we can do is use the built-in featuretools [ selection function to remove ] ( columns that only have one unique value or have all null values .
1410	Predict null data based on statistical method
679	Due to Kaggle 's disk space restrictions , we will only extract a few images to classify here . Keep in mind that the pretrained models take almost 650 MB disk space .
595	Most important or common words in neutral data
984	Load Packages and Data
886	Properly Representing Variable Types
1193	Image Loading & Pre-processing
1121	As you see : The Neutered pets are accepted mostly.The points should be on the Neutereds
352	Selection part of data for automatic FE - 10000 meters and it 's preprocessing
1474	this is the function that sets 75 % of the sirnas to zero according to the selected assignment
1588	But that does not say us much , we want to know exactly what asset codes are not to be found in the news data set
999	Session level CV score : 1 . User level CV score : 1 .
1271	Get labels and their countings
1532	See the variables ' correlation with target
464	Data Section 1 - The Basics
277	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
754	Visualize Tree with No Maximum Depth
1388	Census_OSUILocaleIdentifier - NA
1008	Loading the 32x32 dataset
1152	Some imports ( as usual
197	In 2D-Visualization it is difficult to picture which section is which part of RNA . To make it easier , we will generate a Graph Structure . The neato method can take that as input and create a nice visualization of the graph
1440	TIP 2 Presetting the datatypes
122	FVC Progression by Sex
1480	Learning and giving a score on the training dataset . I used high learning rate value here in order to fight overfitting to some degree . This kind of configuration without any regularization , without even validation set to keep an eye on it , has a high potential for overfitting . But using only 10 epochs and high learning rate gives decent results out of sample .
706	Redundant Household Variables
1250	Compare timing for MixUp
196	Visualize RNA-2D Structure
1183	Create Image Augmentation Generator
1241	Stores table consists of three columns 1. store ID 2. store type 3. store size . A , B , and C are the types of stores and total 45 Walmart stores are selling goods to customers
1455	generate pred_str for submission workflow ( ) is an arbitrary function representing all other steps
603	Public score seems to be usually higher than a private score ( slightly left-skewed distribution ) . Let 's see inverse cumulative plot of absolute difference .
537	Pitch Pitch is a perceptual property of sounds that allows their ordering on a frequency-related scale , or more commonly , pitch is the quality that makes it possible to judge sounds as `` higher '' and `` lower '' in the sense associated with musical melodies . Pitch can be determined only in sounds that have a frequency that is clear and stable enough to distinguish from noise . Pitch is a major auditory attribute of musical tones , along with duration , loudness , and timbre . reference
1415	It seems Goblins are a little similar to Ghouls .
289	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
1041	Unhide below to see all trials results
1333	Time feature
1174	Making Feature Matrices
232	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
369	Support Vector Machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis . Given a set of training samples , each marked as belonging to one or the other of two categories , an SVM training algorithm builds a model that assigns new test samples to one category or the other , making it a non-probabilistic binary linear classifier . Reference [ Wikipedia
183	Firstly , we 'll check some general informations about our dataset
309	How many images are in each folder
1357	AVProductStatesIdentifier - ID for the specific configuration of a user 's antivirus software
944	load mapping dictionaries
1423	Predict by Specify Province
280	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
46	This is a highly skewed distribution , so let 's try to re-plot it with with log transform of the target .
55	So as we suspected , almost 97 % of all values in the train dataframe are zeros . That looks pretty sparse to me , but let 's see how much variation is there between different columns .
749	For the test predictions , we can only compare the distribution with that found on the training data . If we want to compare predictions to actual answers , we 'll have to split the training data into a separate validation set . We 'll use 1000 examples for testing and then we can do operations like make the confusion matrix because we have the right answer .
299	Thanks to March Madness 2020 NCAAM EDA and baseline March Madness 2020 NCAAM : Simple Lightgbm on KFold
986	Modeling : Xgboost
653	Over-fitting ? Try to use less features or use more samples to fit modle .
763	Read in 5 million rows and examine data
105	Loading data etc .
829	We can keep only the features needed for 95 % importance . This step seems to me to have the greatest chance of harming the model 's learning ability , so rather than changing the original dataset , we will make smaller copies . Then , we can test both versions of the data to see if the extra feature removal step is worthwhile .
1081	Display the dropped images
291	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . GaussianNoise_stddev = 0 . LB = -6 .
480	The kernel is going to fail as I am using python 2.7 with pandas 0.19 installed . To install lightgbm in python , one can follow the steps in this link
1397	Census_IsTouchEnabled - Is this a touch device
1358	AVProductsInstalled - NA
982	This images from validation data seem to be really strange labeled ....
188	Top 10 brands by number of products
52	Only marginal improvement - there is a verly small bump close to 15 . Can the violin plot help
911	We can calculate not only the correlations of the variables with the target , but also the correlation of each variable with every other variable . This will allow us to see if there are highly collinear variables that should perhaps be removed from the data . Let 's look for any variables that have a greather than 0.8 correlation with other variables .
1023	Now that we have pretty much saturated the learning potential of the model on english only data , we train it for one more epoch on the ` validation ` set , which is significantly smaller but contains a mixture of different languages .
1267	Check saved checkpoints
66	Plot feature importance ( from [ here Interesting , it looks like by reindexing the data , the feature importance has changed .
410	Let 's check duplicates in the test dataset
503	analyzing the numerical features disturbion in previous application dataset
75	Defining DataBunch for FastAI
590	Lets gets started
1475	I had several spare submissions so I tried to hunt down the public test errors that were plaguing the village . The conclusion so far is that there are 27 corrupted videos in the public test , and the failures for me are specifically at ` Image.fromarray ` call . Some more details below .
1569	Examine how many phases with one ID were labelled problematic . For 80.4 % faulty lines , three phases were all labelled faulty , while one-faulty-phase and two-faulty-phase lines contribute about 10 % and 10 % , respectively .
155	Check out your own CUDA version , version below is Google Colab 's version .
152	Create and set up the model
576	Now , let 's take a look at Brazil
311	Balance the target distribution
774	The test distribution seems to be similar to the training distribution . As a final step , we can find the correlations between distances and fares .
254	Albania
121	There does n't seem to be any particular correlation between the categorical variables . The continous variables exhibit a clear positive correlation , between `` FVC '' and `` Percent '' , while the new engineered feature FVC Difference has an almost perfect negative correlation with Percent .
1175	Count game trainsition
426	This notebook is highly inspired by this work [ M5 First Public Notebook Under 0.50 ] ( but with [ CatBoost
1184	Imports & Settings
1441	This is how you can do your own random sampling Since 'skiprows ' can take in a list of rows you want to skip , you can make a list of random rows you want to input . I.e . you can sample your data anyway you like Recall how many rows the train set in TalkingData has
1095	Plotting errors for one sample .
231	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
535	I am trying to share some knowledge that I acquired while researching for this competition . Librosa is a really powerful python library that can be used for retreving lot of meaningful information from audio files . I am showing few for those here . Hope you find it helpful .
980	Let 's go ahead and see how it looks in our file . By default pydicom reads in pixel data as the raw bytes found in the file
800	We can visualize the learning rate by drawing 10000 samples from the distribution .
453	Imputing Missing variable
304	LGB
602	Let 's plot the distribution of difference between public score and private score .
439	Meter Reading and Meter Type
312	Set Up the Generators
582	Iran since first case
1106	Leak Data loading and concat
1575	In this part we will use Keras without optimisation to forecast . It is just a very simple code to begin with Keras and a Time Series . For our example we will try just with one layer and 8 Neurons .
795	Evaluate Best Model from Random Search
101	Apply Underbalancing Techinique
1212	Make a Baseline model
640	Shuffle part of predictions to simulate a non-overfit model with realistic accuracy
922	Let 's declare variables with those attributes and visualize
1473	In the code below we load the model , make predictions to get the full probabilites matrix , and set 3 out of 4 plates for every sirna to zero , according to the assignment that we previously selected .
909	Calculate Information for Testing Data
160	Hist Graph of scores
1258	Choose the model to use
177	Dealing with color
565	Get predictions
76	Define F1 metric
652	y的异常值 drop samples which have exception value in y
2	Now , let 's fit the model
1245	The median of A is the highest and C is the lowest That means stores with more sizes have higher sales record ( The order of median of size and median of sales is the same
1498	Solve the task
608	Data Prep — Tokenize and Pad Text Data
908	Aggregated Stats of Bureau Balance by Client
298	Prepare Training Data
33	Text preprosesing source
237	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
295	Average prediction
722	We can make a few features using the existing data . For example , we can divide the years of schooling by the age .
948	no NaN in train and test , that 's good some NaNs in both merchants DF , especially ` category_2 ` feature .
72	The Shape of the Data
239	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
654	y hist with defferent timestamps are similar .
1331	After looking the feature , simply I think that the category feature can be divided into youtube , google , other categories .
202	Pretty cool , no Anyway , when you want to use this mask , remember to first apply a dilation morphological operation on it ( i.e . with a circular kernel ) . This expands the mask in all directions . The air + structures in the lung alone will not contain all nodules , in particular it will miss those that are stuck to the side of the lung , where they often appear ! So expand the mask a little This segmentation may fail for some edge cases . It relies on the fact that the air outside the patient is not connected to the air in the lungs . If the patient has a [ tracheostomy ] ( this will not be the case , I do not know whether this is present in the dataset . Also , particulary noisy images ( for instance due to a pacemaker in the image below ) this method may also fail . Instead , the second largest air pocket in the body will be segmented . You can recognize this by checking the fraction of image that the mask corresponds to , which will be very small for this case . You can then first apply a morphological closing operation with a kernel a few mm in size to close these holes , after which it should work ( or more simply , do not use the mask for this image ) . pacemaker example Normalization Our values currently range from -1024 to around 2000 . Anything above 400 is not interesting to us , as these are simply bones with different radiodensity . A commonly used set of thresholds in the LUNA16 competition to normalize between are -1000 and 400 . Here 's some code you can use
1341	Wall material
871	We can calculate the number of top 100 features that were made by featuretools .
1359	AVProductsEnabled - NA
230	dropout_model = 0. hidden_dim_first hidden_dim_second hidden_dim_third LB = 0 .
682	In fact , this is what ~ 2 % of the columns look like . The tranining ( and test ) data set has a massive 4,990 numeric features + ID and target columns . The ` ID ` column is the bank customer 's identification value . The test data is about 10 times the size of the training data . Here is their shape
272	Dropout_model = 0 . FVC_weight = 0 . Confidence_weight = 0 . LB = -6 .
1349	As you can see , repay have a litter more right-skewed distribution . To see more deeply , Let 's divide the overdue feature into several groups .
1177	View Single Image
660	First , consider these figures as categorical features . I will plot some simple bar charts using seaborn 's countplot
1311	preprocess data 1 - generate shift features
314	Create a Classification Report
609	Create the embedding layer
662	Ordinal features mapping
1375	Census_OEMModelIdentifier - NA
1456	Import modules
127	Lung Volume/Total Lung Capacity Estimation
479	Submission
1233	Level 2 Random Forest
412	However as it stated in [ the comment Salt is very difficult to locate accurately , and if faced with ambiguity , a geophysicist will draw the simplest possible polygon to define it . Since salt can play havoc on time to depth conversions ( velocity models ) , you can get some really weird vertical features in the seismic We should carefully select the vertical masks
1016	Refit and Submit
512	Spreading the Spectrum From the histogram , you can see that most pixels are found between a value of 0 and and about 25 . The entire range of grayscale values in the scan is less than ~125 . You can also see a fair amount of ghosting or noise around the core image . Maybe the millimeter wave technology scatters some noise ? Not sure ... Anyway , if someone knows what this is caused by , drop a note in the coments . That said , let 's see what we can do to clean the image up . In the following function , I first threshold the background . I 've played quite a bit with the threshmin setting ( 12 has worked best so far ) , but this is obviously a parameter to play with . Next we equalize the distribution of the grayscale spectrum in this image . See this tutorial if you want to learn more about this technique . But in the main , it redistributes pixel values to a the full grayscale spectrum in order to increase contrast .
1075	Note From the print out data above . We will know The first column is the label of dataset . Other columns are the digital value of pixels of kannada mnist . And there are 784 columns of digital value in 1 row . The dataset is the same as regular mnist dataset .
1465	I found that some of mismatches are caused by setting previous ` visitStartTime ` as ` visitId ` . of 898 duplicates are caused by the mismatches .
1044	For each sample , we take the predicted tensors of shape ( 107 , 5 ) or ( 130 , 5 ) , and convert them to the long format ( i.e . $ 629 \times 107 , 5 $ or $ 3005 \times 130 ,
1011	Padding process and resizing with OpenCV
40	Wow , AUC of 0.9999 is as large as it gets ! Let 's see which columns are the most responsible for this discrepancy .
442	Primary Use and Meter Reading
804	Let 's test the objective function with the domain to make sure it works . ( Every time the ` of_connection ` line is run , the ` outfile ` will be overwritten , so use a different name for each trial to save the results .
256	The analysis showed that many values ​​are only available in stations 1 and 2 , while others have much less data . We propose that at the start code , the BOD5 prediction should be carried out only for data from the first two stations
1314	edjefe , years of education of male head of household , based on the interaction of escolari ( years of education ) , head of household and gender , yes=1 and no replace yes - > 1 and no
1523	Similar to validation , additional adjustment may be done based on public LB probing results ( to predict approximately the same fraction of images of a particular class as expected from the public LB .
1433	We still have a lot of missing value , so we will use lightgbm regressor to impute two important features : apache_4a_icu_death_prob and apache_4a_hospital_death_prob
862	The public leaderboard score is only calculated on 10 % of the test data , so the cross validation score might actually give us a better idea of how the model will perform on the full test set . Usually we expect the cross validation score to be higher than on the testing data , but because of the small size of the testing data , this might be reversed for this problem . Next , we will make predictions on the test data that can be submitted to the competition .
21	Now there appears to be one feature that is not gaussian
1211	card_id : Card identifier month_lag : month lag to reference date purchase_date : Purchase date authorized_flag : Y ' if approved , 'N ' if denied category_3 : anonymized category installments : number of installments of purchase category_1 : anonymized category merchant_category_id : Merchant category identifier ( anonymized subsector_id : Merchant category group identifier ( anonymized merchant_id : Merchant identifier ( anonymized purchase_amount : Normalized purchase amount city_id : City identifier ( anonymized state_id : State identifier ( anonymized category_2 : anonymized category
1067	Load Test dataframe
1219	In this problem I think it 's better not to use the same learning rate during all the training , so let 's make it decrease after each epoch
179	Deriving individual masks for each object
