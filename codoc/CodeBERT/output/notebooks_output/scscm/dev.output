1309	Sample a batch
228	Normalize all ensemble values
51	Extracting the edges
1518	Get Training dataset
563	Average Length , Tokenization
501	show the graphs
457	from sklearn.preprocessing import auc
285	Separate features by target
1508	loop for one iteration
209	FIND ORIGIN PIXEL VALUES
1385	suppose all instances are not crowd
1516	Detect hardware , return appropriate distribution strategy
1116	Returns the counts of each type of rating that a rater made
178	Find Average price over categories
1209	Save model and weights
864	Fitting the model
65	save pneumonia location in dictionary
61	Function to extract sex
191	Display the scatterPlot between Description Length and Price
447	Encoding the Regions
476	Predict the text
1034	Get unique values
1232	Load Train , Validation and Test data
54	Print the trip duration
1149	Preprocess the training data
407	Predict for each variable
1466	Turn off gradients
1330	Encodes a list of BlockArgs to a list of strings
1436	Number of Patients and Images in Training Images Folder
1808	Creation of the number of transactions
859	run randomized search
451	Only the classes that are true for each sample will be filled in
919	Unpack Cash Data
1206	Load the pretrained model
569	Order by hour
1657	get the sentences
1780	The wordcloud of the raven for Edgar Allen Poe
13	Load train and test data
1554	Save the model with new parameters
1650	Evaluating the model
326	Initialize patient entry into parsed
1429	Mapping the training data
865	Set some hyperparameters
696	Exclude background from the analysis
1786	load a piece of dataframe
318	Decision Tree Regression
440	Top Leaderboard Paths
1563	Predict on test dataset
689	functions to show an image
1811	Plot the fit
189	Lenght Length
778	select high correlation columns
198	Otsu Method for Thresholding
735	Classify an image with different models
1735	make a color palette
704	MODEL AND PREDICT WITH QDA
1236	Display the text
541	Run grid search
1652	Load Train and Test Data
88	Example to show results
1494	Get predictions on test data
940	Show the results
1098	Create LightGBM data structures
255	extra trees regressor
775	Plot the heatmap
161	move label to zero
1130	Create Data Loader
600	lifes the mask
1698	Evaluate the model
1287	Function to display shuffle samples
1266	Build test set
740	Adding random line
1182	Weighted distribution
393	Best Optimal features
1442	Process the sub images
142	get the data fields ready for stacking
93	Set the next b and update the index
1354	Fast data loading
466	Plot ROC Curve
1583	MAKE CUTMIX LABEL
592	Read the data
163	RLE encoding , as suggested by Tadeusz Hupa≈Ço
1800	plot distribution of predictions vs Keras
206	CONVERT DEGREES TO RADIANS
1769	SAVE DATASET TO DISK
1776	import the libraries
928	for random building
1301	Get the test set
1708	Importing standard libraries
747	train all layers
333	Read data and merge
758	There are other household members
727	Beauty and Kids
429	Convert year to uint
1372	Submit test predictions
546	Create a pipeline
1437	Number of Patients and Images in Test Images
1399	converting the numpy array to log
1327	Convolutions like TensorFlow , for a fixed image size
146	How many different values are unique
1247	to truncate it
1300	Preprocess text
350	select one feature
1093	use salt parameters here
1493	Compute valid predictions
1815	Create LGBM dataset
334	Print dimensions of dataframe
946	altair is a very nice plotting library by the way
777	map lower case
552	Compute best accuracy
1310	Get feature importances
1409	Sample main variable
1140	Plotting the correlation plot
449	Prepare X and Y
1402	Else cutoff the input data
664	Plotting the average bookings
1573	There are no
1589	to truncate it
114	the rest of the dates
469	Data processing , metrics and modeling
1683	make the polygons with seaborn
1804	Plot Receiver Operating Characteristic
1648	Evaluate on validation data
646	Evaluate for each type
821	Create a random forest graph
548	Compute best accuracy
135	Fbeta score
432	Generate exponential predictions
1161	FIND ORIGIN PIXEL VALUES
1470	Now scale the values
644	Check submission class distribution
435	Visualizing the string
1342	calculate the mean of each signal
1022	remove the column list
810	Store the best estimators
1316	add column features
939	Predict results in dataframe
292	Make a prediction
542	Compute best accuracy
1813	summarize history for loss
505	Plot with transaction revenue
1525	Span logits minus the cls logits seems to be close to the best
1796	Set date index
1103	Loading the data
538	Compute best accuracy
1529	Read candidates with real multiple processes
1197	Create submission dataframe
877	Checking best parameters
1195	Make a prediction
817	so we can create the algorithm
741	Local density transformation
1707	Load and build model
283	Perform a sample of labels
1043	Print some summary information
1010	Store the column name
186	so we can check if prices paid
1547	save the dataframe
96	Plot before array values
224	create the best weight column
313	Label Encoding the categorical variables
1285	Load best model weights
327	Add box if opacity is present
1622	Print the feature ranking
1393	Converting CATEGORICAL columns to CATEGORICAL columns
1805	define memory usage
1221	Find the best score
130	Look at how data generator augment the data
788	Create a function to show the important features
781	creating the level columns
1220	Drop nuisance columns and specify target variable
958	Peek into the data
1083	Read train and test data
514	Make a summary column
1133	Pad the predictions
23	Detect and Correct Outliers
1785	Avoid division by zero by setting zero values to tiny float
1476	MAKE CUTMIX LABEL
234	Filter selected features
1396	Prepare variable data
1099	predict on validation set
1537	Analyzing the sentence
1725	The method for training is borrowed from
1574	Setting the Paths
1312	Analyzing the dataset
1777	plot the heatmap
1819	Join market and news
601	And the final mask
890	Features correlation matrix
323	Evaluate the model
929	Check for some outliers
6	eliminate bad rows
1478	LIST DESTINATION PIXEL INDICES
1473	MAKE CUTMIX LABEL
539	Predicting with best algo
1025	Set up classifier
1560	Correlation with Macro Features
365	Fit the model
1039	Previous applications categorical features
217	MinMax scale all columns
1280	Check for an example
611	Save images to specified format
1308	Sample a batch
1623	Logistic Regression Model
1720	SAVE DATASET TO DISK
1795	Train the model
1661	Lets try to convert a lgbm file into a list
765	Display some scatter plots
1561	Encoding the columns
330	Actual batch detection
1104	Test credit card balance
1594	Tokenize the sentences
1086	Load the training set
1	resize image to desired size
1226	Plotting some random images to check how cleaning works
663	Average year and total
1000	get custom features
39	Get a batch of images
229	Normalize all ensemble values
743	pivot to one hot encoding
629	Split USA Data
490	application histogram of average values
118	HVC line
493	Applying to the train and validation data
1692	Function for LCT function
1755	Add Basic Feature
175	Load image data
1498	Determine the variable
995	Plot the total contract type
141	plot distribution of target variable
1557	Creation of the External Marker
1090	Encoding the Test Data
1568	Pinball loss for multiple quantiles
257	Predict and Submit
262	Scatter plot of parameters
1351	Fast data loading
973	for iteration
1125	Convert to binary
338	plot and visualise the training and validation losses
1682	An optimizer for rounding thresholds
1080	Divide the result by the number of words to get the average
1242	Compute prediction result
866	select some hyperparameters
433	Ignore the warnings
1611	The categorical features
1546	extract some features from date column
1760	Label encode categorical features
1412	Binary extractor
411	Train and predict
1460	checking missing data
638	Create and generate a word cloud image
1671	Read the data
1375	Fix the missing values
1793	Visualizing Web Traffic
764	Create list of markers for scale
897	Check for cumulative importance
1059	Embedding the mask
924	Create a line
247	Apply exponential transf
507	Split the data
460	This function runs the random search
131	Prepare Testing Data
692	Load previous model from previous run
43	Extracting the edges
1204	save the fake
1134	Check validation index
471	Plot ROC Curve
1205	Convert to eval data
1789	Forceasting with decompasable model
14	Histogram of Target counts
145	Read the dataframe
1449	function to create new features
1292	train all layers
120	Function for getting the vocabulary
468	Loading the data
138	Quick look at how
64	Ploting the log
676	Store some data for
1551	Average Day of Month
1052	Train the model with early stopping
487	Reading all data into respective dataframes
570	Order by Day of the week
1370	Applying Label Encoder
994	Plot the most common client type
438	Checking for dimensions of train and test data
1577	Filter continuous features
270	Weights with black areas
1481	Histogram of main features
1169	Print dimensions of clean data
1180	Adds a new train fold
968	Function to remove low features
497	Reducing the distribution of the feature values
1531	Previous applications columns by key
833	Distribution of Fare amount
389	Check for empty images
193	make a prediction
1768	shuffling the data
1349	Leak Data loading and concat
882	Create the cross validation set
725	Model and Predictions
867	Add Hyperparameters to the hyperparameters
841	Aucide Distance by Fare amount
956	Extracting relevant features
1716	FUNCTIONS TAKEN FROM
110	Plotting sales volume per year
1379	if nothing found , look at the same direction
1338	The first block needs to take care of stride and filter size increase
1323	Parameters for an individual model block
201	Function for rendering data
124	Perform percent change in each column
824	Applies the cutout augmentation on the given image
1491	Process some examples
694	one hot encoding
223	create the best weight column
509	Evaluate threshold for thresholding
392	Resize train images
1527	Computes official answer key from raw logits
1758	Add this feature
918	several change in the data
287	Load Model Weights
1656	written by MJ Bahmani
375	Running the model
1540	Check the SMAPE score
947	replace random hyp
511	Average Season Factor
154	Visualizing the missing time features
907	Return the size of a dataframe
1127	Read the data
200	importing the libraries
103	Split into features and targets
1335	Squeeze and Excitation
1107	Load sentiment file
30	Load the data
1802	Function for reading the image
484	make a prediction
340	Convert to dataframe
832	In any case , write out the submission file
1538	Calculate actual and actual predictions
985	Check for Loans over Time
437	Printing the first few predictions
1696	This function tells the estimator to run through the given parameters
1548	Add new Features
337	Train the model
776	construct a grid
4	Remove Unused Columns
799	Train the model
543	Predicting with best algo
931	count possible combinations
584	Process by bird id
1579	MAKE CUTMIX IMAGE
1426	get max length of all strings
1138	Feature importance with SHAP
1355	iterate through all the columns of a dataframe and modify the data type
996	get seed features
317	We fit the SGD model
388	Function to get the distribution of the image
607	Return a normalized weight vector for the contributions of each class
445	Extracting informations from street features
119	The above line is copied from
1186	update the rest of the sessions
1110	Extract processed data and format them as DFs
1512	size and spacing
642	The top words is highly common
117	Formatting for submission
102	grid mask augmentation
1196	Save results in best model
976	update hypopt
1029	Create metric dataframe
1087	Encode the class
322	Evaluating for voting
116	Use the grid of parameters
1040	Merge the Data
164	Function for reading the image
380	Check for length of file
140	Make PyTorch deterministic
1218	Some time statistics
139	Use this kernel
1382	Function for cleaning the document
481	Use the Keras tokenizer
826	Read the image
245	Filter Andorra , run the Linear Regression workflow
1166	process test submission
504	A simple lineplot
1185	If for training is running
1217	Joining the title and event duration
81	to the last cell
1268	Linear Weighted Kappa
167	Only the classes that are true for each sample will be filled in
858	plot second ECDF
1346	Plot some examples
1672	Initialize patient entry into parsed
1157	numpy and matplotlib defaults
1070	Convert to RGB
647	Importing the Libraries
534	Compute best accuracy
418	Display some Images
1371	make a prediction
643	Importing the libraries
488	find categorical features
1475	MAKE MIXUP IMAGE
1686	Combine out the size of the image
268	Split into features
1569	Initialize Bayesian Optimization
1321	Set up TensorFlow config
614	Weight of the class is inversely proportional to the population of the class
936	Create LightGBM model
1428	Add PAD to the sentence
148	We can Click by IP
19	Imputations and Data Transformation
938	Write column names
1272	Here I run a quick check
1153	Show some images and validation images
204	Remove other air pockets insided body
150	some analysis on the dataset
1101	Parameters from TensorFlow
436	Printing the first few predictions
1036	First plot the average of all variables
1422	Merge the results
271	import library for fun
714	Calculate the missing values in each column
1447	Create submission file
500	Separate the zone and subject id into a df
756	Visualizing the distribution
583	Get the probabilities of all models
1632	Get confirmed and fatalities data
1566	for remaining weeks
1112	extract different column types
619	Create list of contours for each contours
1252	Load Model Weights
1339	Final linear layer
1649	Automatic feature engineering
16	To plot pretty figures
1367	Plot the districts map
1135	load timestamp features
613	import required libraries
1358	iterate through all the columns of a dataframe and modify the data type
212	Calculate the mean value of each feature
275	Training Dropout parameters
1763	Remove Missing Value
236	Filter Spain , run the Linear Regression workflow
219	Importing the required libraries
1647	Create our notebook
1775	This enables operations which are only applied during training like dropout
557	Number of times each feature
577	Get sample id
1238	Add global variables
431	Drop target and train
702	ADD PSEUDO LABELED DATA
416	load sample training data
1298	convert class labels to submission file
540	Create a pipeline
1035	returns categorical var
1605	Creating appropriate features
1752	Adding POS balance id
104	Compile and fit model
1770	missing entries in the embedding are set using np.random.normal
1299	Function that converts language to a list of strings
1521	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
566	Logistic regression model
90	fast less accurate
7	declare target , categorical and numeric columns
683	Put into dataframe
267	Remove Player Yards
1304	Delete GPU memory
536	Create a pipeline
1612	Split the train dataset into development and valid based on time
904	Clean up memory
1129	Specify the paths
875	Store parameters in dataframe
1148	Prepare image and input size
1413	It is missing values
1603	Impute missing values in the train set
1496	Compute key features
305	Load Model Weights
1117	Compute QWK based on OOF train predictions
73	create network and compiler
1723	missing entries in the embedding are set using np.random.normal
1192	fill in zero values with zero
1131	Create the dataloaders with the validation set
303	Specify parameters
880	Reading the datasets
261	Scatter plot of parameters
85	Glimpse the fake data
631	prime
746	calculate train size
1792	Plot the heatmap
732	Function to read an image
430	Treating the Categorical Data
1497	Load the pretrained model
210	Check for categorical columns
724	Handle masked columns
1146	load mapping dictionaries
1485	Process the new lines
1271	find out the objectness
316	Fit the model
1487	Train the latest checkpoint
332	Read data and load
362	Load image and return as numpy array
844	Fit the Logistic Regression
50	Do the same for official dataset
367	We fit the SGD model
680	some config values
843	Split data into train and validation
508	Evaluate threshold for thresholding
1639	Histogram of Time Series Forecasting
1784	Compute the STA and the LTA
221	Interactive CA Tool
783	Importing sklearn libraries
79	grab the image and resize it
963	specify a test set
455	draw boxes and display results
408	global count map
942	Show the results
716	Splitting test and score
625	Now lets filter the cases by day
1742	SCALE target variable
456	Basic EDA and DICOM Visualization
48	Convert the bar chart into colors
395	NumtaDB Confusion Matrix
816	It can be more efficient
672	List of models to use
1499	Train the latest checkpoint
1745	Sieve eratosthenes
571	Reorder the correlation between the day of the day
719	SAVE MODEL TO DISK
1667	Predict on test data
818	Store the Predictions in the test dataframe
1504	LIST DESTINATION PIXEL INDICES
678	using outliers column as labels instead of target column
56	Loading required fastai modules and metrics
1444	calculate square matrix
1427	This configuration defines the number of processes
1624	Data Loading and Feature Selection
1189	use the title for the train and test data
1404	summarize submission file
78	save dictionary as csv file
222	select one feature
1655	Draw the heatmap using seaborn
889	align and align
707	PRINT CV AUC
1459	checking missing data
893	Prepare the Data
1241	Load a binary image
1047	Reading loans from csv
1291	We squeeze into a single block
1653	Visualizing the target values
1532	Choose and initialize a model
1505	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
521	Join the tournament results to the training dataframe
1362	Convert to date and hour
1621	We can look at the distribution of the type
3	Reset Index for Fast Update
1064	Generate data for the BERT model
1102	Store predictions for test set
403	Train the model
745	build a dict to convert surface names into numbers
883	Evaluate Bayesian Optimization
143	Build and fit model
1544	save the dataframe
1281	Load the Dataset
1757	Creating a relationship
615	An optimizer for rounding thresholds
1038	Previous applications categorical features
633	Prepare to run
836	Naming the thumbnail
668	Load CSV files
1511	Prepare the model
605	Pads inputted signal
1388	return only bounding boxes
260	if score is nan
1506	FIND ORIGIN PIXEL VALUES
861	Split into training and testing data
1629	Ploting each country
356	Read an image
1165	Set some parameters
616	Preparing the data
831	Now our data file sample size is same as target sample size
1122	Using LGBM params from
0	Display Training Set
622	Examples for usage and testing
587	Combine Bathrooms count vs logerror
1334	Expansion and Depthwise Convolution
1341	create a line
1187	the test set
659	Do feature augmentation
952	Prepare Data for Neural Network
1469	Splitting the binary into a pandas dataframe
905	Create metric dataframe
1482	Load the required folders
1046	Join to main dataframe
969	Prepare Training and Testing
347	import required libraries
173	find unique classes
581	convert batch to Batch
1055	Create metric dataframe
686	Representing the images
1488	Eval data available for a single example
1443	calculate square matrix
635	Create the fitting model
1613	Split the train dataset into development and valid based on time
1616	Computes gradient of the Lovasz extension w.r.t sorted errors
301	save an image
1322	get train and test data
94	Create a whole vset
1715	Ensure determinism in the results
1585	fill all na as
149	Distilling Labels
932	Save the results in hyperparameters
848	Create random ForestRegressor
1178	show a look at the DICOM image
398	NumtaDB Confusion Matrix
786	Plot the important features
1012	Store the column names
1295	Prepare Training Data
499	handle .ahi files
302	Save image to destination folder
11	Compute the STA and the LTA
218	MinMax scale all the columns
870	Write column names
448	Update the data
360	Show the sample
1060	Splitting the train and validation sets
951	Reading the datasets
1457	checking missing data
1141	DataFrame for SHAP
510	process remaining batch
248	drop the target variable
934	Create model and predict
273	get lead and lags features
1199	Plot the results
1453	drop rows with NaN values
1144	Final Regression Rate
649	Prepare the embedding
906	Cumulative variance plot
1033	Select the ids of the parent variable
873	Write column names
1244	Load the data
913	get categorical variables
325	pickle file and show shape of train and test datasets
972	Plot random search results
921	Normalize the important features
530	Compute best accuracy
506	Plot boxplot of transaction revenue
567	A simple Keras implementation that mimics that of
1067	split training and validation data
992	Reconstruct new features
489	Group by t and
562	Checking Best Feature for Final Model
900	Check for shapes
158	Convert image to grayscale
585	Year of Contents
480	lets take a look at the length of the text
556	Function for extracting basic features
687	Get image and mask
654	Function to read test data
1106	Load metadata file
165	find out the threshold for the image
1668	Display catPlot between stores
308	We can grab the padding of the documents
473	Loading the data
784	Create a model
312	Now lets check the categorical columns
1640	Read input data
1625	Confirmed table
849	get feature importances
834	Principal Component Distribution
677	filtering out outliers
1111	Extract processed data and format them as DFs
954	separate the data
851	Add elapsed time to dataframe
127	Determine the dim of the categorical variables
423	Predicting on month
860	Train the model
797	convert a list of features to numpy array
40	load train and test data
779	so we can create it
1455	Reshape the test data
12	This block is SPPED UP
720	Import libs and load data
1620	checking missing data
798	Split into train and valid
1433	Check the link count
1435	display a look of .dcm extension
1118	Manually adjusted coefficients
1553	Average daily week per trip
999	For the largest regularly
1669	gather input and output parts of the pattern
558	if order is present in training set
892	Find out the missing values
1484	create a generator that processes the results
59	Unfreezing the model and checking the best lr for another cycle
796	Hyperparameters search for LGBM
688	draw a rectangle around the image
828	Read the image
1741	HANDLE MISSING VALUES
957	Reconstruct new features
1340	select a sample
1717	LOAD PROCESSED TRAINING DATA FROM DISK
55	Detect clusters in database
806	Save hyperparameters
1123	Set up LightGBM data structures
171	Cross entropy layer
1673	Add box if opacity is present
277	reorder the input data
945	iteration score ‰∏§Âàó
372	Evaluating for voting
1198	Create a submission
532	Create a pipeline
1249	save the test data
670	Dataset Transformation
1578	replace all nan with
1474	number of iterations
669	Exploring the data
691	Computes gradient of the Lovasz extension w.r.t sorted errors
1113	Subset text features
1766	cross validation and metrics
863	Fitting the model
516	Merge team Confusion
1434	Convert to ids
1637	Categorical features to output data
1604	Filling the missing values with
106	without any limitations and dtype modification
1619	checking missing data
459	This function runs the random search
1401	Apply exponential transf
82	Store test predictions
63	plot the distribution of continuous variables
1179	Create new confusion matrix
1710	Keras Libraries for Neural Networks
41	Check for missing values
1160	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
1423	Function for cleaning the sentence list
258	Predict test and Submit
1231	Instancing the tokenizer from DistilBERT model and then applying WordPeice Tokenizer
1643	Count the downloads by device
446	Encoding the Regions
1764	Remove Missing Value
524	Create a pipeline
755	Converting the mapping values
343	function to prepare the generator
1191	Distribution of Test Predictions
335	Print dimensions of dataframe
637	If you like it , Please upvote
1315	Identify some parameters
52	Convert the bar chart into colors
1571	Load the best model
768	Plot the Class values
812	Write column names
406	Remove unwanted features
155	Downloading the distribution over the day
1530	populate the above feature values
208	ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS
617	Splitting the train
1773	for numerical stability in the loss
84	Plot the class distribution
711	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
1119	Distribution inspection of original target and predicted train and test
1729	Add train leak
1592	some config values
1405	For each store
698	Applying CRF seems to have smoothed the model output
25	Load train and test data
1678	convert text into datetime
1003	Get a weighted feature matrix
216	MinMax scale all columns
887	Find the number of different bureau columns
1670	Make PyTorch deterministic
941	Prepare the results
1615	show mask class example
891	Drop unwanted columns
1201	Detect hardware , return appropriate distribution strategy
553	Predicting with best algo
990	Check for Days
1085	resize original image
1380	limits to max
549	Predicting with best algo
660	Computes and stores the average and current value
502	Rescaling the Image Most image preprocessing functions want the image as grayscale
177	Most common category in training set
1726	for numerical stability in the loss
923	Cumulative importance plot
1207	Load and preprocess data
1344	Plot some samples
1490	Read candidates with real multiple processes
1743	EXTRACT DEVELOPTMENT TEST
58	you can play around with tfms and image sizes
1806	Load Train and Test data
665	Create date column
1791	Extracting some features from date
998	Normalize the values
434	OSIC Pulmonary Fibrosis Progression Analysis
726	Lets visualise one of the training photos
529	Run grid search
697	Precision helper function
572	Checking for the remainder
565	Inference on test data
20	Imputations and Data Transformation
391	Submit test image
1595	Pad the sentences
1068	Print CV scores , as well as score on the test data
737	my first check
1373	fit the model
1400	Transform for log transformation
568	Load the Data
1259	Using original generator
246	Set the dataframe where we will update the predictions
1282	get train and test indices
1541	Quickly look at
661	get different test sets and process each
728	Load and concat images
1044	drop missing columns
458	Checking Best Feature for Final Model
811	Create a file and open a connection
17	Now extract the data from the new transactions
95	Get index for each feature
1651	Evaluating the model
226	Visualizing the model
414	Importing the librarys and datasets
708	ADD PSEUDO LABELED DATA
249	Fit the model
1053	save score for best score
679	Splitting the labels into a list
595	Create the test set
377	Print prediction logistic regression
1326	Round number of filters based on depth multiplier
1363	Function to change Europe
1751	Creating an entity mapping
352	Load and Preprocessing Steps
1689	Sort xs with maximum size
763	Plot class distribution
1816	Read the train and test data
1575	Any results you write to the current directory are saved as output
1051	Set up classifier
1297	Make predictions on test images
464	Merge datasets into full training and test dataframe
1691	Let us understand the augmentation model
1001	Get the most recent value of the column
1588	Take care of the accuracy
1277	Plot the identified objects
123	This function cleans the text
738	my first check
197	make a prediction
1397	Plotting the distribution of the var
122	Function for cleaning special characters
760	Plot heads of heads
1695	A single task
1216	Add game time stats
780	Aggregate and divide
706	MODEL AND PREDICT WITH QDA
1366	go through the directory provided
196	To plot the image
1078	Set values for various parameters
1345	Plotting some examples
495	Setting the Paths
1164	If there is only one batch
1227	Preparing the data
603	Encoding the Data
1736	plot the distribution of the numerical values
537	Run grid search
1142	Calculate the importance feature
289	BanglaLekha Classification Report
1115	Check if columns between the two DFs are the same
852	so that we can compare
1679	get some sessions information
1688	Sort by pixelmap
232	Predict for ConfirmedCases and Fatalities after
369	Prepare Training and Validation Sets
183	get brand name dataframe
309	Create an embedding matrix of words in the dataset
1552	Average Day of Year
1522	FIND ORIGIN PIXEL VALUES
1317	Set up TensorFlow config
129	See sample image
280	Visualizing the clusters
46	Group by month
1121	Extract target and remove them
1468	Check for maximum feature values
299	Save image and resizing
1706	Get the best candidate score
949	Plot Learning Rate
653	Function to load training data
1283	drop the labels
770	CONVERT TO HYPERPLES
1011	Create categorical variables
105	without any limitations and dtype modification
1328	Gets a block through a string notation of arguments
1190	Combine event codes into a list
1279	Plot the identified objects
291	visualize test images
1170	Prepare the data for modeling
1600	Detecting the correlated pairs
1137	Month , Year vs
348	Highlight correlation matrix
188	Generating a wordcloud
1077	Remove stop words
1089	Check for test predictions
1523	Distribution of Oversampled examples
1175	add the plane
1451	make a prediction
1520	LIST DESTINATION PIXEL INDICES
66	load and shuffle filenames
410	One VsRest Classifier
503	scale pixel values to grayscale
75	create train and validation generators
590	Gaussian Target Noise
1690	Sort xs by descending length
1374	Drop target , fill in NaNs
1797	Plot rolling statistics
1072	Access the image
152	How many recent Clickers
830	Plot the surface class distribution
576	params we will probably want to do some hyperparameter optimization later
311	Prepare the data
87	Generate fake data
254	Still a very high AUC
121	Check the current comment
1042	Sort the table by percentage of missing descending
782	creating the level columns
426	Distribution of square feet
620	Create list of contours for each contours
610	Create full speech directory
809	Train the model
1258	Get Resizing Images
231	Merge train and test , exclude overlap
794	Getting the selected features with ranking
535	Predicting with best algo
1274	is there a background
461	This function runs the random search
453	draw a line
304	Class weights are extracted from
1239	Run the algorithm
602	Preparing the data
439	TOPIC IDS
1091	save the prediction
582	Get probabilities for all models
1223	Predicting with grid search
1803	Clean the image
624	Selecting the Final Data
757	Check label counts
101	load the image file using cv
1762	checking missing data
1810	Importing the metrics
640	MosT common stopwords
1057	apply transforms to image
1535	Loading Train and Test Data
1263	Using original generator
83	Load training data
160	Visualize Nuclei cells with random labels
1441	Process the test data
1296	Pixel Normalization and Image Augmentation
1026	Train the model with early stopping
76	load and shuffle filenames
874	Fit the model on test data
2	Add new Features
1471	Order does not matter since we will be shuffling the data anyway
894	Calculate feature importances
705	ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I
608	Create full speech directory
1224	select proper model parameters
298	Save image and resizing
33	prophet expects the folllwing label names
237	Filter Spain , run the Linear Regression workflow
295	Create binary target column
723	Mapping ordinal Features
961	Get the feature map
988	Take a look at the test data
987	Previous Loan Factor
72	define iou or jaccard loss function
703	STRATIFIED K FOLD
239	Filter Italy , run the Linear Regression workflow
1065	Model Hyper Parameters
1438	Create generator for data augmentation
1421	Convert to lists
879	Draw all the columns
965	reset index to conform to how kagglegym works
1155	Create strategy from tpu
823	Custom Cutout augmentation with handling of bounding boxes
202	Determine current pixel spacing
993	get interesting features
1458	checking missing data
1378	Generate some random labels
230	Implementing the SIR model
1606	Creating appropriate features
912	Create aggregate data generator
272	basic training configuration
1543	What is the SMAPE scores
1432	calculate the link count of each node
885	import seaborn as sns
169	Batch normalization layer
1009	for numerical features
314	drop the target variable
609	Save images to specified format
766	Add a legend and informative axis label
1056	Split into train and validation sets
1790	Importing the libraries
1253	A model is saved every training error
1151	create data generator
479	A very simple processing of text
1778	Import Train and Test dataset
909	Select the ids of the parent variable
1641	Imbalanced dataset Check
